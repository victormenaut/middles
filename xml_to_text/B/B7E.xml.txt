

COMMENT
Tropical downpour
NO DOUBT every university, college and school throughout the land thinks, when a government wields its axe, that it is a special case, All of them, of course, are special in one way or another, either through their history, their location, or their local connections.
Some have developed lusty connections with industries of one sort or another, and some have developed special international relationships.
But a few are more of a special case than others, and the London School of Hygiene and Tropical Medicine is one.
Yet, what, one might ask, is a tropical institute doing in central London in the 1980s when Britain's influence and claims in those climes are largely reduced to a few dots on the global map?
Likewise, wasn't hygiene something Lord Lister and others sorted out in the 19th century?
Isn't the school just an anachronism?
The ‘London School of Hygiene and Tropical Medicine’(LSHTM) is but a name.
Strangely, the school probably now has firmer links, and certainly greater knowledge, of the tropics and hygiene than in the pioneering days of Ronald Ross and Patrick Manson 80 or more years ago.
It is a centre of excellence, a component of the University of London.
With its expertise in a wide range of tropical diseases, especially malaria (this issue, p 731), it has become an international teaching house and research centre for such matters.
It draws to London large numbers of overseas students for its courses on, and research in, epidemiology, medical microbiology, hepatitis, helminthology and parasitology.
Many of the courses are very specialised, others highly practical, such as that on ‘Refugee camps and appropriate health care’.
The concept of ‘hygiene’ has widened enormously since the school's inception in 1924, to encompass human nutrition, vitamin deficiencies, biochemical and metabolic disorders, louse eradication, ‘sudden infant death syndrome’(best known as cot-death), population studies and community health.
Especially significant nowadays are the courses and research on occupational risks, such as working with cadmium, asbestos textiles, toxic gases, vapours and solvents, and even smoking.
It is also concerned with preventive teratology (embryological deformities) such as spina bifida and other neural-tube defects, and the protection against such defects that a course of multivitamin tablets may give before pregnancy.
When the government decreed that student numbers should be cut, that university grants be cut, and that overseas students should pay their full fees for courses and board in the UK, LSHTM with its large overseas intake suffered initially only as badly as other London colleges.
In the past year or so, however, these changes have dawned on overseas governments, who now find that there are cheaper courses elsewhere in the world for their students.
Courses at the LSHTM now cost a minimum of £6400 to £9000.
Increasingly the school is having to trim its sails and to top up its central resources.
It is actively selling its expertise through research contracts at home and abroad.
For overseas contracts, though, overseas students are more than just vectors of goodwill for the London experience.
Without them, in some countries, contract research may not be possible.
Research (’ soft’) money represents now 45 per cent of the LSHTM's total income and expenditure of about £3.4 million.
Few other medical schools have such a successful record of attracting research funds.
The academic staff (about 55) now supports twice their number on research grants.
This year the school received a grant of £1.5 million from the Wolfson Foundation.
LSHTM is now working closely with the Liverpool School of Tropical Medicine, and collaborating with nearby medical microbiology and community health.
In the academic year 1980–81, the school's finances ran severely into the red and it had to be baled out from a special government reserve.
In 1982–83 the school is the only institute to receive a cut in its cash from the University Grants Committee.
The present drop in overseas students on top of this is having a devastating effect on the school's finances.
Unless central government is prepared to support the school more firmly, LSHTM will accelerate into a financial death spiral.
There are clear indications as to where the emphasis in future research and courses in the ‘new hygiene’ should be.
The tropical research should be supported not just because Britain now has a large immigrant population from tropical regions, but because it could be one of the country's most significant and meaningful contributions to the Third World.
Tales of Two Scientists
Amateur scientists are these days a rare breed, but Ian Ridpath has sought out a couple of the remaining specimens.
Horace Dall and George Alcock are two lively old men who both still actively contribute to various areas of science The Universal  explorer and spare-time inventor
WHAT do the following have in common: a machine that can write the Bible on a pinhead; a single-lens microscope that magnifies 1000 times; a barometer that measures the difference in atmospheric pressure across the thickness of a book; a 15-cm (6-inch) telescope that fits into a jacket pocket; and the first bicycle crossing of the uninhabited central region of Iceland?
The answer: Horace Dall.
A sprightly 81 years old, this English engineer and inventor looks back with satisfaction on a lifetime as an explorer of the heavens, the Earth, and the limits of the technically feasible.
Despite his wide range of achievements, Dall's name is little known outside of a few groups of specialists.
The reason is twofold: he has an unassuming nature, but more importantly, his achievements are the fruits of hobbies.
He is a classic example of a sparetime inventor who has beaten the world.
The image of Dall as one of Santa Claus's pixie helpers would not be far amiss.
His study and attic are like Santa's workshop, filled with a jumble of magic things.
Scientific instruments and optical components, many made by himself, crowd every available space from floor to ceiling jostling with geological specimens, souvenirs from his travels to remote regions of the Earth, and boxes of photographs reaching back 65 years into history, including early colour photographs and stereoscopic pairs.
The centrepiece of Dall's attic is his camera obscura, a periscope-like device whose turret sticks out of the roof of his house on a hillside overlooking Luton, Bedfordshire.
Dall chose the site and the high-roofed design of the house with the camera obscura in mind when he had the house built in 1934.
This celebrated instrument draws visitors from around the world.
As well as viewing the surrounding landscape, Dall uses the camera obscura for astronomical observing.
It projects an image of the Sun that fills a wall, revealing fine details on the boiling solar surface.
Optics have fascinated Dall since he bought a spyglass telescope from a market stall in his mid-teens.
Soon afterwards he was building his own telescopes, at which, like most things he turns his hand to, he excels.
His optics are internationally recognised to be of the highest quality.
Dall is indeed a worthy successor to the great English instrument makers of yesteryear such as Jesse Ramsden and Thomas Cooke.
In 1929 Dall invented a modified form of the widely-used Cassegrain reflector, which is easier to make than the classical Cassegrain design.
Some years later an American, Alan Kirkham, independently invented the same principle, now known as the Dall-Kirkham reflector.
Because of the ease of manufacture, an increasing number of so-called Cassegrain reflectors are being made to the Dall-Kirkham design.
Dall built his own Dall-Kirkham reflector with a 39-cm (15½-inch) mirror in his back-garden observatory in 1937, with which he takes photographs of the Moon and planets rivalling those taken by larger telescopes in professional observatories.
Another of his inventions, the ‘Dall null test’, is a standard technique used by opticians for checking the shape of their mirrors.
At the other end of the scale, Dall holds the record for the most powerful single-lens microscopes.
He showed me one, magnifying 520 times, made of yttrium-aluminium-garnet, an artificial gemstone, mounted against a microscope slide, through which I could clearly see the tadpole-shaped trypanosomes that cause sleeping sickness.
I felt I had strayed into the workshop of a latter-day Anton von Leeuwenhoek, the pioneer microscopist who was noted for his high-performance single lenses.
Dall has made a single lens microscope that magnifies 1000 times, but he considers the image too faint to be of practical use.
As another example of his skill at miniaturisation, he displays his portable 15-cm (6-inch) reflecting telescope, which weighs a mere two-thirds of a kilogram and can fold flat to go in his jacket pocket or between his clothes in a suitcase.
It has been around the world with him.
‘No customs officer has ever spotted it,’ he smiles with satisfaction.
The casual visitor to Dall's crowded study might miss seeing a hyper-sensitive barometer sitting on the shelf, not much larger than a can of beans.
At its heart is a strip of platinum-iridium twisted 400 times.
This barometer easily shows the difference in atmospheric pressure between a table top and the floor.
On a calm day, it can register a pressure difference across the thickness of the Handbook of Chemistry and Physics .
The visitor who examines these marvels is left wondering about the nature of the man who made them.
‘It's a challenge,’ he explains simply.
His wife, Helena, is more forthright.
‘Horace always wants to do better than anyone's done before.’
A prime example of that desire not merely to improve but to achieve the practical limit is his machine for microwriting.
In 1943, Dall heard of an American who claimed to be able to produce the world's smallest writing, on a scale that would allow 30 copies of the Bible to be engraved on a square inch (6–5 sq cm) of glass.
Within a year, Dall had devised a machine that would reduce handwriting to the scale of 140 bibles to the square inch.
The technique is to crush a diamond, select a splinter that comes to a molecular-sized point, and mount it on a pantograph system.
Dall has produced writing to the staggering scale of 250 bibles to the square inch, small enough to write one bible on a pinhead, but the individual letters on that scale are so minute, no higher than a  micrometre (one or two wavelengths of visible light), that they are at the limit of resolution of optical microscopes.
The mind reels at such extremes of scale, yet the man responsible dismisses it lightly: ‘It's only a bit of fun.’
Dall was brought up in an engineering environment.
His father was an instrument maker with the Marconi Company in Chelmsford, before moving to the firm of George Kent in Luton.
Horace left school at age 14, just as the First World War broke out.
He joined the aircraft firm of Hewlett and Blondeau at Leagrave on the outskirts of Luton before following in his father's footsteps to George Kent's.
At Kent's, for 48 years Dall designed and built instruments to measure the flow of fluids, retiring at the end of 1965.
There he invented the ‘Dall tube’, a device still widely used for measuring the flow of water from reservoirs and in water mains.
His spare-time activities during these years resulted in various patents, including one from 1931 concerning the Baird TV system and one in 1933 for the variable density technique of optically recording sound on film.
During the Second World War Dall was one of a dozen experts called to Farnborough to examine the wreckage of a V2 rocket that had crashed in Sweden on a test flight.
From an examination of the rocket's combustion chamber and fuel pumps he was able to calculate the V2's thrust and hence its range, before one was ever fired at England.
As a boy, Dall regularly ventured far on foot and bicycle, and that adventuresome spirit has remained with him.
He is now on his eighth passport, having left England 82 times on trips that have taken him to most countries in the world.
In 1933 he made the first crossing of the uninhabited interior wilderness of Iceland with a wheeled vehicle — his bicycle.
The 260-km journey took five days, travelling 16 hours each day, mostly busing the bike over the broken terrain.
Without proper maps he followed a route set by compass, boiling up pemmican and glucose for food and sleeping under the stars in a sleeping bag at night.
His fitness and determination ensured that he survived a journey on which a lesser man could have been lost without trace.
He had previously pedalled his way to make the first cycle crossings of the High Atlas mountains in Morocco, and of Lapland from the Arctic Ocean to the Baltic Sea.
Stories about Dall's travels abound.
Twice in Morocco he was arrested and interrogated as a spy by the French Foreign Legion who refused to believe that anyone would spend their holidays cycling across the desert in temperatures of 48’ C.
Characteristically, Dall first met his present wife Helena in a cafe in Patagonia 14 years ago.
They talked for two hours before going their separate ways, he up the Amazon and she bound for Canada.
During that two hours they realised that they had much in common.
Both were bereaved, and both were seasoned adventurers.
Back in England, they married that same year.
He was then 67 and she was 63.
They have roughed it around the world ever since, at an age when most people would be content with a game of bowls at Eastbourne.
‘We never travel anything that's classy,’ explains Helena.
That has meant sleeping in bus stations and caves, bumping in lorries across the Kalahari desert, crowded for days onto the trans-Siberian railway, and sheltering in a mud hut from a rainstorm in Ethiopia.
Their travelling days are now all but over, following a recent trip to Guyana, which landed Horace in the Hospital for Tropical Diseases on his return.
But this universal explorer continues his work at home, plumbing the world about him from the astronomically large to the microscopically small.
The Man with the astronomical memory
EVERY clear night, a retired schoolmaster sits in a deckchair in the back garden of his home in the village of Farcet, near Peterborough, and scans the sky with binoculars.
Etched in his memory are the positions of at least 30000 stars, plus 500 fuzzy nebulae.
As his gaze roves across the sky, he checks off what he sees against his mental star map.
He can spot any interlopers among the stars immediately.
In this way, George Alcock has discovered four erupting stars, or novae (a world record for visual observations), plus four comets, including the first comet to be discovered from Britain this century.
In 1978 he was awarded the MBE for his services to astronomy.
Alcock, the man with the astronomical memory, probably knows the sky better than anyone in the world, possibly in history.
At a time when professional astronomers keep the sky under closer scrutiny than ever before, Alcock's world-beating achievements rank him with great observers of the past, such as the famous French comet hunter Charles Messier.
George Alcock is the outstanding amateur observer of this century.
Indeed, his success demonstrates that there is still a role in astronomy today for the modestly-equipped amateur, a role that professional astronomers recognise and encourage.
Amateur discoveries of unpredictable events such as comets and novae feed vital information to professionals, who immediately swing specialised equipment into action.
Alcock, now aged 70, began his interest in astronomy in his teens, as a result of finding a small star map in an  encyclopaedia .
His first major contribution came in the field of meteor-observing in the 1930s.
In conjunction with J. P. Manning Prentice, who was then director of the British Astronomical Association's meteor section, Alcock watched as particles of dust from space dashed at random to their fiery death in the atmosphere, appearing as shooting stars.
From their respective sites at Stowmarket and Peterborough, Prentice and Alcock could work out the heights and paths of the dust particles by triangulation, showing that these sporadic meteors were members of the Solar System, in contradiction to the view of professional astronomers in the United States who believed that they entered from interstellar space.
After the Second World War, astronomers at Jodrell Bank began to track meteors by radar, and Alcock was one of these called in to make simultaneous visual sightings.
The radar results confirmed the conclusions he and Prentice had formed.
During this time Alcock had begun the long process of memorising the night sky; by 1932 he already knew the position of most stars that can be seen by the naked eye.
Following the inroads made by professional astronomers, he abandoned meteor observations, and as a new challenge he began in 1953 to sweep the sky for comets, at first using a 10-cm (4-inch) refractor.
But the telescope's field of view was too small, and it was not until 1959, when he acquired a giant pair of German anti-aircraft binoculars, that success came.
The tripod-mounted binoculars, magnifying 25 times and with lenses 105 mm in diameter, proved ideal for locating faint, fuzzy comets.
At the end of August 1959, after six years of patient searching, Alcock found two comets within a week.
Surprisingly, they were the first comets found from Britain this century.
He discovered other comets in February 1963 and September 1965, so that four of these interplanetary spectres now bear Alcock's name, providing him with immortality written in the sky.
A ludicrous task
During the 1960s Alcock became convinced that many novae, or ‘new’ stars, that flared up to just below naked eye visibility were being missed by astronomers.
Novae are now believed to be caused by gas spilling from one star to another in a close double-star system, but 30 years ago they were a complete mystery; only two dozen had been spotted in time for astronomers to study them while they were still erupting.
Alcock saw this as another opportunity to produce valuable results, but first, he would have to learn the stars down to magnitude eight, over six times fainter than those visible to the naked eye.
Building on his knowledge of the sky gained during the earlier work on meteors, he began in 1955 to memorise the sky seen through binoculars.
At first, he doubted his ability to succeed at his self-appointed task, which seemed ‘utterly formidable, completely ludicrous’.
Slowly, aided by a remarkable memory for visual patterns, he became familiar with the endless series of new star groups.
When the constellations Monoceros and Canis Major  reappeared after five months behind the Sun he found that he had not forgotten the positions of more than 300 stars.
‘This really kept me going,’ he explains.
After six years of study, the sky as seen through binoculars was etched on his memory.
At the start of 1961 he began a serious nova-patrol and since then has divided his observing time between novae and comets, keeping late-night vigils which his wife Mary soon became used to.
For nova-hunting, Alcock uses hand-held binoculars which he can sweep freely over the sky as he reclines in a deckchair.
The tripod-mounted binoculars are too cumbersome and too powerful for this purpose.
On July 8, 1967, Alcock discovered his first nova, in the constellation Delphinus.
He notified the Royal Greenwich Observatory, which confirmed the discovery and cabled the central clearing house for astronomical telegrams at the Harvard-Smithsonian Center for Astrophysics in Cambridge, Massachusetts.
Alcock's nova was world news.
Even the giant 5-m reflector on Palomar Mountain was taken off its normal duties and turned towards Alcock's new star.
He swells with pride at the thought.
The following year Alcock discovered a second nova in the nearby constellation Vulpecula.
These twin discoveries were a fitting reward for more than 800 lonely hours of patient patrolling of the skies over six years.
Two further discoveries followed, in 1970 and 1976.
Among amateur astronomers, Alcock has a reputation for being reclusive.
He sets his own standards and lives by them.
That can lead to misunderstandings.
In a fit of pique, he threw out many of his planetary drawings, made with a 10-cm refracting telescope, after high-placed amateurs refused to believe that anyone with such a small instrument could see the detail he recorded.
The doubters had failed to take into account Alcock's remarkable visual acuity, but photographs taken at professional observatories, and by space probes, have verified the details of his observations.
Alcock will never report anything he is not sure of.
That caution has cost him at least one discovery.
One night he detected a faint comet, went inside to check a star map to ensure there was no nebula in that position, but was unable to see it again when he went back outside.
He decided against reporting it.
The discovery instead went to a professional astronomer, Konrad Rudnicki, who shortly thereafter found the comet on photographs taken on the large telescope at Palomar Mountain.
In private, Alcock is kindly and avuncular, a natural teacher, never happier than when passing on the fruits of his knowledge to the younger generation.
Old school photo graphs show him sitting like a wise and attentive owl, surrounded by his charges.
His former pupils still respectfully call him ‘Sir’.
Astronomy is not his only passion.
‘I observe anything,’ he explains.
His wide range of interests keeps him busy day and night.
With his junior-school pupils he surveyed 100 square kilometres of north Cambridgeshire for the Nature Conservancy's Botanical Atlas of the British Isles.
For the past 20 years he has sent out a nature news sheet every few weeks, detailing his observations of birds, butterflies and flowers, illustrated with his own hand coloured drawings.
In an upstairs room of his house, a radio teleprinter chatters out weather reports from which Alcock plots his own charts to predict conditions for stargazing.
Columns of his own observations of winds, visibility, temperature and cloud-cover fill thick books dating back to 1933.
Even when the weather is too bad for astronomical observing, Alcock still wakes up at two-hourly intervals during the night to make meteorological observations.
His records are a treasure house of information for anyone interested in climatic research.
Alas, increasing glare from streetlights around Peterborough is making stargazing progressively more difficult.
‘It's extremely doubtful if I'll ever find another comet,’ declares Alcock.
Nevertheless, he tirelessly continues to sweep the sky, hoping to add to his tally of discoveries.
Time for a tonic
Stephen Fulder
At one time, medicine meant ‘tonic’, and specific cures were quack remedies.
Now the reverse is true.
But tonics deserve to be taken seriously.
Somewhere in the corner of the vast pile of pharmacological agents available to us, is an obscure bundle of miscellaneous substances labelled ‘tonics’.
It is not that they are not used: vitamins are prescribed and bought on a massive scale for people without the slightest hint of a deficiency, and ginseng is now sold in chemists and health-food shops in the UK to the tune of no less than £7 million a year.
Neither is it that tonics are not requested; large numbers of people get as far as the doctor's surgery to complain of tiredness, lethargy and mild depression, and even larger numbers go straight to the chemist.
But tonics and ‘restoratives are at the bottom of the pile both in terms of their status as pharmacological entities and in terms of research.
A closer look at the evidence, however, suggests that tonics may provide safer alternatives to more powerful drugs.
The roots of modern pharmacology lie in the empirical discoveries of the past century, when medical men found that single therapeutic agents could effect seemingly miraculous cures.
Quinine caused fevers to plummet; morphine dramatically reduced pain; atropine, codeine, aspirin and other drugs, isolated from the old herbal preparations, had instant and theatrical effects.
The new drugs placed traditional therapists in the shadows, mouthing Galenical principles to a fast-disappearing audience.
Scientific medicine was accused of being ‘the worst evil of therapeutic nihilism’, but it never looked back.
In the middle of the 19th century those who proclaimed that specific drugs cured specific diseases were called quacks; a century later, the traditionalists were the quacks.
The present century's successes with drugs, spearheaded by the discoveries of antibiotics, have fixed into Western consciousness the premise that drugs are the only way to cure diseases.
Earlier remedies, which were used as tonics, restoratives or alternatives, were discarded as pharmaceutical flotsam by the tide of progress.
This development is unique to Western medicine.
In indigenous medicine still practised all over the world, and in European traditional medicine, these same tonics are regarded as a necessary and important weapon.
Practitioners use these medicines as a first resort, to promote the energy, circulation and vitality of the person, with the presumption that timely intervention at this stage may prevent later illness.
Tonics have the advantage that they are extremely safe.
Chinese medicine regards tonic and adjustive remedies as the most important (the ‘kings’) of all medicines while curative drugs are the least important (the ‘servants’).
The Chinese value medicines in proportion to their safety and ability to prevent disease.
In the West, effective herbal restoratives, health remedies and ‘blood purifiers’ such as sage, sarsaparilla, liquorice, dandelion root, yarrows or mugwort, disappeared along with the rest.
The synthetic drugs reigned supreme, carrying their subliminal toll of side effects.
Soon after the turn of the century, however, in response to the demand of the market place, tonics returned again.
But this time they were chemicals.
First there was iron.
Iron salts were added to tonics with the vague justification that because haemoglobin contained iron, more dietary iron would somehow enrich the blood.
Naturally, most of the iron tonics sold over chemists' counters were bought because of the universal metaphor that iron equals strength rather than because of a popular awareness of the needs of haemoglobin.
In any event there was never any evidence that iron did any good for the non-anaemic, and it eventually fell out of fashion.
Sugars, in particular glucose, were introduced ‘for energy’, on the notion that because the body digests carbohydrates down to glucose, the main circulating fuel of the body, then taking glucose directly would shortcircuit the digestion and provide more energy.
However the logic is again questionable.
To those with a normal digestion taking glucose cannot provide more energy than any other carbohydrate.
It merely introduces it more quickly.
The surge in blood glucose may produce a temporary ‘burst’, but at the same time imposes a mild metabolic stress on the body.
In another category of tonics are the phosphate tonics, sold today in the form of glycerophosphates.
It was thought that as phosphates were needed to transfer the energy, arising from the dismemberment of carbohydrate, ingesting phosphate would give energy.
In the late 1920s W. Embden, professor of physiology at the University of Frankfurt, found that German miners produced 20 per cent more coal when they were given 7 grams a day, decidedly a harmful dose, of sodium hydrogen phosphate.
Phosphates were given in the form of glycerophosphate, which the membranes of cells contain, in the belief that this would facilitate the assimilation of phosphate into the tissues, particularly in the brain.
Glycerophosphates are still the main constituents of the tonics chemists sell, even though there is little theoretical or experimental justification for their wide or extensive use by people who are tired, chronically sick, aged or in poor health.
Like the sugar tonics, the glycerophosphates have no long-term beneficial effects on health, and indeed perhaps hinder the body from restoring its own efficient breakdown of glucose to release energy.
The effects of such tonics are transient; they add energy from the outside rather than help the body to make its own.
One tonic that came into vogue was even worse: strychnine.
A deadly nerve poison, it was once used as an animal poison but is now banned on the grounds that it kills with extreme cruelty.
Only 15 mg is fatal to a child.
Strychnine is an alkaloid belonging to a group of chemicals called analeptics, which act on nerve cells to make them more easily excitable.
It was included in small amounts in tonics as a stimulant, a practice that all modern pharmacology texts agree is useless and possibly dangerous.
The classic text by Louis Goodman and Alfred Gilman (Pharmacological Basis of Therapeutics ) states that strychnine has no therapeutic value whatsoever despite ‘a long period of unwarranted popularity’.
Yet drug companies, with some cynicism, are still selling so called tonics containing strychnine.
Of the eight or so tonic preparations currently listed in Martindale's Extra Pharmacopoeia , all contain glycerophosphate and half contain strychnine; examples are Glytona, Glykola and Neuro Phosphates.
In a similar fashion caffeine became a familiar component of all kinds of tonics and ‘health drinks’, once it had been isolated from the coffee bean.
Caffeine is the active ingredient of coffee or tea and is a necessary social drug, adding considerably to the overstimulation and agitation of modern man, but in no way can it be regarded as a tonic.
It is a brain stimulant, acting rather specifically on nerve cells in the reticuloendothelial, or ‘wakefulness’ system, to increase brain arousal.
It does not have the slightest beneficial effect on health and is mildly addictive into the bargain.
The side effects of the long-term use of caffeine are mild but extensive.
It affects digestion, exacerbates stomach problems and may join with adrenaline in insidiously damaging the circulation.
In 1980 the United States food and Drug Administration (FDA) issued a warning concerning the side effects of caffeine, and advised pregnant mothers to stop drinking so much coffee.
Yet caffeine is still the major constituent of many so-called health drinks and is added in considerable quantities to Coca-Cola.
So, are there any true tonics and restoratives?
Indeed there are.
But we have to look carefully and dispassionately to find them.
We can classify the truly effective tonics into two groups: those of traditional herbal origin, and those made or isolated by chemicals in more recent times.
Of the herbal tonics and restoratives, we have lost many.
But one that has surfaced in recent years is ginseng, the root of the araliaceous plant,Panax ginseng .
This plant, a relative of ivy and indigenous to the temperate mountains of the Far East, has been used continually for some 5000 years in the Orient as a tonic, restorative and preventive remedy.
Indeed, it is one of the most highly prized all Chinese medicines.
It is recommended for tired, mildly depressed, convalescent and particularly old people.
Athletes, including Sebastian Coe, and cosmonauts are among well-known users of ginseng today.
There are several other plants related to ginseng pharmacologically, chemically and botanically.
For example Eleutherococcus senticosus is used widely as a tonic in the USSR, and is available in Soviet hospitals for people undergoing surgery, chemotherapy or radiotherapy, to improve their resistance and aid their recovery (New Scientist , vol 87, p 576).
These plants, unlike the earlier tonics, have the backing of a large amount of experimental and clinical data.
This is despite the obvious difficulties of devising experiments to test the effects of the substances on the performance of normal organisms rather than the cure of sick ones.
Four recent clinical studies in America and Europe, show improvements in mood, performance and stamina produced by ginseng.
These follow hundreds of published studies reporting biochemical and behavioural effects in animals.
Two highly productive lines of research demonstrate that ginseng increases the efficiency both of adrenal cortical reactions to stress, and of the production of energy during exhaustive physical work.
It is not yet known how the active components of ginseng, compounds called triterpenoidal saponins, achieve these effects, although by analogy with the related steroidal saponins, such as digitalis, they may act on specific cell membranes.
Other tonic or stimulatory plants have been occasionally investigated but the field is ripe for further research.
Thujone, the main constituent of sage, is now known to have mild stimulatory properties.
Kawain from kava kava (Piper methysricum), and catlimore from the Yemini plant khat (Carha edulis), are examples of new stimulatory substances.
Several interesting compounds have been isolated from traditional health-promoting plants such as liquorice, spikenard, the jujube, and senega, as well as many Far Eastern or Indian plants, which do not have European common names.
The active components tend to be compounds called liguans, terpenoidal saponins or simple phenols.
As far as the true chemical tonics are concerned, there are several kinds of active substances available, although most of them are not well known.
Panganic acid, an ester of dimethylglycine, was first isolated from rice in 1948 by Hans Krebs, a chief chemist of the National Cancer Institute.
He called it vitamin 51B, which was unfortunate because proof of its vitamin status was never found and that raised the hackles of the regulatory  authorities in the US.
Nevertheless some clinical and experimental work has indicated that panganic acid can increase the stamina and performance of athletes.
Glenn Shue, a nutritionist with the FDA, summarised the work in a report that denied that the acid is a vitamin.
He admitted that there was evidence ‘that it would be useful as a drug for certain conditions where the body is not able to get enough oxygen’.
Vitamins in large quantities can have certain tonic effects.
‘Megavitamins’, that is doses of vitamins many times larger than that recommended to prevent diseases arising from deficiency, have been found to affect the biochemical reactions for which the vitamins are needed.
For example, vitamin C takes part in the manufacture of corticosteroids from  cholesterol .
Vitamin B6 (pyridoxal phosphate) is needed for the metabolism of several amines, which act as important messengers within the brain.
Nicotinamide, another of the B vitamins, is necessary for the production of adrenaline from non-adrenaline.
Research has shown that significant changes in the levels of these particular vitamins can alter those reactions and thereby affect energy and well-being.
There has then little clinical research in this field, although a recent paper from appeared in Aviation and Space Medicine .
It was a double-blind study showing that athletes taking megavitamins for one month significantly increased their stamina and performance.
Perhaps the most famous of all chemical tonics are those based on dimethyl and diethylethanolamines (DMAE and DEAE).
Their chemical structure is based on ethyl alcohol substituted with an amino (-NH 2 ) group, as well as methyl or ethyl groups.
They occur naturally, forming part of the membranes of cells.
Furthermore, when another methyl group is added to dimethylethanolamine it becomes choline, which is the basis of acetylcholine, one of the most important chemical messengers both within the brain and between nerves and muscles in the body.
Remedies containing DMAE and DEAE have had very different histories but are all linked by their mild tonic, stimulating or restorative effects.
Procaine, the paracohlrophenoxy acetic ester of DNAE, has been used since 1905 as a safe local anaesthetic.
It was soon realised, however, that injections of procaine were effective in relieving the symptoms of arthritis, and this method of treatment became quite popular in Europe before it was eclipsed by the discovery of cortisone.
A Rumanian doctor, Ana Aslan, who was using these injections, noticed that her elderly patients appeared much more lively, and generally healthier after long-term procaine injections.
She opened a rejuvenation clinic and for the next 30 years many thousands of people flocked to receive what became the most famous, or perhaps, notorious, ‘rejuvenatory’ treatment of the century.
Procaine is now available as a geriatric preparation under the trade name KH 3.
Another compound, a different ester of DMAE, called centrophenoxine, was discovered in an equally bizarre fashion.
Jean Thullier, a French scientist at the Institut National de Hygiene, injected various auxins (regulators of plant growth) into animals to see whether they had any effects.
Centrophenoxine had the strongest biological activity, producing a mild stimulation of the central nervous system.
It is now marketed as a geriatric tonic under the trade name Lucidryl.
Pure DMAE and DEAE are also available.
DMAE, as Deaner, is an American product with claims to be a mild stimulant.
It is registered with the FDA, and is used mostly with children.
Pure DEAE has just begun to be marketed in Australia under the trade name Provital-H.
Although there have been conflicting and confusing studies in the literature, two things emerge from fundamental research on DMAE and DEAE.
First, the esters procaine and centrophenoxine are immediately broken down in the body to release their DEAE and DMAE, which are the active portions of the molecules.
Secondly, DMAE and DEAE replace choline in cell membranes and in this manner gradually alter the metabolism of acetylcholine in the brain.
Such mild long-term alterations to this acetylcholine may be expected to produce any observed changes in mood, activity and sense of well-being.
There are some 120 published clinical studies on procaine, yet rarely has pharmacological research been blessed with such endless bickering between opposing camps.
Aslan's spectacular claims that she could add eight years to the average lifespan by means of procaine injections started a spate of clinical trials during 1959–1960, all of which were negative.
However, these studies and many others that followed were seriously undermined by some fault or other.
They either had too few subjects, too short a treatment, lack of double-blind methods or the subjects were institutionalised and demented geriatrics who were too far gone to show any psychological changes.
Examining what is left provides reasonably firm evidence that long-term treatments with procaine are tonic and anti-depressant, and can produce beneficial skin changes.
However, the more extreme claims of Aslan have not been supported.
Centrophenoxine has a similar history as a tonic preparation for geriatrics, used more on the Continent than in England and America.
Research with this substance, though, has been much less equivocal and ambiguous.
Psychological and psychometric testing has clearly demonstrated that centrophenoxine stimulates tired elderly brains and makes them more alert, and the compound can prolong the lifespan of normal mice in laboratory tests.
Researchers, especially at the Gerontological Research Center in Bedford, Massachusetts, are at work exploring another property of centrophenoxine that is causing excitement generally among gerontologists.
It can remove the deposits of pigment that clutter up old cells whether in tissue culture or in the brain.
Tonics have special uses for which no other remedies are effective, namely, the restoration of function in convalescent, chronically-tired and old people.
In addition, tonics from plants can be used preventively by healthy people, as our grandparents may attest.
Yet these tonics are underused and under-researched.
The fact that the strycholine/glycerophosphate mixtures are still sold, in preference to much more effective and well-researched active substances, and despite their clear dismissal by pharmacologists, only goes to show how little modern medicine is concerned with these kinds of treatments.
Yet, this is a serious mistake.
All too often those in need of such remedies are either fobbed off with sympathetic noises or given drugs that are inappropriate and toxic.
For example, mildly depressed and tired old people used to be given amphetamines.
Thankfully that treatment went out of fashion, but the current fashion is to blame the same symptoms on poor circulation in the brain and give cerebral vasodilators, remedies that enlarge blood vessels in the brain.
In many cases a tonic remedy would be far safer and more effective.
There is also sound commercial sense in re-investigating well-tried and harmless remedies that have been passed by.
The toxicity of some drugs has brought the status of the pharmaceutical industry to its lowest ebb yet, and has been partly responsible for a virtual stampede away from chemists towards health shops.
The health industry in the UK is a growing business worth £80 million a year.
Tonics, restoratives and health remedies provide an opportunity to invest in the non-toxic remedies of the future.
Who doesn't want to eradicate malaria?
George Davidson
After 40 years of ‘chasing mosquitoes’ and trying to control the malaria they transmit, an eminent entomologist believes that recent policy changes by the World Health Organisation and the intervention of environmentalists have baulked the malariologists from their ultimate goal
Such was the enthusiasm generated by the initial successes with the organochlorine insecticides like DDT, BHC and dieldrin that most of the malarious nations of the world, excepting those in tropical Africa, gladly joined in the eradication programme that the World Health Organisation (WHO) began in the late 1950s; a programme aimed at eradication of the disease rather than the mosquitoes that carried it.
Outstanding successes followed in the 1960s, some alas to be only temporary, but some quite permanent especially at the northern and southern extents of the distribution of the disease.
Sicily is an example at the northern extreme, of which I have personal experience.
I acquired the relapsing type of the disease there, together with 11 000 other troops of the British Eighth Army, during the Second World War; in the six-week campaign in the summer of 1943, malaria accounted for many more casualties than the battles.
Thanks to DDT there has not been a case of locally-transmitted malaria in Sicily since the early 1960s.
In fact, the only territory on the northern shores of the Mediterranean where malaria is actively transmitted today is in the Chukhurova plain of Turkey.
It has been completely eradicated from Greece since 1973, where before the Second World War it caused an estimated one to two million cases annually.
Despite these successes and very significant reductions in many other parts of the world, including tropical areas, the World Health Assembly of the United Nations at its 22nd meeting in 1969 decided to abandon the concept of eradication, considering it as practically unattainable.
It urged countries to revert to containing the disease at levels that their own general health services, rudimentary though may of them were, could cope with.
Different countries put different interpretations on these manageable levels.
The net result has been a marked resurgence of the disease in many parts of the world to what some people, including the famous American malariologist Paul Russel, consider to be levels that existed before the Second World War.
In my opinion, and in the opinion of many others, this resurgence is partly the result of dropping the eradication principle that provided the drive and final goal.
Malaria is primarily a disease of the blood, and is transmitted from person to person by the bite of females of certain species of anopheline mosquitoes.
About 30 species of Anopheles are now considered important as carriers of malaria.
They lay their eggs in fresh or brackish water where the immature larval mosquitoes develop to pupae, from which emerge the winged adult mosquitoes (see Box A).
Only the females can suck blood and transit malaria.
The disease is caused by unicellular parasites (protozoa) of the genus Plasmodium that undergo cycles of development in both man and the mosquito.
The latter cycle takes about 10 days to complete, and successful transmission demands that female mosquitoes survive and take at least two feeds of blood during this time.
Malaria is the commonest of the tropical diseases.
Before specific control measures were introduced there were around 250 million cases a year, with about 2.5 million deaths.
Even now the World Health Organisation estimates that 107 countries are affected and that about half the world's population is exposed to the risk of infection.
There are 215 million people (mostly in Africa) chronically affected, and an estimated 150 million new cases occur every year.
However, deaths are about half what they were.
The last full analysis of reported cases, made in 1980, shows 41 per cent to be occurring in tropical Africa (where virtually no attempt at control has been made so far), 22 per cent in the Indian subcontinent and 31 per cent in Eastern Asia and Oceania; only 5 per cent of cases were reported from Europe, the Eastern Mediterranean and Central and South America.
Table 1 shows populations and cases in broad areas of the world.
Malaria can be controlled by attacking the parasite (with drugs or — theoretically — with vaccines); by controlling the vectors (mosquitoes) that carry it; or by protecting the human host from being bitten.
Malariologists have used good synthetic anti-malarial drugs such as amodiaquine, chloroquine, proguanil, primaquine and pyrimethamine for more than 30 years now (Table 2).
Such drugs can effect radical cures, but have to be administered over several days, and a reliable treatment by single dose has yet to be found.
However, mass treatment in many countries involves distributing a five-day course of tablets which the recipients usually fail to complete because they often feel better after one or two days.
More often than not, the parasite develops a resistance to drugs and suitable alternatives are not easy to find.
The new regimes of Fansidar (pyrimethamine + sulphadoxine) and Maloprim (pyrimethamine + dapsone) are already showing signs of failure and virtually only one new synthetic drug, mefloquine (structurally related to quinine) remains.
(It is possible that the Chinese qinghaosu , a natural drug derived from the medicinal herb quinghao , Artemisia annua , which has been used in China for at least 2000 years, may prove useful.)
Need for a multiple approach
Over the years, health workers have tried mass therapeutic and preventive measures both by themselves and in combination with measures to control mosquitoes.
Time and time again they have run up against unsurmountable problems of distributing and getting the drugs accepted.
In the 1950s and 1960s, some attempts were made to supply communities with medicated salt, that is with chloroquine or amodiaquine added.
However, all salt had to be treated at source and it had to be consumed regularly, and though there was some success in parts of South America, these conditions could seldom be met.
Vaccination would also meet the same problems if a suitable vaccine were available.
Unfortunately it is not, though recent breakthroughs in cultivating malaria parasites in vitro and in isolating specific antibodies seem to have increased the chances of success.
To protect people from being bitten they must be educated and persuaded to use insect repellents and mosquito nets.
Such measures need cash, and although they can contribute to malaria control, they can never suffice by themselves.
Ronald Ross, who in 1898 was the first to prove that mosquitoes transmitted malaria, was quick to advocate control by an attack on the vector.
Early workers concentrated on destroying the immature aquatic stages in their breeding places using what we would now call environmental management.
They aimed to reduce the source, and set about it by draining and filling-in pools and lakes.
Where they could not achieve this they introduced the fish Gambusia affinis , which eats mosquito larvae, and applied petroleum oils and Paris Green (copper acetoarsenite) as poisons.
However, control by killing the mosquito larvae suffers from two main deficiencies.
First, these mosquitoes escaping face no further artificial threat and some can live long enough to become infective.
Secondly, the number of breeding places needing treatment is often astronomically high, especially in rural areas.
In the main, larval control would have to be near perfect in its efficiency and to allow very few adults to survive.
It might, however, be the method of choice against mosquitoes in urban regions where there may be more houses than breeding places, where there is likely to be opposition to house spraying, and where the breeding places are limited and more easily identified.
In theory, the best method of control is to shorten the life of the adult, vector mosquitoes, and so prevent the parasites from completing their life cycle.
The frequent use of ‘space’ sprays, containing the natural, non-residual insecticidal constituents of the pyrethrum and sprayed into the air in houses in South Africa in the mid-1930s, established the feasibility of this approach.
The discovery of more stable chemicals (Table 3)— the true residual insecticides — which were sprayed on to the walls and roofs of dwellings and left a deposit that was lethal to mosquitoes resting on it for many weeks and even months, produced the ideal control that did not demand an impossibly high efficiency.
In fact, to intercept transmission by most of the malaria vectors, requires a consistent, overall, daily mosquito mortality of around 50 per cent.
This, then, was the method adopted almost universally for the control and eradication of malaria after the Second World War.
In 1957, the World Health Organisation launched an almost worldwide eradication programme based on spraying houses with residual insecticides.
However, tropical Africa with the highest incidence and the most stable type of malaria, was not included, on the grounds that it did not have suitable basic health services on which to build an eradication programme of this kind.
Forty-two countries began eradication programmes and between 1957 and 1969 eradication was achieved in the United States, most of Europe, much of the Middle East, parts of the Caribbean and South America and from the islands of Taiwan and Mauritius.
In many other areas, notably in the Indian subcontinent, there were spectacular reductions.
In India itself the number of infections was reduced to 50000 in 1961, and in Sri Lanka in 1963 only 17 cases could be detected.
Unfortunately, when Sri Lanka stopped spraying, a colossal resurgence followed in 1968 to an estimated one million cases out of a total population of some 12 million.
In 1969, the World Health Assembly decided to abandon the aim of eradication in favour of what it considered to be the more realistic one of control to a level manageable by the existing public health services.
A much more widespread resurgence has since followed, especially in South-East Asia, so that cases reported to WHO rose from 3–1 million in 1972 to 13–5 in 1980 (although the latest figures include 3–3 million cases from China, from which no figures were available in 1972).
The cases reported to WHO represent only a very small proportion of actual people infected.
Many countries and areas have no system of case reporting, and this is why, at the beginning of this article, I put the probable total at 150 million cases occurring annually.
Various reasons are suggested to explain the resurgence in recent years but the weight of opinion is that it is operational problems that are at fault rather than technical  deficiencies — for example, poor application of the insecticide rather than resistance to it and organisational shortcomings rather than aberrant behaviour of vectors.
Certainly some species are difficult to control because they rest in homes only for very short periods; the Anopheles bolobacensis group of species in Thailand and other parts of the Far East are good examples of this behavioural problem.
Some 20 vector species now show resistance to the organochlorine insecticides, DDT, BHC and dieldrin, and 10 of these are also resistant to organophosphates and carbamates.
A few even show pyrethroid resistance but only by virtue of their pre-existing resistance to DDT.
Nearly always, some alternative compound has been found, but changes from DDT to malathion have increased costs some seven-fold, to fenitrothion some 18-fold and to propoxur some 27-fold, both because the chemicals are more expensive and because the organophosphate and carbamate insecticides are less persistent and need more frequent application.
This more frequent application, especially of a compound like malathion which has an unpleasant odour, has led to increasing refusals from householders and a consequent reduction in insecticide coverage.
What alternatives to chemicals?
We are now reaching the stage where we will shortly run out of alternative chemicals.
It takes as long as six or seven years and colossal sums of money (some estimates suggest in excess of £10 million) to put a new compound on the market.
In addition the extremely influential ‘environmentalists’ are inhibiting the control of malaria.
They seem deaf to any plea that most of the things designed to improve the quality of life involve an element of risk.
For example, there is no conclusive evidence from anywhere that DDT has ever killed anybody, yet we have ample evidence that it has saved millions of lives.
We all agree to the need for more rational use of insecticides than hitherto, especially in agriculture; yet insecticides to control mosquitoes are normally applied to the inside walls and roofs of buildings so that, unlike agricultural pesticides, they are not widely dispersed over the landscape but confined within houses.
Resistance, the sparsity of new compounds and the exaggerated opposition of environmentalists have led inevitably to considerations of alternative methods of control.
These include the so-called second generation insecticides: first, the surfactants that when spread on water deny the aquatic stages of the mosquito access to the air necessary for the breathing; secondly, juvenile hormones that prevent the adult stage from emerging; and thirdly, the chitin inhibitors which interfere with the deposition of the hard outer skeleton or cuticle common to all insects.
All these can act against only the aquatic stages, that is to say they are ‘larvicides’ and, as I have already emphasised, malaria control by such an attack is likely to be effective only in limited situations.
Environmental, biological and genetic control methods are also being considered.
‘Environmental management’ includes all those methods of reducing breeding sources that were used before residual insecticides were discovered.
Pre-war textbooks are full of descriptions of methods: draining or filling-in places where water can collect, to stop the mosquitoes breeding.
But not all mosquitoes that breed in puddles or water pots are vectors of malaria, and it takes a competent entomologist to distinguish the dangerous from the harmless and to carry out this ‘species sanitation’.
Health authorities in malarious areas must work closely with engineers in projects involving water, and with agricultural authorities over the use of insecticides.
Environmental management methods usually require a large workforce, are costly and are generally feasible only in townships and major commercial undertakings vital to the country's economy, where they may prove more economical in the long run than the recurrent use of insecticides.
However, local communities might achieve some of the simpler things like filling in small pools and keeping the edges of streams ditches and irrigation canals clear of vegetation as part of the ‘self-help’ aspect of any health education programme.
In its strict sense, biological control means the use of one plant or animal to control another and it exploits predator-prey and parasite-host relationships, either by introducing new predators and parasites or by artificially increasing the proportions of existing predators and parasites by their mass production and release.
The WHO, through the Special Programme for Research and Training in Tropical Diseases, is actively promoting research on mechanisms of biological control.
Its latest report lists two agents as likely to be widely used in the near future,Gambusio affinis , a fish that feeds on mosquito larvae, and Bacillus thuringiensis , a bacterium that kills mosquito larvae.
G. affinis is a small (4.6 cm long), surface-feeding minnow which originated in the United States and has been used for many years in many parts of the world to control mosquito larvae.
It breeds rapidly — a single female may produce 200–300 offspring in one year — and its mass production and distribution present few difficulties, although it does not thrive everywhere, is subject to disease and predation itself and may sometimes eat the wrong prey.
Some 30 countries are using such fish and it has been particularly favoured in Iran and Afghanistan as part of their integrated control programmes.
Bacillus thuringiensis var israelensis (BTI), otherwise known as serotype H-14 of B. thuringiensis , is a spore-forming bacterium that produces a crystal of toxic protein which paralyses the mouthparts and gut and destroys the gut epithelium of any mosquito larva that ingests it.
The crystal seems to be the most important component, but mosquitoes may be killed by ‘bacterial septicaemia’ whereby the bacterial toxin kills the insect but the bacteria subsequently resides and propagates in the environment and so affects further mosquitoes.
Mass production on an artificial medium is now possible and for some formulations the endotoxin is extracted.
Thus, in effect, BTI is a biological larvicide.
After promising laboratory trials at the London School of Hygiene and Tropical Medicine, at the Institut Pasteur in Paris, and at several other important centres, BTI is now being tested in the field in many parts of the world.
One of the problems still to be solved is the production of the right formulation to keep the BTI near the surface where anopheline larvae generally feed.
Numerous other predators and pathogens (viruses, bacteria, protozoa, fungi and nematodes) are also being investigated and the following are ready for evaluation in the field:Poecilia reticula and Aplocheilus spp., fish that feed on mosquito larvae;Bocillus sphaericus, a spore-forming bacterium that attacks mosquitoes;Culicinomyces clavosporus , a fungus that kills mosquitoes; and Romano- mermis culicivorax , a nematode worm that inhabits and kills mosquito larvae.
Since the 1950s laboratory and field research has also focused on the genetic manipulation of insects (genetic control) so as to render them sterile, partially sterile, more susceptible to conventional methods of control, or harmless to man.
The impetus came from the successful eradification of the screwworm (Cochliomyia hominivorax ) by the United States Department of Agriculture (USDA), led by Dr E.F.Knipling, who reared the insects en masse and sterilised both male and female pupae by irradiation.
The pupae were released into the natural (wild) population of screwworm pupae just before the adults emerged.
The emerging sterile adults mated with the wild ones to produce dramatic reductions in the number of screwworms and their eventual eradication.
Now C. homini- vorax has been completely eradicated from the southeastern and south-western states of the US and from Curacao and Puerto Rico.
There have been no serious attempts to eradicate anophelines by the technique of irradiation sterilisation, but another USDA team performed a major trial with Anopheles albimanus sterilised by exposure to a chemical.
The first field experiment started in 1971 under the leadership of American entomologist Donald Weidhaas, and involved 4–36 million insects (14 per cent of which were females), sterilised in the pupal stage by immersion in 1 per cent bisazir (p, p, -bis(l-aziridinyl)-N methylphosphinothioic amide) for one hour.
These were released over five months around the 3 km shoreline of Lake Apastapeque in El Salvador.
At the end of this period, in a month when the natural, or ‘wild’, population of these mosquitoes should have peaked, not a single A. albimanus could be found.
This technique is more efficient when only sterile males are released, but we must be able to sort out the sexes.
At first entomologists depended on a slight difference in the size of the male and female pupae, but the technique was far from efficient.
Jack Seawright, a USDA colleague of Weidhaas, used a technique of genetic sexing.
This involved the translocation of the gene responsible for resistance to the carbamate insecticide propoxur on to the male-Y-chromosome so that only the males were resistant.
The exposure of immature insects to a dosage of the insecticide sufficient to kill the susceptible females, enabled Seawright to ‘mass produce’ more than one million sterile males a day with only two females per thousand.
The American team then released the mosquitoes, over a much larger area (150 sq.
km) of the coast of El Salvador, having first treated all breeding sites with the larvicide temephos and the growth inhibitor methoprene.
These chemicals reduced the wild mosquito population so that after release there was a high ratio of sterile to normal males.
Although the American team eventually achieved good control, various circumstances curtailed the project and they did not succeed in eradicating the anophelines.
They were able to show that at least one of the reasons for their failure was the immigration of insects from surrounding, untreated areas.
I was involved in an attempt to control mosquitoes by genetic techniques some 12 years ago.
The principle was to use sterile males (’ mules’) produced by crossing two closely-related species of anopheline mosquito from tropical Africa.
Such males were extremely efficient at mating in laboratory cages in London and females so copulated laid nothing but sterile eggs for the rest of their lives — the female mosquito usually only mates once in her life and stores the inseminated sperm in a special organ inside her called the spermatheca where they remain viable throughout her subsequent life.
We produced up to 10000 sterile hybrid males per day.
Over a period of two months we released some 300 000 into a small village in Upper Volta in West Africa.
Thus we produced a ratio of three sterile males for every normal one caught resting in the wild; but we could detect little mating between sterile males and wild females.
The wild females apparently did not recognise, nor would have anything to do with, these artificially-produced ‘mules’ even though to human eyes they looked perfectly normal!
Another approach, which we are testing at the London School of Hygiene and Tropical Medicine, is to eliminate the wild, dangerous population of anophelines by replacing it with another more tractable variety.
For example, we might release new strains of mosquitoes carrying genes that prevent the development of parasites within them, or genes that impart susceptibility to previously used efficient and economic insecticides.
In the laboratory we have managed to flood the existing population with fertile anopheline males carrying such genes.
We have also developed genetic sexing methods such as I described earlier for A. albimanus and we are using it with species such as A. gambiae s.s .,
A. arabiensis and A .
culicifacies .
Consequently we are able to mass-produce sterile males for such releases.
There are of course other mechanisms for replacing mosquito populations which depend on the hybrid resulting from the mating between released insect and the natural (wild) one being sterile or partially so.
Elsewhere entomologists have detected genes for susceptibility to the commonly used insecticides, and also genes for controlling the refractoriness of mosquitoes to malaria parasites.
Failing the development of a vaccine for human beings to protect themselves against malaria, or a ‘one-shot’ radical drug, or a long-lasting preventative medicine, the control of the mosquito vector must produce the greatest benefit to the greatest number in the shortest time.
The many successes in the hey-day of malaria eradication programmes are a witness to this, and the most effective method of controlling the vector was undoubtedly the spraying of houses with residual insecticides.
The concept of eradication has been abandoned, and control to ‘an acceptable level’ is the order of the day.
This has largely led to a reduction in overall-activity, but the control of mosquitoes by means of insecticide retains its prominence.
Unfortunately with the appearance of resistance and the necessary switch to less efficient and less persistent compounds, this technique is becoming more and more expensive; and environmentalists and the householders themselves increasingly oppose their use.
The trend must therefore be away from sole reliance on chemical control and towards the integration with other methods, be they environmental, biological, genetic, therapeutic or educational.
This must mean that future progress in the control of malaria will slow down.
As a consequence, the ambitions of the eradicationists, to see in a foreseeable time the disappearance of malaria world-wide, are most unlikely to be achieved.
LET THEM LIVE IN MUD
Even so-called low-cost homes remain  access only to the middle class, and beyond the purchasing power of the poor majority.
The World Bank estimated in 1970 that in cities such as Bogota in Columbia, Mexico City, Madras and Ahmadabad in India, and Nairobi in Kenya, between a half and two-thirds of the households could not afford even a house of £300–£1500 — the cheapest that can be built with modern materials such as cement and fired bricks.
And the cost of houses rose faster than purchasing power in most countries during the 1970s, so the gap between need and availability has grown even greater.
Meanwhile, the rural housing problem, which affects most people in the Third World, is so immense that no government has even tried to tackle it on a national scale.
The failure of the so-called low-cost housing scheme to meet the needs of the poor has led one Indian expert to say that what the poor need is not low-cost housing but no-cost housing.
But housing experts and governments just have not given enough thought to the type of building materials that developing countries need.
There has been a wholesale, and usually inappropriate, adoption of Western materials and techniques, even including large scale imports of prefabricated or modular housing units.
In the past 20–30 years, most parts of the Third World have seen a rapid destruction of their architectural heritage, combined with a total indifference to local cultural traditions, building materials and environmental conditions.
As President Julius Nyerere of Tanzania said in 1977: ‘The widespread addiction to cement and tin roofs is a kind of mental paralysis.’
Cement, or ‘European soil’, as he called it, has become a matter of prestige.
The shortage and rising prices of energy will ensure that modern building materials become even more expensive.
Cement, for instance, is extremely dependent on energy; fuel accounts for a third to a half of the cost of making it.
In Jamaica, which has enormous political problems because of its adverse balance of payments, imported energy makes up more than half the cost even of a bag of cement produced locally.
Moreover, the technology that is commonly available for large cement factories is inappropriate for most developing countries.
Highly skilled industrial workers are needed to erect, operate and maintain large plants, and most countries have to import the entire plant.
Even in a country like India, where there is skilled manpower, it has not been easy to install enough new factories to keep pace with growing demand.
The result: a perpetual cement famine, official rationing and enormous corruption.
The chief minister of Maharashtra recently had to resign when a court indicted him for using his powers to speed up cement deliveries to people who gave money to his private foundation.
In many countries, including oil-rich Nigeria, forecasts of the cost of ambitious housing programmes have been sabotaged by meteoric rises in the price of cement and other scarce materials because supply failed to keep pace with demand.
Fired bricks, too, burn an enormous amount of energy.
Many Third World people use firewood to bake bricks, which contributes to deforestation (New Scientist , vol 96, p 489).
The Indian Institute of Science in Bangalore says that every village would have to add an average of six houses per year to eliminate India's shortage of rural homes.
If all these six houses were built from burnt bricks and potter's tiles, each village would consume 35 tonnes of firewood — about 42 trees — every year.
Across the country, that would mean 23–5 million trees a year.
Clearly, this type of consumption will severely damage the environment.
Firewood will become even more scarce, and its price will soar.
This will make bricks and tiles even more expensive, and in turn reduce demand.
The era of cheap, modern building materials, which coincided with the era of cheap energy, must now draw to a close.
The traditional building materials, on the other hand, offer many advantages.
They are still relatively cheap and more readily available, even though with the destruction of forest and pasture lands many traditional building materials based on plants are already scarce.
Good thatch is difficult to obtain in many parts of the world, and in the Indian State of Kamataka peasants now have to buy bamboo at 1200 rupees a tonne on the free market (about £80 a tonne) whereas the paper industry is still able to get it at 15 rupees a tonne (£1) from the government's reserved forests.
But the importance of traditional building materials is now widely recognised in the housing literature, and many international conferences have paid lip service to them.
The regional paper for West Asia presented to the UN Conference on Science and Technology for Development (UNCSTD), held in Vienna in August 1979 said: ‘Though there exists a rich heritage in design and construction from Hadhramaut to Nubia, there is very little concern for these technologies…these beautiful structures decorate travel literature.’
These ‘beautiful structures’ are largely constructed of mud, the most widely-used material of all, which still provides homes for more than half of humanity.
And it is on mud that the albeit small revival of interest is focused.
Probably the most extensive use of Sun-dried mud bricks today is taking place in rural China.
The Chinese have realised that they cannot build homes for their rural people out of modern materials, even though ingenious mini-cement plants and oval shaped multi-chamber brick kilns have been promoted throughout the communes of China.
Late in 1981 I travelled across northern China, from Peking in the east to Xian, Urumchi, Turpan and Kashgar in the west.
We saw nothing but mud buildings along the way.
Everywhere people had used mud intelligently and in a planned manner, applying the science of structures, and fully exploiting opportunities that mud offered as a building material in the local climate.
For instance, in Xian city (the capital of China, then known as Chang'an at the time of Christ) the annual rainfall is low (750 mm) but still relatively high for fully-exposed mud walls.
None of the walls that we saw there was load-bearing.
The tiled roofs had overhangs to prevent the walls from getting wet during the rains, and were supported on columns of fired bricks.
But the walls themselves were constructed with an infill of mud bricks, set in mud mortar, and smoothed over with a beautiful mud plaster.
To prevent seepage from the soil eroding the bottom part of the mud wall, a layer of fired bricks was set into the wall to separate the mud bricks from the foundation.
With a little maintenance, these houses stand for decades.
In the village of Fenghuo, 65 kilometres from Xian, the villagers have built beautiful blocks of modern, two-storey fiats set along well laid-out lanes.
They had strengthened structurally weak areas around doors and windows with layers of fired bricks set into the mud walls.
The houses are ideal for the climate.
But on the road from Urumchi to Turpan, which passes through the hot, gravelly Gobi desert, the mud buildings had no foundations of fired bricks and no overhanging roofs.
The mud is in no danger from rain, for the rainfall here is very low, less than 450 mm a year.
All the walls there are load-bearing.
The roofs are flat, fully exposed to the rain, made by packing mud on wooden reeds resting on wooden beams.
There is no danger of the walls collapsing under the load of these roofs.
In Turpan itself, summer temperatures touch 47°C in the air and 75°C on the Earth's surface.
Turpan was a major oasis on the ancient silk route, and legitimately was named the ‘Land of Fire’.
Mud is not only the most appropriate building material for such a place because of the low rainfall, only 20 mm per year, but also to ameliorate the living conditions.
Cement buildings would bake their inhabitants alive or demand air conditioners which the Chinese could not afford.
Like Turpan, Kashgar is another extremely beautiful mud city.
In a commune on the outskirts of the city, we saw a new colony of houses being built.
The bricks were being made from the soil in the house's backyard.
The dug-up space was later going to be filled with organic wastes and turned into a vegetable patch.
The Chinese policy is to provide mud and timber to every family as a priority, and thus reduce the use of cement and fired bricks to the minimum.
The tall, fast growing trees that have been planted along almost every road in China — in columns sometimes two to five trees deep — help to provide good timber.
In this way the Chinese are able to give every household the wherewithal to build a decent home.
The most dramatic habitat in China is the cave dwellings in the loess plateau of northern China, which is probably one of the largest loess-covered areas of the world.
Loess is a clay that is easily cut and holds its shape; it can be incredibly accommodating as a building material.
Here the loess is thick, timber is scarce and agricultural land is heavily exploited.
All these factors led the ancient Chinese inhabitants of these parts to live in cave-like dwellings, cut into the loess.
Some caves, technically known as cliffcaves, are dug into natural loess bluffs or mountain sides; others made homes by digging pits, 7–8 metres deep, then building caves underground.
These caves are known as pit-caves; and the pits themselves form courtyards.
The temperature in these parts of China ranges from minus 25°C in January to 43°C in July.
The summer is hot and humid and the winter is cold and dry.
The annual precipitation is low, less than 800 mm.
In such an extreme climate, these cave dwellings have the distinct advantage of being cooler than surface structures in summer, and warmer in winter.
More than a million Chinese live in such cave dwellings.
But outside China, sun-dried bricks are used most extensively in the rich south-west of the United States.
About half a million Americans live in some 176000 adobe houses, 97 per cent of which are in the four contiguous states of New Mexico, Arizona, Texas and California.
Ten per cent of the homes built in 1980 in New Mexico were made from adobe.
The rich people of the southern US like to live in adobe houses because they can virtually be made into sculptures.
But adobe protagonists also value the material's ability to store energy and to stabilise temperatures inside the house — the so-called ‘thermal mass’ effect — and reduce the high cost of air conditioning.
Several organisations in New Mexico have pooled their technical resources to undertake a three-year study, formally known as the Southwest Thermal Mass Study, to measure the performance of both plain and stabilised adobe walls.
This study should generate precise data about the thermal characteristics of mud walls.
Indeed, the enormous demand for electricity to cool homes in the Middle East can partly be attributable to the movement in these countries away from mud structures.
In Kuwait, at the height of summer, up to two thirds of the country's generating capacity goes to air conditioners.
With power-generating capacity failing to keep pace with demand, and occasional blackouts, even oil-rich Kuwait is seriously contemplating programmes to conserve energy.
Could all this lead to a real worldwide movement toward earthen buildings?
It is very difficult to say.
The present interest in mud is not new, and it appears to occur in cycles.
The first big period of interest in earthen structures in this century began in the 1930s and lasted until the early 1950s.
During this phase, researchers in the US experimented with the material in great detail.
Several manuals were published: prestigious journals such as Scientific American reported on the importance of earth as a building material; and in Britain, in 1920, the government built an entire village out of the local earth.
The village, in Wiltshire, is still standing.
This interest in earth arose, as it does now, because of the shortage and high price of construction materials created by wars and preparation for war.
But it soon faded in the economic boom of the 1950s and 1960s.
In developing countries, too, there have been sporadic experiments with mud houses.
The CINVA Ram, a machine for compacting soil developed in Colombia in the 1950s and widely used in Africa and Latin America, is a good example of this effort.
Yet mud remains both the most widely used and most neglected building material in the world.
No school of architecture or civil engineering wants to study or teach about building with earth.
No city or government wants to build earthen structures or allow them to be built.
Most building codes do not permit building in earth.
The entire engineering and architectural profession is thus loaded against the material.
Even the current boom in mud research is imbued with a cement-fixation.
Its entire purpose is somehow to turn mud into a cement-like material — a material with a multi-national answer to housing problems.
Some researchers are pursuing studies in soil stabilisation using cement, lime and bitumen.
These studies pay little attention to whether these added materials will make mud too expensive for most people.
Technically, people in most parts of the world can build stable structures out of mud.
But how mud is used will differ from one climate to another, following the cardinal principle of traditional architecture.
No international solutions with mud are possible, or indeed desirable.
Despite its wet climate, even England had an extensive tradition of earthen buildings before the cement era destroyed it.
In Cornwall, there is a local saying about cob buildings: ‘All cob needs is a good hat and a good pair of boots’, referring to the overhanging roof and the brick layers protecting the foundation.
Finally, certain other elements that have little to do with housing per se , such as proper levelling of site and drainage, are very important for mud settlements.
A drainage ditch around them would probably do more good than anything else.
If all these factors are adequately taken into account — and drainage should be provided in any case to prevent mosquitoes and other insects breeding — mud buildings should last for 50 years and more with adequate maintenance.
But instead of discussing such low-cost, appropriate approaches, the International Workshop on Earthen Buildings in Seismic Areas, sponsored by the US National Science Foundation and held in Albuquerque in May 1981, heard about high technology solutions like ‘prestressed adobe’ and ‘reinforced poured adobe’.
While it is true that most houses in the world's seismic zones are made from earth, and most deaths in earthquakes are attributable to the collapse of earthen structures, it is equally certain that high-tech solutions will never be disseminated.
The attempt to turn mud into a more sophisticated material will result in more seminars and professional activities, but in little real housing for the people.
Already the biggest obstacle to the dissemination of improved techniques of mud building is that scientists and housing experts have decried mud so much that even the poor, who have no alternative to mud, do not want to live in it.
Earth means low status.
The most amazing story is that of China.
Even though the Chinese have promoted traditional building materials and architectural styles and popular participation in the housing process in rural areas, none of this happens in the cities.
Every house in urban China is built by government builders, and they use only prefabricated components of mass housing.
The result is not only hideous, characterless uniformity in Chinese cities, but also a housing shortage of disastrous proportions.
The government never has money to build enough houses.
The per capita residential floor space in urban China was less in 1980 than in 1949 when the communists first took control.
This is despite the fact that the government in China has total control over migration from the country into the towns, and immediately bulldozes any squatter settlements.
In the name of modernisation and professionalism, Chinese architects and civil engineers are not prepared to accept for the towns any of those policies which have made their own rural housing programme a model for the Third World.
Even in rural areas, the Chinese use mud only as an immediate, practical necessity.
Otherwise, mud continues to denote low status as elsewhere in the world.
The rich Fenghuo village outside Xian has taken care not to use any mud bricks in the facade of its new houses.
By the end of the century, the Chinese government hopes that every peasant family will be living in a brick and tile house with its own toilet and water supply: a dream that at the moment looks impossible.
Another powerful reason why improved mud buildings are not catching on in the tropical Third World is that for poor families, housing is not the first priority.
The climate is relatively mild, and food, jobs, water and often even education are more urgent needs.
Until these needs are met, the poor do not bother about the quality of shelter.
But by the time they are met, the poor family has already moved several rungs up the socio-economic ladder and wants a house built with modern materials; not an improved earthen house.
At this point, the family probably cannot yet afford a high quality modern house, but for reasons of status it still decides to move into a sub-standard, ill-constructed house built with modern materials, a house that turns into an oven during the summer and generates demand for electrically-powered cooling devices.
This phenomenon of sub-standard modern housing occurs all across the rural Third World today.
Traditional building materials tend to imply low-rise housing, and urban planners have an ambivalent attitude to low-rise.
The advantages of traditional materials are obvious: they permit an individual to build his own house; involve less capital; can be built in greater variety and according to each housebuilder's choice; and, unlike a high-rise building, do not need expensive construction materials like cement and steel.
But there is a common belief that low-rise building will increase the urban sprawl.
However, many Third World architects are now beginning to appreciate the advantages of low-rise buildings.
‘The five-storey concrete tenement slums built by housing boards all over this country are really the work of pessimists.
What they are saying is: we are not going to have any future,’ says Charles Correa, a leading Bombay architect.
Low-rise houses, on the contrary, can be improved and extended by their occupants as and when it is possible for them to do so.
Residents can do nothing with high-rise buildings once they are completed.
Moreover, says Correa, high densities in Third World cities have not been achieved by building high-rise blocks, but by omitting play spaces, hospitals, schools and other parts of the social infrastructure.
In Bombay, for instance, every thousand people have only 0.1 hectares of open space — and this includes traffic islands.
Several urban planners have also shown that even low-rise homes can give reasonably high urban densities; it is more a matter of equitable distribution of urban land.
Correa suggests that the optimal density for tropical Third World countries is 200–240 people per hectare, which will allow cities to dispense with expensive central sewerage systems and to recycle human and animal wastes locally to provide cooking gas and fertilisers.
‘Under Indian conditions,’ he says, ‘this would have the additional advantage of continuing the pattern of life which people are accustomed to: as though Mahatma Gandhi's vision of a rural India had an almost exact urban analogue.’
In any case, high-rise buildings are seldom built because they are socially needed.
With urban land prices rising rapidly across the Third World, construction companies build high-rise units mainly to maximise their return on what they spend on land.
The land speculation lobby is extremely powerful both economically and politically in most countries and it is difficult to implement any policy that harms its interest.
Mud, as a solution to the world's housing problem, is thus caught up in an intricate economic, social and political web.
The story of mud is one of a resource that has been excluded not for technical but for social reasons.
Mud is today rejected because of the inegalitarian social plan of most developing nations and because it does not allow housing professionals any control over the housing process, and indeed would make them largely irrelevant.
Traditional building materials such as mud will have a role to play only when the national objective is to ensure that everyone has better housing now .
Then there will be no alternative to mud in either rural or urban areas.
For those who have an alternative, mud is clearly no choice.
But for those who have no alternative it is the only choice.