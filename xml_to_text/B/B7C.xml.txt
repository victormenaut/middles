

Test-tube cases
SUING people especially doctors, is a great national pastime in America, and negligence suits against the medical profession are becoming more common in Britain.
In such an environment, an academic speciality known as ‘legal medicine’ flourishes.
Its practitioners have now started to explore the legal hornet's nest likely to be stirred up by in vitro fertilisation.
What will happen.
they ask, when the first defective test-tube baby is horn?
The world's first test-tube baby, Louise Brown, was born in England on July 24, 1978, and since then scores of other children around the world have been conceived in the laboratory test tube.
This week news comes from Australia that this technique has been taken a stage further (p 271).
So far, thankfully, there is no sign that the technique makes it any more likely that a child will be born with congenital defects.
But the procedure does introduce novel hazards.
Natural ‘screening’ processes which may wed out defective eggs or sperm are by-passed, the mother is subjected to a regime of hormone injections to bring on ovulation, and the gametes and embryo are subjected to various mechanical manipulations.
Hence a negligence suit seems likely once the inevitable occurs.
Under English and American law, a child born deformed can sue the doctor for negligence.
Alec Samuels, a reader in Law in the University of Southampton points out in the journal Medicine Science, and Law .
Parental consent to in vitro fertilisation does not deprive the child of his legal right of action.
The difficulty, of course, is demonstrating that the defects arose from the doctor's negligence.
Writing in Legal Medicine 1982 Frank Flannery and James Zimmerly, both of the Armed Forces Institute of Pathology in Washington DC argue that it will be well-nigh impossible to establish a causal relationship between the doctor's actions and the child's defects, as hereditary or environmental factors could be to blame.
To make matters worse,in vitro fertilisation is an experimental procedure and so there are not clearly defined standards of care against which a doctor's actions could be judged.
Because of these difficulties, governments might decide to impose strict liability for compensation on the  practitioners of in vitro fertilisation.
Over 50 MPs recently called for strict liability to be imposed on drug manufacturers.
Under strict liability, the defendant is held liable, even in the absence of negligence, because the enterprise is judged to involve unusual dangers which cannot be eliminated even by the utmost care.
But in order to gain compensation, the child would still have to show that its defects were caused by in vitro fertilisation, and this may be impossible to do in practice.
So the child may still have no redress under law.
The best solution, Flannery and Zimmerly suggest, may be the creation of a government compensation fund which would operate on a ‘no fault’ basis and exclude only those injuries which are clearly unrelated to the use of in vitro fertilisation.
Such a fund would be based on the premise that research into in vitro fertilisation benefits not just the parents but science and society as a whole.
Clearly our troubles are just beginning.
In vitro fertilisation is only the first of a long line of reproductive technologies which may be developed in the future.
Cloning, parthenogenesis (development of the embryo without sperm), and even the creation of cybernetic organisms may eventually be perfected.
Any attempts to put a stop to such developments may fall foul of the European Convention on Human Rights and the American Constitution.
Both guarantee every woman's rights to have a child and everybody's right to found a family.
How will governments justify the violation of such fundamental freedoms?
Awarding experiences
Scientific awards aren't handed out like Oscars — that is, they don't have the same orchestrated ceremonial glitter, with tears choked hack and ‘thanks to everyone back at the lab for making it possible’.
Some people in publishing like to think of theirs as a glamorous medium, so they sometimes copy the Hollywood glitter, They certainly did so last week, when the Periodical Publishers Association handed out its awards for 1983.
For the second year New Scientist was in the running.
Last year,New Scientist was the ‘Consumer Magazine of the Year’.
This year Peter Marsh was runner up in the ‘Specialist Writer of the Year’ category.
(The top prize in this group went to a writer who also started his career in these pages, Duncan Campbell, who now regales readers of another weekly with stories of government intrigue that clearly appeal to competition judges more than do articles on technology.)
Oscars, by contrast, are handed out to an industry that thrives on glittering events and spends money as if it were being withdrawn from circulation tomorrow.
And film makers can be confident that a single prize can earn more than enough to pay for the feasting.
Contrary to rumour, periodical publishing is not an especially glamorous or profitable venture.
(No one believes that last year's award did wonders for the sales of New Scientist —how many of our readers even know about the awards?)
So, in order to lure the winners of the PPA awards along to the chosen banqueting room, the association has to tip the wink to likely winners.
One secret that is kept a little better is the prize list for the ABSW/Glaxo awards for science writing.
Here too New Scientist can claim to have helped a winner on his way.
Robert Walgate, who now works on Nature but who once graced these pages, won an award this year.
(Congratulations Robert.)
We are still looking for a connection between New Scientist and Richard Attenborough's film Gandhi.
The editor of New Scientist spent an evening in the company of brother David not long ago, but that doesn't really qualify.
THIS WEEK
Australia pioneers the freeze-thaw embryo
AUSTRALIAN doctors have successfully frozen and thawed out a human embryo before planting it in a womb.
If all goes according to plan.
the world's first ‘freeze-thaw’ baby should be born this autumn.
Dr Alan Trounson and Dr Carl Wood, Australia's most successful test-tube baby team, announced their new venture with in vitro fertilisation in Melbourne this week.
The mother-to-be. from Victoria.
Australia.
is 14 weeks pregnant.
She had asked for a test-tube baby because her fallopian tubes were irreversibly blocked.
Last year.
following hormonal stimulation, doctors removed four eggs from her ovaries.
All were then fertilised with sperm from her husband.
At the couple's request.
the doctors transferred three of the resulting embryos to the womb two days after fertilisation.
One was implanted successfully, but the pregnancy miscarried after eight weeks.
Meanwhile, Trounson, the team's reproductive physiologist, and his assistant Linda Mohr, had frozen the fourth embryo in liquid nitrogen at the eight-cell stage.
The procedure was in line with ethical guidelines that the Queen Victoria Medical Centre drew up in 1980 (New Scientist .
vol 95. p 74l7).
Australian test-tube teams have been freezing spare embryos since the programme began in 1981 — partly to avoid having to throw unwanted ones away — raising questions ranging from the ownership of the frozen embryo to who pays compensation if it is damaged.
Trounson says that 13 frozen embryos have been transferred over the past two years, but until this week none has been successful.
He said the Melbourne couple.
after the failure of the first implant, asked for the fourth embryo to be thawed and implanted.
The transfer took place during a natural cycle, and, to date, the pregnancy has gone smoothly.
Trounson, who helped to pioneer techniques for freezing the embryos of cattle while working in Britain during the 1970s, says that research in sheep, mice and cattle indicates that vital factors in effective freezing include the chemical composition and the physical size of the embryonic cells.
He will not reveal details of his method of preservation until he has published a scientific paper on the matter.
Some doctors have questioned the need for freezing the embryos, saying that fertility drugs can limit the number of eggs collected to the exact number needed.
Trounson agrees, but says this has several disadvantages: ‘The best eggs may be left in the ovary or discarded, and egg follicles left in the ovary may develop painful cysts which may disturb pregnancy.
‘At the moment, we cannot identify with any certainty the eggs that will develop into normal embryos.
In the case of collecting.
for example, six eggs, three or four may fertilise and develop normally.
But it is possible that more or less than this number may develop.
To provide…the best chance of pregnancy, all the eggs should be fertilised and the required number of embryos immediately transplanted.’
Carl Wood, the chairman of the obstetrics and gynaecology department at Monash University, predicts that the freeze-thaw technique will reduce the number of surgical procedures needed to produce a pregnancy.
‘If a woman did not become pregnant after the first transfer attempt, any frozen embryos could be thawed and transferred in a subsequent menstrual cycle,’ he said.
The logical development of the technique, the establishment of embryo donor banks, will depend on the verdict of committees looking at ethics.
Britain's Medical Research Council.
which published guidelines for research on embryos last year (New Scientist.
vol 95. p 891) does not allow researchers to freeze human embryos.
In the British test-tube baby programme, viable embryos are either implanted or destroyed after a few days.
Ministers may not publish research review
Ros Herman
THE FIRST of the government's promised annual reviews of research in Britain is almost finished and should be complete by the end of this month.
Roger Courtney of the Cabinet Office told science policy-watchers last week.
‘The annual review is, at least potentially, a very valuable tool for analysing the national picture and taking account of science policy studies,’ he said.
The government has, however, not committed itself to publishing the review and, Courtney added, it is unlikely to be published in its  entirety as it may contain sharp judgements about the usefulness or effectiveness of some research activities.
But Courtney said he hoped that at least the factual part of the document would be made public.
Some of the more controversial issues may be raised in a separate document, a joint report from the chairman of the Advisor Council for Applied Research and Development (ACARD) and the Advisory Board for the Research Councils (ABRC).
The first of these documents is likely to appear before any published part of the government's review, and will amount to advice to government on what arrangements the government should make for R&D to best meet national needs Giving ACARD this job as a formal duty is a mark that the Council has established itself as a key body in reviewing and stimulating new industrial technologies.
It will make distinctions between vital and dormant areas, Courtney said.
Both these documents were promised in a White Paper, published last June, which detailed some minor changes to be made to the decentralised system outlined in A Framework for Research and Development , published in 1972.
Among the other innovations described last June were a strengthening of the Cabinet Office's staff resources for science and technology.
Courtney reported that six people report to Dr Robin Nicholson, the chief scientific advisor to the government.
and two more may be coopted.
The new workforce has provided the resources to put together the new reports, and to provide more support for ACARD.
International links were also receiving more attention, Courtney told an Anglo-Indian conference of science policy-makers in Manchester.
Courtney reported how difficult the Cabinet Office was finding the task of procuring comparable figures from the various government departments and agencies for the annual review.
He expected that the report would not be fully comparative for at least another two years.
Government gives a boost to computing
THE BRITISH government last week gave cautious backing to the Alvey committee's proposals for state-funded research into advanced information technology.
Up to £40 million per year, for the next five years, has been earmarked for projects that should keep Britain up with research for the next generation of computer systems.
Four key areas are fast chips, systems that hold human knowledge, methods of mass-producing software and improved ways of talking to computers.
The proposal from the committee under John Alvey, of British Telecom, that, in some cases, as much as 90 per cent of the cost of a project should be government funded has been rejected.
‘There are many examples of big science [projects]in this decade where the results have become increasingly divorced from industry, because the results were not industrially relevant,’ explained Patrick Jenkin, the Secretary of State for Industry.
The government will be putting up 50 per cent of the cost of commercial projects in the four key areas, although academic work will be fully funded by the Department of Education and Science (DES).
Companies which get involved in the scheme will have to undertake to share the fruits of their work.
They will have to exchange information on work they have already done before starting an Alvey project, and agree to license the results to other companies.
There will be no pressure on them to publish these results, however.
Foreign companies with subsidiaries in Britain will be entitled to Alvey largesse, provided the work they do is exploited in Britain and does not end up abroad.
‘We shall need cast iron assurances that the work does not leak abroad,’ said Jenkin.
The £200 million that the government has pledged to Alvey represents 60 per cent of the £350 million total spending proposed in last year's Alvey report.
Alvey had wanted the government to put up three-quarters of the total.
The Ministry of Defence will contribute up to £40 million of this, the DES £50 million and the Department of Industry the remaining £110 million.
There is no guarantee that all this money will be spent; that depends on the organisations which come forward.
Brian Oakley, the secretary of the Science and Engineering Research Council, will head a small directorate of four industry specialists, one for each of the key areas identified by Alvey.
They will report to a board headed by Sir Robert Telford, chairman of Marconi, a subsidiary of the ubiquitous GEC group.
The directorate will be involved in identifying where the money should go.
The Alvey scheme does not cover the business of developing products for sale.
This will be done by the companies taking part.
It is estimated that this will cost 10 times as much as the original research.
Jenkin described the scheme, the largest of its type ever undertaken in Britain, as a research club.
It was, he said, a question of ‘collaborate in the laboratory, compete in the market’.
The programme will begin this summer.
At present there are no more than half-a-dozen British products in the fields covered by Alvey and most of them are being used experimentally.
The German computer company Nixdorf recently announced an expert system designed by a man called Stuart Savory.
He is British.
Cable will undermine the BBC
MINISTERS also published their plans for the ‘cabling of Britain’ last week.
As widely predicted, they have put aside the advice contained in a report by Lord Hunt last summer and have decided to allow pay-TV to be transmitted on the proposed cable network.
The aim is that entertainment on pay-TV will pay for the more innovative plans, including two way transactions down the cables, that lie at the heart of the cabling idea.
One unremarked effect of the white Paper on cabling, which was published last week jointly by the Home Office and the Department of Industry, is to undermine the BBC's monopoly on a second broadcasting revolution set for later this decade — direct broadcasting from satellites (DBS).
The Home Office handed the BBC that monopoly only last year and the BBC has since agreed to pay a British satellite consortium, Unisat, £22 million to use satellites to make use of its supposed monopoly.
Cable TV and DBS are virtually inseparable.
The BBC needs a cable network to distribute its two planned channels of satellite programmes, since few people will want to buy their own dish aerials to receive signals direct from the satellites.
Also, the Unisat satellites will hover low in the sky (over Brazil, in fact) and so will be obscured from many houses by buildings and trees.
For them, cable from a local receiving station would be the only possible means of getting the programmes.
But BBC officials are waking up to the fact that, while they have been allocated both of Britain's channels for high-power satellite broadcasts, the kind that could be received direct into people's homes, they have no monopoly on low-power transmissions from satellites, the kind that could be picked up by central receiving stations — and transmitted via cable to homes.
In recent months, the Independent Broadcasting Authority and Unisat have been negotiating over the use of low-power transmitters by the IBA.
Last month British Telecom promised that such transmitters would be available of Unisat.
And now the White Paper clears the way for these to be used by cable stations.
Radical mop-up saves brain cells
RESEARCHERS in Texas may have found a way of reducing brain damage in road accident victims who have been drinking excessively.
It involves the use of DMSO (dimethyl sulphoxide), which has long been the centre of controversy.
Dr Maurice Albin and others from the Department of Anaesthesiology at the University of Texas, Health Science Center in San Antonio, believe that DMSO will reduce the size of lesions by mopping up highly-reactive substances called free radicals, which are produced in the brain luring the breakdown of alcohol to acetaldehyde.
Left to their own devices, these free radicals cause tissue damage.
Albin's work has been spurred by the large number of road accident victims admitted to the hospital at the Health Science Center, many of whom have been drinking.
His research began several years ago with the design of an instrument for exerting pressure on the brains of anaesthetised experimental animals, and mimicking human brain damage during injury or stroke.
The technique can continually measure how much pressure is being exerted.
Albin reported that DMSO was the most effective of seven methods used to treat the resulting brain damage.
In a later experiment, Albin found that brain injury could be five times greater when the damage was accompanied by a blood alcohol level of 200 micrograms per decilitre, twice the legal limit in the US.
His latest work, reported this week at a meeting of the American Association of Neurological Surgeons in  Washington , indicates that DMSO, administered intravenously, can reduce brain damage.
Again, anaesthetised mongrel dogs were used in the experiment.
They were given ethanol for an hour prior to a blade being inserted into their right cortex.
Albin says it was used indiscriminately as a panacea for ailments such as rheumatism, sprains, kidney disorders, and intestinal conditions.
Consequently, its only approved use under-FDA regulations is in the treatment of cystitis.
Albin adds that DMSO seems to improve blood flow and levels of oxygen around the brain and could be used to restrict the effects of certain kinds of stroke.
No nuclear protection for Navy's  volunteers 
Mark Hewish
BRITAIN'S Ministry of Defence has decided not to protect the voluntary civilian crews of the Royal Navy's new minesweepers against radioactive fallout, in order to save money.
This is despite the fact that the ships would, in war, have to sweep nuclear mines and operate from bases against which the Soviet Union would probably use nuclear bombs or missiles.
The Royal Navy's existing modern warships have what is known as a citadel.
The crew can seal off all vital areas of the ship during a nuclear war, preventing radioactive fall-out from entering the vessel.
Seals around the hatches provide a physical barrier, and air for breathing is filtered, so that the crew does not have to wear protective clothing.
‘Pre-wetting’ provides further protection — jets of seawater spray all over the ship's superstructure, so radioactive dust cannot settle.
The new Gem class of minesweepers will not have a citadel or a pre-wetting facility.
Nor will they carry a damage-control headquarters from where the crew would fight the fires that break out when a missile or bomb hits the ship.
Richards Shipbuilders has the first four Gem-class minesweepers under construction in its East Anglian yards at Great Yarmouth and Lowestoft.
The first will join the fleet in just over a year.
The Royal Naval Reserve, composed of civilians who do regular training, would man the new minesweepers in wartime.
The ships will form the tenth mine countermeasures squadron, based at Rosyth on the Firth of forth, where they would attempt to keep the sea lanes clear for Britain's nuclear hunter-killer submarines.
The Royal Naval Reserve is trying to get the ministry's decision overturned so that the new minesweepers can play an effective rôle in war.
Mururoa tests bring renewed fears
THE RESUMPTION of nuclear tests by France at the Mururoa atoll in the Pacific, last month, has renewed fears that plutonium and other pollutants from past tests may be spreading throughout the Polynesian islands.
Since last year's season of tests, one cyclone, Reva, has swept right across the atoll and on over the nearby Gambier islands.
The winter's exceptionally violent cyclones delayed this summer's series of tests, while checks of the underground testing site's integrity were completed.
But another storm has blown up because cartographers at the French meteorological office inexplicably moved the Gambiers islands from their exposed position, just south-east of Mururoa and in the path of cyclone Reva.
In a map of the cyclone's path they turn up several degrees north and away from Reva's course.
Was this a simple gaff or a plot to lessen fears that radioactivity may have been swept by the cyclone to the Gambiers?
French environmentalists subscribe to the conspiracy theory.
There has been growing concern among Polynesians about radioactive pollution of the area.
Some 800 grams of plutonium lie in the sediments of the Mururoa lagoon, according to official estimates.
It is the fallout of atmospheric tests which ended in 1974.
In 1981 violent storms redistributed some of the plutonium, along with other radioactive wastes stored ashore.
It produced what the French defence minister, Charles Hernu, described to the French parliament as a ‘new radioactive situation.’
At the atoll, Polynesians worry that the radioactivity could be spread beyond Mururoa by storms such as Reva.
No one knows whether the area is contaminated or not.
All surveys are kept secret.
Nor can local suspicions that the incidence of cancer is unusually high be calmed or confirmed.
Health statistics for the area have not been published since atomic tests began in 1966.
Last June, the Defence Department sent scientists, led by Haroun Tazieff, a prominent vulcanologist and President Mitterrand's commissioner for natural disasters, to Mururoa.
After three days of examining records, and taking air and water samples during a test and, according to Tazieff, verifying the safety of the lagoon by swimming in it — they pronounced security conditions ‘satisfactory’.
The team called the study preliminary but there are apparently no plans for a follow-up, and the planned publication of the results has not happened.
None of these goings-on has done anything to calm the Polynesians.
The territorial assembly has been calling for the tests to stop until proper health studies are done.
Australia and New Zealand are also protesting.
Tazieff maintains, however, that there is no contamination or any release of radio activity from underground testing, and that it is ‘scientifically impossible’ for radio active debris to be spread beyond Mururoa by storms.
Defence chiefs make friends
THE FRENCH defence ministry spent two days last week trying to patch up poor relations with scientists.
Defence activities employ a third of France's 270 000 researchers But the ministry's bad image among many scientists has persuaded it to woo the science community more explicitly.
It invited 1000 carefully selected participants, all French, to a colloquium at the Ecole Polytechnique, and gave them the hard sell.
The defence establishment is faced with a dual problem.
First it needs to boost its efforts to get wind of military-useful technology at an early stage.
Second, it is trying to persuade researchers that it is a good thing to work for the military.
Apart from any moral objections, researchers are scared that they will be deprived of fame and fortune by military secrecy.
In the past, inventors have been known to rush into print with their new discoveries to prevent the military stepping in and classifying it.
The defence authorities have had difficulties in finding researchers to cooperate with them in some fields.
In areas, such as particle physics, there are a large number of scientists who do not wish to work for the military.
So defence chiefs and the minister himself.
Charles Hernu, bombarded the conference delegates with sweet talk on how useful defence research was for the civil sector and how good they were to researchers.
More spending on fundamental research, which has stagnated in the defence budget for a number of years, was announced.
One official told the conference: ‘It's the military which is the first to make concrete use of fundamental research’.
Willis has it taped
LORD Ted Willis has introduced an extraordinary bill into the House of Lords which would effectively make it illegal for British shops to sell any audio or video tape recorder.
Although it is intended to protect the film and record industries, the Bill outlaws all cassette recorders used for home computing, because they can also be used to tape music.
It's no excuse that the salesman did not know what the customer planned to do with the recorder, even if it is labelled with a warning about not taping copyright records or films.
The Willis Bill, which is due for a second reading in the House of Lords on 6 May, says that anyone who sells, distributes or otherwise manes available am machine capable of reproducing a sound recording or cinematograph film shall be deemed to have authorised infringement of the copyright, in a sound recording or film Although the Bill has little chance of getting any further it will achieve one aim — to generate publicity for the record and film industry's call for a tax or levy on blank audio and video tape.
Knowledge engineers appointed
ICL, BRITAIN'S only manufacturer of large computers, has set up a ‘knowledge engineering group’ to exploit so-called expert systems.
These are software products in which the expertise of a particular discipline medicine, say — is encapsulated within a computer program so that non-experts can make use of it.
The group, which will initially amount to about 20 people, will be based at ICL's new development centre at Gorton in Manchester.
Its main aim is to give ICL a stake in international markets when expert systems become common place around 1990.
But the company believes that it has some products it can sell new, notably software for diagnosing faults in computers.
The group will concentrate on developing the technology, rather than producing systems it will work closely with the University Research Group set up by ICL a year ago.
Expert systems will be key elements of ‘fifth generation’ computers and ICL's announcement came the day after the government gave its response to the Alvey report (p 272)
Yellow rain: the scientists' verdict
Christopher Joyce, Washington DC
SCIENTISTS from around the world have met in Cambridge, Massachusetts to weigh the evidence on whether chemical war is being waged in South East Asia.
The two-day gathering brought together the widest range of disciplines yet assembled under one roof to discuss the American government's hotly-contested claim that the Soviet Union is backing a ‘yellow rain’ of terror in Laos and Kampuchea.
Both public and press were kept away.
According to participants spoken to by New  Scientist  .
however, there was no consensus on how to fill the gaps in the government's case.
Several scientists questioned the hypothesis that the Vietnamese, Kampuchean and Pathet Lao governments are spraying deadly mycotoxins, produced by the fungus Fusarium and commonly called ‘yellow rain’, on opposition groups like the Laotian Hmong.
Canadian scientists have performed some epidemiology on the alleged victims of attacks who are streaming into refugee camps in Thailand.
According to H. B. Schiefer, a toxicologist at the University of Saskatchewan, Canadians at the meeting argued for more studies of the alleged victims.
Were the cases of poisoning and disease, now acknowledged as extraordinary by virtually all observers, the result of natural contamination or warfare?
Species of pathologists during their investigation.
According to Gary Crocker of the State Department, government scientists tested the hypothesis that these species of Fusaria occur naturally in the region, but rejected it.
‘For the US government to seriously continue to pursue natural contamination as a real possibility seems kind of ridiculous,’ Crocker said.
‘However, we would pursue it to prove our case.’
The value of the meeting, he said, was to help point the government in Washington towards a more scientifically conclusive case.
State Department officials say the presence of Fusaria is not surprising.
But only a few rare species are known to produce the toxic trichothecenes.
One of the species that does, according to a State Department report last year, is Fusarium semitectum .
Schiefer says he brought back from Thai land environmental samples taken well away from areas of reported attacks.
He found spores of Fusarium semitectum on a banana sprout.
But analysis in Canada showed that the fungus did not produce detectable mycotoxins when cultured.
A Thai mycologist, Samaniya Sukroongreung, presented the meeting Fusaria have been discovered naturally in South East Asian soil.
But so far no one has determined whether they are of the type that produces the trichothecene toxins.
US officials have found these toxins, including T2 and other mycotoxins, in soil, water and rock samples taken near areas of reported gas attacks.
The products of mycotoxins have been found in tissues of alleged victims by both American and Canadian with an analysis of several ‘yellow spots’ gathered from leaves eight kilometres from the Kampuchean/Thai border.
They were collected between 22 February and 15 March, 1982, from the Pong Namron district where the State Department says, an airborne attack took place on 19 February, 1982.
Thai scientists at Mahidol University found pollen and ‘fungal elements’ on most of the 22 spots analysed.
They also found Fusarium semitectum on two spots.
A nearly-pure culture on one of the Fusaria was inoculated into rice and corn seed and incubated.
Extracts of the crops were fed to four mice; three died within two days.
The Thai report summarises: ‘The possible producers of trichothecenes exist in Thailand.
Either they are in nature, or they were mixed together with the yellow spots of unidentified sources.’
American scientists analysed samples of leaves taken from the same district.
They found levels of the toxin T2, up to 86 parts per billion and another toxin known as diacetyl-nivalenol, at 30 parts per billion.
The State Department has cited these findings as evidence of deliberate attack.
Crocker expresses ‘confusion’ at the Thai conclusion that their positive samples could have been natural since a wide swath of the district was reported to have been sprayed with yellow rain.
More reports of naturally-occurring Fusaria continue, however.
Dr Bert Pfeiffer, a physiologist at the University of Montana, has obtained samples of soil and leaf litter from a national park in Vietnam that is far from combat areas.
All the samples contain species of Fusaria .
Two of the ten cultures made from the samples have failed to show trichothecenes; the other eight await analysis.
The Cambridge meeting included pollinologists — experts in the analysis of pollen, which has been found in most of the samples of yellow rain.
One scientist, quoting a pollinologist, recalled: ‘He said he had never seen anything like it — windblown pollen mixed with insect-borne pollen.’
State Department spokesmen see this odd combination as evidence that the Soviet Union may be using pollen as a carrier for the toxins.
One European scientist noted the potential military danger of a biological weapon containing vomitoxin, a substance that has been found in some of the samples of yellow rain.
A vomitoxin can be absorbed through the skin, causing a soldier to remove his mask to vomit and thus exposing him to other toxic agents, through his respiratory system.
The main sponsors of the meeting, Professors Matthew Meselson of Harvard University and Carl Kaysen of the Massachusetts Institute of Technology, declined to answer questions about the meeting, Those who did speak to New Scientist did not know whether a summary of the discussions would be made public,
New directions for signposts
Mick Hamer, Hamburg
DRIVING is a perilous business in the south-west of England, according to new evidence unveiled at the World Conference on Transport Research in Hamburg last week.
Apparently, one out of every 16 signposts at crossroads in the region are pointing in the wrong direction.
This fact has emerged during a pilot test of a new computer program developed by scientists for the Department of Transport.
The program is called SIGNPOST.
It contains a complete map of the British road network and an inventory of all the road signs.
The aim of the program, according to John Wooton, the consultant who developed it, is to check for inconsistencies and errors in the national signpost network.
The computer can check for towns that are missed off signs, those that appear for no good reason or appear only intermittently, to the great confusion of amateur navigators.
And it can quickly produce a list of signs that need to be altered for a relief route or during a major road repair.
Apart from uncovering hundreds of signs pointing in the wrong direction, the survey of eight counties in South West England also discovered that, to bring about consistency, an average of seven changes to place names were needed on every road intersection That is roughly equivalent to two changes on every standard rural signpost, such as the one shown in the photo graph (left).
Britain has now adopted the SIGNPOST program and West Germany is also testing the system for possible adoption.
Heads it causes cancer, tails it doesn't
THE MOST widely-used test to detect chemicals that can cause cancer in humans is no more reliable than tossing a coin, says Dr David Salsburg, a statistician with a large American drug company.
Since the late 1970s, scientists have hunted down potential human cancer agents, known as carcinogens, by feeding very high doses of suspect chemicals to mice and rats.
After the animals die, investigators compare the pattern of  tumours produced by the dosed animals with those of matched controls, which did not receive the compound.
Although government agencies and industry use this test routinely, says Salsburg, its actual ability to detect carcinogens has never been properly confirmed.
Salsburg is an employee of Pfizer Laboratories in Connecticut.
He examined the published data on 170 compounds tested in rodents by the National Cancer Institute (NCI) up to June 1980.
He also reviewed the literature on known human carcinogens.
He then applied a series of increasingly restrictive statistical ‘decision rules’ to see how well the test handled the twin demons of false negatives (that is, the failure to detect a carcinogen) and false positives (registering an innocuous substance as carcinogenic).
His conclusion: ‘We have an assay test that may be wrong 50 per cent of the time’.
Regulatory policy on carcinogens under President Carter favoured erring on the side of safety.
The percentage of false negatives generated by a cancer test was therefore of great concern.
Yet Salsburg found that, of 19 known human carcinogens, only seven have been shown to cause cancer when fed to rodents.
Given those odds, claims Salsburg, one might as well flip a coin.
NCI judged 50 per cent of the 170 compounds to be carcinogenic.
But Salsburg wondered how many of these might be false positives.
The direct way to assess this is to feed the rodents with large doses of substances that are known to be innocuous.
Salsburg says this was never done for the NCI test, which throws doubt on the high percentage of compounds found to cause cancer.
And Salsburg found that, when he applied statistical tests to the NCI data, between 3 and 70 per cent of the compounds either caused or protected against cancer, depending on the stringency of the decision rule used.
The National Toxicology Programme which is jointly run by the main federal agencies responsible for public health, is charged with developing and promoting toxicology tests in the US.
The programme's deputy director, John Moore, says Salsburg's critique contains ‘distortions and errors’.
The high number of positives is a result of NCI's ‘conscious bias’ toward selecting test substances that were suspected of posing a risk, Moore argues.
As for the huge variability in the number of contradictory results, Moore says that NCI applies only the most stringent criteria to their decisions: a significant turnout response to a given dose in two species or two sexes.
Moore dismisses Salsburg's coin-flip analogy as ‘not meaningful’, but admits to inadequacies in the two-year feeding studies.
Salsburg is not the first to criticise the rodent assay.
Several years ago, the Occupational Safety and Health Administration held hearings on the problem of identifying and classifying potential carcinogens.
The scientists split into two camps over the validity of animal studies.
Advocates said they were reliable and indispensable; critics, however, disagreed on a number of points.
California to ban research on stray cats
RESEARCH laboratories in California may soon be forced to abandon their use of stray pets for biomedical research.
University officials claim that a new bill before the Californian legislature will quadruple research costs and halt work altogether.
The bill would prohibit the sale of unclaimed stray pets from pounds to research institutions and would require researchers instead to buy specially-bred animals.
Supporters of the bill say that animals that have been kept as pets are too emotionally sensitive to be put in the hands of medical researchers.
The battle lines over animal research are being drawn in California.
A group called the Urban Gorillas has set cats free at the University? of California-Berkeley.
Demonstrations have been held at the primate centre run by the University of California-Davis.
Humane societies plan to file charges against Stanford University and the Palo Alto Veterans Administration Hospital following the discovery of a badly wounded Samoyed dog in the corridors of the hospital, where Stanford researchers were working.
The Stanford School of Medicine (which has some 8000 animals in stock) and the University of California-San Francisco claim that ‘the legislation creates a great irony by increasing the number of animals which each year will be euthanased instead of being transferred to research facilities.
Meanwhile, other dogs and cats will be bred specifically for use in the laboratory.’
The two universities claim that they need stray animals to maintain a genetically mixed animal model rather than a purebred strain.
Officials at the University of California estimate that a ban on buying animals from animal shelters or pounds would cost them up to $3 million a year.
The university uses about 7000 dogs and 2000 cats per year.
Britain's space salesman looks East
Peter Marsh
KENNETH Baker, Britain's minister for information technology, is planning a mission to India to persuade the country to invest in British satellite technology.
India has collaborated with several nations on space projects, but so far Britain has missed out.
British Aerospace could be the main industrial beneficiary if the Indians decide to place contracts in the UK, perhaps for space vehicles to carry telephone calls or TV.
The company is building several satellites for European organisations but has yet to win an order from the developing world.
Baker is keen to develop with India the kind of relationship on space technology that Britain has with Canada.
The two nations are collaborating on several satellite projects, including the L-Sat communications craft that British Aerospace is building for a launch in 1987.
During the trip to India — which, British elections permitting, should take place within a few months — Baker will emphasise government support for any satellite deals that India makes with private industry, for example, the state could provide loans to pay for the work.
India has an ambitious space programme.
Since 1980 it has put into space three experimental satellites with its own SLV-3 launcher.
The last launch, scheduled for 1982, took place last month.
Besides developing its own technologies, India has not hesitated in turning overseas for help.
Soviet rockets took into space two Indian satellites in 1979 and 1981.
And two years ago, the European Space Agency's Ariane launcher put into orbit a prototype communications craft built by the Indian Space Research Organisation.
But, last year, efforts at developing a cooperative approach with the US turned sour.
An American Delta rocket successfully launched India's first operational communications satellite.
But the craft, built by ford Aerospace of Detroit, developed a fault, seemingly because of a design failure.
The Indian government is claiming $65 million to compensate for the craft ceasing to work last autumn.
Japanese computer firms in court again
American computer firms are not letting up on their law suits against ‘copy cat’ Japanese rivals.
The latest firm to take to the courts is Zilog, which makes chips.
Zilog has filed a complaint against two subsidiaries of Nippon Electric in the US, accusing them of copying its Z80 microprocessor.
The Z8Q is an 8-bit processor with worldwide sales.
It is used in the majority of microcomputers on sale in Britain.
The complaints, filed in San Francisco against NCC Electronics and NEC Home Electronics, allege that the companies infringed Zilog's patents and copyright.
For good measure the Americans also accuse Nippon of unfair competition and violation of trademarks.
The Japanese chips in question are the PD780 and the PD780-1.
They were both introduced in 1979 and now account for 30 per cent of sales in the US of Z80-type microprocessors.
Zilog accounts for another 40 per cent of sales and the rest is made up of Z80s manufactured under licence from Zilog by companies like Mostek, SGS and Sharp.
Last year more than 12 million Z80 chips were sold around the world and incorporated into computers, telecommunications equipment and peripherals.
Zilog wants Nippon Electric to stop selling its PD780 chips and to Pay unspecified damages.
New alarm Over Smoke detectors
Michael Cross
DON'T rest too easy in your hotel bed next time you take a break.
Experiments at the government's Fire Research Station have revealed that one of the main safety precautions that British hotels take against fire could be useless.
Smoke detectors installed in hotel corridors have an alarming ability not to detect smoke.
Hotel fires are not a big killer in Britain.
But, when they happen, they attract publicity, and tighter regulations.
All hotels now have to satisfy a local fire officer that their precautions are up to standard before they can do business.
At the moment, the first line of defence is usually a smoke detector on the ceiling in every corridor.
If a fire starts in a room (and most hotel fires do), the smoke leaks through the door, rises to the ceiling and sets off the alarm.
Anyone staying in the hotel should have plenty of time to escape before smoke blocks the corridors.
But a series of experiments in an old airship hangar near Bedford have shown that the sequence of events may not be as simple as that.
The fire Research Station's fire Detection Department built a full sized hotel corridor and, for the first time, has monitored what happens in the early stages of a typical hotel fire.
‘The important thing is to know the time delay between when the fire starts and the detector operates,’ says Peter Burray, head of fire detection at the station.
If the delay is too long, people will not walk through smoke to an emergency exit.
But video films of fires carried out on the test rig suggest that alarms will sound too late.
Instead of drifting along the ceiling of the corridor, the smoke moves along as a solid plug.
By the time it sets off the smoke detector, the corridor is blocked.
The fire researchers cannot say why the smoke behaves as it does — only that the movement is far more complex than they believed.
‘It is quite possible that the corridor could fill with smoke without the detectors knowing,’ Burray said.
Among the factors that affect how smoke moves in a building are the weather outside, the type of heating installed, and the position of emergency light fittings.
Scientists at the fire Research Station are developing a computer program that can model how smoke behaves — but, with the many variables involved, it will be a long job.
Until computers can take over, the only way to carry out experiments is to build a full-sized test rig, and set it on fire.
The research suggests that hotels would do better to install smoke and heat detectors in all rooms — an expensive procedure.
But, even if they did, experience shows that people usually react far too slowly to an alarm, particularly in the middle of the night.
The research station's scientists comment privately that only a big fire disaster will persuade the government to look harder at fire research.
Can you copyright a computer's syntax?
John Lamb
ANYONE using a computer language could end up having to pay royalties for the privilege, if a British court case being brought by a computer company called Filetab Support Services is successful.
The case will establish whether the syntax of computer languages can be held copyright.
Filetab is trying to prevent a firm called Ace Microsystems from selling a computer program on the grounds that the language in which it is written is owned by Filetab.
The case is due to be heard in October.
The language at issue is called RPL-11.
It was devised by Tom Barnard, now a director of Ace Microsystems, between 1967 and 1972.
At the time Barnard was working for the National Computing Centre, a quasi-governmental organisation in Manchester.
The language was incorporated into a program called Filetab, which was designed for looking up information held on file in mainframe computers.
In 1972 Barnard left the NCC and worked for Airfix, the model company, on producing a version of Filetab that would work on its new Digital Equipment PDP-11 minicomputer.
When Airfix went bust in 1980 the rights to Filetab for the PDP-11 were sold to filetab Support Services.
Meanwhile Barnard, who lost his royalty rights to this particular version of the program, wasted no time in setting up Ace Microsystems.
The company produced a program for handling files, using a technique which Barnard had originally used as the basis for Filetab.
The result was a program called Lex-11, which runs on both the PDP-11 computer and a number of 16-bit microcomputers.
The language that Ace Microsystems used was called D-1 1.
Lex-11, can be used for word processing as well as organising files.
It now has 700 customers in Britain.
So far, filetab Support Services has tried four times to get injunctions against Ace Microsystems to prevent the company from selling Lex-11.
At one hearing, filetab Support Services claimed that the syntax of D-11 was copied from its language, RPL-11.
More than half of the commands in D-11 were the same as those in RPL-11.
While copyright cases have been won on the grounds that manuals, and other documents that go with software, have been copied, no one has yet been able to claim copyright on the syntax of a computer language.
What's wrong with German science?
Ros Herman
German scientists are worried that their research efforts are not receiving international recognition.
A shortage of jobs for young researchers is partly responsible — but another important factor is the too rapid expansion of the universities
THE FIRST and most serious change that Professor Horst Rollnik has seen during his 19-year long career at Bonn is the vast growth in the number of students, in all subjects including his own, physics.
Bonn, a university built for 10 000 students, now attempts to teach no fewer than 36 000.
The huge expansion began in the early 1960s, when the federal government decided to change the whole character of higher education in Germany.
After the war, German universities had rebuilt themselves with much help from overseas, particularly the US, but little direction from central government.
The basic education law passed in 1949 gave the responsibility to local, state governments in accordance with the general principle of the government of post-war reconstruction — that decentralised government was less likely to lead to a re-run of the Nazi regime.
It also laid down the constitutional right for anyone who passed the Abitur — the German equivalent of A levels — to study at university.
In the late 1950s a perceived need for many more graduates, combined with political pressure for a less elitist system, resulted in a more centralised approach, with the creation of a Minister for Education and Science in 1969.
Over the next few years, the coalition government worked on the framing of a federal law that laid the foundations of a much larger system of higher education — and opened the door to far more state control over the universities.
By the early 1970s the increasing demand for higher education forced the government to lay down a new structure for higher education, embodied in the Hochschulrahrnengesetz.
This ‘Framework Act for Higher Education’, finally passed in 1976., dictates in detail how universities should be run.
It sets down exactly who can teach and who can study, and on what terms' democratic procedures for taking decisions, involving votes by staff and students' and rules and regulations for examinations.
In short it imposes a uniform structure on the universities (Wissenschaftliche Hochschulen and Kunsthochschulen) and polytechnics (Fachhochschulen) which together with some extras such as teacher and theological training constitute Germany's current provision for higher education.
Those who thought Germany's future lay with a well educated populace were not disappointed.
In 1950 there were 174 000 students at the universities just beginning to emerge from the painful post-war reconstructions.
By 1970 there were more than half a million students at a now expanded network of higher education, and in 1980 the number exceeded a million for the first time, taking in roughly a quarter of the relevant age group.
This number includes 220 000 students at Fachhochschulen, roughly equivalent to our polytechnics.
The growth of student numbers took place far faster than in the UK — and the consequences for the nature of university education have been correspondingly more dramatic.
Firstly, hopes that opening the universities to lower social classes might reveal untapped reserves of brain-power have been disappointed.
Rollnik, for example, finds that he has roughly the same number of really bright students in a year now as in 1960.
So the average standard has fallen.
Far more important than this change, though, is the irreversible change in the character of universities.
They began as small, elitist institutions that owed much to the 19th century concept of the German university.
That, roughly speaking, amounted to a community of scholars surrounded by a small number of dedicated, respectful and conscientious students eager to catch pearls of wisdom from their professor's lips.
Now the teachers tend to describe their places of work as ‘mass institutions’ where students register and drift into and out of courses at will and with little supervision.
Bonn's roll of 36 000 is by no means untypical: Munster has more than 40 000; Aachen's technical university nearly 32 000: Koln nearly 40 000.
The University of Munich has nearly 45 000 and Berlin's free University has more than 47 000, with another 25 000 or so at the Technical University there.
At first, the expansion in student numbers was more than matched by the creation of new academic jobs.
Numbers of teaching and research staff in higher education grew from 19 000 in 1960 to 78 000 in 1975.
In the mid-705, in the Federal republic as elsewhere in the West, the oil crisis gave economic planners a jolt and the bonanza in education spending was over.
Since then, the student numbers have continued to rise but staff levels have not (see graph).
Heavy teaching loads are another fact of life for the modern German academics and are often quoted as a bad influence on research.
In fact the staff student ratios of the early 1980s are not much different from the pre-expansion figures, though they are generally not so good as in Britain or the US.
Those with a broad view of the university system see another factor as more important: that the lowering of standards applies not only to students but also to teachers.
Quadrupling the number of academics in 15 years  inevitably led to some appointments of people ‘not of the first rank’, as some politely express themselves.
Many academics, being civil servants, have secure jobs guaranteed for life.
Moreover, legislation associated with improving workers' rights in Germany took control many of the important details of university life, such as departmental policy and appointments, out of the hands of the heads of departments and transferred them to committees of faculty members at all levels.
In arts and social science  particularly , this led to appointments made for political rather than academic reasons.
We heard a lot about the politicisation of the universities in the late 60s and early 70s: academic life in Germany was punctuated by demonstrations and strikes, which also helped to disrupt university life.
Meanwhile, the right to study now embodied in the constitution was causing its own problems, particularly for the natural sciences.
Even though the university system grew rapidly, demand for places in some subjects began to outrun supply.
In 1967 the association of university rectors established a central registry for applicants in the most popular subjects: medicine, dentistry, veterinary medicine, pharmacy, architecture, psychology, biology and chemistry.
But disappointed students, brought up with the idea that studying the subject of their choice was their legal right, appealed to the courts, who agreed with them, and so criteria for student selection became a matter for central government.
By the early 1970s the federal government had devised a formula for limiting student numbers in over-subscribed subjects: the notorious ‘numerus clausus’ rule.
To add insult to injury the business of selecting students and allocating places was computerised in 1973, and so it remains today.
Subsequent appeals to the courts by disappointed students led the federal government to devise complex formulas that fix minima for the number of students each university department must take.
This too has taken its toll on the traditional freedoms of university life.
An international team of distinguished academics reported recently in the journal Minerv'a that ‘We were particularly amazed to discover that the autonomy of German universities to make admissions policies and admissions has been substantially curtailed by the establishment of a single central admission procedure, the ZVS, located in Dortmund and utilising a programmed computer’.
Medicine, a respected and lucrative profession in Germany as elsewhere, is the discipline that is most oversubscribed, and medical academics are forced to take on far more students than they feel they can reasonably teach.
The faculty of medicine at the University of Munich, for instance, has to take on 350 students every half year.
Each of these students has to be taught neurology in a hospital department that has fewer than 100 beds.
The international academics said, ‘There is a real risk that scholarship and learning will suffer from capacity investigations’.
The capacity rules are just part of a now long list of rules and regulations that surround university life, and make extra demands on the time and freedom of academics.
As one of Bonn's most distinguished physicists, Professor Wolfgang Paul put it, ‘Now it's so well-organised bad things cannot happen, maybe good things cannot either.’
Over the last two decades, then, the place of the university in German life has become most sharply focused on the teaching rather than the research role of the university, and the academics.
But the politicians have also seen research in the arts, social and natural sciences, as a vital part of national development: the government has been a generous sponsor of research, partly to underpin West Germany's thriving high technology industries, and partly to continue rebuilding intellectual life after its almost total dissolution in the 1930s.
In law and in fact every academic has the right to do research, and the universities provide all faculty members with small-scale financial support for this.
As in the UK, researchers who need more for a specific project can also go to funding agencies which can be public, such as the German Research Society (Deutsche Forschungs gemeinschaft or DFG), or private — the Volkswagen research fund is the biggest of these.
The federal government also plans and builds the largest and most expensive central facilities with the active participation of the state governments, who can also be persuaded to find fairly large experimental facilities at universities.
West Germany is fortunate to have a body, the Science Council (Wissenschaftsrat): a fairly influential parliament of scholars that coordinates the planning and execution of such projects between federal and state governments and the scientists in universities and research institutes.
Research institutes are a well-established tradition in Germany: the Max Planck Institutes are internationally famous havens where research can be pursued without teaching responsibilities.
Some have pointed out with justice that the Max Planck Institutes have bled away the best research talent from the universities.
But in numerical terms this factor can hardly have made a major impact — the Max Planck Institutes permanently employ only 2000 researchers, compared with a total of 76 000 university academics of whom more than 20 000 are professors.
Governments also use the more industrially orientated labs under the umbrella of the Fraunhofer society.
Despite a plentiful supply of money and a stable frame work, there is much depression about the state of research in Germany, particularly in the universities.
But it is difficult to pin down the symptoms.
Professor Wolfgang Wild, president of Munich's Technical University has provided some pointers.
For example, he has counted the numbers of German Nobel prize winners.
Between 1901 and 1944 Germans won 34 Nobel prizes: between 1945 and 1982 only 12.
In that period 102 North Americans and 34 Britons were thus honoured.
The Science Citation Index also indicates that the outside world has not been developing ideas form German scientists: 2 per cent of citations in 1975 were for German papers compared with 4 per cent for the UK and 3 per cent from Japan.
Determined to delve deeper into the matter, the DFG, one of the major channels for state funds for research which distributes DM 900 million (£200 million) annually for all subjects, has asked some of Germany's top researchers to review their own areas and try to pin down some of the problems.
A familiar plaint: no jobs for the young
High on the list of complaints is the lack of jobs for young researchers, which discourages bright graduates from going on into research.
Every British academic complains of this problem, but the Germans have it worse because they are almost 30 before they get their degrees, and cannot expect an academic job until they get a doctorate and a further qualification for academic teaching, called the habilitation.
Few people pass this hurdle much before they are 40, at which point they are far too old to begin any other type of career.
As in the UK, bright young people find it extra galling that some of those who are occupying the jobs they need are not nearly as bright as they.
Such people are also not themselves so keen or able to bring on the bright youngsters.
One way round this problem is a scheme called the Heisenberg fellowships, run by the DFG.
These fellowships give Germany's most promising young researchers a small salary to pursue their own research.
The DFG originally wanted to allow the fellowships to run indefinitely until the researcher obtained a permanent post, but the government imposed a time limit of five years.
Surprisingly, the Heisenberg programme, designed to keep the best researchers within the university system until jobs for them became vacant, has not received enough applicants.
Only 3 10 out of a possible 750 awards were made between 1978 and 1982.
Planners reckon departments are reluctant to propose young researchers because of the shame of rejection.
Another complaint is that promising students get lost and discouraged in the mass of huge classes.
Some academics try to counteract this trend by trying to identify the bright sparks and arrange special seminars for them.
Such trends, however, very much go against the grain of the egalitarian principle that pervades the German university system to a far greater extent than the British one.
Officially, each university is equally good as any other in any subject.
Partly for financial reasons, students now tend to attend university in their home town rather than travel to a city with a better department.
Very little guidance is given to students, who enrol (with some exceptions) in the subject of their choice and shift around at will, taking around six years to complete their first degree.
In the sciences, a student's progress is fairly strictly monitored by exams; there is not so much control in other subjects.
Nevertheless, some research-orientated academics feel strongly enough to go against the grain and form a sort of elitist under ground for their best students.
Another egalitarian principle that some researchers think holds back research is the ‘Gieskanne’ or watering can principle for supporting research in universities.
Each professor has a right to money and manpower resources depending only on his level of seniority and not on his track record in research.
This, some researchers think, is not an effective way of stimulating the best research: some element of competition or selectivity is necessary.
Professor Wolfgang Wild thinks that this weakness is shared by university research and much of the industrial research sponsored by the Ministry for R & D (BMFT).
As in the UK, outside organisers sponsor special research projects that need bigger wodges of money than the university can provide.
The DFG is the biggest source here, though the big foundations such as that associated with the Volkswagen company play a more important part than their equivalents in the UK.
Projects to be funded are selected by committees of scientists, occasionally joined by politicians and civil servants.
In fact, until recently few had any serious trouble getting support for research, but now times are getting a bit harder and the peer review system may be more seriously worked to handle the greater volume competition.
The DFG has long realised the shortcomings of the normal 1-year grant: it does not give enough resources and security to build up a substantial group or programme in a particular area.
It, therefore, has two schemes that provide for longer terms funding for particular departments to build up a research programme in a particular area: there are now 150 of these groups at German universities.
This mechanism allows the DFG to encourage research that is particularly timely and of high quality.
Research leaders welcome this for targeting funds to where they can be best used.
Some would like the elitist trend to permeate even further, in the form of elite faculties, or even whole elite universities.
Others think the ‘elitist’ element should be provided by the establishment of graduate schools as in the American system, much admired in Germany, by which the high flyers can continue their specialised research training after fairly general and not too demanding first degrees.
The success of the American system in producing excellent research is very near the front of the German researchers' minds.
A much quoted observation by one grand old man of German science policy, Professor Heinz Maier Leibnitz, points out that German researchers who have done well in stints in the US tend to stagnate when they return to the German system.
Such comments have given rise to much discussion about the ‘climate of research’.
Here the going gets rough and speculative: some talk of the discouraging load of bureaucracy the researcher has to get through, and the lack of opportunity for independent initiative.
But perhaps the most worrying, if most ephemeral, comment is that researchers are unwilling to take risks to open up new lines of research; they tend to stick to piecemeal additions to well-established paths.
Some relate this to the security of a university appointment, with career steps usually totally unrelated to research achievement.
Others, including a senior civil servant in the education ministry, Eberhard Boning, see the unwillingness to take risks as a symptom of a more widespread complacency in German society; the current generation of young people have experienced a combination of material well-being and an insecure and threatening world scene; both factors detracting from incentive for personal achievement.
Many have talked about stimulating research; a few are already promoting it.
The new Institute for Advanced Study in Berlin is a haven where distinguished scholars in all disciplines can work undisturbed and make cross-disciplinary contacts.
Modelled on its Princeton namesake, it is funded mainly by the state of Berlin.
It is rapidly becoming one of those elite research institutions some academics are looking for to improve the quality of research.
It got off the ground in Berlin in 1982 despite an atmosphere of increasing pressure on funds for all forms of higher education in Berlin as elsewhere in West Germany.
Berlin, however, is something of a special case in the West German higher education scene.
To justify their hold on this island far into East Germany, the West Germans must keep up a good show of a thriving community in West Berlin.
The once prosperous and profitable industries of West Berlin have declined as young adults moved out to the West proper in droves.
Now half the income of the city's administration comes from the federal government.
Fortunately Berlin, the site of the Humboldt University, the prototypical nineteenth century university now in the Eastern sector, has maintained its strong academic tradition.
One tenth of Berlin's city funds go on the universities, which still attract tens of thousands of students from the ‘mainland’ of West Germany.
The total student population of 90 000 helps greatly to preserve an element of life and change in a city where one quarter of the population is over 66.
In charge of higher education in Berlin on behalf of the state (local) government is Senator Wilhelm Kewenig, a former university rector.
Like all state ministers for education in Germany, he has enormous powers over the local universities.
He is involved, for example, in appointing professors: he chooses from a short list of three candidates provided by the university.
Kewenig has chosen to use his role in Berlin to impose his vision on the higher education — for example, he is refusing to allow the polytechnics to duplicate the work of the universities and lengthen their three year courses to compare with the universities.
He also chooses which vacant university posts need to be filled to maintain Berlin's attractiveness as a place of study.
The threat of cuts in Bonn's support of Berlin gives Kewenig extra power to be selective, which accords well with his own views: ‘I want to discriminate and give money to people who do something’.
Can brainpower bolster Berlin's economy?
Kewenig has persuaded the state of Berlin to give him DM 11 million (nearly £3 million) to ‘establish some centres of excellence, and attract more first class researchers to Berlin’.
His long term objective is to re-establish the economic basis of Berlin by persuading researchers of industry to put their heads together and devise some new products.
In historical terms, he wants to re-establish the late 18th century symbiosis of research and economy that led to the growth of Germany's industrial giants such as AEG, Bosch and Siemens.
His pet project is a biotechnology institute in which state funding will be matched by contributions from the Schering drug company.
Eberhard Lammert, president of Berlin's Free University, understandably resents state interference with professorships and other internal matters.
However, he endorses Kewenig's views about selectivity, and has put into practice a scheme to modify the ‘Gieskanne’ principle of funding research.
Each university institute will receive 75 per cent of its previous budget as permanent money.
The remaining portion of the departmental budget will be allocated according to the amount of outside research money the department manages to attract — a scheme reminiscent of one on trial by London University.
Lammert is also keen to shorten the stay of each student at university, but is aware of how difficult and unpopular this would be in a time of high unemployment.
He is also conscious of more international trends in higher education such as the encouragement of adult and continuing education — he points out that the city state of Berlin would be particularly well-suited to conduct experiments in the use of techniques such as cable television.
Although there are some special features in Berlin's higher education system, most of its problems are common to many of West Germany's universities, and exemplify the reasons why the climate is not ideal for research.
Bodies such as the DFG do what they can to encourage good research, but there are unlikely to be major changes unless the need to improve research quality becomes a political priority, both at state and federal level.
If such a will were to become evident, then there are a number of steps that could be taken.
Eberhard Boning, for example, would like to see smaller universities, that can differentiate between the teaching and research roles and can allocate teaching loads accordingly.
He would also like to see the hierarchy between professors and younger scientists, stronger in Germany than in the UK and US, broken down a little.
The Federal government should, Boning thinks, help German scientists broadcast their results more effectively by providing facilities for them to publish in English and travel more to talk about their work and learn new methods.
A number of plans lie on the table to alleviate the bottle neck in academic jobs.
A proposal to allow professors to retire on full pension at 63 instead of 65 has met with little support from state finance ministers because of its cost, and also because it would set an expensive precedent for all other civil servants.
Professor Nikolaus Fiebiger, president of the University Erlangen in Nuremberg, has proposed an alternative that may meet with better success: to create a number of professorships that would be eliminated as older professors left the university.
This would fit well with German population trends: university age groups will peak in the late 1980s and fall rapidly after 1990.
A new generation of university teachers will be needed around 1995.
The jobs questions can also be tackled from the other end: by negotiating more flexibility over the short term contracts provided both by the university and by outside research sponsorships.
The Wissenschaftsrat has been trying to sort this one out, but the political factors seem to have prevented progress.
The trend towards tighter state budgets could be exploited to reduce the number of students in some over populated subjects and also to cut back the resources of non-productive researchers.
The high levels of unemployment, though, will make it difficult to reduce student numbers overall: many planners in Germany, in stark contrast to their counterparts in the UK, think it better that young people should be in university than on the dole.
Some think that the universities would do well to look outwards for reassurance about their research role: in particular to the Max Planck Institutes, which, while universally praised as centres of excellence, have structural problems of their own.
Professor Klaus Pinkau, director of the Max Planck Institute for Plasma Physics, points out that there are drawbacks to centralising research away from universities — for example, academics who in theory have time and resources for research are cut off from the best facilities.
This particularly affects the young scientists, who fail to acquire necessary training in techniques.
The MPIs too could benefit from closer contacts with universities: older members who have passed their first flush of creativity would have the opportunity to refresh themselves by teaching, at the same time attracting new young people.
Pinkau thinks that ‘ differentiation ’ between subjects and departments would be good for research.
Such trends are embodied in the DFG's rolling research schemes described above, which it plans to continue in spite of some hassles, for example in getting the university to provide promised backup, or take over schemes when the DFG ‘pump-priming’ funds run out.
None of these measures will, of course, bring back the halcyon days of German science, or the immense amount of scientific talent lost to Germany — and usually to Europe — in the days of the Third Reich.
This factor is always at the back of the minds of today's senior men of science; in fact many of them returned to Germany from studies or refuges abroad to rebuild the system.
Though this must be one of the main reasons why Germany has received so few Nobel prizes since the war, nothing more can be done about it.
What is more disturbing today is that the Germans have been quicker and more successful to recognise the causes of the decline in their research community than we have in the UK.
But they seem even less capable of moving the system to improve matters.
Flat out for pocket TV
Hugh Aldersey-Williams
Pocket-size television may be a technological breakthrough, but who needs it?
Manufacturers want to sell the gadgets as a ‘stopgap’ until more useful large flat screens arrive
ANY SCIENCE FICTION worth its salt makes extensive use of flat, wall-mounted TV.
That is how Captain James T Kirk of the USS Enterprise first sees each week's new adversary; it is by flat-panel TV that Winston Smith keeps on the right side of Big Brother.
Now, reality has caught up with illusion.
After years of research, the coming months will see the arrival of ‘pocket’
TVs based on competing low-power flat panel displays.
The laboratories are still grappling with the problem of large, flat screen TV.
Until these larger sets can be made in production quantities, electronics companies will probably push miniature sets as if they were what the consumer wanted all along.
For the moment at least, smallest seems to be best.
The two contenders for this new ‘market’ are: liquid crystal displays (LCDs), common in calculators (see Box A); and flattened, but otherwise conventional, cathode ray tubes (CRTs) of old fashioned TV (see Box B).
LCDs are built up from segments; only a few are needed for a calculator display.
A LC TV picture is made up, not of scan lines as on present TV pictures, but of a large number of picture elements or pixels.
Each pixel is a fraction of a millimetre square.
It can appear black, white or in shades of grey produced by different orientations of the LC molecules.
This effect builds up across all the pixels to give a picture.
Various LC phase change effects are exploited by different Japanese televisions.
Each pixel must be individually addressed by a voltage to give the right shade at the right moment.
This is done either by mounting the LC cell onto a semiconductor substrate, with one activating transistor behind each pixel, or by row and column electrodes on the glass LC cell walls.
The electrodes intersect at each pixel to produce the required activation voltage.
This is known as matrix addressing.
Here the troubles start.
Conventional television's 625 scan lines are the equivalent of roughly half a million pixels.
Yet the best LC TV, so far, can only muster 50 000 or so, if picture quality and the video update rate are maintained.
While research strives to remove this limit, products which dodge the issue are appearing.
The simplest tactic is to shrink the screen.
This way, though there are not many pixels making up the image, they are at least small.
Because viewing distance from the screen does not fall in proportion to the screen dimensions, this resolution becomes acceptable.
This is what has been done for ‘pocket’ LC TV.
A normal domestic TV using the same LC technology with the same number of pixels — now 10 times larger — would be unacceptable to the consumer despite its feasibility.
The smaller LC TV picture is accepted: it subtends a smaller angle at the viewer's eye giving some illusion of reasonable resolution.
Scientific instruments Other LC screens are available for less demanding, but less trivial, applications.
One of these is a LC oscilloscope developed by Scopex Instruments in conjunction with the Royal Signals and Radar Establishment (RSRE), An  oscilloscope needs only display waves — single-valued functions — so drastic simplifications can be made to the addressing.
The screen no longer needs to be fully multiplexed.
Now only one pixel in each column must be activated; all the others must contrast with it.
These lone pixels together display the oscilloscope trace.
There is no problem in updating the screen, of 128 x 256 elements in this case, as often as needed.
Another trick is to induce LC phase changes thermally as well as electrically.
This is how a number of prototype LC computer terminals work.
Longer addressing times are possible because the display need not be refreshed at TV frequencies while it is read on the screen.
Neither need it be refreshed because, unlike CRT phosphors, these liquid crystals have memory.
One set of signal pulses creates an image which is held, using no power; a second set of signals is needed to erase it.
Thomson-CSF in France and the 3M Company in the United States have built terminals on this principle.
The same technique can be used for projections though this reintroduces some of the disadvantages — notably the bulk — of CRTs.
The LC layer is used as a shutter, blocking light or transmitting it onto a screen.
Some prototypes have been made with the LC molecules activated by laser light.
In these, the light beam is thrown by rapidly moving mirrors onto a small LC cell to create minute, cursively written characters and graphics.
These are then enlarged onto a workstation screen.
Fast writing speeds are possible with the same memory effect as on the truly flat terminal.
While LC terminals such as these may prove acceptable for specialised uses, the Japanese are continuing work on the fully matrix-addressed screen.
Resolution is the highest priority.
Once screens can be made with 500000 individually addressed pixels, equivalent to standard 625-lint TV, the way will be open for larger screens.
In the meantime, CRTs are ready to give still higher resolution.
Both ways to address present LC TVs are limited in scale.
Picture generation from transparent electrodes crossed in a matrix pattern, used by Hitachi and Casio, is limited by the nature of the voltage waveforms sent along the electrodes.
Each LC element requires a voltage within certain limits to be activated — above or below those limits and it will not produce the required response.
As display size increases, it becomes more difficult to provide the correct waveform to every pixel.
These manufacturers have pushed up the resolution a little by breaking the display into submatrices, addressed separately, but connected to give a unified picture.
Nonetheless, theirs are the coarsest images of the first generation of LC TVs.
An advantage of this technique is that, with glass LC cell walls and transparent electrodes, the displays can be lit from behind to create a luminous image more like the TV we all know and love.
Casio has mounted an electro-luminescent display behind the LC layer.
Other firms use ‘on-substrate’ control and place a single crystal silicon chip directly behind the LC layer.
One transistor for each pixel on the chip provides the video signal.
Now the limitation is not in the supply of suitable voltage pulses, but simply the size of silicon wafer that can be manufactured.
The present practical maximum is about 12–13cm.
In theory the display could be huge, if only a perfect, large semiconductor substrate could be made.
Defect rates are still too high on large wafers to mass produce products at reason able prices.
A variety of thin-film transistor semiconductor materials are under scrutiny for these elusive, large LC screens.
The lack of colour on LC TV is another obvious draw back.
Dyes such as those used on Seiko's television ‘wristwatch’ are one possibility.
Many colours can be provided this way using dyes, coloured filters and reflectors.
So far though, only two-colour switching is possible.
Full colour is harder.
One idea is to use a red, green or blue reflector behind each LC pixel in an analogue with colour TV tubes.
But this would require three times as many pixels to get the same resolution as monochrome.
Another possibility is to use birefringence, being developed by RSRE; here different LC molecule orientations reflect different colour wave lengths.
Calculators, watches and pocket video games reveal another failing.
The low contrast between the ‘on’ and ‘off’states of the crystal mixture and the precise viewing angle needed make them hard to read.
Many primitive displays have to be viewed more or less head-on, and even then they produce more of a grey mixture than a black and white image.
Conflicting TV broadcast standards around the world have also delayed LC TV's release, at least in the UK.
The first sets came from Casio and Seiko, both with established LCD expertise from making calculators and digital watches.
Not being tube TV manufacturers, they do not have the worry of competing with an established product line to win a share of the LC TV market.
Predictably, Japan was the first to see the new sets.
They receive American standard signals broadcast in the US and Japan.
LC TVs for the UK and Europe will operate on different standards.
Other Japanese companies-Hitachi, Toshiba, Matsushita — have treated LC TVs more as a design exercise, and do not expect to market models for a while.
They are already major tube-based manufacturers.
Convening production lines to largely automated production of flat panel sets would be costly in the short term.
It would cause lay-offs in Japan and licensed manufacturing outlets elsewhere.
Sony expects that CRT may increase its workforce depending on market interest in televisions.
Sinclair with his similar, hut cheaper set also predicts great things despite industrial hiccups at his Dundee production plant.
He dreams of a mass consumer market, with one set per person rather than today's one per home.
The Sony ‘Watchman’, already launched in Japan and the US, is rumoured (mainly by the manufacturers) to command a black market price of $ 1000 for the privilege of watching a 50 mm screen.
Viewers, it seems, buy the sets to watch action replays at the ball-game.
They are buying them for business.
And, of course, they are buying them to be trendy.
Whether the idea survives as more than a fad remains to be seen.
A: How LC TV works
THE TECHNOLOGY is borrowed from digital watches and calculators; but providing a TV picture is a big jump from displaying a few numbers.
Liquid crystals exhibiting a twisted nematic (rodlike) effect are the ones used in most calculator displays and in some TV schemes.
The display cell must be built between crossed polarisers.
When the cell is switched off (1) polarised light enters the cell.
It is guided through 90o by the twist in the nematic crystalline arrangement adopted because of van der Waals forces between the LC molecules.
When a voltage is applied across the cell electrodes (1a) the molecular axis is realigned perpendicular to the face of the display.
Now, only the polarisers determine whether light passes.
Since they are crossed, but the light is no longer twisted, the light is not transmitted.
Twisted nematic displays operate by reflection or transmission.
Working reflectively, light shown here as transmitted is reflected back through the cell, re-twisted, and seen by the viewer.
The light blocked by the polariser on the other hand never sees the reflector and a dark display results.
This is how Hitachi's LC pixels operate.
Transmissively, the cell can be backlit, for example with electroluminescent panels as in Casio's pocket TV.
Its set can be watched in the dark.
In other LC TV prototypes different crystalline phase transitions are exploited.
Toshiba and Matsushita use the dynamic scattering effect.
The liquid crystal molecules may adopt a random orientation when they are activated (2) and thus scatter light incident on the LC cell.
Alternatively when the display is switched off (2a) the LC molecules are aligned with the light and let it pass.
A third phase takes the form of helical ‘threads’ in the off state (3).
Entwined with these ‘host’ liquid crystals is a ‘guest’dye.
This absorbs light in the off state to give the display an overall dark coloration.
When the display is switched on (3a) both dye and LC molecules realign.
This change of orientation allows the light to pass.
Seiko has used this technique in its TV to give better contrast than is possible with twisted nematic cells and polarisers.
B: Flat screen CRT
CONVENTIONAL TV works with two deflectors at right angles acting on an electron beam to give a two-dimensional, scanned image.
The electrons are shot from a gun aimed at the centre of the phosphor screen.
The gun spacing necessary for  focusing gives the TV its awkward depth, Adding a third set of deflectors can bring the gun into the plane of the screen giving a flatter unit.
This is Sony's and Sinclair's answer to LC TV.
These sets, too, are limited in size; they have yet to master the intricacies of triple deflection focusing on a large scale.
In this, and their inability yet to give colour, these small CRTs compare with LC TVs (1).
Size is not the only requirement for portability — battery power is important.
The screen is bright enough at these lower voltages because the viewer watches from the side that the electrons strike the phosphor.
On a large TV light is lost going through the phosphor layer and the set's glass faceplate.
Sinclair claims his flat TV gives three times the brightness of a normal tube of the same size with one tenth the power.
Larger, flat CRTs are being developed in different ways.
Philips in the UK, RCA in the United States, and Siemens in West Germany illustrate a progression of approaches.
They all have working models with potential for the domestic TV market.
All guide or channel the electrons from the gun.
This extra control permits a flatter display.
Philips' unit uses an electron gun (2) mounted parallel to the screen.
The image is formed by a line deflector acting in the plane of the screen across it, and a frame deflector to deposit the electrons onto the screen at a chosen vertical level on the screen assembly.
This component operates at much lower voltages than conventional TV.
The electrons must be multiplied in a cascade of closely space plate electrodes before they hit the phosphor, to give a bright enough display.
The former deflector is lost on RCA's flat TV.
It uses a line electron source and so needs only one deflection component.
The line source's electron beam columns run parallel between the screen and the row electrodes until deflected through a suit able guide structure by an activated electrode.
The Siemens display goes the whole hog.
It uses a planar electron source and no deflection (3).
The source is a gas discharge placed behind perforated crossed electrodes.
Voltages at the row and column intersections channel electrons through the holes to create a dot on the screen.
Matrix addressing completes the picture.
Fluoridation — are the dangers resolved?
Geoffrey Smith
Fluoride is now added to drinking water to protect teeth.
An Australian dentist now suggests that there is serious risk of overdose
AT THE OUTSET it must have seemed a superbly simple idea.
Add a small amount of cheap chemical called fluoride to a community water supply and Hey Presto! a costly and ubiquitous disease, tooth decay, is controlled.
and perhaps, in time, even eradicated.
Forty years later we are in a stage of chaos and doubt.
Is artificial fluoridation really safe?
Are some people now ingesting too much  fluoride from an increasing number of sources?
For more than three decades health authorities in the United States, Britain and Australia insisted that only a lunatic fringe of cranks, flat-Earthers and right-wing reactionaries opposed fluoridation.
Now come the reservations.
During the past two years alone, reports in a series of highly respected scientific journals, including The Journal of the American Chemical Society, Science , and both The British Medical Journal and The British Dental Journal , have warned that individuals are receiving fluoride from a growing number of sources and that too much fluoride can be harmful.
As yet it may be too soon to press the panic button; however, as John Emsley of King's College, London, pointed out in 1981 (New Scientist , vol 91 p 293)‘A warning bell has sounded: through the agency of the strong hydrogen bond fluoride can change the chemistry of many compounds.
What it may be capable of doing in the living cell whether for good or ill remains to be discovered,’
Back in 1945, when the first experimental fluoridation projects got under way in Grand Rapids, Michigan, and Newburgh, New York, it was envisaged that drinking about 1 litre of  fluoridated water a day would provide 1 milligram of  fluoride .
All authorities agreed that adding the substance to water was the best way to limit the daily dosage.
In those early days, self-medication with fluoride was frowned upon because of the danger of overdosage.
The reasons for the present re-think about fluoridation are twofold.
First, people are now ingesting fluoride from many more everyday sources, including water, food, dental health products, and medicines as well as pesticide, insecticide and fertiliser residues and even the air we breathe.
Therefore the amount received by the individual cannot be controlled.
Secondly, in 1976–77, scientists at Sweden's Karolinska Institute developed a simple and reliable way of measuring levels of ionic fluoride in the blood.
They found that even very small dosages of fluoride may cause ‘normal’ blood fluoride levels to surge to potentially harmful values.
Many scientists have long been intrigued by the idea of adding a known toxic substance to water supplies to reduce cavities in teeth.
Unlike chlorination, which is designed to treat the water and make it safe to drink, fluoridation is meant to influence a human physiological process — the mineralisation of tooth enamel.
Veterinarians, horticulturists and environmental scientists have known for years that fluoride at very low concentrations can damage vegetation, aquatic life and livestock; chemists have learnt to expect the unexpected from this unpredictable element; and biochemists, physiologists and toxicologists all know that fluoride is a potent poison of enzymes.
New evidence regarding the possible action of fluoride on human cells and tissues is emerging all the time.
The powerful hydrogen bonding capacity of fluoride, discussed by John Emsley, has been known for some time, but its potential for interfering with the vitally important hydrogen bonds between biomolecules has only recently begun to receive attention.
In January 1981, Emsley and others reported in the Journal of the American Chemical at Society (vol 103, p 24) that they had found a new strong hydrogen bond which formed between fluoride and amides — organic salts of ammonia.
Many components within living cells contain amide groups, and the hydrogen bonds formed between amides are the most important weak hydrogen bonds in biological systems.
Disruption of these bonds by fluoride in the formation of much stronger bonds may explain how the chemically inert fluoride ion interferes in the healthy operations of living systems.
Fluoride can be harmful; the key question is, at what concentrations does it become toxic in the body?
Studies on rats show that blood ionic fluoride levels of 0.2 parts per million cause dental fluorosis — a serious form of damage to developing tooth cells; and rats are between one-seventh and one-tenth less sensitive to fluoride than humans.
Research has demonstrated that growth in rats is retarded when their blood contains 0–3 ppm of ionic fluoride and that ‘serious’ toxic effects develop when concentrations in the blood reach 1 ppm (Fluorides and Human Health , WHO, Geneva, 1970).
So the crucial argument does not concern the fluoride level in a community water supply per se , but rather whether fluoridation increases the risk that certain people develop, even for a short time, levels of fluoride in the blood that can damage human cells and systems.
Many proponents of fluoridation insist that this cannot happen; to support their view they cite a well-known study, published in 1960, which purports to show that a physiological mechanism ensures that blood levels of ionic fluoride remain stable no matter what the intake (Journal of Applied Physiology , vol 15, p 508).
Unfortunately, this work was faulty, as the US National Academy of Sciences-National Research Council pointed out in 1977 (Report of the Safe Drinking Water Committee , USNCC-NAS, p 373, Washington DC).
In 1977, J. Ekstrand demonstrated that when a healthy adult male weighing 60 kg swallowed 10 milligrams of fluoride, the levels of ionic fluoride in his blood peaked after about an hour to just over 0–4 ppm per kg of body weight (European Journal of Clinical Pharmacology , vol 12, p 311).
As this was a dosage of 0.166 mg fluoride per kilogram body weight, the equivalent amount needed to achieve similar peaks in a 10 kg infant and a 20 kg child would be 1–66 mg and 3.33 mg fluoride respectively.
In fact, in a 10 kg infant a dose of only 0.8 mg fluoride could theoretically cause a peak of 0.2 ppm in the blood ionic fluoride; and a dose of just 0–4 mg fluoride, a peak of 0.1 ppm.
Such levels can damage developing tooth cells and produce dental fluorosis.
An editorial in the British Dental Journal in 1981 (vol 1 50, p 261) warned that fluoride supplement dosage levels recommended 20 years ago are too high and need modifying in the light of recent research, because of the ingestion of small doses of fluoride from many sources.
These included: Foodstuffs and beverages rich in fluoride, such as sardines and tea.
A 50 gram portion of canned sardines could contribute 0.8 mg fluoride; and, in Britain particularly, many receive more than 1 mg fluoride daily from drinking tea.
•More than 90 per cent of toothpaste now sold contains high concentrations of fluoride.
Today, toothbrushing is common even among pre-school children; and more than 75 per cent of children use toothpaste by the age of 18 months.
Researchers have shown that children under 3 years are incapable of rinsing effectively due to poorly developed swallowing reflexes.
Therefore, youngsters may swallow appreciable amounts of toothpaste.
In fact, a pre-school child may swallow 0.3 grams to 0–4 grams of toothpaste at each brushing (Journal of Dental Research, vol 5 p 1317).
Since most pastes contain fluoride at a concentration of 1000 parts per million, a daily intake in excess of 0–5 milligrams fluoride from this source alone is common.
•Many dental and child health authorities still advocate the use of fluoride tablets and drops for pregnant women and for children from birth.
However, the use of such supplements could result in a daily intake of fluoride two to six times the recommended dose.
o In dental surgeries one of the now common methods of topical fluoride treatment is to use acidulated gels.
The fluoride concentration of these gels varies between 0–5 per cent and 12 per cent(up to 6000 ppm fluoride).
Each application delivers 3 to 5 ml of gel and the patient is exposed to as much as 60 milligrams of fluoride for four to five minutes.
As the gels are both flavoured and acidulated, they stimulate the flow of saliva which leads to the swallowing of excess saliva and gel during treatment.
•Adverse reactions following gel applications were reported in the British Dental Journal in 1976 (vol 140 p 307).
More recently, in 1980, Swedish researchers found, in a 25-year-old adult weighing 54 kg, that blood ionic fluoride levels of just over 1 ppm were reached 30 minutes after gel treatment (Journal of Dental Research , vol 59 p 1067).
This level is close to those reported to result in impaired kidney function.
•Czechoslovakian studies show that children aged 6 to 14 years who reside near an aluminium smelter ingest more than 2 milligrams fluoride a day from air, water, animal foodstuffs and plants although their drinking water was not fluoridated.
Aluminium smelters are only one of a score of industries which now pollute the total environment with fluoride emissions and solid wastes.
An article in a recent issue of the British Medical Journal , pointed out that babies in fluoridated areas who drink dried milk formulae made up with water containing 1 ppm fluoride, are ingesting up to 100 times the amount of fluoride they would obtain from mother's milk (vol 283. p 76).
The researchers demonstrated that there is a physiological plasma/milk barrier against fluoride which protects the infant from more than extremely low concentrations of the halogen.
They suggested: ‘Hence the recommendation made in several countries to give breast-fed infants fluoride supplements should be reconsidered.’
Obviously, fluoride ingested from drinking water cannot be considered in isolation from other sources of fluoride intake.
Nevertheless, the artificial fluoridation of a community water supply does have certain predictable consequences.
First, fluoridation will raise the average steady state or plateau level of ionic fluoride in the blood throughout the total population.
Secondly, by introducing large amounts of fluoride into the environment, all locally grown and manufactured foods and beverages may contain increased amounts of fluoride, and foods cooked in fluoridated water will increase the fluoride intake of consumers.
Thirdly, because the overall intake has been increased and the average blood ionic fluoride level of the population raised, individuals who ingest submilligram doses of fluoride will run a greater risk of their blood ionic fluoride concentrations peaking to above the threshold level that can cause dental fluorosis or other ill-effects.
Spokesmen for the dental profession have stated that dental fluorosis, the ‘mild’ mottling of the enamel caused by fluoride, is a sign of ‘good teeth’.
If unsightly, they say, the appearance can be remedied by the fitting of artificial crowns!
But this dental fluorosis is an indication that the person, when a young child, suffered a toxic level of exposure to fluoride.
Dental fluorosis, no matter how slight, is an irreversible pathological condition recognised by authorities around the world as the first readily detectable clinical symptom of previous chronic fluoride poisoning.
To suggest we should ignore such a sign is as irrational as saying that the blue-black line which appears on the gums due to chronic lead poisoning is of no significance because it doesn't cause any pain or discomfort.
Additionally, it is clearly wishful thinking to insist that tooth-forming cells are the only ones in the body sensitive to fluoride.
In 1979, Professor Lennart Krook of Cornell University demonstrated that the primary target cells for fluoride poisoning include certain hone cells.
(Cornell Veterinarian, vol 8, supplement 8).
Undoubtedly, the ‘fluoridation controversy’ has entered a new phase in which genuine doubts are replacing previously held certainties.
For many years, a number of dentists seemed to believe that if a little fluoride is good for you, then more must be better.
This attitude is not only wrong, it is irresponsible.
The issue has also been complicated by legislation making artificial fluoridation compulsory.
Repeal of these laws would involve considerable loss of face for some politicians and their advisers.
For 40 years the ‘debate’ about fluoridation has been remarkably emotive.
Now, at last, scientists appear to be taking a long, hard look at fluoridation and the uncontrolled, indiscriminate use of fluoride-containing dental products.
MONITOR
Microbial dustmen clean up toxic waste
FEW OF nature's poisons can compete with man's concoctions for persistent nastiness.
Compounds such as hexachlorobenzene, trichloroethylene or polychlorinated biphenyls (PCBs) are increasingly building a reputation for life-threatening toxicity, and a habit of escaping from factories or dump sites to play havoc with the environment.
Now a new line of research is taking shape in the US to combat these and other toxic chemicals that are known collectively as chlorinated aromatic compounds.
Microbiologists are bending the new techniques of genetic engineering to the task of training microorganisms to degrade these chemicals in contaminated soil and water.
Supporting this work are the government's Environmental Protection Agency (EPA) and the US Air Force, which are responsible for cleaning up poisoned communities like love canal, or old plants that once made the herbicide Agent Orange.
Organisms are tempting detoxifiers because they have the potential to do their work on site, saving the expense and publicity of digging up tons of soil, and burning or chemically cleansing it.
And because organisms replicate themselves, they could be far less expensive than chemical or mechanical methods.
The EPA plans to spend almost one million dollars over the next three years, looking for organisms that can use the chlorinated aromatics as their sources of carbon.
Perhaps the most intriguing approach is a project, planned by Dr John C. Loper, a molecular geneticist, and professor of microbiology at the University of Cincinnati in Ohio.
Loper hopes to win funding for the idea soon.
Loper is following up recent research by several scientists, particularly H. Weber and colleagues of the Institute of Toxicology in Zurich, Switzerland and John Brooker at the University of Adelaide, Australia: Weber found that dogs fed massive doses of one type of dioxin, TCDD, detoxified the chemical with the liver enzyme, cytochrome p 450 mon-oxygenase, and Brooker used chick embryos as a source of the enzyme to track down the messenger RNA responsible for producing it.
Scientists at the EPA are eager to see cytochrome p450 mon-oxygenase work on other aromatic compounds common to our polluted environment.
Loper plans to isolate the gene that codes for p450 from a library of candidate mammalian gene sequences and insert it into yeast.
The yeast would be manipulated to  express the gene and to manufacture the enzyme in large quantities, and eventually be able to thrive on hexachlorobenzene and other offending compounds.
That much accomplished, the yeast presumably could be spread over Seveso in Italy, Times Beach in Missouri, and other places where chlorinated aromatics such as TCDD have insinuated themselves into the environment.
The EPA won't till the soil with bugs until extensive field work shows whether the microorganisms will work outside the lab — a formidable task, explains David Watkins, manager of the agency's microbiological degradation projects: ‘How will you know you've got complete degradation?
Can you wait long enough for the organism to perform its duties?
And microbiologists must ensure that the engineered organisms are tough enough to eat toxins but will expire gracefully when their job is done’, Watkins adds.
The EPA has already engaged an eminent pioneer in the field of biotechnology to complement Loper's approach: Dr A. M. Chakrabarty, micro biologist at the Medical Centre of the University of Illinois in Chicago.
Chakrabarty is concentrating first on organisms he scraped up from Love Canal and other American dumpsites.
The organisms had learned to live on 2,4,5-T, one ingredient of the infamous Agent Orange.
Working from these, and with microorganisms he is culturing in his laboratory to metabolise related compounds from the chlorodioxin group, Chakrabarty is seeking a single culture that can degrade 2,4,5-T to the point where heavily contaminated soil can be returned in a matter of days to healthy soil that can support plant life.
From his microbial mix, he hopes to find the plasmid DNA responsible for the degradative ability.
He can then arm an arsenal of microorganisms with an inherited lust for a variety of dioxin compounds.
The US Air force has also latched onto microbiology as part of a large pollution control programme run by its Engineering and Services Centre at Tyndalle Air Force Base in the Florida panhandle.
In one project, scientists at Wurtsmith Air force Base in Michigan are taking their first close look at an organism that was found happily growing in the processing tower of a mechanical device for cleaning trichloroethylene out of contaminated ground water.
Trichloroethylene is a toxic industrial solvent.
Dr James Tiedje and Dr Stephen Boyd at Michigan State University, are awaiting final approval of a grant to support further research of a startling and previously unknown anaerobic organism, which they found in sludge from a Michigan sewage treatment plant and in lake sediments.
As yet unnamed, the organism performs a novel trick — it removes the chlorine atom from the chlorobenzoate molecule, making the resulting benzoate compound vulnerable to further degradation by known aerobic microorganisms.
Although chlorobenzoates pose little risk to the environment, this dechlorination magic might also work for other chlorinated compounds: dioxin, hexachlorobenzene, PCB, 2,4,5-T and their irrepressible classmates.
In the long run, Boyd tells New Scientist , he and Tiedje hope to pinpoint the gene that conducts this sleight-of-atom, and join other microbiologists in the latest pursuit of the perfect microbial dustmen.
Dressed for protection
MIMICRY is a curious fact of biology.
One species, perfectly good to eat, looks like another that is in some way nasty, and is assumed to derive some protection against predators thereby.
Now direct experimental evidence has proved the assumption justified, at least when the prey are nauseous chicken eggs and the predators crows.
Lowell Nicolaus and a team from North Dakota State University injected hens' eggs with a substance that causes illness and vomiting.
This kind of experience will readily turn an animal off even a preferred food, and the aversion lasts even in the absence of the emetic.
Taste aversion learning could thus be an important part of the protection enjoyed by mimics, so Nicolaus set out to see whether it would protect eggs from crows, Some eggs were painted bright green and others were left untouched, to act as controls.
At one place, the green eggs were injected with the emetic, at another they were not.
The crows quickly learned to avoid the nasty green eggs.
It took about nine days for them completely to stop eating green eggs, but they continued to enjoy the ordinary white egg.
Nicolaus then arranged a central group of toxic green eggs, surrounded some distance away, by a selection of non-toxic green and white eggs.
The crows avoided the central green eggs, but they shifted to nearby wheatfields rather than travelling to the white eggs.
Even when they did succumb to the lure of tasty white eggs, they did not touch the nearby green eggs, despite the fact that they were perfectly harmless.
The mimics thus are protected by their visual resemblance to the emetic models, despite being far away from the place where the crows discovered the nasty prey.
Aversion lasts at least eight months, and possibly more.
Nicolaus speculates that this could give ecologists a method of controlling predation on eggs of rare birds.
Similar techniques are already being used to deter coyotes which kill sheep, and could prove a very useful management technique (Science , vol 220, p 212).
Carbon monoxide as a chemical feedstock?
RESEARCH into a neglected area of carbon monoxide chemistry has yielded a new reaction which converts carbon monoxide to molecules containing two carbon atoms (C2-compounds), and could be of commercial importance.
Peter Lednor and Peterina Versloot, at the Amsterdam laboratories of Royal Dutch Shell, used solutions of metallic sodium and potassium to transform carbon monoxide into glycollic acid, a close relative of the petrochemical feedstock, ethylene glycol.
Amid the hue and cry about carbon monoxide (from coal gasification) competing with oil as a source of petrochemical feedstocks, a bizarre feature of carbon monoxide chemistry has gone unnoticed in the scientific literature.
Under certain conditions, carbon monoxide reacts with itself to give a family of so-called oxocarbon anions.
These are usually cyclic molecules, containing carbon-carbon bonds and oxygen atoms of general formula.
C O.
The first, CO (see figure), was discovered way back in 1825.
In the 1960's the well-known reaction of carbon monoxide with alkali metals, eg sodium and potassium, liquid ammonia, was shown to give CO (see figure).
The appropriately named ‘squaric’ acid dianion, CO, was produced electrochemically in 1980 while in the same year, the absorption of carbon monoxide onto metal oxide surfaces was shown to produce oxocarbon mixtures in the absence of water.
The interesting thing about all these observations is that they went unnoticed and unconnected for so long.
Time and the specialisation of chemistry into separate disciplines — in this case inorganic, electro-, and surface chemistry — were to blame.
What Lednor and Versloot have done is to unify these unrelated chemical curiosities by suggesting one underlying mechanism.
Carbon monoxide picks up an electron (is reduced) to form the radical anion CO which reacts further with another CO or with carbon monoxide itself to form oxocarbon anions.
Armed with this hypothesis, they set out to find a convenient way of reducing carbon monoxide that might avoid the expensive catalysts and energy intensive conditions of Fischer-Tropsch chemistry — an industrial process that converts carbon monoxide and hydrogen synthesis gas to mixtures of organic compounds.
They found it.
First, they dissolved a sodium/potassium alloy in an organic solvent called tetrahydrofuran (THF) using a phase transfer catalyst.
These are organic molecules with an unusual crown-shaped arrangement of carbon and oxygen atoms (called ‘crown’ ethers), that allows them to bind atoms of sodium and potassium.
In so doing, the metal atoms, dragging an electron behind them, are removed from their ionic environment and ‘dissolved’ into an organic one — they have transferred phases.
In solution, the sodium and potassium anions are a deep blue colour but when carbon monoxide is bubbled in (at around freezing point) the colour fades, indicating that the carbon monoxide has reacted.
Treating this mixture with water produces the glycollic acid anion, in high yield, along with oxalate, CO, and the ‘deltate’ trian gular oxocarbon anion, C O.
(Journal of the Chemical Society, Chemical Communications (1983), vol 284, p 284).
Glycollic acid could be a precursor for the petrochemical feedstock, ethylene glycol but it is also a useful chemical in its own right, serving as a descalent and as a mordant in the dyeing industry.
Union Carbide have patented a process for the catalytic conversion of synthesis gas to ethylene glycol.
Compared with this, Lednor and Versloot's reaction is far milder, not requiring the expensive rhodium catalyst or the high temperatures and pressures of the Union Carbide process.
Unfortunately, the Shell worker's reaction would require large amounts of costly metallic sodium if an industrial process were envisaged.
This might explain Shell's reluctance to commercialise or even patent the reaction.
If the process could eventually be made catalytic, though, they would have lost a money-spinner. h
Sexual rivalry in the hedge sparrow
MALE dunnocks (Prunella modularis)may be willing to share a single female, but they compete intensely for her reproductive favours.
A detailed study by Nick Davies, of Cambridge University, has revealed that the private life of the dunnock involves rivalry on a scale unprecedented in birds.
(Nature , vol 302, p 334.)
The dunnock, or hedge sparrow, lives on a territory during the breeding season.
The biggest males can exclude all rivals and enjoy a monogamous relationship with a female, but slightly smaller males may have to share the territory, and the female, with another male.
In these trios the two males are not closely related, and the larger (alpha) male is dominant over the smaller (beta-male) in competition for food and the female.
When the female was close to laying, the alpha male stayed close to her almost all the time, but still had to spend almost 30 minutes in every hour chasing the beta male away.
Competition, however, goes further than that.
The courtship of dunnocks is, as Davies puts it, ‘an extraordinarily elaborate affair’.
While the female crouches, quivering her wings and fluffing up her feathers, the male hops from side to side behind her, pecking at her cloaca (the joint exit of digestive and reproductive systems).
This pecking can last for up to two minutes, and often ends in the male being interrupted before he can get to copulation.
So it must be important, but why?
Watching closely, Davies noticed that as the male pecked, the female's cloaca became pink and distended.
From time to time it contracted, and Davies saw a small pale mass ejected.
On three occasions a diligent search turned up a small droplet on the ground, and under the microscope it turned out to be a sperm mass.
Hen birds store sperm until the eggs need to be fertilised: the male's behaviour ensured that this store was at least partially emptied before he inseminated the female, thereby giving himself a better chance to fertilise the eggs.
Corroboration came from an analysis of the details of cloacal pecking.
The more time another male spends close to the Female, the more the courting male pecks at the cloaca.
So the greater the risk that another male has inseminated the female, the more trouble a male takes to get her to eject the sperm.
The advantage to the male, then, is that by pecking at the female's cloaca he probably enhances his chances of fathering some of the clutch.
But what is in it for the female?
Davies speculates that it may be a device whereby she ensures that her clutch will in fact be fathered by more than one male.
Females often try to escape from the alpha male's vigilance, and will go up to the beta male and solicit copulation.
Davies saw seven cases in which both males copulated with the female, and in every case both male's fed the nestlings.
But in five cases in which the beta male had not copulated with the female, he did not feed the chicks either.
And chicks fed by two males grow better and are more likely to survive.
So a female who can persuade two males that they each have a stake in the brood will do better in the evolutionary struggle.
Multiple personality not all in the mind
JAMES THURBER'S short story, ‘The Catbird Seat’, tells of mild-mannered Mr Martin, who deliberately turns himself into a boor for one evening.
He smokes, drinks, sticks out his tongue and boasts of being ‘coked to the gills’ on heroin in order to trap an obnoxious colleague.
The ruse succeeds; no one believes the colleague's story of Martin's raucous behaviour.
In fact, the boss decides to fire the colleague.
She is either lying or crazy, he reasons, believing that Martin was incapable of such alien behaviour.
Mr Martin was faking it; he was in control and remembered everything he did, Remove the elements of memory and awareness, however, and Mr Martin's behaviour begins to resemble what psychiatrists call ‘multiple personality disorder’(MPD).
Few things test the conventional notion of self more than MPD.
People accustomed to thinking in a ratio of one mind per body, are both enthralled by, and sceptical of the idea that several discrete, and to varying degrees, autonomous personalities could ‘inhabit’ one individual.
Psychiatrists, too, have been periodically sceptical of multiple personality disorder, though cases of it appear in the medical literature as far back as the mid-1600s.
Though it has largely been eclipsed by schizophrenia as the diagnostic vogue in the last 50 years, MPD is once again attracting the interest of psychiatric researchers.
Studies are emerging which support clinical  descriptions of the disorder.
And more important, they suggest that the different states of consciousness, which manifest themselves as different ‘personalities’ in one individual, may correspond to different patterns in the Physiological activity of the brain.
A signal trait of MPD sufferers is their discontinuous memory.
Personality A may be aware of personality B, but not vice versa, Personality C may be aware of A and B, neither of which is aware of C.
Dr frank Putnam, a staff psychiatrist with the National Institute of Mental Health (NIMH) in Washington, tested nine subjects with MPD in 1980, on their ability to recognise and recall groups of words taught separately to several of their non-overlapping personalities.
Putnam and colleagues Ed Silberman and Herbert Weingartner, both of NIMH, devised lists of 20 words according to various categories, for example, furniture, weapons and parts of the body.
Personality A was shown a list containing words such as‘arm’, ‘finger’ and ‘toe’; personality B learned a list containing words such as ‘leg’, ‘toenail’and ‘thumb’, and so on with the other lists and personalities.
After a couple of hours, the researchers showed each personality the lists, plus a randomly mixed list, and asked the subject to pick out the words he or she recognised.
Compared with normal controls, the MPD subjects fared much better at keeping the lists separate.
But were they were faking — did they simply have better memories?
To check this, the researchers ran the entire test on one personality of each MPD subject, essentially treating them as normal.
They also ran it on several ‘fake personalities’ which the control group was instructed to concoct, in the same way as Thurber's Mr Martin.
Before presenting the subjects with the lists for the second time, the researchers asked them to recall the words.
All four groups remembered roughly the same number of words, so MPD sufferers do not merely have better memories.
They also found that the controls, in their fake personalities, experienced more difficulty in keeping the lists separate than when they behaved normally.
These results confirm the clinical descriptions of memory disruptions in MPD, but they offer no direct evidence that the splitting of consciousness in these people correlated with changes in brain activity.
One way to gather direct neurological data is to measure electrical activity in the brain evoked by stimuli such as flashing lights, tones and electric shock.
When a person's brain is repeatedly stimulated in this way, and the responses (recorded through electrodes on the scalp) are added together, a characteristic waveform emerges which represents changes of voltage (or potential) in the brain, that have been evoked by the stimulus.
Between 1979 and 1981, Putnam and Monte Buchsbaum of the University of California at Irvine, measured the voltage changes in the brains of 10 people with MPD, and 10 controls subjected to flashing lights at varying levels of brightness.
Putnam and Buchsbaurn instructed the controls to create three to four imaginary personalities to match those of the MPD subjects, whom the controls had observed on videotape.
Then, for five consecutive days, the researchers ran trials on each personality of the MPD sufferers and controls.
When Putnam and Buchsbaum analysed the waveforms produced by each personality it seemed they were examining 60 people, instead of 20.
The researchers looked at the amplitude of the brain-waves and the latency of the brain's response to the stimulus.
They found much greater differences between the personalities of each MPD subject than between fake personalities of each control.
In other words, when the controls simulated other personalities, their brains still responded identically to the stimuli.
But for MPD sufferers, each personality produced a consistently different waveform.
How does multiple personality disorder develop?
‘Think of dissociation (the splitting of  consciousness) as a spectrum’, says Putnam.
On one end is daydreaming, on the other, multiple personality.
Everyone experiences altered psychological states, but usually these states are tightly integrated into one ‘self’.
In MPD, this self splinters and the pieces become isolated.
Putnam explains that MPD may be the perversion of a normal  developmental process in children.
Ordinarily, a variety of childhood selves coalesce into a stable adult persona; in MPD they may not.
MPD frequently arises in people who were sexually abused as children, or who experienced what Putnam calls ‘sensory deprivation’— being bound, gagged, stuffed in trunks or locked in closets for example.
One way to endure severe abuse is not to be there when it happens.
This is why personality.
A often has no awareness of events in personality B's life.
Interferon blocks virus entry
AMERICAN scientists have uncovered a previously unrecognised aspect of the anti-viral effect of interferon, to be added to the already wide range of mechanisms by which it fights viral infections.
Patricia Whitaker-Dowling, David Wilcox, Christopher Widnell and Julius Youngner at Pittsburgh University have shown that interferon can prevent viruses from getting inside animal cells, and they believe that this might be the first line of defence set up by the interferon system in response to viral attack ( Proceedings  of the U.S .
National Academy of Sciences , vol 80, p 1083).
Intensive research has identified several ways in which interferon-treated cells can interfere with the viral life-cycle, particularly by preventing the genetic information stored in the viral genes from being converted into viral proteins (New Scientist , 10 March 1983, p 642).
Until now it has been thought that interferon can do little to prevent the initial entry of viruses into a cell.
The Pittsburgh group incorporated a radioactive ‘label’ into the coat proteins of vesicular stomatitis virus (a virus that causes a mild disease in livestock and is much used in interferon research).
By adding the virus to cultured animal cells (mouse, chicken and human) they were able to determine the extent of virus entry by later purifying the cells and measuring their radioactivity.
Prior to this measurement they used an enzyme to remove viruses that were bound to the outside of the cells.
This allowed them to distinguish between the two separate processes of adsorption of viruses onto cells, and actual entry into the cells.
In cells that had been treated with interferon the entry of the virus was strongly inhibited, although in agreement with previous findings, the adsorption of viruses onto the cells was unaffected.
A clue to the possible mechanism of this effect of interferon is the observation that interferon treatment also inhibits a cellular activity known as endocytosis (see Figure).
Endocytosis is a process by which external material, is taken into cells (New Scientist , 10 February, p 372).
Dose-responses of inhibition by interferon of viral entry and endocytosis are similar, suggesting that the effect of endocytosis is inhibited.
Religion and agriculture in Mesoamerica
TEOTIHUACAN, the great pre-Hispanic city in the Valley of Mexico, dominated political, economic and cultural life in Mesoamerica for some seven centuries until about AD 700.
Its influence even reached Maya centres in the Yucatan Peninsula, Guatemala and Belize and neighbouring parts of Honduras and El Salvador.
This was when the Maya civilisation was at its height, during what archaeologists call the Classic period.
Finds of pottery, obsidian tools and religious monuments testify to Teotihuacan's widespread influence on Classic Maya life.
Now comes a suggestion that the city also had control over land use outside its environs, not in this case in the Maya area, but in wetlands near the modern port of Veracruz, west of the Yucatan Peninsula.
A geographer, Alfred Siemens of the University of British Columbia, has found rectilinear remains of canals and planting platforms (raised fields) in central Veracruz, which, he claims, are distinctly oriented about 1 5o east of north (American Antiquity , vol 48. p 85).
This orientation, Siemens suggests, was imposed from Teotihuacan where it had special astronomical and ritual significance.
There the city's builders laid out all the streets, the ceremonial centres and most of the other buildings along a precise rectangular grid aligned with the main ceremonial road known as the Avenue of the Dead, which archaeologists have discovered, was oriented 15–5o east of north.
On the east side of the Avenue was the large Pyramid of the Sun.
The objective of this special orientation, and similar arrangements in several other Mesoamerican cities, were the points on the horizon where the Sun rose and set on the day it passed directly overhead.
On precisely the same day each year the star cluster Pleiades also first became visible in the dawn sky.
To the people of Teotihuacan these celestial events signalled the onset of rains and the time for crop planting and probably other important ceremonies.
The significance of the orientation of fields in Central Veracruz is still unknown.
There is no evidence to suggest that such orientation has the same religious significance as they did for Teotihuacan.
The ‘floating’ gardens or chinampas in the lake region just south of modern Mexico City were also oriented in a similar way when they were first built, probably in Teotihuacan times, and this orientation can still be seen in surviving examples.
These highly productive farm plots were the economic foundation for the great Aztec Empire and most likely also for the earlier state of Teotihuacan.
They are long, narrow strips of land surrounded on at least three sides by canals.
Strong central control would have been needed for their successful operation on the scale archaeologists envisage.
Properly looked after they can produce several crops a year and remain fertile for centuries without needing a fallow period.
Some chinampas are still in operation and experimental models are being built in other parts of Mexico and elsewhere to assess the productivity of this method of agriculture in swamplands today.
The chinampas of central Mexico represent a most sophisticated form of wetland agriculture and the Veracruz raised fields do not seem to have been as elaborately built.
Siemens has no dates yet for them but their orientation certainly suggests they were built during the first centuries AD.
During Teotihuaca/n times in the Veracruz wetlands there may have been occasional impounding of swamp water late in the dry season but perhaps not the sustained maintenance of water levels as happened in the chinampas .
The fields near the margins of the swamp area were probably kept drained by the building of a network of canals: those towards the centre may have been built up above flood level with muck transferred from the canal bottoms.
Some fields were probably cultivated only in the dry season and generally in the Veracruz wetlands there may not have been the intensive year-round agriculture as practised in the upland chinampas 
Teotihuacán's influence on land use was probably greater in Central Veracruz than in the more distant Maya area, Siemens believes.
In Maya territory, complexes of canals and raised fields, some first indicated on a large scale by aerial radar surveys (New Scientist , 12 June, 1980, p 232 and Science .
vol 213, p 1457), have also been reported from several wetland areas.
Unlike the Central Veracruz raised fields, however, most of the Maya ones discovered so far, Siemens reports, are irregularly shaped and none seems noticeably oriented in any special way.
Wetland cultivation during Classic Maya times has been attracting much archaeological attention in recent years.
If it was widespread and intensive, and involved the building of raised planting platforms on a large scale, as some archaeologists have suggested, it could explain the growth of population in lowland areas during the Classic period.
This is a theory favoured by, among others, Richard Adams of the University of Texas who was involved with analysing the radar data from the Maya lowlands.
The collapse of the Maya Civilisation around AD 900, it is supposed, could have happened because intensive crop-growing ceased for environmental or other reasons.
Or the reverse happened: failure of the sociopolitical control by the Maya led to the collapse of the agricultural order.
There have been persuasive arguments for the building of raised fields by Classic Maya farmers at, among other places, the aptly named Pulltrouser Swamp area in northern Belize.
But could the platforms, seen as extensive patterning in aerial surveys of the Maya lowlands just as likely have formed naturally?
Some scientists think so, after studying how soils have accumulated in wetland areas in the Yucatan Peninsula.
At, for example, the site of San Antonio on the Hondo River to the east of Pulltrouser Swamp it seems the platforms are natural formations, although at one time Maya farmers grew crops on them.
According to Pierre Antoine, Richard Skarie and Paul Bloom (in Maya  Subsistence  .
1982, Academic Press), the deposits making up the platforms there accumulated naturally as sea levels gradually rose between about 4000 and 2500 years ago.
In a continuation of this work reported in a recent issue of Nature (vol 301. p 417), Bloom, a soil scientist at the University of Minnesota, and seven colleagues write more specifically about the Maya involvement in cultivation at the site.
What they have to say throws doubt on previous inferences about how labour-intensive Maya agricultural techniques were, at least at San Antonio.
At San Antonio, soil analyses point to Maya cultivators being active there around 670 BC or a little earlier.
That date is before the start of the Classic period about AD 300 and well before the marked population increase at the peak of the Maya Civilisation.
The investigators found no evidence that the Maya returned to the San Antonio locality to farm during Classic times, which, they suggest would probably have been impractical because of increased coastal flooding.
The pre-Classic Maya seem to have first exploited the raised platforms in the swamps by growing crops such as maize only in the dry season (now February to May).
Later they dug ditches for drainage but did not raise the natural platforms artificially.
This relatively simple wetland agriculture, Bloom suggests, would have required far fewer labourers than has been envisaged for raised field cultivation in Maya areas.
The early San Antonio date also upsets explanations for the spurt in population growth during Classic times.
San Antonio is, however, only one site in many to be investigated in detail by archaeologists and geomorphologists and it is too early to extrapolate from there to the Maya lowlands in general.
TECHNOLOGY
Science tackles the muck mountain
ONCE upon a time, farmyard ‘muck’ was a subject of little interest outside the farm.
Waste from animals was a valuable fertiliser — all you had to do was to plough it into the land.
Any resulting smells were simply ‘good country air’.
In most parts of agricultural Britain that state of affairs is now a nostalgic dream.
Today's factory farms, which dispense with straw for bedding and have little use for natural manure, turn out thousands of litres of slurry — dung and urine mixed with water — every day.
Far from being a resource, most farmers see slurry as just another form of industrial waste.
It is also a health hazard.
The Ministry of Agriculture, fisheries and Food estimates that a dairy cow weighing 500 kilogrammes produces 41 litres of slurry every day.
A 100-kg sow will produce 8 litres, more if it is feeding a litter.
There is no way the farmer can deal with the output of hundreds of animals simply by spreading it on the fields.
So the slurry goes into large storage tanks or lagoons.
There, the troubles really start.
Slurry stored for any length of time undergoes anaerobic fermentation — and becomes very unpleasant when disturbed.
Unlike the aerobic reaction that produces farmyard manure, anaerobic fermentation gives off little heat, so disease organisms thrive.
The combination of more slurry being produced, and more townspeople moving to the countryside, increases the risk of disease being transmitted by farm waste.
In 1971, the Agricultural Research Council (ARC) and the agriculture ministry launched a research programme to find a way of turning the problem into a resource.
Last week, the ARC turned out in force at the National Agricultural Centre near Coventry to show farmers its work, and to persuade them to ‘get more out of muck;’.
Scientists at the ARC's institute for research on animal diseases, near Newbury, have been working on ways of reducing the risk of spreading diseases with slurry.
The list of diseases that slurry could spread is a formidable one, including anthrax and tetanus, but the one that receives most attention is salmonellosis.
The institute reports that 10 per cent of cattle slurries and 23 per cent of pig slurries are contaminated with salmonellas — even if the animals themselves are healthy.
Fortunately, salmonellas die rapidly in storage, although they can survive if the temperature stays below 10°C, and if the slurry contains more than 5 per cent solids.
After a month of storage in a tank or lagoon, the slurry should be safe enough for spreading — although cattle should not graze on it for a month.
Separating the solid matter for aerobic fermentation into manure also cuts the risk.
Even when the slurry finally goes onto the land, it is not immediately of much use as a fertiliser.
Scientists at the National Institute for Research in Dairying at the University of Reading have found that only part of the nitrogen, phosphorus and potassium in slurry is in a form that plants can immediately take up.
For example, much of the nitrogen is locked up in organic materials which plant organisms break down slowly into ‘plant-available’ nitrates.
Experiments at Reading have shown that only 15 per cent of the nitrogen in cattle slurry is available to spring crops after spreading in the winter — previous estimates put the figure at 50 per cent.
So even if farmers have the equipment and knowledge to use slurry safely as a fertiliser, it still needs added nitrates.
The most attractive way of dealing with slurry is simply to let it ferment in sealed vessels into safe fertiliser and useful biogas (a mixture of methane and CO).
Biogas plants are already a familiar sight in China and India, although they have had some  unforeseen side effects, such as depriving the very poorest people of a source of cow dung.
Could biogas plants catch on in Britain?
Scientists at the Rowett Research Institute, near Aberdeen and the North of Scotland College of Agriculture have been working on the idea for more than 15 years.
During that time, they have seen the emphasis change from reducing pollution to exploiting muck on a large scale as a source of heat and power.
The goal is to develop a continuous-flow digester, where the farmer feeds raw slurry in one end, and digested slurry comes out of the other.
There are still many problems.
Solid matter takes between 10 and 20 days to digest (cattle waste takes longer than waste from pigs), and the size of the plant has to match exactly the input of slurry, which varies according to the season.
Most importantly, farmers are unlikely to go to the expense of installing anaerobic fermenters unless the gas saves them money.
Until then, the rest of us will just have to keep holding our noses.
MacGregor's miracle is 10 years away
Engineers at British Steel have reacted warily to comments by Ian MacGregor, the corporation's chairman, about the possibility of bringing out of mothballs two direct-reduction ironmaking plants in Scotland.
MacGregor said last month that the plants could ‘put Scotland back in the forefront of steelmaking’.
But the engineers say it will be 10 years before they perfect the technology to make this possible.
British Steel built the plants, in Hunterston, to run on natural gas from the North Sea.
The gas was to reduce iron oxide to make iron which could later be fed to a steel works.
But rising gas prices in the 1970s forced the corporation to abandon the project.
According to MacGregor, the plants could be revived to run on coal.
The coal, from fields in Scotland, would have to be converted to gases such as carbon monoxide for the reduction process.
MacGregor has a new interest in coal as he is due to take over shortly as chairman of the National Coal Board.
Researchers at British Steel's laboratories on  Teeside are working on coal gasification.
Details of the work are secret.
If they are ever opened, the Hunterston plants would be among the first direct reduction iron works in the Western world.
Traditionally, iron oxide is converted to the metal in a blast furnace.
But developing countries with large supplies of natural gas have turned to direct reduction as a way of forcing their way into the steelmaking industry relatively cheaply.
The process takes place at low temperatures.
The iron produced in the process is in the form of small pellets which can be easily handled; with blast furnaces, the metal is molten.
Countries that have developed direct reduction plants include Brazil, Saudi Arabia, Nigeria, Indonesia and New Zealand.
These plants run on supplies of natural gas.
South Africa and the US have experimented with similar works, but using coal as feedstock.
Britain's success will depend upon the price of coal; which MacGregor will be trying to keep down.
Maglev may cross the Rhine
A Magnetic-levitation train travelling at 400 kilometres per hour between Frankfurt and Paris would carry 16.6 million passengers a year according to a study carried out by the West German Ministry of Transport.
Alf Schmitt of the German Transport Research Group, in Cologne, told the
World Conference on Transport Research last week in Hamburg that a ‘maglev’ train on the 500 km route would carry about one tenth the total number of travellers between the two cities.
The maglev would take most of its passengers — nearly 61 per cent— away from conventional trains.
But it would also take 700 000 travellers out of their cars and over a million passengers would switch over from air travel.
The balance (800 000) is made up by newly-generated journeys.
The study was based on an assumption that the maglev route would be running by 1990.
It also draws on the results of the new TGV (Train Grande Vitesse)service between Paris and Lyon, which Schmitt says cut air travel between the two cities by a third.
Julias Lukasiewicz, from Carlton University in  Ottawa , Canada, said that if new high speed trains were to be successful in North America then they had to include a new track like the Japanese  Bullet trains or the french TGV.
He said that the British APT (Advanced Passenger Train) and the Canadian LRC (Light, Rapid and Comfortable)— which like the APT tilts as it enters bends — were failures.
He said that new passenger services could be introduced only in densely populated areas.
In 1981 the Japanese national railway started studying four such routes; Los Angeles to San Diego; Dallas to Houston; Miami to Tampa and Chicago to Detroit.
Last year the American High Speed Rail Corporation was set up, backed by a $750000 Amtrak loan, to study the schemes in more detail.
The 200 km Los Angeles to San Diego route for example could be covered by a  Bullet train in just 59 minutes.
Lukasiewicz says that ‘fast operations on dedicated tracks (such as these) have been most successful in every way.’
o West Germany's experimental maglev train, the 400 km/h Transrapid 06, should make its maiden flight this summer.
It will run on a 20-km elevated track, in Emsland, near the Dutch border.
Computer cartoonists in a bottleneck
AFTER 20 years of technical development designers who use computer graphics to produce animated films are still stymied by a technological bottleneck.
Although computers and software are getting better and cheaper all the time, there is still no way of transferring pictures from a computer's memory straight onto videotape or film.
Animators now face a dilemma of technology overkill.
After years of striving to produce computer graphics that look like human drawn animations or live photography, the bosses in the film and TV business now say they want computer graphics to look more like graphics.
It costs £100000 a minute to produce top quality graphics, on a computer system costing £10 million, so the people paying do not want something that looks as if it was shot with a camera for a few thousand pounds.
So some animators in Europe are looking at much cheaper ways of making computer graphics.
In any graphics system the object to be animated is first drawn on paper in a number of key positions.
These are then traced on a touch-sensitive drawing board to make digital data signals which are dumped in the computer memory.
Each design studio then uses its own software to extrapolate the changing shape of the object as it moves between the key positions.
To simulate reality the moving object must appear to have depth, but not be transparent.
So lines at the rear of the object must disappear as they move behind those at the front.
A century-old optical theory is used to simulate the effect of reflected light.
The latest software can imitate the texture of flesh or the topography of a mountain range.
But now that computer animation looks so real, some advertising agencies are insisting that the images they pay For must look as if they have come from a computer.
In the US, graphic designers at the New York Institute of Technology use hardware costing $20 million.
But in London, the Middlesex Polytechnic teaches 200 students a year with one computer system costing £100 000.
Last year one Middlesex student became the first to submit a postgraduate thesis as a video cassette tape.
Because of its expertise and low cost, the BBC now employs Middlesex Poly to make graphics.
All the snowflake effects used by the BBC to advertise its programmes last Christmas were generated on the Middlesex computer.
Other broadcasters are following suit.
Channel 4 paid a professional firm over £50000 for its main logo, seen every night.
But Channel 4 is now buying in graphics from Middlesex at a fraction of the price.
Every computer studio, big or small, still faces the same technical problem.
As a final step, all the animation data in the computer memory store must be transferred onto video tape or film, one picture frame at a time.
The data can be displayed as a still picture on a TV screen and photographed, or it can be loaded into a video framestore and transferred onto video tape.
Graphic designers compare this bottle neck with the electronics industry's predicament before the transistor was invented.
A fortune awaits any engineer who can devise a way of transferring, in real time, a sequence of animation direct from a computer's memory to video tape or film.
Captain's birdseye
PICTURES from satellites will soon help to steer Japan's fishing fleet to bigger catches.
In a pilot scheme carried out from September to November last year, the Japan fisheries Information Centre combined data from the US NOAA-7 satellite with information from its usual sources to draw up charts predicting the whereabouts of fish.
The information centre sent the charts to the fleet (via a facsimile link, which 60 per cent of Japanese boats carry).
The scheme worked well: four-fifths of the boats taking part found fish in the predicted areas, good news for a country that makes 15 per cent of the total world catch.
The advantage of information from satellites is that it is much more comprehensive than data that aircraft, research vessels or fishing boats supply.
NOAA-7 covers Japan's 200-mile (370-km) fishing zone five times a day.
Its instruments collect information on the temperature of the sea surface, cloud cover, and the movements of tide and currents.
The fishermen will get all this information for nothing: the fisheries information centre receives its funds from the government and fishing organisations.
In the pilot scheme, the centre's staff took about 20 hours to compile one chart, largely because it had to do much of the cartography by hand.
But by the time the scheme goes into year-round service, in 1985, computers will cut the time-lag down to just four hours.
The centre says that taking the guess work out of fishing will save the industry tens of millions of pounds every year.
Hall meter
RESEARCHERS at the University of Manchester Institute of Science and Technology have found a way of measuring the current in an electric circuit without breaking the circuit, or even disturbing the insulation.
Building on an idea by the university's Dr Emanual Cohen, HEME Ltd is marketing a digital ammeter which works by an ingenious application of the Hall effect.
An electric current creates a magnetic field around the wire through which it flows.
The clip-on ammeter measures this magnetic field, which is proportional to the current flowing through the wire, by means of the Hall effect induced in a piece of semi conductor.
Normally, the current flowing in a circuit is measured by connecting an ammeter into the circuit.
But ammeters have their own internal resistance, which interferes with what is being measured.
Because the Hall effect ammeter measures the current remotely, it imposes no load on the system.
Tin from the seabed
THE BACKERS of an unusual operation to recover tin From mud on the seabed off Cornwall are interested in setting up similar ventures elsewhere.
Marine Mining, which is supported by a trio of American investors, will process material dredged from the sea off Cornwall's northern coast.
A processing plant at Gwithian will extract from the mud tin oxide which, over the centuries, has channelled out to sea as waste from mines New Scientist 21 April, p 159).
Gilbert Kerlin, one of the project's backers, says he is confident that the venture will prove profitable.
When it starts up fully in 1985, the plant should produce some 500 tonnes of tin a year, one tenth of Cornwall's present output.
Kerlin is looking at other metals extraction projects that could use similar techniques.
These could, like the venture in the West Country, reclaim sediments washed out to sea from industrial works.
Alternatively, the ventures could obtain materials on the seabed that are deposited through natural mechanisms, for instance through rivers washing away soil.
Kerlin says that other projects could start in Britain — although, for the time being, he will concentrate on Cornwall.
Big screen
VIDEO is hitting the big screen.
Rank has launched a video projection system for around £10000 which will show clear bright pictures on a screen up to eight metres wide.
Anyone designing a television projector faces the problem of generating enough light for the system to work with a flat screen and wide viewing angle.
Rank's Hi Beam has three projection lenses and three 15-cm cathode-ray tubes.
The tubes are driven by a 35 kV supply, and cooled with a liquid heat sink.
When the three colour images converge on the screen, they form a full colour picture, with a total light output of 350 lumens.
The picture quality, even on a large screen, is surprisingly good.
New U-2 comes to Britain
NEARLY a quarter of a century after the Russians shot down Gary Powers' U-2 spyplane, the latest version is starting operations from Britain.
Lockheed is building the TR-1 tactical reconnaissance version (above) on jigs that the United States Air Force kept in storage since production of the original U-2 ended.
The USAF is building up its first TR-1 squadron at Alconbury in Cambridgeshire, from where they will fly spy missions.
The aircraft carries nearly two tonnes of radars and cameras.
The TR-1 can fly over friendly territory, using the radar to detect objects on the other side of the border.
The radar has a range of about 55 km and provides information such as movements of tanks and artillery.
The aircraft flies comparatively slowly, at about 700 km/hr, but its big wings allow it to cruise at a height of more than 21 000 m — nearly double that of a transatlantic airliner.
Lockheed has so far delivered six of the 35 TR-1s on order.
The USAF will operate about half the total from Alconbury, and most of the rest will carry equipment to detect enemy radars.
Digital television comes close to reality
ITT has invested £20 million over the past decade on the design of a television set that handles picture and sound signals in digital code, instead of the conventional analogue waveform.
For the public, there will be no immediate advantage when the ‘Digivision’ sets first come onto the market this winter or next year.
They will cost the same as conventional analogue television sets, and the pictures will look the same.
But the Digivision sets will contain fewer components to go wrong, and will be far easier to service when they do.
Also, they will be easy to upgrade by fitting new chips as they become available, offering features not possible with analogue circuits.
A digital television immediately decodes the incoming sound and picture signals and converts them into an 8-bit digital code.
All-digital circuits, like those in a computer, handle the signal.
The signal is converted back into an analogue waveform just before it is fed to the picture tube and loudspeakers.
The set will have seven integrated circuits or chips, which ITT makes in West Germany.
Although the company has so far finished only four chips, all seven should be ready for the prototypes that the company will exhibit at the Berlin Radio Show in August.
The seven chips replace 282 separate components, for instance transistors and resistors, and cut by over 700 the number of solder connections.
The sets will therefore be very easy to make.
Computers will do the physical ‘tweaking’ needed to adjust the picture, and to control quality.
And if a set goes wrong later, a service engineer can bring the picture back to normal by plugging in a computer handset which re-adjusts the circuits in the same way.
But the most exciting aspect of Digivision is its future potential.
The next generation of chips, which the company could fit in existing digital sets, will offer selective zoom, still pictures, and multistandard operation.
A frame store could freeze any moment of the action on screen, and display it as a still picture.
The zoom control would allow the viewer to enlarge any area of the picture displayed.
And because the signals inside the television are in digits, it is of no consequence to the circuits whether the input is the US standard 525-line or the European 625 lines.
So a single set can cope with either standard.
It is also relatively easy to double the number of lines displayed to improve definition, or to double the number of picture frames displayed every second to eliminate flicker.
A viewer will also be able to check what's on another channel by inlaying a small picture of that channel in the corner of the main picture.
Interference and ghost images caused by reflections of the received signal are more easily cancelled out in a digital television.
Although ITT claims to have a clear lead in the new technology, it is not alone in the race.
Philips of Holland, RCA in the US, and Matsushita and Sony in Japan are all working along similar lines.
Dyes provide the key to optical memories
SCIENTISTS in California are developing a new kind of computer memory, based on a photochemical bleaching process.
The team, working for IBM, is bleaching very narrow holes, with a laser, in the optical absorption band of a dye molecule.
A piece of material 1 cm square, marked with this process, could store more than 1000 million pieces of information.
A dye appears coloured because its molecules absorb light from a part of the visible spectrum.
In the narrow region of the spectrum where the dye absorbs, it will appear opaque.
But a powerful light-source, such as a laser, tuned to the same part of the spectrum, can give the dye enough energy to make its molecules undergo a chemical reaction.
The process is called photochemistry.
If scientists can make all the dye's molecules react photochemically, and make a product that absorbs in a different part of the spectrum, the material will now appear transparent in the region that the dye molecules originally rendered opaque.
The laser has ‘bleached’ the dye.
Imagine a sheet of material that contains the dye, cross-hatched to divide it into an array of small squares.
Each square will be a memory location for one bit of information.
Bleaching a square with a powerful laser produces a’ 1’of binary information.
Squares left unbleached provide the ‘0s’.
Another laser, too weak to bleach the dye, reads the memory.
If the light passes through the material, the square is unbleached — so the scanner reads ‘1’.
If the light is absorbed it reads ‘0’(see diagram above).
The scientists at IBM have taken this idea one step further, by using different light frequencies to store extra information at each location.
When the dye molecules are dissolved in another solid, transparent host material, they do not all absorb light at quite the same frequency.
They occupy a variety of different sites and orientations; molecules in different sites absorb at different frequencies.
If the host is a single crystal, the spread of frequencies is not very great.
However it can be up to 1000 times larger if the host is amorphous, or non crystalline.
The team at IBM uses a laser beam containing a very narrow range of frequencies to bleach the molecules in one particular type of site, but leaving the others unaffected.
They burn a hole in the absorption band — which is why IBM calls this a ‘photochemical hole-burning memory’.
A weak probe laser detects the holes in the same way as for the simple memory.
Of course there are complications.
IBM has found a lower limit to the width of the line that can be burned in the absorption band.
This varies from material to material: a dye called phthalocyanine, dissolved in a polymer called PMMA, can store 1000 bits in its absorption band.
The same dye in  polyethylene can hold 10000 bits.
Another limitation is more serious.
The memory works only if the material is kept at very low temperatures.
High temperatures cause the dye molecules to move, and can reverse the bleaching reaction.
The cost of cooling the memory makes it uneconomical for small systems, but it would be feasible in large memories.
IBM is now looking at different materials and working on the many technical problems of writing on, and reading, the memory.
George Castro, one of the scientists on the project, says optical computer memories are a ‘long shot, but a serious possibility’.
‘It will not be appearing in any systems in the near future, but the cost advantage for large systems makes it an attractive prospect.’
REVIEW
Master of the drive into space?
JPL and the American space program by Clayton Koppes,Yale UP, pp 320, £16.95 
Jack Meadows
THE Jet Propulsion Laboratory (usually abbreviated to JPL) has been a world leader in space research from the earliest days of the subject.
To British universities creating, or contemplating, science parks, it might well seem the ultimate target at which to aim.
It was formed as an offspring of the California Institute of Technology (correspondingly abbreviated either to CIT or Caltech), and has retained its university connection ever since.
The financial consequences for Caltech have been very significant: by the early 1960s it was receiving $1¼ million a year for administering JPL.
It is one of the great virtues of this history of JPL that Clayton Koppes tells us not only about the scientific and technological progress of the laboratory, but also about the financial (and managerial and political) aspects.
In some ways they are more interesting, because less explored.
JPL began life in 1936 as a semi-official group attached to Caltech's Guggenheim Aeronautical Laboratory.
The group's concern with rocketry was hardly respectable in those days: its existence was tolerated mainly because it came under the wing of Theodore van Karman, one of the world's greatest aerodynamicists.
The proto-JPL proved itself during the Second World War.
It worked on both solid and liquid propellant rockets.
The former were more immediately successful, with the development of jet-assisted take-off; but the latter clearly offered greater longer-term promise.
Towards the end of the war, the much-expanded rocket group was named the Jet Propulsion Laboratory —‘jet’, in this case, referring to the ejection of material, and so covering rockets.
With the growing fears of the Cold War, JPL became very heavily involved in work on missile technology.
It played a major role in the production of the first tactical nuclear weapons; and, in consequence, moved sharply away from basic investigations towards engineering development.
By the early 1950s, less than a third of the JPL budget was being spent on basic research.
The escape from this ever increasing emphasis on the ‘D’ part of the R&D was provided by the space race.
Koppes comments on the launch of the Russian Sputnik: ‘JPL personnel could remember years later where they were when they heard the news, what they first thought, and what they did.’
Early in 1958 the laboratory collaborated with Wernher von Braun's group in the launch of Explorer I — the US answer to Sputnik.
Within a short space of time, the laboratory had ceased to be involved in the launch vehicles, themselves (so making ‘Jet Propulsion’ a highly anomalous title), and had become connected to the newly-formed NASA empire.
From now on, JPL pioneered the drive farther and farther into space.
The Ranger series of spacecraft for impacting the Moon were not entirely a success story; but the soft landing Surveyors, which followed them, were.
The full blossoming of JPL space research came with its astoundingly successful  planetary probes — the Mariners to Venus, Mercury and Mars; the Viking soft-landers on Mars; the current Voyagers to Jupiter, Saturn and Uranus.
Koppes describes the tribulations and the glories of all these  projects very well.
If there is a defect in his narrative, it lies in the relatively scanty comment on the impact of JPL on the national and international scientific community.
For example, he mentions the Argus experiment, in which atomic bombs were exploded above the South Atlantic to inject particles into the Earth's radiation belts.
Nobody would guess from his bland description that this experiment infuriated scientists throughout the world.
What he does describe well are the two faces of excellence as they have characterised JPL.
On the one hand, ‘Many observers found that JPL in the early and mid-1960s had that high esprit and cohesiveness that mark extra-performance institutions.’
On the other, this self belief had its defects.
‘JPL's arrogance…received frequent comment.
There was no doubt that the laboratory's arrogance made it all too often unnecessarily hard to deal with and overconfident of its abilities.’
Now, with the waning of expenditure on planetary research, both the cohesiveness and the arrogance of the institution are somewhat less evident.
The organisation of JPL, particularly the lack of tenured appointments, must make for increasing internal, as well as external competition for funding, unless additional sources can be tapped.
With the appointment of a new director, who is a former air force chief of staff, a return to military funding, as in the early days of the laboratory, might seem an obvious option.
But JPL is still associated with Caltech, and acceptance of classified research at US universities is much less ready now than it used to be.
That JPL should continue as a research leader is a matter of considerable concern to planetary scientists everywhere.
We can only hope that the current problems will be resolved.
Meanwhile, potential creators of university-related science institutions in the UK could do worse than read Koppes's book for some idea of what might await them.
The future is delayed
Profiles of the future by Arthur C. Clarke,Gollancz , pp 251, £8.95
Peter Marsh
This book is subtitled: ‘An enquiry into the limits of the possible’.
The first edition appeared in 1962, when space satellites were in their infancy and before anyone had invented the word ‘microelectronics’.
Despite everything that has happened in the interim, the new edition, which includes only minor revisions, is still a thought-provoking and comprehensive primer on the future.
Arthur Clarke skates through questions about mankind's long-term role in space; transport systems for the Earth; antigravity devices; minerals and energy supplies; time travel; and machine intelligence.
All this is done with a deftness — not to say cheekiness — that keeps the reader hooked.
For example, Clarke scorns those who argue that mankind will not eventually move out to colonise the vast expanses of space.
He compares these people with the more conservative of our piscatorial ancestors who, a billion years ago, resisted the temptation to clamber on to dry land and decided to stay where they were.
‘They did,’ says Clarke.
‘They are still fish.’
Clarke writes a good deal more intelligently — and entertainingly — than many who prattle on about the future.
Early on in both editions he writes, convincingly enough, of those who are unable to predict what will happen even when presented with all the necessary facts.
Such people, in Clarke's view, exhibit either a failure of nerve or of imagination, History is littered with examples of eminences who said such items as the aeroplane or the telephone were impossible, Yet Clarke himself demonstrates, unwittingly perhaps another well-known human maxim: if something can go wrong, it probably will, Thus in his 1962 edition, he predicts that nuclear rockets, translating machines and efficient energy storage devices would be around by 1970.
According to his original view of the future, planetary landing would have taken place by 1980.
The new edict is that we will have to wait for the year 2000 to land anywhere in space other than on the Moon.
A good system for storing electricity will not become possible before 1995, Meanwhile, the date for artificial intelligence is shunted back 20 years to 2020 and the time for weather control is 2030 instead of 2015, All kinds of obstacles, not so much technological as human and economic, have interfered with the 1962 predictions, as with many others that people have made with seemingly unimpeachable authority over the past 50 years.
Clarke would do us all a big service if, in his next book, he could produce a law that sorts out the predictions that are too spineless from those that assume the human race to be capable of too much,
Recycled wildfowl plates
The wildfowl of Britain and Europe by Malcolm Ogilvie,Oxford UP,pp 84, £6–95 
Stephen Mills
OXFORD has found an excuse here for recycling 30 pages of wildfowl plates from Volume 1 of its prestigious handbook,The Birds of the Western Palearctic .
N. W. Cusa's ducks are like attractive wooden decoys.
Peter Scott provides the geese tastefully disappearing into a background of midnight green.
The book is very pretty and very slim.
Each plate is accompanied by an explanatory page to aid identification.
Most beginners, for whom the book is presumably intended, will find this useful, although there are one or two slips.
The immature Greenland whitefront, for instance, is accidentally confused with the immature of the European subspecies.
The 20-page text makes a valiant attempt to comprehend wildfowl biology.
Given so little space, however, perhaps we need not have been reminded, as we are in Stanley Cramp's foreword, that ‘Ducks, geese, and swans live in or near water’, nor that the legs of the last two ‘are placed near the middle of the body, so that the bird stands fairly upright’, Malcolm Ogilvie can tell most people something they did not know about wildfowl, but in this case his publishers clearly rushed him.
Idyllic life on the seashore
The Middle Stone Age at Klasies River Mouth in South Africa by Ronald Singer and John Wymer,Chicago UP,pp 234, £2l 
Sarah Bunney
The caves and rock shelters of the Klasies River Mouth, for those without an atlas to hand, are about 100 kilometres west as the crow flies from the Eastern Cape Province town of Port Elizabeth in South Africa.
People first settled there perhaps 130000 or 120000 years ago and the site seems to have been almost continuously occupied for the next 40000 to 8000 years.
Left behind was a remarkable amount (some 22 metres) of soil and occupational refuse consisting of a vast quantity of stone tools, bones and empty shells.
There are some special features about the Klasies River Mouth site, which this book draws attention to, that give it an important place on maps of the prehistoric world.
Certainly it stands comparison with the better-known Haua Fteah cave in North Africa, which hunters and gatherers occupied from about 75000 years ago and where shellfish also played an important part in the people's economy.
The Klasies River Mouth site is made to sound almost idyllic by Ronald Singer and John Wymer, who excavated there during two seasons from 1966 to 1968: ‘while the sea was close, the attractions of the site were numerous: an almost unlimited supply of shellfish; large and small mammals  available for hunting in the immediate hinter land and at watering places along the adjacent river; abundant fresh water; seals, dolphins and fishes from the sea; marine birds to be caught easily on the beach; natural caves and rock shelters for protection; and an unlimited supply of good quality siliceous quartzite for making tools and weapons.
It is thus not surprising that the archaeological record infers a continuous or near-continuous occupation of the area for many hundreds of generations.’
The site's occupant's did indeed take full advantage of the abundant shellfish nearby and as Elizabeth Voigt, one of the contributors to the book points out, they were also selective in what they ate.
It is, however, unlikely that the people fished; the few fish remains archaeologists found at the site were probably washed ashore after a high tide.
Seals, dolphins and penguins may have been hunted but they too were probably scavenged from the beach.
The dependence on shellfish by the people of the Klasies River Mouth for a large part of their food continued for tens of thousands of years and is one of the most interesting of Singer and Wymer's findings.
It represents the earliest and most extensive exploitation of marine resources yet known in southern Africa and possibly anywhere else in the world.
As for the physical appearance of the people who left behind their stone tools and food refuse, Singer and Wymer can say little because, compared with animal bones, there are few human remains in the deposits and they are mostly fragments of skulls and jawbones and teeth.
What little the authors can say, however, should stir up debate in the palaeoanthropological world.
Their main conclusion is that the earliest occupants of the Klasies River Mouth site (before 100 000 years ago) were modern-looking people (Homo sapiens sapiens)who would have been ancestral to the indigenous African Negro and Klioisan peoples.
Their call for a reconsideration of Negro origins reinforces a claim late last year by two South African scientists that southern Africa not West Africa was the home of the first Negroes (New Scientist , 13 January, p 90).
This book is an important contribution to the archaeological literature of southern Africa.
Inevitably, in a detailed report of this type much of the information is specialised.
Readers interested in the shellfish-collecting side of life at the site might find it helpful to read Elizabeth Voigt's chapter alongside Betty Meehan's fascinating Shell Bed to Shell Midden , a book newly published by the Australian Institute of Aboriginal Studies in Canberra on the role of shellfish in the life of a group of Australian Aborigines who live in Arnheim Land in Northern Territory.
Mysticism in the right hemisphere
The mind field by Robert E. Ornstein,Octagon Press, pp 139, £5 
Stuart Sutherland
IT is seldom remarked how curious it is that so many people need to find a meaning to life.
If man has evolved to maintain his selfish genes, why cannot he be happy to do just that and to fulfil the other biological drives like curiosity, sociability and the desire to dominate that at one time presumably aided the survival of his genes?
One possibility is that only those who fail to fulfil their biological drives need mysticism as an escape, but the rich and famous seem no more immune to the lure of the Maharishi than the rest of us.
In The Mind Field Robert Ornstein, a psychologist, attempts to give a balanced view of the cults, current in the US, that purport to bring self-knowledge and to change their disciples' consciousness.
He maintains that their attraction is based on modern society being too materialistic, but it is doubtful if Western man is any more materialistic today than in Roman or Victorian times.
Moreover, mystical movements are no more prevalent in the West than they have ever been from the Dionysian rites to the temple of Aimee McPherson.
Unfortunately, Ornstein is vague about the nature of the change brought by the successful pursuit of enhanced consciousness.
He can do no better than argue that it involves apprehending things simultaneously rather than in succession though he does not explain how the successful mystic can transcend the limits of the magic number seven, which most psychologists agree is the maximum number of entities that can simultaneously be held before the mind.
He also claims that it is desirable to think ‘intuitively’ and not by logical steps, but again logic is the exception not the rule in human thought and all the greatest scientific discoveries have been made by the use of informed intuition, Ornstein believes that the right hemisphere is underdeveloped in Western man and the spiritual growth depends on expanding its activities.
In most people the right side of the brain has little or no verbal capacity but is slightly more important than the left for spatial ability.
Unfortunately for his thesis, women, who are normally thought to be more intuitive than men, have on average less spatial ability, while spatial ability is more important for scientific thinking than is verbal ability.
It is difficult to reconcile such findings with Ornstein's claim that intuitive non-logical thinking is a function of the right hemisphere.
He goes on to argue that as the right side of the brain has no language capacity, the knowledge it acquires cannot be put into words: this may explain the failure of his attempts to do so.
Although he might perhaps have done better to observe Wittgenstein's adage, ‘Whereof one cannot speak, thereon one must be silent’, one cannot help regretting that he has not tried to be less vague about the nature of personal growth.
Is it acceptance of oneself?
Freedom from petty worries, from envy, jealousy and avarice?
Or since these desirable states can be put into words, do they belong only to the left hemisphere?.
Ornstein is rightly scathing about many of the spiritual nostrums on offer in America from the Marin County psychotherapists to ‘pop yoga’.
‘instant yen’ and ‘the searches for Shamans in the desert’, but unfortunately he offers his readers no Good Cult Guide and one feels that many of them may continue to go astray.
The only method of teaching the right hemisphere on which he sets his seal of approval is contemporary Sufism, a Muslim sect which teaches largely by ingenious parables, several of which he reproduces.
Although the book as a whole is unsatisfactory, Ornstein does have some sensible things to say.
He points out, for example, that personal change is a slow business and is unlikely to be achieved by attending an instant weekend run by est or some other faddish organisation.
He also notes that the fact that transcendental meditation changes one's brain waves does not imply that it is in any way helpful; he might have added that the brain waves  during meditation show a remarkable resemblance to those of someone who is drowsy or even asleep.
Moreover, he urges his readers not to join sects that make their adherents abandon their families, give up their money and worldly interests, and lead restricted lives that are ordered entirely by the rulers of the sect.
This advocation makes one pause.
Surely the lives led by some nuns and monks are both admirable and happy.
Perhaps the reason is that they are dedicated to something outside themselves, namely to God, and are not interested merely in the selfish development of their right hemispheres.
Earth-shattering events
A history of Persian earthquakes by N. N. Ambraseys and C. P. Melville,Cambridge UP, pp 219, £33 
Robert Muir Wood
EARTH tectonics is remarkably slow.
In one lifetime the Atlantic may widen by a person's height.
The sluggish movements deep in the Earth eventually cause the rigid crust to fracture along great fault planes of weakness.
These sudden movements quake the Earth.
Earthquakes, therefore, provide access to active tectonics; the dynamic Earth.
Even at the simplest and most active plate boundaries, a major earthquake caused by a few metres movement of a fault recurs perhaps six times a  millennium .
Professor N. Ambraseys was arguably the first to recognise the full implications of the study of the history of earthquakes to the science of tectonics.
The book that he has written with his former research student, a linguist and historian, although concentrating on a remote and antique land, is the first full definition of the scope of this new historical science.
The reconstruction of ancient earthquakes requires painstaking research into diaries, histories, chronicles, travellers tales etc; in this book many of the accounts are collected into a synoptic map for the first time.
The information is not only in written descriptions; it is also archaeological; and even mechanico-archaeological.
Until the creation of the standardised network, only 20 years ago, of worldwide seismic recorders, there were in operation a diverse and uncoordinated variety of instruments, run by a scattering of seismophiles.
To be interpreted, the earthquake traces must first be decoupled from the instruments that created them.
The total enterprise is vast and this book is extraordinarily rich in historical detail.
The footnotes run to one-fifth of the main text, and can be read on their own, Byzantine, merit for insight into ‘The decreasing number of standing columns at Persepolis’ or ‘disapproval of the assessment of earthquake intensities from a helicopter’.
The wealth of scholarship in which the book luxuriates sometimes deflects from the far simpler scientific story; that seismicity in the different regions of Iran varies through time, and that it is possible, from a complete analysis of the history, to find patterns in this shifting activity.
Such history reveals the instability common to continental plate boundaries, from China to Turkey, and can bridge the scientific investigation of the present and the aeons of geological history, in which the individual tectonic event becomes lost.
The new research area turns historians into scientists and vice versa.
Its vitality is such that A History of Persian Earthquakes should be read by anyone wishing to discover one of the most cultured and intriguing developments of the new Earth sciences,
Anthropomorphism and automata
The intimate machine by Neil Frude,Century, pp 190, £4–95 
Chris Reynolds
DAVID HUME in 1757 wrote ‘There is a universal tendency among mankind to conceive all beings like themselves and transfer to every object those qualities with which they are intimately acquainted and of which they are intimately conscious.’
Neil Frude's book brings together a wide range of stimulating ideas related to the association of human attributes to non-human objects.
For instance, he examines the development of dolls from 3000 BC, via the 18th century mechanical automata, through to the possible future delights (or horrors) of pornographic robots which entertain their human companions in ways that might well be measured in kilocuddles.
He also highlights the well-known dangers of people becoming addicted to computers (New Scientist , 24 February, p 532).
However it would be misleading simply to concentrate on the more provocative statements in Dr Frude's book.
The text is divided into three sections and each has something to offer the reader.
‘Future friends’, the first part, describes the history of computers and the way that they are currently developing.
This is an extremely readable account aimed at those with no computing knowledge.
The only serious flaw is his use of BASIC to provide examples of how a programming language works.
Modern high-level languages are concerned with the symbolic manipulation of information, and meaningless data names and commands detract from the otherwise easy flow of text.
Frude starts the second section, ‘Encountering the inanimate’, by examining the various types of interaction that people and computers can have.
On the negative side, there is addictive programming and game playing while, on the positive side, conversational systems and teaching aids.
In one evocative passage, during a discussion of animism and the development of automata, he refers to a six-year-old girl showing her doll how a computer works and explaining that it is a friend.
He suggests that there is something a little unnerving about a young child assuring a piece of moulded plastic that a metal machine is a playmate.
The final part ‘Towards the intimate machine’, discusses the possibility of developing computer systems with personalities and the ability to adjust to the need of the human user.
The book concludes with a review of the possible impact of more intimate computers, for good or ill, in various areas of human life.
These include education, politics, law and order, trade, leisure, religion and sex.
If I have any criticisms of this section it is that it underestimates the difficulties of writing software and ignores the influence the historical development of computing has on the acceptance of new ideas (what Seymour Papert calls the QWERTY  phenomenon ).
The Intimate Machine raises many issues concerning the social impact of computers in an invigorating and highly readable manner.
It also provides an informative popular account of current research in man-computer systems.
I wholeheartedly recommend it.
Talking heads — the cinderellas of this world
WITHOUT apology, I return this week to TVS's excellent Men of science series.
Two reasons for doing so.
First, because these interviews by Professor Ian Fells ought to convince producers elsewhere that talking heads are by no means a disaster recipe.
Secondly, because programming chiefs should recognise that topics of crucial importance for education, industry and national life bear examination in more crowded regions of space time than Sunday midnight in the south.
Last week's (24 April) discussion with Professor John Ashworth made the point well.
Here was the Cabinet Office's former chief scientist, now a vice-chancellor engaged in pulling Salford out of UGC-imposed penury, who chatted entertainingly about issues in the two domains.
Though not a  conspicuously modest chap (he confirmed that he had taken ‘an ethos of academic excellence’ with him to his original chair in Essex).
Ashworth is certainly out-going and in-thinking, a perky televisual subject.
Most interesting was the contrast between Ashworth on the Think Tank and on the University Grants Committee.
Parrying criticism of the former's 1976 energy report by emphasising that two-thirds of the Central Policy Review Staff's work was unpublished, he illustrated its merits largely via one of his own individual contributions.
Pre-Windscale, he recalled, two Secretaries of State had been on a collision course.
Energy was anxious that the CEGB's programme was not inhibited, and concerned about a contract in the offing for Japanese nuclear waste reprocessing.
Environment was ‘very worried indeed’ about Cumbria's difficulties in tackling a problem of planning and land use.
So how did the Chief Scientist proceed?
‘I sat down at my desk with a piece of paper, and asked myself what a minister required to know in order to contribute to the debate’.
It turned out that no more than three basic facts were essential, and that just two critical questions needed to be asked.
The rest could be pared away.
With one bound, Jack was free.
And this, Ashworth explained, was a key role which he and his ‘Think Tank’ colleagues played — preventing politicians from being blinded by science or pressured by lobbies.
And the composition of the Think Tank?
Lord Rothschild, Ashworth opined, was ‘a splendid 18th century figure who had recruited his original colleagues in a splendid 18th century sort of way’.
Such a procedure was irrational, but criticisms of those arrangements as undemocratic were ‘unfair’.
Turning to Britain's educational system, the vice chancellor of Salford adopted a different perspective.
Too often in the past, he said, politicians had been bamboozled by specialists into believing that the problems were so great that the answer was to set up bodies like the UGC and let them make the decisions.
‘In a democracy, that is very dangerous, because at the end of the day ministers are elected to take responsibility and if they fail in that, they are not doing their duty.
Ministers must themselves address these problems and not leave so-called quangos to tackle the job for them.
That is not democratic.
It is the antithesis of democracy.’
Ashworth, with more reason than most VCs to resent the UGC's butchery in July 1981, also commented on its coyness about revealing the thinking behind those decisions.
He could only guess that the mood was one of retrenchment and retreat, with a desire to preserve at all costs a ‘rather narrow, English concept of academic excellence’.
Describing his own.
alternative vision, he then threw out the thought that, far from varsity virtue being reflected in high A-level grade inputs, the real test was to accept lesser qualifications but achieve a high quality output (as at Salford).
On this basis, Oxford and Cambridge had it easy.
Only one thing wrong with this programme — too many unexplained allusions.
Geometry comes up to date 
Robert Dixon
That oldest of disciplines has been  neglected by 20th-century educationalists.
Modern geometry finds expression in computer graphics.
They open up obscure geometric ideas, such as the transformation called inversion
GEOMETRY is the branch of mathematics, or more precisely the root, that derives from spatial intuition and insists upon visual expression of its theory.
For
Euclid, the founding father of geometry in the 3rd century BC, the drawing instruments were compass and templates.
To construct a right angle, or a regular pentagon with these tools, you were given a series of instructions: do this, followed by this, followed by this…
The instruments, the algorithms and the theory were inseparable parts of the whole study.
In the intervening  millennia however, mathematics has grown many new theories, often explicitly spatial, which go beyond the nature of these ancient instruments.
But there has been no corresponding development of graphics technology, until now, that is.
Computer graphics is, among other things, just what the mathematician needed: an appropriate mechanism for turning algebra into forms.
Transformational geometry is one particular area of mathematics for which the computer is the ideal tool.
Indeed, geometric transformations are fundamental to much computer graphics technique, for producing perspective drawings, for rotating ‘objects’ in space, and other data manipulations.
With the aid of a computer what was hitherto discussed in theory and indicated in schematic diagrams can now be drawn electronically, with great speed, accuracy, and richness of detail.
The basic way a computer performs a geometric transformation relies on the Cartesian coordinate system, an invention of the French mathematician and philosopher René Descartes in the 17th century.
This allows us to describe uniquely the position of any point in a plane (or in space) by an ordered pair (or trio) of numbers, called coordinates.
Starting from a fixed origin point O we can reach any other point by travelling so many units across (x) and so many units up (v).
So, for example, the corners of a square are points located by four distinct ordered number pairs of the form (x, v).
If we feed these numbers into a computer that is programmed to alter them according to some rule or another then the output is four new number pairs ().
By reinterpreting these number pairs as points in the plane and connecting them with lines we are drawing a transformation of the original square.
The rule for changing the numbers (x y) into new numbers () can be virtually anything, as simple or as complicated as we like.
Geometry disciplines our approach by analysing the variety of possible transformations, starting with the simplest and moving on to take each new complication in turn.
It is in this way that we are able to develop an appreciation of spatial order.
Figure 1 shows several of the most elementary transformations together with the rules that generate them.
The formula, , for example, generates a translation : all points in the square are moved to new locations three units to the right.
A translation is a transformation that alters only the location of the square (or whatever) without altering its shape, its size or its orientation.
Transformations that leave shape and size unaltered are obviously the most basic, and geometers call them isometries .
As well as translation,rotation , and mirror reflection are isometries.
Both of these alter orientation; the latter is also sense-altering, turning a left hand into a right hand.
Obviously, one transformation followed by another together constitute a transformation, and following one isometry by another isometry produces an isometry.
However, following one reflection by another reflection constitutes a rotation about the intersection of the mirrors, unless the mirrors happen to be parallel, in which case the product is a translation (Figure 2).
Even in this case we can consider the translation to be a special case of rotation about a centre infinitely far away.
Another class of transformations contains the similarities , transformations that alter size, but not shape.
If we feed the following rule into our computer, , then the consequent transformation makes the polygon, or whatever, three times bigger without altering any of its relative proportions.
Like congruence the concept of similarity is as old as Euclid.
Yet it is currently playing a vital role in contemporary geometry, as in the work on self-similar fractals by Benoit Mandelbrot, at IBM's research centre at Yorktown Heights in New York (Figure 3a).
More simply, the similarity transformations can be seen to describe the growth and form of mollusc shells (Figure 3b).
Then there are the affrine transformations, which destroy shape and size but maintain straightness and proportions in any line, as well as parallels; and projective transformations which destroy shape, size, proportions and parallels, maintaining only straightness, as in perspective.
But it is with inversion that we find the first really unfamiliar transformation.
To describe its rule most clearly it is best to use polar coordinates to locate points in the plane.
Instead of defining position in terms of x-across and y-up, we describe each point by its distance r and direction q from the origin point, O (Figure 4).
q is in fact the angle between a line from O to the point and the horizontal axis to the right of O (the ‘3 o'clock’ direction).
Once again, each point is specified by an ordered number pair (r , q), and any manipulation of these numbers corresponds to a geometric transformation.
Inversion is the transformation that takes (r, q) to  according to the rule: .
What does this mean?
We have selected one point, O, in the plane as our reference origin, from which the distance and direction (r , q) of every other point is measured.
The rule tells us, or the computer, where to move every point in the plane by computing the new location .
It is a ‘one-to-one’ rule, providing a unique image (new location) for each point.
The rule tells us to leave the direction of each point from the origin as it was, , but to alter its radial distance from O according to the calculation .
So what is ?
Well, it is a constant, which we choose.
I could have called it k instead, but for reasons that become apparent in a moment we will be interested in the size of the square root of the number, so I have used .
 can be any number we care to choose; different values merely alter the scale of the inversion.
So for every point (r , q) we tell the computer to divide our chosen constant  by r to give its new radial distance from O. In this way all points move to and from the direction of O in radial lines.
The rule of inversion is almost as simple (in algebraic terms) as a translation or a similarity, but the effects are not nearly so obvious, and to understand them requires a little more thought.
First, as , it follows that , which means that points interchange their location in pairs.
A point near the origin interchanges with a point further out.
Secondly, any point whose distance from O equals R , that is, lying on the circle of radius R centred on O, stays put.
In fact, these are the only points that do not move with inversion; they are said to be ‘invariant under inversion’.
The invariant circle is called the circle of inversion, and its centre, the centre of inversion.
All points inside the circle interchange in pairs with all points outside.
Inversion is thus a kind of ‘turning inside out’ effect.
The formula  gives an unambiguous answer to the question of what is r after inversion for every point in the plane but one; O itself, the origin.
What do we do with this point, for which r = 0?
Try telling a computer to divide by zero  and you will receive the abrupt reply ‘Error: cannot divide by zero at line…
’ According to inversive geometry this point interchanges with ‘the point at infinity’.
This not only completes the symmetry but also makes sense of the fact that as r approaches more and more nearly to zero, then  will become larger and larger without limit.
So what happens to figures, that is, configurations of points, under inversion?
Because the whole of the plane outside the circle of inversion interchanges with the whole of the inside, a great deal of distortion takes place.
And like mirror reflection, inversion alters sense, turning a left hand into a distorted right hand.
But three  properties do remain unaltered by inversion.
First, continuity is invariant.
Connected sets of points remain connected, and curves remain topologically-equivalent curves: the rubber sheet may be distorted but not broken.
Secondly, angles are preserved.
A triangle will be transformed into a curvilinear (actually, circular) triangle, whose curved sides will not possess the relative proportions of the original triangle, but the angles measured at the corners will not be altered.
Finally, and this is the essential theorem of inversive geometry, circles remain circular.
Figure 5 shows the effect of inversion on various circles and lines.
Notice how a straight line inverts into a circle.
The circular images of straight lines must pass through O, the centre of inversion, because lines extend to infinity.
Conversely, circles passing through O invert into lines.
Surely, this contradicts the theorem that circles remain circular?
No, because a line is the special case of a circle whose radius is infinite.
Another interesting special case is the circle in Figure 5, which cuts the circle of inversion at right angles.
This circle self-inverts; that is, its inverse is the same circle.
The arc outside the circle of inversion interchanges point for point with the arc inside, while the two points of intersection stay put.
One of the best known illustrations of inversion is the model on a Euclidean plane, such as this page, that the French mathematician Henri Poincaré made of the non Euclidean ‘ hyperbolic ’ plane, A Euclidean plane is one that obeys Euclid's famous fifth postulate: in other words, given a line, and a point not on that line, only one other line in the plane (a parallel to the first line) can pass through the point and not meet the given line.
But geometers in the l9th century finally realised the possibility in theory of a space that defies this rule.
In a hyperbolic plane an unlimited number of lines can pass through the point and not meet the given line.
In Poincaré's model of a hyperbolic plane the lines, or geodesics, are represented as circles and infinity as the finite circular boundary.
In his Circle Limit series, the Dutch artist M. C. Escher borrowed a version of Poincaré's model from H. S. M. Coxeter, professor of mathematics at Toronto University, and embellished its symmetries, just as elsewhere he had embellished the more worldly symmetries of the familiar Euclidean plane (Figure 6).
Taking any of the geodesics (described by the backbones of the fish) as a circle of inversion the whole pattern self-inverts (as in Figure 5b).
This inversive symmetry on our Euclidean page corresponds to mirror symmetry in the hyperbolic plane.
There are two other pictures by Escher that, if taken together, illustrate inversion.
One is a wall mural consisting entirely of fishes spiralling towards and outwards about a single pole (Figure 7).
Their paths are logarithmic spirals, which means that each orbit of the pole brings about the same proportional enlargement of their distance from the pole.
In this similarity pattern the fish must maintain a constant bearing with respect to the pole and therefore cannot actually reach the pole in any finite number of orbits.
Any magnifications of the pattern reveals, in theory, precisely the same pattern.
The other picture,Sphere Surface with Fishes , shows an analogous pattern, with the fish swimming between opposite poles on the globe.
Again they must perform an infinite number of circuits to reach either pole, as their paths are loxodromes , from the sailing term for keeping to a constant bearing.
The transformation that turns the pattern on the plane into the spherical version is none other than inversion.
To see this go back to Figure 5d.
The diagram shows a line being inverted into a circle.
Now imagine the whole diagram being rotated out of the page about an axis that connects the centres of the two circles.
The line will sweep through a plane, and the circles will sweep through spheres.
So in three dimensions inversion turns an infinite plane into a finite sphere in a one-to-one mapping of every point.
This is where computer graphics comes to the geometer's aid.
Drawing three-dimensional objects is quite difficult, but for computers the task is almost as easy as drawing a two dimensional object.
The computer simply projects the object onto a plane, in an act of transformational geometry called perspective.
Figure 8 shows a computer's rendering of a plane and its image (a sphere) after inversion.
Each star on the plane corresponds to its inverted image on the sphere; you can see how the angles are preserved under the transformation.
There are various ways of mapping a plane on to a sphere (or vice versa) but there is no way of doing this without producing some kind of distortion or another; Mercator's projection of the globe on to a flat map, leads to a diminutive Africa and an exaggerated Greenland.
Inversion has the advantage of preserving circles and angles, and hence proportion locally if not globally.
The upper pole of the sphere is the centre of the inversion and the increasingly small stars that approach it correspond to the increasingly large stars lying at greater and greater distances on the plane.
To produce this picture a computer program included a specification for both the position of the eye (the view-point) and the plane on to which the three-dimensional configuration is projected.
It also had to solve the problem of ‘hidden lines’: how do you tell the computer not to draw those lines that the eye would not be able to see?
Computer graphics can produce an image in a variety of ways, on a video screen, for example, or directly on to film.
For Figure 8, I used a plotter, a mechanical drawing board whose moving pen is controlled by the electronic signals coming from the computer.
Watching it in action is like seeing some fantastic automatic sewing machine, dropping and lifting its pen to draw and terminate the thousands of lines with a speed that belies its great accuracy.
Having created a plane pattern of tessellating hexagons and hexagrams (stars) and having programmed the computer to invert it, I watched the pen race round the board and wondered, would it draw anything sensible?
Would it draw a sphere?
I knew it must in theory, but to see it happen was still a lovely surprise.
From its beginnings more than a century ago, modern mathematics has aimed at a total abstraction in the interests of rigour and distillation of its logic.
One consequence of this has been a distrust of geometric intuition and the downgrading of geometry by educationalists over the past 50 years or so.
This demise of the oldest discipline in the world is clearly absurd and ripe for serious reconsideration.
To begin with, geometry has more value than simply as a means of acquiring a grasp of mathematical concepts.
The applications of a disciplined spatial intuition to art and design, and to the study of natural morphologies in every conceivable science is so great that perhaps we might think of geometry as a semi autonomous department of mathematics with different as well as overlapping purposes to abstract mathematics.
Nor did geometry end with Euclid.
The past 50 years have in fact proved a very active period with several important mathematicians putting the ancient house in order and opening new doors to entirely new possibilities.
The arrival of computer graphics will not only facilitate a wider access to geometry but will be, in an important sense, integral to doing geometry.
My final drawing, Figure 9, is a light-hearted look at a general idea concerning the concept of symmetry and its relation to transformation geometry.
The usual understanding of symmetry focuses on equality of size and shape between the repeated parts of a pattern or motif.
Geometers focus on something else; invariance of the whole configuration under the operation of a transformation.
You can rotate a swastika by a quarter turn about its centre and it will self-coincide.
You can reflect a butterfly (and most animals) about its mid-plane exchanging left for right without altering the form.
These transformations are both isometries, but geometers need no encouragement to generalise the principle to include any transformation.
If there is any transformation under which a form is invariant then the form is symmetrical.
Thus my twelve eggs have inversive symmetry.
Taking the circle that connects their points of contact as the circle of inversion, each egg self-inverts.
Radio waves spell trouble for industry
James Mannon and Eric Johnson
Stray radio transmissions can interfere with control equipment in industrial plant.
The effects range from the annoying to the catastrophic.
Yet the signs are that industry largely turns a blind eye to what appears to a growing problem
THE PEOPLE who run industrial operations are often haunted by what can happen when stray radio transmissions spark off electrical discharges in parts of their plants.
The radio waves may come not only from transmitters but power supplies, motors or other electrical devices.
The currents so produced can create havoc, even causing whole plants to shut down.
An example of radio-frequency interference or RFI — which is how the phenomenon is termed in industrial circles — occurred last year at the disabled nuclear reactor at Three Mile Island in Pennsylvania.
Workmen cleaning up one of the less contaminated rooms hastened out in alarm after their air meters showed they were breathing dangerously low levels of oxygen.
But later in the day, a detailed analysis showed the concentrations to be safe.
The monitors had responded falsely to RFI generated by walkie-talkie conversations between the labourers and their supervisors.
The problem is by no means limited to analytical instruments.
Circuits in computers and other control instrumentation can likewise act as unintended receivers, menacing the operation of whole manufacturing plants.
Even the steelwork in such plants can perform as an aerial, converting wave energy to sparks strong enough to ignite flammable gases.
‘We've heard some fairly horrific stories of what can happen,’ says John Bull, of the industrial electronics division at ERA Technology, a research company in Surrey, England.
In plants for the most part unprotected, RFI can create eerie havoc: valves can open inadvertently, controller set-points can wander, and alarms can go off seemingly inexplicably.
Bull cites one incident in which transmissions from a walkie-talkie caused a thermocouple to indicate — wrongly that part of a process plant supposed to be operating at 10°C had overheated to 200°C.
In another case that Bull cites, a similar incident caused the complete shutdown of a major American textile plant.
Just how acute the RFI threat has become for industry can be pieced together only from indirect evidence, as no statistics exist on the number of incidents.
Theoretically, RFI could cause a disaster at installations such as refineries and chemical plants.
So neither plant owners nor instrumentation makers are at all keen to report on RFI problems that could later be used against them in legal proceedings.
But Bull acknowledges that his firm, through its work in testing electrical equipment for susceptibility to RFI, has learnt about numerous incidents.
Indeed, these have become widespread enough to prod the International Electrotechnical Commission, of which Bull is a member, to draft international standards to safeguard industrial facilities against RFI (see box).
One of the few companies willing to talk about RFI is Foxbro, based in Massachusetts, which makes control equipment for petrochemical plants.
According to quality assurance manager Edgar Laderoute, at one time Foxbro received more than 20 reports per year about RFI mishaps concerning its equipment.
That was in the late 1970s, before the firm redesigned its hardware to make it less susceptible.
Laderoute says that industrial RFI has worsened in recent years.
One reason for this is the proliferation and strengthening of radio sources.
At many industrial sites, for example, workmen routinely carry portable radios, whereas a few years ago, only security guards had them.
Citizens band (CB) radio and ham-operated stations are also on the increase.
In little over a year since the British government gave its approval to CB, the Home Office has been inundated with six to eight thousand complaints of interference each month.
But another important reason for the worsening RFI problem is today's widespread use in industrial equipment of integrated circuits.
The equipment can perform a wide range of control functions, from simply turning a pump on and off to supervising a complete oil refinery.
Most of these control devices are built around microprocessors.
The circuits of these devices can rectify currents induced by RFI, especially at higher frequencies, into pulses confusingly similar to desired digital signals.
The impact of RFI on microprocessor-based control equipment can be far-reaching.
‘It [the microprocessor]doesn't just hiccup; it locks up completely and can shut down the whole plant,’ says John Bull.
James Mearns, an instrumentation specialist with Shell, says that engineers cannot always predict how a microprocessor will behave if stray transmissions interfere with it.
When the microprocessor shuts down, he explains, its final digital state is uncertain, unless additional microprocessor chips are provided to compare signals for accuracy.
To attain the high reliability achieved by conventional relay devices necessitates costly backup circuits, as well as elaborate RFI protection in the form of filters and screens.
To complicate matters, microprocessors can themselves generate interference.
Mearns reports that he has measured in excess of 1 volt of stray radio transmissions coming from a single microprocessor.
Shell became knowledgeable about the possible pitfalls of RFI when, five years ago, objections were raised over the company's plan to site a gas-separation plant near a NATO radio transmitter in St Fergus, Scotland.
The objectors said radio waves from the latter could cause explosions in the plant.
Only after an expensive study was it concluded that the steelwork used in the £240 million plant (jointly owned with Exxon and inaugurated in October) would not be susceptible to RFI.
Shell has recently extended its precautions against RFI to include strict tests of instrument susceptibility.
For example, a flame detector on a North Sea drilling platform might well be exposed to RF field intensities of 0–3 to 0–8 volts per metre, due to the platform's communications links.
Shell has specified that the detector will be deemed safe to use only if it can withstand intensities of up to 8 v/m.
Engineers have long wrestled with the problems of safe guarding equipment against RFI.
One tactic is simply to shield the device, by enclosing it in a metal or plastic box of low impedance.
Unfortunately, unless the device is skilfully earthed the shielding often has little effect.
In another technique, noise from power supplies can be filtered out by installing networks of inductors and capacitors to reduce trouble some frequencies.
On signal-bearing computer leads, twisted-wire coaxial cables curb some types of interference.
Another solution is to substitute fibre-optic signal links.
In these, messages are carried as flashes of light rather than electrical signals so there is no possibility of interference.
As an alternative to protecting the ‘victim’ device, the noise can be  suppressed at source, by screening it or attenuating its power.
But, often, engineers in charge of such devices have no control over the transmission equipment, whose owners may not be prepared to take the necessary precautions.
Ships are noisy places Ocean-going tankers and oil platforms are formidable places in terms of stray radio transmission.
Over the past five years, shipboard transmitters have boosted their average out put from 400 watts to 2 kilowatts, according to the British Ship Research Association.
Radar transmitters blast out up to 100 kW, with the result that many areas of the deck are exposed to field intensities greater than 30 W/m.
Thankfully, fields are less severe, and space less cramped, in most shore-based industrial plants.
But whereas suppliers of logic devices under defence contracts, such as Ferranti, are accustomed to meet demands that their equipment withstand paralysing fields of 40 to 100 volts/metre, many makers of standard industrial hardware are ill-equipped to cope with the hazards of far weaker fields.
Often, the manufacturers do not even know to what extent their equipment will stand up to RFI.
Michael Crabtree, a technical director with Marconi, notes that some microprocessor makers discourage the use of their products in potentially hazardous industrial applications, because they cannot control the RFI-immunity of the final product.
ERA Technology has warned its industrial clients of the adverse effects on logic devices at field intensities above 1 v/m, and on other refinery instrumentation at refineries above 3 v/m, particularly when the source is broadcasting radio waves at frequencies in the UHF or VHF bands.
Although the Geneva-based International Electrotechnical Commission is making progress on standards to safeguard against RFI, not everyone agrees that international bodies (or individual governments) should act to stamp out RFI.
‘It's like saying the government should advise people not to buy houses with leaky roofs,’ says Michael Marcus, of the R & D division of the Federal Communications Commission in the US.
‘They [industrial firms]ought to figure it out for themselves.’
Marcus contends that responsible manufacturers are already dealing with RFI.
He believes that manufacturers simply need to be aware of the electromagnetic environments surrounding their products just as they would be aware of temperatures.
But Marcus's confidence is not universally shared.
Neil Glenny is president of Confacs Corporation, a company in Phoenix, Arizona, that diagnoses and administers the industry's RFI complaints.
He says that the companies with which he deals are often like ‘the shoemaker's kid who wears shoes with holes’.
He concludes that ‘the people who should know most about the problem often don't recognise it’.
Engineers draw up guidelines to stop interference
INTERNATIONAL Electrotechnical Commission in Geneva is drafting a set of standards to safeguard against radio-frequency interference.
The guidelines, due for later this year, will assign electronic equipment into four categories depending on its susceptibility to RFI.
The classification will state that the hardware is unaffected by fields of either 1 volt/metre, 3 v/m, 10 v/m or an unspecified strength of field.
This information would be made available to consumers, who could then buy accordingly.
Edgar Laderoute of the American company Foxboro argues that this approach leaves the customer with a choice between buying equipment that is immune to RFI or suppressing offending radio waves at their source.
If the source cannot be located or suppressed, then at least its effect can be neutered.
The commission's  guidelines would also prescribe a standard way of testing hardware.
Electronic gear would be exposed to radio waves at frequencies between 27 and 500 MHz, depending on where it is to be installed.
The test would take place in specially shielded (anechoic) chambers.
Laderoute, who is a member of the committee drafting the standard, thinks that the guidelines will gain favour in most of the 40 or so nations that are members of the commission.
Foxboro itself have spent more than $6 million to protect the equipment that it sells against RFI.
Most of the cash went on redesigning logic devices, For instance, engineers installed circuits to filter out noise from mains supplies; twist-wire cables are specified for data  highways .
Other cable links to peripheral computer equipment are shielded or enclosed in magnetic toroids, The company also encases vital components in earthed metal housings.
To reduce outgoing noise from circuits, designers softened the sharp, nearly-square signals emitted by microprocessors used in Foxboro's equipment.
The signals can be rounded to give a less obtrusive signal.
This is done by deliberately putting delays into the circuitry.
The survival of the theory
Paul Harvey and Tim Clutton-Brock
Allometry, the change in the relative size of body organs as an animal grows, has been presented as a challenge to the idea that the most significant characteristics of an animal are shaped by natural selection.
But seek an adaptive reason why a  particular organ should be extra large or small and ye shall find
THE CRUCIAL ELEMENT in Charles Darwin's theory of evolution was not the general idea that living things did and do evolve, but that the course of evolution is determined principally by natural and sexual selection.
Those creatures that are best adapted to the environment (the ‘fittest’) survive and reproduce and so, too, do those that are adapted to compete for mates.
But those that are less well adapted do not.
Virtually all biologists now accept that evolution is a fact, and that natural and sexual selection are significant forces in it, but many have questioned the extent to which those forces shape the course of evolution.
The central question is whether animals possess important characteristics that are not adaptive, and which therefore, by definition, would not have been shaped by natural and sexual selection.
According to Sir Julian Huxley, writing in the 1930s, the huge antlers of large species of deer, which are certainly among their most conspicuous assets and absorb a great deal of energy as they grow, may be non-adaptive features; and Richard Lewontin, from Harvard University, recently applied Huxley's arguments to other features of other animals, including tooth size in Old World monkeys, and brain size in great apes.
If Huxley and Lewontin are right, then their examples indeed throw doubt upon the all-pervasiveness of natural and sexual selection.
Are they correct?
The point at issue is the significance of the phenomenon known as allometry .
Allometry is widely observed in nature, and has been defined by Stephen Jay Gould, of Harvard.
as ‘the study of proportion changes correlated with variation in size.’
What it means in practice, at least in the simplest cases, is that as an animal increases in overall size, various of its organs may not keep pace.
Some organs may grow faster than the rest of the body so that their size increases not only in absolute terms, but also relatively to the rest of the body, which is positive allometry .
or they may grow more slowly than the rest of the body, and so decrease in relative size, which is negative allometry .
The fiddler crab provides an excellent example of positive allometry.
In young male fiddler crabs the large claw is only about 2 per cent of the weight of the rest of the body, but among adults the large claw may eventually reach 70 per cent of the body weight (above).
The significant point, at least according to Huxley, is that the allometric variation that is so clearly observable between different sized individuals of the same species, can also be observed between animals of different size in different (though related) species.
Thus, to revert to the example of deer, small species such as the roe have antlers that are small not only in absolute terms, but are small also relative to the rest of the roe's body; but the antlers of big species, like red deer, are not only bigger than those of the roe but also bigger relative to the body size of their possessor.
Huxley argued, in his classic Problems of Relative Growth , published in 1932, that allometry was found between related species as well as within species because of a common growth mechanism; a red deer is, in a broad sense, like a roe deer that has gone on growing.
Of course he was not excluding totally the role of natural selection in determining antler size: it would be natural selection, after all, that primarily determined body size in the first place.
But Huxley did envisage that selection acted on genes that had more than one effect on the form (morphology) of an animal, so that although the large body size would be adaptive, the extra large antlers were simply a consequence of prolonged growth.
Thus, as Richard Lewontin put it, ‘it is then unnecessary to give a specifically adaptive reason for the extremely large antlers of deer’(Box 1).
It is dangerous, however, to argue that there is no reason for the large antlers just because it seems unnecessary to give a reason.
Stephen Jay Gould has argued that ‘there has been, in my opinion, mistaken emphasis on the non-adaptive nature of simple allometric trends’.
More specifically, we have found that there are adaptive reasons that seem to explain at least part of the variation that Lewontin cites in each of his examples.
The formal challenge to Lewontin and Huxley must be based on mathematical argument but the general biological points are as follows.
One of Jewontin's examples concerns the brain-size of gorillas and chimpanzees.
Gorillas are four times the size of chimps.
Their brains are larger in absolute terms than those of chimps but relative to body size, they are considerably smaller.
This, says Lewontin, is an example of negative allometry, and need not be considered adaptive.
But if you consider the order of primates as a whole, you find there is a significant relationship between relative brain size and diet.
Primates are divided into a number of families, including the lemurs, New World monkeys (spider monkeys, woolly monkeys.
marmosets, and so on), old World monkeys (such as baboons and rhesus monkeys), and apes (chimpanzees, gorillas, gibbons and orang-utan).
We have found that within those families, the genera that eat fruit have larger brains in proportion to their body size than those that eat leaves.
It may not be, as Lewontin claims, ‘useless to seek an adaptive reason for gorillas having a smaller brain than, say, chimpanzees’.
The diet of a gorilla consists of nearly 90 per cent foliage, while chimpanzees eat mainly fruit (Box 2) Dr John Eisenberg and Dr Don Wilson, working at the National Zoo at Washington DC have shown a similar relationship between diet and relative brain size in bats: fruit eaters such as flying foxes and fruit bats) have larger brains relative to their body size than their insectivorous relatives (such as horseshoe bats).
Among rodents, species such as gophers that eat leaves or grass have smaller brains than those such as squirrels, which eat fruit, seeds and nuts.
It is not precisely clear why there should be this relationship, but fruit, in general, is more difficult to find than leaves, and requires an animal to range further afield to acquire it.
If we regard the brain as a kind of three-  dimensional map of the environment, then those with the larger, more complex environment need larger maps.
Among primates again, large species like the baboon tend to have extra-large canine teeth, just as large deer have oversize antlers.
But primate canines and deer antlers are used as weapons, in conflicts between males.
Both characters indeed show allometry; but we have found that relative tooth size or relative antler size are related to the extent to which males have to fight for females.
The males of species that are polygynous (in which successful males may mate with many females during a single breeding season) have larger antlers or teeth than the males of similar size from monogamous species, which do not fight so hard for mates.
(see above).
Although the term ‘allometry’ has usually been reserved for morphological and physiological variables related to size, this restriction is unfortunate.
Behaviour is also size dependent.
For example, we have been able to show that the proportion of time that different species of primate spend feeding, resting, or moving about their home-range or territory, is correlated with body size.
These differences are probably adaptive and familiarity with the literature on allometry makes it easy to test specific hypotheses.
Thus, among a sample of small mammals, Brian McNab of the University of Florida showed a clear relationship between the size of the home-range, and body size.
In fact the relationship was negatively allometric; bigger animals had home-ranges that were bigger in absolute terms from those of smaller animals, but smaller relative to their body size.
The nature of the relationship suggested to McNab that the size of the home-range was determined by the animals' food requirements and the hypothesis was strengthened by the fact that, for their body sizes, specialist feeders had particularly large home-ranges (Box 3).
More recent studies have demonstrated similar relationships in other groups of vertebrates including lizards, birds, carnivores and primates.
In the mammalian order Carnivora, for example, the hunting species such as lions, wolves or weasels have larger home-ranges for their body size than the more omnivorous species such as most bears.
As data begin to accumulate on daily food requirements of different species, we can test more fine-grained hypotheses.
In all such studies it is important to take account of confounding variables.
For example, capuchins or howlers are New World monkeys from South America, while the lemus are confined to Madagascar.
Differences between species belonging to those families could result from differences in their habitat, but might also result from differences in, say, the structure of the placenta, which varies between the two families.
However, studies of allometric variation can be useful at several stages of scientific inquiry.
We have seen how the form of the relations between body size and the size of organs or of territory can be used to generate and test hypotheses.
In addition, animals that differ from established allometries alert the biologist to the need for further study.
For example, one of us (PH) recently examined data on the relative size of the testes among primates.
Roger Short, of the Medical Research Council's Reproduction Research Unit in Edinburgh, had predicted that in species where females mate with more than one male during a reproductive cycle, the males would have larger testes for their body size than in species whose females had only a single mate per cycle.
When the females were promiscuous, the sperm of each male would have to compete with those of other males, and the male producing the most sperm was most likely to generate offspring.
Short's prediction fitted the data beautifully, except for the proboscis monkey.
This species has small testes for its body size, even though the literature records that females associate with several males.
But more recent, detailed field studies have shown that the proboscis monkey is not an exception after all.
Females associate with only a single male during times when they are most likely to conceive.
Thus, to dismiss allometry as non-adaptive is, in many cases, incorrect.
On the contrary, allometry provides a powerful tool for investigating adaption.
1: Allometry — problems with interpretation
Organ size plotted against body size for a group of animals.
In a no account is taken of the animals' life styles.
The same data are plotted in b but the species are categorised into three different life styles denoted by different symbols.
The three dotted lines have different elevations and denote differences in relative organ size associated with changes in life style.
In a organ size is increasing roughly at the same rate as body size: the slope of the continuous line thus is about 1, which denotes isometry, rather than allometry.
In b the slope of each dotted line is less than unity — in each case denoting negative allometry.
The data are hypothetical and the axes are logarithmically transformed to produce straight lines instead of curves.
The line with the steep slope that appears on both graphs is the best fit line through all data points.
2.
Brains, teeth, and antlers
Relative organ size his been calculated as before.
The bar lines denote standard errors.
Animals with different life styles clearly have different relative organ sizes and, therefore, allometric relationships can be adjusted by natural selection.
Fruit eating primates have relatively larger brains than those that eat leaves.
Polygynous male primates have relatively larger teeth than monogamous male primates.
Large relative antler size in deer is associated with strongly polygynous breeding systems.
The degree of polygyny in deer breeding systems is derived from the sizes of female groups.
3.
Allometry in home range
Mean home-range size plotted against weight of the group that inhabits that home-range for different genera of primates.
O insectivores and fruit-eaters; O leaf-eaters.
The upper line shows the least-squares regression for insectivores and frugivores, the lower one that for folivores (from Clutton-Brock and Harvey 1977).
FORUM
Industrial springtime
Peter Marsh investigates the government's plan for a rich harvest
CONSERVATIVE ministers have been in charge at the Department of industry (DoI) for a little more than four years.
Yet observers of the department could easily be fooled into thinking that a party of radically different philosophy had taken over midway during this period.
This is a measure of the degree to which Patrick Jenkin, the Industry Secretary, has changed the stance of this part of the government following his appointment in September 1981.
In the words of one former senior civil servant, who has recently retired, the DoI ‘has become an interventionist department within a non-interventionist government’.
The previous Industry Secretary, Sir Keith Joseph, took power following the general election in May 1979.
He was committed to reducing the involvement of the state in industry.
In an interview in November 1980, he said that ‘successive governments have overspent, over-borrowed, over-taxed, over-regulated and over-rescued’.
Sir Keith made plain his dislike of the financial mechanisms with which governments in the West habitually support industrial companies.
‘It [the UK government]could only provide substantial extra resources to industry either by cutting public spending elsewhere or by raising taxes or by increased borrowing…
. The damage done to industry by any of these three methods would probably be more than the good done to it by the direct help and, anyway I am not clear on the sort of direct help that might be intended.’
Today, these words embarrass officials at the department.
While Sir Keith encouraged speculation that the DoI would eventually run out of jobs and be merged with the Department of Trade, Jenkin has found more for his civil servants to do.
Much of this involves spending money on science and technology.
The department's budget in this area has increased from £218 million in 1981–82 to £310 million this financial year.
The DoI runs a mass of schemes to support research in particular areas, ranging from robots to fibre optics.
Only last week, the department agreed to contribute £110 million to a five-year, £350 million programme on computers.
What is more, officials in the department have become more, not less, involved in the affairs of companies.
Civil servants sit as non-executive directors on some firms' boards.
Others are seconded to work in industry.
Government officials meet regularly with banks to advise them on lending to technology-based companies.
Likewise, people from industry come to the department to work for short periods.
For example, within the special unit in the DoI that coordinates its biotechnology activities are three people from ICI, Glaxo and BP.
Even more at odds with the ‘hands off’ policies of Sir Keith Joseph is the list of policy objectives that Jenkin drew up last autumn as a guide to help civil servants.
This contains no fewer than 1000 ‘operational targets’ concerning the ways that government should influence industry.
Such detailed planning would have provoked snorts of derision from Sir Keith.
Central to the department's new orientation is the role of Oscar Roith, the DoI's chief engineer and scientist.
Roith came to his job in June last year from a career in the oil and chemicals industry.
He was previously chief executive in charge of engineering with BP.
Roith spent his first nine months within the government listening to people.
He visited companies in areas of business he knew little about.
He went to the US, Japan and West Germany.
Roith also had intensive discussions with civil servants at the department's head quarters and with some of the 3000 staff at its six research establishments.
He says he was enormously impressed.
‘When you come into this department from the outside then you realise the wealth of information inside the government.
There is someone in the department who could give you a good, competent briefing on almost anything.’
Roith is determined to make science and technology central to the department's policy.
He has set up a science and technology management group within the DoI which coordinates the work of its different divisions.
Equally, he wants to stress that the government should listen to industry and respond to its needs.
Hence he attaches importance to spending more of the government's research cash in industry as opposed to within the government's own establishments.
Research hand-outs to firms should total £189 million this year, compared with £85 million in 1981–82.
Industrialists have complained of delays in getting applications for cash through the governments bureaucracy (New Scientist , 7 April, p 22).
Roith admits to problems in the past.
But following a shake up last year in internal procedures, he says that most applications for cash are dealt with in a couple of months at the most.
In the 11 months following the reorganisation, in which the two routes for obtaining research funds from the government were merged into one, the department has approved grants worth £100 million.
Officials within the department have complained, privately, that staff numbers are insufficient to handle the requests for aid.
These have grown as the department has increased the money available and introduced more support schemes.
Yet as a result of the government's determination to cut the number of civil servants, staff handling the applications cannot increase accordingly.
Roith dodges questions on the issue.
He says simply that his civil servants work extremely hard.
Roith is particularly proud of what is called the teaching company scheme.
In this, the DoI and the Science and Engineering Research Council finance groups of researchers who work in industry.
He says the scheme has proved extremely successful; through the scheme almost 200 researchers from academia are broadening their outlook by working on production engineering projects in factories.
Questioned on the exact role of government in stimulating technological innovation and industrial growth, Roith pulls out a sheet of paper entitled the ‘DoI's cocktail’.
This lists the jobs that the department should be doing to help companies.
The jobs range between running projects to make firms aware of new technologies to organising programmes for training.
Such lists would have been regarded as underground literature during Sir Keith Joseph's tenure.
The government, says Roith, should take some of the credit for the growth in computer literacy in Britain over the past half a decade.
The Labour administration started its microprocessor awareness programme in 1978, as an effort to tell people in industry how computers could produce greater efficiency.
The programme was continued both under Sir Keith Joseph (though he had some initial misgivings) and Patrick Jenkin.
‘If you had a Rip Van Winkle who went to sleep before the programme started and woke up now, he would have difficulty in recognising what had happened.’
Above all, says Roith, Britain should draw lessons from its successes in translating technical ideas into the world of business.
An example is in farming, where he says British researchers have produced important innovations that have been commercially lucrative.
It is an apt example for people in the government machine who, according to Roith, have a great part to play in sowing the seeds of industrial success.
Public face of artificial intelligence
Rory Johnston on the brain behind the computer
ABOUT FOUR YEARS ago the British popular media suddenly discovered the silicon chip.
Acres of newsprint and countless hours of air time became filled with elucidations on the new technology, some of them distinctly bizarre.
It was all too clear that the writers actually understood little about the subject matter, and that the understanding they succeeded in imparting to the public was correspondingly less.
Since then things have improved markedly and it is rare now to see such howlers as the Wirral Globe's explanation of a 16K RAM as a chip with ‘up to 16 000 different uses’.
Still, when it comes to the treatment of the frontiers of computing, the areas of advanced research that come under the general heading of ‘artificial intelligence’(AI), there is still a great deal sadly lacking.
Hardly a month goes by without one newspaper or another announcing the development of a computer ‘with which at last you can converse in ordinary English, not a programming language’.
One such system was described recently by the Observer as ‘extrasensory’.
As it happens, in the early days of computers it was thought that it would be only a few years before computers would be able to understand natural language.
The truth is that it is a much more difficult task than that, not least because we don't understand as yet how people comprehend language.
For this reason, computers are programmed in various subsets of English known as computer languages.
Some of these languages are rigid and stylised: others are more sophisticated, allowing considerable variations in the syntax of an incoming sentence.
Nevertheless, they all work basically by trying to fit the input into a set of stored templates, and if some input does not fit anything the machine has been given, it simply replies ‘I do not understand.’
Yet the public is regularly given the impression that a general solution to the problem of natural-language understanding has been found, and that computers with the ability to chat to us and with various other human characteristics are just around the corner.
While the power of technology in such activities as conversing is exaggerated, the really important and exciting discoveries that are being made in Al, speculation aside, are not got across.
The issues of Al are fascinating and of enormous social importance, but they are largely over looked in the wallow of ‘gee-wizz’ about things that are of no significance.
Laurie John's recent BBC2 Horizon programme on Al.
called Better Mind the Computer .
is a case in point.
It was admirably researched, hut it glossed over the important questions while pointing up the trivial ones.
It kept confusing computer developments that are genuinely intelligent with those that are entirely mundane.
It mixed the profound ideas of some of the best brains in the world, conceptions of major importance to us and our technology today, with absurd speculations about an unforeseeable future.
The result in viewers' minds can have been little other than confusion and fear, instead of the excitement that there should have been.
This is not to belittle the producer's problem.
All journalists face a continual struggle between the need to present a balanced view of the issues and the need to attract and to stimulate an audience.
There is an ever-present temptation to take the easy way out and concentrate on items that are ‘sexy’ even though they may be irrelevant or misleading.
An example which writers and producers are forever citing, and John was no exception, is Joe Weizenbaum's conversational program ELIZA, which parodies a psychotherapist.
Although this seems clever when one is told about it, a few minutes-hands-on experience demonstrates that it is not AI at all— it is a parlour trick.
In practice it makes mistakes all the time but Horizon showed its English as flawless.
While it was stated that ELIZA was looking for key phrases in the ‘patient's’ input, its illusory importance was re-affirmed by repeating the computer's dialogue at the end of the program.
Donald Michie's idea of the ‘human window’, the vital importance of making machines comprehensible to human beings, only got a passing reference, while pride of place was given to Ed Fredkin's musing about computers of the future which will think about ‘weighty problems we simply can't understand’.
A real science is now growing up around the concept of the human window, while there is no evidence or logical reason to accept Fredkin's supposition at all.
Horizon made the usual mistake of talking about computers that can ‘correct spelling errors’, when in fact all they can do is flag words that are not in a dictionary.
It discussed computers writing stories, with out asking the fundamental question, could computer-produced stories ever be of any value?
There has to be human purpose behind art, and how could a computer ever have that?
How about the view that in order for a computer to produce a real work of art, it would have to want to produce that art?
So how can a computer want?
The significance of computers in art is only one of a fascinating range of issues, technical and philosophical, that Al raises.
What are the real limitations of technology, at present and in the foreseeable future?
How is it possible for a computer to discover things?
How can we give computers common sense?
How can computers create things that are genuinely new and not just ‘what you put in’?
How can we build technology that is fundamentally human-oriented?
On all these topics there have recently been tangible results, not speculation, coming out of laboratories in both the US and Britain.
The public have had some sideways glances at them — they now deserve to see the full picture.
Satellite Insurance
THE US's space shuttle is getting preferential treatment in the cut-and-thrust world of satellite insurance.
Underwriters in the insurance markets in London and New York are making no bones about being more willing to insure craft launched with the American orbiter than with Europe's Ariane rocket, which has suffered a number of teething problems.
The cost of building and launching a satellite can run to around $100 million.
Not surprisingly, the people in charge of such bits of hardware want to insure against anything going wrong during the risky business of putting the vehicles into orbit.
The normal premium is about 10 per cent of the total sum insured.
For launches with the space shuttle, however, underwriters are concluding deals with satellite operators where the premium is set at just 5 per cent.
To insure against complete catastrophe on an average launch would thus cost about $4 million.
The European Space Agency (ESA), developers of Ariane, is not getting such a good deal.
For satellites launched with this vehicle, which has failed on two launches out of five, the premium is set around 12 per cent.
So ESA is having to cough up rather more than it bargained for to insure the European communications satellite that, if all goes to plan, Ariane will place in orbit on its next launch in June.
Insurers are more nervous about Ariane because if something goes wrong the whole rocket is likely to blow up, destroying everything on board.
With the space shuttle, the astronauts in control have a fair chance of correcting any faults, possibly rescuing from oblivion any spacecraft in the cargo bay.
Of course, the US has by no means a perfect record.
Although the shuttle has returned safely to Earth on each of its six nights so far, the orbiter managed on its most recent mission to nudge the tracking and data-relay satellite it was carrying into the wrong position.
Engineers are still trying to steer the craft into the correct path.
But it is good news for the insurance fraternity: the US government, which owns the satellite, did not bother to take out insurance on it.
Desert island risk
James Serpell on a new bounty that could cause a Mutiny
HENDERSON ISLAND — a tiny British territory on the outskirts of eastern Polynesia — is one of the last remaining undisturbed islands in the South Pacific.
Thanks to its remote situation and forbidding terrain, this 12 square miles of raised coral has never been successfully colonised, either by people or other introduced species.
Instead, it supports at least 55 species of native flowering plant (10 of which occur nowhere else); four endemic landbirds, including a large flightless rail (Porzana atra ), a diminutive nectar-feeding parrot (Vini stepheni ), a fruit pigeon (Ptilinopus purpuratus insularis ) and a warbler (Conopoderas vaughni taiti ); various endemic snails, crustaceans and insects, and a colony of nine different species of breeding seabird.
Conservationists are therefore understandably alarmed by news that this remarkable community of organisms may soon be joined by Arthur M. Ratliff Jr, a tobacco-chewing, hillbilly millionaire from the backwoods of Virginia.
According to a recent profile in the Washington Post .
‘Smiley’ Ratliff's extravagant ambitions should not be taken lightly.
Although he describes himself as a ‘a damn uneducated mountain fella’, he managed to convert a 1500 dollar bank loan into a 100 million dollar fortune in less than 20 years.
He now owns five Rolls-Royces. a 20 000 acre hone and cattle ranch (the largest in Virginia), and a white-brick colonial mansion with twin gun-towers.
Politically, Ratliff stands somewhere to the right of Robert E. Lee.
He believes that the United States has been destroyed by morally subversive elements (such as Sigmund Freud, Franklin Delano Roosevelt and, of course, Elvis Presley), and he also claims that the government ‘ruint’ him by interfering in his coal-mining business.
To cut a long story short, such is Smiley's loathing for government that he has decided to establish his own private nation on Henderson Island, The Foreign and Commonwealth Office (FCO) has neither accepted nor refused Smiley's request For permission to build himself a family-sized mansion and an airstrip on Henderson.
The FCO is waiting, it says, for Ratliff to clarify his plans before it comes to a decision.
Meanwhile, representatives of the International Union for the Conservation of Nature, the World Wildlife Fund and the International Council for Bird Preservation have been quick to oppose the whole scheme which, they argue, would irreparably damage the island's fragile ecology.
Unfortunately, it is not simply the future of Henderson Island which is at stake.
In return for permission to reside on Henderson, Smiley Ratliff has promised to provide the inhabitants of nearby Pitcairn Island with the use of his proposed airstrip, a launch to ferry them the 90 or so miles between Pitcairn and Henderson.
and an unconditional gift of one million dollars.
Since becoming the legendary home of the Bounty mutineers, Pitcairn Island has had its ups and downs, But never before has the future of this remote British colony looked quite so bleak.
Twenty-five years ago there were 144 people living there, mostly descendants of the original mutineers and their Tahitian wives.
Ships called every fortnight, providing the islanders with medical and dental care, and commercial and travel opportunities.
Today, only 44 inhabitants remain.
Conditions on the island have deteriorated, and their only contact with the outside world is via an antiquated radio link with the British Consulate in New Zealand and (weather permitting) two passing ships a year.
In other words the Pitcairners are desperately in need of the kind of aid which Ratliff has to offer.
Although far and few, the Pitcairn islanders are not without supporters.
In Britain their cause has been championed by Glynn Christian — writer, broadcaster, TV gourmet and great-great-great-great grandson of Fletcher Christian who led the famous mutiny in 1798.
As chairman of the Pitcairn Island Fund, he believes that £2 million would be sufficient to buy Pitcairn a reasonably healthy, self-sufficient future, and he has spent the past two years trying, unsuccessfully, to persuade the British government to make some contribution.
Needless to say, Christian is delighted by Smiley Ratliff's flamboyant venture.
Not because he particularly approves of the exploitation of Henderson Island, but because he hopes that this latest development will force the government to come to some decision about the future of Pitcairn.
He may well have a point.
If the government now rejects Ratliff's offer, but fails to compensate the Pitcairn islanders, it will certainly be accused of callously placing the survival of animals and plants before that of people.
If, on the other hand, it accepts Ratliff's scheme, and in the process allows the destruction of Henderson Island, this will inevitably be seen by conservationists as a sell out — a deliberate evasion of our colonial responsibilities for the dwindling community on Pitcairn.
Fortunately.
there is a third option.
For roughly £2 million (little more than the cost of one Falkland islander) the British government can preserve both Henderson, Pitcairn and its own reputation for colonial fair play.
Early conference organiser
John Little commemorates the 200th anniversary of Dr John Lee
THURSDAY 28 April marked the bicentenary of one of the great Victorian patrons of science, John Lee.
Born John Fiott, he changed his name to Lee in 1815 in compliance with his uncle's will.
With this and a later inheritance he was able to indulge his passion for science by organising what might be considered the forerunners of today's scientific conferences.
Thomas Sopwith FRS describes what one of the meetings was like: ‘In the month of September 1856 it was my good fortune to receive an invitation from Dr John Lee.
LL.D., to form one of a company of scientific visitors who were to meet at his residence at Hartwell Park, near to Aylesbury.
It was the fancy of the good Doctor to bring together, from time to time, a considerable number of men whose lives were devoted to the advancement of science, and to entertain them, not for a day merely, but for several days, so that they might discuss together, without hurry or excitement, those matters of practical and theoretical science which were at the moment engaging the attention of the scientific world.
‘I remember that my invitation extended to 14 days, but it was so arranged that any visitor who might have to leave for the day could do so and could return again.
Carriages met every train in order to bring the visitors to the mansion, and carriages were despatched to every train with those who were leaving the mansion.
In short, everything was made as free and homely as was possible.
‘During the visit in September 1856, to which I refer, as many as from 30 to 40 visitors were brought into communion with each other, establishing acquaintances and friendships of lifelong duration.
We were representatives of so many branches of sciences that we used to speak of ourselves, in a jocular way, as a British Association, in miniature, for the amusements of science.
However, we did in some degree resemble the real British Association, by meeting every morning, under the presidency of Dr Lee, in the library of Hartwell House.
‘One of the company was called upon often without a word of preparation — to treat on a subject with which he was  presumed to be familiar, and so to express himself that what he said could be discussed afterwards.
These conditions, difficult to sustain, led occasionally to a great deal of embarrassment, mixed always with a compensationary dose of fun and good humour, and sometimes followed by the communication of useful information which was none the less pleasant because of the piquancy incident to a little merriment and unexpected light of knowledge.’
In his youth Lee had read mathematics at Cambridge before taking his doctorate in law.
He eventually became Queen's Counsel and magistrate despite ‘an eccentric manner’ and beliefs that included opposition to drink and tobacco and advocacy of votes for women (the latter provoking ‘considerable amusement’during his four unsuccessful attempts to become MP for Aylesbury).
He was a man of strong, though unconventional, religious beliefs which his interest in archaeology and astronomy partly fuelled.
A founder member of the Royal Astronomical Society (RAS), he donated the livings of Hartwell and Stone — of which he was Lord of the Manor — to the society with a view to the promotion of astronomy in connection with theology (he also founded the Lee Fund for the relief of widows and children of deceased fellows of the RAS).
Lee's chief claim to scientific consideration, apart from his patronage, was the construction of an observatory in the grounds of Hartwell House.
The observations made there by Lee, his friends and assistants were published at his own expense in several volumes.
He was treasurer of the RAS from 1831 to 1840 and became president a few years before his death in 1866.
An exhibition to mark Lee's bicentenary will be held by the Buckinghamshire Records Office at the Aylesbury County Museum later this year.
It will be opened by Patrick Moore on 7 October and will run to the end of the month.
Carry on protesting for Soviet refusniks!
Sarah White on academic boycotts and human rights in the USSR
ACTION by US scientists may have saved Academician Andrei Sakharov's life when he was exiled to Gorky in 1980 — according to an anonymous letter recently circulated in Moscow.
The letter is said to be written by friends of Sakharov, who have to remain anonymous to protect both themselves and their families.
It claims that the decision of the US National Academy of Sciences in the spring of 1980 to suspend all bilateral symposia, seminars and workshops involving the Soviet Academy, shows how actions often speak louder than words with the mammoth Soviet bureaucracy.
‘It forced the KGB to change their tune from ‘the traitor Sakharov deteriorated as a scientist long ago’ to ‘Academician Sakharov has every  opportunity for scientific work in Gorky’…
It may have saved Sakharov's life.’
Sakharov's friends warn the West, however, against any complacency, in particular after the success of Sakharov's hunger strike in 1981.
Then the Soviet authorities bowed to pressure and allowed the fiancée of Sakharov's stepson to join him in the United States.
‘The decision to meet Sakharov's demand and save his life was taken at the highest level, evidently at a meeting of the politbureau.
The volume of support for Sakharov broke through to the Moscow centre where power is wielded over the world from lofty isolation, where ‘they’ make their decisions.
But it could not last.
The bureaucratic swamp soon recovered its grip.
On 24 December.
1981, just ten days after he ended his hunger strike, despite a coronary infarct, Sakharov was evicted from the hospital and returned to his Gagarin Street apartment which is kept under surveillance and frequently searched.
Since that time Sakharov has been deprived of the medical care which he requires, Sakharov's friends wrote about this to Anatoly Alexandrov, president of the Academy of Sciences, in January 1982, but they received no reply.
When Sakharov suffered last summer from thrombophlebitis, a serious condition, he was treated only by his wife.’
Since then, Sakharov's health has continued to be poor and his friends are scathing about the role of Soviet scientists — in his case the Academy of Sciences: ‘A few words about the USSR Academy of Sciences and Sakharov's scientific contacts.
During private conversations with foreign visitors, president Alexandrov, vice-president Velikhov and scientific secretary Scriabin worked hard to prove to foreign colleagues that the ‘Sakharov problem’ does not exist, that Sakharov is in fine shape in Gorky.
Academician Scriabin has even claimed that Sakharov has a private secretary!
The KGB wants to prevent possible protests and to involve the academy's leadership in this game.
Visits to Sakharov by his colleagues from Moscow's Lebedev Physics Institute are cited as evidence of his ‘welfare’.
The first such trips in the spring of 1980 represented a victory over the KGB.
(This breakthrough coincided with the boycott announced by the US National Academy of Sciences.)
But these visits had no effect on the other conditions of Sakharov's Gorky exile.’
The authors warn visiting foreign scientists that their presence in the USSR will be used by the KGB to show that Sakharov is forgotten or that his exile is at least accepted.
Protests have to be made public to be effective.
The dissident movement in the Soviet Union has been under heavy attack over the past few years.
Members of the Moscow Helsinki Watch Committee and the Working Commission to Investigate the use of Psychiatry for Political Purposes have been silenced through imprisonment or exile.
Now a sinister trend is emerging with the rearrest of some of those whose prison sentences are drawing to an end.
Vyaches lav Bakhmin, a member of the working commission, was due to be released from a camp in the Tomsk region on 12 February.
Instead he was transferred to a prison in Tomsk for investigation.
At the beginning of March he was brought to trial and sentenced to a further one year and one month in a labour camp.
The procurator protested at the leniency of the sentence.
Sergei Kovalyov, biophysicist and co-founder with Sakharov of the Initiative Group for the Defence of Human Rights in the Soviet Union, was sentenced in 1974 to seven years in a labour camp followed by three years of internal exile.
He is at present in exile in Magadan and due to be released in December 1984.
However, Kovalyov was recently  summoned to the local District Department of Internal Affairs and told that it had ‘new material’ against him.
Jewish  refuseniks are also facing a new and possibly sinister clampdown.
There have been increasing reports of people who have applied to emigrate receiving final rejections — 43 instances are known.
This means that these 43 people have lost any right to apply for an exit visa, whereas normally Jews waiting to leave keep on applying hoping that eventually the visa will be granted.
The numbers being allowed to leave the USSR have fallen drastically.
In 1982, 2692 persons left on Israeli visas, compared with 9447 in 1981.
It certainly seems that the change in personalities at the top of the Soviet hierarchy has not resulted in any easing of the repressive policy against dissidents.
From sealing wax and Shoestrings
Tam Dalyell visits the Rutherford-Appleton Laboratory
AN OLD CAMBRIDGE toast is, ‘Here's to pure mathematics — may she never be of any use to anyone!’
In similar vein, Ernest Rutherford opined that his work on radioactive substances and their radiations would be of little or no practical use.
He died in 1937, 29 years after receiving his Nobel Prize and before witnessing anything that might have changed his opinion.
In 1947 Edward Appleton received a Nobel Prize for work on electro-magnetic waves in the upper atmosphere.
The Science and Engineering Research Council's (SERC) Rutherford-Appleton Laboratory.
between Didcot and Wantage, perpetuates their names.
Here is carried out the most theoretical physics of any in Britain.
The laboratory's Spallation Neutron Source (SNS) will provide facilities for several hundred British and other scientists to use neutrons as a probe for studies in physics, chemistry, and biology.
The capital cost of the SNS and its associated instruments will be about £20 million.
Appleton was proud of having done much of his own research on a shoestring.
He would have approved of the extensive use on the site of existing equipment and buildings.
When 1 commented to the hosts of the Parliamentary and Scientific Committee (under whose auspices 1 went) on the use of steel for guarding the rings, I was told that the full, true installation (’ green field’) cost would have been near to £50 million.
Short, high-intensity bursts of neutrons will be produced 50 times a second by accelerating protons by potentials of up to 800 million volts, and steering them to hit a target of heavy material — uranium.
For each proton, 30 neutrons are produced.
They will have to be slowed down by moderators to the energy required, and then guided to the instruments in which the scientists will study their materials.
About 20 such instruments will be supplied with neutrons simultaneously.
Interest in all this is being displayed not only by the United States, Germany, France and Italy, but also by India and China, whose students, I was told, made a worthwhile contribution to the work of the Rutherford-Appleton.
My colleagues and I were particularly interested in the Central Laser facility, whose staff do research of their own, but also ‘run the shop’ for some 36 groups in 26 university departments, involved in the programme.
High-powered lasers are capable of heating matter to temperatures around 10 million degrees.
Multi-beam lasers, we were told, are capable of compressing hot gases to densities greater than those of solids at room temperature.
The neodymium glass laser, nicknamed Vulcan, provides pulses of intense laser light to two target laboratories.
Vulcan is probably the most powerful laser in Europe, and may be the most versatile high-power laser in the world.
At lower powers, the Vulcan operates a range of advanced ultraviolet laser systems for research in photo-chemistry, atomic physics and semi-conductor processing.
Its potential advantage to British industry seemed to me to be colossal.
Yet the Rutherford-Appleton laboratory cannot meet more than one third of the demands on it from universities, sometimes acting in coordination with firms such as GEC.
Why?
Because there is no money to pay for the extra care and maintenance staff, janitors, and others, who would be required to make greater use of the facilities at night or at week-ends.
Going on until 10 pm two or three nights a week is all that can be managed.
Our hosts did not complain.
They have a high regard for SERC.
My own assessment is that as a country we are being daft about candle ends.
In a nation with more than 3 million unemployed, many skilled youngsters would give their proverbial eyes and ears to work at the Rutherford-Appleton.
It is absurd not to give them the chance, and help British industry and world science.
For example, when we saw the Sprite 200 J KrF laser, we were told that no one else in the world had a prototype of this quality for future laser fusion work.
Is it to be the old, old British story, of our doing the earliest work, and other nations taking advantage of our brains?
The immediate political problem for Whitehall and Westminster is what should be done when the value of the pound sterling falls internationally and our subscription to CERN and other organisations goes up accordingly.
It is wrong that other SERC competitors for money should not get it because international high-energy physics is.
Yet we cannot refuse to cough up our subscriptions.
The French and the Italians seem to get round this problem by handing such matters to their Foreign Offices to deal with.
For the sake of British science, we should do the same.
Otherwise we put the continuity of vital work in jeopardy by failing to shelter it from the winds of international currency exchanges.
The conditions of international science are so different from the days of Rutherford, and Appleton with their sealing wax and shoe strings.
Governments ought to acknowledge that.
Rutherford once quipped ‘we haven't the money, so we've got to think’.
As a young scientist, Appleton did not need vast capital equipment to discover the phenomenon of ionised gas molecules in the ionosphere —‘the Appleton Layer’.
Rutherford's earliest research at the Cavendish was on a detector for electromagnetic waves, an essential feature of which was a small coil containing a tiny bundle of  magnetised iron wire.
The requirements of modern particle and laser physics are totally different.
A return to Victorian values of thrift will not meet them.
LETTERS
Cropspray tests
We refer to D. R. Goldsmith's letter on ‘Human cropspray tests’(Letters, 10 March p 677).
The publication by Rao et al referred to by D. R. Goldsmith in his letter, is based on reports by Ciba-Geigy of India Ltd.
These were submitted to the Central Insecticides Board, Faridabad, upon the authority's request.
The Insecticides Act of 1968 requires aerial spraying measurements under practical conditions to demonstrate safety for human beings, cattle and fowl.
This is a necessary condition for obtaining clearance.
The requirement has been officially confirmed and specified in a Request to the Registration Committee 1978.
It is pertinent that WHO has been sponsoring comparable field trials in African countries to confirm the adequacy of safety margins in case of inadvertent exposure to pest control agents.
In the particular case in question, to meet the authority's request, exposure measurements on Nuvacron 40 were conducted in June and October 1975, under the supervision of government officials.
The purpose of these measurements was to ascertain through conscientious planning and medical surveillance that inadvertent exposure during spraying does not create any hazard to a rural population.
There were no adverse health effects resulting from the exposure.
Teenagers were allowed in the project as junior members of the regular rural work force.
Under the given conditions, changes of cholinesterase activity would have been demonstrable long before reaching a danger level, and would have provided an early warning.
While test animals are used to observe toxic reactions, the group of farm workers in this project helped to demonstrate the absence of a reaction to unintentional exposure.
Results from these kind of measurements provide a basis for the design of monitoring programmes and recommendations for the safe handling of pest control agents under practical use conditions.
They establish safety without endangering the subjects, and thus should not be considered unethical.
Multilingual micros
You state that ‘machines made by Apple are the only microcomputers that can understand PASCAL (Technology, 14 April, p 79).
In fact, PASCAL is available on almost every microcomputer.
Indeed, it is an important language because of its wide availability on both microcomputers and mainframes.
It has been standardised by the International Standards Organisation, and the National Physical Laboratory has produced a set of test programmes for checking compliance with the ISO standard.
The Quality Assurance Division of the British Standards Institution will soon be offering a service using the NPL's test facilities, to validate PASCAL compilers.
Real aid
Tam Dalyell's views on the Brandt Commission's second report (Forum, 7 April, p 40) are perhaps just another addition to the literature on the subject.
Much has been said on North South dialogues; some sense and some nonsense.
Much more has been written; nothing gets done.
Apparently industrialised nations and their politicians torture themselves into believing that if Third World countries are turned into amphitheatres of food and agricultural research, lustily cheered on and abetted by Western ‘gurus’, everything will be hunky-dory.
Not so easy.
The South is talking not of aid and advice, but of a new economic order, with just and equal sharing in trade, finance and technology.
Partisan statement
I am a long-standing admirer of John McComnick's writing, but his review of Reagan and the American Environment by Friends of the Earth (21 April, p 176) alarmed me.
McCormick correctly points out that the authors are partisan, and acknowledges that the book contains some truth about the Reagan Administration's assault on the environment.
But he goes on to say: ‘To take (the book) as the last word would be as unwise as putting one's faith in a hand-out from the (US) Department of the Interior.’
To perceive Friends of the Earth and the current US Department of the Interior as similar types of institution, with similar methods of public relations and similar sets of motives, and whose statements both have similar orders of truth value (or lack of truth-value) indicates a moral and intellectual short-sightedness.
All the key people that Reagan has appointed to protect the US environment have had close financial involvement with companies that are polluting and destroying the environment.
Many of them were lawyers employed by those companies to find loopholes in environmental protection laws.
The Environmental Protection Agency has been experiencing a mini-Watergate scandal for the past year, with no less than six Congressional committees investigating very serious charges against the top officers involving misuse of public funds and conflict of interest.
No doubt some contemporaries of the journalists who uncovered Watergate accused them of partisan bias and lack of balance — but I doubt that any of these commentators would have drawn an analogy between this investigative reporting and the repeated and transparently mendacious press statements of the Nixon Administration.
Glorious mud
Bernard Dixon neglects to mention the beneficial side of geophagia (Forum, 31 March p 906).
For instance a colleague of mine returning from a remote part of Kenya commented on the habit among pregnant women of eating a particular sort of yellowish mud.
The practice, which I gather is fairly widespread in Africa, appears to be effective in warding off anaemia, presumably due to a high iron concentration in the mud.
It puts me in mind of Marco Polo's observations on the health promoting qualities of the soil found around the tomb of St Thomas in South India.
Christian pilgrims always made sure to take a sample home with them, and then ‘when anyone falls sick of a quartan or tertian ague or some such fever, they give him a little of this earth to drink.
And no sooner has he drunk than he is cured.
The remedy never fails, and you should know that this earth is of a red colour.’
Iron at work again, or something more mystical?
Another colleague, a soil scientist, labelled the two above soils yellow and red podzols, and helpfully added that white clays with a high kaolin content are also very good against stomach and intestinal disorders.
I suspect that many more instances of the pharmaceutical use of mud, glorious mud will be known to your readers.
The subject clearly merits serious study, if only because there ought to he money in it.
Promiscuous pithecanthropus
How fortunate that dear Ruby was discovered by Hansom and Luckey and not by ignorant persons, such as those dreadful Russians who were discovered feeding mammoth steaks to their dogs.
We must all rejoice in her successful regeneration (14 April, p 81).
Just two points — the first is that I can't imagine what Ruby's guardians are thinking of to let her brave the London traffic on a moped.
Surely one of the press lords could rustle up a limousine for her use?
The second point is this, could Ruby be indulging in a leg pull of her own?
A mate for keeps three million years ago?
Surely not!
I remain, firmly in defence of happily promiscuous hominids.
Big whopper
I feel that such hybrids as those made at the University of Hamburg (Monitor, 31 March, p888) are hardly innovative.
We have been involved in research of a very similar nature for some time, and although we are not publishing in such august journals as The Phyrologist you may be interested to learn of our most recent advance.
Success in fusion of cellular material from B. stearo thermophilus and Canis familiaris metris optimi has resulted in our ability to culture, again in an isotonic monosodium glutamate medium, tubular elongated fruiting bodies.
The technique is of course patented and the trade mark ‘Hot-Dog’ is being sought.
Fast food
Congratulations on the addition of a humour section to the New Scientist .
Your selection of the MacDonald and Wimpey story from The Phyrologist , ‘Scientists make first plant-animal hybrid’(Monitor, 31 March, p 888) was a delight.
One might have thought the work more appropriately done on the Isle of Sandwich.
What remains For MacDonald and Wimpey, of course, is patenting their process and cloning their hybrid for the purpose of franchising.
Undoubtedly this is but the first of competing efforts to develop ever more tempting animal/plant combinations.
Commiserations to all readers who were taken in by our April fool.
But you were not alone!
We had one anxious telephone call from a Swedish newspaperman, who had run the story in his weekly science column, and was much perturbed when two professors challenged him to prove that the story was true.
Monitor editor
De-railed
Your tribute to Derailleur, who as you reminded readers (This week, 21 April, p 135)‘invented his [bicycle gear]system in the 1920s’ was as welcome as it was belated, But Derailleur is only one of the many forgotten pioneers of road transport.
Some of course achieved recognition in other fields.
But they were not the greatest: the mechanical acoustic early warning device of Alexander Graham Bell (his first essay in telecommunications) is rarely heard nowadays.
Likewise, Jean Monod's inertial approach, for all its Gallic elegance, has failed to displace the old non-inertial guidance system of Handel and Barr — now quite forgotten.
Monod, by his collaboration with Coque, at least left a name in the automotive industry, but who today ever gives a thought to Le Chassis, without whose basic contribution to the theory of tetracyclic separation that industry would never have got off the ground?
Do give us more.
AIDS strikes at all I feel it necessary to respond to Nancy Heneson's ‘Monkey Business in AIDS Research’(Forum, 17 March 1983, p 747).
Among other things, she has indicated that the NIH, ‘…creates markets to insure its survival…
. All the ingredients needed to revivify an ailing research enterprise were on hand: a hefty portion of anxiety over an enigmatic public health problem, seasoned with a dash of desperation over coming cuts in the budgets of NIH's seven primate research centres…
Could it be that the bureaucrats of primate research saw a way to parley an admission of poor husbandry into a public campaign for the primate centres?
Opportunism is not limited to microbes…
More meetings on the order of this…can only hurt the cause of good biomedical research.’
To these points I must agree that it appears that the NIH has most reprehensibly sought to exploit the acquired immunodeficiency syndrome (AIDS) crisis to its own ends.
Unfortunately, I find Heneson's presentation virtually as abusive, trivialising the AIDS crisis while evidencing a serious lack of good taste.
Moreover, while Ms Heneson does mention in passing the other categories of AIDS victims — Haitian immigrants, IV drug users and transfused  haemophiliacs — she refers to AIDS as the ‘latest manifestation of Sodom and Gomorrah,’ which betrays a serious ignorance of current Biblical scholarship, and she emphasises ‘sexually hyperactive, urban gay men’.
Let it be said that committed, monogamous gays and non-gays have also been victims.
Ms Heneson's generalisations distort the issue.
While reported AIDS cases continue to double every six months, approaching epidemic proportions, and while research funds are urgently needed for real 
AIDS research, neither exploitation nor homophobic and insensitive reporting can help the increasing number of AIDS victims.
Killer weeds
Michael cross reports that Kenya is to ban dangerous pesticides (This week, 17 March, p 705).
The article correctly points out that British-American Tobacco Company no longer recommends the use of aldrin in Kenya.
But I should like to make it clear that the suggestion that BAT has been ‘dumping’ this pesticide in Kenya has not foundation of truth whatsoever.
Agricultural development in Kenya is of course a national priority and BAT's subsidiary, BAT Kenya, is making a significant contribution.
The company helps the farmers to grow better tobaccos and also gives advice and help in the important task of producing better food crops.
ARIADNE
HIGH jinks in the Manpower Services Commission, I reckoned for one gleeful moment.
Looking for an exposé in the Sunday papers, I was, though I could not quite understand why the MSC was being so open about its possibly scandalous activities.
Disappointment followed, the lurid projector of mental pictures shut down and I was left feeling I ought to have known better.
My scurrilous expectations were fired up by a headline on a handout from the Commission.
It said, ‘Time for punitive education’.
At once, all those postcards outside newsagents filed past the internal eye.
Blondes offered strict discipline, French model was willing to dominate.
But it was Dr George Tolley, director of the MSC's Open Tech Unit, whatever that is, who was having a go about professional people not keeping up with developments in their professions.
As far as I know, ‘punitive’ means inflicting or involving punishment.
What Dr Tolley was advocating was withdrawing membership of professional bodies from people who did not keep abreast.
1 should have thought that that was the punishment, not the education, though God knows there are plenty around who think of education that way.
‘VERY cut glass’ was the phrase in my youth for hinting that somebody or something was a quality product.
I can see why.
Cut glass was what is now called crystal, though it is nothing of the kind, and it has advantages apart from being elegant.
Used as I am, of course, to drinking from glittering crystal, I admit that for everyday use it is more likely to be the ordinary glass you buy from Woolies or Habitat — sensible, cheap, and reasonably designed.
But it will not stand up to detergent.
Only weeks after first use, the glasses I am drinking from are etched into cloudy streaks.
As a volunteer washer-up remarked ‘if they can get to the Moon, why can't they make an etch-proof glass?’
SEARCH is a journal of science in Australia and New Zealand, its cover pronounces.
I had not seen it before taking a look at the February/March issue.
It contains a  riveting academic argument about the value of a CSIRO research programme on dung beetles, whether or not the native Australian dung beetle is efficient at removing dung and in the process eating the larvae of the buffalo fly, a scourge of cattle.
What stopped me in my tracks was a sentence reading, ‘The dung beetle program really cannot be justified on the grounds that it is cheaper than a horse’.
There is no space to go into detail about the ding-dong and the relevance of that remark, but it is gentlemanly mayhem at its best and I wish I had seen the first encounters between the CSIRO and its critic, who, I infer, also had a go at the research on skeleton weed.
One depressing surprise, though, was afforded me by a report on neighbourhood shopping centres.
Many of us have the idea of Australians, above all, being people who call a spade a spade.
But town planning and sociology in Australia evidently produce the same junk-words as they do elsewhere.
‘There was no change in the number of establishments devoted to the sale of comparison goods’, for example.
Bought any comparison goods lately?
GNASHING teeth make a sort of sliding, scraping noise inside the head, I find.
I was gnashing mine at some news from France, received just after another example of the present service from British Telecom.
Needing the number of a firm in Guildford (Candy Domestic Equipment) I had waited for five minutes for the inquiry people to answer, been cut short when I attempted to give the address and then handed out two minutes of silence.
Finally I was given the number, but not the code for Guildford, connection being broken before I could ask for it.
I rang the number and got a Mrs. Candy.
Almost immediately I ran across the news that the French telephone monopoly, PTT, is well on the way with a scheme for putting all the phone numbers in France into a computer that a subscriber will be able to communicate with from a terminal that is connected to his instrument.
The terminal will not cost him anything.
It is called Minitel and the French government had already ordered 600 000 of the things for a couple of years of an experimental run.
By 1990, 1 understand, everyone in France with a phone will be able to call up a display of any number he wants, in just 15 seconds.
That seems to me one way of dealing with inquiries about telephone numbers.
Another is the American way, which is to be fast, polite and cheerful.
The last time 1 wanted a telephone number in the US, I dialled information in New York for a number in Washington, for which I had no address.
After a minute or so of explanation and apology, the voice the other end said.
‘Oh, I've got the number.
I just wanted to keep you talking.
I love that English accent’
So it seems that both state monopoly and private enterprise are capable of running a telephone system in which the customers are a prime consideration.
What is it about the British that makes them confuse service with servility
DAEDALUS once invented an everlasting car tyre.
It was inflated with liquid rubber monomer under its own vapour pressure.
As fast as rubber wore from the tread, fresh rubber formed on the inside, while the whole tyre slowly expanded to keep the same size.
Now Daedalus plans to make such tyres propel the car.
Many solid surfaces catalyse the oxidation of hydrocarbons.
So, says Daedalus, a road surfaced with catalytic material should oxidise the surface of his tyres.
There would be a fierce ignition, but in the zone of contact with the road, a thin smear of rubber would be combusted in the small amount of oxygen adsorbed from the air onto the  surfaces .
As the tyre rolled, each point on it would enter the contact zone.
Gas-pressure from the reaction would develop steadily over the brief time of contact, and would be greatest at the rear of the zone where the tread was about to lift off.
This maximum pressure, constantly pushing the rear of the contact-zone off the road, must propel the tyre.
Daedalus's catalytic cars will need fuel, delivered into their tyres, but not engines.
A push from a simple starter will set them rolling, after which their tractive tyres will accelerate them automatically.
The driver will control his speed via an adjustable governor, up to a limit set by the local road surface.
In built-up areas, for example, special slow-reacting catalytic surfaces will automatically enforce the speed limit.
Only slowly revolving tyres will contact the road for long enough to generate sufficient drive.
Non-catalysed areas (pavements etc), will be immune from automotive invasion.
Any car that does enter such a forbidden zone will lose all power, and remain until the traffic-warden arrives.