

Computerised flights of fancy
AFTER the razzmatazz of Information Technology year comes the sober assessment of how Britain can remain a force in this discipline.
Over the coming weeks the government will make up its mind on a programme of computing research that could cost £350 million.
This R&D effort was proposed by a group of electronics experts, headed by John Alvey, technology director at British Telecom.
More important than the cash itself, which would be spent over five years, with the taxpayer providing two thirds, is the suggestion that within the Department of Industry (DoI) there should be a small commercially-minded unit to run the project.
The unit would supervise research contracts in academia and industry — but especially in the latter, thus breaking important new ground in the way Britain conducts its R&D.
As the House of Lords Select Committee on Science and Technology points out this week (p 634), the amount of money spent by government on R&D in industry is ‘pitifully low’.
Officials at the Treasury are now examining Alvey's proposals.
Their counterparts at the Department of Education and Science.
DoI and Ministry of Defence support the project.
But any government hesitation in approving the programme may have been reinforced by comments last week by Frank Land, professor of systems analysis at the London School of Economics.
Land suggested that Alvey's group did not consult enough of the people who would use advanced computers; that it had paid too little attention to international collaboration; and that the committee focused on too narrow an area of activity.
The proposals, according to Land, add up to‘what is probably the best possible programme for building an information-technology Concorde’.
There is a shred of substance in these claims.
In particular, the programme, as it is set out, perhaps puts rather too much emphasis on ‘knowledge-based systems’.
These are computers containing sets of rules that approximate to those people use during the thinking process.
The other areas earmarked by Alvey's group are: software engineering; ways of making it easier for people to communicate with computers; and methods of cramming more electronic components onto a chip.
Land says that programmes should include work in telecommunications.
It is difficult to disagree with him for the conjunction of computers and communications is perhaps the most important aspect of information technology.
Perhaps too, Land is right in suggesting that computer users should have been brought more into the argument.
Japan's plan to build ‘fifth-generation’ computers, which started the British debate, was formulated only after discussions with people representing many sectors of society who have nothing directly to do with computers.
But Land's central thesis — that there may simply not be much of a market for the products from the work recommended by Alvey — does not stand up.
Whereas very few people want or need to travel across the Atlantic in Concorde, hundreds of millions of people use computers or related equipment.
The numbers have grown from next to nothing just 30 years ago.
It is sometimes easy to deride work in artificial intelligence — and some researchers are their own worst enemies when it comes to persuading people to back their efforts — but there is every sign that advances in this discipline will play a large part in the kind of machines that tomorrow's computer users will buy.
Certainly we must hope that the government in its final response will take the arguments put forward by Alvey somewhat further; after all it has had the best part of six months to consider them.
There can be no doubt that if Britain wants to look forward to a level of economic health even approaching that to which it has grown accustomed over the past 50 years, it must back Alvey's proposals.
Interferon busybodies
THE only surprise in the history of interferon is its meteoric rise to fame as a cancer cure, after its earlier promise as an anti-viral agent.
It is certainly no surprise to learn that it is not a cure for all cancers.
As a recent review points out, in the early days researchers were hampered because they could not obtain sufficient quantities of pure interferon, and in any case there is no such thing as ‘interferon’— there are at least three families of interferon (British Medical Journal , 5 March, p 739).
It is thanks mostly to the press and a handful of biological entrepreneurs that this ‘wonder drug’ has been elevated to the status of a modern-day penicillin.
For the media, interferon was a glamorous new arrival on the pharmaceutical scene.
Not only could it cure cancer, but it was also ‘natural’, and not one of those chemical horrors dreamed up by juggling with molecules.
Add to that the fact that the new techniques of genetic engineering were invoked in the manufacture of interferon and you almost have the makings of a script for a soap opera.
Now, it seems, interferon is less than perfect.
It does not immediately cure all known cancers — something that no one with a modicum of medical knowledge would have expected in the first place given the different causes and the versatility of cancer — nor is it without undesirable side effects.
As yet the only people to suffer from this setback are the investors.
For interferon has been dangled as a bait before investors with more money than scientific sense.
Many millions of dollars have flooded into new biotechnology companies in pursuit of interferon and other potential moneyspinners.
So far the new companies have produced few products beyond the glossy financial and other potential moneywealth, one day.
They may be right, but it will certainly be a long hard road for interferon at least.
THIS WEEK
Battle rages over fusion contracts
THE FIASCO that has surrounded Europe's Super-SARA experiment into nuclear safety — a project that has cost £100 million but is now to be abandoned part-built — is spilling over into another arena of big-budget research, nuclear fusion.
An international row is brewing among scientists who say that the EEC's ISPRA establishment in Italy is trying to make up for its loss of Super-SARA by stealing other plum research contracts for which it is ill-equipped from specialist fusion laboratories throughout West Europe.
One principal casualty could be Britain's chances of building the prestigious successor to the JET fusion project, based at Culham in Oxfordshire.
At the centre of the new storm is a document prepared by the European Commission.
It will be discussed by research ministers at their meeting today (Thursday).
The document sets out a strategy for Europe's nuclear research after the expensive and  embarrassing demise of Super-SARA (New Scientist , 27 January, p 215 and 10 February, p 354).
ISPRA, the EEC's main nuclear research establishment, is now certain to be a ‘favoured competitor’ in an orgy of bidding for some £300 million worth of EEC-funded research into fusion technology that is now getting under way.
The first round of tenders from European fusion laboratories is now in.
And some scientists are angry that ISPRA seems certain to grab a large slice of the action, at the expense of the other laboratories.
The pressure to find work for the 2260 staff at ISPRA is intense because of a commitment by research ministers to keep on all of ISPRA's nuclear scientists — even though their biggest project, Super-SARA, has been abandoned.
But many scientists spoken to by New Scientist believe that ISPRA is ill-equipped for the work.
One fumed: ‘We never in our wildest dreams associated this work with the capabilities of ISPRA’.
Now it threatens to become the cuckoo in the nest of European Fusion research.
Nuclear fusion is becoming an ever-more important aspect of the EEC's research programme.
It seems set to grab a large part of the 15 per cent gap in the EEC's joint research programme left by the demise of Super-SARA.
Among the Commission's proposals outlined to ministers this week are:
•
More cash for the technological preparation for NET, the Next European Torus that will follow on from JET, the Joint European Torus which should be switched on in May.
•
More work on fusion-safety in preparation for giving ISPRA a key role in licensing future fusion reactors in Europe.
•
The Commission has also lined up ISPRA for a major study into the handling of tritium, the radioactive isotope of hydrogen.
•
It also suggests that, in the long term, ISPRA might be turned into Europe's premier location for big-science machines.
•
And there is a speculative project to investigate the  Ignitor -fusion concept.
This idea is for a much smaller fusion reactor of the Tokamak design, but with a much higher magnetic field.
But at the back of everyone's mind is the coming battle for the site of the biggest project on the horizon, NET.
Everyone wants to avoid the chaos that surrounded its predecessor JET.
The JET project was delayed for several years in the 1970s, while the council of ministers vacillated between giving the project to ISPRA, Garching in West Germany and Culham.
Already the signs for Culham winning NET do not look good.
The Commission says that, if the Community decides to build the NET machine, it could be expected to be built on a Community site.
That probably means the ISPRA facility.
But, after the fiasco of Super-SARA, the British are not alone in asking whether the cumbersome  bureaucracy and politicking of European Community big-science, is providing value for money.
Line-up for Europe's space-boss
A LEADING West German scientist is the front runner for the post of coordinating Western Europe's efforts in space, when the job becomes vacant next year.
Up for grabs is the post of director-general of the European Space Agency, the 11-nation ‘club’ that spends about £400 million annually.
The issue will be resolved at a meeting of the agency's council next month.
Only two serious candidates have emerged to replace Erik Quistgard, a Dane.
Britain has proposed Sir James Hamilton, a career civil servant who is an under-secretary at the Department of Education and Science.
West Germany has put forward Reimar Lust.
He is a distinguished astrophysicist with a lifelong interest in space.
Moreover, he commands respect as the head of a body that coordinates some 50 German research institutes.
By contrast, Sir James is relatively little-known in international circles.
If he gets the job, Lust can be expected to follow German policy and steer the agency towards scientific and technical research rather than commercial projects, such as the development of communications satellites.
West Germany believes that the agency should leave commercial projects to individual nations.
Complicating the whole issue Quistgaard, who joined the agency in 1980 from a career in industry, but is reluctant to step down.
He could stand for a second four-year term.
But other nations are likely to opt for a change; as long as four vote against him, Quistgaard must go.
Peter Marsh
The trade winds die…and the Queen gets a soaking
WHAT LINKS the storms that soaked the Queen in California last week: the bush fires that scorched Australia last month, famine in Indonesia, a state of emergency in Ecuador and the collapse of the Peruvian anchovy fisheries?
The answer, according to climatologists, is the return of the El Nino.
The El Nino is a massive surge of warm water that, once every decade or so, builds up in the eastern Pacific along the South American seaboard.
It happens when the southeast trade winds that blow for thousands of kilometres across the equatorial Pacific unaccountably lose their force or, as happened last summer, give out altogether.
Together these events disrupt the climate and sea currents across wide areas of the Pacific, causing havoc to animal, fish, and bird life as well as to humans.
Some climatologists say that this year's El Nino outbreak could be the worst for 100 years.
El Nino (’ the child’, because it first becomes apparent around Christmas) is a warm ocean current that recurs every seven to 14 years.
It sweeps southward from the Pacific coast of Central America, diverting the cold, nutrient-rich Humboldt Current away from the South American mainland, or sometimes forcing it as much as 50 m beneath the waves.
In any event, the fish (especially the anchovies) either die or become unavailable to man and wildlife.
El Nino seems to result from a reduction in strength of the prevailing southeast trade winds that sweep across the Pacific.
As these winds fail the mass of ocean water that is piled up in the far western Pacific (where the sea level is several centimetres higher than in the east) comes flooding back.
Whatever the exact cause, the effects are clear.
In the Galapagos Islands water temperatures reach 30°C (86°F)— five degrees higher than usual.
Sealions have departed for cooler waters and in some places seals lie dying on the rocks in their fur coats.
Blue-footed boobies have virtually given up nesting.
Enormous waves batter the archipelago's western shores, destroying beaches.
Ornithologists on Tower Island report hat, at the end of January, more rain fell in two-week period than in the whole of the previous six years.
The heavy rains have brought luxuriant green vegetation to these normally arid islands.
Land birds and reptiles, like giant tortoises and land iguanas, are having a field day.
Butterflies, moths, mosquitos and horseflies, are all abundant.
On the mainland huge areas of coastal Ecuador are under water, with food and medical supplies ferried in by air.
Heavy rains have swept northern Peru, and in
Lima, where it rains so seldom that the city's roofs are not designed to withstand more than a mild shower, the inhabitants are anxiously awaiting the worst.
Farther south, the millions of seabirds that normally nourish on the famous guano islands are being decimated.
Hardest hit is the guanay cormorant, the most numerous species.
Perhaps most important from the human point of view, the Peruvian fishermen face disaster as the anchovies disappear.
The effects of the El Nino extend far from the coast of South America, however.
As the trade winds give out, atmospheric pressure over Australia and Indonesia rises bringing dry weather, bush fires and, in the case of Indonesia, famine (New Scientist , 9 December 1982, p 628).
This wider effect is known as the Southern Oscillation.
The northern hemisphere suffers too.
The seas around California are today warmer than at any time since 1957 and there are record high tides.
According to Dr Eugene Rasmusson, of the Climate Analysis Center in Silver Springs, Maryland, very warm sea-surface temperatures in the eastern Pacific are also associated with a deepening of the semipermanent low pressure in the Gulf of Alaska.
This typically brings storms to California.
So the weather conditions disrupting the Queen's visit could have been predicted.
One final conundrum concerns why the El Nino chose this winter to make its return — the first since 1972.
There is intense speculation that the massive cloud of dust and debris that shot into the upper atmosphere last April when the Mexican volcano, the El Chichon erupted may be to blame.
This volcanic cloud could have warmed the upper atmosphere in the tropical latitudes and blotted out the differences in temperature between surface and upper air that when combined with the effects of the earth's rotation, cause the trade winds.
But it could be a coincidence.
Manchester trams are set for a return trip
MANCHESTER is considering  re-introducing trams — 34 years after it scrapped them.
The plan is the result of a joint study of the future of the city's railways by British Rail and the Greater Manchester Council.
Some of the lines urgently need modernising.
The Bury line for example has an archaic four-rail electrification system.
The study identifies three options.
One is to run half a dozen of the city's suburban railways as a limit rapid-transit system, with light-weight vehicles, described as a cross between a tram and a light train.
In the city centre these vehicles would run on the streets.
This option would cost £85 million, or if the trains were put in tunnels through the city centre, £105 million.
A conventional railway service, including a link between the Piccadilly and Victoria stations (a plan which has been dearly cherished by local politicians for many years) would cost £156 million.
The study is also considering ripping up all the railway lines and converting them to busways.
This it estimated would cost between £120 million and £225 million because it would need major civil engineering works.
Greater Manchester's transport chairman, Andy Fender said that the cost of these schemes would be compared with the ‘many tens of millions’ it would cost to replace the existing rolling stock.
British Rail is to install modern electric power lines on a  railway line which it described as ‘totally unsuitable’ for modernisation two years ago.
Then, BR told a public inquiry that it would cost £5 million to convert the line, which runs from Manchester east to Hadfield and Glossop.
Now it puts the bill at £700 000.
And that sum will be paid for out of the scrap value of the old thick copper cable used in the original 1950s electrification.
The line was once part of a longer route that ran to Sheffield, using the Woodhead tunnel under the Pennines.
This main line was shut two years ago.
BR's claim that it might cost £33 million to convert to a modern power system was a crucial element in the decision to close the tunnel.
The Glossop to Manchester run was only kept open to carry a passenger service subsidised by the county council.
The line was first electrified in the 1950s, but at 1500 volts DC it is very different from today's standard of 25000 volts AC.
Mick Hamer
EPA purges its best scientists
NOTHING, not even science, is sacred any more at the embattled US Environmental Protection Agency.
Now the object of a handful of inquiries into influence-peddling and the mismanagement of taxpayers' money, the EPA has at least been able to claim that its science is pure.
No longer.
An incriminating document, apparently pilfered from EPA's files, casts the agency's attitude toward science in a new light.
Ninety independent scientists who advise the agency part-time are listed, along with pithy recommendations on what to do with them.
Matthew Meselson, a leading toxicologist from Harvard University, is described as ‘poison’.
Lester Lave, an acknowledged master of cancer risk assessment, earns the commendation: ‘Deepsix him’.
Others of varying eminence in the scientific community win eulogies such as‘bleeding heart liberals’; ‘very definitely out’; ‘invidious environmental extremists’; or simply ‘get him out’.
EPA officials say they don't know who prepared what's come to be known as the EPA's hit-list.
As for the list's members, who serve terms at the pleasure of the President, many have not been asked back.
With budget cuts slicing the agency's own scientific staff to the bone, that leaves the agency with plenty of bureaucrats, but fewer experts than ever.
Hunt for the last wolf in Norway
WHAT MAY be the only wolf in Norway now has a £2000 price on its head.
The Norwegian fish and game directorate, based in Trondheim, has authorised a five-man team to shoot the wolf, which is accused of killing 140 ewes and lambs in the hill pastures of Vegarshei, 160 km southwest of Oslo.
Wolves have been protected in Norway since 1973.
But last month, hunting by special licence was approved where too many livestock have been killed — provided that the population of wolves remained viable.
Opponents of the change were assured that permits would be issued only if wolf numbers suddenly improved.
Now, the directorate's chief, Helge Vikan, claims wolves are increasing, that they total 18 animals, and that a viable, breeding population may already exist near Vegarshei.
Figures from his own department, however, do not support these assertions.
Scientists have only managed to find at most seven wolves, and there is no evidence that pups were born in 1982.
Wolves have not bred in neighbouring Sweden since 1978.
Norwegian conservationists are furious that the directorate, which is reluctant to fund studies of wolves, is authorising the hunting of the endangered species on the basis of so little information.
The directorate, they say, has incited the popular press to  fantasise about vast packs of wolves ravaging Norway's countryside.
They also suspect that the move against wolves is part of a wider campaign against predators.
Plans are under way to license the killing of wolverines and eagles.
Meanwhile, the conservationists have until 14 March to persuade Norway's environment minister, Mrs Wenche Frogn-Sellag, to commute the sentence on Vegarshei's lone wolf.
Congress seeks science teachers
THE US Congress last week made its pitch for a massive increase in spending on maths and science education.
The House of Representatives passed a $425 million bill that would increase tenfold the spending proposed by President Reagan.
Though Reagan and Congress may disagree over the amount of money that government should spend, alarm over the present state of US science education is general.
The worry in Washington is that the economy and national defence are suffering because young Americans have neither the technical skills of the Japanese nor the linguistic and scientific proficiency of Russian students.
Recent surveys by the National Science Teachers Association (NSTA) show an 80 per cent decline over the past ten years in the number of maths and science teachers turned out by colleges and universities.
The result, according to the NSTA's Bill Aldridge, is that fifty per cent of new science and maths teachers in elementary and secondary schools are unqualified in those subjects.
Reagan's original proposal allowed $50 million dollars in grants for teacher retraining.
The House Bill sets aside $250 million in 1984 to retain teachers and improve schools' equipment and services — particularly access to computers.
Another $20 million goes in scholarships for students and non-science teachers who agree to train as science teachers.
An aide to representative Don Fuqua, a leading sponsor of the bill, said he expects the more conservative Senate to offer a $250 to $300 million programme.
But President Reagan could still veto my science-education bill that appears too expensive for his taste and priorities.
Nancy Heneson, Washington DC
Boffins battalion
THE FRENCH have called in the troops to speed up the spread of computer learning.
Instead of square-bashing around their Berlin  barracks , 500 graduates will do their military service teaching the young jobless how to use computers.
The first battalion of boffins will swing into action this August.
By the end of next year, the government hopes 10 000 national servicemen will be mobilized.
The aim is to reduce the ranks of the young unemployed who now account for more than half the nation's two million jobless and to give them an added skill which will help them find work when the training is over.
Each teacher should be able to train 40 pupils in a year.
If the targeted 10 000 is achieved, France will have 400 000 more youngsters a year with computer skills.
One problem the French planners have still to solve is the role of female graduates, some of whom have requested to take part in the new scheme.
There is as yet no national service women.
Another pain-killer withdrawn
ZOMAX, a new pain-killing drug given to a million people in the UK over the last two years is being withdrawn by the manufacturers, pending changes in the prescribing information issued to doctors.
A severe allergic reaction to the drug has killed five Americans.
The American manufacturers, Johnson and Johnson, ad asking American doctors to stop prescribing the drug.
Ortho-Cilag Pharmaceuticals, which markets Zomax in the UK, has followed suit.
The manufacturers plan to emphasize their earlier warning to doctors that the drug should not be given to patients allergic to aspirin or other non-steroid anti  inflamatory inflammatory drugs.
Two of the deaths occurred in patients known to be sensitive to aspirin.
Zomax, the trade name for zomepirac sodium, is one of some 20 similar drugs widely used to treat arthritis.
No deaths linked to Zomax have been found in Britain.
But severe side-effects have been reported.
Agent orange was safe
FRESH evidence that there is not a link to toxic chemical sprays, such as Agent Orange, and  increased birth defects in veterans' children has been provided in a study by the Australian Commonwealth Institute of Health.
It examined the records of 8517 deformed children born at 34 hospitals between 1966 and 1979.
Each child born with a defect was matched as closely as possible to a healthy infant born in the same hospital, at the same time.
The study found that 127 of the deformed infants were fathered by Vietnam Veterans while 123 of the healthy children were also fathered by veterans.
This, the study concluded, indicated that there was no evidence that army service in Vietnam had increased the risk of birth defects in veterans' children.
Members of the Vietnam Veterans Association have rejected the findings and called for a Royal Commission into the issue.
Choosy over Chooz
THE BELGIAN government has put off a decision on whether to stump up cash for the nuclear plant being built by France at Chooz — a finger of France sucking into Belgian territory.
The project is part of a two-way deal, in which France would have a chunk of a Belgium reactor, Doel 5.
The arrangement is the brainchild of Etienne Knopps, the Belgian energy minister.
But his environment colleague, Finnin Aerts, wants a moratorium on further nuclear plants.
Aerts is backed by miners in Limburg, the Flemish part of the country.
They say that the French cannot be trusted to maintain safety at a plant which, though on French soil, is surrounded by Belgium.
Lords call for engineering universities elite
THE BRITISH government should increase its support for industrial R&D by between two and three times to keep the country abreast with its competitors.
But Whitehall should contract out as much as possible of this research to industry itself rather than giving it to government establishments.
These are the main conclusions of the House of Lords' select committee on science and technology in a report this week (Engineering research and development , HMSO, £6. 10).
The committee also calls for stronger links between industry and universities.
It says the Department of Industry (DoI) and Department of Education and Science should earmark at least six universities as bodies especially suitable for doing industrial R&D.
The cash would be channelled from the government to the universities via a commercial company so that in the words of the report, ‘industrial commitment and market considerations are guaranteed’.
According to the committee, the government should be more imaginative in supporting research.
Under existing grants schemes, the DoI contributes between 33 and 50 per cent of the cost of individual research projects.
But the government should put up 90 or even 100 per cent of the cash in cases of long-term R&D that are by nature speculative.
‘When DoI grant schemes require detailed proof that applications for grants are soundly based, they prejudge the conclusions to which the R&D is directed and this can be self-defeating.’
Like many similar inquiries in recent years into Britain's research spending, the report points out that the UK is unusual among many Western nations in the way it allocates its R&D.
Of the total research budget, the government contributes about half (the rest coming from private companies).
But of the government's share, about half goes on defence activities.
Other large proportions go to space and civil aviation, leaving little left over for industrial R&D.
According to the report, the British government spent £85 million on industrial R&D in 1980.
The comparable figures for research connected with defence, space and civil aviation were £1343 million, £52 million and £69 million.
In government supported industrial R&D, Britain laps behind other European nations (see chart).
The committee takes the view that government research projects carried out by private industry have a better chance of leading to commercial rewards in terms of new products and processes.
Yet it says that the amount of government research cash spent by industry is ‘pitifully low’.
In the area of civilian (non-defence) R&D funded by Whitehall, only 16 per cent is spent in industry and over half of this concerns aerospace.
When it comes to defence research, the picture is very different: 64 per cent of this slice of government R&D is carried out in industry.
Even here, however, the outlook is none too rosy, according to the committee.
The government's military-related research work is done by a small group of generally large firms which form a closed community.
Therefore ‘spin-offs’ into the economy as a whole from advances in military R&D, in telecommunications or electronics from are few and far between.
The committee believes this is ‘disruptive to the UK's competitive position’.
One proposal is a series of tax incentives that would encourage defence contractors to share the benefits of military R&D with firms connected with civilian technologies.
The report points out the dominance of Britain's big firms in the area of industrial R&D.
In 1978, companies with 10000 or more employees performed 79 per cent of the total.
But useful product developments — undertaken outside conventional R&D units — often take place in small firms.
A study at the Science policy Research Unit at the University of Sussex found that 37 per cent of innovations in mechanical engineering between 1969 and 1980 were made in firms with fewer than 1000 employees.
One language for government computers
THE British Government is taking the first tentative steps to standardise the computers and computer equipment that are bought by civil servants.
The latest move is the introduction of a scheme to test whether computers can handle a standard version of Cobol, a language used by business.
This scheme follows the publication last year of 32 other standards, mostly to do with methods of storing and capturing data.
Government suppliers do not have to follow these standards, which are international ones rather than the government's own.
But those that do will get preference, other things being equal.
The Central Computer and Telecommunications Authority (CCTA), which advises government departments on information  technology , says there are no plans to set compulsory standards.
But a French report on the ‘economic impact or the absence of standard in informatics’ estimated that savings of up to 30 per cent could be made by standardising.
The drawback is the cost of insisting that everyone goes for the same type of equipment.
A survey conducted in 1977 by the US federal government concluded that it costs as much to change from one computer to another as to buy the machine itself.
Cloud find shrouded in mystery
LAST WEEK'S announcement that a gigantic cloud of dark gas has been found in space is leading to speculation about an ‘invisible mass’ hidden within it.
Astronomers from Cornell University in the United States identified the cloud of neutral hydrogen in January, using a radio antenna in Puerto Rico.
‘Nothing like this has ever been seen before,’ says Dr Yervant Terzian, chairman of the university's astronomy department and head of the research team.
It is the first example of such a cloud found outside a galaxy.
‘It is as large as a galaxy itself and may be the original material of a potential star system that never really formed,’ says Terzian.
The cold neutral hydrogen, radiating energy at its characteristic wavelength of 21 centimetres, shows the cloud to be about 30 million light years away from the earth.
It is 300 000 light years long and 100 million times more massive than our Sun.
The extremities of the cloud are rotating so fast that the cloud would disperse if it were not held together by some force of gravity.
‘We must assume there is some invisible mass, approximately 100 times larger than the mass of the gas cloud itself,’ says Terzian.
Scientists agree this is an exciting prospect to pursue.
Harvard radio-astronomer, Dr John Ball, says:—‘An invisible galaxy is reasonable but it is unreasonable that no mass can be seen.’
The discovery was made almost by accident.
The Cornell team was searching for neutral hydrogen inside other known galaxies.
‘When we moved the telescope off the galaxy to what we expected to be blank sky,’ Terzian explained, ‘we got a signal instead.
We thought it was interference but when it continued we mapped the entire area.’
US takes pot luck on cancer risks
GIVE a rat formaldehyde to breathe and it will get cancer.
Three years ago enough laboratory rats developed malignant tumours from the chemical to convince a US consumer protection agency to ban urea formaldehyde foam insulation in homes.
But when Ronald Reagan came to Washington a year later, another government department, the Environmental Protection Agency (EPA), looked at the same evidence and decided not to limit most other, potentially higher human exposures to the chemical.
The fortunes of formaldehyde are an example of how cold, hard science can turn to putty in the hands of public officials.
As one of the scientists who vetoed foam insulation told New Scientist .
‘The differences (of opinion) come in the superimposition of the political process, and as a result, there appears to be significant scientific disagreement when there really is not’.
What might otherwise be a dry debate on science policy has raised a hue and cry among the American public, for the issue is which of the 70000 chemicals on the market may cause cancer.
In the US, there is no single set of rules to judge carcinogenicity.
The agency that approves new pesticides, EPA, uses a different set from the agency that regulates household goods, the Consumer Product Safety Commission.
Chaos sometimes results, as in the  regulation schizophrenia over formaldehyde.
Recently, an expert panel of the august National Academy of Sciences (NAS) tried to weave some consistency into cancer risk assessment.
The panel found that government gets into trouble when scientists get intimate with those who regulate.
‘It puzzles the public and the scientific community’, notes one panel member, Dr Morton Corn of Johns Hopkins School for Hygiene and Public Health, ‘that you can hire your own gun nowadays and get the results that you want’.
The NAS panel would like to see a single set of rules for drawing scientific ‘inferences’, or conclusions, from frequently confusing and inconclusive data.
For example, a massive dose of the pesticide permethrin (a suspected carcinogen recently used on vegetable crops) of say, 5000 parts per million, causes a 60 per cent incidence of liver tumours in mice.
What mathematical model should be used to determine the safe level for bean-eating humans, who would never eat anything near 5000 ppm at one sitting?
Or how important are liver tumours, which often appear spontaneously in certain types of lab mouse?
The NAS's panel wants an independent, non-governmental Board on Risk Assessment Methods to answer these questions with some sort of rulebook for cancer.
But attempts at writing rules of thumb are already underway at the White House, and the task has proven a lightning rod for complaints that politics are corrupting science.
Last year, health scientists at government agencies tried to put together a paper outlining the state-of-the art of identifying possible carcinogenic substances.
Entitled ‘Potential human carcinogens: methods for identification and characterisation’, it was promptly kicked back through the door by several well-respected cancer specialists who were asked to review it.
Critics view the paper, and similar drafts at the EPA, as back-door attempts to ‘fuzz up’ the science to justify relaxed rules for industry.
John Todhunter, who runs EPA's pesticide programme denies any hidden agenda for a new view of cancer risk.
He admits, however, that attitudes under Reagan have undergone a ‘continuing evolution’.
Some scientists claim the White House's cancer paper mirrors views toward carcinogenesis that chemical companies have pushed unsuccessfully, for years.
For example, the report appears to distinguish between ‘genotoxic’ substances, which are mutagenic and directly damage DNA, and ‘non-genotoxic’or ‘epigenetic’substances, which apparently act indirectly on the genome.
The distinction would give the latter types freer rein, since they are not ‘complete’ carcinogens and their effects are not as patently threatening.
Last year, the head of EPA's Carcinogen Assessment Group, Dr Roy Albert, proposed a new risk assessment system for water pollutants that revolves around just such a distinction.
For genotoxic chemicals, risk to the public would be calculated traditionally — that is, one assumes that even the smallest exposures pose risks of excess cancer, and that any increment of the chemical in an organism increases risk of cancer in a linear fashion.
But for epigenetic chemicals, Albert would use a combination of this method and the system used for simply toxic, non-carcinogenic substances, like cyanide, that are poisonous but have a one-time effect only.
Toxic chemicals are assigned a ‘no observed effect level’, a dosage that produces no apparent harm in a certain percentage of lab animals.
That level is then divided by a ‘safety factor’ for humans, usually 1000, and the result is considered safe.
The trouble is, Albert admits, this method would allow anything from 10 to 369 times higher concentrations of dangerous substances like chloroform, benzene or DDT in water supplies, compared to the traditional formula.
Research scientists hacked Albert's idea to pieces.
Dr Umberto Saffiotti, an experimental pathologist at the National Cancer Institute at the time, said that type of no-effect level toxicology posed was ‘developed during the Stone Age of toxicology’, and has nothing to do with self-replicating effects of carcinogens.
Dr Arthur Upton, formerly director of the NCI and now with New York University Medical Center, was gentler, but agreed with many others that the distinction between genotoxic and epigenetic is useless for judging risk to humans.
Recent research in tumour promotion backs up this objection.
Tumour ‘promoters’ are substances such as the phorbolesters that aren't carcinogenic by themselves but encourage the growth of malignant tumours in cells that have previously been exposed to a low dose of a true carcinogen (known as an initiator).
Researchers have now discovered that even by themselves, promoters can stimulate cells to produce activated forms of oxygen, known as ‘free radicals’.
These can damage DNA.
Promoters presumably would fall within Albert's nongenotoxic or epigenetic category, a daunting prospect to those who favour erring on the side of safety.
Albert's philosophies crop up in the White House's cancer policy paper, which is to be as a blueprint for everything from pesticides to pills.
Dr Frederica Perera, professor of public health at  Columbia University and senior scientist for a prominent environmental group, says the government is also playing down in vitro tests of chemicals in cell cultures.
These are often the earliest warning that a substance may be mutagenic.
Also, says Perera, the government tends to play down the importance of prevention and of synergism (different chemicals in the environment working together), while they call for more epidemiology — or, as many environmentalists put it, ‘counting the bodies’.
The storm of objections seems in have set Reagan's science policymakers back on their heels for the moment.
They are rewriting the White House draft, while Congressional investigators are prying open the doors at EPA, where officials have been quietly setting ad hoc cancer policy in the interim
Sizewell's computerised safety system is not ready
DOUBTS about the unique computer package planned by the Central Electricity Generating Board to prevent an accident in the reactor of its PWR station have been voiced by the Nuclear Installations Inspectorate.
Last week the Sizewell inquiry, at the Snape Maltings was assured by the board that the issues ‘can be left for resolution in the normal way at an appropriate time in the project, since they do not require any fundamental system re-design to accommodate them’.
The safety watchdog is not so sure.
In a new report it complains that ‘it is disappointing that much of the information the Inspectorate has expected to receive by now is not yet available to it…. the Inspectorate is still finding it particularly difficult not having a schedule and specification for the complete protection system; this is basic information which should have been provided in the Pre-Construction Safety Report (PCSR) which will not be available until September 1983’.
A feature of the safety approach favoured by the CEGB is the use of microprocessor technology in a bid to side-step the errors by operators which helped cause the Three Mile Island accident in the US four years ago.
Reactor protection systems are designed to limit releases of radiation during nuclear plant accidents.
Crucial factors are the efficiency of reactor shut-down, emergency cooling by flooding the reactor with water and containment of any release from the reactor.
All this should be controlled and monitored by a system which keeps tabs on the state of the reactor and responds when a fault occurs.
Microprocessors have not been used for this work up to now and the first-ever system for a PWR is currently being installed by the French at their new power station at Paluel.
Known as SPIN (Systeme de Protection Integre Numerique) the French system is similar to a Westinghouse design of microprocessor-based system, called the Integrated Protection System (IPS).
It is likely to be used for Sizewell B. ‘Considerable operational experience with SPIN will be available before Sizewell B could be put into service’ says the generating board.
The NII accepts that, at this point in the safety assessment, all the details of the protection system will not be decided.
‘However it does expect that all elements of the system should be identified and a specification provided’, says its report.
The question mark over the reactor protection system is one of a queue of unresolved issues which now do not look like being settled before the public inquiry ends in the autumn.
Hopes that the NII would have completed the work needed to give the PWR a nuclear site licence by the time the hearing opened have long-since evaporated.
Ron Anthony, the NII's chief inspector made it clear last week that delays over settling the safety case were largely the fault of the CEGB, it was 10 months late with its PCSR and is still filling in gaps in key areas, like the integrity of steam generator tubes and the issue of fuel-clad ballooning.
This week the hearing reached a milestone with the first session of full cross-examination — a baptism of fire for the Department of Energy, which surprised everyone by agreeing to be interrogated on the full gamut of Whitehall's current thinking on energy policy.
Claylands of England can take nuclear waste
THE CLAY lowlands of southeast England should prove suitable for burying the operating wastes from pressurised-water reactors like that planned for Sizewell, says the National Radiological Protection Board in a report published today (Thursday).
The plans, which will be announced later this year, are to bury a variety of sludges, filters and other materials from PWR stations in shallow ‘graves’ in clay.
The waste is less dangerous than that intended to be buried in deep tunnels,(New Scientist , 24 February, p 503).
But even so, says the NRPB, the burial grounds should be safeguarded from building or farming for up to 150 years.
The burial trenches will be 18m deep and 110 m long.
They will be lined with concrete and covered with 11m of clay.
A risk assessment by the NRPB concludes that the risk to individuals from radiation from the trenches  will be greatest 450 years after their closure.
From 200 years on, risks are mainly from contaminated water infecting crops.
Child-killer was loose on cardiac ward
FOR ALMOST a year, the world-famous Hospital for Sick Children in Toronto had a murderer loose in its cardiac wards.
It may still have.
That's the inescapable conclusion of months of investigation by police and doctors into the mysterious deaths of children at the hospital two years ago.
The story begins in early 1981 with the deaths of 46 child patients, most of them in the hospital's paediatric cardiac wards.
A local coroner on a routine inspection became suspicious when he discovered that one baby had unusually high levels of a potent heart drug, digoxin, in its tissues.
The police opened an inquiry and ordered the exhumations of several bodies.
Before long their list of suspicious deaths had grown to more than two dozen.
Meanwhile the deaths stopped.
The police charged a young nurse with four murders.
But the charges were thrown out.
At the same time, the medical detective work threw up new uncertainties.
Autopsies showed levels of digoxin ranging from 16 to 39 times those expected from the doses prescribed.
In some children the drug had not been prescribed.
Yet virtually nothing was then known about how digoxin reacts in tissue after death.
Did the drug decompose or spread through the body or accumulated in certain organs?
How did the drug react with embalming or preserving fluids?
Although digoxin is easily detected in fresh blood it is far more difficult to trace in fresh body tissue and harder again to detect in old samples of either flesh or blood.
The police called in Alois Hastreiter, a retired paediatric cardiologist from the University of Illinois.
His preliminary findings and those of the hospital differed.
Hospital studies indicated that normal doses of digoxin could produce the high body-levels found.
Dr Hastreiter felt ‘it is unlikely that any explanation could be found other than overdose.’
A further study was set up involving two Canadian specialists and doctors from the Centers for Disease Control in Atlanta, Georgia.
Digoxin levels were taken in more than 400 cardial patients to provide further data.
Several things became clearer.
Before death, the drug accumulates in the heart muscles, the kidneys and the liver.
But after death the muscles release the drug into the blood stream, thus raising post-mortem levels.
But in the 24 ‘suspicious’ deaths, levels were still above those predicted.
To further complicate this medical nightmare the hospital was  already under scrutiny for a death in which a patient had been given the drug epinephrine instead of vitamin E. It is known that, when epinephrine is given at the same time as digoxin, it can upset heart rhythms.
In a final report, which has not yet been made public the Atlanta group concludes that seven babies died of deliberate overdoses and that another 2 I deaths could have been avoided if the hospital had been better equipped to spot trouble.
But, despite the new medical findings, police appear to be no closer to finding the murderer.
Doug Payne, Toronto
Hawke eyes Tasmanian dam scheme
THE UNEQUIVOCAL promise from Australia's new prime minister, Bob Hawke, last weekend that he will stop construction of the dam across the Franklin river in south-west Australia, is by no means the end of the matter.
He still faces obdurate opposition from the premier of Tasmania, Robin Gray, who claims that a swing away from Hawke's Labor Party in Tasmania (against the trend throughout the rest of Australia during Saturday's general election) shows Tasmanians want the dam.
Moreover there is, at present, no constitution way that the federal government can stop the dam.
That awaits the passage of the World Heritage (Properties Protection) Bill, which has already been passed by the Senate and is now guaranteed an easy passage through the House of Representatives.
But meanwhile clearance for the dam continues and the likely new environment minister, Barry Jones, says he may attempt to get a court injunction to halt work.
The normally unpolitical and reticent Australian Academy of Science has come out strongly against construction of the dam.
The President of the Academy, Professor Arthur Birch, last week appealed for a stop on construction.
The dam would deprive science of a number of unique biological and  archaeological opportunities.
He described the region as one of the few remaining pristine areas of the world, which must be retained for long term investigation.
Little was known of the microflora or invertebrate populations of the area.
Last week a structural geologist, Ron Peters, claimed that the proposed dam would leak.
He said he had spent four days taking rock samples in the area of the dam and had found that there were interconnected limestone layers above and below the dam site through which the water might leak away.
He claimed that the site was geologically unsound and any stress caused by a shift in water levels could cause an earthquake.
Jane Ford, Canberra
Swedish tax men breach data secrecy
THE SWEDISH government is in open conflict with the country's data protection committee over the individual's right to secrecy on money-matters.
Swedish courts will soon decide whether a bank is within its rights in handing over details of the finances of their clients to the local tax authorities, in defiance of a ban imposed by the data protection committee.
The Jamtlands Folk Bank was approached by the tax authorities for details of the balances of account holders and of interest receipts.
The committee refused its permission.
‘They asked us twice and twice we told them no,’ says Jan Freece, the chairman of the committee.
But the bank went ahead and legal proceedings began last month.
In another case, the committee has refused permission for health authorities to link their computer files with personal tax records.
The authorities want to make wealthy pensioners pay their hospital costs.
The government is still planning to go ahead with the link.
Diamond: strength in symmetry
Pick up a diamond and the play of light on its clear multifaceted surface will hold you fascinated.
That, though, is far from the only reason for contemplating this remarkable substance
Gordon Davies
DIAMONDS are magnificent jewels, once thought to have magical powers.
The power of the diamond is indeed  formidable , but there is no need to invoke the supernatural to explain its modern uses.
Diamond combines strong bonds with simple stable structure: this recipe accounts for its remarkable strength as a cutting tool, and its resistance to extremes of temperatures and pressure.
The full implications of these remarkable properties are only now being exploited, in applications ranging from cooling semiconductor chips to  exploiting the atmosphere of Venus.
Mankind has marvelled at diamonds for at least 2000 years.
Some natural diamonds are found as perfectly formed octahedrons, with a considerable visual appeal.
Stones like these began to be highly valued in India, the earliest source of diamonds, about a few hundred years BC.
Merchants carried diamonds from India along trade routes through the Middle East.
They sold the best diamonds en route .
so that by the time the eastern goods reached the  Mediterranean only the most uninspiring of the diamonds were left.
Consequently in the western world diamonds were at first not regarded as objects of beauty — other stones such as emeralds and red garnets were preferred.
Even so, in Roman days diamonds had a far greater monetary value than gold, because they were reputed to be indestructible, and, more important, they would safeguard their wearer against a whole range of misfortunes, from poison to madness.
These magical attributes persisted through the ages and died out only with the rise of scientific investigations in the 16th century.
By this time diamonds were well-known to western Europe, and were worn as jewels, but even so it was not until as late as 1796 that the chemical nature of diamond was finally established.
In that year a London chemist, Smithson Tennant, burned diamonds and found that carbon dioxide was the end product; the same amount of carbon dioxide was produced by burning equal weights of diamond and charcoal.
Evidently diamond was another form of carbon.
To many 19th century chemists the fact that diamonds were made of carbon had the appearance of a blank cheque.
By crystallising cheap carbon, fortunes would be made.
The reality was very different.
Despite an enormous amount of effort it was not until the 1950s that diamonds were successfully synthesised.
Earlier workers failed; one reason was that they did not have the technology to reach the necessary high temperatures and pressures.
Another was the lack of an accurate theory to help them estimate those conditions.
The discovery of X-ray diffraction in 1912 made a significant difference too: it then became possible to determine unambiguously whether a small, hard crystal was indeed a diamond.
Many of the early experimenters spent a lot of time trying to improve the tiny transparent crystals they had made — crystals which they erroneously believed were only silica or other hard but non-diamond crystals.
Two of the pioneers of X-ray diffraction were William Bragg and his elder son, Lawrence.
In their first joint publications in 1913 the Braggs described the atomic structure of common salt and diamond (Figure 1).
They chose to work on diamond because it contains only one species of atom, and because highly perfect crystals were easily available.
They expected diamonds to have a single atomic structure.
Their choice of diamond was significant: it was one of the first scientific studies of crystals to exploit the simplicity of diamond: a property we will return to later.
The structure of graphite, the other common crystallographic form of carbon, was finally established in 1924 by the crystallographer J. D. Bernal.
For the first time the problem of synthesising diamonds could now be expressed in terms of the ways the atoms were arranged and rearranged (Figure 2).
Although graphite and diamond both exist at ordinary temperatures and pressures, diamonds are actually unstable and are continuously reverting to graphite.
If a group of carbon atoms are detached from the diamond and allowed to re-crystallise, they will rebond not as diamond but as graphite.
Fortunately the energy required to break the atoms away from the diamond is very large, and so this process occurs very slowly at room temperature.
In fact at room temperature a diamond will survive for longer than the present age of the Universe.
But nevertheless diamond is the unstable form and if we crystallise carbon at  atmospheric pressure it will form as the more stable graphite.
To make diamond the crystallisation must be done at high pressure, at several tens of thousands of atmospheres.
High pressures are essential because graphite is less dense than diamond.
By changing from graphite to diamond, the carbon lowers its volume, and so reduces one of the contributions to its energy: the contribution given by the product of pressure and the volume.
As long as the pressure is large, this energy saving is sufficient to tip the balance in favour of diamond making it the more stable form of carbon.
In modern commercial synthesis graphite is mixed with a metal solvent — cobalt or nickel is used today.
When the temperature is high enough to melt the metal, it dissolves carbon atoms from the graphite.
Suppose now the pressure is increased so that diamond is the more stable form of carbon: that is, so that carbon has less energy when bonded as diamond than is  graphite .
It is now energetically favourable for the solvent to dump its carbon as diamond, and dissolve more of the graphite.
Using a solvent, the conditions, for synthesis are temperatures of about 1 600 K and pressures of about 60 000 atmospheres.
These can be obtained easily in production line equipment.
Today synthetic diamonds are commercially available in a range of sizes up to the present maximum, the de Beers ‘Synthetic Rotary Dresser’ stones, which have a weight of 2 milligram,(equivalent to a cubic diamond of 0.8 mm edge).
Some 20 tonnes of synthetic diamond are now being produced annually throughout the world.
Exact figures are not available, either because of commercial secrecy or, in the case of the USSR, ‘state security’.
Most of this production is bought for working materials.
For example, tungsten carbide tools, used extensively for machining steel, are themselves shaped by diamond tool-pieces.
Slabs of rock cut from quarries arc formed into building materials using diamond-tipped saw blades.
Diamond-tipped milling wheels are used to groove airport runways and station platforms to improve their friction when wet, thus preventing slipping.
The high efficiency of diamond drill bits for cutting through reinforced concrete walls has even resulted in their being chosen for obtaining illegal entry into bank safes!
The most important single reason why diamond is a good abrasive material is that the individual carbon atoms in a diamond are bonded together extremely strongly.
In abrasion, the strongest material survives.
A diamond, mounted on a spinning saw-blade, slams into the workpiece at a speed comparable with the cruising speed of a car.
A chunk of the workpiece is either knocked off as a result of fracture along a weak fault, or a layer of atoms is stripped away from its surface.
The diamond survives the impact, usually, because its chemical bonding is far stronger than the bonding in the workpiece.
The strength of bonding in diamond derives from the small size of the carbon atoms.
When small atoms bond together through ionic or covalent chemical bonds, the compactness of the bond makes it extremely strong.
Larger atoms, in less  intimate contact, tend to have weaker bonds.
Carbon is the smallest atom in the fourth column of the periodic table of elements.
The next elements in this column are the semiconductors silicon and germanium, with larger and less strongly bonded atoms.
The difference in strength of the bonds in these elements has a practical use: silicon wafers, used in integrated circuits, are sliced from large ingots of silicon using diamond-tipped saws.
The silicon ingots are highly perfect single crystals and on the atomic scale the cutting has to proceed through breaking bonds.
The success of diamond in cutting silicon is a measure of diamond's extreme resistance to abrasion.
The strong bonds in diamond are also responsible for many of its non-abrasive uses.
For example, the strong bonds allow the atoms in diamonds to vibrate at very high frequencies.
The frequency of any vibrating system is governed by two factors: the forces pulling it back into the equilibrium position, and its mass.
In diamond, the chemical bonds are strong, favouring high frequencies of vibration.
Also, the atoms are light with mass number 12 compared, for example, with 28 for silicon, the next column four element.
Again this favours high vibrational frequencies.
In fact the atoms in diamond can vibrate at about 40 x 10 Hz.
For comparison, atoms in silicon are limited to less than half that frequency while atoms in a crystal of common salt have a maximum frequency of only 6 x 10 Hz.
Several things will make the atoms in a crystal vibrate.
Shining infrared radiation on the crystal is one.
How the radiation is absorbed depends on the crystal.
For a crystal of common salt the frequency of the radiation has to equal the frequency with which alternate ions vibrate against each other(Figure 3).
The electric field of the radiation can then resonate with the vibrations of the charge ions, transferring energy to them.
Diamond absorbs infrared radiation by a slightly more complex process.
The atoms are not charged as in common salt.
They are all chemically identical, and in equivalent lattice sites, so there is no reason why charge should be transferred between them.
Consequently in a static diamond there are no charged atoms to interact with the electric field of infrared radiation.
Diamonds can, however, absorb radiation by a two-stage process.
(These stages occur simultaneously, but it is convenient to think of them as being sequential.)
The first step is to create a vibration in the diamond so that the carbon atoms are no longer in equivalent positions in the lattice.
Charges can now be transferred between them.
The electric field of the radiation can interact with these charges and so radiation can be absorbed, as in common salt.
Infrared radiation is absorbed by a diamond in a process that involves the creation of two vibrations, not just one as in common salt; the frequency of the radiation equals the combined frequency of the vibrations.
The light, strongly-bonded carbon atoms can vibrate at unusually high frequencies, so the frequency of the radiation absorbed is very much higher for diamond than for other materials.
This result has extremely useful applications.
If we want to study the way some material absorbs infrared radiation we can put it into a cell fitted with diamond windows.
The low frequency radiation absorbed by the material cannot be absorbed by the diamond: the diamond acts as a perfectly transparent window.
Diamond forms a good window for infrared cells, because it is highly transparent, chemically non-reactive and exceptionally strong.
The cell can therefore be pressurised to study how materials behave under high pressure.
Cells fitted with diamond windows have been operated at pressures of over one million atmospheres at the Carnegie Institute in Washington.
The same qualities — mechanical strength, chemical inertness and infrared transparency — were the reasons why diamond windows were selected for the NASA space-probes sent to Venus in 1978.
As the probes descended to the surface of the planet, arrays of detectors measured the infrared radiation being emitted at different wavelengths by the atmosphere.
From the measurements scientists could assess the temperature and the chemical composition of the atmosphere.
The windows covering the detectors had to withstand decelerations of over 500 times the acceleration due to gravity on Earth.
They also had to survive the effect of a highly corrosive atmosphere at red heat and a hundred times the atmospheric pressure on Earth, and of course they had to be transparent to infrared radiation.
Only diamond could meet this specification.
The high frequencies of vibration in diamond has other consequences too.
According to quantum theory, a microscopic system will only vibrate if it is given an amount of energy at least equal to the frequency of vibration multiplied by Planck's constant.
Vibrations with very high frequencies, such as can occur in diamond, require such a large amount of energy that they are unlikely to occur at room temperature.
Consequently, at room temperature diamond is a relatively static crystal.
When something hot touches diamond, the atomic vibrations that constitute the passage of heat pass through the crystal undisturbed by local  vibrations due to the crystal's own finite temperature.
The relatively static lattice in a diamond ensures that the scattering is at a minimum and the thermal conductivity is exceptionally good.
As a result, diamond is an excellent conductor of heat.
In fact, at room temperature a diamond has a thermal conductivity about six times higher than a piece of copper.
This high thermal conductivity gives diamond a small but useful role in the semiconductor industry.
As electronic components become smaller and more powerful it becomes increasingly important to extract waste heat from them.
For example, one type of silicon diode used in microwave communications can operate at a power density of I kilowatt per square millimetre of base area.
By mounting the diodes on diamond blocks, the heat generated at the base of the diode can be conducted away far more rapidly than by mounting it directly on a copper heat sink.
There is another way of looking at how stable the structure of diamond is — that is by asking how easy it is to break the bonds that hold it together.
In fact, breaking one of the bonds, by pulling an electron out of it, requires a lot of energy.
Electrons, being charged particles, interact readily with electromagnetic radiation.
An electron can be given enough energy to break out of its bond if the frequency of the radiation is high enough — quantum theory tells us that, as with atomic vibrations, the energy in a light wave comes in increments of its frequency multiplied by Planck's constant.
To excite an electron out of its bond requires high frequency radiation, falling in the ultra-violet part of the spectrum.
To the joy of jewellers visible light has too low a frequency to excite an electron in a perfect diamond.
It passes straight through without absorption and the diamond looks bright and clean.
In fact, the diamonds we see in jewellers' windows are typical of only a small percentage of natural diamonds.
Most diamonds are brown or yellow with little visual appeal and are fit only for industrial use.
The colour is produced by imperfections and impurities in the diamond, for example, a yellow colour results when the diamond contains nitrogen atoms clustered together in equilateral triangles with each nitrogen atom replacing one carbon atom of the perfect diamond (Figure 4).
At these defects the electron orbitals are very different from those found in the perfect regions of the diamond.
The electrons may be excited from one orbital to another, still localised at the nitrogen impurity, by much less energy than is required in the perfect diamond: in fact, blue light has sufficient quantum energy.
Because the diamond is absorbing blue light, it appears yellow in ordinary light.
The colours produced by different atomic sized defects are specific to the particular defect.
Diamonds may be brown, mauve, green or blue.
These colorations are by no means mere curiosities.
Atomic-sized defects in covalent semiconductors are conceptually very simple, but currently there is no reliable way of predicting their properties by calculation from first principles.
This is a serious problem, for defects like these have a dramatic effect on the functioning of an electronic device, and it would be desirable to be able to predict their properties.
Diamond, the simplest of the elemental semiconductors, is an obvious test material for these calculations.
This brief article has necessarily picked out only a few aspects of diamond.
We have seen that diamonds are now essential tools in heavy engineering, infrared optics, electronic heat sinks and basic solid state research.
They also perform surgery, detect radiation and play records.
But curiously, as in Roman days, diamond is still best known as a Mystical symbol: as a symbol of everlasting love.
The search for a viral penicillin
Because viruses replicate by hijacking the genetic machinery of host cells, it becomes extremely difficult to make effective anti-viral drugs.
Inhibiting the turnover of viruses often means inhibiting the host cells too.
But some new anti-virals are overcoming these obstacles
Andrew Scott
THE arrival of effective anti-viral drugs has been eagerly awaited ever since penicillin began its war against bacterial infections during the 1940s.
Since then a host of antibiotics have been developed to fight bacteria but in the battle against viral infections we are still largely dependent on the body's imperfect natural defences.
The continuing search for drugs to combat viral infections presents modern medicine with one of its greatest challenges.
Much has been written about the potential of genetic engineering to provide plentiful supplies of interferon and new vaccines.
But the challenge is also being met in more conventional ways, and many chemicals look promising as potential  anti-viral agents.
There are good biological reasons for why it is so difficult to produce drugs that will knock out viral infections.
Viruses multiply inside cells by hijacking the cells' own molecular machinery and forcing it into the service of the viral life-cycle.
Some viruses rely entirely on the enzymes of the infected cell to catalyse every step in the production of more viruses.
Other viruses make many of their own enzymes but still depend on cellular proteins to perform some essential  tasks .
This insidious behaviour means that many drugs which interfere with the life-cycle of the virus, for example by inhibiting the activity of crucial enzymes, will do the same to normal host cells.
The toxicity of such drugs severely limits their use in anti-viral treatment.
Drugs that are selectively toxic, affecting only viruses or cells already infected with a virus, have proved extremely hard to find.
Nevertheless, as more is learnt about the molecular details of viral infections, points at which viruses might be vulnerable to selective attack are being uncovered.
Obvious targets would be the unique viral enzymes, coded for by the genes of the virus.
Viruses can contain from one to over 200 genes, many coding for enzymes which perform highly specialised tasks in the viral life-cycle.
If chemicals can be found that inhibit the activity of these viral enzymes, while leaving cellular enzymes unaffected, then the path to anti-viral drugs will have been cleared.
Alternatively a drug might be ineffective until converted into an active form by a particular viral enzyme, thus restricting the action of the drug to only those cells that are infected with the virus.
One of the most publicised of the new anti-viral agents (and one of only three anti-viral, drugs currently listed for prescription in the UK Monthly Index of Medical Specialities) is acyclovir — the trade name for 9-(2-hydroxyethoxymethyl) guanine.
Acyclovir's anti-viral activity is based on just the kind of selective enzyme interactions outlined above.
The structure of acyclovir shares crucial features in common with two of the precursors of DNA-thymidine and deoxycytidine (Figure I).
Before these precursors can be incorporated into viral DNA, they must be phosphorylated Certain viruses (the herpes group for example) produce an enzyme called thymidine/ deoxycytidine kinase, which is responsible for adding a phosphate group to these two precursors.
Because of its similar structure acyclovir can also receive a phosphate group from the thymidine/ deoxycytidine kinase enzyme.
This is the first step in the conversion of acyclovir into its anti-viral form.
In the next step enzymes native to the infected cell add two more phosphate groups to make acyclovir triphosphate.
Acyclovir triphosphate has a very similar structure to deoxyguanosine triphosphate (dGTP), the direct precursor of one of the four nucleotide ‘building blocks’ that are linked together in precise sequences to make DNA.
This similarity allows acyclovir triphosphate to bind to the enzyme (called DNA polymerase) that links the nucleotides together during the replication of viral DNA.
This somehow inhibits copies of viral DNA being made, and is the basis of acyclovir's anti-viral activity.
Binding of acyclovir triphosphate with DNA  polymers could inhibit viral DNA synthesis in two ways.
First, acyclovir triphosphate might simply compete with dGTP for a binding site on the polymerase enzyme, and so prevent the proper incorporation of dGTP into DNA.
Second, acyclovir triphosphate might itself be mistakenly incorporated into DNA, in place of dGTP.
If this occurs the chemical structure of the drug makes further growth of the DNA chain  impossible , and so the copies of viral DNA would never be completed.
Acyclovir is a specifically anti-viral drug because the  enzymes of normal healthy cells cannot efficiently perform the initial phosphate addition achieved by the viral enzyme.
So acyclovir phosphate and triphosphate are not formed in healthy cells, whose DNA synthesis is therefore not affected.
Also, the drug seems to inhibit the viral DNA  polymerase enzyme much more effectively than it does the DNA polymerase used by the cell to copy its own DNA.
Acyclovir has been developed and marketed by the Burroughs Wellcome company, and while it is certainly not an anti-viral panacea it is proving of definite benefit in the treatment of herpes virus infections.
Herpes viruses cause many diseases in humans, including genital herpes shingles and cold sores.
In patients whose immune system is deficient for any reason (after transplant surgery or anti-cancer therapy for example) herpes virus can cause very serious infections which can occasionally prove fatal.
Acyclovir given intravenously, orally or applied directly at the site of infection can accelerate healing and restrict the spread and recurrence of many herpes infections (British Medical Journal, 1982, vol 285, p 1223.).
Because of its selective anti-viral action acyclovir treatment avoids many of the toxic side-effects found with earlier drugs.
According to Professor Morag Timbury of Glasgow Royal Infirmary ‘acyclovir seems set to become one of the first effective anti-viral agents that can be given systemically in safety.’
Many other chemicals that act in a similar way to acyclovir are under investigation.
They all have structural similarities to the nucleotide building blocks of DNA (or RNA), allowing them to interfere with the replication of viral genetic material.
Of course in order to be effective anti-viral agents they must at some stage be dependent on a viral enzyme to convert them into their active form, or they must selectively inhibit viral enzymes directly.
Another of the three anti-viral drugs currently prescribed, called vidarabine, shares several structural features with acyclovir but the precise mechanism of its action has not been worked out.
One viral enzyme that is an obvious target for selective attack is ‘reverse transcriptase’ found in the RNA — containing retroviruses.
This enzyme copies the viral RNA genetic material into DNA, which is then integrated into the cell's–chromosomes as an essential step in the viral life-cycle.
A wide range of chemicals have been shown to inhibit the reverse transcriptase enzymes but their medical value has not yet been determined.
An alternative to attacking the enzymes that catalyse important steps in the viral lifecycle is to try and interfere with the viral genetic material directly.
One possibility is to make a short section of DNA or RNA that can bind to a particular region of viral genetic material to form a double-stranded complex, just like the familiar double helix of cellular DNA.
In order to be able to bind in this way the sequence of nucleotides in the synthetic piece of DNA or RNA needs to be complementary to the viral sequence.
This means that it must be capable of forming the weak hydrogen bonds between the two strands which hold together all double-stranded forms of DNA or RNA.
The formation of such complexes might prevent the viral genes from being replicated, or from being used to direct the manufacture of viral proteins.
Paul Zamecnik and Mary Stephenson of Harvard University have used this approach to interfere with the activity of Rous sarcoma virus (a retrovirus that causes cancer in animals).
The sequence of nucleotides at each end of the single-stranded viral RNA was worked out in 1976.
Zamecnik and Stephenson linked together 13 nucleotides to produce an ‘oligonucleotide’(oligo= a few) that was complementary to a short region found at both ends of the viral RNA.
They added this oligonucleotide to cultured chicken embryo cells infected with Rous sarcoma virus and found that the production of new viruses within the infected cells was strongly inhibited (Proceedings of the US National Academy of Sciences .
1978. vol 75, p 280).
Presumably the synthetic oligonucleotide was taken up by the cells and became bound to the complementary sequence on the viral RNA.
There are several stages at which binding of the oligonucleotide could inhibit virus production.
It could bind to the viral RNA (Figure 21), preventing it from being copied into the double-stranded DNA which later integrates into the host chromosomes.
Secondly, it could bind to the DNA copy of the viral RNA.
This might prevent the integration process (Figure 2.2) or interfere with the copying of integrated DNA into the messenger RNA molecules that are used to direct protein synthesis (Figure 2.3).
Finally, the oligonucleotide might bind to the messenger RNA, preventing protein synthesis from taking place (Figure 2.4).
Working out the nucleotide sequence of viral genetic material and making oligonucleotides of specific sequences is steadily becoming easier, more efficient and more accurate.
This opens up the prospect of developing the preliminary work done by Zamecnick, Stephenson and others into a general strategy for anti-viral therapy.
The success of this approach will depend on finding sequences for oligonucleotides that bind sufficiently well to viral genetic material to cripple the virus, and yet do not bind to normal cellular DNA or RNA.
The approaches to anti-viral drugs discussed so far have concentrated on interfering with viruses once they have entered cells and the process of infection has begun.
Some research teams are trying to prevent viruses from getting inside cells in the first place.
Particular regions on the coat proteins of viruses must bind to receptor molecules in the cell membrane before the viruses can enter animal cells.
Professor Purnell Choppin and his colleagues at the Rockefeller University in New York have made peptides (short sections of protein) with a similar structure to the regions of virus proteins that bind to membrane receptors.
The hope is that these peptides might compete with the virus proteins for binding to the receptor molecules.
If enough of the synthetic peptide molecules are supplied then there might be very few free receptors left for the viruses to bind to.
Some preliminary trials of this approach have been encouraging.
When appropriate peptides are added to cultured animal cells the ability of viruses such as measles and influenza virus to infect the cells is considerably reduced (Virology , 1980, vol 105, p 205).
Of course the true potential of these synthetic peptides will be revealed only when they are tried out against infections in live animals, and Professor Choppin's research group is currently performing such tests.
Stop viruses disrobing
When viruses succeed in binding to cell membrane receptors they still have to enter the cell and break up into separated proteins and genetic material before they can replicate.
Dr Kai Simons of the European Molecular Biology Laboratory has found that at least some animal viruses enter the cell surrounded by an envelope of cell membrane.
This membrane then fuses with the membrane of a cell body called a lysosome and the virus is eventually released from the lysosome to initiate infection (see New Scientist , 10 February, p 372).
Simons has suggested that drugs preventing the fusion between the lysosome membrane and the piece of membrane surrounding entering viruses might be very effective anti-viral agents.
It remains to be seen whether or not this idea can be put into practice.
The final chance of preventing virus replication beginning, is to inhibit in some way the uncoating stage where coat proteins of the virus dissociate from the genetic material they surround, The third anti-viral drug available now is a cage-like hydrocarbon amine molecule called amantadine, which seems to inhibit the uncoating of influenza viruses in some way.
Although the details of how amantadine works are unknown, the drug does seem to accumulate inside lysosomes.
Perhaps it acts at the lysosomal stage of the entry process outlined by Simons.
Another possible point of attack is the final assembly of new viruses and their release from the infected cell.
During these final stages host enzymes attach carbohydrate groups to certain viral proteins, converting them into glycoproteins.
Drugs that interfere with this process (called glycosylation inhibitors) have been found to inhibit the production of several kinds of virus.
Two examples of promising glycosylation inhibitors currently under  investigation are 2-deoxyglucose and glucosamine, but again the mechanism of their anti-viral activity is not fully understood.
The most publicised aspect of recent anti-viral research has of course been the work on interferon — part of the body's natural defence against viruses.
The interferons are a series of small glycoproteins whose synthesis is induced by many viruses.
At least one trigger of interferon production appears to be the double-stranded RNA molecules that are formed during the lifecycle of many viruses (including the DNA-containing viruses).
After it is released from the cell, interferon becomes attached to neighbouring cells and converts them into an ‘anti-viral state’ in which the multiplication of viruses is strongly inhibited.
The mechanism of this anti-viral effect is still not fully worked out, but it certainly involves the increased production of two particular enzymes.
The first of these enzymes makes an unusual series of oligonucleotide molecules with the general name of 2',5'-oligoadenylate.
These molecules go on to activate a ‘nuclease’ enzyme that breaks down messenger RNA molecules used to make viral proteins.
The second interferon-induced enzyme adds a phosphate group to a protein that is essential for the synthesis of new proteins to begin.
Addition of this phosphate group impairs the function of the essential protein, and viral protein synthesis fails to begin.
Both enzymes appear to be activated by viral double-stranded RNA.
But exactly how viral protein synthesis is selectively inhibited, leaving cellular protein synthesis relatively unaffected, is still not clear.
The great appeal of interferon as an anti-viral agent is its effectiveness against a wide range of viruses of varied structure and containing different types of genetic material.
This is in contrast to most potential anti-viral agents, whose action is usually restricted to only a few of the many different types of viral agents that cause disease.
Interferon is of course not a new discovery — it was first reported in 1957 by Alick Isaacs and Jean Lindenmann working at the National Institute for Medical Research in London.
The recent upsurge in interest has been generated by the possibility of producing large quantities of interferon using modern techniques, including genetic  engineering .
Trials of interferon in viral disease have certainly met with some success.
It has proved of benefit when used against herpes viruses or viral hepatitis for example, and researchers at the MRC's Common Cold Unit in Wiltshire have shown that it can provide effective protection against colds.
For the time being however, interferon remains very much an experimental drug whose full clinical value has yet to be determined.
An alternative to using interferon itself as an anti-viral drug is to use chemicals that induce cells to make their own interferon.
One approach has been to make synthetic double stranded RNA molecule– which can mimic the effect of viral double-stranded RNA as an inducer of interferon  production .
Several  research groups have performed clinical trials with a simple synthetic RNA called polyinosinicpolycytidylic acid (polyl:C).
While interferon production was certainly induced by this compound its efficacy in treating viral infections bas so far been disappointing.
Another idea thrown up by interferon research has been to make synthetic analogues of the 2',5'-oligoadenylate molecules formed in response to interferon and which activate the nuclease enzyme to break down viral messenger RNA.
Paul Doetsch and co-workers at the Temple University Medical School in Philadelphia have administered a 2',5'-oligoadenylate analogue to human lymphocytes (a type of white blood cell) infected with a herpes virus called Epstein-Barr virus.
This virus causes glandular fever and is also associated with a human cancer called Burkitt's lymphoma.
Doetsch found that his synthetic analogue prevented the normal development of the viral infection within the cells, prompting him to suggest that it might have therapeutic potential (Proceedings of the US National Academy of Sciences , 1981, vol 78, p 6699).
This work with polyl:C and 2',5'-oligoadenylate analogues demonstrates that even if interferon itself doesn't live up to our expectations, research into its mechanism of action might suggest new and more effective ways to attack viral disease.
The battle to conquer viruses is unlikely to be transformed by the sort of dramatic advance that penicillin made in the earlier fight against bacteria.
There is more likely to be a gradual wearing down of viral disease, with a wide variety of drugs becoming increasingly effective against specific types of virus.
The first successes are already being achieved with acyclovir, vidarabine, amantadine and interferon.
French Science — finance rules aux quais
The Socialist government in France has achieved a change in the country's R&D programme that has astounded observers.
Critics say the figures are fudged
Andrew Lloyd
FRENCH R&D has gone through an irreversible change even though budgetary ‘rigour’ looks certain to clip the massive spending central to the government's strategy to encourage research, development and high technology industry.
Just how much spending has been and will be hit is the talk of the research world in France at the moment.
But even without the blank cheques originally promised, enough has been achieved in the 22 months since the Socialists came to power to make sure French research will never be the same again.
‘Even sceptics have to admit that the French have something to show for their research efforts over the past two years’ concedes one scientific counsellor at a foreign embassy in Paris.
Money, new priorities, institutional changes, tax incentives and other measures have been used to influence progress in high technology (New Scientist , vol 93, p 19).
Just as importantly, the authorities have urged the French to change their attitude to science.
For once, foreign opinion of France is more positive than that of the French.
‘The thrust [the French]are giving to science and technology is really unique,’ declares Louis Berlinguet, scientific counsellor at the Canadian Embassy in Paris.
More fulsome in his praise than the anonymous diplomat above, Berlinguet approves of the measures taken.
‘The balance sheet is very impressive.
Everything has been set up so that science can get moving,’ he says.
France, he adds, is one of the few countries where science and technology has a future.
‘Chevenement, the minister for research and technology, and President Mitterrand are surrounded by good scientists.’
It was Francois Mitterrand, even before his election in May 1981, who turned attention to science and technology.
He promised a powerful research ministry to push the niggardly 1.8 per cent of gross domestic product spent on research to 2.5 per cent by 1985.
The man charged with carrying out this policy, the ‘advocate of the future’, was Jean-Pierre Chevenement, son of a schoolteacher, came to power with the reputation of a left-wing idealist.
Since then he has shown every sign of being a pragmatist, an adroit politician and a very hard worker.
Under Chevenement, the government has created changes affecting nearly every aspect of scientific life.
The cornerstone of the new policies is a law piloted through parliament by Chevenement last July.
His big breakthrough was to get parliament to accept a high technology spending programme up to 1985 with a commitment to priorities.
The plan was to avoid annual parliamentary wrangles.
The top priorities agreed are: rational production, use and diversification of energy, biotechnology; electronics; scientific  research and technological innovation in the service of the developing nations; research into employment and working conditions; promotion of French as a scientific language and the discrimination of scientific and technical culture; and the development of technology use in industry.
The first three and the last have obvious economic implications, the other priorities have long-term political value — though it could be argued that even these will provide indirect economic benefits.
So far, electronics has gleaned the bulk of aid and much of the limelight.
Last year, the government announced it would receive a $20 thousand million investment over the next five years, including private sector funds.
Substantial amounts have been earmarked for energy — mainly to a new agency for energy management and the atomic energy agency.
Biotechnology spending has been more diffuse but it will get at least £100 million a year in government spending.
Leaving aside the vexed question of exactly how much money the government is going to spend, there is heated argument about its approach.
First, the government is pumping a lot of cash into obsolescent industries.
Secondly, the new research and technology law itself singles out 10 or so second-priority sectors worthy of attention.
‘They've adopted a sheepdog approach and put priorities on everything, except for environment and university research,’ says one observer.
A lot of money is going to the losers.
Steel and textiles are examples, though the government would reply that it is good marketing policy to renovate machinery, improve productivity and quality.
While finance is central to the new policies, other measures have been hugely successful.
The first move was to galvanise scientific opinion by a series of regional and national debates on science policy.
It was theoretically out of these debates that the legislation on research and technology policy was framed.
The programme may easily be criticised as a political ploy designed to make decisions already taken look like the result of democratic process.
Most observers feel that the discussion remotivated some of the more dispirited sections of the scientific community.
One diplomat observed: ‘The national colloquium and the discussions leading up to it gave the lower ranking researcher better status, and a feeling of more involvement in his work.’
A verdict not shared by all.
Henry Bordes-Pages, spokesman on research for the white-collar union CGG says: ‘The government has spent too much time on debates.
And a year later, researchers have lost their motivation now that they find promises of more money have been broken.’
Moreover, some senior research directors are particularly chagrined by credits which are frozen as soon as they are granted.
The science establishment itself, in particular the Centre National de la Recherche Scientifique (CNRS), France's 25 000-strong state research body, was rendered wide-eyed by a reorganisation at senior levels at the end of 1981.
Later, senior researchers in some science establishments lost their security of job tenure.
Chevenement set up a new management structure within the research ministry to coordinate national ad industry research efforts.
The new organisation rivals DNA in its complexity.
But Chevenement has created a body which is better equipped to achieve the objectives laid down in the law.
On the negative side, as political opponents are only too ready to point out, Chevenement has installed his own men in key positions.
The reorganisation of the CNRS and other bodies is one facet of a major principle behind the government strategy: the researcher must be tempted down from his ivory tower to talk to industry and think about the needs of the real world.
This message has begun to percolate through the corridors of science power.
People in universities are beginning to think about how they relate to the rest of the world.
The French, it must be said, have been lost for centuries in wonderment at science.
They have accorded it blind respect while considering it totally irrelevant to the real world.
Scientists for their part have tended to consider the layman's admiration as their right and the real world as irrelevant.
Another aim of the new policies is decentralisation.
Previous governments have paid lip service to the idea but achieved little.
Perhaps it was too difficult to let the ‘keys to power’ leave Paris.
Chevenement has installed a system of regionalising research.
He has created regional delegations, encouraged bodies that finance innovation, such as ANVAR, to move away from the capital, and generally supported initiatives in research at regional level.
These moves have gone down fairly well in most quarters though again political cynics see decentralisation as a long-term ploy by Chevenement to build up personal political support in the provinces.
There is widespread approval for all these measures with occasional examples of carping criticism.
There is unanimous scepticism on the reality of the huge budget increases voted parliament and confirmed by Chevenement.
In mid-January, the minister made his frankest admission yet that money is tight.
‘The forecast growth for the civil research budget is 17.8 per cent.
It is possible that this may be a little reduced by budgetary adjustment,’ he told a gathering of French researchers and administrators on the Cote d'Azur.
The 1982 budget has already fallen short of spending targets — thanks to a prime ministerial freeze on 25 per cent of capital spending.
But just how far short is anyone's guess — and a guess that few are prepared to make.
Even a parliamentary question received such an ambiguous answer that deputies were little the wiser after the government's reply.
But for the moment it seems that Chevenement is fighting to keep up appearances of big budget increases while losing at least a little ground to other ministries in that fight for limited funds.
Scepticism over official figures comes from both ends of the  political spectrum.
‘It's difficult to say just how much spending will fall short of target,’ says Henry Bordes-Pages.
But some of this year's rumoured ‘freeze’ on credits may mean there is little real increase at all, he says.
At the CNPF, France's business and industry lobby, there are complaints that the government has been fudging figures to make it look as though targets have been met.
Some of the present government's high technology spending is being inflated by including money already earmarked by the previous government.
Many people wonder for how long cosmetic accounting can hide broken promises.
A brighter future
But despite the doubts, it seems at least probable that substantial research spending increases have occurred, at least in the seven areas singled out for special treatment.
Chevenement said last month that preliminary estimates for 1982 science spending show that the percentage of GDP spent on R&D has already risen to 2.1 per cent, in line with the 1985 target of 2.5 per cent .
Nevertheless, there are areas where France's research policy has made little progress.
Apart from union complaints about the few jobs generated, there is little sign that the much vaunted investments in high technology are pulling the French out of recession, once a basic  tenet of the Chevenement philosophy.
Rival ministers must even claim that heavy spending in one area is holding back growth in others.
Most critics say it is too early to judge.
The future still looks brighter than it would have done without the new plans.
Industry is another area where little progress has been made.
From his early days as minister, Chevenement pointed out that industry's share of R&D spending compared badly with the United States, the German Federal Republic and Japan.
To reach the 2.5 per cent of GDP target by 1985 industry, R&D had to increase by 8 per cent a year.
Last year, it managed 5 per cent.
The CNPF points out that industry spending will always appear lower in countries where a lot of government military research is carried out (compared with Germany and Japan for example).
They also say it was government research which slipped back the most between 1973 and 1981 and that industry needs to be allowed to make profits before upping R&D.
Chevenement was actually given responsibility for industry as well as research just under a year ago.
This removed one source of opposition to his plans (an independent industry ministry) but put more pressure on him to achieve results today rather than tomorrow.
And, to mix a metaphor, France's lame ducks may have become an albatross around his neck.
Chevenement's influence has yet to be seen in the banks, now mostly state owned.
His plan was to push the banks into investing more risk capital — a pressure which the banks so far have been able to resist.
The number two at the Banque Nationale de Pans, Jacques Masson, refused to answer a New Scientist question on whether BNP had increased its financing of innovation since the socialists had come to power.
Chevenement is finding that finance from the treasury, the banks or from private industry still rules aux quais de Paris.
MONITOR
Sugary foods may promote breast cancer
CERTAIN Foods, particularly sugar, may encourage the development of cancer of the breast, suggest Stephen Seely of the University of Manchester and Dr D. F. Horrobin of Efamol Research Institute in Kentville, Nova Scotia.
Their epidemiological studies, to be published in the July issue of Medical Hypotheses , highlight a striking correlation between dietary intake of sugar and mortality from breast cancer across 20 countries.
Breast cancer is primarily a hormone dependent disease; thus, when the cells are grown in tissue culture, their proliferation is temporarily checked by the absence of female sex hormones.
So the ovaries of women suffering from breast cancer are often removed.
But the relief is only temporary because when hormone-dependent cancers are deprived of their hormone supply, sooner or later some cancer cells mutate to become independent of them and the growth of the tumour is resumed.
But diet also appears to have a strong connection with breast cancer.
The most important evidence, as in all diet-related diseases, comes from immigrant studies.
The incidence of breast cancer in women of Japanese origin living in the United States has been intensively studied.
The disease is rare in Japan but begins to become more common in the first generation born in the United States, and approximates the rate prevailing in the US by the second generation.
Yet suspecting that a disease is linked to diet is a far cry from being able to identify the item of diet that has toxic or pathogenic properties.
But dietary pathogens can be traced by looking for a match between the geographical distribution of a disease and the consumption of various food-stuffs.
A statistical survey carried out by Seely and Horrobin shows that the geographical distribution of breast cancer varies with age: not a surprising discovery for a disease in which hormonal and dietary Factors overlap.
It is reasonable to assume that hormonal effects predominate in younger women, while dietary factors may become increasingly important in older women.
So the geographical distribution of the disease in older women is more likely to point to dietary pathogens.
Among the 20 countries belonging to the Organisation of Economic Cooperation and Development, for which reliable statistics on both mortality and food consumption are available, the highest mortality from breast cancer in older women (65–74 group), in descending order, occurs in the United Kingdom, the Netherlands, Ireland, Denmark and Canada, and the lowest in Italy, Spain, Portugal, Yugoslavia and Japan.
This ranking order is based on the latest available mortality statistics, ranging from 1977 to 1979.
The researchers found that this geographical pattern is best matched by that of sugar consumption, in which the leading countries are, in descending order, the UK, the Netherlands, Ireland, Canada and Denmark, and the countries with the lowest consumption are Italy, Yugoslavia, Spain, Portugal and Japan.
The data on the consumption of food antedate mortality statistics by 10 years, and include all foodstuffs with high sugar content, like honey, syrup, and glucose, in proportion to their sugar content.
Insulin is probably the connecting link between sugar consumption and breast cancer this hormone is secreted in response to rising levels of glucose in the blood.
The public image of insulin is that it is some kind digestive enzyme, necessary for the metabolism of carbohydrates which, in fact, is not the case.
Its function can be best visualised as a form of rationing which, in lean seasons, determines priorities at which various tissues can draw glucose from the plasma.
Some essential and irreplaceable tissues, like the brain, kidney tubules, intestinal mucosa and red blood corpuscles, can take up glucose in the absence of insulin, which other tissues, notably muscles which are normally the largest consumers of glucose, need its presence.
When food is plentiful, sufficient insulin is secreted for all organs to satisfy their needs, but when food is scarce, some organs or tissues are starved.
The female breast is a luxury organ, low in the order of priorities.
In tissue cultures insulin is an absolute requirement for the growth proliferation of normal mammary tissue and breast cancer cells are, at least initially, also dependent on it.
Its connection with breast cancer is essentially the same as that of oestrogens and prolactin.
The apparent pathogenic effect of sugar, in this light, is due to its easy digestibility.
Commercial refined sugar (sucrose) is a disaccharide consisting of glucose and fructose.
When they are split, glucose immediately enters the circulation.
Fructose is inverted in the liver and follows after a brief interval.
The sudden appearance of glucose in the bloodstream and the corresponding rise in blood glucose level evokes a quick response from the pancreas and results in a brief state of apparent overabundance.
This state seems to act on low-priority tissues as a mild carcinogen.
According to this  argument glucose is a supersugar.
When consumed as such, its entry in the bloodstream is not preceded by any metabolic work.
Glucose is used by the food industry mainly in the manufacture of  confectionery .
In Britain and in the Netherlands it accounts For about 10 per cent of the total consumption of sugar, in Ireland for 8 per cent, in Germany and France for about 5 per cent .
The fact that the three largest consumers also have the highest rates of mortality from breast cancer is certainly suggestive.
Synthetic oestrogens (in oral contraceptives) and phytooestrogen (mimics of oestrogen present in many food plants) may also act, in conjunction with natural hormones, to promote breast cancer in women.
Seely and Horrobin have explored the influence of oestrogen-mimics.
Leguminous plants are the most important producers of phytooestrogens.
The most common legumes are pasture plants such as clover, and soya and other beans eaten by humans.
Sheep are particularly sensitive to phyto-oestrogens (presumably because they crop pastures more closely than cows); the substances can cause infertility, difficult labour and lactation in unbred ewes.
Oestrogen-mimics, therefore, can have an indirect effect on the breast.
The traditional consumers of soya beans are countries of the Far East.
Other leguminous plants play a more important part in the diet of Mediterranean than North European countries; so the consumption of leguminous plants seems negatively correlated with breast cancer.
It is interesting to note, however, that in these countries breast cancer in younger women is comparative more prevalent than in older women.
In Japan, for example, breast cancer mortality in the 65–74 age group is 14 per cent of that in Britain, while in the 35–44 age group it is 36 per cent .
The corresponding figures for Spain, Portugal and Yugoslavia are 42/63, 38/64 and 30/58 per cent respectively.
Phyto-oestrogens are thus unlikely to represent an important factor in the development of breast cancer, but they may have some contributory effect in younger women.
An ear for rhyme helps children read
D0 CHILDREN learn to read by recognising whole words or by associating sounds with letters?
Educational theory has embraced both views, with the ‘phonetics’ and ‘look-and-say’camps firmly entrenched.
A new study by Peter Bryant and Lynette Bradley at Oxford University provides the first good evidence that the ability to  analyse a word into its constituent sounds is most important in the early stages of learning to read (Nature , vol 301. p 419).
 Bryant and Bradley were not the first to notice that poor readers were also insensitive to the sounds of words, but their study is the first to establish the causal connection between the two.
They achieved this by combining two different methods: a longitudinal study, following the same children for four years from before they were able in read until they had been at school for two or three years; and a training programme that looked at the effect of giving pre-school children specific training in categorising sounds.
They began by testing 403 children on their ability to pick the odd one out of a list like ‘hill, pig, pin’.
None of these four and five-year-olds could read at that stage.
Four years later they gave the 368 children left in the study standard reading and spelling tests.
They also gave them general intelligence tests so that they could exclude the effects of variations in intelligence.
The results showed clearly that children who were good at categorising sounds were better at reading and spelling regardless of their IQ.
But does the one cause the other, or are both due to an unknown third factor?
The training study answered that question.
Bryant and Bradley chose 65 of the children who had not been very good at categorising sounds at the beginning of the study and divided them into four groups.
Two groups received intensive training over two years in recognising the common sounds in sets of simple words represented by pictures.
In one group they were also shown which letters of the alphabet went with which sounds.
The other two groups acted as controls, one receiving no training at all, the other being taught how to sort pictures into conceptual categories (animals, for example ).
Both groups trained to attend to the sounds of words were significantly better at reading and spelling at the end of the study than the untrained group.
The group that was introduced to the sounds of individual letters was better at spelling than the group trained only to pick out sounds in words.
Bryant and Bradley conclude that a child's awareness of the sounds in words, revealed by their ability to detect rhyme and alliteration, ‘has a powerful influence on their eventual success in learning to read and spell’.
They also point out that this awareness may well be influenced by children's experiences before they go in school.
Parents bored with the endless repetition of nursery rhymes should pause to consider that this monotonous activity may be giving their children a head start in their education.
Add-on drugs fight insecticide resistance
ONE OF the insecticides recommended by the World Health Organisation for use against Culex and Anopheles mosquitoes is malathion, a fairly cheap and relatively safe organophosphate.
But malathion-resistant strains of mosquito are increasing.
Control authorities may respond by increasing the dose of insecticide, but they run the risk of exceeding ‘safe’ dosages and increasing environmental contamination.
This strategy also increases the selection pressure for genetically resistant insects — hastening the inevitable switch to a newer, possibly more dangerous, and certainly more expensive insecticide.
At the London School of  Hygiene and Tropical Medicine, work by Janet Hemingway on anopheline mosquitoes has shown that malathion specific resistance is inherited as a single gene controlling a carboxylesterase enzyme which breaks down the malathion to its monoacid.
However, addition of a synergist such as triphenyl phosphate (TPP) can overcome this resistance, probably by competing with the malathion and saturating the carboxylesterase enzyme (Pesticide Biochemistry and Physiology , vol 17, p 149).
Further work in collaboration with G. P. Geoghiou of the University of California has shown that organophosphate resistance in Culex quinquefasciatus , a common nuisance mosquito and vector of filariasis, can also be suppressed by the addition of synergists such TPP, DEF (a defoliant) or the fungicide Kitazin (IBP).
During selection with malathion and IBP, resistance gene frequency of a stock of Culex quinquefasciatus decreased at the same rate as untreated mosquitoes, whereas with malathion alone the resistance gene frequency increased fourfold in three generations.
Field applications of malathion and IBP could therefore delay the onset of resistance or even reduce the frequency of resistant genes already present.
Kitazin does increase the mammalian toxicity of malathion approximately eightfold, but even at this level malathion would remain amongst the less hazardous insecticides.
The thinning of the British hare
HARES are steadily disappearing from the British countryside, say researchers at the Game Conservancy in Hampshire.
Modern agricultural practices are mostly to blame, it seems.
Dr Richard Barnes, funded by the Natural Environment Research Council, and Dr Stephen Tapper of the Game Conservancy, have documented the decline by looking at hunting records.
Before 1965 hunters bagged 10 hares per sq.
km, on average every year, but by 1980 only five hares were shot for every sq.
km.
Declines in East Anglia are even more striking.
Predators and bad weather have helped to halve the number of British hares.
Hares make up some 20 to 30 per cent of a rural fox's diet, and foxes have increased by three-fold over the last 20 years.
But the main culprit seems to be modern farming techniques.
Barnes radiotracked a group of hares living on Hampshire farmland and found that the animals are very selective foragers.
A hare feeds only very briefly on any given crop — it will visit winter wheat, for instance, only in late February or early March, when the corn is still short and growing rapidly.
For the rest of the year it will look elsewhere for its food, timing its trips to turnips or spring wheats with equal  finesse .
Over a year a hare may travel over an area as large as 50 hectares, in search of the right food.
To satisfy its taste for diversity, a hare tries to pick an area where it has access to half a dozen fields.
Hares that cannot avoid areas with huge fields under monoculture compensate by establishing very large home ranges.
Barnes and Tapper find that a small change in the diversity of crops in an area drastically reduces the number of resident hares.
If some 60 hares live in a sq.
km that contains four fields, the removal of a single field will cause numbers to plummet to less than 20.
‘The tendency in modern arable farming to knock out field boundaries and devote fewer fields to grass seems to be largely responsible for the decline in hares,’ Tapper says.
Pesticides may also be contributing to the fall of the hare.
Hares (but not rabbits) are particularly vulnerable to paraquat, a herbicide often applied to stubble in the autumn so that it can be easily burned.
Hares are killed when licking fur with the herbicide.
But hares were declining before paraquat was introduced, and the effects of agricultural chemicals on wildlife are notoriously difficult to assess.
Isotope effect has astronomical implications
AMERICAN scientists have found an effect that may cast doubt on a whole body of research in the evolution of the Solar System, as well as making physicists look again at the subject of isotopes, forms of the same element that differ in the number of neutrons in the nucleus.
The conventional wisdom on isotopes of the same element is that they can a distinguished only in two ways: by the different masses of the nuclei, and by subtle differences in the way they absorb and emit light of specific wavelengths.
Both effects can be exploited to separate isotopes — for example in enriching uranium by increasing the ratio of uranium 238 compared with uranium 235, although the first is currently the most used for large-scale enrichment.
But now Mark H. Thiemens and John E. Heidenreich III have found a specifically chemical effect that allows oxygen atoms of different isotopes to be distinguished.
They have found that molecules of oxygen (02) split up at different rates according to whether the molecule contains two atoms of oxygen-16 or one atom of oxygen-16 combined with another of the rarer oxygen-17 or oxygen-18 isotopes.
A strange finding was that the rate was the same for all the ‘mixed’ molecules, regardless of what ‘unusual’oxygen isotope they were composed of.
Measurements of the resulting isotope ratios closely approximated those found in carbonaceous chondrite meteorites.
Unlike material from the Earth and other meteorites, carbonaceous chondrites have oxygen-isotope abundances which cannot be explained by purely mass-related effects.
The discovery of that fact in the early 1970s puzzled astronomers.
One proposed explanation was a supernova explosion near the Solar System as it was forming, injecting a large quantity of carbon-16 into the pre-solar nebula and thus skewing the oxygen-isotope concentrations in carbonaceous chondrites.
Thiemens says the new isotope effect provides a simpler explanation.
That explanation depends on the presence of molecular oxygen in the gas cloud that was condensing to form the Sun, something the San Diego chemists think was likely, although they cannot be certain.
Ultraviolet radiation could do the same job as the electrical discharge in their laboratory.
The Sun now emits ultraviolet radiation, but a young T Tauri type star would have emitted up to 10 000 times more.
Such a star would probably have been close at hand — it is considered to be an early stage in the evolution of stars like the Sun.
Strictly speaking, the isotope selection effect observed by the San Diego chemists is not the only one which can be independent of mass.
Photochemical effects also occur which rely on the absorption of a particular wavelength of light by Molecules containing one isotope but not by those containing other isotopes.
However, such effects require a laser light source producing only a very narrow band of wavelengths, a source not available in nature.
Existence of a natural isotope effect not dependent on mass complicates efforts to deduce the history of the Solar System by studying isotope ratios.
However, it does explain why measurements of silicon and magnesium isotopes yield results which do not agree with what would be expected from a supernova explosion.
Thiemens told a January meeting of the American Astronomical Society in Boston that similar isotope effects may exist for other diatomic gases such as hydrogen and nitrogen which, like oxygen, have one isotope much more abundant than others.
Such effects could account for shifts in the isotopic abundances of hydrogen and nitrogen in carbonaceous chondrites.
Thiemens said that it might even be possible to apply the effect to enrichment of certain isotopes.
Why aquatic plants have varied leaves
AQUATIC flowering plants have two distinct forms of growth.
Underwater, the leaves are thin and feathery.
Above the surface they are more like ordinary leaves.
There are other differences too — for example, aquatic leaves have fewer stomata — all of which make feathery leaves better underwater and stouter ones better in air.
Research has now revealed that the shape of the leaf is determined simply by the surrounding medium (Science , vol 219, p 505) Paula Deschamp and Todd Cooke looked at growth in Callitriche heterophylla , a common water plant.
They discovered that there were lots of things that would make the ‘wrong’ sort of leaf develop.
Giberellic acid, a plant hormone, caused shoots in the air to grow underwater — shaped leaves.
Abscisic acid, another plant hormone, caused  underwater shoots to grow aerial leaves, as did higher temperatures and when mannitol was added to the water.
The common factor to all the manipulations was a change in the water balance around the growing shoot.
Underwater, the turgor or pressure inside the cells, which is the difference between the osmotic potential bringing water into the cell and the water potential driving water out of the cell, is high.
This makes the cells very elongated.
and that in turn makes the whole leaf, elongated.
In air, osmotic potential is almost balanced by the water potential: the turgor pressure is low and so the cells — and leaf — are rounder.
Mannitol in the water lowered both osmotic and water potential, and made them almost equal, which is why the leaves adopted the form seen in air.
Giberellic acid causes cells to elongate by increasing the uptake of water, and so the leaves took the water form.
Abscisic acid and high temperature do not affect the hydrostatic pressure of the cell.
Instead, say Deschamp and Cooke, they probably make the cell less yielding, so that even high turgor cannot elongate the cell into the feathery form.
The shape of the leaf is thus very simply determined.
Merely the presence or absence of surrounding water ensures that the leaf will be suited to life underwater or on top.
TECHNOLOGY
Geologists probe deeper for new metals supply
BRITISH researchers have started a project that could open up to metals mining many areas of the world that are geologically unexplored.
The programme could make it much easier for geologists to discover deposits of minerals in rocks hidden beneath layers of relatively young sedimentary material.
By far the greatest proportion of the world's metals mines are in places where the relatively old mineral-bearing rock is near to the surface.
This is only partly because digging and operating deep mines is, in itself, extremely difficult.
No mine goes deeper than about 3 km, because heat and air pressure become too great.
The other reason for the scarcity of mines in areas where sediments have overlaid the older rocks is that the tools with which geologists look for deposits a the main, useful only when the deposits are near to the surface.
Workers at Britain's Institute of Geological Sciences aim, however, to come up with a new set of tools that can indicate minerals much further under the ground.
In a three-year project funded by the European Commission and the Department of Industry, the researchers aim to use a variety of methods to capture data about the Earth's rocks.
These encompass information on surface fractures obtained by remote-sensing satellites: details of rocks perhaps 2 km beneath the surface obtained by drilling bore holes; and anomalies in the Earth's magnetic field and gravitational force that are measured in instruments on board aeroplanes.
A further set of readings come from chemically  analysing soil samples collected from streams.
By themselves, the data may not be all that illuminating.
But the workers hope to marry them together with the aid of computer techniques to come up with sets of data ‘signatures’ that will indicate deposits far beneath the Earth.
The centre of the investigation is the area of the East Midlands around Nottingham.
At this point the layer of sediment is up to 2 km thick.
Underneath are the shale and granite deposits which workers think stand a good chance of containing metals.
It is these deposits which (due to  millenia of weathering and other natural forces) are close to the surface in Cornwall, the Highlands of Scotland and other areas of Britain which traditionally have been mined.
The institute's workers, based in London and Keyworth, are looking in the East Midlands for traces of tin, tungsten, lead, zinc and barium.
There is little chance that new mines will open in Britain to excavate these metals.
The mines would create havoc in built-up areas.
Also, in the UK, it is extremely difficult to find who owns the mineral rights for specific pieces of land.
So mining companies are reluctant to waste years — and much cash on legal fees — in determining to whom they should pay  royalties once they have found a deposit.
In many countries, this problem is considerably reduced because the state owns all the mineral rights.
But the institute's researchers think that the results of their work could hold equally in other areas of the world where fewer obstacles are in the way of mining companies.
One difficulty remains in that a mining venture would still need to find a way through to the deposit in a place where the sedimentary layer is not too thick.
But many of the layers of sediment are not at all uniform and may be relatively thin in places.
For example, the sedimentary rocks that form the top geological layer in much of southern Britain may be only a few hundred metres thick in a few isolated sites.
There is already evidence that mining corporations are interested in probing beneath the sedimentary rocks to find new deposits.
In South Australia, for instance, geologists recently discovered a source of copper hidden under such a layer.
Satellite data were very useful here.
Robots displace slide rules from the classroom
TIME was when a slide rule was the ultimate in classroom one-upmanship, but these days, it seems, no pupil should be without his, or her, robot.
That at least was the impression given by an exhibition in Birmingham last week at which schools throughout the country demonstrated projects in electronic control.
There were any number of cranes, cars and mechanical arms fashioned from the modern equivalents of Meccano.
Under the control of the ubiquitous BBC Microcomputer, these creations whirred through their paces, lifting  polystyrene chips out of boxes, drawing pictures and finding their way around.
Among the home-grown equipment were a number of cheap devices that are sold to schools.
The most famous is the BBC Buggy, a box with two wheels and several sensors.
The £120 Buggy, sold by a firm called Economatics, has light-sensitive cells, an infrared receiver, bumpers which sense collisions and a pen holder.
It can also be equipped with a mechanical arm.
The Buggy is connected to a BBC computer by an umbilical cable.
Under the direction of 13 different programs, it can read codes like the ones found on supermarket goods, play music, find and describe objects and draw maps of its surroundings.
Aimed at school physics departments is a device called the VELA.
This £175 black box, produced by Leeds University, records  measurements from pieces of laboratory equipment like digital thermometers, oscilloscopes, resistors, timers and so on.
Measurements are shown on a liquid crystal display.
Up to four devices can be attached to VELA at once.
The speed at which VELA operates makes it possible to record results from experiments that are difficult to measure any other way.
Apart from the battery of robots and electric motors under computer control, firms are selling gadgets for teaching the principles of microprocessors.
Scientific Systems makes what is called the Exploded View Computer, a £150 machine which displays its workings on a board of coloured lights.
The computer can be slowed down so that each step it takes can be followed on the display of lights.
The Birmingham extravaganza was organised by the Department of Education and Science, which has run a scheme called the Microelectronics Education Programme (MEP) for the past three years.
Junior education minister William Shelton took the chance to announce a further £9 million cash injection for the MEP, bringing total government funding to £20 million.
The programme trains teachers and produces software for use in schools.
It has 14 regional information centres scattered around the country.
Despite the large amount of funding that has been pumped into the scheme teachers are worried by the time it takes for materials and information to get through to them.
‘It's a very complex process,’ said one.
‘I am well aware that criticism is levelled at the MEP by classroom teachers who argue that too little has penetrated to the level of the teacher from the programme,’ said Roger Conibear, director of the programme's unit in the West Midlands.
‘But I promise the  the next 18 months will see the MEP having an effect.’
America rushes into optical fibres
COMMERCIAL fibre-optic telecommunications is barely half a decade old, but already companies in the United States are ordering massive quantities of ‘third generation’ equipment based on singlemode fibres.
Some observers predict that ground-based fibre-optic systems may take over many of the point-to-point telecommunications tasks performed by satellites.
But others are concerned that the telecommunications industry may be rushing to use novel types of fibre before the technology is proven.
The first two generations of commercial fibre-optic systems use multimode fibres, in which light is carried (along a central core some 50 micrometres in diameter) in a variety of possible modes.
The light pulses spread out as they travel: this limits the information the fibre can carry.
First generation systems use such fibres with  gallium -aluminium-arsenide semiconductor lasers producing light at about 0.8 micrometres.
The second generation uses a new type of semiconductor laser, made of a mixture of iridium, gallium, arsenic and phosphorus.
This laser emits light at 1.3 micrometres, a wavelength more suitable for glass fibres as attenuation and pulse spreading are reduced.
Single-mode fibres also operate at 1.3 micrometres, but they have cores only a few micrometers across.
Light can travel in only one mode, enhancing the capacity of the fibre to carry data.
 Courses for single-mode fibres can present problems.
It may be difficult to channel light into the tiny core of such a fibre.
But telephone companies building long-distance, high-capacity transmission lines have not been slow to take up the challenge.
Last  last year, AT&T showed it could transmit 432 million bits of information per second through a single-mode fibre system in Atlanta, Georgia.
Meanwhile AT&T Long Lines — the part of the Bell System which handles long-distance telephone lines — plans a 1500 km network of single-mode optical cable.
The project will be finished in 1985.
Even more ambitious plans for single-mode fibre systems were announced early this year by MCI Communications, one of AT&T's competitors in the long-distance telephone market.
The company plans to install fibre transmission lines along 6600 km of railways in the eastern half of the United States.
MCI has already signed contracts to buy most of the fibre for the network, which should be completed in 1987.
Such orders put into the shade the contracts British Telecom has placed for single-mode fibre.
The corporation is installing six lengths of such fibre, totalling about 400 km.
British Telecom plans to install about 100 000 km of fibre by the end of the decade, the great bulk of it multimode.
In the US, some observers point out that single-mode fibre has been produced only on a small scale and the technology is not proven.
The telephone companies may have ‘purchased something that doesn't exist’, according to John Kessler, a consultant in Newport, Rhode Island.
Such difficulties notwithstanding, fibreoptic systems could bring back to Earth some of the telephone communications that are routed through satellite links.
Optical fibres are ‘intrinsically superior to satellite radiocommunications for most applications,’ says one observer.
Automation comes home in Japan
After pouring energy into factory automation the inventive Japanese want to do the same for people's houses and flats.
A two-year research project has produced what is called the home bus system, a set of standards for wiring up the home with coaxial cable.
From a control panel in the living room, a householder would supervise any number of appliances, ranging from heaters to door locks.
The system carries signals in three frequency hands.
One is used for control messages, another carries TV pictures and the third high-speed data.
The universities and electronics firms participating in the project say the wiring scheme in each house will link up with digital telecommunications networks carrying messages between cities and individual homes.
The project is supervised by researchers working at the Kansai Electronics Industry Promotion Centre.
All this might sound far-fetched; but one firm, Sanyo, has already made a step in the direction of home automation.
The firm has developed an electronic chip that can bring piecemeal systems under unified control.
In this way, a mechanism for running the bath that automatically cuts off before the water overflows could be linked with, for example, a control unit for the central heating.
Sanyo is selling the £5 chip to companies such as house-builders and sensor manufacturers which then incorporate it in their own projects.
The electronics firm says it will make about 250 000 per month if the demand catches on.
And later this year, Sanyo plans to enter the business of home automation itself — albeit in a limited way.
It will sell a remote’ controlled lighting system that incorporates the new chip.
The system, incorporating a control panel similar to that used with TVs will take a householder's living room from pitch black to blinding light in sixteen steps
Wired-up libraries
BRITISH Telecom is developing a sound and vision library for use with cable TV.
In return for an extra payment, a subscriber will call up feature films, audio entertainment or computer software.
These items could arrive at night for use the next day.
The commercial TV stations are working on a similar idea using broadcast signals; meanwhile, the telephone corporation is seeking commercial partners so it can try out its idea.
In the plan, British Telecom would install videodisc players at the point in a cable system which links up subscribers TV sets.
To watch a firm, the subscriber would issue a set of instructions that loads the relevant disc onto the player.
The signals are then fed via the cable link to the person's home.
The result is that, for a fee of about £1.50 per hour, the subscriber receives a programme from a disc, but without having to buy an expensive player.
In a more sophisticated system, a subscriber can control the pulses from the videodisc player via a keyboard in his home.
A mail-order catalogue stored on the disc could be used in ‘teleshopping’..
Germany wheels on
The fastest railway wheels in the world are on a new  bogie developed by the German company Messerschmitt-Bolkow-Blohm.
The German Federal Railway set the record by running the  bogie , on a test bed, at 400 km/h.
The new  bogie does not have rigid axles.
Instead, two short stub axles are coupled together allowing the wheels to slip independently of each other.
An important application could be in low-speed urban rail systems, which often have tight corners.
Messerschmitt is developing another  bogie , with controlled slip and with wheel profiles designed for high speeds, which it hopes to test on a new inter-city experimental train at 350 km/h.
Wind of change
THE cold wind of technology could soon be blowing through Britain's pig sties — helping farmers to reduce animal mortality and so increase their commercial fortunes.
The draught comes from a machine called the Medata Blowaway Unit.
It directs a stream of cool air at the compartments of pig sties in which piglets normally nuzzle up to their mothers.
Often, when the sow lies down, it squashes sheltering piglets and suffocates them.
According to James Burnhill and Sons of Cleckheaton, the firm selling the unit, piglet losses with the new system can be reduced by up to 94 per cent.
The company has tested the mechanism on piglets reared by 300 sows on a farm near York.
In one trial, installation of the equipment reduced infant mortality from 10 per cent of the litter to 0.5 per cent.
With the £295 system a current of air is produced at piglet level around the sow when it is standing up.
But as soon as the animal sits down, a photoelectric cell switches on the air stream, so stopping the pig from getting cold.
According to the manufacturer's theory, piglets soon learn to avoid the air current and retreat to a warmer place in the  sty .
Monoclonal tests promise good news for mice
MONOCLONAL antibodies are evolving from a scientific technique to a true biotechnology.
The antibodies are sets of biological identity tags created by hybrids made from immune cells and tumour cells.
(Such fusions arc called hybridomas.)
Reports at the Second Annual Congress for Hybridoma Research in Philadelphia suggested ways to ‘sanitise’ the antibodies' manufacture, making this more like a bench process with a reduced need to involve live animals — or even live humans.
Monoclonals' great strength, in theory at least, is that they are remarkably ‘clean’.
They eliminate the need to purify the most useful of the wide range of different antibodies produced by an animal's immune system after an injection of foreign material.
However, methods for making monoclonals produce their own contaminants.
These arise when, for example, the hybrids are made in part from mouse cells and raised inside mice.
For relatively large volumes of antibodies, industry has relied on raising the hybrids in ascites (fluid in the peritoneal cavity) in mice, rather than in tissue culture.
But Stephen Fazekas de St Groth of the Basel Institute of Immunology has come up with a different method — an automated system in which a technician collects the antibodies off an ‘anti-immunologlobulin’ column.
Fazekas de St Groth calls his system ‘ascites without the mouse’; it is based on a method used for decades to Brow bacteria in suspension.
He raises his hybrid cells under a controlled atmosphere in large jars.
These contain a spinning mechanism that turns at a gentle 15 to 30 revolutions per minute — just enough to ensure convection and prevent cells from adhering to the glass walls of the bottle.
This allows him to grow cells at about twice the number per unit volume that will survive in tissue culture — and at just below the density at which they begin to die, poisoned by their own wastes.
Many researchers have tried to obtain a medium for growing antibodies that is free from calf serum.
A team from the University of California at San Diego has concocted such a medium in which a particular type of cell that causes cancer in mice grows readily.
These cells, called the NS I mouse myeloma line, are commonly used in making hybridomas.
The vital ingredient that serum provides to a medium is lipid, which cells need to maintain the integrity of their outer membranes.
A serum-free medium must contain something similar.
The medium obtained by the workers in California is based on oleic acid.
The acid is in complex with fatty-acid free bovine serum albumin and human low-density lipoprotein, added to an enriched mixture of standard cell media.
Workers have also turned their attention to obtaining all-human monoclonal antibodies.
Virtually all monoclonal antibodies today are made by hybrids of mouse cells or mouse-human hybrids.
Before fusing with a tumour cell, an antibody-producing cell (one known as a B cell) must be stimulated to react to the substance its antibodies will recognise.
It is easy to inject a mouse to stimulate its B cells, but injecting a human with something just to create monoclonal antibodies is another matter.
Several speakers at Philadelphia reported stimulating human antibody — making B cells in vitro , rather than in live humans.
Joy Cavagnaro and Michael Osband of Boston University have provoked antibody production in vitro in ‘naive’ human B cells — in other words those never before stimulated to produce antibodies.
They did this by tipping the scales in favour of antibody production — identifying and removing those cells known to suppress antibody production.
US sheds light on materials
INDUSTRIALISTS in the US hope that a new research facility providing high-energy radiation will recover for them lost ground in areas as diverse as microcircuitry and metals processing.
The $138 million National Center for Advanced Materials is planned for the Lawrence Berkeley Laboratory in California.
One of the centrepieces is a synchrotron, due to start up in 1989, that generates light beams some 10 000 times more brilliant than existing sources.
The instrument should provide new clues about the structure of materials.
‘We will be trying to anticipate the needs of industry some five to 15 years down the road,’ asserts David Shirley, Lawrence Berkeley's director.
And Earl Hyde, the number two at the laboratory, says that the centre is part of a ‘very big rethink’ within the government about how publicly funded laboratories can solve industrial problems.
The centre will comprise the synchrotron and three adjoining laboratories.
It is an answer to institutes in West Germany and Japan that pose a threat to American technological supremacy in some areas of industry.
Shirley wants American companies to become deeply involved in planning the centre.
He has discussed the venture with research groups from IBM, AT&T, Exxon, US Steel and Hewlett-Packard.
According to scientists at Lawrence Berkeley Laboratory, the proposals for the centre have surfaced at a good time.
The federal government has suddenly ‘discovered’ high technology, say the scientists, and is likely to back plans for the new institute.
To set the ball rolling, President Reagan set aside $35 million for the centre in his 1983–84 budget.
The synchrotron at the centre, called the advanced light source, will be the first such device designed for use with what are called wigglers and undulators.
These use magnetic fields to force electrons to ‘wobble’ as they speed along, increasing the intensity of the light.
The synchrotron will flash light every two thousand-millionths of a second.
The three laboratories built around the high-energy source will each have specific roles.
One will make and  analyse new kinds of polymers, ceramics and alloys.
Some of the materials will be important in electronics: for instance, researchers will work on new manufacturing techniques for gallium arsenide, which is used in highspeed chips.
The second laboratory will investigate surface science and catalysis, while the final member of the trio will develop instruments to probe materials and find new applications.
A 100 kW hybrid solar power plant (above) has entered operation in Meekatharra a remote town 750 km north-east of Perth in Australia.
The plant uses 960 sq metres of solar arrays built by the West German company MAN.
The $A3.6 million plant should save 100 000 litres of  diesel oil per year, worth about $A40 000.
In the plant, 50 kW of power is provided by recovering waste heat from the town's existing power system.
Critics in a spin over Flettner's ships
Ships driven by revolving metal pillars sticking up from the deck sounds like Victorian science fiction.
In fact, they enjoyed a successful but brief  existence 
Wolf Seufert and Ulrich Seufert FLETTNER ROTORS are tall, spinning cylinders mounted on a ship's deck.
They propel the vessel through the water using the same principle that allows a bowler in cricket or a baseball pitcher to swing a ball through the air.
The technology was invented in the early 1920s by Anton Flettner, a German naval engineer, and has since fallen into undeserved oblivion.
But before they faded from view, Flettner rotors proved to be a reliable form of wind propulsion during several years of full operation at Sea.
The stout cylinders rose from the decks of Flettner's amazing rotor ships to about two-thirds the height of the masts on a sailing vessel.
They were capped by flat discs and had a smooth surface without any semblance of an aero dynamic profile.
The cylinders were not driven by the wind but rotated in it.
They extracted power to propel the ships by a corollary of Bernoulli's theorem (which describes the principle of energy conservation for ideal fluids in steady flow).
As the cylinder turns, it is pulled to that side which accelerates the airflow (figure I).
The forces generated in this way are surprisingly strong.
Their origin was explained by the German physicist Gustav Magnus in 1852, who thus solved the vexing problem of why spinning projectiles veer off course.
A cylinder at rest produces lines of flow in a passing current of air (Figure 2a).
The pattern is similar to that produced by the flow of electrical current around a circular defect in a plate.
If the cylinder spins in a fluid medium at rest, under ideal conditions, it sets up a circulatory profile (Figure 2b).
The pattern is like the magnetic field produced around an electrical conductor going into the plane of the paper.
The superimposition of both flow movements gives a grid pattern (Figure 2c).
The streamlines of air, blown at a spinning cylinder, obtain from diagonals drawn into the grid since each cell approximates a parallelogram of forces.
The Magnus effect is more pronounced the faster the cylinder rotates.
The greater air speed produces a correspondingly greater pressure drop to which the cylinder is pulled, or lifted.
A spinning cylinder generates lift in direct proportion to the acceleration it imparts on the air streaming by.
Its efficacy is determined by the ratio of the circumferential velocity and the wind velocity.
The Magnus effect is familiar in sport.
A pitcher can spin a baseball to make it follow a trajectory in any direction.
A cricket bowler knows how to make the ball swing to or from the batsman by positioning its seam.
Tennis players use the ‘cut’ ball, as do golfers.
The same effect is at work in all these examples: aerodynamic lift deriving from the ball's rotation acts either vertically, affecting range, or sideways, affecting curve.
Flettner put the Magnus effect to work.
He commissioned the Aerodynamische Versuchsanstalt in Gottingen, Germany, a foremost centre on aerodynamics, to study the fluid profiles around cylinders at rest and formulated his theory of laminar flow.
Some engineers were sceptical about the efficacy and dynamic stability of rotors on ships.
Flettner argued that rotating cylinders on the deck of a ship should replace sails in favourable wind conditions and bring great advantages in handling.
At the time the rotor experiments were undertaken, Flettner was already famous and wealthy.
His name and fortune was based on the successful ‘Flettner rudder’, a freely pivoting rudder for ships actuated by a small control surface in a strategic position of leverage.
His reputation, as well as the meticulous and comprehensive work on the Flettner rotor done at Gottingen, were instrumental in finding the Germania shipyards in Kiel (owned by the krupp family) and the Hamburg-America Lines as partners.
They were willing to underwrite, in part, the construction of a ship with rotors as an auxiliary power source.
The experimental results that encouraged the financiers to support a rotor ship were truly spectacular.
In wind-tunnel tests, the performance curves of rotors (Figure 3) exceeded those of sails by far.
The surface that the rotor presented to the wind was only one-tenth the projected sail area.
Several of the rotor's advantages were immediately apparent: because of its axial symmetry the position relative to the wind is not at all as critical as with a sail.
The rotor presents the same frontal area even if the wind changes directions; only the direction of the thrust developed varies.
Furthermore, the lift generated will act in a diametrically opposite direction when the rotation of the cylinder is reversed.
And, rotors are smaller than masts and sails.
A vessel equipped with Flettner rotors gains in  manoeuvrability as lift is diminished by slowing a cylinder's speed of rotation.
Two rotors turning opposite ways can turn a ship on the spot.
Later wind-tunnel experiments were performed to convert the 460 tonne schooner Buckau to a rotor ship.
A model of the hull carrying the Buckau's spread of sail and an identical one with two rotors showed the immense superiority of the latter in all wind directions (Figure 4).
The most severe criticism was that wind pressure on the huge rotors could capsize the ship.
The aerodynamicists replied that, in fact, the resistance of the rotors grew more slowly, with an increase in wind speed, than did the Buckau's rigging.
The wind pressure on the rotors increased rapidly when they were rotated at full speed but levelled off at wind velocities beyond about 12 m/sec.
This meant that the rotor could be used in heavy winds and should not compromise the ship's safety.
The Buckau with rotors stopped appeared to be safer than the Buckau with sails doused.
We can also infer from the trends depicted in Figure 2 that gusts did not unduly affect the rotor ship's dynamic stability.
Since the lift generated depends on the ratio of two flow patterns (created by the cylinder's rotation and the wind acting on the cylinder) a sudden increase in wind speed while the rotors turn at a constant speed reduces the lift in proportion.
This compensatory effect produces a steady power curve.
Buckau is converted
Some arguments could be answered unequivocally only by operating a rotor ship.
They concerned principally profitability and factors such as optimal vessel size.
In spite of all the work done by Ludwig Prandtl and his colleagues, it was a daring step to commit the resources of the mighty Germania yards, in early 1924, to install two metal cylinders more than 12 m tall on the elegant schooner Buckau, so soon after the idea had ignited Flettner's imagination in 1921.
The outer shells of the Buckau rotors sat on pivots that came to about two-thirds of their height.
They had a diameter of nearly 2.7 m and were topped by discs a little more than 3.6 m across to prevent the pressure gradient spilling over the top.
Two electric motors below deck, delivering a maximum 11 kW each from a 220 V direct current generator, drove the rotors independently at a peak 125 rpm.
Every part of the installation was accessible.
Particular attention had been given to the design of heated support bearings to prevent an increase in their mechanical resistance when the lubricant stiffened with a drop in temperature.
At three tonnes the cylinders were relatively light.
In all, the Buckau weighed only 20 tonnes more after her conversion.
Sea trials began in November 1924 and were everything the wind-tunnel experiments had promised.
As reported by the chief naval engineer of the Germania yards, the rotor ship handled beautifully and was able to tack a full 20 degrees closer to the wind than a commercial sailing ship.
She could turn on the spot by rotating the cylinders fore and aft in opposite directions and at different speeds.
The helmsman and the engineer controlled her easily and she reacted without hesitation to all commands, naturally much faster than a sailing vessel.
Her loading deck was unobstructed by staying or control ropes and wires.
Doubts concerning the dynamic stability of a rotor ship were eliminated with further trials.
Could a rotor's circumferential velocity be maintained in gales at the ideal ratio: 3.5 times that of the wind?
Would the wind pressure on the cylinders become too high?
The pressure bearings into which the rotors were set had been designed to withstand the strongest winds with a margin of safety.
If the ship should become threatened by strong winds, the rotors were simply stopped and their resistance dropped drastically.
In light weather, she was handicapped only if driven by her rotors alone, and only in comparison with a fully rigged sailing vessel dead before the wind.
The rotors were intended as auxiliary sources of power, and, as such, to be run only in an envelope of conditions useful for their operation.
They were able to harvest energy from the relative wind produced by the ship's screw-driven movement through a calm.
As had been realised already in the model experiments, a rotor ship would not make full use of a wind blowing directly from astern.
This was offset by the fact that the ‘Buckau’ could sail, as predicted, approximately 20 degrees closer into the wind than commercial sailing ships.
They were usually limited to a tacking angle of 50 degrees.
The Buckau's first series of trials at sea as a rotor ship was completed successfully in January 1925.
She had done a total of 62 test runs, past the measured mile up and down the Baltic Sea coast near the port of Kiel, with a full load of coal (350 tonnes).
In 47 of these tests, the rotors were used either alone or to supplement the power delivered by the ship's 120 kW Diesel engine.
When the Buckau was driven by her screw only she reached a top speed of 14.5 km/h, with rotors alone 14.3 km/h and with both systems running, 18.5 km/h.
Buckau enters service
The rotor's operation added consistently more than 15 per cent to the ship's speed, on all headings before and against the wind.
In favourable conditions, the rotors were very effective power sources indeed: running more than 55o upwind.
The Buckau once reached a speed of better than 7.4 km/h on rotor power alone, about half her design speed of 16.5 km/h.
Maximum rotation was 120 rpm and the rotors delivered up to 27 kW.
These data do not give the ship's full performance but are experimental results collected under a variety of real operating conditions.
The rotors were often run at a circumferential velocity only slightly higher than that of the relative wind, that is at far less than optimal efficacy.
The little ship had given remarkably good results and was immediately put into service to carry bulk cargo across the North Atlantic and the Baltic Sea.
Her ‘revolutionary’ form of propulsion and distinctive profile attracted great attention when she visited American waters in 1926, after a refit, under her new name Baden-Baden.
Her performance left no doubt as to her reliability and sea worthiness.
Impressed with the Buckau's success, the German Admiralty commissioned the Weser Shipyards in Bremen to build a new, larger rotor ship on a proven full design.
The Barbara was launched in 1926; two 6-cylinder Diesel engines in series generated a total 745 kW to give her 2840 tonne gross a maximum design speed of 19 km/h.
Her three rotor towers were taller and wider but very similar in construction and weight to those of the Buckau, thanks to the use of a new light aluminium alloy for their mantles.
Independently controlled electric motors turned the rotors at a maximum of 160 rpm.
All three rotors fed on one single Diesel generator with an output rated at 45 kW only.
Full data on the Barbara's performance in service do not seem to be available any longer.
She carried perishables under the colours of the Sloman Line from the Mediterranean to Germany, at least until 1929.
Brief enthusiastic accounts surfaced in the popular press every now and then, offering highly suspect details on her ‘phenomenal’ sailing characteristics.
By a conservative appraisal of the few data at hand, she was an engineering success.
The speed gained by operating her rotors from a paltry 45 kW source was the equivalent to that obtained had another Diesel of 370 kW been coupled in series to the existing two.
In spite of all, she ultimately disappointed her owners who could not amortise her high capital costs.
In the early 1930's, both the Buckau and the Barbara had their rotors taken off and they went back to work as ordinary motor vessels.
The principle of propulsion by Flettner rotors suffers still from their ignominious fate.
The ships were taken out of service in circumstances entirely different from the set of priorities and considerations that apply today.
One forgot in the early enthusiasm that the rotors had never been intended to be a ship's prime movers, too much was expected of them.
Fuel was considered expensive then, but the high capital outlay required for the construction of a rotor ship was far more disconcerting.
The great depression came along.
The rotor concept was condemned to die as innovations are never well received in difficult times; when they are needed most.
Even very recent reports on the relative profitabilities of wind propulsion systems for ships dismiss the rotor as inferior to sail designs, improved by sophisticated technology.
These conclusions were often reached on the basis of incomplete or wrong information.
The few existing technical articles on rotors were written in German more than 50 years ago and are not readily accessible.
Prandtl's conclusions as to the efficacy of the Magnus effect in Flettner's application were confirmed only recently by the results of an extensive study done carried only recently by the results of an extensive study done at the University of Dayton's Research Institute.
It is to be assumed that his results on the Flettner rotor are as valid.
Shall we see commercial sailing vessels ply the seas again?
Exhaustive computer simulations done in the US show that even at the present prices for fuel, conventional sailing ships are only marginally competitive when all is counted.
Sails to supplement a ship's principal engines, not to replace them, could help us save energy where it is easiest to harness the wind, namely at sea.
Anton Flettner's way of extracting power from the wind presents too many advantages to sink into oblivion.
Ten years to slake a global thirst
The world has committed itself to providing water and sanitation for all by the year 1990.
It will not reach that target without the right hardware
Anne Charnock
SIX YEARS AGO, at a conference in Argentina the United Nations set a target for the 1980s: clean water and sanitation for all.
The 1980s became the International Drinking Water Supply and Sanitation Decade.
The need for the decade is only too obvious.
Four-fifths of diseases in the Third World are linked to dirty water and lack of sanitation.
Among the many on the list are the blinding disease Onchocerciasis, or river blindness, caused by a parasitic worm; the debilitating schistosomiasis, also caused by a worm; the killer malaria caused by a protonoan and of course diarrhoea, mainly caused by bacteria, which kills something like 20 000 children every day.
Three out of five people in the Third World have no easy access to clean water.
In rural areas, the figure drops to one in three.
Women and children often spend most of their day collecting water; in the hilly areas of Kenya women spend nine-tenths of their time on this single task.
Attempts to do something about the problem have to concentrate on underground resources.
The world has about 3000 times as much groundwater as it has water in rivers and lakes, and groundwater is far cleaner.
The problem is getting it to the surface, and in rural areas the best way is by handpump.
International aid agencies, such as UNICEF, the UN children's fund and the World Bank, are now striving to find a design for a handpump that is cheap and easy to maintain.
For sanitation, the statistics are even worse.
0nly one in four people in the Third World has any kind of sanitation system.
In some areas, ignorance and taboos make it difficult to introduce new customs.
Education, and the participation of the community are crucial if schemes are to have any chance of success.
But above all, engineers are only slowly acknowledging that many Western solutions, principally water-borne sewerage systems, are not the right answer for many parts of the Third World.
The UN has set a tough target for developing countries to reach.
Although many are taking the decade seriously (Africa is doubling its spending) no one pretends that the world will reach its target.
That would entail providing half a million people a day with new or better services, The cost: between $300 billion over the 10 years.
Current spending is around $6 billion to $7 billion a year.
The world could pay for all this by redirecting 10 days military spending, but this is an unlikely prospect.
And the climate of recession and public spending cuts in the US and Europe has hit overseas aid.
The only way out is to adopt low-cost technologies, which local manufacturers can produce and which villagers can maintain.
The aid organisations realise that the task is too great to embark on any other course.
WHEN HANDPUMPS are installed this year in villages in Malawi, Pakistan and the Philippines, thousands of people will have their first chance of leading a healthy life.
It is likely to be their one and only chance.
When handpumps break down, they can stand idle for want of maintenance, not for a week or two, or even for a few days, but for the lifetime of all the villagers.
Developing countries are littered with the rusting remains of unsuccessful projects which have left communities with little choice other than to resume old lifestyles.
The women return to their daily treks to distant rivers, springs or canals to collect polluted water.
For the whole community this marks the return of infections and, for many, a premature death.
But in the eyes of government departments, trying to meet the goal of ‘clean water for all by 1990’, those villages have had their share.
They stay on the statistics as villages with ‘adequate water supplies’.
The same goes for sanitation schemes.
If the designers of pit latrines ignore local customs, or allow the latrines to become fly-ridden and smelly, people will stop using them.
They will return to the fields, rivers, canal banks or waste ground.
Unfortunately, governments in the Third World are buying unreliable technologies and poor advice in the guise of expert opinion.
At the extreme, rogue manufacturers are trying to cash in on this huge market, and have won multi-million pound orders for technologies that may be sound as pieces of machinery but have not been tried adequately in the field.
Britain's Consumer Association, which has been testing handpump designs for the UN and the World Bank, has identified some of them.
Donald Unwin, chief engineer at the association's Testing and Research Laboratory, said ‘Many manufacturers just do not look at the problem through the eyes of the Third World countries.
Some are completely ignorant.’
Manufacturers in five countries, Britain, Canada the US, Japan and the Philippines, have been selling farmyard pumps, which the association says have ‘failed miserably’ because they cannot stand up to all-day use.
Unwin singles out the Sumber Banyu pump, which an Indonesian firm, P. T. Celco, makes to a design by the ‘Battelle Institute in the US.
The pump is built to exacting tolerances, requiring high standards of quality control.
But most workshops in developing countries cannot meet these tolerances, ‘so the spares simply don't fit,’ says Unwin.
In Lesotho, the US Agency for International Development is paying for the installation of Robbins & Myers pumps, made in the US.
Their underground components, an  Archimedean screw with a complex helical rotor and stator, need very special manufacturing techniques.
‘They don't often go wrong, but if they do they must be replaced.’
Unwin said.
Local mechanics cannot do the job, so once the US experts have left, the government will have the burden of spending limited foreign currency to buy spare parts from abroad.
But equally to blame are governments that commission consultants to design over-sophisticated schemes, which are bound to fail, with cash coming from aid organisations.
This occurs not so much because the engineers are callous, but because of a blinkered approach by all parties.
The water decade has succeeded in forcing engineers, both in consultancies and in governments, to question whether their designs and ideas are meeting the needs of people in developing countries.
Engineers have invariably passed through Western universities and polytechnics, which train them more to run projects for the Thames Water Authority than for undertakings in the Third World.
Foreign students who return home rise through the ranks quickly, but are often more familiar with activated sludge treatment, as used in Britain, than with simple ponds for the stabilisation of waste.
However, most engineers now agree that low-cost systems are better.
This means that, whenever possible, open wells and handpumps should take priority over the more expensive piped water supply schemes (see Box B).
Phrases such as‘use low-cost methods wherever possible’ pepper terms of reference for designs.
But just as crucial, if any new project is to avoid dereliction, engineers must investigate maintenance down to the most minute of details.
Handpumps provide the classic example of what can go wrong.
Until recently, most government handpump schemes had next to no provision for maintenance.
Bearing in mind that in many villages there is little chance of finding even a monkey wrench, it is easy to see why speedy repairs are often impossible.
In any case, with most handpump maintenance, workers need a rig to lift the pump head off its base.
And travelling maintenance crews may not turn up from one year to the next because of the immense areas they have to cover.
This catalogue of repair problems is forcing engineers to redesign the well-head and borehole components of many existing pumps (see Boxes A and B).
Worn bearings, gaskets and seals go unreplaced because there are no spare parts.
Leaking oil and spilled water around the wellhead percolates through to the groundwater, carrying pollutants.
Welds split because of continual use, and poorly constructed boreholes let sand and grit into the well water.
Many of the new designs make a quantum leap in the choice of materials; more and more parts are being made of plastics.
This may appear heretical in terms of‘appropriateness’, but plastics are cheap, light and easy to replace.
More to the point, many developing countries have manufacturers of injection moulded plastics which can make well casings and pump components to specification.
More futuristic, but by no means on the lunatic fringe, solar energy is now generating power to pump groundwater.
The UN Development Programme has paid for tests on existing photovoltaic systems, and a British consultancy, Sir William Halcrow and Partners is carrying out field work in Sudan, the Philippines and Egypt.
Intermediate Technology Power, part of the British Intermediate Technology Development Group, is developing the machinery.
The consultants reckon that pumping water from depths of 20 metres would cost less than 50 cents per cubic metre with solar power.
Considering that in Tamanrasset in the Sahara people are prepared to pay water sellers $1 a litre (or $1000/m3), photovoltaics are already economically feasible.
However, according to Peter Fraenkel of IT Power, existing systems need to be more reliable.
Solar power introduces another step where breakdowns can occur, but the cost of maintenance is far lower than with diesel pumps.
Diesel pumps need daily attention: starting up, checking oil levels, and so on.
However, Fraenkel says engineers are realising that wind power is more immediately cost-effective for pumping groundwater.
‘If wind speeds are over 3m per second then it's crazy to think of anything else.’
Inadequacies of design soon reveal themselves in even the simplest of hand-dug water wells.
If the edges of the well are not protected, the well head becomes a pool of mud and a cycle of parasitic infections, such as guinea worm and hookworm, quickly develops between people and water.
Simply building a wall around the top of the well breaks the cycle.
Another simple solution, which Loughborough University's department of water and waste engineering suggested at a water decade conference in Madras last year, is to put a pot chlorinator in each well.
These chlorinators are earthenware pots containing a mixture of coarse sand and bleaching powder.
The pot hangs on the end of a rope and when in contact with the water makes a disinfecting solution of chlorine.
Sanitation causes more controversy than the problem of water supply, possibly because it affects people in cities.
If a city cannot dispose of its sewage properly, it will pollute water, and negate any progress made on that front.
For many technology-conscious officials in developing countries, getting rid of sewage means installing underground pipes leading to treatment plants on the western pattern.
City authorities from Calcutta to Cairo have laid sewers, but they are too expensive to keep up with the growth of the cities.
They work in the central and usually wealthy areas; people in the shanty towns do without.
What enthusiasts often overlook is that western communities do not always depend on sewer systems.
Many Americans, for example, have individual septic tanks, and in only a few countries, such as England and West Germany, does sewerage dominate sanitation systems.
Transferring the twentieth-century technology of sewerage is fraught with technical problems.
Some cities, such as Dar es Salaam in Tanzania, do not have enough water, either from rain or from industrial waste, to flush the sewage through the network.
So the sewage goes septic, giving off hydrogen sulphide which corrodes the pipes and makes a nasty smell.
Engineers now realise that sewerage cannot be technically viable without a large domestic water supply.
And the cities of the Third World often have unpaved roads.
Dust, sand and grit get into the sewers.
Within a few years, the pipes begin to clog.
The inhabitants of Cairo often remove manhole covers and dump all sorts of rubbish, from old car tyres to building rubble, into the sewers.
Regular clearance is crucial, but allocations of aid and government budgets rarely take the cost of maintenance into account.
Cairo's city authority is only now training its sewer gangs properly.
As well as keeping the pipes clear, the gangs are learning to avoid accidentally connecting sewers to water pipes.
The higher the technology, the greater the pitfalls.
The Ugandan capital Kampala has seen the devastating effect that civil war has on a sophisticated engineering scheme.
Nothing is maintained, sewer networks, water pipes, or treatment plants, so health hazards have flourished.
What is the alternative?
At one extreme, officials regard traditional latrines as primitive and unhealthy (Box C).
But new-wave sanitation experts say sewerage offers little more than convenience when compared to well thought-out latrines.
The UN decade has encouraged engineers to think seriously about this age-old system.
The two problems they have had to tackle are ventilation and structural safety.
In the large Ethiopian and Eritrean refugee camp at Towawa, near Gadaref in eastern Sudan, where 20 000 refugees have lived for over two years, parents stop their children using latrines because of the risk of the ground caving in during the rainy season.
In old sections of Sudan's capital, Khartoum, stories abound of latrines collapsing, with people falling through ancient crumbling squatting slabs to unspeakably horrible deaths.
Yet latrines can work extremely well.
The island of Lamu, off the Kenyan coast, has 600 years experience with pit latrines in its densely-populated harbour area, where as many as 260 people live in each hectare.
There, pit latrines inside homes take pride of place, their arched entrances lavishly embellished with stone carvings.
They are well maintained, and there are no problems with smells.
Workers known as ‘fundi’ carry out the specialised three-day task of emptying each pit.
The fundi seal off the street alongside the house, and dig a hole outside the latrine.
When they are sure the hole is deeper than the pit, they break through the lime and coral wall of the pit and the contents run into the hole.
The fundi then backfill the hole to street level.
By the time the latrine needs emptying again, in about 15 years, the sludge will have decomposed and can be dug out and spread on neighbouring farms.
It sounds an unsavoury task, but it is less unhealthy than the job of ‘night soil’ collectors who in many countries spend their nights emptying buckets.
Lamu's latrines stand as a challenge to the old engineering belief that sewerage is the only feasible system of sanitation for crowded cities.
Yet the latrines are illegal, because Kenya's building regulations, written in colonial days, forbid them inside houses.
The World Bank has published a guide to help nonspecialists choose a sanitation system.
A sanitation field manual sets out the different types of low-cost sanitation.
It prompts planners and engineers to ask questions such as, ‘Are there water taps in the houses to be served?
Are plot sizes large enough for septic tanks and soakaways?
Is there a strong social preference to re-use excreta?’
Some questions may be obvious, but, as the authors explain, ‘Few engineers are aware of the need to consider the socio-cultural aspects of excreta disposal.’
Despite this failing, a clearer understanding of all these design issues is emerging among donor agencies, private firms and government ministries.
But there is still the temptation for designers to lean toward high-technology schemes — partly for prestige, partly because their fees might be higher.
And corrupt officials have more chance of a rake-off when expensive goods are involved.
Aid donors, too, lean toward capital intensive projects.
Administrators are keen to sign supply contracts with Third World governments for expensive desalination plants, mechanised sewage treatment works (as in Kampala) and to agree on projects that will steer business toward foreign designers and contractors.
The British and US governments have told their aid agencies that they see aid projects as a source of export orders.
Britain's Overseas Development Administration spent nearly £75 million out of its gross aid budget of £1029 million for 1981/82 on water and sanitation projects.
However, of the 44 loans, £50 million went on just one scheme — Cairo's sewers.
Of the rest, 36 projects had less than £1 million.
half of them less than £100000.
No one is saying that Cairo does not need the repairs, but the disparity does show the imbalance that arises when aid goes to urban, high-technology programmes.
Money from multilateral agencies is also in short supply.
The World Bank's support for the decade's first two years fell short by as much as $800 million on a planned spending of up to $2000 million.
President Reagan's delay in replenishing the bank's soft loans affiliate, the International Development Association, caused much of the shortfall.
The association lends money to the world's very poorest countries.
The World Health Organisation has called for individual developing countries to commit themselves to decade programmes.
India, for example, has set itself a target of providing 80 per cent sanitation in towns, 25 per cent in rural areas, and water for all.
A tall order in itself.
For community self-help programmes, Malawi in southern Africa stands as a shining example, having completed well over 30 piped water schemes with the willing toil of rural communities.
Malawi can make good use of mountain water by piping supplies under gravity to lowland settlements, at a cost of £3 per person.
All these schemes begin with a public meeting which local chiefs and members of parliament attend.
The village chiefs ask their people if they are willing to work on the water projects, and once agreed a project committee is set up carrying the authority of all involved.
Since 1969, Malawi's self-help schemes have laid 300 km of pipes, supplying water for over 640000 people.
Once fully exploited, these mountain resources will be able to supply one third of Malawi's rural water needs.
‘Low cost’ and ‘self help’are therefore the buzzwords of the decade.
Money and skilled manpower are the main constraints.
During the next seven years, requests for assistance will undoubtablyoutstrip the  resources of international agencies, and UN officials are already taken aback by the response of the Third World.
It will be up to the engineers to spread what resources there are as far as possible.
A: Water, water (almost) everywhere
GROUNDWATER is a massive resource but it cannot be found everywhere, and sometimes lies too deep to exploit economically.
Roughly speaking groundwater can be extracted in useful quantities from depths up to 800 m below the earth's surface.
Groundwater is found in aquifers (waterbearing rock) or overburden (sedimentary rock over mineral deposits).
It fills all the interconnected voids, and the volume of water depends on the porosity and permeability of the rock.
Sandstone, for example, can hold very large volumes of groundwater because spaces are formed between the rock's rounded quartz grains.
Similarly, porosity can be high in stream-laid gravels and sands.
However, if the sands are mixed with silt, porosity may be low because the fine silt particles tend to clog the void dense igneous rocks can hold and transmit water if they are fractured, although the pore space is negligible.
The water table is the upper level of the  roundwater zone, and slopes in rough conformity with the surface topography.
Occasionally the water table intersects the earth's surface, forming streams and lakes.
However, irrigation can raise the water table artificially and, as happens in Egypt, can bring groundwater very close to the soil surface.
The depth of the water table determines how to get water out.
If the water table is within 15 m of the surface, the cheapest method is to dig a well by hand and protect the walls from collapse by building a masonry lining.
Water comes up in a bucket.
Alternatively, a handpump at the top of the well does the job.
In most shallow wells the pumping mechanism sits at the well head (which allows easy maintenance), and water is ‘sucked up’ through a rising main.
Extracting water from deeper aquifers involves drilling a well and lining it with slotted steel or PVC pipes surrounded by a packing of pea-sized gravel.
The perforations allow water to enter the well, and the packing reduces the amount of sill carried with it.
To deliver water from these depths the pumping machine has to be at the bottom of the well.
A series of connecting rods links the pump handle with the piston pump situated below the water level in the borehole.
However, in many developing countries geological data are often incomplete.
And because drilling is expensive, hydrogeological investigations must be done before water supply projects can start.
B: Quest for a better handpump
THE WORLD BANK and United Nations Global Handpump project is grasping the mettle of pump design by testing many best-selling models in the field.
Among them are the India Mark II, Britain's Consallen, the Madzi pump (a modified version of the Zimbabwean Blair pump) and also the lesser-known but very promising Malawi handpump.
The testers hope to develop from these best-sellers a near-ideal ‘village level operation and maintenance’ pump — which decade followers have long dreamed of.
In other words, they are  trying to develop a pump that a village caretaker could maintain with a single spanner.
Much of the field work is being carried out on the Upper Livulezi in Malawi.
Around 200 pumps are now in place and during the next three years villagers will learn to maintain them.
At the same time, project staff will redesign and incorporate new parts for the borehole components, irrespective of the type of pump they serve.
According to David Grey, working for the Malawian Department of Land, Valuation and Water, under a British technical assistance programme, there is no point in improving the design of pump heads until problems are sorted out with the boreholes.
Engineers must prevent sand and grit from getting into the well water.
Until recently, all Malawi's boreholes had heavy steel well cores, but the department has introduced substitutes with the help of a local maker of injection moulded plastics.
These plastic well casings are not only cheaper but have a greater number of slots cut in them than steel easings.
As a result the casings'’ open area’is increased a hundredfold, allowing more water in and, because the slots are much narrower, keeping more grit out.
None of the boreholes installed properly during the past 18 months has broken down.
For Malawi, the pilot project is a novelty.
for the first time the government is encouraging communities to take part in a groundwater scheme.
As Grey's colleague John Chilton says, the villagers will take more care of their new systems if they feel they belong to them.
Backing up this policy, the  department has delegated all community liaison work to Malawian professional staff.
Project manager Francis Msonthi, a geology graduate with just two year' experience, is therefore in charge not only of drilling rigs, well-digging teams and maintenance workers, but also travels around the district discussing the department's plans for water supply.
When siting the pumps, he explains, the community has the final say.
For example, people would rather walk further than have to carry water pots a shorter way uphill.
None of the team will say at this stage which of the pumps appears best, but the Malawi pump is certainly popular.
The villagers find it easy to use because of the ‘T’ piece at the end of  the handle, and also like it because the pump produces a good flow of water.
More important for maintenance, however, is that there is no need for a winch to lift the pump head off its foundations.
In just 15 minutes the connecting rods, piston and foot-valve can be pulled by hand through the pump head.
The engineer gets in by unbolting the top plate.
However, Grey wants to see modifications in the connecting rods.
At present the steel rods screw together, but in future they will have hinged connections so that no tools will be needed to remove the rods from the borehole.
Equally important, this redesign  would make repair work a one-man operation.
Beyond that, there are plans to make the pump pistons and footvalves in plastics and to devise a system so that when the connecting rods are removed they also withdraw the piston and footvalve.
The Upper Livulezi project will supply 30 000 people with water for only 6 Kwacha (about £4) a head — far less than other aid-financed groundwater schemes.
A part of this can be credited to the team's frugality.
For example, only one tractor serves four mobile drilling rigs, though one per rig is the norm on many projects.
Expatriate vehicles are kept to an absolute minimum, and site offices are merely caravans.
According to the Malawian government if three large-scale projects were initiated along the lines of the Upper Livulezi pilot project, each serving up to 25 000 people Malawi could get somewhere near the decade target.
But, of course, donors are in short supply.
C: Latrine designs in regular Third World use
If water is easily available from a yard tap, a water-vendor or a public standpipe, then a pour-flush toilet is appropriate.
It involves simply flushing with water, poured from any container.
This system is used with a septic tank and soakaway if space is available.
Advantage: the toilet is built in the house and the U-bend trap prevents smells.
This system can be upgraded when more money and/or water is available by connecting the sewers or installing a cistern-flush toilet.
As with the composting toilet and the pit latrine no water is needed with this system.
The pit is completely off-set.
Disadvantage: the chute can become fouled and therefore should only be built if the users are willing to keep it clean.
Advantage: the closet is larger than a pit and therefore lasts longer also it is easy to empty.
In parts of southern Africa individual units have been in use for over 20 years.
 Ventilated improved pits: One of the oldest and cheaper forms of sanitation, the pit latrine can be upgraded by increasing ventilation.
A pipe from the pit is fixed to the outside of the shelter.
Air inside the pipe heats up creating an updraught through the pipe and a corresponding downdraught from the shelter into the pit, reducing the problem with smells.
Sunlight at the top of the vent pipe attracts flies from within the pit and if a gauze is fitted over the top they fall back and die in the pit.
A ‘double-pit’ latrine can be used which employs a common superstructure.
Advantage: relatively cheap,
Aquaprivy: With this system the squatting slab is positioned directly over the septic tank and the inlet pipe extends below the water level in the tank.
Advantage: the toilet is within the house and could in the future be connected to sewers rather than a soakway.
Disadvantage: regular topping up is essential.
Double-vault composting toilet: If people wish to reuse excreta in their gardens or to sell it to local farmers composting toilets can be built as long as users agree to follow strict rules of maintenance.
Ash and biodegradable waste must be added regularly to achieve the right carbon nitrogen ratio.
When ¾ full, the vault must be sealed with earth and the movable superstructure positioned over the second vault, which has been emptied and its contents used on the land.
Advantage: the DVC can be converted to a pour-flush toilet with one vault used for excreta and the other for household water waste (sullage).
REVIEW
Critical year's of the revolution in Earth science
The road to Jaramillo by William Glen,Stanford UP.
pp 459, $37–50 
Robert Muir Wood
SOMETHING happened to the old rock and mountain studies in the mid-1960s.
To put it simply, geology died, Earth sciences was born.
A transfer of power and of objectives took place that has all the makings of a revolution.
The origins of this transformation may be traced back into the late 19th century but the upheaval finally came at the time of Vietnam, flower-power and the campus revolutions.
William Glen has approached the revolution in Earth science from a most specific vantage point — that of the University of California at Berkeley.
Here he has learnt the craft of the historian after training first as a geologist, and here he has written his doctoral dissertation on which The Road to Jaramillo is based.
The place and the biography are needed to locate the viewpoint, because Glen collected much of the material on his doorstep in interviews with scientists from Berkeley.
The story of plate tectonics, the inspiration of the revolution, was the eventual weaving together of a whole series of disconnected threads.
The most important of these, at least as it appeared to those involved, was the discovery of sea-floor spreading, which was transformed from a brave conception to a probability in 1963.
The mid-ocean spreading ridge was shown to be a ‘tape recorder’ for the planet's magnetic pole reversals.
The ocean floor was the tape slowly and symmetrically drifting away from either side of the ridge.
When this unlikely idea was first launched in England and Canada, the exact pattern of the past few million years of pole reversals had yet to be charted.
At Berkeley, as elsewhere in the US, there was ingrained hostility to continental drift, and these early proposals were not taken seriously.
The story of the creation of the time-scale of magnetic reversals (the Jaramillo Reve provided the final entry in the time-scale) goes back to the early 1950s when the scientists at Berkeley perfected the detection accuracy of the potassium/argon method of rock-dating for samples less than a million years old.
The success of this development ensured that Berkeley won the race to construct the magnetic reversal time-scale.
When in 1966 this confirmed sea-floor spreading it came as a revelation — a revolution.
These were the critical years, but it is arguable that this was the critical place.
By choosing to write from interviews, Glen is being more of a journalist (in the great American style) than a historian.
Yet this is the first serious attempt to write about the revolution since the heyday of the early 1970s.
In contrast to the books written then,The Road to Jaramillo is full of insight into the interactions, communications, thoughts and impressions that make scientific problem solving enlightening.
This personal detail displaces the  historical overview; the most creative, external aspects of the story of sea-floor spreading are relegated to footnotes, appendices and an extra chapter.
This bias is more unfortunate because Glen writes well: even the footnotes are rewarding.
One only wishes that he had spent more time gaining a perspective outside America.
For while the Berkeley geomagnetists walked the road to Jaramillo other scientists had travelled there on the power of imagination.
Women's place in the machinery
Women, technology and innovation edited by Joan Rothschild,Pergamon. pp 88, £9.50 
Ursula Huws
IN THE HOME, as in industry, it is overwhelmingly men who are the designers and repairers of machines, while women are the operators, expected to perform neatly and swiftly on typewriters, sewing machines and the rest, but not to understand how they work.
The power that this greater control over technology gives men lies somewhere near the root of many of the problems which beset women both inside and outside the workplace, yet until recently it is a subject that has been strangely neglected.
Women.
Technology and Innovation is therefore to be welcomed as a first step towards changing this state of affairs.
However anyone looking to it for an explanation of how women have come to be excluded so completely from the control of machines, or even for a theoretical framework within which to pose such a question, is in for a disappointment.
The book's origins as a special issue of the journal Women's Studies International Quarterly are all too apparent and it offers a series of partial glimpses rather than a coherent overall picture.
The first three pieces are celebrations of women's past achievements as inventors: in pre-agricultural societies, in ancient Alexandria and in American Shaker communities.
The last five are one or two page descriptions of women's studies courses, all in the United States.
For analysis of women's relations to technology in the present day, the meat at the centre of this sandwich, we are left with just three articles.
The most disappointing of these is the only British contribution to this otherwise all-American anthology, a study of the impact of word processors by Erik Arnold, Lynda Birke and Wendy Faulkner at the Science Policy Research Unit at Sussex University.
After an extensive survey of the literature on patriarchy, technological change and the clerical labour process (already much reviewed in this context), they leave us with neither new evidence nor any fresh insights.
Their main conclusion, that ‘more than ever, the typist of the new era will be the office wife’, is a simplistic one that ignores the contradictory nature of the evidence.
Certainly it seems likely that women will find themselves ever more firmly trapped at the bottom of the office hierarchy as a result of the introduction of information technology.
However it is also likely that many ‘wifely’ tasks such as teamaking and the bosses' shopping disappear, and that typists gain some protection from sexual harassment as they increasingly become machine operators, tied to work stations.
Where the SPRU team takes us at a brisk trot through the literature, Jan Zimmerman adopts more of a wild canter in her survey of the likely effects on women of a range of new technologies, in a piece that makes up in polemic what it lacks in argument.
In a lower key, Sally Hacker describes her research on how engineering students see the world, with some thought provoking insights into the part played by mind/body dualisms in perpetuating the contempt for all things female which characterises their views.
Much research and debate is required before we have a feminist theory of technology; in the meanwhile, this book will serve a useful purpose if it places the problem on the agenda of women's studies.
A measure of intelligence
Handbook of human intelligence edited by Robert Sternberg Cambridge UP, pp 1031, £45, pbk £15 
Hans Eysenck
THIS is a very long, very expensive and very highlevel book on human intelligence which will undoubtedly become, for many years to come, and as far as it goes, a standard book of reference, It has five parts, dealing respectively with the nature of intelligence and its measurement; cognition, personality and intelligence; society, culture, and intelligence; the phylogeny and ontogeny of intelligence; and a metatheory of intelligence.
The contributors are well chosen, none having a special axe to grind, and all being experts in their fields.
In view of all these positive things, is it true to say, as the blurb does, that ‘the Handbook of Human Intelligence will guide thinking about intelligence for the rest of the century’?
I doubt it myself, for the reason that introduced the words ‘as far as it goes’ above: the book simply leaves out of account the most important recent developments which foretell a revolution in the measurement and understanding of intelligence.
These developments can be understood only in the context of the long continued debate between two sides concerning the nature of intelligence, one captained by Sir Francis Galton, the other by Alfred Binet.
For Galton intelligence was a unitary, global mental capacity; for Binet it was merely the average of a number of independent, disparate abilities.
For Galton it was a gentle predisposition; For Binet it was strongly influenced by socioeconomic, cultural and educational factors.
For Galton the best method of measurement was physiological, or by reaction time; for Binet it was in terms of problem solving, learning, remembering following instructions, and other activities strongly involving learning.
Almost from the beginning Binet's approach won the day, and this book is a celebration of his approach; unfortunately it may also be its epitaph.
In recent years, it has become clear that very simple measures of reaction time (particularly choice reaction time) and variability in reaction time correlate very highly with ordinary IQ measures, to an extent that makes much of the theorising in this book inapplicable, Similarly, measures of inspection time have shown uncommonly high correlations with IQ, particularly in below average subjects.
In these tests, two lines of obviously different lengths are exhibited for a very short period of time, and the subject has to say whether the right or the left line was the longer.
The shortest exposure time at which the subject succeeds better than 97.5 per cent is taken as the inspection time, with short inspection times being indicative of high intelligence.
Despite these outstandingly interesting and important findings, which support Galton and throw an entirely new light on the whole topic of intelligence, none of the contributors mentions the work of S. Lehrl and others in Germany, which demonstrates very high correlations between IQ tests and simple speed of naming of numbers and letters; this too throws much doubt on the theories embraced in this book.
Worst of all, there is no reference to the work on physiological measures of intelligence, begun by J. P. Ertl in Canada a dozen years ago.
Ertl showed that evoked potentials on the EEG showed shorter latency and greater amplitude for high IQ subjects than for low IQ subjects, In recent years this approach has been broadened and improved by the Hendricksons, and we now have physiological measures on the EEG evoked potential which correlate as highly with typical IQ tests as one of these IQ tests correlates with another!
Yet neither Ertl's work nor the approach using evoked potentials and other physiological measures is discussed.
It is difficult to understand how such selectivity can be justified, in view of the fact that these studies and their results are quite incompatible with the major theoretical thrust of the book, which is complexly cognitive in the best Binet tradition.
This is a great pity; the battle between these two paradigms, that of Galton and that of Binet, is a very real one, and to pretend that it does not exist, and that there is no evidence against the paradigm adopted here, is disingenuous.
The book also shows the extremely irritating parochialism of American textbooks in psychology, to wit an almost complete exclusion of papers and books not written in English.
I have already commented on the book's failure to mention the work of Lehrl, from West Germany: equally missing is the outstanding work of Volkmar Weiss and the Mehlhorns in East Germany, of A. Firkowska and others in Poland, of N. G. Lipovecheja and his colleagues in the USSR, and many others.
In a science which is supposed to be international, such provincialism is inexcusable.
As I said before, the Handbook of Human Intelligence is excellent as far as it goes, but it certainly does not go far enough; perhaps the second edition will improve on this sad state of affairs!
Greed in the heart o the country
Whose land is it anyway?
By Richard Norton-Taylor,Turnstone, pp 350, £6.95 
Kenneth Blaxter
In 1066, by right of conquest, William I owned all the land.
Now more people have a stake, with some owning far more than others and most owning none at all.
In the millennium Richard Norton-Taylor thinks that worker-farmer cooperatives should prevail, that large estates should be managed by local communities, and that the land should be used to meet expressed needs of the people.
At least, this might be the view that Norton-Taylor wishes to promote but it is not readily evident in his book.
He is certainly concerned with the inequalities, injustices and inequities in the world as a whole and the UK in particular; with the greed, short-sightedness and self-interest of some of its inhabitants; and with the inadequacy, incompetence and inefficiency of the institutions which relate to land as a primary resource.
There is of course no reason why he should propose solutions to the questions he implies about the use of land and its ownership.
It is perfectly reasonable, and in the classic mould of 18th century pamphleteers, to be angry about our present state, to criticise and to apportion blame to those who exercise irresponsibility.
To ask for solutions as well is perhaps to ask too much, for he has found more than most to be angry or cynical about.
The land is used for many purposes: for housing, factories, rail and roadways, airports, water reservoirs, recreation, amenity, forestry and farming.
Land varies in its quality from the highly fertile soils in equitable climates to the poor, thin or peaty soils in the uplands and hills.
Our ancestors could recognise good land, and early settlements were almost invariably sited on the better land.
This land is now mostly built upon and its value augmented by a contiguity.
The sites of Claridges and the United States Embassy, which are owned by the Duke of Westminster, are obviously of greater financial value than those of similar area on the Isle of Mull.
Norton-Taylor has  analysed the ownership of all this land, drawing heavily on studies made by the Centre for Agricultural Strategy, and embellished his account by sub-analyses of the land holdings of members of Margaret Thatcher's Cabinet, by a fairly definitive listing of Ministry of Defence holdings, and an examination of the acreages associated with the entries in Debrett's Landed Gentry .
The extent to which land has and is being used as a long-term security by pension funds and by finance houses, and the entry of foreign buyers into the land; market, are dealt with anecdotally.
This is understandable.
Even the official inquiry led by Lord Northfield in 1977 could do little more, For there is no modern equivalent to the Domesday account.
Land lies derelict, land is poorly farmed, and one rather has the feeling that Norton-Taylor regards it as a sin to own land at all.
Yet the system is changing.
More farmers than ever before now own their farms.
The past 30 years has seen a continuation of the break-up of the old landlord and tenant system.
Whether more participants spreads the load of sin is not altogether clear.
Farming comes in for much criticism, largely it appears because farmers have responded to the need to make a livelihood from the land by producing more primary commodities using industrial techniques.
The reduction in employment of farm labour, the use of machine power, the adoption of simplified farming systems and the replanning of farm layout to ease farm operations are actions taken to ensure solvency in the light of changes in the structure of costs.
Since 1975, the prices farmers receive for commodities expressed in real terms has fallen by 16 per cent.
Input costs have also fallen in real terms, but by only 3 per cent.
Output, again in real terms, has increased by 25 per cent.
The food manufacturing industry has benefited from the lower costs of the commodities it processes.
Its prices have increased, however, and the consumer has not benefited from the efforts of farmers.
Admittedly, Norton-Taylor castigates the food industry as well as the landowners and the farmers — he spreads his castigation very evenly.
He also realises that it is the structure of costs that determines the ways in which farmers as entrepreneurs respond.
He also points to the ecological problems that this industrialisation of farming and forestry has brought about, to the increased reliance that has to be placed on chemical pest and disease control under monocultures.
It is a pity that he attributes to Emrys Hughes remarks relating to these matters made by Sir Emrys Jones.
Neither gentleman is likely to be pleased; this is not the way to make friends and influence people.
But then one does not have to make friends with people in order to influence them.
One can bludgeon them with facts, anecdotes and innuendoes about the ways in which land is being misused, show that even the best of intentions are misguided, and that most people who have to do with land and what it provides hold views which are wrong.
This Norton-Taylor has attempted to do, but his book betrays too much anger and too much cynicism to be wholly effective.
Threat to a great ape
The orangutan: its biology and conservation edited by L. E. M. de Boer,Dr W. Junk, pp 376, Dfl 175 
David Chivers
I PERSONALLY find the orang-utan to be the most appealing of the great apes: it is magnificent in its tropical-forest home, and very entertaining in captivity.
The orang is not as endangered as was estimated 15 years ago, but it is threatened.
Rehabilitation programmes have served a valuable role in conservation education, and in rescuing pets and animals from cleared habitat, but have done little to boost wild populations — indeed, they could endanger them.
Their varied fruit diet highlights the orang-utan's crucial role in rainforest perpetuation, and the prime need to protect this ecosystem to conserve all its inhabitants.
The large size and wide-ranging habit of the orangutan make it especially vulnerable to devastation in tropical rainforests.
A book synthesising knowledge of this endangered species is long overdue.
Leobert de Boer has rectified this deficiency, following a workshop in Rotterdam in October 1979, by editing a volume comprising eight chapters on biochemistry and physiology, five on maintenance and behaviour in captivity and four on ecology, behaviour and conservation in the wild.
Most appropriately, the introductory chapter on distribution and evolution is an up-to-date review by that eminent physical anthropologist G. H. R. von Koenigswald.
De Boer has produced an equally stimulating epilogue, collating well the topics in the book into an integrating conservation framework.
The studies of genetics, haemoglobins and chromosomes by de Boer and his colleagues, and reproductive physiology by J. J. van der Werff ten Bosch, Susan Kingsley and Ronald Nadler, provide a much-needed collation of basic biological information and highlight the distinctiveness of Bornean and Sumatran orang-utans, as well as the likelihood of distinct populations within each island.
A detailed review of orangutans in captivity by Marvin Jones is supplemented by information on veterinary care by David Jones and  behavioural studies by S. D. Edwards and T. J. Maple.
They highlight the social and manipulative skills of this ape and the need for care in reducing boredom and stimulating breeding, as well as the distinctness of Bornean and Sumatran animals.
The field studies of C. Schurmann, B. M. F. Galdikas, R. J. Aveling and H. D. Rijksen focus on mating strategies, the role of orang-utans in seed dispersal, conservation by habitat protection and education, and socio-ecology in relation to survival prospects, respectively.
Mating strategies are clarified, as predicted from and elaborated in captive studies: subadult males are more active sexually than adults, but the latter focus on peaks of female fertility.
Hence, the frequent incidence of ‘rape’, and the females enhancing the reproductive success of adult males by their  proceptive behaviour around ovulation (references to ‘oestrus’ in an ape are misleading).
The Orang-utan is especially welcome for stressing the conservation problems and providing some answers, for collating so much biological information on this fascinating ape (some results expected, others not), and for clearly posing problems for future field and laboratory.
It is essential reading for all researchers interested in apes and in appreciating the mutual benefits of field and laboratory study.
Too many ‘conservationists’ fail to appreciate the value of captive studies and even of management in disturbed habitats — all approaches are very valuable, but the protection of adequate areas of natural habitat is paramount, These approaches, and the critical plight of  South-East Asian wildlife, are well summarised in the epilogue; Dutch primatologists are to be congratulated on their initiative in this venture,
Publicity agent for whales and seals
Sea shepherd by Paul Watson,Norton, pp 258, £10–50 
Mark Cawardine
A ‘NOXIOUS TWIT’ one Newfoundland writer once called Paul Watson.
Born in Canada, but now spending most of his time charging around the world getting into trouble, Watson is a young radical determined to stop animals such as whales and seals being hunted to extinction.
While most of his contemporaries in the conservation world sit behind desks or on committees, Watson and friends race ‘kamikaze’ inflatable dinghies in front of Soviet and Japanese whalers' harpoons or handcuff themselves to the seal hunters' vessels.
It's an exciting life by any standards and this description of Watson's incredible feats certainly makes exciting reading.
His dedication is infectious.
He has been beaten, nearly drowned and jailed on numerous occasions — but never actually stopped.
The thought of someone boldly (or foolishly?) risking life and limb for his cause cannot fail to capture the imagination, though the effect is sadly marred in places by over-reacting and extreme sentimentality.
At least no one can accuse Watson of being half-hearted.
One particularly interesting aspect of the book is the continuous discussion on the use of violence.
It is clear that Watson spends a great deal of time wrestling with his conscience over the choice between morals and results.
Despite the fact that he was dropped from Greenpeace in 1977 for being too much of an activist, it seems that he is not blindly reckless.
While he strongly believes that violence produces results (presumably by dramatising the point in question) he agrees that it is morally wrong.
In a compromising philosophy he allows himself violence against property but never against life, human or otherwise.
The book is a mixture of varying successes and failures.
Although very often his actions have little or no practical effect, they certainly succeed in raising public support.
In the long term, manoeuvring an inflatable dinghy between whales and whalers is not alone a dramatic event as far as the world's whales are concerned although, of course, it temporarily delays a killing.
The same goes for diving in front of a seal pup to protect it from the sealers.
But, on the other hand, the public support such actions encourage can be overwhelming — and surely do nothing but good.
But Watson does not only see himself as publicity agent for his cause.
‘If there are riot police,’ he says, ‘then vigilantes will appear, because there will always be somebody to see to it that crime is never given a free rein.’
This one-man vigilante squad is not such a noxious twit after all.
Big money and old wives scrutinised
BACK in 1975, when Reg Prentice was Secretary of State for Education and Science (remember?),
I asked him how long it would take the then government to assess the success of Lord Rothschild's reforms.
He had no answer — save that British scientists had been reorganised so often in recent years that it was time for stability.
That exchange came to mind as I watched Gavin Scott's Horizon, British science — on the wrong track?
(BBC2, 28 February).
Here, it seemed, was a chap who had got up one morning and resolved to query the £4 billion we spend annually on science.
‘How come, if we're so clever, we're so poor?’, he asked, observing Aaron Klug and John Vane receiving their Nobel prizes last year.
Daresbury was fine, he agreed, but no one pretended that its work would be converted into saleable products.
Some fields get disproportionate funds, he opined, while others go hungry.
There was much here to interest New Scientist customers — in particular a vivid portrayal of the unprecedented butchery visited on academe by the Thatcher government.
But the lack of historical perspective was discomfiting.
An excellent, pointed illustration of how Britain failed to capitalise on Cesar Milstein's monoclonal antibodies was linked to an innocently superficial recap of the penicillin affair.
The paymasters were blamed for favouring high energy physics as against engineering, despite the past decade's shift of resources in the opposite direction.
And viewers were given the ridiculous impression that two blokes from Sussex University, rather than Eugene Garfield, had invented citation analysis as a means of assessing quality in research.
What Scott lacked above all, however, was any visible grasp of the reports and councils, White Papers and Green Papers, which have reconnoitred this terrain before and illuminated the thorny issues at the heart of science policy.
It really is not good enough to ignore such debate entirely and grouse instead over simplistic comparisons between expenditure on different specialities.
Nor should a serious programme leave unchallenged Mary Kaldor's hollow phrases about how science should ‘meet people's needs’.
But then again, one might forgive such callowness in gratitude for several gems in this otherwise disappointing report.
Especially the moment when Sir Edward Parkes, surveying university gloom, announced: ‘Unknown genius must be allowed to flower.’
Gavin Scott has now moved on from science to other things (he is reading the news on TV-am's Good Morning Britain), so we shall not, presumably, see the further development of his short career in science.
One man who has been with science for many years now — and his experience certainly tells — is Mick Rhodes, whose new series of QED began last week.
Old wives' tales (BBC 1, 2 March)…was a delight and a pleasant bout of instruction.
Does thunder really make the milk go sour?
Yes it can, if close, humid weather curbs evaporation, allows the temperature of milk to rise, and thereby promotes bacterial production of lactic acid.
Do we really have.
to tell the bees everything, especially about births, marriages and deaths?
No, because that is nonsense.
Ranging wide over ancient aphorisms, Professor Noel Dilly struck precisely the right note of scepticism, receptivity and curiosity.
And some illuminating principles emerged.
Inoculated with rhinoviruses, people are no more likely to develop a cold (or ‘chill’) if kept cold and damp than if tucked up warm.
One year's seeding really does mean seven years' weeding, when we consider the succession of seasons required for astronomical numbers of poppy seeds to decay to zero.
Just two questions remain.
Did a single viewer understand the explanation of why peptide 401 and other substances in bee venom, by causing inflammation, should relieve rheumy joints?
Any why is this series, every episode of which has been totally different from the one before, called a series at all?
FORUM
Beware the hazards of inactivity!
Karl Hammond has been reading the Royal Society's report on risk
HALF A DOZEN years ago, it was almost fashionable for scientists who had made a quick excursion through Britain's mortality statistics to adopt a righteous posture and preach the message that people were worried about the wrong things (usually nuclear power).
They, the experts, had examined the statistics and concluded that the risk of being killed by an accident in a nuclear power station, or by cancer’ causing saccharin in coffee was so low that it was ‘acceptable’.
Lord Rothschild appeared on television to give a Dimbleby lecture in which he authoritatively reassured the public that the risks of nuclear power were negligible: if those risks were not, in fact, accepted, it was because the public were too stupid or ill-educated to realise that they were objectively ‘acceptable’.
Less accomplished lecturers repeated the message at meetings of the second-rate institutions that infect academic life.
The word they brought down from their ivory towers was ‘acceptability’.
If, their litany ran, risks are low, we ought to accept them.
The spate of similar lecturers, articles and tracts was a stunning confirmation of the critics who argued that scientists could spout such one-dimensional, unrealistic drivel only because they and their science was equally one-dimensional.
Any competent social scientist could have shot them down in flames.
Some social scientists did: most ignored the debate because it was, frankly, naive (and often consisted of nothing more than special pleading from scientists working for institutions that had much to lose).
The gap between natural and social science seemed enormous.
The natural scientists, pointing to ‘objective facts’, pleaded for more rationality.
The few social scientists who bothered with the issue pointed out that the control of risks was a social activity and in its analysis ideas like objective facts and rationality were a positive hindrance.
While the academic debate continued on the sidelines, the people who actually ran most of the risks being discussed took matters in their own hands.
Trade unions began, if slowly and unadventurously, to assert that they didn't give a hoot: their members were not going to run risks they didn't like.
The public inquiry into the expansion of the Windscale nuclear reprocessing factory gave the anti-nuclear movement the impetus it needed not only to challenge the official estimates of risks, but also to question repeatedly the idea that the simplistic estimation of the numerical size of a risk should govern its regulation.
Now, the academics (or rather the best of them) have caught up with the real world.
The Royal Society has produced a report (Risk Assessment ) that firmly knocks on the head the  asinine logic of Rothschild and the other one-dimensional men.
The report points out that quantifying risks is not without its uses: it teaches you a great deal about the way that the engineering systems you're analysing behave.
It can be important in choosing sites for factories that will create risks or in choosing between different kinds of equipment with different risks.
Neither is the size of a risk unimportant in influencing how people regard it.
But it's only one factor.
There are many others.
For example, many people see no reason why they should run any risk from factories making nuclear weapons when they disapprove of nuclear weapons in the first place.
Others refuse to accept risks from activities which, they believe, bring no benefit to them or to society at large.
Why should I accept even the very small risk of living near a dump site for nuclear waste when I believe that nuclear power is costly, dangerous and unneeded?
When Yorkshire Television's documentary on asbestos showed a 10-year-old boy dying horribly of mesothelioma probably (or even possibly) caused by the asbestos he breathed while watching his father change asbestos-lined brake shoes, it does the asbestos industry no good at all to point out that the risk of contracting mesothelioma is quite low.
Quite rightly, the injustice, pain and horror of that boy's death outweighs any ‘objective’ argument.
A society that allows such deaths is not humane.
The Royal Society's report accepts that people's attitudes to risks are just as important as the numerical value of the probability of the risk being realised.
Much of the credit for this advance in the  establishment 's thinking should go to Professor Terence Lee from Surrey University.
Reading the report, it is clear that many of the ideas in it about people's perception of risks are his.
Equally, the presence of John Dunster on the working party that produced the report has clearly brought some fresh air into otherwise tired thinking.
Dunster, who is head of the National Radiological Protection Board, and was deputy director of the Health and Safety Executive, obviously drafted the sections of the report on how risks should be controlled.
His thinking is original.
He starts with the premise that, in many activities, safety is impossible.
On the other hand, there are clearly risks that most people find unacceptable.
So the regulatory tactic should be to ban the risks consensus defines as unacceptable and to impose (if necessary) a duty to limit other risks as far as money allows.
Neither does Dunster (or the Royal Society's report) dodge the implications of this tactics.
First, it means that there must be some form of cost/benefit analysis, however distasteful it is to assign money values to life or ill-health.
Secondly, and much more importantly, politicians, industrialists and the powerful in general, must realise that you can persuade people that risks are acceptable, only when they have enough information to decide for themselves.
The consensus must be open and public.
That means publishing research, collecting data on morality in factories and constructing some forum where consensus can be agreed by negotiation.
And above all, it means that the government cannot, in private, pre-empt consensus.
Margaret Thatcher's government so far refuses to listen to these ideas.
For example, it  steadfastly refuses to ban lead in petrol even though there is now no scientist of repute willing to say that leaded petrol is, on balance, blameless and even though opinion polls have shown a massive public consensus against lead in petrol.
Having produced a report that talks sense about risks, the Royal Society probably thinks its job done.
That's a pity.
For none of its analysis, none of its prescriptions will matter a jot unless the government can be persuaded to take notice.
With more than three million people unemployed (even by Norman Tebbit's figures), the trade unions have other things to think about.
With Thatcher running amok through the welfare state, lobby groups are preoccupied defending what was once thought unassailable.
But it would be too much to ask the Royal Society to act on its own analysis.
That might disrupt its comfortable life of leisure in its government-owned mansion overlooking The Mall.
All that is to be hoped is that individual fellows of the society will use this report to press, alongside others less privileged, for an openness, a democracy that  hardly distinguishes the Royal Society itself.
Scientific Promotions (Inc)
Milton Love has some ideas for advertising scientists
MY WIFE, Regina, is forever  characterising me as possessing an almost childlike naivete — one attuned with a more idealistic time.
I cheerfully agree with this assessment.
I am forever bemused by the verities of our modern-day society.
This may explain my confusion on reading a newspaper advertisement touting a book on baseball by Roger Angell.
Prominently displayed in the ad was the statement, ‘People buy the New Yorker for the  damndest reasons.
I happily pay my yearly subscription just to read Roger Angell's occasional essays on baseball.’
This statement was attributed to a book review in the Boston Globe by one Stephen J. Gould.
Strange, I thought, I have not heard of a prominent baseball authority with that name.
Perhaps it was a lesser figure, possibly a locker room attendant for a minor league team, such as the Opeleika, Alabama, Hog Throttlers.
A quick call to the Globe proved that the book review was indeed by the Dr Stephen J. Gould, guru of New Wave Evolution.
Now, while I am at least mildly interested in his thoughts on evolution, why would the publisher of Angell's book believe I would care what Gould thought about a baseball book?
Perhaps Gould's work on snail speciation gives him some peculiarly perceptive insights into the baseball world.
For instance, could the infield fly rule be in some way analogous to punctuated equilibrium?
Or is base running a societal metaphor for gene flow?
I am ashamed to admit to the same bafflement on hearing Dr Sylvia Earle — algalogist , marine biologist and self-promoter extraordinaire — extolling the virtues of a moving and storage firm in a radio advertisement.
Perhaps, unbeknownst to me, she had worked her way through graduate school pushing grand pianos down stairways, or dropkicking boxes of Wedgwood into waiting vans.
But if not, how has her work in algal taxono or her dives into the Galapagos Rift prepared her as an expert on the moving business?
Seeking an answer, I visited the laboratory of Dr Alice J. (Apple) Pion, until recently a little-known physicist at Dismal Seepage, North Dakota, Community College.
Little-known, that is, until she received the Snavely Award for Least Useful Research for her development of schools where backward quarks might learn charm.
I found Pion to be most eager to discuss any and all aspects of both the physical and metaphysical universe.
‘My, my, yes’, she burbled, settling back into her chair.
‘Since my ascendancy to the hallowed ranks of Snavely winners, my opinions in all matters have been sought most eagerly.
Why, in the past week alone I have given my views on nuclear disarmament, the Middle East, world hunger, racism in the dairy industry, the proper depth to set okra seedlings and the supremacy of angst in the first two volumes of Winnie the Pooh .
Three firms — Dismal Seepage Savings and Loan, Dismal Seepage Carwash and Dismal Seepage Goat Packers Inc. — have asked me to do advertisements.
Soon I begin a round of personal appearances for the Dismal Seepage Home for Unwed Optometrists.
‘You see, dear boy, what you have stumbled upon are classic cases of what has happened to me.
We have reached the occasion where an individual receiving publicity in one field is thought to be an expert, or at least a saleable item, in all others.
In my case, before becoming a Snavely winner, I was but crabgrass in the Garden of Physics.
After the award, I have to beat away publishers, admen and sycophants with a sharpened stick.’
It is indeed a shame that prominent scientists of the past did not share in this phenomenon.
We might then have seen such  advertisements as: Galileo Galilei, for Famagusta Instant Golden Enriched Noodles, from atop the tower of Pisa: ‘Let me tell you how it is.
I'm on the roof studying the planets and its cold.
Mama mia, its cold.
Then my neighbours start screaming, ‘Galileo, you lazy bum, get into bed.
You got to go look for work tomorrow.
Quit wasting your time looking at the sky.’
Then my kids start up.
‘Yeah, Papa, stop looking at the sky.
All the kids at school think you're some kind of heretic.’
‘Here I an trying to change man's perception of the Cosmos and the neighbours and kids tell me to get a job.
That's when I need a big bowl of Famagusta Instant Golden Enriched Noodles.
‘Now I hear the Pope has decided to bring me to trial.
The first thing I think is ‘Uh oh, Galilei, your rear end is in a pasta maker now’.
But the next thing I do is buy a case on Famagusta Instant Golden Enriched Noodles.
If they throw me in prison, at least I know that with Famagusta and maybe a rodent or two from my cell, I can have a meal fit for a Borgia.’
Madame Marie Curie, for Breast-Arrest Bras, from within a small wooden hut: ‘Zut, alors!
Before I started wearing my Breast-Arrest my physics career was in a shambles, for my ample balcon kept plopping into my radium, disrupting one experiment after another.
I tried everything, even hiring an off-duty castrato to hold them out of the way, but nothing worked.
‘How, I wondered, would I ever win the Nobel Prize?
‘Then I remembered Breast-Arrest, with the patented material Z-28, the same material used in suspension bridges across the Seine.
Breast-Arrest keeps me cool, comfortable and in control!’
Charles Darwin, for Dregs Ale, from the public bar at the Dodo Agogo Inn: Joseph Hooker has just triumphed in his debate with Bishop (Soapy Sam) Wilberforce over Darwin's Origin of Species .
Hooker, Thomas Huxley and their friends are in a local pub celebrating by playing darts, setting fire to the proprietor and, of course.
drinking lots of Dregs Ale.
In the midst of this revelry, Charles Darwin strides in.
He is immediately besieged with cries of ‘Good Old Charlie’, ‘Mr Evolution’ and ‘Slimy Old Pruneface’(from a disgruntled Wilberforce supporter who had slipped into the festivities).
Darwin hoists a pint of Dregs saying, ‘I've had my share of good moments, such as when I cheated that Wallace simpleton out of any credit for natural selection.
Or when I come down with psychosomatic diseases so that my wife babies me and Huxley and Hooker fight all my battles.
Yes, and when I celebrate good times, I do it with Dregs.
Don't let other ales make a monkey of you, drink Dregs!’
Darwin then pours the ale on the semi-comatose Hooker amid general merriment and the screams of the burning proprietor.
Hard pill to swallow
Donald Gould has been digesting a report from the OHE
PROFESSOR GEORGE Teeling-Smith has provided the nation with a grim warning, and one we should do well to heed before we are all poisoned to death.
The professor is director of the Office of Health Economics, and has been ever since that admirable bureau was established in 1962.
The OHE is funded by the Association of the British Pharmaceutical Industry, and is housed in the association's premises in Regent Street.
There is nothing covert about this operation.
OHE is not a ‘front’ organisation for the pill merchants, but a proudly acknowledged agency, and is far and away the most successful PR exercise (perhaps the only successful PR exercise) which the industry has ever staged.
Its function is ‘to undertake research on the economic aspects of medical care’, and to ‘investigate other health arid social problems’, and to ‘collect data from other countries’, and to publish the results of these wholly commendable activities.
This brief has been fulfilled with distinction, and every three or four months for the past 20 years the office has produced an objective, well-documented, literate, and attractively presented booklet encapsulating the known facts about subjects ranging from suicide, rabies and heart disease, to the cost of running hospitals and what the public expects from the NHS, always adding a few percipient comments and conclusions of its own for good measure.
This activity has earned the gratitude and, indeed, the respect of hacks such as myself, who have come to rely on the OHE's booklets and fact sheets and other publications as accurate and marvellously convenient source material.
George himself has deservedly achieved the status of a guru, and a year or so back was made a part time professor of medical economics at the City University.
He and his staff have created a fund of goodwill for the industry which employs them, precisely because they stick to examining and explaining facts, and don't flog the party line, and never indulge in propaganda — well,hardly ever.
I suspect that some of the drug barons who shell out money in support of what must be a pretty costly operation regard the academically-minded staff of the OHE as a bunch of eggheads, not to say ‘wets’, who ought to be earning their keep by polishing up the industry's somewhat tarnished image, and by encouraging sales, instead of spending their time and the companies' profits on airy-fairy projects which aren't going to rustle up a pennyworth of business.
And I suspect also that from time to time the director feels that he has to placate the more hard-nosed and less imaginative of his many paymasters by producing something that could be regarded as promoting trade.
How else are we to explain the  grotesque document which this highly intelligent and normally level-headed man gave birth to the other day?
It has an innocent enough title —The Needs of the Pharmaceutical  Manufacturers  from their Medical Departments in the 1990s — and in it he very reasonably argues that an increasingly sophisticated public, provided with alarming insights concerning drugs like Opren and thalidomide by such philosophical weeklies as the Sunday Times , is going to demand a great deal more information about the many medicines doctors so recklessly prescribe,(Those aren't quite the terms he uses, by the way.)
Yes, fine.
And we must all agree with George when he states that in future the pill pedlars will not only have to show that their nostrums work, ‘but also that they actually bring benefits in social and economic as well as in purely medical terms’.
So far, so good.
But it is the means by which these desirable ends are to be achieved that must shock and dismay.
The professor claims that the customers are going to want ‘more authoritative advice than their doctors can give…
.‘And how are they going to get this ‘authoritative advice’?
Why, straight from the manufacturers, of course.
Cut out the costly and ignorant middleman.
You can see his point.
The doctors may broadcast their  scripts to a drug-hungry nation like showers of autumn leaves, but they still act as an intolerable bottle-neck between manufacturers and their clients, the latter, poor sods, often not even knowing what wondrous remedies are to hand for the asking.
It may be, says George, that the public should be privy to the codes which doctors use to rustle up data on drugs on their TV screens.
Anyway, one way or another there must be much more direct contact between the pill makers and the pill swallowers.
Drug companies will have to start regarding themselves as suppliers of health rather than just medicines, and ‘In the extreme case, companies could possibly start to provide general health counselling’ to accompany the use of their products.
Now, I'm all for the members of the Great British Public knowing a great deal more about the nostrums they consume, and having the information needed to help them share in deciding which are worth having and when, and which are best left to the birds.
But are the pill and potion merchants the right, proper and natural sources of guidance and advice?
Not on your nelly!
After all, would you like to have the fat cats who make Exocets and Harrierjets tell us when and where we should go to war?
It's bad enough having to rely on old daddy Reagan and Maggie Thatcher.
A time and a place for jargon
Colin Tudge on the living language of science
IN THIS game (known as science-writing, as in I science-write, you science-write, he/she science writes) it pays to know about jargon.
If you don't, if you throw it into articles without laying the ground first, you tend to lose your readers.
Worse, if you have an astute sub-editor, or even a conscious one, he/she has a tendency to write all over your nice clean copy with a felt-tip, which is like having people tread on your geraniums.
A firm knowledge of what jargon is, then, should take its place with a stout pair of boots and a spare pair of braces, in every science-writer's knapsack.
The word ‘jargon’, according to the Shorter Oxford English Dictionary , comes from an Old French root that has to do with the warbling of birds.
It came, in English, to mean twittering in general; and in Middle English meant ‘ unintelligible or meaningless talk or writing; nonsense, gibberish’.
But medieval England, no less than now, was full of all manner of self-appointed scholars, administrators, and general schemers with a vested interest in obfuscation; and with what pleasure, by 1651, says the OED , was it ‘applied contemptuously to the language of scholars, the terminology of a science or art, or the cant of class, sect, trade, or profession’.
The trouble now is that jargon, both the thing and the word, have become a kind of battle ground.
There are nasty people (a pox on their house) who use fresh-minted language not simply or even primarily to communicate with their peers, but to repel boarders; just as prep-school boys use cabalisms to confuse rival Football teams.
On the other hand, there are philistines who apparently believe that all new ideas, no matter how subtle, can be conveyed in the terms they happened to have picked up at their mother's knee, and reject any attempt to expand the language into areas beyond the nursery.
(A pox on their house too.)
The battlefield must be cleared of a little smoke.
It should be obvious first of all, but surprisingly to many people isn't, that each field of expertise needs its own vocabulary.
I am constantly astonished by the technical terms of building: breastsummers and dados, screeds and spandrels, rebates and mullions.
Where do the words come from?
Whoever thought to give a word to a bit of stone sticking out of a wall, or a bit of wood stuck in one?
The point, however, is that to build a house that stands up requires a knowledge of where every bit goes and what it does — and that knowledge is an order of magnitude and greater than one would have been able to guess at.
A breastsummer isn't just a piece of wood stuck over a gap, it's a piece of wood that holds the walls together, and the rest of the building up.
The fact that it is actually a bit of wood is not important; what is important is the function, which can be carried out most conveniently by a bit of wood in the right place.
Breastsummer doesn't actually describe a piece of wood at all; it describes an idea.
Like all ideas, the idea of breastsummer is subtle, with all kinds of connotations.
Like all ideas, it is most easily controlled if it is first given a name.
Once you move into particle physics, or molecular biology, then you may deal with phenomena that are not only esoteric, but literally have no equivalent in the everyday world.
It is reasonable, to start ideas in train in children, to compare an electron with a ping-pong ball, or the whole atom with a tiny solar system; but the longer you stay with homely parallels, the harder it ultimately becomes for the child to move out of the imagery of pong-pong balls and into an appreciation that atoms aren't really like that at all.
The new ideas, that come from the grey area where energy and matter interchange, need new words, just as rough country needs Land-Rovers; without the vocabulary, the traveller in new ideas is permanently aboriginal.
Thus is ‘technical language’ vital.
It is no part of the job of anyone who pretends to convey any area of scholarship to a lay audience to shield that audience from the technical terms, for the basic vocabulary is the chief requirement of initiation.
It is part of the job (perhaps the main part) to introduce the principal terms and to convey some sense of what they mean: not only as defined in a dictionary, but also the connotations, the extra shades of meaning appended to words by the people who use them.
To suppose that technical vocabulary Per se is ‘jargon’, the warbling of birds, is simply foolish: and to suppose that popular ‘science-writing’ should be stripped of technical terms is to misconstrue the nature of the craft.
However, technical vocabulary is one thing and technical language is another.
Vocabulary is words, lists of, abacus beads, each encapsulating a precise idea.
Language is what people speak.
Beads have no meaning in isolation: language has meaning by definition, even if there isn't a bead in sight.
Scholars in every field use their technical terms all the time, just to get through the day.
Like all other words, like the word ‘jargon’ itself, the technical terms are changed with use: corrupted, as they are used more and more cavalierly: enriched, as their origins are taken more and more for granted and they begin to be used as metaphors.
Because technical terms are genuinely a part of language (and not intended simply as items in a dictionary), they ebb and flow, and grow and contract, and shift in connotation.
Everyday language, the language of tea and scones, when it has been beaten by use into everyday forms, is termed vernacular.
True, it is studded with words that have dictionary definitions, and held together by formally definable rules of syntax; but the real matrix of vernacular is the common understanding of people who have shared experience.
To foreigners, no matter how generously equipped with dictionaries, the everyday language of everyday people is incomprehensible.
That is a pity, perhaps, but it does not mean that everyday language is bad; it is imply the way of things that language is not alive unless it is vernacular, because the adornments and abbreviations are the adornments and abbreviations of human thought.
Jargon is technical language that has become vernacular and, similarly, it is not intrinsically bad; if technical language were not vernacular to those who need to use it, then it would actually serve very little purpose.
So when is jargon bad?
When is the sub-editorial felt-tip justified?
It is bad when the inevitable exclusiveness of vernacular becomes the reason for using it.
It is necessary if you are a scientist, as it is if you are a juggler or an  aficionado of the French horn, to employ jargon.
It is bad, if you are a scientist, to use it specifically to put outsiders down.
Similarly, if you are a ‘science-writer’ it is good to convey as much as possible of the jargon of a discipline, because the jargon is a large part of the living language of the discipline, and therefore of the thought processes that direct it.
It is bad to lapse into the jargon without conveying its meaning and its connotations, because that is lazy; and unforgivable to use it just to give the impression of being in with the chaps.
‘A journalist, proud of that epithet’
Bernard Dixon on Arthur Koestler, the science writer
TWICE IN MY LIFE I have consciously avoided meeting someone, because I held them in such awe.
One was Arthur Koestler.
And the substance of my apprehension is to be found in the formidable dossiers of attainment which have been catalogued in numerous obituaries over the past week.
That, plus the devastating reputation of a man who, uniquely in the 20th century, shattered the myth that intellectuals always avoid the action.
It was a stupid  misjudgement .
When I did come to know Koestler I found him electrifying but not at all intimidating.
Even during recent years of poor health, his outstanding qualities were riveting charm and mental vitality.
Small, compact, puckish, he had the aura of an insatiably inquisitive vole, continually turning over ideas, playing with them, and challenging you to come clean about what you thought.
Sharing a glass of whisky with him in his Montpelier Square home, you were liable to be quizzed about theories of antibody formation, David Bohm's work, or any one of 101 other topical issues that had caught his attention.
He had strong opinions.
But his curiosity about what others were doing was itself astonishingly forceful.
Koestler was, of course, a journalist, proud of that epithet.
After serving as a foreign correspondent in the Middle East and Paris, he took up a new job in Berlin on the very day in 1930 when the Reichstag election heralded unprecedented barbarism in Europe.
As he later recalls, in Arrow in the Blue , ‘the arrival of a new science editor at that particular moment struck them as exceedingly funny’.
But so it was that this Hungarian son of a would-be inventor moved towards his abiding preoccupation with scientific ideas.
And just as Koestler the man seemed devoid of small talk, so too all the writings that followed were pertinent and direct, never trivial or sloppy.
The Sleepwalkers must be unrivalled as a scholarly history which has also aroused life-long interest in science among many youngsters.
The Case of the  Midwife  Toad is a controversial but brilliant account of the Paul Kammerer affair.
And Beyond Reductionism (edited with J. R. Smythies) reviews notions, ridiculed at the time, that are already being recognised as a valuable corrective to some simplistic paradigms of biology in the late 1960s.
Many of these writings have attracted hostility.
One recalls Peter Medawar's opening line (’ Arthur Koestler is a very clever, knowledgeable and inventive man, and The Act of Creation is very clever too’) largely for the blast that followed.
But Koestler's replies have invariably upstaged the most erudite critics.
And, despite appearances it is questionable whether he always wanted to prove his case to the hilt.
Far better, surely, to be remembered as one who scorned a wealth of speculations which were constructive and exciting.
I like to compare him with a footballing genius, putting through passes others are too slow to take, and making moves that would be brilliant if only the rules were different.
And I shall remember his courage.
‘Reason tells us — when not choked by panic — that before we were born we were all dead, and that our post-mortem condition is no more frightening than the pre-natal twilight,’ he wrote in EXIT's Self Deliverance .
‘Only the process of transition, of getting unborn, makes cowards of us all.’
Those words, remember, were penned by one whose physical and intellectual fortitude had been amply demonstrated in the fullness of life.
Sanitary Sanity
Tam Dalyell believes that where there's muck there are votes
VICTOR DE SABATA, favourite son of Arturo Toscanini, brought the orchestra and the entire chorus of La Scala Milan to the Edinburgh festival in the 1950s.
The occasion was magical with glittering performances of Verdi's Requiem .
Nobody who was lucky enough to go to the city's Usher Hall can have forgotten the splendid Dies Irae .
Rudolf Bing, the director of the festival, was then about to take up his post as director of the Metropolitan Opera in New York.
He vigorously defended against all comers the enormous expense of bringing La Scala to Scotland.
But his self-confidence was dented by one salty, working-class Edinburgh councillor, who loudly opined to the great impresario, ‘This city can always find the cash to powder its nose: but we forget to wipe our bottom!’
Rude, but true, because not for another two decades did the Scottish capital get around to paying for proper effluent disposal into the River Forth.
Edinburgh, like many other British cities, has a superb inheritance of Victorian sewers.
But we cannot expect to live on this inheritance for ever.
In Britain, about 10 sewers collapse each day, and the highest incidence is in the North West of England.
Like all else, sewers deteriorate, and become old and dilapidated.
In towns, the ever-increasing motor traffic takes its toll of crumbling sewers.
In the countryside, in ribbon developments and even new housing estates, Victorian sewerage systems have to serve populations ten times larger than those for which they were designed.
One thing is certain: we cannot just amble on and it is going to cost us dearly — £40 000 million according to the National Water Council and others.
But one should not be too dismayed by the sum that we shall have to fork out before the end of the century, to replace sewers.
Increased public investment is essential to economic recovery.
Spending money on the sewers certainly has benefits in the battle against national depression and the creation of jobs.
Of course, investing at times of  depression creates additional demand.
But I believe that for every £100 million spent on sewers, the present real cost to this nation of tax payers would be rather less than £50 million.
Contractors would have to employ more people.
Sewer renovation requires large numbers of labourers.
The ordering of building materials would help greatly the building supplies industry that is at present in the doldrums.
Much less unemployment benefit would be paid out.
More would be paid by contractors' employees in tax.
Some people claim there is not much difference between the after-tax earnings of low-paid workers and their social security entitlements when out of work.
My reply is that there are many on the dole who do not share this view and who would far rather have a job.
There is a difference in the money involved but self-esteem comes into it.
Probably the most urgent sewer rejuvenation work is required in the West Midlands, the North West of England, and the Greater Glasgow  conurbation .
These are precisely the areas where the unemployment is highest.
A substantial increase in water industry investment should be possible without stimulating higher inflation, or a deterioration in the balance of payments.
There would be ‘real jobs’, to borrow Margaret Thatcher's phrase of the moment, and of great significance they would be British.
When potential investors, and more particularly their banks, look at the possibility of setting up in a district, they invariably inquire about the water supply and the standards and controls of effluents and sewerage systems.
Local authorities that have modernised their water systems stand a much better chance of attracting new industry than do those that have not.
Could this point be ‘sold’ to the public, who are thought by the  politicians to be uninterested with matters ‘out of sight, out of mind’?
Certainly.
I would have no difficulty whatever in going on any hustings and waxing  eloquent about foul sewage coming up in the wrong places.
There are votes in sewage for those who argue the case in a constructive and sincere way.
Politicians are of course at fault.
On the local councils, water and drainage committees are not among the most prestigious: if importance and utility were the criteria then becoming a member of these committees or their chairmen would carry kudos.
Nationally, few MPs think that they will gain much from chuntering on about the drains…but I just wonder if they are not wrong.
I attended an excellent meeting on 28 February, that the Institution of Civil Engineers (ICE).
Though it sent invitations to 250 MPs, and though the venue is precisely three walking minutes across Parliament Square, only two of us turned up — Albert Costain of Folkestone, whose firm is famous in the world of civil engineering, and myself.
All right it was an inconvenient day for some.
All right, the secretary of state may have had a Cabinet committee meeting.
All right, Giles Shaw, the junior minister at the Department of the Environment, was in Brussels.
At Engineers' Hall we listened to experts like D. A. D. Reeve, chief executive of the Severn Trent Water Authority, D. Gaulter, director general of the Federation of Civil Engineering Contractors, and W. T. Devenay, director of water, Strathclyde Regional Council.
Silently, watching the proceedings from the walls were portraits of a host of 19th century presidents of the ICE.
No less than five of them, men like Robinson and Locke, had also been members of parliament.
For all I know they may have been part-time MPs, or they may have had a limited electorate to represent.
Some may have had ‘rotten boroughs’.
But I am jolly sure that they would not let the government of the day, or their contemporary House of Commons, forget how important it is to supply water and to look after the sewers and drains.
For all the success of 20th century wonder drugs, 19th century improvements in sanitation did more for the health of the nation.
Let's get back to the dynamism and outlook of the men who built our great and marvellous sewers.
Our Victorian grandfathers recognised the paramount importance of sewers — we should recognise it too.
LETTERS
Human cropspray tests
The children of Maharashta State, India, share with Egyptian teenagers the distinction of being sprayed with Ciba-Geigy pesticides (This Week , 10 February, p 355).
In 1979 R. R. Rao and colleagues published a report in Ecotoxicology and Environmental Safety (vol 3, p 325) of trials of ‘Nuvacron 40’(chemical name — monocrotophos), ‘a potent systematic insecticide.’
Trial one consisted of a single aerial dousing on five female and 12 male volunteers (!), ages ranging from 13 to 57 years.
Trial two consisted of crop dusting 21 males aged 22 to 50 years for three consecutive days (only 12 men remained to be sprayed all three days).
Rao and colleagues blithely observed that males were sprayed shirtless: females wore light garments and all remained in the cotton fields for 60 minutes after each dousing.
The purpose of the ‘study’ was to observe the ionic effects on cholinesterase activity.
red and white cells, and  hematocrit .
The spraying of Third World children and the pseudoresearch described by Rao and colleagues raises serious ethical and scientific issues.
These issues include the lack of informed consent, the exploitation of children as ‘volunteers’, and the unconscionable use of humans as test animals.
Publication of a peer reviewed article by members of the Ciba-Geigy Department of Toxicology and Pathology (Bombay) serves to legitimise the methods of the ‘study’, and thus the indiscriminate spraying of pesticides on children.
In India, the responsibility clearly lies with the company.
 D. R. Goldsmith Chapel Hill North Carolina, USA 
Acid rain: look for the dust
The subject of the effect of acid rain on the environment (Forum, 24 February, p 535) has received increasing recognition and it certainly behoves those with influence to reduce the quantity of sulphur compounds being released into the atmosphere.
Leaving aside what Tam Dalyell credits ‘many ecologists’ view on the Falklands, one could read the article and infer that spruce forests aggravate the consequences of acid rain.
The statement ‘If rain falls on spruce forests, the rain can become even more acid’ may have been drawn from a recent paper from the Institute of Terrestrial Ecology that certainly showed that water flowing down the stem was more acid than that caught in the open.
The villain though, is the dust collected from the atmosphere since the previous rain.
Had the trees not intercepted this dust, it would have fallen onto the surface — or blown to somewhere such as Scandinavia.
Spruce needles certainly take longer to decay than some other leaves, releasing humic acid over a longer period but at a slower rate, but being a long standing constituent of the northern latitudes coniferous belt, spruce (Picea) is quite compatible with lakes well stocked with fish.
No doubt a research project to assess the increase of acidity by rain by heather (Calluna vulgaris)would also pick up the effect of accumulated dust.
This species is the main alternative to spruce in much of the Uplands but has a much reduced capacity to take up sulphur compounds than any forest when assessed on the basis of weight per unit area.
 M. T. Rogers Commonwealth Forestry Association, Oxford 
Car lead data
The accusation by David Price (This Week, 17 February, p 425) that the Commission of the  European Communities exerted censorial pressure on the report of the Isotopic Lead Experiment is ludicrous.
If indeed the sentence referred to has disappeared during the preparation of the final draft, its content is still explicit in the report.
For instance on page 57 of the status report, we read: ‘If equilibrium had not yet been fully established by the time of the switch back at the end of 1979, then later equilibrium state Pb-206/Pb-207 ratios would presumably be lower than those seen in 1979.
Consequently the contribution of petrol lead to blood lead of adults would be higher than the average 24 per cent calculated for Turin.’
On the 61, it says: ‘The contribution of petrol lead…appears to be at least 24 per cent…
’ This is followed by a discussion of the possible reasons.
Thus the points made in the sentence quoted by Price are more than underlined in the final text.
 Dr D. Turner Associated Octel Ellesmere Port South Wirral 
SERC confused about physics
The report which gives such a gloomy view of the state of physics (Comment, 24 February, p 502; This Week, 3 February, p 287) covers only a very small part of physics.
It excludes, not only astronomy and particle physics, but nuclear physics, much of radiation and medical physics, acoustics, materials physics, and almost all applications of physics.
It is not surprising that the Science and Engineering Research Council's review committee found evidence for a low level of activity when they chose to compare such a small part of physics with the whole of chemistry or biology.
The SERC's antiquated view of physics makes it extremely difficult for physicists working in some of these lively fields to obtain consistent support for their research.
For example, people working here in acoustics and radiation physics never know which SERC committee will finally examine their applications — some of our recent applications have been to the committees for Biology, Chemistry, Chemical Engineering, Mechanical Engineering, Polymer Engineering, the joint SERC/SSRC committee or even sent off to ARC.
From the wide experience, we can say that the committees of the Engineering Board are generally more tolerant of new ideas.
There are four active research groups here, all receiving substantial support from Research Councils, charitable foundations, government departments and industry.
But only a section of one group receives sustained and sympathetic support from the SERC Physics Committee.
 Professor Daphne F. Jackson Department of Physics University of Surrey Guildford 
Sliderules
Regarding the appeal in your columns for sliderules for Mozambique (Forum, 27 January, p 256), I will collect and deliver slide-rules in the Bath area.
Phone Trowbridge 3130 9 am-4 pm. or drop them into my home.
 Richard Scrase 6 Thomas Street, Bath 
Ariane failures
You and your readers should have a more factual account of the Ariane launcher success rate than you recently presented (Forum, 10 February, p 392).
The  launcher has not failed ‘more often’ than it has flown successfully.
Three of the first four development launches were successful, and Ariane was declared qualified for operational service.
The fifth launch was the second failure.
(Moreover, the two unsuccessful rockets certainly did not ‘crash to the around’.)
As my letter perhaps indicates, I am not from ‘the old school of diplomacy’ as your article would have readers believe, and I do not like destructive reporting in an area which merits a more measured and informed approach.
 R. Gibson La Boissière Ecole France 
Internal evolution
In the article on ‘Molecular Drive: a third force in evolution’(vol 96, p 664) Gail Vines claims ‘Evolutionists, up to now, have considered only two processes that could spread mutations of genes to all individuals of a population and so create evolutionary change…. natural selection…genetic drift’.
This assertion is incorrect, for it ignores the interest by many evolutionists with orthogenesis, also known variously as ‘laws of growth’‘oriented evolution’ and ‘nomogenesis’etc.
Very simply, these terms cover a process by which evolution is internally directed.
This is not just genetic drift but a process whereby the entire structural basis of an organism is determined along particular lines (not to be confused with rectilinearity) of development.
Although ‘orthodox’ theory disregards  orthogenesis (often confusing it with rectilinearity) the existence of ‘laws of growth’was recognised by Darwin, although he failed to synthesise their role in relation to the environment and vastly overemphasised the power and importance of natural selection.
There appears to have been a greater interest in orthogenesis in the earlier part of this century and writers such as Berg considered the subject in some depth.
However more recently the evolutionist Leon Croizat has intensively evaluated the evolutionary role of orthogenesis against a background of space, time, form by pan-biogeographic method.
Given the prior interest in orthogenesis, the assertion by Gail Vines that ‘molecular drive’ is a new mode of evolution cannot be accepted unqualified.
Yes, ‘new’ perhaps in a technical sense but in the context of internally directed evolution, molecular drive is at best only one possible mechanism for orthogenesis.
With respect to a concept of orthogenesis the validity of internally direction evolution does not stand or fall on what various specialists such as molecular biologists, have to offer.
 J. R. Grehan Victoria University Wellington 
Recipe for hypochondriacs
Bernard Dixon (Forum, 24 February, p 534) may be interested to know that Jerome K. Jerome was a fellow sufferer.
In Three Men in a Boat he tells how, having gone through a medical dictionary at the British Museum — to check if he had hayfever — he decided he had everything in the book except housemaids knee.
Taking his troubles to his doctor, he was given a long prescription, which he presented to the chemist without first reading it.
The chemist suggested he try the Co-op, or an hotel.
The prescription read: 1 lb beefsteak, with 1 pt bitter beer, every six hours 1 ten mile walk every morning 1 bed at 11 sharp every night and don't stuff up your head with things you don't understand.
I would suggest that Bernard Dixon, and any other hypochondria sufferers, try the prescription.
It can't do any harm.
 Robert Kennan London N6 
Of ducks — or men?
The article by Dennis Lendrem (’ A safer life for the peeking duck’, vol 97, p 517) held no surprises.
Why?
Because it took anthropomorphism to the nth degree, Mr Lendrem's conclusions may be summarised as follows:(1) When ducks sleep they close their eyes.
(2) When ducks have their eyes open they are awake (Lendrem uses the ‘I know what's happening term’‘vigilante’.)
(3) Ducks watch out for predators.
(4) Ducks cannot distinguish cricket bats, scaffolding or umbrellas when they are carried by men.
(5) The more ducks a cat has to choose from the less likely it is to take one duck in particular.
(Lendrem uses the ‘random cat’ analogue P=1 -e-n-Bt .
I have yet to find a random cat or a one-dimensional flock of 30 ducks.)(6) Ducks feel more at ease when with other ducks.
(7) Male ducks frequently look at female ducks.
(8) The more female ducks the males have to look at the more time they spend looking at them.
(9) Brightly coloured ducks are easier to see.
(10) Ducks like sleeping.
Mr Lendrem has gone to great pains to establish one thing: that all of his preconceptions concerning bird behaviour are true.
Lendrem's conclusions, which from hereon should be termed ‘Lendrem's Axioms’, simply demonstrate that the behaviour of ducks and human beings (ethologists, for sure) are motivated by identical factors.
If all science were as Lendrem portrays it, we could ascertain everything by resort to introspection and examination of our own motives.
 Philip A Rea Bioengineering Unit John Radcliffe Hospital Oxford University, Oxford 
Energy from refuse
Your report on Corby and Edmonton refuse-fired power stations could be misleading (This Week, 2 January, p 219).
Edmonton recovered less than half its total annual costs from electricity sales last year, as you say.
But it is still cost-effective since the rest of its annual costs, £3.85M or about £10/tonne, are 27 per cent less than the average cost of refusal disposal to the GLC and only half those of long-distance hole-filling contracts (up to £23/tonne).
The price of refuse-derived power is not unreasonable, averaging 2.1 p/kWh for Edmonion.
Nuclear power, for comparison, averages 2.4–2.8p/kWh including capital charges, as shown by Professor J. Jeffery (Energy Policy , June 1982).
This ignores the saving on refuse disposal (which would have the effective price) and the fact that one method reduces landfill pollution, while the other generates a type we do not yet know how to clear up.
 James E. Thing Lambeth London SE 11 
Who will work?
Does Patrick Jenkin really believe that microelectronics holds any hope for the unemployed (The unemployed cannot blame automation, 24 February, p 526)?
The small percentage of jobs he speaks of being created are all highly skilled, whereas the majority of the unemployed are untrained, unskilled and have not the qualifications for further training.
How can people get further training when the government is closing down existing training centres?
They say that these trades taught there are no longer needed.
If this is so where are the new training facilities?
It is the fault of this government and past ones that have allowed people to train in narrow, restrictive fields, and now, when the need for change arises, we are not in the position to do so.
 Colin Reid Darlington 
ARIADNE
THIS should be the last word about exploding rats and other devices that appeal to the sadistic sense of humour in most of us.
According to a reader, the Americans thought of such things first.
In 1864, the Confederates had the idea of making torpedoes to look like lumps of coal.
This was in the days when torpedoes were not missiles that moved underwater, hence the nowadays somewhat obscure meaning of Admiral Farragut's remark, ‘Damn the torpedoes, full speed ahead!’
The torpedoes in question were fixed charges in a channel.
The coal torpedoes had a powder charge inside a hollow cast iron block that was disguised as coal with tar and coal dust.
They were dropped into coaling barges and slung into boiler furnaces with the rest.
Results, or alleged results, include the sinking of a Federal warship.
A problem with underwater limpet mines in the second world war was solved by aniseed balls, I am told.
The mines were going off too early, but aniseed balls placed between contacts proved effective, for aniseed balls dissolve at a constant rate no matter what the temperature.
They were also quite sensitive as fuses.
They could be sucked beforehand to reduce the time before explosion, though the imagination sags a bit at conjuring up a frogman sucking aniseed balls underwater alongside explosive charges.
SPECIFICALLY designed to improve the usability of a wide range of personal computers by raising the back to a calculated level.
Fundamental ergonomics reveal that this makes the keys more comfortable to operate and their printed surfaces easier to identify.
Those two sentences are straight quotations from a leaflet sent to us from Warp Factor Eight, a firm in Ware.
The product is called Hi-Stak.
It consists of two plastic wedges, with sticky surfaces to attach to the computer and rubber Feet.
Two of them can be yours, giving ‘a professional look to the personal computer set-up’, for £3.95, Enterprising I reckon.
Royal Society's study group report on risk assessment is a tough read; For example, ‘At the administrative level, there is a need to acknowledge the validity of public perceptions, while at the same time purveying the fullest information on objective risk estimates, not least because these data  form a major component of the public's perceptions,’ But it concentrates the mind and, of course , it deals with every activity of man.
All, you might say, human life is here.
I do not want to give the wrong impression by quoting that slogan of a well-known Sunday newspaper.
Those who see that one section of the report is concerned with fates worse than death may be disappointed.
This is not the place to discuss the report at length, but to mention one or two points that were aired at the press conference to introduce it.
Nuclear energy as a hazard came up ineluctably, in a discussion of the familiarity of a risk, such as that in driving a car, making it generally acceptable.
Professor T, R. Lee said that people were exercised about the dangers to life from railways when they were new, but they were eventually seen as negligible, with the implication for some that the same sequence would happen with nuclear energy.
Analogies are always dangerous, I think.
John Dunner, the head of the National Radiological Protection Board, in an aside, made the interesting suggestion that perhaps there had been altogether too much information available about nuclear energy, rather than too little, ready material to use, either uncomprehendingly or fully aware, to stoke up polemical fires.
All the same, there may be one or two risks that the Royal Society's Group have not taken into account.
One was launched into my consciousness by Linda Lee Potter, in the Daily Mail.
A paragraph about her reaction to the house in Muswell Hill being investigated by the police stated that, no matter what renovation was done there, she would not live in it.
‘Evil’, she wrote, ‘does get into bricks, tragedy into mortar,’ It is new light to me on the properties of bricks and mortar.
It raises all kinds of questions, such as how do the evil and the tragedy get out again, how long can they stay there without deterioration and many more.
Fortunately there is an answer.
Evil and tragedy, lurking in the brickwork, are apparently destroyed by fire, so burning the house down would be efficacious.
Daedalus
A FLYING MACHINE powered by the wind is an elusive goal.
A kite on a line generates lift from the wind streaming past it; but if released it soon reaches the same speed as the surrounding air and loses it all.
Daedalus, however, recalls that the wind is always gusty and turbulent.
So his prototype ‘Antiparachute’ is a big circular membrane in a tensioned wire rim, pierced with thousands of valve-like holes carrying film or fabric flaps.
These are designed, rather on the principle of the weathercock, to orientate themselves to the instantaneous local air movement.
If the air-flow at a valve is downwards, it will open to admit it; if upwards, it will close to entrap it and if it is horizontal from any direction the flap will tilt to allow it through at an angle, gaining aerofoil-type lift from it.
So the Antiparachute will always rise.
The energy spectrum of atmospheric turbulence persists down to the smallest scales, and to trap the maximum amount of energy the valves must be as small and numerous as possible, The payload of Daedalus's new craft will be conventionally suspended from cords around its rim.
Unfurled into the air it will at once fill and rise.
Its intrepid pilot will control it by hauling on the suspension lines to angle the lift generated by the canopy.
Even so, the craft will probably have difficulty in ‘tacking’ upwind.
It will have to rise to those heights where shear between two wind layers gives a choice of wind-direction, and plentiful turbulence to sustain lift.
With good meteorological predictions, directed Anti-parachute flights might be confidently undertaken, perhaps even on a commercial basis.
Even so, the first use of the Antiparachute are likely to be rather tentative and sporting.
Of course, if by some atmospheric freak the wind and its turbulence should fail, it will simply revert to a normal parachute, delivering its burden safely back to the ground.