

Storm over weather  Satellites 
IF YOU were an American government official named John McElroy, you would be feeling confused, not to say schizoid.
McElroy is in charge of the National Environmental Satellite, Data and Information Service.
This is the body that runs the American remote-sensing and weather satellites.
The first reason for McElroy's unease is that President Reagan has just told him to prepare a brief for doing away with his own job — and that of most of the other 1100 people on the administrator's staff.
All this is being done in the name of free enterprise.
President Reagan and his advisers want to loosen the government's hold on satellites; hence the instructions to McElroy to solicit tenders from companies to take over the craft that his organisation now operates.
McElroy's confusion will hardly be reduced by the preparations for his forthcoming trip to Western Europe.
During this, he will talk to the British and French governments about their sharing the costs of the American weather-satellite system (This Week, p 788).
On one hand, therefore, McElroy is preparing for private companies to take over the reigns of the weather craft; on the other, he is inviting other countries to become involved in what could be a link only between governments.
All this raises the question of why the Reagan administration wants to quit the business of satellites in the first place.
Enticing private enterprise to take over the American Landsat remote-sensing system is fairly easy to understand.
Since 1972, the US government has orbited four such satellites, initially under the auspices of NASA with the responsibility handed to McElroy's organisation only last year.
President Reagan is probably right in thinking that private companies could introduce innovations to what has become rather a moribund operation.
For instance, they could launch technically more advanced satellites and devise market-orientated ways of selling the data around the world.
But selling off the American weather satellites is another matter.
In an unusual example of global cooperation, countries that run these craft swap data freely to enable scientists in many parts of the world to build up comprehensive weather forecasts.
At the least, President Reagan's plans of involving the private sector could disrupt this system.
Furthermore, the plans to sell the satellites throw up some curious anomalies that may be difficult to resolve.
The American meteorological craft cannot be looked at in isolation: they depend not only on Earth stations but also on processing equipment on the ground that makes sense of the data that the craft provide.
The American  government may find problems tempting buyers For the satellites without selling this ground equipment too.
At this point the government would find itself sliding toward selling off the whole of the National Weather Service, something which is surely not the intention.
A second point is that any company that takes over the satellites would want to sell the data to private individuals and organisations around the world.
It would also, one presumes, sell the information to the American government which would then give it away to other countries in order to meet America's international obligations.
Data that, then, cost nothing if obtained through  government channels would have a price if sold through the medium of free enterprise — a peculiar state of affairs.
It could well be, of course, that after examining questions such as this, John McElroy decides that the whole scheme is vaguely hare-brained — the sort of proposal to which people in Britain have grown accustomed from the likes of Sir Keith Joseph.
To put it another way, President Reagan's plans are likely to attract the kind of remarks that another John — McEnroe not McElroy — normally reserves for bad line calls.
It would be in everyone's interests if the plans were quietly dropped.
Self -help for graduates
FOR THE UNEMPLOYED there is at present little consolation.
Comments that the ‘recession is bottoming out’ seem more often than not to have a hollow ring about them.
For those students who expect to graduate this year, the prospects for employment seem no better than last, with around 12 per cent of last year's crop still without jobs nine months after graduating.
But there is some good news, because it seems that some people are beginning to take seriously the problem of graduate unemployment.
Careers officers in universities and polytechnics are starting to provide ‘after-care services’ for their students, as for example the Sheffield Unemployed Graduates Association (SUGA).
Here, the university provides premises for a social centre For the graduates; the Manpower Services Commission provides the running costs.
While SUGA is mainly a group for social events and self-help, the local carers officer, help members to plan sessions on careers guidance for the 50 or so members.
Sheffield's chief careers officer, Dr Bernard Kingston, is also working with the MSC on schemes that could help graduates, for example in relaxing the restrictions that prevent recent graduates from taking part in Training Opportunities Scheme (TOPS) courses.
Many graduates could benefit from TOPS courses because their degree courses are often too specialised to allow them to acquire the generalised skills useful in many jobs.
Too often, entrenched attitudes in the UK discourage students from taking extra courses, say, in languages or typing, or broadening out, by taking business courses with engineering, for example.
There is a good case, therefore, for working out a new perspective on undergraduate courses and problems of study, perhaps by enlisting the aid of careers officers.
They are the ones who, at present, have to sort out the mess after the degree ceremonies have been long forgotten.
THIS WEEK
Yellow rain charges still not proven
AN ATTEMPT by an Australian scientist to verify American claims that Vietnamese forces are spraying chemical weapons in South-East Asia has failed to either prove or disprove the accusation.
Australian scientists went to Washington DC last week bearing an analysis of several dozen leaves and pebbles supposedly contaminated with ‘yellow rain’ during a chemical attack somewhere in Laos or Kampuchea.
H. D. Crone, a scientist with Australia's Defence Science Technology Organisation examined the samples and found that tell-tale yellow spots on them ‘are obvious fakes’.
The samples were obtained by the Australian Department of Foreign Affairs from an unnamed source in Bangkok.
Australian officials who attended last week's meeting between Crone's team and state department analysts said American experts are unperturbed.
Indeed, Crone said in his report that ‘…the examination of these fake specimens shed no light at all on the main question as to whether mycotoxins have been used as warfare agents in Laos or Kampuchea’.
Mycotoxins are produced by certain fungi and are suspected to be the deadly ingredient in yellow rain.
Crone found that the yellow spots on the leaves and pebbles were pollen from the tropical or subtropical bush of the genus Rapanea and of a tree of the genus Harpullia , common to rainforests.
After removing the yellow residues with a solvent, Crone mixed them with dimethyl sulphoxide and painted shaved mice with doses of 5, 10 and 25 micrograms.
There was no apparent effect, even though equal amounts of real mycotoxins normally cause unmistakable lesions.
News reports have ‘blown up’ these negative findings, said the Australian official.
Crone himself was visiting American toxicology laboratories last week observing methods for analysing toxins.
He talked with a leading expert, Dr Joseph D. Rosen, an organic chemist at Rutgers University's Department of Food Sciences who has analysed samples taken by the US military and intelligence agencies.
‘At this point, to start doubting yellow rain after all the other evidence…it just doesn't make sense,’ Rosen told New Scientist .
He pointed out that Crone did not use gas chromatography or mass spectrometry to determine whether mycotoxins were present in the samples; he determined only that the samples, in Crone's words, ‘cannot represent a military effective residue’.
Much is being made of the presence of pollen in the samples, yet Crone states that the mixture of pollen and fungus on the leaves and rocks probably could not have occurred naturally.
Pollen has been found in most of the samples taken by American officials.
It may be a carrying agent that delivers the toxins to the victim's lungs.
Rosen said an Australian intelligence agent told him that the word is now out in South-East Asia that money can be made selling to investigators and journalists phoney samples of ‘contaminated’ articles.
The failure of other countries to confirm their claims must disappoint US military experts, however.
They have been pressing allies to corroborate their findings.
Last December, US officials unveiled results of tests on the blood and urine of alleged victims that showed mycotoxins and their metabolites — their strongest evidence yet of chemical warfare (New Scientist, 9 December, p 627).
The United Nations has examined several sets of leaf samples.
However, said Rosen, their analysts found mycotoxins in control samples that contained none, and then missed them in controls that had been doctored with mycotoxins.
A report by the UN late last year neither confirmed nor denied US claims.
Reaction to Crone's analysis here appears to treat his findings much the same.
Smokers, chain leads to radioactive isotopes
A FRESH link has been forged in the chain between cigarette smoking and lung cancer.
New research suggests that smoke particles act as collecting grounds for natural radioactive isotopes in the air and cause dangerous levels of alpha radiation to build up in hot spots in lungs, triggering cancer.
The research results are published in the current proceedings of the National Academy of Sciences in the US by Edward Martell, a radiochemist at the National Center for Atmospheric Research in Boulder, Colorado.
He warns that the combination of the alpha radiation from decay products of the gas radon, which often build up indoors, and cigarette smoke heighten the risk of lung cancer for both smokers and those who share smoky rooms with them.
Large particles of tar in cigarette smoke, in particular, attract the radioactive products, which include isotopes of polonium, lead and bismuth.
When inhaled, the large tar particles and their radioactive passengers concentrate in ‘hot spots’ in the lung.
So small volumes of lung tissue are exposed to high doses of alpha radiation.
Martell's studies of the behaviour of radon decay products show that someone who smokes 20 cigarettes per day for 40 years has received a cumulative dose of about 100 rads at tissue sites in the lungs — a dose powerful enough to spark cancer.
Martell stresses that the situation should concern health authorities because of the ubiquity of airborne radon and its decay products indoors.
‘Radon gas, emanating from radium in the soil, is present at high concentrations in soil gases and enters homes though unpaved basements and through wet and porous structural materials,’ Martell says.
‘Building materials containing radium-226 and dissolved radon in water also contribute.’
Smoking statistics support the idea of a link.
Uranium miners who smoke, for example, can contract lung cancer within 8 to 10 years, four times faster than smokers in other jobs.
Brittle prospects for 3D snapshooter
Barry Fox
APRIL is crunch-time for the 3D camera firm, Nimslo.
The US company backed with British money has made a hasty deal to put its much-publicised camera on general sale in Britain after Easter.
If the Nimslo 3D system, like all previous 3D snapshot and movie systems, fails to catch on, then investors who have put up around £30 million will catch a cold.
When the company was trying to stimulate confidence amongst its backers, Nimslo proudly claimed that it had ‘received’ nearly £3 million in government grants to tool up for British production at the Timex factory in Dundee.
Now that the camera is being made in Japan and some 400 Timex workers have lost their jobs, Nimslo says it actually received no taxpayers' money.
Even if 3D snapshooting does catch on, and the company's hits its target of between 3 and 5 per cent of the world domestic snapshot market by 1985, there may still be no gravy train for Nimslo's backers.
Competitors are certain to jump on the bandwagon with rival systems and Nimslo's much-vaunted patents could be unable to stop them.
The company claims that its system is a ‘radically new  approach to 3D’ and is covered by patents.
But much of the basic technology is old, and thus unpatentable.
If challenged in court, Nimslo's patent coverage could well prove to be cosmetic.
The web of confusion that has surrounded the whole Nimslo venture is now so tangled that not even Nimslo can say exactly how much investment the company has attracted.
Dr Jerry Nims and Allen Lo started work together in the mid-1960s.
Between 1970 and 1978, they say, ‘several million dollars were raised from private investors’.
Then city brokers raised £25 million, making a current total of perhaps £30 million.
Nimslo will also give no firm figures for sales in the US where the system has been in the shops for a year.
But even though Nimslo dumped Timex because of inadequate production there are clearly plenty of cameras left unsold.
Photopia, the British photographic company which has now signed an exclusive sales deal with Nimslo, plans to have 3D cameras in the British shops for £I30 each by the beginning of April.
The first consignment of Nimslo 3D cameras for sale in Britain will arrive on 26 March.
There is no processing laboratory in Britain that is capable of making Nimslo prints.
So exposed films will have to be sent to the company's HQ in Atlanta, Georgia for processing — via a courier air service arranged by Nimslo.
Nimslo will charge £9.99 to produce 18 prints (4 half-frame exposures are needed for each print).
Photopia denies that the evident haste is the result of poor sales in the US.
At the end of 1982, when Nimslo last talked about US sales, it claimed plenty of trade interest.
But it is easy to generate trade interest and early sales with a gimmicky new product in gadget-hungry North America.
It is much more difficult to sustain growth once the novelty has worn off.
Nimslo first showed its 3D snapshots at the Photokina Exhibition in Cologne in October 1980.
Until then the company's reputation had been built on promises.
The 1980 pictures looked like a stacked combination of flat images.
In an effort to boost flagging confidence, Nimslo loaned cameras to the Fleet street city press.
The Sunday Telegraph tartly commented ‘If someone is nasty to me again, I shall send a photographer round to take their pictures in 3D.
That is one of the most serious threats I've ever made.’
Although Nimslo's backers may well not realise it, there is little new in the 3D technique used.
The camera has four lenses which form four conventional 2-dimensional images side by side on two frames of conventional 35-mm film.
This is then processed in the normal manner.
At the printing stage the four slightly different images are optically sliced into vertical lines.
These are printed under a grid of tiny cylindrical strip lenses or lenticules embossed on the surface of the photographic paper so that the viewer's eyes see different views.
The idea of printing 3D pictures this way was first patented by Walter Hess, a swiss optician, in 1912.
Since then it has been many times re-invented and used for 3D picture postcards.
Although the number of new patents held by Nimslo is impressive the legal monopoly which they offer is much less so.
For instance, US patent 3 852 787 has 57 drawings and meticulously describes how the camera is constructed.
But the patent has just two brief claims to legal monopoly.
These cover only the provision of a rounded rib in the camera film chamber to reduce friction as the film is wound on between shots!
So the patent does not protect any of the other details disclosed.
Crops research seeks Third World home
Anna Lubinska, Brussels
THE UNITED Nations is planning to set up the first international research centre for applying genetic engineering and biotechnology to crops.
Six countries — Thailand, India Pakistan, Cuba Italy and Belgium — are competing for the site of the $50 million research programme.
The research will concentrate on the use of crops to produce energy and fertilisers; the development of human and animal vaccines to combat tropical diseases; the improvement of fermentation techniques and the applications of genetic engineering in agriculture and food processing.
Most of this research so far has been done by private companies (mostly American).
Its markets have been the developed world and its aim a quick profit.
But Wasa Kamel, the coordinator of the project, which is being masterminded by the UN's Industrial Development Organisation (UNIDO) in Vienna, says he aims at helping the developing countries by finding new products fitting their needs.
Scientists and technicians for the 116 research posts will come from all over the world.
Some 120 trainees from Third World countries will get a two-year post.
Now all that is left to do is find a home for the project.
A meeting in Belgrade in December could not agree.
So an eight-man team of scientists is to make a lengthy tour of the candidate countries.
The final decision is expected to be taken at a special ministerial meeting in July of this year.
Acorn grows a defect
ACORN Computers, the maker of the BBC Microcomputer, last week announced it was replacing 3600 defective tape recorders sent out under the Department of Industry's scheme to provide schools with half-price computers.
The recall came after customers discovered that the tape machines, used to store data and programs for the BBC Microcomputer, damaged their programs.
If users pressed the ‘play’ or ‘rewind’controls while a program was being loaded from the tape recorder into the computer's memory, a small voltage went across the erase head, leaving a blip on the tape.
The defect was not noticeable when the recorders were being used for voice or music, but it was enough to mess up software.
The tape recorders are made in Hong Kong for Acorn.
This is the latest in a series of problems that have plagued the BBC Microcomputer since its launch in 1982.
There has been trouble with its chips, delays in providing disc-drives for the computer and a lack of software to run on it.
Some teachers have questioned the wisdom of supplying tape machines at all for the computer.
They take some time to load programs and try the patience of young pupils.
Many teachers prefer the quicker disc system.
Unravelling the politics of plutonium
Roger Milne, Snape
THE PLUTONIUM knot unravelled a shank more at the Sizewell inquiry last week when the Department of Energy revealed that it is the reluctant owner of more than 80 kg of the material — some of which does not yet exist.
A two-hour cross-examination of the department's undersecretary Robert Priddle by the Campaign for Nuclear Disarmament revealed the existence of the plutonium.
It results from the department intervening in deals under which aluminium firms paid part of the cost of building two nuclear power stations — Hunterston A and Dungeness B — in return for assured cheap power and a share of the plutonium produced in the reactors.
The department decided that it would not be politic for private companies to own plutonium and is negotiating to purchase the stocks.
The embarrassing fact is that, to secure this bizarre deal, the department is having to pay for material not yet produced, because Dungeness B was so late in coming on-stream.
Priddle also agreed that, because of a barter arrangement with the US under which Britain obtained enriched uranium for weapons-making in return for British plutonium, the civil nuclear programme had aided Britain's defence programme.
(The generating board's chief witness, John Baker, had strongly refuted this claim earlier in the inquiry.)
The deal had now  been put on ice, though material had been exported for experimental use in the Argonne and Batelle reactors in the US, said Priddle.
He refused, however, to give the isotopic content of the exported material — information that is needed to work out if the plutonium is of weapons grade.
Although Priddle was able to say where most of Britain's exports of plutonium were destined, he could not immediately account for 0–75 tonnes of plutonium from British Nuclear Fuel's military reactors at Chapelcross and Calder Hall, which has gone overseas.
The international safeguards which apparently prevent civil nuclear material from being used for arms fuelling can be side stepped by nuclear powers like the UK ‘for national security reasons’ Priddle admitted.
The  government has given repeated assurances that it has never sought such dispensation.
Anger over delay to Alvey reply
COMPUTER scientists and entrepreneurs are worried that delays by the British government in responding to the Alvey report on advanced information technology may be harming Britain's chances of joining an elite of computerised nations lining up for the 1990s.
The report called for £200  million of government spending on fifth generation computing, that is computers that can think for themselves and get along with humans.
It is now six months since the report was handed in to the Department of Industry — as long as it took to prepare the report in the first place.
But all that has emerged from Whitehall so far is rumours of interdepartmental conflict over the report.
‘We did bust a gut on the report and we naively hoped it would be decided upon sooner,’ said Dr Ken Warren, director of technology and strategic planning at Plessey and an Alvey committee member.
‘The competition is well under way, so if we don't get going now we really won't have a chance.
We can't wait until the next election.’
Warren pointed out that for the first time Britain was aiming to carry out speculative research.
‘A high level of government funding is the only way that industry can find its way through this maze,’ he said.
Philip Hughes, chairman of Logica Holdings and another member of the Alvey committee, said ‘I am showing great patience at the moment…the matter is at Prime Ministerial level.
‘It is imperative that the government responds positively to the report or else it will be psychologically damaging to research,’ said Dr Jim Howe of Edinburgh University's Artificial  Intelligence Department.
If accepted in full, the committee's recommendations will involve cooperation between three ministries — industry, education and the treasury.
They will fund up to 90 per cent of the research in some areas and will coordinate the work through a new directorate.
All three steps are causing ministerial head scratching.
Another novelty proposed by Alvey is the idea of demonstrator projects which would involve industry and academe in pooling their knowledge.
Howe said that there was no shortage of interest by industry in the work in his department.
But his staff of four could not cope.
John Lamb
Put research back on the road
MPS TOLD the government this week that it should increase spending on research into road maintenance at the Transport and Road Research Laboratory (TRRL).
The lab has suffered a 40 per cent cut in its staff since 1975.
But the all-party transport  committee of the House of Commons said, in a report on road maintenance, that the cuts should be reversed.
The Department of Transport should ‘take immediate steps to speed up the rate of progress of road maintenance research,’ the MPs said.
There should be new types of pavement design and extra research into the relative costs of concrete and bitumen roads.
Green light for Belvoir
The SURPRISE ministerial announcement last weekend to allow mining close to the Vale of Belvoir in Leicestershire means that the plan for a new pit at Ashfordby will get the green light before the contentious issue of where to put the mounds of unsightly spoil that the pit will produce has been settled.
Two working parties are due to report this summer on the issue.
They are likely to support back-stowing — that is putting the spoil back in the ground.
Leicestershire County Council is due to give the go-ahead to mine 1.5 million tonnes of coal a year from the north-east Leicestershire coal field on 14 April.
Last year the government turned down a more ambitious National Coal Board proposal for three mines in and around the vale.
Lead in Europe
CONSERVATIVES in the European parliament are being blamed for holding up a plan to push through reductions of the lead content in petrol throughout the EEC.
Ken Collins, the Labour MEP and chairman of the parliament's environment committee, says that British Tories' attempt, at a committee meeting on 16 March, to seek an opinion on lead from the energy  committee could fatally delay his proposal.
Collins wants to push through a motion calling for lead levels in petrol to be reduced to 0–15 grams per litre (the level Britain intends to impose by 1985).
He wants approval from the full parliament, as well as the environment committee, in time for it to be discussed at a meeting of environment ministers in June.
The ministers could then get to work on a directive to turn the votes into law.
Conservative MEP, Dr Alexander Sherlock, denied that his party lacked enthusiasm for reducing lead levels.
He said that a recent report in New Scientist that the Turin lead experiment had been censored, coupled with the conversion of several eminent scientists to the anti-lead cause had convinced him that action was urgent.
Collins still hopes to push his proposal through  committee sessions in time to get it to a plenary session of the full parliament in May.
Cruas
LICENSING difficulties may slow down the building of nuclear power plants in the United States, Britain and West Germany — but the French still press on almost untrammelled by objectors.
France's highest legal body has finally dismissed an appeal against a light water reactor complex at Cruas on the river Rhone.
The appeal has been five years in the hearing.
But the plaintiffs, local authorities and anti-nuclear associations must have abandoned hope of success long ago.
For Electricite de France carried on building throughout the appeals procedure and the first Cruas reactor is due to be coupled to the national grid in two months.
The protesters  appealed against both the authorisation to site a plant at Cruas and against a building permit.
They cited an inadequate environmental impact study, high social costs, and dangers to health.
Forensic silence sniffs out glue and cannabis
FORENSIC scientists are using techniques originally developed to catch arsonists to detect traces of glue in sniffers' blood.
Leslie Russell from the Metropolitan Police Forensic science Laboratory told a Royal society of Chemistry meeting last week that gas chromatography could trace toluene in the blood of glue sniffers, by the same method used to detect flammable liquids in fires.
Russell said that the police once used either steam-distillation or vacuum extraction to find traces of the common accelerants such as petrol, paraffin, paint thinners and white spirit, that spread fires.
But steam-distillation suffered from interference from products of the fire.
And vacuum extraction often suffered blockages because the debris was sodden.
Now the police have developed a new method, which is used at 60 per cent of fires where arson is suspected.
The air above the debris of a fire is sucked through a tube containing an absorbent solid to concentrate the sample.
For this the police have settled on a compound called Tenax G C (poly p 2.6. diphenyl phenylene oxide) which has the advantage that it does not mix with water.
The tube of Tenax then fits directly into a gas chromatograph.
The method can be used for accelerants like diesel fuel, which is not very volatile, by warming the debris.
The only disadvantage of the method is that it cannot be used for certain solvents, such as alcohol, which don't appear to be absorbed by Tenax.
Russell quoted one case in which a person had died in a fire.
The method had found unburnt paraffin in the person's lungs, thus suggesting that the person had inhaled paraffin and so been alive when the fire started.
He said that the method can detect toluene in glue sniffers' blood.
Barry Taylor, from the Government Chemist's Laboratory, told the meeting that thin-layer chromatography could be used to discover the origin of the seeds of cannabis plants grown in this country.
Home-grown cannabis plants have a ‘European look’, regardless of whether they have  been grown from Sri Lankan or Moroccan seeds.
However, although home-grown plants do not look like their parents they retain the chemical characteristics.
Cannabis plants not only contain THC (tetrahydrocannabinol) but also two other related compounds, tetrahydrocannabivarin and cannabidiol.
Cannabidiol, for example, occurs in plants from Morocco, Lebanon, Sri Lanka, and some plants from India.
In some cases the identity can be highly specific, in Sri Lanka only female plants have cannabidiol while in  Morocco male plants have it.
All aboard the fast trains from Munich
David Price, Brussels
WHILE British Rail frets about the faults in the wheel bogies of its new high-speed train, the West Germans have discarded wheels altogether on their new 4O0km/h Transrapid 06.
The train, which is powered by a linear magnetic motor and hovers 1 cm above its single-rail track, should make its maiden flight today (Thursday) on a 20 km run in Emsladn in Bavaria.
Its Munich manufacturer, Krauss-Maffei, hopes that by the end of this year the machine will be tested up to 300km/h.
The train hovers above an elevated rail over the fields of Bavaria.
Its magnetic motor means that the only noise from the train is a quiet aerodynamic swoosh.
The train is likely to go into service first between Frankfurt and the industrial towns of the Ruhr.
Another possible route is between the Bonn-Cologne airport and
Dusseldorf.
Export men are eyeing the 34 km stretch between Riyadh airport and the Saudi Arabian capital.
And a recent trip by the mayor of Las Vegas has raised hopes for the sale of the train as a gamblers' special to link the city with Los Angeles.
At an average speed of 305 km/h the trip would take only 65 minutes.
Quicker than a plane flight, say its enthusiastic backers.
Technology on the agenda for summit
Andrew Lloyd, Paris
WORLD'S leading industrial powers have agreed on 18 joint research projects in key areas of new technology.
The move towards cooperation stems from the Versailles summit last June and is one of the few positive initiatives to come out of the seven-nation meeting.
Some of the projects will be just an up grading of existing work.
But several new ventures will start.
Most of them will be shared among all the nations present at Versailles, plus the EEC.
But one or two countries will act as leaders and organisers.
One of the most significant will be a look at the next generation of fast-breeder nuclear reactors.
It will be jointly led by the United States and France, Britain will share the lead with France on both food technology and a project to build a network of biotechnology research labs.
The aim is also to train students from developing nations in biotechnology — bringing the venture into competition with a UNIDO project (see p 784).
Britain will also lead a project dealing with the acceptance of new technologies, and will team up with the US on new materials.
Japan will lead a venture with the French on advanced robotics and mother on solar cells and photosynthesis.
Other projects include high-energy physics, fusion, high-speed trains, and the impact of new technology on education, training and culture, and on traditional industries (in which the US is participating).
Details of how the projects will be implemented should be spelt out by project leaders at the Williamsburg summit in May when the nations review progress.
French science giveaway
FRANCE is to step up its science aid to the Third World.
A package of new loans made public last week reveals that the nation will move away from its traditional hunting ground of French-speaking Africa, where it has always had a science presence, and towards English-speaking and Latin American nations and the Far East.
The aid is not pure altruism.
‘In the long term, there should continue to be significant commercial interest (in such policies),’ Bob Miller an economic analyst with Hudson Research Europe told New Scientist .
‘Contacts at professional and academic level, seminars, familiarisation with techniques, will build up a rapport which tends to pay off in the long run,’ he said.
The Third World provided France with its only significant balance of payments surplus last year.
Top of the French hit-list are India and Brazil (where cooperative agreements already exist), Vietnam, Indonesia and Mexico.
Egypt and Zimbabwe are also targets for deals.
A major exchange between the French science body, the Centre National de la Recherche Scientifique, and its Vietnamese counterpart  is in the offing.
It will cover biology, electronics and data processing as well as engineering.
France is under pressure to reduce its involvement in the scientific affairs of Francophone nations.
It is ‘decolonising’ the Ivory Coast, where some 450 French scientists, technicians and their families are working.
Meagre contributions from Britain to Third World science bring forth expressions of sympathy from French officials for the Overseas Development Agency.
‘We hear lots of talk of science cooperation with Sweden, Germany and Canada, but not with Britain,’ said one.
Nuclear nations plump for LOFT safety research
THE UNITED States has stepped smartly into the lead in research into the safety of nuclear reactors, following the death of Europe's Super-SARA project earlier this month.
Last week nuclear scientists from Britain, Japan, and several
West European nations met in the US to haggle over a new joint programme of research at the LOFT (loss of fluid-test) reactor in Idaho.
British representatives from the UK Atomic Energy Authority are anxious to be able to show that, after the Super-SARA debacle, LOFT will be able to demonstrate the safety of their design for the Sizewell B reactor, Britain's first pressurised-water reactor (PWR).
The European Council of Ministers on 10 March got fed up with Super-SARA — which was to have been Europe's model for testing the conventional wisdom on safety designs — and axed the £200 million project.
It had become a political hot potato, and time ran out as backers bickered over what tests to run.
While Europe quibbled, the US Department of Energy scored a coup by winning international support for the seven-year-old LOFT facility.
The first of its kind, LOFT is a 50 megawatt thermal, scaled-down PWR located at the heart of a vast experimental reactor station in the plains of Idaho.
It is designed to act out potential accidents that could occur in a full-scale, commercial reactor, and it claims to come closer to recreating the real thing than any other testing facility.
But, like Super-SARA, it is costly.
Now a joint project involving the US and the  Organisation for Economic Cooperation and Development (OECD), has saved the LOFT reactor just as Washington was about to shut it down to save money.
Over the next three years, Britain, Japan and West Germany will pump $5 million apiece into accident simulations at LOFT.
Other participants, who will pay less, include Italy, Switzerland, Sweden, Austria and Finland.
The US government's Nuclear Regulatory Commission and the private Electric Power Research Institute will also pay into the pot, which totals $93.5 million, for tests and for eventual decommissioning — far cheaper than Super SARA.
Don McPherson, Britain's man on the project, said the shadow of Super-SARA was a ‘tremendous hindrance’ to negotiations for a LOFT consortium.
But, he added, ‘we could have been a factor in the, failure of Super-SARA.
In fact, British scientists and engineers stand to save money handsomely, given the comparative cost of joining the LOFT consortium and helping to build Super-SARA.
Participation in LOFT is a key to  Britain's budding romance with PWRs, said McPherson.
‘The UK felt it very important to be a member of LOFT while the (Sizewell) inquiry was going on, so they could demonstrate their seriousness in picking up the questions on safety’.
One of Britain's representatives, Dr Jesse Fellow of the UK Atomic Energy Authority, is trying to persuade the members to run tests that will help the case for the Sizewell PWR at the inquiry.
Because of LOFT's small size, Fell says, previous tests in which coolant water is shut off from the core have overstated the effectiveness of emergency systems designed to keep temperatures down.
Fell wants to simulate loss-of-coolant accident and then use about two-thirds the usual emergency cooling water to mimic what could occur at Sizewell.
Fell would also like to allow temperatures to rise high enough in the reactor to test theories about how fuel cladding could ‘balloon’ at high temperatures as high-pressure helium builds up inside fuel rods in an accident.
The danger here is that this ballooning could cut off the emergency cooling water altogether.
These new trials won't come in time to make a difference at Sizewell.
Fell told New Scientist , ‘but Sizewell's case has got to be made without relying on LOFT’.
Nonetheless, the assurance that someone will continue to put safety assumptions to the test could assuage some concern over the Sizewell reactor.
LOFT's former director, Larry Leach, is now in Britain keeping tabs on the Sizewell inquiry for the US government.
Leach says LOFT is a boon to Britain because it can answer new questions raised by Britain's stricter philosophy on safety margins.
That philosophy requires tests that have not been done before in the US, and that may never have been done but for the consortium's help.
Save the Black Forest
WEST GERMANY wants its partners in Europe to speed up work on combating acid rain by stricter controls on air pollution.
Last week, to reinforce its case, it brought the President of the European Commission, Gaston Thorn, to Baden-Wurttemberg, where acid rain is blamed for damage to some 64000 ha of firs and pines in the Black Forest.
The West German government has just adopted its own controls on emissions from power stations and industrial furnaces.
But critics point to wide exemptions.
Those let off include old power stations and new plant which would be ‘unreasonably expensive’ to clean up.
The new regulation will cost the Germans some £1500 million over 10 years.
The government is now worried that its industry will suffer from unfair competition if other EEC countries do not have to fork out for the extra equipment necessary to reduce  emissions .
West Germany wants to push the case before its presidency of the Council of Ministers expires in June.
Hence last week's walk in the Black Forest for Gaston Thorn.
Thorn gave a cool reception to a suggestion for a European research centre in Karlsruhe to study the causes of forest destruction.
But the Commission is at work on a draft directive that would require up-to-date pollution controls in new factories.
The directive could be ready for adoption later this year.
The Commission is also under pressure from the Swedes who blame nations like Britain and West Germany for the acidification of their lakes.
The Swedes recently sought support from the Commission to beef up the 1979 Geneva convention on long-range trans-boundary pollution.
At present the convention, which came into force this month, only provides for an exchange of information, consultation, research and monitoring.
Super-SARA'S hopeful legacy
OUT OF the ruins of super-SARA, the EEC's science commissioner Etienne Davignon has won a major reform to decision-making for the EEC's Joint Research Centre.
The JRC is to get a new Council of Administration to keep the formulation and management of research programmes at arm's length from the politicians.
The chairman of the new council will be Sir John Adams, a science advisor to the EEC and virulent critic of the mishandling of super-SARA (New Scientist , 10 February, p 354).
The formation of the council will allow the abolition of a vast number of advisory committees.
And community research will no longer be artificially divided up according to what proportion of the bill the Commission pays.
Instead a senior Scientific and Technical Policy Committee will oversee the lot.
Apart from expanding into fusion research (New Scientist , 10 March, p 631) the JRC hopes to fill the gap left by the passing of Super-SARA with a number of other big projects.
They include a laboratory for handling tritium, the heavy radioactive isotope of hydrogen which is a fuel for fusion reactors.
At present no capacity for tritium handling exists in Europe outside the defence community (Britain manufactures tritium for the defence programme at Chapelcross, Dumfries).
A second project is a large vibration generator for use in research of the resistance of larger structures (including nuclear power stations) to earthquakes, explosions and aircraft crashes.
Now Reagan wants Europe to pay for weather eyes
Peter Marsh
THE US is sending envoys round the world to persuade developed nations to cough up money to help pay for US weather satellites.
The end product could be a jointly-owned network (unless the Reagan administration decides, instead, to sell its  satellites off to private enterprise).
Discussion will continue at the summit of leading industrialised countries at Williamsburg, Virginia, in May.
Under existing arrangements for weather satellites, the US, USSR, Japan and Western Europe operate weather craft that either hover above the Equator or pass over the poles.
India will soon join the club.
Data from the craft are swapped via a global telecommunications system coordinated by the World Meteorological Organisation in Geneva.
The US is worried about the costs of running its five weather satellites, two of which travel over the poles.
By cutting one of the polar craft, it could save up to $50 million of its annual budget for the craft of $230 million.
If other nations want the data, they should share the costs, it says.
To test the water, John McElroy, an American official, is visiting Europe for two weeks from 14 April.
He is the administrator of the National Environmental Satellite, Data and Information Service, which runs all the US's remote-sensing satellites.
He will meet civil servants from Britain on 15 April.
Next week Harold Yates, McElroy's deputy, holds similar discussions in Japan.
In Britain, the main body that would benefit from a joint approach to weather satellites is the Meteorological Office, run by the Ministry of Defence.
Scientists at the Met Office are prepared to go along with the US plan.
But Britain is likely to press for a deal under which its own industry provides hardware for the satellites.
To complicate the issue, the American government may soon hand over responsibility for operating the weather craft to private industry instead (New Scientist , 17 March, p 706).
This week, the nations of Western Europe met in Paris to discuss a new organisation to run meteorological satellites.
Eumetsat would launch three satellites over eight years for £250 million.
It would replace the current informal arrangements coordinated by the European space Agency.
Reprocessor corpse revives
BIG-MONEY court-battles over the dereliction of the American nuclear industry are growing popular.
In the aftermath of the recent initial settlement over the wreck of the Three Mile Island power station, a new battle began last week over the half-built shell of the Barnwell nuclear fuel reprocessing plant in South Carolina.
The plant was begun by a consortium of private companies headed by the chemicals conglomerate, the Allied Corporation.
In 1977, after $217 million had been spent, President Carter slapped a ban on commercial reprocessing.
He feared that the plutonium reclaimed at the plant from irradiated fuels could fall into the hands of terrorists and be used to make crude atom bombs.
President Reagan lifted the ban two years ago, but the Barnwell consortium says this is not enough to expunge the injury caused by Carter to its plans.
The government has still not come up with a dump for the masses of intermediate wastes, such as fuel cladding, that the plant would produce.
And rules prohibiting the commercial use of some products that the plant was designed to produce have not been rescinded.
So last week Allied, with its partners Gulf Oil and Shell, filed a suit in the Us claims court for more than $500 million in damages.
Forest firms fear conservationist's backlash
Catherine Caulfield
BRITAIN'S timber importers are worried that public concern about the destruction of tropical rainforests could harm their business and they are taking unusual steps to defuse protest.
They have recently succeeded in persuading the World Wildlife Fund to change the wording of an advertisement that launched an international campaign to save tropical forests.
A phrase blaming ‘the greed of commercial interests’ for the destruction of forests has been changed to point the finger at ‘ever-increasing consumer demand’.
The industry has also prepared its own publicity material which emphasises the damage peasant farmers do to forests.
The National Hardwood Importers Section of the British Timber Trades Federation donated money to the World Wildlife Fund (WWF) after the advertisement was changed.
The first threat of a consumer boycott in Britain came in an article in mid-1981 in the Architects Journal , calling upon architects not to specify tropical hardwoods in order‘to save what is left’ of the world's rainforests.
Early last year the North Hertfordshire branch of Friends of the Earth conducted a survey of importers of tropical timber.
The group said it was compiling a list of companies with ‘an interest in destroying the tropical forest’.
This caused some alarm in the British timber trade.
At the same time British and European importers heard a rumour that the World Wildlife Fund was planning to call for a boycott on the use of tropical timbers.
An article in the Timber Trades Journal reported Michael Latham, chairman of a major UK timber firm and then president of the European tropical timber importers union, as saying that: ‘The entry of the WWF into the field enlarged the scale of the problem for the timber trade, since before that time the trade had been dealing with small local conservationist groups’.
Latham wrote to the Duke of Edinburgh, the president of WWF, to arrange a meeting.
Subsequently sir Arthur Norman, the WWF chairman in Britain, and Peter Palmer, the coordinator of the fund's international tropical forests campaign, showed a draft of a WWF advertisement for the tropical forest campaign to members of the hardwood importers group (NHIS).
The woodsmen took exception to the phrase about ‘the greed of commercial interests’.
Latham later said that Norman and Palmer were told that ‘if the WWF wanted the cooperation of the timber trade that phrase would have to be removed’.
Norman agreed that the wording was unnecessarily offensive and it was changed.
WWF director, George Medley told New Scientist : ‘We have to work with all those people who are likely to be donors.’
Officials of the WWF in Britain deny receiving any funds from the NHIS.
‘We have no trace here of such a donation being received,’ Medley said.
But according to Christopher Holmes-smith of the Timber Trades Federation, the NHIS sent ‘not a terribly large’ cheque to sir Arthur Norman in October of last year ‘as a sign of our support for what they are attempting to do.’
In Switzerland the WWF has persuaded the Swiss Timber Association to support efforts to switch from tropical hardwoods to local species.
Welcoming the association's decision, or Charles de Haes, the director of inn, said ‘In some cases developed nations prefer to exploit inexpensive and easily obtainable tropical timber, at a considerable environmental cost to the producing country, rather than cut their own substantial timber reserves.’
Lasers probe the atomic nucleus
Derek Eastham
Laser light cannot interact directly with the atomic nucleus, but experiments with lasers are revealing details of nuclear size and shape and could help to solve key problems in nuclear physics
THE NUCLEUS sits at the heart of the atom, its constituent protons and neutrons bound together by the strong nuclear force.
The energies required to probe the nucleus are therefore high, so it may at first seem surprising that visible light with its moderate energies can reveal important information about the nucleus.
Such research has become possible only with the advent of tunable lasers, which can emit light over a range of wavelengths.
Experiments with lasers are now being carried out in nuclear physics laboratories throughout the world, revealing clues as to the shape and size of nuclei, for example, and providing a precision and sensitivity of measurement not possible before.
There are a number of types of experiment, but I shall concentrate on only two techniques in order to illustrate how lasers come to the aid of the nuclear physicist.
One very popular experiment is to measure the size and shape of a nucleus, and its intrinsic magnetism. or magnetic moment.
To do this we look at the very small differences in energy levels of electrons in the atom that these nuclear properties produce.
Atomic energy levels depend principally on the nuclear charge, which is determined by the number of protons in the nucleus.
The positive charge of the protons generates an electrostatic field, which binds the negative electrons of the atom to the nucleus.
If the nucleus has a net intrinsic angular momentum, or spin, then, because it is charged, it produces a magnetic field which influences the energies of the electrons.
And if the nucleus is nonspherical the electrostatic field it produces, especially close to the nucleus, alters the atomic energies.
These two effects split the atomic energy levels into several components, producing the so-called hyperfine structure.
Another effect that can be measured is the ‘isotope shift’.
As its name implies, this is a shift in the atomic energy levels from one isotope of a particular element to the next.
Isotopes of an element have the same number of protons in the nucleus — and hence the same chemical properties — but different numbers of neutrons, and thus have slightly different masses.
The isotope shift is caused partly by the change in the mass of the nucleus and partly by the alteration in its size.
The latter affects the energies because the atomic electrons can actually penetrate into the nucleus, or, in the jargon of quantum mechanics, the wave function describing the position of the nucleus overlaps with that of the electrons.
Inside the nucleus, the electrostatic field no longer varies as the inverse square of the radius, as it does outside the nucleus, and the overlap of wave functions here determines the size of the isotopic shift.
Both the isotope shift and hyperfine splitting are very small in comparison with average atomic transition energies.
The differences between electron energy levels in atoms are typically in the region of a few electron-volts, which corresponds to the energy of photons of light in the visible part of the electromagnetic spectrum.
The isotope shift and hyperfine splitting might alter these energy transitions by about one part in a million.
Nevertheless physicists have known about these effects for a long time; indeed, measurements of splitting using radio frequency techniques (with correspondingly low-energy photons) provided the first direct evidence of non-spherical nuclei.
The reason for the excitement in using lasers is that they enable us to study unstable nuclei, which was not previously possible.
Most known nuclear species are in fact unstable, so the laser provides us with a single technique to measure directly shapes, sizes and magnetic moments of many more nuclei than we could study before.
Experiments using lasers give very accurate results; they are also extremely sensitive, so only a small number of atoms are required.
This means that we can now study ‘exotic’ nuclei produced in small numbers in nuclear reactions using particle accelerators.
Exotic nuclei have more protons or neutrons than do stable nuclei, and studies of them provide valuable information on the balance of nuclear forces when atoms deviate from the most favourable ratios of neutrons to protons.
In addition the laser methods are so powerful that they can be applied to excited nuclear states.
So far this has been confined to states with lifetimes longer than about one millisecond (’ isomeric’states) but experiments are under way to study excited slates in nuclei with lifetimes as short as microseconds.
There are many different ways to make these measurements on unstable nuclei.
I will describe two techniques that employ the method known as resonance fluorescence spectroscopy (RFS) for determining atomic transition energies.
Both methods are currently being used at the Nuclear Structure Facility (NSF) at the Daresbury Laboratory near  Warrington .
This is an accelerator constructed for nuclear structure research using heavy ions, and it can produce exotic nuclei.
In RFS a parallel beam of light at a single frequency (or energy) from a tunable laser is directed at the atoms.
This light is tuned so that the photon energy exactly matches the desired atomic transition energy.
When this happens the atoms absorb photons and spontaneously re-emit light in all directions.
The emitted light thus signals the excitation of the atomic transition.
The main problem in these experiments is to overcome doppler effects caused by the random thermal motion of the atoms.
The doppler effect alters the frequency of light ‘seen’ by the moving atoms; because the atoms are moving in all directions, they see a broad range of frequencies.
So, if a laser is shone directly into a gas the motion of the atoms relative to the laser will broaden considerably the range of frequencies absorbed.
To overcome this, the light is shone across (usually at 90o) an atomic beam (Figure la) or directly along a beam of fast atoms all moving with the same velocity (Figure 1b).
In the work at Daresbury the atoms to be studied are created in nuclear collisions produced by an accelerated beam of particles from the NSF.
The unstable (radioactive) atoms formed in this way are ionised (have one electron removed), accelerated and separated according to their mass, the latter by means of a magnetic field.
The separated ions are converted back to atoms by passing them through lithium vapour, from which the ions easily pick up their missing electrons.
The atoms can then be made to interact directly with the laser beam, as in the so-called collinear methods, or they can be converted to an atomic beam by heating in a small hot tantalum tube.
Both these schemes have their relative merits.
A considerable advantage of the collinear technique, however, is the ability to keep the laser frequency fixed while changing the frequency of light seen by the atoms by altering their velocity, using the acceleration voltage of the mass separator.
Figure 2 shows the results of some work using an atomic beam containing all the stable isotopes of the element samarium (atomic number, or number of protons, 62).
Each isotope is clearly resolved.
This would not be the case if a gas cell were used, as the doppler broadening would cause the peaks to merge together.
From these results the change in the nuclear charge radius with isotope can be obtained (Figure 3).
(The data shown here come from the work of John Griffith and colleagues at Birmingham University who have pioneered many of the techniques in this field.)
The nuclear size changes in a fairly smooth way above and below mass number 150 (total number of protons and neutrons) where there is a sudden change in average radius.
This change shows up directly in the uneven spacing of Figure 2, and corresponds to a change in shape of the nucleus from a sphere to a prolate spheroid, or ‘rugby ball’ shape.
Changes in nuclear shape are an extremely sensitive way to test our ideas concerning the nucleus.
These ideas are usually framed in terms of‘models’ because it is not possible to solve exactly the complex quantum mechanical equations that determine the way in which neutrons and protons are bound together in a nucleus.
One simple model of the nucleus, known as the liquid-drop model, likens the nucleus to a simple electrically-charged drop.
Of course, the parameters that determine the drop's energy refer to nuclear matter and not to any familiar liquid.
This model allows us to calculate the average trends in the binding energies of nuclei, but it predicts incorrectly that all nuclei should have a spherical shape, which varies as the cube root of the atomic weight, just as the radius of a liquid drop varies as the cube root of its mass.
To make predictions about nuclear shapes a model needs to include aspects of the individual motion of the neutrons and protons.
The nuclear shell model does just this and assumes that the individual neutrons and protons move in an average spherical or deformed (spheroidal) potential field.
The shell model can be combined with the collective aspects of the drop model to produce formalisms called macroscopic-microscopic models.
Although these have had a great deal of success in calculations of nuclear masses and shapes, recent laser experiments have produced some results that do not agree with them.
For example, the new work shows that in some regions of the periodic table the average change in radius along an isotopic chain does not vary as the cube root of the atomic weight.
This can be understood by assuming that the neutrons do not mix uniformly with the protons but tend to form a skin on the surface of the nucleus.
Indeed it is possible to construct a model, called the droplet model, that puts neutrons and protons in separate concentric liquid drops of different radii.
Such a model facilitates accurate predictions of the average change in charge radii for nuclei.
The next stage might be to combine the droplet model with the shell model though no one has done this yet.
Another aspect of the data that has not been adequately explained is known as odd-even staggering.
The improved accuracy of the laser measurements reveals that along an isotopic chain the radius of the nuclear charge does not vary smoothly but shows a saw-tooth effect with a nucleus with odd mass number often lying below the straight line between the two adjacent nuclei with even mass number.
This illustrates dramatically the influence of the individual nucleons on the nuclear size, but as yet no quantitative calculations have been able to reproduce the effect.
A second type of experiment at the NSF concerns the use of lasers for ultra-sensitive detection of isotopes and elements.
Figure 2 shows that as well as providing values of isotope shifts we can use RFS to measure the amount, or abundance, of a particular isotope in a sample.
Experiments that can accurately measure the abundance of naturally-occurring rare isotopes, such as beryllium-10, carbon-14, aluminium 26, silicon-32, chlorine-36 and iodine-129, would be important in archaeology and geology, because the proportion of these isotopes can give a measure of the age of rocks and artefacts.
Normal methods of mass spectroscopy do not work in these cases because the isotopes are often obscured by molecular impurities of the same mass.
With laser techniques, however, the molecular energy levels of the impurities are quite different from the atomic ones.
A most important consideration in laser techniques is the sensitivity of detection, which depends on the probability (the ‘cross section’) for absorption of photons by atoms.
Because this is a resonance process the effective area or cross section of each atom is increased by perhaps 100 000 times.
This means that certain atoms, which decay straight back to the ground state, can be made to absorb and re-emit thousands of photons during their transit across the laser beam.
The detection of single atoms in this way has caused great excitement because of its application in many areas.
One of the problems with laser techniques however is that often the first excited atomic transition — to the next available energy levels of the atom — corresponds to energies in the ultraviolet part of the spectrum, which is as yet inaccessible to lasers.
We can overcome this by employing the simultaneous absorption of two photons or by using a molecular compound containing the element of interest, although both these methods have certain drawbacks.
Another similar technique to RFS is known as resonance ionisation spectroscopy (RIS).
This was pioneered by scientists at the Oak Ridge National Laboratory in the US specifically for detecting single atoms of one particular element.
In this process electrons are pumped into the first excited atomic level by absorbing light from a laser tuned to the correct frequency.
But instead of allowing the excited atom to decay by spontaneous emission, as happens in RFS, the atom is photoionised using a second laser beam.
Processes of three or more steps are required, involving other atomic levels, when the ionisation potential is large.)
The advantage of this scheme over RFS is that ions are detected rather than photons and there are no background problems from scattered photons.
Experimentally the system is extremely simple and consists of a device rather like a geiger counter filled with an inert gas and containing the atoms of the element in question.
The laser beam(s) are shone directly into the gas and ions are detected as pulses across the high-voltage electrodes.
One of the many proposals for using RIS in nuclear physics concerns the mass of the neutrino and its implication in the missing mass of the Universe (New Scientist , vol 86, p 308).
The  neutrino is produced in the beta decay of nuclei, when a neutron converts into a proton, and an electron.
Theories of the weak nuclear interaction which describe this process also predict that it is possible for the nucleus to decay by the simultaneous emission of two electrons or positrons (double beta decay).
The rate for double beta decay is extremely sensitive to the mass of the neutrino and would provide an accurate method for measuring it.
However, the half-life of the process is something like 1022 years; in other words half a sample of nuclei would decay by this means in 1022 years.
The technique therefore requires extremely sensitive methods.
One proposal is to take a 2-kg piece of tellurium and leave this for a year; tellurium is chosen because it transmutes into xenon, which is unreactive and relatively easy to detect.
During this time approximately 1000 of the 1025 atoms of tellurium in the sample would change to xenon by double beta decay, and the xenon atoms could be detected by RIS methods.
Another important rare process that lasers may help us to detect is the decay of the proton (New Scientist , vol 85, p 1016).
The lifetime of the proton in a nucleus is predicted to be greater than 1025 years, so it is not feasible to wait for a sufficient number of transmuted atoms to accumulate.
But it is possible to assess the ratio of elements in geological samples, which have been present since the formation of the Earth four thousand million years ago.
This geological technique may also be used to estimate the flux of neutrinos from the Sun during the Earth's history.
Although I have described two applications of lasers in nuclear physics there are many others, and the number continues to increase as laser technology advances.
The ideal laser for nuclear physics would be one working in the gamma ray part of the spectrum, called a GRASER.
Such ‘photons’ could be made to interact directly with the nucleus, where transition energies are comparable with gamma-ray energies.
The problems in constructing such a device seem at present insurmountable, but who can predict the future?
Can we measure social relationships?
Joe Crocker
Observing the way that humans respond to each other is one thing; interpreting those observations to give a useful insight into human social behaviour is more difficult
THE WAY that humans react together socially is something we all feel we know about.
Yet can we quantify what we know?
And if we can, do we learn anything that we did not know already?
Human social behaviour is certainly not a subject to describe in mathematical formulas; and to some people its study is not a ‘true’ science.
But in recent years students of human ethology — human patterns of behaviour — have begun to follow the lead of their counterparts who observe animals, and to collect and catalogue raw empirical data.
How useful these data are in revealing anything new about human social relationships depends very much on how we interpret them.
Some ethologists favour a purely quantitative approach, while others prefer a more subjective treatment.
But as researchers are beginning to realise, the way to learn the most about ourselves is probably to combine the two approaches.
The first great observers of animal behaviour, such as Konrad Lorenz and Niko Tinbergen, stressed the need to observe meticulously how animals behave together in their natural environments.
Before we can begin to understand social interaction we must have a sound descriptive data-base: an ‘ethogram’ or compendium of all the behaviours an animal typically performs, how often they are performed, with whom and to whom.
For any social species whose behaviour is less regular than clockwork, even this ground-clearing goal is a daunting task.
Most ethologists have balked at the prospect of constructing an ethogram of human behaviour.
Besides, it is neither easy nor ethical to perch with notebook or video camera over spontaneous scenes of human mating or aggression.
Inevitably, human ethologists have kept to more circumscribed activities, such as children playing in a nursery or mothers looking after their babies.
Mothers and babies are suitable cases for treatment because they tend to stay in one place, there are only two subjects at a time, and their behaviour together is relatively stereotyped and uncomplicated.
All babies need feeding and changing and there are not too many ways of doing it; the range and subtlety of young babies' gestures is restricted (they do not for instance converse), and in response, mothers tend to simplify and exaggerate their own gestures.
Although ethologists do not underestimate the complexity and variety of interaction between mothers and their infants, they are at least hopeful that they can deal with the situation.
Supposing, then, you have a mother who is willing to be observed with her baby, where do you begin your objective description?
For how long do you look?
The time span of observation in these studies varies from a few minutes to several hours.
But even if one were to follow a couple for a whole day on repeated occasions, would this do justice to the full range, texture and history of the changing day-to-day relations between mother and child?
Marian Yarrow and Carolyn Waxler at the National Institute of Mental Health.
New York, have warned against sampling behaviour as if it were a constant physiological state.
’ By drawing a drop of the patient's blood, it is possible to assess a host of conditions.
whereas a five-minute drop of mother-child interaction or child aggression fails to accomplish similar miracles.’
Choosing what to sample creates even more problems.
No two observers see the same sequence of behaviour in quite the same way.
Two observers interested in, say.
some aspect of ‘harmoniousness’ between mother and child may pitch their descriptions at different levels.
The one with a high speed camera may analyse, frame by frame, the synchrony of the baby's movements with the dynamics of its mother's voice.
On the other hand, the psychology student may evaluate the content of the mother's comments to her baby, perhaps rating them as ‘affectionate’ or ‘prohibitive’.
Are social interactions adequately captured by separate (albeit simultaneous) accounts of the partners' actions, or do we need to describe the interaction between the couple?
Do we, for instance, place a tick in the square labelled ‘baby burps’ and another in the box called ‘mother pats back’; or do we include a box called ‘mother responds to baby’; or at a higher level, does she score a point on an index of ‘sensitivity’?
Although observers will each solve the problem by tailoring the categories to suit their special interests.
‘lumping’ behaviours according to common sense or ‘splitting’them into units have competing attractions.
Ethologists graduating to human beings from animals have tended to be ‘splitters’.
They spurn any subjective dressing up of the naked data.
Nicholas Blurton-Jones of the Institute of Child Health at the University of London, insists that ‘Whenever we find ourselves wishing to qualify a description with an adverb or an adjective, it is clear that there is something we have failed to describe.’
Categories should be specified unambiguously and therefore objectively.
The ‘lumpers’(usually psychologists) counter that objectivity is too often gained at the expense of something more valuable, namely, ‘meaning’.
What does it profit us to list the number of times a mother rubs her baby's back when we really want to know if she did it with affection or out of nervousness?
These important nuances are often recognised only after a long and intimate experience of the couple under study.
Primatologist Hans Kummer, professor at the University of Zurich, confesses that, while anyone who knows monkeys is aware that ‘a single social act such as a scream may serve widely different goals…we have not usually had the courage to face the task of differentiating, but dutifully entered the act into our record sheet as what it clearly was: a carefully defined and carefully counted scream.
The splitters reply that only fine-grained analysis can identify the precise patterns of actions on which we base our subjective impressions, and perhaps uncover other coherent patterns for which we have no names.
The clinching argument in favour of splitting is that the categories can always be recombined into the original, overall meaning; whereas detail is permanently lost from approaches that are impressionistic from the outset.
While some experts doubt that behavioural ‘parts’ can in fact be reconstituted to form some meaningful ‘whole’, there are other, more practical, problems in overenthusiastic splitting.
Is it actually feasible to reduce social matter to components of time and space?
The smaller one's units of analysis the more of them are needed to cover the spectrum of interest.
Exhaustive observations make for exhausted observers, and the subsequent re-synthesis becomes quite mind-boggling.
In half an hour of watching there are 360 five-second units.
Each of these may be scored for as many as 50 categories and repeated for say 30 individuals.
Having coded the behaviour, a simple pairwise correlation analysis for 50 variables gives a matrix of 1225 unique combinations to mull over: a psychological Rubik cube to be twisted and turned in the grim hope that some sort of pattern will emerge.
Cheap and accessible computer packages have unburdened the splitters of much donkey work; the computer can take over the job of interpretation too.
Statistical packages will now perform a battery of sophisticated multivariate analyses that reduce unwieldy correlation matrices to a few succinct dimensions.
The experimenter need simply put a name to these dimensions, preferably recognisable names such as‘aggression’, ‘intelligence’ or ‘attachment’.
But, to lump or to split?
Methods that rely on a kind of regimented subjectivity, directing observers to rate relation ships along scales from ‘warm’ to ‘cold’, ‘responsive’to ‘unresponsive’and so on, have proved useful in distilling lasting qualities of individual personality.
Individuals rated on these scales as ‘socially skilful’ in one context often score highly on the same scale under different circumstances, perhaps years later.
It seems that the quality of information built up through this kind of rating is different from that collected during more detached observations.
This weakens the splitters' argument that to reconstitute high-level meaning from the basic ingredients requires only the right recipe.
A study of early interactions between mothers and their preterm babies demonstrates this point nicely.
Roger Bakeman and Josephine Brown at the Georgia State University have compared the social behaviour of preterm babies and their mothers with that of mothers with babies born at the end of the full nine months.
They recorded 120 types of behaviour continuously during several feeding sessions.
From these categories the authors designated 42 as acts of communication by the mother and 32 that could be interpreted as communication by the infant (burps and smiles for example).
They split the 50 minutes of observing time into 5-second units and classified each period either as ‘infant acts’, ‘mother acts’, ‘both act’, or ‘both quiescent’.
Bakeman and Brown found that mothers of preterm babies spent more time actively coaxing responses from their infants; that joint communication was more likely to be initiated by full-term babies; and that the flow of action between the four states was more varied and less predictable among full-term babies and their mothers.
These measures, derived from the basic data, contrasted the two groups much more clearly than did simple counts of frequency of behaviour in the original catalogue of 120 categories.
By intelligent manipulation the authors were able to organise a pile of numbers into a convincing conclusion.
Mothers have definite expectations of the amount of social feedback from their infants; and mothers of preterm babies work harder to achieve their quota.
Bakeman and Brown were hopeful, understandably, that their neat and powerful descriptions of early interaction would help them to predict how the children would perform later in their development.
But this was not the case.
The result of social and cognitive tests administered to the children between the ages of one and three obliged the authors to conclude that their data provided ‘little or no support for the notion that how a mother interacts with her baby during the baby's first few months of life has any particular consequences for later social or cognitive development’.
However, there was one measure of early interaction that did predict social competence in three year olds.
That was a subjective rating of the infants' responsiveness to their mothers during those first months.
Given the interest of Bakeman and Brown in finding early signposts to later development, it seems ironic that their sophisticated and hard-won ‘dialogic variables’ should fail to match up to personal intuition.
The success of rating scales in latching onto enduring aspects of human behaviour is also their biggest drawback; they cannot tell us how individuals change and adapt.
Rating scales exploit the consensus among human observers in judging which aspects of an interaction are ‘surface noise’, and which carry ‘the tune’.
It says a lot for our common social perceptions that various observers can agree in their assessments of an individual seen with different people, in different places at different times, but only a detailed inventory, as Robert Cairns and James Green at the University of North Carolina at Chapel Hill, have pointed out, can tell us how actual forms of behaviour are elicited, maintained and organised.
A child rated as ‘attached’ to its mother at nine months (crying when she leaves the room, for example) may express its attachment to her again at 18 months but through a quite different behavioural repertoire (leaving mother but repeatedly checking back).
Scores on behavioural catalogues may appear fickle from one context to another precisely because they are sensitive to subtle changes.
If ratings are good at picking out enduring themes, while detailed observations are indispensable to explanations of how these themes arise, then the best analytical method should not choose between splitting and lumping but rather combine them.
Many students of social behaviour are coming to agree that both methods must be employed together.
That way they should develop a rounder picture and perhaps some insight into the way simple actions must be patterned before we recognise them as ‘sensitivity’, ‘passivity’ or whatever.
Hitherto, the empirical watchers and the grand theorists have largely ignored each others' work.
One area of child development that the child psychiatrists appeared to have made passably tidy, has been somewhat roughed up by ethological methods over recent years.
The child psychiatrist Rene Spitz first gave theoretical weight to the phenomenon in infants known as ‘fear of strangers’.
Spitz noticed (what many mothers also notice) that, quite suddenly around eight months of age, babies who had suffered him more or less happily till then, became wary, often bursting into tears.
He named his discovery ‘eight months anxiety’ and it gained the same status for psychologists that saying ‘mama’or cutting the first tooth has for parents.
More experimentally-minded workers have since found many soft spots in what had seemed a solid concept.
They have reported for example that fear of strangers may peak anywhere between 6 and 12 months; that it varies in onset and intensity according to the child's sex, its rank in the family, the number of people it meets regularly, its attachment to its mother and her responsiveness, and the age, size and sex of the stranger; that it is different in the laboratory from at home.
The response depends critically on the particular experimental method used to provoke it.
The child's reaction changes according to whether or not the stranger appears abruptly through a doorway while the mother is out of the room, or whether mother and child are introduced to the stranger together; whether the child is approached quickly, unsmilingly and is physically picked up, or whether the  stranger hovers deferentially, smiling and offering a toy.
No one is quite sure what should be counted as fear.
A child who cries is undoubtedly afraid, but a child who looks away may be as much indifferent as wary.
Children often begin by smiling but end in tears; are they therefore neutral overall?
Depending on the particular combination of methods researchers have used, they have reported a fear of strangers in all or nearly none of the babies studied.
Some workers believe they can see through the ‘emperor's clothes’, arguing that if a fear of strangers is not actually an illusion, it is irredeemably elusive.
Most students of the phenomenon however, are convinced that it is ‘real’ but that it depends exquisitely on the context.
Indeed, Alain Sroufe at the Institute of Child Development at the University of Minnesota, urges that we stop trying to find the response in the child's physiology, and learn instead to think of it as an ‘organisational construct’, an economic way in which the experimenter can characterise the pattern of behavioural linkages between the child and its environment.
By changing the experimental conditions, social psychologists are not really exposing the instability of the child's fear but are causing the child to be afraid in different ways.
Fear of strangers is not a property of the child alone, but a social triangle between child, mother and experimenter.
Change the relations between them and the fear also changes.
The assumption that a fear of strangers is a uniquely childish reaction has been questioned in a clever experiment by Karol Kaltenbach, Marsha Weinraub and William Fullard at Temple University, Philadelphia.
As well as confronting the child and recording its responses in the normal way, they also walked up to its surprised mother and noted how she behaved.
They found that mothers averted their gaze more than their babies did, especially as the experimenter moved closer.
They concluded that the wary response is more characteristic of the social circumstance than of the individual's level of development.
Wary babies show that they have learnt to react as adults do to disturbing social intrusions ‘where they are approached regardless of their wishes and whether or not they provide appropriate invitational cues’.
The value of measurement
The example of an infant's fear of strangers shows, as we move from the detailed anatomy of behaviour to the rich, ambiguous concepts we use to make sense of everyday social intercourse, that the particular acts referred to matter less than the way they are organised, just as our interest in houses, rarely focuses on mud or wood or bricks.
There are so many things that might be incorporated in to ‘fear of strangers’, or ‘attachment’, or ‘intelligence’, that to propose that it is somehow ‘multi-determined’ misses the point.
‘Determined’ is the wrong word.
The elements of behavioural repertoires are resources actively mustered by self-directing organisms.
An over-ambitious social physics, albeit statistically sophisticated, would give an oddly flat theory of social relationships precisely because unique, yet meaningful, patterns would be smoothed out under general statistical laws.
We may not be very interested in tracing the history of collisions a given atom has with other atoms, as one atom is very much like another.
But the knotted tensions between people and groups of people give us plenty to think about.
And though we often generalise, calling one person more or less intelligent or fearful, we know that our appreciation of these qualities is rooted in our knowledge of the context in which the intelligent action or fearful response was made, and also in the history of its development.
The treatment of social beings as colliding billiard balls must ignore important idiosyncrasies.
Individuals do not move through a smooth physical vacuum; they negotiate structured social contexts in company with other individuals.
Can you measure a social relationship?
Of course, anything that can have a name put to it can be measured more or less usefully.
But by the same token, it can be understood more or less differently.
The examples of splitting and lumping therefore, are not alternative ways of doing the same thing.
Although they use the same subject matter, the stories they tell about it may not match.
Measurements are ‘objective’ inasmuch as scientists agree on the thumb to be used as the ruler.
Primatologist Emil Megel at the State University, New York, at Stoney Brook, concluded a conference on methods in the analysis of social interaction by asking workers to ‘develop and refine methods which explicitly recognise and exploit, rather than attempt to eliminate, the observer's prescientific, intuitive and global forms of judgement…
The key issue is not how to reduce the observer's role to zero (or even to an absolute minimum) but simply how to render it at least as explicit, as clear, and as quantitatively specifiable as any other aspect of the system under study, so that other observers will know how to look at the same phenomena from the same perspective, should they care to do so.’
Can we save the Chesapeake Bay?
Christopher Joyce
Nature can control the complex chemistry of life in an estuary.
But when things go wrong, can humans step in to take charge?
LOOK TO WHERE the river meets the sea and you will find, as did Kubla Khan, a place to build your ‘stately pleasure dome’.
When the first English settlers sailed up the Chesapeake Bay on America's eastern coast, they found an estuary worthy of Samuel Taylor Coleridge's fable — vast, limpid and dense with marine life, sustained by the unique mix of brackish water and nutrients that make estuaries the richest marine incubators in the world.
The Chesapeake is known as the ‘queen of the nation's estuaries’.
Its thickly wooded shores, pastoral rivers and mercurial weather draw naturalists and artists.
Its peerless blue crab and shellfish support fleets of fishermen whose rakish skipjacks still travel under canvas.
Few spots in a country that is bent on careening into the future so effortlessly preserve the past in clapboard churches, colonial custom houses and a sailing tradition that stretches back 350 years.
The bay harbours two of the country's five largest ports, Baltimore and Norfolk, and it has its own occasionally sighted, neo-mythic monster, ‘Chessie’.
But the Chesapeake is in trouble.
In 1975, two nature lovers were boating on one of the bay's rivers.
Old friends of the Chesapeake, they noticed something alarming beneath the water: the submerged grasses that used to carpet the river's bottom were disappearing.
In addition, over the past 10 years, the number of shad, trout and other anadromous fish (those that spawn in freshwater tributaries) in the bay had mysteriously dropped; and crab and oyster catches had tumbled.
The boaters happened to be Russell Train, who ran the Federal Environmental Protection Agency (EPA) in Washington, just 50 km to the west, and Senator Charles Mac Mathias of the state of Maryland, which shares with Virginia the bay's 13 000 km of shoreline.
Over the following months, they pushed through Congress a five-year multidisciplinary research project, one of the world's most intense examinations of marine pollution, to diagnose the ills of the bay.
The Chesapeake Bay Program was born in 1977 and almost died before it could take a breath.
A showcase for science struggling through its adolescence, the federally funded programme suffered both from environmental science's immaturity and from a sometimes unhappy marriage between political goals for ‘managing’ the bay and the scientific means for determining how to do this.
There were of course political difficulties; delays in getting money from Congress and in persuading the three principal states involved, Maryland, Virginia, and Pennsylvania, to cooperate with each other.
But if these were difficult, the scientific problems were horrendous.
The first problem was not simply a lack of data: indeed, for two years, the EPA, which was funding the programme, could not decide where to start.
It was blinded by details.
As a natural ecosystem the bay is astonishingly complex: its waters, some fresh and some saline, some warm and some cold, mix to provide an ephemeral backdrop whose characterisation would challenge the most sophisticated computer.
The natural chemistry, complicated by run-off of sediments from a variety of lands, is correspondingly complex.
Overall.
the biology of the Chesapeake was clearly of an intricacy beyond present comprehension.
The scientists could not even take for granted that human factors were causing the bay to deteriorate; cryptic changes in the natural world could, for all that was known, have been responsible.
Tom DeMoss joined the project in 1979 (after the first team had been given the boot).
By then, he says, ‘we had done 1.5 years of study and we were saying, what the hell are we going to do with this?
How were we going to characterise something so big?’
The answer was to go to Britain, to talk to the Thames Water Authority, whose success in reviving the fetid Thames won world renown.
‘Their fundamental hypothesis was that ecosystems were too diverse and complex either to study or manage as one unit,’ says DeMoss.
Following the Thames Water Authority's lead, but with some significant embellishments, the EPA broke Chesapeake Bay and its drainage basin/catchment area down into geographical, biological and chemical units, keeping in mind that each unit works dynamically with the others.
Geographical units included the tidal freshwaters at the heads of any of the dendritic tributaries that feed the bay, and the turbidity-maximum area (see map) where circular currents raise a curtain of sediments and living things.
The activity of submerged vegetation and phytoplankton were among the biological markers; salinity and the presence of herbicides in the water were among the chemical ones.
Scientists then set a goal: fishable, swimmable water that could support existing biota — including the eight million humans on its shores — for most parts of the bay.
To reach that goal, they had to answer certain questions.
If one part of the bay could support a certain amount of aquatic life, then why shouldn't other parts be able to do the same?
If they could not — as with many in the industrial northern area of the bay, near the busy port of Baltimore — why not?
Was dissolved oxygen in the water greater than 80 per cent of saturation?
Was biochemical oxygen demand (the amount of oxygen in the water needed for the aerobic decomposition of sewage plant effluent) no greater than 8 mg/litre?
Was nitrogen, an important but delicately balanced nutrient, no greater than 0.4mg/litre?
After three years on this course, the EPA now has some, but not all, of the answers.
In the coming months, the project will take a stab at recommending solutions to the decline of the estuary.
Not surprisingly, the ecologists have found that human influence looms largest among the causes of the bay's ills.
They eventually narrowed their focus to three subjects: submerged aquatic vegetation, such toxic substances as chemicals and herbicides, and the bay's ever-fluctuating supply of nutrients, nitrogen and phosphorus in particular.
Toxic substances and nutrients were the prime suspects along with the well-known recidivist, sedimentation.
Along rivers whose names betray their English and Indian heritage — the James, Wye, Severn, Nanticoke, Wicomico, Pocomoke — a careful observer can find any of 10 species of submerged grasses.
Some, like sea-lettuce (Ulva lactuca ) thrive on the highly saline water of the southern bay and on the abundant nitrogen and phosphorus.
Others, like the noxious Eurasian water milfoil (Myriophyllum spp ) enjoy anything from freshwater to a brackish cocktail of 33 per cent sea water.
Submerged plants rarely attract attention except when they are a nuisance as in the rotting infestation of sea lettuce a few years ago that produced enough hydrogen sulphide gas to discolour housepaint and tarnish silverware in the unfortunate town of Deale, in Maryland.
Natural ‘saline invasions’ from the mouth of the bay or mysterious and undiagnosed maladies like ‘Lake Venice’or ‘Rhode River’disease, have at various times thinned out these plants.
But now their numbers have ebbed to their lowest level in recorded history, perhaps permanently.
The causes are complex, and they act in harmony like unseen instruments in a muddy symphony.
Submerged grasses require light to grow; turbidity reduces light.
As more farms, some extending right to the water's edge, grow up around Chesapeake, spring and summer rains dump new sediment into its waters so that the trophic zone is reduced: bottom-growing plants are now able to photosynthesise only in shallow water.
Dredging, dam-building and housing construction add still more sediment.
Marshes play an important part in keeping sediment out of the bay's waters.
The Chesapeake's 5 million hectares of shoreline marshes buffer the eroding influence of waves and act as sediment screening sieves for water running off the shore.
Unfortunately until recently the philosophy of ‘wetland is wasteland’ reigned.
Wetland marshes were filled in systematically to make land more ‘productive’ or simply to discourage the hordes of mosquitoes that rule the eastern shore in the summer.
A filled-in marsh is a sluice for sediment.
Because these sediment particles have a large surface area, they act as ‘chemical sweeps’, bringing in unwanted passengers, such as oil, herbicides, organic chemicals like PCBs, and heavy metals like cadmium.
Sediment also clogs the gills of fishes.
In deeper channels, black and highly carbonaceous muds eventually form to create anaerobic conditions.
The slimy floor of shifting sediment is useless to young oysters in search of a hard sea bed for a lifetime's home.
Marsh grasses, like the tall, coarse Spartina genus, are broken down by bacteria to form detritus, food for shellfish.
zooplankton and other animals of the floating or scuttling kind that inhabit a marsh's blackened waters.
Marshes also provide life-saving cover for female crabs that have shed their shells in preparation for mating.
And they are habitat for the millions of mallard duck, Canada geese, American  widgeon and other waterfowl that pause here as they cruise the Atlantic Flyway toward warmer climes each autumn.
Yet sedimentation and disappearing marshes alone cannot explain the thickening of the bay's waters and its desultory humours.
The ultimate culprit, the scientists believe, may be the estuary's ever increasing supply of nutrients.
The bay's richness, like a bad case of gout, is crippling it.
At the mouth of the Patuxent River on the bay's western shore, there is a dollop of land called Solomons that is known for its commercial fishing community — the bay's watermen’.
The watermen have seen first-hand the decline of grasses: the disappearance of the shad and trout and the dwindling number of striped bass, blue crab and oyster.
Scores of sewage plants along the Chesapeake's tributaries are disgorging nitrogen and phosphorus, natural products of biological sewage treatment.
In addition, nitrogen from fertilisers washes into the bay from farms.
In proper balance, these substances are essential to aquatic life.
But an overdose causes ‘eutrophication’.
Simply put.
eutrophication is enrichment.
It works through plankton communities, particularly those known loosely as algae.
When infusions of nutrients inflate their ideal carbon/ nitrogen/phosphorus ratio of 106: 16: I, the algae gorge them selves and proliferate, forming large ‘blooms’, like the infamous ‘red tide’.
These blooms block light.
Eutrophication also spawns epidemics of epiphytes, microalgae that attach themselves to the leaves and stems of underwater plants and further obstruct light.
Phytoplankton have adapted to low-light conditions, and even orient themselves in the water column to bask more efficiently and store energy with which they fix carbon during darker periods.
Submerged vegetation is not so lucky; immobile, it takes whatever light it can get, preferably in the narrow range of blue light (450–500 nanometres).
The EPA found that most submerged plants in the bay are what it calls ‘light-stressed’.
More damage is done when the algae blooms begin to die.
Bacteria and fungi go to work breaking down the dead organisms' protein into amino acids and converting them into ammonia, a useful source of nutrition.
They also remove the carbon and make it available to plants as carbon dioxide.
But in doing so, they burn the water's dissolved oxygen.
Most obviously, low levels of oxygen asphyxiate fish.
And at low levels, bacteria tend to reduce nitrate, useful to plants, to elemental nitrogen, which as a dissolved gas is not available to most aquatic plants.
It is difficult to decide how much nitrogen and phosphorus is too much; the EPA's scientists haven't settled on the outer limits of acceptability.
Besides, in tidal freshwater during the spring, phosphorus can have the opposite effect, limiting the growth of some types of phytoplankton.
In the summer at high water temperature, nitrogen may do the same.
Determining where these nutrients come from is somewhat easier.
Over half the nitrogen runs straight off the land, from natural sources or fertilisers.
About one-fifth comes from sewage treatment plants, and perhaps 9 per cent escapes from bottom sediments.
Two-thirds of the phosphorus comes from run-off and sewage plants, split evenly between the two, and another 25 per cent from bottom sediments.
To some degree, these sources can be controlled.
‘No-till’ farming (known as minimum cultivation in Britain), for example, relies on heavy use of herbicides instead of annual tilling.
This reduces the run-off of sediments when combined with wise use of sedimentation ponds, berms and shore line vegetation to hold the soil together.
Unfortunately, there are other, newer sources never suspected of contributing to eutrophication that may be beyond the political if not the scientific, reach of conservationists.
At the Chesapeake Bay Center for Environmental Studies, David Correll, a chemist, spends his summer days wading through the tiny Rhode River collecting samples.
His findings do not support the idea that nutrient and sediment bear overwhelming responsibility for the decline of the bay's flora and fauna.
‘We found some plants did best in high turbidity.
Then there'd be a big thunderstorm and a lot of run-off from the land and everything would die’— without evidence of significant new loads of phosphorus and nitrogen or algae blooms.
Correll thinks herbicides, more potent than they are given credit for, may be to blame.
Correll found that a common herbicide, Atrazine, at levels as low as 0.5 to 2 parts per million in sediments and 0.1 to 1.4 ppm in the water column, drastically inhibited the net oxygen production of horned pondweed (Zanichallia palustris ).
Planrainwater'sts died when exposed to 2 ppm or more for 14 days.
This was surprising; Atrazine and other herbicides like Alachlor are  made to degrade quickly, and discharges are normally only in the parts per billion range.
But the bulk of the chemical run-off binds to particulates and is concentrated rather than dispersed evenly through the water column.
Submerged plants already are ‘stressed’, from exposure to such organic chemicals as polychlorinated biphenyls, DDT, and other herbicides such as paraquat.
The EPA's analysis, found much of our world in a single oyster — 40 different chemical compounds that natural selection didn't leave there.
‘The scary thing,’ says Correll, ‘is the synergistic effect of all these things.
The organisms out there are exposed to maybe hundreds of toxic chemicals.’
Filter feeders, like the hard-shelled clam (Mercenaria mercenaria — no reflection on the noble character and taste of this self-sufficient bivalve), are especially vulnerable.
Mercenaria pumps up to 11 litres of water an hour through its tissues, becoming the penultimate depository for toxic chemicals and metals until served up as chowder or clams casino.
EPA and local officials note that evidence implicating herbicides and even toxic chemicals, which accumulate in the northern, industrial tributaries, is still inconclusive.
Correll suggests chemicals may simply be a more difficult target.
Herbicides may be only a short-term threat, however.
Correll has examined the way nutrients enter the bay, and discovered that improving sewage treatment and farming practices may not be enough.
In 1973, Correll and his associates began placing polythene bottles at sites near the Rhode River to collect rainwater.
They compared the chemistry with that of run-off for the land, looking for sources of nitrogen.
It was a rare study; most precipitation analyses of late have concentrated on a sexier topic, acid rain (which also appears to be on the rise over the Chesapeake).
Correll found that during the summer about half the river's estuarine loadings of nitrogen come from the sky.
Though this rate drops during the rest of the year, the small estuary of the Rhode River received for the entire year an average 5.1 tonnes of total nitrogen from bulk precipitation (snow and rain), and 7.3 tonnes from watershed run-off.
This pattern holds for nitrate, which accounts for most of the nitrogen in the bay, and especially for nitrogen from ammonia, which, though a small part of the total, is far more prevalent in rainwater than in run-off.
Unless farm fertilisation increases, atmospheric deposition of nitrogen, especially as nitrate, may well overtake deposition from land runoff.
Nitrate concentrations in rainwater have been increasing for the past seven years.
‘This is not surprising,’ notes Correll, ‘because nitrogen oxide emissions due to fossil fuel combustion also increased during this time.’
The list of the Chesapeake's problems begins, then, with nutrients.
Sedimentation is a known contributor, and more careful use of land is its relatively straightforward solution.
But the effects and even the extent of toxic substances and herbicides remain a relative unknown.
Clearly, Chesapeake Bay is not ‘dying’— it is only sick.
Local governments have seen enough of the evidence to prepare a politically delicate agenda that could require millions of dollars on a gamble that secondary and tertiary treatment of sewage will lower nutrient levels to make a difference.
They also may try to help farmers to opt for no-till methods.
Nobody's betting that the trend will turn.
Sewage treatment is expensive, and federal grants that in the 1970s helped to build the nation's treatment networks are drying up, the victims of Reaganomics.
Besides, advanced treatment — a process of extra filtration, biological decomposition and chemical treatment — squeezes out more phosphorus but is not proven to reduce large amounts of nitrogen.
And no-till farming means more herbicides.
Like fossil-bearing lake beds, estuaries preserve the past, be it good or bad.
What is happening to the Chesapeake will happen to other estuaries; they will trap most of what humans put into them.
But the bay's defenders don't want to wait until their estuary is a disaster before they act.
Life is complex where fresh and salt water meet
CHESAPEAKE BAY Has created over a period of about 17000 years.
Twenty thousand years ago, at the end of the Pleistocene, the melting of North America's last great ice sheet engorged what is now the Susquehanna River, which then enlarged the Chesapeake Valley.
Some 10 000 years later, a rising Atlantic began to press the mouth of the Susquehanna back toward its source, forming an estuary — a partially enclosed bay where fresh and salt water mix — which took its present form about 3000 years ago.
River and sea now coexist by the rules of a peculiar estuarine current.
Every tidal day (just over 24 hours) the bay experiences two flood and two ebb tides.
This twice daily peristalsis creates tidal currents every six hours, pushing sea water first north, then south.
To the casual  observer , the tide may seem to be the only movement of water in the estuary.
But at the same time, fresh water from the Susquehanna and other northern and western rivers skims south across the top of the denser salt water.
Friction between the two layers causes some mixing all along the length of the estuary, as does the gradual dissipation of the northern now of sea water as it nears the shallower northern bay.
The heaviest mixing occurs at several points known as turbidity maximums (see map).
The sum of this mixing is an overall estuarine now whose net progress, measured at points near the middle of the Chesapeake, averages about 1 ½ km a day seaward — the sum of 9 ½ km seaward and 8 km landward progress a day.
It is complicated by intrusions, where sudden squalls or now from the bay's 50-odd tributaries causes local mixing.
The Earth's rotation adds to the confusion.
In what is called the Coriolis effect, the northward moving sea water is pushed to its right, while the southward fresh water moves to its right.
Thus, the eastern shore is saltier than the western shore.
This aquatic ferris wheel churns the Bay's waters into a brackish and turbid soup — an inhospitable place for many types of life.
For example, the ocean's euphotic zone — the depth to which light penetrates sufficiently for phytoplankton to photosynthesise — can reach 100 metres.
The bay's shifting zone is sometimes as shallow as 30 cm.
And because the bay is long and shallow, like a peapod, with an average depth of only 9 m, it cannot absorb heat well.
Temperatures in its waters vary from 0o to 29°C over the year.
As temperature rises, biological reactions speed up, depleting oxygen dissolved in the water.
At times during the dog days of summer, deeper parts of the bay become anoxic.
Chesapeake Bay blue crabs, among nature's hardiest creatures, have been observed dancing with death along the bottom; they hover just above the anoxic zone, dart down to forage along the bottom, then come up for ‘air’ to the oxygenated mid-depths.
Yet the bay is fabulously productive, many times more than an equal volume of ocean.
Nutrients are the passengers of sediments that stream into the bay from its tributaries.
As sediments settle out of the top freshwater now, they are caught by the bottom's incoming salt now and returned for deposition at the mouths of the tributaries.
The movement is thought to be aided by copepods, 1-mm-long zooplankton that ingest the smallest particles, agglomerate them in their guts and expel them as large and heavier pieces that sink before they can escape to the sea.
The circular estuarine flow then stirs up these sediments along with plankton, fish eggs, shellfish larvae, minerals and nutrients, to make a  briny tonic that supports over 2700 species of flora and fauna.
But the very things that make the hay rich are the things that now make it poor.
 MONITOR 
Alcohol can damage unfertilised eggs
ALCOHOL can disrupt the chromosomes of an unfertilised egg (that is, even before conception), according to Matt Kaufman of the Anatomy
Department at Cambridge University.
Alcohol appears to interfere with the segregation of chromosomes during egg maturation, resulting in embryos that have an abnormal number of chromosomes (such embryos are called aneuploid).
In the female mammal, all the eggs that will ever be ovulated are already present at birth and are stored in a state of ‘suspended animation’ in the ovary.
In response to fluctuating hormone levels during each oestrous cycle, some eggs are released to resume development.
This involves completion of the first meiotic division, followed by ovulation, fertilisation, and the resumption of the second meiotic division.
During meiotic divisions, the chromosomes group together, genetic material is mixed, and the chromosomes separate again.
This is all organised by an intracellular organelle called the spindle.
Disruption of the spindle can cause chromosomes to segregate abnormally at either of the two meiotic divisions, resulting in aneuploidy.
The work really began when Kaufman noticed that if mouse eggs were exposed briefly to a 7 per cent solution of alcohol a significant proportion (up to 20 per cent) of the embryos which developed were aneuploid (Journal of Embryology and Experimental Morphology , vol 7 1, p 139).
This led him to investigate in more detail the effect of alcohol on chromosome segregation at about the time of conception, and its effects on early development of the mouse embryo in vivo .
He gave pregnant mice a dilute solution of alcohol by mouth, and again, fertilised eggs exposed to alcohol while in the process of completing the second meiotic division show an incidence of aneuploidy, varying with dose, of up to 20 per cent (Nature , vol 302, p 258).
Thus he has found that eggs are particularly susceptible to the effects of alcohol when the chromosomes are arranged on the spindle during the second meiotic division (during metaphase) immediately prior to their segregation into the two daughter cells.
This is exactly what would be expected if alcohol acts as a spindle disrupting agent.
The implications of these findings are doubly serious because it is equally likely that the spindle-disrupting action of alcohol could occur during the first meiotic division.
This would mean that even before fertilisation, the female gamete could be damaged if alcohol is consumed.
Indeed, evidence already exists that the first meiotic metaphase is as susceptible as the second (Development in Mammals , vol 1, p 145).
Bearing this in mind, an examination of clinical observations of spontaneous abortions in humans would suggest that over 25 to 30 per cent of all such abortions are due to aneuploidy.
These figures include only the spontaneous abortions that occur late in the first trimester, so that an even higher proportion of conceptuses are probably at risk but die soon after the egg is implanted and are therefore difficult to detect.
One final point on this sobering subject: the dose of alcohol used in Kaufman's experiments is equivalent, in human terms, to only 2 to 2 ½ times the ‘legal limit’ which is by no means beyond the limits of many drinkers.
New theory says our Sun is in a special place
A NEW theory of the origin of our Solar System suggests that the Sun is in a special place in our Galaxy, and that it may be incorrect to draw inferences about the Galaxy at large from studies of our immediate stellar neighbourhood.
These far reaching conclusions, suggested by L. S. Marochnik of the USSR Academy of Sciences, Moscow, stem from a revision of the accepted figure for the rotation rate of our spiral Galaxy.
Unfortunately the detailed work on which that revision is itself based has only been published in Soviet journals in Russian, so has not stood the test of scrutiny by Western astronomers.
But Marochnik's extrapolation from that work has such broad implications that it deserves investigation.
It is widely accepted that the spiral pattern formed by the stars of a galaxy like our own is produced by a density wave moving through the stars.
On the standard picture, this wave rotates around the Galaxy; and the solar System, orbiting the centre of the Galaxy at-a distance of about 32 500 light years (10 kiloparsecs), repeatedly passes through the density wave.
This wave pattern is a so-called short wave mode, with the rotating wave progressing inward from the periphery of the Galaxy to the centre.
The repeated passage of a presolar cloud of material through the spiral arms caused it to be compressed until eventually it collapsed to form the Solar System.
But Marochnik disagrees.
His argument is that the density wave is of a different kind, the so called long wave mode, which propagates from the centre of the Galaxy outward, such a wave pattern rotates much more rapidly than the short wave mode and it just happens that, if this picture is correct, our Solar System is almost exactly at the place in our Galaxy where it orbits at the same speed as the wave (Astrophysics and Space Science , vol 89, p 61).
This special location in the Galaxy is called the co-rotation circle, and each spiral galaxy has only one such co-rotation circle.
On this picture, the cloud of material from which our Solar System formed had a very unusual history, taking as long as 4.6 thousand million years to travel from one spiral arm to the next and then taking several hundred million years to pass through the compressed region of the Galaxy, the arm itself.
This special place of the Sun in the Galaxy, if confirmed, certainly alters our picture of even the recent geological history of our planet, since we have only just emerged from a spiral arm.
Wisely, perhaps, Marochnik stops short of drawing any dramatic conclusions, but two things are clear.
If the solar System is at a special place, then the concept of terrestrial mediocrity (that we are so ordinary the Universe at large must be just like our neighbourhood) takes something of a knock.
And although that may mean that stars with planets are less likely to be found in places other than co-rotation orbits, the corollary would be that they are likely to be found within such orbits.
 Melatonin drives the internal clock
KEPT in constant conditions, many animals nevertheless continue to alternate periods of activity with periods of sleep.
But the so-called free-running or circadian rhythm, is only roughly 24 hours long and does not tie up exactly with day length.
Changes in the environment, most notably switching lights on or off, can entrain the free-running rhythm, bringing it back under outside control.
It seems that those external changes are somehow detected by the pineal gland, for experimental animals deprived of their pineal are slower to entrain, but the mechanism is not known.
Now, from a team at La Trobe University in Victoria, Australia, comes evidence that a pineal hormone, melatonin, is responsible for bringing the free-running activity into line with the outside world (Science , vol 219, p 1089).
Jenny Redman, Stuart Armstrong and Kim Ng had previously tried injecting rats who were kept in total darkness and free from other environmental signals, with chemicals known to be made in the pineal.
Arginine vasotocin, melanocyte stimulating hormone, and the nerve-blocker pindolol had no great effect on the resulting free-running (circadian) rhythms.
But melatonin did.
Daily injections of melatonin given at 3.30 each afternoon, entrained the rats to a 24 hour cycle.
Control injections,without melatonin, had no effect, with the exception of one unusual rat who seemed to use the injection to order his life.
Interestingly, the timing of the injections is very important.
The rat does not immediately adjust its daily rhythm to the melatonin.
Rather, it locks onto the injection only when the injection coincides with the start of the active period.
Starlings, which are active during the day, lock onto melatonin when it coincides with the end of their activity.
In diurnal and nocturnal species melatonin reaches a peak just after the dark, so for starling and rat the injection mimics the onset of night.
For the starling this is signal to calm down, while for the rat it is the start of its active period, but in both cases the same hormone seems to be responsible.
The results raise an interesting problem for people studying other effects of melatonin.
The hormone is known to affect reproduction and gonadal function, and to have an influence on the growth of tumours.
It now seems that the time of injection may be of crucial importance in determining the effects of melatonin.
New organic superconductors — by practice and design
The number of organic compounds that become superconducting, losing all resistance to an electric current in the right conditions, continues to increase.
Recently researchers in the US discovered a new family of compounds that exhibit superconductivity.
Like most of the organic superconductors already known, the new material loses its resistance only under high pressure.
However, another group working in the US believes it has gone some way to understanding the link between high pressures and the superconducting state.
Organic superconductivity came to light only four years ago when a group led by Denis Jerome of the Universite Paris-Sud at Orsay and Klaus Bechgaard from the H.C.
Oersted Institute in Copenhagen found superconductivity in (TMTSF)2 PF 6 , where TMTSF stands for tetramethyltetraselenafulvalene (New Scientist , vol 87, p 104).
Since then a number of related compounds, all of the form (TMTSF)2 X, where X is an anion, have turned out to be superconductors, at sufficiently low temperatures (around 1 K) and, generally, high pressures, up to 12 000 atmospheres.
Now S. S. P. Parkin and colleagues at the IBM Research Laboratory at San Jose in California have discovered superconductivity in a different form of organic compound, namely (BEDT-TTF)4 (ReO 4 )2 where BEDT-TTF is bis (ethylenedithiolo) -tetrathiafulvalene (Physical Review Letters , vol 50, p 270).
The team found that this salt is a superconductor at temperatures lower than 2 K, and pressures greater than about 4000 atm.
This is the first evidence for superconductivity in an organic compound in which the electron-rich ‘donor’ is based on sulphur.
At lower pressures the material changes from a metal to an insulator at around 8 1 K, as the temperature is lowered.
As with the (TMTSF)2 X salts, the structure of (BEDT-TTF)4 (ReO 4 )2 may well be important in determining how well it conducts electricity at low temperatures.
The BEDT-TTF molecules form skew stacks, with channels between, where the anions (ReO 4 ) sit.
This is similar to the structure of the (TMTSF)2 X salts, where the TMTSF molecules are stacked like pancakes.
But whereas TMTSF is more or less planar, in BEDT-TTF methylene groups (CH 2 ) at the ends of the molecule lie out of the main plane.
The importance of the structure of organic superconductors has come under the scrutiny of another group working at the Argonne National Laboratory in Illinois.
There, Jack Williams and colleagues have been attempting to understand why most (TMTSF)2 X compounds require high pressures to turn super conducting, while (TMTSF)2 CO 4 does not (Journal of the American Chemical Society , vol 105, p 643).
The group has studied the role of interactions between the selenium atoms in different stacks of TMTsF molecules (see Figure).
Using data from diffraction studies of TMTSF salts, they observed how the unit-cell volume of the crystal varied with the average ‘interstack’ distance between selenium atoms.
It turns out that the minimum cell volume and interstack distance are around the values (TMTSF)2 CO 4 , the only organic salt known to go superconducting at atmospheric pressure.
Other salts have larger distances between selenium atoms, leading the researchers to suggest that pressure reduces this distance until the appropriate structure for superconductivity is reached.
They go on to suggest that it will be valuable to synthesise new TMTSF salts, with anion sizes similar to CO 4 .
Sitting pretty
TOILET seats are the wrong size and provide insufficient thigh and buttock support.
This is the startling conclusion reached by Ian McClelland and Joan Ward of the University of Technology in Loughborough (Human Factors , vol 24, p 713).
They recruited 205 members of the public to test new types of toilet seat.
Throughout the experiment subjects wore no shoes and were naked below the waist.
Subjects were asked to try five types of toilet seat ranking them in order of preference.
Each subject was able to adjust both the height and the angle of a toilet seat by remote control.
The five types included one built to British Standards, a second ‘standard’ seat with an aperture 0.07 m longer than currently recommended, and three other elongated seats contoured to give various degrees of thigh and buttock support.
Men preferred a seat height of about 0.430 m and women a height of about 0.404 m.
There was an overall preference for a level seat.
Both men and women rated the standard seat as the most uncomfortable.
The elongated seats were much preferred.
Of these elongated seats those contoured to give good support to the thighs and buttocks were most popular.
The message is loud and clear.
Give longer seats and more contours!
Solitary waves of polyacetylene conduct
WHAT have the River Severn and polyacetylene in common?
The answer is that both provide a medium for solitons.
Solitons are solitary waves; the Severn bore is a good example.
They also occur in polyacetylene, an organic, one-dimensional semiconductor, where they can be invoked to explain many of that material's properties.
This discovery has given scientists a unique opportunity to study these unusual phenomena.
The idea of solitary waves travelling along rivers and canals has been around since 1844. it was considered a novelty until the early 1960s, when scientists discovered that two solitary waves travelling in opposite directions need not destroy one another, but could in fact continue unscathed.
In 1965 the term soliton was coined to describe waves with this remarkable behaviour.
In the 1970s the search for a room temperature superconductor led chemists to look at one-dimensional organic conductors and semiconductors.
The simplest of these is polyacetylene.
It consists of a chain of carbon atoms, with one hydrogen atom attached to each carbon.
Carbon has four electrons that it uses to form chemical bonds.
Each atom in the skeleton of polyacetylene uses three electrons, to form bonds with the two adjacent carbon atoms and one hydrogen atom.
This leaves one electron on each carbon atom unaccounted for.
The simplest model predicts that this electron will interact with those on both neighbouring carbon atoms, resulting in a continuous ‘bond’ along the whole polymer chain.
(This happens in benzene and other ‘aromatic’ hydrocarbons.)
However, this view is now known to be rather naive.
scientists found that the linear chain is more stable if it undergoes a Peierls (pronounced piles) distortion; each electron interacts with only one of its neighbours to form a structure of alternating short (double) and long (single) carbon-carbon bonds.
The polyacetylene chain can adopt two different shapes, called cis and trans ; where the trans configuration is the most chemically stable.
Trans- polyacetylene also has two configurations, depending on the position of the double bonds.
Think of five atoms in the middle of a chain and label them in sequence, 1 to 5.
The double bonds can be between atoms 1–2 and 3–4, or between 2–3 and 4–5.
The two forms have identical energy — in chemical terms they are degenerate.
Chemists initially make polyacetylene in the cis form, and convert this to the trans by heating it to 200°C in helium.
The thermally-induced conversion begins anywhere in an individual chain, often in more than one place.
As a result, many chains contain both configurations.
Where the two configurations meet, a disturbance occurs in the orderly structure of the polymer — and this is a soliton.
Polyacetylene is a semiconductor, with an energy gap of 1.7 electron-volts between the highest filled set of electron energy levels and the lowest empty levels.
The carbon atom at the centre of a soliton forms single bonds with both neighbours.
It is left with one unused electron that sits in an isolated energy level in the middle of the energy gap.
(It turns out that two adjacent double bonds are not the most stable arrangement.
In practice the distortion is spread over 1 5 carbon atoms.)
What effect do solitons have on the properties of polyacetylene?
Scientists predicted that solitons should be very mobile, but only along the polymer chains.
They verified both predictions, and calculated the effective mass of a moving soliton. it is about the same as an electron's mass.
They also expected it to show a particular type of magnetic behaviour called paramagnetism because of its isolated electrons — and again, it does.
If the electrons on the solitons are removed the paramagnetism should disappear.
An electron acceptor, like arsenic pentafluoride, will remove these electrons (leaving charged solitons), and the paramagnetism does indeed go away.
Polyacetylene has created a lot of technological interest because it, like other semiconductors, can be doped to improve its conductivity, which can be increased ten million, million times.
Dopants, like arsenic  pentafluoride (electron accepting) or iodine (electron donating) produce additional charged solitons.
Charged solitons moving along the polymer chains could account for the greatly improved conductivity, but, there is a catch that makes their importance questionable.
The dopant adds, or removes an electron from the polymer, so the soliton and the dopant molecule are oppositely charged.
They attract each other and the soliton becomes trapped or pinned.
Charged solitons are not as mobile as their neutral counterparts.
Another problem is that the chains in polyacetylene are not neatly ordered, but convoluted and higgledy-piggledy.
Scientists have questioned whether solitons can move freely in the real material.
To avoid these problems, S. Kilveston from the University of California, has suggested that the improved conductivity of doped polyacetylene comes from electrons hopping between solitons, and not from the movement of solitons themselves.
Red peppers, capsaicin and nerve cell death
OCCASIONAL eaters of red peppers are familiar with the masochistic sensation of having their mouths burned.
Regular consumers may find that they find peppers less ‘hot’ as time goes on.
Both effects can be traced to the compound capsaicin, which is now creating a great deal of interest in the neurophysiological as well as the culinary community.
Capsaicin destroys small sensory nerve fibres if it is injected under the skin of newborn rats.
These cells contain the peptides (small proteins), substance P and somatostatin (among others).
In older animals, capsaicin causes these peptides to be released from the nerve endings, and repeated doses result in their depletion.
Until recently, no one had any idea how these effects came about.
Uwe Otten, H.P. Lorenz and F. Busiger of the University of Basel and Hoffmann-La Roche looked for a possible connection with the protein nerve growth factor (NGF).
The sensory nerve cells that are affected by capsaicin depend on the presence of NGF to develop properly, and to maintain high enough levels of peptide to carry out their functions.
The researchers at Basel suggested that capsaicin might be interfering with the action of NGF.
They tested the idea by injecting newborn rats with NGF alone, capsaicin alone or both together.
As usual, capsaicin had the effect of destroying small sensory nerve cells.
But animals that had received NGF as well lost fewer cells than those that had not.
Immuno-histochemical tests showed that animals treated with capsaicin apparently lost all the substance P from the ganglia (cell clusters) containing sensory nerve cells; those that had received NGF alone had even more substance P than untreated control animals, and those that received both had fewer cell bodies containing substance P, but such cells as they had stained more strongly for substance P than those in controls.
NGF is normally taken up at nerve terminals and transported along the nerve fibres to the cell body, where it presumably regulates the amount of peptide.
Otten and his colleagues suggest that capsaicin destroys cell bodies by depriving them of NGF.
They could do this by interfering with the transport of NGF along the fibre; as in this experiment, the application of extra NGF directly on the cell body could help to stop the damage.
The destructive effects of capsaicin have made it attractive as a tool to scientists interested in mapping the representation of sensation in the brain and peripheral nervous system.
But a better understanding of how it works could also shed light on other questions concerned with the development of sensory nerve cells.
PATENTS
Barry Fox
Trademark apathy
A BRITISH lord is trying to make the government honour a 25-year-old promise to trade and industry.
British trademarks law, which dates back to 1938, lets people register trademarks only for goods.
You may not register a mark for services, such as consultancies, banking, engine tuning or education.
But in 1957, Britain signed an international agreement to introduce trademarks for services.
After 10 years nothing had happened, so in 1968 the Institute of Trademarks Agents called for urgent action.
In 1974, the Mathys Committee on trademark law reform said Britain should follow the US and France and protect service marks as soon as possible.
But still nothing happened.
Service industries are now in a worse predicament than ever, because it is no longer possible to register even a business name in Britain.
So Lord Campbell of Alloway is trying to go it alone with a private member's bill in the House of Lords.
Back-seat pilot
BRITISH AEROSPACE believes that by taking decisions out of the hands of the pilot it can reduce the amount of noise that an aircraft makes.
In British patent application 2 101 060 the company acknowledges that pilots can reduce the nuisance value of aircraft noise by reducing the thrust of their engines while flying over densely populated areas near airports.
The snag is that the pattern of these areas is different for every airport in the world.
So pilots often boost their engines when they should be cutting, and vice versa.
The patented plan is to feed data about the relevant airports into a small computer on the flight deck before take-off.
During the subsequent flight, the computer puts out a series of warning signals which tell the pilot when he can make noise, and when he can safely cut the engine power.
Defence ministry ‘Exocets’ yacht radar
A TANGLE over patents with the Ministry of Defence has done wonders for sales of a new radar system for small boats.
On 22 November, 1982, Lokata of Falmouth, Cornwall, filed a patent application for its Watchman radar detector.
The company spent £50000 developing the system.
Watchman is ideal for small boats because it does not incorporate a transmitter and can thus be made and sold for around £165.
But Lokata had not realised that it was treading on a defence secret.
Whereas conventional radar systems emit a beam and analyse its reflections from other ships in the area to avoid collision, Watchman simply pinpoints the radar beams that other ships transmit.
It works in two stages.
With the detector in omnidirectional mode it bleeps when it receives any radar swap from within 5 miles (8 km).
The small boat skipper then holds the sensor in his hand and moves it round to pinpoint the source accurately.
The system of course relies on the fact that large boats and ships will be using active radar systems, not passive systems like Watchman's.
Unfortunately an examiner at the London Patent Office realised that this is the principle on which modern missiles such as the Exocet work, and on 3 December slapped a prohibition order on the patent.
This prevented the firm selling the gadget.
But under pressure from the local MP for Falmouth, the Ministry of Defence relented and lifted the prohibition order.
Thanks to the publicity that resulted, the firm has been inundated with inquiries.
Get wise to the patent gamblers
PEOPLE use patents either as a valid legal weapon to block rival manufacture or as a bluff to put off rival manufacturers who know nothing about patent law and do not take legal advice from a patent agent.
No one wants to end up fighting a patent battle in court because financially only the lawyers win.
So playing with patents is like poker.
The trick is to push a dispute just far enough to make your opponent cave in for fear of a court action, but not so far that it goes to court.
Often the easiest way out is for an infringer to design around the claims of a patent, for instance by changing vital features.
Inventors and manufacturers facing this dilemma may be interested in a conference on
‘Designing round a patent’ on 26 April at the Bowater Conference Centre, London.
The only snag is that it costs £125 + VAT to attend.
Further details from European study  Conferences , Kirby House, 31 High Street East, Uppingham, Rutland.
Telephone (057 282) 2711.
The easiest way to earn money from a patented invention is to license someone else to use it, and collect royalties.
But there are pitfalls.
Some computer inventions have to be protected by copyright rather than patents.
The area is a legal and financial minefield, so the City University Business School is organising a two-day seminar at the Law society in Chancery Lane from 25 April.
The fee is £180.
Secrets of the flat screen
ENGINEERS at Philips in Britain have secretly been working on a flat screen television for several years, while hotly denying the fact.
Now British patent application 2 101 396 tells all.
The tube is a flat glass panel like a thick picture frame.
It has a phosphor screen on the inside of the front glass face plate, and an electron gun tucked away at the bottom back corner.
A flat metal panel behind the phosphor screen divides the panel into two layers.
The electron gun shoots a beam of electrons along one side of the metal divider onto a deflector plate which turns the beam through 180o.
So it ends up behind the phosphor screen.
More deflector panels sweep the beam backwards and forwards across the screen to trace out the picture lines.
By making the beam double back on itself in this way, Philips achieves an effective throw of twice the length of the tube.
This means that the beam deflectors and gun need sweep the beam only through a small angle.
This in turn means that the tube consumes less electricity.
Philips reckons that panels just 5 cm thick can support a 30cm screen which consumes only 5 watts of power.
Warm welcome
A GERMAN company, Schoeller, has filed a British patent on a clever idea to ensure that car door locks never freeze up.
Schoeller's patent, number 2 100 025, describes a car lock containing a small electric heating coil connected to the car's battery.
To make sure the coil is not continually draining power, there are three switches in its circuit.
One senses temperature so that the look is only heated when temperatures fall below freezing point.
Another senses movement in the car handle so that the coil heats up only when the handle is rattled.
A time switch disconnects the heating coil after it has had time to warm the lock.
TECHNOLOGY
Big Brother learns oldspeak
BY 1984 those who sift through your files in police stations, hospitals and local authority housing departments could be talking with artificial intelligence.
This is the name of a computer language that an American firm called Microdata developed to speed up the business of interrogating a database.
Microdata's subsidiary in Britain CMC is talking to police forces about its Artificial Intelligence Language.
The company has already sold the computers on which the language can be run to several forces including Devon and Cornwall, West Yorkshire and North Humberside.
‘The language is ideal for research, looking up references and generally pulling information together,’ said Peter Mangan, who works in CMC's marketing department.
The Artificial Intelligence Language (the name is only temporary) is still under development in Irvine, California, but last week New Scientist was given a demonstration of it.
The idea behind the language is to enable untrained operators to scan through a data base posing questions in ordinary English, rather than in the stilted query languages that most database systems involve.
CMC's present query language is rather inappropriately called English.
If you wanted to use English to find out how many clerks you employed under 25, your question would have to look like this: ‘List Staff with age le ‘25’ and with position clerk’.
Using the Artificial Intelligence Language the question can be put more loosely: ‘How many clerks under 25?’,
‘List clerks aged less than 25’ and so on.
If the computer does not understand what you have said it will ask questions, offer alternatives or tell you to rephrase your question and try again.
The Artificial Intelligence Language uses pattern recognition, a technique used in computers that can hear or see, to decide what question is being asked.
Combinations of consonants, vowels and spaces are stored in memory and compared with what has been typed into the machine, so that it can make an identification.
The computer also parses the sentence, deciding which words are verbs, nouns, adverbs and so on.
Much of what is typed in is discarded, because the computer is really looking for key words which refer to files kept in its disc store and what should be done to them.
One of the advantages of CMC's new development is that it can learn from its operator.
Once the system has established what is being asked of it, the request is stored so that the next time it comes up there is no need to waste time asking the same questions over again.
According to CMC, the system only fails to identify one in 50 words typed into it.
The pattern recognition technique enables the computer to cope with a certain amount of operator error, minor misspellings make no difference.
In addition, the system can be instructed to accept different spellings for the same word, or different words with the same meaning.
Unlike many data bases which try and make things easy for operators, the Artificial Intelligence Language does not require vast amounts of storage.
This is because the patterns stored in memory are compressed, they are snapshots of true words.
Typically the system would take up about 250 000 bytes.
Space is also saved by the way in which the data are stored.
It is a relational database which is rather like a filing cabinet in which not only the dividers have been removed but also all pieces of paper without words on them.
Information is retrieved from this type of database according to its relationship with other data, rather than by its position in a file.
Despite all this crafty programming, there are still a few rough edges that need to be shaved off the system.
At the moment the computer tends to think out loud.
It will come back with questions couched in the English query language which still exists beneath the Artificial Intelligence Language stuck on top.
In the demonstration New Scientist played with, which involved data about a doughnut factory, there were confusing references to SPVSRS (supervisors) as well as quite a lot of mathematical symbols.
And what would be the reaction of the average police officer to this response from the computer: ‘Gosh…
I can't locate what file you're interested in.’?
Japan unveils an optical transistor
SCIENTISTS at Kyoto University, Japan, have built a prototype optical transistor which could form the basis of computers that will run on laser beams instead of electric signals.
Professor Akio Sasaki and Assistant Professor Shigeo Fujita claim that their ‘ multifunctional optical element device’ can amplify light signals, store them in the same way as a conventional computer stores electric signals, and channel the flow of rays in one direction by absorbing random reflected light.
The device looks like a button cell for a quartz watch.
It has a substrate made of phosphor indium crystal, and a 200-micrometre lamination of seven layers of indium, gallium arsenide and phosphor.
When the device is powered with 3–4 volts of electricity, a 50-microwatt laser beam applied by optical fibre from below the substrate, is first absorbed, amplified four times, and emitted from the upper surface.
The device amplifies only signals above a certain intensity.
It remembers which signals it has amplified and which it has not, and this information forms the 0s and 1s of a binary memory.
There is no need to convert the optical information first into electric signals — a big stumbling block in the race to develop optical electronics.
One firm that has expressed interest is Nippon Telegraph and Telephone, the Japanese telecommunications giant.
It is working on an ambitious scheme to develop an ‘integrated network system’ which will link electronic media and data processing systems with fibre optics.
Japanese computer firms such as Fujitsu, Hitachi and Nippon Electric (NEC), which are competing with Us firms to build the world's fastest ‘supercomputer’ have also expressed interest in using the idea to develop an optical processing  technology .
Industry snaps up robot Vision research
BRITAIN is making progress in the tricky business of converting video images into useful information.
In recent weeks, two teams of researchers making the computers to do this job have signed agreements with companies to market their work.
The equipment can be useful in many areas of research and industry — from equipping robots with ‘eyes’ to analysing the human body.
In the first contract, British Robotic Systems has teamed up with the Department of Industry's National Physical Laboratory to sell a ‘Linear Array Processor’.
With a network of microprocessors, as many as 512 depending on the version, the machine analyses picture elements of an image received from a TV camera.
The processor can interpret several elements of the picture at once; in other words it acts in parallel fashion.
Conventional serial machines do only one processing job at a time.
With its enhanced ability, the computer should speed up the operation of British Robotic Systems's main product, the Viking vision unit.
The £24 000 product captures information about objects with a TV camera.
It can act as an inspection tool, for instance to determine whether the design of postage stamps is correct; alternatively, the computer can feed instructions about the shape of objects to a robot in enough time for the machine to pick it up.
The London company, which the British Technology Group owns, says the Linear Array Processor can be added to Viking for a further £5000 or so.
Alternatively, the computer could form a part of other automation systems that the firm sells.
The second link between the worlds of commerce and research effectively ends probably the British computer community's longest running saga.
Since the mid-1970s, academics at London's University College have tried to find a commercial sponsor for their Cellular Logic Array Processor (CLIP).
Like the National Physical Laboratory's invention, this also ‘thinks’ in parallel fashion but is more sophisticated and can contain up to 1024 separate processors.
A small firm in Horsham called Stonefield Omicron Electronics has agreed to sell the fourth version of the computer, the CLIP.4.
The Science and Engineering Council's Rutherford Appleton Laboratory is building two more machines for university research, while University College itself is designing a cheaper version as well as a more advanced unit called the CLIP-7.
The units that the Horsham company sells will cast between £15 000 and £50 000.
So far, medical researchers have been prominent in using the CLIP devices already built.
With the machines, hospitals have analysed the pattern of blood vessels in tissue and identified components in jaw bones.
Meanwhile, workers at the Imperial Cancer Research Fund are using the machine to investigate the structure of proteins.
Fire Fortress
A NEW material that could replace asbestos, silica or ceramic fibre based products is about to come on the market.
It is made by coating individual glass fibres, formed into tissues, with clay platelets, one billionth of a metre thick.
The result, says ICI, is a material that retains physical and dimensional stability at 1000°C — twice the temperature needed to melt glass — and some tensile strength at 750°C.
The treated tissue is marketed as Fortress T which has a base weighing 0.05 kg/sq.m and a coating of 0.05 or 0.1 kg/sq.m.
It is the first of a series of similarly treated glass fibre products.
ICI at Runcorn has developed ‘a form of molecular scalpel’ to delaminate minerals, in this case a clay called vermiculate.
An ion exchange process creates an aqueous slurry of platelets of  molecular dimensions.
The process starts with the crystallography of the mineral, essentially atoms of  silicon , oxygen, metals and molecular water in layers.
By substituting some of the layers the mineral can be delaminated and the platelets kept apart physically.
ICI is not talking about the platelets.
What it will say is that the chemistry can be modified to take account of the particular mineral and its final base.
For example, it will work with kaolin and woven fabrics.
The platelets produced are enormously long and wide in comparison with their thickness.
Their thinness means that short-range molecular forces give them exceptional coating and film forming properties.
They are attracted to any fibre dipped into the slurry and wrap themselves around it to form a sheath.
A number of effects help the vermiculite to protect the glass fibre from the effects of heat.
The sheath blocks heat from going through and, at the same time, removes it along the plane of each platelet.
The vermiculite is also as efficient at radiating heat as it is at absorbing it.
The coating helps  the fabric to keep some strength as the glass softens and melts.
When the glass hardens again, Fortress T regains a proportion of its original strength.
ICI sees the tissue as an alternative to asbestos.
It will not normally be used on its own but as a facing or interlayer in composite materials.
Cobra makes light work of cutting
A BRITISH company has successfully married a 400 W laser with a robot arm in a way that preserves the arm's five degrees of freedom.
An optical path reflects the beam along a series of 10 mirrors mounted alongside the arm's joints before focusing it with a zinc selenide lens.
Flexible Laser systems of Scunthorpe used techniques developed by the laser group at Culham laboratories.
‘Cobra’, as the £14000 system is known, is connected to a Ferranti CO2 laser.
Trevor Johnson, the company's founder, says that focused laser beams have many applications in flexible manufacturing systems.
Their advantages include fast stopping and starting, lack of tool wear because nothing touches the workpiece, and — potentially — great accuracy.
Johnson had great difficulty finding a robot that could cope with the laser's accuracy.
Cobra can cut, trim, weld, and even drill holes in a variety of materials.
It slices through 5 mm of mild steel with ease, and the beam can be angled to put ‘rubbish’ on the waste material and give a clean edge to the workpiece.
Gases are introduced under pressure around the lens to cool it before passing out along the beam axis.
If an inert gas is chosen oxides are not produced by burning.
Oxygen helps to increase the beam's cutting power.
In both cases, gas pressure blows fumes and waste away from the cutting area.
Fluids are an alternative cooling medium.
The joints that hold the gold plated, cylindrical copper mirror are standard components.
The mirror holders have to resist the effects of expansion once the optical path has been calibrated.
The beam reflects from minor to mirror inside a series of tubes.
Race for the cheap fibre
COMPANIES that supply optical fibres to British Telecom are stepping up their efforts to cut the price of the fibres in the face of international competition.
They have a long way to go.
Low grade index fibre costs around £200 per kilometre, and best quality fetches £350.
But the monomode fibre that British Telecom favours for its trunk lines costs up to £700 per kilometre.
GEC, which has been making fibres for Telecom for two and a half years, says it is aiming to get the cost down to £100 per kilometre by the end of the 1980s.
The other suppliers, Plessey and STC, have similar goals.
However, if every home is to have its own fibre link, the price will have to fall to around £25 per kilometre.
Despite the cost, Telecom is now irrevocably committed to optical fibres in all its new telephone speech and data trunk lines.
The organisation planned its first fibre field trials in 1974, and began them in 1977.
The latest inter-city fibre trunks can carry 1920 simultaneous telephone calls in digital code at a pulse rate of 140 million bits every second.
Next year this capacity will double, as will the spacing of repeater stations, to 60 km.
After the end of 1983, Telecom will order no more copper coaxial cable for its trunk routes, and by the end of the decade one half of its trunk lines will be fibres.
The rest will be microwave radio links or old copper coaxial.
If the cost of fibres falls sufficiently, Telecom will start using fibres for its junction networks inside city areas.
The long-term aim: a single fibre into every home, carrying cable television and telephone calls.
High speed for high rollers
THIS SUMMER a new type of high-speed craft will join the cut-throat competition to carry gamblers from Hong Kong to the casinos of Macau.
Vosper Hovermarine of Southampton is building four HM 527 sidewall hovercraft, worth £12 million, for the Hong Kong company Sealink Ferries.
The first will be shipped out over the next few weeks to start competing with the conventional hydrofoils, Boeing Jetfoil and high-speed catamarans that vie for business on the 78 km route to Macau.
The 27-metre HM 527 cruises at 67 km/h with a full load of 200 passengers, and gives a smooth ride in waves up to 2 m high.
Vosper Hovermarine is the world's largest manufacturer of sidewall hovercraft, having built more than 100 smaller designs over the past 15 years.
A sidewall hovercraft uses the air-cushion principle of trapping air  between partially immersed longitudinal twin hulls (sidewalls) and flexible rubber seals (skirts) at the bow and stem.
The air cushion reduces the amount of the structure in contact with the sea.
Four marine diesel engines provide the power to lift fans that keep the air cushion inflated and to drive the twin underwater propellers.
True hovercraft, such as the API-88 (New Scientist , vol 97, p 297), do not have immersed sidewalls and use air propellers.
The HM 527 uses angled water rudders to bank the craft into a coordinated turn, rather than skidding sideways, and Britain is the only country in the world that still regards such a vessel as a hovercraft (with its special regulations) rather than a high-speed ferry.
Vosper Hovermarine prefers to call it a surface effect ship.
The glass-fibre HM 527 accelerates to a speed of 65 km/h in only 90 seconds and can stop from full speed in 25 seconds.
During this time it travels only five times its own length, and the widely spaced propellers allow it to turn in its own length at low speed.
This is particularly important in Hong Kong's crowded waters.
Local conditions have placed unusual constraints on the HM 527's designers.
If it is to be profitable on the Hong Kong to Macau route, a craft has to operate in darkness as well as during the day.
The problem is that unlit wooden sampans or floating logs, which are difficult to see with radar, can suddenly appear in front of a vessel travelling at speed.
Vosper Hovermarine is developing a night-vision system that combines television cameras, sensitive to low light levels, with infrared illuminators.
The low-light television is suitable for use in harbour, where there is always a certain amount of light.
When en route, the HM 527 will see through an array of infrared illuminators covering an arc of about 25o.
Although  Vosper Hovermarine has no other customers for the HM 527 at present, several operators are showing interest.
These include the Merseyside Transport Executive and Red Funnel Line, which runs ferries to the Isle of Wight.
Swan saver
FEWER swans could be poisoned by lead from anglers' lines if the fishing community adopts a new type of weight.
The Nature Conservancy Council has estimated that 3500 mute swans, one-sixth of Britain's population, die every year after swallowing lead weights.
The new weight is a malleable plastic impregnated with powdered stainless steel.
If swallowed, the substance rapidly breaks down in the swan's gizzard, and passes out with no harm done.
Tests at the Institute of Terrestrial Ecology, at Monks Wood near Huntingdon, have shown that the new weights do not harm swans Researchers fed plastic weights and lead weights to pigeons.
The birds that ate the new weights showed no ill effects.
The next obstacle is to persuade anglers to adopt the new material.
Although it looks like lead, it is only one-third as dense.
So anglers need larger weights to do the same job — which could distract the fish.
Computer poly
STUDENTS on a part-time MSc computer course to start next year at Kingston Polytechnic will get their own microcomputers so that they can work away from the polytechnic's classrooms.
The course, called Information Systems and Design, is the first of its type in Europe claim its sponsors, ICL and the Science and  Engineering Research Council (SERC).
Angled at business applications of information technology, the course covers such relatively novel ideas as office automation, the human aspects of computing, computer aided design and the use of computers to aid in management decision making.
‘We are moving away from conventional computer engineering and concentrating on how to get improved productivity from computers,’ said Reg Key, director of ICL's consultancy and training division.
Swift shooter
KODAK is soon to start selling 35 mm colour film which is 10 times faster than Kodacolor II, and can thus take pictures with one-tenth the light.
This means that photographers can shoot inside with available light, reach further into the dark with flash and use longer telephoto lenses with faster shutter speeds.
The key is a change in the shape of the silver halide crystals in the emulsion.
Kodak says this change is the company's biggest advance in emulsion technology for more than 50 years.
It is a spin-off from development work done on the Kodak disc camera, which has a circular disc of film instead of a roll, and lets very amateur photographers take usable photographs in unfavourable light.
In a conventional film emulsion the silver halide grains look like cubes, octahedra or irregular pebbles.
Faster films need larger grains of silver to form an image in less light.
Normally, larger grains mean thicker layers of emulsion and coarser images.
Kodak found that by flattening the grain into tabular or T-shape, the same amount of silver intercepts more light.
So the emulsion gets faster without losing clarity.
The new Kodacolor film emulsion is called VR 1000.
Architects take lessons from indoor earthquakes
JAPANESE and Us engineers have made important advances in understanding how buildings behave in earthquakes.
The work, the result of a long-term joint research project, is likely to influence building codes in both countries.
The Americans, at the University of California at Berkeley, have successfully tested a one-fifth scale model of a seven-storey reinforced concrete building in Tsukuba, the — science city’ near Tokyo.
The model behaves so much like the real thing that the  Berkeley scientists believe they can predict what would happen to the building during an earthquake.
The best way to mimic an earthquake is to place a building on a shaking table which subjects its Foundations to the same force as the acceleration of the ground during an earthquake.
Electro hydraulic jacks under the table produce the movement.
But to build a shaking table that can support a real seven-storey building would cost more than $100 million.
The building itself, and the instruments that it would need, would come to another $2.5 million.
The long-standing US-Japan Cooperative Earthquake Research Program, which the National Science Foundation and the Japanese government pay for, decided to look for a cheaper way of doing things.
The largest shaking table in the US is at Berkeley's Earthquake Engineering Research Center.
It is 6 metres square, can cope with loads of 60000 kg, and cost $1.5 million in 1971.
Japanese researchers have been subjecting the real seven-storey building to ‘pseudodynamic testing’.
They do this by applying forces through hydraulic jacks on each floor, connected to a retaining wall.
Berkeley's replica of the building, which cost $250 000 to build, has the same  physical and stress characteristics of the real thing.
It weighs 10 tonnes and stands 5 metres high.
Over recent months, the Japanese and Us teams have been increasing the intensity of the forces they apply to building and model, and have been comparing the results.
According to Vitelmo Bertero, a civil engineering professor at Berkeley, the correlation between the two structures has been ‘good to excellent’— especially in the acceleration range of up to 20 per cent g .
(Ten per cent g is enough to cause structural damage, extreme earthquakes can generate more than 40 per cent g.)
Bertero believes the findings show that the model can legitimately mimic damage to real buildings.
But what can scientists learn from the experiments?
Professor Bertero reports that the model has withstood forces of 40 per cent g and 48 per cent g .
‘In the second test the steel reinforcement buckled, but the building did not pancake,’ Bertero said.
The roof moves 7.5 cm relative to the base, equivalent to 37 cm in a real building.
This kind of movement would cause panic and injuries in the top two floors.
Bertero advocates stronger codes to govern the non structural elements, such as ceilings, windows and partitions, in tall buildings.
The US should follow Mexico and New Zealand and set windows in rubber so they flex during an earthquake.
The research also supports the need for a shear wall down the centre of buildings: it absorbed 85 per cent of the initial lateral force of the earthquake.
Bertero says that architects have reluctantly accepted the need for shear walls, but they are now common in earthquake areas.
Dr Phlogiston, the ‘honest heretic’
Peter Austerfield
This month marks the 250th anniversary of the birth of Joseph Priestley, chemist, cleric and reformer, whose scientific discoveries and powerful imagination, together with outspoken views on freedom of thought, education and religion, inspired contemporaries and future generations
THE NUMBER of times their new minister was seen slipping into Jakes and Nell's brewery in Meadow Lane, Leeds, worried some of his congregation.
For a man of the cloth to spend so much of his time there suggested all manner of things.
Perhaps the demon drink had won?
Why else would a man spend hours over foaming vats other than to anticipate the pleasures to come?
Joseph Priestley's pleasures were, however, of a different kind.
It was the plentiful supply of ‘fixed air’(carbon dioxide was so called in the 18th century) above the vats that held his attention.
The gas, he discovered, was easily dissolved in water, giving a sparkling drink with a ‘pleasant acidulous taste’.
It could hardly be distinguished from ‘good Pyrmont, or rather seltzer water’.
Pumping the gas in under pressure produced a beverage almost equal to that of the best mineral springs.
Such then was the origin of soda water and a major industry, a service, according to Thomas Henry Huxley, writing over 100 years later, ‘which all those thirsty souls with parched throats and hot heads could not too greatly acknowledge’.
‘Aerated water’, or soda water as it came to be called, was but one of Joseph Priestley's many inventions.
His scientific discoveries and his fight for religious and political freedom, form equally important parts of an exceptionally industrious life.
A man of strong character and like so many Yorkshiremen used to speaking his mind, Priestley's energy and powers of application were intense and the range of his work wide.
His many biographers have attempted to come to terms with his prodigious output of educational, religious and political texts and his scientific publications spanning nearly 40 years.
He achieved the rare distinction of being equally famous in more than one field at the same time.
Admired and revered or loathed and seen by some bigots as an enemy of society, Priestley was an intellectual champion of many causes.
Yet, ask the average mastermind of today what Priestley is best remembered for, the reply is usually ‘He discovered oxygen’.
Well yes, and ammonia, hydrogen chloride, sulphur dioxide, nitrous oxide and silicon tetrafluoride among other gases.
‘He showed that growing plants could restore the respirability of used air’.
Yes again, and this led to the study of photosynthesis.
‘His house in Birmingham was burned down by a mob’.
True, but then so were several others.
This had nothing to do with his chemistry.
Already the story is getting complicated and Priestley's full life made it so.
Soda water, Priestley thought, was his happiest idea and the one with least  scientific merit.
But soon his frequent visits to the brewery had to cease.
The brewers rightly became enraged one disastrous day when Priestley, trying to determine the solubility of carbon dioxide in ether, let some of the solvent escape into the fermenting vat, ruining the whole brew.
Forced to work at home, he developed techniques for preparing and collecting gases using what was to hand.
Basins from the kitchen and beer glasses provided excellent utensils.
He adapted from the earthen vessel used by Yorkshire housewives for washing linen, a ‘pneumatic trough’ for collecting or keeping gases over water.
Soluble gases he collected in smaller apparatus over  mercury .
A gun barrel thrust into the fireplace provided him with the means of heating substances to high temperatures.
‘But’, wrote Priestley, sadly, ‘the only person who gave much attention to my experiments was Mr Hey, the surgeon’.
The whole of Europe was soon to be interested in his work.
In the early 1770s much hope was placed on supplying drinking water for the navy using a plan suggested by the naval surgeon Charles Irving of making fresh water from sea water by distillation.
At the same time, ways and means were being sought of preventing scurvy, the scourge of all who crewed ships on long voyages.
An opinion made popular by a Dr David Macbride held that scurvy was due to an inadequate supply of fixed air in the human body, and that it might be cured by a dose of that gas.
The idea was proposed to Priestley that his carbonated water, which could be prepared on board, might provide doses of carbon dioxide and thus prevent scurvy (as well as make the distilled sea water more palatable).
After all, Macbride had shown that carbonated water prevented putrefaction and had antiseptic properties.
Such a suggestion soon reached the attention of Lord Sandwich, then the First Lord of the Admiralty.
The Board of the Admiralty asked the College of Physicians to investigate the possibilities of such a cure against scurvy, a disease that had hitherto lost naval engagements, without a fight.
The report from the college was favourable and as a consequence two warships were fitted with the apparatus that Priestley devised.
It is unlikely though that many sailors were saved from the ravages of scurvy before the naval brass hats were convinced, in the 1790s, that the answer lay with the diets devised by Scottish physician James Lind.
(His introduction of lime juice (and hence vitamin C) into the sailors' daily rations cured them of scurvy and gained for the British tars the nickname of ‘limeys’.)
The members of the Royal Society, although they probably didn't adopt the social manners and customs of the 1770s and actually drink Priestley's soda water at their Dining Club, were greatly impressed by its merits.
The Council of the society rewarded him with the much coveted Copley medal for his work on airs and soda water.
One of Priestley's main chemical interests was to determine what different kinds of gases (or ‘airs’) there were and he advanced both their chemistry and number.
That there were chemically different gases had been shown earlier in the 18th century.
Joseph Black had described carbon dioxide in 1756 while Henry Cavendish described hydrogen in 1766.
Priestley's most famous addition in 1774 is widely claimed to be oxygen.
Actually oxygen had been independently discovered by the Swede Karl Wilhelm Scheele the year before, but his findings were not published until 1777.
Oxygen later provided, in the hands of the great French systematiser Antoine Lavoisier, to be the key to a new era in chemistry.
Oxygen gas fascinated Priestley.
He produced it by heating red mercuric oxide with a 12 inch diameter burning glass.
Investigating some of its properties he noted the violence of the explosion when it was mixed with hydrogen and ignited.
This led him to suggest that perhaps bladders full of oxygen and gunpowder might be used to good effect in mining.
Priestley did not neglect the medical possibilities of the gas.
He breathed a quantity of it, noting that.
’…my breast felt peculiarly light and easy for some time afterwards.’
It could well be useful in certain respiratory conditions he thought, but there was a danger that just as a candle burns out much quicker in oxygen, one might ‘live out too fast’.
Perhaps, he reflected, ‘the air which nature has provided for us is as good as we deserve.’
Although at the time of his discovery Priestley could not have foreseen the use of oxygen in aeronautics as Vital Air for the aeronauts to breathe when the atmosphere became too thin, such use was suggested as early as 1784.
Only a year after the invention of balloons, a book was written describing a voyage, by balloon, to the newly discovered planet Uranus.
The author, Vivenair, made sure that the intrepid traveller had a supply of ‘Dr Priestley's Vital Air’ in bottle for the journey.
More seriously, however, Priestley's work on gases did influence the development of balloons.
His Experiments and Observations was avidly read by the Montgolfier brothers during their search for light permanent gases as lifting agents in 1782–83.
In the end they stuck to hot air (see New Scientist .
vol 96, p 521); the cost of producing hydrogen, the lightest gas, from sulphuric acid and iron of zinc, frightened them.
It was not long before a cheaper method of making it became possible.
That ‘Mr Lavoisier had found a method of making inflammable air (hydrogen) for a halfpenny per cubic foot’, a fifth of the usual price, attracted Priestley's attention.
With his Yorkshireman's eye for economy he was soon suggesting that the more impecunious aeronauts might seriously consider the process.
The basic ingredients, water and iron, were cheap.
It took no more time to produce the gas by the new method than it did using metals and acids.
Furthermore, following his own careful experiments and measurements (the results agreeing closely with modern figures), he noticed that a third more gas was produced from a given amount of iron using decomposition.
For it was by this method, the decomposition of water or steam by red-hot iron, that Lavoisier had produced his cheap gas.
While disagreeing with his explanation, Priest ley was quick to describe suitably scaled up apparatus for filling balloons.
Later he considered a centralised system able to produce gas for a whole neighbourhood.
Little came of this but in the next decade, the 1790s, the French armies were to use the large-scale production of hydrogen by decomposition to some effect when balloons first found military use.
All this, and much more, would be discussed over a good dinner and some equally good bottles of wine during an afternoon.
Even in the 1780s travel was difficult and on foot or by horse — it took at least four days to get from Edinburgh to London.
Consequently scientists did not move around as they do today, or even in the 19th century, and local learned societies and journals flourished.
There was in Britain no organised social and state support for science.
Few scientists held university posts, and education in science was weak.
Birmingham.
however, had a technically cultured society without parallel elsewhere in Great Britain — except possibly in the metropolis.
A group of like-minded individuals used to meet at each other 's house on a Monday nearest the full moon.
The conversations of the ‘Lunatics’, as they were called, would often carry on as they took advantage of the moonlight to find their sometimes uncertain way home over the ruts and rubbish in their path.
Priestley's company was always welcome, for he was the most eminent scientist of the Lunar Society in Birmingham, a group that included Matthew Boulton, James Watt, Erasmus Darwin and Josiah Wedgwood.
Priestley's arrival in Birmingham had given fresh inspiration to the members but his influence, for once, was not beneficial.
His best scientific work was behind him.
As well as his chemical discoveries he had made a reputation for himself from his electrical investigations.
He had been promoting and popularising science for over 20 years.
But he was now fighting a losing battle in support of the ‘Phlogistic Theory’— the idea that the processes of combustion released a mysterious substance named ‘phlogiston’.
It seems sometimes as if this theory, used to explain the processes not only of combustion but also calcination, definitely had been handed to Priestley for special keeping as he adhered to it unswervingly all his life.
The hypothesis was overloaded and overworked, and Lavoisier used Priestley's own discovery of oxygen to demolish it and explain combustion and calcination correctly.
Within a few years most chemists had adopted the anti-phlogistic theory.
In Birmingham, however, Priestley's influence retarded the reception of the new theory.
Group identity, as Karl Hufbauer shows in his recent book The Formation of the German Chemical Community (New Scientist , 17 March, p 744) has shown, can be a powerful force in rejecting a new theory, especially if it was a ‘passing french fad’ and some thing typical of the ‘flighty French’.
Landlord Dadley had begged the Birmingham committee of the ‘Friends of Freedom’— the group for ‘revolutionary’ political change — not to cancel their dinner at his hotel.
Five shillings per head for 80 guests represented considerable business.
‘Leave early,’ he advised, ‘and there should be no trouble.’
That there should be trouble on a July afternoon in Birmingham might seem strange until one notes the date: 14 July, 1791, was the second anniversary of the storming of the Bastille.
For some it was an occasion for celebration.
The ‘Church and King’ front however, now that Louis XVI was held in Paris (and soon to lose his head), regarded the date with repugnance.
With increasing ferocity its members lost no opportunity to whip up feeling against anyone who supported the aims of the French Revolution.
Priestley supported them and made it well known, so much so that he was asked not to attend the dinner — just in case.
A list of guests was obtained and published and a threatening handbill distributed.
Those who gathered there were hissed and hustled as they went in and by evening a large and riotous mob had collected outside the hotel.
After breaking the windows, enraged by the early departure of the guests, the mob set fire to two meeting places and then moved on to Priestley's house.
Fortunately he had been warned and had left.
The mob destroyed his house, library, laboratory and notes, but they took advantage of his well-stocked wine cellar.
His enemies rejoiced.
Even the king was not sorry that Priestley was ‘the sufferer for the doctrines he and his party have instilled’.
That it came to this shows how involved Priestley was in the religious disputes and politics of his day.
Priestley was not a Christian, at least as far as George III and the Anglican Church were concerned.
He was a Unitarian and his criticism of Biblical texts and his political beliefs seemed to threaten the foundations of both Church and state.
Priestley campaigned with pen and from the pulpit for the right of a man to declare and defend his religious principles.
In political matters he supported American independence, ridiculing the ‘very idea of distant possessions’.
Although on friendly terms with Matthew Boulton before going to Birmingham, he would not give him the details of a cannon ‘that the Americans have constructed on a new principle’.
He was convinced that Boulton would use his ‘superior ingenuity to improve it’ and use it against the Americans.
The waste of public money for war produced sentiments that are all too familiar today.
The expense of the American War of Independence, Priestley pointed out, ‘would have converted all the waste ground of this country into gardens.
What canals, bridges and noble roads would it not have made for us?’
His outrageous treatment brought him sympathy despite his denigrators.
Even the judge, summing up at the trial of some of the rioters, made it quite clear where the duties of the jury lay.
‘Every man’, he reminded them, ‘has a right to hold his particular opinion, and if you do not convict you are enemies of your country’.
A shock for many as the rights of a citizen were now being made plain.
There was however no future in England for Priestley.
He and his wife Mary lived in London, unable to travel freely and even avoided by some of his fellow scientists.
The Priestleys followed their three sons to America.
His death in 1804 removed ‘one of the few lives precious to mankind’, in the words of Thomas Jefferson, third president of the United States.
In the hands and minds of another generation, Priestley's work was not in vain.
It produced profound social changes.
Designers get the picture
John Bell
Computer-aided design is a complex technology with complicated effects.
It replaces traditional drawing abilities yet it can preserve the centuries of craft skills built up by shoemakers.
Buyers are understandably wary of this rapidly evolving market
COMPUTER-AIDED DESIGN is a method of drawing ideas on a monitor screen and recording them.
It first appeared in the American aerospace, microelectronic and motor industries.
Interest in the UK began in the early 1960s when firms began to experiment with interactive graphics using mainframe computers.
Microcomputers arrived at the start of the 1970s, by which time a body of expertise in computer-aided design (CAD) had been built up.
Turnkey systems arrived soon after and the technology began to evolve and grow rapidly.
Interest has intensified in the past few years, encouraged by the scarcity of draughting skills, tough markets and the tumbling costs of CAD.
One American market research firm reckons that the computer graphics industry will grow at a compound rate of 41–6 per cent until it tops $17 billion in 1987.
The trend in CAD is away from‘number crunching’ systems that use a lot of computer memory.
It is towards high definition programs that take a long time to write.
The result is that CAD is now used by architects, pharmacists, shoemakers, the rag trade, lighting manufacturers, surgeons, the chemical industry, graphic designers, video and television companies as well as those industries where it all started.
The next industry to adopt the technique widely is likely to be mechanical engineering.
Not only has this industry failed to invest in new-product development in the past, it has had to adopt electronics too.
Both metal bashing and circuitry are technologies well suited to benefit from CAD.
Computer-aided design is not a specific term.
It includes experimental systems that provide tactile feedback from a monitor image.
The term applies also to draughting machines that have increased productivity in some UK drawing offices by 300 to 400 per cent.
Between the extremes are solid modellers that treat an image as if it were solid — a kind of ‘clay model’ in a computer.
Another type draws three-dimensional sketches like wire frames.
Any electronic drawing system that allows the designer to sketch, visualise and change plans or ideas may be said to offer CAD.
A basic system consists of a monitor screen, a programmed computer, means to change the screen image, and a printer to make copies of drawing and figures.
Pictures are changed with an electronic pen, a data tablet (an instruction board wired into the system), a keyboard and digitizers to turn analogue into digital information.
An example of a digitizer might be a probe that will let the computer ‘read’ a shape or colour.
The experimental GROPE system, which gives tactile feedback, is being developed at the University of North Carolina.
GROPE has a manipulator that controls a grab shown on the screen.
(Like the tools used for remote handling.)
Make the grab move some blocks around on the screen and the manipulator gives the user a sense of the weights and solids involved.
CAD can, in theory, enable shapes or text shown on a screen to be altered by hand.
It could be done by combining a video shot of the user's hands with the screen picture.
The system would treat both images as solid objects.
Most CAD installations in the UK are turnkey units — so-called because they come complete and all the buyer has to do is turn the key and start learning.
The cost of a turnkey set-up with three-dimensional, interactive graphics in full colour is about £75 000 per terminal, and falling.
One machine which is especially useful for creating video and film titles costs under £10 000.
Cost-effectiveness
The CAD market is highly competitive.
Sales staff push their own brands hard.
Newcomers to CAD say that it has its own jargon.
Mechanical Engineering News — the newspaper of the IMEE — complained in January: ‘It is obvious that few engineers understand the terminology and jargon that is prevalent in editorial copy and advertisements for computer systems.
Even the suppliers themselves use the same acronyms to describe different processes.’
The Department of Industry provides CAD information centres up and down the country to give the tyro some help.
So the newcomer need not be totally submerged at first contact with CAD systems suppliers.
CAD is most economical with repetitive or complex designs.
Shortages of trained draughting staff has increased its cost-effectiveness.
The advantages and disadvantages vary from company to company and between industries.
Speed of design is an early benefit which is often quoted.
CAD shortens lead times, makes quotations more accurate and faster to prepare.
It gives draughting staff more time to think which increases job satisfaction and creativity.
Most gains are intangible.
One that takes time to appear is the creation of a data base of information applicable to a whole range of products.
Another is the chance to try many variations before deciding on a design.
Because everyone uses one set of data, specifications are consistent, standards are uniform, there will be no inaccuracies caused by recopying, and products can be updated quickly.
Establishing common data gives the stylists, managers and engineers an unexpected surprise: the opportunity to start computer aided manufacture (CAM), usually via numerical control (NC) machine programs.
Employee response and reaction to CAD depends on their attitude and ability, as well as age.
Older people do not adapt as easily as young people.
Friction between the generations is exacerbated when younger staff grasp the new idea and their creativity is suddenly released.
Diplomacy in a CAD manager is to be treasured.
Computer-aided design may make a good designer out of a sloppy one.
A survey by the Engineering Industry Training Board found that the technology widened the gap between people with different levels of skills.
The EITB survey covered 34 engineering firms.
They were of various sizes and worked in the vehicle, aerospace, mechanical engineering and electronics industries.
Businesses in the sample reported a typical three-stage learning process: ‘The first phase of learning involves the basic ‘driving’ skills: the use of CAD as little other than an electronic pencil.’
The next step involves staff learning complex commands to tell the machine to carry out a sequence of tasks.
It requires a change in thinking because ‘there is no longer a simple correspondence between the way things are done on a drawing board and using the machine.’
The final phase is the development of a sense of the machine as a system to make effective use of the data base.
It may require the operator to learn simple programming.
‘Initially, in moving from one phase to the next, there is a loss of productivity as the operator explores and experiments with new facilities.
Further operator learning…is largely dependent upon management learning,’ the report warns.
One firm that has recently installed a CAD system is the Somerset shoemaker Clarks Ltd.
For 150 years Clarks thought the household mangle was the best way to solve the mathematically impossible problem of converting a three-dimensional design into a flat leather pattern.
The firm now has a computer that uses a touch of ‘consistent compromise’ to do the job.
Shoemaker steps in
It took four years to develop a CAD system for shoe making.
The software, which was written by CAD Centre in Cambridge, gives pictures and patterns of finished shoes on a high-quality monitor screen.
It uses the Centre's Polysurf program which can define free-form surfaces.
The program also has an art’ module’which allows a designer to draw lines and paint up to 16 million colours on the screen.
Pattern flattening is done by additional mathematics on a specially written part of the Clarks program.
Until recently, none of Clarks' designers had used the CAD system.
They preferred to stick with the old ways.
The first person to produce work on the screen was a cobbler.
Shoemaking begins and ends with a last, the three dimensional pattern of the inside shape of a shoe.
Traditionally designers start the process by holding a last, vacuum coated with a plastic film called a drape, and drawing on it.
The cobbler then slits the three-dimensional pattern down the heel, removes it from the last and cuts away excess material.
Next the film is gently flattened by hand.
The cobbler slices through the plastic at points which he judges to be under tension.
The flattened shape is transferred to a sheet of leather by mangling carbon paper between the two.
Finally the leather is made into a shoe around the last.
No two cobblers ever cut a pattern in the same way; individualism is a matter of pride in traditional cobbling.
Pattern making is a time-consuming process that must be repeated for every design prototype.
Clarks knows that CAD will cut its lead times because the system allows design, cobbling and marketing decisions to be made before patterns are cut.
Everybody can watch the same design on the screen and see styling improvements and changes as they are made.
Shoemaking with CAD starts with a last marked with a grid and clamped in a digitizer.
This machine gives the computer digital information about the last's freeform surface.
A pointer is touched against each point of intersection on the grid and the digitizer records the position in the computer.
When the digitizing is complete the computer displays a wire-frame model — hundreds of rectangular planes joined together in a last-like shape.
The planes are then smoothed out, the surface is coloured and hidden areas are removed from view.
The last displayed can be rotated, enlarged and panned.
Areas are subtracted or added as they leave or come into view.
The designer draws on the last by using a digitizing pen on a tablet.
When the basic style is agreed, the computer draws a two-dimensional pattern of it complete with stress points marked.
Computer-drawn patterns have a slightly different shape to human-made ones.
When the system's first efforts were shown to Clarks' cobblers, many a grey head was scratched and shaken.
No one thought the patterns would work.
In fact, they were stitched into shoes that required 3 per cent less leather than before.
Problems with a pattern made by CAD can be sorted out before detailed styling begins.
Clarks' system will draw details, change colours, experiment with textures and vary the shading at will.
It is simple to drop unsuccessful ideas because the computer retains the last's shape and specific designs made on it.
‘We are trying to quantify an art and to get rid of the mystery around the shoemaking craft,’ said Geoffrey Egan, the development and engineering manager.
‘It has been an act of faith to keep going.
Looking back it is difficult to justify what we did at times.
We worked on gut feeling and it was very difficult to control and manage all the development work because of the technology involved.
At least now we know it makes smaller patterns, which we did not expect to see.
‘Clarks decided to automate the design end rather than in the factory because, over the years, we have produced smaller run sizes but more styles.
Now we hope to build a data base to analyse why one last is more successful than another.
This will provide cost data and improved mould making.
We carve moulds in wood and it takes a craftsman with a spokes have a day to do each one.
‘Information from the CAD system is already used to program four NC stitchers.
The next step will be to link them together directly.’
The idea of CAD was suggested to Clarks in 1968 by Tony Darvill, its technical consultant.
He got his inspiration from the aerospace industry.
The firm reckons it will be years before it finds the system's limits.
Potential and actual savings which have been identified include: shorter lead times, presently as long as six months; producing better, more up-to date styles; and shoes that fit better.
Last-making will be to a higher standard.
Model lasts — from which 10 to 12 production lasts are made for left and right feet — have to be made in five widths for each of 10 sizes.
Sometimes when a small pattern is enlarged to fit a bigger foot the resulting shoe looks out of proportion.
The computer last is a parametric model.
It will be used to program NC tool paths to cut model lasts in all sizes and width fittings.
Proportions can be checked before any lasts are made.
CAD had, and still has, its doubters at Clarks.
When they wanted to stop the investment programme, it was managing director Malcolm Cotton who fought for the idea.
‘We have been putting 150 years of empirical behaviour onto one program so there have been inevitable delays,’ he said.
‘CAD at Clarks is capturing the skills of the industry for ever.
It will also cut training needs at a time when people no longer want to serve long apprenticeships.
I also want to cut the overheads of employing staff not directly involved in shoemaking who make up one third of our workforce.’
Cotton is convinced Clarks has a unique system that is way ahead of any competition.
It can be adapted to clothing and Clarks intends to sell its expertise to the rag trade.
One manufacturer of ladies underwear is interested.
You may use your imagination to decide what will be digitized.
What does CAD mean?
THE ACARD report on computer-aided design and manufacture gives two definitions of CAD: Computer-aided draughting: ‘The use of a computer-based system for translating concepts or sketches into drawings suitable for use in manufacture, or for storing drawings or parts of drawings in a data bank to be available on call for modification, incorporating into a revised drawing, or as input to a subsequent manufacturing process.’
Computer-aided design: ‘The use of a computer-based system to assist in translating a requirement or concept into an engineered design, utilising a data bank of design principles and information on such matters as properties of materials, followed by production of information (which may be in the form of drawings) for manufacture.
Such designs may include ‘simulation’— the modelling of a design and calculation of performance, for example to reduce the need to build  experimental equipment.
This may be extended to ‘optimisation’— the search for the most suitable design to meet the requirements.’
There are many terms and acronyms in
CAD.
The following is a brief glossary: 2D means ‘computer-aided draughting’.
This type of simple system imitates the drawing skills of draughting staff.
It takes about the same time to do a simple drawing on a 2D machine as on a drawing board.
CAD gains dramatically when components have to be redrawn many times, when it may out-perform a human by 100: 1.
It is a very powerful tool indeed when standard parts are used, especially with small batch production on a flexible manufacturing system.
Draughting machines build up information about components in the computer memory.
They provide coding, classification and lists of parts.
Sometimes a new component can be drawn by modifying an old one held on file.
The 2D market is mainly for turnkey systems.
It is a very large part of the whole CAD market.
More and more manufacturers are offering to tailor-make a turnkey system from their own components.
‘It is potentially disastrous to mix CAD machines from different suppliers’, said one expert.
‘The interfacing would be a nightmare.’
2 ½D means a 2D drawing in the x-y plane repeated and viewed from another angle or moved along the z axis.
If a 2D line represents a pass along an NC tool path, using 2 ½ can show how material is cut away.
Each ‘pass’ may be in a different colour.
3D means being able to draw in the x, y and z planes.
A basic representation of a solid is a wire frame in which lines define plane edges.
Pictures can be rotated about any axis.
Some 3D systems can calculate volume, weight, moments of inertia, and centre of gravity of fairly complex shapes.
The machines must be told which parts of the drawing represent solid volumes and which do not.
The more detail in a wire frame, the harder it is to understand Most 3D systems have a colour algorithm (steps of procedure in a program) to show separate lines.
Eventually.
the number or position of lines make the picture incomprehensible.
A technique called hidden line removal is used to simplify matters.
Lines defining any surface hidden by any other surface are removed by the computer.
Layering is another way to sort out the confusion.
A layer is a defined group of components.
‘layer 5’ might refer to all the bolts in a design.
Call ‘5’ to the screen and only bolts are shown.
When a 3D model, with its hidden lines removed, is rotated about an axis, only the forward surface remains in sight.
Say it rotates left to right about the vertical axis, as a line segment reaches the right extreme, it is removed.
At the same time an equal segment is added on the left.
Surface modelling is the next level of sophistication.
The computer does this by drawing a coloured and/or textured surface on a defined area.
If the model is rotated, hidden lines and their surface remain hidden.
If the CAD system has a lighting algorithm then an apparent light source can be used to change the position and intensity of highlight and shadow detail.
Solid modelling is the most recent major development in CAD to be marketed.
It displays an unambiguous and realisable image — the computer's version of a designer's mock-up.
Sometimes it is impossible to tell the difference between a solid model and a 3D drawing.
Either can be a wire frame or surface model.
If the image shows perspective and not parallel projection of lines, it is likely to be a solid model.
Cutting a section is a more certain test.
A 3D model will show points and lines, where the sectional plane cuts through the ‘wires’.
The solid model will reveal a plane.
Solid-model CAD has interesting properties.
It differentiates between solid and space and calculates solid properties with out any need for extra information.
The benefits of solid modelling are often outweighed by its cost at the moment.
Aerospace is one industry that uses it.
Weight is crucial to planes and spacecraft so the expense is paid for by the extra detail it provides.
Solid modellers are slow to use.
They need perhaps ten times as much computational power as a 3D system to work at a similar speed.
A big benefit of solid modelling is its ability to detect interference between components If, say, two components are put together on a screen.
one can be moved in discrete steps relative to the other.
Should they ‘touch’, the solid modeller will refuse to move the component another step.
The wildlife of Britain's wasteland
Oliver Gilbert
Britain s industrial cities have many areas of wasteland, awaiting transformation to neat grassland.
But these ‘urban commons’ harbour a wide variety of wildlife and deserve to be preserved
MANY of Britain's industrial towns and cities are surrounded by wilderness, which sometimes has acquired the official designation of ‘urban common’; but all of them have wasteland within their bounds which may be no less deserving of such recognition.
This wasteland comprises demolition sites and vacant plots, usually owned; by the local authority.
Often they are left unused for years at a time, except, that is, for all those informal human activities, like adventure play, gypsy encampments, bonfires, dumping rubbish, and grazing goats, that are not allowed where the grass is mown and the trees are carefully planted.
These impromptu, ‘unofficial’ urban commons often have a natural history that is both rich and unique to a particular area: a blend of wild animals and plants with ‘escapes’from gardens long gone.
Citizens should see such areas as an asset, worthy of conservation.
Land left to its own devices is not always attractive to look at.
From early May until late September our urban commons are green jungles, reflecting seasonal changes in a way rarely seen elsewhere in towns.
But for the rest of the year they are dominated by dead plants.
A stand of rosebay willow-herb (Epilobium angustifolium), for example, with its striking tall spikes of pink flowers looks attractive for at most four months of the year.
For the rest of the time it is a mass of dead stems which can remain standing for up to two years, a sight distasteful to town-dwellers who, thanks to the energies of gardeners and the preference of landscape architects for evergreens, are not accustomed to seeing decay.
However, urban commons may provide an alternative to more formal ways of treating ‘spare ground’, cheaper and in the long run more popular, not only with the wild life, but with the human residents as well.
The ‘succession’ of plant life as it colonises new territory takes the vegetation of the areas through a series of phases, culminating in woodland.
With practice you can tell how long a site has been vacant by studying its flora, although the exact species vary from town to town.
In Sheffield in the first stage, when annuals and short-lived perennials move in, Oxford ragwort (Senecio squalidus), knotgrass (Polygonum aviculare), orache (Atriplex hastata)and fat-hen (Chenopodium album)are common.
Three to six years later tall herbs take over; in Sheffield we find rosebay willow-herb, tansy (Tanacetum vulgare), goat's rue (Galega officinalis), wormwood (Artemisia absinthium), mugwort (A. vulgaris)and occasionally golden rod (Solidago canadensis)among others.
Grasses gradually increase and after 10 years the appearance is mainly one of grassland with scattered clumps of tall herbs.
During this period woody plants also begin to appear and eventually the flowery meadows turn into open scrub and, after 40 years, incipient woodland.
Two features of this succession on urban wasteland are particularly interesting.
First, many of the early colonisers belong to groups of plants (genera) that were widespread during the Late glacial period 10 000 years ago.
These include Epilobium, Artemisia, Polygonum, Potentilla (cinquefoil) and Polemonium (Jacob's ladder), all of which are well established at several sites in Sheffield.
The presence of these genera suggests that conditions on the waste ground — which include intermittent disturbance, little grazing, low competition between species, and unleached soil — must have similarities to those in Britain 10 000 years ago.
Secondly, the deciduous woodland that eventually takes over has a rather surprising composition.
The best examples, in Newcastle upon Tyne and Birmingham, comprise mixtures of ash (Fraxinus excelsior), sycamore (Acer pseudo platanus), laburnum (Laburnum anagyroides), hawthorn (Crataegus monogyna), Swedish whitebeam (Sorbus intermedia ), crab apple (Malus sylvestris), goat willow (Salix caprea), guelder rose (Viburnum opulus)and broom (Cytisus scoparius).
Oak (Quercus spp)is never present.
Such wood land is quite unlike any other in Britain.
It is composed of species adapted to the urban environment and is influenced strongly by the availability of seeds.
Willows, which  pioneered the succession during the Late-glacial period, do the same on urban areas.
Many of the species arrive simultaneously during the early years, but grow at different rates.
In this way it appears as though new species arrive continuously.
The vegetation of urban commons varies region by region, and so unwittingly contributes to local character in contrast with most urban landscapes.
In the summer of 1981 I investigated this pattern of variation by surveying seven towns scattered throughout Britain: Bristol, Birmingham, Leeds, Liverpool, Manchester, Newcastle upon Tyne and Sheffield.
The most distinctive urban vegetation is undoubtedly in Bristol, with buddleia (Buddleia davidii)common everywhere, and wild fig (Ficus carica)one of the most exotic residents.
Newcastle upon Tyne is very different.
I found buddleia only once; instead large amounts of crab apple and elder (Sambucus nigra), and frequently hawthorn and bramble (Rubus fruticosus), characterised the woody vegetation.
Leeds, in 1981, was dominated by large areas of mugwort and wormwood, and because of the proliferation of this genus deserves to be known to botanists as ‘Artemisia City’.
It also contains more spontaneously-seeded ash trees than any of the other towns I studied.
The flora of most wasteland contains occasional species typical of moist soils, such as Himalayan balsam (Impatiens glandulifera)and hemp agrimony (Eupatorium cannabinum ).
But in Manchester wetland species are rife.
And Liverpool once boasted the closest approach to a ‘village pond’ on an urban common.
Until recently flooded cellars near the docks supported a well-stocked aquatic ecosystem.
Sheffield has a particularly high density of colourful garden escapes such as Michaelmas daisy (Aster novi-belgii), tansy and goat's rue.
In Liverpool woody species are unaccountably rare, legumes (pea-flowers) reach their highest abundance and diversity, and evening primroses (Oenothera spp)are widespread.
Citizens of Birmingham are treated to some of the finest displays of the North American golden rod to be seen in the country.
Why are there such large differences in the vegetation of wasteland throughout Britain?
The ecological factors responsible are largely unexplored, but the availability of plants to provide seeds seems to be of prime importance.
Bristol is penetrated by calcium-loving vegetation growing on the limestone cliffs of the Avon Gorge.
Some of this, such as traveller's joy (Clematis vitalba), has extended its range on to the alkaline mortar-rich soils of the urban wasteland.
The large numbers of evening primrose in Liverpool reflect its abundance on the sand dunes just outside the city.
In his English Botany (1806) James Sowerby mentions them as in their millions on the coast a few kilometres off Liverpool.
But many variations remain enigmatical.
Why, for instance, is the Michaelmas daisy so scarce in Birmingham?
A common thread to the sites throughout Britain, however, comes from plants originating in gardens that formerly occupied the sites.
The most predictable of these are the mints (Mentha spp).
The  common mint of gardens is spearmint (M.spicala), with its pointed hairless leaves.
But the apple mint (M. rutundifolia)with rounded hairy leaves is also frequent, as are the peppermints (M. piperita).
The latter are vigorous sterile hybrids which send out long rooting runners, and so are equipped to survive the rigours of demolition and grading.
Our urban commons also support rich animal communities.
Sheffield has a very active Natural History Society, which has accumulated a good knowledge of this city's fauna.
The species found fall into three loosely-defined ecological groupings.
First, there are those that can tolerate a wide range of ecological conditions, for example the centipede Lithobius forficatus , the millipede Polydesmus angustus , a number of earthworms, slugs, and several woodlice.
These are common throughout the region, including wasteland.
Several butterflies, such as the wall (Lasiommata megera)and meadow brown (Maniola jurtina), also quickly build up breeding populations.
And by late summer, every fragment of the urban common is loud with the chirping of the common field grasshopper (Chorihippus brunneus), which hops into carparks, on to pavements and even into shops.
The second group of species common on Sheffield's wasteland are the invertebrates that can be described as synanthropic, or culture-favoured.
Examples are the woodlice Androniscius dentiger.
Cylisticus convexus and Metoponorthus pruinosus (in Rotherham); the slugs Deroceras caruanae and Milax budapestensis .
and in Sheffield the fiat snail Oxychilus draparnaudi .
Many of these species also occur in gardens and may, like the mints, survive from former gardens or be introduced with dumped garden refuse.
The third group is the least well known, because most naturalists are reluctant to study urban sites.
This comprises a small number of true wasteland species, commoner there than in other habitats.
The larvae of certain moths, for example the wormwood shark (Cucullia absynthii), depend on food plants growing on wasteland.
And rare microlepidoptera feed on the roots of tansy, or mine the leaves of fat-hen.
The list of species is growing as more and more animals adapt to the urban commons.
In South  Yorkshire , the snail Cepaea hortensis and the pill woodlouse (Armadillidium vulgare), both normally centred on limestone terrain, are beginning to turn up on fragments of the urban common that are rich in mortar and therefore calcareous.
Studies of the diet of kestrels in Sheffield and Manchester have shown that urban populations take fewer small mammals than do those in the countryside.
But the town-based predators capture substantially more birds such as sparrows, starlings and pigeons.
So it is not only the composition of species that is different on urban commons, but also the life-styles of those species.
The extent of our urban commons is difficult to calculate.
The Civic Trust undertook a survey in 1976 and estimated that there were then 104 670 hectares of urban wasteland in Great Britain.
The average size of these plots was less than 1 hectare in the inner city, but up to 10 hectares at the edges.
In 1978, 5–58 per cent of inner London was wasteland; in 1979 inner Liverpool had some 400 hectares of waste ground.
Attitudes to wasteland vary but the official view is that it is unsightly and depressing, contributing to an air of dereliction and offering an example to vandals.
Liverpool is currently spending in the region of £4 million a year reducing its urban common.
As dormant land is an inevitable product of the planning system in the UK, we need a definite policy to deal with what will be a long-term feature of the inner city.
Councils are reluctant to allocate finance for temporary schemes, so community groups sometimes take a hand.
These groups find positive uses for the land, such as urban farms, allotments, community gardens, play parks, or car-parking.
In Liverpool a group called COMTECHSA encourages this kind of positive action.
And the city's Greensight Project, started by the Rural Preservation Organisation, is now attracting a great deal of attention.
Its philosophy is to treat sites in such a way that wildlife is encouraged back into the city.
The project's treatments cost as little as 54p per square metre.
On the other hand, the standard aesthetically-sterile interim solution of grass and trees, which most local authorities favour, costs between £5 and £7.50 per square metre.
The Greensight approach is not only cheap to implement and maintain, but also flexible, so that sites can be treated individually.
Members of the project carry out their own maintenance, as few sites would survive long if handed over to the local authority.
The designs are not static; they represent only a beginning.
Eventually Nature becomes the designer, and the maintenance required is that sufficient to ensure that the sites remain acceptable to the public.
The Greensight Project harbours the rudiments of a  solution to the problem of our urban commons, which suffer from being either totally neglected or changed into featureless mown grass.
What the mass of urban wasteland needs lies between these extremes.
The minimum effort required is to treat the edges in some permanent way, to clear up rubbish regularly and, as many waste areas are rather flat, to give a structure by planting clumps and belts of fast-growing shrubs and trees.
On most sites the physical condition of the soil is ideal for plant growth and its store of nutrients (except for nitrogen) is comparable with many topsoils.
The addition of nutrients has made little difference to the rate of succession at sites in Sheffield.
With this minimum treatment, and a few surfaced paths, many urban commons could quickly become valuable and inexpensive assets.
They represent true urban communities; they would not be trying to mimic the  countryside .
The inhabitants, having colonised naturally, would be well-adapted to the local conditions; no two commons would be alike; and garden escapes, which are often very colourful and have a long flowering period, would be welcome.
With a higher initial investment many of the most attractive species, such as  buddleia , Michaelmas daisy, tansy, feverfew (Chrysanthemum parthenium), and evening primrose, which are easy to manipulate, could be introduced in drifts.
As all these, including buddleia, flower in their second year they soon supply more seeds to the young sward.
Workers at the Heeley Urban Farm in Sheffield have spent several days collecting seeds from these flowers with the intention of growing them as a crop, eventually to sell the seed as an urban flower-mix.
The acceptance of tidied-up urban commons as an alternative to mown grass may be well in the future as far as most local authorities are concerned.
But there could be some surprises.
At Slotermeer, a suburb of Amsterdam, a large park remained unfinished due to lack of funds.
In time, the natural succession of plants turned this into an informal landscape which became very popular with the local residents.
When finance became available to complete the park in a traditional gardenesque manner, there was so much opposition that, apart from surfacing a few paths and strengthening the natural scrub, it was left.
And it remains an urban common by popular request.
There is a lesson here for many local authorities in Britain.
LONDON
THE WILDLIFE of London enjoys a huge variety of habitats.
On the outskirts these range from the marshes of Walthamstow to the heaths of Hounslow, and from Ruislip woods to Ruxley gravel pits.
Suburban London has large tracts of common land such as Hampstead Heath and Richmond Park.
Railway lines channel wildlife into the city centre along their cuttings and embankments.
The metropolis also houses a host of smaller patches of urban wasteland from a few hundred square metres to several hectares.
Historically, the plant associated most exclusively with the capital was the London rocket (Sisymbrium irio).
This alien from the Mediterranean was noted in large numbers in 1666 when it spread over the ground laid waste by the great fire of London.
A fellow alien of the same genus,S. loeselii , is also a London plant, but neither species is at all widespread today.
London grew with the railways in Victorian times.
The sprawl has continued along and between them though its outer edges are now held in check by ‘green belt’ policies.
The main growth of wasteland, as opposed to building, came with the bombings in the Second World War.
After the war, the succession of plants and animals that colonised these newly wasted sites was studied in detail by the London Natural History society (LNHS).
The most notable takeover was by the rosebay willow-herb (Epilobium angustifolium)which flourishes throughout London as it does in other cities.
Apart from the bomb sites, now largely built on, London's wasteland habitats arise in the usual ways: derelict land, demolition sites, old  canal and dock sides, land around utilities and along railway lines, cemeteries and so on.
In common with other urban wasteland these sites may sport such species as buddleia (Buddleia davidii), Oxford ragwort (Senecio squalidus)and Canadian fleabane (Conyza canadensis).
sycamore (Acer pseudoplatanus)is the least welcome of trees on these sites, while the London plane (Platanus x hispanica)is the most familiar in the streets.
Both are well adapted to London's dry climate and dirty air.
Garden escapes are also a distinctive feature providing drifts of iris (Iris spp)and Lupin (Lupinus spp)alongside railway tracks, rhododendrons (Rhododendron spp)in woodland and numerous other species such as bladder senna (Colutea arborescens).
And plants exploit the Thames in the few areas where tidal scouring is not too great: the LNHS has recorded wild and garden angelica (Angelica sylvestris and A. archangetica)and hemlock water dropwort (Oenanthe crocata)at Battersea.
The LNHS is the long-established basis for studies of natural history in London, and the society's approach has been complemented by local groups with environmental interests.
The foundation of the London Wildlife Trust a couple of years ago provided a focal point for all bodies involved in urban conservation, and the recent creation of the post of Chief Ecologist to the Greater London Council (New Scientist , vol 95, p 520) gives a point of contact within the city's governing body.
Other bodies encouraging a new look at urban wildlife in the city include the Ecological Parks Trust.
The opportunities for improving the treatment of London's wild and waste areas have never been greater.
Julia Grollman — The London Wildlife Trust is at: 1 Thorpe Close, London W10.
MANCHESTER
THE URBAN COMMONS of Manchester are dominated by Japanese knotweed (Polygonum cuspidatum), forming large clumps and thickets on nearly every site.
This tall plant, growing up to 2 metres, is often accompanied by the even taller giant knotweed (P sachalinense).
They are among the last plants to bloom, often not being fully in flower until early September.
These knotweeds readily invade waste ground and are very persistent.
As the natural succession proceeds the stands stop spreading, but continue to provide a highly distinctive urban niche with interesting animals as well as plants.
Manchester is also distinguished by the abundance of wetland species.
Reedgrass (Phalaris arundinacea)is everywhere, with some particularly fine stands by the university; at one site it grows alongside amphibious bistort (Polygonum amphibium ).
In Bellevue four species of rush grow on wasteland together with two semi-aquatic grasses and creeping yellow cress (Rorippa islandica).
The region's relatively high rainfall may encourage these wetland species, but their presence may also reflect the wider tolerance plants show when freed from having to compete.
A recent study of small urban mammals showed that vacant land carries woodmice (Apodemus sylvaticus)deep into Manchester.
But it is not a favourable habitat for either the bank vole (Clethrionomys glareolus), which does live in gardens, or the familiar house mouse (Mus musculus).
BRISTOL
BRISTOL has a most distinctive urban vegetation.
Buddleia (Buddleia davidii), a native of China introduced into British gardens as recently as 1890, dominates the city centre.
Every where it forms thickets: on derelict houses, along roadsides, on wasteground.
A slogan on one wall reads ‘Buddleia rules OK!’
Often buddleia is associated with another calcium-loving plant, traveller's joy (Clematis vitalba).
At a site by the Royal Hotel that has remained undeveloped for 40 years, sycamore (Acer pseudo platanus)is beginning to appear above a dense patch of buddleia; the sycamore is probably the natural successor to buddleia in this city.
The other plant that makes a visit to Bristol unforgettable is the wild fig (Ficus carica).
some 30 to 40 large fig trees have become naturalised on the banks of the River Avon at the heart of the city.
Bristol's equable climate probably favours the survival of this warmth-loving species; though present in certain northern cities, such as Sheffield where five are known, the wild fig elsewhere remains small and vulnerable.
The origin of the fig trees is probably similar to that of wild tomato plants (Lycopersicum esculentum), that is, from seeds distributed in sewage!
The most remarkable feature of Bristol's urban animal life is its population of foxes which, with nearly two family groups per square kilometre is the densest so far recorded anywhere in the world.
The reasons for this high density are not yet known.
The fox's role in shaping the ecology of the city's wasteland is limited by stray dogs, which appear to be the animal's main enemy in its urban surroundings.
NEWCASTLE
URBAN WOODLAND flourishes on slopes above the River Tyne around Newcastle's famous high-level bridge.
Rooted in the foundations of old buildings, the vegetation contains large amounts of crab apple (Malus sylvestris)and elder (Sambucus nigra), with frequent hawthorn (Crataegus monogyna)and blackberry (Rubus fruticosus)and lesser amounts of guelder rose (Viburnum opulus)and sycamore (Acer pseudoplatanus).
The apple trees probably originated from the pips of discarded dessert apples.
So far, ivy (Hedera helix is the only species typical of the woodland; ground flora gaps in the tree cover are occupied by false oatgrass (Arrhenatherum elatius), hawkweed (Hieracium umbellatum)and thistles (Cirsium arvense).
Newcastle's younger urban sites contain a high proportion of species with connotations of the countryside, such as bracken (Pteridium aquilinum), hardhead (Centaurea nigra ) and hawkbit (Leontodon cutumnatis), growing along with the calcium-loving wild parsnip (Pastinaca sativa)and nodding thistle (Carduus nutans).
Sow thistles (Sonchus spp)are also unusually abundant.
The insects on Newcastle's wasteland probably deserve to be better known.
In August 1981 every piece of the city's urban common carried a population of wall butterflies (Lasiommosta megera), the first time this species had been seen north of the Tyne this century.
The common field grasshopper (Chorthippus brunneus)was equally prevalent at that time.
SHEFFIELD
WASTELAND in Sheffield contains particularly large numbers of colourful garden escapes.
These provide a succession of blooms from mid-June — with feverfew (Chrysanthemum parthenium), goat's rue (Galega officinalis)and lupin (Lupinus nootkatensis )— through the summer with tansy (Tan- acetum vulgare)and soapwort (Saponaria officinatis), until mid-October when the Michaelmas daisies (Aster novi-belgii)finally die down after their two-month flowering period.
How the heavy seeds of goat's rue become so widely dispersed is a mystery; nor is there a satisfactory explanation for the very large numbers of Michaelmas daisies.
At several sites surprising species such as  centuary (Centaurium erythraea)and red bartsia (Odontites verna), more associated with the countryside, play a part in the early stages of succession.
In the later stages, goat willow (Salix caprea)is the commonest woody species to invade wasteland, being accompanied occasionally by the Duke of Argyll's Tea-plant (Lycium halimifolium ).
The wasteground of Sheffield also supports in some places a number of species believed to be native only in dry places in the south of Britain, for example, yellow vetchling (Lathyrus aphaca), sickle medick (Medicagofalcata)and the annual grass Vulpia ambigua .
They may grow in Sheffield because the urban climate is slightly warmer than the surrounding countryside.
Thus thermophilous (warmth-loving) species may reach their northern limit in urban areas.
Succession: how plants colonise new territory
THE PLANTS growing on urban waste ground not only reflect local conditions, but also the length of time the land has lain waste.
As plants move in to colonise newly available ground different species take over from each other in a series of stages known as ‘succession’.
At first annuals and short-lived perennials move in (above left); fat-hen (Chenopodium album)is typical of these (left).
Then three to six years later, tall herbs take over (centre).
These may include garden escapes, such as lupins (Lupinus nootkatensis).
Grasses then begin to colonise the ground and after 10 years, and the appearance gradually turns to one of open scrub (above right).
Woodland, the final stage in succession, appears only after 40 years or so (right).
REVIEW
Definitive epic for castaway physicists
The historical development of quantum theory by Jagdish Mehra and Helmut Rechenburg,Springer, vols 1–4 
John Gibbins
QUANTUM THEORY is the great feature of 20th century science.
Relativity may have had a better press, but it is quantum theory that we have to thank, at a practical level, for our understanding of chemistry, molecular biology and solid-state physics, as well as nuclear physics and the deeper puzzles of particle theory.
Without quantum theory there would be no genetic engineering, no solid-state computers, no nuclear power stations (or bombs); even Albert Einstein's Nobel prize was for his pioneering contributions to quantum theory, not for his ideas on relativity.
The success of Alain Aspect's team in confirming experimentally one of the more subtle predictions of the theory (New Scientist , 6 January, p 17) came just at the time when, in an echo of the great days of J. G. Crowther,The Guardian published Terry Clark's report of an experiment in which a macroscopic object can be made to behave, in some respects, like a single quantum ‘particle’, and when these weighty tomes arrived for review.
The authors of The Historical Development of Quantum Theory seem to have succeeded 100 per cent in their self-imposed task.
The four volumes of this work are to be accompanied soon by a further five, the whole series presented as making up the definitive study of of quantum theory in an  historical context.
If Jagdish Mehra and Helmut Rechenburg succeed in maintaining the standard set so far, the result will be one of the most significant scientific works ever published, a series well eligible for inclusion in any time capsule of the 20th century.
This epic work is very much Mehra's creation, the result of an obsession dating back to 1952, when he decided to make the study of the history of quantum theory his life's work.
It is a first-hand account by someone who has met and talked with many of the creators of modern quantum theory, and who remarks that by the 1970s ‘my collection of notes and transcripts of tape recordings of conversations, discussions and interviews had become quite large’.
Prodded by such luminaries as Eugene Wigner and Werner Heisenberg and with the aid of a grant from the Fonds National Suisse de la Recherche Scientifique, he eventually set about writing the book in Geneva.
Helmut Rechenburg, his collaborator on this mammoth project, was Heisenberg's last doctoral student, and clearly well suited to the task in hand Faced with so much material, it must be hard to know what to leave out, and the authors have chosen the best solution — leave nothing out.
They start at the beginning and tell the whole story, pulling no mathematical punches but providing so much historical and biographical material, as well as physical explanations, that a complete mathematical ignoramus could gain much from the work by reading the words and skipping the equations.
People who were just names on laws (Max Planck, Louis de Broglie, Werner Heisenberg, Paul Dirac…) come alive in these pages as the reader discovers how each learned his trade and how the different characters interacted with one another.
To someone like myself with a traditional physics-orientated education the first volume is a revelation.
Too much physics teaching today is still ‘cook book’ stuff, with students told how to manipulate equations to solve puzzles, but given no insight into the deeper implications of those equations or of the significance of their development.
This volume alone redresses the balance and should be compulsory reading for all physicists.
Inevitably there are some ups and downs in such a lengthy work, and volumes two and three represent a slight decline from this lofty pinnacle of excellence.
There is some repetition as the authors backtrack to report the same developments from different points of view and new characters are introduced; the detailed mathematical discussions do become daunting for the non-specialist reader.
But volume four takes us right back to the peak with a marvellous account of Dirac and his contributions to quantum theory — hardly surprisingly, in view of Mehra's previous involvement with Dirac and celebrations of his work.
This volume, too, could stand alone in pride of place in any physics library.
Can the team maintain these standards?
Twenty-five years covered in one volume, with four other volumes following to tell the story of 1926 alone, shows that the key problem is that of seeing the whole wood among the wealth of trees.
If I were looking for a fault with the conception of the series, it could only be that it lacks a single volume overview of the whole of the development of quantum theory, a volume concentrating on the broad  sweep of ideas and leaving out the mathematical detail.
Such a companion volume, though different in flavour from the rest, could provide the necessary framework on which the detailed volumes could be hung in context, and to which the reader could refer to maintain an overall perspective of the series.
But why carp.
Even without this icing on the cake, I suspect that the set would be the ideal ‘book’ for a physicist to be cast away with on a desert island, provided paper and pencils were also supplied.
It certainly relit an old flame of interest within me, and sent me off in two different directions at once.
First, I looked again at an old favourite, Arthur Eddington's The Nature of the Physical World (Cambridge The detailed insights provided by Mehra and Rechenburg put this in a fresh perspective (and Eddington still stands up, half a century later, as a superb writer who knew how to present his material).
Then, I looked again at a superficially rather strange SF trilogy that appeared a few months ago (Schrodinger's Cat , by Robert Anton Wilson, sphere, 1982).
This tale of parallel realities claims to be based on quantum theory, and is dressed up with suitable jargon.
A nice gimmick; but read this trilogy immediately after an in depth study of quantum theory and you find that it represents possibly the most scientific of all science fiction novels.
It must have taken Wilson years of research.
I don't suppose Mehra can possibly have had time to knock out some SF on the side, but I am deeply curious to know which great scientist lurks behind the mask of ‘Robert Anton Wilson’.
Are cave communities islands in evolution?
Cave life by David Culver,Harvard UP.
pp 189, £17.50 
Mark Ridley
THE animals that live in caves, David Culver believes, should be of particular interest to biologists.
‘Because of their simplicity, cave communities in many cases are close to the assumptions of various ecological and evolutionary models.
The environments are relatively constant, selective pressures are more or less identifiable, the amount of isolation is often known, and the entire set of interacting species is known.
For these reasons, models can be tested more completely,’ he writes.
We might reply that the simplicity of cave communities is idiosyncratic rather than typical: we might ask what general ecological insights we are going to obtain from communities that lack photosynthetic plants and herbivores, and are maintained by detritus flushed down from above: but Dr Culver gives us no answer.
Arguments of this kind are easy to invent, but difficult to settle: in the end, the proof of the pudding will be in the eating.
So let us turn to the main part of Culver's book, and see whether his hopes are substantiated.
Has, in fact, the study of cave life settled any important questions in evolution or ecology?
The answer, as it appears in the conclusions of one chapter after another, is always the same.
It is always no.
So we might modify our question, and ask whether the study of cave life has told us anything at all about the great questions of evolution and ecology.
Again the answer is no.
The most we learn, from caves, is that the questions are difficult.
We cannot settle the theory of life histories because we cannot be sure whether cave organisms put more effort into reproduction than do their relatives at the surface; nor can we be sure whether they have reduced metabolic rates, which makes it difficult to sort out the question of adaptation.
When we turn to that most distinctive feature of cave organisms, regressive evolution, we cannot decide whether it is caused by selection or genetic drift; nor can we settle whether allozymic variation is due to selection or drift.
When we have finished (in the last main chapter) with the theory of island biogeography, we still end up concluding ‘that there are  insufficient data to either validate or falsify the hypothesis that caves or cave regions are virtual islands’.
The chapter on species interactions is the most effective.
Culver has estimated, in the laboratory, the competition coefficients among the three species of isopods and amphipods which inhabit a set of caves, and then used the coefficients to predict which species will co-exist in nature.
The fit between fact and theory is good, but again the conclusion strikes up the familiar refrain: ‘the results are only  marginally significant…
The very simplicity of the community…makes statistical testing difficult.’
All this is hardly Culver's fault.
The same difficulties are to be found in all other parts of evolutionary ecology.
But he did advertise his book saying that caves were an exception to the general difficulties, so he can hardly object if that is how his book is judged.
If he has only tinned fruits and custard in his larder, he should not serve them up as coupe de fruits avec creme anglaise .
Cave Life was written for professional biologists, and.
indeed, the reader will need some professional commitment to get through the monotonous rhythm of the prose, and the technical sophistication of the language.
But the effort will, for a biologist, be worthwhile.
It is an authoritative review of the research that has been done on a fascinating fauna.
If I do not believe that caves hold the key to the future in ecology and evolution, I am certainly grateful that David Culver and his fellow biospeleologists continue to descend into those dangerous caves and bring up to the light so many biological discoveries.
The ugly duckling comes of age
The handbook of artificial intelligence, volumes 2 and 3 Edited by A. Barr & E. A. Feigenbaum (vol 2); P. Cohen & E. A. Feigenbaum (vol 3),Pitmanpp 427 and 639. £22.50 and £28.50 
Kenneth Owen
TOO MUCH, too soon is expected of the branch of computer science known as artificial intelligence (AI).
If the subject were undersold (to put it mildly) by Sir James Lighthill in his report to the Science Research Council in 1973, it is now truly oversold by over-enthusiastic experts and bandwagonning commentators.
For those in search of the true state of this extraordinary art, there is no better guide than the massive, three-volume Handbook , written by the cream of the US artificial intelligentsia and skilfully edited at Ed Feigenbaum's AI citadel at Stanford University.
The editors are aware of the dangers accompanying the recent transformation of AI from ugly duckling to golden-egg-laying goose: ‘The crisis we now face is a crisis of success, and many wonder if the substance of the field can support the high hopes.’
The substance of the field makes impressive reading Volume 2 covers AI programming languages; applications research in science, medicine and education; and automatic programming.
Volume 3 deals with models of cognition; automatic deduction; vision; learning and inductive inference; and planning and problem solving.
Applications research (volume 2) embraces the fascinating world of ‘expert systems’, in which human expertise in specific areas can be captured and manipulated in computing systems so as to provide problem-solving advice of high quality and intelligibility.
Applying AI problem-solving techniques in this way is in itself a notable advance but, as the editors indicate, the research has also addressed fundamental questions on the nature of knowledge, both in terms of formal representational systems and as an essentially social phenomenon — knowledge as something that must be shared and transferred among men and machines.
And ‘knowledge’ is not restricted to facts and logical rules; reasoning with the hunches and educated guesses of human expertise has been the key idea behind expert systems.
The AI people call it knowledge  engineering .
But, beyond hunches and educated guesses, what about other human characteristics such as beliefs, prejudices and emotions?
We know they exist, but how do they affect the reasoning process?
This question is addressed in an iceberg-tip section on ‘belief systems’ in the chapter on models of cognition (volume 3).
Here the editors venture carefully into the world of cognitive science, which lies at the intersection of AI and cognitive psychology.
Computing systems have been designed to explore the structure and influence of human beliefs, prejudices and emotions.
As an example, a ‘paranoid’ program was written (by a psychiatrist) to simulate human paranoid behaviour in a consultation dialogue with a doctor.
(’ How much do you get paid an hour?’the doctor asked.
‘My salary is none of your business,’ the program replied…
) More generally, the challenge of belief systems is immense.
We know that humans do not reason entirely from facts, using consistent rules of inference.
somehow we integrate prejudices, bias, selective memory, degrees of confidence and emotional states into our ‘rational’ reasoning.
But how?
AI might help us find out.
Combustion of a retarded technology
Biomass gasification in developing countries by Gerald Foley and Geoffrey Barnard,Earthscan. pp 174. £10 
Phil O'Keefe
EARTHSCAN, which has developed an unparalleled reputation for providing briefing documents on environmental issues, has begun an energy information programme.
This volume on biomass gasification is the first publication of the programme.
The impact on Third World policy makers is likely to be large if Earthscan's outreach is utilised, so it is important to review this new departure critically.
The technique of biomass gasification by partial combustion has a long history.
Over a million gasifier units, usually powering road vehicles, were used during the Second World War.
The extensive use of gasification reflected the unavailability of oil supplies but there was a considerable cost attached through inconvenience and vehicle wear.
Gasification is a ‘retarded’ technology, a technology that may benefit the Third World in in era where the perceived cost of oil is reckoned to be high.
Retarded technologies are those energy technologies that were abandoned, technologies on which little R & D continued because of cheaper alternatives.
The attraction of gasifiers to the Third World is that, in theory, any biomass material can be used as feedstock.
Gerald Foley and Geoffrey Barnard argue that the only proven fuels are wood and charcoal for the current gasifier technology.
Given the potential biomass residues, in the Third World, it must not be long before gasifiers are produced which are not constrained by the current composition of fuel.
The brief descriptions of gasifier types, including the modern, fluidised bed gasifier, and the shaft power and direct heat systems are clear, although there is little comparative assessment of the different gasifier designs.
The section on the chemistry of gasication is not so definitive partly because the temperature gradient, which varies according to fuel and gasifier design, affects both the position of chemical equilibria and the relative rates of different reactions within the gasifier but also because few experimental results are available for interpretation.
The critical performance factors, namely design, fuel type and chemical interaction, dictate the ability of the gasifier to respond to load changes.
Clearly, there is need for more detailed work on thermodynamic and chemical processes in small gasifiers.
The weakest section of the report is that on economics.
Although the authors accept that the niche for these systems is essentially ‘up market’ and that the support structure needed for a successful, gasification programme precludes gasification from the ‘appropriate technology’approach, their form of economic analysis is closer to a sales pitch than an economic evaluation.
Alternative energy technologies will be successfully diffused only if realistic assumptions are made about the real economic situation in the Third World.
Conventional benefit cost analysis, using a Third World example, would enhance presentation.
It is striking, however, that it is the richer developing countries, with a strong biomass base, that are utilising the technology and producing systems that are cheaper than those manufactured in the developed countries.
The report presents a strong case for continuing work on gasification although south south cooperation would seem to hold most promise.
Earthscan should be congratulated for its initiative, especially as it has managed to produce readable technical material.
If other publications from the energy programme summarise the ‘state of the art’ as well as this gasification report, then there will be less need for the spurious and  repetitious consultant reports that daily besiege the desk of the Third World policy maker.
Grovel on your knees for good pictures
Practical wildlife photography by Ken Preston-Mafham,Focal Press, pp 168, £11.95
Christine Sutton
IT IS not long before anyone interested in wildlife photography discovers that behind every good picture lies a fairly heavy investment in film (to allow for the shots that did not work) and a nigh on infinite amount of both time and patience.
Ken Preston-Mafham has over 35 000 colour slides and more than 10 years of experience behind him, so he is well qualified to offer sound advice on the trials and tribulations of photography ‘in the field’.
But he does not simply pontificate from his position as an excellent photographer, or regurgitate standard procedures.
Instead he makes you privy to the way he took certain shots, coping with available light, wind, wet knees and so on.
Indeed, this is the first time I have found a photographer reveal the position he was in when he took a particularly impressive shot of slime mould, for example.
I also like the way Preston-Mafham dismisses most complex (equals expensive) equipment.
He has already stopped me from saving up for a large format SLR camera; my trusty 35-mm model will do well enough for the closeups of butterflies and flowers that interest me.
Instead, I will simply buy more film and, encouraged as any amateur should be by this well-presented book, I will get back to grovelling on my knees in the Oxfordshire hedgerows.
Rich variety from remarkable passerines
The Cotingas by David Snow,Oxford UP/BM (NH), pp 203, £30 
Sephen Mills
THERE has always been some doubt as to what exactly a cotinga is.
This family of south and Central American passerines has been swelled and diminished by a century of taxonomic debate.
David Snow settles for 65 species including one, the grey-winged cotinga, which he discovered himself in 1980.
They vary from warbler to crow-size, many have remarkable plumages and astonishing voices, and all are fruit-eating forest birds.
Compared with European or North American species, the cotingas have been studied little and, consequently, almost every recent observation is of interest.
In his systematic accounts of each species, Snow is careful to stress the main areas where knowledge is lacking.
This sense of being on fresh ground where new aspects of behaviour, new distributions and even new species are still to be discovered, adds greatly to the book's fascination as a comprehensive account of the family.
The accounts, accompanied by scrupulously clear distribution maps, provide eyewitness descriptions of the ecology and behaviour of cotingas, based chiefly on 20 years of fieldwork conducted by the author and his wife.
Reliable nesting records are scarce or absent, partly because most cotingas build small, inconspicuous nests, which some will even dismantle if disturbed.
The downy chicks are often perfectly camouflaged to merge with surrounding lichen.
Nevertheless, Snow pieces together sufficient evidence to reveal some intriguing breeding strategies.
A pair of swallow-tailed cotingas, for example, may build two nests, laying eggs in both.
While the female attends one, her mate takes charge of the other.
Purple throated fruit-crows, on the other hand, operate a ménage system; a dominant pair being aided by several non-breeders in feeding and defending the chick.
Cotingas, including the dazzling orange and black cocks-of-the-rock, employ some startling nuptial and aggression displays.
Perhaps the most amusing is the three-wattled bellbird's method of repelling competitors.
Bellbirds have possibly the loudest voices of any bird.
The male sits on a favoured perch and when this is visited by a rival he sidles up to the intruder.
Having edged him to the end of the branch, he utters a shattering ‘bock’ in his ear and the rival falls off.
One of the many compliments that can be paid this finely produced book is that it is worth reading from cover to cover.
Martin Woodcock's paintings of all known cotingas are generously proportioned and  The Cotingas is an inspiration to any adventurous ornithologist looking for areas of study where he might still contribute to science by straightforward observation.
The inside and insides of boffins
IF YOU want to watch television around midnight on the Sabbath, and if you happen to be in a region where nothing less important has taken precedence, you can follow one of the most excellent science series of recent times.
Produced by TVS, Men of Science is now being shown in a few other areas for those not already suffering exophthalmos after an evening of Rising Damp, Cuffy, The Avengers and Pro-Celebrity Golf 
Last week's starter with Heinz Wolff (ITV, 13 March) was all the more impressive for getting off to such a terrible start with the tortuous answer that interviewer Ian Fells elicited to his opener about the meaning of biomedical engineering.
‘Well it's really very difficult,’ Wolff tried to explain, in what was once called broken English, ‘but in the proverbial nutshell it's the application of engineering thought and engineering practice, which really means technology, to medical and biological problems, both on the research side and in terms of producing products which help in the treatment of patients.’
And this was followed up by a point-blank refusal to deal with Fells's next query.
Not a promising start, one felt, for an (ill titled) series designed to overcome the idea that boffins are tedious, wordy and evasive.
In the event, this proved to be a smashing half hour — about an 11-year-old lad who left Germany in 1939 and who, through career accidents as well as conscious choices, found himself wearing his present hat as a champion of ‘tools for living’.
Man was only as good as the tools he had available to impress his will on the environment, Heinz suggested.
Handicapped people were simply those who had not been given the right tool kit.
To underline the point, he opined that interrogator Fells would be a very different chap if deprived of his car, telephone and bow tie.
Similarly, disabled persons needed to be enabled by appropriate technology.
This was a solidly practical view of science, with Wolff setting himself clearly apart from researchers who interpret their role as increasing the store house of knowledge.
At one point he even cheerfully admitted that he might not be a real scientist at all.
Instead he demonstrated some gadgets — grandma's tilting teapot, self-adjusting spanner, mini radio transmitter to summon help via telephone and data bank — and indicated a wealth of similar innovation.
A man who thrives on steering ideas  between fields, Heinz Wolff also sees the opportunities for a major new industry to be built around the growing need for devices to enhance the quality of life.
A subject who not only finds science exciting but also believes it can be applied to help people and produce wealth and employment could hardly be bettered in answer to the white coated caricature with which Ian Fells began.
But Wolff is no utopian technocrat.
He also aired his anxieties — particularly about the distinction between pharmaceutical developments that make treatment easier and cheaper, and technology that mobilises more resources for fewer people.
Heinz Wolff is an enthusiast, of course.
The record of his own work and ideas meant that the only dull part of this programme was a tired, conventional exchange about the balance between pure and applied research.
But buoyancy aside, it was also sobering to hear a practising optimist calling for a ‘mathematics of misery’— an agreement, by consensus, about the costs and benefits of different medical treatments, and where the unavoidable choices should be made.
One possible future for biomedical engineering not essayed by Heinz (but which is being taken seriously by electronics wizard Clive Sinclair) is the sending of minuscule probes into the bloodstream to repair damage at distant sites.
The idea came vividly to mind during Fantastic voyage (15 March), by far the best so far of BBC2's SF Film Festival series.
Here not just micro-tools but miniaturised medicos in a submarine are injected into a moribund boffin's carotid artery.
Can they reach his brain in time, obliterate the clot, and learn the secrets the man holds?
Raquel Welch has performed better than this.
And the ultra structural detail compares unfavourably with that in George Gamow and Martynas Ycas's excellent Mr Tompkins Inside Himself But it's a stirring tale, with some stunning effects.
Wake up, Dr Wolff.
FORUM
Secrets of Davy Jones's locker
Tam Dyell is concerned about nuclear weapons in troubled water
YOU COULD be forgiven for not having heard of the Treaty of TIatelolco.
But you are going to hear a great deal more about it in the coming months.
The non-aligned nations, meeting in Delhi, have just dragged it on the stage by backing an Argentine call for the withdrawal of all nuclear weapons from the Falklands.
The Treaty of Tlatelolco was signed on 20 December, 1967, and ratified on behalf of Britain by the Wilson government on 11 December, 1969.
Under Article 1 of Protocol 1 of the treaty, the United Kingdom undertook to apply the statute of denuclearisation in respect of warlike purposes as defined in Articles 1, 3, 5 and 13 of the treaty for the Prohibition of Nuclear Weapons in Latin America in territories for which dejure and/or defacto , the UK is internationally responsible, and which lie ‘within the limits of the geographical zone established in that treaty’.
The Falkland Islands do.
The arcane language of the Treaty of Tlatelolco needs a little clarification.
Article 1, Protocol 1, requires the total absence of nuclear weapons, by prohibiting,inter alia , the receipt, storage, installation, deployment and any form of possession of any nuclear weapons, directly or indirectly, by the parties themselves, by anyone on their behalf or in any other way.
My reading of Article 5 of the treaty is that nuclear weapons are defined broadly enough to cover missile warheads, bombs, depth charges, torpedoes or mines, in whatever form, whether armed or not.
This requirement to keep nuclear weapons out of the southern part of the western hemisphere is reinforced by Protocol 11 of the treaty.
The UK signed and ratified this, too.
In Article 1 of Protocol 11, the signatories agree to respect fully the statute of denuclearisation of Latin America…‘in all its express aims and provisions’.
The express aim of the treaty, as stated in its preamble, is to keep the whole region, including the Falkland Islands and the adjoining seas ‘forever free from nuclear weapons’.
Ironically, of the states that border the South Atlantic, only Argentina has not yet ratified the Treaty.
But this does not diminish the British obligation to our co-signatories, and it is such states as Brazil and Venezuela which are pressing Britain to renegotiate over the Falklands, before more bloodshed occurs.
I am told by a friend who was a delegate to the Delhi conference that the concern over Britain's importing nuclear weapons into an area hitherto free of them was real and not fictitious.
I understand from people returning from the Falklands Garrison that Britain certainly has various nuclear weapons there.
Can we be sure they will never be used?
Professor von Hayek, a luminary much admired by the Prime Minister, writes to The Times suggesting that if attacks are made against the forces in the Falklands, Britain should retaliate on the Argentine mainland.
Furthermore, can we be sure of the actions of local commanders?
On 7 April, 1982, the Att Ln David Tinker, RN, wrote to his wife, ‘I doubt if the Argentinians will want to risk sending their ships out.
If they are sunk, they will have nothing to stop us bombarding Buenos Aires…. even said ‘Drop a big white job (Polaris) on them’.
Thank goodness he is not in command.’
(A Message from the Falklands .
p 158, Junction Books.)
Such suggestions are spine-chilling.
Units of the Fleet, sailing south from Gibraltar on Monday, 29 March, 1982, were indeed carrying nuclear weapons.
Keith Speed, MP, the former Admiralty Under Secretary, said on television (BBC Newsnight)that he would have been astonished if those ships, from Exercise spring Train, had not been carrying nuclear weapons.
Some of the ships which left Portsmouth on Monday, 5 April, 1982, were carrying nuclear weapons.
There was a tremendous row about this inside government and Whitehall.
As a result,some of the nuclear weapons were lifted back by helicopter and other boats, before  the Task Force reached the Western Approaches.
The rest stayed on board, Stenor Inspector and Stenor Sea-Search, oil-rig recovery vessels, were despatched to the South Atlantic from Aberdeen and Charleston, South Carolina, to retrieve nuclear devices from the tombs of HMS Sheffield and HMS Coventry.
Whether they were successful we do not know.
Even more elusive have been facts about retrieving nuclear depth charges from lost helicopters.
On 23 April, I982, we lost a Sea King mark 4 helicopter at 29o 47'S, 26o 53'W.
On the 12 May, a Sea King mark 5 went down at 50o 53'S, 54o 40'W.
Seven days later a Sea King mark 4 went down at 49o 43'S, 5 1o 43'W.
All carried nuclear depth charges.
Heaven knows what pollution of the ocean is occurring in the form of emission of  radionuclides , and building up in the various food chains in which plankton play a part.
Worse could have happened.
For instance, after the nuclear-powered SSN Conqueror had sunk the Belgrano, she was depth charged for two hours by the escorts, Pidra Buena, and Hippolito Bouchard.
Had she been damaged or sunk, another mass of nuclear material could have been released.
If negotiation about sovereignty does not take place, there will inevitably be a ‘Falklands re-play’.
It is urgent that Britain makes a candid statement on its current attitude to the Treaty of Tlatelolco, before any such tragedy.
Hold the front page
ALTHOUGH Sir Derek Rayner has completed his assignment as the prime minister's hit man on Civil Service waste, his spectre continues to haunt government research establishments.
Following Rayner's discovery of such scandals as the laboratory with a 20-year supply of embossing tape, administrators all over the country are rooting out uneconomic practices.
One such exercise is going on at the Building Research Establishment.
The Rayner men decided that the establishment's information papers, on such subjects as ‘performance of cavity wall ties’ should earn their keep.
The staff are now feverishly working on ways to persuade the building trade and public to pay for the publications.
But we are assured it will be a long time before we see bingo or boobs in the sober pages of BRE News .
Knocking science into them
Roy Herbert recalls some early scientific influences
THE VICTORIAN red-brick building which housed the grammar school I went to still stands.
In 1932, the school was promised a new building and never got it.
Post-war lateral thinking solved the problem by abolishing the school, but the building survived that, too.
It has now opened again as a private one.
It had, and for all I know still has, laboratories on the ground floor, with bronzed taps and bunsen burners.
For quite some time I thought that bunsen was something that the burners burnt.
In these labs I made my first acquaintance with physics, scattering iron filings on a paper placed over a magnet to produce a pattern which I then had to reproduce in an exercise book.
I never got an answer to my question, a reasonable enough one I maintained, what was magnetism?
The question placed me in danger of a box over the ear from the broken-nosed physics master who, despite his pugnacious appearance, failed to keep order in any class.
This failure took him spectacularly on to be director of education in another county.
He was the junior physics master.
The senior one was a fearsome, mustachioed amateur cello player who addressed every class in a terrifying bawl.
The physics classroom had a spiral iron staircase leading from it to some region inhabited only by science masters.
Sometimes the cello player was late for a class below.
He would silence noise by poking his head down the first step and yelling threats of appalling punishment.
The appearance of this fiery face, upside down, struck instant dumbness in 30 boys, though on one occasion a lad, startled into a panic stricken reflex, swept an inkpot off the desk.
It sailed out of the horizontally opening window and fell on the bowler hat of a ratepayer on the street underneath.
I know he was a ratepayer because he repeated this fact in the subsequent inquiry, a dozen or so times, adding that he didn't pay rates to have his hat ruined, a view I could understand.
The physics master, long after John Cockcroft and Ernest Walton had successfully split the atom, was still dictating to his pupils, ‘Matter can neither be created nor destroyed’.
The only solace for all this was the weekly reading of Modern Boy , a magazine which I have never seen mentioned in anthologies, books about boys' magazines or reminiscences.
Modern Boy included real science, with pictures showing how powerful ants were by imagining them as big as horses, in which case they could move around carrying skyscrapers.
It illustrated up-to-date transport, such as rotor powered ships, the rotors being driven by the wind, and buses that could run on railway lines.
I cannot remember whether it soared off into space from time to time, though I guess it must have.
To tell the truth I have only hazy memories of the magazine that I took for a long time and until it ceased publication for reasons that were beyond me.
But after all these years I still remember the Krooms.
Modern Boy , you see, also printed fiction and the Krooms stalk through my head to this day.
They were scaly creatures, walking on two feet and they were out to conquer the Earth and every living creature on it.
‘On it’ is the important phrase, for the Krooms lived in it.
What on earth they had been doing all this time before being consumed with their mad dreams of conquest I do not know.
But there they were, in their thousands, or it may have been millions, and they were a horrific threat to the survival of humanity on this planet.
Come to think of it, the Krooms may have come from outer space, though outer space was not then as popular as it later became for launching expeditions against Earthmen.
Be that as it may, the most desperate measures were necessary to provide even the most slender chance of defeating the malevolent hordes.
Fortunately there was a scientist about who had invented and brought to a high pitch of development, a time machine.
Fortunately, too, he had at least one dauntless and resourceful boy of school age to assist.
They must have been especially successful in exploiting whatever was available from them, because I don't remember anything of it.
The best idea they had was to travel back to the Wars of the Roses and bring two armoured knights with them into the age of the Krooms.
After expressing astonishment at this turn up for the book, gazing round and saying things such as‘By my halidom, sir knight,’ and ‘Gadzooks, fell work shall be done this sennight,’they took grimly to hewing and slaying Krooms left, right and centre by the baker's dozen.
As far as I recall, the Krooms were not the best  tacticians and kept trying to throng out of holes in the ground only to have their bodies dismembered and decapitated by the two knights.
In breathing spells, up to their chainmailed hocks in Kroom blood and bits, the Knights would lean on their axes or swords, breathing hard, and express appreciation at being able to take part in such a splendid struggle.
Perhaps they called for stoups of ale or goblets of Rhenish, or supped on plain fare, my masters.
I don't know.
What I do know is that we owe a great debt to these doughty fighters of the past in banishing the menace of the Krooms for ever from this  green and pleasant land.
Everybody said as much when they went cheerfully back to the battle of Tewkesbury or Bosworth Field.
I am not sure, either, whether it was in the struggle against the Krooms that the Griffin gun made its mark, or rather, three marks.
This was the famous gun that could put a shell on to a target and then another two on the same spot precisely.
Hardly any thing could resist this accuracy at which I was much impressed, though a bit alarmed at Griffin himself, who sat around on a stool amid the most dreadful carnage and what was later known as flak in order to observe the behaviour and results of his phenomenal piece of artillery.
Coolness was his stock in trade as he was a scientist on our side.
Scientists on the other side were maniacs, to a man.
It seemed to me at the time that the teachers of science at school, who had certainly shown themselves to be opposed to me were, if not actually off their trolleys, a trifle on the demented side and undoubtedly strangers to coolness.
The wounds they inflicted left me with a permanent scepticism about science and the progress of research.
That, I think, is a healthy enough attitude.
On the other hand, my opinion is that, efficacious though it proved to be, there are better methods of inducing scepticism than applying a hard rubber, wielded in the sleeve of a gown, at high speed to the side of the head.
Boisterous gathering
THE AWARD for meritorious service under adverse conditions at a scientific conference goes to Charles Weissman of the University of Zurich who gave a keynote address at the recent recombinant DNA congress in Philadelphia.
Without warning, Weissman and his audience found themselves separated by no more than a cardboard partition from a noisy meeting of a local school maintenance workers' union, voting not to end its strike.
Weissman carried on courageously with a complex but fascinating lecture on RNA splicing, ignoring the loud hubbub and intermittent squawks from his own microphone, and pausing often to wait out an eruption of roars and whistles from next door.
To add to the insult the phone took to ringing at odd intervals.
At one point, Weissman asked the projectionist to dim the lights and he flipped the slide off instead — and then doused the lights as well.
‘If I survive this talk,’ Weissman murmured into the void, ‘I can survive anything.’
Inexplicably the rowdy gang on the other side fell silent during the following address, about the sexual behaviour of the hermaphroditic giant sea snail,Aplysia .
Perhaps they were just listening.
Plastic explosion…
Martin Sherwood takes the wraps off 50 years of polythene
WORLDWIDE, the annual consumption of polythene is between 5 and 10lb per head.
It has reached that level in half a century, for 50 years ago today the experiment was begun which led to the first recorded observation of this polymer.
On 24 March, 1933. at ICI's laboratories at Winnington, Cheshire, Reginald Gibson and Eric Fawcett began a series of high pressure experiments with gas-liquid mixtures.
The work was part of a programme to study the effects of high pressures — in excess of 1000 atmospheres — on chemical reactions.
Although it was being carried out at Winnington, home of ICI's Alkali Division, it was being sponsored in part by the company's dyestuffs interests, in the hope that it might provide new routes to organic chemicals used as intermediates.
The programme had been under way for some time, studying liquid-liquid reactions with disappointing results.
The ICI team pinned its hopes for the programme's success on the untried gas-liquid reactions.
These were more difficult to undertake, because of the problems of pressurising gases with the primitive equipment available and maintaining the pressure.
In the first experiment in this series, Gibson and Fawcett used ethylene and benzaldehyde.
They chose the ethylene because they had a cylinder of it.
Out of six proposed gas-liquid reactions, ICI scheduled five with carbon monoxide.
When they returned to work on Saturday morning, they found that the pressure in the apparatus had stayed high but, by Monday, it had fallen, indicating that a leak had developed.
When the chemists dismantled the apparatus, Eric Fawcett noticed that part of the reaction tube, through which ethylene had been admitted to the vessel, was coated with a white waxy solid.
Fawcett says that he had noticed scraps of similar material in earlier test runs on the apparatus, but this was the first time that he drew it to Gibson's attention.
They repeated the experiment and obtained variable able results: often the ethylene decomposed to carbon and hydrogen.
Nevertheless, the two chemists obtained sufficient of the material to indicate that it was a polymer of ethylene.
Within four months, however, work had stopped on the reaction and the project moved on to other reactions.
Polythene was virtually forgotten for the next two and a half years.
This is not as surprising as it may seem.
As Eric Fawcett recalls, ‘It was a polymer so unlike the polymers known at the time that no one could envisage a use for it.
And we couldn't make it consistently.
The longer we went on, the more decompositions we got.’
The very idea of a polymer of ethylene went against conventional wisdom.
When Fawcett announced the discovery at a scientific meeting in Cambridge in 1935, it was ignored.
A number of chemists had experimented with the polymerisation of ethylene using catalysts.
ICI had worked on it in an attempt to make motor fuel from coal, a project with which Fawcett had been involved.
Under these conditions, however, ethylene forms short chains or rings, rather than the long chains of the solid polymer.
Many people believed that such low molecular weight products were all one could hope for from ethylene.
When Gibson and Fawcett's research programme continued with other reactions, it became clear that there was little to interest ICI's dyestuffs people.
They withdrew their support and Fawcett, whom they had seconded to the project, moved on to other research.
Michael Perrin, a young research worker who had just returned to Winnington from Amsterdam took his place.
He had earlier co-authored a report for ICI, proposing a fundamental physical chemistry study of the effects of high pressure on chemical reactions.
The lobbying from the dyestuffs interest gave the programme its empirical direction.
As that interest had waned, Perrin now wanted to go back to fundamentals.
Perrin subsequently took charge of the high-pressure research and Gibson was transferred to other work.
In December 1935, Perrin decided to look at ethylene.
‘I never went back and studied all the experiments that Gibson and Fawcett had done,’ he says.
‘I picked ethylene as a substance on physical chemical grounds.’
By this time, a better designed apparatus was available than the one Gibson and Fawcett had used.
On 19 December, 1935, Perrin produced several grams of polythene.
During 1936, he and colleagues worked out the details of how to make the reaction occur consistently, without decomposition, and explored the commercial  possibilities for the new material.
It had been realised by this time that, even if polythene did not fit into the then accepted mould for plastics, which tended to be glassy substances like polystyrene and Perspex, it might still be useful.
Its chemical structure, for example, indicated that it should be a good electrical insulator.
The real breakthrough for polythene came, however, from yet another accident.
A sample was passed to ICI's Dyestuffs Division for evaluation.
There a chemist who had recently worked for a company involved in the manufacture of submarine telephone cables recognised its similarity to gutta percha — the material used to sheathe such cables.
Tests showed that polythene was superior to the natural material.
Within a short time, his former employers ordered 100 tons of the new material, thus making it worthwhile for ICI to build a production plant.
The world's first polythene plant came on stream the day that Hitler invaded Poland.
Subsequently, polythene played an important part in radar during the Second World War.
Afterwards, when ethylene became available on a large scale from oil refining processes — previously it had been made from alcohol — the price of the polymer dropped.
It then entered the domestic market to begin the explosive growth towards today's production level of millions of tonnes a year.
Australia's new technological guru
Jane Ford cables an interview with a new science and technology minister
LATERAL THINKER, rapid-fire ideas man, ex quiz king, schoolteacher and lawyer — Australia's new Science and Technology Minister, Barry Jones, is something of a phenomenon in Australian politics.
Unlike most politicians he is passionately interested in technology and its social implications.
Jones's recent book —Sleepers Wake !
Technology and the Future of Work —crystallises many of his most radical ideas and has become compulsory reading, particularly in the corridors of his new department.
The book has already sold out a sixth edition.
In it Jones challenges the concept of full employment, questions the relevance of the work ethic and advocates the redefinition of work to include domestic labour and hobby activities.
He emphasises the need for recurrent education and presses for guaranteed incomes for all and a national superannuation fund.
The new minister admits that all this could take eight to ten years and an improved economic climate but he is adamant that attitudes must change.
Australia, he adds, has entered a post-industrial period and without radical alternatives and a revolution of the traditional industrial base its society will crumble.
Almost single handedly in the past three years Jones has made high technology and the ‘sunrise’ industries household words and created a climate of industrial and community awareness.
He describes Australia's traditional industrial base as geriatric and virtually past salvation.
The country's only hope for economic recovery, he claims, is for it to turn to the highly skilled, ‘sunrise industries’ that will generate wealth and cope with an expanding welfare bill.
Support for 16 of these sunrise areas, including biotechnology, custom made chips, computer software, scientific instrumentation and solar technology, is the cornerstone of his science and technology policy.
He plans to introduce new grants, government loans and schemes to stimulate the new technical industries.
Whether this will be sufficient to catalyse the country's moribund industry and boost its declining research effort is debatable.
But Jones is the perpetual optimist — even in the face of a declining economy and a budget deficiency of $A96 billion.
‘The size of the deficit simply indicates the need to change the whole economic base of the country,’ he says.
‘Unless we move immediately into high growth, high technology areas our standard of living and quality of life will deteriorate and we will degenerate into a Third World nation.
It's a desperate race against time.’
The minister sees biotechnology as Australia's best opportunity.
The country's scientists, he claims, are ‘World class’ and their potential is enormous.
‘Given the right stimulus this country could emerge as a world leader.’
He is pushing for initial funding of $A5 million rising to $A15 million in three years.
Jones is also determined to increase funding for basic research, which he says the previous government allowed to run down.
He has pledged to increase support by 10 per cent above the rate of inflation for the next three years.
He also talks of boosting Antarctic research by 300 per cent, and continuing the support for the Australia telescope and the starlab space telescope project.
But Jones may be confronted by more problems than he anticipates when he seeks funds for his ideas.
Not only is the government reappraising all its election promises in the light of the national deficit, but also the science minister's post is not part of the powerful 13-member inner cabinet.
However, there is some hope.
The policy for science and technology has been developed in tandem with the new Labour government's economic and industrial policies and is generally considered part of any plans for national recovery and reconstruction.
Another of Jones's major policy drives is information — a commodity in which he himself is extremely rich.
In the 1950s and 1960s he was Australia's television quiz king — something he would prefer to forget.
He reads voraciously, with a target of 60 books a year (though he admits he won't reach that in 1983), and he has a squirrel-like capacity for collecting new ideas and facts.
Jones reserves his highest praise for lateral thinkers — one of whom he identifies as the new Prime Minister Bob Hawke.
The new Science and Technology Minister sees information as an instrument of power and he believes that it should be freely available to all.
His 10-point information policy stresses free access, establishment of information resource centres and public access to data banks.
Jones rejects the philosophy of technological determinism and believes that given sufficient information people should be allowed to adopt the most appropriate technology for their needs.
A key point is the need to raise public awareness and to demystify science and technology.
Already Jones is attempting to establish a joint parliamentary standing committee on science and technology, with the specific purpose of raising the level of understanding among parliamentarians.
He is also keen to set up a body similar to the US Office of Technology Assessment, or New Zealand's former Commission for the Future to stimulate public discussion and look at the long-term implications of technological change.
One of his main bugbears is the foreign ownership of technology.
He believes that Australia must move to greater technological sovereignty, must negotiate better technology transfer agreements with foreign owned companies, and, of course, develop its own indigenous technology.
He is extremely critical of the recent takeover bid by Australia's largest company, BHP for the US Coal Group, Utah.
A move that will allow Utah's parent company General Electric to expand into high technology.
‘General Electric must be laughing all the way to the bank,’ he says.
‘BHP should have used at least part of the money to invest in new technology and upgrade and improve its competitive standing in Australia.’
Jones has one particular pet project to establish the equivalent of an Australian Nobel Prize.
It would be worth no less than $A100 000 and be administered by the Australian Academy of Sciences.
He sees it as having symbolic  significance , directing world attention to Australian research, development and intellectual quality.
‘It would indicate to the rest of the world that Australia is not just a hillbilly, resource-rich country but a country that places great store on its scientific and technological expertise.’
Jones may be an idealist and will probably face a rude awakening when confronted with the realities of the economy and the hidebound attitudes of a bureaucracy that is watching his advent with some trepidation.
Nevertheless his enthusiasm, unbounded energy and genuine passion for technology promises a period of unprecedented activity with new ideas and innovative developments not seen in Australia for a long time.
AM TV
THOSE EARLY BIRDS at breakfast
television are obviously having trouble rousing experts to appear at ungodly hours to comment on the morning's news.
One of our hacks reeled home the other week after a particularly intensive study of the behavioural effects of alcohol at the ‘Three Compasses’.
He passed out, fully clothed, on his sofa.
A couple of hours later, the phone rang, with an invitation to appear with Selina Scott.
‘Hello,’ beamed one of those impossible BBC accents, ‘it's Breakfast Time !’
‘No it bloody well isn't,’ said our man groggily.
New scientists begin here
John Gribbin takes on the role of New Scientist's agony aunt
AMONG THE MANY letters we receive each week there are some that clearly merit publication, and not a few that clearly merit filing in the waste paper basket.
But there are always a few that fit neither category, and seem most of all to require a personal answer to a specific question.
These are the letters that raise questions to which there are, indeed, well-known answers, but answers of which the writers are ignorant.
It is not that the questioners are unintelligent, but rather that they are uninformed — and, after all, as science writers information is our business.
If you didn't buy New Scientist in order to find out more about science, the magazine would not exist for long.
While it brings a warm glow of a job well done (and, perhaps, the thought that a happy subscriber is more likely to keep on subscribing) answering such queries personally does leave a nagging worry.
The letter writers among you, our readers, are very much in a minority.
Perhaps for every letter we get asking about neutron decay, to take one recent example, there are a dozen, even 100, readers who would like to know the answer.
A hundred satisfied subscribers would be even better than one.
So here is an experiment — a column with answers, for all to see, to some simple questions raised recently.
If you like the idea, let us know; if enough of you like the idea and say you do, I might be able to persuade the editor to make this a regular event.
First, then, what about neutron decay?
Jess Artem, of London NW6, has clearly heard that a neutron left to its own devices will decay, with a half-life of about 11 minutes.
Why, then, do neutron stars not ‘decay’?
And why does any star which is radiating energy not lose so much mass (thanks to E=MC 2 )that gravity can no longer hold it together?
The first part of the puzzle is resolved by recalling that only free neutrons decay; the ones locked up in atomic nuclei are stable on any timescale comparable with the age of the Universe (about 15 billion years) although they may remain stable for ‘only’ about 10 years overall.
In this context, a neutron star is effectively a single atomic nucleus.
As for the conversion of mass into energy, this does affect stars in a calculable fashion.
Our Sun ‘loses’ 4 million tons of mass this way every second — but it has so much mass to play with that this rate of mass loss would only add up to 7 per cent of the total after about a trillion (million million) years.
Neutron stars may produce more energy than the Sun soon after they form, when they are active as pulsars, but this phase of energetic activity only lasts for a few thousand years.
Another astronomical question comes from Martin Lowe, of Richmond in Surrey.
He asks if the redshift of distant galaxies could not be explained by a universal contraction towards a point which is, as yet, beyond our observational limits.
Unfortunately for such exotic ideas, even if the ‘centre of the Universe’ were out of sight, such a peculiar behaviour would be revealed by an asymmetric pattern of redshifts on the sky.
The observed pattern, that in all directions the redshift of a galaxy is proportional to its distance from us, can be explained (if we are not at a special place in the Universe) only by a uniform universal expansion.
Another reader takes Christine Sutton to task on a semantic point.
She told us (27 January, p 221) that a neutron decays into a proton, an electron and a neutrino, but in the diagram accompanying her words we were shown a representation of a neutron decaying into a proton, an electron and an antineutrino.
‘Which is correct?’, asks Richard Tylor, who hails from Nivenskowe, Midlothian.
Strictly speaking, the decay does indeed produce an antineutrino, not a neutrino. but Christine offers two defences.
First, ‘neutrino’ can be used as a general term describing both the particle and its antiparticle counterpart, in a general discussion; secondly, when Wolfgang Pauli coined the name to describe a then hypothetical particle in the 1930s, he used ‘-neutrino’as the name for the decay product.
It was only later that the theory was refined to include anti neutrinos, when, for consistency with the conservation laws, the name ‘antineutrino’ was given to the variety produced by neutron decay.
But either way we should have been consistent and we apologise for confusing at least one of you.
Chris Sutton's imminent departure from the full-time staff of the magazine is not, however, a direct consequence of this semantic confusion, merely one of personal fission.
Finally (for now), J. Whitby-Smith, of London E11, highlights a similar confusion about one of my own articles (’ Stand by for bad winters’, 28 October, 1982, p 220).
He (I'm sure of the pronoun, since all too few women carry the initials C ENG, MIEE, after their names) refers to the Chinese proposal that the combined pull of the four giant outer planets influences our weather, and objects to using the term ‘synod’, meaning an alignment of all the planets to describe this event.
If the inner planets are left out of the argument, says Whitby-Smith, then we are really dealing with ‘half-synods’ involving only four planets, and ‘without doubt these occur too frequently to support the argument’.
In fact, that is a doubtful statement.
The inner planets all move quickly around their orbits, even Mars taking less than two of our years to circle the Sun once, and produce a rapidly shifting pattern of alignments.
It is the stately motion of the gas giants that is alone responsible for the long interval (about 180 years) between full synods, and an ‘outer half-synod’ recurs at exactly the same interval.
Whether we ignore the inner planets, or assume they add a tiny contribution to the overall effect, the key dates that come out of the analysis are the same — and I should have made that clear in the article.
So, with a self-inflicted slap on the wrist, I leave you.
Keep asking the questions, and we'll try to answer them.
Abstract corner
‘Sleep walking, as an occasional event, is quite normal for most children.
There is a tendency for it to run in families.’
Sleep Topics
BAYS away in South Kensington
HARD TIMES do not seem to be quenching the natural scepticism of Britain's youngsters.
Their healthy attitude to ‘establishment’ views continues.
At least that is one message 1500 of them gape at the first ever BAYSDAY, held earlier this month at various institutions in London's museum heartland.
BAYS is the ‘Young Scientists’ wing of the British Association for the  Advancement of Science.
Whereas BA senior finds it hard to survive between annual meetings — and now has difficulty in attracting 1500 people to what should be an annual science feast —BAYS is a thriving operation, with branches all over the country.
BAYSDAY was the BA's attempt to create a more national image for the organisation.
The first event focused on balloons.
The independent turn of mind of the youngsters first showed itself when they were invited to choose who to throw out of a descending balloon.
Those ‘in’ the balloon — the infamous, the famous and the worthy — had to make the case for not being treated as ballast.
Perhaps knowing the likely outcome, New Scientist's editor made a pathetic excuse about being in Canada at the time and dropped one of his old colleagues into — and swiftly out of— the balloon.
Lawrence McGinty, late of these pages and now science's man on Channel 4's news programme — had the indignity of being the first to be ejected from the balloon.
(It's just possible that the youngsters were expressing a scientific talent rather than a dislike of the press, for their is no denying that McGinty's ample form would produce an immediate uplift in the balloon.)
McGinty was swiftly followed by Tony Bond, of the Engineering Council, and Alan Hall, a headmaster from Kent, leaving David Winwood, a methodist minister, as the sole survivor.
Either the youngsters have a view of the world that is the complete opposite to that of their seniors, or they decided a man of the church should be left to get closer to the heavens.
The next rough ride came when the youngsters grilled Lord Kearton, Sir Andrew Huxley and William Shelton, a junior minister at the Department of Education and science.
Science and education may have been on the minds of the panel, but the youngsters preferred to range over topics such as nuclear disarmament and the ethics of energy.
On the other hand they didn't ignore education, and there were the usual perceptive comments on the science curriculum in Britain's schools.
Balloons featured elsewhere in the programme, with a competition for model hot-air balloons in the main hall of the Geological Museum.
Practical matters also reared their head when 15 teams were given three hours and a selection of Meccano, paper cups, cotton reels and other bits and pieces to make a machine that would take a bag of coins all in one go, and dispense them one at a time for sorting.
Could it be that the sponsor of the competition — Lloyds Bank — expects to revolutionise its money sorting system?
Through the dark cloud shining
Jack Harris discovers some particle tracks during a museum visit
AN HOUR to spare on a recent visit to London gave me the opportunity to visit the famous Science Museum's well-advertised new gallery ‘Physics and Nuclear Power’.
I found it to be a splendid panegyric on the peaceful uses of atomic energy, though to present a history of nuclear power without even mentioning the existence of hydrogen bombs, may be thought by some to overdo the euphoria.
However, my purpose here is not to cavil, nor to review the exhibition as a whole.
Rather I wish to draw attention to a particular exhibit in the gallery which is quite simply the best that I have ever seen and which appeals at many levels: intellectual, artistic and aesthetic.
It is the last exhibit in the show — an alcohol-vapour cloud chamber that displays continuously the tracks of the Earth's background irradiation and made by the Phywe company, West Germany.
At first sight it is unremarkable; a simple Formica-clad cube, but as one draws closer it can be seen that the top face is transparent and through it one can view the tracks in the cloud chamber — the long thin lines due to cosmic rays and the short stubby ones due to alpha particles.
It is like looking through a magic window into the real world of natural irradiation.
The patterns created by the tracks are very beautiful, like a succession of half-completed Jackson Pollock paintings; an action picture par excellence .
In another mood, while gazing down into the vapour one could imagine the turbulent creation of the Earth with the alpha tracks like mountain ranges constantly forming, disappearing and reforming.
By some optical illusion the vapour appears always to flow towards the observer; as one moves around the cube the flow rotates to follow one, like the eyes of the Gioconda.
I had been aware,intellectually , that the background level of irradiation is really quite high (as I write the clicking of a geiger counter left switched on in an adjacent room reminds me) but it took the Phywe cloud chamber to make me realise that irradiation is not a separate thing but truly a part of life.
All the time we are immersed in a veritable sea of irradiation that not only surrounds us and permeates our bodies but suffuses from inside our very being.
A sight of the chamber should be essential viewing for all those who have an irrational fear of the minute quantities of irradiation (less than 1 per cent of natural background) that emanates from our nuclear plants.
While I was gazing at the chamber a schoolgirl who was touring the gallery with her father came up and asked me how it worked.
I told her about the Scottish physicist Charles (C.T.R.) Wilson's interest in meteorology and of his accidental discovery of the tracks.
She was interested to learn more about natural irradiation and how our bodies are transfixed by a million cosmic rays a day, like the arrows that pierced Saint Sebastian.
We discussed the cosmic background microwave irradiation and the support the discovery of it gave to the big bang theory.
Her father came up and listened for a while and then said that he thought it was all marvellous but he didn't understand a word of it.
‘How can I understand it, how can I understand it?’ he went on and on accusingly, as though scientists are to blame for not inventing a magic serum that would give him instant knowledge.
I did not know what to say.
‘You'll have to read a bloody book,’ said his daughter.
Rocks plant
ICI's original polythene plant developed one spectacular problem — it exploded!
Every two years or so, production staff would be faced with a choice: shut down the plant for two weeks to avoid an explosion or let the contraption blow up and restart full production on the following day.
Understandably ICI chose the former.
On one occasion, however, warning of the impending pyrotechnics came too late, the plant blew up.
To save time and money, ICI decided that damage to the plant could be repaired with cheap prefabricated units.
It was also discovered that the cost of replacement parts was less than two weeks' loss of production.
Thus, every three years or so, a strident siren echoed across the Dantesque landscape of Winnington to signal the moment for everyone within eye- or ear-shot to retire to a safe distance and to watch the plant reduce itself to a concrete skeleton.
Two hours later, they would dutifully file back and recommence production.
The original 1930s plant was replaced by an explosion-proof variant some five years ago: Winnington has been a quieter, if less spectacular, place since then.
LETTERS
Computing out of focus
I read your notes ‘Computerised flights of fancy’(Comment, 10 March, p 630).
Your comment that the central thesis of my lecture does not stand up stems from a misunderstanding of what I was trying to say.
My main point was that the Alvey Committee was proposing to focus R & D resources on a particular type of system based on their guess that intelligent knowledge-based systems were the system of the future.
My own view is that while that type of system will be important in the future it may not have the central role assumed for it by John Alvey — hence my plea For more resources to be devoted to looking at real user needs and trying out alternatives to the IKBS approach.
I compare the Alvey proposals with the Concorde because I felt that as with the Concorde project too much of our limited resources are being put into one programme.
Of course I am in full agreement with you that the market for IT products is growing and will continue to expand in the foreseeable future.
I too applaud the aims of the Alvey  Committee .
However, to achieve these it is necessary to modify and amplify their proposals.
Mud houses for all
Two comments on New Mexico which have appeared in recent issues of your magazine give an ill-informed and unbalanced impression of this state.
I refer to the caption accompanying ‘Let them live in mud’, by Anil Agarwal in your 16 December issue —‘Half a world away, in New Mexico, mud bricks are the plaything of the rich’(p 740) and the description in the review by Colin Moorcraft in your 13 January issue of the exhibition ‘Down to earth’ by Jean Dethier, ‘encompassing all manner of earth buildings from…to the trendy latter day eco-huts of New Mexico’(p 110).
Mud buildings in New Mexico date from before AD 1300.
When portions of present-day Indian pueblos occupied since that time are uncovered, their walls prove to be laid up of long pours of puddled mud.
The Spanish conquerors introduced the use of bricks of sun-dried mud — adobes — by AD 1600.
There can be no reliable estimate that —’ 10 per cent of the homes built in 1980 in New Mexico were made from adobe’, because Spanish Americans and Anglo Americans far from rich, can so easily convert the soil from beneath their feet into walls around new rooms, and they do this regularly with their own hands in enlarging their houses.
The combination of adobe construction and design for solar heat gain, is one of the few vital ideas in architecture today.
The rich who are flooding into parts of northern New Mexico include British, French, German and Arabs as well as‘Anglos’ from other parts of the United States.
The houses of some of these people are exaggerated and ‘trendy’.
Overall, they are not nearly so trendy as articles about architecture which let the fortuitous effect of individual photographs lead the text into exaggeration and bombast.
Gentle genes
As most of the readers of my review of The Handbook of Human Intelligence (Review, 10 March.
p 668) will have realised, there is a somewhat Freudian printing error which has changed Sir Francis Galton's theory of a genetic predisposition into a gentle predisposition!
Weather wisdom
Well?
Are we going to fry (’ The curious case of the shrinking Sun,’New Scientist , 3 March, p 592), or freeze (’Stand by for bad winters’,New Scientist , 28 October, 1982, p 220)?
Any answers please, John Gribbin?
Windmills at Sea
The article on Flettner's rotors (’ Critics in a spin over Flettner's ships’, 10 March, p 6561 interested me greatly but I feel that another, better form of wind propulsion for ships is either unknown or is being ignored.
Certainly I have never seen any article on the ‘Two to One’ panemone.
The principle is simple.
If a flat sailed (for descriptive convenience) ship were to be constantly sailed anti-clockwise around an island with the wind blowing from the north, the sail would always lie in line with a point due east on the sailed circle (due west if sailed clockwise).
The sail would revolve around its man once for every two circuits of the island.
I have constructed several windmills based on this principle of one (non-selfstarting), two, three and more sails of both rigid and floppy types and all work well.
the three-vaned the best though I have no means of measuring aerodynamic efficiency.
A ship equipped with two or three of these panemones could have the best of both worlds.
With the wind in a favourable position, the device could be locked and the sails used in a conventional manner.
In an unfavourable position, with the panemone operating driving screws, drawing reserves of power (stored in  flywheel/gyro stabilisers?) generated when the ship was at anchor.
With the sails furled, the device could be made to fold inward to the mast for loading and unloading I would suggest that here, a good idea lies begging.
Not a dirty disease
In your article ‘Ten years to slake a global thirst’(10 March.
p 661) you include onchocerciasis among the diseases ‘linked to dirty water and lack of sanitation’.
Reference to your own back issues would have quickly revealed onchocerciasis to be associated with running water with little, if any, pollution.
Indeed cleaning up a polluted river could even allow invasion of blackflies and the possibility of starting a new focus of infection for onchocerciasis!
Computer slips
Most British people and publications I am  acquainted with are quite proud of their command of the English language and engage in quite a lot of patting themselves on the back.
New Scientist is no exception here, but nearly always this is rightly so and well deserved.
I was therefore appalled to see three glaring errors, in large type, in the 17 February issue.
In a picture of a motorcar on p 425, the caption spelled ‘survey’ as ‘ servay ’.
A book review by Paul Davis on p 446 proclaimed the author to be ‘Abraham Paris’ yet consistently used the name ‘Abraham Pais’throughout.
Lastly the worst of all was on p 456 calling the former British colony ‘ Vanuata ’ instead of‘Vanuatu’.
It would seem there are more printing devils in computerised typesetting than old linotype.
Sugar and cancer
Last year the British Medical Journal reported that researchers in the U.S had discovered a correlation between alcohol consumption and the incidence of breast cancer in women.
The correspondent, however, was sage enough to emphasise that the relationship was far from causal.
Such counsel was sadly missing from Monitor's (10 March.
piece ‘Sugar foods may promote breast cancer’.
Indeed, readers were informed that the correlation was ‘certainly suggestive’.
They all are!
The Greening of Europe
Did Debora MacKenzie, who wrote the article in Forum (3 March.
p 601) about the French and German ‘Greens’, do all her research in Switzerland as well as being based there?
It's the only reason I can think of for her to get so much wrong.
First of all, there are Green Parties in Britain and the US.
In Britain, the Ecology Party is 10 years old, has had councillors elected at all levels of local government and will have over 100 candidates in the general election.
The party has also entered an alliance with ‘Women for Life on Earth’: the women at Greenham Common and the other women's peace camps.
They will be standing against Mrs Thatcher, Mr Heseltine and several right wing multilateralist Labour MPs on a joint Women for Life on Earth/Ecology label.
In the US there is the Citizens Party.
Secondly, Green politicians are not environmentalists, they are ecologists.
Environmentalists are groups, such as Friends of the Earth, who run single issue campaigns, recycling bottles or saving whales for example.
Though often very successful they are attacking the symptoms of the world's problems, not the causes.
Political ecologists-Greens-are radicals in the true sense of the word, going to the roots of the problems that beset our planet; resource depletion, militarism, the exploitation of the Third World and so on.
Finally, could you leave the insults to the gutter press please?
Calling the most important political movement since the rise of Socialism a load of eco-freaks doesn't give you any credit and just gets me annoyed.
Foxes and hares
It is ironic that the organisation which is researching into the falling hare population, the Game Conservancy (Monitor.
10 March, receives support from the fieldsportsmen, the same people who in hunting counties, encourage foxes to bred in artificial earths, thus producing an increase in foxes which is a contributory factor in the decline of the hare (being 20 per cent to 30 per cent of the rural foxes' diet)!
Hacking epidemic?
As one who is fighting against becoming a hacker myself, I was interested in your article ‘Hacking with the hackers’, by Neil Frude (Forum.
24 February 83, p 532), although I think your illustration is a little out of date.
Few hackers of my acquaintance smoke.
I should like to know whether there are any occupational diseases to which hackers are prone.
Two youngish (less than 35 anyway) ones of my acquaintance have recently suffered cancer of the digestive system.
One died leaving a small son, the other is back at work but very much a shadow of his former self.
Have you any statistics to show what hackers die of?
And whether they die prematurely compared with other occupational activities?
Might it be their working environment?
Or lack of regular meals?
Or what?
I read with interest Neil Frude's feature on compulsive programmers ‘Hacking with the hackers’.
Anybody who is involved with teaching computing students will recognise many of the traits mentioned in the article.
I must take issue, however.
with Frude's assertion that one of the advantages of hackers is that they generally become very proficient programmers.
I think experience shows that in fact the opposite situation is much more prevalent.
Although ‘hackers’ are interested in writing very clever programs or manipulating the machine to do unusual things, they are generally oblivious to any concept ‘quality’in their programs, particularly as judged by another person.
I think that the other worrying feature about the potentially  obsessive nature of programming is the point Dr Frude makes about it happening to people who are already ‘introverted and socially inept’.
I find it disturbing that many people, particularly school teachers, still label programming as a lonely.
dedicated task for meticulous and logical individuals with a mathematical bent.
There is a real danger that the kind of compulsive behaviour described by Neil Frude and others will become much more widespread given the massive increase in personal computing unless we alter our image of the subject and give the social skills necessary greater emphasis..
Job Jitters
I considered your article on the response to the BBC's advertisement for a science correspondent (Forum.
3 March, p 605) to be in very bad taste.
I thought that job applications were meant to be treated in confidence.
Is it a new editorial policy that New Scientist will investigate the number and source of replies to advertisements within its pages and then print snide remarks about why the unsuccessful applicants shouldn't have applied in the first place?
If so, then I shall cancel my subscription to,New Scientist and take out one of those half-price offers on Nature which are so abundant at the moment.
One further point.
I looked up the original advertisement.
It did not say that the BBC wanted an experienced journalist': it asked for relevant experience.
Maybe the ‘flood of academics’ felt that a career in real science was relevant experience.
At least it teaches one respect for facts.
Stanley airport
Tam Dalyell (Forum, 3 March, p 605) perpetuates two myths about the Falklands.
Liver fluke is not and never has been present in the islands.
I have been resident here for about two years and I have always drunk water from available streams and will continue to do so without any ill effects.
We have cool summers and mild winters with relatively modest seasonal variation.
The annual rainfall is about 25 inches around Stanley, but decreases in the south and west of the islands.
The climate is a good deal more hospitable than many parts of Britain.
ARIADNE
The other night I was asked to a wine tasting.
It is hard to know how to comport yourself on such an occasion, the choice appearing to be between solemnity and frivolousness, with nothing in between.
The wines under examination were American, or more accurately, Californian.
I had tried three of them, swallowing the small amounts in the glass with what I thought was a superbly noncommittal expression when I discovered that in the middle of the room were two iron spittoons, waist-high.
The more seasoned tasters, I saw, were rolling the wine round their tongues and their eyes at the ceiling, after which they spat out the wines in the receptacles provided.
It takes little to get me on the right lines.
In no time I was rolling with the best of them and filling the intervals between wines by chewing a morsel of cheese.
But what pitfalls there are, to be sure.
One of the pourers reproved me for eating cheese before trying the wine.
Of course, it was James Thurber who epitomised wine snobbery in the famous line, ‘It's a naive domestic Burgundy but I think you'll be amused by its presumption.’
There was not much of this kind of nonsense as far as I could tell by eavesdropping around the room, but there were cries of anguish when the prices of some of the wines were announced.
One of the reasons for the high prices was, according to the expert talk, that there was not much of the wine made.
That seemed to me to be daft, bordering on snobbery.
American readers (there are some) should be reassured.
I have no intention of denigrating Californian wines.
Most of them, to my uncultured palate at least, were good.
Interestingly, I found out that there are fashions in wine.
South Africa Spain, Australia and New Zealand have been in fashion I was told.
I missed New Zealand.
Clearly, although of a good colour and nose, I lack balance and finish and am short of follow-through.
You would think that, combined, the talents of the Department of Transport and the Central Office of Information would be able to handle the native language and its syntax.
You would be wrong.
Enclosed with all the notices going out about renewal of car licences is a pamphlet, prepared by the two.
It exhorts the motorist to keep his car in good shape by regular maintenance and such.
It says you can cut your motoring costs and use less petrol ‘and that doesn't mean doing less miles’.
Like everyone else, including Oscar Wilde, I sometimes wish I had said what someone else has said.
The same feeling broke in when I read an answer that the President of Venezuela gave at a press I conference.
He had been asked a question on the influence of campaign promises and its effect on the election of particular candidates.
What did he think?
He replied, ‘On that matter, what I have always thought.
Better said, what I think is what I have thought, or that is to say, that I continue thinking the same.’
That is one kind of fog.
Here is another and it is an interesting and rewarding exercise to translate this into language with plain meaning It comes from a Japanese journal, clearly influenced by trans-Pacific pomposity.
‘Developing nations place their greatest hopes on the actualisation of their human resource potential.
Technical cooperation on a person-to-person basis is the surest means to this end’.
Over the past few weeks I have been dipping into the annual report — for last year — of the Medical Defence Union, an outfit that deals with legal claims against doctors for alleged incompetence, negligence and other failings.
It leaves me in no doubt that legal claims are on the increase and so are the amounts that are awarded in damages.
The cases that are discussed in the report make fascinating and occasionally tragic reading, for example the illegibility of handwriting leading to wrong prescription and the death of a patient.
Mixed feelings follow.
I can remember the days when doctors were considered not far short of the Almighty.
My parents, expressing reverence, addressed the family doctor as doctor as in ‘Doctor says…
’ He was infallible, kindly and not above having a go at the medicinal whisky.
My attitude to doctors has been severely changed since then by having a kidney stone diagnosed as psychosomatic and ulcerative colitis as a stomach upset, so that I am far from reverential and have occasionally flirted with contempt unfairly.
An abatement of reverence and forelock touching is all to the good, and I cannot really think that doctors should be denied their fair share of human fallibility.
It is an old subject for jokes and not worth going into.
Doctors are the only people who can bury their mistakes and all that.
One of the problems is that some doctors still behave as if they are deserving of awe, or as if patients are fools, when every patient is an expert on himself, at least.
An endless topic this is.
Having watched someone die after being put in succession on steroids, Opren and Distalgesic, I have a hard time trying to be objective.
DAEDALUS
The conventional way of cleaning fabrics is to cart them off to the washing machine, wash them, dry them, and cart them back again.
Daedalus dislikes this tedious and wasteful business, and is inventing an in-situ laundering machine.
Its compact cleaning head delivers a stream of detergent solution, which is continuously recovered by suction, filtered and recirculated while ultrasonic piezoelectric agitator-pads dislodge the dirt.
A second stream of recirculated rinse-water removes the detergent, and an air-suction device sucks away surplus water.
The main problem is drying the cloth instantly.
Spinning is impractical, and a fierce enough heater would probably ignite the fabric.
But Daedalus remembers that microwave heating acts specifically on the water in a sample (because of the high dielectric-loss factor of water).
So a microwave system would heat only the wet areas of cloth.
Dry areas would no longer absorb energy, and would not be at risk from the high local power-input.
The first application of the device will be in hotels and hospitals.
Their beds are changed at least daily, at great cost in labour.
Daedalus's ‘Autobed’ has its cleaning head under the main frame, stretching the  length of the bed.
Sheets and blankets are continuous sleeves, threaded through the cleaning head.
To change the bed, these sleeves are simply wound round the bed by special rollers.
The ‘used’ section enters the cleaning head while clean linen comes up from beneath.
And if the under-sheet is rotated the opposite way to the top one the patient need not even get out.
He will simply rotate gently between the counter moving sheets.
Vast amounts of labour will be saved.
The domestic version resembles a cylinder vacuum-cleaner, with a head that can be moved over chaircovers, carpets, curtains, etc.
Indeed, only the dangers of microwave radiation prevent the machine from cleaning clothes while the are still on the wearer.
With protective, all-enveloping aluminium-foil undergarments even this would be possible.