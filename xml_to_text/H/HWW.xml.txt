

ARTICLES
Extracellular hippocampal glutamate and spontaneous seizure in the conscious human brain
An alteration in excitatory and inhibitory influences may underlie epilepsy.
We used bilateral intrahippocampal microdialysis to test the hypothesis that an increase in extracellular glutamate may trigger spontaneous seizures.
The concentrations of glutamate and -aminobutyric acid (GABA), the brain's major inhibitory neutrotransmitter, were measured in microdialysates before and during seizures in 6 patients with complex partial epilepsy investigated before surgery.
Before seizures, concentrations of glutamate were higher in the epileptogenic hippocampus, whereas GABA concentrations were lower.
During seizures, there was a sustained increase in extracellular glutamate to potentially neurotoxic concentrations in the epileptogenic hippocampus.
Moreover, the increase preceded seizure.
GABA concentrations were unchanged before seizures, but increased during them, with a greater rise in the non-epileptogenic hippocampus, suggesting that a rise in extracellular glutamate may precipitate seizures and that the concentrations reached may cause cell death.
Introduction
The neurochemical mediators involved in seizure onset, spread, and spontaneous arrest are unknown.
A chemical imbalance has been suspected to underlie the development of spontaneous seizures and epilepsy-related neuron damage.
Epilepsy may be due to either an increase in glutamate-mediated excitation or a reduction in GABA-mediated inhibition.
Local infusion of glutamate, can cause seizures and neuroanatomical injury similar to that found in epilepsy, while glutamate-receptor antagonists have anticonvulsant and neuroprotective properties.
Further evidence that glutamate is involved in epilepsy has come from animal models of focal epilepsy, including intracerebral application of cobalt, folate, and kainate, as well as in electrically kindled amygdala, in which seizures are associated with release of glutamate.
However, investigators were unable to find an increase in glutamate during seizures induced by systemic kainate, bicuculline, picrotoxin, locally-applied bicuculline, or quinolinic acid.
Invasive recordings have recently been done in humans.
Do et al measured increases in excitatory aminoacids by push-pull cannulation in anesthetised human beings undergoing electrical stimulation.
Similarly, Carlson et al found an increase in aspartate and glutamate by microdialysis during intraoperative recording.
However, these studies were brief, had no controls, did not investigate spontaneous seizures in conscious human beings nor release of the brain's major inhibitory neurotransmitter, GABA.
We have developed a combination depth electrode (Spencer probe, Ad-tech Instrument Co., Racine, WI) and microdialysis probe (dialytrode) suitable for implanting in the conscious human brain to measure extracellular concentrations of glutamate and GABA in epilepsy.
Patients and methods
Patients with complex partial epilepsy refractory to medical treatment, and who had been assessed for epilepsy surgery, were studied between April, 1989 and October, 1992.
Patients were admitted to an epilepsy unit for assessment, where scalp electroencephalogram and audiovisual behaviour were continually monitored during at least three typical seizures.
They also had a neurological examination, cognitive tests, magnetic resonance imaging (MRI) with measurement of temporal lobe and hippocampal volume, computed tomography (CT), and single photon emission computed tomography, followed by angiograms, and intracarotid sodium amytal determination of hemispheric language dominance and memory capacity.
If these investigations showed the epileptogenic region, surgical excision was offered; if not, invasive recording using intracranial electrodes was done.
Patients participating in this study gave fully informed consent to a protocol approved by the Yale University School of Medicine Human Investigation Committee.
Hippocampal depth electrodes were placed stereotaxically.
A thin-sliced T-weighted MRI and magnetic resonance angiography was obtained with a Radionics MRI ring and fiducial system attached to the patient's skull.
A computerised imaging workstation designed posterior occipital parasagittal trajectories for multicontact depth electrodes through the length of the hippocampus with the anterior tip resting in the amygdala on both sides (fig 1).
A CRW frame (Radionics Inc., Burlington, MA) was put in place, bilateral occipital burr holes drilled, and the electrodes inserted after translation of the coordinates to the frame.
The microdialysis probe is a modification of our concentric rodent probe as previously described.
The probe is attached to a polyurethane/silastic flexible depth electrode with nichrome contacts.
The day following implantation, the intrahippocampal localisation of depth electrodes is verified by MRI.
Subdural strips are also inserted to ensure additional EEG coverage of mesial and lateral neocortical temporal areas (fig 2).
Patients were transferred to the Neurological Intensive Care Unit for 24 h audiovisual and EEG monitoring.
Dialysis probes were perfused at 2.5 L/min with a sterilised pyrogen-free, artificial extracellular fluid (NaC1 135 mmol/L, KC1 3 mmol/L, MgC1 1.0 mmol/L, CaC1 1.2 mmol/L, ascorbate 200 micromoles /L, and a sodium phosphate buffer of 2 mmol/L to pH 7.4).
The dead volume of outlet tubing (25 L) was equivalent to a 10 min delay in sampling at this flow rate.
Samples were collected continuously with a refrigerated autosampler (CMA200, Carnegie Medicin, Stockholm, Sweden) over 30 min intervals with the frequency increased to every 3 min during a seizure.
The 10 min delay enabled three 3 min baseline samples to be collected before the seizure.
Aminoacids were analysed by a BAS 200A ternary gradient high performance liquid chromatography system (Bioanalytical Systems Inc., West Lafayette, IN, USA) with a CMA200 autoinjector (Carnegie Medicin, Stockholm, Sweden).
Results
Of the patients we studied, ages 18–35, 6 fulfilled these conditions: a) the probes both worked; b) spontaneous seizures with hippocampal onset and secondary generalisation happened during microdialysis; c) there was an epileptogenic hippocampus shown by intracranial EEG and subsequent confirmation of mesial temporal sclerosis in tissue removed at operation.
The dialysate concentration of glutamate before the seizure — the mean of 3 consecutive samples collected from 30–3 min — was (SEM): 22.6 (7.0) mol/L in the epileptogenic hippocampus, and 20.3 (6.6) mol/L in the non-epileptogenic hippocampus (p<0.05).
2 of the 6 patients had unusually high concentrations of glutamate: 75 (1.3) and 49 (2.4) mol/L in the epileptogenic hippocampus, and 68 (2.8) and 47 (1.8) mol/L in the non-epileptogenic hippocampus respectively.
The other 4 patients had glutamate concentrations of 2.7 (0.1), 2.1 (0.1), 5.2 (0.2), 1.7 (0.1) mol/L, and 1.1 (0.1), 1.9 (0.1), 2.3 (0.1) and 1.4 (0.1) mol/L in the epileptogenic and non-epileptogenic hippocampi respectively.
These latter values are similar to those from 40 additional subjects studied since April 1989 (0.5 to 10 mol/L).
The concentration of GABA in dialysates before seizures was unchanged throughout the study: 56 (8) nmol/L in the epileptogenic hippocampus and 74 (10) nmol/L in the non-epileptogenic hippocampus (p<0.01).
Continuous 16 to 64 channel split-screen EEG was recorded from the intracranial electrodes, with a Telefactor Beehive system (W. Conshohocken, Pennsylvania, USA) equipped with automatic seizure and spike detection.
The onset of a seizure was determined from EEG recordings; all seizures began in one hippocampus and rapidly propagated to the other.
The duration of seizures ranged from 40 to 200, s.
The concentrations of glutamate and GABA in dialysis samples collected at 3 min intervals before and during seizures are shown in fig 3.
Concentrations of glutamate are increased 1.5 min before to 16.5 min after the seizure in the epileptogenic hippocampus, whereas in the non-epileptogenic hippocampus, glutamate was increased only from 1.5–4.5 min after the seizure.
The differences between the hippocampi are significant at 1.5 min before and from 4.5 to 16.5 min after seizure (p<0.05).
GABA concentrations increased in both hippocampi during and after the seizure.
There were significantly higher concentrations of GABA in the non-epileptogenic hippocampus (p<0.05) after the seizure.
Of particular interest is the increase in glutamate in the epileptogenic hippocampus before the seizure, and that the increase in this hippocampus is greater and more sustained than for the other.
Quantitative MRI showed a reduction in volume of the epileptogenic hippocampus compared to the contralateral hippocampus.
These measurements correlate with neuronal cell count and show a relative reduction of neurons in the epileptogenic hippocampus.
All patients subsequently had surgical resection of the epileptogenic hippocampus.
Microscopy of the brain samples showed moderate to severe pyramidal neuronal loss and reactive gliosis in CA1, CA3 and the dentate hilus.
Discussion
Our results are the first to show, in vivo, that the release of neurotransmitter aminoacids occurs in the conscious human brain during seizures.
Of great interest is the rise in glutamate in the epileptogenic hippocampus before the seizure.
Since perfusion of glutamate into the hippocampus in vivo induces seizures, our results suggest that an increase in endogenous extracellular fluid glutamate may do the same.
Information from animal studies support this suggestion: in rat hippocampal slices in a high potassium (8.5 mmol/L) medium, the frequency of spontaneous excitatory postsynaptic potentials increases just before a synchronised population burst — an in-vitro equivalent of epileptiform activity.
A similar increase in neuronal firing preceding spontaneous burst activity has been reported following 4-aminopyridine application.
Chamberlin et al propose that the neuronal synchronisation characteristic of epileptiform activity requires the initial activation of a small, critical number of neurons.
The activation of these neurons may be accompanied by glutamate release before the seizure, and glutamate may act as a paracrine factor to produce synchronisation.
It is not known why 2 patients had high basal concentrations of glutamate, but the concentrations further increased when they had seizures.
Although we were not able to collect complete data from more than one seizure in any of the 6 patients, the incomplete data from repeated seizures suggest that a similar rise in extracellular fluid glutamate before seizures is a consistent finding.
The mean peak glutamate dialysate concentration of 65 mol/L reached during seizures in the epileptogenic hippocampi represents a theoretical extracellular fluid glutamate concentration of about 0.5 mmol/L.
However, complex gradients in extracellular fluid glutamate are likely, with higher concentrations in the synaptic cleft and lower concentrations in the extrasynaptic, interstitial fluid around the dialysis probe.
10–100 mol/L glutamate is toxic to neurons grown in culture, so the concentrations reported here may be sufficiently high to produce neurotoxicity.
However, neurons in vivo are less vulnerable to glutamate than in vitro: presumably the efficiency of the uptake of glutamate prevents accumulation within the synaptic cleft.
Although glutamate is the presumed mediator, other neurotransmitters may modulate its effects.
In particular, GABA may act to counter the glutamate-induced excitation.
Treatments that facilitate GABA neurotransmission have been shown to prevent neuronal injury in both the substantia nigra and hippocampus, so the balance between excitatory and inhibitory transmitters may be more relevant than their absolute concentration in extracellular fluid.
In this study, the glutamate/GABA ratio is raised in the epileptogenic hippocampus, particularly in the postseizure period when glutamate remains high as the GABA concentration returns to normal, which may cause the hippocampal damage characteristic of temporal lobe epilepsy, since a similar pattern is produced in normal animals by stimulation of excitatory input to the hippocampus.
GABA concentrations in the epileptogenic hippocampus are lower than in the non-epileptogenic side, which suggests that there may be altered release or reuptake despite the relative preservation of GABA neurons.
Glutamate-induced GABA release is diminished in the epileptogenic hippocampus and this release is not calcium-dependent (unpublished observations) and is transporter-mediated.
The increase in extracellular glutamate in the epileptogenic hippocampus despite a loss of glutamatergic neurons may reflect decreased uptake.
High-affinity uptake occurs via nerve terminals with glutamatergic deafferentation which decrease glutamate uptake in vivo.
Moreover, there is an increase in the release of glutamate (but not GABA) from glia cultured from epileptogenic tissue, suggesting that the increase in extracellular fluid glutamate may be partially of glial origin.
The in-vivo neurochemical pattern of a rise in extracellular glutamate before a seizure and a delayed rise in extracellular GABA was a consistent finding in the epileptogenic hippocampi of all patients.
Clinical microdialysis coupled to depth electrode EEG recordings might therefore be useful as a diagnostic aid and help in defining the epileptogenic region for surgical resection.
Parathyroid hormone-related protein and responseto pamidronate in tumour-induced hypercalcaemia
To find out if the concentration of parathyroid hormone-related protein (PTHrP) predicts the response of tumour-inducing hypercalcaemia (TIH) to pamidronate, we studied 44 patients.
Pretreatment measurements of serum PTHrP, calcium and phosphate, nephrogenous cyclic AMP, tubular threshold for calcium and phosphate (TmP), and the presence of bone metastases were correlated with response to pamidronate.
Response was considered good (normal calcium concentration corrected for albumin [CCa]for >14 days), or poor (failure of CCa to fall, or a rise above normal 14 days).
PTHrP correlated significantly with response (good vs poor, p=0.02).
Undetectable PTHrP (<2 pmol/L) was associated with a good response in all seven treatments, PTHrP in the range 2–12 pmol/L was associated with good response in 10 of 14 treatments, while PTHrP 12 pmol/L was associated with a poor response in all 11 treatments.
Tubular threshold for calcium correlated with the fall in CCa by day 6 after treatment (p=0.02).
Urinary clearance estimations in poor responders suggested that there was an incomplete reversal of the renal tubular component of hypercalcaemia.
Serum PTHrP correlates with response to pamidronate in the treatment of TIH; which may be associated with a renal tubular mechanism not significantly affected by currently available treatment.
Drugs that inhibit tubular reabsorption of calcium or PTHrP secretion may help in patients who do not respond to pamidronate.
Introduction
In tumour-induced hypercalcaemia (TIH), chemical mediators produced by cancer cells increase bone resorption and impair calciuim excretion by directly affecting renal tubular excretion of calcium.
Parathyroid hormone-related protein (PTHrP) is the main humoral mediator of hypercalcaemia in solid tumours and acts on both bone and kidney.
Like parathyroid hormone it also increases nephrogenous cyclic adenosine monophosphate (AMP) and decreases the tubular phosphate threshold.
Agents for the treatment of TIH act mainly by inhibiting bone resorption.
Pamidronate, a second-generation bisphosphonate, is a potent inhibitor of resorption, and has been successful in the treatment of TIH.
A poor response to pamidronate occurs in patients who have evidence of renal tubular stimulation, as indicated by a low tubular threshold for phosphate or high threshold for calcium.
Since PTHrP is the potential cause of this renal effect in many solid tumours, the amount of PTHrP produced by the tumour may determine the response to pamidronate.
We therefore examined the correlation of plasma concentrations of PTHrP with response to pamidronate in the treatment of TIH.
Patients and methods
This was a prospective study of patients with histologically confirmed malignant disease and hypercalcaemia.
Before treatment, plasma PTHrP concentration, electrolytes, nephrogenous cyclic (cAMP), and tubular excretion of phosphate and calcium were measured; and the presence of bone metastases determined.
Serum calcium and albumin concentrations were measured for the first 7 days and at least weekly for 4 weeks after treatment.
Patients were excluded from the study if they had received mithramycin, calcitonin, steroids, or any other antihypercalcaemic agent within 14 days; received chemotherapy within 14 days, or had a change or introduction of hormone treatment within 6 weeks; a serum creatinine 200 mol/L after hydration; or were pregnant.
Hyperparathyroidism was ruled out by a serum parathyroid hormone assay.
After diagnosis of hypercalcaemia (corrected serum calcium >2.85 mmol/L), 2–3 L of 0.9% sodium chloride were given intravenously over 8–12 h for rehydration.
This was followed by pamidronate in 500 mL 5% dextrose infused at a rate of 15 mg/h.
Restricted (<2000 mL/24 h) or no intravenous fluid was given after pamidronate for up to 3 days.
No patient received forced saline diuresis.
The amount of pamidronate given depended on the pretreatment corrected serum calcium (CCa).
For a CCa of 2.85–2.99 mmol/L, 3.0–3.49 mmol/L, 3.5–3.99 mmol/L, and >4.0 mmol/L, the doses were 30 mg, 45 mg, 60 mg, and 90 mg, respectively.
Patients could be retreated if more than 4 weeks had elapsed since the initial treatment.
Calcium, creatinine, phosphate, albumin and electrolytes were measured in fasting urine specimens and in plasma using a standard autoanalyser.
Serum calcium was corrected for albumin concentration (CCa) with the formula: CCa=serum calcium+ (0.02 [40-serum albumin]).
PTHrP was measured in plasma by N-terminal radioimmunoassay.
Parathyroid hormone was measured in serum by radioimmunoassay directed towards the amino terminal region.
Cyclic AMP was measured in 2 h fasting urine and plasma samples with a commercial kit (Amersham International plc, Amersham, UK).
Nephrogenous cAMP was calculated with the formula of Broadus.
The presence of bone metastases was assessed by technetium scan or radiographs.
Tubular threshold for phosphate and calcium (TmP, TmCa) were calculated from fasting spot plasma and urine measurements according to the methods of Walton and Bijvoet, and Nordin.
Tubular calcium threshold was corrected for urinary sodium excretion (TmCa:Na) by the formula TmCa:Na=TmCa+0.24(urinary sodium excretion).
Relative contributions of reduced glomerular filtration rate, increased tubular reabsorption, and increased bone resorption to the total component of hypercalcaemia were calcualted by urinary calcium excretion estimations by the method of Nordin.
A good response to treatment was defined as a normal CCa for 14 days or more; a poor response, a raised CCa (>2.65 mmol/L) within 14 days.
The acute fall in calcium was calculated by the difference in the pretreatment CCa compared with the CCa at day 6 or 7 after pamidronate.
Statistical comparison between response (good vs poor) and the pretreatment variables (PTHrP concentrations, nephrogenous cAMP, TmCa, TmCa:Na, and TmP) was made by logistic regression.
Correlation between acute response (day 0 minus day 6 CCa concentrations), and the pretreatment variables was made by Spearman's Rho test.
Results
44 patients (table I) received 51 treatments with pamidronate. 5 patients died within 5 days of receiving pamidronate. 4 patients died with uncontrolled hypercalcaemia and of these, 3 had markedly elevated PTHrP (34.4, 23.5, 43.3 pmol/L).
1 patient died of a pulmonary embolism on day 4.
These patients were excluded.
Pretreatment parameters showed that high PTHrP correlated with high cAMP (p<0.001); TmCa:Na correlated with TmP (p=0.01), low serum phosphate correlated with high PTHrP (p=0.01), and low TmP (p<0.001).
There was no significant correlation between PTHrP and TmP.
There was no statistical difference in the pretreatment CCa between the good and poor responders (p>0.4).
27 patients (32 treatments) had a clinical response assessment and 33 patients (36 treatments) had day 6 or 7 CCa mesurements available to assess acute response.
There were 17 good responders and 15 poor (fig 1).
CCa did not return to normal in 2 patients after 3 treatments.
PTHrP was raised in all 3 (29.8, 30.5, 78.0 pmol/L).
The CCa returned to normal initially in the other poor responders, but rose again within 14 days.
Of the pretreatment variables, the PTHrP concentration and serum phosphate correlated best with response (table II): higher PTHrP was more likely to be associated with a poor response, whereas all treatments associated with undetectable PTHrP led to a good response.
A logistic regression model was used to separate 3 groups: undetectable PTHrP (<2 pmol/L) was associated with good response in all seven patients; PTHrP in 2–12 pmol/L with good response in 10 of 14 treatments; and PTHrP 12 pmol/L with a poor response in all 11 treatments (fig 2).
PTHrP of 12 pmol/L separated good from poor responders.
5 patients were retreated with pamidronate after more than 4 weeks.
2 had raised PTHrP and a poor response after both treatments (PTHrP levels 30.5, 78.0 initially and 29.8, 80.0 on retreatment).
1 patient had undetectable PTHrP and a good response on both occasions.
One patient had a good initial response (PTHrP=11.0) and a poor response on retreatment (PTHrP=21.0); another a poor response initially (PTHrP=4.4) and a good response on retreatment (PTHrP=8.3).
TmP and bone metastases were also correlated with response.
Poor response was more likely to be associated with a low TmP; good response with the presence of bone metastases, but the reverse was not true for poor response.
There was no statistical correlation between nephrogenous cyclic-AMP, TmCa and TmCa:Na, and good vs poor response.
Response assessed by the acute fall in CCa showed the difference between day 0 and day 6 or 7 CCa correlated significantly with TmC and TmCa:Na (p=0.02 and p=0.03 respectively).
Post-treatment urinary calcium clearance estimations were available for 5 of the patients with a poor response.
4 patients had complete abolition of the bone component of the hypercalcaemia by day 5 and 1 had more than a 50% reduction on day 3.
Mean bone resorption component fell from 0.4 to 0.1 mmol/L (range 0.17–0.78 pretreatment, 0.1–0.48 post-treatment).
The renal tubular component improved in 4 and increased in 1.
Mean tubular component fell from 0.4 to 0.24 mmol/L (range 0.66–0.48 pretreatment, 0.14–0.55 post-treatment).
Discussion
In patients with TIH, the main circulating mediator is PTHrP., In breast cancer, PTHrP may also contribute to the ability of metastases to grow in bone by acting as a local mediator.
Despite the dual action of PTHrP on bone and kidney, the agents available for the treatment of TIH are mainly aimed at reducing bone resorption, and no agent has a substantial effect on calcium excretion.
Pamidronate binds to bone and reduces resorption, but has no effect on the renal tubule.
Calcitonin and mithramycin have a weak calciuric effect but the main effect of these agents is inhibition of bone resorption.
Patients with TIH who have a detectable renal component to the hypercalcaemia have a poor response to bisphosphonate treatment.
We have previously shown that, of the biochemical indicators, a low tubular phosphate threshold correlated best with a poor response to pamidronate, and we suggested then the poor response was due to high concentrations of PTHrP, which has been confirmed in this study.
PTHrP was the best predictor for response, with high PTHrP levels correlating with poor response and vice versa.
A PTHrP value of 12 pmol/L gave a clear distinction between good and poor responders in this study.
Biochemical indicators of renal tubular stimulation predicted response to treatment, but these indicators depended on the response criteria used: low serum phosphate and low TmP correlated with a poor response when length of response was examined, whereas TmCa correlated with a fall in CCa from day 0 to day 6, and accord with other studies that used this response criterion.
The acute fall in blood calcium and maintenance of normocalcaemia are both important in the treatment of TIH.
Sodium load during rehydration determines calcium excretion, but has no long-term effect.
Sustained response relies on the use of specific antihypercalcaemic agents, so the length of normocalcaemia should be measured to evaluate response to these agents.
We would recommend a similar response definition in future studies of the treatment of TIH.
It is not entirely clear why a high PTHrP concentration is associated with a poor response to treatment.
Our earlier study of bisphosphonate treatment in patients with TIH showed that a fall in plasma calcium does not stimulate tumour production of PTHrP.
In the present study, the poor response patients who had post-treatment calcium clearance estimations show that pamidronate effectively inhibited bone resorption, but did not correct the tubular defect.
This implies that the failure to inhibit the renal mechanism of hypercalcaemia is the main cause of a poor response.
However, it is also possible that persistent high PTHrP, unaffected by pamidronate treatment, could have a more potent effect on bone in these patients.
Currently, the prediction of a poor response will not change clinical practice because agents are not available that conveniently increase calcium excretion.
Development of drugs that are inhibitors of the tubular reabsorption of calcium, specific inhibitors of parathyroid hormone or PTHrP action, antibodies to PTHrP, or inhibitors of PTHrP secretion may allow better control of hypercalcaemia in these patients when used in combination with the available inhibitors of osteolysis.
Differentiation of acute from chronic renal impairment by detection of carbamylated haemoglobin
Detection of carbamylated haemoglobin, measured as valine hydantoin per g haemoglobin (VH/g Hb), may be useful in differentiating between patients with acute or chronic renal failure.
To assess this test, we measured carbamylated haemoglobin prospectively in 42 consecutive patients referred to the regional renal unit with a serum creatinine in excess of 500 mol/L and a provisional diagnosis of acute renal failure (ARF).
Patients were subsequently classed on clinical criteria as having ARF (20) or acute on chronic renal failure (AonCRF, 22).
24 patients with stable chronic renal failure (CRF), matched for degree of renal impairment, were also studied.
Standard biochemical tests and haemoglobin were similar among the three groups.
Patients with ARF of 10 or less days duration had a lower median (interquartile range [IQR]) carbamylated haemoglobin concentration than those with a longer duration of ARF (29 [27–35]vs 72 [60–83]g VH/g Hb; p<0.01).
Carbamylated haemoglobin concentration was lower in the ARF group than in the AonCRF or CRF groups (42 [31–67], 116 [83–119], and 148 [122–210]g VH/g Hb, respectively; p<0.001).
All patients with a degree of acute and potentially, reversible renal failure (ARF and AonCRF) had a carbamylated haemoglobin concentration below 190 g VH/g Hb (sensitivity 100%, positive predictive value 62% for this cut-off).
When the ratio of carbamylated haemoglobin to serum creatinine was calculated, to correct for degree of renal failure, a value of less than 0.2 had a 100% sensitivity and 80% positive predictive value for dividing patients with potentially reversible renal failure from those with CRF.
Measurement of carbamylated haemoglobin was useful in identifying patients with acute and potentially reversible forms of renal failure.
This test could be of clinical value in deciding which patients require urgent referral for further management and treatment in hospitals without specialist nephrological care.
Introduction
Urea is one of many metabolites that accumulates in renal failure.
In solution, urea exists as an equilibrium pair with cyanate.
Isocyanic acid, the reactive form of cyanate, reacts with amino groups to form a stable compound and with sulphydryl groups to produce an adduct that is unstable to acid hydrolysis.
The terminal valine group of both alpha and beta haemoglobin chains can thus undergo carbamylation by this nonenzymatic post-translational chemical process.
The degree of carbamylation can be assessed by measuring the amount of valine hydantoin (VH) formed after acid hydrolysis of haemoglobin, and a correlation between stable renal function, as determined by blood urea concentration, and the amount of carbamylated haemoglobin has been reported.
Carbamylated haemoglobin correlates with the time-averaged urea concentration in patients receiving regular haemodialysis and measurement of this substrate may be of use in assessing the adequacy of renal-replacement therapies.
After successful renal transplantation carbamylated haemoglobin concentrations decline with improving renal function and increased erythropoiesis.
The time course of the in-vivo reaction between isocyanic acid and terminal valine residues of the haemoglobin chains is unknown, but measurement of carbamylated haemoglobin may give an indication of the degree of uraemia in the preceding weeks.
This measurement could be of clinical importance in assessment of a patient referred with impaired renal function, by helping to determine whether the deterioration in renal function was acute (ARF), acute on chronic (AonCRF), or part of stable-chronic renal failure (CRF).
Patients with ARF due to medical causes, such as rapidly progressive nephritis, should be transferred quickly to the care of a nephrologist so that rapid diagnosis can be made and treatment instigated to obtain the maximum possible renal recovery, whereas such urgent transfer would be inappropriate in cases of CRF.
Glycosylated haemoglobin is also a product of a nonenzymatic post-translational reaction, and its measurement has been of use in patients with diabetes mellitus as an indicator of the degree of hyperglycaemia during the preceding 8 weeks.
To discover the value of measuring carbamylated haemoglobin in assessing rate of deterioration in renal function, we studied new patients with a serum creatinine in excess of 500 mol/L referred to our renal unit.
Patients and methods
Patients
42 consecutive patients referred to the regional renal unit for management of acute renal failure (serum creatinine >500 mol/L) were enrolled into the study, which had been approved by the local medical ethics committee.
None of the patients studied had received a blood transfusion in the 3 months before admission to the renal unit, and none had a history of diabetes mellitus.
In every case, blood was taken on admission for determination of carbamylated haemoglobin.
Patients were later divided into two groups — those with ARF (20 patients) and those with AonCRF (22 patients)— based on relevant past medical history and knowledge of previous blood creatinine measurements, renal histology obtained at percutaneous biopsy or necropsy, renal size determined with ultrasound or computed tomography scanning, and, finally, on the eventual diagnosis and clinical outcome.
Blood samples were also taken from 24 patients who were attending the general nephrology clinics and had a similar degree of CRF. 12 (50%) patients in the CRF group were receiving phosphate binders and 9 (38%) were receiving sodium bicarbonate, but none had been treated with erythropoietin.
All patients in the CRF group had been advised to take a low-protein diet (0.8 g protein/kg per day).
Characteristics of the three patient groups are shown in table I. The causes of renal failure in the ARF group were acute tubular necrosis in 15 (75%) patients, haemolytic-uraemic syndrome in 2 (10%), rapidly progressive nephritis in 2 (10%), and renal-vein thrombosis in 1 patient.
In the AonCRF group, acute deterioration was secondary to fluid-volume changes in 8 (36%) patients, infection in 5 (23%), uncontrolled hypertension in 3 (14%), and drug administration in 2 (9%) patients.
The underlying renal diseases in this group were glomerulonephritis in 11 (50%) patients, obstructive nephropathy in 4 (18%), and 3 (13%) cases each of hypertensive nephrosclerosis and chronic interstitial nephritis, and 1 patient with bilateral renal-artery stenosis.
The CRF group comprised 12 (50%) patients with glomerulonephritis, 5 (21%) with chronic interstitial nephritis, 3 (13%) with hypertensive nephrosclerosis, 1 case each of obstructive uropathy and renal artery stenosis, and 2 (8%) patients in whom no definitive diagnosis could be made.
Blood-sample analysis
Blood samples taken on admission were analysed routinely for haemoglobin (STKR analyser, Coulter Electronics, Luton, UK), electrolytes, calcium, phosphate, and uric acid (SMAC III analyser, Bayer, Basingstoke, UK).
For estimation of carbamylated haemoglobin, red blood cells were separated and washed, and then hydrolysed with a mixture of concentrated hydrochloric and acetic acids.
Carbamyl valine thus released immediately and quantitatively converted spontaneously to VH, which was extracted and quantitated by high-performance liquid chromatography.
Chromatography was done under isocratic conditions with a 1050 series liquid chromatograph (Hewlett Packard, Winnersh, UK) and an Apex 2 (bonded octadecyl silica) analytical column (Jones Chromatography, Hengoed, UK).
The between assay coefficients of variation were 11% at 0.25 g/L VH (equivalent to 28 g VH/g haemoglobin [VH/g Hb]in normal subjects), and 10.5% at 0.91 g/L VH (equivalent to 113 g VH/g Hb in patients with renal failure).
The sensitivity, specificity, and positive predictive value of the use of carbamylated haemoglobin as a biochemical discriminant in determining the time course and degree of uraemia were calculated.
Statistical analysis
The Mann Whitney U test and the chi square test, with Yates' correction if appropriate, were used to analyse results.
Statistical significance was taken to be 0.05.
Results are expressed as medians (interquartile ranges [IQRs]), unless otherwise stated.
Results
Patients in all three groups had reduced median values of haemoglobin on the day of admission, but there were no differences between the groups (table II).
Bicarbonate concentration was reduced and urea, creatinine, phosphate, and urate were increased in all groups, but there were no significant differences between groups for these values.
Serum calcium (corrected to an albumin concentration of 40 g/L) was at the lower end of the normal reference range in all three patient groups (table II).
AonCRF patients tended to be older than those with ARF or CRF (table I).
The AonCRF group had a higher blood urea on presentation than the CRF group, but there were no other significant differences in routine haematological or biochemical results among the groups.
Carbamylated haemoglobin concentration was greatest in CRF patients, intermediate in patients with AonCRF, and least in the ARF group (fig 1).
However, even patients with ARF had a greater median carbamylated haemoglobin concentration than 20 subjects with normal renal function.
Patients with ARF were subdivided according to whether their duration of renal failure was known to be greater than or less than 10 days.
Median carbamylated haemoglobin concentration was greatest in ARF patients with the longer duration of renal failure (29 [27–35]vs 72 [60–83]g VH/g Hb; p<0.01)(fig 2).
Previous reports and our own experience have shown that carbamylated haemoglobin concentrations in patients with chronic renal failure are related to degree of renal impairment as measured by serum urea or creatinine (r=0.7, p<0.001 [Spearman's rank correlation test], for carbamylated haemoglobin [g VH/g Hb]vs serum creatinine [mol/L]in 70 patients with CRF attending general nephrology clinics).
To correct for the wide range of renal impairment in the patients studied, the ratios of carbamylated haemoglobin with both serum urea and creatinine concentrations were calculated.
The three patient groups had significantly different carbamylated haemoglobin/urea ratios (fig 3).
To assess the usefulness of carbamylated haemoglobin in separating patients into those with potentially reversible elements of renal failure (ARF and AonCRF groups), CRF, ARF, or chronic renal impairment (AonCRF and CRF groups), we calculated the percentage of patients in each of these groups with carbamylated haemoglobin concentrations, carbamylated haemoglobin/urea ratios, and carbamylated haemoglobin/creatinine ratios below varying cut-off values (table III).
All patients with a potentially reversible element of renal failure had values of carbamylated haemoglobin below a cut-off of 190 gVH/g Hb (ie, sensitivity 100%); thus, this cut-off had a specificity of 33% and positive predictive value of 62% for distinguishing reversible renal failure from CRF.
Similarly, the cut-off value of 5.5 for the carbamylated haemoglobin/serum urea patients had a sensitivity of 100%, specificity of 38%, and positive predictive value of 73% for separating reversible renal failure from CRF, and a cut-off value of 0.2 for the carbamylated haemoglobin/serum creatinine ratio had a sensitivity of 100%, specificity of 58%, and positive predictive value of 80% for separating the two conditions.
Patients with true ARF could be separated from those with chronic renal impairment by having values of carbamylated haemoglobin below 95 g VH/g Hb (sensitivity 100%, specificity 80%, positive predictive value 69%).
Similarly, a cut-off value of 2.6 for the carbamylated haemoglobin/serum urea ratio had a sensitivity of 100%, specificity of 76%, and positive predictive value of 65% for distinguishing the two conditions, and a cut-off value of 0.15 for the carbamylated haemoglobin/serum creatinine ratio had a sensitivity of 100%, specificity of 73%, and positive predictive value of 63%.
Discussion
The differential diagnosis of ARF, end-stage renal disease, and acute renal deterioration on a background of chronic renal disease can be difficult.
However, in some patients such as those with rapidly progressive nephritis, early diagnosis and treatment are essential for effective renal recovery.
When a patient is seen for the first time, a single measurement of serum urea or creatinine alone is not helpful in differentiating the time course of renal failure.
Daily sequential measurements are useful in determining the rate of increase in plasma creatinine and help separate patients with ARF or CRF.
It has been widely held that normochromic anaemia is indicative of chronic renal failure, but recent epidemiological evidence supports our findings that anaemia is also commonly present in patients with both ARF and AonCRF, and is, therefore, not a helpful discriminant.
Similarly, hyperphosphataemia, hypocalcaemia, and metabolic acidosis have been described as more common in patients with CRF.
However, we found that routine biochemical measurements taken on admission were not helpful in discriminating patients into those with ARF, AonCRF, or CRF.
Metabolic acidosis, hypocalcaemia, and phosphate retention have been reported in patients with ARF.
In our series, some patients in the CRF group were in receipt of calcium and bicarbonate supplements, and these supplements may have reduced any difference in serum calcium and bicarbonate between CRF patients and those with some potentially reversible element to their renal failure.
Emergency renal ultrasound examination has been advocated as a way of separating ARF from CRF, because small kidneys usually indicate CRF.
The reliability of such ultrasound measurements depends on the experience of the operator, and in some patients with end-stage renal failure caused by diabetes mellitus, amyloid, myeloma, or tumour infiltration renal size can be well maintained.
In our study, renal size was adequate for ultrasound-guided biopsy in 15 (68%) patients with AonCRF and was, therefore, not a good discriminator between ARF and AonCRF.
The slightly increased serum urea concentration in the AonCRF group compared with the CRF group may reflect the lower protein diet in the CRF group and sepsis and dehydration as causes of renal failure in the AonCRF group.
Blood transfusion, reticulocytosis, and reduced red blood cell survival could be anticipated to cause reduction in carbamylated haemoglobin.
We excluded recently transfused patients and no patients had increased reticulocyte counts.
Shortened red cell survival in patients with uraemia may have caused a reduction in carbamylated haemoglobin in all study groups.
Measurement of carbamylated haemoglobin was useful in differentiating patients with ARF from those with CRF.
This measurement may, therefore, be of clinical value in the assessment of a patient with impaired renal function when seen for the first time.
By expressing the result as a ratio of carbamylated haemoglobin to serum urea or creatinine the effect of variations between groups in renal function was corrected and statistical power for differences between groups was increased.
The fact that carbamylated haemoglobin concentration was partly dependent on duration of renal failure is emphasised by the observation that the concentration was lowest in patients with ARF of 10 days or less duration.
We deliberately chose a cut-off point for carbamylated haemoglobin that included all patients with a potentially reversible element to their renal failure, since these patients were in greatest need of rapid diagnosis and treatment.
However, by taking such a high cut-off point the specificity and positive predictive value of the test were reduced.
Although in practice this means that some patients with chronic stable renal failure would have been referred for rapid investigation and diagnosis, we believe that this is preferable to taking a lower cut-off for carbamylated haemoglobin, such as 125 g VH/g Hb which would reduce sensitivity to 80% but increase specificity to 75%, since this would exclude some patients with a potentially reversible element to their renal failure that may not be so readily reversible if investigation and appropriate management were delayed.
Thus, measurement of carbamylated haemoglobin can be used to ensure that patients with a potentially reversible element to their renal failure are referred without delay to a specialist centre for further investigation and treatment.
The specificity of the test is sufficient to prevent emergency transfer of large numbers of CRF patients, and, if developments in assay technique were to produce an automated test, measurement of carbamylated haemoglobin would be of help in management of patients admitted with renal failure to hospitals without specialist nephrological facilities.
Accelerated detection with prospectivesurveillance for cutaneous malignant melanomain high-risk groups
In 1983 a classification scheme was proposed for patients with atypical naevi, according to their personal and family history of melanoma and atypical naevi.
To assess the predictive value of these features we undertook prospective surveillance of patients at high risk of primary melanoma.
We followed up 116 patients each with 3 or more clinically atypical naevi for at least 5 years.
Patients are examined and naevi are photographed every 3–6 months; lesions showing disturbing change are excised for histopathology.
Among 85 patients with no personal or family history of melanoma, 5 invasive (level 2 or deeper) melanomas developed during 583 person-years of follow up.
The expected number of invasive melanomas in this population would be 0.054; the increased risk is significant (p<0.001; relative risk 92 [95% CI 30–216]).
There was a similarly increased risk of new melanoma also among 24 patients with atypical naevi plus a history of previous melanoma (observed 2, expected 0.022, p<0.001; relative risk 91 [11–328]).
By comparison, no second melanoma developed among 25 patients with previous melanoma but a normal naevus pattern during 213 person-years of similarly intensive follow-up.
The risk of melanoma was highest among 7 patients with atypical naevi and a family history of melanoma (observed 6, expected 0.009, p<0.001; relative risk 444 [121–1138]).
The median thickness of surveillance-detected melanomas was 0.75 mm (range 0.40–1.05 mm) in this group.
This study shows the value of clinical follow-up of high-risk patients to detect early thin melanomas.
Introduction
In 1983 Kraemer and colleagues suggested classification of patients with atypical naevi into four categories (A–D), based on the personal or family history of melanoma and atypical naevi (table I).
They emphasised the need for long-term follow-up to establish the clinical relevance of the proposed subdivisions.
A 1992 National Institutes of Health consensus conference on diagnosis and treatment of early melanoma re-emphasised that the relative risk of melanoma developing in patients in the suggested subgroups was still poorly established.
Such data are clearly needed to show the value, if any, of the classification scheme in assigning priority for screening and surveillance programmes aimed at detecting early, curable invasive melanoma.
Since 1983 it has been established that the strongest risk factor for the development of malignant melanoma is the presence of a large number of benign naevi.
Goldgar et al have shown that total naevus density, or percentage of body surface area covered by naevi, is a measure that correlates well with melanoma risk.
At present, some centres use photographic records to follow patients with clinically atypical naevi, on the assumptions that only lesions that show evidence of progressive change need to be excised and that such change can be clinically detected at a time when evolving melanomas are thin and curable by local excision.
The method is used mainly on a research basis, and validation of the assumptions made is necessary before widespread adoption of such photographic surveillance in routine clinical use can be advocated.
We report photographic follow-up of patients with 3 or more clinically atypical naevi and 20 or more benign naevi.
Patients and methods
We studied 116 patients referred to the pigmented lesion clinic at the Western Infirmary, Glasgow, because of concern over an atypical naevus pattern.
All have been followed up for at least 5 years.
We classified our patients in three groups.
Group 1 consisted of 85 patients who had 3 or more clinically atypical naevi (>5 mm in diameter and irregular edge, irregular pigment, or inflammation), more than 20 benign naevi, and no personal or family history of melanoma (Kraemer et al 's categories A and B).
We could not classify patients into Kraemer et al 's group A or B, since this grouping depends on relatives' naevus status; not all patients could bring first-degree relatives for examination, and their opinions of relatives' naevus status were unreliable in those who could be examined.
Group 2 consisted of 24 patients; each had an abnormal naevus pattern as defined above and had already had 1 invasive primary melanoma (Kraemer et al 's category C).
They were compared with 25 patients who had had primary melanomas but had a normal naevus pattern.
Group 3 consisted of 7 patients from six separate families; each of these had an atypical naevus pattern and melanoma in a first-degree relative (Kraemer et al 's category D).
2 had already had one primary melanoma diagnosed before referral to the surveillance clinic.
At the initial visit patients were examined in good light, and naevi over the whole body were counted.
General photographs (‘body maps’) were taken with a 35 mm camera and Kodachrome colour slide film.
Close-ups of atypical naevi were taken at approximately actual size with an adjacent 1 cm scale.
At review appointments, every 3–6 months according to the degree of concern, the colour transparencies were projected on a Caramate projector and the current and previous patterns were compared.
Excision biopsy and histology were considered for naevi showing change or new naevi not present at the time of the previous photographic session.
All patients were encouraged to become familiar with their own mole pattern and to return to the clinic before the next regular appointment if they observed any change.
Patients with naevi mainly on the back, a common pattern in men, were encouraged to ask a family member to inspect this area.
Expected numbers of melanomas in each group were based on incidence rates in the west of Scotland, standardised for age (5-year groups), sex, and time of observation.
Standardisation with time was necessary because the incidence of melanoma is rising steadily in Scotland.
Observed numbers of melanomas were treated as data from a Poisson distribution with mean equal to the number expected.
Confidence intervals were calculated by the exact method.
Results
Group 1: Category A/B patients
These 85 patients have been followed up for a total of 583 person-years.
The mean age is 24 (range 13–49) years.
During this time 0.054 melanomas would be expected (from the rate in the local population) among these patients, but 5 patients (3 women and 2 men aged 18–28 years) have each developed 1 invasive primary malignant melanoma (level 2 or deeper)(table II).
Tumour thickness ranges from 0.3 mm to 2.0 mm.
All were detected by comparison of the lesion at the follow-up visit with the previous photographic record.
Only 1 melanoma was suspected by the patient herself, and no patient had returned before the scheduled appointment.
4 patients also developed level-1 or in-situ melanomas; 3 of these patients have not yet developed any invasive melanomas but the other, patient 5, has also had an invasive melanoma.
For invasive level-2 or deeper lesions only, the difference between the observed and expected numbers of melanomas is highly significant (p<0.001); the relative risk of invasive malignant melanoma in this group compared with the local population is 92 (30–216).
Group 2: category C patients
The 24 patients in this group with an abnormal naevus pattern have been followed up for 178 person-years.
The mean age at the start of surveillance was 35 (range 6–63 years).
2 patients in this group each developed 2 new invasive primary melanomas, and another patient developed an in-situ melanoma.
Over this time, 0.022 melanomas would have been expected in the local population.
Again, the ratio of observed to expected numbers is highly significant (p<0.001); the relative risk is 91 (11–328).
Our calculations took into account only the second primary invasive melanoma in each case of primary melanoma.
The 25 patients (11 men, 14 women; mean age 54 [28–79]years) included for comparison who had neither excessive numbers of normal naevi nor atypical naevi have been followed for 213 person-years.
No second primary melanomas were detected in this group (expected 0.026).
Group 3: category D patients
7 members of six families with a history of melanoma in at least 1 first-degree relative have been been followed for 56 person-years.
All were selected for surveillance on the basis of an abnormal naevus pattern.
6 invasive level-2 or deeper melanomas have developed in 4 patients (expected 0.009); 2 have also developed level-1 or in-situ lesions.
The thickness of the surveillance-detected melanomas ranged from 0.4 mm to 1.05 mm, whereas the two index melanomas diagnosed before referral to the surveillance clinic were 2.6 mm and 2.8 mm thick at detection.
The increase of observed over expected numbers was significant (p<0.001), with a relative risk of 444 (121–1138).
All patients
If observed and expected numbers of level-2 or deeper melanomas in categories A–C are combined (observed 7, expected 0.076) the relative risk is 92 (37–190).
The difference in relative risk for categories A–C and category D is not statistically significant (p=0.1).
Discussion
The ideal study to confirm the hypothesis of Kraemer et al would have included a control group drawn randomly from the population, and followed in the same intensive manner as the study group.
We could not include such a group because of practical difficulties and limited medical manpower; also there would be inevitable concern that individuals agreeing to be part of such a control group would be self-selecting members of a group with additional risk factors.
There is little evidence that level-2 or deeper lesions are non-progressive or undergo total spontaneous regression.
We therefore believe that, provided level-1 lesions are not included in the calculation (because of uncertainty about whether they are progressive), our findings support the hypothesis that patients in categories A–D are at increased risk of developing melanoma.
For category A/B and category C patients the extent of this risk is similar, but for category D patients it is substantially higher.
Thus, the presence of a family history of melanoma in a first-degree relative seems to identify a biologically distinct subset.
We speculate that individuals in category C have the same genotypic characteristics as those in categories A and B but are identified at a slightly later stage in disease progression.
This idea is consistent with the difference in age (mean 24 vs 35 years) between our groups 1 and 2 (A/B vs C).
There is a further point of interest in the comparative subgroup of group 2 (1 previous primary melanoma and a normal naevus pattern).
No second primary melanomas were seen in this group, whereas 0.026 would have been expected.
This finding confirms those from studies of multiple primary melanomas that large numbers of naevi are a risk factor for second and subsequent primary tumours.
Among patients with previous melanomas the difference in age between those with atypical and normal naevus patterns (35 [16–63]vs 54 [28–79]years) is significant (p<0.01); this finding suggests that patients with an atypical naevus pattern have either genetic or early life environmental factors that accelerate the development of melanoma.
It is tempting to speculate that this factor may be early childhood sun exposure, which not only causes development of large numbers of naevi but also acts as an initiating agent for later development of melanoma.
Patients and their relatives in category D are clearly at greatly increased risk of a first or subsequent primary tumour and need life-time surveillance.
The lack of a significant difference in risk between combined categories A–C and category D may reflect the small numbers and thus wide confidence intervals in group D.
Photographic recording of naevi and use of these photographs to detect change suggesting development of melanoma has been essential in this study.
Only one of the surveillance-detected melanomas was suspected by the patient.
The value of surveillance is also supported by the thickness of the tumours at detection; only 2 were thicker than 1.5 mm and both of these, on review of photographs, showed a greater degree of inflammation at 6 months before clinical suspicion and excision than any of the thinner melanomas.
Although we and others have previously described inflammation as a clinical characteristic of benign but atypical naevi, we would recommend caution in dismissing persistently inflamed melanocytic lesions as benign, and suggest excision.
In-situ or level-1 melanomas were not included in calculations of relative risk, since it is not clear whether all in-situ lesions progress to invasive melanomas.
The lack of an appropriate animal model makes it impossible to investigate this question further at present.
If even a proportion of in-situ lesions are progressive, our calculations are an underestimate of the magnitude of the relative risk.
The incidence of melanoma was 709 per 100,000 in category A–C individuals and 4429 per 100,000 in category D; the rates in the general Scottish population range from 5–10 per 100,000 for the relevant age groups.
By comparison, current incidence rates for breast carcinoma and cervical cancer, in the age ranges that merit specifically funded screening programmes, are 224 and 22.4 per 100,000.
There is, therefore, a strong case for funding of melanoma surveillance programmes or clinics aimed at facilitating early diagnosis for individuals with atypical naevi, especially given that, despite intensive research, there have been few advances in the management of melanoma that has spread beyond the primary site.
SHORT REPORTS
T-cell-mediated response inDupuytren's disease
The cause of Dupuytren's disease is unknown, but inflammatory cells might have a role.
Enzymatic digestion of diseased tissue permits identification and immunofluorescent labelling of a cell subset displaying inflammatory cell morphology.
Cytofluorimetry of this cell population demonstrated the presence of CD3-positive lymphocytes and expression of major histocompatibility complex (MHC) class II proteins.
These results raise the possibility that Dupuytren's disease is a T-cell-mediated autoimmune disorder.
The development of medical treatment on this basis may reduce the need for surgery, with its associated morbidity and high recurrence rates.
Individuals with Dupuytren's disease have a genetically-determined predisposition to the condition.
The precise aetiology remains unclear but fibroblasts and myofibroblasts are thought to have a central role, and superoxide free radicals might be the stimulus to myofibroblast proliferation.
The condition may also occur in association with various other medical disorders.
The prevalence and clinical significance of inflammatory cells in diseased tissue is controversial.
Baird et al have demonstrated cytokine expression in tissue from Dupuytren patients, and certain inflammatory cells are known to be potential sources of these intercellular signalling molecules.
We have examined inflammatory cells in tissue from patient's with Dupuytren's disease by flow cytometry of digested specimens to reduce sampling errors in histological examination of this heterogeneous tissue.
Specimens of subcutaneous tissue were obtained from 13 male and 1 female (aged 43–77 years, mean 63) Dupuytren's patients undergoing palmar fasciectomy.
Control tissue was obtained by combining palmar fascia specimens excised from 3 male and 3 female (aged 38–67, mean 58) patients having carpal tunnel decompression.
These specimens were pooled to obtain adequate cell numbers for analysis.
All operations were done under regional anaesthesia with exsanguination of the limb.
Tissue was diced into 2 mm fragments and digested at 37C in collagenase 0.1%/DNAse 0.01% for 3 h followed by trypsin 0.05%/edetic acid 0.02% for 1 h.
Filtration yielded a cell suspension in which residual enzyme acivity was neutralised by washing and resuspending cells in Dulbecco's modified eagle medium containing 10% fetal calf serum (Gibco).
Cells were plated out at a maximum initial density of 510 cells per dish and incubated overnight to allow separation of adherent cells (fibroblasts and macrophages) from inflammatory cells.
This procedure also allowed regeneration of cell surface markers depleted by enzymatic activity.
Non-adherent cells were harvested and samples of 2.0–3.010 cells were resuspended in 200 l medium, then labelled with fluorescent monoclonal antibodies designed for flow cytometry (Dako).
The panel of paired fluorescent antibody combinations used and the inflammatory cell subsets that they recognise were: CD45/14 (pan-leucocyte/monocyte); CD3/19 (pan T-cell/pan B-cell); CD3/4 (pan T-cell/T-helper cell); CD3/8 (pan T/cytotoxic T-cell); HLA-DR/CD3 (activated cells/pan-T); and CD16+56 (natural killer [NK]cells).
Cells were labelled at room temperature for 10 min with 10 l of neat antibody, washed with phosphate-buffered saline, fixed with 1% paraformaldehyde, and fluorescence was measured with the Lysys II programme on a Becton Dickinson FACScan flow cytometer.
Dedicated software permitted statistical analysis of the data obtained.
Cytospin preparations of both adherent and non-adherent cells were stained with Giemsa's stain for examination by light microscpy.
Light microscopy of adherent cells showed a homogeneous subpopulation of relatively large cells with foamy cytoplasm.
By striking contrast the non-adherent subpopulation was heterogeneous, with many small, darkly-stained cells having the appearance of lymphocytes.
The table shows the results from flow cytometry of non-adherent cells.
15–30% (mean 25%) of all Dupuytren cells in the lymphocyte gate were CD3-positive T-lymphocytes, compared with less than 0.5% of non-adherent cells from normal fascia.
HLA-DR antigen was detected in 15–41% (mean 27%) of all gated cells from Dupuytren's patients, compared with 1.2% of controls.
All other markers were expressed infrequently on Dupuytren cells, indicating a low prevalence of CD4 T-helper cells (1.6–4.3%) and CD8 cytotoxic T-cells (2.7–8.0%).
Not shown in the table (because of uniformly low-labelling frequency) are CD16+56 NK cells (<1–1.1%), CD19 B-lymphocytes (<1–5.0%), and CD14 monocytes (<1%).
Very low frequencies of antibody labelling were also demonstrated by the pooled control fascia cells (less than 1.5% in each case).
Tissue in Dupuytren's disease contains substantial numbers of CD3-positive T-cells, suggesting that they are important mediators in the pathogenesis of this condition.
The low prevalence of CD4, CD8, and CD16+56 antigens indicates that most of these cells may belong to a further subset of T-lymphocytes, such as the recently described population of ‘double-negative’ T-cells found in epidermis; this population needs to be defined.
The increased frequency of HLA-DR-positive cells in Dupuytren's disease indicates expression of major histocompatibility complex (MHC) class II molecules, and the potential ability of these cells to present antigen to T-lymphocytes.
HLA-DR is generally recognised as an indicator of cell activation, and activated T-cells also release cytokines that upregulate expression of MHC class II proteins encoded by genes of the HLA-DR locus.
These findings are consistent with the inappropriate expression of fibroblast-stimulating cytokines reported in this disorder.
Although T-lymphocytes probably act as mediators in the pathogenesis of Dupuytren's disease, we do not know whether they act as regulator or effector cells; nor has any specific antigen been identified.
The precise role of the T-cells requires definition, and further studies are underway to characterise the other cells present in diseased issue (eg, macrophages and their interactions with lymphocytes).
Dupuytren's disease might be triggered by the interaction of environmental factors with the primary genetic defect.
One hypothesis consistent with our findings is that the defect might occur in genes coding either for MHC proteins or for T-cell receptor proteins.
T-lymphocytes are certainly implicated in the pathogenesis of autoimmune disorders, several of which are associated with Dupuytren's disease.
The HLA-antigen status of Dupuytren's patients has been recorded, and at least one possible pattern of expression has emerged.
Type I diabetes is associated with HLA-DR3 and DR4, and up to 30% of diabetics also have Dupuytren's disease.
Conversely, there is a negative correlation between Dupuytren's disease and rheumatoid arthritis, a condition known to have strong association with HLA-DR4.
Dupuytren's disease arises in 36% of patients infected with HIV, and subcuticular fibrosis nodules with histological appearances similar to Dupuytren's nodules have been reported in simian acquired immune deficiency syndrome.
In alcoholic hepatic cirrhosis, T-cells are thought to support the production of cytokines by liver macrophages and these factors then regulate the fibrotic process.
Finally, the onset of Dupuytren's disease following injury in genetically-susceptible individuals might be related to the large pool of activated T-cells and macrophages present in the wound.
The subcutaneous nature of the disorder allows its natural history to be followed with ease; if we can confirm that the disease is caused by cell-mediated immune mechanisms, medical therapy might be developed as an adjunct or alternative to surgery, allowing a more conservative surgical approach with the prospect of reduced postoperative morbidity and recurrence.
Coelocentesis: a new technique for early prenatal diagnosis
Chorionic villus sampling and amniocentesis have disadvantages.
In 100 women undergoing termination of pregnancy, coelomic fluid was successfully aspirated in 96% of cases at 6–10 weeks' gestation, 42% at 11, and 10% at 12 weeks.
Cytogenetic analysis always failed with coelomic fluid, but fetal sexing was always successful with fluorescence in-situ hybridisation and polymerase chain reaction, and the results agreed with those obtained from chorionic villi and amniotic fluid in all cases.
Coelocentesis may be suitable for prenatal diagnosis in the first trimester.
Prenatal diagnosis in the first trimester provides early reassurance to most mothers that their fetus is not affected by the genetic disorder under investigation.
To those with an affected fetus, early diagnosis provides the option of early termination.
Embryonic tissues suitable for early prenatal diagnosis are obtained by chorionic villus sampling (CVS) or early amniocentesis.
However, CVS may be associated with a higher risk of pregnancy loss compared with second-trimester amniocentesis.
Furthermore, the possible association between CVS at less than 10 weeks and fetal limb reduction defects is likely to confine its application to pregnancies beyond 10 weeks., Amniocentesis can be done from 10 weeks onwards, before which the number of viable cells is small and there is a high failure rate.
The safety and diagnostic accuracy of early amniocentesis remains to be determined.
We report a new procedure, coelocentesis, that is best done before 10 weeks.
During the first 12 weeks of pregnancy the amniotic sac is surrounded by coelomic fluid in the extraembryonic coelomic cavity, which is a derivative of the extra-embryonic mesoderm.
In 100 women with normal singleton pregnancies at 6–12 weeks' gestation, written consent was obtained for coelocentesis and amniocentesis immediately before elective termination for psychological indications.
The protocol was approved by the Research Ethics Committee of King's College Hospital.
After the administration of general anaesthesia, transvaginal ultrasonography was used to confirm gestational age (crown-rump length) and identify the placenta and amniotic membrane.
A 20 gauge needle was introduced transvaginally into the coelomic cavity under continuous ultrasound monitoring and the fluid was aspirated.
In all cases the needle tip was located.
The procedure was not associated with alterations in fetal heart rate and there was no evidence of haemorrhage into the coelomic cavity.
Amniocentesis was then done with a new needle.
Subsequently, suction termination was carried out and placental tissue was collected.
Coelomic fluid was successfully aspirated in nearly all cases up to 10 weeks' gestation, but the success rate fell at later weeks (table I).
The mean volume of aspirated fluid doubled between 6 and 10 weeks, and then fell.
For amniocentesis, the rate of successful sampling increased with advancing gestation from 31% (4/13) at 7 weeks to 77% (10/13) at 8 weeks and 100% (68/68) at 9–12 weeks.
Coelomic fluid was always bright yellow, while amniotic fluid was clear.
The nature of the samples was confirmed biochemically.
In 30 cases, sampled at 8–10 weeks' gestation, an attempt was made to determine fetal sex by analysing amniotic fluid, coelomic fluid, and placental tissue.
10 cases had standard cytogenetic analysis.
The second 10 cases had polymerase chain reaction (PCR) with Y centromeric primers.
The final product was run in 1% agarose gel and stained with ethidium bromide.
The remaining 10 had fluorescence in-situ hybridisation (FISH) with alpha satellite repeat probes for X and Y chromosomes (Cytocell, Lewknor, Oxfordshire).
The slides were examined by fluorescence microscopy without the need for signal amplification.
Cytogenetic analysis was successful in all placental samples.
However, cells failed to culture in half the amniotic fluid and in all coelomic fluid samples (table II).
FISH and PCR were successful in all samples from the three compartments, with concordance in fetal sex prediction.
We have demonstrated the feasibility of coelocentesis and its potential for prenatal diagnosis before 10 weeks' gestation.
Although the risks of coelocentesis in continuing pregnancies remain to be assessed, the procedure may be safer than early amniocentesis, which involves the removal of a large proportion of amniotic fluid with a potential adverse effect on fetal pulmonary development.
In addition, coelocentesis does not involve puncture of the amniotic membrane and therefore the risk of direct trauma to the embryo or chronic amniotic fluid leakage would be expected to be lower than with amniocentesis.
Coelocentesis may also be preferable to CVS for prenatal diagnosis.
Coelomic cells are derived from the extra-embryonic mesoderm, and thus there may be fewer problems with pseudomosaicism than are encountered with trophoblastic cell preparations.
Additionally, coelocentesis does not involve puncture of the definitive placenta, reducing the possibility of placental vascular damage and consequent fetal abnormality.
EDITORIAL
Ticket to dignity beyond a brick wall
Picture a town in Rwanda where 70% of hospital beds are occupied by patients with HIV-related diseases and where the number of AIDS-associated orphans increases inexorably.
Picture a teenage girl in Morocco for whom premarital loss of virginity is culturally intolerable and who faces the ‘choice’, under male duress, of tolerating anal intercourse, or of submitting to vaginal penetration knowing that she will thereby have to leave home for a life of prostitution; she may even know that both are related to acquisition of HIV.
Picture a once decent apartment building in the Bronx, New York, a part of the city where poverty, drug abuse, AIDS, and prostitution abound, and witness over a few years how it is transformed into a miserable squat and ultimately bulldozed to make way for a new police station.
Picture an area in war-torn former Yugoslavia with a population suffering from malnutrition, lice, scabies, tuberculosis, water and food borne illness, and hepatitis B; here rats flourish on piles of rubbish, dogs roam the streets notwithstanding the endemicity of rabies, and there is an influx of United Nations troops from countries in Africa and Asia with a high prevalence of HIV.
All the ingredients for an AIDS epidemic that has yet to begin.
And then acknowledge, if you need any convincing, the sheer impossibility of considering AIDS, public health, and human rights in isolation.
All these poignant images and many more were presented during the course of a colloquium held at the Fondation Marcel Mérieux near Annecy, France, last weekend.
(Full proceedings of this symposium, the third of its kind, will be published.)
Human rights issues, as illustrated by AIDS, have to be seen in a wider context and, conversely, one cannot contemplate the totality of public health without the human rights component.
While everyone agrees that far more could, and should, have been done to contain the AIDS epidemic, the key issue now is that positive steps can still be taken.
Despite the wisdom of giving priority to prevention in low-prevalence areas, it is absurd, as Peter Piot (WHO, Geneva) noted, to abandon countries such as those of subSaharan Africa with high rates of infection.
Even in Uganda, for example, there are large and often unexplained differences in HIV rates in neighbouring villages.
Equally, many countries would do well to acknowledge the honesty of subSaharan nations in being largely open about the nature and scale of the epidemic within their borders.
Moreover, the contribution of developing countries to research is considerable and seldom mentioned.
What is global about HIV when there are so many factors specific for countries, even for small regions within a country and for groups of individuals?
The central insight, as Jonathan Mann (Harvard) pointed out, and as is emphasised in the report of the Global AIDS Policy Coalition, is that discrimination by society lies at the root of individual and community vulnerability to AIDS.
Another key point made in that report, and raised independently by many participants at the colloquium, was that the capacity of people to learn about and respond to HIV can be improved without resorting to the aid of high technology.
Discrimination against various groups in society was raised over and over again.
The commonality in this respect of homosexuals in Iran, intravenous drug abusers in New York, and Nepalese women with ‘India disease’ as a result of cross-border prostitution is readily apparent.
Yet why should drug users in Bangkok be able to hold down steady jobs and be part of mainstream society whereas their counterparts in the USA who seek  clean syringes are promptly imprisoned?
There, surely, is one small example of how rational global thinking to frame intravenous drug use as a public health and not a moral issue might influence local strategy.
No-one can ignore the way in which women feature prominently in every aspect of the AIDS epidemic.
Their inherent vulnerability in many societies — a condom often costs more than a prostitute — is often encouraged in political manipulations of religious precepts.
Their overall inferior education is held to be a key factor world wide, and has been emphasised by data produced by the World Bank.
The high-tech aspects of AIDS will and must continue.
There is a profound need for better understanding of the virus and its variants, of its variable transmission patterns, and for new drugs.
Vaccines will undoubtedly come as a practical development, and with efficacy against mucosal transmission.
The intranasal route of vaccine administration is being explored.
But even then simple measures such as condoms, clean needles, bleach, and spermicides will be required.
These techniques are not freely available everywhere, but with concerted effort they could be.
When we talk of human rights in relation to AIDS, these are ‘our’ rights too, not merely ‘theirs’.
The fundamental issue, June Osborn (University of Michigan) reminded everyone, is human dignity.
And the brick wall?
Margaret Somerville (McGill), fresh from the human rights conference in Vienna, displayed a cartoon — a solid brick wall with a paper thin door of human rights.
AIDS, after all, is far more than an epidemic.
COMMENTARY
COMPLEMENTARY MEDICINE
Scrutinising the alternatives
Complementary medicine is more popular than ever; meanwhile, the attitude of the medical profession ranges from total rejection to uncritical adoption of its techniques.
Some point out that complementary rationales differ from the scientific model of conventional medicine whereas others emphasise the holistic approach or argue that unorthodox views add a missing dimension to today's orthodox medicine.
Terms such as alternative, natural, unorthodox, or unconventional are closely related but not synonymous.
One might tentatively define complementary medicine as diagnostic, therapeutic, and preventive procedures excluded from mainstream medicine; some may have passed the test of time, most are of unproven effectiveness, and all are based on unscientific philosophies.
They include herbalism, acupuncture, homoeopathy, and even massage.
A particular complementary technique may be recognised by orthodox circles in one country but condemned in another; an example is osteopathy.
Moreover, terms such as nutritional therapy mean different things to the orthodox and to the alternative practitioner.
So why is complementary medicine gaining popularity?
There are several possibilities.
Time and empathy are often lacking in the doctor-patient encounter; high-tech medicine sometimes turns patients into numbers labelled with a diagnosis; and since many diseases have become curable, some patients expect a cure for all ailments.
Even if scientifically unsound, complementary medicine is often viewed by patients as more ‘human’.
Treatments are associated with improvement or deterioration for several reasons: the natural history of the disease, the placebo effect, the activity (if any) of the prescribed treatment, and the discontinuation of ill-conceived orthodox treatments.
If one doctor treats condition ‘y’ with therapy ‘x’, he may observe clinical improvement in the absence of any real treatment effect; if this experience repeats itself, he is destined to become an enthusiast for therapy ‘x’.
A more critical colleague might remain sceptical and even become convinced that ‘x’ is sheer quackery.
This is why randomised controlled trials are essential for evaluating the effectiveness of any therapy, complementary or not, and such studies are feasible, even for highly individualised therapies.
Endpoints such as consumer satisfaction or treatment costs, which are sometimes used as substitutes to evaluate alternative approaches, are meaningful only after effectiveness has been established.
Some complementary therapies meet this prerequisite: homoeopathy works in certain situations; manual therapy is effective for low back pain; electroacupuncture relieves symptoms of fibromyalgia; plant products are helpful in various conditions; hydrotherapy can prevent common colds and alleviate the symptoms of varicose veins.
When a complementary therapy is shown to work, further questions arise.
Is it more effective than an orthodox option?
How do the risk-benefit ratios compare?
What are the costs?
Is the philosophy behind the method correct?
Today many physicians admit that complementary medicine has some potential and deserves a fair trial.
Thus the anxiety of the profession is growing.
With standards often being poor, unregulated, and uncontrolled, disaster seems imminent.
We therefore should aim at integration rather than separation from orthodox medicine.
EPILEPSY
Intracerebral microdialysis and seizure initiation
Intracerebral microdialysis began more than twenty years ago when Delgado sealed the end of Gaddum's push-pull cannula with a semipermeable membrane, combined it with a multiple electrode, called it a ‘dialytrode’, and stuck it in a monkey brain.
Subsequent use of intracerebral microdialysis in rodents, cats, and non-human primates has contributed to our understanding of the role of the cerebral microenvironment and of particular neurotransmitters in states of arousal, appetitive and addictive behaviours, movement disorders, epilepsy, cerebral ischaemia, and traumatic injury of brain and spinal cord.
Has this technique an application in human neurosurgery and if so in which condition?
The front runner has long seemed to be head injury, in which a microdialysis probe might readily accompany a pressure monitor and provide a guide to metabolic state.
The opportunity to test microdialysis acutely in man was provided by stereotaxic treatment in parkinsonism but its clinical value in this context is not evident.
Epilepsy has now come in ahead of the field thanks to the ingenuity and determination of During and Spencer at Yale.
Their latest observations (p 1607) are noteworthy in that, in addition to making a strong case for the clinical usefulness of cerebral microdialysis (by aiding identification of the site of seizure origin), they show that the technique contributes to our understanding of the pathophysiology of epilepsy.
In patients with refractory complex partial seizures requiring depth electrode studies, During and Spencer implanted microdialysis probes bilaterally along the hippocampus.
Their measurements of glutamate and gamma-aminobutyric acid (GABA) concentrations in relation to the electrographic onset of spontaneous seizures show an increase in glutamate concentration directly before seizure onset on the epileptogenic side.
This result accords with earlier preliminary findings in man and in laboratory animals.
The non-epileptogenic hippocampus shows a smaller increase in glutamate concentration beginning after seizure onset, and a simultaneous increase in GABA concentration which is larger than that on the epileptogenic side.
Rises in extracellular neurotransmitter concentration may indicate increased synaptic release but they may also depend on non-synaptic release or diminished or ‘reversed’ uptake.
Epileptologists have long speculated as to what governs the transition from the interictal to the ictal state.
The data of During and Spencer provide the clearest evidence yet that enhanced activation of glutamate receptors contributes to the initiation of focal seizure activity.
GABA may act to delay or limit the spread of seizure activity.
Adenosine release probably has a role in terminating seizure activity.
This study does not contribute decisively to the debate about excitotoxic cell death in epilepsy.
From animal experiments we know that activation of N-methyl-D-aspartate receptors contributes to cell death in hippocampal pyramidal neurons during lengthy limbic seizures.
The brief seizures described by During and Spencer would not, however, be expected to give rise to nerve cell death.
Observations of toxicity of glutamate in tissue culture are of uncertain relevance in vivo.
During and Spencer's study differs from earlier ones in which microdialysis was used in that it involved chronic monitoring (up to 16 days) in conscious patients.
Thus the practical and ethical difficulties can be overcome.
At present microdialysis is only justifiable when there are compelling reasons for neurosurgical intervention and when the inclusion of the microdialysis probe does not add appreciably to the risks.
Intracerebral microdialysis has helped us to understand the pathophysiology of ischaemia, central nervous system trauma, and epilepsy in laboratory animals and will have increasing application clinically.
Many factors such as the time and space available in intensive care units will determine how soon such discoveries can be translated into routine procedures for investigating and monitoring patients.
IMMUNOLOGY
Superantigens and infectious disease
The mechanism by which foreign antigens are presented to T lymphocytes is central to the whole of immunology.
The essentials are increasingly familiar to clinicians, particularly those who deal with diseases with an immunological basis.
Nevertheless, a brief résumé of normal antigen presentation may be helpful.
T cells recognise conventional antigens not as whole proteins but as short peptide fragments derived from the protein by intracellular processing, and bound in a groove on the heavy chain of the class I and class II molecules of the major histocompatibility complex (MHC).
Certain aminoacid residues at critical points in the peptide sequence ‘anchor’ the peptide in the groove, and this interaction is moderately specific for each of the different allelic forms of MHC molecules.
The end result is that a complex of peptide and MHC molecule (and the MHC molecule must be the same allelic form as that on the T cell) is recognised by the T-cell receptor (TCR), with consequent activation of the T cell and expression of its effector functions.
The TCR consists of and chains, each composed of variable (V) and joining (J) segments (with an additional diversity [D]segment for the chain); these segments are encoded by separate genes which undergo DNA rearrangement to give many different combinations.
The association of peptides with MHC alleles on the one hand, and the diversity of the TCR on the other, allows T cells to recognise a huge number of antigens whilst conferring extreme specificity on this recognition.
All of this means that only a small fraction of all T cells will respond to any given antigen (see figure, top).
However, in the past few years a class of antigens has emerged that does not conform to this conventional pathway of antigen presentation.
These so-called superantigens have been named for their ability to activate simultaneously large numbers of T cells.
Superantigens do this by bypassing the normal route of intracellular processing and binding directly as intact proteins to class II MHC molecules at a site distinct from the peptide binding groove; they also bind to most allelic forms of class II molecules rather than to restricted alleles as do conventional peptide antigens.
The superantigen/MHC complex then reacts with the TCR through the V segment of the chain (V), rather than with the normal antigen-binding site on the TCR composed of both and chains.
Furthermore, superantigens react with all T cells carrying a particular V gene.
Since there are 50 different V genes in humans, a superantigen reacts with more than 1 in 50 T cells whereas a conventional peptide antigen will react with 1 in 10 to 1 in 10 T cells (see figure, bottom).
Two groups of proteins have been shown to act as superantigens, one exogenous to the host and the other carried endogenously in the host's genome.
The first group are nearly all bacterial exotoxins.
They include exotoxins produced by staphylococci — the toxins causing food poisoning (enterotoxins A, B, C 1–3, D, and E), toxic shock syndrome (TSST-1), and the scalded skin syndrome (exfoliating toxins A and B)— by group A streptococci (their pyrogenic exotoxins A, B, C, and D), and by Clostridium perfringens and Yersinia enterocolitica (their enterotoxins).
Mycoplasma arthritidis also produces a superantigen.
The bacterial exotoxin superantigens are small proteins that bind to class II molecules with high affinity and each stimulates T cells through several different V segments.
They are especially strong T-cell mitogens and activate both CD4+ and CD8+ T cells (whereas conventional antigens presented by class II MHC molecules activate only CD4+ T cells).
The second class of superantigens has been clearly defined only in the mouse.
A group of antigens capable of stimulating a strong mixed lymphocyte reaction between mice of the same MHC haplotype was recognised some time ago, and these were termed minor lymphocyte stimulating (Mls) determinants.
They also act by binding to class II MHC molecules and stimulating large numbers of T cells through their V elements.
We now know that Mls determinants are proteins encoded by genes of mouse mammary tumour viruses integrated and vertically transmitted in the mouse genome — in effect they are self-antigens, and as with any self-antigen encountered by T cells during their maturation in the thymus, T cells that react with Mls determinants are clonally eliminated in their early development with consequent ‘skewing’ of the animal's V T-cell repertoire.
An interesting consequence of this process is that the mice are then resistant to infection with exogenous mammary tumour viruses.
What are the clinical implications of all this?
The different ways in which superantigens activate T cells casts light on the pathogenesis of infectious disease.
The ability of bacterial exotoxins to activate large numbers of T cells with consequent release of cytokines could directly account for their pathogenic effects and the clinical syndromes they produce.
For example, experimental staphylococcal enterotoxin shock depends on superantigen-induced release of tumour necrosis factor mediated via T cells.
Widespread activation of T cells, with selective representation of particular V elements, is also a feature of some inflammatory joint diseases linked with infective agents and of Kawasaki disease, and superantigen effects could account for these associations.
Intravenous immunoglobulin (IVIg) is reported to contain specific antibodies to exotoxin superantigens; this feature might provide an explanation for the therapeutic actions of IVIg in diseases in which superantigens have been implicated in the aetiology.
No human equivalent of the endogenous mouse retroviral superantigens has yet been described.
However, one suggestion is that acquired human retroviruses (such as HIV) might act as superantigens, and that this property could contribute to the T-cell abnormalities they produce.
Retrovirus integration and replication require activation and division of T cells, so the ability to stimulate T-cell division by a superantigen effect would be an advantage to T-cell-tropic retroviruses.
Reports of selective depletion of a common set of V elements in people with HIV infection, and that HIV selectivity stimulates and replicates in CD4 cells with particular V elements in vitro, are compatible with (but not proof of) HIV encoding a superantigen.
Already it seems clear that superantigens are yet another example of microbial pathogens evolving mechanisms to use and subvert the immune response; we shall hear more of their role in infective and autoimmune disease.
PERIOPERATIVE BLEEDING
Drugs for surgical blood loss
In 1988 a Lancet editorial inquired ‘Can drugs reduce surgical blood loss?’.
The answer is yes.
The question now is: ‘Which drug is best?’.
Ideally the agent should reduce bleeding sufficiently to allow the patient to undergo surgery without the need for blood transfusion and should obviate the risk of postoperative anaemia.
Other requirements are applicability to various procedures.
The early promise of desmopressin in this role has faded and there is now little to support its routine use except in patients with uraemia.
The antiplatelet agents prostacyclin and dipyridamole are likewise not routinely used because of their cardiovascular effects and poor efficacy, respectively.
Consequently, the choice seems to lie between the lysine analogues, tranexamic acid and -aminocaproic acid, and the serine protease inhibitors (SERPINs), aprotinin, nefamostat, and gabexate.
Such agents have been used effectively in patients undergoing dental, orthopaedic, hepatic, vascular, and neurosurgery procedures, but most of the data on the use of such drugs have accrued from cardiac surgery.
After major surgery, either class of drug reduces perioperative bleeding.
However, if the primary endpoint for efficacy is the need for donor blood, differences become apparent.
With the exception of aprotinin, none of the agents has been shown to influence the requirements for blood or blood-product transfusion consistently or substantially, particularly in the high-risk categories of surgical, and especially cardiac surgical, patients.
SERPINs also suppress some potentially damaging inflammatory pathways.
Platelet and white cell activation are inhibited, as is the release of agents such as arachidonic acid metabolites, specifically thromboxane and cytokines such as interleukin-8.
What about concerns over safety?
In particular, are blood-sparing agents associated with a higher mortality or morbidity as a result of excess clot formation?
The lysine analogues are antifibrinolytics that inhibit certain platelet functions and have no effects on the coagulation process; they cannot initiate or accelerate clot formation but will delay clot lysis.
SERPINs are inhibitors of both platelet function and coagulation.
They inhibit coagulation even when there is a hypercoagulable state and reduce the deposition of organ microemboli in cases of disseminated intravascular coagulation associated with sepsis.
Aprotinin has been used to reduce the incidence of deep-vein thrombosis following hip surgery.
However, this inhibition of coagulation has some drawbacks, and has led to considerable difficulties in ensuring appropriate heparin administration during cardiac surgery.
In large studies there is no evidence of increased mortality when either class of agent was used, but what about morbidity?
Thrombosis of cerebral vessels is a reported complication when tranexamic acid is given over several days, but not when use is restricted to the period of cardiac surgery.
Nonetheless, stroke was observed only in patients in the treatment group in the studies from Philadelphia.
The rate of stroke in high-risk cardiac surgery patients receiving aprotinin therapy is lower than would be anticipated.
Concerns have also been expressed about the possibility of graft occlusions and increased myocardial ischaemia, especially in patients undergoing myocardial revascularisation.
With both lysine analogues and aprotinin, more postoperative myocardial infarctions have been reported in treated patients although the increase is not statistically significant.
The effect of lysine analogues on graft patency has not been formally investigated.
In low-risk patients there is no effect of aprotinin on graft patency; in high-risk patients there have been reports suggesting graft occlusion in patients receiving the drug.
In one study, necropsy was undertaken in about half of the patients who died, and showed graft occlusion only in patients given aprotinin.
A surprising observation is that the thrombosed grafts were confined to the patients who received a low dose of aprotinin; graft occlusions were not seen in patients allocated to receive a high dose or placebo.
There are several outstanding issues.
We need more information about which agent and dose is appropriate for all these compounds, and we also need to know more about their efficacy in reducing the need for donor blood and their safety profile in high-risk procedures.
Lysine analogues cost less than SERPINs.
With both classes of compounds the challenge is to show that the risk of administration is very low, and certainly less than that of blood products.
ONCOLOGY
Molecular basis for hormone-related cancer
Clinical oncologists have long recognised a group of cancers that respond to hormonal manipulation — eg, tumours of the breast, endometrium, thyroid, and prostate.
Tumours of the ovary and testis are likewise hormone-related, although they do not usually respond to hormone therapy.
Hormonal factors may also be involved in osteosarcoma and malignant melanoma.
The common clinical and epidemiological features of some of these tumours seem to be closely linked at the molecular level.
Most cases of ovarian cancer arise on the epithelial surface of the ovary, not in the ovary itself.
Many years ago Fathalla proposed that repeated minor trauma to ovarian epithelium caused by ‘incessant ovulation’ increased the risk of cancer.
Some epidemiological studies support the Fathalla hypothesis.
Ovulation is driven by follicle stimulating hormone.
Anything that inhibits ovulation — eg, pregnancy or oral contraception — reduces the risk of ovarian cancer.
These findings now have a molecular basis.
In cases where a tumour suppressor gene is inactivated, repetitive cellular repair may lead to uncontrolled cell division and malignant transformation; a putative tumour suppressor gene has been described on the long arm of chromosome 17.
Similar mechanisms may account for other cancers of the reproductive organs.
The physiological cycle of incessant ovulation, frequent menstruation, and intermittent breast stimulation is largely wasteful.
Cell proliferation is constantly being turned on and off in the target organs.
Thus it is hardly surprising that malignant diseases of the breast, ovary, and endometrium all occur more commonly in nulliparous women.
Moreover, the age-incidence curves for this group of diseases begin to flatten around the time of menopause.
A predisposing tumour suppressor gene for breast and ovarian cancer families, BRCA1, has been mapped by linkage analysis on the long term of chromosome 17, close to but probably distinct from the region noted in sporadic ovarian cancer.
Differentiated thyroid cancer occurs much more commonly in women than in men, largely in the premenopausal years.
This feature is thought to be related to the reproductive cycle.
The sex difference begins to fade in postmenopausal women and disappears completely in older patients.
There is a positive association between thyroid cancer and parity, and an association between breast cancer and thyroid deficiency has likewise been recognised clinically.
The fact that the BRCA1 gene for breast cancer is flanked on the long arm of chromosome 17 by the THRA1 gene for the thyroid hormone receptor may be important in this respect.
Moreover, an 80% allele loss has been noted at the THRA1 locus in sporadic breast cancer cases.
Osteosarcoma is not usually classified with hormonal cancers, but its onset largely coincides with the final hormonal growth spurt in teenagers.
Most tumours are located near the ends of long bones and become apparent around the time of epiphyseal closure.
The growth hormone locus has also been mapped on the long arm of chromosome 17, at 17q 22–24, close to the BRCA1 gene in breast and ovarian cancer families.
The molecular properties of prostate cancer are being intensively investigated.
A study in 1982 of Utah Mormons showed clustering of prostate cancer with breast cancer.
Ten years later a cohort study in Iceland confirmed that male blood-relatives of women with breast cancer had an excess risk of prostate cancer.
The Icelandic report also noted coaggregation between breast cancer and cancers of the prostate, ovary, and endometrium.
Some studies have linked the risk of prostate cancer with testosterone concentrations and the degree of sexual activity, but the latter is difficult to measure and reports are inconclusive.
An obvious physiological difference between the sexes is that female hormonal concentrations fluctuate throughout the reproductive years whereas male concentrations are relatively constant, declining gradually with age.
The absence of intermittent hormonal stimulation in men implies that a similar underlying genetic defect might not give rise to malignant disease until later in life.
Thus it is noteworthy that prostate cancer is primarily a disease of elderly men.
The age-incidence curve is steeper and the average age of onset older than in any other form of cancer.
An underlying hormonal element has long been suspected in many cases of malignant melanoma.
In a study of eight linked kindreds of breast/ovarian cancer cases, seven melanomas were found in blood relatives but none in spouses.
There was also an excess of endometrial and thyroid cancers.
Is there a link running through all these tumours in the gene-rich area of chromosome 17?
SCIENCE & PRACTICE
Pathogenesis of anorexia nervosa
Once thought to be rare, anorexia nervosa has become an important public health concern in western countries.
Predominantly affecting young women, the central feature of this disorder is an abnormally low weight achieved by extreme caloric restriction.
This behaviour is motivated by what has been described as a relentless pursuit of thinness, a morbid fear of fatness, or a weight phobia.
The seriousness of the low weight is often denied by the patient.
Long-term follow-up studies indicate that anorexia nervosa is associated with serious morbidity and mortality ranging from 4% to 18%.
During the past decade, much research has addressed possible causal factors.
Although there have been important advances in our understanding of anorexia nervosa as a multifactorial disorder, the precise mechanisms determining its expression remain elusive.
Epidemiology
There is debate about the long-term trends in incidence and prevalence of anorexia nervosa.
However, it seems to have become more common, especially among young women.
While it occurs most often in adolescence, the age of onset can range from pre-adolescence to middle age.
Most cases are in women, yet 5% to 10% of those presenting for treatment are men.
Once thought of as a disorder of upper social classes, it now seems to be well represented across the socioeconomic spectrum.
Eating disorders are common in rural as well as urban settings.
There is evidence that certain athletes, especially those in sports that emphasise leanness to improve performance or appearance (eg, gymnastics, figure skating, ballet) are at increased risk for anorexia nervosa or associated eating disorders.
Early descriptions
Descriptions of self-starvation among early religious ascetics suggest that some variant of anorexia nervosa may be traced to medieval times.
Richard Morton is generally credited with the first medical description in 1689 and he poignantly captured the key clinical feature: ‘I do not remember that I did ever in all my practice see one, that was conversant with the living so much wasted with the greatest degree of a consumption (like a skeleton only clad with skin)’.
Although debate has persisted about the priority of rediscovery in the 19th century, the disorder emerged as a distinct clinical entity with the descriptions of Gull in 1873, who coined the term anorexia nervosa, and Lasegue in 1873, whose portrayal was perhaps the most incisive.
During the 20th century, anorexia nervosa was ascribed a psychological cause by other physicians (eg, Charcot, Dubois, Fenwick, Janet) but then became confused with pituitary insufficiency in 1914 by Simmonds, leading to a controversy that persisted for 30 years.
Its psychogenic origin was reaffirmed with the descriptions of Ryle, Sheldon, Berkman, and Venables in the 1930s, a point of view that has predominated to the present time.
Diagnosis
As diagnoses become more refined, they move from a simple description of signs and symptoms to an understanding about pathogenesis, and finally to knowledge of aetiology.
The current diagnostic criteria for anorexia nervosa reflect this evolutionary process but they have yet to achieve the final category.
Table I summarises the American Psychiatric Association DSM-IV diagnostic criteria for anorexia nervosa.
These new criteria will formalise overlapping conventions for subtyping anorexia nervosa into restricting and binge-eating/purging types based on the presence or absence of bingeing and/or purging (ie, self-induced vomiting or the misuse of laxatives or diuretics).
Patients move between these two subtypes, although chronicity leads to an accumulation of patients in the binge-eating/purging subgroup.
Figure 1 shows that the symptoms of bingeing, purging, and restrictive dieting develop at different body weights, the critical feature distinguishing anorexia nervosa from other eating disorders being the appearance of these symptoms at an abnormally low weight.
Restricting patients are characterised as more obsessive-compulsive, stoical, perfectionistic, introverted, and emotionally inhibited, while binge-eating/purging patients are described as impulsive, depressive, socially dysfunctional, sexually adventurous and, as substance misusers, with high levels of general emotional distress.
These differences, together with the serious physical health consequences and metabolic abnormalities associated with vomiting and purging, provide support for the DSM-IV subtyping of anorexia nervosa.
Pathogenesis
During the past several decades, single-factor causal theories have been replaced by the view that anorexia nervosa is a multifactorial disorder.
Its symptom pattern represents a final common pathway resulting from the interplay of three broad classes of predisposing factors shown in figure 2.
The role of individual (biological and psychological), familial, and cultural predisposing factors is presumed to vary across this heterogeneous patient population.
The precipitants of the disorder are less clearly understood except that dieting is invariably an early element.
Perhaps the most practical advances have come from the increased awareness of the perpetuating effects of starvation with its psychological, emotional, and physical sequelae.
There is enormous variation in psychopathology, family dysfunction, and medical complications associated with anorexia nervosa.
Some cases are uncomplicated with a good prognosis while others are difficult to manage with a poor anticipated outcome.
Individual predisposition
Psychological factors
Most researchers have concluded that the premorbid personality is characterised by substantial emotional instability.
There have been serious methodological limitations to much of the research implicating personality factors in anorexia nervosa.
Most studies do not use representative samples, and personality traits are typically assessed after the eating disorder has developed, thus making it difficult to determine whether the personality features caused, maintained, or simply resulted from the eating disorder.
Studies rarely include a psychiatric control group.
Depression.
— Depression has been documented in many clinical samples.
Some studies indicate that depressive states of varying intensities foreshadow the development of the disorder, with lifetime prevalence rates of major depressive illness ranging from 25% to 80% across different samples.
Halmi et al, found a lifetime prevalence of 68% for major depression in a sample of severely ill anorexia nervosa patients.
Depression can be secondary to starvation and coexisting complications, since improved mood often follows nutritional rehabilitation.
One recent population study found that depression did not precede the eating disorder, although it did correlate with onset.
Anxiety.
— A lifetime prevalence of social phobia has been identified in more than 25% of anorexia nervosa patients.
The most common anxiety symptoms are obsessive-compulsive.
In reviewing comorbidity in anorexia nervosa, Rothenberg found obsessive-compulsive symptoms reported in 11%–83%, of patient samples, either during the active phase of the disorder or after weight restoration.
However, not all studies indicate a high rate of obsessive-compulsive disorder among anorexia nervosa patients.
Moreover, 11% of 151 women presenting with obsessive-compulsive disorder had a history of anorexia nervosa.
Personality disorder.
— Data on the incidence and prevalence of personality disorders in anorexia nervosa are inconsistent.
Some studies indicate strikingly high rates, with avoidant personality disorder developing in as many as 33% of anorexic restricters and borderline personality disorder developing in almost 40% of anorexic bulimic patients.
Other studies suggest that personality disorders are relatively uncommon in anorexia nervosa.
Impulse control difficulties, such as self-mutilation, suicide attempts, and stealing, are reported in a subgroup of anorexia nervosa patients, especially those with purging and/or bulimic symptoms.
While personality disturbances are not uniform in eating disorders, their presence suggests meaningful subtypes that may be relevant to treatment planning and prognosis.
Cognitive and emotional deficits.
— Anorexia nervosa patients have been described as displaying stereotypic distorted beliefs and reasoning errors.
Cognitive disturbances may play a predisposing part and may also maintain the disorder once it has been initiated.
A strong concern about physical appearance is part of the premorbid picture.
The conviction that weight, shape, or thinness can serve as the sole or predominant referant for inferring personal value or self-worth is central to anorexia nervosa.
Cultural and family values often reinforce this belief.
Anorexia nervosa patients seem unable to identify and to respond accurately to their emotional state, a deficit that has received some empirical support.
Body image.
— Bruch suggested that body image disturbance is pathognomonic of anorexia nervosa and that this is shown by indifference to emaciation and a misperception of body size.
Experimental evidence has provided some support for attitudinal and perceptual disturbances; however, the current methodologies have failed to define adequately the body-image construct or to prove that disturbances are unique to anorexia nervosa.
Psychological or physical trauma.
— Although anorexia nervosa has been thought of as developing in the context of a ‘perfect childhood’, there is some evidence that psychological trauma may predispose to the disorder.
A high incidence of sexual abuse has been identified in anorexia nervosa; however, recent research has suggested that this may reflect a more general vulnerability to psychiatric illness.
Even if psychological or physical trauma are not causal, they can be associated with symptom severity and should be considered central to management.
Genetics
The evidence for genetic vulnerability to anorexia nervosa comes from about 100 twin pairs taken from selected-twin case-report summaries and from twin studies.
These reports indicate concordance rates of over 50% for monozygotic twin pairs compared with under 10% for dizygotic twins.
This result is in contrast to one recent study of 11 twin pairs (5 monozygotic) where none of the co-twins was found to be concordant for anorexia nervosa.
These studies suggest that there may be a genetic component to the transmission of anorexia nervosa; however, it is not at all clear what is inherited.
Is it the specific disorder, a particular personality trait associated with the disorder, or a general vulnerability to psychiatric disturbance?
Moreover, the concordance data on twins reared together do not distinguish conclusively between genetic and environmental transmission.
Genetics may contribute to a specific vulnerability, or may operate indirectly by predisposing to obesity or personality traits that make restrictive dieting and anorexia nervosa more likely.
Constitutional and physical vulnerability
Prenatal or perinatal risk factors, including infections, trauma, convulsions, exposure to alcohol, older maternal age, and low birthweight seem to be common in selected patient samples.
These risk factors have been linked to a poor prognosis.
A high rate of gastrointestinal difficulties, such as severe abdominal pain, peptic ulcer, or ulcerative colitis, have been identified in the early feeding history of anorexia nervosa.
Physical features, such as a propensity for obesity or tallness may reflect predisposition to early maturation; in these cases, anorexia nervosa may represent an attempt to prevent or reverse the maturational process in those who feel unprepared.
Neuroendocrine and metabolic abnormalities
Disturbances in the hypothalamic-pituitary-gonadal axis are the most consistent of the various endocrine abnormalities associated with anorexia nervosa.
Abnormalities in several neuroregulatory systems have been identified in anorexia nervosa; however, in most cases, these reflect incomplete weight restoration or persistent eating disturbances rather than primary dysfunction.
Nevertheless, to abandon consideration of primary biological vulnerability to anorexia nervosa would be premature.
Amenorrhoea develops in many patients before substantial weight loss has occurred and age-inappropriate gonadotropin secretion patterns are present in some patients who are weight recovered.
Recent data indicate that the serotonergic systems implicated in the regulation of feeding and mood remain altered in anorexia nervous patients even after weight restoration.
Familial predisposition
Some of the earliest descriptions of anorexia nervosa emphasise the pathogenic role of the family.
Mothers have been described as dominant, intrusive, and ambivalent while fathers have been portrayed as passive and ineffectual.
The disorder is sometimes seen as serving a homoeostatic or stabilising role in the family.
Specific patterns of interaction have been observed, including enmeshment, overprotectiveness, rigidity, and in the family conflict avoidance.
The family pathology may well be a result of the disorder rather than a cause.
Nevertheless, eating disorders do show a familial aggregation.
In studies of the lifetime prevalence of eating disorders in families of anorexia nervosa patients, female first-degree relatives have a significantly higher risk of developing an eating disorder than controls.
Cultural predisposition
Recognition that cultural pressures on women to diet contribute to anorexia nervosa has had a fairly recent history.
Shape dissatisfaction, endemic to young women in western culture, elicits restrictive dieting that increases the vulnerability to eating disorders.
A strong concern about physical appearance seems to predate the development of anorexia nervosa.
Disordered eating correlates with assimilation into our ‘thinness-conscious’ culture.
The stigma associated with obesity, and even the ill-fated treatments advocated for obesity, have been considered risk factors for eating disorders.
There is evidence that anorexia nervosa itself has developed a positive connotation and that eating disorder symptoms are viewed as neither unusual nor abnormal by the general public.
Given the profound cultural pressures on women to diet, it is perhaps pertinent to ask why all women do not develop some level of disordered eating?
An interesting, yet largely unexplored area relates to protective factors that tend to prevent the expression of eating disorders.
Other cultural risk factors, such as role conflicts experienced by women, may also be implicated in the pathogenesis of anorexia nervosa.
Precipitating factors
As shown in figure 1, restrictive dieting and feelings of insecurity can induce anorexia nervosa in those who are vulnerable.
Dieting may be prompted by perceived plumpness or by comments from others.
Depression may be a precipitant in many cases.
Neilsen has indicated that anorexia nervosa has a pattern of seasonal onset (maximum in May) consistent with that for certain mood disorders and suicide.
In the face of inadequate coping skills, anorexia nervosa may arise in response to new experiences, such as puberty, leaving home, or entering college.
Adverse life events have been identified as potential precipitants, especially in anorexia nervosa with a late onset.
The failure to specify clearly the timing of important events has been a limiting factor in research, since it has not allowed predisposing factors to be distinguished from precipitating events.
Perpetuating factors
The psychological and physical effects of starvation have been well documented under natural and experimental conditions.
These are important for understanding anorexia nervosa because first, symptoms previously assumed to reflect primary psychopathology are now recognised as common to starving people whatever the cause and, second, starvation leads to secondary symptoms that then play a critical part in perpetuating the disorder.
Thus, while depression may lead to dieting aimed at increasing self-esteem, starvation prompts further deterioration in mood.
Heightened vigilance around food is required to control the urge to eat; starvation increases food pre-occupations, making food (and control of eating) even more important imperatives.
Delayed gastric emptying, together with reduced gastric mobility, can augment the perception of fullness and thereby contribute to further dieting.
There are also psychological, interpersonal, and cultural factors that can maintain the disorder.
Eating disorder symptoms can produce positive feedback in psychological and family systems or they can preserve equilibrium in either of these systems by deflecting away from other difficult areas.
Conclusion
Anorexia nervosa has been viewed historically as a complex and often perplexing disorder.
Different single-factor causal theories have been proposed, but the trend in recent years is to view the disorder as heterogeneous and multifactorial, arising from the interplay of psychological, familial, cultural, and biological predisposing factors.
Current formulations place great emphasis on the role of maturation and psychological vulnerability in initiating the development of symptoms.
It can be triggered by long-standing discontentment with body weight and shape or by specific threats to individual or family adaptation.
Thereafter, it is maintained by the effects of starvation and, in some cases, by psychosocial factors.
While there is still some debate about whether or not anorexia nervosa has become more prevalent, there can be no doubt that there has been an enormous increase in research interest into this subject.
Much has been learned about the pathogenesis of this disorder, together with related eating disorders such as bulimia nervosa and the binge-eating disorder.
Findings that eating pathology persists in the absence of intervention suggest the need for prevention programmes aimed at identifying potential cases early and then offering remedial advice or referral.
Treatment of anorexia nervosa
Anorexia nervosa usually develops in adolescent and young women: it is the third most common chronic illness in teenage girls, is seen only rarely in males, and is more properly a dieting disorder than an eating disorder.
The endorsement our society gives to an ideal of thinness for young women is likely to be responsible for the current high frequency of the illness, but other factors must also be involved since anorexic patients have been reported for at least the past 150 years.
Its most striking features are an intense pre-occupation with weight and shape and a relentless pursuit of thinness.
On these features are imposed several other psychological symptoms, many of which are known to be common to semistarvation, irrespective of its cause.
These include depressed mood, irritability, social withdrawal, loss of sexual libido, pre-occupation with food, obsessional behaviour, and eventually reduced alertness and poor concentration.
While these symptoms impart a conformity to the clinical presentation, the underlying psychodynamic psychopathology is varied.
Each patient must be understood as an individual.
Various behaviours are used to induce weight loss.
Some patients use only those restrictive behaviours that are commonly associated with dieting, such as undereating, avoidance of high-energy foods, and strenuous exercise.
But they differ from normal girls in the extent to which they pursue these activities and their inability to desist from them.
Others go beyond restriction to more dangerous behaviours, such as selfinduced vomiting, purgation with high doses of laxatives, and the misuse of diuretics and appetite suppressants.
The resultant emaciation may be severe, with patients losing up to 60% of normal body weight.
The consequences of starvation depend on the type of dietary deficiency from which it arises.
The starvation effects of anorexia nervosa are very different to those found in conditions such as protein-calorie malnutrition or famine.
In the early phase of her illness, the anorexic patient chooses a diet that is low in energy-dense foods but relatively high in proteins and other essential nutrients.
Dietary protein, together with the high activity levels that are characteristic of the illness, exert a nitrogen-sparing effect, and initial weight loss is due almost entirely to loss of adipose tissue.
However, when fat reserves are exhausted and when food refusal becomes more severe, protein catabolism increases.
Water loss is accelerated especially from the intracellular compartment, leading to metabolic and electrolyte disturbance.
Treatment must address all aspects of this complex disorder (table I) and begins with a thorough appraisal of the patient's physical condition, together with identification of important medical complications.
Medical complications
Complications result from starvation or from the behaviours adopted to induce it.
They are not indicative of underlying pathology.
The inexperienced clinician who undertakes unnecessary investigations to exclude all possible causes for each abnormal finding is doing the patient a disservice by delaying appropriate treatment.
Rather, all clinicians should be aware of the wide range of physical abnormalities that are commonly found in anorexic patients (table II).
Many of these, such as decreased serum concentrations of gonadotropins and steroid sex hormones, alterations to the peripheral metabolism of thyroid hormone, and raised circulating concentrations of cortisol and growth hormone are best regarded as physiological adaptations to the state of starvation and do not require treatment.
However, some medical complications are not only clinically important but actually life-threatening; these require special attention.
Serious complications present usually in the chronic patient who is severely emaciated, abuses laxatives, and induces vomiting.
They have often gone unnoticed before admission, and may be induced when initial refeeding unbalances the patient's precarious equilibrium.
Electrolyte disturbance typically takes the form of a hypokalaemic, hypochloraemic, metabolic alkalosis.
Severe potassium depletion is associated with muscular weakness, cardiac arrhythmias, and renal impairment, but over vigorous replacement poses dangers and potassium is best given orally.
Once one electrolyte abnormality is found, others are likely to emerge.
Hypomagnesaemia is especially important in refractory cases of hypokalaemia, while hypocalcaemia and hyponatraemia or hypernatraemia are also common.
Hypophosphataemia frequently emerges during refeeding.
Cardiac complications are the most common cause of death in these patients.
Bradycardia and hypotension are characteristic.
The heart decreases in size and assumes a more vertical position, due to a loss of pericardial fat, reduced left ventricular mass, and decreased blood volume.
Exercise capacity and oxygen consumption are reduced, and the response to physical activity is impaired.
These disturbances are probably due to decreased resting catecholamine concentrations and an attenuated response to exercise.
Mitral-valve prolapse is common, but without clinical significance.
The electrocardiogram will confirm sinus bradycardia.
First-degree heart block, ectopic atrial rhythms, nodal escape, ventricular premature complexes, ST depression, and U waves may all be found.
However, the major cause of anxiety is prolongation of the corrected QT interval, since this is thought to predict the onset of serious cardiac arrhythmias.
Raised concentrations of plasma creatinine and urea are common, and are a reflection of muscle protein catabolism.
Creatinine clearance may be reduced.
Renal insufficiency occasionally develops as a consequence of vomiting, laxative misuse, and the resulting electrolyte disturbance.
Dependent oedema often complicates refeeding.
It is not usually associated with lowered plasma proteins or cardiac failure, and has been attributed to increased capillary permeability, reduced tissue tone, and an inhibition of natriuresis secondary to increased tubular filtration of insulin.
It should be treated conservatively.
Glucose metabolism and insulin secretion are disturbed; hypoglycemia is common and potentially lethal but usually responds promptly to dietary correction.
Anorexia nervosa sometimes contributes to poor metabolic control in patients with type I diabetes, leading to a high rate of serious physical complications, but whether there is a higher than expected co-incidence of the two disorders remains controversial.
Osteopenia is a common and serious complication.
Dietary deficiency, low circulating oestrogens, hypercortisolism, laxative misuse, and disturbed acid-base balance are thought to be responsible.
There is continuing debate about the use of exogenous oestrogen as a preventive measure for all anorexia patients but no controlled trials have yet been reported.
Amenorrhoea with anovulation has long been recognised as invariable in anorexia nervosa (decreased libido and low testosterone are the equivalent in the male).
It is associated with low circulating levels of pituitary gonadotropins secondary to inadequate stimulation by hypothalamic releasing factor (LHRH), itself a direct effect of undernutrition.
Because the menstrual disturbance often precedes severe weight loss and may persist after weight is regained, other factors must also be involved.
These include a disturbance of oestrogen metabolism and a fault in oestrogen feedback to the hypothalamus.
The demonstration by ultrasound of a resumption of follicular activity presages the return of normal ovarian function, which occurs in patients who regain and maintain a normal weight.
However, the long-term outlook for reproductive function is poor in patients who conceive before they have fully recovered from their illness.
Low birthweight and a higher incidence of spontaneous abortion, congenital malformations, prematurity, and perinatal mortality, together with poor parenting, have been reported.
Induction of pregnancy by methods such as pulsatile LHRH is thus injudicious.
The need for laboratory investigations should be determined by the clinical problems posed by individual patients and only a few tests need be considered as routine (table III).
Role of the family doctor
The diagnosis is often obvious from the patient's history of pre-occupation with dieting and body shape or a relative's concern about weight-losing behaviour, social withdrawal, and depression.
In other cases, suspicion may first be aroused by the presence of characteristic physical symptoms.
Eliciting the behavioural and psychological disturbance then requires tact and perseverence.
Although most girls and young women engage in weight-losing behaviours, the anorexic patient's pervasive concern and uncompromising attitudes are typical of the illness.
The family doctor may choose to manage patients who are well motivated and not severely emaciated.
If so, it is necessary to adopt a therapeutic rather than simply a directive approach, and to pay particular attention to establishing such a relationship with the patient.
However, if there is no improvement within a few weeks an appropriate referral should be made.
It is important not to countenance unorthodox therapies or unnecessary laboratory investigations and consultations to exclude unlikely physical causes for the symptoms.
Patients may propose options that are ineffective to avoid the necessity of changing, and relatives may collude to deny the presence of a psychiatric problem.
Initial interview with the specialist
The patient is often accompanied by a relative or friend, and it is wise to include that person in the initial discussion since the patient may attempt to dismiss or deny the problem.
Excessive anxiety should be allayed.
The patient should also be seen alone and have a careful history and physical examination taken.
Many of the symptoms of anorexia nervosa, such as the extreme thinness and the pattern of restrained eating, are highly valued by the patient.
This makes it difficult for her to accept the need to change.
Hence, the first challenge may be to convince a disinterested or oppositional patient that treatment is necessary.
Patients should be told that resumption of normal eating will decrease their obsessive preoccupation with food, allay urges to binge eat, relieve tiredness and depression, and allow better relationships with family and peers.
The progressive and insidious nature of the illness should be explained, unreasonable expectation of a miraculous cure dispelled, and emphasis placed on the need to change behaviour and attitudes about eating.
The rationale of treatment should be explained fully, reassuring the patient that she will not be confined to bed, subjected to high doses of drugs, or isolated from her family and friends.
Patients or relatives may attempt to manipulate the therapist into compromising on issues such as the amount of weight to be regained or the kinds of food to be eaten.
It is important to be open and not be trapped into promises which will have to be broken later.
Outpatient treatment
One way to deal with an oppositional patient is to begin a brief trial of outpatient therapy, if only to convince her that she is unable to alter her behaviour despite her belief to the contrary.
If no improvement is observed within a few weeks, the therapist must insist on admission since further continuation of ineffective treatment is counterproductive.
However, a substantial proportion of patients do well as outpatients — eg, those whose emaciation is not severe (body mass index >17), who have no serious medical complications, who are motivated for change, and who have supportive friends and relatives.
Outpatient treatment follows the same lines as inpatient therapy, and is undertaken by the same team.
If successful, its advantages are that it is cheaper, avoids the stigma of admission and the danger of being influenced by the behaviours of other patients, and causes less disruption to the patient's life.
But unsuccessful outpatient treatment can significantly increase the risk that the patient will become invalided at home.
Hospital treatment
Although day-care programmes are gaining increasing recognition, most patients who are moderately or severely ill are still admitted to hospital.
Treatment is best undertaken in a specialist unit with a dedicated multidisciplinary team skilled in dealing with anorexic behaviours.
The ease with which patients are managed in these settings contrasts with the chaos that commonly ensues when a single anorexia nervosa patient is treated on a general psychiatric or medical ward, where she is either afforded special status or rejected because her illness is seen as self-inflicted.
An association of the specialist unit with a general hospital is desirable since this approach allows prompt management of serious medical complications that may arise.
The long-term mortality of patients treated in a specialist unit is significantly less than that of patients treated in a general psychiatric setting.
The ward milieu is important.
Its structure should be orderly, communications unambiguous, and strategies firmly implemented to prevent splitting.
This is important in order to ensure that patients and their families do not categorise staff members as good or bad depending on their role in the ward — eg, viewing the nurse supervising the nutritional programme as authoritarian and the sympathetic night nurse as a special confidante.
It is essential that the director spends time educating, consulting, and supervising staff and supporting their decisions about day-to-day management issues.
Tendencies to stereotype patients as unrewarding and responsible for their illness must be firmly discouraged.
Staff will need help in understanding their own reactions to the patient's behaviour.
Producing weight gain
Deriving their influence from establishing a relationship of trust while remaining firm in countering attempts to avoid treatment, skilled nurses have an unique contribution to make.
They are able to persuade all but the most recalcitrant of patients to resume eating.
Unnecessary confrontation is avoided, but some supervisory strategies are necessary, such as a mandatory period of bed rest after meals to prevent surreptitious vomiting or strenuous exercising.
A specially trained dietician helps plan the refeeding programme and shares responsibility for its implementation.
Special foods are not required.
An eating plan should give a weekly gain of about 1.5 kg.
Initially, a daily intake of as little as 1500 kcal (6000 kJ) is prescribed because larger amounts impose considerable discomfort after prolonged restriction.
Increases are made only when the patient manages to complete each meal.
The daily energy intake to effect weight restoration may be as high as 3500 kcal (16,000 kJ), depending on the size of the patient, her metabolic rate, and her level of activity.
At these high levels, some of the energy may be derived from concentrated, high-calorie products, but the emphasis is always on resuming normal eating.
Dietary supplements are seldom indicated; tube feeding or parenteral nutrition are rarely necessary and should be avoided.
These procedures are detrimental to the therapeutic relationship, and may have unforeseen effects on physical status.
Eating behaviour is disturbed and must be the focus of explicit attention.
But rather than repeat the conflicts about poor eating habits which had taken place between the patient and her family before admission, pressure should be taken off the meals themselves, during which patients are encouraged to interact in an appropriate social context.
Specific eating difficulties are addressed in individual counselling sessions later in the day.
There is no point in bargaining about the level of target weight.
Patients need to return to a normal weight, and that is determined by reference to scientific norms rather than by special pleading and personal opinion.
The body mass index is a convenient measure on which to make this judgment.
For women over 16, a health body weight is that which is consistent with a body mass index between 20 and 25.
For younger girls, lower values are appropriate.
Behavioural techniques
Most treatment programmes include behavioural modification techniques, making the patient's access to pleasurable activities contingent on weight gain.
Many such programmes are unnecessarily harsh and increase isolation by confining the patient to bed.
The programme itself becomes the focus of treatment, rather than therapy to resolve the underlying psychological problems.
A more lenient and flexible behavioural approach is preferable.
It is more acceptable to patients, does not compromise weight gain, and allows staff greater opportunity to engage patients in meaningful psychotherapy rather than continually assume a supervisory role.
Pharmacotherapy
Antidepressants should be considered only where depression is not improving (as it usually does) with weight gain.
Malnourished anorexic patients are more prone to side-effects and less responsive to medication than are other patients with depression.
There is also the possibility that tricyclics may add to the risk of cardiac arrhythmia and severe hypotension, especially in those in whom purging and vomiting has led to dehydration and cardiac dysfunction.
Clomipramine, which may seem to be indicated because of the patient's obsessional symptomatology, sometimes causes severe nausea or intractable constipation.
Fluoxetine, the serotoninergic reuptake blocker, is a safe alternative, but its onset of action is slow and it may cause an initial increase in anxiety.
A dose of 20 mg a day is adequate, unlike its use in bulimia nervosa in which higher doses are recommended.
Antipsychotic drugs in small doses are occasionally useful to reduce high levels of anxiety early in treatment, especially when these are associated with a mild delirium.
The benzodiazepine anxiolytics are best avoided.
Insomnia is treated by reassurance that sleep will improve when weight is regained and by behavioural techniques that also reduce anxiety.
Supervised activity
Excessive and uncontrollable physical activity is a characteristic of anorexia nervosa.
A supervised exercise programme helps moderate this behaviour as well as relieve the abdominal discomfort associated with rapid weight gain.
It should be restricted to patients who are reasonably well nourished, medically stable, gaining weight, and co-ordinated by a specially trained nurse or physiotherapist.
Not only do patients accept weight gain better when allowed to exercise, but also the programme allows the opportunity for guidance about healthy levels of activity after discharge,
Discharge and aftercare
While research dealing with the optimum length of hospital stay is sparse, significantly fewer relapses are seen in patients who complete an inpatient programme and are discharged at a normal weight than in those who leave prematurely.
Because of the severity of their undernutrition, most patients require 10 to 12 weeks to achieve full nutritional rehabilitation.
They should not be discharged immediately they reach their target weights, but should become accustomed to eating normal meals rather than the high-energy diet necessary for weight gain.
The transition from ward to home is usually difficult, and patients are likely to relapse if they do not receive adequate support and supervision.
One useful strategy is to give patients weekend leave on several occasions before discharge, encouraging them to discuss the difficulties that they have experienced on their return to the ward.
A period of day-hospital care may also be useful, if the patient's home is near enough to the hospital for it to be practical.
Consistent after-care is important.
After discharge, patients should attend weekly outpatient appointments, moving to fortnightly or monthly attendances as appropriate.
Counselling and psychotherapy
Psychotherapy commences during hospital admission and continues long after discharge.
Because of the multidimensional nature of the illness, a combination of approaches is advisable (table IV).
Nutritional counselling aims not to prescribe yet another rigid diet but to guide the patient back to eating in a normal and relaxed way, with sufficient flexibility to adapt to various social settings such as going to a restaurant or eating with friends.
Education must be an adjunct to rather than a substitute for psychotherapy, but it is worthwhile if it does no more than alert the patient to the dangers of behaviours such as laxative misuse.
A cognitive model for anorexia nervosa, similar to that for depression, proposes that errors in thinking such as inappropriate personalisation are responsible for the attitudes and beliefs that underlie the behavioural disturbance.
Cognitive therapy addresses this issue and is usually undertaken by a psychologist.
It aims to help patients recognise connections between their dysfunctional thoughts and maladaptive behaviours; to evaluate critically their attitudes and beliefs about weight and eating, rejecting those that are invalid; and to substitute more reasonable interpretations for dysfunctional ones.
The anorexic patient identifies with her wasted body and declares her condition a form of self-realisation and self-expression.
Moreover, the illness leads to profound regression, and the patient needs help to resume normal psychological growth.
For these reasons, insight-oriented psychotherapy is indicated.
The patient needs help to build trust and to establish her identity.
But anorexia nervosa is a heterogeneous illness, and the psychotherapy that is necessary depends on several factors.
However, its initial goals are to encourage expression of feelings and the tolerance of uncertainty and change; to promote a realistic appraisal of personal strengths and weaknesses; to help with separation and individuation; and to provide a secure relationship that nevertheless allows the patient to develop independently.
Much of the patient's psychological disturbances relate to separation anxiety and difficulties with boundaries and identity formation.
Although undue influence has been given to the alleged psychosexual immaturity of anorexia nervosa patients in the past, issues of sexuality need to be addressed as with other adolescent patients.
Starvation confers a spurious sense of control over impending bodily changes, while the ‘pathogenic secret’ is often related to sexual or physical misuse, which resulted in the intense sense of shame that the patient attempts to assuage by losing weight.
This issue must not be overlooked, but it should be pursued gently since self-destructive behaviour may result from the arousal of traumatic memories.
The pace of therapy must not be forced.
Determined efforts should be made to keep in touch with the family, irrespective of the patient's age, and to give practical advice.
An understanding of the family's experience of the patient and her illness, together with the patient's experience of her family, is important.
Parents may need to be empowered to insist that the patient accept the treatment that they know is necessary.
Some families displace their frustration and anger onto the therapeutic team, colluding with the patient or disrupting treatment in other ways.
This behaviour leads to triangulation between patient, staff, and family, and should be defused so that treatment can continue.
The family should be warned against condoning the patient's anorexic behaviour, which they may do either because they feel unable to tolerate the patient's anger or because they themselves gain vicarious gratification from the illness.
Sometimes the resistance of an anorexic patient to regaining weight has as much to do with the mother's overinvolvement in the patient's life as her own unwillingness to change.
In such instances, a firm insistence on the importance of physical rehabilitation is necessary.
Self-help and support groups have become popular in recent years, and the best of these include a skilled professional.
A broader perspective
The American Psychiatric Association has chosen eating disorders for the first of its series of detailed practice guidelines, derived from the literature and from a consensus of expert reviewers, to assist doctors in clinical decision making.
These guidelines are sensible and will undoubtedly exert a beneficial influence on patient care.
However, they can do no more than recommend the best of the available options and even in expert hands the treatment of anorexia nervous must still be considered unsatisfactory.
There are few controlled studies of treatment and because of the complex nature of the disorder such studies are difficult to undertake.
Therapy is handicapped because patients are encouraged in their behaviours by inappropriate societal attitudes.
Nevertheless, most acute patients can be treated successfully, although the relapse rate is unacceptably high.
Follow-up studies after 4 or more years report less than 50% of cases as recovered, and even ‘cured’ patients have not necessarily returned to a normal life, at least as far as their attitudes and behaviour are concerned.
Up to 25% of patients progress to chronicity and invalidism, their psychiatric difficulties compounded by medical sequelae, such as an increased susceptibility to infection, osteoporosis, or renal failure, which severely undermine their quality of life.
Many die prematurely from the medical complications of the illness or by suicide.
From a health service perspective, there are other reasons for concern.
The average duration of admission for a patient with anorexia nervosa at a specialist unit in Sydney is 62 days (unpublished data), in the UK 96 days, and in New Zealand 64 days.
The cumulative length of stay for all admissions over 5 years in New Zealand was 135180 days, third among psychiatric conditions after organic disorders and schizophrenia.
As an illness that starts in adolescence and which often has a chronic course requiring repeated admissions, anorexia nervosa imposes a substantial financial burden on the community.
In the USA, insurance companies have challenged the need for hospital care of anorexic patients, seeking instead to limit reimbursement for treatment, although the need for hospital care is well documented.
A management approach that is cheap and effective in the short-term may ultimately be counterproductive and lead to increased rates of relapse, disability, and social handicap.
Although there is good evidence that the long-term prognosis is better for patients who are treated in expert units,, the provision of specially designated beds is inadequate.
In the UK there are only 11 such units, with a total of 58 designated beds.
The situation in New South Wales, with our far smaller populatin, is better, with loosely integrated public and private facilities providing 40 special beds under the consultant care of university staff (unpublished data).
But there are few advocates for patients with anorexia nervosa and they carry the risk of being squeezed out by the increasingly technological emphasis of general medicine and the concentration of psychiatric services on the needs of chronic psychotic patients.
The onus of care falls on individual practitioners, both medical and non-medical.
They must remember that anorexia nervosa is often a chronic illness.
Even patients who eventually recover may take years to do so, and they sometimes require several admissions.
Others are likely to remain severely handicapped.
To best serve patients' needs, care givers must be realistic in their expectations.
While relapses in a chronic schizophrenic patient are accepted as being outside the patient's own control, relapse in a chronic anorexic patient is often met by anger and frustration.
But those who realise that the difficulties of the patient with anorexia are just as real of those of any other sick person will find the treatment of these patients both interesting and rewarding.
ARRHYTHMIA OCTET
Cardiac arrest
Sudden cardiac death is an important public health issue in the industrialised world.
In the US alone there are 300,000 deaths annually; this number accounts for over half of all cardiovascular mortality.
In most cases the mechanism of death is ventricular fibrillation (VF) occurring in the setting of coronary artery disease.
However, even in carefully screened populations sudden death is often the first manifestation of a cardiac disorder.
The long-term epidemiological risk factors for sudden cardiac death greatly resemble those for coronary artery disease.
For men, body mass index, cigarette smoking, glucose intolerance, left ventricular hypertrophy on the electrocardiogram (ECG), increased serum cholesterol, and raised systolic blood pressure have all been identified as independent risk factors.
For women, in whom the risk of sudden death is appreciably less, decreased haemoglobin and non-specific ST and T wave changes on the ECG replace body mass index and glucose intolerance as independent risk factors.
Prevention strategies have focused on high-risk patients such as those who have already manifested ventricular arrhythmias in the convalescent phase of an acute myocardial infarction or who are survivors of either in-hospital or out-of-hospital cardiac arrest.
Nevertheless, these individuals contribute only a small percentage (<25%) to the overall incidence of sudden death in the general population.
Long-term strategies for the prevention of sudden cardiac death must therefore include efforts to reduce the overall incidence of coronary artery disease in the community as well as earlier recognition and protection of individuals at high risk.
Mechanism
Ambulatory monitoring during cardiac arrest has shown that the mechanism is usually VF preceded by a variable period of organised ventricular tachycardia (VT).
Underlying structural cardiac abnormalities, most commonly obstructive coronary disease, are present in the vast majority of patients.
There are several ways in which coronary artery disease can cause sudden death.
At one end of the clinical spectrum are patients who have a new Q-wave myocardial infarction due to thrombotic occlusion of a major coronary artery; the risk of sudden cardiac death in such cases recedes as the infarct evolves.
At the other end are those with a remote myocardial infarction in whom an arrhythmogenic substrate is provided by an anatomically constant but electrically unstable border zone between scar tissue and viable myocardium.
In these individuals islands of functioning myocardium interspersed with fibrous tissue provide an environment conducive to re-entrant VT.
Similar but more diffuse changes may occur as the result of a generalised myopathy, or the substrate may be provided by primary or secondary muscle hypertrophy.
Transient ischaemia, often with plaque rupture and formation of platelet fibrin thrombi, distal embolisation, and possibly local vasoconstriction may serve as a primary cause of cardiac arrest or may modulate a fixed arrhythmogenic substrate.
Cyclical changes in myocardial blood flow may interact with other factors such as fluctuations in autonomic tone, haemodynamic failure, hypoxia, acidosis, electrolyte imbalance, and even antiarrhythmic drugs to facilitate the induction of VT or VF.
More rarely (<5%), sudden cardiac death occurs in patients whose heart appears structurally normal; in such cases possibilities include idiopathic VF, idiopathic long QT interval syndrome, or ventricular pre-excitation over an accessory atrioventricular connection (Wolff-Parkinson-White syndrome).
Cardiopulmonary resuscitation
Successful defibrillation following lengthy closed chest massage as described by Kouwenhoven in 1960 was first applied to cardiac arrest victims in the community in Belfast in 1966.
Widely varying figures for successful resuscitation (2–44%) have subsequently been reported.
The standardised format for reporting survival rates proposed in 1991 — the Utstein style — should simplify intersystem comparisons and may partly reduce these wide discrepancies.
Cities such as Seattle, where a tiered rapid response of both basic and advanced life support is provided by the fire department, are likely to provide a gold standard.
In Seattle the average response time by emergency medical technicians is 3.5 min after dispatch; advanced cardiac life support response time is approximately 7 min.
In addition, over 60% of the adult residents over the age of 12 years have some formal training in cardiopulmonary resuscitation (CPR).
The annual survival rates to discharge from out-of-hospital VF since 1975 have varied between 24 and 33% (mean 28.7%).
However, the success rate over the past 15 years has remained almost constant.
Earlier defibrillation with automatic defibrillators may improve these figures, but they probably represent near-optimum achievable results with existing techniques.
The performance of most cities falls substantially below this standard.
A consensus has gradually evolved on the optimum methods of both basic and advanced cardiac life support.
–
Cardiac compression
Closed chest compression was initially thought to produce mechanical systole by direct compression of the cardiac ventricles.
This view has been superseded by the concept of the thoracic pump, whereby sternal compression causes a generalised rise in intrathoracic pressure which is selectively transmitted to the arterial circulation because of closure of the venous valves.
The two critical perfusion pressures are those across the coronary and cerebral circulations.
During CPR, chest compression at a rate of 80–100/min with sufficient force to produce 4–5 cm of sternal displacement in adults provides a coronary blood flow under 10% of normal.
Cerebral blood flow is better maintained but is still less than 30% of normal.
In an effort to improve myocardial and cerebral blood flow, several workers have used abdominal compression interposed during the relaxation phase of chest compression.
The results have been encouraging enough to merit further investigation.
By contrast, open-chest compression after prolonged closed-chest compression is very rarely successful and this method should be attempted only in highly selected patients in hospital.
Artificial ventilation
Endotracheal intubation remains the ideal method of airway management during cardiac arrest despite the development of several alternatives that involve blind passage of devices into the airways.
If the airway is protected, ventilation at a ratio of one: five compressions can be carried out either between or simultaneously with chest compression, with the latter shown to improve cerebral perfusion in laboratory animals.
Precordial thump
An immediate sternal thump during a witnessed cardiac arrest may revert VT (11–25%) and rarely VF to sinus rhythm.
There is a danger of precipitating asystole or electromechanical dissociation, so this technique should not be used for VT with a pulse unless a defibrillator and pacemaker are immediately available.
Defibrillation
Early defibrillation remains the most important determinant of survival in cardiac arrest victims.
The standard sequence of serial defibrillations with energies of 200J, 200–300J, and 360J is now well established.
If defibrillation fails to restore a satisfactory rhythm, adrenaline 1 mg should be given intravenously and repeated every 3–5 min as necessary.
The beneficial effects of adrenaline on myocardial and cerebral perfusion pressures are mediated by the alpha-adrenergic effects of this agent.
Whether the beta-adrenergic effects of adrenaline, which tend to increase myocardial work and reduce subendocardial perfusion, are detrimental is still controversial.
The effects of adrenaline on perfusion pressures are dose dependent and experimental work suggests that an optimum haemodynamic response might be achieved with doses higher than the 1 mg currently recommended.
However, the results of several recent comparative trials have shown no survival benefit on outcome of high vs low dose adrenaline in either the pre-hospital or in-hospital setting.
If defibrillation after adrenaline is ineffective, lignocaine 1.5 mg/kg should be given and repeated after 3 min.
If this fails, then bretylium 5 mg/kg should be given and repeated in a dose of 10 mg/kg every 5 min.
The choice between lignocaine and bretylium has varied over the years, with comparative trials showing no difference in outcome between the two agents.
For familiarity and safety, lignocaine is the agent of first choice.
Bicarbonate, widely used before 1986, is no longer regarded as a first-line therapeutic agent except when cardiac arrest is due to documented hyperkalaemia.
Although there are few data to support the effectiveness of bicarbonate, there are laboratory and clinical data indicating potentially harmful consequences of administration, especially a paradoxical increase in intracellular acidosis due to diffusion of carbon dioxide across cell membranes.
Calcium chloride, formerly widely administered during cardiac arrest, is likewise no longer regarded as a first-line agent except in the treatment of hyperkalaemia, hypocalcaemia, or calcium channel blocker toxicity.
Magnesium sulphate has lately been shown to reduce mortality in suspected myocardial infarction and may reduce the incidence of ventricular arrhythmias.
This agent is favoured for torsades de pointes and will undoubtedly be assessed as an earlier therapeutic option in the treatment of refractory VT/VF.
Asystole
When cardiac arrest is due to asystole or electro- mechanical dissociation, survival rates are dismal (<5%) unless reversible causes such as cardiac tamponade or tension pneumothorax are present and rapidly treated.
Route of drug administration
If immediate intravenous access cannot be achieved, adrenaline, lignocaine, and atropine can all be administered endotracheally.
Doses 2–2.5 times those recommended for the intravenous route should be used, diluted in normal saline or distilled water.
Intraosseous drug infusion (into the narrow cavity of the proximal tibia via a rigid needle) is also satisfactory, especially in children.
In-hospital management of cardiac arrest survivors
As many as 50% of patients admitted to hospital following a successful resuscitation from out-of-hospital cardiac arrest will die before discharge, mainly as a result of cardiogenic shock or the consequences of lengthy anoxia.
During this initial phase of hospital admission it is common practice to administer prophylactic antiarrhythmic drug therapy, usually lignocaine or procainamide, for 36–48 h.
In a small proportion of patients, intravenous bretylium or amiodarone may be necessary to suppress recurrent unstable ventricular arrhythmias.
However, once stabilised, all patients should undergo a full cardiac evaluation including assessment of left ventricular function, cardiac haemodynamics, coronary arteriography, and a comprehensive electrophysiology study.
The exceptions to this are:(a) patients with clear evidence of a new acute transmural (Q wave) myocardial infarction, in whom assessment of risk should follow routine post-infarction guidelines;(b) those with severe irreversible anoxic brain damage or concomitant terminal illness; and (c) those with uncorrected cardiac outflow obstruction (eg, aortic stenosis) or severe, life-threatening, or unstable coronary artery disease (eg, severe left main stenosis).
Left ventricular ejection fraction, which is the most powerful determinant of recurrent cardiac arrest and death, can be estimated at the time of cardiac catheterisation or non-invasively by radionuclide ventriculography or echocardiography.
The latter technique may prove especially useful in identifying patients with hypertrophic cardiomyopathy or mitral valve prolapse.
Coronary revascularisation
Coronary ateriography displays occlusive coronary artery disease, often involving multiple vessels and frequently with evidence of previous myocardial infarction, in a large majority of patients.
In a few individuals, especially young men with exercise-induced cardiac arrest, isolated anomalies of the coronary arteries may be found, most commonly an anomalous origin of the left main coronary artery from the right coronary sinus.
Assessment of the functional significance of coronary artery stenoses may be helped by the use of thallium perfusion scintigraphy in conjuction with exercise, dipyridamole, or adenosine stress testing.
These results should guide decisions about coronary revascularisation.
Data from the Coronary Artery Surgery Study and European Coronary Surgery Study have shown that coronary artery bypass grafting (CABG) reduces the incidence of sudden death in selected patients with chronic stable angina, particularly those with impaired left ventricular function.
Although there are no controlled trials of CABG in cardiac arrest survivors, in at least one uncontrolled study the presence of CABG was an independent predictor of long-term survival.
Data for coronary revascularisation are limited to coronary artery bypass surgery; results of percutaneous coronary angioplasty are not yet available.
Nevertheless, in view of the 30% incidence of restenosis with this technique, it should not be regarded as the revascularisation mode of first choice in cardiac arrest survivors.
On the other hand, in patients who have previously undergone CABG, repeat surgery is often a very risky and unattractive proposition.
In such cases, improvement of coronary blood flow may sometimes be most practically achieved by use of either balloon or laser angioplasty or atherectomy.
In patients undergoing coronary artery revascularisation, epicardial implantable cardioverter defibrillator (ICD) patch electrodes and rate sensing leads can be placed at the time of surgery.
The only patient group with sustained life-threatening ventricular arrhythmias in whom we do not routinely place ICD electrodes at the time of surgery are those with well-preserved left ventricular function who have evidence of reversible ischaemia and who do not exhibit sustained monomorphic VT during electrophysiological testing.
Studies have shown that, while surgical revascularisation alone may render VF non-inducible postoperatively, the substrate for sustained monomorphic VT is seldom abolished by CABG.
In rare selected patients, either those with a discrete left ventricular aneurysm or those with very frequently recurring VT prohibiting ICD use, map-guided endocardial resection may be the treatment of choice.
Antiarrhythmic drug therapy
The beneficial role of beta-adrenergic blockers in reducing the risk of sudden death and reinfarction in individuals with coronary artery disease and a previous myocardial infarction is well established.
We use beta-blockers in all patients with cardiac arrest due to VF, irrespective of aetiology, unless there is a strong contraindication.
Many of the data regarding the use of other classes of antiarrhythmic drugs for the treatment of cardiac arrest survivors antedate the widespread availability of the ICD.
Historically, there have been three methods of choosing antiarrhythmic drug therapy in this high-risk patient group.
Empirical therapy with class I antiarrhythmic drugs (quinidine, procainamide, disopyramide) was once widely practised but has been shown to be at best ineffective and at worst dangerous.
Empirical amiodarone, though probably the best alternative when electrophysiological testing and ICD are not available, is associated with a 10% recurrent cardiac arrest rate at 1 year (table I).
About 50% of cardiac arrest survivors show high-grade ventricular ectopic activity, which suggests a possible role for drug selection guided by ambulatory ECG monitoring.
Interest in this approach has been reawakened by the preliminary results of the Electrophysiologic Study versus Electrocardiographic Monitoring (ESVEM), which compared ambulatory ECG monitoring with electrophysiologically guided drug therapy in patients with sustained ventricular arrhythmias.
About 20% of patients recruited for this study were cardiac arrest survivors.
ESVEM showed no difference in the predictive accuracy of ambulatory monitoring or electrophysiological testing in guiding the selection of antiarrhythmic drug therapy in this population.
Whether the results of ESVEM can safely be applied to all survivors of out-of-hospital cardiac arrest is uncertain.
As with ESVEM, none of the previous studies of the use of ambulatory monitoring has dealt exclusively with cardiac arrest survivors.
In one study in which all patients had experienced either an episode of VF or VT associated with hypotension, effective antiarrhythmic therapy could be defined in 52% of patients by use of ambulatory monitoring.
For drug responders, the incidence of sudden death at 1 and 5 years was 5 and 29%, respectively, and for non-responders 19 and 48%, respectively.
Programmed ventricular stimulation not only helps to guide the selection of antiarrhythmic drug therapy but also provides important prognostic information.
The results of electrophysiological testing and the degree of left ventricular dysfunction are the two most powerful independent predictors of recurrent cardiac arrest in this patient population.
Electrophysiological assessment should not be limited to programmed ventricular stimulation but should include a comprehensive evaluation of sinus node, AV node, and His-Purkinje function.
Other potential substrates for life-threatening arrhythmias such as an accessory AV connection should likewide be excluded.
In cardiac arrest survivors, sustained monomorphic VT will be induced in 40–50% of patients (highest in patients with coronary artery disease), polymorphic VT/VF in 20–25%, and non-sustained VT as many as 10%; over 20% may have no inducible ventricular arrhythmia.
Although induced polymorphic VT/VF is often regarded as a non-specific response in patients with syncope or recent myocardial infarction, there is some evidence that, in cardiac arrest survivors, polymorphic VT/VF may be a valid endpoint by which to evaluate the efficacy of therapeutic interventions.
The ICD has added a third powerful treatment method to coronary revascularisation and antiarrhythmic drug therapies for the management of cardiac arrest survivors.
These three treatment options should not be regarded as mutually exclusive; rather, their relative roles should be tailored to the needs of the individual patient.
However, in recent years, particularly following the results of the Cardiac Arrhythmia Suppression Trial (CAST), there has been a gradual move away from lengthy serial electrophysiologically guided drug testing in cardiac arrest survivors; many clinicians now regard the use of the ICD as the treatment of choice for this patient group.
There is general agreement that this device has considerably reduced the incidence of sudden cardiac death, with an incidence of 1.9% at 1 year and 4% at 5 years in the largest reported series of 555 patients.
Nevertheless, the impact of the implantable defibrillator on total mortality has not yet been firmly established in randomised controlled trials, although several such studies are in progress (table II).
Until the results of these studies are available, our clinical practice is guided by the significant reductions in both sudden death rate and cardiac mortality in cardiac arrest survivors treated with the implantable defibrillator by comparison with historical controls.
Clinical choices in specific patient subgroups
In patients with a recent cardiac arrest and a left ventricular ejection fraction less than 30%, we recommend insertion of an ICD irrespective of the results of programmed ventricular stimulation.
Previous data, before the availability of the device, showed that, in this patient subgroup, suppression of inducible sustained ventricular arrhythmias is associated with a significantly better prognosis than if a suppressive regimen cannot be found.
However, the 4-year actuarial incidence of recurrent cardiac arrest in the former group is still unacceptably high at 33% (vs 55% in non-suppressed patients).
For those patients with a left ventricular ejection fraction less than 30% who have no inducible ventricular arrhythmia at baseline electrophysiological study, the prognosis is intermediate between the above two groups and an ICD is likewise indicated.
However, this patient group will include some individuals with severely compromised left ventricular function in whom insertion of an ICD may prevent sudden death but cannot realistically be expected to extend overall survival.
Patients with advanced (class IV) heart failure should not, in general, undergo ICD implantation.
For patients with lesser degrees of heart failure, left ventricular ejection fraction alone is not a sufficiently accurate measure of survival potential to deny an individual patient ICD therapy.
In the future, more detailed assessment of functional capability — eg, aerobic exercise capacity and the assessment of haemodynamic responses to exercise or pharmacological manipulation — may provide additional prognostic information and help to identify patients who are unlikely to benefit from ICD therapy.
Patients with coronary artery disease, moderately impaired left ventricular function, and inducible VT comprise the largest subgroup of cardiac arrest survivors.
In this patient group, the use of intravenous procainamide may help to determine the likelihood of achieving a suppressive oral antiarrhythmic drug regimen.
Once again, left ventricular function plays a key role.
If the left ventricular ejection fraction is less than 40% and the patient responds to intravenous procainamide a suppressive oral drug regimen will be found in a high percentage of patients.
If the ejection fraction is less than 40% and the arrhythmia is not suppressed by intravenous procainamide, the chance of finding a suppressive drug regimen is less than 20%.
In this latter group of patients it may be more efficient to proceed directly to a trial of amiodarone or ICD placement rather than to proceed with a lengthy course of serial drug testing.
Although the value of programmed stimulation in assessing efficacy of amiodarone is debated, ventricular stimulation 7–10 days after oral loading with 1200–1800 mg/day does provide useful prognostic information.
If a rapid haemodynamically unstable ventricular arrhythmia is induced, or the clinical arrhythmia is induced more easily, then amiodarone therapy is unlikely to be effective and an ICD should be inserted.
In patients with well-preserved left ventricular function whose arrhythmia is suppressed by intravenous procainamide, serial drug testing will identify a suppressive regimen in most cases.
However, because of concerns about the long-term efficacy and safety of class I drugs in patients with advanced and progressive structural heart disease, we no longer use these agents alone in cardiac arrest survivors in the absence of an ICD.
This essentially limits the choice of antiarrhythmic drugs to class III agents in a large majority of patients with previous cardiac arrest.
Patients with left ventricular ejection fraction greater than 30% and no inducible ventricular arrhythmia comprise a heterogeneous group.
Those with near-normal left ventricular function and advanced obstructive coronary artery disease in whom reversible ischaemia is believed to be responsible for cardiac arrest can be treated with revascularisation alone.
In patients with dilated cardiomyopathy with mild/moderate impairment of left ventricular function, those with valvular disease, hypertrophic cardiomyopathy, or with cardiac arrest in the setting of a structurally normal heart, ICD implantation is generally recommended.
Radiofrequency catheter ablation of VT is a useful adjunctive therapy in patients with sustained monomorphic VT in the setting of coronary artery disease.
It is especially useful in patients with frequently occurring, slow, haemodynamically well-tolerated tachycardia.
At present, however, this technique is very infrequently applicable to cardiac arrest survivors and is not recommended as sole therapy for this patient group.
The above recommendations are broad guidelines: therapy should be chosen to meet the needs of each patient, in whom many other factors, including individual preference, will play a key part.
Implantable cardioverter defibrillator
When the concept of a wholly implantable automatic defibrillator was the first suggested by Mirowski in the 1970s it met with a sceptical and even hostile response from some sections of the cardiological community.
Despite this opposition, the first human implant took place in 1980.
Initial ICD systems were capable of delivering up to four 25–30 J countershocks between a coiled spring electrode (to maximise surface area) placed at the superior vena cava/right atrium (SVC-RA) junction and a mesh patch electrode placed over the cardiac apex by thoracotomy.
The SVC coil/left ventricular patch configuration was later replaced by a two-patch epicardial system.
Tachycardia recognition was initially guided by a combination of heart rate and the morphological characteristics of the electrical signal obtained from the electrodes used for defibrillation.
Signal morphology was analysed by the probability density function for the amount of time the signal spent away from the electrical baseline.
Sinus or supraventriuclar rhythms spent a smaller proportion of time away from the baseline that the sinusoidal rhythms of VT or VF.
Tachycardia recognition is now more commonly achieved by heart rate analysis alone, with either separate epicardial, extrapericardial, or endocardial sensing electrodes.
Early ICD devices were characterised by their lack of programmability, moderately long charge times, early battery depletion (<18 months), and lead fractures.
For example, the rate criterion for tachycardia detection could be chosen only by request from the factory and could not be altered after implantation.
These limitations contrast starkly with the flexibility and multiprogrammability of current ICD devices.
Tachycardia recognition seldom proves difficult, pulse generators have a projected life of 5 years, and tachycardia therapies can be tailored to the individual needs of the patient.
Devices are now available that can sequentially deliver antitachycardia pacing, low-energy synchronised cardioversion, and high-energy defibrillation in a tiered fashion.
For those patients with pace-terminable VT, antitachycardia pacing avoids unpleasant and energy-consuming countershocks.
A ‘second look’ capability to confirm that the arrhythmia is still present during device charging avoids unnecessary shocks for self-terminating arrhythmias.
These devices also incorporate the ability for back-up bradycardia pacing.
Storage of intracardiac electrograms, which allows objective analysis of triggering events and therapies together with transtelephonic device interrogation, all facilitate patient management (fig 1).
ICD lead sytems can be inserted by median sternotomy at the time of coronary or valve surgery or by a more limited left lateral thoracotomy or a subxiphoid approach.
ICD implantation can now be accomplished with a mortality rate (including those 30% of patients who undergo concomitant bypass surgery) of considerably less than the 3–5.4% reported for early implantation experiences.
Significant reductions in operative morbidity and the length of hospital stay are already being achieved with the use of non-thoracotomy ICD systems.
These systems use two endocardial defibrillator coils placed transvenously either as a single lead (12F) or two separate leads.
One coil electrode is placed at the SVC-RA junction and the second in the right ventricle (fig 2).
In most patients, a subcutaneous left axillary patch electrode is used in conjunction with the endocardial leads for effective defibrillation.
Rate sensing is achieved by use of the distal tip of the right ventriuclar lead.
With these lead systems, a non-thoractomy approach can be used successfully in about 75% of patients.
The use of biphasic defibrillation waveforms and systems in which the pulse generator casing is used as a defibrillation electrode will probably lead to completely transvenous lead systems with pectoral pulse generator implantation being feasible in most cases.
Since this simplified technique makes widespread implantation a practical option, cost-benefit issues will come to the fore very quickly.
Prophylactic use in high-risk patients
ICD therapy is currently limited to the treatment of patients who have already experienced a life-threatening arrhythmia.
However, there are several groups of patients at high risk for sudden death who might potentially benefit from prophylactic ICD implantation.
These include patients with impaired left ventricular function and spontaneous non-sustained or inducible VT, and those with class II and III heart failure who do not yet require cardiac transplantation.
Others who might benefit are high-risk patients with hypertrophic cardiomyopathy or idiopathic long QT interval syndrome with a history of cardiac arrest or a ‘malignant’ family history.
Several prospective randomised trials addressing prophylactic ICD use are underway (table I).
Availability of non-thoracotomy ICD systems will make the prophylactic use of the ICD more attractive and the technique may ultimately find a wider application in the management of high-risk patients after myocardial infarction.
Tests of autonomic function (baroreflex sensitivity, heart rate variability) in combination with assessment of left ventricular function, ambulatory monitoring, signal averaged ECG, or programmed ventricular stimulation may add specificity to post-myocardial-infarction risk stratification.
This increased specificity, in conjunction with smaller pulse generators and non-thoracotomy lead systems, would make prospective evaluation of prophylactic ICD therapy in high-risk post-infarction patients a realistic and practicable proposition.
Complementary therapies In addition to these impressive developments in ICD technology there have been several important pharmacological advances which, in the long term, may have equally important and complementary roles in the prevention of sudden cardiac death.
We now know that beta-adrenergic blocker therapy reduces the risk for sudden death and reinfarction in individuals with previous myocardial infarction and should be used in all patients with cardiac arrest and coronary artery disease unless there is a strong contradiction.
The beneficial role of aspirin in the primary and secondary prevention as well as in the acute management of myocardial infarction has become increasingly clear.
Thrombolytic therapy for acute myocardial infarction and the use of angiotensin converting enzyme inhibitors to prevent ventricular remodelling and dilatation may help to preserve left ventricular function, which is a powerful determinant of risk for sudden as well as non-sudden cardiac death.
VIEWPOINT
WHO recommendations for IVF: do they fit with ‘Health for All’?
Introduction
Recently, two World Health Organization (WHO) reports on in-vitro fertilisation (IVF) were published; one from the headquarters in Geneva, and the other from the Regional Office for Europe in Copenhagen.
The reports illustrate the tensions between clinical medicine and public health in the formulation of health policy.
The Geneva meeting used a clinical approach to health policy development which focuses on individual patients and their available treatment options, whereas the Copenhagen meeting used a public health approach which strives for the best mix of curative and preventive health services, given the existing resources, to maximise the health status of a population.
Both approaches are necessary and, in fact, complementary, yet there may be misapprehension about the appropriate place of each, leading occasionally to unnecessary conflict even within an organisation.
Content analysis of the two reports
The participants at the Geneva meeting included one lawyer and sixteen physicians or biologists involved in research on, and who provided clinical services with, the new reproductive technologies.
The Copenhagen meeting was attended by three physicians who provided clinical services with the new reproductive technologies, one neonatologist, one lawyer, two sociologists, one psychologist, two economists, two epidemiologists, two health service researchers, one consumer advocate, and one representative of the media.
The reports were addressed to different audiences: the Geneva report to ‘those concerned with the planning and implementation of programmes’, and the Copenhagen report to health policymakers and governments.
Effectiveness continues to be a source of confusion in assisted conception.
Although the two reports agree that the best definition of effectiveness is the number of livebirths per 100 treatment cycles, the Geneva report gives the ‘ultimate success rate’ as judged by livebirth rate to be 25%, a figure not based on population data.
In contrast, the Copenhagen report cites rates derived from population-based data (eg, around 10 livebirths per 100 treatment cycles) because effectiveness rates for individual clinics are not useful for health planning.
Prevention is a cornerstone of public health, yet the Geneva report does not include recommendations or discussion of the prevention of infertility.
The Copenhagen report, however, discusses options and makes recommendations for the integration of preventive health services into an overall plan for the management of infertility in the community.
Contrasting views on health services planning are seen in the two reports.
In clinical practice a new procedure becomes accepted treatment when it is offered by a critical mass of physicians or centres.
Geneva states that in the 1980s, ‘IVF and allied procedures changed from being purely experimental in character to become accepted treatments for certain types of infertility and the numbers of centres offering them increased rapidly’.
In contrast, Copenhagen recommends that a technology or procedure should have proven effectiveness, safety, and benefit, as evaluated by clinical trials and other epidemiological methods, before acceptance as standard treatment.
Clinical practice and public health often differ over the rationing of health care.
From the clinical perspective, services should be available to all who request them.
Geneva recommends: ‘Respect for the principle of quality of services requires the availability of medically assisted conception to the population requiring such service’.
On the other hand, the public health approach requires service provision to be determined by the prevalence of the condition, the priority for infertility services within all human services, the medical and social options available for infertile people, and consumer views and choices.
Two subsequent recommendations make it clear that the public (not just clinicians or infertile people) must have a voice in setting these priorities.
Differences between clinical medicine and public health in their views on quality assurance are also illustrated.
The Geneva group describes peer review, voluntary, and mandatory monitoring schemes, but makes no recommendations.
Copenhagen recommends mandatory reporting of treatment outcomes and adverse reactions, and is disenchanted with peer review as a quality assurance panacea: ‘The monitoring and audit of IVF centres must be done by a team, the majority of members having no direct involvement in providing IVF’.
How much control of clinical practice should be included in the monitoring schemes?
There are three contentious issues in assisted conception: age limits in eligibility for services; the maximum allowable number of stimulation cycles to be tried in each woman; and the maximum allowable number of eggs or embryos to be transferred in one cycle.
The Geneva report lists the problems associated with age, cycles attempted, and eggs or embryos transferred, but makes no recommendations for standards of practice.
In contrast, Copenhagen states: ‘Governments should consider limiting eligibility for IVF to women of 40 years of age or under’; ‘Governments should consider limiting the number of IVF treatment cycles per woman’; and ‘All clinics should…0 limit the number of eggs or embryos transferred during an IVF treatment cycle to no more than three’.
Moreover, because accountability is an essential element in quality assurance, the Copenhagen report recommends that: ‘the results of the monitoring and audit should be available to the public’.
The Geneva report discusses the problems involved in public accountability but makes no recommendations.
Quality assurance is ultimately dependent upon sanctions, an unpopular measure among clinicians, for failure to comply with practice or to report treatment outcomes.
No mention of sanctions is made in the Geneva report, whereas Copenhagen recommends: ‘Sanctions for non-compliance with established standards should include closing the centre’.
Health policy formulation includes recommending research priorities.
The Geneva report has 23 recommendations for research into laboratory and clinical problems, but the need for epidemiological, social, and health services research receives little attention.
The Copenhagen report, however, gives priority to epidemiological, social, and health services research.
Discussion
To achieve an integrated health policy and to improve the health status of populations, the public health approach is essential.
So why has WHO headquarters, which is funded by member states to give advice on setting health policy, used its limited resources to organise a meeting for setting policy on a controversial technology, and invited as participants, with one exception, people whose only qualification is that they develop and provide that same technology?
Clinicians are primarily interested in clinical issues and, therefore, are less inclined to address questions of health planning, quality assurance, health services research, and health policy.
Moreover, even if what is needed is a review of the present state of laboratory-based and clinical aspects of the new reproductive technologies, it would be more appropriate to use the resources of universities, government agencies, and professional societies for this purpose.
Given that countries are struggling to develop a rational system for formulating health policy, based on democratic principles and the principles of public health found in the WHO's ‘Health for All’ documents, might not the WHO assist by formulating recommendations on health policy with these same principles?
BOOKSHELF
The Language of the Genes
Biology, history and the evolutionary future.
— Steve Jones.
London: Harper Collins.
1993.
Pp 236. £15.
ISBN 0-002550202.
When Theseus took away the slain Minotaur's head, did the Cretans barbecue the rest?
It seems quite likely, since the ancient inhabitants of Crete and other Mediterranean islands wiped out the indigenous populations of elephants, pygmy hippos, giant dormice, and owls, for no better reason than that they needed to eat.
Man is a fairly disgraceful species.
Perhaps this is why Steve Jones has preferred to spend most of his career studying snails.
He was never keen on dissecting them but turned his attention instead to the banding patterns on their shells.
These patterns and the genes that encoded them enabled him to study snail evolution — how and why snail populations diverged from each other.
In his Reith lectures, broadcast in 1991 and now published as a book, he has set aside his snails to give us a glimpse of human evolution through his geneticist's eye.
The charred remains of long-forgotten feasts tell us something of our forebears' habits, but when it comes to tracing human evolution there's nothing to beat comparison of genetic polymorphisms between modern populations.
Thor Heyerdahl's cherished notion that the Pacific islanders are descended from South American voyagers drifting westward doesn't stand a chance against the genetic evidence, which says that they came from Asia.
Genes also tell us that native Americans originated from a small group of migrants who crossed the Bering land bridge from Siberia.
These founders also took their genes, virtually unchanged, on to South America, where their DNA is immortalised in 3000-year-old Chilean mummies.
Jones' book is a witty tale of curious mutations, molecular clocks, and genetic bottlenecks; and it illustrates biological principles with memorable examples from everyday life.
Thus, the short fat people of the chilly north, who conserve heat better than tall lean Africans, demonstrate one of a number of ‘ecological rules’.
The child of a black white mating, safe from both cystic fibrosis and sickle-cell disease, is hybrid vigour personified.
Language differences, a huge barrier to mating, exemplify genetic isolating mechanisms.
Population genetics owes its origin to Francis Galton, who put the study of human heredity on a mathematical footing.
Unfortunately the darker side of human genetics is also Galton's responsibility, although his ideas for ‘improving the human stock’ through selective breeding were the product of his undeniable eccentricity rather than malevolence.
Galton, whose 300 scientific papers included a statistical study into the efficacy of prayer, a dissertation on arithmetic by smell, and a survey of good and bad temper in English families, was also a keen cartographer: when in Africa he even measured women's buttocks with a sextant.
The idea of mapping the human genome, a ‘stupefyingly tedious task’ in Jones' estimation, would have appealed to him.
And he would have approved of Jones' description of this mammoth work as ‘a map of Mr Average’— it has a nice statistical ring to it.
That a taxonomist from Mars, armed with a DNA hybridisation machine, would classify men, chimpanzees, and gorillas in the same family comes as no surprise.
That the story of evolution is a story of genetic errors perpetuated may not have crossed our minds.
Put like that, all we can do is gasp and exclaim ‘Isn't natural selection wonderful!’.
Jones predicts that we are entering a period of enhanced genetic wellbeing, as international travel and migration bring new opportunities for outbreeding.
The benefits, he says, will dwarf the efforts of scientists to improve genetic health.
Controversies in Obstetric Anaesthesia
Edited by Barbara Morgan.
London: Edward Arnold.
1993.
Pp 139. £19.95.
ISBN 0-340557389.
August Bier (1861–1949) said ‘In America there exist professional anaesthetists.
This specialty is also being praised in Germany.
I cannot think of anything more dull.’
Obviously he was not referring to obstetrical anaesthesia, in which controversy thrives.
No other subspecialty in anaesthesia has as many conflicting papers on the same subject or as many opinions as to the best way to provide analgesia/anaesthesia to the patient.
There are several reasons for this disharmony.
First, pregnancy is always likely to trip up the unwary anaesthetist.
The physiological changes of pregnancy alter responses to drugs, normal anatomy (eg, the position of the airway), and cardiovascular responses to postural change, anaesthetic agents, and surgery.
Second, anaesthesia for the parturient is a case of ‘two for the price of one’.
Administration of a drug to the mother also means its administration to the fetus or neonate through placental transfer or breast milk.
This dual effect frequently incites fears in the mother that the anaesthetic may harm her offspring.
This fear is often out of proportion to reality — the glass of sherry consumed at bedtime throughout the pregnancy is more likely to produce important long-term adverse effects.
Third, emotion is an important factor to take into consideration.
There is no other area of anaesthesia or medicine in which rare events (such as aspiration or epidural haematoma) dictate the pattern of practice so strongly.
Do our surgical colleagues stop doing total hip replacements for pain because of the incidence of possible fatal embolism?
No; yet variations on this theme commonly take place in obstetrical anaesthesia.
Finally, obstetricians and anaesthetists frequently hold different views on management.
The woman who believes that pregnancy is a physiological process and not a pathological one may decide that neither of her doctors is correct.
As these debates frequently arise in the delivery suite, anaesthesia is often at the centre of this maelstrom.
Controversies in Obstetric Anaesthesia is a summary of debates on common topics of disagreement.
The debaters are well-seasoned obstetric anaesthetists from both sides of the Atlantic.
The reader will find the lucid and well-referenced arguments both educational and challenging.
The subjects covered include: anaesthesia for caesarean section (percentage of nitrous oxide for general anaesthesia, maintenance of general anaesthetic skills, fetal distress, and spinal anaesthesia), regional analgesia/anaesthesia (prophylactic epidural blood patch, epidural opiates, and fluid preloading), and updates on previous debates.
William Hazlitt (1778–1830) stated, ‘When a thing ceases to be a subject of controversy, it ceases to be a subject of interest.’
This will never happen with obstetrical anaesthesia since almost every subject is controversial.
NEWS
London Perspective
How to finance health care
Mao lives on.
Revolutionary thought continues in the NHS.
Not content with a restructured health service, a reorganised community care system, and a reinvigorated preventive health strategy, health managers have decided to take another look at NHS finance.
So far the exercise has been confined to the managers' professional body, the Institute of Health Service Management (IHSM).
But it was the IHSM who five years ago produced a report suggesting that new ways of delivering health care should be explored including separating providers from purchasers.
No-one thought then that much would come of the idea, but just two years later it was picked up by Margaret Thatcher's review and emerged as a central plank of the restructured NHS.
Now the IHSM has published six reports with some uncomfortable recommendations for ministers, including the need to be much more open and explicit about rationing decisions.
The root of the problem is familiar enough; the low proportion of gross domestic product spent on health care in the UK compared with other developed nations.
The managers produce a league table of 24 states with the UK third from the bottom with only Greece (5.5%) and Turkey (4%) spending less than Britain's 6.2%.
The figures relate to 1990, before the traditional political increases began with the approach of the 1992 election, but the pre-election boost is already being pruned back.
In a world in which voters place health at the top of their concerns, the UK is still only spending just over 6% compared with 7% plus by Belgium, Finland, Ireland, Italy, Luxembourg, New Zealand, Norway, and Switzerland; 8% plus by Australia, Austria, France, Germany, Iceland, Netherlands, and Sweden; 9% plus by Canada; and dear old USA on its own in the stratosphere with 12% plus.
If only the Government would finance the English system as generously as the Scottish, the UK would at least reach the ‘seven per centers’.
Paradoxically, it was the financial crisis in the NHS that prompted Mrs Thatcher to set up her review in 1988.
Yet the one area that was ignored by the review was finance.
The last serious look at finance was the Royal Commission on the NHS that reported in 1979.
It duly noted that extra money would not necessarily buy better health.
As it pointed out, we could double our expenditure on dentists but if people were still not prepared to brush their teeth (and governments still refuse to fluoridate drinking water) then the same number of teeth would probably have to be extracted: an appalling 8 million that year, with almost 30% of the people in the country having no teeth of their own.
Having run through the preventive health catechism (more exercise, less tobacco and alcohol, and better diet) the 1979 report bluntly dismissed the idea of replacing Britain's tax-funded health service with some form of insurance scheme.
The Tories had been talking about just such a move in opposition.
The Royal Commission concluded: ‘Some advocates of an insurance system evidently see it as a mechanism for automatically increasing expenditure on the NHS as costs rise.
They delude themselves if they do’.
The Commission observed that if the founding principles of the NHS were to be preserved (health care for all, according to need, free at the point of service) then the only insurance option would be a compulsory one.
Yet taxation was a far more efficient method of collecting premiums and distributing payments than a separate state insurance scheme.
The Commission went on to rule out all the obvious options: supplementary insurance, charges, a hypothecated tax, local tax, and even a state lottery.
It relied instead on retaining general taxation (plus national insurance, which is income tax by another name although more regressive) as the main source with the hope that a growing economy would provide the NHS with the extra necessary resources.
Has anything changed since the Royal Commission?
After two years of negative growth, the economy did grow in the mid 1980s at a phenomenal rate.
Health expenditure increased, but not by enough to lift us to a comparable position with our main industrial competitors.
There was another more serious development: the dramatic change in the structure of taxation.
Radical cuts to income tax plus large increases in national insurance and indirect taxation have made the tax system far more regressive.
The direct tax take is now more than £30 billion below what it would have been if the old system had remained in place and indexed.
The richest 1% are now paying £15,000 less in tax on the £52,000-plus salaries.
Two consequences flow from these changes.
First, the NHS is no longer as progressively financed as it once was because of the current regressive system of taxation.
Second, obtaining extra funds for the NHS will become increasingly difficult because of the taxpayers' revolt against higher taxation.
Is there another way?
The health managers have taken another look at the options examined by the Royal Commission.
They rule out charges because of their clinical implications — even non-clinical fees like hotel charges are shown to have clinical affects by deterring some patients from coming into hospital or persuading others to leave too soon.
The new report notes that New Zealand, which introduced hotel charges, has subsequently abandoned them because of the cost of collection and their political unpopularity.
Even the new British review of public expenditure has ruled them out.
The managers believe that a social insurance system would be more responsive and transparent, while still providing care free at the point of delivery.
Yet they concede it would also be more complex and bureaucratic, and not really a runner until general taxation is more widely perceived as unable to deliver adequate funding.
Vouchers, top-up insurance, and a payroll tax are also considered but their disadvantages are listed.
The most attractive option would be a hypothecated tax.
The Liberal Democrat Party's proposal in the last election for a 1p increase on income tax for education proved popular.
A similar ear-marked tax for health could prove even more attractive.
Round the World
USA: Health reform and the constitution
The American Medical Association may go to court if the Clinton administration tries to limit doctors' fees or to cap the nation's spiralling health budget.
According to a report presented on June 14 by the AMA's Board of Trustees at the organisation's annual meeting in Chicago, the Fifth Amendment to the US Constitution severely restricts the US government's power to set prices or to limit an individual's spending.
The amendment states that no person shall ‘be deprived of life, liberty, or property without due process of law, nor shall private property be taken for public use without just compensation’.
Thus, the AMA argues, the Fifth Amendment ‘requires that prices must be set at a level that permits any enterprise, including a physician's medical practice, to operate successfully, to maintain its financial integrity, to attract capital, and to compensate investors for the risk assumed’.
The AMA admits that the federal government has set prices on doctors' services before, and those controls have withstood court challenge.
But those limits applied only to services rendered to patients covered by Medicare and Medicaid, government programmes that pay for the care of the elderly and poor.
One reason why the courts upheld the fee limits was that doctors are not forced to treat Medicare or Medicaid patients and thus are accepting the fee limits voluntarily.
If, however, price controls were imposed for all patients, the AMA argues, fee limits would no longer be voluntary and would therefore be liable to Constitutional challenge.
The report also contends that any attempt by the government to establish a fixed budget for the nation's total health care spending also would run foul of the Fifth Amendment.
While the government may decide how much it wishes to spend on health care and it may seek to discourage employers from spending by changing tax incentives, the report said, the Fifth Amendment prohibits the government from ‘barring individuals from using their own funds or their own insurance coverage to obtain appropriate medical care’.
The Clinton administration has said that under its health reform plan all Americans would have access to a basic health care package that would provide all the services most private plans offer today.
Therefore there should be little reason for individuals to want additional insurance.
Nevertheless, there is concern that if wealthy individuals are able to purchase their own higher-priced coverage, a two-tiered health system will develop.
According to the AMA argument, any attempt by the government to prevent individuals having high-priced plans would, again, violate his or her Fifth Amendment rights.
Kenya: Problems for a problem-based curriculum
The Faculty of Health Sciences at Moi University, Eldoret, is now 5 years old, with its first undergraduates half-way through the 3rd year of a 6-year course.
Its curriculum stands in sharp contrast to the strongly traditional course at Kenya's first and much older medical school in Nairobi.
The Moi University course offers problem-based learning, relatively few lectures, early patient contact, and a strong bias towards experience in the community (8 weeks of every year are set aside for this).
Its students are drawn from Kenya's top achievers at school.
Their performance, as far as can be judged at this early stage, matches and in some areas surpasses that of their conventionally trained colleagues 300 km away across the Rift Valley.
Certainly few of the Moi University students state any wish to revert to a traditional course, and many already show enthusiasm for the process of self-learning that underpins the whole philosophy of a problem-based approach to medical education.
Yet there are rumblings of discontent among the Kenyan medical establishment.
The concern is whether proper doctors can be produced from a system that does not include lecture-based preclinical training in basic science, where learning strategy is left to the students, and where time is ‘wasted’ in poorly defined community work.
This ill-informed comment might be laughed off were it not for a threat, prominently reported in the national press, that the Moi University medical students might not be eligible for full registration upon qualification because of their ‘sub-standard’ education.
The same critics also claimed that the Kenyan Medical Association and the Medical and Dental Practitioners' Board (the licensing authority) had been insufficiently consulted about the programme at its inception.
There may be a touch of sour grapes here, for there were in fact extensive consultations at many levels — one prominent Kenyan academic has dubbed the curriculum the most discussed in the world.
The faculty now needs to embark on some vigorous self-promotion to convince the sceptics in Nairobi (and regrettably some of its own staff) about the virtues of its programme.
In the end, the best ambassadors will be its own graduates, but until then the support of its friends both in and outside Africa will be invaluable.
It is fortunate in having strong backing from several prominent schools in America and Europe (including the founding father of innovative medical education, McMaster).
In Africa the medical faculties at Harare, Makerere, and Western Cape are recent converts to problem-based medical education, and it is ironic that the advice of Moi University faculty staff is being widely sought outside Kenya at the very moment that their own programme is under attack from within the country.
As a visiting teacher here, I have been amazed to see what can be achieved in a young medical school without many of the traditional resources.
There are no lecture theatres, no dissecting rooms, a tiny library, and inadequate laboratories.
These deficiencies are being slowly corrected.
To some extent they are compensated for by the availability of a wealth of clinical material, and by the experience of working in urban and rural communities.
The faculty has lately embarked on an exciting programme of research in a large urban community near the medical school, with most of the projects being pioneered and conducted by the students themselves.
This work seems so much more relevant to the training of doctors in a developing country than theoretical exercises in high-technology medicine still promoted in the undergraduate syllabus of many a traditional school.
Australia: Nurse practitioners
The movement to elevate the professional status of nurses has received a boost from the new federal Minister for Health, Senator Graham Richardson.
Richardson has tacitly supported the notion of the nurse practitioner, following recent discussions with the Australian Nursing Federation.
In the senate, Richardson referred to the continuing uneven distribution of doctors in remote rural areas, and implied that the objections of the medical profession to the granting of provider numbers (which enable patients to claim a rebate on services provided) to nurses should be ignored, unless doctors solve the problem of doctor distribution.
Richardson said that, especially in rural areas, it was inevitable that some services would in future be delivered by nurse practitioners.
These services could include prescribing certain medications and the referral of patients to medical specialists.
Nursing organisations in New South Wales have for some time been discussing specific roles of the nurse practitioner with the Department of Health, the outcome of which has been eagerly awaited by the Australian Medical Association (AMA) and nursing organisations in other states.
Newly elected AMA federal president Dr Brendan Nelson has maintained the line that the AMA is opposed to people less qualified than doctors delivering health care usually given by doctors.
He says that the AMA, the Royal Australian College of General Practitioners, government, and nursing professional bodies should consult to decide on how and under what circumstances nurses might be granted independent practitioner status.
He has alluded to existing clinical skills and appropriate postgraduate formal education being necessary before registration could take place and provider numbers granted.
All of this ignores the nurses who have been unofficially deputising in medical roles for many years, according to senior nurse advocates.
The working party in New South Wales is close to making recommendations to the Minister for Health.
The report's contents are presently tightly protected and are expected to be used as a model by nursing organisations in other states.
Medicine and the Law
HIV transmission comes to court in Canada
Medicine and law in Canada have recently been jointly involved in the AIDS issue.
Last autumn a conference in Toronto brought them together for an update and planning meeting.
Legal interest centres on a HIV-infected man charged on several counts for wilfully transmitting the virus to his sex partners.
Two years ago, when he was in London, Ontario, he was thought to be spreading the disease through promiscuous sex.
This upset the local medical officer of health so much that he legally required the man to refrain from having sex.
This was challenged and the matter proceeded to the Ontario Court (General Division).
An interim restraining order was placed on him and has been under appeal.
In the meantime the individual moved to Vancouver, where he was arrested on criminal charges for having sex when HIV positive.
This spring he faced a preliminary inquiry in Ontario, where the Crown was required to show that there was sufficient evidence to warrant a full trial on the charges, and, subsequently, the accused was ordered to stand trial on a number of charges, including criminal negligence causing bodily harm and aggravated sexual assault.
Nevertheless, he was discharged at the preliminary inquiry on charges of administering a noxious substance, namely semen, and committing a common nuisance.
What does all this mean?
Is there a precedent?
In 1987, in R v Thornton, a man was found guilty of giving blood for transfusion when HIV positive.
He was sentenced to 15 months in jail for being a common nuisance, on grounds that he was thought to endanger the life, health, safety, and comfort of the public.
This verdict was upheld at appeal.
In the London, Ontario case, the Court discharged the accused on the charge of being a common nuisance.
The judge found that to be convicted of this offence the Crown must establish that an accused posed a danger to the general public.
In the Thornton case the conduct of the accused had contaminated the blood supply, which presented a risk to the public at large.
But the accused in London, Ontario, had not, as the judge stated, ‘offered himself to the general public’.
Since the evidence was limited to a series of sexual relationships with specific individuals, the judge concluded that there was no basis to prosecute a charge of being a common nuisance.
A critical issue in the medicolegal context is the matter of consent.
Each person has a legal duty to refrain from any physical touching of another without consent, either express or implied.
In the London, Ontario case, each of the three complainants admitted that they had participated willingly in the sexual relationship, so the accused's solicitor argued that there was no evidence of a sexual assault.
This led the judge at the preliminary inquiry to consider whether the complainants' consent had been vitiated by the accused's failure to disclose his HIV status before engaging in unprotected sex.
It may come as no surprise that the Canadian Court turned to an analysis of the game of ice-hockey to address this issue.
The Crown argued that, in the same way a deliberate cross-check to the back of the neck might exceed the implied consent to risk of injury in a hockey game, the known presence of HIV is so inherently dangerous that sex with someone who is HIV positive extends beyond the norm of conduct to which one can validly consent.
In ordering the accused to stand trial on the charge of aggravated sexual assault the judge stated,
‘Should policy-based limits on the freedom of adults to engage in sexual activity be more restrictive than those placed on willing participants in the game of hockey?’
‘Upon review of medical evidence regarding HIV and AIDS together with the evidence of the complainants, I am satisfied that the arguments as presented by the Crown with respect to the vitiation of consent to unprotected sex with the accused are sufficiently persuasive for me to conclude there is some evidence upon which a jury, properly instructed, could…convict of aggravated sexual assault.’
Late last month, the trial judge acquitted the accused on the charge of aggravated sexual assault.
He accepted that the complainants were under no misapprehension about the nature of the acts they had engaged in.
He concluded that the presence of HIV did not vitiate the consent given.
The accused, however, remains charged with criminal negligence causing bodily harm and the trial continues.
Accountability is, therefore, of considerable importance.
This applies to both free love and apparently stable marriages.
It behoves everybody — victim, potential victim, and even medical attendants — to be honest.
Although some tolerance must be given to those who have unwittingly contracted the disease, the fate of those who cavalierly spread it should be seen less sympathetically.
Nevertheless, opinion as to action is divided.
In common law countries there is no clearcut consensus.
Some punish offenders whereas others counsel.
Since common law is based on precedent, the Ontario verdict, expected shortly, will be viewed with no little interest.
Duty of UK health authority to consult over closure plans
In July, 1992, the bone marrow transplant unit at Westminster Children's Hospital, London, agreed to perform a bone marrow transplant on Rhys Daniels (now aged 2½), who like his 5-year-old sister has Batten's disease, provided the ethical committee approved and a suitable donor could be found.
It was vital that 3 months' treatment be given to Rhys before his third birthday to avoid irreversible deterioration.
Without such treatment, he would soon develop dementia and blindness and would die by age 7.
His sister already has severe symptoms.
However, the hospital was due to be closed and its services transferred to the new Chelsea and Westminster Hospital, London in 1993.
In May, 1992, the regional health authority informed Riverside Health Authority (the district health authority) that the unit's transfer was being reconsidered and it could only commit capital to the unit if the case load was increased and if the unit could withstand a reduction of charitable funding.
In September, 1992, it was clear that the unit would not transfer to the Chelsea and Westminster Hospital.
Proposals to transfer the unit to Great Ormond Street Children's Hospital were being considered; however, there were problems with the negotiations (which ultimately failed in December) and essential staff left.
New negotiations began with Hammersmith Hospital.
In November, 1992, the ethical committee approved Rhys's treatment and in February, 1993, a donor was found, but on April 1, 1993, the unit closed.
Although Rhys has now been offered treatment in Bristol, subject to ethical approval and a suitable donor being forthcoming, ‘valuable, perhaps literally vital time has been lost’.
On his behalf, Rhys's father sought judicial review and a declaration that the decision to close the London unit was unlawful.
A failure to consult in accordance with a clear duty to do so before the decision to close a unit may be ruled unlawful, but may amount to little more than a Pyrrhic victory for the applicants for judicial review.
Here, instead of consulting as they were obliged to, the health authority had kept even the head of the unit, Prof John Hobbs, ignorant of what was happening.
The resultant loss of staff confidence caused important members of the team to leave.
Giving the leading judgment on June 17, 1993, Lord Justice Kennedy said that a district health authority was under a positive duty to consult a community health council on any proposals for the substantial development or variation of the district's health service.
The failure to continue to provide the service offered by the unit anywhere in the regional health authority's area amounted to a substantial variation in the provision of the service.
If this resulted from a positive proposal to shut down the unit it could not lawfully have been brought about unless the district health authority had first consulted the community health council.
The district health authority could not be in a better position because it allowed the situation to drift.
Here, the failure to consult was material because if the community health council had been consulted when it should have been in May, 1992, it might have helped to ensure that the commitment to transfer the unit to the new hospital remained until a satisfactory new home for the unit could be found.
However, although the failure to consult was unlawful, the fact was that the unit had already been closed.
The court therefore exercised its discretion and refrained from granting a formal declaration since it would achieve nothing.
The court also refused to grant a declaration saying that health secretary Virginia Bottomley had broken the law, since she had properly delegated the task of consultation.
A spokesman for the health authority said the family was owed a sincere apology, and Virginia Bottomley (without reference to the case) said she was pleased that Rhys was to receive treatment at the Bristol Royal Hospital for Sick Children and hoped it would be successful.
Noticeboard
Specialty care in London
The uncertainty surrounding the future of London's specialty care services continues despite the publication this week of 6 reviews by working groups set up by the London Implementation Group after publication of the UK Department of Health's response to the Tomlinson report on London's health services.
Each review group was asked to assess current and future needs for each specialty.
Questions about the implementation of proposals, their costs, and staff cuts remain.
Most of the changes, if they become policy, are recommended to take place within the next 3–4 years.
Statutory consultation is expected later this year after receipt of responses to these proposals from purchasers, providers, and professional bodies.
The review of specialist cancer services concludes that the current 15 cancer centres in London should be replaced by fewer larger centres, which, the report says, would be more effective in delivering high quality service, teaching, and research.
The report emphasises the need to strengthen the roles of primary care and of acute local hospitals.
Each tertiary centre should be able to treat 3000–4500 new non-surgical oncology patients per year, and should be on the same site as a major general hospital.
In inner London, the report recommends that cancer centres should be sited at the Royal London, Guy's, Charing Cross (linked to the Hammersmith), and the Middlesex Hospitals.
The Royal Marsden should be based solely at the Sutton site and should combine with St George's to form a new Trust.
Mount Vernon Hospital is unlikely to be affected by the report's recommendations.
The report of the cardiac services review group states that none of London's 14 cardiac tertiary care centres is ideal.
The report recommends that the Royal London and the London Chest Hospitals should be expanded at the cost of the unit at St Bartholomew's, which should close.
Other inner London cardiac tertiary care centres should be sited at the Middlesex, the Royal Brompton, St George's, either Guy's or St Thomas's, and King's College Hospitals.
Closure of Harefield Hospital is recommended, although the transplantation programme is likely to be maintained at a different site (at Northwick Park, the Royal Brompton, or Oxford).
The provision of specialty neurosciences services is recommended through the creation and development of 6 centres to replace the 11 that exist in south-east England.
A joint Royal Free/National Hospital for Neurology and Neurosurgery centre is proposed.
The Royal London, Guy's, Charing Cross or the Hammersmith, and Atkinson Morley's Hospitals should be the other London-based tertiary centres.
Atkinson Morley's Hospital, however, is likely to be subject to further review in 3 years.
5 tertiary centres for renal services are proposed in London, each of which should include a transplantation service and have a university component.
The recommended sites are: Guy's/St Thomas's, Royal London, UCH/Middlesex, the Hammersmith (with links to Charing Cross and maintenance of some services at St Mary's), and St George's.
In addition, 5–7 autonomous tertiary centres should be based in District General Hospitals in counties around London.
These centres should not have transplantation or university provision.
Plastic surgery and burns services should be centred at the Royal Free, the Royal London, Guy's/St Thomas's, St George's, Charing Cross, and Mount Vernon Hospitals.
Specialist services for children (cardiac, neurosciences, nephrology, oncology, and plastics) should each be available in 2 centres (perhaps 3 for neurosciences).
The Hospitals for Sick Children (which should link with UCH/Middlesex and Royal Free paediatric services) is recommended as a centre that could offer the full range of tertiary services.
Guy's/St Thomas's Hospital could become the second major centre, although it faces competition from the Royal Brompton/Chelsea and Westminster for the provision of paediatric cardiac services, and from St George's linked to the Royal Marsden (Sutton) for cancer services.
Money for family planning
Control of population growth is important for economic and social progress in developing countries but whether the world's population will reach two, three, or four times its present size before it stabilises will depend very much on the rate of fertility decline in this decade, says a report from Population Action International.
Surveys indicate that at least 125 million married couples who want family planning do not have satisfactory access to contraceptive information and supplies.
Such estimates of unmet demand, the report points out, do not adequately incorporate the growing numbers of sexually active youths or those discouraged from family planning by poor quality of services.
At a United Nations sponsored meeting in Amsterdam in 1989 the international community agreed that industrialised countries should allocate 4% of overall foreign aid to population programmes, but in 1991 the average amount given for population assistance was only slightly more than 1%.
Of the twenty countries that were members of the Development Assistance Committee (DAC) of the OECD by 1991, only Norway allocated 4% of economic assistance to population programmes in 1989–91.
Finland and the USA reached the high 2% range and Sweden just 2%.
Denmark, the Netherlands, Canada, and the UK allocated 1–2%, while Germany, Switzerland, and Japan allocated 0.5–1%.
Despite their large aid programmes, France and Italy allocated only 0.04 and 0.06%, respectively, to population assistance.
The report describes in detail each DAC country's policy and performance for population assistance.
Japan, for instance, has political support for such assistance, but the report points out that the body implementing bilateral aid programmes has been criticised for tying aid too closely to Japanese commercial interests, such as its high-technology products.
France recognises its poor performance in population assistance and is said to be developing a new programme, which, however, is expected to focus on public education and demographic research and training rather than family planning assistance.
The report calls on donor countries to make a commitment to increase sharply their contributions to make universal access to family planning achievable in the year 2000.
It estimates that if total development assistance were to double in this decade, as it did between 1977 and 1987, the 4% target for donor countries remains adequate for population assistance.
Indian medical journals
The future of medical journals in India may well be grim, if discussions at a seminar on improving them, held at the All India Institute of Medical Sciences in New Delhi, are anything to go by.
A panel of experts did a spot assessment of 10 Indian medical journals and found only one, The National Medical Journal of India (NMJI), to be of international standard.
Concern about the dismal state of medical publishing in India has been voiced before (see Lancet 1992 339: 1589–91).
Seminar participants accused international journals of attracting all the best papers from Indian authors and said that the resulting ‘paper drain’(besides the brain drain, of course) deprives Indian medical journals of material that is rightfully theirs.
Dr Richard Smith, editor of the British Medical Journal, disagrees.
He pointed out that the best papers should indeed go to international journals because it is good both for the author and for international medical literature.
He suggested that to get around the problem, duplicate publication of such papers should be allowed in national journals, with a clear statement of where it was originally published.
Dr Samiran Nundy, editor of the NMJI, however, says that if such papers are on topics relevant to, say India, they should originally be submitted to Indian rather than international journals.
The NMJI offers Rs 1000 (£25) to all accepted papers and a token honorarium to reviewers —‘a policy which has been criticised widely’, says Nundy.
There are other reasons why Indian authors prefer to publish abroad.
In job applications, more weight is given to publications in international journals.
Until recently, there was an unwritten rule in some universities that a PhD candidate would get their doctorate only after their having been published in international scientific journals.
While the perils of ‘publish or perish’ have yet to perturb Indian authors, seminar participants also voiced concern about the fact that out of the 350,000 practitioners of western medicine in the country today, only 4% read journals.
Dr K. Satyanarayana, an expert on Indian medical journals, argues that this is because journals contain little of practical use and interest for the general practitioners.
Perhaps the recent inclusion of doctors in the Consumer Protection Act and the resulting litigations will force doctors in India to read medical journals regularly to keep themselves updated, the seminar participants remarked jokingly.
LETTERS to the EDITOR
Doctors and police custody
SIR,— Your May 15 editorial is timely and wide-ranging both in geography and scope, but not always appropriate to our situation.
As a senior forensic medical examiner (FME) working in the Metropolitan Police District since 1964, my experience is derived from that constabulary, but I know that conditions are similar elsewhere in the UK.
Yes, independence is vital when examining detainees who allege that police have mistreated them; the FME service of the Metropolitan Police is indeed independent, as the courts recognise.
We are not employees and — for the most part— we do not rely on our police fees to keep us solvent.
Our name derives from abrogating the title police surgeon several years ago; the main argument against a full-time service has always been the loss of this independence.
Access by detainees to FMEs and doctors of their own choice is statutory, incorporated in the Police and Criminal Evidence Act 1984.
Visits may well be in private, except if a specific procedure is undertaken — eg, taking blood for identification purposes, which needs to be witnessed.
Additionally, FMEs are free to have prisoners transferred to hospital.
Private access of this kind raises other matters that you mention — confidentiality and informed consent.
If a detainee tells a FME that he wishes to give him confidential information, he is warned by the doctor that confidentiality does not apply to anything against the public interest, for it would be improper for the doctor to collude in deception.
Otherwise, confidentiality is fully respected.
In the matter of police access to HIV status, shedding blood in police stations is common, and police, like health professionals, have their occupational health to consider, but drawing attention publicly to this issue is certainly inappropriate.
Detainees in police stations should have no more difficulty in obtaining therapeutic agents than the average citizen at home, and probably less, since the latter does not generally have a car and driver to collect his medicines from the local pharmacy.
With respect to unhygienic conditions, cells are too often soiled by their inmates.
Keeping remand prisoners in police stations is another matter and is reprehensible.
Most important issues are training and accreditation.
I entirely agree that an FME service should be supported by a professional organisation not linked to the police.
Attempts to start a faculty or institute of clinical forensic medicine failed at the Royal College of Physicians in London; at least the formation of a section at the Royal Society of Medicine (RSM) has been a successful venture.
A university link with an academic society bonding practising FMEs may be the answer.
In the meanwhile the Association of Police Surgeons, the RSM, and several constabularies will continue to promote training to avoid those situations in other parts of the world to which you have properly drawn our attention.
SIR,— Your editorial highlights the neglect of detainees in police custody and the conflict of interest for ‘police doctors’.
This, allied to poor training, poor supervision, and problems with recruitment (especially of women), means that doctors are contributing to the criminal justice system less than optimally.
Reforms, however, should focus not only on the quality of care for detainees and on advice and evidence for the courts but also on services for the victims of crime.
There is often suspicion on both sides: doctors associate the police with uncaring attitudes whilst the police are suspicious of medical intervention.
There are also logistical problems because most violence and much interrogation takes place late at night.
However, both sides have much to offer each other.
Medical sources of information have helped to clarify trends in violent crime especially in relation to unreported domestic and street violence and have identified causes of injury more precisely.
Surprisingly, though, it was the criminal justice system, not doctors, who pioneered the concept of victim support to which many police authorities are highly committed.
Many victims, perhaps still recovering from their injuries, face silent menace from defendants and their supporters or find themselves blamed for their plight, even in the hospital emergency department.
The adversarial system of criminal justice in Britain and the USA, together with less emphasis on community policing, may be partly responsible for the alienation of doctors and police and for a lowering of standards.
Reforms however, must not leave health professionals open to the accusation that they are more interested in detainees than victims.
Well-structured inter-agency collaboration, as with the detection and management of child abuse, would ensure that police recognise that offenders and victims alike have medical needs and that doctors recognise that their patient may be a victim or an offender.
Improved collaboration may also reduce the risk of miscarriages of justice.
SIR,— As FMEs we fully endorse your comment on the importance of independence.
In the Judith Ward case the Court of Appeal, criticising three senior forensic scientists for being partisan to the police, said that one lesson to be learned from that miscarriage of justice was the clear duty of such people ‘to act in the cause of justice’ and that ought to be spelt out to ‘all engaged or to be engaged in forensic services in the clearest terms’.
Independence from police would be illustrated if FMEs were to take on more defence work.
FMEs prepared to advise defence lawyers seem to be in growing demand, and that is borne out by the experience within our group, whose members go out of their way to stress that we work as independent consultants.
Our experience within the Metropolitan Police area is not that the police are suspicious of medical intervention but that they recognised — µespecially since the Police and Criminal Evidence Act 1984 covering treatment of detained persons — that independent and effective medical intervention in police custody is in the interests of justice.
That FMEs are appointed by the police does little to project the reality of independence, and transfer of this appointing responsibility to the Home Office (or the Lord Chancellor) and a register of accredited FMEs may be the way forward here.
Your comment that there should be ‘a clear distinction between medicolegal intervention requested by the police and health care’ may be an ideal but in practice it is often difficult if not impossible to  distinguish clearly between the two.
What starts as a straightforward health issue can turn out to be an important medicolegal examination with implications subsequently at trial.
Informed consent is essential.
We explain the results of our examinations and conclusions to the detainee at the time.
The two-tier structure under which forensic work would only be done by specialists, as envisaged in your editorial, may or may not become a reality, but it will not lessen the need for all doctors to be trained in the elements of medicolegal matters if their work brings them into contact with those in police custody.
We share your concern about the lack of professional training and accreditation but would draw your attention to the joint efforts of the Association of Police Surgeons and the Association of Chief Police Officers in setting up the Durham introductory course for police surgeons.
Excellent training has been offered by the Metropolitan Forensic Science Laboratory for over a decade.
The South East and London Academic Group has been set up to prepare candidates for the diploma in medical jurisprudence.
Clinical forensic medicine is a specialty and the FME's practice falls into it.
Doctors whose work brings them into contact with detainees owe it to themselves and to the detainee to take the trouble to make a special study of the subject and undertake further training.
SIR,— In your editorial you make many observations — some acceptable and some less so.
It seems, however, that your leitmotiv is that no doctor could be expected to carry out the multifacetted role that doctors ‘helping the police with their enquiries’ have to perform, that they cannot do so with their usual amount of skill, and that they cannot carry out some of the more sensitive tasks without bias or being influenced by their paymasters — the police.
With respect to the clinical aspects of the police surgeon's duties, there is no reason to suspect that the medical practitioner in the UK who serves as a police surgeon would in any way be adversely biased against any patient for whom he cares merely because that person is in police custody, whether he has been injured or become ill during enforced loss of liberty, and however such conditions have been brought about.
The present system in no way requires, condones, or restricts the autonomy of the police doctor and of his clinical judgment in terms of investigation and disposal of any of his patients.
‘The doctor is a doctor is a doctor’, first and foremost, in whatever contractual relationship, with respect to duty of care to patients: patients in police custody are no exception.
I accept that the patient may not feel confident enough to speak and discuss medical matters with a police surgeon, especially if allegations are made of trauma acquired during police custody.
The police surgeon will invariably reassure the patient on such matters before his examination.
Indeed, the clinical examination is very often done outwith the presence and earshot of police offices, and the relationship is one between a patient and a doctor.
Nor should there be a reason why the patient should not be allowed to call a doctor of his own choice if the matter to be resolved is only one of diagnosis and medication.
The issue here is one of compensation of the chosen doctor for his visit.
Most general practitioners may be very reluctant to travel in the early hours of the morning to see a patient because he had demanded that his own doctor attends, especially when they find that their practice cannot be reimbursed for such a visit.
The police surgeon is often a close colleague in general practice.
Despite the safeguards in the Police and Crime Evidence Act 1984 in England, one cannot complacently assume that all is well or at least that the service given by police surgeons could not be improved.
There are no nationally agreed guidelines for the appointment of police surgeons: this matter rests entirely on the discretion of the local Chief Constable who has to decide who to appoint from the applicants for the particular post, which is advertised by the police.
There is no stipulation — and very little incentive available — for police surgeons to acquire postgraduate qualifications, or to attend up-dating and in-service training.
Nor is there audit of police surgeon activities, apart from the occasional case that comes to public scrutiny at Coroner's inquests or the Crown Courts.
We should seek improved communication and co-operation between National Health Service emergency medical services, acute psychiatric services, drug abuse and addiction clinics, and the prison medical service.
SIR,— Your most welcome editorial will be the foundation of any public policy formulated to address this most difficult issue.
I should like to remind your readers that in the USA in all incarcerational jurisdictions — municipal, state, and federal — much of the early medical assessment, evaluation, and treatment is done by physician assistants.
This situation has arisen as a result of the serious manpower shortage of physicians willing to undertake these difficult and often conflicting types of care.
Any attempt at clarifying the sundry commitments and obligations for the provider in a detention and/or incarcerational situation must recognise this fact and make sure that a clearly defined ethic is agreed to by the licensing bodies of the so-called physician extender groups, the physician assistants, and the nurse practitioners.
SIR,— In 1988, colleagues and I formed the Forensic Academic Group in the North (FAGIN).
As experienced clinical forensic practitioners we provide training for ‘police surgeons’ but consider ourselves a product of the Association of Police Surgeons' efforts to improve academic standards.
This training covers independence, consent, confidentiality, and inhuman treatment, and the importance of total independence and objectivity is accepted and encouraged by the Greater Manchester Police Force, for whom I am the senior police surgeon.
I and an ‘ex-student’ of this course have formed a university base for clinical forensic medicine within a division of legal medicine attached to the department of pathological studies of Manchester University.
The delay in starting such an institute is not due to any lack of effort by members of the Association of Police Surgeons.
The problem has been one of acceptability, and that may be a reflection of the practical rather than the academic nature of police surgeons' work.
Protein-A-column plasma immunoadsorption in refractory autoimmune neutropenia and thrombocytopenia
SIR,— Protein-A immunoadsorption of plasma is effective in the treatment of patients with refractory idiopathic thrombocytopenic purpura (ITP).
There are few data for its use in other autoimmune cytopenias, although there is a report of successful use in a child with autoimmune pancytopenia.
We treated a 29-year-old man with life-threatening autoimmune neutropenia and thrombocytopenia with protein-A-column immunoadsorption (Prosorba column, Imre Corporation) in combination with COP chemotherapy (cyclophosphamide 600 mg/m, vincristine 1.4 mg/m, both intravenously on days 1 and 8, and prednisolone 25 mg/m orally on days 1–8).
ITP was first diagnosed in 1986.
Failed medical treatment was followed by a 2 year remission post-splenectomy before relapse refractory to all  conventional therapy except cyclosporin 50–200 mg daily.
This response was maintained until September, 1992, when he re-presented with recurrent severe staphylococcal skin infections and severe bleeding manifestations secondary to profound neutropenia and thrombocytopenia.
The neutropenia was considered to be autoimmune in view of the increased neutrophil-associated IgG and active myelopoiesis on bone marrow examination.
He failed to respond to increased doses of cyclosporin, up to 150 mg twice a day.
In December, 1992, with his clinical condition rapidly deteriorating, he started a programme of extracorporeal protein-A-column plasma immunoadsorption with two cycles of COP.
He received six column treatments over 33 days (figure), adsorbing 2 L of plasma on each occasion and improved clinically after four cycles.
3 weeks after the final treatment his neutrophil count had risen from 0.26 to 4.64×10/L and platelets from 5 to 308×10/L.
Over the same period neutrophil-associated IgG fell from 36% to zero and previously increased platelet-associated IgG and IgM became undetectable. 3 months after his final column treatment, he remains symptom-free and with normal counts.
The simultaneous administration of two treatments makes it difficult to differentiate which induced the response.
However, the previous failure of multiple treatments, including vincristine with prednisolone (two of the three components of COP), supports our view that an important part was played by immunomodulation induced by protein A adsorption.
This treatment is worth considering for other patients in a similar situation with life-threatening neutropenia or thrombocytopenia in whom conventional treatment has failed.
Household bleach as disinfectant for use by injecting drug users
SIR,— The US National Institute on Drug Abuse (NIDA) published on March 25 of this year a warning about the efficacy of bleach for cleaning needles, syringes, and other drug-injecting paraphernalia to prevent the transmission of HIV and other blood-borne infections.
Evidence reviewed at a meeting at the Johns Hopkins University School of Hygiene and Public Health had suggested that hypochlorite-based household bleach does not eradicate or inactivate HIV from injecting equipment.
At a 10% dilution (0.525% sodium hypochlorite) household bleach does not inactivate the virus even after 5 min of exposure.
This has serious implications for HIV prevention amongst drug injectors.
In the USA some state legislation forbids syringe provision and possession, and bleach distribution has been the mainstay of many HIV-prevention initiatives aimed at drug injectors.
In the UK syringe exchange is more readily available.
Syringe sharing is less common than it was before 1987, but it has not been eliminated, and bleach is commonly recommended in the absence of sterilised syringes.
In a recent survey of drug injectors in London, 28% (138/499) reported injecting in the previous six months with syringes previously used by someone else.
Of those who shared, all except one made some attempt to clean the syringe and 83% always cleaned shared equipment.
Of those cleaning their shared equipment, 24% used bleach, 58% water (hot or cold, but not boiling), 4% alcohol, and 11% boiling.
Qualitative research has shown that drug users not in contact with treatment services are less likely to have access to sterile equipment and so are more likely to try to clean and reuse syringes.
Of 70 injectors interviewed in depth, only 25% reported never having shared syringes; 57% reporting reusing syringes in the previous six months, using a range of cleaning agents including household bleach.
The new information on the inefficiency of bleach, alongside the trends indicated by our London data, suggests that household bleach should only be recommended as a last resort in the absence of sterile syringes and only then at full strength (5.25% sodium hypochlorite).
NIDA recommends that all equipment should be filled and soaked in full strength bleach for at least 30 s.
However, bleach at this strength may corrode the syringe.
Drug injectors who continue to share syringes should not be encouraged to clean used equipment but to ensure a regular and reliable source of sterile equipment.
Failing this, instructions on how to clean equipment should be unequivocal, with clear guidelines on boiling and the relative efficacy of disinfecting agents.
Healthcare professonals working with drug users should reconsider their policies on the use of bleach.
Tuberculosis beliefs among New York City injecting drug users
SIR,— New York City injecting drug users (IDUs) and the people with whom they live and work are at high risk for Mycobacterium tuberculosis exposure and active disease.
As part of a continuing cohort study of HIV infection among New York City IDUs and their sexual partners, we interviewed 571 subjects between November, 1992, and February, 1993, with respect to their tuberculosis (TB) -related beliefs.
Most subjects were recruited through methadone maintenance clinics: 32% were female, 39% African-American, 35% Hispanic, and 24% Caucasian.
The median age was 38 years (range 18–66).
Mean years of full-time education completed was 12 (range 1–18).
88% reported a history of injected-drug use, and the remainder were sexual partners of IDUs. 50% had injected drugs in the six months before interview.
71% were unemployed with a regular income from the government or sickness benefits; 13% were employed with a regular salary.
30% of subjects worried ‘a lot’ about catching TB, 35% worried ‘some’, and 35% were ‘not at all’worried.
42% said they were actively doing something to ‘avoid getting TB’ and were asked the open-ended question ‘what are you doing to avoid getting TB?’.
Responses, ranked in order of frequency (with more than one response per subject possible), were:(1) avoid people who ‘have it’ or are ‘infected’(36%);(2) avoid ‘risky’environments (subway, crowds, jail)(18%);(3) avoid people who are coughing (13%);(4) do not share cigarettes or crack pipes with anyone (13%); and (5) do not share drinks or food with anyone (13%).
Over half the subjects  interviewed (57%) believed that people with positive purified protein derivative (PPD) tuberculin test can transmit TB to others.
Subjects were asked the open-ended question: ‘why do you think there is more TB going around recently?’.
The six most common responses, in order of frequency, were:(1) people do not take care of themselves/are careless (26%);(2) do not know (22%);(3) lack of medical care/education and prevention efforts (17%);(4) homeless people spread it (15%);(5) HIV (11%); and (6) crowded living conditions (9%).
58% of subjects believed that people with TB should be quarantined, and 63% perceived TB as a severe social stigma, endorsing the item ‘if you have TB no-one wants to be around you, even people you thought were your friends’.
New York City IDUs accurately perceive themselves as vulnerable to TB, and correctly identify some of the major reasons for the recent resurgence in TB.
However, most subjects reported that they are doing nothing to avoid TB exposure, and many of those who report doing something are taking precautions that are not likely to be effective.
Their responses reflect considerable misunderstanding of the specific mechanisms of transmission.
The perceived social stigma of a tuberculosis diagnosis may be so severe as to cause people to avoid testing or treatment.
Failure to understand the distinction between PPD skin test reactivity and active TB may serve as a further disincentive to PPD testing.
In view of the very high risk of active tuberculosis in this population, tuberculosis education needs to be added to existing HIV education and public health campaigns aimed at drug users.
Risks of transmission of HIV to patients and need for follow-up studies
SIR,— In your April 24 editorial you state that the risk for patients exposed to an HIV-infected healthcare worker (HCW) cannot be accurately calculated but must be small.
Although I generally agree with this statement, the continued denial of risk for patients exposed to HIV-infected HCWs without scientific data does not reassure the population.
In fact, it is basically seen as the professionals banding together to protect their own — especially so when set against the various Royal College of Surgeons' statements that testing of patients after operative accidents is reasonable whether or not the patient agrees.
Such apparent double standards need rigorous scientific data if they are to be justified.
The first study of risks to patients reported 400 patients ‘operated’ on by an HIV-infected urologist. 80% of procedures were endoscopies, which are low risk in terms of percutaneous injury for the surgeon and therefore also presumably for the patient.
The estimated parenteral exposure rate for urology was 0 per 1000 operating hours by comparison with 13 per 1000 operating hours for gynaecology, plastic, or orthopaedic surgery.
No active follow-up was initiated for patients but their names were matched with the Florida AIDS register.
For 347 (87%) this matching was done 3 years after surgery and for the remaining 53 it was 2 years after surgery.
No matches were found but no data were available on how many had left the state of Florida.
The risk of developing AIDS by 2 years is around 0–2% and by 4 years only 5–10%.
Clearly, the combination of low-risk procedures and poor follow-up does not provide useful information.
The serological follow-up of other groups of patients exposed to HIV-infected HCWs is also low — for instance, only 75 of 1804 patients in one US study and only 76 of 339 from a UK study.
In a 7-year follow-up of patients operated on by a surgeon in the USA, a review of death certificates of 264 did not suggest any HIV-related death.
The remaining 1896 patients were actively contacted but only 32% responded and were tested.
1 patient proved positive but because the patient was an injection drug user undergoing lymph-node biopsy it was reasonably assumed that this was not an occupational exposure.
Although all but one HIV test were negative such low active follow-up in any other study would be such that it would not even be considered for publication.
Certainly, voluntary anonymised seroprevalence studies are discouraged because it is felt that even a 5% refusal rate may invalidate the results.
How then can we accept as reassurance for the public follow-up studies with inclusion rates of between 4–28%?
Against this rather scanty evidence we have to set the 5 patients probably infected from an HIV-infected dentist.
In addition, we have the experience of hepatitis B infection in HCWs, which has shown an overall transmission rate to patients of 1–9%, but a rate as high as 15% in those patients undergoing caesarean sections or hysterectomies.
The evidence for HIV transmission to HCWs and patients was also reveiwed by the Royal College of Pathologists, and one of their recommendations was that a policy on the follow-up of exposed patients should be urgently developed.
They also urged further research into the frequency and risk factors for patients exposed to blood during invasive procedures.
Without wishing to alarm patients, I believe that we should do a comprehensive anonymised HIV seroprevalence study based around the medical practice of individuals recently found to be HIV infected.
Such patients could be matched with similar HCWs in the same area.
Unless such a study is organised then how long will it be before hospitals begin to pressurise surgeons to be regularly tested?
Can any proposed or established Trust afford the costs of helplines, counselling, follow-up, and possible litigation without sound scientific data that the risks are small and do not justify screening of HCWs undertaking invasive procedures?
Antibody to specific HIV-1 proteins in oral mucosal transudates
SIR,— It has been reported that HIV-1 antibodies can be detected, with a high degree of sensitivity and specificity, in saliva and in oral mucosal transudate (OMT) from HIV-1-infected patients.
It has also been suggested that testing for HIV-1 antibodies in these specimens is cost-effective and suitable for screening in developing countries.
OMT contains immunoglobulin, mainly IgG, and can be collected by an absorbant pad placed between the lower gum and buccal mucosa (Orasure, Epitope).
The procedure is simple and non-invasive and is safer for healthcare workers than venepuncture.
OMT specimens can be  kept for many weeks at room temperature without losing antibody reactivity.
Therefore, OMT specimens may be suitable for future field studies of HIV vaccines in developing countries.
We have investigated HIV-1-specific antibody to gp120, p24, and the HIV-1 V3 major neutralisation epitope in OMT collected from HIV-1-infected individuals, compared with serum specimens, by specific enzyme immunoassays (EIAs).
140 paired OMT and serum samples were collected from 50 British and 90 Tanzanian individuals, including 52 with symptom-free confirmed HIV-1 infection.
All specimens of sufficient volume were screened for antibodies to p24, gp120, and the HIV-1 MN V3 neutralisation epitope.
The OMT were tested at 1-in-2 dilution (1 in 100 for serum) and specific HIV-1-bound antibody was detected by biotin/anti-human-IgG followed by avidin-peroxidase.
Anti-gp 120 was detected in all OMT and sera (table).
Anti-p24 was less common in OMT than in serum, a finding consistent with the results of Soto-Ramirez et al that anti-gag products were less common in OMT when analysed by western blot.
Antibody to V3 was also found in only 77% of OMT samples but in 100% of sera.
OMT or serum from HIV-1-negative subjects was unreactive in our assays.
Antibody titres to gp120, p24, and V3 in 11 pairs of OMT and serum specimens from the UK were measured.
In general, the amount of antibody in OMT is less than in serum.
However, there was an excellent correlation between the specific antibody titres in sera and in OMT. *
This study suggests that OMT is a potential source of specimens for measurement of antibody to gp120.
However, if the response to the HIV-1 major neutralisation determinant or to p24 core protein is of major interst, serum is better.
Oral lichen planus and coeliac disease
SIR,— Dr Fortune and Dr Buchanan (May 1, p 1154) report an association of erosive oral lichen planus with coeliac disease.
As they point out, lichen planus has been associated with a number of different diseases, most recently with chronic liver disease, at least in some populations.
However, most of these associations are rare or even accidental.
We have investigated 103 patients with oral lichen planus for serological or haematological evidence of coeliac disease.
None had a history of coeliac disease; there were 10 patients with a reduced level of serum ferritin, vitamin B 1 2 , or corrected whole blood folate, and 3 patients had reticulin antibodies at a titre over 1/32.
None of those with reticulin antibodies had changes in blood indices or haematinics, except 1 with a reduced serum ferritin, and none had jejunal biopsy evidence of coeliac disease.
Neither of the 2 patients with gastrointestinal symptoms attributed to irritable bowel syndrome had haematological or serological abnormalities.
It would appear that oral lichen planus is only rarely associated with coeliac disease.
Identification by PCR of meningitis caused by herpes B virus
SIR,— We report rapid diagnosis of herpes B virus infection in a neurobiologist who presented clinically with aseptic meningitis.
The patient had no recollection of any recent exposure to contaminated macaque tissues or cells, although numerous such samples were processed routinely by the laboratory.
The patient was treated medically and released without further consideration of potential zoonotic disease.
After hearing about the case third-hand, one of the institution's veterinary pathologists requested that herpes B virus infection be ruled out.
We subsequently applied a technique based on the polymerase chain reaction (PCR) to detect herpes B virus DNA in cerebrospinal fluid (CSF).
This technique has been successfully used in our laboratory for the identification of B virus DNA in previous cases of fatal B virus infection.
200 L of CSF was centrifuged and the pellet incubated for 3 h at 55°C in 100 L lysis buffer (50 mmol/L trometamol, 1 mmol/L edetic acid, 0.5% Tween 20) containing 400 g/mL proteinase K. After inactivation of the proteinase K at 95°C, 40 L of the mixture was used for amplification.
PCR was done in 100 L reaction mixture containing 150 pmol of the primers BV1 (5ACC TCA CGT ACG ACT CCG ACT 3) and BV2 (5 CTG CAG GAC CGA GTA GAG GAT 3) and 2.5 units Taq DNA polymerase.
After an initial denaturation at 94°C for 5 min, the reaction was subjected to 30 cycles of amplification of 1 min at 94°C, 1 min of primer-template annealing at 56°C, and 1 min at 72°C for DNA synthesis.
The BV1 and BV2 primers promoted the amplification of a 128 bp fragment from herpes B virus and herpes simplex viruses (HSV) 1 and 2.
Unequivocal discrimination of herpes B virus DNA was achieved by enzymatic digestion of the B virus DNA amplimer with SacII restriction endonuclease.
Herpes B virus was amplified and identified in the CSF samples of the patient from an ethidium- bromide-stained agarose gel after electrophoresis (figure).
The results were confirmed by hybridisation with an internal oligonucleotide probe specific for herpes B virus.
The patient was treated with intravenous ganciclover 5 mg/kg twice daily for a month followed by oral acyclovir.
A short admission followed by home/work intravenous therapy was a successful approach.
Virus shedding was not observed at any time during treatment.
Recovery from meningitis has been complete with no neurobiological sequelae.
Oral acyclovir has been maintained at 4 g orally per day.
More than 80% mortality has been reported for the 32 documented cases of herpes B virus infection in man.
Survivors not treated with acyclovir often report significant neurological deterioration over the years after the acute onset of infection.
However, favourable outcomes have been noted repeatedly when adequate treatment is given before the onset of central symptoms.
Although herpes B virus infectons in man are not common, 4 researchers have succumbed to infection over the past 5 years.
These events have underscored the importance of using PCR and other techniques that will readily identify herpes B virus infections.
When meningitis of unknown cause occurs, it is important to rule out herpes B virus if there is any history of association with macaques or their tissue/cells.
Detecting and biotyping Vibrio cholerae O1 with multiplex polymerase chain reaction
SIR,— The rapid spread of cholera in Latin America necessitates surveillance for Vibrio cholerae O1 in foodstuffs, water supplies, and individuals with diarrhoea.
Furthermore, there is concern about foodstuffs imported from newly endemic areas.
Current methods of detecting and classifying cholera vibrios take several days and involve culture in alkaline peptone water (APW) and on thiosulphate/citrate/bile-salts agar, slide agglutination with specific antisera, and finally toxin assays.
We investigated the usefulness of the polymerase chain reaction (PCR) simultaneously to identify and biotype pathogenic V cholerae O1 quickly and accurately.
Cholera-causing strains can be distinguished from non-cholera vibrios by the presence of the virulence genes ctxA and tcpA, encoding, respectively, the cholera toxin (CT) enzymatic subunit and the major subunit of the colonisation factor (TCP).
We exploited a sequence difference between classic biotype and El Tor biotype tcpA genes to distinguish the two biotypes.
Bacterial strains in APW were centrifuged, resuspended and heat-treated in lysis solution containing 50 mg/mL proteinase K, 3% w/v Brij 35, and 0.1 mol/L EDTA.
After multiplex PCR with the three primer pairs, the reaction products were fractionated by agarose gel electrophoresis (figure).
The amplified fragment sizes were 301 base-pairs (bp) for ctxA, 617 bp for classic tcpA, and 471 bp for El Tor tcpA.
The identities of the PCR products were confirmed by DNA sequencing.
The detection limit we established was <10 bacteria/mL APW.
Results were available in under 4 h.
We screened a range of pathogenic and environmental strains, including V cholerae of both biotypes and serogroups, V vulnificus, V parahaemolyticus, V hollisae, and diverse pathogenic Escherichia coli strains.
We successfully detected and classified pathogenic cholera vibrios from a range of other organisms in agreement with the published results.
Anomalous results in the field could indicate the emergence of an important new strain worthy of further analysis.
Multiplex PCR offers a practical means to improve V cholerae surveillance operations.
Ethics of experiments in animals
SIR,— Your May 22 editorial contains some remarkable assertions, especially about man's evolutionary status as champion predator.
You should be well aware that evolution can only be viewed with hindsight, and whether our species' rapacious predatory record will prove successful in the long term can only be speculation.
Furthermore, the notion that ethics and morals can be dismissed before the throne of evolution is fascistic.
Similar arguments have been used by white men to justify slavery and genocide of native peoples.
Turning to the issue of the care of laboratory animals, you suggest that ‘the needs of the animal must come first’.
These needs must include the needs to live a natural life and not be used for invasive procedures or be kept in restrictive confinement.
I have no knowledge of conditions in the University of Guelph.
However, several investigations by animal rights workers in the UK have uncovered appalling conditions of animal housing that fail to meet Home Office guidelines, inadequate staff training, and poor animal handling.
In the case of one supplier of primates, this failure has been acknowledged by the UK Home Office.
The British Union for the Abolition of Vivisection does not believe that the needs of animals can be adequately met under present UK laws and monitoring systems.
The Lancet should be calling not for slightly improved laboratory conditions, but a massive reduction in animal use, and a shift of resources to issues of relevance to human health.
Femur bone densities
SIR,— Ms Lees and colleagues (March 13, p 673) establish that the bone density in femora from 18th and 19th century burials in Christ Church, Spitalfields, London, was greater than that in present day women, and relate their data to a current increased incidence of hip fractures.
They suggest that the difference, present at puberty and persisting beyond the menopause, is due to the greater physical activity imposed on women of the earlier period.
Ivanhoe had earlier studied a similar 18th century population from Saint Bride's columbarium in the Fleet Street area.
Thin sections of the first and third molars allowed him to demonstrate in roughly half the skeletons the residual lesions of sunlight deficiency rickets that had developed between infancy and age ten.
At that time the hours of insolation became adequate to the need, and the skeletal lesions healed.
During healing, rachitic long bones become harder and denser than normal, appearing eburnated or ivory-like to older pathologists.
These robust bones of healed rickets provide an explanation for Lees and co-workers' findings.
Virchow, it is worth noting, provided a precedent for the suggestion when he held that the simian cast to bones of Neanderthal man reflected not a separate line in the evolution of Homo but rickets in man living during the Wu∘rm ice age.
A more comprehensive description of the bones would probably reinforce one explanation or the other.
Did any of the femora excluded from the Christ Church study show hip or knee deformities, bowing, or other stigmata of childhood rickets?
My observer bias forbids any attempt at interpreting Lees' fig 1.
Paracelsus
SIR,— As an addition to Professor Feder's thoughtful article on Paracelsus (May 29, p 1396), I would like to draw attention to one most important finding of this giant predecessor of medicine.
In his third ‘defension’ from August 19, 1538, he declared ‘Everything is a poison, the dose alone makes a thing not a poison’.
This statement contains a central concept of modern experimental and clinical pharmacology and toxicology — namely, the dose-effect relationship, a principle of relevance not only to theoreticians but also to clinicians.
It appears to be the most durable consequence of, and could well be considered to be a ‘surviving paradigma’ of, Paracelsus.
Reappraisal of endotoxin in gram-negative sepsis?
SIR,— We should like to compare the ‘classical’ view on the course of events leading to gram-negative sepsis, organ failure, and death with that proposed by Dr Hurley (May 1, p 1133).
Classically, endotoxins are thought to play a central part in gram-negative sepsis by their induction of toxic amounts of cytokines, which cause the clinical symptoms.
Hurley suggests that endotoxin in the bloodstream is an epiphenomenon accompanying the transition of bacteria to their cell-wall deficient L-forms, and that it is these L-forms that are responsible for the symptoms.
Unless one wants to deny the involvement of cytokines in the course of gram-negative sepsis, one is left with the question: which cellular component of the L-form is responsible for the induction of cytokines?
L-forms lack a rigid cell wall, are deficient in peptidoglycan, and possess a single membrane directly surrounding the cytoplasm.
This membrane contains endotoxins; although on a cell-to-cell basis, L-forms contain less, they were sometimes found to be more endotoxic than complete bacteria.
There is evidence that L-forms shed more endotoxins that parent bacteria.
Thus, transition to L-forms is not necessarily followed by a halt in the release of endotoxins.
For the L-form concept to be relevant to the understanding of gram-negative sepsis it is not necessary to dismiss the part played by endotoxin.
The word ‘endotoxin’ may be a misnomer, but we think that there is sufficient evidence obtained by injecting endotoxins into healthy volunteers to prove that it can give rise to the clinical symptoms of gram-negative sepsis.
In our opinion, the novel hypothesis of Hurley is not a reappraisal of the role of endotoxins, but a reappraisal of the bacterial form giving rise to those endotoxins — ie, possibly L-forms.
Tuberculosis in India: question of compliance
SIR,— The figure of 500,000 Indians dying every year from tuberculosis reported by Mangla (May 1, p 1142) makes it clear that three decades of National Tuberculosis Control Programme in India has achieved little — either in reduction in incidence or prevalence.
However, the reasons for non-compliance reported by Mangla are ambiguous.
Poor performance of this programme is due to very high (more than 50%) rate of default in treatment.
Some of the causes of default, such as socioeconomic conditions, cannot be changed in the foreseeable future.
Nevertheless, one important factor — prolonged duration of treatment — can be tackled immediately if short-course chemotherapy is introduced.
In fact, the Government of India has grudgingly adopted this strategy in selected districts.
Motivation of patients during the course of treatment (yet another factor in default) can also be maintained if treatment duration is shortened.
Mangla is not explicit about the World Bank's improved drug delivery system.
It would be tragic if World Bank assistance is used to pursue the ineffective treatment regimen.
Heliotrope poisoning in Tadjikistan
SIR,— The seeds and roots of Heliotropium lasocarpium contain a pyrrolizidine alkaloid that causes toxic liver injury and occlusion of centrolobular veins in the liver.
In southern Tadjikistan, the Farkhar region was under blockade from May to November, 1992.
This led to a famine and a delay in the wheat harvest of 2 months.
At the same time, a drought affected the area, and heliotrope had time to grow and go to seed.
The wheat was collected with heliotrope, ground, and made into bread.
On Oct 15, 6 weeks after consumption of the contaminated bread started, the first case of liver toxicity was reported to the Farkhar hospital and an epidemiological surveillance system was established.
At the request of the Tadjik government, the Association Européenne de Developpement de Santé, and Médecins Sans Frontières, we reviewed data collected in the hospitals of Farkhar and the ten surrounding kolkhozes (state-run farms serving 5000–10,000 people).
Toxic liver injury was defined as such if recorded in the log book of the hospitals.
Up to March, 1993, 3906 cases were recorded, an attack rate of 4%.
The attack rates were respectively 0.4%, 5.4%, 4.0%, 2.8%, and 1.5% for the age groups less than 1, 1–14, 15–30, 31–50, and older than 50 years.
Within the same age groups, case fatality ratio increased with age: respectively 0%, 0.8%, 1.3%, 2.3%, and 5.9%, and the overall case fatality ratio was 1.3%. 2 of the 10 kolkhozes contributed 83.3% of the cases (figure) with attack rates up to 16.9% and 23.6%, compared with less than 3% for each of the 8 others.
Clinical signs and symptoms were recorded in 1102 cases.
Four stages of illness were defined: I, abdominal pain, nausea or vomiting, and weakness; II, hepatomegaly; III, ascites; and IV, alteration of consciousness.
Each stage was represented, respectively, by 55.5%, 29.9%, 13.7%, and 0.9% of the cases.
The last case was reported on March 4, 1993.
At the time of the investigation, 300 cases were still in hospital and it is not known how many will later develop cirrhosis of the liver, to which veno-occlusive disease is known to be contributory factor.
Toxic liver injury: hospital admissions 1992–93.
The population was informed of the danger of eating contaminated bread early in the outbreak.
However, because of the blockade and the related famine, they continued to eat the contaminated flour until the local authorities started an exchange for uncontaminated flour in mid-December, 1992.
Similar outbreaks have been observed previously (India, 1977, Afghanistan, 1975, Iraq, 1970, South Africa and Jamaica in the 1950s, and USSR in the 1930s).
Owing to the difficult political and economic situation faced by ex-USSR nations, such outbreaks may occur again.
They should be detected early and recorded both to increase the awareness of the local population and to motivate prompt reaction from the international community.
Adverse reactions and sumatriptan
SIR,— I was interested in the two case-reports of Dr Luman and Dr Grey (April 24, p 1091).
They correctly point out ‘that the half-life of sumatriptan is 2 hours’ and ‘the intervals between drug administration and cerebral infarction of 1 week in the first case and 12 hours in the second, exonerate sumatriptan’, but they then go on to state ‘no other risk factor for vascular disease was identified in our patients’.
However, migraine itself seems to be a risk factor.
They also state that ‘the hemiparesis in the second case was on the opposite cortical side from the migraine-associated ‘hypoaesthesia’, but in their case-report both the numbness and the hemiparesis were right-sided.
Author's Reply
SIR,— We thank Dr Rose for his letter.
Neither of our patients had typical migrainous headache before their hemiplegia.
However, their headache could have been relieved by sumatriptan, though we are unable to explain the failure of sumatriptan to prevent migraine-induced hemiplegia (if this was indeed the case).
With regard to the second patient, recent correspondence with her general practitioner reveals that she initially developed intermittent leftsided headache and right hypoaesthesia without motor signs 3 years ago when she was in her first pregnancy.
The same symptom recurred a year later when she took the oral contraceptive pill.
Before her mentioned admission she developed leftsided numbness on re-introduction of oral contraceptive and subsequently left hemiparesis 12 hours later after sumatriptan treatment.
She was well on review a month later and did not have any residual motor or sensory signs.
Her GP informed us that she has had a further recurrence of left hemiparesis and, after investigation, the provisional diagnosis is demyelination.
This information was not available at the time of the report in April.
The Committee on Safety of Medicines has received other reports of suspected neurological reactions, including hemiparesis, facial palsy, dysphasia, dysarthria, paraesthesia, and hemianopia  (Committee of Safety of Medicines, personal communication).
We are sorry that we cannot prove a cause and effect conclusively, but suspect that the hemiparesis could be a reaction related to sumatriptan and suggest that intra-cerebral binding sites of sumatriptan should perhaps be studied more closely.
Tretinoin and pregnancy
SIR,— The report by Jick and colleagues (May 8, p 1181) is reassuring in its conclusion that the use of topical tretinoin (Retin-A) in the first trimester of pregnancy does not cause birth defects.
The relative risk of major anomalies among babies born to exposed women was less than among babies born to non-exposed women.
However, the authors qualify their findings by pointing out that the group of exposed women was, in fact, only presumed to have been exposed since they had been selected by a computer matching of topical tretinoin prescriptions to pregnant women 3–4 months after their estimated dates of conception.
The patients do not seem to have been asked if they had used their prescriptions.
Lipson and colleagues subsequently report (May 22, p 1352) from Australia a girl born with multiple congenital defects to a mother, who before conception and for the first 5 weeks of gestation, had used a topical liquid preparation of 0.05% tretinoin in 45% alcohol.
Fairley has reviewed the widespread and sometimes controversial use of tretinoin in the treatment of post-inflammatory hyperpigmentation.
Its safety in pregnancy does not seem yet to have been established.
Almitrine effect on nitric oxide inhalation inadult respiratory distress syndrome
SIR,— Nitric oxide (NO) inhalation in adult respiratory distress syndrome (ARDS) improves hypoxia and decreases pulmonary hypertension.
This improvement may be due to vasodilation of the ventilated zones of the lung inducing a ‘pulmonary steal’ of blood away from hypoxic vasoconstricted zones with low ventilation/µperfusion ratio.
We hypothesised that inhaled NO might be more effective if the pulmonary steal were to be increased by increasing hypoxic pulmonary vasoconstriction.
This was tested in two patients with severe ARDS.
To a constant concentration of inspired O 2 , either a continuous low flow of nitrogen or of NO in nitrogen (225 ppm tank, CFPO, Bonneuil/Marne, France; adjusted to administer 18 ppm) was added.
60 min later, almitrine (Lab Euthérapie, Neuilly/Seine, France), a drug known to potentiate hypoxic pulmonary vasoconstriction, was given intravenously at 16 g/kg per min (patient 1) or 12 g/kg per min (patient 2).
Haemodynamic (arterial and Swan Ganz catheters) and gas exchange (mixed venous and arterial blood gases) characteristics were measured or calculated after 60 min in each situation (table).
The increase in arterial oxygen partial pressure during NO inhalation confirms the benefit previously shown.
Addition of almitrine enhanced the effect of inhaled NO on arterial oxygen partial pressure with only a small effect on haemodynamics.
However, pulmonary arterial pressure increased to the pre-NO inhalation value with an increase in cardiac output when almitrine was given, suggesting that right ventricular function was not affected.
The further increase in arterial oxygen partial pressure with almitrine during NO inhalation suggests enhanced pulmonary blood flow to normal ventilation/perfusion ratio zones, despite the increase in cardiac output and partial pressure of venous oxygen.
Bleeding time prolongation and NO inhalation
SIR,— Recent reports advocate the use of nitric oxide (NO) inhalation as an addition to conventional treatment in acute pulmonary hypertension and abnormal gas exchange in conditions such as adult respiratory distress syndrome, primary pulmonary hypertension of the newborn, and in postoperative pulmonary hypertension after thoracic surgery.
During inhalation, the NO is absorbed by erythrocytes as they pass through the lungs and is thought to be inactivated by tight bonding to the haem, which is subsequently metabolised to methaemoglobin.
Indeed, methaemoglobin formation increases during NO inhalation.
It has been assumed that inhaled NO exerts its effects in the lung and that there are no extrapulmonary effects.
However, we noticed that rabbits mechanically ventilated with small amounts of NO bled for a longer time than controls when arterial catheters were removed, suggesting that systemic effects occur.
We know that endogenous NO from endothelial cells as well as organic nitrates and nitroprusside treatment mediate suppression of platelet adhesion and aggregation.
Examinaton of rabbits exposed to 30 and 300 ppm NO over 15 min showed that bleeding time increased 46 [SE 14]% (p<0.001) during 30 ppm and 72 [20]% (p<0.05) during 300 ppm NO inhalation.
We subsequently studied six healthy volunteers.
The bleeding time was measured by a standard incision (Simplate II, Organon Teknika) on the ventral aspect of the left forearm.
The blood was blotted with a filterpaper every 10 s until no blood appeared on the paper.
The bleeding time was measured before, after 15 min of inhaling NO 30 ppm, and 30 min (n=3) and 60 min (n=3) after stopping NO.
The bleeding-time ratio increased to 1.33 (SE 0.5, p<0.001) after 15 min of inhaling 30 ppm NO. 30 min after the withdrawal of NO, the mean ratio had fallen to 1.14 (SE 0.05) in the 3 volunteers monitored at that time.
The other 3 subjects returned to pre-inhalation values after 60 min (1.04 [SE 0.01], see figure).
An increased risk of bleeding should be borne in mind when exposing patients to inhaled NO.
On the other hand, it may also represent a so far overlooked beneficial aspect of NO.
The exact mechanism behind this systemic effect remains to be elucidated.
Epoetin in elective hip replacement
SIR,— The contention that ‘erythropoietin was well tolerated’ in the article by the Canadian Orthopedic Perioperative Erythropoietin Study Group in elective hip replacements (May 15, p 1227) is unsubstantiated.
They describe the power of their study to detect changes in outcome but the study does not have sufficient power to detect significant differences in the rate of adverse effects.
If the two treatment groups are combined, the relative risk of deep vein thrombosis (DVT) in treated versus untreated patients is 1.92 (95% confidence interval 0.73–5.04).
This does not prove that there is an excess of DVT in the treated patients, but it raises a justifiable concern.
A larger study is needed to make sure that this excess is a chance finding.
SIR,— The main thrust of the Canadian study is that the use of human recombinant erythropoietin (epoetin) in patients with a normal haemoglobin preoperatively reduces blood transfusion requirements.
However, this 23% reduction was achieved by giving large doses (300 units/kg daily) at a cost of Canadian $3938 per patient, yet one-third of the patients still required transfusion.
The 14-day course of epoetin resulted in only a 9 g/L rise in haemoglobin, illustrating how difficult it is to ‘override’ the endogeneous erythropoietin control of red cell production in a non-anaemic individual.
How this dosage of epoetin was decided upon is not mentioned.
The evidence from giving preoperative epoetin to anaemic Jehovah's Witnesses is that 75–150 units/kg three times a week for 2–4 weeks is adequate.
In our experience, doses as low as 60 units/kg three times a week have proved sufficient.
Dose reduction is mentioned in the conclusions of the paper but this could easily have been incorporated in the design; instead, one of the randomised groups in the study received 9 instead of 14 days of epoetin, a seemingly meaningless comparison.
The reluctance of drug companies to sponsor trials in which the optimum dose of drug is sought is all too apparent with cytokines.
For hip replacements at least, and on the basis of the data and costs in this Canadian study, the preoperative use of epoetin in non-anaemic patients cannot be justified.
Successful treatment of haemolytic uraemicsyndrome with recombinant tissue-typeplasminogen activator
SIR,— Haemolytic uraemic syndrome (HUS)— the triad of thrombocytopenia, microangiopathic haemolytic anaemia, and acute nephropathy — is a common cause of acute renal failure in children.
In HUS there are endothelial cell lesions and microthrombia in the smaller arterioles and glomerular capillaries similar to the renal fibrin clots that can be caused in rabbits by the injection of endotoxin.
Glomerular fibrin deposits are absent in animals receiving tissue-type plasminogen activator (tPA) 5 h after endotoxin injection.
The physiological inhibitor of tPA, plasminogen activator inhibitor (PAI1), is increased in patients with HUS compared with patients with acute renal failure for other reasons.
The investigations of Bergstein et al showed a correlation between the duration of raised PAI1 and renal outcome.
A decrease of PAI could be achieved within 2 days by transperitoneal dialysis.
We report the first successful treatment with recombinant tPA (rtPA) in a 4-year-old girl with HUS.
On admission she had been anuric for 24 h (haemoglobin 4.4 g/dL, potassium 4.1 mmol/L, creatinine 937 mmol/L, urea 119 mmol/L, lactic dehydrogenase 3327 IU/L, platelets 71×10/L).
Fluid and electrolyte substitution, together with frusemide, heparin, and packed red cell transfusion, without transperitoneal dialysis was started.
3 h after admission, lysis with rtPA (Actilyse, Behring Thomae, Germany) in a dose of 0.2 mg/kg bodyweight per hour over 5 h was started, subsequently reduced to 0.05 mg/kg bodyweight per hour for 14 days.
2 h after beginning the thrombolytic treatment micturition started again.
Assay of PAI1, tPA antigen, tPA activity, fibrin and fibrinogen degradation products, and D-dimer showed indirect fibrin clot lysis.
Consistent with selective fibrin lysis, fibrinogen, plasminogen, and alpha-2-antiplasmin stayed normal throughout rtPA  treatment.
Creatinine clearance improved continuously, PAI1 was high (38 iu/mL) on admission, but decreased rapidly during rtPA treatment to 20 IU/mL (figure).
D-dimer, fibrin and fibrinogen degradation products were also high initially and then decreased, whilst renal function improved to normal on day 14 and remained so.
As with Bergstein et als' patient, ours had a high PAI1, but whereas they did transperitoneal dialysis to lower the PAI1 and enable fibrinolysis, we rapidly decreased the PAI1 by intravenous rtPA, effective lysis being demonstrated by clinical outcome and the changes in fibrin degradation products.
Our results confirm those from animal experiments, in which the possible benefit of rtPA in HUS has already been shown, and suggest a new approach to treatment.
Abdominal lymph-node granulomas andHelicobacter pylori
SIR,— We report two patients who had granulomas in abdominal lymph nodes and who were subsequently found to have Helicobacter pylori-associated gastritis.
Case 1 — A 65-year-old woman had a duodenal ulcer 28 years previously.
She continued to complain of epigastric pain and flatulence but repeated investigations were negative.
In 1979, gastroscopy showed antral gastritis and duodenitis.
Laparotomy showed duodenal scarring with enlarged lymph nodes in the pyloric and duodenal areas and white nodules in the liver.
Histology of the lymph nodes showed non-caseating epitheloid granulomas with scattered Langhans'-type cells and Schauermann bodies, but no acid-fast bacilli or fungi.
The liver biopsy contained parencyhmal aggregations of epithelioid granulomas of similar appearance.
Chest radiograph, abdominal ultrasound, Kveim test, tuberculin test, brucella serology, and all other routine tests were negative.
3 years ago she had a recurrence of symptoms.
Gastroscopy showed antral gastritis and biopsies showed helicobacter-like organisms (shown by Giemsa staining), but no granulomas.
Treatment with H 2 receptor antagonists, bismuth, and triple therapy (bismuth, amoxycillin, and metronidazole) failed, although she reported improvement after a combination of tinidazole and tetracycline.
Case 2 — This 59-year-old woman presented 5 years ago with a 2-year history of postprandial abdominal discomfort, flatulence, and loose stool.
She had a normochromic anaemia, and a small rise in liver enzymes and erythrocyte sedimentation rate.
Barium meal showed a filling defect in the prepyloric region.
Barium follow-through, gastroscopy, sigmoidoscopy, barium enema, abdominal ultrasound scan, and chest radiograph were all normal.
Her symptoms continued and she underwent laparotomy, which showed enlargement of the pyloric and duodenal lymph nodes, with granulomatous changes similar to those seen in case 1 on biopsy.
Tuberculin and Kveim tests and brucella serology were negative.
Gastroscopy was repeated the following year and she was found to have antral gastritis.
Biopsies showed H pylori.
Treatment with H 2 receptor antagonists, other anti-ulcer drugs, and triple therapy has not led to any symptomatic improvement, although gastric biopsies became negative for H pylori after triple therapy.
Computed tomography scan of the abdomen 10 months later showed no abdominal lymphadenopathy.
These two cases are similar: both middle-aged women with long histories of dyspeptic symptoms that led to laparotomy and the finding of granulomatous lymphadenitis, and subsequently they were found to have H pylori-associated chronic gastritis.
Granulomas can be due to a wide variety of causes but other investigations were negative and there has been no evidence of tuberculosis or other infecton on follow-up.
Kveim tests were negative and sarcoidosis restricted to the abdominal lymph nodes appears to be rare.
H pylori is associated with gastritis and peptic ulcer and possibly with non-ulcer dyspepsia, so both patients may have had chronic infection.
Treatment may eliminate the organism without symptomatic response, and the failure of our patients to improve is not against H pylori infection.
Although H pylori does not invade the mucosa, bacterial proteins may activate monocytes with a local T-cell mediated immune response.
Abdominal lymphadenopathy may be under-recognised, but a recent study of computed tomography scans in 61 patients with H pylori gastritis showed no evidence of remarkable lymphadenopathy.
We suggest that chronic H pylori can cause granulomatous lymphadenitis in susceptible individuals.
Quality control of computed tomograms intesticular tumours
SIR,— Harding and colleagues (April 17, p 999) confirm our observation that patients with malignant germ cell tumours treated in specialist units have better survival than those treated in hospitals that see only a few patients a year with this malignant disorder.
Each year about 90–100 new patients with testicular cancer are treated at our hospital.
The patients are followed up partly at the outpatient department, partly at local hospitals.
All computed tomograms (CTs) done at local hospitals are reviewed here, when the patient is referred initially or at follow-up.
Most Norwegian local hospitals are able to do the initial clinical CT staging of patients and the follow-up examinations.
In March, 1990, radiological consultants from the local hospitals in southern Norway and their colleagues from the National Radium Hospital agreed on procedures for adequate imaging in testicular cancer.
The important recommendation was that all non-seminoma patients should have abdominal (10 or 12.5 mm spacing) and thoracic (20 mm spacing) CTs at diagnosis and at fixed intervals during follow-up; the thoracic CT was to be done with both lung and mediastinal window.
2 years later the CTs of 89 patients from local hospitals were reviewed.
Deviations from the agreed procedure occurred in 10 patients with abdominal and 24 with thoracic CTs.
In 14 of the latter, the spaces between sections were too large or the thoracic examinations were taken without a lung window, and 3 non-seminoma patients had no thoracic diagnostic CT.
29 patients who had CTs at local hospitals had to be re-examined by CT (25) or ultrasonography (4) at the National Radium Hospital, which led to major alteration of management in 4.
Oncologists routinely review histological sections to ensure that optimum treatment is given.
Review of radiological films is, however, less common.
Our study shows that CT imaging was adequate in about one-third of patients at hospitals with limited experience of testicular cancer, which may contribute to the lower survival of patients treated in local hospitals.
Regular quality control seems necessary to achieve high quality CT examinations of testicular cancer performed at radiological units with limited experience in this condition.
Screening for colorectal cancer
SIR,— Dr Bolin and Dr Vatn (May 15, p 1279) both criticise the rationale for our proposal that a single screening flexible sigmoidoscopy at age 55–60 would be a cost-effective means of preventing colorectal cancer.
Bolin suggests colonoscopy at ages 40 and 55; Vatn also supports colonoscopy, but in the early 60s.
There is currently no satisfactory means of preventing proximal colon cancer in the general population.
Colonoscopy is expensive, has to be done by a specialist, is uncomfortable for the patient, and requires complete bowel preparation and often sedation and analgesia.
It also carries a risk of perforation and haemorrhage and, when undertaken repeatedly in low-risk people, this risk may exceed the benefit.
Nor do we yet have evidence that removal of proximal adenomas prevents proximal colon cancer.
For the distal bowel, there is strong evidence for the protective effect of adenoma removal in the prevention of fatal colorectal cancer.
At what age should endoscopic screening be done?
We chose 55–60 for screening by flexible sigmoidoscopy because few cancers occur before age 55 and the prevalence of distal bowel adenomas plateaus towards the end of the sixth decade, suggesting that most adenomas destined to develop in this region will have done so by this age.
No such levelling off has been identified in the proximal colon.
Indeed Vatn and others have demonstrated that proximal adenomas develop later in life and increase in prevalence and multiplicity at older age.
To protect the proximal bowel repeated colonoscopy may be required.
For prevention of death from colorectal cancer at younger ages, a more effective strategy than population screening would be to identify and target high-risk individuals, such as those with a family history of colorectal cancer.
To be effective, a screening procedure should be available and acceptable to the whole population, not just the privileged or health-conscious few.
Offering screening colonoscopy from age 40 (even age 60) would probably prevent more colorectal cancers among those screened but compliance is likely to be low and this strategy may prevent fewer cancers than the 5500 we estimate for our less invasive policy.
The provision of a basic screening regimen in no way prohibits physicians from recommending more extensive or more frequent examinations if clinically indicated.
Reversion of tachycardiomyopathy after β-blocker
SIR,— Chronic tachycardia may induce cardiomyopathy (tachycardiomyopathy).
We report a case that alludes to a facilitating mechanism.
A 43-year-old man was admitted because of congestive heart failure.
The electrocardiogram (ECG) showed atrial fibrillation with a ventricular rate of 180 per min.
Radionuclide left-ventricular ejection fraction was 18%.
The echocardiogram showed global left-ventricular hypokinesia (fractional shortening 17%).
Acute myocarditis, hyperthyroidism, and coronary artery disease were excluded.
Bumetanide, captopril, nitrates, and oral anticoagulants were started.
Despite adding digoxin 0.25 mg twice a day and diltiazem 120 mg three times a day, heart rate before discharge still exceeded 120 per min.
After 6 months on this regimen, left-ventricular function was unchanged (fractional shortening 20%, ejection fraction 19%).
With a modified Naughton protocol, exercise tolerance was severely depressed (11 min) and heart rate was still poorly controlled (figure).
Plasma noradrenaline concentrations obtained at three stages of exercise were indicative of an excessive early adrenergic response to exercise and an attenuated maximum response (figure).
Metroprolol was then added and titrated against heart rate until within a week a final dose of 25 mg thrice daily was attained, resulting in a resting heart rate under 80 per min.
After 4 months, cardiac function was reassessed: fractional shortening and ejection fraction was 33% and 47%, respectively.
Exercise tolerance also improved (24 min).
Moreover, heart rate and adrenergic response to exercise were normal (figure).
In patients with heart failure, catecholamine concentrations are increased at rest and at low levels of exercise, and attenuated at maximum exercise.
Our findings suggest that this ‘inappropriate’ adrenergic response may facilitate the development of tachycardiomyopathy and could be a target for therapy.
Reperfusion arrhythmia
SIR,— The Grand Round on unstable angina (May 22, p 1323) discusses myocardial ischaemic reperfusion and stunning.
Arrhythmias may be another manifestation of myocardial reperfusion injury.
Until thrombolytics and recanalisation for myocardial infarction, little attention had been paid to this possibility in man, although arrhythmias are associated with reperfusion of ischaemic myocardium in animals.
A 65-year-old man had a 3½ h history of severe chest pain and electrocardiogram features compatible with acute inferior myocardial infarction.
He was normotensive and in sinus rhythm.
Coronary arteriography showed occlusion of the proximal  dominant right coronary artery distal to the sinus node artery.
The left coronary artery was normal.
The occluded artery was successfully reopened by percutaneous transluminal coronary angioplasty with a 3.5 mm balloon.
Within 1 min of balloon deflation, idioventricular rhythm developed (rate 93 per min), with a fall in blood pressure to 65/45 mm Hg.
Reinflation of the balloon catheter produced a return to sinus rhythm and a blood pressure of 93/58 mm Hg within 15 s.
However, on balloon deflation, idioventricular rhythm (98 per min) again developed with hypotension (75/55 mm Hg).
A third balloon inflation and deflation resulted in the same events.
A 100 mg intravenous bolus of lignocaine did not restore sinus rhythm, which spontaneously returned 20 min after the last deflation.
Although the mechanisms of reperfusion arrhythmogenesis remain uncertain, hypotheses include cytotoxic oxygen-derived free radicals causing disruption of the cellular membrane and electrical instability.
Studies in animals show these radicals are electrophysiologically active and that anti-free-radical agents protect against reperfusion arrhythmias.
In human beings, free radicals have been detected in coronary venous effluent blood in the immediate post-reperfusion phase of acute myocardial infarction, although the effects of anti-free-radical agents on reperfusion arrhythmias have not been studied.
Pregnancy in elderly women
SIR,— Successful attempts at inducing pregnancies in women after menopause have prompted a response from a higher authority, who feels that publication of His work has been overlooked by modern authors.
I have faithfully copied his words.
‘This reduction to my citation index is probably partly due to the rejection of my paper by another prestigious journal for reporting experimental therapy not done in accordance with ethical rules nor with the Helsinki code or charter.
I was not aware of the need for informed consent nor the prohibition of the use of any physical or psychological constraint in patients or relatives.
This point emphasises the difficulty in access to first rank journals for work from third-world countries where customs and ethical values are different.
I made two attempts.
The first led to a pregnancy in a 90-year-old woman (Sarah X).
To exclude any chance of natural pregnancy, I ensured that on the day of fecundation, all males in the proximity, including her 99-year-old husband (Abraham X), were circumcised.
A boy (Isaac X) was born by natural delivery.
Despite this successful pregnancy, the paper was rejected by The Lancet because of my use of an unethical procedure and for premature disclosure of information before delivery.
Referees also emphasised the lack of a control in the study.
I was not aware that circumcision might be a painful, controversial, and perhaps an unethical procedure, but I planned the next attempt in compliance with the editor's requests.
I performed fertilisation in an old woman (Elisabeth X) and a young control female (figure).
The control arm of the study was carefully designed, choosing a virgin girl (Mary X) among relatives to retain, as far as possible, a similar genetic background.
Premature disclosure of the pregnancy was avoided by rendering Elisabeth's husband (Zacharias X) mute until delivery, with a painless procedure.
Mary's fiancé (Joseph X) was not thought to be a risk in this respect.
The pregnancy was uneventful apart from bedrest for the first four months.
A fine boy (John X) was born to Elisabeth at term, as well as another to the young control girl.
Unfortunately, the information leaked and became known locally.
To my great surprise, people focused attention on the control fertilisation rather than on the core of my work: fertilisation in elderly women.
Press reports were poorly informative: one paper did not even mention Elisabeth nor, incidentally, the procedure I used for fertilisation.
Pressure on the families from the media and neighbours led to psychological disturbances in the children, especially Mary's child.
They became difficult teenagers, leading to disputes with local political authorities.
Both suffered violent deaths.
The sad consequence on the children's lives of the circumstances of their birth led me to the conclusion that pregnancy in elderly women might not be appropriate and the whole programme was stopped.
The emergence of new programmes in the same specialty leads me to make two comments.
The first is the need for those reproducing my procedures to acknowledge the previous work that I and my co-worker Archangel Gabriel defined as God Assisted Pregnancies (GAP).
The second is a warning of the problems that children from such pregnancies may face in later life.’