

ACKNOWLEDGEMENTS
This research study was commissioned by The British National Bibliography Research Fund.
The views expressed in it are, of course, entirely my own and do not necessarily represent those of The British National Bibliography Research Fund or The British Library more generally.
I would like to thank Dr Terry Cannon and Mr Martin Nail of The British Library Research and Development Department for their help and support throughout the course of the research.
The methodolgy of the research has not been innovative.
It has relied on the basic techniques of literature searching, interviews with individuals and organisations and an examination of key industry forecasts, reviews and analysis.
Warm thanks are due to my many colleagues within the information industry and to those organisations who have been so helpful in providing guidance, views, information and explanation.
Tony Feldman London, 1991
ABSTRACT
This BNB Research Fund study examines the development of multimedia, assesses its current implications for key applications areas and considers the likely course of multimedia's future development.
While not specifically a market report, the study is commercial in outlook and includes a brief analysis of the scale of emerging markets and some forecasts about growth.
The study begins by seeking a working definition of the term ‘multimedia’ and by placing multimedia in the context of the electronic information industry as a whole, indicating where it is likely to have greatest impact.
The study then describes the underlying technologies of multimedia and goes on to evaluate the implications for such fields of use as education and training, business and professional applications and leisure and entertainment in the home.
This is followed by an a view of the impact multimedia may have on traditional information industry sectors such as print-on-paper publishing, bookselling and library services.
Finally, the study examines some important strands of longer-term development which may become important in multimedia applications in the latter part of the decade.
INTRODUCTION
Multimedia is fast becoming one of the most widely used words in the information industry.
There is good reason.
Looking forward just a few years, multimedia is clearly going to become a standard, highly influential feature of a vast range of computer applications in all sectors of our personal, professional and business lives.
The title of this study sounds very ambitious.
To avoid misunderstanding, even disappointment, it needs some careful qualification.
First, we have to be aware that ‘multimedia’ is a confusing and ill-defined term, meaning different things to different people.
Although we will clarify it in the course of this study, multimedia is hard to pin down to a rigid definition.
Rather than waste much time searching for a single, unambiguous statement specifying multimedia on which everyone will agree, we will aim instead for a working understanding of the term.
Second, the title suggests an assessment of multimedia spanning the decade, examining developments as far ahead as the year 2000.
Realistically, it is hard enough to speculate on how multimedia will have developed in a few years time without looking as far ahead as the beginning of the next century.
Indeed, the relatively brief history of information technology has already demonstrated the dangers of even short-term forecasting.
So much has taken place in so little time, it is hard for us to really grasp the rapid pace of development.
US futurologist, Alvin Toffler, placed modern technological development in a graphic perspective when he analysed the whole of human history in terms of lifespans.
‘If the last 50,000 years of man's existence were divided into lifetimes of approximately sixty-two years each, there have been about 800 such lifetimes.
Of these fully 650 were spent in caves…the ovewhelming majority of all the material goods we use in daily life today have been developed within the present, 800th lifetime.’
If the history of our technological age is just a flicker of time, then multimedia, amidst all the bustle of change, is a word still forming on our lips.
It is an embryonic concept in a fast-changing environment and we have to be realistic about the limits of our power to forecast or even speculate about its future.
Third, most importantly, the title may suggest an examination of multimedia on a cosmic scale.
Obviously, it would be best to be as comprehensive as possible.
But multimedia is a complex, wide-ranging subject and to address it fully would be well beyond the scope and funding of this study.
Our approach, therefore, focuses on essential themes in multimedia information, providing an overview of the technological environment from which it is emerging and an assessment of the key areas of application in which multi-media may become most influential.
Although we cannot ignore purely entertainment applications, our main concern will be information products and services rather than‘content-free’ fun.
The emphasis throughout is largely on the UK and Europe although we will range more widely to examine global issues when appropriate.
Above all, we will try to address the significance of multimedia in a general sense and assess its likely impact on the way in which we may be using information in the final years of the century.
Who is the study for?
It is certainly not intended to add much to the knowledge of multimedia experts.
The community of hardware and software developers, technocrat multimedia designers, information technologists and industry pundits, have many other sources of expert commentary.
Nor is it intended to be any kind of technological treatise.
Indeed, as far as this study is concerned, multimedia is not essentially a technological issue.
The key focus throughout is the use to which multimedia information resources can be put in commercial applications and this is much more a human issue than a technological one.
Why, in particular, commercial applications?
Because the single driving force which ultimately determines whether new technological developments flourish is the willingness of people to buy products based on them.
If multimedia has a future, it has nothing to do with the fact that it can do clever things.
Certainly, multimedia systems can perform spectacular, even awesome feats.
However, like teaching a dog to dance on one leg, the exercise is impressive but ultimately futile if we cannot see some practical purpose for it.
Although technology cannot realistically take a back seat, we will try to tackle concepts at a simple level.
The information industry is crying out for simplification and struggles desperately under the burden of its own complex, often incomprehensible jargon.
Multimedia is no exception.
Keeping things simple, however, is not the same as being simplistic and the guiding intention throughout is to demystify without compromising the basic, underlying issues we need to address in any serious study of the subject.
So, for whom is the study written?
It is aimed primarily at the generalist reader concerned with information resources and their use, who is looking for a non-technical assessment of what multimedia is, what it does and how it may be used in a range of applications.
The study also specifically addresses the impact multimedia information products may have on traditional information sectors such as library services, publishing and bookselling so may be of interest to readers active in these areas.
Any useful understanding of a new approach to presenting information must start with a context.
Multimedia does not, indeed cannot, exist in isolation.
It is a feature of a changing pattern of information use, emerging through a variety of electronic media.
The technologies involved, however, are no more than delivery methods, platforms on which new kinds of information products can be built.
The products themselves succeed or fail in the context of a global information industry.
Accordingly, it is important for us the view multimedia in this wider industry context if we are to assess its significance and likely future impacts.
We will start therefore with a brief sketch of the business of trading electronic information across different market sectors, using a variety of underlying technologies.
But since we are inevitably already using the term ‘multi-media’, we must first consider what it means.
1 WHAT IS MULTIMEDIA?
In February 1990, the well-known US computer journal, ‘Byte’, ran a special feature on multimedia.
It was detailed, provocative and refreshingly easy to read.
However, it seemed to flounder whenever it tried to pin multimedia down.
Wisely avoiding any rigorous definition, it offered at one point:
‘Even if you're not sure what multimedia is, you probably know it when you see it (or hear it)…’
Others have attempted a little more precision but floundered just the same.
The Information Workstation Group's study called ‘Micro Multimedia’, published in 1990, suggested:
‘Micro multimedia is multimedia supported by a microprocessor(s).
It uses multiple media data types.
The Information Workstation Group definition requires the presence of graphics or natural images unless Micro Cinema is present.
The use of audio or numerics with text does not qualify as multimedia.
The presence of text or numerics is optional.
Motion is not required.
Interactivity is not required…
Programmed passive experiences can be multimedia…
Materials excluded from micro multimedia include: television receivers where the end user can not effect (sic) the onboard computer programming, CD-V (linear play), programmable digital watches with graphic displays and audio alarms.’
There is more but just this sample suggests the deep and unintelligible waters it is easy to fall into when trying to be explicit about multimedia.
What is striking about the Information Workstation Group's efforts is that their actual study of multimedia is far better than their attempt to define it.
1.1 An Identity for Multimedia
A new British magazine, ‘Multimedia: computing with sound and motion’, published in 1990 by EMAP, took a much more robust approach to defining multimedia.
Its first editorial suggested:
‘Multimedia is all things to all people.
The name can convey a highly specific meaning or less than nothing, depending on your audience.
In fact, multimedia is a singular mix of disparate technologies with overlapping applications in pursuit of a market and an identity.’
This may be true but if multimedia is really in search of an identity, it is a curious irony because right now it suffers from having rather too many of them.
We obviously need to pin the term down more firmly.
If we allow multi-media to remain so shapeless and subjective, how can we ever hope to understand its implications for the information industry and the generations of information users who may (or may not) benefit from it?
A useful starting point is to go back twenty years to a time when computers were still huge, unfriendly devices demanding air-conditioned environments and costing millions.
Even if desktop computing had not yet been invented, the term ‘multimedia’ was already in use.
In the world of educational publishing, in particular, multimedia meant kits, typically a combination of pupil booklets, teachers' guides, film strips, audio tapes, and photographic slides.
Such products, attractively boxed and delivered to the classrooms of the early 1970s, certainly offered multiple media — text, images and sound — but each was delivered as an independent element in the package.
The different media were fragmented, difficult to integrate.
Welding them into a single resource for classroom teaching was a difficult task for the teachers and a confusing, unsatisfactory experience for pupils.
Despite a strong commitment to resource-based teaching at the time, most schools found multimedia kits more trouble than they were worth.
Most were commercial failures and publishers quietly dropped them and returned to textbooks, the monomedia which had always been the core of their business.
We could spend much time analysing the possible reasons for the failure of classroom multimedia in the 1970s.
One clear reason, directly relevant to our pursuit of an identity for multimedia, concerns the way in which the different media were being presented.
Although excellent in themselves, each of the elements of the multimedia kit was delivered independently.
This fragmentation was, of course, unavoidable.
Twenty years ago, the technology needed to bind the individual media together simply did not exist.
There was no way of combining the information in the workbooks, the sounds of the audio tape or the images of the slides and film strips.
Today, however, the multimedia environment is dramatically different.
Electronic technology provides a single medium with the power to integrate diverse types of information.
So the first stage of identifying modern multimedia is to focus on its power to draw together different forms of communication, smoothly integrating them within a digital environment, and providing access to the stored information using computer systems which are fast, friendly and, above all, interactive.
We now have an identity for multimedia, a working definition against which we can measure developments and potential impacts.
We can describe it as the seamless integration of data, text, images and sound within a single digital information environment.
1.2 Delivering Multimedia
This way of seeing multimedia immediately suggests some important characteristics for the technology needed to deliver it effectively.
Integrating image data in particular, implies storing, manipulating and displaying very large amounts of data.
The reason is straightforward enough.
Representing an image accurately requires a great many bytes of digital information.
While just one byte can represent a whole character of text and therefore convey at least some intelligible meaning, thousands, even millions of bytes are needed to build up a single image, particularly if it is in colour and of high resolution.
If that image changes rapidly in time, as it would in a moving video sequence, for example, then a huge amount of digital information must be stored, transferred and processed to keep up with the requirements of delivering the motion video to the user.
This suggests four important factors in all multimedia systems.
First, they generally need very large memory stores, typically of the order of hundreds of megabytes.
Second, specialised and powerful processing technology is required to handle the retrieval, processing and display of the high volumes of information.
Third, in addition to text and numerics, output and display architecture has to be capable of delivering both sound and images to the required standards of any given application.
Fourth, this rich and complex information environment is worthless unless users can easily find their way around it, locating and accessing the information they require.
These factors tend to dictate the character of successful multimedia platforms.
The large memory requirements suggest the use of optical disc storage.
The demanding processing requirements imply very fast, high performance computers, probably equipped with special chip sets either to compress and decompress the fast-changing, high volume information flows or to digitise sources of information drawn into the multimedia environment which are not initially in digital form.
Output and display requirements dictate architectures which support high-resolution colour graphics, can display both RGB (‘Red-Green-Blue', the colour screen standard in which the three basic colours remain individually controllable) and composite video information, and provide an audio capability able to handle anything from a feeble bleep to high fidelity stereo.
Last, but perhaps most importantly, the whole system must be easy-to-use.
In other words, a successful multimedia system must offer an interface with its user, so friendly that it is effectively transparent.
These are exacting requirements.
Do we possess the technology to address them?
The very fact that multimedia is becoming such an intense area of activity today suggests that, at least in the view of hardware and software developers, the technological capability has arrived.
However, even if this is true, there is a more fundamental and less easily answered question.
If the technology is available, can we muster the human design skills needed to put the technology to work and create the powerful and effective applications which will ultimately shape the longterm future of multimedia?
1.3 Interactivity
Although we have not yet used the word, interactivity is a key feature of successful multimedia.
However, it is so much woven into the fabric of multimedia, we will not focus on interactivity in itself in this study but allow it to feature implicitly throughout our assessment of multimedia design, technology and application.
It will help, however, to outline what interactivity is and why it is such a vital building block of successful multimedia.
Interactivity in an information system gives the user some influence over access to the information and a degree of control over the outcomes of using the system.
In practice, this usually means that, in one form or another, the system presents the user with choices.
The decisions taken influence the path the user follows through the information.
Each decision point is rather like a crossroads.
After reading the signpost, the user moves off in the direction of his choice until he arrives at the next crossroads.
In a digital information system, multimedia or otherwise, the ‘crossroads’ and the resulting network of possible pathways are created by a computer program designed to control and moderate the user's access to the information.
Although interactivity of this kind places some control over access and outcome in the hands of the user, the degree of control is strictly limited by the computer software providing the interactivity.
The software determines how many crossroads there are, where they are located and how many options are offered by each of them.
All possible pathways are therefore defined by the design of the software.
The user simply selects available options based on what the user wants to get out of the system.
For example, choices may be driven by a search strategy with the user seeking a particular item of information or instead may be an unstructured, browsing investigation, as much a reflection of the user's curiosity as a desire to locate anything particular.
In an educational application, the user may be firmly channelled by the controlling software into pathways that suit the particular didactic purpose of the application.
There is little really new in the notion of interactivity in electronic media.
From the earliest times, electronic databases have been accessed by means of search and retrieval software.
The design of the software coupled with the internal structuring of the database, define the interactions users can have with the database.
In other words, interactivity is really just another word for the ways in which a user can search and browse through an electronic database, the process being more or less constrained by the control software.
The same, of course, is true whether the database is textual or multimedia.
The real difference in designing interactivity for multimedia lies in multimedia's added richness and complexity.
To design a means of navigating effectively amongst thousands of images, video sequences, sound, text and numerics, all seamlessly combined as a single information resource, is a challenging problem and one that lies at the heart of successful multimedia applications.
Indeed, interactivity largely defines the user's experience of a multimedia product.
After all, the content and potential benefits of such a product will be irrelevant if its users do not find the means of accessing it easy-to-use, powerful and compelling.
In other words, interactivity brings a vital element of added value to all electronic information, whether multimedia or not.
The degree of added value is determined by two main features: the design of the interactivity and its implementation.
1.4 Designing and Implementing Interactivity
Designing the interactivity means programming the number and location of the ‘crossroads’ in the database, establishing the decisions a user can take at each of them and determining the consequences of those decisions.
A form of interactive access to databases which is having a major impact in multimedia design is know as hypertext.
Strictly, the term applies to textual databases.
When used with multimedia it is adapted to ‘hypermedia’.
We will look at these concepts in more detail when we examine the technology of multimedia (see 3.25).
The implementation of the interactivity involves two key issues.
First, the information system must work.
This means putting the software, hardware and data in a practical and reliable configuration.
For example, consider an application involving an interactive video system.
There would be no point in designing software to control access to the video disc, unless suitable interfaces are also available to make it possible for the computer in the system to communicate with the video disc player and control its operation.
At this level, therefore, implementation is an issue of system integration, making sure all the elements, software and hardware, communicate and function in an appropriate and concerted way.
The second issue focuses on how the user interacts with the information system.
In other words, what kind of interface exists between the user and the database?
At a mechanical level, the interface may be a keyboard and a screen.
At a design level, the user interface determines the screen design and its functionality.
In multimedia systems, the most successful type of interface today is known as the Graphical User Interface (GUI).
The classic GUI is the one designed by Apple in the early 1980s for their Macintosh range of computers.
The Macintosh user interface has been called the first ‘intuitive’ interface, suggesting that a user can learn how to use it by instinct alone without the need for instruction manuals or training.
The design of the interface is based on its use of what have become known as ‘real-world metaphors’.
For example, the underlying metaphor for the working space on the screen, is a desk top.
Data files are visually represented by on-screen icons, designed to look like tiny office files.
Unwanted files are removed by being deposited in a dustbin icon which even bulges to show that something has just been put in it.
The full power of the Macintosh interface and, indeed, all GUIs, however, runs much more deeply than the clever choice of icons.
It is the overall functional design which creates the attractive combination of power, convenience and ease of use.
For example, the icons are usually addressed by inputs from a mouse rather than a keyboard and many of their operational facilities are controlled by pull-down menus.
The most recent GUIs also incorporate a windowing facility enabling users to create a window within one application through which they can simultaneously run another.
This brief discussion of user interfaces merely touches on their significance.
The important point to emphasise, however, is that the design of interactivity in multimedia systems, including the choice of user interface, fundamentally affects the experience of using them.
This means that the effective design and implementation of all aspects of the interactivity is crucial to the success of multimedia.
2 THE ELECTRONIC INFORMATION INDUSTRY
In recent years, the business of trading electronic information has become known simply as ‘the information industry’.
The abbreviation makes two important points.
First, it tells us that the basic currency of the industry is not bits and bytes and the complex trappings of electronic technology.
It is information and the benefits information can offer to users.
In other words, the industry is concerned less with the cleverness of technology than with using it to confer value on information and so create products that customers will want to buy.
Second, it suggests that the underlying currency which shapes all kinds of information products and services is essentially electronic in nature.
This being the case, there is no need to qualify the term, ‘information industry’.
Indeed, in a sense, the very absence of the word ‘electronic’ suggests its overwhelming significance.
These days, all information media are built on electronic foundations so the qualification is simply redundant.
This is as true for print-on-paper publishers who may deliver products in hard copy but invariably originate electronically, as it is for electronic information traders who both originate and deliver their products in electronic form.
2.1 The Utility of Information
An important underlying concept running through the information industry is the principle of creating distinctive products by adding value to information.
Every information medium has its own individual set of defining utilities.
In other words, each offers different and characteristic communication features which are more or less valuable to information users, depending on the use to which those users want to put the information.
This is the same as saying information delivered in a particular form is more useful in certain applications than in others.
Television is an excellent, friendly medium for entertaining viewers but, by itself, it is bad at communicating large bodies of detailed information.
A book, on the other hand, generally requires more intellectual application from its readers but is good for delivering detailed information.
Together, books and television form a useful alliance which has another set of special characteristics.
These examples are both simplifications and generalisations.
However, they make two important points.
First, we should not necessarily measure different kinds of information media against one another as if there was a competition taking place to find the most powerful way of communicating with people.
Rather, we need to concentrate on the notion of fitness for purpose.
Every medium, whether it uses the oldest or newest of technologies as its means of delivery, has an appropriateness to particular applications.
Ultimately, the people who decide which media flourish in a given application and which are swept away, are the customers, the people who decide to buy one information product in preference for another.
Second, customers decide their preferences with a large pinch of subjectivity.
The particular characteristics chosen above to characterise television and print-on-paper media, are one way of seeing their utilities.
While it is a matter of fact that ‘books offer text and illustrations fixed on a page’, it is largely (but not, of course, entirely) a matter of opinion and personal preference as to the benefits books offer in particular applications.
So utility in information media is not some absolute fact of human experience.
Much of an information user's appreciation of such utility can depend on many intensely personal factors such as education, age, cultural conditioning, and past experience of using information resources.
For example, consider the example of books once more .
Books have particular utilities which many will probably agree upon.
They are portable, cheap, easy-to-use, browsable, annotatable, and engaging.
However, if you do not know how to pay for your next meal, you may not consider books cheap.
If you cannot read, you will not find them easy to use.
And if you have experienced nothing but a television culture and failed to learn the skills of using books effectively, you will not be able to browse and are unlikely to find the written word sufficiently engaging to hold your attention.
The key point about this example is that the features offered by a information medium only confer value and add to the medium's utility if the customer wants the features and can use them.
In this context, therefore, a benefit is only of value if it is perceived as a benefit by those who receive it.
In other words, it is not enough for designers of information products to recognise the benefits of those products.
In fact, it is almost beside the point whether they do or not.
The person above all else who must appreciate the benefits is the user of the product, the customer who has paid money for it.
This issue, largely determining the success or failure of the information industry, is such a simple one it may seem almost too obvious to be worth saying.
However, even a cursory glance at the output of the information industry shows that fitness for purpose is not always considered carefully enough in designing new products.
This is particularly true of the new electronic media now emerging.
With the experience gained from centuries of print-on-paper publishing, why should this still be the case?
One cause is those very centuries of print-on-paper publishing.
It is a case of one kind of information medium fixing attitudes towards the presentation of information for all others.
It has proved very hard for publishers to make the conceptual leap from information that is essentially fixed and pre-determined, such as print, to dynamic information which can be shaped in its applications by the end user as well as by the publisher.
If there is a single divide between the traditional world of printed information and the new electronic media it is this contrast between static and dynamic information.
Another cause comes from quite a different quarter.
It arises from too much enthusiasm clouding the judgement of those developing the new electronic products.
Enthusiasts become so excited by the many varied potentials of the media, they try to use them all.
They justify what they do by seeing their potential customers as they would like them to be, rather than as they are.
A major challenge facing the information industry is to develop the maturity of judgement that allows the utility of the different available media to be accurately and dispassionately matched to the real requirements of information users.
2.2 Electronic Information Markets
As we will see in later sections, the development of successful multimedia applications is deeply affected by this crucial issue of market awareness among multimedia information publishers.
However, before we begin to examine multimedia specifically, it is important to our quest for a context for multimedia to get some sense of the size and scope of the information industry more generally.
This is all the more important because multimedia is not a discrete technology or a communications medium in itself and could potentially have an important role in many different industry sectors.
Before providing a framework within which to see the information industry, we can get some sense of scale by comparing the size of the electronic media markets with print-on-paper markets.
2.3 The Scale of the Markets
In the 1990 edition of the annual United States government review of domestic industrial activity, ‘Industrial Outlook’, the US Department of Commerce looked at the growth trend in printed and electronic information.
The figures for 1990 and 1991 are forecasts, the others are actuals.
All are in US$billions.
It should be said that the forecast figures have probably turned out to be slightly high, particularly for 1991, since it seems unlikely that the US Department of Commerce would have foreseen the downturn in the global economy which is now biting so hard in all industries.
However, it is not the absolute figures that are so striking.
It is the comparison between growth rates.
In particular, books show an average annual growth rate of 8.25 per cent while electronic information shows a rate of 20.30 per cent .
If this continues, electronic information in the United States will be generating greater revenues than books by 1996.
Many commentators in the United States believe that the gap between the two growth rates will actually widen as electronic markets mature and as they are enriched by the added values of multimedia.
The same analysis for the UK seems to suggest a similar picture.
The British Publishers Association's figures for the retail value of the UK book market indicates a growth from £2.2 billion in 1988 to £2.4 billion in 1989, an increase of 9.1 per cent.
Although no figures are yet available, this rate has probably flattened in 1990 and 1991 and may well prove to be around eight per cent or less at the current time.
Analysis by the European Commission's Information Market Observatory suggests growth rates in electronic sectors of around 20 per cent in Europe more generally.
The size of the UK's electronic information market can be crudely (and probably very conservatively) estimated at just over £1 billion.
This means that if we apply the 8 per cent growth rate to the book market and the 20 per cent growth rate to the electronic market, the gap between the two begins to narrow rapidly by the middle 1990s.
Electronic information overtakes books by the year 2000.
If the relative difference in growth rates remains steady in the 1990s, the unfolding global picture will be one in which traditional book markets are remorselessly overhauled by the value of the new electronic ones.
This is both a commercially and psychologically important fact.
In a sense, it will signal the end of the centuries-old dominance of the printed word.
The book will at last be plainly evident for what it is: one option for the delivery of information among a range of possibilities, rather than the unquestionably natural platform.
It is a moment that is likely to reshape the strategic thinking of traditional, print-on-paper publishers, perhaps encouraging them at last to re-define their activities and participate more extensively in information media of all kinds.
Indeed, for some, it may be a matter of survival to do so.
2.4 The Scope of the Media
What are the main electronic media in the current information industry?
There are many ways of seeing the same picture and what follows is offered as a useful framework rather than any kind of definitive blueprint.
We can identify six key sectors.
Computer software
Online databases
Optical disc media
Audiotex
Fax-based services
Handheld electronic reference media
We will only touch on some of these sectors here and will certainly not attempt any detailed analysis.
In particular, we will neglect audiotex and fax-based publishing altogether since neither look like having a significant role in multimedia applications.
However, it is important before examining any of the sectors to emphasise that the framework suggested contains some important overlaps.
For example, recent developments such as Sony's Data Discman and other portable CD-ROM systems are rapidly blurring the rather artificial distinction between optical disc and handheld media; fax-based services are frequently offered as mechanisms for providing hard-copy of online or audiotex information; computer software can be delivered on an optical disc which is, after all, essentially just another kind of electronic storage medium; and online services are often offered to support optical disc information products.
Indeed, the entrepreneurial flair and ingenuity of players in the industry are ensuring the appearance of a constantly changing variety of hybrid systems delivering information services addressing a range of needs and circumstances.
Nevertheless, even if our way of sectoring the industry is flawed with some inconsistencies, it at least gives us a convenient means of overviewing it.
2.5 Computer Software
If we restrict ourselves to products with serious and substantial information content, our focus in the computer software sector is largely on educational markets.
The turbulent and volatile consumer market is dominated by games, few of which embody any real information content.
Although games software has some significance in terms of the technology of multimedia, it is of little relevance to the information industry more generally.
We will return to computer games when we look at multimedia applications in entertainment and leisure.
There was a time when educational software looked rich with commercial promise.
In the early 1980s, some leading print-on-paper educational publishers confidently expected school software to be a major new area of diversification for their businesses.
Today, after ten years of commercial failure, most of the print-on-paper publishers who entered the educational software market, have withdrawn.
Some are still licking quite substantial wounds.
What went wrong?
One part of the answer is simple.
Although, computers are now widely and fruitfully used in classrooms throughout European schools, they are not used for the purposes commercial publishers expected.
Instead of software specific to curriculum applications — programs to teach maths, science or history, for example— generic packages have swept the market.
Ironically, it is the small educational software specialists rather than the big battalions who saw this trend developing.
Now it is these companies with their modest commercial requirements that are prospering even in a small and under-funded market.
It is their word-processing, desktop publishing, spreadsheet, database and graphics applications programs that dominate the market.
However, even if the business may be buoyant for some, it remains a small and internationally fragmented one.
In Europe, for example, it is probably worth no more than £30 million with a proportion of this amount relying on Government and other subsidies which tend to inflate the true underlying market size.
Ultimately, therefore, even if the big publishers had properly understood how software would be used in classrooms, they might well have found the size of the industry too small to be worth their attention and resources.
2.6 Online Databases
Typically, online systems consist of a central computer housing large, predominantly textual databases which can be delivered to information users at remote terminals.
The terminals are generally connected to the central computer by telecommunications links.
Organisations running the computer systems are known as ‘hosts’ and those owning the data stored on them are called ‘information providers’.
Customers dial up the central computer, effectively making a telephone link via a modem between their terminal and the computer housing the databases.
Using powerful search and retrieval software available from the host, customers then locate and extract the information they want.
The cost of the service is usually related to the length of time customers are actively connected to the host computer.
The first commercial online databases appeared in 1967 and since then the industry has grown slowly.
Currently there are about 850 vendors in 38 countries offering over 4,600 databases.
The main barriers to growth have been the complexity of using the systems, their inability to handle anything but the simplest graphics, the high cost of connect-time charging and poor publishing decisions about which databases to offer online.
Despite the barriers, the online industry is beginning to generate substantial revenues.
In 1988, for example, the online market in the United States was estimated by analysts Frost & Sullivan to be worth about US$6 billion with an overall annual growth rate of 20 per cent .
In Europe, latest European Commission figures suggest a market worth about US$2 billion in 1988 with more than half generated by UK sales.
The European Information Industry Association estimates that this market will have grown to US$6.7 billion by 1995, an average annual growth rate of 19 per cent.
Nowadays it is important to distinguish between two main types of online information: historical and real time.
Traditionally, online systems have delivered information that may well be kept up to date on a regular and frequent basis but is still essentially a record of what has already taken place, an accumulation of historical information.
Real time services, by contrast, provide information about what is happening right now, moment by moment.
The key area in which these ultra-current databases flourish is the field of financial information.
Since the computerisation of the world's money and equity markets, real time services have become crucial to successful trading.
Because transactions in these markets are typically of very high value, dealers, analysts and market makers readily pay large premiums for instant access to real time information.
In this field, online systems are not merely desirable.
They are indispensable.
All these systems are largely restricted to text or numeric data.
They cannot readily handle image information except for rudimentary line graphics.
One of the main reasons is that the connections between terminals and host computers are telecommunication links, usually simple coaxial cables, with what is technically known as low band width.
The expression ‘band width’ is used to describe links for passing electronic information as if they were pipes through which the information is pumped rather like water being distributed across a national supply network.
Traditional telecommunication links suffer from a restricted band width which means they are like very narrow pipes capable of carrying only limited supplies of water, even pumped at the highest possible pressures.
Translating the metaphor back to electronic information, low band width means that the substantial volumes of data needed to represent image information cannot easily be delivered to the remote terminals.
It is both a question of sheer volume and the rate of delivery.
The restriction on volume and speed of delivery means that images can only be built up slowly, at a pace determined by the rate at which data arrives at the terminal.
If the images themselves are changing in time, as in the case of animations or motion video, the problem is compounded.
The restriction on volume and speed of delivery means that the system would be hopelessly slow in updating the image information.
If it is hard enough to display a single frame of a complex image, it would be extremely difficult to create the sequence of rapidly changing frames on which the impression of natural movement depends.
Developments in telecommunications which are now taking place, however, will mean that during the 1990s, broadband distribution will become available enabling online, simultaneous transfer of a wide range of digital information.
These powerful new networks, usually referred to as broadband Integrated Services Digital Networks (ISDNs) are already being established and may soon transform the global distribution of electronic information.
In addition to such new network technology, there are also techniques of solving band width problems by compressing data before transferring it and decompressing it afterwards so that it can be displayed in its entirety for the information user.
We will consider the role of data compression in different contexts more fully in the next chapter.
What is clear is that either through the use of broadband distribution or by means of data compression-decompression, multimedia will play an important and innovative role in a range of online applications in the 1990s.
2.7 Optical Disc Media
Optical discs are soon likely to become the standard means of storing multimedia information.
They are already playing a crucial role in multimedia design and development.
We will deal with optical discs more fully in the next chapter when we examine the technological environment in which multimedia is emerging and, for the moment, will concentrate on gaining some sense of the way in which the use of optical disc information systems is expanding.
We will concentrate in particular on compact discs (CDs) rather than videodiscs because, while videodiscs undoubtedly have an important and continuing role in multimedia systems, the longterm future rests with the various evolving forms of CD media.
The dominating force in CD information products is the Compact Disc Read Only Memory (CD-ROM).
Although CD-ROMs look much like CD audio discs, they are encoded not with music but information which can be read and displayed by a computer.
The first databases on CD-ROM were largely textual and appeared in 1985.
Since that time, CD-ROM capabilities have developed rapidly and both CD-ROM itself and other evolved forms of CD information media now look like being the single most important carriers of multimedia in the 1990s.
The following world market analysis from the US Optical Publishing Association gives a clear picture of the dramatic growth taking place.
The analysis was published in the second quarter of 1990 and the figures for 1990 -1992, therefore, are the Association's forecasts.
It is plain from even a cursory glance at these figures that, after a slow start, CD-ROM is becoming a remarkable success.
The rate of growth in the last two years has been explosive and forecasts suggest this rapid rise will strengthen during the middle and late 1990s as CD-ROM readers become a standard built-in element of most desktop computer systems.
Currently, the principal users of CD-ROM systems remain libraries.
At the CD-ROM Europe Conference in 1990, Charles Oppenheim of Reuters, estimated that 80 per cent of all CD-ROM readers were then located in libraries, mainly those with academic or professional specialisms.
In the United States in 1990, OCLC conducted a survey among libraries and found that out of their sample of 249, the proportion using CD-ROM had risen from 6 per cent in 1986 to 66 per cent in 1989.
Of those not yet using CD-ROM, over 70 per cent said they expected to be doing so by the end of 1990.
However, the huge increase taking place in the installed base of readers is a signal that CD-ROM is now being bought and used much more widely.
While libraries currently remain the biggest CD-ROM user group, the numbers of business, professional and individual users is plainly rising rapidly.
There is now a growing, widespread acceptance of CD-ROM as an effective electronic publishing medium and its established and growing base of users are as much a potential, almost captive, market for the textual databases of the past as they are for the multimedia databases of the immediate future.
Already, increasing numbers of published CD-ROM titles are incorporating graphics and sound.
These developments are the forerunners of the full multimedia CD products which will soon be appearing, designed for a wide range of applications.
Some of these new CD products are aimed squarely at consumer markets, promising for the first time a breakthrough into the same mass market in which audio CDs have been so dramatically successful.
2.8 Handheld Electronic Reference
This is the newest sector in the industry.
It has not emerged in isolation and is a strand of the overall trend in personal computing towards extremely small, powerful, easy-to-use devices.
The first electronic reference books, therefore, have utilised the same underlying technology as the latest laptop and palmtop computers.
In commercial terms, the product category has been established mainly by US players such as Franklin Electronic Publishers and Selectronics.
Both have used chip technology to produce handheld database readers that fit comfortably into the palm of your hand.
Their early publications have been linguistic products such as spellcheckers, dictionaries, thesauri and multilingual translators.
More recently, they have added general reference products to their ranges, including Bibles, encyclopaedias and almanacs.
All the Franklin and Selectronics products rely on chip memories to store their databases and the search and retrieval software that enables users to access them.
Originally, the devices were dedicated readers.
In other words, the database in the reader was the only one you could access.
However, both companies are now moving towards a more generic approach.
Their most recent products have facilities for slotting in a range of proprietary memory cards, each with a different database.
This means, that the basic reader can be used to access as many different information products as the publishers care to make available in a compatible memory card form.
This approach to palmtop information publishing is also being taken by some of the electronic organiser manufacturers.
Sharp, in particular, are rapidly expanding the range of memory cards which can be slotted into their IQ organisers.
Psion, creators of the first personal organiser, have for some time made plug-in modules available to provide dictionary and other data facilities.
Recent moves to establish a world standard in memory cards suggest that this kind of publishing may soon extend to desktop computers of all kinds as well as handheld reference devices.
None of the existing handheld products can deliver multimedia.
At best, limited speech synthesis and a few simple graphics can be offered to accompany the essentially textual databases.
The main reason for this is the restricted memory storage available using solid state technology.
Even using the most advanced chips commercially available and the most powerful text compression algorithms, there is insufficient capacity to store even the simplest images let alone animations or video.
There are of course also limitations in current screen and power source technology which would make it difficult to display high resolution colour images of any complexity.
However, the biggest single barrier remains the memory limitation inherent in chip storage.
While the current generation of chip-based handhelds continue to sell well, the first signs of a major shift towards handheld CD media are becoming apparent.
In July 1990, Sony launched their Data Discman in Japan.
This palmtop, self-contained CD-ROM reader, selling for Y58,000 (about £250), has been a major success.
In the first five months, Sony sold as many as they could manufacture with unit sales topping 70,000.
When the Data Discman was launched, seventeen discs were launched with it.
Eight more discs have been published since the launch.
The discs themselves are 3-inch CD-ROMs, the same physical format as CD Audio ‘singles’ which the Data Discman can also play.
The capacity of each of these discs is 230 megabytes, about 40 per cent of a full size CD-ROM.
A database of this size is nearly fifty times greater than the largest that can be stored in a chip-based handheld.
When Sony bring the product to European and US markets late in 1991, they will probably call it the Sony Electronic Book Player in the clear belief that they are offering a practical platform for the first true electronic books.
The current product, however, suffers from some drawbacks, not least the poor legibility of its screen, and while it may have some powerful ‘look it up’ reference applications it will never succeed in circumstances where the reading of continuous text is important.
It is clear, though, that the Data Discman is only the forerunner of more sophisticated electronic book players.
The key feature is its optical memory as distinct the vastly more limited chip memories characterising other handhelds.
Having the capacity of a CD-ROM at its disposal means that palmtop textual databases may soon give way to multimedia.
Indeed, at about the same time that Sony were launching the Data Discman in Tokyo, they were already previewing in London a similar handheld player which runs Compact Disc Interactive (CD-I), a CD information format designed specifically for multimedia.
2.9 Multimedia in Context
This rapid glimpse of some of the principal sectors of the information industry emphasises an important point about multimedia.
It is not, in itself, an information medium but a facility which will enrich established information media, enhancing and extending their applications.
We can see, even from the brief description given above, that electronic information is growing into a sizeable and influential industry.
It is has already transformed areas of information use such as industrial, professional and academic research and has made possible the first practical real-time information services.
Technological developments are making the enabling systems cheaper, easier to use and more versatile.
The current generation of devices has already established a growing acceptance of electronic media among information users and demonstrated that they have a choice in the way they get access to the resources they need.
Print-on-paper publishing remains the dominant force in the information industry more widely but electronic media have established practical, affordable alternatives.
The multimedia dimension which bolts onto the existing industry could make those alternatives both more extensive and more compelling.
Most industry observers believe that there is a huge commercial momentum building up in multimedia development.
Massive commitments are being made by the world's biggest hardware and software companies.
John Gale's industry report, ‘Micro Multimedia’ published in 1990 by the Information Workstation Group, began with a sixfold growth forecast for multimedia hardware and software sales.
He estimated that in 1989, the multimedia market place was worth US$4.7 billion.
By 1994, he expects this to grow to over US$24 billion.
The Inteco Corporation, in their 1990 multi-client study of multimedia, presented some equally dramatic forecasts.
Looking at growth rates in the computer hardware industry in Europe for the period 1990 -94, they forecast the following breakdown:
In the same period, Inteco predict that multimedia software will expand its share of Europe's total PC software market.
These estimates suggest a sudden, huge growth in applications using multimedia, perhaps foreshadowing radical transformations of information media more generally.
Some observers have even described the emergence of multimedia as the most significant event in the history of information technology since Gutenberg invented the printing press.
Whether this is true or not, an extraordinary powerhouse of commercial and creative activity is now propelling multimedia forward.
The key question is whether, when the applications come to market, they achieve the degree of commercial success they need to fuel ongoing multimedia developments.
We began this chapter by emphasising that fitness for purpose is the chief factor deciding whether an information product succeeds or fails in a given application.
The emergence of effective multimedia capabilities will provide a new dimension of utility for electronic information products and may hasten the growth and development of their markets.
However, it will not do this simply because people forecast that it will.
It will happen only if the designers and publishers of the new generation of products use multimedia capabilities to some purpose.
In other words, multimedia must confer real benefits.
It must suit the purpose for which it is used.
If it fails to do this, multimedia will be an irrelevance.
It could conceivably even damage the electronic information industry by overloading its customers with a glittering array of sophisticated new features that no-one actually wants or needs.
3 THE TECHNOLOGY OF MULTIMEDIA
Before we consider the technological issues surrounding multimedia, it may be useful to emphasise a key division in the world of information.
Many issues in multimedia involve the interplay between analogue and digital information.
We can imagine the analogue world as a being the world human beings can perceive all around them.
The digital world belongs to the computer.
3.1 Analogue and Digital Information
Whether we know it or not, we are all familiar with a kind of information called analogue information.
Typical examples are the continuously varying intensities of natural light, the meanders in an audio record's groove, the variations in an electrical current and the mechanical fluctuations in the air which we interpret as sound.
The information embedded in an analogue signal therefore is always built into some continuously varying value which can be measured.
By measuring it, we extract the information.
When we listen to music, for example, our eardrums vibrate with the rhythm of changing air pressure.
This creates a signal in the auditory nerves which are interpreted by our brain as the pleasing notes of an orchestra.
Vision is a response to changing values in the intensity and wavelengths of light reflected on to the retina of our eye and transmitted to our brain by our optic nerves for decoding and interpretation.
Another familiar example is an old-fashioned wristwatch.
When we glance down at its face we see hands sweeping across the dial at various speeds.
They move continuously of course, covering every part of the dial at some stage of their circular journey.
We judge the time of day by the relative positions of the hands against the scale of hours, minutes and seconds printed on the watch face.
This kind of information display is analogue.
We read the message of our watch by looking at a constantly changing display.
There are no gaps in the information.
It is a continuous flow.
Digital information is different.
Its character is essentially discontinuous.
Far from reflecting continuously varying values, digital information is based on just two distinct states.
In the digital world, things are there or not there, ‘on’ or ‘off’.
There are no in-betweens.
This is the reason why digital computers talk in a language called binary code.
It consists of just two symbols, the digits 0 and 1.
Everything a computer does, it does in this starkly simple language.
Significance in the information is created by placing the symbols in different orders.
In other words, the sequence 00011000 means something different from 00010100.
A rich and powerful language is built up in this way.
In computer jargon, each of these binary symbols, either 0 or 1, is known as a bit (a contraction of ‘binary digit’).
A character of computer information — the smallest information element carrying significance in itself — usually consists of eight bits arranged in a characteristic sequence.
The overall sequence is called a byte.
To further contrast analogue and digital information, we can return to our wristwatch parallel.
You can always tell someone who is using a digital watch.
Ask them the time and they will invariably say something like, ‘…it's 4.41’.
How often do you really need to know the time to the exact minute?
An analogue watch user will just glance at the timepiece and say, ‘…it's about a quarter to five’.
In digital timekeeping it is always one time or another.
It is never ‘about’ anything.
However, there is an instant when a digital watch is speechless.
When the display flashes from one second to the next there is a tiny gap in the information.
So, although the watch seems to supply a constant and exact reading of time, it is in fact a discontinuous display.
3.2 The Importance of the Digital Environment
Why is the distinction between analogue and digital information so important in multimedia technology?
The reason lies in our original explanation of multimedia as the manipulation of different kinds of information — text, numerics, sound and images — within a single, digital environment.
Clearly, we would not need to concern ourselves with the meaning of analogue information if all sources of text, numerics, sounds and images were digital.
In many cases, however, multimedia applications have to convert analogue sources of information such as sound, photographic images and video, into their digital counterparts so that they can then be manipulated within that single information environment.
Why is the digital environment so fundamental to multimedia?
The answer is essentially a practical one.
Digital data must eventually be the single language of multimedia because it is also the language of computers.
It is a practical issue because by taking information out of the analogue world, the ‘real’ world, comprehensible and palpable to human beings, and translating it into the digital world, we make it infinitely changeable.
In the analogue world, the reshaping of a page of information or a physical object requires some exercise of brute force.
Usually, however careful we are, the change inflicted causes damage.
The process is difficult, slow and untidy.
If we translate analogue information into a digital form, we can suddenly manipulate it freely in almost any way we wish.
Using the computer environment as our tool we can process and reshape all kinds of information swiftly and perfectly.
Then, when we are ready, we can reconstitute and display it as new information which human beings can once again understand.
This is of course a highly simplified way of seeing the distinction between analogue and digital information but it does emphasise why the future of multimedia — indeed, the future of all information media — is essentially digital.
Analogue sources of information may continue to play an important role for many years but the heart of multimedia will increasingly be the fast and flexible world of digital information.
3.3 Handling Images in Multimedia
The single most important technological issue in multimedia concerns the effective handling of images in general and full motion, full screen video in particular.
This is by no means the only important issue and other aspects of multimedia technology are also rich with problems and complexities still to be addressed.
However, it is now widely (and wisely) accepted that screen-based communication is a weak platform for predominantly textual and numeric information and is more naturally the platform for sound and images.
The major impact of multimedia, therefore, and its chief advantage over all previous information media is based on multimedia's ability to deliver an unprecedented range of high quality image and sound information, seamlessly integrated with accompanying text and numeric material.
Images raise complex problems in multimedia for one main reason.
They require a great deal of digital information to describe them.
The exact amount depends on the quality of resolution with which the images are rendered.
In a computer environment, screen displays are made of thousands of illuminated pixels, points on the screen which can be individually illuminated to form a viewable image.
By refreshing the pixels with new instructions, images can be made to change and if they do so sufficiently quickly, one result can be animation, computer-generated motion.
There is a straightforward relationship between the resolution of a particular type of screen and the amount of data need to support that resolution.
For example, one of the standard monitor resolutions now widely used in computing is called VGA.
A typical VGA screen consists of 640 pixels across and 480 pixels down.
This means that a screenful of information is provided by 307,200 illuminated dots.
The quality of resolution depends not only on this absolute number of pixels but on the number of colours and intensities which can be utilised at each pixel.
The data needed to provide this colour/intensity control is provided in the form of a bit code.
Codes based on a higher number of bits can deliver more colours and intensities than those using very few bits.
In the case of VGA, the size of the code is typically 4 bits for each pixel.
In other words, every two pixels accounts for one byte of information.
At VGA resolution, therefore, a static screenful of information needs 153,600 bytes (153.6 kilobytes).
Now let us look at television information and gain some sense of the amount of data needed to represent motion video.
In the United States and Japan the system of encoding television signals is called NTSC.
It defines a television image in 520 horizontal lines of information, updated 60 times each second.
In the UK and most of Europe, the PAL standard is used.
Although similar in concept to NTSC, PAL uses 625 lines updated 50 times each second.
Each update in fact provides half a frame of information.
Since television pictures are built up by interlacing two half-frames, the factor of 50 updates per second means that the picture is updated at 25 complete frames each second.
The horizontal resolution along one of the PAL lines is equivalent to about 700 dots, each a separately luminescing spot on the cathode ray screen.
This means that a screenful of PAL television is represented by roughly half a million dots.
Translating the colour quality required into digital terms means that each of these dots has to be addressed by the equivalent of 16 bits of information.
25 frames per second, therefore, equates to around 20 megabytes of data every second.
This is the rate of data transfer needed to produce television quality video in full motion on a full screen.
Even a single, still image at this quality requires about 800 kilobytes of data.
The huge amounts of information needed to convey images such as colour stills or motion video, represent one of the greatest challenges for multimedia technology.
The problem is both one of storage and processing power.
It may be more of one than the other, depending on the basic approach of the multimedia system.
There are two broad approaches.
3.4 Digitising Video in Real Time
First, there is a kind of ‘online’ approach.
Video and other images can be captured, manipulated and displayed in real time.
For example, some types of multimedia applications will involve online systems which combine live television information with other digital data.
In this situation, the video input requires no longterm storage.
The process is one of real-time digitisation.
The television signal enters the computer environment via a special chip set.
The chip set continuously digitises the signal as it flows in at 25 frames per second (or 30 frames per second if it is NTSC) and passes the data to another chip set (in reality, a complete microprocessor in itself) which enables the digitised signal to be manipulated.
Finally the digitised, manipulated television picture appears on the screen of the computer.
Manipulations can vary from simple windowing of the video, presenting it in a small area of the screen, to sophisticated visual processing to produce the spectacular special effects familiar from broadcast television.
Of course, the in-coming signal does not have to be live television.
It could be video from any other analogue source such as video tape or videodisc.
The key point is that this approach takes video information out of the analogue world and converts it into the digital world of the computer and this conversion is an on-going, realtime process.
Whatever the source of the video information, therefore, the technical issue is principally one of processing power rather than the storage of large volumes of data.
Probably the single most important European developer of such systems is UK-based Videologic.
Their flagship product is currently a Full Motion Digital Video Adapter, the DVA-4000.
Although a number of DVA type boards are available for PCs, none can match Videologic's ability to manipulate the images in real time, shrinking and enlarging, saving frames to hard disc and displaying signals from multiple sources on screen simultaneously.
It also allows all the fading and mixing features you normally expect from broadcast television.
A commonly demonstrated feature of the system is the real time manipulation of video windows, allowing them to be re-sized, zoomed, tiled and shrunk using extremely simple control commands.
It is the ease of control coupled with the manipulative power of DVA-4000 which is Videologic's most impressive achievement.
The video is displayed in place of one or more of the colours available on a conventional VGA screen.
So, putting video into a window simply involves restricting the video image to the area designated as the window and replacing the background colour in that area by the video.
Because DVA-4000 provides two frame stores, it is also possible to keep the video running in one window while freezing an image from the same video sequence in another window.
Using an add-on TV tuner with the system allows live television to be handled in the same way.
3.5 The ‘Off-line’ Approach
The second approach is ‘off-line’, based on the use of stored digital information which is retrieved and processed as required by the user.
No conversion from analogue to digital data is needed.
The major technical problem is twofold.
There is the difficulty of finding a big enough storage medium to hold all the required data and there is the problem of retrieving large volumes of data from the storage medium fast enough to update the screen for moving information.
3.6 Mixing Digital and Analogue Information
There is a third route which, bearing in mind our concept of multimedia existing within a single, digital environment, is not strictly an exercise in multimedia as far as this study is concerned.
However, it exists, works well and has benefited a great many users, particularly in education and training applications.
This third route, does not attempt to integrate analogue sources of images and sound within the digital environment.
Instead, it allows the analogue and digital information to co-exist.
A typical system would be a videodisc player operating under computer control.
The multimedia output from such a system can either be displayed on two screens with the digital information on one and analogue on the other, or on a single screen with digital information generated by the computer, overlaid on the analogue images.
The principal technical issue in this bilateral approach arises from the fact that computer information requires quite different display technology to analogue information.
The use of two separate screens evades this problem but produces a clumsy and inconvenient solution.
A far superior approach is to manage the analogue signal in such a way that it can be displayed on a computer screen.
This has been the method used by the most successful interactive video systems of recent years.
The heart of the technique is an interface card inserted into the computer which synchronises the different information sources.
This is referred to as a Genlock card, a term arising from television broadcasting jargon meaning ‘generator lock’.
Genlock enables the computer to handle rapid picture switching and can overlay computer-generated text and graphics by ‘cutting into’ the analogue picture from the videodisc.
The interface card itself can usually also handle sophisticated on-screen effects such as picture fades and multiple colours.
However, so long as the images are in analogue form, they cannot easily be processed and manipulated.
While applications do not always positively demand this capability, the overwhelming trend in modern multimedia is towards providing it.
This implies, therefore, the conversion of analogue to digital and the integration of all the media elements within a unified information environment.
3.7 Storing Digital Images
Let us return to the digital multimedia problem of massive information stores and the handling a very high speed of information retrieval.
Obviously, the speed of retrieval is not an issue if the images involved are still pictures.
However, even if each is to be produced to VGA resolution standards for display on a standard computer system, a single image will consume about 160 kilobytes of data.
Multimedia databases for computer display containing thousands of such illustrations would therefore need hundreds of megabytes of storage for the images alone, leaving very little room for anything else.
For display through a television, one image needs about 800 kilobytes, making the storage problem even more acute.
In the case of full motion video, the need for very fast information retrieval is acute.
The problem of volume storage is compounded by the need to refresh the screen at the equivalent of 25 frames each second to achieve the same effect as television video.
As we have seen, at PAL television standard, this means retrieving and displaying over 20 megabytes of data per second.
It is in fact possible to take different views about what genuinely constitutes full motion and we address this more fully in section 3.20 below, when we discuss compact disc interactive (CD-I) technology.
As far as motion video, in particular, is concerned, the constraints of available technology force multimedia into a seemingly impossible situation.
The only kind of storage that can cope with the sheer volume of data involved is an optical disc and the only practical format that can be used is the compact disc (CD).
However, because of the design of CD systems, data can only be retrieved from the disc at a rate of about 150 kilobytes per second.
Some CD technologists have described this rate as compact disc's ‘speed of light’ because it is an ultimate limiting factor of the system.
Clearly, there is no way in which digital video information can be effectively retrieved from a disc, at least not at the equivalent of around 25 frames per second.
In fact, the maximum a CD could manage would be one frame every seven or eight seconds.
As far as storage capacity is concerned, the situation is equally dire.
A standard disc could contain only 25 seconds of video.
One way of improving the situation is to compromise on frame rate, amount of the screen used for the video, or both.
Obviously, if the frame rate was cut by 40 per cent and reduced to 15 frames a second, the data required would be cut correspondingly.
The trade-off is a significant loss in the quality of the motion which depends on a reasonably high frame rate for the illusion of smooth, flicker-free movement.
Data volumes can also be cut by not using full screen video.
If the images are confined to a box area covering no more than a 15 or 20 per cent of the whole screen, the data volumes required are again correspondingly reduced.
Both these restrictions can be used in concert to obtain a combined advantage.
But even reducing the frame rate and using partial screen displays, still does not solve the problem.
To get around the CD's ‘speed of light’, a more drastic measure is needed.
The solution is to find some way of compressing the data.
This means making a small number of bytes do the work of many.
3.8 Data Compression
Data compression techniques are not special to multimedia.
They are simply a much more urgent requirement than in most other areas of application.
The benefits of data compression have always been obvious.
If a message can be compressed one hundred times, it can be transmitted in one hundredth of the time, or transmitted at the same speed through a channel with one hundredth of the band width, and it can be stored in one hundredth of the volume of the original.
Of course, the message will only make sense if it can be successfully decompressed when it reaches the user.
Although this analysis is not strictly accurate — there is always a compression/decompression overhead of about 5 per cent— the benefits and potential cost savings are substantial and compression techniques have been a major field of development from the time of the very earliest digital databases.
A full analysis of compression techniques is beyond the scope of this study.
However, because compression is of such great significance in the development of multimedia, it is important to touch on some key ideas.
3.9 Compression Through Redundancy
One of the most important approaches to compression is to exploit the inherent redundancies in information.
The simplest example of redundancy is to imagine a personnel record held in a computer system.
If you user ‘M’ and ‘F’to denote male or female in the gender field of the record, you require 8 bits of data to represent either character.
If you decide replace the characters with an agreed, fixed-length one-bit code, you immediately achieve an 8:1 compression of the gender field.
This kind of simple redundancy occurs widely in databases and a significant level of compression can thereby be achieved through little more than an exercise of commonsense and ingenuity.
3.10 Video Compression
In the case of images, considerable redundancy can be found simply by locating parts of the image that are the same.
The basic technique is called run length coding and works by replacing a long run of unchanging data by the shortest possible code.
For example, imagine a television picture, one third of which is showing a uniformly blue sky.
In digital terms, the sky would normally require about 0.4 megabytes of data.
However, a massive compression can be achieved by creating a relatively short code of just a few bytes which effectively tells the computer that ‘the next 400,000 bytes will be blue’.
The short code would then be decompressed just before display to produce the appropriate number of blue pixels to represent the sky.
This concept underlies a technique for compressing video which has become vitally important in multimedia.
The basic idea is extended, however, to keep track of a constantly changing picture and to detect those elements in it which do not change.
The process, called Digital Video Interactive (DVI), was invented in the late 1980s in the RCA Laboratories of the David Sarnoff Research Centre, Princeton, New Jersey.
Now owned and being developed by chip manufacturer, Intel, DVI has become a major focus for multimedia.
We will consider DVI further below in 3.23 but it is helpful to review here the basis of its approach to data compression.
DVI originally used a DEC VAX minicomputer to break each frame of a moving video image into about 250,000 pixels, to compare each pixel in one frame with the pixels in the next and to identify points with no change in them.
In this way, the amount of digital information needed to code each image was cut by 120:1 to 0.8 per cent of the broadcast quality original.
Picture quality was poor and it took 30 seconds to process each frame.
In other words, compressing one minute of video took an hour and a half of real time.
DVI has been progressively enhanced since then, however, and is now based on an evolved form of a technique called Discrete Cosine Transform (DCT).
Instead of examining pixel-by-pixel differences, DCT concentrates on block patterns.
For example, if a car is shown moving across the screen, the DCT process starts by coding the car once and creating a reference frame of complete image information.
Between the reference frames are frames containing details only of the information that has altered.
They are called delta frames and, typically, may consist of no more than 5 kilobytes of data.
In our example, these would code the movement of the car and any slight changes in shape caused by varying perspective.
Other ‘full-data’ reference frames have to be inserted periodically in the sequence or else the picture quality rapidly degrades.
Their number for a particular application, can be specified before compression.
Video showing rapid and intensive change, for example, will need frequent reference frames to maintain an intelligible motion sequence.
Currently, DVI operates at compression ratios of around 160:1 allowing 72 minutes of full motion, full screen video to be stored on a CD-ROM.
The quality of the playback is at best comparable to a consumer VHS video.
3.11 Compressing Sound
Audio compression can also be achieved by identifying acceptable levels of redundancy and stripping them out.
For example, digitising an analogue sound signal requires the repeated sampling of the analogue waveform.
The value of the different levels arising from each of the samplings are turned into a bit code.
Reproducing a smooth waveform in digital code requires a very high rate of sampling every second, particularly if the wave is complex with many different component frequencies.
However, because the wave is being represented by discrete sampling operations, the digital representation will be a ‘stepped’ shape rather than a continuous smooth curve.
The more samples taken, the smoother and less stepped the representation will be and the better quality of the sound.
One way, therefore, of reducing the digital data is simply to reduce the number of samples taken.
This works but quickly leads to reduced quality.
Practical compression techniques try to cut the data with the minimum effect on quality.
Most serious work has focused on improving Pulse Code Modulation (PCM)— the basic method of breaking a waveform into discrete elements for digitisation — and on the transmission of the bit codes that measure the value of the wave form.
An important variation known as Adaptive Differential Pulse Code Modulation (ADPCM) is today used as the basic compression standard for the extended architecture form of CD-ROM (CD-ROM ‘XA’).
3.12 Extreme Data Compression
Other compression systems are being designed for special applications such as videoconferencing schemes.
There is also leading edge research taking place into pushing the limits of compression as far as they can possibly go.
Recent developments in a field of study known as Chaos Theory has focused attention on elements in natural shapes called ‘fractals’.
The idea is that large, complex systems are often made up of a single, simple shape repeated over and again.
In a sense, the concept of fractals suggest a huge underlying redundancy running throughout nature.
A fractal approach to data compression is still in its infancy but spectacular progress is being made.
In 1987, US researcher, Michael Barnsley, used fractal compression on a single 780 x 1024 pixel image.
It took him 100 hours but he achieved a ratio of about 1000:1.
Barnsley believes ratios of 10,000:1 are possible.
He has already applied the fractal technique to video data.
In 1990, at a London conference on Chaos Theory, he showed 45 seconds of 30 frames per second video on a desktop computer, retrieved from a 1.2 megabyte floppy disc, a compression of over 600:1.
3.13 Data Compression Standards
Data compression is so vital to the future of information media that considerable international efforts have been made to create global standards governing the way in which compression is handled.
The standards will eventually form the basis of all multimedia video communications services and publishing.
They will cover ISDN and Broadband ISDN, satellite and terrestrial broadcast systems, local area networks and storage media such as magnetic hard disc, optical disc and digital audio tape (DAT).
Each of the standards initiatives are seeking a common method of compression based on the DCT process.
Although we will not examine the technical aspects of these standards, it is important to identify briefly the most important initiatives.
There are three.
First, there is a standard which will apply specifically to videoconferencing.
It is called H.261 (or, sometimes, px64) and has been formulated by the European Commission's Consultative Committee on International Telephony and Telegraphy (CCITT).
It comes into force as an international standard during 1991.
Second is the Joint Picture Encoding Group (JPEG) which has now created a draft standard for compressing still images.
Images compressed under the JPEG standard at a ratio of 20:1 have been shown in printed form.
Even at this high ratio, quality is excellent with little visible degradation from the uncompressed image.
Curiously, one of the first applications of the JPEG standard has been not for still images but for motion video.
Hardware developer C-Cube of California, produced a single chip compression-decompression processor for video.
Although the degree of compression is not enough to allow retrieval from a compact disc, using a fast magnetic hard disc, reasonable full screen full motion video can be achieved.
The key factor in the C-Cube development is that the compression and decompression takes place in real time.
At the Microsoft Conference in March 1991, C-Cube showed they had gone a step further.
They jointly announced a collaboration with JVC (Victor Company of Japan), to manufacture a commercial chip set which appears to offer the same real time facility but with much higher compression than required under JPEG.
A demonstration by Mauro Bonomi of C-Cube showed excellent quality real time video playing back at 200:1 compression and at a CD-ROM data transfer rate.
As we will see, other systems such as DVI currently rely for top quality on lengthy off-line compression at expensively equipped laboratory facilities.
The third compression standard initiative is called the Motion Picture Experts Group (MPEG).
While H.261 and JPEG standards are both more or less finalised, the MPEG standard is still under discussion.
As its name suggests, MPEG is trying to define a standard for coding moving images and associated audio.
The Group works through six sub-groups.
The Video Group concentrates on coding moving pictures.
The Audio Group is developing standards for coding audio information.
The Systems Group is working on the joint representation of coded audio and moving pictures.
The VLSI Group is examining hardware implications for the coding process.
The Digital Storage Media (DSM) Group is considering storage media for the coded information.
Lastly, the Subjective Tests Group is defining a method of assessing the quality of the coded information.
The first-stage draft proposals from MPEG were published in September 1990.
Further announcements are due over the next two years.
The result will be, amongst other things, a standard for the compression, decompression and display of video.
This should make the video elements of multimedia playable across a range of MPEG-compatible hardware.
3.14 Optical Disc Media
Optical discs play a fundamental role in multimedia.
We have already touched upon the significance of compact discs and their increasingly standard use as storage media in multimedia systems.
However, we need to start with an older, non-digital technology which still has an important role in current developments.
3.15 The Videodisc
In the early 1980s, at the same time that the home computer boom was at its height, the LaserVision videodisc was exciting considerable interest as a publishing medium.
It is not hard to see why.
LaserVision (originally a proprietary name coined by Philips and, today, known generically as ‘laser disc’) technology emerged from the laboratories of Philips in the late 1970s.
It is based on encoding multimedia information on the reflective surface of a silvered 12-inch disc.
The encoding is done by a fine laser beam burning a series of microscopic pits into the disc surface.
During playback, another laser beam passes over the surface of the spinning disc, acting as a kind of non-material stylus.
The way the beam is reflected from the surface of the disc is of course affected by the presence of the pits, areas where the reflective surface has been burned away during the original encoding process.
This modulation in the laser beam caused by the varying reflectivity is itself detected, decoded and reconstituted as the original multimedia information.
Using this process, full motion, full frame video can be delivered together with the full range of other media types.
There are none of the problems we have been discussing concerning image data because the videodisc is an analogue medium.
The laser beam reading the pit code on the disc, is affected by the varying length of the pits.
This creates the continuously changing value, characteristic of analogue information.
Although not a digital medium, the videodisc was the starting point for modern multimedia development.
There are two main reasons.
First, a standard disc offers up to 55,000 frames of information each with its own unique electronic address or frame number.
This makes it possible to access any individual frame by calling up that address and displaying its contents.
Second, because the disc offers radial access to its contents, an individual frame or sequence of frames can be located and retrieved very quickly.
Contrast this with efforts to use a linear medium such as videotape interactively.
Even with careful design, the time taken to wind tape back and forth to locate required information, makes it laborious and frustrating to use, even in the simplest applications.
The process of locating and retrieving required information from the disc is carried out by a microcomputer linked to a laser disc player or by microcomputing power built into the player itself.
Software suitably designed for the particular application is employed to control the user's interaction with the multimedia data on the disc.
This means that according to the choices made by the user, a particular pathway is followed through the information, with the computer drawing as required on the text, sound, still images and moving video stored on the disc.
In this context, the video disc is no more than a peripheral to the computer, much as any external disc drive might be.
It is simply a source of displayable information, utilised under computer control as a part of an interactive information product designed for a given purpose.
The manufacturers of videodisc players in the late 1970s and early 1980s had high hopes for commercial success across consumer, professional and business markets.
The reality has fallen far short of these early expectations.
Although we cannot properly analyse the fortunes of the interactive video industry here, we can at least summarise.
The only substantial markets have proven to be in dedicated applications.
In these, interactive videodiscs have been commissioned to address a single, bespoke purpose rather than to provide a product speculatively for a wide range of possible customers.
Generally, these bespoke applications have been in the fields of industrial and professional training and in retail promotion such as point-of-sale and point-of-information systems.
There are, however, a significant range of generic publications available.
The 1991 courseware catalogue from the European Multimedia Centre (formerly National Interactive Video Centre, a significant change of name in this context), lists over 350 video disc titles in categories such as computing, education, health and safety, electronics, languages, management, marketing and sales, and medical.
However, sales of such products are severely limited by the small and patchy installed base of players.
Indeed, very few players have been purchased outside the requirements of dedicated applications.
Selling generic discs, therefore, often means trying to bundle hardware with the discs.
One of the few companies to have had any serious success in selling generic products is Interactive Information Systems (IIS) who offer their customers the opportunity to lease the hardware needed to run the laser disc applications.
On the whole, however, the commercial picture has been bleak.
It is too soon to say that the videodisc should be consigned to the scrap heap of technological history.
Old and non-digital as it is, videodisc technology is today still the only platform available that can deliver high quality, full motion, full frame video complete with a range of trick frame features such as forward and backward variable speed motion and perfect freeze frames.
As well as offering excellent interactive video potential in itself, it continues to provide an useful stored source of video imagery for use in real-time digital video systems using digitising interface adapters such as Videologic's DVA-4000 or IBM's M-Motion Video Adapter.
While the future may well belong to thoroughbred digital systems, right now it is a question of satisfying today's needs with today's technology.
In an article entitled ‘The Videodisc is Dead’ published in the newsletter Interactive Media International in October 1990, Jonathan Davies writes:
So why is one of the Japanese juggernauts — Pioneer Electronics — boosting its commitment to laser disc?
And why have major consumer electronics hardware and software companies chosen 1990 to launch the European Laser Disc Association?
Pioneer believes that, as in Japan and the United States, analogue videodisc is growing, especially in the consumer sector.
Pioneer must be assuming this trend is going to continue for some time; its Wakefield plant does not go fully on line until 1994.
Davies makes the important point that, superior as the unified digital environments for multimedia may be, right now they simply cannot deliver television quality video in a form you can easily buy.
The promise is there but, for the moment, if we want high quality interactive video to use today, we have to compromise on the digital ideal.
Of course, you will have to want interactive video quite badly because videodiscs and their players are costly.
So, to some extent, the commercial issue remains one of a promising medium in search of paying customers.
It is of course only a question of time and evolution.
Videodiscs have a window of opportunity to exploit, perhaps lasting a few years, before the digital media at last deliver their promise fully and affordably.
The videodisc is not dead, therefore.
It is merely dying.
3.16 Compact Disc Arrives
The compact disc looks physically like a smaller version of the videodisc.
Its diameter is only 12 centimetres rather than 12 inches but its silvered surface is covered in laser-etched pits like a videodisc.
There is, however, a fundamental difference.
In the case of CDs, the length of the pits is of no significance.
The CD laser stylus is designed to detect only the presence or absence of a pit.
It reads ‘pit’ or ‘no pit’, ‘on’or ‘off’, the basic two-state language of binary code.
In other words, the information on CDs is digital.
The first compact discs carried digitally encoded high fidelity music.
Philips released CD Audio (CD-A) players in 1983 and their success was immediate and dramatic.
It was a stark contrast with the commercial failure of the videodisc.
Today, CD-A looks like becoming the biggest success in consumer electronics since the television.
One reason for this was the lesson Philips and other consumer electronics companies had learned about the importance of global standardisation.
Part of the failure of videodiscs, particularly in consumer markets, was due to the lack of standardisation among discs and players.
In the case of CDA, however, Philips were determined to avoid this problem.
They collaborated with Sony to produce a single, de facto standard enshrined in what has become known as ‘The Red Book’.
It was a standard which they readily made public allowing other manufacturers to produce, under licence, both compatible discs and players.
The existence of the standard meant that any Red Book disc could be played on any Red Book player, anywhere in the world.
This degree of global standardisation gave a powerful boost to both disc and player development.
The combination of technical standardisation, unprecedented sound quality, the robustness of the medium and the huge consumer demand for high fidelity music, had an almost explosive effect on sales.
In the same year Philips launched CD-A, player sales topped 350,000 and over 1,250,000 discs were sold.
In 1990, estimates suggest about 50 million players and around 350 million discs were sold worldwide.
The Record Industry of America's figures show that CD-A disc sales in the United States have remorselessly overhauled and eroded sales of vinyl recordings and are rapidly catching the booming cassette market.
They are in millions of US$ at retail prices.
3.17 Compact Disc Information Products
It was a short step from encoding music on compact disc to encoding information a computer could read.
After all, CDs are digital.
The bits and bytes could just as easily represent a large textual database as the music of a rock band.
3.18 Compact Disc Read Only Memory
In 1985, following further collaboration between Philips and Sony, the first Compact Disc Read Only Memory (CD-ROM) emerged.
Although generically similar to CD-A, CD-ROM technology is far less friendly and accessible.
CD-ROMs store computer data.
This means that reading the discs requires the paraphernalia of a computer.
Specifically, the CD-ROM player (sometimes, as if to emphasise the computer connection, called a ‘disc drive’) has to be linked via a suitable interface to a computer.
Search and retrieval software loaded into the computer serves as the user's ‘electronic hands’ to sift and access the data on the disc which is then displayed on the computer screen.
This computer orientation immediately places the appeal of CDROM largely (though not exclusively) outside the consumer environment.
While the easy-to-use, appliance technology of CD-A continues to flourish in the consumer electronics market, CD-ROM finds its applications in more specialised business, professional and academic sectors.
Indeed, as we have noted in chapter 2, librarians are amongst its biggest users.
As with CD-A, Philips saw the crucial importance of worldwide standards.
This time, however, the issues were more complex.
Whereas CD-A discs really needed little more than a physical standard, CD-ROMs also had to address logical standards, defining the format for structuring data records, volumes and files.
The logical standard took a few years to agree but, in the late 1980s, was finalised and ratified by the International Standards Organisation as ISO 9660.
Today, almost all CD-ROM players and discs are manufactured in line with IS0 9660.
This more or less guarantees that all CD-ROMs, players and computers will work together.
In practice, there are sometimes problems.
On the whole, however, CD-ROMs are now as physically integrated with computers as any other peripheral; indeed, more so since new generation microcomputers are increasingly being designed with built-in CD players alongside the conventional magnetic floppy and hard disc drives.
In other words, CD-ROMs are fast becoming a standard form of computer storage.
What makes CD-ROM so remarkable?
In a sense, nothing.
Essentially, it is just big filing cabinet for keeping digital information.
The significance lies in what can be done with it.
Its basic characteristics offer plenty of possibilities.
First, at a physical level, it is robust and longlasting.
Second, it offers a massive storage capacity.
Reading data from a CD-ROM depends only on recognising the presence or absence of pits in its silvered surface, so the pits can be both very small and tightly packed.
This allows about 650 megabytes (650,000,000 bytes) of data to be stored on a single disc.
This is equivalent to about 300,000 pages of text.
If printed on 60 gram per square metre paper, this is a stack about 36 feet high, the output of eight medium-sized trees.
Third, CD-ROM, as its name implies, is a read-only medium.
This immediately commends it to traditional publishers who have been in the business of selling paper-based ROMs since Gutenberg first invented printing.
In other words, CD-ROM is a publishing medium.
Fourth, as with any suitably designed electronic database, the contents of a CD-ROM can be searched with the appropriate software and data can be located, correlated and retrieved in seconds.
CD-ROM has one serious drawback.
Compared to traditional computer storage such as magnetic discs, CD-ROM is slow.
This is a consequence of its origins.
The original CD players designed for the audio playback market, used a technique of reading the disc called Constant Linear Velocity (CLV).
As the laser stylus tracks over the surface of the disc, the rate at which the laser reads the disc data is kept constant by changing the speed at which the disc is spinning.
When the laser is reading information from the small, inner diameters of the disc, the disc's rotation is significantly slower than when it is reading data from the larger, outer diameters.
CLV was chosen because it maximises the CD's playing time, an important feature for CD audio discs.
However, it also makes data retrieval from different parts of the disc slow compared, say, to random access from a magnetic disc.
The difficultly of achieving high-speed random access can be reduced by careful design of the data layout.
The inherent slowness can be disguised by locating various data that are likely to be retrieved or correlated, in close physical proximity on the disc.
It is a formidable problem of optimisation but it is, at least, a potential solution.
CDs have a much more absolute restriction which no amount of optimisation can avoid.
It is a consequence of the playing time of a CD running under CLV.
The maximum is 74 minutes which is, therefore, the maximum time available to read about 650 megabytes of data.
Simple arithmetic quickly tells us that this is equivalent to an average data transfer of 150,000 bytes per second.
In practice, some systems can be made to deliver short-term bursts at higher rates.
In general, however, this is the limit for pulling data off the disc.
We have already referred to this as the ‘speed of light’ for CD information media and in the earlier section dealing with image handling in multimedia we have seen how serious this limitation is when CDs are used to deliver video.
We re-emphasise this limitation now because CD-ROM is the evolutionary starting point for CD multimedia systems and its ‘speed of light’ underlies the greatest single technological challenge in multimedia development.
3.19 CD-ROM and Multimedia
CD-ROM's central role in multimedia is far more evident today than when it first emerged.
In 1985, even its originators, Philips, argued that CD-ROM was a text-only medium.
Accordingly, most products developed were based on large, textual databases and it was only slowly that developers began stretching the medium to deliver images and sound in addition to text.
However, not surprisingly, Philips are fundamentally correct.
CD-ROM is essentially a textual medium.
Certainly, images and sound can be stored as data files on CD-ROM and retrieved when required.
Digital data is digital data and whether it represents text, pictures or sound, it can be stored within the prescriptions of ISO 9660 on CD-ROM.
The real difficulty lies in the fact that CD-ROM can only process one type of data at a time.
In other words, if you need to access text and images together, CD-ROM can only retrieve and display the text and then subsequently address the related image data.
Alternatively, if sound is being played back, it will be interrupted by a search for, say, some further text.
This laborious process can be disguised (but never eliminated) by careful product design, such as holding one type of data in the computer's memory while another is retrieved.
Indeed, ingenious CD-ROM publishers nowadays often regard it as little more than a development constraint to be accepted, designed out and disregarded.
This means that, despite the constraints, CD-ROM is increasingly regarded as a viable multimedia platform, particularly attractive because it is going to fast become a standard, built-in feature of mass market microcomputers.
An emerging answer to CD-ROM's deficiencies in delivering multimedia is CD-ROM ‘extended architecture’(CD-ROM ‘XA’), a new, interim CD-ROM standard developed jointly in 1988 by Philips, Sony and Microsoft.
‘XA’ addresses CD-ROM's deficiencies by permitting different types of data — text, sound, images — to be interleaved and thereby accessed in parallel from the disc.
The interim standard also allows for an initial step towards data compression which, in the future, will be extended to permit full multimedia capabilities.
The initial ‘XA’ standard deals only with audio compression, offering a ratio of 16:1.
To process the interleaved, multiple data streams from the ‘XA’ disc and to decompress the audio, a special plug-in card is required for the computer.
CD-ROM ‘XA’ has similarities to compact disc interactive (CD-I) which we examine in more detail below and, in principal at least, it is possible to build bridges between the two media to make them compatible with one another .
As a demonstration of what ‘XA’ can do, Sony have produced a disc which offers a sequence of text, still images and 110 by 92 pixel, colour animated video running at 15 frames per second, combined with a simultaneous spoken commentary and background music.
The commentary can even be switched between English and Japanese.
The fairly long seek times of CD-ROM, caused by its CLV retrieval system, can still cause pauses of between a half and one second while the read head moves from one part of the disc to another.
The ‘XA’ standard overcomes this by allowing some audio data to be downloaded directly into the computer's memory and then uploaded into the audio processors on the plug-in ‘XA’card.
This makes it possible to keep audio output going even when the read head is on the move and the CD-ROM is not being accessed.
It also means that, in principal, audio could be imported from other sources such as hard or floppy discs and combined in the CD-ROM environment via uploading through the ‘XA’ board.
The current ‘XA’ standard covers audio encoding and graphics.
Further developments will include the processing of still and moving video but decisions on this will probably await the final ratification of both JPEG and MPEG standards.
Meanwhile, relatively few ‘XA’ applications have emerged and, for the moment, the installed base of ‘XA'-equipped CD-ROM systems remains small.
Multimedia delivery from CD-ROM, of course, does not only place demands on the software developers in terms of design finesse.
It has important implications for hardware manufacturers too.
Although Apple's Macintosh machines have always offered high-resolution screens and excellent graphics and audio architecture, they represent little more than 12 per cent of the personal computers in use today.
The rest are mainly those functioning under the MS-DOS operating system and are typically machines with display, graphics processing and sound capabilities falling far short of the demands of multimedia.
However, major changes are imminent in MS-DOS hardware development.
Microsoft have already moved strongly to meet the new generation of MS-DOS multimedia computers which is about to emerge.
Early in 1991, they announced an extension to their popular Graphical User Interface, Windows 3 which has itself sold nearly 2.5 million copies since it was launched in May 1990.
The new software, called Windows M, is the result of a cross licensing deal between IBM and Microsoft.
It can be added to a Windows 3 or OS/2 personal computer enabling it to handle a range of multimedia elements, including high resolution colour graphics, sound and animation.
The specification for computers running Windows M includes special sound recording, synthesis and playback hardware, as well as a CD-ROM interface.
A number of third parties have already announced they will be providing one-board upgrade kits to support Microsoft's specification.
Many major PC manufacturers will also soon be selling computers with all the necessary hardware built-in.
Companies expected to start shipping this new generation of multimedia PC during 1991-2 include Zenith Data Systems, NEC, AT & T, Tandy, Olivetti and CompuAdd.
Others are sure to follow, forming a powerful vanguard of machines which will become the standard multimedia workstations of the 1990s.
In spite of these prospects, traditional CD-ROM media still have inescapable multimedia limitations.
Their inability to do more than one thing at a time is only one problem.
The limit on the rate at which data can be retrieved and the consequent failure to deliver video is another.
This latter problem is now being addressed by compression-decompression techniques, the most important of which is Digital Video Interactive (DVI).
We will examine DVI shortly.
Before we do so, we should describe an important response from Philips to the need for a consumer platform for multimedia as distinct from one appealing to more specialised CD-ROM markets.
3.20 Compact Disc Interactive
Whatever may be done to turn CD-ROM into a vehicle for multimedia, it is never likely to appeal in volume to the consumer market.
The computer industry has only recently realised that, on the whole, ordinary people do not like computers and are only comfortable using them when they do not know they are doing so.
This simple idea was the starting point for Philips’ effort to enter the consumer market with a multimedia CD product that felt and looked more like a domestic appliance than a computer.
The core of their problem was that such a device necessarily had to be a computer and a powerful one.
The answer they devised is called Compact Disc Interactive (CD-I).
Dick Fletcher who runs New Media, one of Europe's leading CD-I development companies, has for long suggested a simple rule of thumb for deciding whether a device is a computer or an appliance.
Computers, Fletcher maintains, always have their on-off switch on the back.
Appliances have them on the front.
Accordingly, CD-I players begin the battle to represent themselves as appliances by having their power switch on the front of the cabinet.
But the disguise runs deeper.
Physically, they look much like CD-A players.
The computing power is hidden in a simple, compact box.
To display CD-I, the player plugs directly into a domestic television.
There is no keyboard and CD-I users interact with the on-screen information using a simple infra-red keypad which controls a cursor on the television screen.
By moving the cursor and clicking the keypad's fire buttons, options can be selected and an interactive multimedia programme can be enjoyed in the comfort of the average domestic living room.
CD-I players and discs conform to a greatly extended version of the CD-ROM standards originally established by Philips and Sony.
The result is the same and is crucial to commercial success.
Any CD-I disc will play on any CD-I player using any television set, anywhere in the world.
Television standards do not affect CD-I development.
The contents of the same CD-I disc can be displayed in NTSC as easily as in PAL or SECAM.
From a CD-I publisher's point of view, this is important because a single edition of a CDI product, perhaps with multiple audio tracks to address language differences, can be distributed to markets throughout the world without fear of incompatibilities in the playback systems.
This strong international potential may well be one of the reasons why Philips has been joined in its CD-I initiative by partners such as Sony and Matsushita.
Together, the three companies represent the most powerful alliance of consumer electronics giants ever seen and represent formidable financial muscle with which to back the launch and worldwide distribution of CD-I.
An additional strength of CD-I is that the hardware will double as high quality CD-A players.
This means that initially CD-I may be seen by those either upgrading their CD-A players or buying for the first time, as an added available dimension.
In other words, the extra cost — the entry level price for the first CD-I players will probably be between £500 and £600 — may be seen as an acceptable premium on a CD-A player which buys, in addition, a multimedia information system.
CD-I is specifically designed to deliver high quality multimedia on a domestic television receiver.
Unfortunately, it suffers from an important drawback.
Although the operational environment of CD-I ensures that, unlike CDROM, it is ideally suited to processing high quality sound and pictures together with computer graphics, text and numerics, it cannot deliver full screen, full motion video.
The ‘speed of light’ of CD-ROM discs applies equally to CD-I discs.
The data necessary to convey video simply cannot be pulled of the disc fast enough.
Nor could much video information be stored on the disc.
3.21 CD-I and Video
This limitation has only been seen as a major crisis by Philips since the emergence of DVI compression-decompression.
We have already considered how DVI works in our section on video compression (see 3.10) and we will look in more detail at its practical implications below in 3.23.
From Philips' point of view, DVI looks threatening because it enables full frame full motion video to be accessed from a standard CD-ROM.
This suggests a generation of CD video products which conceivably might compete with CD-I or, at the very least, highlight CD-I's shortcomings.
So serious has this issue become to Philips that the launch of CD-I has been substantially delayed while Philips engineers attempt to produce their own compression-decompression architecture to make video possible within the CD-I standard.
The delays have become so prolonged that there are rumours that Philips' Japanese partners are becoming restive.
Real fears are emerging that Philips may have seriously eroded confidence among potential CD-I publishers to the extent that the right critical mass of discs to drive the technology from its initial launch may not be achieved.
This has become so serious a concern that early in 1991, less than a year before their latest deadline for the launch of CD-I, Philips themselves established their own CD-I publishing operation, perhaps in an effort to energise CD-I disc investment or to make up for the lack of it.
Some industry analysts believe that DVI has started a false hare which Philips and others are now chasing.
Is full frame, full motion video really important?
After all, CDI can deliver video even though it is certainly not full screen and may not be considered full motion.
Typically, CD-I can deliver video, for example, at 12.5 frames per second in a box covering 16 per cent of the screen area.
In the United States, the frame rate is 15 frames per second because the domestic electricity supply pulses slightly more rapidly than in Europe.
Two questions need to be asked.
First, is between 12 and 15 frames per second full motion?
Although it is true that European video runs at 25, US video at 30 and movies worldwide at 24 frames per second, these frame rates are not essential to flicker-free motion.
Depending on the kind of images being viewed, flicker can often be imperceptible at as little as 18 frames per second.
So, at worst, CD-I could look very close to full motion.
Second, can CD-I deliver full frame video?
The blunt answer is, of course, ‘no’.
However, behind this question lies another more important one.
Does full frame really matter?
After all, a large screen TV showing video in a 16 per cent window will be more easily viewed than a small screen television showing full frame video.
What matters is how the screen is used in a given application.
Does the overall effect communicate effectively?
The fact that full screen video has become an apparent norm, imposed by our experience of television, cinema and — amongst a select few — videodisc, does not mean it has to be an essential ingredient of interactive multimedia.
This is a key issue in multimedia only because full motion, full frame video has been talked up by the industry until it even has its own acronym (FMFFV or FFFMV, depending on preferences).
It is unclear whether any of the proponents of FMFFV have actually tested the theory that the consumer market really wants it.
Yet, on the basis of a supposed connection between television culture and consumer interactive multimedia, huge commercial efforts are being expended that are literally shaking the foundations of the industry.
It is quite possible to see DVI and its capabilities as distinct from CD-I.
DVI, after all, is a process by which video can be carried in a CD-ROM database.
We have already indicated that CD-ROM is essentially a professional medium with only marginal appeal in the consumer market so DVI can also be seen addressing this sector rather than competing with CD-I.
The likelihood is that DVI will form an important element of the world of computers and data processing while CD-I, with or without FMFFV, will appeal to the world of consumer appliances and entertainment.
True or not, this will not alter Philips' determination to provide CD-I video.
Sadly, the latest evidence suggests that, despite the delays, Philips may still not be ready with video at the launch and that the first CD-I consumer players will be sold with an expansion slot for a FMFFV cartridge upgrade, available at a later stage.
The issue of standards also deeply affects the efforts to deliver FMFFV.
Any major player in this field has to take the imminent MPEG standard very seriously.
Because DVI is a programmable technique it will be possible to revise its algorithms with little difficulty to ensure that it conforms with MPEG requirements when they are known.
The same is unlikely to be true of CD-I and part of the delay in delivering FMFFV may be due to Philips' concern to know the requirements of the standard before committing themselves.
Philips also point out that the omission of FMFFV in initial CD-I shipments will not be much noticed because no CD-I discs are currently planned incorporating it and none are likely to appear for up to two years after the launch.
3.22 Commodore Dynamic Total Vision (CDTV)
While CD-I has remained a promise for more than four years, Commodore has used existing technology to produce a consumer multimedia system that is seen by many to be a serious competitor for CD-I.
Commodore's extraordinary name for the system was determined by their concern to avoid any potential trademark problems by actually calling it CDTV, the name they favoured and by which the system is now universally known.
Although it is not clear where the danger lay, it appears that, used as an abbreviation, CDTV cannot infringe third party rights.
Commodore's initiative has a generic similarity to the Philips CD-I concept.
CDTV uses a CD-ROM system coupled to a powerful computer, disguised as a domestic electronic appliance.
Physically, it looks much like a video cassette recorder with a matt black livery chosen, no doubt, to suggest its connection with the world of hi-fi audio, televisions and video.
Like CD-I, the system plugs directly into a television receiver and the CDTV user operates it with a remote, infra-red handset.
Internally, CDTV is substantially different to CD-I.
Commodore have ingeniously integrated established technology to create the appearance of a new kind of multimedia system.
Essentially, CDTV is a CD-ROM player and a Commodore Amiga home computer packaged together in a black box.
Some modifications have been made to both the hardware and the Amiga's operating system.
In particular, the operating system has been enhanced to include an ISO 9660 file handler and a data compression-decompression facility.
The CD-ROM drive hardware conforms to the standards established by CD pioneers, Philips and Sony.
Used in a purely audio mode, the drive acts as an CD-A player providing quality equivalent to a fairly basic (eight-times oversampling), dedicated CD-A player.
In its data reading mode the drive can deliver ISO 9660 standard CD-ROM data, displayable on an ordinary television or, if preferred, on a computer monitor.
In April, 1991, the same month as CDTV was officially launched in the UK, Commodore announced a software facility they call CDXL.
This delivers, through the unmodified hardware configuration, motion video at between 12.5 and 15 frames per second over about 25 per cent of the screen.
Although CDTV, like CD-I, attempts to disguise its computer origins, it comes with appropriate ports and connection interfaces to make it possible to add a keyboard or mouse, external floppy disc drives and other standard computer peripherals.
In other words, although CDTV is designed to look like appliance technology, it discreetly provides a conversion option making it possible, if desired, to turn CDTV into an overt computer system.
Although CDTV is positioned as a multimedia platform, it has not been designed from the ground up specifically for the purpose.
It is an astute re-combination of existing, readily available hardware and software.
Commodore's innovation lies more in their business concept than in their technology.
Because of this, CDTV is not an ideal multimedia platform.
Like any CD-ROM system, it effectively only does one thing at a time.
Accessing text, images and sound is essentially a serial process and this inevitably limits the full integration of the various multimedia ingredients.
It cannot deliver full frame, full motion video although this may be available in later models.
Finally, excellent as the Amiga is in its handling of images and sound, CDTV is unlikely to approach the presentation standards of CD-I, a medium designed by Philips from the outset to be a high-quality consumer multimedia platform.
Despite the shortcomings compared to CD-I, at the time of writing CDTV has one major and unanswerable advantage.
Commodore's product is actually available while CD-I is still merely a promise.
So long as this remains true, CDTV cannot suffer by comparison.
For the moment and until Philips and their partners finally launch their product, CDTV has an opportunity to scoop a substantial segment of the market.
3.23 Digital Video Interactive
We have already referred in some detail to Digital Video Interactive (DVI) above and in the section dealing with video compression (3.10).
We will not therefore repeat the underlying methodology but consider DVI's more practical aspects.
As we have seen, DVI is essentially a process by which digital video information is compressed and decompressed to enable it to be stored and retrieved as full frame, full motion video in a CD-ROM system.
It may of course be used in other applications requiring high levels of compression, possibly using conventional hard discs or other magnetic media, but currently it remains most significant in relation to CD-ROM systems.
Originally developed in 1986 by RCA and General Electric, the technology was subsequently acquired by chip manufacturers, Intel.
In 1989, both IBM and Microsoft issued major endorsements of DVI, suggesting that DVI processing architecture will eventually be integrated into the next generation of desktop computers and their operating systems.
Currently, to obtain the best possible quality, known as production level video (PLV), DVI compression can only be achieved using a powerful mainframe configuration running Intel's proprietary software.
This can compress video of 30 frames per second (as required by NTSC, the US television standard) at a resolution of 256 by 240 pixels.
This is slightly better resolution than consumer-level videotape playback but falls short of broadcast quality television.
It is also possible to achieve desktop DVI compression, known as Real Time Video (RTV).
The chip set required for this captures and compresses in real time both still and moving video but at a significantly lower resolution than PLV.
Working at 30 frames per second, its best resolution is 128 by 120 pixels.
At present, RTV is intended as a development tool allowing applications developers to capture their images digitally and model a DVI application on their magnetic hard disc which they then can run and verify before sending video away to Intel's US laboratories for processing to full PLV quality.
At the Princeton Laboratories, even using powerful mainframe technology, it still takes an average of four seconds of real time to compress a single video frame.
At this rate, it takes Intel two hours to compress a minute of video.
Turnaround times from Princeton are seldom less than four weeks because of limited facilities.
At the time of writing, the cost of the service is a standard US$250 per minute of video compressed.
Whether PLV or RTV, once inserted in the CD-ROM database, the DVI file can be retrieved like any other data element.
However, to be interpreted as full frame, full motion video, it must of course be decompressed and the decompression has to take place in real time, while the application is running.
Currently, Intel offer a plug-in board for MS-DOS personal computers, embodying what Intel call their i750 chip set.
It consists of two main components: a pixel processor and a display processor.
Both are programmable.
This means that by changing the microcode in their chips, Intel can readily adapt DVI to conform with both JPEG and MPEG standards when they are known.
Currently, the chips are able to handle a maximum resolution of 1,024 horizontal by 512 vertical pixels allowing delivery of extremely high resolution still images.
Being digital, the DVI video data can of course be processed after decompression and before being displayed.
This makes it possible, for example, to achieve a range of special effects.
These include freeze frames, variable forward motion including frame-by-frame step throughs, windowing, fades, wipes, zooms, mosaics and spectacular user-controlled 360-degree panning views.
Reverse motion is impossible, however, because of difficulty in trying to reverse engineer the intermediary video frames between each individual reference frame.
There is an important trade-off between the amount of time available for image processing and the amount of data in the image.
At 30 frames per second, the amount of time available both to decompress and process each image is one-thirtieth of a second.
If the image being decompressed contains a great deal of data, as will be the case if there is much varying information frame to frame, decompression will take longer and less time will be available for processing.
The converse, of course, is also true.
Currently, DVI's compression ratio is about 160:1 and, applied to both still images and video, offers interesting combinations of different types of media on a standard CD-ROM.
For example, DVI can offer about one hour of full screen, full motion video with accompanying audio.
Applied to still video images, DVI offers 5,000 at very high resolution and around 40,000 at medium resolution.
A typical combination of elements might be 20 minutes of full screen, full motion video, 5000 high resolution stills, 6 hours of audio over the stills and the equivalent of about 15,000 pages of text.
Combinations are infinitely variable and the ultimate choice of elements will be determined by the requirements of a particular application.
DVI is not a longterm promise.
It is available today for use in CD-ROM development.
There are already signs that DVI is being seen as a delivery medium in itself rather than just a process.
The first dedicated DVI player was announced in October 1990 by Matsushita Graphics Communications.
Develped jointly with Intel, the Matsushita player is aimed at the consumer market and will be on sale in 1991 under both Matsushita and Intel badges.
Others are following.
The US Aplix Corporation is shortly releasing the Digital Pop, a DVI player for point of sale and presentation applications.
Data East USA Inc have announced a joint venture with Intel to bring DVI to arcade games while UK games firm, ACE Coin, have already launched a low cost DVI platform for the coin-op games market.
In some areas, particularly training and point of sale, it looks as if DVI applications may compete head to head with interactive videodisc.
Currently, however, DVI quality falls short of the quality of analogue video available from videodiscs and looks poor compared to broadcast quality television.
However, DVI is evolving rapidly.
It is only a matter of time before RTV reaches the current quality of PLV.
By then, PLV will have exceeded broadcast quality.
The eventual aim is for RTV to replace PLV altogether and, by the end of the decade, to offer a resolution of the standard of High Definition Television.
By mid-decade, Intel hope that personal computers using Intel chips will be available standard with DVI decompression chips included on the motherboard.
By the late 1990s, Intel predict they will be able to etch DVI into the silicon of microprocessors.
When this happens, everyone buying a personal computer based on Intel microprocessors will be getting DVI decompression capabilities built in.
DVI could then quickly become the de facto worldwide standard for delivering desktop video.
3.24 The Authoring Environment
Although perhaps not strictly a technological issue, a review of multimedia's technological environment would be incomplete without at least a broad assessment of the influence and significance of authoring tools.
Today, as multimedia technology proliferates, the number of available tools is growing correspondingly.
It would be impractical to attempt to review or catalogue them in this study and, although we will mention some specific products, we will confine this section mainly to illustrating general authoring issues.
The key issue is straightforward and it arises from a simple fact.
John Sculley, the founder of Apple Computer, put it well:
Let us remember that the printing press never wrote a single book.
Authors write books.
So, too, will it be with the new technologies…
So the question to be asked is how can the authoring of multimedia be taken out of the hands of technical experts such as computer programmers and be placed firmly in the control of the content, applications and creative experts?
One answer is that it never can — at least, not completely.
Multimedia is unavoidably a complex, technical environment and many design and application matters necessarily involve an understanding of the technology of the multimedia platforms and other special expertise, particularly in programming.
However, increasingly sophisticated and powerful software tools are emerging which make it possible for non-programmers to program.
Indeed, a major theme which is influencing the use and impact of computers in all kinds of applications is the increasingly simple way in which users can make them do complex and powerful things.
There has traditionally always been an inverse relationship between the power of a tool and how easy it is to operate.
In other words, the more powerful the tool, the harder it becomes to use it.
Examples of this relationship between power and ease of use are all around us.
The controls and instrumentation of a modern fighter aircraft, for example, are bafflingly complex.
Pilots need years of intensive training in order to master them.
But the controls are complex because the machine is immensely powerful.
The situation is dramatically different in an ordinary motor car.
The controls are simple but, of course, the machine can do very little by comparison with a warplane.
This relationship between ease-of-use and power is crucial to the global development of computer applications in general and of multimedia in particular.
So long as the techniques that determine both how software is used and how it is made are only in the hands of technical experts, computer systems will never find the universal application which is certainly within their grasp.
In terms of multimedia, this means two things.
First, the multimedia systems must be easy-to-use, however powerful their capabilities.
We have already addressed this idea to some extent in our discussion of user interfaces.
Second, the means of designing and developing powerful multimedia must also be simple and easy-to-use.
In this section, we focus briefly on these special tools for creating interactive multimedia easily.
As we noted above, some technical expertise is essential to an understanding of what different delivery platforms can or cannot do.
The tools are then available to let a non-technical author design and build the multimedia application.
The object of the authoring tool is to make it possible for the user to concentrate just on the information and design requirements of an application rather being diverted and befogged by the technical problems of putting them together in a system that works.
The multimedia hardware industry has for long recognised the vital importance of these tools.
For example, Videologic's real-time video capture and manipulation system, the DVA-4000 board, could not have become the great success it is today without Videologic's excellent accompanying authoring software.
The software, called Multimedia Interactive Control (MIC), is rather more than an authoring tool.
It also serves as a comprehensive set of operating system extensions which help to turn any MS-DOS computer into a multimedia workstation and controls not only the real-time display of video but the video source itself.
It does, however, allow complete interactive applications to be created using an extremely simple and easily mastered command set.
The DVA-4000 product including MIC costs a little under £2,000.
The world's most important computer manufacturers, Apple and IBM, have also created tools to encourage multimedia development using their hardware.
Apple, in particular, have been building their repertoire of authoring tools ever since they launched their quintessential multimedia platform, the Macintosh.
Their most important product is HyperCard, a kind of multimedia construction kit, using a highly simplified programming language called HyperTalk.
As its name implies, it uses the metaphor of a card as the basic information element and enables users to link together all the ingredients of multimedia in a rich and powerful, hypertext-like environment.
We examine hypertext briefly below (see 3.25) but it is worth noting right away that the hypertext model appears to be extremely effective in structuring multimedia applications.
HyperCard has been a huge success, both because it combines power and ease-of-use so well and because Apple have given the software away bundled with every Macintosh sold since 1987.
It can also be purchased independently for under £40.
Indeed, HyperCard has created such interest that IBM swiftly produced their own HyperCard lookalike which they call LinkWay.
IBM also now offer a more comprehensive multimedia tool called the Audio Visual Connection (AVC).
AVC is a software product designed for their PS/2 range of computers which integrates image and audio capture with media production, authoring and presentation capabilities.
At a practical level, the complete package requires IBM's PS/2 audio capture /playback card and their video capture adapter.
Software and cards together provide a complete authoring and delivery environment of unrivalled power.
It is the kind of system with which experts can quickly get to grips but it is also targeted at non-technical authors, enabling them to quickly edit text, sound and images either to create multimedia presentations or to produce interactive multimedia applications.
AVC is not a cheap solution, however.
The software together with the audio and video boards costs about £3,000.
Many other authoring tools are available.
Some are comprehensive like AVC while others provide part of the total multimedia solution.
For example, MacroMind Director is a highly successful animation tool for the Macintosh which provides a scripting language so that interactivity can be built in and also allows control of external devices such as videodisc players.
It costs about £700.
AutoDesk Animator is another proprietary animation tool but, in this case, designed for MS-DOS machines.
It offers a huge range of easily used animation features for under £200.
3.25 Hypertext and Hypermedia
Hypertext is becoming of some significance as an underlying structure in multimedia design and it is important, therefore, that we describe it, if only briefly, before we conclude our survey of the technological environment from which multimedia is emerging.
Hypertext is not a new concept.
As long ago as 1945, Vannevar Bush published a proposal for an information system he called Memex (‘Memory Extender’).
He described it as ‘a sort of mechanised private file and library in which an individual stores his books, records and communications, and which is mechanised so that it may be consulted with exceeding speed and flexibility’.
Given the technology of the day, Bush foresaw Memex storing information on microfilm kept in a special desk.
The desk would have various microfilm viewing positions to enable a user to compare different items alongside each other.
He struck on the underlying concept that is now called hypertext when he wrote:
All this is conventional except for the projection forward of present-day mechanisms and gadgetry.
It affords an immediate step, however, towards associative indexing.
the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another.
This is the essential feature of the memex.
The process of tying two items together is the important thing.
Bush was in fact foreseeing hypertext, a term not actually coined until twenty years later.
Ted Nelson used it in connection with his ambitious Xanadu project, an effort to create a single repository of everything that anybody has ever written.
Nelson has been struggling to implement this grand vision ever since but, in the course of his work, he established the hypertext model as a means of getting around a huge database.
Of course, hypertext is a reality today because of the availability of computer environments which make it a practical possibility.
The basic idea is an ‘interlinkedness’ in information which allows a nonsequential progress through a document.
Imagine reading some text.
We seldom just start at the beginning and read straight through sequentially to the end.
We often wish, if we could, to pause momentarily to examine some other related information — a footnote, reference or glossary, for example.
We may even wish to veer away from the main subject to investigate an interesting sidepath.
Hypertext makes this nonsequential approach possible by offering the very connections needed to jump instantly to other locations in a database where you find related information which interests you.
In a sense, an ideal hypertext environment would emulate the way in which the human mind examines and researches information, not in a step-by-step, linear fashion, but moving in different directions and by jumping backward and forward within the information resource.
Hypertext systems, therefore, are defined by a network of links between key elements (called ‘nodes’) in the information.
An author of a hypertext document sets up a number of alternatives for his readers which define pathways among nodes.
They can decide which pathways to follow and explore them at the time they ‘read’ the document.
Using software tools now available, it is possible to use the same approach for ‘documents’ made up from multimedia ingredients.
In the multimedia counterpart to hypertext, therefore, the key information elements may be text, sound, images or a combination of all three.
Such an environment of interconnected multimedia elements is generally called hypermedia to distinguish it from hypertext.
Often the two terms are used to mean the same thing.
The first major tool to establish hypermedia as a practical environment for multimedia applications was Apple's HyperCard.
As we saw above, it is based on the process of linking together multimedia data elements and allowing users different options for getting from one to another.
It is a particularly powerful approach in knowledge-based systems where a large information resource can be explored in a fast, compelling fashion that suits the natural strategies, intuitions and curiosities of the human mind.
A major issue in hypermedia, however, is the danger of users getting lost among the complex network of multimedia nodes.
Most adults are skilled in using sequential resources such as books.
The conventions of navigating printed information — utilising page numbers, chapter and section structures, contents pages and indexes — are well established.
Within a hypermedia environment, the situation is far less familiar.
as if they are in a complex maze, users of hypermedia can quickly become disorientated.
It is essential therefore that easily remembered navigational devices are built into the environment.
For example, it is helpful to include a continuous on-screen message to tell a user where they are, particularly if it can refer to some form of structural hierarchy in the information.
There should be a system allowing users to backtrack over previous links.
It is also helpful to be able to refer at any time to a ‘map’ of the pathways followed and nodes visited in the course of a particular trip through ‘hyperspace’.
4 MULTIMEDIA APPLICATIONS IN EDUCATION AND TRAINING
The increasingly related and intertwined fields of education and training have for long been a battlefield for learning technologies.
Many past investments have proved fruitless and no doubt there will also be failures in the 1990s among the new technologies.
A constant strand over the last twenty years has, however, been the clear benefits of interactivity.
There is a convincing body of research indicating that people are educated and trained more effectively if they are able to interact with the educational or training medium.
Clearly, this is a gross generalisation.
Interactivity will be more effective in some circumstances than in others and there will always be cases where passive learning is best.
On the whole, however, interactivity is found to be an important ingredient.
This should mean that multimedia is an obvious candidate for both training and educational applications.
It not only offers interactivity but brings a range of different media to bear on the issue of clarifying, communicating and informing.
4.1 Precedents for Multimedia in Training
There are substantial precedents for using interactive media in training.
Computer-based courseware has been in action for over twenty years with considerable effect although the quality of materials has been patchy.
Indeed, in the training industry the acronym CBT meaning ‘computer based training’ is from time to time reinterpreted to mean ‘computer based trouble’.
Broadly speaking, however, computer systems have become more powerful and friendly while good authoring software has appeared to put courseware design into the hands of those who understand it.
These factors have meant substantial growth in recent years in the use of CBT.
European Community estimates suggest that the Community market for CBT hardware and software was worth about £ 325 million in 1988.
This is expected to have doubled by the end of 1990.
CBT was the first precedent.
Interactive video has been the second.
Although, as we have seen, videodisc systems have generally been a commercial disappointment, training has been an constant and effective area of application.
Areas of greatest importance include management and business training, technical training, health and safety, finance, and training in the professions such as medicine.
Has interactive video been effective?
During the past ten years, numerous evaluations have been conducted indicating the power and efficiency of interactive media as a training tool.
Findings include significantly improved training times and excellent retention.
For example, Janet Rothwell's article, ‘Interactive Video for Effective Training’(published in Training Technology, April 1988) describes findings that training times have been improved by between 40 and 60 per cent by the use of interactive video.
Another study cited by Rothwell compared the effectiveness of classroom training with interactive video.
Delayed tests of the interactive video trainees showed that 75 per cent were successful in retaining what they had learned compared with 59 per cent of the classroom trainees.
Margaret Bell of British Telecom's Management College is quoted by the National Interactive Video Centre (March, 1988) pointing out the economy of interactive video.
With some 275,000 employees nationwide, British Telecom faced a massive communications job when they needed to explain the introduction of a new technical system to every manager in the company.
Normally, this would have meant a day off work to attend a training session at a cost of at least £150 per head.
By presenting the information on a videodisc which managers could view quickly, locally and at a convenient time, BT reduced the cost of the exercise to £15 per manager.
However effective interactive video is in training, it is this kind of tenfold cost saving that is its most influential argument.
Cost saving on this scale is particularly important in videodisc applications because videodiscs and their workstations are costly.
Indeed, costs of originating and authoring good interactive video together with the high cost of equipping trainees with laser disc players has been one of the key reasons why the commercial market for such applications has remained relatively small.
Being an available technology and the only one that can deliver high quality interactive video, videodiscs continue to be used in training, especially in dedicated applications commissioned by large corporations who can make sense of the economics.
Other, more modest training applications rely on non-commercial sources of funding.
The true extent of the use is unknown but its high cost has certainly ensured that videodisc can claim only pockets of success and has by no means been widely influential.
However, even if its impact has been modest, interactive video has at least helped to establish the effectiveness of multimedia.
It may therefore prove a useful trailblazer for the digital multimedia systems which will be cheaper, more powerful and more readily available during the 1990s.
4.2 Multimedia and Training: A Scenario for the 1990s
It has been argued that the existing investment in interactive video may act as a barrier to the use of other future multimedia platforms in training.
For example, Roger Wilson argues in his report ‘The Market for Multimedia’(Dragonflair, 1991):
Current (training) applications use videodisc.
Many of the largest users have a significant investment already in software, both generic and bespoke.
Even if the value of that software has been recovered, there will be no willingness to replace it until its natural life has expired.
Although Wilson's point is a good one, there is a considerable momentum building up within the digital multimedia industry and even if reluctance to reinvest in new kit slows the pace of change, it is unlikely to deflect the overwhelming trend.
Also, emphasis on previous users of interactive multimedia fails to address the potential in the 1990s for large numbers of new users.
If and when this body of new users emerges, they will have a wide range of technological choice aside from interactive video.
Why are the 1990s likely to be years of expanding use of interactive multimedia?
After all, despite the large and growing market place for CBT, many commentators argue that it has failed to deliver really effective training at other than procedural levels.
While CBT is good for teaching people how to carry out a practical procedure, the argument runs, it is less useful in helping people to understand why the process is necessary or how it might be improved.
In other words, CBT has not yet proved itself in conceptual training where, perhaps, knowledge-based systems and artificial intelligence may need to be deployed.
Similar criticisms are voiced about interactive video.
While interactive systems have certainly established themselves in the training armoury, there is by no means an overwhelming move towards their increasing use based on their performance to date.
Why, then, may interactive media yet become a major force in training over the next few years?
One part of the answer has nothing to do with technology in itself but in the structural changes that are taking place in European societies.
One aspect of these changes is the strengthening view that existing boundaries between education and training are artificial and that both can better be seen as a single, lifelong process offered in different ways, to different people at different times.
The concept of ‘continuing education’, for long a part of European thought, is now taking firm root, even in the UK.
Together with the urgent need for skills flexibility at all levels in the workforce in the 1990s, the philosophy of continual retraining, the enhancing and refreshing of existing skills and the establishing of new ones, is likely to be a major theme of European training initiatives throughout the 1990s and well into the next century.
We cannot rehearse here the analysis supporting this scenario of great educational and training change but perhaps our own experience in the UK suggests how plausible it is.
Many factors — political, economic, social and demographic — have combined to alter radically British commercial and industrial infrastructures.
There has been steady movement towards service-based business as manufacturing has declined.
Increasing numbers of employees are active in information and information technology related work.
The notion of full employment is now seen as an ideal of the past.
Workers at all levels face a future which may of necessity or choice embrace a number of quite different careers.
Change has become an ever-present aspect of life, dispelling many of the comfortable dogmas of the past, and offering the working population a new diversity and richness of opportunity.
Rapid and continuing change, diversity of opportunity and the skills flexibility they require, have a single powerful message for education and training.
They imply massive demand.
John Whiting of Interactive Learning Services brought this to life in a speech at the Interactive Learning Federation's conference in July 1989.
If the economically active population of Europe is set at about one third of the total (very conservative!), this means about 110 million people.
Assume that every person needs seven days of training out of each 221-day working year.
This means, according to my solar powered calculator, three per cent of each person's employed time or 770 million working days each year engaged in training.
If ten trainees are ‘done’ at a time throughout the year for one week at a time by a single trainer, this trainer can cope with 300 trainees a year.
This leads immediately to an estimate of about 360,000 heavily employed trainers needed in addition to school, college and tertiary education staff.
This simple calculation is of course incorrect…
First, it ignores the infrastructure necessary to support the trainer (let alone the training of trainers themselves) which doubles or triples the resources required.
Second, it ignores the comparison between ‘real need’ as opposed to‘estimated need’for training, allied to on the job experience and training, known in teacher training colleges as the process of ‘sitting with Nellie’.
In other words, the traditional structures available in the UK and Europe are simply unable to meet — let alone pay for — the training needs of the 1990s.
What has this to do with interactive multimedia?
The answer lies in the inadequacy of current training provision to resource these imminent training requirements.
There is an urgent need to revitalise the old concepts of open and distance learning, frameworks for individualised study supported and moderated but not delivered by trainer/tutors.
The Commission of the European Communities is already placing strong emphasis on developing these frameworks through heavily funded transnational development programmes.
Among the programmes focusing on what the Commission calls ‘education, training and youth’ are Comett, promoting cooperation between universities and industry in the field of the new technologies; Lingua, promoting foreign language learning; Eurotecnet, developing vocational training in the new technologies; Iris, developing equal opportunities and vocational training among women; Petra, preparing young people for adult and working life.
Perhaps most important for our present considerations, however, is DELTA (Development of European learning through Technological Advance).
Established in 1985 with an initial funding of about £14 million, DELTA has now commissioned 30 projects for its exploratory phase, most due to report back to the Commission early in 1991.
Unlike many of the other Community programmes, DELTA concentrates exclusively on technology and it does so in the knowledge that it is technology rather than social/educational concerns that will bridge the yawning gap between training needs and training provision in the 1990s and beyond.
So far, there are indications that technology developments can, within five years, increase the capacity of existing trainers tenfold.
In other words, a trainer today managing 50 trainees will, with advance technological support, be able soon to manage 500 trainees without any loss of teaching quality.
Among the key issues DELTA is examining are:
the feasibility of creating usable, affordable authoring systems employing artificial intelligence, vitally important if interactive media are going to be designed with the right combination of instructional and content design;
the building of universal compatibility between different hardware and software environments;
satellite and terrestrial communication for diffusing networked training media;
the creation of the right regulatory, legal and financial conditions to encourage the learning technology market.
The over-riding framework for all this activity is the utilisation of technology-based open and distance learning.
This is the key to creating the vastly increased capacity to train so urgently needed without dramatically expanding the available human resources.
Only through such approaches to training needs can the huge requirement for continuing education and training can be met.
However, DELTA and many other current initiatives, are not seeking merely to enlarge existing open and distance learning systems.
A methodological and technological leap is needed.
The intention, therefore, is to create a new generation of systems using interactive media distributed either directly to trainees on appropriate media or diffused through broad band networks, using satellites, terrestrial transmission or a combination of both.
This then is the environment within which multimedia will contribute to training needs in the 1990s.
Clearly, the issues are less about individual technologies than the requirements of this new order in European education and training.
In terms of the training materials themselves, it seems certain that digital multimedia will play a major role.
Whether it is based on CD-ROM, CD-I or any other specific platform is impossible to determine and, indeed, is no longer the most important issue.
Of course, individual, stand-alone initiatives experimenting with the use of CD-ROM, CD-I, DVI and even CDTV, are emerging, often either funded by industry or national training agencies.
However, if networking concepts in open and distance learning are to succeed, the types of platform may be less significant than the means of diffusion.
Electronic communication — probably telecommunication-based teleconferencing and close derivatives — will be the key to multimedia's effectiveness.
Ultimately, the 1990s will doubtless see a synthesis of different platforms and types of delivery.
Distribution of training material on optical disc for ‘off line’, local use will be easily and fruitfully integrated with networked online training.
For example, to illustrate the point, one such combination might draw together real-time networked tutoring using desktop teleconferencing, windowed within a local CD-ROM application running simultaneously with the tutoring.
There are many other powerful combinations.
The key factor in all these possibilities is the capability of drawing together and distributing a wide range of interactive facilities within a single information environment.
This implies the global application — on whatever platform and however diffused — of digital multimedia.
4.3 Multimedia in Schools
As we have observed, distinctions between education and training are becoming blurred and look increasingly artificial.
In this section, however, we concentrate on education taking place within a particular physical environment.
The school is both a place and a system of education which continues to play a crucial role in preparing young people for adult and working life.
Technology is already playing a significant role in classroom teaching and the general trend towards technology-based learning which we have already described, may well accelerate related developments within schools.
We need to be aware, however, that the school environment is a delicate ecosystem, highly sensitive to political whim.
This means that what is true in one European country may not be even remotely true in another.
For this reason, we will not attempt an international analysis but focus closer to home on the situation in UK schools.
4.4 The Background to Technology in UK Schools
The 1950s and 1960s were periods of experimentation in educational technology.
Attempts to create multimedia resources were really ahead of their technological time.
While the concept of enriching teaching through the use of various and varied media, was clearly sound, there was no technology available to draw the ingredients of multimedia together and unify them with a single, practical teaching resource.
Film loops, slide sets, audio tapes, played a part.
Interactivity was attempted through paper-based, programmed learning texts.
In retrospect, it is not surprising that these efforts largely failed.
Certainly, they have had little lasting impact on teaching practices.
The early 1970s continued to be years of intense experimentation in curricula and methodology.
Innovative new curricula in science, mathematics and the humanities combined with fresh approaches to classroom method.
Mixed-ability, resource-based teaching made its strong, if sometimes controversial mark.
But the middle and late 1970s, with a new Government in power, were years of retrenchment.
The heady climate of experiment evaporated and a ‘back to basics’ approach became the dominant theme.
Ironically, it was within this more sober mood in school education that the British Government initiated a bold new technological experiment.
Under the auspices of the Microelectronics Education Project (MEP), Britain became the first nation worldwide to implement a unified policy to get computers into the classroom and to develop computer-based curriculum materials.
We will not track the development of the MEP in detail but, despite many criticisms of the way it achieved its objectives, it certainly established the widespread use of computers in British schools.
Like most pioneering initiatives, serious mistakes were made.
Among them was the overly rigid, early commitment to ‘approved’ computer hardware.
While this was a huge boost to such manufacturers as Acorn and Research Machines, it meant that British schools still found themselves committed to relatively basic, 8-bit technology at a time when the rest of Europe was establishing a base of more powerful 16-bit MS-DOS technology.
Apart from the limitations of the technology itself, it also meant that UK software was incompatible with European software.
This implied that whatever export or collaborative potential that may have existed between the UK and Europe, was severely weakened by the need for expensive software conversion work.
Coupled with under-funding, this made commercial software development unattractive and many of the major educational publishing companies such as Heinemann, Longman, Oxford University Press, and Macmillan, who had initially invested heavily in software development, eventually retrenched or withdrew from their activities.
What also became clear was that computers were much better used with generic programs than with specific software designed to teach, say, mathematics, history or geography.
Database management, statistical analysis, wordprocessing were far more fruitful used as generic resources than programs teaching curriculum-specific information and ideas.
Used in this way, the computer has become a valuable background resource in British schools rather than an agent of revolutionary change.
The picture at a quantitative level was described recently by Michael Fallon speaking on behalf of the Government and the Department of Education and Science, at the BETT ‘90 (British Education and Training Technology) exhibition in London in January 1991.
According to current DES figures, he said, there were an average of 41 computers per secondary school and 4 per primary school.
This means about 285,000 machines.
It compares with 23 and 2.5 respectively in 1988 (166,000 machines).
He also pointed out that recent research showed pupils now leaving school are more computer literate than adult workers and managers in industry.
Plainly, the figures are impressive but they fall far short of the early hopes that computing might have a pervasive influence on school education.
At secondary school level, for example, 41 computers in a school of perhaps 1,500 pupils suggests that the exposure of individual pupils to computers is still minimal.
4.5 The Background to Multimedia in Schools
Computer-based learning has had an impact, even if it is not the impact that was expected, and may well become an important foundation of the use of multimedia systems later in the 1990s.
At the simplest level, many of the systems require an ‘external’ computer and an existing installed base will be helpful — so long, of course, as the computers can be used with the multimedia systems (see 4.10 below).
In addition, the established place of computers has reduced techno-fear among teachers and has probably made subsequent technology innovation more acceptable to the people who will have to put it to work.
4.6 The Domesday Project
There have already been important multimedia initiatives in British Schools.
As long ago as 1986, the BBC launched their Domesday Project to coincide with the 900th anniversary of the Domesday Book, compiled following the Norman invasion of Britain.
The Domesday software came in the form of two laser videodiscs: the Community Disc and the National Disc.
Together they were intended to provide an interactive, multimedia snapshot of British society 900 years after the Conquest.
The Community Disc covers the geography of Britain and comprises 24,000 Ordnance Survey maps, 500 satellite and 900 aerial photographs and 150,000 screens of text.
The National disc covers cultural, social, environment and economic aspects of Britain.
It contains a huge amount of such data including amongst many other surveys and statistics, the 1981 National Census, 500 maps, 22,000 photographs, 1,500 items of text in the form of essays and published articles and, on side two of the disc, about an hour of video recording some of the events of 1986.
Domesday was an attractive but deeply flawed concept.
Built around a supremely non-standard hardware configuration, costly to acquire and complex to use, it was not even supported by a steady subsequent flow of new, compatible software so the investment in the hardware could be amortised across a range of classroom applications.
A few further discs did eventually appear, the Ecodisc and the Volcanos disc among them, but they were not enough to revitalise the fortunes of the Domesday initiative.
Educationally disappointing and commercially disastrous, Domesday sold probably sold little more than a 1,000 units against a forecast of many times this figure.
4.7 Interactive Video in Schools (IVIS)
At about the same time that Domesday was launched, the Department of Trade and Industry funded a £2 million interactive video project called Interactive Video in Schools (IVIS).
The project generated eight laser videodisc-based courses and involved 96 workstations being installed for evaluation purposes in classrooms throughout the UK.
The pilot stage was concluded early in 1988 and evaluations carried out by the Centre of Applied Research in education (CARE) at the University of Essex, indicated, in general terms, that the project justified further development in interactive video resources.
Accordingly, the DTI subsequently extended its funding, offering a further 353 interactive video workstations to schools and Initial Teacher Training Establishments.
4.8 Interactive Video in Industry and Further Education (IVIFE)
Although not, of course, a school-based programme, IVIFE follows logically from IVIS and therefore needs to be mentioned briefly.
The programme began in September, 1987 with a three year, £1.9 million funding to explore the use of interactive video as a further and higher education training medium.
Although related to IVIS, the approach is quite different.
While the IVIS discs had only loosely defined objectives and were rather vaguely intended as an underlying educational resource, IVIFE discs had very clear objectives.
They were intended, primarily, to provide interactive training to a level equivalent to BTEC and City and Guilds, acceptable to the Council for Vocational Qualifications and marketable subsequently as commercial, generic courseware.
Today, IVIFE is an ongoing project which even has its own newsletter, ‘IVIFE News’.
4.9 The Videodisc Emphasis
The specific projects mentioned above are by no means the only multimedia initiatives worthy of note.
More recently, for example, the DES provided funding for the National Curriculum Council (NCC) to procure five interactive discs supporting mathematics teaching in both primary and secondary schools.
Another project, announced in 1991, also funded by the DES, is for two discs, one on careers education, the other on careers guidance.
This project is being administered by the National Council for Educational Technology.
Industrial funding has also played its part.
The British Nuclear Forum has funded research at Newcastle University called the Interactive Learning Project.
The output has been three double-sided video discs on energy, radiation and risk aimed to cover appropriate attainment levels decreed by the National Curriculum.
The best news for schools is that a special price is available making it possible for schools to acquire the discs, a laser vision player and an Acorn Archimedes computer for £899.
This must make it the best value in multimedia ever offered to schools.
A striking feature of all these initiatives is their use of analogue technology.
Indeed, strictly speaking, so far we have not mentioned a single school development that falls within the scope of this study's working definition of multimedia.
The reason is straightforward.
Interactive video is a medium that is available right now.
The new digital media are still evolving.
The current climate of Government attitude to education is pragmatic and based on getting value for money from existing technologies.
Increasingly, Government funds are targeted for highly specific purposes.
In other words, if there is an available technological answer to an educational problem, then that technological answer will receive funding.
For example, it is no coincidence that interactive video is currently getting funding in subject areas such as mathematics where there is an acute teacher shortage.
In reality, what is being funded is not interactive video but a solution to an urgent educational problem.
The danger, of course, is that this ‘here and now’ policy may lead to short term benefits at the cost of longer term lost opportunities.
However, there is no real danger yet that the UK school system is becoming committed to analogue or any other kind of multimedia.
These are still very early days and the options are wide open.
So far, for example, interactive video has hardly received enough evaluation to determine what its effect on education actually is.
Neither are there significant numbers of workstations in active use in schools.
Although official figures are not available, Leslie Mapp of the NCET believes that of about 2,000 interactive video workstations in British schools, less than 30 per cent are actually in current use.
Even development initiatives such as the NCET-administered mathematics project is not yet committed to any particular delivery platform.
Although slated as a videodisc initiative, the early stages are keeping options open as far as possible and the project may ultimately be implemented using CD-ROMs or even CD-I discs.
Obviously, there are limits to how long options can be kept open and fairly soon a commitment will have to be made.
However, the strategy emphasises the pragmatic, uncommitted nature of current educational multimedia development, at least at the establishment level.
This pragmatism is under pressure from enthusiasts and commercial interests who want such development to very quickly go digital and, if possible, support one or other of the specific digital multimedia platforms.
4.10 Digital Multimedia in Schools
There is already some movement to strengthen the digital option for schools.
One might suppose it was already quite strong even if nothing else was done, especially so far as CD-ROM is concerned.
After all, nearly 300,000 computers in schools suggests a ready base of machines to which low-cost CD-ROM drives could be connected.
Unfortunately, the picture is misleading.
A DES survey in July 1989 indicated that less than three per cent of school computers were then capable of running a CD-ROM drive.
If the installed base cannot even support a CD-ROM drive, the gap between the current use of technology and an influential installed base of machines with the sophisticated facilities needed for digital multimedia is huge.
This suggests that if the Government wanted to equip all schools with a CD-ROM workstation, it would need to spend over £300 million — an unlikely prospect at anytime but more especially in the ‘value for money’ 1990s.
CD-ROM initiatives are emerging, however, even if they fall far short of multimedia.
In particular, early in 1991, the DES announced a project to inject £500,000 into putting CD-ROMs into schools.
The NCET is administering the project and although no guidelines have yet been finalised it looks as if schools will be able to buy approved hardware at supported prices.
The level of funding suggests that between 700 and 1,000 schools may acquire CD-ROM systems as a result of the project.
Simultaneous with the DES announcement came a joint statement from Toshiba and Times Network who are making an educational package of a CD-ROM drive plus four discs available to schools at £795.
Presumably Toshiba and Times Network are counting on the Government funding to assist their initiative otherwise their market will be restricted to those 3 per cent of schools with computers capable of supporting a CD-ROM drive.
These initiatives are really no more than very small, first steps towards finding out whether CD-ROM has anything worthwhile to offer schools.
Even if these developments start a serious move towards using CD-ROM, it may be several years before multimedia CD-ROM can gain any kind of foothold.
The extra cost of upgrading computers alone would be a daunting prospect for already over-stretched education budgets.
This suggests that CD-ROM may be a prohibitively expensive path towards digital multimedia in schools.
So, if multimedia is going to get established in other than the very long term, it is more likely to be delivered by a low cost stand-alone system such as CDTV, CD-I or even DVI, if a low cost dedicated player can be devised.
Currently, however, there is little evidence of detailed or widescale evaluations of these systems.
Their manufacturers are of course trying to place equipment in schools for the purpose of trials but these measures still fall short of the impact need to get serious attention from educational decision makers.
Realistically, there may be little prospect of making better progress than this so long as the systems themselves remain largely no more than promises rather than commercially available products.
Perhaps it is inevitable that interactive video will remain a favoured medium, at least until the digital options actually become easily, widely and affordably available.
What then are the real prospects for multimedia having a serious impact on school education?
Clearly, for the moment and for some years to come, the prospects are slight.
Before such an impact can really be felt, two existing trends have to mature and eventually meet.
First, multi-media technology must become cheaper, more portable (to enable it to be moved around within schools), easier-to-use, and reliable.
Platforms such as CD-I and CDTV are a step in this direction.
Second, education must move towards studies based more widely on coursework.
The GCSE has already begun this trend but it must strengthen to create the real hunger for resources and, perhaps, a changed role for the teacher which will jointly encourage the use of multimedia resources.
When these two trends mature and meet, digital multimedia may at last become a real force in our classrooms.
This convergence, however, is some years away and meantime direct influence will come only from modest initiatives with little longterm significance.
Indirectly, however, influence will be felt from the home.
As major platforms such as CD-I and CDTV find success in consumer markets, the widespread use of the technologies for home education, self-improvement and popular reference may well have a push-through effect into the schools.
At the very least, their existence in the home may create a climate of familiarity and acceptance in schools and this may influence the shape of educational commitments to multimedia when they eventually come.
4.11 Education and Training in a Changing World
From all that we have said about both training and education, we can see a genuine and urgent need for the kind of flexible, variegated, self-paced learning environment that multimedia can offer.
As we noted earlier, rapid and profound change is now an integral part of the world in which we live.
This means that one of the most difficult challenges facing an educational system is preparing its students for a world that has not yet even come into existence.
In his introduction to the book ‘Interactive Multimedia’(edited by Sueann Ambron and Kristina Hooper, published by Microsoft Press, 1988), Apple Computer's president, John Sculley wrote:
Thinking of education as simply the transfer of knowledge from teacher to student, pouring from one vessel into another, is no longer possible.
It is not as if we can give young people a ration of knowledge that they can draw on throughout their careers.
We don't even know what their careers might be.
Students today cannot count on finding one, smooth career path because, by the millions, jobs that exist today will change radically in the near future.
To succeed, individuals will need tremendous flexibility to be able to move from one company to another or from one industry to another.
What tomorrow's students need is not just a mastery of subject matter, but mastery of learning.
Education cannot simply be a prelude to a career; it must be a lifelong endeavour.
5.
BUSINESS AND PROFESSIONAL APPLICATIONS OF MULTIMEDIA
Specific applications of multimedia in business and professional environments are to some extent determined by commercial ingenuity.
We have already seen how shrewd information publishers have devised new means of delivering products and services by creating novel links between different electronic media.
A good example right now is the increasing numbers of fax-based publishing operations which are linked with audiotex services, bringing together voice and printed information over the same telephone line.
To attempt to foresee the many varied business ideas that future entrepreneurs will devise would be a fruitless task.
Instead, we will focus only on broad areas of application which can serve as exemplars of the kinds of impact multi-media has in store.
In particular, we will examine multimedia databases, multimedia in online systems, the related area of videoconferencing and interactive, multimedia marketing.
5.1 Multimedia Databases
We need first to make clear that we are focusing here on searchable, electronic databases, information structures which offer a potentially networked service to users.
We are not immediately concerned whether they are based on off-line, optical discs or on online technology by way of broadband networks.
To some extent, all that we have so far said about multimedia development is really a description of multimedia databases.
A single CD-I disc, for example, could be called a multimedia database.
In this brief section, however, we are concerned more with the way in which multimedia elements can be integrated into an orthodox text-and-data database environment and how multimedia can be described in the same logical terms as the conventional elements with existing database structures.
We can then go on to see how these integrated structures can offer the networked services which will be the key to many multimedia business and professional applications.
A lead article in ‘EP Journal’(November/December 1990) summarised this as follows:
As compression techniques improve and storage becomes greater, we can expect to see more and more disc based products which combine, in consumer entertainment, education, training and even in business communications, digital sound, data, graphics and moving pictures to create genuinely multimedia information products.
The availability of CD-I, CDTV and DVI in 1991 makes it clear that this fundamental change has come to stay.
But these optical storage products are essentially just that — products rather than interactive, multimedia services.
So far, the other great convergence of the 1980s — the ability to interconnect computers in order to move information through data networks from one site to another — has affected information publishing fundamentally, but only insofar as‘hard’ knowledge is concerned.
The ability of the modem to translate signals makes the information services market we have today a possibility…information publishers must now prepare for a new challenge and for a similar degree of networking capacity for video and other multimedia as they currently experience for data…’
The concept of multimedia databases is not new, of course.
As long ago as the late 1970s, the US Library of Congress was implementing a pilot plan to archive many thousands of historic documents within a networked database using analogue videodiscs to store the document images.
Of course, this kind of archive is not a traditional electronic database structure and only allows the retrieval of whole documents and cannot, for example , permit electronic searches based on their contents.
However, the project represents a classic archiving application.
At the time, the only technology available to store large numbers of high quality images was the analogue videodisc.
As we have seen by all that was covered in Chapter 3, the technology of multimedia suggests a multimedia environment geared by definition to handling such images and a lot more besides.
How, then, will digital multimedia influence archiving and other database applications?
A helpful first step is to remind ourselves of current databasing options.
Text only Such databases have established structural and indexing conventions.
Numerous proprietary retrieval software packages are available incorporating classic full text retrieval features such as word adjacency and the use of Boolean search operators.
Image only A typical database might be a scanned collection of 35 mm slides, perhaps reproducing the contents of a major art gallery.
It is not, of course, possible to apply full text retrieval to such a database and an effective option would be to use a relational database management system with keyword-based searching.
Text and images handled separately A combination of the above but not a particularly helpful treatment.
Text and images linked Currently a common method of combining text and images in a range of database types, including CD-ROM.
The basis of the linking is to insert pointers within searchable text fields which effect the image retrieval once the text field retrieval has taken place.
This last approach using linkages between text fields in the database records looks like being the most promising model for multimedia database design.
However, in a true multimedia database the non-text elements may include can photographs, diagrams, motion video, spreadsheets, program code, and digitised sound.
In fact, a multimedia database may be called upon to store anything you can store on a computer or use in an information application.
This implies the need to store potentially very large, unstructured data objects as just another field in a database record.
The database can then offer any combination of fields whether they are data, images, motion video, text or sound objects.
A key difference to conventional relational databases lies in the sheer size of data objects which multimedia databases must handle.
Multimedia objects can typically consist of hundreds of megabytes.
In the jargon of multimedia databases therefore, a new category of data types has been created called Binary Large Objects (BLOBs).
Since BLOBs can be very large, current multimedia database wisdom suggests that each BLOB is best located on its own separate partition of a disc or on its own magnetic or optical disc.
Developers describe the logical region of the database containing BLOBs as ‘blobspace’.
Although blobspace can be located on any device or medium or across several, the field in a database record corresponding to a particular BLOB will always point to its location when interrogated by the database user.
The actual location of the BLOB does not affect an application.
Being able to store BLOBs separately has two advantages.
First, it ensures that you can still utilise the database without retrieving BLOBs and suffer no loss of speed in your application because of the presence of BLOBs within records.
Second, it provides a choice of storage media for BLOBs which may have advantages both of cost and convenience.
This gives at least some indication how multimedia databases can be constructed.
However, we have barely touched on some of the practical issues.
Not least is a problem well-known in current image databases: the difficulty of ensuring all data elements are held in a consistent and suitable data format.
The problem is compounded when the range and number of data types is substantial, if only because the data may be acquired from a very wide variety of sources.
Converting a large number of data elements currently captured in a host of different formats to a single, standard digital format could involve substantial time, cost and management skill.
Networking such databases on a local area basis presents an even more serious problem.
Extremely powerful local area networks will be needed to handle the kinds of data volumes that could be involved at any given moment.
For example, 50 users working simultaneously with, say, 20 different blocks of multimedia data each of 100 megabytes implies 100 gigabytes of data in the network!
Today's local area network systems are totally inadequate for such gigantic flows of information.
In terms of applications, multimedia databases offer obvious advantages to any organisations whose operations are based on documents, drawings and images.
Insurers, financial institutions, regulatory agencies, libraries, museums and dealers in mechanical parts in a range of spheres.
They may even be able to revolutionise some kinds of businesses by providing new kinds of selling tools.
Travel agents could sell holidays by providing prospective customers with a full multimedia account of their travel options.
Estate agents could market property in a similar manner.
So far, although applications have been piloted using particular delivery media, such as CD-ROM, CD-I and DVI, these have generally been databases confined to the individual media.
Development of the first true multimedia, relational databases where multimedia is described as just another set of fields is still in its infancy.
5.2 Multimedia in Online Systems
The concept of a multimedia database could obviously revolutionise the character of online information products.
While building such a database, using the kinds of structural concepts described above, is technical challenging and certainly a costly business, the greatest difficulties come when such a database is delivered online.
The difficulties are twofold.
First, there is the problem of transmitting the huge volumes of data and managing it on its way through a network to users.
Second, there is the need for users to have the right kind of workstation in order to enjoy the benefits of the multimedia elements.
We can consider these problems and get a sense of how online multimedia might work in practice by looking at one of the first projects to attempt to offer multimedia online database services.
In the Boston metropolitan area of the United States, a major initiative called the Media Broadband Service (MBS), is being piloted by a regional telephone company, New England Telephone, and Nynex Science and Technology, an off-shoot of the New York based Regional Bell Operating Company.
Four Boston area hospitals and a newspaper publishing company are also involved in the project.
MBS is exploiting the fibre optic network that has been installed between all telephone exchanges in the New England area.
It is the availability of the fibre optic network that helps MBS address the problem of setting up the interactive links between the service itself and its users.
Such a network has vastly greater band width than conventional cable networks and enables information such as ordinary telephone calls to coexist with a flow of large amounts of data, video and audio traffic.
Sophisticated network management software is needed to control the information flows and MBS has had its own system designed from scratch for the needs of its particular applications.
Once established, advanced switching technology within the network will allow users of MBS to tie their own local area networks using the service into a wide-area network.
Workstations accessing the service are Unix-based but can be chosen from a number of manufacturers including DEC, Macintosh or Sun.
High resolution monitors are needed for the images, of course, and the current pilots are using 2000 x 2000 pixel Sony off-the-shelf monitors.
When they announced the pilot scheme in September 1990, Nynex claimed that the service ‘will eventually allow businesses to send, receive, store and retrieve any type of voice, data and video communication in any combination at any time.
Lower band width ISDN services will be rolled out for home use.’
This suggests, of course, that the data could be real time information and that MBS could therefore be used as a videoconferencing facility.
Indeed, there is significant overlap in these online applications with videoconference and it is artificial to try to separate them completely.
We look in more detail at videoconferencing below and concentrate here mainly on the database aspects of the MBS initiative.
How is MBS being used from the outset?
The Children's Hospital of Boston is using the service to develop three applications.
A report application will allow the hospital staff to create multimedia reports for patients, composed of images, voice and text that can be displayed, manipulated and analysed.
Doctors will be able to access the information immediately or later by means of an electronic mailbox.
A review rounds application permits doctors and nurses, either in the hospital or at remote locations, to access the image-based records created in the report application at a convenient time.
The idea is to help doctors and nurses who may otherwise have difficulty in conferring because of geography or timing, to review and analyse non-emergency patient cases.
The third is a consultation application allowing two or more doctors to confer by telephone while simultaneously viewing diagnostic images and associated voice and textual reports.
Nearby, the New England Medical Centre in Boston is also developing a MBS application.
In this case, they are creating an online imaging system for remote conferencing and analysis in the delicate surgical field of cardiac catheterisation.
The catheter inserted into the heart is filmed using moving X-ray photography and the images — typically five to ten second loops of motion — are transmitted in bursts over the fibre optic network.
This kind of broad band system is a model for those that will certainly be used when existing conventional online databases are enhanced with multimedia elements.
These enhancements may most obviously be a matter of adding value to commercial offerings through images and sound.
However, multimedia may also be utilised to make online systems more friendly.
In the past, one of the most serious barriers to the growth of online services has been their unfriendly character.
Once multimedia delivery is possible, major opportunities become available for offering immediate, accessible guidance on the use of the system.
The most obvious possibility is to be able to call up a video window on the screen which can deliver a helpline operator's face and words and enable you to resolve your problem.
Such a help system could be based on a pre-stored video segment selected under a system control sensitive to your problem or the nature of the help you request.
It could also, of course, be a real time connection with an operator who can discuss your difficulty with you and resolve it.
In this latter case, we are considering a form of videoconferencing and there seems little doubt that desktop, dial-up videoconferencing is going to become a major area of growth both as an application in itself and as a component of other multimedia applications.
5.3 Videoconferencing
Videoconferencing is a two-way, real time communications facility which enables participants to see and speak to each other.
It is therefore a kind of online multimedia application.
The idea of videoconferencing is to draw together people in widely separated locations for ‘meetings’ without having to spend time and money travelling large distances.
It is exclusively a business concept although simpler versions of videoconferencing may one day enter the home as the first videophones.
Because recent licensing arrangements in Europe now permit licensees to operate two-way, interactive services using satellites, broadcasting via satellite is becoming an increasingly important option for videoconferencing.
Other satellite services have for some time been offering a kind of one-sided videoconferencing facility.
For example, the European network Eurostep transmits training programmes by satellite but students can interact with what they see by asking questions or raising points for discussion using a voice telephone link to the studio from which the programme is being transmitted.
This approach is widely used in the United States in educational applications.
For the purposes of this study, we will be focusing more on videoconferencing using digital links such as digital telephone lines because it is here we are likely to see the most rapid developments.
The UK market for videoconferencing in 1990 was about £10 million, compared to £6 million the previous year.
In 1991 it is expected to have grow to £17 million and to double in 1992.
Traditionally, however, videoconferencing has been associated with huge multinationals, permanent videoconferencing suites equipped like television studios and expensive dedicated broadband or satellite links.
The cost of using such established systems is high, typically about £1,500 per hour to hire.
The advent of low-cost multimedia technology looks set to dramatically change this.
It now seems certain that cheap, convenient videoconferencing is at last going to reach a mass market of computer users.
A key factor in current videoconferencing developments is the compression of data so that complex multimedia information can be transmitted in reduced form and decompressed on arrival at its destination.
Typical videoconferencing systems utilise special telecommunication lines and satellite links.
Crucial to the effectiveness of such systems, in particular the quality of the video, is the band width of the transmission channel.
Ordinary analogue telephone lines used for voice communication have a low band width and cannot transmit much data per second.
The new digital telephone lines now becoming common have higher band width while special data links have still bigger band widths allowing much larger amounts of data to be transmitted per second.
Broadcast data via satellites also have a wide band width enabling good data transmission.
Use of telephone lines of any kind for videoconferencing requires the multimedia equivalent of a modem, known as a codec (compression-decompression unit).
While a modem acts as a two-way interface for text and numeric data between a computer and a telephone line, a codec does the same job with multimedia information and especially motion video.
Different codecs have been designed each with their own performance characteristics.
Amongst the most important of these is the efficiency with which they compress video.
For example, the US videoconferencing systems specialists, PictureTel, who claim to hold 80 per cent of the US videoconferencing equipment market, has a codec called the C3000 which enables video transmission rates of data of 128 kilobits per second (kbs) while maintaining adequate picture quality.
This compares with data rates of at least 384 kbs needed for the conventional videoconferencing systems using special leased line and satellite facilities.
Very high levels of compression like this make it possible to transmit image data over ordinary dial-up, digital telephone lines rather than expensive, special broadband lines.
For example, a special leased line from British Telecom will cost about £100,000 per year.
A dial-up call from London to Aberdeen costs about 30p a minute.
In effect, the videoconferencer will be dialling up a video call in much the same way as an ordinary telephone call.
A British company, Data Control and Equipment (DCE), have just released their V3100 dial-up videoconferencing system using the PictureTel codec.
They claim that the dial up approach cuts the cost of two-way motion video by up to 90 per cent.
Using DCE's equipment, a ‘meeting’ with colleagues in the United States would cost about £2.80 per minute, about £1.60 per minute in France and in the UK the normal cost of a digital telephone line for the time the users are connected.
These kinds of videoconference systems permit conferencing only between stations using the same equipment.
Until recently, there have been no established codec standards and one system can only speak to another if it is using the same compression-decompression algorithms.
This now looks likely to change with the emergence of the European Commission's CCITT px64 videoconferencing standard (see 3.13).
Many codecs already developed are designed to be upgradeable when such standards become widely accepted and it is therefore likely that within a few years dial-up videoconferencing will be possible between a wide range of different proprietary systems.
The key to rapid future development will be in the growing sophistication of codec capabilities.
This will mean that ever better quality video and larger volumes of other data elements will be available via ordinary telecommunication lines.
This will radically alter the use of videoconferencing.
While teleconferencing has for long been the province of very large organisations, codec development together with the digitising of telephone networks and the emergence of ISDNs, will bring teleconferencing within reach of a mass market.
Indeed, this is the development that begins to combine videoconferencing facilities with online computer network applications, the kind of overlap we foreshadowed in the previous sections of this chapter.
In practice, we will soon see desktop videoconferencing becoming a standard feature of computer communications.
While network users currently can communicate online with colleagues using text inputs to their computers, videoconferencing facilities will enable users to see each other in a video window in their screens.
Not only will they see each other and converse in real time but they will be able to share and manipulate together the contents of their computer screens.
At a simple level, a London-based business executive working on corporate budgets could ask a colleague, perhaps in New York, to examine his spreadsheet analysis and, if necessary, add to it while seeing and talking to each other.
The video window would also enable them to study relevant documents together, if required.
Full multimedia interactivity, networked through cheap, standardised communications systems is only a few years away.
It draws together all the elements we have so far discussed — multimedia databases, online and videoconferencing — combining them into the most complete form of online communication ever devised.
5.4 Interactive Multimedia Marketing
Using interactive, multimedia for marketing purposes refers largely to interactive point-of-sale (POS) and point-of-information (POI) systems.
These days, of course, huge numbers of interactive terminals are also available for simple transactions such as automated bank telling, but these are rarely multimedia applications and we will not cover them here.
The earliest Interactive Multimedia Terminals (IMTs) were developed using videodisc technology and, even today, videodisc continues to dominate all current applications.
The varied audiovisual possibilities of videodisc together with fast, random access of information and ready availability of the necessary hardware, makes it an ideal means of offering a fast, friendly information resource for such people as customers in shops, visitors to museums, travellers at airports, and tourists in foreign cities.
To maximise the friendliness of the systems, bespoke kiosks are usually designed offering large screen colour displays and touch screen or highly simplified keypad interfaces.
More recently, CD technology has begun to displace videodisc systems but the transition is only in its earliest stages.
There are two broad types of IMT: purely information-based and information with transaction facilities.
Transaction facilities are built into kiosks either by providing online or other forms of communication facilities, sometimes nothing more sophisticated than a voice telephone link, or by allowing on-site, consolidated storage of customer requests which can be manually collected at a later time.
Typical transactions are the purchase of goods, reservations of travel or hotel arrangements, and requests for further information — perhaps brochures and catalogues — about goods or services.
An industry study called ‘The Interactive Marketing Report 1990’, published by ItMk of Atlanta, Georgia, gives an interesting analysis of the use of IMTs.
According to the study, by the end of 1989, there were about 12,000 IMTs in use worldwide, mainly in the USA and Europe, and projected growth rates suggested that their numbers will at least double every two years through the 1990s.
Not all are fully multimedia, although most offer at least some of the main ingredients.
At present, the majority (71 per cent ) are in retail locations.
Over the last few years, there has been a strong trend towards transaction-based terminals.
In 1984, for example, 80 per cent of all IMTs were information-only systems and 20 per cent , transaction systems.
By 1989, the transaction systems had grown to about fifty per cent of the total.
Not surprisingly, product sales account for most of the transaction applications, with sales of greeting cards, shoes, financial services, car accessories, furnishings and clothes being the among the most popular.
The important distinguishing feature between applications in these different product categories is the value that a particular system can offer, both to customers and retailers.
For example, Hallmark's use of IMTs for its card sales in the United States incorporates a customising facility which allows purchasers to print their own greetings and insert them in the cards of their choice.
This has obvious benefits for the customer.
They have a much wider range of cards to choose from and they can also compose their own personalised message.
From the retailers point of view, there are benefits in reduced stockholding and floor space.
The most common applications for information-only systems is product information, particularly and most obviously, in locations where the information reinforces in-store purchasing decisions.
Attractively presented product information is also a good public relations vehicle, enhancing the image of the store in a general sense.
Among the most popular uses of such POI systems is the support of sales of cars, furnishings, jewellery and leisure electronics goods.
A recent trial in the United States is also showing the value of IMTs for selling music videos.
In the Musicland chain of stores, customers can preview clips of music videos on a videodisc database.
Initial evidence suggests that 75 per cent of customers purchase a previewed video.
Exhibit applications are popular and include multimedia information presentations in such locations as museums, art galleries, theme parks, trade shows and major trade fairs such as the various ‘EXPOs’.
The best interactive applications allow visitors to enjoy a sense of participation in the exhibits they have come to see.
A typical museum application is the UK's National Gallery initiative, utilising 16 interactive information terminals to give visitors a glimpse of the gallery's treasures and a simulated tour.
The workstations are based on Apple Macintosh computers which combine images of some of the principal paintings together with a hypertext database of biographical notes, glossaries and a catalogue of the works on display.
More spectacular concepts include the Spanish Government's plan to create a multi-million pound interactive exhibit in the Spanish pavilion of the 1992 EXPO which they are hosting.
The exhibit will celebrate major aspects of Spanish life, history and culture using state-of-the-art interactive technology, much of it probably based on CD-I.
Currently, 70 per cent of all IMTs are located in the United States with most of the balance in Europe.
The UK ranks only fifth in the world for installations with a total of 342 terminals in operation at the end of 1989.
This is just 3 per cent of the world installed base and only 11 per cent of the installed base in Europe.
Will videodisc technology continue to dominate in these applications?
Clearly, this must be the case so long as it remains the only easily available platform for interactive multimedia.
Organisations funding and developing current applications are less interested in debates about the underlying technologies than in the three commercial imperatives: functionality, availability and cost.
What factors might influence a trend towards one or more of the digital platforms?
Quite simply, the same three that are currently determining the use of videodiscs.
The new platforms need to show enhanced functionality, lower software and hardware costs, and, above all, easy availability.
Right now, for example, all the digital media we have considered fail to compete with videodisc in an area of functionality general considered highly important in many areas of the IMT sector: the delivery of high quality, full screen, full motion video.
This is just one key area in which CD-ROM, CD-I, DVI, CDTV and the rest need to make significant progress before they can displace their established, distinguished and unashamedly analogue predecessor.
One possibility for the near-future which has hardly been considered by developers so far, is a marriage of an old-fashioned storage such as the magnetic hard disc with a newcomer such as DVI.
Protagonists in the optical disc industry may have overlooked the fact that while they have been arguing the merits of their various platforms, hard disc storage has been growing ever larger.
Today, capacities in excess of 100 megabytes are commonplace.
Until recently, it appeared that 300 megabytes was probably the limit of the technology but now magnetic media enthusiasts are talking about gigabytes (thousands of megabytes) soon becoming commercially available.
In the United States, early in 1991, for example, CD-ROM publishers SilverPlatter demonstrated a huge database system called ‘SilverPlatter Magnetic’.
Their system incorporates the contents of several CD-ROMs onto a single, gigantic hard disc of about 2 gigabytes.
This can then be searched over a Local Area Network by up to 25 users simultaneously, a speed performance ten times better than the best CD-ROM can normally offer.
In a multimedia application using an ultra-large hard disc, DVI could be used to compress and decompress video and images.
The high speed of magnetic compared to optical media would help to disguise some of DVI's current limitations and would also facilitate a smooth, seamless multimedia presentation.
Moreover, unlike most current CDs, a hard disc is a read/write medium and could be easily updated.
For example, a network of hard disc multimedia kiosks each delivering highly current product information could be refreshed daily by point-to-multipoint broadcasts of data from a satellite.
In this way, magnetic media could offer a digital multimedia solution which is functional, timely, cost-effective and available ‘off-the-shelf’ today.
6 MULTIMEDIA APPLICATIONS: ENTERTAINMENT AND LEISURE
The consumer market excites boundless excitement in the multimedia industry and so far has merely been the source of disappointments.
The reason for the excitement is quite simple.
The consumer market is potentially very large.
For example, television has probably been the biggest consumer electronics success of all time.
Investigations by Nielsen Media Research, for example, show a remarkable level of penetration by television in the United States.
They found that in 1988, over 98 per cent of all US homes had at least one television.
About fifty per cent had two or more.
Worldwide, there are probably over 300 million televisions in use.
Alongside the sale of the television receivers themselves, a massive broadcast and cable industry has emerged.
John Gale's ‘Micro Multimedia’ report indicates that in 1988, in the United States alone, broadcast television networks grossed US$ 8 billion and cable networks, US$ 16 billion.
By any standards this is a large industry.
Much of the momentum behind consumer multimedia, therefore, is fuelled by the quest for even a small slice of a very large cake.
Results so far have been mixed.
In the late 1970s and early 1980s, the home computer boom looked like a major consumer success in the making.
Many software and hardware companies enjoyed rapid expansion before the bubble burst and market growth slowed.
After several years of shake-outs and restructuring, the home computer market remains substantial — games software sales in Europe last year were worth about £200 million — but there is little sign that it is going to scoop the true potential of the consumer market.
The reason is that the games industry relies on selling computers to a market that is more comfortable with appliances.
6.1 Multimedia Appliances
The real promise of multimedia success in consumer markets dates back to 1982 and the introduction by Philips of compact disc audio.
We have already traced the development of CD information media from their origin in CD Audio but it is important to re-emphasise that much of the prospects for multimedia's future success lies in the established and fast-growing base of CD Audio players.
The main contenders for CD multimedia so far are based on an initial strategy of offering adding value to a base of actual or potential CD-A users.
CD-I players for example will be positioned as CD-A players with the in-built added value of interactive multimedia.
Their pricing will initially reflect a premium over and above the normal price of a CD-A player but it is likely that this will progressively be reduced.
Within a few years, the distinction between CD-I and CD-A may become blurred to such an extent that purchasers will simply shop for CD players with the tacit understanding that they all offer both audio and multimedia facilities.
Commodore's CDTV also benefits from an association with CD-A but approaches it from the other direction.
By contrast with CD-I, CDTV's audio capabilities are more limited in terms of oversampling and, therefore, quality.
This means that CDTV primarily offers multimedia with CD-A as an added value dimension.
In other words, to succeed, Commodore must sell multimedia strongly while CD-I can hope to gain a large measure of success merely through ‘piggy-backing’ on the momentum of CD-A sales.
A key point about these inter-relationships is that CD-I, CDTV and any other similar platforms, are positioned firmly in a consumer electronics environment.
Although they rely upon sophisticated, on-board computer technology, they are emphatically designed to be ‘non-computers’ as far as potential customers are concerned.
Philips, Sony, Matsushita, Commodore have all learned the lesson of the home computer industry.
Devices that are explicitly computers appeal to a niche market.
To strike the mainstream, products must be appliances — simple to install, easy to use and physically appropriate to the domestic living room environment.
On a scale of user friendliness and acceptability, major success in consumer multimedia must lie somewhere between the electric kettle and the television set.
6.2 Consumer Applications
Major areas of application of consumer multimedia include music, games, home education and self-improvement, popular reference, ‘how to’ applications such as DIY, car maintenance etc, and child-minding software designed to keep children quiet and entertained while adults get on with their own lives for short but blissful periods of time.
Both CD-I and CDTV have been firmly positioned as family entertainment and leisure resources.
Neither are intended to be dominated by games.
CDTV will probably have more problems than CD-I in this regard, being based on the Amiga computer.
The Amiga is already supported by a large body of games software which can be easily transferred to CD-ROM for sale to CDTV owners.
Commodore is putting vigorous efforts into ensuring a base of non-games software to compensate for the potential flood of what they call ‘dumpware’— Amiga games dumped onto disc for a quick, easy sale in the CDTV market.
At their official launch of CDTV in January, 1991, Commodore announced 46 titles either in development or ready for launch.
Of these, only 17 were games.
The following is a sample from their announcement.
Home Reference
King James Bible
Guinness Disc of Records
The American Heritage Encyclopaedia
Dr Wellman — Family Medical Advisor
Illustrated Works of Shakespeare
Advanced Military Systems
Children's Classics
Cinderella
Peter Rabbit
Scary Poems for Rotten Kids
Children's Fun
Animated Colouring Book
Snoopy
Leisure
Gardenfax
Silver Palate Cookbook
Music Maker
Education
Fun School (Three discs for different age ranges)
Barney Bear Goes to School
North Polar Expedition
Hallo Freunde!
(German for beginners)
Thinking Games
Sherlock Holmes
Battle Chess
Many Roads to Murder
Defender of the Crown
Classic Board Games
In many of these cases, Commodore is funding either most or all of the development and is acting as the software publisher.
This is a policy which surprised some software specialists who originally felt that Commodore would avoid becoming software publishers themselves in order not to compete with, and thereby discourage, third party publishers.
Commodore argues that its commitment to support a third party CDTV publishing industry is as strong as ever.
Without such third party activity, CDTV cannot hope to have long term success.
In the early stages, however, Commodore believes that it needs to ensure a broad range of titles are available both to support the positioning of CDTV as a family leisure platform and to generate the initial installed base which will itself encourage the third party publishing on which the future of CDTV depends.
The course of CD-I software development is less clear.
The long series of delays to CD-I's launch has undoubtedly sapped the confidence of potential publishing investors.
This has been all the more damaging because the costs of CD-I development can be very high indeed.
In other words, publishing commitments have to be substantial and it does not take much to sap confidence when the risk is so high.
Although there is no such thing as a typical CD-I development budget, we can gain some sense of the scale of cost involved from the fact that many of the discs currently in production are budgeted at well over £250,000.
Philips seemed to scent this problem quite early and responded by establishing a number of companies tasked to stimulate software development through joint venture agreements.
By underwriting some of the heavy costs of development, Philips reasoned, they would break the confidence barrier and get the right levels of preliminary investment to secure a healthy catalogue of titles to support their launch.
The main companies involved (all technically jointly owned by Philips and Polygram) are American Interactive Media (AIM), European Interactive Media (EIM) and Japan Interactive Media (JIM).
Their success has been patchy.
The US company, AIM, has succeeded in initiating a number of major titles.
While few are completed at the time of writing, previews of work in progress suggest some excellent material.
Examples include a walk-through tour of the Smithsonian Museum, a multimedia version of Grolier's electronic encyclopaedia (currently available in online and CD-ROM forms and based on their 20-volume American Academic Encyclopaedia), a Time-Life disc teaching the skills of photography, several sports simulations, children's educational discs based on the Sesame Street characters, and a multimedia juke box disc.
Production values are extremely high and many of the discs include major investments in cartoon animation and stills photography.
The degree of interactivity varies with the disc but levels of interactivity in some of the children's discs, in particular, are well judged for the intended applications.
For example, a cartoon ‘juke box’ in one of the discs allows children to edit some popular children's songs together with their accompanying animated cartoon stories.
Children can alter words in the songs, choose different languages (including, in one case, ‘chicken’!), vary the rhythm and style of the songs, and vary the colours of the animated images.
The interactivity does not in fact run very deep but it is pitched well to suit children's interest levels and attention spans.
Development activity in Europe has been less successful with fewer disc start-ups taking place.
In fact, Philips recently subsumed EIM into its UK-based Interactive Systems operation and has just announced plans for its own CD-I publishing company.
Philips have always regarded indigenous publishing as essential to the successful launch of CD-I in Europe.
Currently, there appears to be a real danger that the European catalogue of titles available at the time of the planned launch in the first quarter of 1992 will be dominated by discs developed and published for the US market.
There seems no doubt, however, that whatever developments take place because of the efforts of Philips and its Japanese associates, much of the initial impetus for CD-I will come from music-based materials, tapping into an established and seemingly unending vein of demand.
Many CD audio discs are already being sold with a hidden extra of words and pictures.
When the first CD-I players are launched, many potential purchasers will find they already own CD-I discs in the form of audio CDs with words and pictures that a CD-I player can decode.
Philips hope that the wide availability of such CDs will both encourage existing CD audio owners to upgrade to CD-I and first-time CD audio purchasers to decide to pay the additional premium to acquire in a single package CD-I as well as CD-A.
It is important that we do not altogether ignore videodiscs aimed at the consumer market.
Analogue though they may be, videodiscs have remained doggedly on the scene in a range of niche applications since the late 1970s.
Today there is renewed speculation about a consumer market comeback.
The first efforts to bring videodisc to the consumer market flopped badly.
RCA, in particular, using a non-laser disc system, found that linear movies on disc were simply not sufficiently attractive to consumers to encourage them to buy hardware systems that could do nothing else than playback such movies.
At the time, VCRs with their programmed record capabilities, probably looked much better value for money.
Attempts by Philips and others to bring laser discs to the same market were also doomed to failure.
With a background such as this it is hard to be optimistic about renewed efforts to achieve consumer success with a medium so firmly rejected by the market.
However, the emergence of omni-players might change this.
Such hybrids will be able to play both CDs and videodiscs.
Another factor which may influence the chances of conventional videodisc players is that in the 1990s, the collection of high quality, permanent recordings of movies and other programming might appeal in a way it never could in the early 1980s.
Perhaps, in a curious way, CDs, which look so much like small-size videodiscs, may have paved the way for a collecting mentality.
Just as CD-A provides a means of collecting sound recordings of incomparable and permanent quality, videodisc may be seen as a means of acquiring a similarly high quality collection of movies and television programmes.
Plausible or not, it is hard to imagine any other justification for the renewed confidence being shown by hardware and software manufacturers for the prospects of finding a new generation of laser disc customers in the 1990s.
6.3 The Size of the Market in the 1990s
Since the most mesmeric quality of the consumer market is its sheer size, it is important that we try to gain some sense of what this means in terms of hardware and software sales.
Quantifying the size of the market is a game anyone can play.
Almost any figures will do in such a volatile and inherently unpredictable sector.
However, it is interesting to try to make some estimates even if we have to accept that they are, at best, only crude guidelines.
We need to start somewhere in an analysis and it might be useful to review the rate of penetration of another, related consumer electronics platform in recent years and to use this as some guide to the penetration we might expect from multimedia platforms.
This will give us a basis for quantifying the size of the market over the next decade.
We will not attempt to sector market potential between different platforms such as CD-I, CDTV, DVI or any other as yet unlaunched competitors.
Instead, we will consider the market for ‘CD-X’, a mass-market, appliance-based multimedia platform which, in reality, is a hypothetical amalgam of all the possible players.
Probably most significant for our enquiry is the progress of audio CD players.
Although CD-A has only been available commercially since 1983, we can estimate its likely performance over ten years and use a mix of actual and projected penetrations in our analysis.
Industry figures suggest the following penetrations in the three major consumer electronics markets.
The percentages are measures of the number of households acquiring CDs as a proportion of the total households in the different territories.
The picture is one of a slow start followed by years of sustained rapid growth.
Let us make the conservative assumption that CD-X follows the same trend but that by mid-decade it achieves a penetration of about 25 per cent of CD-A, rising to 60 per cent of CD-A by the year 2000 (obviously much depends on when CD-X appears!).
The projected penetration of households therefore is:
We will assume highly conservative figures for the number of households in 1995 and hold this number steady for the year 2000.
Specifically, we will assume 100 million households in the United States, 40 million in Japan and 80 million in ‘Western’ Europe.
Based on these figures and assuming sales of five CD-X discs per player, the market looks as follows in terms of unit sale.
We can calculate retail value by assuming CD-X players sell for an average of (in today's terms) £350 in 1995 and £200 in the year 2000.
As far as discs are concerned, prices will vary widely but we can reasonably assume an average of £15 in 1995 holding steady throughout the decade.
The total world retail value of the CD-X markets is then as follows (in billions sterling).
The problem with projections like these is that they are founded on highly arguable assumptions.
After all, while it is plausible to anticipate that CD-X may follow CD-A's pattern of growth, the real outcome may be dramatically different.
Economic and social conditions in the 1990s may prove so unlike the previous decade that CD-X purchases may either be much greater than CD-A or significantly less.
Average sales values for both hardware and software may also be different from the figures we have chosen and people may buy more or fewer discs for their players.
However, we have built a considerable degree of caution into the percentage penetration compared to CD-A and into the assumed numbers of households.
As far as households in particular are concerned, we have completely ignored regions other than the United States, Japan and Europe.
In practice, of course, many hundreds of millions of households in the rest of the world will contribute to the overall sales picture.
The calculations therefore give a very conservative view of how large the markets to be shared by different multimedia platforms could become by the turn of the century.
Moreover, these market figures only take account of multi-media delivered on a stand-alone optical disc platform.
They take no account of the markets for multimedia delivered into homes through broadband networks, an area of certain substantial growth, particularly in the latter years of the decade.
There are also suggestions that multimedia computing will become widespread and commonplace in homes as a further wave of more mature computer application finds its way into the home market.
This will add significantly to the total market for both hardware and software.
For hardware manufacturers, network operators, network service providers, software developers and publishers and distributors of all kinds, there are clearly massive opportunities ahead for business growth and for the founding of a new information industry sector that will flourish and prosper well into the next century.
7 MULTIMEDIA IN PUBLISHING, BOOKSELLING AND LIBRARIES
Traditional print-on-paper publishing industries and sectors related to them have already felt the impact of electronic information media.
Early in this study we showed that the scale of electronic information markets is already substantial.
In terms of growth, our figures suggest that print-on-paper products are being steadily overhauled in terms of sales (if not influence) by electronic products and services.
We may argue about the timescale of change or its effects on our lives, but what is beyond doubt is that change, massive and fundamental, is taking place in the way we acquire, use and are influenced by information.
It is both irreversible and irresistible.
What then are the implications for sectors of traditional activity where books, journals and newspapers have for so long been the unwavering mainstays?
7.1 Publishing
Traditional, print-on-paper publishers have some hard strategic questions to ask themselves.
Not least of these is whether they are really in the business of print-on-paper publishing.
A century and a half ago, the canal owners of England thought they were in the business of owning canals.
Then railways appeared and canal owners realised too late they had really been in the business of transportation.
The railway moguls went on to make fortunes while undiversified canal companies went broke.
Multimedia on its own may not, of course, prove to be the publishing industry's equivalent of the railway revolution and it would be unfair to characterise publishers as short-sighted canal owners.
However, we have seen how electronic information media are finding increasing numbers of major applications in our schools, businesses and homes.
This is not a transient phenomenon.
It is a reflection of profound underlying change in the way human society utilises information.
Powerful, established and loved as the printed word surely is, the emergence of electronic media cannot possibly occur without serious impact on the applications and markets for printed information.
Publishers, for all their short term concerns and justifiable scepticisms about gimmicks and gadgetry, need to consider their futures very carefully while they still have some options.
The single question they need to ask is: ‘Are we in the business of publishing print-on-paper products or information products?’
If publishers decide that the basic currency of their business is information not specific delivery platforms, they need then to review their commercial options and consider in detail the likely effect of shifting patterns of information use on all sectors of their traditional markets.
For the many publishers who already originate their products in electronic form — particular those active in reference publishing who actively database their properties from the start — the step from electronic origination to electronic delivery is not great, at least not in practical terms.
Fundamental issues of investment strategy, product design, manufacture and marketing need, of course, to be addressed with care and caution.
However, for such publishers, the launching pad is there if they wish to see and use it.
Others publishers, with an eye on longterm prosperity, perhaps even survival, may wish to start building their launch pads from scratch and without delay.
Multimedia is one ingredient in this climate of change and potential diversification but it has some particular implications which publishers need to understand.
Most obviously, multimedia depends on the use of pictures.
Most print-on-paper publishers may own the text of their publications or suitable electronic rights in it, but will rarely own or control electronic rights to the photographs they use to illustrate those publications.
There are of course notable exceptions and one of these forms an excellent model for a particular strategy towards multimedia: involvement through strategic alliance.
In the UK, Dorling Kindersley, one of the world's finest publishers of high quality illustrated non-fiction, own virtually all the hundreds of thousands of photographs and pieces of artwork that illustrate their books.
This positions them uniquely in exploiting the multimedia potential of their list.
Being book packagers by tradition also means that they immediately grasp the concept of exacting the maximum possible payback from their investments through as many diverse exploitations as possible of their information assets.
It came, therefore, as no surprise when Dorling Kindersley aligned themselves with a major player such as Microsoft who bought 26% of their equity in March, 1991.
Many such hardware and software companies sense that real money remains to be made in publishing.
Major Japanese hardware companies such as Matsushita are already buying substantial interests in the movie world.
US giants such as Microsoft have a strong commitment to multimedia CD-ROM publishing have long been searching for suitable properties.
The deal between Microsoft and Dorling Kindersley is the first signal that the worlds of software and hardware specialists and publishers are beginning to coalesce.
The key to the strong position of a company like Dorling Kindersley is their ownership of three key ingredients: a huge resource of illustrative material; a large body of textual material; and some strongly branded, bestselling popular reference titles.
With these valuable assets, it is natural to see an innovative company deciding to take opportunities in multimedia wherever and whenever they emerge.
For a company in such a strong position, a strategic alliance with a software giant or with another major player in the computer world makes good commercial sense.
Strategic alliance as a means of entry into these new markets is a serious option for many publishers, depending on the nature, quality and extent of their information assets.
A more aggressive attitude, of course, might counsel strategic acquisition and this is clearly an approach favoured by such major European publishers as the Maxwell Communications Corporation, Bertlesmann, Hachette and others.
For many publishers, however, the strength of their information holdings and the rights they hold over them, may suggest a quite different response.
Multimedia markets can still provide a rich source of added value revenue and margin but without major commitments, restructuring or risk-taking.
The low-risk approach is through licensing, a process familiar to most publishers from their traditional subsidiary rights activities.
Good deals can be done with would-be multimedia publishers who need intellectual property, with or without illustrations, on which to base their products.
There is a strong perception that multimedia products related to well-respected, successful books will have a greater chance of success because of that relationship.
Moreover, creating large bodies of worthwhile intellectual property is a specialised and costly business.
The new generation of multimedia publishers have enough problems without adding this to the list.
Licensing deals are based on a royalty on sales of the multimedia product with an advance, sometimes substantial, payable ahead of publication.
Publishers negotiating such deals need to bear two immediate issues in mind.
First, they must take great care in their choice of partners to avoid deals with enthusiastic amateurs who cannot deliver success where success is due.
Second, they must appreciate that their textual properties may only be a single element in a diverse multimedia development.
In other words, before a successful multimedia product can be created, the licensee will have to add a considerable amount of value — in terms, for example, of acquiring images, video segments, planning interactivity, doing the underlying computer programming and ultimately manufacturing and selling the product.
This, sadly, has to be reflected in the size of royalty paid to the publisher.
Obviously, a publisher must bargain hard, and may have special cause to do so if the property in question has a strong and valuable branding, but, if ever a deal is to be made, the publisher needs to be sensitive to the real commercial worth of his material in the broader context of multimedia development.
Licensing is low risk but possibly not ‘no risk’.
It may be that the licensing of their properties is the publisher's equivalent to ‘selling the family silver’.
After all, unless they are careful, they will gain no experience of these new kinds of products and their markets.
They may exchange short term financial gain for longterm strategic disadvantage.
After all, they may want to publish multimedia editions of their works themselves when the time is right.
Such important considerations can be allowed for in licensing negotiation so that the issues are properly addressed.
The first step, however, is to know the issues exist.
Above all, this means taking a strategic view of multimedia and its impact on traditional publishing practice and markets and to do so while there is still time.
Otherwise, events will overtake the publishing industry and strategy will become regretful hindsight.
7.2 Bookselling
Bookselling has for long been stereotyped as a sheltered world, separated from the usual pressures of everday business concerns, where change is almost instinctively resisted.
It is a stereotype which, like all stereotypes, is rooted in a little truth.
However, the overwhelming reality today is that the Book Trade is highly professional, commercial, competitive, flexible and increasingly computer literate.
It has been an almost Darwinian process, of course, with those failing to shrug off the stereotype simply going out of business and removing themselves from the genetic pool.
The pattern of change has lead in recent years to the growth of large book retailing groups, some with a ready will to diversify into other product ranges.
In the Netherlands, Wolters-Kluwer Bookshops are extending their range of book stocks with up to 100 CD-ROM titles and have appointed a new media adviser to help them plan future developments.
In the UK, W.H. Smiths, once consisting of a modest number of bookselling outlets, is now one of Europe's largest retail chains, aggressively selling a wide range of non-print products, including satellite television services, computer software, audio CDs and other technology-related items.
Other major groups such as Pentos and Sherratt & Hughes & Waterstones, continue to focus on books but are sufficiently commercial and pragmatic to be ready to re-define what is meant by a book in the light of technological developments.
There are a number of important questions for the Book Trade to address.
First and most fundamentally, are the members of the Trade sufficiently aware of coming new generations of information products to have a view on whether they should be selling them?
Second, do the new products have some organic relationship with their current stock-in-trade?
In other words, will existing customers be likely to want to buy the new electronic products alongside their print-on-paper counterparts, and, perhaps more interestingly, will those electronic products draw new customers into their shops?
Third, can booksellers make a greater profit using their shelf space for displaying electronic products than the book stocks that may have to be sacrificed?
Is there enough margin and what are the implications (and therefore, costs) associated with after sales service?
There are no pat answers to these questions.
The Book Trade needs to examine them and reach its own conclusions.
In terms of multimedia, however, there is a particular context to be kept in mind.
As we have indicated in the previous chapter, there looks like being a very large potential market for consumer multimedia.
By the year 2000, a world market worth over £5 billion at retail prices suggests a margin of well over £2 billion for retailers.
However, while the prospects are exciting and real, there is little to suggest how the various products will be distributed and sold.
In other words, we do not yet know who these retailers will be and who, therefore, will be adding £2 billion to their gross contributions.
Right now, no established wholesale or retail infrastructure exists to channel these new products to the customers.
CD-ROMs are not a good model to use because their appeal so far is more specialised than the popular multimedia that will characterise consumer market products.
There are plausible suggestions that computer hardware and software outlets will play a major role as will music stores which already quite commonly stock a wide range of audio CD products and videotapes.
However, for the moment, the Book Trade could exploit the uncertainty by taking a strong position as potential multimedia traders.
Multiples such as W.H. Smith will certainly not miss the boat but the more traditional, book-focused groups may not even know there is a boat to catch.
There is an urgent need for the Book Trade to take a proactive role both in terms of educating itself about the new trading opportunities and by establishing links with the principal multimedia publishing interests worldwide.
There two immediate steps that the Trade can take.
First, it can gain knowledge, ask questions, talk to publishers getting involved in multimedia, attend conferences and exhibitions (there are plenty!) and experiment with early multimedia products.
Second, it can make exploratory contacts with such organisations multimedia players as Philips, Sony, Matsushita, Commodore, Microsoft, Apple Computer, IBM and others.
This is not hard to do and could be the start of longer term relationships leading to the Book Trade having a central role in retailing the new products.
It would be all very well if the Book Trade were not in fact a vast collection of individual trading interests.
Clearly, the big groups can make their own approaches but how can the small, independents hope to play a serious role in the proceedings?
The obvious possibility is for their trade associations to play a role on their behalf.
In the UK, for example, the Booksellers Association could begin immediate initiatives to assist its members.
This means of course a commitment of resources and money.
One way to fund this would be through a kind of ‘electronic media levy’ on the membership to enable the Association to address the electronic product sector in a serious, concerted and systematic fashion.
Underlying any such proactive initiatives must be a real belief that electronic media in general and multimedia in particular, represent major new retailing opportunities.
It is really a Catch 22.
The belief will come from researching the media and their prospects but the research will not be funded without the belief.
The circle has to be broken if the Book Trade is not to miss a major opportunity for a logical, practical and lucrative diversification.
Like the canal owners of the last century and the publishers of the present one, booksellers also need to ask themselves what business they are in.
Are they in the business of selling books or information products?
The difference may be worth more than £2 billion.
7.3 Libraries
Libraries too are traders and like all traders, need to be responsive to demand.
For years, libraries have met the demand of their customers for reference services, book loans and, more recently, loans of music recordings and even computer software.
Most good library professionals have demonstrated time and again that they understand their remit as extending to all kinds of information, independent of the way in which it is delivered.
Part of the reason for this enlightened attitude is the lesson of the past.
When libraries have not addressed the demands of information users, other institutions have sprung up to do so.
In his book, ‘The Electronic Library’(Neal-Schuman Publishers, New York, 1984) Kenneth Dowlin wrote:
If Library Information Services professionals do nothing to encourage their institutions to change, then new institutions will emerge in their place.
The reluctance of school librarians to accept audiovisual material into the scope of their operation was a contributing factor to the creation of the profession of audiovisual specialists, and the reluctance of librarians to provide information and referral led to the creation of Information and Referral Centres…
Barbara Markusen, a noted author on library networking, has commented that although at one time the public library was the largest source of books and research materials in most cities, this is no longer the case.
Libraries have failed to recognise the change in the information-seeking patterns of the individual, and so the market share of library services has decreased.
Markusen fails to mention, of course, the traditional underfunding that has dogged progress and development in many public libraries and her judgement here may be unduly severe.
However, the point she makes is important.
If libraries do not address changing patterns of information use, then others will.
We have already noted in our examination of CD-ROM markets (see 2.7) that libraries have been pioneer users of CD-ROMs.
They still remain by far the biggest group although most use is concentrated in the academic library sector.
A recent British Library survey, for example, indicated that 30 per cent of UK libraries were using CD-ROM and that, of these, 50 per cent were academic libraries.
There is increasing acceptance of these powerful new information media and the beginnings of widespread understanding both of their operational technicalities and their most effective applications.
Most of the CD-ROM databases being used at present are, not surprisingly, strongly bibliographic in character.
For example, the British Library survey mentioned above also showed that the most popular CD-ROMs in use were Bookbank, The British Library Pilot Disc, Medline, Books in Print and the Science Citation Index.
There is, therefore, little tradition yet of using even the growing numbers of full text CD-ROM databases let alone those embodying the first elements of multimedia.
Indeed, a major hinderance to the uptake of multimedia databases will undoubtedly be the limitations of the installed base of computers in libraries.
For many, the costs of upgrading in order to be able to benefit from multimedia may be prohibitive.
The force of change, when it comes, will be generated by the increasing availability of good multimedia databases and the demand of the libraries' mainstream customers for access to these databases, both for reference and loan.
At the level of loans, there seems little real obstacle to rapid progress, so long as multimedia publishers do not prohibit loans through their conditions of sale.
The kinds of multimedia products involved are likely to be robust, read-only discs, ideal for loan applications.
All that is needed is a demand from library users and an understanding of that demand by librarians.
For reference purposes, multimedia implies decisions about support for specific hardware platforms.
The fact that consumer platforms will generally be convenient, all-in-one, low cost units needing nothing more sophisticated than a domestic television for display purposes, suggest that many libraries may be able to support all available platforms and so give their customers the widest possible choice.
In addition to disc based, off-line systems, there may well be demands for multimedia online services too.
In particular, libraries may be able to offer dial-up video conferencing, once such facilities become cheaply available, so offering users a complete package of information and communication services under one roof.
Certainly, ISDN services in some form will provide opportunities for libraries to increase the range of their offerings by allowing occasional access on demand to some of the varied multimedia and other information services that will be offered on the new networks.
In the field of archiving, low cost digitising scanners for both text and illustrations may soon make possible practical, on-site multimedia archiving.
Although making bespoke CDs with WORM (Write Once Read Many) or erasable optical disc systems is likely to remain expensive for some years to come, libraries might well be able to make use of very large magnetic storage systems such as the newly emerging ultra-high capacity hard discs.
There are issues of database design, indexing and retrieval to be resolved in this area but by whatever technology, bespoke electronic archiving is already close to a practical reality for libraries offering a rich opportunity to build major archives of various kinds of multimedia data.
The diverse and increasing wealth of information options that will characterise the 1990s, will present both challenge and opportunity to Library and Information Services professionals.
More than ever before, imagination, insight, understanding of new media and enthusiasm for the processes of ongoing change are going to become essential ingredients to effective librarianship.
Above all, librarians will have to become ever more skilful managers of information resources.
All of this implies an urgent, increasing and ongoing need for training and retraining throughout the profession.
In the British Library's recent publication ‘Information UK 2000’(Bowker-Saur, 1990), the authors write:
…(there are) great implications for the education and training of information professionals both initially and throughout their career.
As well as traditional skills, information professionals will have an expanding role in the training of end users, as gatekeepers to electronic media, in quality control of data and will confront much more difficult management issues.
In all professional areas, library, information and archives, there will be greater emphasis on and greater demand for the inclusion of training in management methods.
Finally in the new age of networked multimedia information, libraries may well benefit from some old-fashioned networking of their own.
In such a complex, fast-changing information environment, it will be vital for library professionals to maintain dialogues with other interests in the information community.
This means publishers, booksellers, electronic media software and hardware companies and, of course, other library professionals both nationally and internationally.
It is only through a continuous, open exchange of views and experience that libraries will make the optimum use of the varied powerful electronic media opportunities that will characterise information options of the 1990s.
8 THE MULTIMEDIA FUTURE
When we began this study we qualified its title very carefully.
One of the points we made was that even a few years in multimedia development is a very long time.
It is certainly impossible to describe with any confidence developments that may be affecting multimedia information systems by the turn of the century.
Like weather forecasting, we can be fairly accurate in our predictions over a very short period but looking further ahead is really a matter of guesswork.
Instead of ranging widely over a many possibilities, therefore, we will focus briefly on just four important and current aspects of multimedia which are sure to extend their influence dramatically during the 1990s.
The four we have selected are high definition television, networked multimedia, handheld multimedia and virtual reality.
8.1 High Definition Television (HDTV)
One might expect that a technology already sporting a much used acronym must surely be a part of present not future considerations.
However, HDTV remains firmly in the future despite the intense interest and development work it is currently exciting.
HDTV is an effort to create very high resolution television capable of delivering a level of clarity and detail hitherto seen only in top quality cinema productions or 35 mm transparencies.
The implications for image rich environments such as multimedia are plain.
Indeed, the issues are already emphasised even by the existing levels of television definition.
For example, the NTSC standard favoured in Japan and the United States, uses 525 lines to create a screen of information while European standards, PAL and SECAM, both use 625 lines.
A quick comparison of the two line standards is enough to demonstrate how even an extra 100 lines of definition can greatly enhance the sharpness and colour of images.
HDTV systems are attempting to raise definition to well over 1000 lines so the quality jump is likely to be dramatic.
There is a standards battle currently being fought and for the moment it looks almost as if consumers may soon be confronted with a variety of competing, incompatible television systems in different regions of the world.
In some ways, although this mirrors the current NTSC, PAL and SECAM situation for ordinary television, it is surprising that while global standards are an intense concern in multimedia development, the same is not true for HDTV.
One of the major divisions has been between the Japanese and the Europeans.
As long ago as May 1989, at the International Radio Consultative Committee meeting in Dusseldorf, Germany, European manufacturers blocked a Japanese move to get their 1125-line MUSE system accepted as a world standard.
Despite this, MUSE has now been widely adopted in Japan and TV sets are being designed, tubes are being manufactured and large-screen projection systems are coming to market.
In addition, a massive development programme is under way to produce the first large, LCD HDTV screens.
In Europe, there is broad agreement that HDTV standards should be based on the MAC (Multiplex Analog Component) system, currently characterising the signals used for satellite and cable transmissions.
MAC signals are required by a European Commission directive covering transmissions from the high-powered direct broadcast satellites.
However, the directive does not apply to low-powered PAL transmitters such as Luxembourg's ASTRA satellite and the possibility of such PAL systems gaining ground over the direct broadcast MAC satellites is causing some concern over adopting MAC as the basis for HDTV standard (HD-MAC).
In spite of these doubts, a powerful consortium using both corporate and national government funding has been formed consisting of such major electronics interests as Philips, Bosch, Thomson and Thorn EMI.
The consortium has led to the creation of the Eureka 95 HD-MAC project.
Since the project's inception, major British broadcasting, communications, video and television companies have become involved including the BBC, the Independent Broadcasting Authority, the Independent Television Association, Applied Video Systems, British Telecom, Ferguson and Rank Cinetel.
By the end of 1990, taking account of all national interests, thirty three companies are now involved in Eureka 90.
In the United States, a complex wrangle is taking place over a range of possible standards.
The major proponents are NBC with ACTV I and ACTV II; NHK (Japan's national broadcasting company) with Narrow MUSE, MUSE 6 and MUSE 9; North American Philips with HD SNA; Faroudja Laboratories with Super NTSC; BTA with ClearVision I and II; and Zenith with their Spectrum Compatible system.
The American Federal Communications Commission has said that it will not decide on a HDTV format before 1993.
Many US commentators believe a final decision may not in fact be made before 1995.
Meanwhile, HDTV has been preceded by an enhanced television signal called Extended Definition (EDTV).
America's largest television network, NBC, is backing EDTV as an immediately available interim stage while the issue of HDTV standards continues to be debated.
It delivers significantly better quality pictures than NTSC with definition somewhere between PAL and HDTV.
Pilot products and experimental broadcasts have been continuing in Europe, Japan and the United States as interests groups jockey for position and strategic advantage.
In Japan, for example, NHK recently began broadcasting daily one-hour test transmissions in MUSE from a direct broadcast satellite, showing the pictures on 200 large screen receivers located in various prominent public places.
In the UK, the BBC has established a HDTV outside broadcast unit.
So far the unit has covered major sporting and cultural events and broadcast the results by satellite around Europe to demonstrate the potential of HD-MAC.
In Europe more generally, June, 1990 saw fourteen European electronics companies, programme makers and broadcasters establishing a cooperative project called Vision 1250 to promote HDTV and to encourage producers to make HDTV programmes.
Despite the confused position over HDTV standards, the issue of bringing HDTV to market has created an atmosphere of unprecedented cooperation among companies that are longstanding competitors.
Huge amounts of money are being pumped into research and development.
In Japan, over US$800 million has already been spent with every major electronics firm having an involvement in some aspect of HDTV development.
In the United States, three major research laboratories are working on HDTV around the clock.
These include the David Sarnoff Research Laboratories in Princeton, Philips' Laboratories in Briarcliff, New York and Zenith Laboratories in Glenview, Illinois.
Even Korea has announced a US$200 million HDTV research programme.
This remarkable level of interest is driven by a general belief that the potential market for HDTV production equipment, transmission facilities and TV receivers is huge.
Estimates vary widely but by 1995 many industry analysts expect HDTV hardware and programming to be worth many billions of pounds.
In Japan alone, the market may exceed £4 billion.
What does all of this ferment mean for multimedia development?
Before we can answer this question we need to address another.
Will HDTV succeed in bringing a new generation of TV receivers into homes and other environments?
This, of course, is the central aim of the proponents of HDTV.
However, virtually all the proposed HDTV standards mean that consumers will have to write off their existing televisions and buy new sets in order to receive HDTV.
This in turn suggests that HDTV has to offer something radically new, a benefit so persuasive that consumers will be prepared to spend billions on new equipment.
There must be real doubt the power of HDTV to persuade on the basis of picture quality alone.
It is more likely that the eventual, inevitable success of HDTV will owe more to the power of huge marketing campaigns which will soon be waged throughout the world than to any urgent need among consumers.
The professional persuaders, with so much at stake, will eventually convert HDTV from a ‘nice-to-have’ to a ‘need-to-have’.
In other words, the commercial band wagon is huge enough to be unstoppable.
The real issue is timescale.
While many forecasts suggest mid-decade for substantial HDTV impact, the complex wrangles over standards and the uphill task of convincing customers to buy, may well mean that HDTV is unlikely to become firmly established before the turn of the century.
Its impact on multimedia may be considerable, however, and manufacturers and developers need to prepare their strategies now.
There are clear technical issues such as the data transfer and memory implications of addressing much higher levels of display definition than are generally required today, particularly, of course, in consumer environments.
Processes such as DVI, for example, will need to improve radically if it is to provide acceptable video at HDTV standards.
Indeed, Intel are well aware of this issue and promise that DVI will meet these requirements by the end of the 1990s at latest.
Systems such as CD-I, CDTV and their derivatives will also need modification in order to deliver both the appropriately encoded TV signals and the display quality that will be expected of them.
In terms of product development, HDTV may not have profound implications but it may change some design factors.
For example, the quality of source material will need to be consistent with HDTV delivery.
In some cases, quite new kinds of source material will become practical.
For example, consumer multimedia is currently unable to incorporate detailed mapping information because existing TV screens cannot reproduce the fineness of detail required.
HDTV will allow the use of these and other intricate images and, by so doing, may significantly widen the range of potential products and applications.
In terms of the use of multimedia generally, the visual impact of HDTV may heighten interest in using multimedia across a broad range of applications, most obviously as a component of HDTV-based POS and POI terminals.
Indeed, HDTV may prove to be a powerful ally for multimedia.
By promoting an increased visual awareness and by providing such superior image qualities, HDTV could do much to consolidate and strengthen multimedia's influential position in the information markets of the late 1990s.
8.2 Multimedia Networking
It is already clear that connectivity will be a major component of future information systems.
In an age of increasing information networks, strengthen by the needs of globalised industrial, social and economic structures, there is little doubt that standalone information platforms are a peculiarity.
In educational, professional and business environments the precedents for networking are clear.
In the home, by contrast, communications other than voice telephones, are unfamiliar and comparatively rare.
However, the growth of optical-fibre, ISDN services in the mid to late 1990s will radically change the way in which information is made available in the home.
Cable television systems already reach huge numbers of homes.
In the United States, for example, nearly 60 million homes subscribe to basic cable facilities.
Many such systems are already capable of delivering services other than ordinary, one-way television.
The wide band width suggests that such provision may not only be diverse but, where the application requires it, fully multimedia too.
In particular, fibre-optic cabling can readily supply a wide range of two-way interactive services including home banking, security, shopping, travel, hotel and entertainment agency, videophone facilities and even forms of interactive television.
The implication is that multimedia applications may not so much be an issue related to the relative success of particular standalone platforms but rather an inherent feature of information services piped directly into homes, offices, libraries, schools and colleges through broadband information highways.
Off-line storage will remain important, of course, and the home, office, library and school of the future will still have its equivalent of multimedia CD-ROM, CD-I or CDTV — probably in erasable, re-recordable form by the turn of the century — built into its otherwise network-fed integrated information centre.
8.3 Handheld Multimedia
In the networked, multifaceted information future we have suggested, existing trends towards portability will not slacken.
In the recent British Library publication, ‘The Emergence of the Electronic Book’(BNB Research Fund Report 46, 1990), we saw how handheld electronic reference devices based on chip memories were already foreshadowing the first true electronic books.
Today, portable optical disc players are also emerging.
In June 1990, for example, Sony exhibited a prototype for palmtop multimedia in the shape of an all-in-one, handheld CD-I player.
It now looks as if the first commercial versions will be brought to market soon after CD-I is made generally available worldwide.
At about the same time that they unveiled portable CD-I, Sony also launched a text-only counterpart known as the Data Discman.
Nowadays, Sony prefer to call it the ‘Electronic Book Player’.
It is in fact a palmtop 3-inch, 230 megabyte, ISO 9660 CD-ROM system combining drive, search engine and display screen in an easy-to-use, compact, low cost unit.
Sony will shortly upgrade this to the Extended Architecture standard enabling CD-ROM to beat portable CD-I to becoming the first multimedia handheld units to reach the commercial market.
This trend towards ultra-portability is also mirrored in the computer games industry.
A powerful surge in handheld games consoles has taken place during the last year.
Small dedicated games units had been available for some time but had always had only crude multimedia capabilities.
The latest devices such as the Lynx and Gamegear consoles offer plug-in games facilities, high resolution, full colour screens, excellent sound capabilities and, in the case of the Gamegear, the facility to replay games through a full-sized television receiver.
This latter connection is important because it emphasises a probable key trend.
In the information environment we sketched above where networked services are integrated with off line platforms to form a total, unified information and communications resource, users will want to take elements of that resource with them when they are on the move.
The Gamegear reflects this desire, allowing youngsters to play their favourite games at home but also to take them away from their homes and enjoy them on the move.
The same may well be true of the comprehensive resources we have suggested may soon become available in fixed sites.
The handheld multimedia system that eventually emerges, therefore, will be much more than a single delivery platform.
It will be a palmtop information and communication centre, a hybrid mix of computer, videophone, fax, note-book, organiser, online terminal, entertainment centre and multimedia database.
The enabling technologies for such an information and communications centre are already more or less in place.
Indeed, its individual components are already coming to market as separate portable products.
It only remains for miniaturisation, integration and market demand to reach the point at which the various elements can be drawn together into a single commercially viable device.
8.4 Virtual Reality
As multimedia systems are able to create and deliver increasingly complex and rich information environments, it becomes ever more difficult for users to manipulate the environments in simple, comprehensible ways and so enjoy their full benefits.
We have already touched on the importance of user interfaces (see 1.4) and have emphasised how crucial it is to provide lay users of multimedia systems with interfaces that offer extreme friendliness without sacrificing power for simplicity.
One technique under intense development focuses on the way in which multimedia environments can be experienced as virtual worlds, to be explored and utilised by information users through direct manipulation and experience.
Such virtual reality systems are already in existence today and foreshadow far more powerful developments likely within a few years.
Virtual reality is a way of enabling information users to participate directly in complex multimedia environments, often by actually entering those environments with their own human senses.
The technology of virtual reality uses a computer to map a user's body and senses directly into a digital world.
Wearing special data gloves, for example, a user's hand can be projected in a virtual form into a 3-D computer-generated multimedia environment.
By manipulating the glove, the user can interact with the virtual world, handling and manipulating objects or using a repertoire of gestures as commands.
In other applications, users can wear a full body suit and data goggles with built-in stereoscopic viewscreens, to step directly into the virtual world.
The kinds of worlds that can be created are limited only by the multimedia software designed to generate them and the computer processing power available to bring them to life.
One of the key distinctions between this kind of application and platform-based, optical disc multimedia such as CD-ROM, CD-I or CDTV, is that the generation and exploration of the multimedia environment takes place in real time.
This means that 3-D animations, interactivity and worldscape modelling is an ongoing process require immensely fast and powerful computing.
Crude but adequate systems are already appearing in relatively cheap ‘desktop’ form.
For example, UK-based W Industries, have created a strap-on virtual reality system, available at a price of a few thousand pounds, that enables the wearer to walk through 3-D worlds.
Among the applications already being tested include the use of virtual reality in entertainment.
One of W Industries products is a virtual reality flight simulator which uses a mock-up cockpit to simulate tactile feedback and a data helmet to provide stereoscopic, full colour vision and stereo sound.
When users physically turn their head, the helmet maps the movement.
A new 3-D perspective is generated in real time and delivered to the helmet's eyescreens to create the effect of ‘looking around’ the virtual world as if it was a real one.
Networked virtual reality systems of this kind already enable their users to meet each other in various virtual realities allowing multi-player games.
In the future, such systems will no doubt give a whole new dimension to videoconferencing.
Other major applications under development include computer aided design systems which enable designers to actually enter 3-D models of their creations, to explore their functionality, aesthetics and to alter them using their computer-generated, virtual hands.
In the field of architecture, for example, two architects could enter a model of one of their buildings and examine it together, changing a wall here or a window there by means of a virtual touch or gesture.
In telerobotics, virtual reality is being used to allow human intervention in environments which are too dangerous for actual human presence.
Such systems are being engineered for hostile environments such as nuclear reactors, battlefields and even space stations.
In scientific visualisation applications, researchers can both model and step inside complex molecular structures.
This helps them assess the effects of changing those structures by allowing them make such changes directly through the virtual reality interface.
Over the next few years, virtual reality may well revolutionise our access to 3-D multimedia information.
It will have major impacts in such areas as education, industrial and professional training, entertainment and leisure, simulation, computer-aided design, telerobotics, scientific visualisation, military and security applications and in creating a new generation of interfaces for users of large, complex multimedia databases.
Virtual reality is perhaps the ultimate expression of our human involvement in multimedia information.
It is spectacular even in its present, relatively primitive form.
By the turn of the century, computing speed and power may be so great that real time, 3-D photo-realism may become a possibility.
When this happens, we will encounter a strange blurring of the edges between virtual and true realities.
Multimedia will have evolved from a technological issue into a metaphysical one.
As futuristic as the concept seems, it addresses here and now one of the most important issues in multimedia development.
We may well succeed in fashioning profound and powerful information products but there is a danger that our capacity to produce such products is outstripping the ability of human beings to grasp the full range of benefits they offer.
If designers, developers and publishers are ever to utilise multimedia to the full, they will need to transform not only their concepts of content and presentation but the means by which their customers participate in the medium.
Clearly, the existing technologies are a limitation and multimedia must temporarily be constrained by the platforms currently available to deliver it.
However, these constraints need to be tested and stretched to their limits.
More importantly, a new item needs to be put at the top of the multimedia agenda.
How can we create affordable interfacing which allows users to participate directly in rich new multimedia environments and how can those same interfaces simplify and enhance the presentation of high volumes of complex data in a way that meets the real needs of the users?
When we have the answers, multimedia will at last be ready to deliver the revolution that electronic publishing has long been promising.
8.5 The Human Factor
Multimedia is not, of course, a technological issue.
This may be a curious thought with which to conclude a study of multimedia, especially one which has offered so much analysis and commentary about technology.
Nonetheless, it is a point worth making.
In any consideration of multimedia, we must, of course, focus on technology and its varied promises.
However, we must never loose sight of a vital fact.
It is people who design multimedia systems and it is people who use them.
This has quite simple implications.
Designers, developers and publishers of multimedia need to make a careful and accurate analysis of the human needs their products are meant to address.
Then they must translated their analysis into effective interface and product design.
Although simple in essence, this notion is difficult to implement.
True mastery of design and development in multimedia demands more than an incremental change in existing established skills.
It requires a genuine transformation or, perhaps, convergence.
In the creation of effective multimedia the professions of information scientist, designer, computer programmer, systems analyst, film maker, financier, and publisher must somehow be drawn together, either in a single extraordinary person or into a well-managed and effective creative team.
By doing this we may begin to see products that people will want to buy and use.
We need to remember that however dazzling the technology may become, someone must be there to pay the bill.
Multimedia, like any technological advance, can only be driven forward by commercial success.
This means addressing and meeting the needs of the people who will part with money for the multimedia products created for them.
Ultimately, therefore, the real issue in multimedia is not technology but the paying customer.
IMPORTANT REFERENCES
Ambron S. & Hooper K. (Eds), ‘Interactive Multimedia’, Microsoft Press, Redmond, Washington, USA, 1988
Barker J. & Tucker R.N., ‘The Interactive Learning Revolution’, Kogan Page, London, 1990
Kenneth E. Dowlin, ‘The Electronic Library’, Applications in Information Management & Technology Series, Neal-Schuman Publishers, New York, 1984
Gale J., ‘Micro Multimedia’, Information Workstation Group, Alexandria, Virginia, USA, 1990
‘Interactive Marketing Report 1990’, ItMk, Atlanta, Georgia, USA, 1990
‘Interactive Media Courseware 1991’, NIVC (now the European Multimedia Centre), London, 1990
Martyn J., Vickers P., Feeney M. (Eds), ‘Information UK 2000’, Bowker-Saur, London 1990
Nielson J., ‘HyperText & HyperMedia’, Academic Press, San Diego, California, USA, 1990
Wilson R., ‘The Market for Multimedia’, Dragonflair/European Multimedia Centre, London, 1991
USEFUL SOURCES OF ONGOING INFORMATION
The following newsletters and journals are a useful source of information about multimedia developments.
‘Digital Vision Interactive’, Ed.
Isabel Pring, Suite 310, The Blackfriars Foundry, 156 Blackfriars Road, London SE1 8EN.
Telephone: 071 721 7053
Mature, accurate news and comment focused largely on developments in DVI and DVI-related technologies and media.
‘EP Journal’, Ed.
David Powell, Electronic Publishing Services, 104 St John Street, London EC1M 4EH.
Telephone: 071 490 1185
Covers all aspects of the electronic information industry providing an excellent balance between news, authoritative comment and strategic analysis.
‘European Multimedia Bulletin', Ed.
Claire Bayard-White, European Multimedia Centre, 24 Stephenson Way, London NW1 2HD.
Telephone: 071-387-2233
News and updates on all aspects of multimedia.
First issue appeared in April 1991.
Lavishly produced but too new as yet to judge its consistency and long-term value.
‘Inside IT’, Ed.
John Barker, Tossa House, Main Road, Smalley, Derby DE7 6EF.
Telephone: 0332 881779
Despite its name, ‘Inside IT’ deals largely with multimedia in education and training.
Often called the ‘Private Eye’ of the multimedia industry, it is gossipy and irreverent.
Despite being fun to read it is sometimes short on accurate analysis and often spoiled by its own spitefulness.
‘Interactive Media International’, Ed.
Tony Feldman, Electronic Publishing Services, 104 St John Street, London EC1M 4EH.
Telephone: 071 490 1185
Addresses the full range of interactive media and their applications.
Particularly strong on analysis and commentary with good news coverage of key developments.
‘Multimedia Review’, Ed.
Sandra Kay Helsel, Meckler Corporation, UK Office: 247-249 Vauxhall Bridge Road, London SW1V 1HQ.
Telephone: 071 931 9985
Serious articles on topics in multimedia.
Somewhat academic in character but sound material contributed by leading industry and research figures.
‘Scottish Interactive’, Ed.
Tony van der Kuyl, Scottish Interactive Technology Centre, Moray House College, Holyrood Road, Edinburgh EH8 8AQ.
Telephone: 031 557 2005
Useful, low-priced newsletter providing updates on multimedia developments in education and training in Scotland with forays further afield.
‘Videodisc Monitor’, Ed.
Rockley L. Miller, PO Box 26, Falls Church, Virginia, USA.
One of the longest established newsletters in the industry.
These days, despite its name, it covers all forms of optical media providing a valuable (if US-biased) amalgam of news and comment.