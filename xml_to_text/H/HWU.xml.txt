

EDITORIALS
Do doctors short-change old people?
See page 30
‘No-one is so old that he does not think he could live another year’
Cicero, De Senectute, c 44 BC
‘The following are not to be resuscitated: very frail elderly, over 65 years.
The top of the yellow treatment card is to be marked NTBR.’
This memorandum, issued by the physician superintendent of Neasden Hospital, London, to all his medical officers over 25 years ago, attracted widespread opprobrium from the UK national press and the chronologically correct if sexist response from the British Medical Journal that ‘every patient whatever his age or his disease should be resuscitated unless the doctor looking after him has no doubt at all that he should be allowed to rest in peace’.
Subsequently, guidelines concerning cardiopulmonary resuscitation (CPR) have become widespread as the procedure has become more familiar.
Simultaneously, the populations of developed nations have aged; and, despite general agreement that age per se should not preclude CPR, controversy still smoulders over the application of modern drugs and complex expensive technology to patients of advanced years.
It is easy to denigrate age-related treatment policies by tarring them all with the brush of the neologism ‘ageist’.
However, we should be wary lest use of such an emotive and pejorative term leads to premature dismissal of legitimate arguments.
Ageist policies are sometimes advocated on the basis of compassionate and humanitarian concerns — perhaps one might regard this stance as reasoned ageism.
But age discrimination may also be practised at a primitive, subcortical level, with aged patients being accorded substandard treatment simply through the exercise of knee-jerk responses based on blind prejudice.
Although it is generally true that increasing age exerts an unfavourable effect on outcome of diseases and medical interventions, this influence is weaker than is generally supposed and is mainly due to age-associated disorders.
Thibault lately drew attention to the potential benefits of heroic interventions for aged subjects with an account of an 87-year-old woman who was doing well after aortic valve replacement and simultaneous triple coronary artery surgery.
This report offered a useful opportunity to discuss the extent to which the age factor might influence clinical decision making.
Now Brancati and co-workers in this issue (p 30) report that age itself was not strongly associated with in-hospital mortality or 2-year survival in their series of patients admitted with community acquired pneumonia, whereas severity of the acute illness and of other coexisting chronic disease certainly was.
Those who say that certain individuals are ‘too old’ to receive high-technology medicine will hotly deny any parallel with discrimination by race or sex.
They argue that the poor response of older patients to active medical and surgical measures and their reduced actuarial expectancy will often make the cost-benefit equation much more finely balanced for the patient.
And, from a broader perspective, some people fear that the ageing population of the developed nations will, if permitted unlimited access, consume resources in unaffordable quantities and will deny these and other benefits to younger patients whose claims may be stronger, both because of their greater life expectancy and because of their social responsibilities in terms of work and dependants.
This argument was persuasively propounded by the director of the Hastings Center for Medical Ethics, who concluded that ‘…after a person has lived out a natural life span [eg, into the late 70s or early 80s], medical care should no longer be oriented to resisting death…[but]will be limited to the relief of suffering’.
The champions of optimum treatment irrespective of age point to the surprising number of years of expected life even at advanced ages (for example, about 5 years for a healthy 87-year-old American  woman) and to moral entitlements accrued over years of work, tax paying, family rearing, and possibly military service.
Besides, it is not always easy to distinguish treatments that alleviate symptoms and enhance life from those that prolong it; this point is well illustrated by the use of angiotensin-converting enzyme inhibitors for cardiac failure.
In any case, others have doubted that this type of thinking would make worthwhile savings and have countered that ‘…we should allocate resources according to the probability that a patient will benefit rather than his or her age’, and that ‘physiology not age should determine care’.
Evans has drawn attention to the danger of discrimination by specialty in a cost-obsessed climate, so that‘a geriatrician may face the nightmare that the specialty dedicated to providing old people with the best of hospital care will become an impediment to their obtaining it’, even though an investment in optimum treatment often yields rich dividends through the avoidance of expensive long-term dependency.
The whole question was given a fresh airing in the March issue of the Journal of Medical Ethics, and if ethical considerations prove indecisive perhaps legal pressures might exert an untypically beneficial influence in persuading all doctors to give their patients the best professional counsel.
A fundamental principle of that counsel is that chronological age should not be used as a proxy for biological fitness and that the advanced years of the patient do not relieve physicians of the responsibility of assessing cardiac, respiratory, metabolic, and cerebral reserve.
So much for the arguments: what of the practice?
There are some areas of medical activity whose beneficiaries are almost invariably of advanced years and whose practitioners could thereby be claimed to favour older citizens.
Examples are hip joint replacement, cataract extraction, prostate resection, and cardiac pacemaker insertion.
However, in other areas there is good evidence that archaic and illogical ageist practices survive.
One well-documented example is the management of acute myocardial infarction.
There is a clear-cut favourable benefit-to-risk ratio for thrombolytic treatment of the elderly, but in the USA, patients over the age of 75 are six times less likely to receive such therapy than are younger patients.
In the UK, one-fifth of coronary care units operate an age-related admission policy, and two-fifths an age-related thrombolysis policy.
Yet in situations such as this, where old patients fare much worse than younger ones through the natural history of the disease (nature's ageism), elderly people stand to benefit proportionally much more than their juniors from technological advances.
Another example is the management of cancer.
In both Europe and the USA we know that patients are too often relegated to palliative care instead of being referred to an oncologist for possibly curative treatment.
It is hard to disagree with the claim that ‘advanced age is a risk factor for inadequate treatment’.
And what of that other ageism, the irrational variety based on fear, resentment, and revulsion?
Does this gerontophobia exist in our liberal and enlightened profession.
And if so, does it influence decision-making?
Facts are hard to come by; anecdotes are plentiful.
One thing is certain as we enter the next millenium, physicians, surgeons, and nurses in the developed nations must learn to enjoy working with elderly people.
Epidemiological hazards and lifelines
See page 27
Over 50 years ago, Bradford-Hill risked ‘damnable iteration’ by devoting three articles in his Principles of Medical Statistics to ‘fallacies and difficulties’.
Since then, epidemiological methods have advanced, and ease of access to computers has streamlined (certainly) and refined (seemingly) the collection, analysis, and presentation of data.
Why seemingly?
Papers that survive the hurdle of first peer-review at The Lancet are sent for statistical assessment.
Only just over half are coded acceptable or/acceptable after revision, the others being seriously flawed in design, analysis, or interpretation.
‘Could do better’, is the message for many authors, including, in our experience, those based in academic centres of excellence.
In this issue (p, 27), a nine-part series on epidemiological pitfalls in medical research begins.
The series is designed to inform and exemplify without mathematical wizardry, and is aimed at those who read scientific papers as well as those who write them.
The basics meet with the more complex: avoidance of glitzy analysis and presentation, importance of the  denominator, study design, bias, confounding, and inference; and even ‘is the study worth doing at all?’
This series would have been a non-starter without the help of Judith Glynn from the London School of Hygiene and Tropical Medicine.
She nurtured the concept and worked tirelessly with the contributors.
It looks different
Astute readers will note that our pages look different this week; and those who know about journal redesign might expect to see a trickle of editorial blood emerging from The Lancet's front door.
Not so; but what you see is the product of long debate and anxious reflection.
What are the main changes?
Editorials and Commentaries are where you read this, rather than in the middle of the journal; the typography has changed; now and then we break from two columns to three on the page; and there is a spot of blue on the cover.
Many lesser touches have been applied.
These changes are a response to the wishes of subscribers, as expressed in readership surveys, and of the editorial staff, as expressed in repeated cries of woe.
The chosen redesigners were Information Design unit of Newport Pagnell, UK; and we asked them to make The Lancet easier to read by attention to the typography, layout, and production techniques.
A panel of subscribers on whom we tested a dummy issue was torn between fondness for the old style and revolutionary zeal, the revolutionaries being in a clear majority.
Let us now see how readers react to the real thing.
The editors like it, and there is no blood on our floor.
COMMENTARY
Pseudomonas cepacia in cystic fibrosis: transmissibility and its implications
See page 15
Pseudomonas cepacia is an important and increasing cause for concern among patients with cystic fibrosis (CF), their families, and health professionals who look after them.
Why are they worried?
There is published evidence that P cepacia is transmissible, directly or indirectly, between individuals, and that in a proportion who acquire it the infection is associated with rapid deterioration or death, even when they were previously in good health and clinically stable.
Moreover, the P cepacia problem has emerged at the same time as social networks have developed between adult patients with CF, and their families; many individuals have derived great mutual benefit from these contacts, which provide for most of their social activity.
Development of national and international associations for CF adults and CF families (and the conferences, holidays, and local support groups they provide) has been welcomed.
In this issue (p 15) the paper by Govan and colleagues supports other work suggesting that transmission may occur as a result of social activities outside the hospital environment.
So, should these social activities be abandoned?
The epidemiological evidence presented in this paper and elsewhere suggests that P cepacia is transmissible between individuals by direct or indirect routes, that some individuals experience a change in clinical condition after acquisition of the infection, and that policies of segregation within CF centres have led to a reduction in incidence of P cepacia colonisation.
Whether P cepacia causes or is a result of clinical deterioration in newly colonised individuals remains uncertain.
Proving Koch's postulates would of course be unethical and controversy is fuelled by this lack of scientific certainty.
Meanwhile, individual CF associations and CF clinics are left to decide the way forward.
The response of hospitals has varied considerably — eg, introduction of full barrier nursing of all CF patients; segregation of inpatients and outpatients; no action other than separation for physiotherapy and sleeping; and expansion of home intravenous therapy programmes.
The personal consequences of complete isolation in hospital for patients and families who have previously socialised freely are potentially enormous.
Patients with P cepacia can likewise feel outcast if they are removed from the normal hospital CF clinic or ward, from their friends, and from familiar hospital staff.
And patients feel confused and angry at the differences between hospitals, unsure whether they are enduring isolation unnecesarily or being exposed to undue risk.
The US CF Foundation has ceased to advocate, support, or sponsor summer camps for all patients with CF, to reduce cross-contamination.
A consensus conference was held in April in Fillerval, France, where there was sufficient consensus as to the degree of risk that the need for urgent advice was perceived.
Consensus was reached that treatment centres should use cohorting, together with infection control measures, and other measures inside and outside hospitals as appropriate for the individual centre.
The issue of abolition of summer camps and social meetings was discussed, but consensus was not achieved (a report will be issued by the Association Française de Lutte Contre la Muscoviscidose).
The CF Trust has issued interim guidance about social contacts for CF in adults in the UK pending further research.
The Association of CF Adults (UK) recently held a fully segregated conference.
However, segregation depends on the ability of hospital laboratories to isolate P cepacia successfully, and on the facilities available to medical services to implement a policy.
Although guidelines to reduce the risk of cross-infection have been produced, there is continuing controversy and lack of information on the degree of risk conferred by various activities and the measures that need to be taken to reduce risk.
Epidemiological research suggests that P cepacia is a transmissible pathogen, that transmission is not confined to the hospital setting, and that the consequences of acquisition may be catastrophic for some patients.
Further research is urgently needed to establish the true degree of risk, not only of colonisation but also for clinical outcome in CF patients, and the optimum method  of reducing transmission in hospital and social settings.
We also need to know the way in which these factors might vary between strains, and further refine our ability to detect different strains.
Although patients need to be protected from risk, we must be sure that the precautions taken are justified and that social support networks that have taken years to develop are not unnecessarily shattered.
We need to pay attention to the social and psychological consequences of enforced isolation or segregation of patients.
Finally, patients and families need clear advice and accurate information to avoid the persistence of myth, the generation of fear, and the frustration of uncertainty.
Is chemotherapy for non-small cell lung cancer worthwhile?
See page 19
When is a toxic treatment useful as palliative therapy?
Before any therapeutic policy is adopted for a disorder, three questions must be answered.
The first two are straightforward — does the treatment produce a therapeutic benefit and if so, how much?— but the third is far more difficult — is it worthwhile?
The existence and magnitude of a beneficial effect of treatment can usually be determined by meta-analyses of unbiased trials, such as that of the treatment of early breast cancer.
In this issue, Souquet et al(p 19) describe a meta-analysis of combination chemotherapy for non-resectable non-small-cell lung cancer (NSCLC).
Their conclusion — that chemotherapy should be recommended for such patients — is at variance with current practice in many centres, so we need to consider whether it is reasonable.
In assessing the value of treatment, the impact on quality of life is just as important as duration of survival.
Unfortunately, in the NSCLC meta-analysis, the evidence on quality of life is incomplete, partly because it was not available in individual trials.
No summary data are presented and some of the individual trials did not measure quality of life at all.
In one, chemotherapy had no advantage in improving quality of life as measured by Karnofsky performance status.
On the other hand, the lower economic cost of one of the chemotherapy arms in the Canadian trial reflected decreased rates of hospital admission and thus provided indirect evidence of improved quality of life.
How should we marry assessment of quality of life with assessment of conventional trial endpoints to reach a sound treatment decision?
In advanced breast cancer, chemotherapy has been shown to improve both quality of life and conventional endpoints, and in such circumstances treatment can readily be advised.
The decision is more difficult with a treatment that modestly prolongs survival at the cost of worse quality of life.
The trade-off involves assigning relative values to periods of time in different health states, such as those with and without the side-effects of the treatment and with greater or lesser symptoms from the disease.
The Q-TWIST model, developed for early breast cancer, provides a general framework for this type of analysis and allows one to strike a balance by using any desired vaues for the effects of treatment and disease symptoms.
Even if no exact values can be assigned, such evaluation allows the definition of a ‘break even’ point beyond which one or other treatment is uniformly preferable.
If all plausible values fall on one side of this point, this result may clarify the issues and thus assist decision-making.
A different approach to evaluating the balance between quality and quantity of life is to ask the patient.
Although this is to some extent part of any clinical consultation, it can be approached more quantitatively.
This has been done in assessment of moderately toxic adjuvant chemotherapy for early breast cancer.
Patients who had experienced such treatment reported, on average, that small improvements in survival would make the treatment worthwhile.
This observation may suggest that what at first sight seems a limited survival advantage in the meta-analysis reported by Souquet et al may loom larger to the patients concerned.
We need more direct assessments of costs vs benefits of toxic therapies in this and other clinical settings.
Meanwhile, clinicians and patients who continue to face these decisions will be grateful for each new step that elucidates the factors involved.
Legionnaires' disease: the infective dose paradox
The species of Legionella bacteria that cause legionnaires' disease are widely distributed in aquatic habitats.
Infection via inhalation of aerosols containing bacteria is the main, though contested, mode of trasmission.
Most outbreaks have been associated with aerosols from evaporative cooling systems and complex hot water systems.
There are several gaps in our understanding of the transmission of legionnaires' disease.
One area of uncertainty is the size of the infective dose of the organism required to produce disease in human beings.
Animal experiments suggest that a high dose is required, and the fact that there is no person-to-person spread supports this view.
However, low concentrations of legionellae seem to be emitted from water systems, and epidemiological evidence indicates that infection can occur at some distance from the source of aerosol.
Environmental concentration of legionellae might have been underestimated because of technical obstacles to detection.
There are difficulties in culturing this slow-growing and unusually fastidious organism.
Thus, the preferred culture media contain antimicrobial agents that inhibit competing organisms.
One of these antimicrobial agents is cefamandole, but since some legionellae are themselves sensitive to this drug the concentration of organisms in water and air specimens could be underestimated.
Moreover, viable non-culturable but pathogenic forms of legionellae can occur in water systems.
Microbial air sampling, which is technically difficult and only occasionally useful in establishing the source of outbreaks of legionnaires' disease, has shown low concentrations of Legionella pneumophila in aerosol: 0.02 colony-forming units per litre of aerosol around a cooling tower and 0.19 colony-forming units per litre of air in shower aerosol.
However, the measure is of the number of organisms in aerosol at the time of testing and not the dose of organism to which the patient was actually exposed.
Furthermore, most air samplers are designed to detect small particles (about 5 m) and probably underestimate the concentration of particles of 10 m or more.
Extrapolation from experimental data from one study on the infective dose in animals provided an estimate of 14 million organisms required to produce disease in human beings.
Estimates of legionella concentrations around cooling towers (2–258 bacteria per 100 litres of air) signify that 9 years of inhalation would be required to produce human disease.
Another study suggested that a subject would need to breathe aerosolised air around a shower for 227 minutes to inhale one bacterial cell.
The epidemiological evidence suggests that the aerosol travels and remains infective at distances of at least 150 metres and perhaps 900 metres or more and that the closer the residence to a cooling tower the higher the risk.
But most pathogenic bacteria rapidly lose their viability in air and even in fully saturated conditions aerosols evaporate quickly, with desiccation of micro-organisms.
However, legionellae suspended in aerosol at 65% relative humidity in laboratory conditions can survive for at least 2 h, so it is possible that legionellae are also robust in air in natural conditions.
Legionellae survive better if suspended in aerosol with blue-green algae.
The estimates of infective dose were from studies in guineapigs, and this model might have been misleading since the lethal dose may be as small as 2400 organisms and as large as 100,000, although about 130 organisms can cause an infection.
In monkeys, 6 million organisms can induce fever and microscopic lung lesions but not macroscopic changes of pneumonia.
Rats and hamsters are easily infected but the disease is rarely fatal.
Many animal species have evidence of past legionella infection but mice, unless immunocompromised, are extremely resistant to it.
One explanation for the different susceptibilities is the different capacity of macrophages and lymphocytes to confer resistance.
Certain human beings, including those who are immunocompromised, may be unusually susceptible.
Overall, the animal evidence conflicts with the environmental and epidemiological observations.
One possibility is that the infective hazard of aerosols containing legionellae depends on the survival and stability of the bacterium, which in turn depend on several factors related to the bacterium itself (metabolic activity, effects of passage, and virulence of the strain) and its relation with other micro-organisms, notably blue-green algae and amoebae.
Legionellae in the enviroment are found as parasites of protozoa in which intracellular multiplication can take place.
Amoebae not only provide an important reservoir of intracellular legionellae, increasing uptake of the bacteria in adverse conditions; warm water systems they can also provide a continuous supply of extracellular bacteria.
Amoebae can be isolated from the air.
Droplets up to 100 m in size can be suspended in the air and travel for long distances, so a few deeply inhaled legionella-laden amoebae could cause infection, especially since legionellae successfully outgrow the host cell at body temperature.
Death of amoebae within the lower respiratory tract would result in the release of large numbers of bacteria at a temperature that favours their rapid multiplication.
Perhaps non-clinical legionellosis follows exposure to small numbers of bacteria alone, the clinical form occurring as a result of exposure either to a large dose of bacteria or to legionellae packaged in amoebae.
Solving the infective dose paradox might lead to new strategies for elimination of this preventable pneumonia.
Ischaemic preconditioning: can the protection be bottled?
Several years ago it would have been incomprehensible for a rational scientist or physician to conclude that myocardial ischaemia might somehow act to protect the heart from necrosis.
Then Murry et al reported their improbable observations that an isolated 40-min occlusion in dogs resulted in an infarct whose mass could be decreased by 75% if the animals were pretreated with four cycles of 5-min coronary occlusion plus 5 min of reflow.
Preconditioning with brief periods of ischaemia has now been shown to increase the heart's resistance to infarction in numerous species, including rabbit, rat, and pig.
Recognition of the benefits of ischaemic preconditioning has unleashed efforts to define its characteristics, study its mechanism, and apply the principles clinically.
In rabbit heart, full protection by ischaemic preconditioning is afforded by a single 5-min ischaemic period, but there is no protection with two 2.5-min ischaemic periods; this observation suggests a very sharp response threshold.
The reflow period following this brief ischaemia may be as short as 1 min, but protection wanes if the reflow is extended beyond 1 h.
A second preconditioning stimulus in rabbits can fully reinstate protection after the initial protection has waned.
Recent studies have given us insight into the mechanism of the anti-infarct effect of ischaemic preconditioning.
Adenosine is a ubiquitous product of ischaemia, and occupation of A 1 adenosine receptors seems to be the first step in preconditioning.
Agents that block adenosine receptors prevent the protective effect of preconditioning whereas intracoronary infusion of adenosine or A 1 -selective agonists mimics the effect.
Although adenosine receptors couple to several signalling systems in myocytes we now believe that protein kinase C (PKC) is the important one for preconditioning.
Inhibitors of this pathway such as staurosporine or polymyxin B block protection, and pretreatment with activators including phorbol myristate acetate or oleyl acetyl glycerol mimics preconditioning.
One of the novel findings of this research is the mechanism of the memory.
We believe that the heart ‘remembers’ that it has been preconditioned through a translocation of PKC from the cytosol into the cell membranes.
Population of A 1 receptors activates phospholipase C, which releases diacylglycerol in the membrane and this, in turn, triggers translocation and activates any PKC residing in the membrane.
Cytosolic PKC is inactive and can phosphorylate protein only after it has been translocated to the membrane.
Thus, the first ischaemic episode primarily translocates PKC into the membranes, where it will stay for about an hour.
Repopulation of adenosine receptors during the second occlusion can then result in phosphorylation of proteins early in the ischaemic period and these mediate the protection.
Colchicine, which disrupts the microtubules responsible for pulling PKC into the membrane, blocks protection completely.
Conversely, initiating translocation with phorbol myristate acetate protects, but that protection can still be blocked if an adenosine receptor antagonist is then administered; repopulation of adenosine receptors during the second coronary occlusion is required to reactivate the translocated PKC.
Preliminary data suggest that ischaemic preconditioning also occurs in human beings.
For example, the second coronary occlusion during the course of coronary angioplasty results in less evidence of myocardial ischaemia than the first.
Similarly, a bout of angina may be protective; but the protection may well be lost with recurrent angina.
Biological systems become tolerant to most A 1 -adenosine-mediated responses if receptors are continuously populated, and PKC is also known to downregulate quickly.
Thus, we were not surprised when we found that the effect of preconditioning was lost in rabbits that had forty or more 5-min occlusions over 3–4 days (unpublished observation).
Moreover, we could not maintain a continuously preconditioned state by infusing an A 1 -selective adenosine agonist into rabbits for 3 days.
How might one design a pharmacological approach to confer preconditioning's anti-infarct effect to patients?
Occupation of any receptor coupled to phospholipase C (and hence to PKC) in the heart should theoretically precondition it.
Thus, we find that the muscarinic agonist carbachol (through the M 2 -receptor) and the adrenergic agonist noradrenaline (through the 1 -receptor) are equipotent with adenosine as preconditioning agents.
Patients developing tolerance to adenosine-mediated protection because of recurrent anginal episodes may be amenable to preconditioning with either a muscarinic or an adrenergic agonist depending on the level in the signalling pathway at which tolerance occurs.
Unfortunately, all of these agonists have serious side-effects when given parenterally, and direct activators of PKC can be tumour promoters.
Perhaps additional opportunities for drug design will become apparent when we identify which phosphorylated protein is responsible for the anti-infarct effect.
Putting preconditioning in a bottle remains a pharmacological challenge.
Articles
Vitamin A supplementation in northern Ghana: effects on clinic attendances, hospital admissions, and child mortality
Summary
Although most studies on the effect of vitamin A supplementation have reported reductions in childhood mortality, the effects on morbidity are less clear.
We have carried out two double-blind, randomised, placebo-controlled trials of vitamin A supplementation in adjacent populations in northern Ghana to assess the impact on childhood morbidity and mortality.
The Survival Study included 21,906 children aged 6–90 months in 185 geographical clusters, who were followed for up to 26 months.
The Health Study included 1455 children aged 6–59 months, who were monitored weekly for a year.
Children were randomly assigned either 200,000 IU retinol equivalent (100,000 IU under 12 months) or placebo every 4 months; randomisation was by individual in the Health Study and by cluster in the Survival Study.
There were no significant differences in the Health Study between the vitamin A and placebo groups in the prevalence of diarrhoea or acute respiratory infections; of the symptoms and conditions specifically asked about, only vomiting and anorexia were significantly less frequent in the supplemented children.
Vitamin-A-supplemented children had significantly fewer attendances at clinics (rate ratio 0.88 [95% CI 0.81–0.95], p=0.001), hospital admissions (0.62 [0.42–0.93], p=0.02), and deaths (0.81 [0.68–0.98], p=0.03) than children who received placebo.
The extent of the effect on morbidity and mortality did not vary significantly with age or sex.
However, the mortality rate due to acute gastroenteritis was lower in vitamin-A-supplemented than in placebo clusters (0.66 [0.47–0.92], p=0.02); mortality rates for all other causes except acute lower respiratory infections and malaria were also lower in vitamin A clusters, but not significantly so.
Improving the vitamin A intake of young children in populations where xerophthalmia exists, even at relatively low prevalence, should be a high priority for health and agricultural services in Africa and elsewhere.
Introduction
An association between vitamin A deficiency and an increased risk of childhood morbidity and mortality has been reported in observational studies, and intervention trials in Asia have reported reductions in mortality ranging from 6% to 54%.
By contrast, a study in Sudan reported a small increase in overall mortality.
In the three studies that reported large reductions in mortality and listed causes of death, mortality rates for diarrhoea and measles fell but the rate for acute respiratory infections (ARI) did not change.
Two of three studies reporting on morbidity and mortality found no effect on the prevalence of any illness studied despite substantial falls in mortality (the other reported no significant effect on morbidity or mortality).
Clinical trials have shown that treatment with vitamin A reduces the severity of illness and mortality in children with measles, even in areas where eye signs of vitamin A deficiency are rare.
We have carried out two randomised, double-blind, placebo-controlled trials to assess the effect of 4-monthly large doses of vitamin A on childhood mortality (the Ghana Vitamin A Supplementation Trials [VAST]Survival Study) and morbidity (the VAST Health Study) in northern Ghana.
Subjects and methods
The trials took place in the guinea savannah area of Ghana in the Kassena-Nankana District, on the border with Burkina Faso (figure 1).
The area has a sub-Sahelian climate, and the mean annual rainfall was 852 mm during 1981 to 1990, with 90% of the rain falling between April and September.
The study populations were exclusively rural.
Extended families live in a group of houses (a compound) connected by a single outer wall, and surrounded by their farmland.
The main staple foods are millet, sorghum, and groundnuts, and the diet is deficient in carotenoids and vitamin A. Red palm oil, a major source of carotenoids in coastal West Africa, was found in only 2% of compounds during baseline surveys.
However, about a third of compounds owned at least one mango tree, and about 10% one or more pawpaw (papaya) trees.
Vitamin A deficiency and xerophthalmia are recognised as problems locally; words exist for night blindness in all the local languages and cases of xerophthalmia are reported by local ophthalmological services.
There was no vitamin A supplementation programme within the study area.
The district is underserved by health services; one hospital and three health centres, with limited outreach services, serve about 180,000 people.
Immunisation coverage and access to other maternal and child health services are correspondingly low.
All compounds in the study areas were included in the trials.
They were listed and marked on specially prepared maps (1/5000 scale) and a full census of all residents was carried out shortly before the start of each trial.
All children living in compounds within the  study areas were eligible to enter the trials at any of the 4-monthly dosing points, once they had reached the age of 6 months.
Children born from 1984 onwards were included in the Survival Study, but the Health Study restricted admission to children born from 1986 onwards.
The Survival Study trial was carried out between September, 1989, and December, 1991, and the Health Study trial between June, 1990, and August, 1991.
At enrolment, the dates of birth of the children and their mothers were assessed by means of detailed local events calendars, supplemented by any written documentation available.
Other information collected included an illness history, vaccination history, and anthropometric measurements on the child, the survival status of siblings, parental education, and mother's age.
For each compound, data were collected on the type of house, possessions, animals, and access to vitamin-A-rich foods.
At the start of each trial, blood samples were collected to establish baseline serum retinol concentrations.
In the Health Study, all children had serum retinol measurements at baseline and then a different, randomly chosen one-third sample was tested at each of the three subsequent dosing rounds.
In the Survival Study, a random, stratified sample of 520 compounds was selected and blood samples were taken from all the children living in these compounds at baseline and at the end of the trial.
A fingerprick blood sample was taken into a 1 mL amber microtube with serum separator gel (Becton Dickinson, Oxford, UK).
Samples were stored on ice and centrifuged within 12 h.
The serum was stored at -40°C in foil-covered tubes, then transported on dry ice to the Nutritional Biochemistry Laboratory in the Institute of Child Health in London, where the serum retinol content was measured by high-performance liquid chromatography (Spectraphysics, Hemel Hempstead, UK) with a 10 cm ODS2 column (Pharmacia, Milton Keynes, UK) and an ultraviolet detector (Phillips, Cambridge, UK).
The Survival Study area was divided into 185 geographical areas (clusters) based on boundaries such as roads, paths, or streams, with 30–77 compounds in a cluster (mean 51); the cluster was used as the unit of randomisation.
Overall, 92 clusters were assigned vitamin A and 93 placebo treatment.
In the Health Study, individual children were randomly assigned either vitamin A or placebo.
Randomisation was blocked in both studies to ensure similar numbers of children in each group in each part of the study area.
Randomisation was carried out in London by an independent statistician, who held the randomisation code and who also did an interim analysis of the mortality results from the Survival Study for the trial's data-monitoring committee after a year of follow-up.
Vitamin A and placebo were supplied by Hoffmann-La-Roche's Sight and Life Programme, and were similar in taste and colour.
In the Survival Study, liquid vitamin A or placebo was supplied in opaque 150 mL bottles containing 20 IU/mL vitamin E alone (placebo) or plus 100,000 IU/mL retinol equivalent as retinyl palmitate (vitamin A) in purified peanut oil.
Each bottle had a unique number, and was labelled with a cluster code before despatch to Ghana.
In the Health Study, vitamin A and placebo were supplied in gelatin capsules, individually packaged in opaque envelopes labelled with one child's unique identification number.
The capsules contained 200,000 IU retinol equivalent as retinyl palmitate (vitamin A) or peanut oil only (placebo).
Supplements were stored in sealed bottles at room temperature.
Random samples of the vitamin A bottles and of the capsules were returned for testing of retinol content by Hoffmann-La-Roche's laboratories in Basel; there was less than 20% loss of potency, even in vitamin A stored for up to 2 years.
In both trials, at each dosing round, children aged less than 6 months were not eligible, children 6–11 months old received 100,000 IU retinol equivalent or placebo, and children 12 months or older were given 200,000 IU retinol equivalent or placebo.
Vitamin A was given to all children during the final dosing round of each trial.
In the Survival Study, children were visited and dosed by trained fieldworkers every 4 months for 2 years in seven survey rounds.
At each visit each child was recorded as being present, temporarily absent, moved away, or dead.
A child was classified as being temporarily absent if he or she was not in the compound when visited at least three times during a week.
The parents of children found to be suffering from any illness at a fieldworker's visit were advised to take them to the nearest health facility for diagnosis and treatment.
In the Health Study, morbidity surveillance was based on weekly home visits by fieldworkers.
At each visit, a detailed interview was conducted about the occurrence on each day of the week of 21 listed symptoms, signs, and conditions, using locally defined terms where appropriate.
Recall was aided by use of a pictorial daily health diary, on which mothers were taught to record that the child was healthy or to mark the occurrence of diarrhoea, cough, or any other illness.
Further questions and observations were made to assess the severity of any diarrhoeal or respiratory illnesses reported (to be published elsewhere).
The Health Study area had no static health facility, but weekly mobile clinics were held, at which study children were offered a highly subsidised service.
At their weekly visits, fieldworkers were  instructed to refer ill children to the clinics, according to specified criteria.
All study children who attended the clinic were seen by a physician, who recorded the diagnoses and treatment.
A standard, detailed clinical assessment was also carried out on a subsample of these children.
Children presenting with acute respiratory infections were referred to the district hospital for chest radiography, and severely ill children were admitted to hospital, where they were treated by standard protocols, and monitored daily by the study physician.
Direct admissions of study children to the district hospital were also notified to the physician for assessment, monitoring, and treatment by the same standard treatment protocols.
Children were screened for signs of xerophthalmia every 4 months.
In the Survival Study, fieldworkers at their 4-monthly home visits screened children for suspected night blindness, Bitot's spot, or corneal abnormality, and referred affected children for further assessment by a study physician.
In the Health Study, each child was examined every 4 months by a physician.
All children with confirmed active xerophthalmia or its sequelae (corneal scars) were withdrawn from the trial at diagnosis.
They were given three large doses of vitamin A over a week, and received vitamin A in all subsequent survey rounds.
Because of the frequent follow-up in the Health Study, we could detect and treat cases of measles with vitamin A. These children were also excluded from the trial from that point.
Child deaths were identified at the time of the home visits, and also through a network of about 100 key informants — members of the study community who were asked to record all pregnancies, births, and child deaths in their area.
Information was collected from the informants every 2 weeks.
Specially trained staff interviewed the parents or the nearest relative of any study child who died.
They recorded information on the circumstances of the death and the symptoms and signs that preceded it on a form that included both an open history of the final illness and screening questions for the presence of common symptoms, followed by the application of appropriate modules with precoded answers.
More than 90% of interviews were done within 7 months of the child's death (median interval 2.9 months).
Verbal autopsy questionnaires were examined independently by three physicians who assigned the cause of death to one of eleven categories, which included ‘miscellaneous’ and ‘unknown’.
If at least two of the three physicians assigned the death to the same category, it was taken as the cause, otherwise the cause was classified as ‘unknown’.
Several methods were used both to promote and to check on data quality.
These included weekly visits to each fieldworker by a supervisor, who observed interviews, revisited compounds, and either conducted full re-interviews or checked a more limited set of information on a sample of recently completed interview forms.
Meetings were held weekly between supervisors and headquarters field and data-processing staff, and at least once every 2 weeks between groups of fieldworkers and their supervisors to ensure good two-way communication.
Both fieldworkers and supervisors received regular training courses.
Completed forms were checked manually by the supervisors, before independent data entry into computers by two clerks.
After typing errors had been corrected, the data were checked for range and consistency.
Any discrepancies that could not be resolved in the headquarters led to a revisit to the child's home and checking of information on the relevant variables.
Regular computer analyses of the results reported by each fieldworker were done to identify workers whose results differed substantially from those of their colleagues; they were given extra supervision and, where necessary, retraining.
All data entry and processing were done in the study headquarters in Navrongo with IBM PS/2 computers and dBase III+ software.
Analyses were carried out in Navrongo and London with dBASE III+, Epi Info 5.00, SPSS PC+ 4.0, and GLIM 3.77.
Two-tailed significance tests were used throughout, and continuity corrections applied where appropriate.
Preliminary results were reported to the study communities, the district and regional health services, and the Ministry of Health of Ghana within 4 months of the end of data collection.
Children who were at least 6 months old entered the trials at the first dosing round or at a subsequent round and left at the last dosing round, or when they moved out of the study area, died, or were withdrawn from the trial.
Confirmed xerophthalmia cases were excluded from the date of diagnosis.
In the Survival Study, children could move from one treatment group to the other by moving between clusters with different treatment assignments.
Such children were excluded from the date they received their first dose of the different treatment.
Since children were randomised to treatment group by cluster in the Survival Study, the results, both of the baseline comparisons and the mortality rates, were analysed by a comparison of the mean of the results in each of the 92 vitamin A clusters against the mean of the results in each of the 93 placebo clusters.
Each cluster's mortality rate was calculated by dividing the number of deaths by the child-years of follow-up.
The ratio of the mean mortality rate in the vitamin A and placebo clusters was used to measure the mortality impact of vitamin A supplementation, with 95% CI calculated by the method of Armitage and Berry.
An independent data-monitoring committee reviewed the mortality results from the first 12 months of follow-up in the Survival Study, and recommended the continuation of the trial to the end of the planned 24 months of follow-up.
The results of this interim analysis were not revealed to the members of the study team until after the trial.
In comparing mean daily prevalence rates of morbidity between vitamin A and placebo groups in the Health Study, separate daily prevalence rates were calculated for each child for each of the intervals between dosing points.
These were then averaged to give overall mean rates for each treatment group.
The CIs for the rate ratios were calculated by means of a formula for determining the confidence interval of the ratio of two normal means.
Ethical approval for both trials was obtained from the Ministry of Health of Ghana, and from the ethics committee of the London School of Hygiene and Tropical Medicine.
The trials were explained in detail to all the local, regional, and district authorities and paramount chiefs, who gave their approval.
Efforts were made to keep the communities informed of the studies' purposes, and of any new developments, through meetings open to all community members, and by announcements at market places.
The trial was also explained and consent was sought from the head of each compound and the parents or guardians of each eligible child before enrolment.
Results
The prevalence of xerophthalmia at baseline was 0.7% in the Survival Study children and 1.5% in the Health Study children.
In each study, the rates in the vitamin A and placebo groups were similar.
The proportions of children with low baseline serum retinol concentrations were substantial in both trial populations.
In the Survival Study, 14.4% were severely deficient (<0.35 mol/L) and 42.5% moderately deficient (0.35–0.69 mol/L); the equivalent rates were 15.8% and 57.6% among the Health Study children.
The treatment groups were compared at baseline with respect to more than fifty characteristics.
In both trial populations, the treatment groups were significantly different (p<0.05) for only two variables, which is the number expected by chance.
No parent refused permission for a child to enter the trials.
Losses to follow-up were less than 10% in both trials (figure 2) and similar in the treatment groups.
The 21,906 children who entered the Survival Study were followed up for 33,287 child-years (16,508 vitamin A group, 16,779 placebo group).
Dosing compliance was similar in the two groups; an average of 89.5% of eligible children were successfully treated in each round.
The commonest reason for missing a dose was absence from home at the time of the fieldworker's visit.
Dosing errors were rare (less than 0.7% of all doses given).
Most were due to mislabelling of bottles.
1455 children entered the Health Study and were followed up for 1185 child-years (596 vitamin A group, 589 placebo group).
Morbidity information was missing for 5.7% of the weekly follow-up visits owing to temporary absences of the study children or their mothers, but the missing data were equally distributed between the treatment groups.
At each dosing point, an average of 94.7% of eligible children received the supplement or placebo.
There were only two significant differences between the vitamin A and placebo groups in the mean daily prevalence of the symptoms/conditions investigated at the weekly visits in the Health Study (table 1)— prevalence of vomiting and refusal of food or breastmilk.
In the Health Study clinic attendance rates were significantly lower in the vitamin A group than in the placebo group (1193 vs 1341 attendances, rate ratio 0.88 [95% CI 0.81–0.95], p<0.001).
A child in the vitamin A group was significantly less likely than a placebo-treated child to have made several clinic visits (p=0.019); for example children in the vitamin A group were 27% (95% CI 4–45%) less likely to have attended the clinic 3 or more times during a 4-month dosing interval.
Similarly, hospital admission rates were significantly lower in the vitamin A group than in the placebo group (36 vs 57 admissions, rate ratio 0.62 [0.42–0.93], p=0.02).
The rate ratio for admission was similar when only the first admission of each child was counted (27 vs 42, rate ratio 0.65 [0.41–1.04], p=0.09).
There were 892 deaths among the children in the Survival Study, which gave an overall mean mortality rate for all clusters of 27.1 per 1000 child-years of follow-up.
397 of the deaths were in vitamin A clusters (mean mortality rate 24.4 per 1000 child-years) and 495 in placebo clusters (29.9 per 1000 child-years).
The ratio of the mean mortality rates was 0.81 (95% CI 0.68–0.98, p=0.03, table 2).
The ratio of mean mortality rate was smaller in boys than in girls (0.73 [0.59–0.92]vs 0.90 [0.71–1.15]) but the difference was not significant (p=0.3).
A protective effect of vitamin A was apparent in five of seven age groups, but there was no consistent trend in the size of the ratio by age.
A probable cause of death was established for 697 (78.1%) of the 892 deaths (table 2).
The mortality rate due to acute gastroenteritis was significantly lower in the vitamin A clusters than in the placebo clusters.
Mortality rates for all the other causes of death except acute lower respiratory infections and malaria were lower in the vitamin A group but none was significant at p<0.05, and, overall, the difference in the size of the effect of vitamin A supplementation by cause was not statistically significant.
There were 26 deaths among trial children in the Health Study, 6 in the vitamin-A-supplemented group and 20 in the placebo group.
Discussion
These trials broke new ground in several ways.
The two trials were carried out in adjacent populations in West Africa; one looked in detail at the effect of vitamin A supplementation on morbidity and the other at its effect on mortality.
We assessed the effect of vitamin A supplementation on the care sought for illness (clinic attendances and hospital admissions).
The trials were carried out in a population with xerophthalmia rates that were very close to the threshold used by WHO to define a population as having a xerophthalmia problem of public health significance (1%); previously reported studies were done in populations with substantially higher rates of xerophthalmia.
Our results may explain the puzzling results from at least two previous trials, which failed to find an impact on morbidity even in the presence of a large effect on mortality.
Those studies collected data on a limited number of symptoms/conditions, without detailed information on their severity or the care-seeking provoked.
We also found no effect on reported morbidity, though there was a suggestion that the prevalence of vomiting and refusal of food or breastmilk, both of which tend to be associated with severe episodes of illness, were lower in the supplemented children.
However, we did find a strong influence on the occurrence of episodes severe enough to lead the mother to take the child to a clinic, and those that subsequently resulted in the child being admitted to hospital, as well as an effect on mortality.
Thus, it seems that vitamin A supplementation reduced the frequency of severe and lethal illnesses without decreasing the frequency of less severe illnesses.
Vitamin A has two main effects on the immune system — enhancement of non-specific immunity by maintaining the physical and biological integrity of epithelial tissue as the first barrier to infection, and increasing the effectiveness of the immune response to infection once the epithelial barrier has been breached.
The first should result in a lower incidence of infections, possibly in combination with decreased severity due to a lower average ‘dose’ of pathogens crossing the mucosal barrier, whereas the second will tend towards a decreased severity, but not incidence of infection.
The overall observed impact of supplementation will depend on the relative enhancement of these two components of the immune system in the population.
Our results imply that the effects of vitamin A supplementation on the immune response to infection after the epithelial barrier has been breached are of greater functional significance than the effects on the integrity of the epithelial barrier itself.
The trials are the first to show an effect of prophylactic vitamin A supplementation on morbidity or mortality in Africa.
The only other similar trial in Africa (in Sudan) did not show an effect of vitamin A supplementation on child survival (relative risk=1.06 [95% CI 0.82–1.37]).
The investigators speculated that the lack of effect in their study population was due to the 6-monthly rather than 4-monthly dosing schedule, the low mortality rate in the control group, or the postulated lack of other nutrients, such as fat or zinc, in the diet.
Our findings of a 19% reduction in all-cause mortality with vitamin A supplementation and the substantial reductions in clinic attendance rates and hospital admissions are consistent with the results of five large trials in Asia, and observational studies in Asia.
Only one trial in Asia did not find significantly lower child mortality with vitamin A supplementation; however, even in that trial, the lower 95% confidence interval on the ratio of the mortality rates was below 0.81.
The all-cause mortality results of our trials are important for two reasons.
Firstly, they showed that improving the vitamin A intake of at least some populations of young African children can substantially reduce their mortality.
Secondly, this effect on all-cause mortality was in a population that had a vitamin A deficiency problem of marginal public health importance; baseline xerophthalmia rates were substantially higher in the Asian trials.
Nonetheless, there was evidence that vitamin A supplementation reduced the incidence of xerophthalmia.
Our study found a rate ratio of 0.66 for acute gastroenteritis deaths and of 0.67 for deaths due to chronic diarrhoea, malnutrition, or both.
A reduction of at least 30% in child mortality due to diarrhoeal diseases has now been found by all trials that have found a significant effect on overall mortality and have also examined causes of death.
By contrast, we found no reduction in deaths attributed to acute lower respiratory infections in the vitamin-A-supplemented group.
The effect of vitamin A supplementation on such deaths has not been consistent in previous trials, though none has found a significant change.
Similarly, we found no reduction in deaths due to malaria.
Malaria was probably a much more important immediate and underlying cause of death in this population than in any other trial population, with the possible exception of the Sudan, where ‘fever’ was the second commonest symptom associated with death.
The other major cause of death in our trial was measles, in which the ratio of the mortality rates was 0.82, though this ratio did not significantly differ from 1.0.
The results of this study have important health policy implications.
They show that improving the vitamin A intake of young children in areas where xerophthalmia exists, even at low prevalence, should be a high priority for both health and agricultural services.
If routine interventions can be devised that effectively improve vitamin A status, the burden of xerophthalmia, other severe illness, and mortality in children will be substantially reduced.
As well as these direct benefits to the population, there will be substantial indirect benefits owing to substantial reductions in clinic attendances and hospital admissions.
Health services appropriate a significant proportion of national budgets, and the economic and social costs incurred by the family of an ill child are also large.
Since the impact of improvements in vitamin A status is likely to be related to the extent of deficiency in the population, there is an urgent need for surveys of the prevalence of vitamin A deficiency and xerophthalmia.
Remarkably little is known about the extent of the problem in most countries in Africa, the Middle East, Asia, and Latin America.
It will then be important to gather information on local dietary and agricultural practices and beliefs to assess the feasibility of improving the production, storage, and consumption of local vitamin-A-rich foods, the introduction of new foods, or fortification of a locally consumed food.
Other potential strategies include the provision of vitamin A supplements to at-risk groups.
Although likely to be the least cost-effective strategy for the improvement of vitamin A status in the long term, even this approach is a highly cost-effective child survival intervention.
The Ghana Vitamin A Supplementation Trials (VAST) were a collaborative research project between the London School of Hygiene and Tropical Medicine and the School of Medical Sciences of the University of Science and Technology, Kumasi, Ghana, with support from the Ministry of Health of Ghana.
The project was funded by the Health and Population Division of the UK Overseas Development Administration.
Preliminary exploratory studies were supported by the Wellcome Trust and Save the Children Fund (UK).
Hoffmann-La-Roche's Sight and Life programme supplied the vitamin A and placebo.
We thank all the VAST field, laboratory, computer centre, and administrative staff, especially Mr Martin Adjuik, Mr Azumah Amidini, Mr J Kwabena Badu, Ms Margaret Gyapong, Mr Eric Kasise, Mr Ogyebre Owusu-Agyei, and Mr David Pendlebury; the population of Kassena-Nankana District and their leaders; the Regional and District Administration and Health Services; Dr Moses Adibo and Dr Sam Adjei,(Ministry of Health of Ghana) for support and encouragement; Prof Patrick Vaughan and other colleagues at the LSHTM for support and advice; Ms Nina Saroi for administrative and secretarial support in London; Dr Sandra Saenz de Tejada and the WHO-ARI Programme for preliminary study of local concepts and terminology of illness; Ms Gilly Maude for the randomisation and interim analysis; Ms Sharon Huttly, Mr Ben Amenuvegbe, and the late Mr Steve Tulloch for helping to design the data management system; Dr Hazel Inskip and Mr Jerry Wheeler for advice on data processing; Ms Penny Fennell for help with the training of the computing and secretarial staff; Ms Rebecca Abbott and Dr Suzanne Filteau (Institute of Child Health, London) for measuring serum retinol; Dr Andy Hall for helping to code the cause of death questionnaires; and the members of the data monitoring committee for reviewing the interim results.
Effects of oxygen on dyspnoea in hypoxaemic terminal-cancer patients
Summary
Dyspnoea is a frequent and devastating symptom of advanced cancer.
The purpose of this prospective, double-blind, crossover trial was to assess the effects of supplemental oxygen on the intensity of dyspnoea.
14 patients with hypoxaemic dyspnoea due to advanced cancer were randomised to receive either oxygen or air; the gases were delivered at 5 L/min by mask.
After 5 min of stable oxygen saturation (pulse oximetry), patients were crossed over to receive the other treatment.
The crossover was repeated twice.
Dyspnoea was assessed with a visual analogue scale (0=no dyspnoea, 100=worst dyspnoea).
Mean difference in dyspnoea visual analogue scale between air and oxygen treatment was 20.5 (95% confidence interval 13.5 to 27.6).
12 patients consistently preferred oxygen to air; similarly, the investigator consistently chose oxygen for the same 12 patients.
In a global rating questionnaire, patients reported little or no benefit during the air phase compared with moderate to much benefit during the oxygen phase.
We conclude that oxygen is beneficial to patients with hypoxia and dyspnoea at rest.
Introduction
Dyspnoea has been defined as an uncomfortable awareness of breathing.
The symptom occurs in 29–74% of patients with terminal cancer and is perceived as one of the most devastating features by patients and their families.
In patients with chronic obstructive pulmonary disease (COPD), the effects of supplemental oxygen on the subjective sensation of dyspnoea have been controversial.
While some investigators found that oxygen gave direct symptom relief, others suggested that any benefit was mostly the placebo effect of air on the rhinopharynx.
Oxygen therapy has also had controversial results when given to patients with congestive heart failure.
However, in patients with terminal cancer the normal cause of dyspnoea is restrictive pulmonary failure.
Could this different pathophysiological basis influence the response to oxygen?
We did a prospective, double-blind, crossover study to assess the effects of supplemental oxygen in patients with hypoxaemic dyspnoea and terminal cancer.
Patients and methods
Of our 107 patients with cancer dyspnoea, 18 were capable of participation in this investigation.
14 of these 18 patients with dyspnoea and hypoxaemia due to advanced cancer agreed to enter the study.
All had terminal disease (defined as no available antineoplastic therapy).
All patients had normal cognitive function (Mini-mental State Questionnaire Score of at least 24/30).
In all cases, hypoxaemia (defined as <90% saturation, measured by pulse oximetry) occurred when the patients breathed room air for more than 5 min.
All patients were receiving oxygen via nasal prongs at a rate of less than 4 L per min.
None of the patients had a clinical history of COPD.
All patients gave written consent.
Patients had a baseline assessment after at least 30 min of bed rest and a minimum of 5 min of stable oxygen saturation when breathing room air.
Dyspnoea was measured with a visual analogue scale (VAS; 0=no shortness of breath, 100=worst shortness of breath).
This method is reliable in the assessment of the intensity of dyspnoea.
Respiratory rate was measured for 1 min twice, and the two results were averaged.
An index of respiratory effort was also calculated.
Patients were assigned a score of 1 (respiratory rate <20/min), 2 (20–25/min), 3 (26–30/min) or, 4 (rate >30/min).
1 point was added if use of accessory respiratory muscles was observed, and 1 point was added if the patient became cyanosed.
Results could range from 1 (minimum effort) to 6 (maximum effort).
Pulse oximetry was used to monitor oxygen saturation and heart rate.
After the baseline assessment, patients were randomised to receive oxygen or air; both gases were delivered by mask at 5 L/min.
After 5 min stable oxygen saturation (pulse oximetry), the assessment was repeated.
Patients were then crossed over to the opposite treatment.
Another assessment was done after 5 min stable oxygen saturation.
Both the patient and the investigator were then asked to choose blindly in which phase patients did not have dyspnoea.
After this blinded choice, patients were randomised again to receive air or oxygen.
They were crossed over to the opposite treatment after a fourth assessment; patients and investigators then chose blindly the more effective gas for the dyspnoea.
In summary, patients were randomised to receive 2 phases of oxygen therapy and 2 phases of air therapy and made 2 blinded choices of the more effective treatment.
At the end of each of the two crossover phases, patients rated globally the difference in the intensity of their dyspnoea at baseline and during the therapies.
The global rating scale was used as described by Jaeschke.
Results were analysed with the paired t test for crossover trials as described by Hills and Armitage.
The choices of patients and investigators were analysed binomially.
Patients were counted as having chosen oxygen only if it was their choice on both occasions.
Results
All 14 patients (6 female, 8 male) could be evaluated on completion of the study.
The patients' median age was 64 (range 49–79); cause of dyspnoea was a lung tumour (5 patients), lung metastases (6), pleural effusion (2), and   carcinomatous lymphangitis (1).
Oxygen saturation, respiratory effort, respiratory rate, and the visual analogue scale for dyspnoea were all significantly better with oxygen than with air.
Results did not differ between air and the baseline assessment.
No residual or period effects were detected with Hills and Armitage test (table 1).
Patients chose oxygen twice in 12 cases, no preference then oxygen in 1 case, and oxygen then no preference in 1 case (p<0.001, binomial distribution).
The investigator chose oxygen both times for the same 12 patients, air then oxygen for 1 patient, and air then air for 1 patient (p<0.001, binomial distribution).
Table 2 summarises the global rating by the patient after the completion of the first randomisation.
Patients reported little or no benefit during the air phase compared with moderate to much benefit during the oxygen phase (p<0.01).
Discussion
Our results suggest that oxygen supplementation decreased the intensity of dyspnoea in patients with terminal cancer.
All our patients had measurable stable dyspnoea while resting.
Only a small proportion of our patients could and would participate in the study.
However, we believe that our results may be more reliable than those obtained during tests such as walking or treadmill exercises.
These tests do not always accurately reproduce the disease symptoms.
All patients had hypoxaemic dyspnoea and no history of COPD.
Patients had more consistent benefit from oxygen than hypoxaemic patients with such pulmonary disease in previous studies.
In congestive heart failure, increased inspired oxygen concentrations improved the subjective scores for fatigue and breathlessness, although the oxygen saturation was more than 90% even in those patients who breathed room air.
Other studies have not been able to reproduce these results.
Oxygen has also been found to reduce dyspnoea during exercise in normal subjects and in athletes.
The effects of oxygen on subjective dyspnoea in cancer patients with normal oxygen saturation should be studied in the future.
Reports from hospices have recommended against the use of oxygen for dyspnoeic cancer patients.
Although our results need to be replicated, they suggest that at least those patients with hypoxia and dyspnoea at rest get significant benefit from supplemental oxygen.
Future research should define the patients who would be likely to benefit most from oxygen therapy.
The ideal level of oxygen saturation and of oxygen administration (continuous vs ‘as needed’) should also be established in future trials.
Evidence for transmission of Pseudomonas cepacia by social contact in cystic fibrosis
Summary
Pulmonary colonisation with Pseudomonas cepacia in patients with cystic fibrosis can be associated with increased morbidity and mortality.
The modes of transmission of P, cepacia are, however, unclear.
We used selective media and phenotypic and genomic typing systems to investigate the acquisition of P cepacia by adults with cystic fibrosis.
An analysis of isolates from 210 patients attending regional clinics in Edinburgh and Manchester between 1986 and 1992 showed that the main cause of increased isolations of P cepacia from 1989 was the emergence of an epidemic strain that had spread between patients in both clinics.
Epidemiological evidence indicated that social contact was important in spread of the epidemic strain within and between clinics.
We suggest that guidelines to limit the acquisition of P cepacia should not be restricted to patients in hospital, and that intimate or frequent social contact is associated with a high risk of cross-infection.
See Commentary page 3
Introduction
The acquisition of Pseudomonas cepacia is a major concern among patients with cystic fibrosis (CF) and their carers.
Case-controlled studies have shown that pulmonary colonisation with P cepacia is associated with increased morbidity and rapidly fatal deterioration.
However, the exact pathophysiological role of this organism remains controversial.
Some clinicians and microbiologists continue to believe that P cepacia is a marker for, rather than the cause of, pulmonary deterioration.
In the past decade, some CF centres have reported increased isolations of P cepacia that cannot be explained only by improved laboratory proficiency in culture and identification.
Epidemiological evidence also suggests that P cepacia is transmissible.
Thus, pressure is increasing to identify sources and modes of transmission, and to segregate colonised from non-colonised patients.
The modes of transmission of P cepacia are unclear.
Previous reports have focused on the evidence for direct cross-infection from patient to patient, and indirect nosocomial transmission from contact with contaminated equipment.
Some centres practise barrier nursing and geographical segregation of colonised patients from non-colonised patients as inpatients and outpatients by the use of separate wards and clinics, but these do not eliminate cross-infection.
Evidence of transmission between 2 patients while at a CF educational camp suggests that social contact outside the hospital may be an important means of acquisition.
A key question concerns the types of social contact that may be associated with a high risk of transmission of P cepacia.
Between 1986 and 1989, isolations of P cepacia from the sputum of CF patients who attended regional adult clinics in Edinburgh and Manchester in the UK remained low despite the use of reliable selective culture media and identification procedures.
The organism was not believed to be a clinical problem.
From 1990, however, isolations of P cepacia increased substantially in Edinburgh and Manchester.
We report bacteriological and epidemiological evidence for spread of the same highly transmissible strain of P cepacia (here called the epidemic strain) by social contact within and between both clinics.
Methods
From 1986, sputum from every CF patient who attended the Edinburgh and Manchester CF clinics was routinely cultured for P cepacia with selective media and identification procedures.
The clonal relation between individual isolates was assessed by bacteriocin typing, ribotype analysis, and pulse-field gel electrophoresis with the clamped homogeneous electric field (CHEF) technique.
Patients with P cepacia were questioned about their social contacts with other CF patients.
Clinical details, dates of hospital admissions, and contacts with other patients at outpatient clinics were also recorded.
The presence of airborne P cepacia in the  hospital waiting room before and immediately after occupation by 3 P cepacia-colonised patients was investigated with a selective culture medium (Mast Diagnostic Ltd, Bootle, UK) in association with a bacterial sampler (Casella Ltd, London, UK).
Air sampling was also done within 1 metre of each patient during physiotherapy.
Bacterial swabs were taken from the hands of doctors, nurses, and physiotherapists before and after the clinic; the hands of patients before and after spirometry; spirometry mouthpieces; and the arms of a wooden chair grasped by patients during spirometry.
Bacterial swabs were cultured for P cepacia with selective and enrichment media.
Results
The epidemic strain (CF5610) was first cultured from sputum of a CF patient in August, 1989, and identified as P cepacia by the API 20NE system (API-bioMérieux, Marcyl' Etoile, France) at the 99.7% confidence level.
This culture, and all subsequent isolates that were identified as the epidemic strain, belonged to bacteriocin type S3/P0:ribotype A, and shared the same CHEF profile.
Additional and unusual phenotypic characteristics of the epidemic strain included the appearance of dry colonies, production of a brown melanin-like pigment that was most apparent during growth on media containing 1% tyrosine, lack of the exoenzyme C14 lipase (API ZYM system, API-bioMérieux), and absence of smooth lipopolysaccharide.
The epidemic strain also exhibited resistance to tobramycin, ciprofloxacin, co-trimoxazole, chloramphenicol, temocillin, imipenem, meropenem, and to the new quinolones PD 127391 and PD 131628.
Epidemiology of P cepacia in Edinburgh
The number of patients attending the Edinburgh adult CF clinic increased from 20 in 1986 to 60 in 1992.
During this time, P cepacia was cultured from 22 CF patients (12 women; aged between 16 and 27 years).
2 patients were colonised transiently and produced only single cultures of P cepacia strains that were phenotypically and genotypically distinct from the epidemic strain (figure 1A, patients E1 and E5).
All 13 patients who acquired the epidemic strain remained colonised permanently.
The prevalence of P cepacia increased from a single case of transient colonisation in 1986 to 16 cases (26.7%) in 1992.
No patients were colonised in 1987.
The epidemic strain was responsible for 1 of 5 patients colonised with P cepacia in 1989, and 12 of 16 cases by 1992.
The clinical significance of colonisation by the epidemic strain varied from patient to patient.
The index case, patient E7 (figure 1A), has remained colonised for almost 4 years with only very slight pulmonary deterioration.
In contrast, patient E22, who was awaiting lung transplantation, deteriorated rapidly and died within 6 weeks of colonisation.
Between 1990 and 1992, 7 deaths (5 female) occurred: 6 were colonised with the epidemic strain; and 1 (patient E4) had a different strain of P cepacia and died after heart-lung transplantation.
In the same period, there were no deaths among P cepacia-negative patients.
All sputum cultures from the index case have continued to grow the epidemic strain.
He attended the adult CF clinic regularly and all previous sputum cultures, including a specimen in June, 1989, were P cepacia-negative.
Before March, 1990, he had not travelled outside Scotland and there was no evidence of CF contacts other than with Scottish patients.
Between June and October, 1990, 4 other CF patients (figure 1A, patients E8-E11) became colonised with the epidemic strain.
This transmission was the first evidence of cross-infection with P cepacia in Edinburgh CF patients.
All 5 of these patients had been in hospital between August, 1989,(the first isolation of P cepacia) and their acquisition of the epidemic strain, which prompted surveillance for P cepacia contamination in the hospital outpatient environment and analysis of social contacts of patients.
Surveillance studies
Bacterial surveillance of the outpatient clinic failed to culture P cepacia from air sampled during physiotherapy, in the waiting room, or from the hands of doctors, nurses, and physiotherapists either before or after the clinic.
In contrast, the hands of 3 patients who harboured the epidemic strain in their sputum became heavily contaminated with this organism after spirometry and associated coughing.
In addition, the disposable spirometry mouthpiece filters used by each of these patients became heavily contaminated.
P cepacia was not recovered from the external surface of the spirometer handpiece, or from the arms of a wooden chair that had been grasped by each patient during spirometry.
Social contacts
Analysis of social contacts between the index case, patient E7, and patients E8–E11 revealed that all 5 patients lived in East Fife, an area north of Edinburgh, and that all were regular attenders at a weekly fitness class held in a local school (figure 2).
Patients E8–E11 had been regular attenders since the class began in August, 1989, but patient E7 did not attend until early 1990.
Subsequently, 4 other patients from East Fife, who had also become regular attenders at the class, became colonised with the epidemic  strain (figure 1A, patients E12, E13, E15, and E17) so that by November, 1991, all 9 members of the fitness class were colonised.
Patient E12 had only attended the class for two months in early 1991 during an intimate relationship with patient E8.
Social contact between E17 and other patients was confined to attendance at the fitness class.
4 patients (E9, E10, E13, and E15) had regular social contact outside the fitness class.
In December, 1991, the class was disbanded after the death of patient E10, and epidemiological investigations were halted.
Non-nosocomial transmission of P cepacia has implications for CF adults involved in intimate relationships.
In October, 1991, patient E16 became colonised by a strain of P cepacia that showed no phenotypic or genotypic relation to the epidemic strain.
In March, 1992, however, in addition to the original P cepacia strain, his sputum cultured the epidemic strain for the first time,.
an event that caused considerable anxiety to himself and to his girlfriend (patient E22) who was P cepacia-negative.
Patient E22 had no direct contact with the Edinburgh adult CF facility, although, aged 21, she still attended the separately located Edinburgh CF paediatric facility, where between 1986 and 1992, only 1 case of P cepacia colonisation (not involving the epidemic strain) had been found.
After much discussion, and aware of the risks, the couple decided to continue their relationship.
In May, 1992, patient E22 became colonised with the epidemic strain.
She died six weeks later.
Although patient E16 harboured two strains of P cepacia, only the epidemic strain was transmitted to patient E22.
Patient E16 experienced grief and a period of self-neglect after the death of E22 and became profoundly unwell.
Despite aggressive antibiotic therapy, the epidemic strain continued to be isolated from his sputum and subsequently from blood cultures.
He died one month later.
Patient E16 had only been in hospital once, three years before his terminal illness.
Neither of these patients was co-colonised by P aeruginosa.
In fact, only 8 of the 13 (62%) patients colonised with the epidemic strain were co-colonised with P aeruginosa.
Epidemiology of the epidemic strain in Manchester
The number of patients attending the Manchester adult CF clinic increased from 80 in 1986 to 150 in 1992.
The prevalence of P cepacia increased from a single case (1.2%) in 1986 to 38 (25.0%) by 1992 (figure 1B).
In September, 1990, the epidemic strain was isolated in Manchester for the first time (patient M6) and by the end of that year was responsible for 4 of the 11 P cepacia cases.
By 1992, the organism accounted for 18 of the 35 P cepacia cases.
Patient M17 was the only individual who was colonised by the epidemic strain at the time of first referral to the unit.
The first Manchester patients (M6, M7) colonised by the epidemic strain had attended a Canadian CF camp in September, 1990 (figure 2).
This strain was first found in sputum from patient M6 immediately before her departure for Canada, and first cultured from patient M7 when he arrived in Canada.
Patient M7 subsequently moved to another part of the UK and broke all social ties with patients attending the Manchester CF clinic.
His outpatient visits have been infrequent thereafter and timed to avoid organised CF clinics.
However, patient M8 (the CF sibling of patient M6) became colonised with the epidemic strain of P, cepacia within two months of her sister's return from Canada.
A few weeks later, the epidemic strain was cultured from the sputum of M10, a close friend of patient M6 and M8.
Many of the Manchester patients subsequently colonised by the epidemic strain were found to have had close social contact (in and outside of hospital) with patients M8 and M10.
Patients M24, M25, and M26 were inpatients when the epidemic strain was first cultured from their sputum.
All 3 patients had been kissed by patient M10 (also an inpatient) under the mistletoe several days earlier during Christmas festivities in the hospital.
Within three months, the epidemic strain was cultured from 3 other patients (M30, M32, and M34) who had been inpatients over the Christmas period, and had had close contact with patients M8 and M10.
Transmission of the epidemic P cepacia strain between regional CF centres
In April, 1990, 4 Edinburgh patients colonised by the epidemic strain (E7–E10) travelled to a weekend camp near Manchester that was also attended by Manchester patients M6 and M7, and 29 CF patients from other regional centres.
The aim of the camp was to select delegates to attend a CF camp in Ontario, Canada to be held in August/September of the same year.
Sputum testing was not carried out as part of the selection process.
Based upon previous sputum bacteriology results, at the time of selection camp the only patient from Manchester or Edinburgh known to have the epidemic strain of P cepacia was patient E7.
Patient E7 travelled to Canada with 12 other selected CF patients.
Among these patients was patient M6 whose records show was by then also colonised with P cepacia.
On the first day of camp, 11 of the 13 patients from the UK (including M7) were found to be colonised by P cepacia. 5 of these P cepacia-positive individuals have been traced and all are now colonised by the epidemic strain.
Patient M17 had the epidemic strain of P cepacia in his sputum when first referred to the Manchester adult CF clinic from the Liverpool CF clinic.
At the Liverpool centre, his close acquaintances had included 1 girl who had attended the Canadian CF camp in 1990 and who had been colonised with the epidemic strain on her return.
Patient M14 has had little contact with patients M6, M8, and M10, or with other Manchester patients colonised with the epidemic strain.
However, she has had protracted social contact with patient M17.
Discussion
Failure to prevent cross-infection with P cepacia in CF patients attending adult CF clinics, despite hospital segregation of colonised from non-colonised patients, has led to increased awareness that social contact may be an important mode of acquisition.
LiPuma et al provided the first  evidence that person-to-person transmission may occur.
They documented the transmission of P cepacia between 2 CF patients attending a summer educational programme, but did not clarify whether transmission had occurred by direct transfer of body fluids from one person to the other (eg, droplet spread), or whether a contaminated fomite (eg, a shared drinking container) was responsible.
LiPuma and colleagues cite previous failures to recover P cepacia from environmental surfaces as evidence that person-to-person transmission may be the primary mode of nosocomial acquisition.
However, Nelson et al showed that colonised patients could contaminate their environment.
Therefore, the possibility of indirect transmission via contaminated drinking containers and other fomites cannot be discounted.
For ethical reasons, experimental proof of direct person-to-person transmission is not feasible.
Our study documents the largest number of cases of person-to-person transmission of a single P cepacia strain within and between regional CF centres, and describes social contacts before colonisation.
The precise nature of the social contacts that facilitate P, cepacia transmission remains unclear.
Patients in our study have reported coughing over one another, kissing each other socially, and sharing drinking containers.
Outside hospital, these patients have done chest physiotherapy close to one another and have shared nebulising facilities.
The extent to which guidelines have influenced social behaviour is difficult to assess.
In bacteriological surveillance done in the Edinburgh adult CF outpatient clinic, P cepacia was grown from the hands of each of the 3 P cepacia-colonised patients, and from the spirometry mouth filters after spirometry.
However, the organism was not cultured from the spirometry handpiece or from the wooden arms of a chair gripped by each of the patients during spirometry.
In our previous surveillance, 2 hospitalised patients were shown to contaminate their hands, the outside of sputum pots, and drinking containers with the same strain of P cepacia (the epidemic strain) in their sputum.
The significance of these observations is unclear.
In Edinburgh, social contact outside hospital was strongly implicated in transmission of the epidemic strain.
The subsequent spread of this strain to Manchester and other regional centres also provided strong circumstantial evidence that social contact outside hospital played an important role in the spread of this strain.
The observation that CF patients could contaminate environmental surfaces with P cepacia in their sputum suggested that this cannot be excluded as an indirect mode of spread of the epidemic strain within the Edinburgh fitness class.
Contamination of environmental surfaces within the Edinburgh outpatient clinic did not appear to be of clinical significance given the scarcity of evidence for nosocomial spread.
As a precaution, however, present policy is to treat P cepacia-colonised patients at a separate outpatient clinic and, to put these patients in separate rooms when inpatients.
It is also policy to discourage social contact between P, cepacia-colonised and non-colonised patients and, where such contact exists, to advise on the possible risk of transmission.
Encouragingly, there have been no new cases of P cepacia colonisation with any strain in Edinburgh patients since patient E22 in May, 1992.
Moreover, in Manchester, only 1 CF patient has acquired P, cepacia since June, 1992.
This patient has never been in hospital, but became colonised after sharing a car journey with a P, cepacia-colonised CF adult.
Based on epidemiological data and the use of phenotypic and genotypic typing systems for P cepacia, we present strong evidence of the transmission of a particular epidemic strain by social contact.
Transmissibility is not unique, however, to the epidemic strain documented in this study.
Our continuing studies confirm a previous report that spread of individual strains occurs within regional centres.
However, in a comparative study, the epidemic strain showed a six-fold greater adhesion to CF tracheobronchial mucin than 45 other CF and non-CF isolates of P cepacia.
It is possible that the risk of transmission may be strain-dependent, but also may vary with the degree of contact between individuals.
Prognostic statements on the clinical complications of colonisation by the epidemic strain in individual patients are difficult, but we have found survival to be influenced by poor pulmonary status of the patient at the time of acquisition.
Each of the Edinburgh patients colonised by the epidemic strain showed a rising IgG antibody response to P cepacia-specific cell wall antigen.
In some patients, there was four-fold increase in antibody response before the first sputum culture of P cepacia.
This response suggests that patients could be infectious before sputum culture which might decrease the efficacy of relying on negative sputum culture before acceptance to meetings and other social gatherings.
In conclusion, the segregation of colonised from non-colonised patients generates a fearful attitude towards those already colonised and encourages fear of acquisition in others.
Policies designed to prevent cross-infection, therefore, must be scientifically justified, clearly defined, and widely adopted.
If guidelines on attendance at conferences are strictly enforced, then laboratories serving regional CF centres need to be vigilant in identifying P, cepacia colonisation, a procedure that requires proficiency.
Our results suggest that transmission of P, cepacia can occur through social contact, and that such contacts need to be considered in assessing the efficacy of segregation within hospital, and in providing guidelines to reduce the risk of acquisition.
Although the question posed by a CF adult: ‘Are you sure beyond reasonable doubt that we should live our lives wondering who or what will contaminate us with P, cepacia’ has not been fully answered, our investigations support the conclusion of a recent editorial that the ‘risks cannot be ignored’.
Polychemotherapy in advanced non small cell lung cancer: a meta-analysis
Summary
We did a meta-analysis of all published polychemotherapy vs supportive care clinical trials in patients with non-resectable non small cell lung cancer.
7 studies with more than 700 patients were selected.
We used the number of deaths at 3, 6, 9, 12, and 18 months as the endpoints because we were unable to obtain all the individual data.
Our analysis showed a reduction in mortality during the first 6 months with polychemotherapy.
Although small, this increase in survival, together with an improved quality of life, suggests that polychemotherapy should be recommended for patients with non-resectable non small cell lung cancer.
Introduction
Conclusions cannot be drawn about the effectiveness of polychemotherapy for non-resectable non-small-cell lung cancer (NSCLC) because the published randomised trials of polychemotherapy vs supportive care have been too small.
Consequently, we did a meta-analysis using the data from all randomised controlled trials of this treatment.
Methods
Trials were selected if they were randomised controlled trials assessing polychemotherapy vs supportive care with palliative treatments in non-resectable NSCLC (stage III B and IV — International Union Against Cancer 1987).
We found 10 trials by a manual and computerised search of medical journals.
We also looked for unpublished trials but found none.
2 were excluded because they studied monochemotherapy (etoposide and nitrogen mustard), and 1 because the supportive care group was in fact a ‘wait and see’ group with treatment by chemotherapy or radiotherapy only when symptoms appeared.
Although in some trials we noted post-randomisation exclusion of ineligible patients, early deaths, and protocol violations, we did not exclude them.
We invited the principal investigators from the remaining 7 trials to participate in this analysis and they agreed.
Information obtained from the trials is shown in table I.
Statistical methods
We used the number of deaths at 3, 6, 9, 12, and 18 months as the endpoints, because information on individual patients was not available.
We chose a p value of 0.01 and judged results greater than this as not statistically significant.
We felt that the use of median survival as an outcome was unsuitable in this meta-analysis since this effect-size approach, although suitable for quantitative outcomes, would not have given reliable pooled results.
The meta-analysis was done with the statistical technique which gave either the highest p value for association or the lowest p value for heterogeneity (Cochran's method, Mantel-Haenzel method, Peto's  method, difference of rates, logarithm of odds ratio, Der Simonian's method).
Results
706 patients were included.
Only 3 trials showed a statistically significant benefit in favour of polychemotherapy (table 2).
The logarithm of odds ratio for mortality (figure 1) showed a significant reduction in the mortality rate in the polychemotherapy group at 3 months.
There is a similar reduction at 6 months.
At 9 and 12 months the results are not significant (p=0.02 and 0.03, respectively), nor at 18 months (p=0.15).
Figure 2 shows the odds ratios over 18 months for all trials with 95% confidence limits.
Although the difference for death rates at 9, 12, and 18 months are not significant, there is no evidence to suggest a trend towards a decreased benefit over time.
These results can also be expressed as the risk reduction (polychemotherapy compared with supportive care), which was found to be 0.65 at 3, 0.73 at 6, 0.86 at 9, 0.91 at 12, and 0.96 at 18 months.
All meta-analysis techniques gave similar p values for association and homogeneity, except for the 6th month.
At 3, 9, 12, and 18 months there is no difference within the control group (for association, p>0.05); at 6 months the association is low (p<0.05) with the usual techniques (Cochran's method, logarithm of odds ratio).
These values show there is an interaction between the treatment effect and the trial effect, but there is also a real treatment effect.
Table 3 shows that a fixed effect method (logarithm of odds ratio) and a random effect method (Der Simonian's) gave similar results for odds ratio and for homogeneity, except at 6 months.
The benefit of polychemotherapy in non-resectable NSCLC appears to be real, but it is unknown whether the benefit is still present after a year.
Discussion
Meta-analysis increases the statistical power of tests but does not eliminate causes of bias and difficulties in interpreting the results.
There were differences between the trials: some included only stage IV NSCLC whereas others also included stage III B; the eligibility criteria varied (age, presence of brain metastasis, previous chest radiotherapy); and different chemotherapy regimens were used, although a vindesine (or vinblastine) and cisplatin regimen was used in 4 trials.
We were unable to do subgroup analysis because not enough patients had been included in the trials.
One of the main problems in meta-analysis is incomplete collection of data (it is possible that some unpublished trials were not identified), another is the difficulty of obtaining complete data on an ‘intention-to-treat’ basis.
Some patients were excluded, ineligible, or lost to follow-up, but the number with inadequate data was less than 6%.
There were some striking differences in protocols.
In one trial, palliative thoracic radiotherapy was allowed: 15 patients in the supportive care group and 8 in the polychemotherapy group received this treatment (less than 42 Gy).
In this trial the responders received no more than 4 courses of chemotherapy.
In another, involving patients from Australia and the UK, patients in Australia were randomised with Zelen's method (randomisation before patient's consent was obtained), whereas those in UK were not.
In order to avoid bias, we did two different meta-analyses, one using data for all patients in the trial, a second using data for the UK patients.
We found no difference in the values of odds ratio or association.
Two other aspects of polychemotherapy are also important.
The first is the quality of life, which is important due to the toxicity of the drugs.
In 2 studies the quality of life was not assessable because of poor compliance and a rapid deterioration of the patients' physical and cognitive status.
However, there is evidence that polychemotherapy can reduce many symptoms such as pain, cough, and dyspnoea.
The second aspect is the cost of the chemotherapy.
In Canada a study concluded that polychemotherapy was less expensive than best supportive care although patients survived for longer in the chemotherapy group.
The reduction was due to more severe symptoms with longer hospital stay in the supportive care group.
This meta-analysis showed a statistically significant reduction of mortality with polychemotherapy in advanced and disseminated NSCLC for up to 6 months.
Although the risk reduction is low, we believe that polychemotherapy should be given to patients with non-resectable NSCLC.
Short reports
In-vitro synthesis of antibodies to Toxoplasma gondii by lymphocytes from HIV-1-infected patients
Toxoplasma-specific in-vitro antibody production by peripheral blood lymphocytes (PBL) was investigated in 124 adults infected with human immunodeficiency virus (HIV-1).
PBL from 20 of 21 patients with cerebral toxoplasmosis showed spontaneous in-vitro secretion of antibodies to Toxoplasma gondii antigens.
Among 103 HIV-1-infected patients without signs or symptoms of toxoplasmosis, PBL from 19 produced toxoplasma-specific antibodies in vitro; 5 of these patients, who discontinued prophylaxis for toxoplasmic encephalitis, showed in-vitro antibody production 3–15 months before the diagnosis of toxoplasmic encephalitis.
In-vitro production of toxoplasma-specific antibodies could improve the diagnosis of toxoplasmic encephalitis in HIV-1-infected patients.
Involvement of the central nervous system is common in patients with acquired immunodeficiency syndrome (AIDS); in many it is caused by opportunistic pathogens such as Toxoplasma gondii.
In clinical practice, presumptive diagnosis of toxoplasmic encephalitis implies specific therapy, and the firm diagnosis is usually made on the basis of the clinical and radiographic response to treatment.
Serum antibody concentration does not predict toxoplasmic encephalitis and IgM and IgA antibodies to T, gondii are rarely found.
The importance of antigenaemia, parasitaemia, and presence of toxoplasma DNA sequences in body tissues and fluids is still unclear.
Toxoplasmic encephalitis can be definitely diagnosed only by histology or by isolation of T gondii organisms from brain biopsy material.
The difficulties of surgical intervention limit wide application of this method.
Peripheral blood lymphocytes (PBL) from immunocompetent patients with acute toxoplasmosis (not infected with human immunodeficiency virus [HIV-1]) transiently secrete antibodies to T gondii in vitro.
We tried to find out whether reactivation of T gondii in AIDS patients induces similar antibody secretion.
We studied 124 adults with HIV-1 infection (104 male, 20 female; aged 19–46 [mean 31]years).
The Centers for Disease Control and Prevention category was A1 in 18, A2 in 13, A3 in 3, B1 in 4, B2 in 15, B3 in 16, C2 in 12, and C3 in 43. 100 patients were positive for T gondii specific antibodies.
Among the patients classified as C3, 21 had central nervous toxoplasmosis defined by the presence of new focal or general neurological signs of symptoms, focal hypodense mass lesions verified by computed tomography, magnetic resonance imaging, or both, and clinical and radiological responses within 15 days of starting antitoxoplasma therapy (pyrimethamine 75 mg daily plus sulphadiazine 4 g daily or clindamycin 2.4 g daily).
We also studied 17 adults (8 male, 9 female; aged 22–41 [mean 29]years) positive for T gondii specific serum IgG antibodies (<300 IU/mL) but without detectable IgM or IgA antibodies and negative for HIV-1 specific antibodies.
In-vitro production of anti-toxoplasma and anti-HIV-1 by PBL was detected as previously described.
Briefly, plasma antibodies present only by adhesion to cell membranes were removed by preliminary incubation of PBL and careful washing.
PBL were finally cultured (2.5 × 10 cells/mL) for 7 days.
For each sample, a control culture of PBL was done with 50 g/mL cycloheximide.
ELISA results were expressed as the ratio of the absorbance value for the PBL supernatant and the cutoff, defined by the mean absorbance value plus 3 SD of PBL culture supernatants from 9 adults negative for both anti-T gondii (cutoff 0.158) and anti-HIV-1 (cutoff 0.108).
Toxoplasma-specific antibodies were found in supernatants from 39 (31%) of the HIV-1-infected patients.
ELISA index values ranged from 2.03 to 18.98 (mean 11.33 [SD 6.13]).
In all but 1 of these patients (ELISA index <1) immunoblotting revealed antibodies to the 30 kDa toxoplasma polypeptide and in some cases to the 28.5, 43, and 68 kDa polypeptides.
Since culture of PBL in the presence of cycloheximide greatly reduced ELISA index values (mean 0.87 [0.11]), the antibodies detected must have been newly synthesised.
No toxoplasma antibodies were detectable in supernatants from the 85 other HIV-1-seropositive patients (ELISA indices <1) or the 17 HIV-1-uninfected subjects with chronic toxoplasmosis.
Toxoplasma-specific antibody production was confirmed in the positive patients by study of second samples taken 2–4 weeks later.
To find out whether the presence of toxoplasma-specific antibodies was related to clinical status we classified the patients in three groups (table).
Group I consisted of AIDS patients (class C3) with acute cerebral toxoplasmosis and CD4 T lymphocyte counts below 0.200 × 10/L (mean 0.046 [range 0.001–0.169]× 10/L).
PBL from 20 produced anti-toxoplasma; all 21 produced anti-HIV-1. 12 patients had immunofluorescence antibody serum titres (IFAT) below 300 IU/mL without IgM or IgA antibodies, 8 had IFAT titres of 300–10,000 IU/mL (with IgA in 2), and 1 patient had 76,800 IU/mL with IgA.
PBL from the patient who did not produce toxoplasma antibodies did produce anti-HIV-1 antibodies, but at a much lower level than in other patients in this group (HIV-1-specific ELISA index 1.85).
Group II was patients without signs or symptoms of toxoplasmosis at first assessment (classes A1–A3, B3, and C3; 2 patients had cryptococcal meningitis, 3 Kaposi's sarcoma) but who produced toxoplasma-specific antibodies (table).
PBL from sequential samples (2–7) taken 1–15 months later also secreted antibodies. 12 patients had serum IFAT titres below 300 IU/mL, 5 had stable antibody titres between 300 and 10,000 IU/mL, and 2 without signs or symptoms of toxoplasmic encephalitis had high and constant concentrations (19,200 IU/mL and 40,960 IU/mL) of IgG antibodies with the presence of IgA.
4 patients were  given co-trimoxazole prophylaxis for toxoplasmic encephalitis during follow-up.
5 other patients initially classified as B3 (3) and C3 (2) discontinued prophylaxis at entry; signs or symptoms of toxoplasmic encephalitis appeared 3–15 months after detection of toxoplasma-specific antibody production.
Initial CD4 T lymphocyte counts were lower in these 5 patients than in the 14 without toxoplasmic encephalitis (mean 0.093 [range 0.022–0.143]vs 0.314 [0.005–0.846]× 10/L).
The third group consisted of 84 patients without signs or symptoms of cerebral toxoplasmosis who were negative for toxoplasma-specific antibody production.
Some patients had Pneumocystis carinii pneumonia (n=8), opportunistic infections with Cryptococcus neoformans (n=2) or Leishmania infantum (n=1), and malignant disorders such as Kaposi's sarcoma (n=6) or primary cerebral lymphoma (n=3).
60 (71%) of these patients had positive IFAT concentrations of IgG serum antibodies to T gondii (12–300 IU/mL) but negative for IgM and IgA; 24 patients were negative for all three antibody isotypes.
The production of toxoplasma-specific antibodies by PBL from AIDS patients probably results from immune-system stimulation by antigens released from T gondii activated cysts.
In patients with advanced AIDS, T gondii specific antibody secretion by PBL seems to be independent of in-vivo antigen-specific T-lymphocyte help.
Indeed, purified antigen-specific B lymphocytes from HIV-1-infected patients can be activated in vitro by HIV-1 antigens to secrete cytokines and immunoglobulins.
Thus, in HIV-1 patients with toxoplasmosis reactivation, germinal centre specific B cells of lymphoid nodes might be directly stimulated by toxoplasma antigens and then enter the bloodstream to reach other functional sites.
As reported for some HIV-1-infected patients, B cells spontaneously secreting antibodies are not always present.
Furthermore, numbers of peripheral activated B cells may vary among individuals.
The findings in group II suggest that T gondii antigens can be released by in-situ activated cysts, inducing activation and differentiation of specific B cells before the onset of cerebral toxoplasmosis.
In HIV-1-infected patients with sufficient CD4 T lymphocytes, without prophylaxis for toxoplasmic encephalitis the immune system might control the development of T gondii in-situ reactivated cysts.
The lack of in-vitro antibody production in group III suggests that infectious agents such as P carinii, C neoformans, L infantum, Mycobacterium avium-intracellulare, and cytomegalovirus or disorders such as cerebral lymphoma and Kaposi's sarcoma are not able to activate in vivo T gondii specific B cells by a polyclonal activation mechanism.
In HIV-1-infected patients, positive toxoplasma-specific in-vitro antibody production was rarely associated with a significant rise in serum antibodies.
In a model of spontaneous anti-tetanus toxoid antibody production after vaccination, PBL secreting specific antibody do not bring about an increase in the serum antibody concentration.
Similarly, when we measured the frequencies of B cells spontaneously secreting T gondii antibody in vitro(7 patients), we had results similar to those for other models of in-vitro antibody production (enzyme-linked immunospot ranged from 1/150 to 1/12,000 PBL; data not shown).
In AIDS patients with neurological abnormalities, toxoplasma-specific in-vitro antibody production could be a useful biological marker for the diagnosis of toxoplasmic encephalitis.
Specific antibody production could help to distinguish between intracranial mass lesions caused by T gondii reactivation and those caused by other processes such as lymphoma or progressive multifocal encephalitis, thus limiting the need for brain biopsy.
Furthermore, toxoplasma-specific in-vitro antibody production appears several months before the onset of encephalitis.
Pneumocystis carinii pneumonia during primary HIV-1 infection
3 patients with severe CD4 lymphocytopenia (62–91 cells/µL) and inverted CD4/CD8 ratios (0.13–0.15) developed Pneumocystis carinii pneumonia during symptomatic, primary HIV-1 infection.
Within four months of symptom onset, their CD4 counts and CD4/CD8 ratios returned to normal.
Twenty-nine to forty-eight months after acquiring HIV-1 infection, they show no signs or symptoms of progression to AIDS.
Pneumocystis carinii pneumonia can occur, therefore, in primary HIV-1 infection, and profound CD4 lymphocytopenia can revert to normal without antiretroviral therapy.
Primary infection with HIV-1 associated with seroconversion is characterised by an acute illness with variable manifestations that are thought not to include opportunistic infections other than oesophageal candidosis.
We describe 3 patients in whom primary, symptomatic HIV-1 infection was associated with severe immunosuppression and Pneumocystis carinii pneumonia, an opportunistic infection that is usually an AIDS-defining disorder.
Patient 1,(female, 23 years old) presented with fever, sore throat, anorexia, malaise, myalgia, headache, and cervical, and inguinal lymphadenopathy.
She was the sexual partner of an intravenous drug user who had HIV-1 infection and AIDS-related complex.
Three months before these symptoms, HIV-1 antibodies and serum p24 antigen were non-detectable by enzyme immunoassays (EIA; Abbott), and her CD4 count (Coulter Electronics) was 920/L (41.9% of total lymphocytes).
At presentation, her white cell count was 2.2×10/L (40% neutrophils, 50% lymphocytes, 9% monocytes, 1% basophils), and her platelet count was 53×10/L.
HIV-1 antibodies were non-detectable by EIA or western blot assays (WB; Du Pont), but serum p24 antigen (Abbott) was 60 pg/mL.
Her CD4 and CD8 counts were 280/L (25%) and 390/L, respectively.
Ten days after presentation, she developed Pneumocystis carinii pneumonia which was confirmed by identification of the organism in bronchial lavage specimens.
Her CD4 and CD8 counts were 91/L (7.5%) and 580/L, respectively, p24 antigen was 1620 pg/mL, and HIV-1 antibodies were non-detectable.
Co-trimoxazole (15 mg/kg trimethoprim plus 75 mg/kg sulphamethoxazole) was given intravenously.
One week later, the p24 antigen had fallen to 80 pg/mL and WB showed reactivity to HIV-1 p24 and gp160.
One month after the onset of symptoms, she was symptom free, p24 antigen was undetectable, HIV-1 antibodies were positive by EIA and WB (with reactivity to all proteins tested), CD4 and CD8 counts were 390/L (27%) and 470/L, respectively, and her platelet count was 240×10/L.
Three months after the onset of symptoms, her CD4 count was 942/L (43.6%) and CD8 was 458/L.
Four years after primary HIV-1 infection, she is symptom free and her CD4 count is 589/L (33.8%).
Patient 2,(male, 24-year-old, bisexual, intravenous drug user) presented with fever, pharyngitis, oral candidosis, fatigue, anorexia, diarrhoea, and a red macular rash over his face and upper body.
Two months before symptom onset, HIV-1 antibodies (EIA and WB) and serum p24 antigen were non-detectable and his CD4 count was 1216/L (51.8%).
At presentation, his white cell count was 2.4×10/L (42% neutrophils, 46% lymphocytes, 7% monocytes, 5% eosinophils), and his platelet count was 61×10/L).
His serum p24 antigen was 100 pg/mL, HIV-1 antibodies were non-detectable (EIA and WB), CD4 count was 325/L (29.4%), and CD8 395/L.
Fluconazole 100 mg daily was started.
Two weeks later, he developed Pneumocystis carinii pneumonia.
His CD4 count had fallen to 62/L (5%), his CD8 count was 452/L, serum p24 antigen was highly positive (1480 pg/mL), and HIV-1 antibodies were non-detectable.
Co-trimoxazole (20 mg/kg trimethoprim plus 100 mg/kg sulphamethoxazole) was started intravenously.
Ten days later, his p24 antigen was 70 pg/mL and reactivity to HIV-1 p24 appeared in WB.
Forty-five days after the onset of symptoms, his pneumonia and oral candidosis had been successfully treated, all other symptoms except fatigue had gone, p24 antigen was not found in the serum, HIV-1 antibodies were detectable by EIA and WB, CD4 count was 210/L (18.3%) and CD8 512/L, and platelets were 252×10/L.
Four months after symptom onset, his CD4 count had increased to 590/L (35.9%) and CD8 was 476/L.
One year after primary infection, he had 715/L (48.5%) CD4 cells.
Thirty-nine months after acquiring HIV-1 infection, he has 523/L (41.2%) CD4 cells and is symptom free, despite continuing intravenous drug use.
Patient 3,(male, 26-year-old, homosexual, intravenous drug user) presented with fever, chills, weakness, headache, arthralgia, myalgia, nausea, and dyspnoea on exertion.
Fifty days before symptom onset, HIV-1 antibodies (EIA and WB) and serum p24 antigen were non-detectable, and his CD4 count was 895/L (45.5%).
At presentation, his white cell count was 2.7×10/L (53% neutrophils, 39% lymphocytes, 6% monocytes, 1% basophils, 1% eosinophils), and his platelet count was 58×10/L.
His serum p24 antigen was positive (140 pg/mL), HIV-1 antibodies were non-detectable (EIA and WB), CD4 count was 174/L (16.5%) and CD8 642/L.
Eight days after presentation, Pneumocystis carinii pneumonia was diagnosed.
His CD4 count had dropped further to 83/L (6.4%), CD8 count was 589/L, p24 antigen was 1010 pg/mL, and HIV-1 antibodies were still undetectable.
Co-trimoxazole (20 mg/kg trimethoprim plus 100 mg/kg sulphamethoxazole) was started intravenously.
Three days later, he developed oral candidosis and fluconazole 100 mg daily was started.
Fifteen days after symptom onset, WB assay showed reactivity to gp160 and his p24 antigen was 63 pg/mL.
Thirty days after symptom onset, he still complained of weakness and myalgia, but his pneumonia had been cured, his oral candidosis had disappeared, serum p24 antigen was undetectable, HIV-1 antibodies were detectable by EIA and WB, CD4 and CD8 counts were 343/L (26.7%) and 516/L, respectively, and his platelet count was 285×10/L.
Three months after symptom onset, his CD4 count was 687/L (47.1%) and CD8 488/L.
Twenty-one months after primary HIV-1 infection, his CD4 count had fallen to 416/L (28.9%) and zidovudine 500 mg daily was started.
Twenty-nine months after acquiring HIV-1 infection, he remains free of symptoms on zidovudine and has 494/L (34.2%) CD4 lymphocytes.
Symptomatic, primary HIV-1 infection is generally characterised by a mononucleosis-like illness, with or without aseptic meningitis.
The only AIDS-defining opportunistic infection documented, in a few cases, in primary HIV-1 infection is oesophageal candidosis, which appears to be associated with CD4 counts between 260 and 630/L, oesophageal ulcerations, and inappropriate use of antibiotics.
Our cases differ from others in several ways.
First, CD4 counts were profoundly reduced both in absolute numbers (62–91/L) and in the percentage of total lymphocytes (5.0–7.5%).
Second, Pneumocystis carinii pneumonia, which is one of the opportunistic infections listed in the clinical definition of AIDS by the Centers for Disease Control and Prevention, and the most frequently reported, developed within two weeks of the onset of symptoms of primary HIV-1 infection.
Third, the CD4 counts and percentages returned to normal within four months as did the inverted CD4/CD8 ratios (a new finding to our knowledge after symptomatic, primary HIV-1 infection).
Fourth, twenty-nine to forty-eight months after acquiring HIV-1 infection, our 3 patients show no signs or symptoms of progression to AIDS, which contrasts with the concept that individuals who experience an acute illness associated with seroconversion have a more rapid progression to AIDS.
Our findings confirm the transient, intense viraemia observed by others in symptomatic, primary HIV-1 infection, which was associated with an especially large reduction in platelet count, perhaps directly related to intense viral replication.
Although the mechanisms leading to effective control of HIV-1 replication in primary infection remain to be determined, our observations show that these mechanisms can be activated even after a high degree of clinically relevant immunosuppression has been reached, and that CD4 cell depletion is easily reversible in peripheral blood (perhaps through release in the circulation of CD4 lymphocytes previously concentrated in lymph nodes).
In conclusion, Pneumocystis carinii pneumonia can be associated with primary HIV-1 infection, and it is important that such cases are not misdiagnosed as AIDS.
Mild cystic fibrosis and normal or borderline sweat test in patients with the 3849+10 kb CT mutation
Different mutations in the cystic fibrosis (CF) gene appear to contribute to heterogeneity of the CF phenotype.
We investigated 15 patients with CF who have the 3849+10 kb CT mutation.
All were Ashkenazi Jews.
Their clinical features were compared with those of CF patients with the F508/F508, W1282X/W1282X, W1282X/F508 mutations, which are known to be associated with a severe disease.
Patients with the 3849+10 kb mutation were older, had been diagnosed as having CF at a more advanced age, and were in a better nutritional state.
Sweat chloride values were normal (below 60 mmol/L) in 5 3849+10 kb patients (33%).
4 of these patients and 6 others (total 66%) had normal pancreatic function.
However, age-adjusted pulmonary function did not differ between the two groups.
None of the patients with 3849+10 kb CT had had meconium ileus or had liver disease or diabetes mellitus.
We conclude that this mutation is associated with a mild type of CF.
Since the identification of the most common mutation in the cystic fibrosis (CF) gene, F508, over 200 more mutations have been reported.
That the disease ranges widely in severity has led to suggestions of a genotype-phenotype correlation in CF. The most common mutations in Israeli patients with CF — W1282X and F508 — were shown to be associated with a severe disease.
In addition, a mutation in intron 19 (designated 3849+10 kb CT) was detected in a 19-year-old Pakistani woman with mild disease and normal sweat chloride values.
The aims of this study were to determine the frequency of the 3849+10 kb CT mutation in Israeli CF patients and to assess the disease severity in patients with this mutation compared with that of patients who have the mutations that are known to cause severe disease.
Our CF patient cohort included 117 families from two ethnic groups, 20 of Arab origin and 97 of Jewish origin.
Two subgroups were involved in the present study: i) 40 patients carrying 54 chromosomes with unidentified mutations, ii) 57 patients who were either homozygous for the F508 and W1282X mutations or compound heterozygous for both of them.
Diagnosis had been based on typical clinical features, family history, increased sweat chloride concentration, and genetic analysis (or any combination of these features).
The 54 chromosomes of CF patients with unidentified mutations were tested for 3849+10 kb CT.
This mutation was detected by amplification of the DNA sequence that spanned the 3849+10 kb CT base substitution in intron 19 with 2 respective oligonucleotide primers.
Amplified DNA fragments were digested with HphI restriction endonuclease for 2 h at 37°C and electrophoresed on 12% polyacrylamide gels.
Restriction enzyme digestion of genomic DNA was done as recommended by New England Biolabs.
Agarose gel electrophoresis and blot hybridisation analysis were done by methods similar to those described elsewhere.
15 patients (9 men, 6 women) were found to have the 3849+10 kb CT mutation (table).
All were compound heterozygous: 5 had the W1282X, 5 the F508, 1 the G85E mutation, and 4 had an unidentified mutation on the other chromosome.
14 patients were Jews of Ashkenazi origin, but in 1 patient the ethnic origin could not be verified.
DNA  marker haplotype analysis revealed that chromosomes with the 3849+10 kb CT mutation were associated with the haplotype C at the H2.3A/TaqI and E1/PstI loci.
The similarity between these DNA markers and XV2C/TaqI and KM19/PstI, suggests that all patients had a CF ancestor in common.
So far six mutations have been identified among Ashkenazi Jews: F508, W1282X, G542X, 1717-1GA, N1303K, and 3849+10 kb CT, which account for 96.9% (127 of 131 chromosomes) of the CF alleles in Ashkenazi Jews.
To assess the disease severity in the 15 patients with the 3849+10 kb CT mutation (group A) we compared their clinical phenotype with that of patients homozygous for F508 or W1282X and compound heterozygous for F508/µW1282X (group B).
As shown in the table, the 3849+10 kb mutation is associated with milder disease and these patients live longer (we assumed that advanced age indicated a lower mortality rate) and were in better nutritional condition.
In 5 patients of group A, sweat chloride concentrations were normal (<60 mmol/L) and in another 7 patients the concentrations were 60–75 mmol/L. 10 of the 15 patients in group A were pancreatic sufficient and 1 other patient had borderline coefficient fat absorption.
4 patients had both normal sweat chloride concentrations and normal pancreatic function.
However, there were no differences in lung function between the two groups.
4 patients had nasal polyps and were all compound heterozygous for 3849+10 kb and W1282X. 1 patient had a normal spermatogram.
The cloning of the CF gene enables the association of phenotype with a specific mutation.
However, analysis of genotype-phenotype association is possible in only a few frequent mutations; the majority of the mutations are rare.
To explain the milder course of the disease in patients with the 3849+10 kb mutation, Highsmith et al hypothesised that this C to T base substitution in intron 19 creates an alternative splicing site, which results in insertion of 84 base pairs into the cystic fibrosis transmembrane conductance regulator (CFTR) coding region.
This change might cause synthesis of a protein with normal CFTR function, together with a non-functioning protein.
Alternatively, this mutation might lead to production of a protein that is only partly functional and causes a milder disease.
It has been suggested that alleles associated with a severe disease are dominated by those that produce mild disease.
The milder disease of patients with the 3849+10 kb mutation, supports this hypothesis.
However, questions still remain, since certain individuals within this group are pancreas insufficient.
The effects of other genes and environmental factors may influence the different course of the disease in this subset of patients.
Lung function in patients with the 3849+10 kb mutation was variable despite their mild disease, similar to the variability in results of such tests for patients with the F508 and W1282X mutations.
Another mutation (G551S) is said to cause a disease with variability in expression in different tissues (normal sweat tests, pancreatic sufficiency, moderate lung disease, and abnormal nasal epithelial ion transport).
Interaction of the CF gene with tissue-specific protein has been suggested as an explanation.
Since 1959, measurement of sweat electrolyte concentration has been the cornerstone of CF diagnosis.
33% of our patients had sweat chloride concentrations of less than 60 mmol/L, which is thought to be normal.
A few additional CF patients with normal sweat test results have been reported; their diagnosis was established by the detection of CF mutations.
We conclude that further diagnostic tests are required for patients whose clinical course suggests CF, but whose sweat test result is normal.
In such cases DNA studies are very useful, especially in populations such as the Ashkenazi Jews in which 97% of CF mutations are detectable.
However, the situation is complicated by there being more than 200 CF mutations and many of them have been detected in only 1 patient.
Electrophysiological studies may be of help in cases where neither sweat test nor DNA analysis were conclusive.
Facts, figures, and fallacies
The glitter of the t table
‘…there are innumerable situations in which they [tests of significance]are totally unnecessary — because the difference is grotesquely obvious, because it is neglible, or because, whether it be formally significant or not, it is too small to be of any practical importance.
What is worse, the glitter of the t table diverts from the inadequacy of the fare.’
Bradford Hill
Bradford Hill exhorted common sense in the analysis of epidemiological data over 25 years ago.
The t table has since been superseded by the fatter chips and bigger bytes of modern computing; the pop-up menus of today's statistical packages tempt the unwary with an appetising range of p values hardly imaginable in 1965.
Medical research could benefit from a prudent diet of simple data displays and careful thought about bias and confounding.
Consider this example: ‘The censuring [sic]of the follow-up period indicated that progression to high grade dyskaryosis would need to be assessed by Kaplan-Meier life table techniques and comparisons made by the log rank test’(the natural history of cervical cancer had been investigated in the west of England in 1981–90).
The 9 year follow-up of cytological screening histories for 437 women with ‘borderline’ cytological changes in 1981 was compared with that of an age-matched sample of 437 women with ‘normal’cytology in 1981.
The appearance of at least one subsequent smear showing high-grade dyskaryosis was recorded.
22.4% (98/437) of ‘borderline’ women progressed to high-grade dyskaryosis compared with 0.7% (3/437) in the controls.
Do we really need Kaplan-Meier and log rank techniques to dismiss chance (ie, accept that there was a real effect) as the explanation of the outcome difference?
Evident evidence?
The epidemiologist's ‘patients’ are whole populations, in which illness must be measured by averages and percentages, rates and risks.
Statistics, the science of chance, is integral to medical research and to publish, editors urge authors to ‘quantify findings and present them with appropriate indicators of measurement error or uncertainty (such as confidence intervals)’.
This marriage between epidemiology and statistics is reinforced in schools of public health, where the subjects are usually taught in parallel.
Nevertheless, statistics is at most complementary to the breadth of knowledge and judgment that medical research demands.
Many authors find a challenge in t and tests and regression coefficients.
With attention focused on p values and CIs, could the role of chance in scientific reasoning appear falsely magnified?
Common sense in data analysis is often overlooked.
This example is from a standard medical statistics book: ‘Consider the following results from an influenza vaccination trial carried out during an epidemic.
Twenty out of 240 (8.3%) persons given real vaccine contracted influenza compared to 80 out of 220 (36.4%) given a placebo.
Is this convincing evidence that the vaccine was effective?’
An unskilled teacher might insist that students begin by computing.
Fear and dislike of statistics is reinforced and the ‘correct’ answer seems remote from the real problem of vaccine efficacy.
The wise teacher will urge students to judge intuitively (panel 1), and, if doubt remains, to confirm their opinion with a CI.
This is, of course, not only a plea for common sense, but also an argument in favour of estimation rather than hypothesis testing — a vaccine efficacy estimate of 77% for these data has more meaning than a of 53.1.
The 95% CI for the true vaccine efficacy is 61–86%; values outside this range are inconsistent with the known data.
The CI conveys ‘significance’ and magnitude of effect, prompting Gardner and Altman to provide simple formulae for CIs to replace most of the standard statistics for hypothesis tests.
The ability to judge the potential role of chance without the aid of complicated statistics is valuable, if only because the time and pain of calculation can be saved.
The judgment, for example, that the sample size is sufficient and the observed difference so great that chance may be dismissed can and should be made when one is confident that the decision is ‘obvious’.
Similarly, when confronted with the results from small numbers, an experienced researcher should be able quickly to judge whether statistics are worth calculating at all.
To have a sensible cut-off for meaningful differences makes such judgments easier.
Any observed difference between two groups, no matter how small, can be made to be ‘statistically significant’— at any level of significance — by taking a sufficiently large sample.
Thus the wise medical investigator assesses an apparent difference for biological implications before rushing to statistics.
A balanced perspective of the role of chance is difficult to develop or maintain when common sense has been drilled out during training.
Bias, in its many insidious forms, is a more dangerous enemy.
We are warned by Rose and Barker: ‘Biased error strikes at the roots of comparisons.
It is ‘dirty dirt’ and in epidemiology it cannot be tolerated’.
Without a balanced perspective, the temptation to concentrate on the less important, more quantitative role of chance is strong, at the expense of possibly far more serious biases hidden in the design.
In our cervical cancer example above, follow-up was probably more intense in the ‘borderline’ group, and this potential follow-up bias between groups was not addressed.
Scientific graphics
Statistical computing has not been the only impact of modern technology on epidemiology since 1965.
The personal computer provides a window through which datasets can be explored for hidden and unexpected relations.
Data analysis has become interactive, with the scientist interrogating the data and deciding new paths for investigation based on immediate feedback.
Paul Tukey in particular provides new methods for exploratory data analysis, and his work is an excellent start for the curious.
Graphical displays are integral to interactive analysis.
Laser printers and high-resolution videoscreens provide canvases on which unseen patterns in the data can reveal themselves.
Windowed environments, or the ‘graphical user interface’ of modern operating systems, give a natural platform for simultaneously displaying results and probing deeper into underlying relations.
Programs for interactive analysis, such as Egret, S, and its derivative SPlus, are growing in popularity.
However, just as the revolution in statistical computing is a two-edged sword, so too there are many pitfalls in the unthinking use of graphical data exploration.
Most important, the formal statistical reasoning that can be applied to a-priori hypotheses is strictly invalid for exploratory analysis.
Rothman quotes a vivid allegory of this false reasoning: the ‘Texas sharpshooter’ fires a shot randomly into the side of a barn, then draws a target centred on the hole.
Dredging data for ‘significant’ p values fits targets to holes.
Findings discovered in this way should be seen as sources of hypotheses for further testing, not as results in themselves.
‘Business graphics’ packages are another trap.
With bold images designed to convince bosses and clients quickly, such programs make it easy to break the rules of objective data display.
For example, column charts with non-zero baselines are standard output from some popular spreadsheets and differences between series are distorted by truncating the columns whose lengths represent magnitude.
The error is compounded by three-dimensional block charts with false perspectives (figure 1).
Multiple-outcome scales, often used to show two series of data simultaneously, are another danger in business graphics.
Almost any correlation between the series can be manufactured by juggling one or both scales.
A simple scatterplot of one variable against the other will show the true relation, but is surprisingly difficult to request from some programs.
Epidemiological graphics need careful design.
Exposure and outcome must be distinguished: a graphic should display variation in outcome as a function of exposure, and not the other way around.
For instance, for a binary outcome variable, such as recurrence of disease or death, it makes little sense to display group means and CIs of explanatory variables such as age or dose.
More information can be educed from the proportions recurring (or dying) across natural groupings of the explanatory variable.
Graphics are generally more readable when explanatory variables are shown along a horizontal axis and outcome variables vertically.
The scales of measurement of explanatory variable and outcome must be considered before selecting a graphic type.
Age, for example, is a continuous exposure variable and yet column charts with age as a categorical variable are common.
Panel 2 suggests graphical types for common combinations of scales of measurement.
Showing percents requires no more than dots with lines for CIs of a binary outcome variable.
Percents and means for an ordered categorical explanatory variable should be connected between categories so that the eye can judge differences between adjacent pairs of categories (figure 2).
Only when the outcome variable is ordered do cumulative, or stacked, displays make sense.
An area chart, or connected cumulative plot, is useful when both dimensions are ordered — but beware, such charts can be ruined by gaudy hatching or colours between connecting lines.
Quantitative explanatory variables usually need to be reduced to the ordered categorical scale when the outcome variable is not quantitative; if natural cut-points are not apparent (such as 5 year age intervals), then quantiles can be used.
Axis labels should reflect the underlying quantitative scale, and data points generally should be connected.
Boxplots, showing median, interquartile range, and extreme values, are useful for quantitative outcome variables, especially when broken down by explanatory strata and  displayed together.
A scatterplot is needed to assess the linearity assumption underlying each correlation or regression coefficient between pairs of quantitative variables.
New methods of non-linear local regression can reveal hidden patterns in a scatterplot.
Graphic style
Tufte has developed five principles for elegant, informative data displays (panel 3), which should be borne in mind at all stages in data analysis but particularly when preparing figures for publication.
Given two graphics that display identical information, the better graphic uses less ink.
Tufte introduces the idea of data-ink to describe any part of a graphic that conveys measured quantities by, for instance, position on the page, by length, or by the word or numerals it forms.
It is distinct from non-data-ink, which can be erased without loss of information: gridlines and hatching are good examples.
Finding a maximum ratio of data-ink to total ink (data-ink plus non-data-ink) underlies each of Tufte's principles of scientific graphic design.
To illustrate these principles, figures 1 and 2 show the same data from an Australian study of diet and cancer.
Serum beta-carotene (as geometric means) is displayed for nine strata, corresponding to combinations of country of birth and smoking status of 750 subjects.
The predominant feature in figure 1 is non-data-ink, as grid lines and hatching; the effect is visually excessive, and the data are hard to find among all the noise.
In figure 2, the nine data points are the principal feature of the display, with a minimum of non-data-ink to interrupt the message; its data-ink-ratio is close to one.
Besides unnecessary non-data-ink, figure 1 shows an abundance of flaws.
There is redundant data-ink: each column repeats its one datum at least eight times counting the three visible verticals, the four sides of the top, and the height of each column's shading.
The three-dimensional perspective falsely adds to the difference in height across smoking categories, but fails to recognise their ordered nature.
The hatching, especially that for past smokers, creates the illusion of vibration, arising from the equal width strips of light and dark.
And, of course, the baseline is not zero.
Figure 1 also attempts to represent geometric means by columns, which makes no scientific sense.
Many physical quantities (serum beta carotene included) do not have naturally occurring zeroes, and their distribution is skewed, suitable for geometric means or logarithmic transformation.
Even if the column baseline is zero, the column height cannot be said to represent anything real.
Connected means, as in figure 2, or box plots, are more appropriate for these data.
Unlike column charts, they have no implicit proportionality between quantity of ink and magnitude of datum, and are therefore free of the requirement for a zero baseline.
Neither figure displays the within-group variation or the numbers of subjects within each stratum, both of which are necessary to assess the meaningfulness of the between-group differences.
In figure 2, vertical lines representing CI could be easily added.
The clutter of figure 1 leaves no room.
Conclusion
In epidemiological reasoning, several alternatives should be considered to explain an observed association: chance, bias, or confounding might pre-empt true causation.
Bias and confounding should be considered before complicated statistical procedures to assess the role of chance.
Computers offer a host of statistical and graphical temptations.
Choice of the right combination is often more a matter of common sense, careful thought, and good taste than of brute computing power.
Clinical practice
Is pneumonia really the old man's friend?
Two-year prognosis after community-acquired pneumonia
Summary
Is pneumonia ‘the old man's friend’— a terminal event for patients who will otherwise die soon of underlying chronic disease?
If so, chronological age might influence treatment policy.
We investigated the predictors of 2-year mortality after patients' admission to hospital for community-acquired pneumonia, and focused on the predictive value of age.
In a prospective cohort study 141 consecutive patients were admitted to hospital with community-acquired pneumonia.
Clinical, laboratory, and sociodemographic data were collected on admission.
Comorbidity was categorised as mild, moderate, or severe by a physician based on the patient's medical history.
Survival was assessed at 24 months after discharge.
22 (16%) patients died in hospital.
Of the remaining 119, 38 (32%) died over the next 24 months.
In a Cox model, 2-year mortality was independently related to severe comorbidity (relative risk [RR]=9.4) or moderate comorbidity (RR=3.1), and to haematocrit less than 35% (RR=2.9)(all p0.005).
However, compared with patients aged 18–44 years, patients aged 45–64 (RR=0.84), 65–74 (RR=1.28), and 75–92 (RR=1.99) were not significantly more likely to die during the 24 months after discharge (all p0.2).
Old age should not be a sole criterion for withholding aggressive treatment of community-acquired pneumonia.
See Editorial page 1
Introduction
‘Pneumonia may well be called the friend of the aged.
Taken off by it in an acute, short, not often painful illness, the old man escapes those ‘cold gradations of decay’ so distressing to himself and to his friends.’
Pneumonia accounts for over one million hospital admissions annually in the USA and is the eighth leading cause of death.
Short-term mortality among patients admitted to hospital with community-acquired pneumonia has ranged from 6% to 33%.
The clinical predictors of short-term mortality in hospital and within 6 weeks of discharge have been examined.
Long-term outcome after community-acquired pneumonia, however, has not been as well investigated.
Indeed, only two studies have followed up patients for more than 6 months after discharge: one involved the rural elderly, the other involved patients with severe pneumonia who required intensive care.
No previous study has identified predictors of outcome beyond 12 months among patients admitted to hospital with community-acquired pneumonia.
Such data could provide a better understanding of how pneumonia fits into the course of the chronic diseases it often accompanies, and thereby form a basis for therapeutic decision-making.
Therefore, we conducted a prospective cohort study of patients admitted to hospital with community-acquired pneumonia to identify the predictors of mortality for 2 years after hospital care.
We emphasised one particular question: is pneumonia a terminal event for elderly patients who would otherwise die over the succeeding months from underlying, chronic illnesses?
In other words, is pneumonia really the old man's friend?
Methods
All patients admitted to either of two urban teaching hospitals between July 1, 1988, and May 31, 1989, with a diagnosis of community-acquired pneumonia were identified.
Criteria for inclusion of patients were (i) a new pulmonary infiltrate seen on a chest radiograph that had been taken on or within 24 h of admission and (ii) one of these features: cough, temperature greater than 37.8°C, or subjective dyspnoea.
We excluded patients if (i) they had had an episode of pneumonia within the past 6 months,(ii) they lived in a nursing home or had been admitted to another hospital within the past 2 weeks, and (iii) there was subsequent clinical or laboratory evidence of a different diagnosis.
We obtained baseline clinical and sociodemographic data by interviewing the patient and reviewing the notes within 72 h of admission.
A physician prospectively classified comorbidity into three categories according to the patient's past medical history:(i) mild=no important chronic illness;(ii) moderate=moderate or severe disease of the heart, lungs, or digestive tract;(iii) severe=any cancer (other than skin cancer), end-stage renal disease, end-stage liver disease, or HIV-related disease.
Microbiological investigations included:(i) sputum gram stain,(ii) routine bacterial cultures of sputum,(iii) sputum culture for legionella,(iv) sputum direct fluorescent antibody testing for Legionella pneumophila and Legionella micdadei,(v) blood cultures, and (vi) serological testing in the acute and convalescent stages for influenza, varicella, Coxiella burnetti (Q fever), Mycoplasma pneumoniae, L pneumophila, L micdadei, and Chlamydia pneumoniae.
Additional data were obtained from procedures done at the discretion of the attending physician.
Final microbiological diagnosis was made by two infectious disease specialists who weighed all available clinical evidence.
Inpatient follow-up was conducted by review of the active medical record every 3 days.
Follow-up after discharge was done by mail and telephone.
2-year follow-up was possible in 131 of 141 patients (92%).
The 10 patients who were lost to follow-up were younger than the rest of the cohort (mean age: 48 yr vs 60 yr, p=0.05), but otherwise had similar baseline characteristics.
The patient's death was the endpoint.
Survival analysis was the primary analytic approach.
Patients lost to follow-up were censored.
Kaplan-Meier survival plots were compared by use of the Mantel-Cox log rank test.
Relative risks of in-hospital death were calculated with total numbers irrespective of survival time.
Multivariate Cox proportional hazard models were used to estimate independent associations of baseline characteristics with 2-year mortality.
The overall statistical significance of multicategorical variables was determined by applying a chi-squared test to the change in log likelihood ratios after withdrawal of all categories of the variable from the saturated model.
The assumptions underlying the proportional hazard model were confirmed by demonstrating the insignificance of time-dependent covariates.
Cut-offs for clinical variables were taken from the predictive models of Farr et al.
Results
During the 11-month interval, 147 patients with community-acquired pneumonia were admitted to the two hospitals.
Of these patients, 6 were not enrolled; thus the cohort was composed of 141 patients at baseline.
Baseline characteristics of the cohort are summarised in the table.
Compared with their older counterparts (aged 45–92), more patients in the younger age group (18–44) had either severe (45% vs 30%) or mild (20% vs 11%) underlying comorbidity.
Nonetheless, there was no statistically significant overall difference in the distribution of comorbidity by age group (p=0.2).
Microbiological diagnoses were made for 67 (48%) patients.
36 (25%) had bacterial pneumonia caused by: pneumococcus (n=17), legionella (n=7), Haemophilus influenzae (n=7), Staphylococcus aureus (n=2), or gram-negative enteric rods (n=3).
14 patients had atypical pneumonia caused by Chlamydia pneumoniae (n=9), mycoplasma (n=4), and Coxiella burnetii (n=1).
Pneumocystis carinii was the causal agent in 11 (8%) patients.
6 (4%) patients had viral pneumonia with influenza (n=5) or varicella (n=1).
No definite microbiological diagnoses could be made for the remaining 74 (52%) patients.
22 patients (16%) died in hospital during the admission for the acute episode of pneumonia.
Comorbidity showed a strong, multiplicative association with in-hospital mortality,(5.3%), 9 (11.8%), and 12 (26.1%) patients died in the mild, moderate, and severe groups respectively (relative risk [RR]=2.58 per level of severity, 95% CI 1.17 to 5.67).
A respiratory rate of more than 30 per minute (RR=2.86, 95% CI 1.37 to 5.95), and blood urinary nitrogen greater than 7 mmol/L (RR=4.02; 95% CI 1.75 to 9.17) also predicted in-hospital death (all p<0.001).
Compared with the 18–44 year age group, the groups aged 45–64 years (RR=1.30, 95% CI 0.59 to 2.82), 65–74 years (RR=0.67, 95% CI 0.21 to 2.09), and 75–92 years (RR=1.39, 95% CI 0.59 to 3.24) did not have significantly higher risk of in-hospital mortality.
The remaining 119 (84%) patients were discharged from hospital after treatment.
During the following 2 years 38 (32%) of these 119 patients died.
After losses to follow-up had been accounted for, 90% of patients were alive after 3 months, 75% after 12 months, and 62% after 24 months.
The figure (top) illlustrates survival among 119 discharged patients, who were classified by underlying comorbidity.
The severity of comorbidity at baseline was strongly related to subsequent mortality (p=0.0001).
There was only 1 death over 2 years in the group of 14 patients with mild comorbid conditions.
By contrast, those patients with severe comorbid conditions had a consistently high mortality during the 2 years after discharge: 60% were alive after 12 months and only 30% after 24 months.
As we expected, the group with moderate comorbid conditions had a mortality intermediate between these two extremes.
The figure (bottom) shows survival categorised by age group.
Although the oldest group (75–92 yr) had the poorest survival, there was no overall association between age group and survival (p=0.3).
The overlap among the three younger groups, especially during the first year, appears to account for this finding.
Similar results were obtained after exclusion of the 17 patients infected with HIV, although the consequent improvement in survival in the youngest group pushed the relation between age and mortality towards statistical significance (p=0.2).
The remaining baseline characteristics were similarly examined for associations with 2-year survival after discharge.
Characteristics that were significantly related to increased mortality included diastolic blood pressure less than 60 mm Hg (p=0.01), haematocrit of less than 35% (p=0.0001), and a body-mass index of less than 22 kg/m (p=0.001).
Blood urea nitrogen, heart rate, and respiratory rate were not related to 2-year survival.
In addition, 2-year survival was not related to any sociodemographic characteristic such as sex, race, education, and type of health insurance (all p>0.2).
Analysis of survival by microbiological diagnosis was done with total events since the small number of definite diagnoses in each patient category precluded the use of the Kaplan-Meier approach.
2-year mortality was similar in patients with bacterial (29%), atypical (20%), and viral (40%) pneumonias, but was significantly higher in patients with pneumocystis pneumonia (89%)(p<0.01, Fisher's exact test).
Multivariate analysis revealed only two independent predictors of 2-year mortality: comorbidity (RR=3.06 per level of severity, 95% CI 1.66 to 5.63, p=0.0003) and haematocrit <35% (RR=2.86, 95% CI 1.36 to 5.83, p=0.005).
Adjustment for these two characteristics revealed no strong relation between age and 2-year mortality: compared with the 18–44 year age group, there was no significantly higher mortality in the 45–64 year (RR=0.84, 95% CI 0.34 to 2.09), the 65–74 year (RR=1.28, 95% CI 0.50 to 3.32), and the 75–92 year (RR=1.99, 95% CI 0.72 to 5.46)(overall p=0.4) age group during the 2 years after discharge.
None of the remaining baseline variables showed a significant association with mortality after adjustment for comorbidity and hematocrit.
Similar results were obtained when the 17 patients with HIV infection were excluded, when age was treated as a continuous variable, and when comorbidity was modelled as a non-linear, categorical variable.
An illustrative case
The oldest patient in the study was a 92-year-old retired college professor who was admitted from the emergency department with a 3-day history of productive cough, dyspnoea, and fever.
She had no underlying disease and took no medication.
On physical examination heart rate was 100 per min, respiratory rate 18 per min, and temperature 39.5°C; she had dullness, egophony, and diminished breath sounds at the base of the left lung.
The white blood cell count was 12,100 per L. Arterial blood gas measurements while breathing room air were: pH 7.43, pCO 2 35 mm Hg, pO 2 48 mm Hg.
A chest radiograph revealed a patchy infiltrate in the left lower lobe.
Sputum gram stain showed many neutrophils with occasional intracellular diplococci.
Sputum culture yielded only normal flora, two blood cultures were negative, and the serological evaluation was unremarkable.
She received an 11-day course of empirical antibiotic therapy and was discharged.
Over the next 2 years, she remained quite active, continuing to shop, cook, keep house, visit friends, give dinner parties, and to deliver occasional lectures.
Discussion
These data confirm that signs of acute illness and underlying comorbidity are strongly related to in-hospital mortality among patients admitted to the hospital with community-acquired pneumonia.
The overall in-hospital mortality of 15.6% of this cohort was similar to short-term mortality of similar cohorts in previous studies.
The study's design provided the opportunity to examine the predictors of long-term survival after an acute episode of pneumonia.
Comorbidity emerged as the best predictor of 2-year survival.
Haematocrit, although associated with comorbidity, was also independently associated with survival.
Our most important finding, however, was that age was only weakly associated with mortality, even after adjustment for the severity of comorbid conditions.
Patients 75 years of age and older were more likely to die over the 2 years after discharge than patients between 18 and 44 years of age, but this difference did not reach statistical significance.
These results suggest that underlying health is much more important than age in determining prognosis after hospital care with community-acquired pneumonia.
Several potential limitations should be considered in interpreting the results of this study.
First, only inpatients were evaluated since we aimed to study prognosis in this more severely ill group.
These results, therefore, may not apply to persons with community-acquired pneumonia who are managed as outpatients.
Second, because nursing home residents were excluded, the finding that age is not strongly associated with 2-year mortality may not apply to these people who are at high risk for pneumonia.
Third, the severity of comorbidity was categorised broadly and was based only on past medical history.
Nonetheless, the sharp differences in survival curves between the three comorbidity groups indicate that this simple categorisation was valid.
Finally, since our main finding was the absence of a strong relation between age and mortality, it is appropriate to ask whether the study had sufficient power.
With 38 deaths over 2 years of follow-up among the 119 survivors of  the hospital admission, the study had 80% power to detect a relative risk of 3.0 between the oldest and youngest age groups.
We thought that a risk ratio of at least this size would be required for age to drive clinical decision-making.
Three strengths of the study add weight to its conclusions.
First, the prospective design made for high-quality data and minimised biases in ascertainment or classification.
Second, the degree of completeness achieved in obtaining follow-up (92%) minimised any possible bias due to different responses.
Most important, we know of no other study of community-acquired pneumonia to have obtained follow-up over 2 years after discharge.
Advanced age predicted short-term mortality from community-acquired pneumonia in 28 of 29 studies reviewed by Farr.
Why was age not as equally strongly correlated with 2-year survival in our study?
One possibility is that in excluding nursing home residents we left out an especially vulnerable group of elderly people who were more likely to die than their counterparts in the community.
Another possibility is that advanced patient age prompts a less aggressive hospital treatment so that its effect on mortality is exerted before discharge.
Age was not, however, significantly associated with in-hospital mortality in this study.
A third possibility is that physicians are less likely to admit elderly patients to the hospital for treatment, so that age's effect on mortality is exerted before admission.
This seems unlikely given the skew of this sample towards older age groups.
Physicians are more likely to admit old patients than young patients for any given severity of illness.
We believe the most likely explanation is that, over 2 years, physiological age is much more important than calendar age.
The severity of underlying illness simply overwhelms the effect of age.
The proper role of age in clinical decision-making is controversial.
On one hand, there is increasing support in the USA and abroad for the establishment of age criteria for withholding medical therapy.
Physicians may favour age criteria because they believe that the elderly are unlikely to benefit from treatment, that any benefit these patients enjoy will be of short duration, or that a diminished quality of life after treatment negates any possible benefit.
On the other hand, physicians also worry that such criteria may reflect a tendency to stereotype or depersonalise their elderly patients.
Age criteria in the treatment of myocardial infarction and respiratory failure have been criticised even though age has been shown to predict mortality of patients with these disorders.
The idea of basing decisions on generalisations about the elderly rather than on the needs of individual elderly patients has also met with criticism on purely ethical grounds.
Although preliminary, our study suggests that elderly patients with community-acquired pneumonia are almost as likely to obtain a lasting benefit from treatment as are their younger counterparts.
Thus age should not be the sole criterion for withholding aggressive treatment of community-acquired pneumonia in older patients.
This conclusion is supported by evidence that age does not influence the rate of recovery of quality of life after hospital care for pneumonia.
The population has aged since Osler's day; many of its older members are healthy and active.
To these older men and women, it seems that community-acquired pneumonia is no longer a friend.
Review article
Towards improved thrombolytic therapy
Cardiovascular diseases are an important cause of death and disability, especially in western societies.
They comprise three main categories: coronary artery disease, cerebrovascular disease, and venous thrombosis.
The triggering event in myocardial or cerebral infarction is not the atherosclerotic lesion of the blood vessel wall but the obstruction of the artery by a thrombus.
Thus, the commonly encountered cardiovascular diseases have, as their immediate underlying aetiology, thrombosis of critically sited blood vessels with loss of blood flow to vital organs.
One approach to the treatment of an established thrombosis consists of pharmacological dissolution of the blood clot by intravenous infusion of plasminogen activators that activate the fibrinolytic system (figure 1).
The fibrinolytic system includes a proenzyme, plasminogen, which is converted by plasminogen activators to the active enzyme plasmin, which in turn digests fibrin to soluble degradation products.
Inhibition of the fibrinolytic system takes place at the level of both the plasminogen activators (mainly PAI-1) and plasmin (mainly antiplasmin).
Six thrombolytic agents are either approved for clinical use or under clinical investigation in some patients with acute myocardial infarction.
These are streptokinase, two-chain urokinase (tcu-PA), anisioylated plasminogen streptokinase activator complex (APSAC), recombinant staphylokinase, recombinant tissue-type plasminogen activator (rt-PA, prepared either as alteplase or as duteplase), and recombinant single-chain urokinase-type plasminogen activator (rscu-PA, prourokinase).
Thrombolysis in acute myocardial infarction
The causal role of coronary artery thrombosis in acute transmural myocardial infarction is well established.
Rupture of atheromatous plaque leads to occlusive thrombosis that produces myocardial ischaemia and cell death leading to loss of ventricular function and possibly death.
The hypothesis underlying thrombolytic therapy in acute myocardial infarction is that early and sustained recanalisation prevents cell death, reduces infarct size, preserves myocardial function, and reduces early and late mortality.
Late opening of an occluded coronary artery may also have some beneficial effect.
Placebo-controlled studies in patients with evolving acute myocardial infarction have shown that thrombolytic therapy with streptokinase, anistreplase, and alteplase reduces mortality with acceptable frequencies of side-effects.
These data have led to increasing use (about 450,000 patients treated worldwide in 1992) and widening indications (mainly myocardial infarction but also pulmonary embolism and peripheral arterial occlusion) for thrombolytic agents.
Comparative studies of thrombolytic agents have shown differences in efficacy for early coronary artery recanalisation, but the two large GISSI and ISIS-3 trials that have compared the effect of different thrombolytic agents on mortality in patients with acute myocardial infarction have not revealed differences in early deaths.
Because of uncertainties about the design and monitoring of these trials, the GUSTO study (Global Utilisation of Streptokinase and t-PA for Occluded coronary arteries) has been undertaken in over 40,000 patients.
This trial found that the combination of an accelerated t-PA infusion regimen and intravenous heparin produced a higher 90-min coronary patency rate than standard 1.5 mega units of streptokinase with intravenous or subcutaneous heparin (81% vs 58%, p<0.001) as well as a lower 35-day mortality rate (6.3% vs 7.3%, p=0.001).
Thereby the ‘open-artery hypothesis’ of a correlation between early sustained coronary artery recanalisation and clinical benefit seems to have been validated.
Limitations of thrombolysis
Currently available thrombolytic agents have several important limitations.
Resistance to recanalisation within 90 min is observed in 15–40% of patients, whilst angiographically documented acute coronary reocclusion occurs in 5–25% of patients.
Time to reperfusion is delayed with restoration of anterograde coronary flow requiring on average 45 min or more from start of therapy.
Substantial bleeding may take place (intracerebral bleeding, 0.3–0.7%) and the residual mortality is at least 50% of that without thrombolytic treatment.
Furthermore, all available thrombolytic agents except anistreplase are cleared rapidly; their therapeutic use requires continuous intravenous infusion of large amounts of material.
Thrombolytic therapy has been based on the premise that dissolution of the fibrin component of a thrombus is necessary and sufficient for recanalisation.
However, the composition of the intraluminal thrombus formed in association with the rupture of an atherosclerotic plaque is heterogeneous, consisting of platelet-rich material contiguous to the area of plaque rupture and erythrocyte-rich material extending both proximally and distally.
This heterogeneity suggests two alternative and potentially complementary targets for coronary thrombolysis: the erythrocyte-rich whole blood clot and the platelet-rich thrombus.
While the opportunities and limitations of fibrinolytic agents for the dissolution of whole blood clots are well known, the potential for dispersion of platelet clumps and platelet-rich thrombus has not been explored adequately.
The mechanism of bleeding and strategies to reduce this important complication also need to be explored further.
Improving thrombolytic therapy
At least three complementary approaches to improve thrombolytic therapy exist (figure 2).
First, earlier and accelerated treatment to reduce duration of ischaemia.
Second, development of alternative or engineered plasminogen activators with increased thrombolytic potency and/or specific thrombolytic activity to enhance coronary thrombolysis.
Third, the use of more specific and potent anticoagulant and antiplatelet agents to accelerate recanalisation and prevent reocclusion.
Impact of early treatment
There is compelling evidence from most clinical trials that mortality reduction is greatest in patients treated early, although beneficial effects have been shown with treatment initiated up to 12–24 h after onset of symptoms.
This temporal dependence of benefit was most clearly seen with the 50% mortality reduction obtained with streptokinase given within the first hour in the GISSI-1 trial and was recently confirmed in the MITI and EMIP studies with alteplase and anistreplase, respectively.
Thus, early recanalisation must remain the main objective of pharmacologically induced coronary thrombolysis.
Continued and intensified education of the public, paramedical personnel, and physicians, together with the development of rapid and efficient triage systems, is essential to achieve these goals.
Improvements in this area may well turn out to be the most difficult to achieve.
New plasminogen activators
Several attempts either to increase the efficacy of plasminogen activators or to reduce their clearance have been undertaken.
rt-PA mutants with deletion of the finger (F), epidermal growth factor (E), and/or first kringle (K 1 ) domains have a substantially reduced plasma clearance that is, however, often associated with reduced thrombolytic activity, resulting in an unchanged or only marginally improved thrombolytic efficacy.
Domain deletion and substitution mutants of t-PA are not superior thrombolytic agents.
The exception might be a combination triple mutant of rt-PA, rt-PA-TNK, with threonine substituted for asparagine (introducing a glycosylation site), asparagine substituted with glutamine (eliminating the high mannose glycosylation site), and Lys-His-Arg-Arg replaced by alanine (resistance to PAI-1) that has a 3-to-5 fold reduced clearance, intact specific activity, and a 3-fold higher thrombolytic potency towards venous blood clots in dogs.
The t-PA of saliva from the vampire bat (Desmodus rotundus) is homologous with human t-PA but lacks the second kringle domain and the plasmin cleavage site for conversion to a two-chain form.
It is more fibrin-selective than human t-PA in experimental animal models but has not yet been tested in man.
Several chimeric plasminogen activators, consisting of various portions of t-PA and urokinase-type-PA, have been constructed in an effort to combine the mechanisms of fibrin-selectivity of both molecules.
However, the thrombolytic properties and fibrin-specificity of these chimeras are normally similar but not superior to those of the parent molecules.
Combination of a partial fibrin-affinity of t-PA with the enzymic properties of single-chain-urokinase-type PA has not improved the thrombolytic potency of the molecule.
There may be one exception: a chimera consisting of the two kringle domains of t-PA and the serine protease domain of single-chain-urokinase-type PA was found to have a 10-fold delayed in-vivo clearance with a maintained specific thrombolytic activity and a strikingly enhanced thrombolytic potency in venous and arterial thrombosis models in animals.
Murine monoclonal antibodies conjugated with plasminogen activators may be valuable for targeting the therapeutic agent to thrombus.
A thrombus contains both fibrin-rich and platelet-rich material, and targeting of plasminogen activators could be achieved by either antifibrin or antiplatelet monoclonal antibodies.
A recombinant fusion protein of a single-chain antifibrin antibody with a truncated scu-PA is the only reported example of a plasminogen activator with a significantly increased in-vivo thrombolytic activity in animal models.
Its potential clinical value remains to be evaluated.
In summary, several approaches have produced engineered plasminogen activators with an increased thrombolytic potency in experimental animal models.
Whether these agents will be safe enough for clinical use in man remains unknown.
Furthermore, if the recently reported patency rates of 85% obtained with front-loaded alteplase and conjunctive intravenous heparin can be linked with strategies to prevent reocclusion, the potential additional benefit that could be gained with engineered plasminogen activators might only be marginal.
Staphylokinase, a 136-aminoacid protein produced by certain strains of Staphylococcus aureus, was shown to have profibrinolytic properties more than four decades ago.
Initial evaluation of this agent's thrombolytic properties in dogs gave discouraging results and further interest in the development of staphylokinase as a thrombolytic agent diminishes.
However, staphylokinase has now been found to be more potent than streptokinase for the dissolution of platelet-rich arterial eversion graft thrombi; moreover, it seems to be less immunogenic.
In a pilot study with 10 mg recombinant staphylokinase given over 30 min to 5 patients with acute myocardial infarction, angiographically documented coronary artery recanalisation was obtained within 40 min in 4 patients.
Plasma fibrinogen remained unaffected, but neutralising antibodies, which did not cross-react with streptokinase, were consistently found in plasma at 14–35 days.
Antithrombotic co-therapy
Aspirin and heparin have a limited impact on the speed of coronary thrombolysis and on the resistance to lysis, and do not consistently prevent reocclusion.
This result could have been anticipated on the basis of aspirin's non-selective inhibition of the synthesis of both pro-aggregatory and anti-aggregatory prostaglandins, and of the ineffectiveness of heparin for the inhibition of clot-associated thrombin.
Several more specific approaches to reduction of platelet aggregation are currently being explored — eg, thromboxane synthase inhibitors, antagonists of serotonin and endoperoxide receptors, monoclonal antibodies against the platelet GPIIb/IIIa receptor, arginine-glycine-aspartic acid (RGD) -containing peptides derived from viper venoms, and small synthetic RGD-containing peptides.
Another approach is the use of selective thrombin inhibitors, including hirudin and its derivatives, or synthetic thrombin inhibitors.
Some of these agents have been shown to be more effective than aspirin and/or heparin for the prevention of arterial thrombosis, for the acceleration of arterial recanalisation, and for the prevention of early and delayed reocclusion after reflow.
Unfortunately, most of these combinations also produce a substantial lengthening of the bleeding time, which may be suggestive of an increased bleeding risk.
Conclusions
The beneficial effects of thrombolytic therapy in acute ischaemic coronary syndromes, and especially in acute myocardial infarction, are now well established.
However, the limited efficacy and potentially life-threatening side-effects of currently available thrombolytic agents remain a difficulty.
Available evidence suggests that the efficacy of coronary thrombolysis could be augmented by shortening the time to treatment, by improving of the potency and specificity of fibrin-dissolving agents, by improving co-therapy with anticoagulant and antiplatelet agents, or by a combination of all of these.
While it is clear that compounds with antithrombin or antiplatelet properties may enhance and sustain the action of thrombolytic agents, their optimum use and potential haemorrhagic side-effects remain to be explored.
Improved thrombolytic therapy will most likely consist of potent specific plasminogen activators in combination with targeted anticoagulant and/or antiplatelet agents.
BOOKSHELF
Cell Interactions in Atherosclerosis
Edited by Horst Robenek, Nicholas J Severs.
Boca Raton: CRC Press.
1992.
Pp 318. £136.
ISBN 0-849355052.
I find it puzzling that books are published which describe meetings that took place several years beforehand.
Promotions committees are not impressed with book chapters.
Editors are not well paid (one hears) and contributors are usually not paid at all(one knows!).
Is it the opportunity to slip in some data that are not strong enough to form a peer-reviewed paper or the liberty to digress and speculate which entices authors?
Or is it that a book still has some special appeal in this electronic age?
Cell Interactions in Atherosclerosis does not address all aspects of its broad title, but it does provide an interesting collection of articles focused largely on morphological studies of atherosclerosis, transplant arteriopathy, and experimental atherosclerosis in the rabbit.
Despite a pile of unread reprints now measuring 16 inches tall, I read this book because of the nagging suspicion that in a field that has grown as rapidly as vascular biology and medicine I might be missing more than my 16 inch stack suggested.
I was happy to find that in the scholarly (237 references in 1991) introductory chapter written by the editors most of the territory was familiar.
The authors cover the anatomy and physiology of large and small arteries; endothelial and smooth muscle cells; haemostasis; and some aspects of the control of growth migration and synthesis of extracellular matrix.
Macrophages, their growth factors, and their receptors for low-density lipoprotein (LDL) and oxidised LDL are also included.
They also review the history of the main hypotheses of atherogenesis: response to injury, lipid infiltration, monoclonal smooth muscle proliferation, thrombogenic ‘encrustation’, and autoimmunity, but they weave these together into what can fairly be called a consensus view.
High circulating concentrations of LDL and plasma cross the endothelium by transcytosis (probably accelerated when endothelial cells are biochemically injured by oxidised LDL, cytokines, thrombin, carbon monoxide, viruses, or low shear stress opposite branch points) leading to subendothelial accumulation of LDL.
Minimal oxidation of LDL by endothelial and smooth muscle cells leads to the release of chemoattractants for monocytes which attach to activated endothelial cells.
Monocytes pass between adjacent endothelial cells, proliferate, and differentiate into macrophages that take up oxidised LDL through their scavenger receptors.
Macrophages are also likely to contribute to LDL oxidation and to release of growth factors — such as platelet-derived growth factor, transforming growth factor, fibroblast growth factors, heparin-binding epidermal growth factor, various interleukins, interferon gamma, and tumour necrosis factor alpha — that contribute to monocyte and lymphocyte recruitment and proliferation.
Macrophage death probably accounts for the cellular debris and cholesterol pools found in advanced lesions.
Oxidised LDL may also contribute to atherosclerotic lesions by toxic effects on endothelial cells, including inhibition of release of nitric oxide, inhibition of endothelial cell migration and proliferation, and formation of immune complexes with oxidised LDL.
Atherosclerotic lesions also show a loss of heparan sulphate proteoglycans — inhibitors of smooth muscle cell proliferation — and an increase in chondroitin sulphate proteoglycan, which retains LDL, thereby inhibiting HDL-mediated cholesterol loss from the vessel wall.
Finally, the authors advance the widely held view that thrombosis contributes not to the initiation of the process but to its progression.
Procoagulant factors include lipoprotein (a) that contains plasminogen-like domains that allow it to compete with plasminogen for binding sites on endothelial cells, thus impairing fibrinolysis.
Oxidised LDL may also be procoagulant since it has been described as stimulating endothelial tissue factor activity and inhibiting activation of protein C. Despite these tremendous advances, the authors wisely remind us that many patients with coronary disease have few or none of the known risk factors.
Nor do we understand why some atherosclerotic plaques are especially prone to ulcerate or rupture, or even why some arteries, such as the internal mammary or brachial arteries, uncommonly develop atherosclerosis.
This book contains many descriptive data, most of which had either been previously published or confirm previous studies.
Topics that are not discussed, except for a brief mention in the introductory chapter, include nitric oxide, endothelin, vasomotion, homologous recombination, thrombosis, vascular innervation, genetics of atherosclerosis, and the roles of flow and shear, selectins, vasa vasorum, hypertension, cigarette smoking, and psychosocial risk factors.
Many growth factors are ignored including my favourite, basic fibroblast growth factor.
It is also unfortunate that in such a morphologically oriented book there was not some attention to atherosclerotic aneurysms.
Nevertheless Cell Interactions in Atherosclerosis will be useful to lipidologists who wish to update their morphological knowledge.
Inhibitors of Monoamine Oxidase B
Pharmacology and Clinical Use in Neurodegenerative Disorders — Ih Szelenyi.
Basel: Birkhauser. 1993.
Pp 360. 148 Sfr.
ISBN 3-764327820.
Monoamine oxidase (MAO) is involved in the endogenous metabolism of several neurotransmitters and has an important position in the metabolism of several amines derived from our natural environment.
Thus, like other xenobiotic enzymes, it has an important protective function in coping with our chemical environment, as witnessed by the effects of cheese when MAO is inhibited.
Inhibition of MAO-B is also of topical interest since this may prove to be the first manoeuvre that has slowed the progression of Parkinson's disease; the mechanism is uncertain but it was originally thought of because this approach blocked the toxicity of the selective dopaminergic poison, MPTP.
Szelenyi has compiled a wide-ranging selection of reviews on Parkinson's disease, the chemistry and pharmacology of MAO-B inhibitors, and the potential applications of these drugs to other disease states such as Alzheimer's disease and depression.
Unlike most xenobiotic isoenzymes MAO has not yet been linked with drug metabolism and classed under pharmacogenetics.
But as with many enzymes, there is enormous variability within the population.
They are involved in drug metabolism and endogenous metabolism, as well as detoxification or activation of protoxins.
They form a vital component of our defences against chemical attack from trace compounds found largely in our diet.
There are obvious analogies to the immune system, including the fact that these proteins can be induced and that there are extremes of individual variability often related to the presence of polymorphisms that can lead on occasion to hypersusceptibility to the effects of environmentally derived agents.
Yet there has been remarkably little research activity in this area.
If a small proportion of the effort and money given to the study of immunology and human disease had been given to the study of pharmacogenetics in specific disease states much would have been learned.
An obvious starting point is to look at conditions where chemicals are known to produce similar syndromes and these have to include not only the neurodegenerative diseases but also migraine, myasthenia, and cancer.
The authors in this volume have accepted conventional wisdom and have not realised that such an approach could lead to the understanding of fundamental and avoidable risk factors for these ecogenetic diseases.
The Pharmacology of Chinese Herbs
Kee Chang Huang.
Boca Raton: CRC Press.
1993.
Pp 388.
ISBN 0-84934915X.
Literature on the use of traditional Chinese medicine has existed in China for over 4000 years but it is only in recent times that western scientists have been alerted to its hidden potential.
Perhaps the most difficult thing for non-Chinese to understand about Chinese herbal medicine is the terminology used to describe symptoms for which various herbs are effective.
Hence there are no readily understandable anatomical or physiological correlates to the excesses or deficiencies of Ying, Yang, or Qi or to the expression of liver and spleen disharmonies.
Therefore there is a different perception of illness between traditional Chinese and western physicians and this needs to be understood before the individualised prescription of treatments can be explored.
The next most difficult aspects to understand about Chinese herbal medicine are the rationales for using specific parts of plants and the mixing of herbs to prepare a particular formulation.
The whole plant, the root, stem, bark, leaves, flowers, petals, seeds, fruit, or even pollen could be used and these may be fresh, dried, or boiled.
Combinations of herbs have been proposed on the basis of their synergistic effects and the quenching of their toxic effects.
It is not surprising that the skills of Chinese herbal medicine handed down from generation to generation are viewed with scepticism by many outsiders.
Publication in the Chinese language of anecdotal cases treated in this way has not helped in disseminating knowledge about the use of traditional Chinese herbal medicines.
What is clearly needed now is for rigorous western-style clinical trials to explore the efficacy and safety of formulations of herbs and for the pharmacology of these herbs to be investigated.
The isolation of ephedrine from ma huang in 1923 and more recently a group of antimalarial drugs qing hao from Artemisia in the 1970s has exemplified the important compounds in the armamentarium of traditional Chinese herbs.
Multiple active compounds in individual herbs makes understanding the pharmacology of these medicines exceedingly difficult and this is further complicated when multiple herbal formulations are used.
The encyclopaedic compendium written by K. C. Huang focuses on 428 individual herbs and classifies them according to their therapeutic value in different body systems and then in terms of their chemical composition, pharmacological action, toxicity, and therapeutic use.
Altogether this book contains a wealth of information and I doubt whether it will be long before physicians are heard expounding the benefit of Liu Ye for tuberculosis, Jia Zhu Tao for its cardiotonic effects, Man Tao Luo as a general anaesthetic, or Yin Chen to treat hepatitis.
Predicting the Future
Edited by Leo Howe and Alan Wain.
Cambridge: Cambridge University Press.
1993.
Pp 195. £18.95/$29.95.
ISBN 0-521413230.
This collection of eight essays is derived from the annual Darwin College lectures delivered in Cambridge in 1991.
As Ophelia said, we know what we are but know not what we may be.
That uncertainty urges us to look beyond the present, with a faint hope to control our future.
Between extrapolation from the present and crystal-ball gazing there is a wide field of predictions, speculations, hopes, and fears.
Those invited to lecture at Cambridge represented all types of prophets and the collection makes fascinating reading.
Stephen Hawking begins with the future of the universe.
In a billion years the universe will be on the knife's edge of expansion or collapsing again, depending on its critical density.
Hawking thinks it can go either way but we will never know.
Ian Stewart's lecture brings a complementary view of chaos and order in the prediction of patterns in science and the art of quilts.
More down to earth are essays on predicting the economy by Frank Hahn and the frontiers of medicine by Ian Kennedy.
They are inter-related and Kennedy's discussion of bioethics at life's end makes compelling reading.
Other parts of the book deal with the last things on a global scale, both looking back and looking forward.
Divine providence, the last judgment, comets and the world's end, and Buddhist predictions as guiding ideals or admonitory pictures are dealt with, sometimes in a rather esoteric exposition.
This book probably presents what we all expect from the future: that anything can happen, between cosmology and religion.
Female Pelvic Floor Disorders
Edited by J Thomas Benson.
London: W W Norton.
1992.
Pp 435. £65.
ISBN 0-393710130.
As a gastroenterologist working at a tertiary care centre, I am frequently asked to evaluate patients with faecal incontinence and defaecatory difficulties.
The complexities of anatomy and pelvic-floor physiology may be a challenge to even the best general physicians who are rarely faced with these patients.
Adding to physiological disturbances is the psychological distress that is invariably involved.
One can find it difficult to discuss a patient's complaint when society's puritanical view of the anorectal and urogenital region is so strong.
Patients are often embarrassed or hesitant to describe in detail why they have difficulty evacuating their bowels or what the sensation is like before an incontinent episode.
The physician must establish a sympathetic rapport with the patient to help elucidate possible causes and contributing factors.
Many patients attribute their symptoms to an antecedent event (eg, post-partum faecal incontinence associated with a perineal tear) or they may present with chronic symptoms that have recently progressed (eg, stress urinary incontinence or chronic constipation).
In my experience, the information gleaned on the interview and physical examination is frequently as valuable as objective data obtained by diagnostic examination.
Clearly, they are complementary and appropriate studies — radiography, anorectal manometry, electromyography, cystometry — must be performed.
Benson's text is a welcome addition for those who treat female patients with various urogenital and anorectal complaints, and it should appeal to a broad range of readers, from medical students to subspecialist physicians.
The grouping of urogenital and anorectal dysfunction under the term ‘pelvic-floor disorders’ is beneficial since the two are commonly encountered together in clinical practice.
Introductory chapters provide a solid foundation covering anthropology, anatomy, and ‘functional mechanics’ of the female pelvic floor.
The physiology of micturition, anal continence, and defaecation is reviewed as an introduction to the comprehensive section on diagnostic studies.
Most of the text is devoted to a multidisciplinary approach to various pelvic-floor disorders.
Psychosocial issues, pathophysiology, and medical and surgical approaches are addressed.
Excellent reviews covering urinary and faecal incontinence, constipation, postmenopausal vaginal dysfunction, and anatomical defects — cystocele, rectocele, rectal prolapse — are supplemented by clear and instructive photographs and diagrams.
Despite the large number of expert contributors, a uniformity of style is maintained that preserves authors' unique approach to their own particular subject.
Our knowledge and ability to treat pelvic-floor disorders has benefited enormously from the availability of newer diagnostic and surgical approaches.
Each patient presents a challenge that requires a specific strategy.
As physicians encounter an increasing number of these patients, Female Pelvic Floor Disorders should prove invaluable as a primary reference source and guide to diagnosis and management.
Selected Books: Anaesthesia
Handbook of Clinical Anaesthesia (second edition).
— Paul G Barash, Bruce F Cullen, Robert K Stoelting.
Philadelphia: Lippincott. 1992; Pp 656. $29.95.
ISBN 0-39751297X.
Anaesthesia for Burns, Maxillofacial and Plastic Surgery.
— Edited by H Patel.
London: Edward Arnold.
1993.
Pp 160. £45.
ISBN 0-340553022.
Percutaneous Local Anaesthesia.
— David Woolfson, Dermot McCafferty.
Chichester: Ellis Horwood.
1993.
Pp 270. £65.
ISBN 0-136563724.
Baillière's Clinical Anaesthesiology: The Liver and Anaesthesia.
— Edited by L Strunin, S Thomson.
London: Baillière. 1992.
Pp 956. £27.50.
ISBN 0-702017361.
Effects on the Baby of Maternal Analgesia and Anaesthesia.
— Felicity Reynolds.
London: W B Saunders.
1993.
Pp 282. £25.
ISBN 0-702015741.
Clinical Applications of Magnetic Transcranial Stimulation.
— M A Lissons.
Leuven: Peeters Press.
1992.
Pp 300. £45.
ISBN 9-068314343.
Endpiece
The glomerulus
The glomerulus filters about 120 mL plasma/min.
This rate is maintained over a wide range of aortic pressures by means of afferent and efferent arteriolar autoregulatory mechanisms, such as angiotensin II, renal prostaglandins, contractile myoepithelium, and the glomerular mesangium.
The figure shows the key structures of the renal filtration barrier.
Taken from the third edition of Cecil Essentials of Medicine, edited by
T E Andreoli, J C Bennett, C C J Carpenter, F Plum, and L H Smith.
(Philadelphia: W B Saunders.
1993.
Pp 921. £29.95.
ISBN 0-721648126.)
NEWS
WASHINGTON PERSPECTIVE
Another recipe for financing science
The sciences continue to have a bad season in Washington.
Budgets are flat for the two mainstays of academic research, the National Science Foundation and the National Institutes of Health.
Clinton and company remain focused on industrial technology, to the seeming neglect of fundamental research, and Congress is in a sour, anti-spending mood.
On June 24, the House voted 280–150 to kill the Superconducting Super Collider, saying the nation cannot afford the colossal accelerator, now estimated at $11 billion.
Resurrection is possible, but increasingly doubtful, even with nearly $2 billion already spent.
On the previous day, the Space Station, priced at $30–$40 billion, with $8 billion spent, survived by only one vote, mainly because it provides jobs in a sickly aerospace industry.
Scientists disavow the Space Station as irrelevant, but Capitol Hill tends to bunch these things together as ‘science’, and insists that we can't have it all.
Clearly, an unwholesome condition has developed in the previously productive, though often contentious, relationship between science and government.
What's needed is a new rationale for opening the treasury.
Now comes the National Academy of Sciences, the advice-spewing, honorary pinnacle of American science, with a super-nationalistic design for re-establishing harmony between science and its balky patron: Science, Technology, and the Federal Government: National Goals for a New Era (54 pp, free from the NAS Committee on Science, Engineering, and Public Policy, 2101 Constitution Ave, NW, Washington, DC 20418).
Produced by a cast of mandarins drawn mainly from academe, the new recipe proceeds from a basic assumption: that international economic competition must be regarded as the principal successor to the Cold War in motivating federal support of science.
Other factors apply, too, such as recognition ‘that leadership in science has become one of the defining characteristics of great nations’.
What it adds up to, says the committee, is that the US ‘should be among the world leaders in all major areas of science’.
But, at the same time, the US ‘should maintain clear leadership in some major areas of science’.
The two categories would be determined ‘by government decisionmakers with appropriate advice’.
‘Major areas’, says the report, ‘refers to broad disciplines of science (such as biology, physics, mathematics, chemistry, earth sciences, and astronomy) and to their major subdisciplines (such as the neurosciences, condensed-matter physics, and seismology)’.
‘Among the world's leaders’ means that the United States should have capabilities and infrastructures of support that are not exceeded elsewhere.
‘Of course, there will be specific areas or skills in which other nations lead the world.’
The areas in which the US should maintain ‘clear leadership’ are to be selected on the basis of a melange of criteria.
One is that ‘the field is demonstrably and tightly coupled to national objectives that can be met only if US research performers are clear leaders’.
An example is condensed-matter physics, because it ‘drives technological advances in such industrial sectors as microelectronics, advanced materials, and sensors’.
Another spur to ‘clear leadership’ would be a field ‘that so captures the imagination that it is of broad interest to society’.
The example here comes from astronomy's ‘recent detection of differences in the cosmic background radiation related to the creation of the universe’.
Finally, the committee recommends, the US should be No 1 where a ‘field affects other areas of science disproportionately and therefore has a multiplicative effect on other scientific advances’.
Molecular biology is the example offered here.
The committee rejects as ‘inadequate’ the traditional measures of ‘dollars spent or numbers of scientists supported’, asserting that ‘comparative international assessments for scientific accomplishment are a better yardstick for policy decisions’.
To produce these yardsticks, it recommends ‘independent panels’, including the ‘users’ of research and ‘outstanding foreign scientists’.
Once they have gauged US performance against that of other nations, adjustments would be made to assure parity or leadership.
The committee suggests that its recommendations could be accomplished without additional spending — a novelty in science's advice to government.
This optimism is keyed to the possibility that the US might lead in a field in which superiority is not a national goal.
In that case the panel might recommend reductions in funding, which could then be applied to other areas.
By the current standards of research finance, this is a far-fetched, politically naive scheme, long on committee intricacies so beloved at the committee-bound Academy.
Will Washington listen?
Eventually perhaps, but a Senate hearing on June 22 on the proposal drew only two Senators, one of whom quickly departed for another engagement.
The major deficiency in the Academy plan is the assumption that science suffers from the end of the Cold War.
In fact, politics is as keen as ever for research.
But the public is persuaded that deficit reduction comes first, which spells austerity for science and much else.
Tainted-blood commission
Canada's provinces have agreed to appoint a three-man commission to investigate the efficiency, safety, and management of the country's blood-supply system.
But it remains to be seen whether the commission will examine events of the early 1980s which led to 261 Canadians being infected by contaminated blood and about 800 haemophiliacs becoming infected by tainted blood products.
While health ministers agreed that there was a need for an inquiry, as requested by former federal health minister Benoit Bouchard (see Lancet 1993; 341: 1465–66), the June 22 gathering of health officials was unable to define the scope of the commission.
Moreover, the provinces believe there is no need to determine if the management of the blood-supply system was negligent in the pace at which  it implemented heat treatment and blood screening measures.
Federal assistant deputy minister (health protection branch) Kent Foster repeatedly stressed that the provinces wanted to ensure the ‘future’ safety of the system.
When asked point-blank if the inquiry will examine 1980s events and assign liability, Foster told reporters, ‘there is no intention to put any sort of difficulties in front of the commission.’
The deputy ministers will present a plan outlining the inquiry's terms of reference, cost, and scope to a Sept 14–15 meeting of Canada's health ministers.
The inquiry will be conducted by three commissioners, appointed by Ottawa from provincially submitted lists of candidates and its findings are expected to lay the groundwork for a complete overhaul of the blood system within a year.
Meanwhile, the deputy ministers claim progress on compensation for blood-transfused HIV-infected Canadians.
Alberta deputy minister Don Phillipon said several of his counterparts agreed to consider a ‘common framework’ for compensation and will be discussing the package with their respective governments.
Phillipon refused to say whether the framework assigns nominal dollar amounts for each individual or uses a funding formula.
However, a provincial offical says privately that it merely outlines ‘the need for commonality on such elements as income replacement and survivors’ benefits but leaves to each province responsibility for determining the level of compensation’.
Phillipon told reporters that Quebec and Nova Scotia are not party to the framework since they had broken ranks with a 1991 provinical agreement to disdain unilateral compensation by earlier this year unveiling vastly discrepant compensation programmes.
In short, it does not appear that there will be anything like standardised compensation across the nation.
And with health care a provincial jurisdiction, there is no way to force a province to provide a specified level or form of benefit.
Still, that most provinces have reached a stage where compensation of any form is under consideration is itself a remarkable turn of events.
In 1989, the federal government unveiled a three-year ‘humanitarian aid’ package, which provided $30,000 annually to blood-transfused HIV-infected persons who agreed to waive federal liability in the tainted-blood scandal.
Some 943 Canadians, including 677 haemophiliacs, set the test.
For their part, the provinces promised to provide $360,000 to each affected person to cover medical costs and survivor benefits.
But the provinces reneged on the promise (prompting a spate of civil law suits) and in 1991 made a silent compact to leave all matters of compensation to Ottawa.
When Ottawa's programme expired in April, pressure mounted on the provinces to provide assistance.
Nova Scotia became the first province to break the compact earlier this year when it agreed to provide 16 infected individuals with a non-taxable $30,000 per year until they die, and to reimburse the cost of drugs, most of which are not covered by Medicare.
In addition, because most HIV-infected people are not eligible for life insurance, these people will each receive $50,000 in survivor benefits and $5000 for funeral expenses.
And the postsecondary education expenses of their children will be covered for four years.
Earlier this month, Quebec also broke ranks, announcing that $10 million in aid will be issued to the province's 256 known infected haemophiliacs and transfusion patients.
But health minister Marc-Yvan Cote said the monies won't be divided equally.
Rather, a means test will be developed and introduced to determine eligibility and level of payments.
But the Quebec offer has been widely criticised as inadequate.
Canadian Haemophilia Society (CHS) president David Page dismisses it as ‘abysmally insufficient’ and calls Cote's plan to introduce a means test an affront.
Still, the final details of the Quebec package have yet to be determined, although it appears that the province won't be asking these people to renounce their right to sue.
Whether that will hold true across the country remains to be seen.
Alberta, with an estimated 80 affected people, and British Columbia, with 115, have now entered negotiations with provinical CHS chapters to determine compensation, while Prince Edward Island, and Saskatchewan are expected soon to follow suit.
Meanwhile, Ontario health minister Ruth Grier says she'll press her cabinet colleagues to provide relief for the province's estimated 500 affected people.
At $30,000 a year, that would cost about $15 million annually.
Health for WHO?
Almost half of patients with hypertension in Europe are being treated inadequately, according to a report from the WHO. 39,000 men and women from eight countries — Hungary, Slovenia, Italy, Romania, Israel, Germany, Switzerland, and Spain — were randomly examined.
The prevalence of hypertension ranged from 5–15% in industrial populations to 20–27% in the general population.
Only 29% of those with hypertension had received treatment to achieve normal blood pressure levels.
Since at least 17% of hypertensives were newly discovered, at least 46% of confirmed cases were being treated insufficiently.
Investigations — fundoscopy, serum potassium, and chest radiography — varied substantially, up to one quarter of physicians rounded the blood pressure reading up to 10 mm Hg, and although risk factor intervention was said to be important, cholesterol measurement was done by as few as 27% of clinicians.
The disappointing results of this survey come at a time when the second WHO review of its Health for All policy concludes that ‘the implementation of strategies to achieve those aims has in many cases slowed down’.
Ageing populations, the growing importance of non-communicable disease in developing countries, the rise in outbreaks of tropical disease, and the threat of the HIV pandemic have placed added burdens on already stretched programmes.
151 countries out of 168 submitted reports to WHO, representing about 5200 million people.
A particular concern is the slowing of economic growth — sustainable development for many countries has been impossible.
The report identified five challenges that face the Health for All strategy.
First, accountability by governments to their least-favoured populations; second, definition of the role of governments in health care; third, resources for health care, health inequalities, and human rights' issues; fourth, implementation of health measures; and fifth, international cooperation over health.
Sexuality goes public in Turkey
The surprise election of the first woman Prime Minister in Turkey has coincided with other small signs of liberalisation in an otherwise traditional society.
Two weeks ago, for example, Halim Hattat, professor of uroandrology at Istanbul University, issued a well publicised invitation for Turks with sexual problems to come forward for treatment.
In the West, such a suggestion would have barely attracted the attention of the press, but in Turkey this call made national headlines.
According to Hattat, 10% of the country's 30 million males between the ages of 20 and 60 suffer from some kind of sexual dysfunction.
In his view this figure is comparable with that in other countries, but until recently men with such problems would not have admitted the fact, and would certainly not have sought treatment.
Now, he finds people are beginning to be more open about their sexuality, and consequently he has been promoting the activities of the centre for sexual problems in Istanbul.
So far, approximately 5000 patients have been assessed at the centre which is run by a multi-disciplinary team of physicans.
The vast majority of those who have come forward are men with erectile dysfunction.
Although the centre is also anxious to attract women, this will take some time.
According to Hattat, even when men can begin to discuss their virility openly, women who did the same would be branded as immoral.
The government has a particular interest in encouraging this increased openness in issues relating to sex and family life.
Although not particularly liberal on moral issues, it recognises that a climate of opinion in which people can discuss sexual and emotional matters will be one in which the birth control campaign is more likely to succeed.
In Turkey, an annual population growth of 1 million is predicted, on an existing population of 60 million.
So far, women have borne the brunt of the responsibility for birth control.
IUCDs are the most common method used, followed by oral contraceptives.
Barrier methods such as condoms are not favoured by Turkish men, and campaigns to encourage men to have vasectomies have proved a dismal failure.
According to figures supplied by Dr Hattat, only 300 vasectomies have been done in Turkey so far, and he has done fewer than 10 in the past year.
‘What is needed’ he said ‘is to reassure men that such a procedure does not affect sexual function.’
With unemployment rising steadily and currently standing at around 5 million, and inflation running at 70% per annum, the country can ill afford a rapid increase in population.
Only 25% of the population are covered by the basic health care system provided by the State, and the rest have to look after themselves.
According to a recent issue of the Turkish journal Economy, 40 out of every 1000 infants born in the country die from dietary deficiency and malnutrition.
The new Prime Minister Mrs Tansu Ciller is a self confessed disciple of Margaret Thatcher, and as such may not have much time for a liberalisation of sexual attitudes.
However, if the result is a more effective birth control programme, it may gain her active support.
Pesticide levels in food to be reduced
The US government announced on June 25 a major effort to reduce the level of pesticides in the food supply.
Dr David Kessler, commissioner of the US Food and Drug Administration said that his agency, the Environmental Protection Agency, and the Department of Agriculture will for the first time make a concerted effort to ‘create incentives for the development of safe pesticides and to remove those pesticides that pose the greatest risk’.
In the past, these three agencies have, for the most part, worked independently on pesticide issues.
Kessler called the policy change a ‘landmark in the history of food safety’.
In a related development, the National Research Council (NRC) issued its long-awaited report on June 28 on pesticides in the diets of infants and children.
The report concluded that current US regulations must be changed to protect infants and children from pesticide toxicity.
Under current US law, the government must set a legal limit, called a tolerance, for the amount of pesticide residue that can be in or on a food.
To determine these tolerances, regulators use data based on the average exposure to a particular pesticide of the entire population.
But children, the panel noted, eat fewer foods than adults, diffferent foods from adults, and take in more calories per unit body weight than adults.
As a result, the dietary exposure to pesticides in infants and children may be very different from that of the general population.
In addition, the danger of toxicity may be greater for children during ‘special windows of vulnerability — brief periods early in development when exposure to a toxicant can permanently alter the structure and function of an organ system’.
The panel recommended that regulators take the deficiency habits of different age groups, ethnic groups, and of the populations of different geographic regions into account when setting pesticide residue tolerances.
They also recommended that regulators take into account the fact that children may be eating foods containing several different pesticides that share common toxic effects, and that children are exposed to pesticides from other sources, such as drinking water, air, and soil.
Future safety testing of pesticides should be conducted on immature animals to ascertain the effects of exposure on the young, the panel said, and researchers should develop bioassay protocols that provide information on the contribution exposures to chemicals at different ages have on lifetime risks for disease.
Medicine and UK prisons
The issue of the prison doctor's role in the management of potential suicide was the most important part of the conference Medicine within Prison 1993, organised by Physicians for Human Rights (UK), in London, last week.
Inquest was established in 1980 to support families and friends of people who die in controversial circumstances.
Co-director Deborah Coles recounted how she had spent the last 3 days attending the inquest on a man known to be at risk, who committed suicide in prison.
‘F’ had a previous history of depression, drug use, and self-harm in prison.
Soon after being remanded to Pentonville Prison he was attacked by his psychotic cellmate who had set fire to the cell.
Three weeks before his death, F, terrified of being attacked again, became increasingly depressed and was transferred to the hospital wing.
Without authorisation he was placed in a strip cell (a room without furniture) clad in only a canvas suit for 3 days, and was then transferred to a cell, still on his own, where he was observed every 15 minutes.
2 days before his death the chaplain expressed concern about F's suicidal state of mind but his family were assured by staff that he was being properly cared for.
The day before F's death the prisoner who had attacked him was moved into a nearby cell, albeit under continual observation.
Deborah Coles described the testimony of the medical officer responsible for F as an attempt to avoid questions about the justification for F's treatment.
The Howard League for Penal Reform is also concerned about the high incidence of prison suicide — last year 42 people took their lives.
Its director, Frances Crook, spoke of Philip Knight who is likely to become a cause celebre as a result of a television programme to be screened in August.
He was only 15 when he hanged himself in the hospital wing of Swansea Prison hours after being found guilty in court.
He had made 2 previous attempts to commit suicide.
He also had been confined to a strip cell.
The last thing a child of 15 needed, Frances Crook forcibly suggested, was cell confinement and sensory deprivation.
What these people do need, she insisted, is kindness and support and access to their family.
Doctors should not collude with the authorities and ‘medicalise’ people who are suicidal.
They must refuse to authorise the use of strip cells — they must not be involved in punishing people who are desperate and suicidal.
Fundamental change is needed agreed Dr Sinclair, who is assistant director of the Prison Health Service.
But the main problem, he stressed, is one of resources.
There are about 110,000 people passing through prisons annually but only 125 full-time and 137 part-time doctors to care for them.
Frances Crook acknowledged that the high incidence of suicide at Brixton Prison has led to a redistribution of resources in London.
Brixton now has 96 health-care staff but Pentonville, with 100 more prisoners, has only 22.
The last convoy to Gorazde?
As it was decided that there would be a better chance of getting across the front line without a military escort, WHO's Head of Office in Sarajevo Dr Risto Tervahauta and the UNHCR Chief of Operations Larry Hollingsworth left for Gorazde, Bosnia, at 6 am on May 1 unaccompanied by UN soldiers.
At Podromanija they met up with the Swedish relief convoy of 10 trucks, 9½ being filled with food and the remainder with medical supplies.
Despite assurances that the checking at the Serbian side of the line would be a formality the convoy was delayed at the checkpoint for twenty-eight hours.
Eventually at 5 pm on May 2 the convoy crossed the Serbian front line and advanced cautiously through ‘no man's land’ with the UNHCR Land Rover leading and the WHO Toyota in close support.
A huge hole due to a previous mine explosion and rocks in road tunnels caused further delays.
At the Bosnian line, the checkpoint was unoccupied but the road was blocked with a vast heap of earth.
After a long wait they tried to attract attention by sounding their horns.
Eventually two Bosnians appeared and removed some mines.
Meanwhile the noise was ear-splitting: this road passes along the bottom of a gorge and the Serbs and Bosnians on the two sides were exchanging shells and mortar bombs which caused tremendous reverberations from the cliffs.
The convoy arrived in Gorazde at 9.30 on May 2.
The town contains about 70,000 people of whom 30,000 have fled from other areas.
The next morning Dr Tervahauta visited the hospital, which is a converted health centre with fifty beds of which twenty-eight are surgical.
The operating theatre is staffed by two surgeons from Sarajevo who work in two-week shifts.
Although tap water was available for sterilisation by boiling, the generator had little fuel left.
There were many wound infections and the first floor of the hospital stank of suppuration.
Most of the patients were soldiers and civilians with abdominal wounds or with limb injuries requiring amputation or external fixation.
One patient had suffered grade three frostbite and had had both feet amputated.
There were no sheets.
About one hundred and fifty wounded patients are treated and eighty major operations conducted each month.
In addition about thirty babies are delivered.
In the street Dr Tervahauta saw many amputees and was told that two hundred amputations had been done within the town since the beginning of the siege.
During this winter approximately two hundred other patients requiring amputations had been carried or hauled in sledges to other hospitals in central Bosnia.
So many amputations had been necessary because of the lack of facilities for sophisticated vascular surgery.
There had been many injuries due to frostbite.
The general sanitary situation in Gorazde seemed reasonably satisfactory.
Although during the winter there had been no running water this had been restored at least in the centre of the town.
The Drina River flows through Gorazde and there are wells.
Fortunately, people have enough firewood to boil the water.
The nutritional situation is similar to that in Sarajevo.
There are many  thin people in the streets and no one was seen to be overweight.
The home stocks of food have run out.
Until one month ago it was possible to carry food into Gorazde from outside the line by a journey of more than thirty kilometres over the mountains.
This route has now been cut.
Convoys have been intermittent and this was the first convoy for four weeks.
In any event even a convoy of ten trucks can only provide 1.5–2.0 kg per person — enough food for about two days.
Dr Tervahauta was told that people have fought for a share of the food which has been dropped by air.
He was shown a viedotape of people swimming in the Drina River in bitter winter weather after an air drop, trying to save food that was floating downstream.
The military ready-to-eat rations, which are the only sort of food dropped, are better than nothing but are inappropriate for sharing in the community.
They tend to be hidden by individuals for their own use.
Inevitably it is the fittest who are able to find these rations while children, the old, and the weak go without.
It would be much better to drop in additional flour, fat, beans, dried milk, and sugar but a method of safe delivery of these in bulk by parachute has yet to be found.
In a Red Cross collective centre a lot of emaciated elderly people were having a soup made of nettles.
They claimed they had lost up to 30 kg in weight since the siege began.
Two of them spontaneously told Dr Tervahauta that they wished to commit suicide instead of‘living like animals’.
There were hundreds of people in this centre but only two toilets and no running water.
At 11.30 a large-calibre shell exploded close by and fifteen minutes later a woman aged 60 was admitted to the hospital with an open fracture of the thigh.
The convoy decided to leave.
Two dentists had been killed in their clinic by a shell just before the convoy arrived, and after it left shots were fired at the medical shift coming from Sarajevo to relieve the surgeons.
Two of the team were killed and two were wounded.
Gorazde has been under siege for more than four hundred and fifty days.
It is currently under fire from a further attack by the Serbs.
The makeshift hospital that Dr Tervahauta described has subsequently received a direct hit by a shell with a heavy loss of life.
Access to records
The Danish parliament on June 17 passed a law on patients' records, which from January, 1994, will give every citizen the right to access their medical records.
Previous legislation, which came to force on January 1, 1987, permitted patients to see only their hospital records, but now the right has been extended to records made in all medical institutions, including records made by private practitioners.
Patients can choose either to see the original records or to have a copy.
With the previous legislation, a third party, with the patient's authorisation, can also demand to see the records.
The Danish Medical Association's (DMA) reaction to the new law is reserved.
‘Although the records a general practitioner makes are for his own personal use, we accept this opening.
But we disagree that a third party with power of attorney can see the patient's records’, said Per Hors, the DMA's chairman.
A report published in this week's Journal of the Danish Medical Association (Ugeskrift for Laeger, June 21, p 1985) says that since 1987, insurance companies, employers, unemployment and retirement institutions, banks, and public authorities render their services on the condition that they obtain from appliers permission to check their medical records.
According to Hors, the new law will aggravate this situation, as these institutions will regularly force general practitioners to disclose information.
Until now, Danish GPs' notes were for private use, except when they involved drug prescription, accidents, and insurance claims, or when malpractice was suspected (see Lancet 1992; 340: 719).
‘Now everything the patient tells doctors might be passed further.
This will undermine the trust patients have in them’, he said.
‘The only way to avoid this is to approve new legislation that protects patients from pressure by third parties’.
Academic defiance in Sudan
The medical profession and the Sudanese authorities are on a collision course once again.
In June the Academic Council of the Medical School of Khartoum University decided to defy the Government's directive to ‘arabise’ the medical curriculum, and reverted back to giving instruction in English.
Senior medical figures including Dr Sulaiman Salih, the dean of the Medical School, supported this decision.
In a statement, Dr Salih said that the government decision was ‘a hasty experiment which was introduced without adequate preparation’.
This criticism was also echoed by Dr Mudthir El Tinquawi the former director of Khartoum University who accused the Government of making a political decision to implement the concept of ‘arabisation’ in the country's universities, while failing to provide financial support.
He went as far as to say that the universities were not convinced of the value of this ‘experiment’ and that there were fundamental defects in the way it was being implemented.
In theory, the use of Arabic as the medium of instruction in the medical schools of the Arab world is a long term objective decided upon by all Arab governments.
Earlier this year, a number of high level meetings between Arab health ministers and deans of medical schools were held in Damascus and in the Gulf to co-ordinate this policy (see Lancet 1993; 341: 167).
During those meetings it was emphasised by all participants that a great deal of work would need to be done in the translation of medical textbooks, and that this would take considerable time.
Like their Sudanese counterparts, senior medical figures throughout the Arab world are concerned that this decision is based more on political than academic considerations, and they have pointed out that most medical literature is published in English, and that even some Arab medical journals are published in English with a section in Arabic.
The issue of the medical curriculum in Sudan is only one aspect of the wider conflict between the State and the medical profession which started back in 1989.
Many prominent physicians have spearheaded campaigns opposing the Government's policies and its stance on human rights.
This in turn has led to increasing Government interference in matters relating to both the organisation and professional functions of medical groups.
Recently, medical students at Khartoum University took part in the campaign by the students' union, and called on the Government to respect academic freedom and the autonomy of the university.
In their statement they accused the security forces of raiding their seminars and arresting  some students, a number of whom were later expelled by the university administration for alleged political activity.
They also complained of alleged interference by the country's Revolutionary Command Council in issues relating to the appointment and replacement of deans of faculties and the university's vice-chancellor.
Fertility rights and wrongs
The history of obstetrics and gynaecology is littered with half-baked attempts by leading (male) members of the medical profession to blame women for involuntary childlessness.
For instance, if a woman ‘failed’ to conceive it was frequently attributed to the unfavourable environment provided by the woman for otherwise vigorous sperm.
Profluvium seminis was a common diagnosis in which, according to Edwardian gynaecologist Victor Bonney, one could see ‘sperms retreating en masse from an environment hostile to them’.
Now another myth about female infertility has been finally laid to rest.
Although traumatic damage to the uterus — eg, that associated with an induced abortion — can lead to intra-uterine adhesions and amenorrhoea (Asherman's syndrome), the effect of an uncomplicated termination of pregnancy on subsequent fertility is reportedly less clear.
Despite several retrospective studies suggesting no association, some obstetricians still argue against performing induced abortions on the grounds of adverse outcome on future fertility.
Frank et al prospectively examined women who had an unplanned pregnancy at recruitment which ended in either an induced abortion (n=433) or naturally (n=1035).
When fertility rates were compared between these two groups, the ratio was 0.94 (95% CI 0.83–1.07).
Future fertility was not related to induced abortion.
Kidneys: buy or die
Dr K C Reddy, an urologist in Madras, India, reopens the argument for allowing people to sell their kidneys for transplants through a state-controlled system of ‘rewarded gifting’, asserting that in a country with no cadaver transplant programme and where long-term dialysis is impracticable, there is no alternative for those with renal failure but to ‘buy or die’.
Reddy acknowledges objections that rewarded gifting might delay the development of a cadaver transplant programme, but argues that as ‘the medical, legal, economic, technological, and logistic infrastructure required to set up such a programme in India simply does not exist…is the payment of money to a willing, informed adult, who happens to be poor and needy, so unethical or immoral that it alone determines whether people should be allowed to live or die?’
No western country condones payment for transplant organs, yet most permit altruistic donations.
Reddy's assertion that ‘no harm’ comes to donors who part with a kidney is disengenuous, but does not weaken his plea for the ethical objections to organ sale to be reassessed.
Rewarded gifting has the hesitant support of the ethical committee of the Transplantation Society ‘…paid organ donation must not be condemned out of hand’ and of the Vedic scholar, Ramakrishna Dikshitar ‘…even if money changes hands, it is still in keeping with the Hindu belief in helping others’.
Even as an interim solution, the problems of rewarded gifting are formidable: does the State set a standard price for a donated kidney, tailor the price to compatibility, or the means of the recipient?
Western practitioners who find the idea of putting a price on a kidney unethically distasteful might bear in mind that their fellow citizens are free to damage themselves for profit in other ways, and with medical approval.
How much more likely is it that a professional boxer, for example, would suffer permanent bodily damage, than a kidney donor?
The ethical distinction between allowing one poor and needy citizen to run the risk of brain damage and to deny another the right to sell a kidney may prove hard to define.
Transplants with transgenic pig organs?
The increasing demand for organ transplantation is not matched by supply of appropriate human organs.
One solution might be to use organs from other species, if the problem of rejection can be overcome.
The hyperacute rejection reaction is known to be mediated by complement which in itself is regulated by a family of membrane glycoproteins including decay-accelerating factor (DAF) and membrane cofactor protein.
UK scientists led by Mr John Wallwork and Dr David White in Cambridge have succeeded in incorporating the human gene for DAF into pig DNA.
37 transgenic pigs, all of whom express human DAF, have been born.
The gene for membrane cofactor protein is the next target.
Clinical trials of the suitability of organs from these pigs may begin within 3 years.
Competition for Canada's hospitals
Canada's hospitals say the national health system must be radically reformed to restrict more severely the core list of essential services covered by Medicare.
And at the top of the list of services that should be de-insured are accommodation and meal costs associated with hospital stays, argues Canadian Hospital Association (CHA) president Carol Clemenhagen.
In a sweeping report adopted last month by the annual CHA convention, Canada's 1210 hospitals also urged that a measure of market-style competition be introduced in the system to force hospitals to compete for patients and tax dollars, as well as reward the efficient institutions.
To that end, the CHA proposes adoption of a convoluted notion called ‘prospective payment mechanisms in performance-based funding’.
Essentially, it urges development of national funding and quality standards which would be used to determine the level of compensation awarded to hospitals for services.
For example, if a national standard determined that $100 should be issued for performance of an obstetric service, and a hospital performed the service for $90, it would pocket the $10 difference for use in other programmes.
If a hospital spent $110, it would have to achieve the economies of its counterparts, or absorb the additional $10 internally.
Patients would be steered by doctors towards hospitals which have repeatedly demonstrated frugality and regional health boards would target monies and population-based programmes towards spendthrift institutions too.
While such a proposal might well serve large cities with many facilities, Clemenhagen argues it would also benefit isolated or rural hospitals because they would have standards by which to compare themselves with their counterparts.
Far more controversial is the CHA proposal to open up the 1984 Canada Health Act.
Currently, the act's ‘comprehensiveness’ criterion states that any service delivered by a physician is de facto insured.
But all provinces have developed their own list of core servies and many are now de-insuring specialties such as cosmetic surgery in a bid to constrain the growth rate of health budgets.
Clemenhagen says federal and provincial governments, Medicare providers, and consumer groups must now be brought to the negotiating table to draft a ‘clinically sound’ list of core services.
‘Because we are moving into an era of a closed envelope of dollars for health care, the important thing is to safeguard an insurance programme to cover only those services judged as essential.’
Anything off the list would be subject to‘alternative revenue sources’— eg, direct charges to patients and taxation of medical expenses.
She adds that while the CHA is not presently advocating de-insurance of any specific service, a practical example is the hotel cost of hospital stays.
Although many believe coverage of such hotel costs (along with universal access and comprehensiveness) is one of the distinguishing features of the Canadian system that prevents it from becoming a US-style two-tiered system, Clemenhagen says the need to maintain the system's integrity must be balanced by ‘the need for responsible deficit reduction’.
‘We have to ask ourselves what it is that we are really trying to do with the $66.8 billion we spend on health care?
I think Canadians will respond positively to the CHA report because what we're saying is: let's be serious about what we're trying to achieve.
Let's approach choices systematically.
But the bottom line is that choices have to be made.
We don't have an economy that's able to support all of our services, needs, wants, and desires.’
Clemenhagen adds that the CHA and its member hospitals will lobby all governments to create some form of national mechanism or process to draft reforms.
The CHA also plans a nationwide campaign to make its proposals an issue in the coming general election.
Generic Ptolemic
When royal forefathers were generically called Ptolemy, as was the dynastic custom, knowledge of family pedigree could have given the edge in court intrigue.
Cyrillic is a new pedigree-drawing program that runs, not on an abacus, but on Windows, the computer front-end.
Genetic markers, phenotypes, and haplotypes can be displayed.
The program copes with multiple pregnancies and marriages, and, apposite for the Egyptian royals where it was the norm, consanguinity.
In 51 BC, Cleopatra, aged 18, became Queen on the death of her father Ptolemy XI (Auletes).
Her destiny would have been marriage to her brother, then aged 9.
Fate decreed flight from Egypt and liaisons with one or two famous Romans.
Curious that Windows is icon-driven, reminiscent of hieroglyphs.
Navajo hantavirus
The outbreak of hantavirus respiratory illness in the southwestern USA, largely among Navajos, continues to spread (see Lancet 1993; 341: 1993); the fatality rate so far is about 50%.
The Centers for Disease Control now reports evidence of hantavirus infection in 12 out of 42 deer mice (Peromyscus maniculatus) trapped around the homes of infected people, which suggests that this illness may have a rodent vector.
Letters to the Editor
High domestic and occupational radon exposures: a comparison
SIR — Tomasek and colleagues (April 10, p 919) report on cumulative radon exposure and lifetime risk for twenty-eight sites and types of cancer among uranium miners in West Bohemia.
There was no evidence in these miners that a radon-rich atmosphere increased the risk of any cancer other than lung cancer.
Possible exceptions were cancer of the gallbladder, extrahepatic bileducts, and multiple myeloma.
A radon-related excess of lung cancer is well established.
The applicability of such data derived from uranium miners to the general population is central to the radon issue.
The unusually high indoor radon levels measured in Umhausen (2600 inhabitants, Western Tyrol, Austria) led to lifetime radon exposures of the same order of magnitude as those reported for the uranium miners in West Bohemia.
We have compared the cancer mortalities of the two populations.
We used measurements of the seasonal course of indoor radon concentrations (296 houses, 51% of total housing stock) to calculate the mean annual radon concentrations (ca) on the first floor of the houses:
The radon concentrations are not distributed evenly over the village but show a clustering of very high radon concentrations in an area situated between two rivers (area A, median ca=1868 Bq/m).
Geologically, this area is an alluvial fan of a giant rock slide of granitic gneisses.
The radon concentration in the rest of Umhausen is lower by a factor of 10 (area B, median ca=182 Bq/m).
Areas A and B have roughly equal numbers of inhabitants.
The lifetime radon exposure were calculated according to BEIR IV.
The median lifetime radon exposure is 242 working-level-months (WLM) in area A, 23 WLM in area B, and 111 WLM in the total area of Umhausen:
The high lifetime exposure in area A coincides with a clustering of observed lung cancer deaths.
The lifetime radon exposure of 242 WLM in area A is comparable to the category 210–329 WLM for uranium miners in West Bohemia.
Equally high domestic lifetime radon exposures are rarely found.
They permit a comparison between the ratio of observed to expected deaths from lung cancer in Umhausen and among uranium miners in West Bohemia (area A 6.17, uranium miners 6.23).
The agreement between the figures is surprisingly high.
The lifetime exposure in area B is only moderately increased at 23 WLM and is comparable with the category less than 110 WLM for uranium miners.
Smoking habits and alcohol consumption were not considered in either study.
Can data on radon exposure and cancer risks in uranium miners be applied to the general population?
The inhabitants of area A of Umhausen have about the same lifetime radon exposures as one category of uranium miners in West Bohemia; given about equal levels of lifetime exposure, the ratio of observed to expected lung cancer deaths is about the same; and mortality from cancers other than lung cancer is not significantly raised.
We conclude that the cancer risks per unit dose to the population in Umhausen and the uranium miners in West Bohemia do not differ considerably.
Hypoglycaemia by inhalation
SIR — Drug poisoning can happen in unexpected ways.
A 33-year-old man was brought to the emergency room in coma.
He had previously been well, except for chronic otitis media.
The day before admission he developed an unsteady gait with slight alterations in mood and behaviour.
He was taken to another hospital where a diagnosis of vertigo due to otitis media was made, and he was discharged on oral antibiotics.
The next morning the patient could not be roused.
In the emergency room he appeared deeply comatose: his eyes were closed and he did not react to noxious stimuli.
Plantar cutaneous reflexes were abnormal, pupils were equal and reacted slowly to light.
Temperature was normal, blood pressure was 120/80 mm Hg, heart rate 100 beats per min, and respiratory rate 10 per min.
Blood glucose, measured as part of a routine, was 1.3 mmol/L. 50 mL of glucose 33% was given, with slight improvement in the neurological picture.
There was no personal or family history of diabetes and the relatives denied any access to hypoglycaemic drugs.
The patient had never had psychiatric problems or previous admissions for the same symptoms, making the diagnosis of Mu∘nchausen's syndrome unlikely.
The patient recovered but needed further glucose infusions for recurring hypoglycaemia and remained somewhat confused for the next 48 h.
An electroencephalogram showed subcortical non-specific alterations.
Computed tomography (CT) of the head was normal.
Plasma insulin levels were normal.
Pancreatic echography and CT scan were also normal.
Further information about the patient was sought.
He worked at a pharmaceutical firm processing chlorthalidone and nifedipine, but 5 days ago had been moved to the micronisation of glibenclamide, only recently introduced at the firm.
He did not actually touch the substance, but, in the absence of adequate safety measures, he was likely to inhale microparticles.
The patient was discharged after 8 days, well and normoglycaemic.
After safety measures were enforced at his firm he went back to work and never experienced symptoms again.
To our knowledge this could be the first report of poisoning from sulphonylureas by inhalation.
Warfarin in chronic atrial fibrillation in the elderly
SIR — In his May 29 commentary Ramsay suggests that we should screen the population for chronic non-rheumatic atrial fibrillation (AF).
Individuals thus identified would be offered treatment with low-dose warfarin, the efficacy of which in the primary prevention of thromboembolic stroke in AF has been established in five controlled trials.
Since the prevalence of AF rises with age and is 8.8% in those aged over 80, many identified by screening would be elderly.
Some could easily be found by incorporating screening for AF into the routine health checks on over 75s already carried out by UK general practitioners.
AF remains a risk factor for cerebrovascular disease even in the ninth decade, and older people thus have much to gain from primary stroke prevention.
However, if the controlled clinical trials were used as the ‘gold standard’ on which anticoagulation with warfarin was based, it is probable that most older people with AF identified by community screening would not be treated.
The stringent exclusion criteria in trials of antithrombotic therapy ensured that less than 10% of those with AF were actually considered suitable for randomisation, and in one study, being over 75 was itself regarded as a contraindication to anticoagulation until later into the enrolment phase.
Other exclusion criteria included unstable gait or a tendency to fall, and concomitant administration of non-steroidal anti-inflammatory drugs, none of which is an uncommon finding in the ‘free-living’ elderly.
In addition, as Caro (May 29, p 1381) points out, we do not know if the baseline embolic risks and responses to treatment of people with AF in the community would match those seen in highly selected study populations.
Furthermore, despite the proven efficacy of warfarin in primary stroke prevention, physicians remain reluctant to prescribe oral anticoagulants for their older patients with AF because of fears about haemorrhagic complications and understandable concerns about drug compliance in elderly outpatients.
Widespread use of warfarin to prevent stroke in people with chronic AF is theoretically desirable but would be difficult to implement, and many of those most likely to benefit, namely the elderly, would unfortunately probably remain untreated.
Long-term follow-up of heparin to prevent myocardial infarction
SIR — The Italian multicentre study pubished in The Lancet in 1987 investigated the efficacy of subcutaneous low-dose heparin in preventing recurrence of myocardial infarction. 728 survivors of acute myocardial infarction were randomised 6–18 months later to subcutaneous heparin (12,500 IU daily) plus usual treatment (363), or usual treatment alone (365).
During the observation period (708 [SD 265]days for heparin group, 687 [251]for controls) heparin treatment induced a significant reduction in reinfarction rate (-63%), general mortality rate (-48%), and cardiovascular mortality due to thromboembolic events (-86%).
In order to find out if discontinuing heparin caused untoward effects, the participating centres in the trial and the co-ordinating centre did a follow-up investigation.
Information was obtained on 289 patients previously on heparin (90.4% of the analysable patients) and in 280 controls (85.7%).
After the end of the study, no patient in either group was involved in a prevention programme.
End-points were new acute myocardial infarction and death.
The mean follow-up was 2582 days for the heparin group and 2488 days for the control group, with a maximum follow-up respectively of 2812 and 2677 days after the end of the study.
During this time, 30 reinfarctions (10.4%) were recorded in patients previously on heparin and 27 (9.6%) in controls.
The cumulative reinfarction likelihood, calculated by a log-rank life-table, is not significantly different in the two groups (Wilcoxon p=0.87).
Similarly, the cumulative survival likelihood, assessed by the same statistical method, overlaps in the two groups (45 deaths [15.6%]in the group previously on heparin and 49 deaths [17.5%]in the group previously on usual treatment, Wilcoxon p=0.52).
The almost identical risk pattern for reinfarction and mortality shows that discontinuing heparin is not associated with any increase in the risk of reinfarction or death.
Therefore, the protective effect of heparin treatment against reinfarction and mortality is not offset by a rebound after stopping treatment.
Tissue-type plasminogen activator and risk of myocardial infarction
SIR,— Ridker and colleagues (May 8, p 1165) emphasise the role of endogenous tissue-type plasminogen activator (tPA) as a risk factor for myocardial infarction (MI).
In addition, the prognostic correlation with plasminogen activator inhibitor (PAI), as described by Hamsten et al in young MI survivors, was not as evident.
Our results are in accord with those of Ridker with respect to the importance of endogenous tPA, but suggest opposite findings.
We describe an inverse relation between tPA antigen values in patients recovered from an MI and the cardiovascular complications recorded during the first year of follow-up.
We investigated the fibrinolytic response of 36 patients (27 male and 9 female, mean age 56.7 [8.5]years) with MI (14 anterior, 11 inferior, 3 lateral, and 8 non-Q wave).
In all patients, 3 months after their qualifying MI, we measured plasma concentrations of tPA antigen, PAI activity, fibrinogen, plasminogen, and 2 -antiplasmin.
All blood samples were obtained between 0830 and 0930 h to keep to a minimum the spontaneous circadian variation of fibrinolytic system.
The  relation between fibrinolytic indices and the occurrence of cardiovascular events during follow-up was assessed by simple linear regression analysis with weighted regression, to allow for non-constant error variance.
The estimated regression coefficient b 1 indicates the probability of a cardiovascular event within the 1-year follow-up for each unitary increase of the mean value of a single fibrinolytic index.
During the 1-year follow-up, 1 patient had a reinfarction and 4 had residual angina requiring readmission (2 of these underwent coronary revascularisation).
Finally, 1 patient had tachiarrhythmic disturbances, followed by an acute episode of cardiac failure.
When the model of fitting linear regression was applied to the fibrinolytic indices, the only significant index (b 1 measure) was tPA antigen.
The estimated response function, y=0.32436–0.11110 x, showed b 1 =0.01110 with the t test (p<0.005), indicating that a relation exists between the occurrence of events and a decrease in tPA values.
As in Ridker's study, this correlation was not shown for PAI activity.
It is difficult to speculate on the discrepancies with previously published studies.
Nevertheless, taken together, the data so far reported for the role of endogenous fibrinolysis in patients with coronary ischaemia seem to suggest that the evaluation of tPA and PAI values might be helpful in the prognostic assessment.
Human parvovirus infection in sickle cell disease
SIR — Serjeant and colleagues (May 15, p 1237) report an epidemiological study of parvovirus infection in a Jamaican cohort, emphasising the epidemic and familial pattern of the ensuing aplastic crisis.
Around 300 children with sickle cell disease are registered at King's College Hospital.
During March of this year 5 of these patients all with homozygous sickle cell anaemia were admitted with aplastic crisis associated with parvovirus B19 infection.
There were two sibling pairs, and in each of these, admission of second sibling followed within about a week of the index case.
The table shows haematological and virological indices.
All except patient 2B presented to the emergency department with fever, coryzal symptoms, and features of a painful crisis.
In this patient symptoms were directly related to anaemia.
In 2 cases specific IgM antibodies were not present, although parvovirus B19 DNA was detected.
The emergency department at King's holds a sickle database which enables rapid identification of patients in whom there is a pronounced fall in steady-state haemoglobin.
This allows early transfusion before virological confirmation of parvovirus.
These cases highlight the need for vigilance in patients at risk and suggest that the epidemic of parvovirus in the UK over the latter part of 1992 is still continuing.
Parental awareness of this clinical problem is low, and since this population is highly mobile, some families may not have been seen regularly or counselled.
The importance of information about steady-state values should be emphasised, as well as the need for these values to be carried by the patient at all times.
A data card with this information, including the patient's usual treatment centre would be useful.
Our findings also confirm the observation that siblings of affected cases at risk should be seen immediately and dealt with proactively.
Menopausal flushes and calcitonin-gene-related peptide
SIR — Hot flushes occur because of oestrogen deficiency in women during the menopause or in men after oestrogen treatment.
Since oestrogen is a potent inhibitor of bone resorption, we investigated changes in calcium metabolism in women with hot flushes. 37 postmenopausal women (at least 9 months after the last menstrual period) were divided into groups according to the frequency of hot flushes over 4 weeks:(A) subjects without flushes (18), and (B) with less than 2 (7),(C) 3–9 (6), and (D) 10 or more a day (6).
Among these groups, there were no statistical differences in the mean age, body size/height, or time since menopause, or in serum concentrations of oestrogen, follicle stimulating hormone, progesterone, triiodothyronine, thyroxine, thyroid stimulating hormone, prolactin, calcium, phosphate, intact parathyroid hormone, and calcitonin.
Serum concentrations of ionised calcium were higher in women with more flushes (mean [SE]: 1.24 [0.01]mmol/L in A, 1.25 [0.02]in B, 1.30 [0.03]in C, and 1.25 [0.12]in D), but not significantly so.
Only the concentrations of calcitonin-gene-related peptides (CGRP) were significantly different: 3.13 (0.50) pg/mL in (A), 4.39 (1.50) in (B), 7.43 (1.99) in (C), and 9.50 (2.63) in (D)(A vs D, p=0.009 by U test).
Observation of CGRP concentrations, before, during, and after hot flushes in the same subjects, showed a significant percentage increase in CGRP: 197.9 (40.9) % at the start of the flushes, 341.6 (126.9) % at the height, then down to 227.9 (46.6) % immediately after the flush (p<0.05, analysis of variance).
CGRP is a potent vasodilator, and can cause hyperaemia in patients with medullary thyroid carcinoma.
Our results suggest that the sporadic over-secretion of CGRP associated with a lack of oestrogen, rather than gonadotropins or steroid hormones, contributes to hot flushes at the menopause.
CD4 T-lymphocytopenia without opportunistic infections in HIV-seronegative Ethiopian immigrants to Israel
SIR — In early 1991, about 15,000 Jews were airlifted from Ethiopia to Israel.
The prevalence of HIV infection in this Ethiopian community is 1.74%, and they were all infected in the year before arrival to Israel, when they stayed in transit camps near Addis Ababa.
Despite the recency of infection and the very low rate of patients with symptoms (<10%), CD4 T-cell counts were low in a high proportion of infected individuals.
Median CD4 counts on first assessment were 391/L, and 31% had counts of 300 or fewer.
CD4 cells were first counted in May-September, 1991, soon after the arrival of the immigrants, who were all well nourished and in good physical condition.
No evidence of nutritional deficiencies was found.
Haemoglobin concentrations in females were 12.5 (0.8) g/dL (compared with 12.7 [0.9]g/dL in the Israeli population) and in males, 13.2 (0.8)(13.9 [1.1]in Israeli males).
Albumin was 4.1 (0.5) g/dL.
Also, CD3 counts were normal and CD8 counts were increased.
Thus, we do not think that protein deprivation had any effect on CD4 counts.
Our findings prompted us to assess the prevalence of CD4 T-lymphocytopenia in HIV-seronegative healthy Ethiopian immigrants who arrived in Israel in the same immigration wave.
100 age and sex matched Ethiopian Jews were tested for CD3, CD4, and CD8 T-cell subpopulations and for response to the T-cell mitogens concanavalin A (Con-A) and phytohaemagglutinin (PHA).
8 individuals had CD4 counts under 300/L and 16 (16%) had counts below 400 without evidence of opportunistic infections or immunodeficiency.
Overall, median CD4 number was 591/L, whereas in age and sex matched non-Ethiopian Israeli Jews the median is 912.
Response to PHA or Con-A was low (below 2 SD of mean) in 41 individuals, 35 of whom had CD4 counts under 600/L.
Thus, low CD4 counts without HIV infection, immunosuppressive therapy, or other known causes of immunosuppression occur in Ethiopian Jews who recently arrived in Israel.
This population is characterised by a high prevalence of hepatitis B virus carriers (10%), intestinal parasites (90%), and sexually transmitted diseases (48%).
We do not know whether these underlying conditions are responsible for the low CD4 counts.
There are also no clear data about CD4 counts in African blacks.
A study from Zimbabwe showed normal numbers of CD4 cells among HIV-seronegative subjects.
Pulmonary tuberculosis in HIV-infected patients as criterion for AIDS in Europe
SIR — The European Centre for the Epidemiological Monitoring of AIDS has urged all European countries to adopt its expanded AIDS case-definition, which includes pulmonary tuberculosis, recurrent pneumonia within twelve months, and invasive cervical carcinoma in HIV-infected individuals.
In Barcelona, both AIDS and tuberculosis are monitored with active epidemiological surveillance: in 1991, tuberculosis incidence was 67/100,000 inhabitants and AIDS 26/100,000.
We calculated the impact of the inclusion of pulmonary tuberculosis in HIV-infected cases as a criterion for AIDS reporting.
The increase of AIDS cases was 17% in 1991 and 23% in 1992 (provisional data).
In European countries with a high prevalence of HIV and Mycobacterium tuberculosis co-infection, the new definition would significantly increase the AIDS incidence.
Spain would continue to show the highest annual incidence in Europe.
Although AIDS diagnosis must imply immunodeficiency, this is not always the case in the HIV-infected patient with pulmonary tuberculosis, since some show no clear immunodepression.
We think that pulmonary tuberculosis in HIV-positive patients should be considered a criterion for AIDS only if immunodepression is shown by a negative tuberculin skin test and/or low CD4/CD8 ratio.
Such is the case for about 50% of our patients.
In consequence the hypothetical restriction of the European definition to pulmonary tuberculosis patients in the presence of immunodeficiency may halve the calculated increases of 17% and 23%.
A survival study also supports the expansion of the AIDS case-definition to patients with pulmonary tuberculosis and severe HIV-related immunodepression.
Several case-definitions for AIDS surveillance are in use worldwide (CDC 1987 and 1992, Caracas, Bangui).
The introduction of a European AIDS definition complicates comparisons between European and other countries.
From a public health viewpoint, it is more useful to monitor HIV infection (because of the long AIDS incubation period) and to analyse, for example, transmission in newly infected cases to implement control measures.
Nevertheless major interest in the European expanded definition, despite overloading AIDS surveillance, will hopefully contribute to health services focusing on the possibility of HIV-infected patients with pulmonary tuberculosis transmitting HIV and/or M tuberculosis infections to the community, thus promoting better control of both infections.
Is a dominant superantigen involved in AIDS pathogenesis?
SIR — The association between certain retroviruses and superantigens has provided new insights into the interaction of viruses with the immune system and prompted the idea that HIV might cause, in conjunction with class II genes, cell anergy and depletion of non-infected T cells by encoding a superantigen expressed on activated infected cells.
Superantigens bind to major histocompatibility complex (MHC) and the T-cell receptor (TCR), bridging a site in the outer faces of MHC class II and the V polypeptide chain.
Because only the V chain is required for recognition, virtually all T cells expressing the appropriate V chains are activated by superantigens, regardless of their V. When exogenous superantigens are injected into adult animals, T cells expressing the appropriate Vs initially proliferate and then, paradoxically, become anergic and die.
Therefore, anergy of a given V T-cell subset strongly suggests previous activation by superantigen.
We have analysed the in-vitro response to a bacterial superantigen of peripheral lymphocytes from 33 symptom-free HIV-infected individuals and 13 HIV-seronegative control donors.
The superantigenic erythrogenic toxin A (ETA) of Streptococcus pyogenes stimulates and amplifies specifically CD4 and CD8 T cells from control donors expressing the V8 and V12 elements.
Analysis by flow-cytometry of V usage by T cells from HIV-infected patients after 4 days' stimulation with ETA, with antibodies against members of the V5, V8, and V12 families (Diversi-T TCR, T Cell Sciences) revealed complete lack of activation of the VB T-cell subset, whereas the V12 subset responded normally to the superantigen.
This V-specific anergy affected both CD4 and CD8 cells expressing the V8 element and represented an intrinsic functional defect since it was also observed in response to stimulation with V8 monoclonal antibody (figure).
No such anergy was observed in the controls.
The unresponsiveness of the V8 T-cell subset is frequent, since it occurred in 55% of the patients studied and longitudinal study of 10 patients indicated an effect lasting at least a year.
Comparison of the clinical status of responder versus anergic patients showed no correlation with previous viral or bacterial opportunistic infections.
In addition, unresponsiveness of the V8 subset may be an early event since it was found in a patient at CDC stage I. The only known common factor between patients in all groups was HIV infection.
These observations suggest that the V8 specific anergy in peripheral T cells early during HIV infection is the consequence of previous activation by a superantigen that might be encoded by or tightly associated with HIV.
Except for the acute pathogenic variant of simian immunodeficiency (SIV) PBj14, no HIV or virus strains activate normal T cells or isolated viral proteins.
Moreover, a viral superantigen would be expected to be present in all HIV-infected patients, unless its activity depends on half the HLA phenotypes.
We must therefore also consider the possibility that the V8-specific superantigen originates from another infectious (viral or microbial) agent.
Activation of a significant fraction of CD4 cells by a superantigen would greatly increase the number of cells susceptible to virus infection and replication and, at the anergic phase, allow provirus to remain latent in some of these cells.
Cytomegalovirus retinitis: when looks can deceive
SIR — Clinically evident cytomegalovirus (CMV) retinitis affects up to 25% of patients with AIDS, and is the commonest cause of vision loss in these patients.
Retinitis usually presents unilaterally, but generally progresses to bilateral involvement because of associated viraemia.
A 39-year-old homosexual man with HIV infection presented with right-sided loss of vision.
He was evaluated by two ophthalmologists each of whom diagnosed CMV retinitis.
His CD4 cell count was over 500/L at the time.
Initially he was treated with ganciclovir for 3 months, which was later changed to foscarnet for lack of response and presumed resistance.
After 9 months of continued treatment he was referred to our institution, at which time his CD4 cell count was 740/L, his  CMV serology (IgG+IgM) was positive (titre 256), and his HIV seropositivity was confirmed.
He was receiving no antiretroviral treatment.
A third ophthalmic evaluation concluded that the patient had CMV retinitis, and continued treatment was recommended.
However, in view of his high helper lymphocyte count the diagnosis was questioned and yet another ophthalmic evaluation sought.
On examination, the best corrected vision in the right eye was 8/200 (with a 2+ afferent pupillary defect) and in the left eye, 20/20.
Fundoscopic examination revealed a raised disciform scar involving the macula with surrounding exudate.
There were scattered regions of retinal angiomas, retinal aneurysms, and tortuous vessels, with intraretinal haemorrhage and retinal exudate.
These angiomatous lesions were consistent with a variant of Coats' disease.
The left fundus was normal.
When questioned directly about these findings, the patient admitted to the presence of visual abnormalities since early adolescence.
However, since the diagnosis of CMV retinitis was an AIDS-defining illness and therefore qualified him for permanent disability and other benefits, he had concealed this fact from those involved in his care.
Coats' disease is an exudative detachment of the retina, associated with vascular abnormalities including telangiectatic vessels, microaneurysms, areas of non-perfusion, and saccular venous dilatations.
It is usually unilateral but rarely may occur bilaterally.
The cause is unknown but seems to be related to congenital vascular abnormalities.
The diagnosis of CMV retinitis is based on the immune status of the patient and the clinical appearance of the fundus.
Documentation of an immunodeficient status is important for establishing the diagnosis.
Although immunodeficiency was never documented in our patient, he received a prolonged course of antiviral drugs associated with considerable toxicity.
Although a biopsy may be taken from the retina itself for evidence of the virus, this procedure involves risks.
To our knowledge, only two cases of possible CMV retinitis have been reported in non-immunocompromised hosts.
One was in a 59-year-old woman with relapsing uveitis who had retinal necrosis with CMV inclusions in the enucleated eye.
The other occurred in a 39-year-old woman with CMV mononucleosis followed by ‘pinpoint chorioretinitis’; her presenting fundoscopic findings were poorly described.
No cases of CMV retinitis have been described in HIV-positive patients with greater than 500 CD4 cells/mm.
Although modifications of the immune response occur in HIV-infected patients at all stages, opportunistic infections only affect those with profound disruption of their cellular immunity.
Since CMV retinitis in immunocompetent patients remains exceptionally rare and present antiviral therapy is toxic, confirmatory testing should be considered before such treatment is instituted.
Twin studies in medical research
SIR — Braun and Caporoso, Duffy, and Macdonald (May 29, p 1418) provide little defence against the argument that twin studies are misleading.
Two thirds of monozygous twins have a common placenta and as a result have a more similar nutritional environment than dizygous twins.
Hence it is difficult to determine whether concordance in monozygous twins is due to shared genes or shared early environment.
Duffy is correct, therefore, in predicting greater disease concordance in the subset of monozygous twins who share a placenta than in monozygous twins who do not share a placenta.
His data support this.
Hypertension, a disease strongly linked with fetal growth, was four times more concordant in single placenta monozygous twins than in monozygous twins who had separate placentas.
He and Macdonald also contend that as monozygous twin pairs show greater birthweight dissimilarity than dizygous twin pairs, this would tend to lead to disease discordance and underestimation of the genetic contribution.
But the greater birthweight variability within monozygous twins merely adds to the difficulty of distinguishing between the effects of genes and early environment, and is a further reason for distrusting estimates of heritability derived from twin studies.
Leslie and Pyke's (May 29, p 1418) assertion that the association between glucose tolerance and birthweight applies only to impaired glucose tolerance and not diabetes has already been refuted.
Macdonald is right to state that future research must find out how genes and environment operate (or co-operate?) and that newer twin study-designs may help with this.
Nevertheless, uncritical acceptance of the results of classical twin studies may have misled a generation of researchers.
Mental incapacity and medical treatment
SIR — We are concerned about one of the proposals in the Law Commission's consultation paper on mentally incapacitated adults and decision-making (May 1, p 1143)— namely, ‘…consent or refusal to treatment may be valid if the treatment and consequences are broadly understood when expressed in simple terms’.
Emergency medical treatment is often given against a patient's wishes, even though the patient's decision is taken during a lucid interval and no section of the Mental Health Act (MHA) 1983 applies to detain the patient.
When a patient is detained under the MHA 1983, but the physical disorder is not caused by or is not itself the cause of a mental disorder, a restrictive interpretation of this proposal could lead to the ridiculous scenario of doctors having to wait for such a patient to deteriorate or become unconscious before medical treatment could be initiated under the protection of common law duty of care and the doctrine of necessity.
Defensive medical practice results from fear of prosecution for assault and misunderstanding of a patient's common law rights to withhold consent before examination or treatment.
The proposal seems to be mainly aimed at situations in which a patient's testamentary capacity is in doubt; however, the generalisation of its interpretation with respect to medical emergencies may expose doctors to legal proceedings even if they had acted in good faith and in the patient's best interests.
In your May 1 editorial, you rightly call for great care not to erect ‘an elaborate bureaucracy’ in investing persons or courts with powers to give consent for medical treatment in individual cases.
With respect to psychiatric treatment, under Section 29 of the MHA 1983, the county court can appoint the local social services authority as acting nearest relative for a psychiatric patient on application by an approved social worker on the  grounds ‘that the nearest relative of the patient unreasonably objects to the making of an application for admission for treatment…in respect of the patient’.
In practice, this is a complicated, time-consuming, and expensive procedure.
Not surprisingly, social workers are very hesitant to use this section and patients do, albeit rather infrequently, continue to deteriorate in the community because of their nearest relatives' wishes.
If the Law Commission avoids creating similar pitfalls for medical treatment, it can only be in the mentally incapacitated patient's best interests.
SIR — Your conclusions about the Law Commission recommendations are timely, because the House of Lords Select Committee on medical ethics will soon comment inter alia on this issue.
In response to repeated pleas not to use life-sustaining and life-prolonging technology inappropriately to extend the process of dying, doctors are learning to say no.
It is important for lawyers and others considering this issue to realise how frequently such decisions are now made informally as part of good practice, outside the controversy with respect to vegetative patients.
A report from two intensive care units from San Francisco found that almost half the deaths followed a decision to limit treatment.
Only 5% of patients were competent, and only 12% of the incompetent had living wills.
Most decisions were therefore reached informally in consultation with families.
We have reported that half the deaths in the Glasgow neurosurgical unit followed a written note in the medical record to limit treatment, and audit of perioperative deaths in all Glasgow hospitals showed that 41% followed such a decision.
When intensive care or emergency surgery are used inappropriately doctors sometimes offer as an excuse their uncertainty about the law.
It is to be hoped therefore that those considering these matters will be able to reassure doctors about the propriety and legality of withholding or withdrawing non-beneficial or unwanted treatment, so that they can continue to act in their patients' best interests without the need for the kind of bureaucracy feared by The Lancet.
That some formality may continue to be required for certain controversial treatments should not be allowed to complicate the much more commonplace decisions that have to be made many times every day in acute hospitals.
Diphtheria after visit to Russia
SIR — A 43-year-old healthy Finnish man visited St Petersburg in Russia for 3 days during Easter, 1993. 12 h after he returned he became acutely ill with a sore throat.
He contacted a doctor and was given oral penicillin.
The next day he felt worse and was sent to Tampere Univeristy Hospital.
On admission he had an extremely sore throat and could not eat or speak.
There were large greyish membranes on the hypopharynx and the right tonsil, which was necrotic.
The pharynx and right side of the neck were heavily swollen.
The patient had a slight fever (37.4°C), moderate leucocytosis (15.5×10/L), and high C-reactive protein (240 mg/L).
On the day of admission, after an unsuccessful attempt to draw pus from the right peritonsillar tissue, a bilateral tonsillectomy was done.
No abscess was found in the operation.
Parenteral clindamycin was started.
On the fourth day after the operation gram-positive rods were seen in a bacterial culture from the tonsil tissue.
The next day the rods were identified as Corynebacterium diphtheriae var gravis, and were shown to produce toxin.
No other pathogens were found.
The culture has been sent to the Diphtheria (WHO) Reference Unit, Colindale, UK, for further typing.
A throat culture taken on the day of microbiological diagnosis was negative.
Because of the delayed diagnosis no antitoxin was given.
After 4 days of clindamycin with a clear clinical benefit, the treatment was changed to oral erythromycin acistrate (0.4 g three times a day).
9 days after his first symptoms there was an increase in creatine kinase (1281 U/L), a high proportion of which was isoenzyme B (97 U/L), indicating myocardial origin.
The electrocardiogram showed a prolongation of the QT interval and ST wave changes in the anterolateral recordings.
In two-dimensional echocardiographic recordings anteroseptal myocardial hypokinesia was detected.
30 days after the sore throat neurological symptoms appeared: paraesthesia of the right mandibular area and decreased muscular power on all four extremities.
On the second week of neurological disease, weakness of the pharyngeal musculature developed.
2 months after falling ill he is still in hospital.
There were no records available on the patient's possible childhood diphtheria vaccinations.
He had one dose of diphtheria vaccine during military service in 1970.
A serum sample for diphtheria toxoid antibodies was negative on day 7.
During his stay in St Petersburg the patient had taken part in a birthday party where drinks were taken from shared unwashed glasses and the patient kissed his local girlfriend.
It was later confirmed that she has remained healthy.
No throat culture from her is available.
33 health-care workers, who had a confirmed or probable contact with the patient's saliva were screened for diphtheria.
All the cultures were negative.
35 travellers on the same trip to St Petersburg were also screened, and 1 middle-aged healthy man was shown to harbour a toxin-producing strain in his throat.
This traveller had no contact with the patient during the trip, but he too, had a local intimate contact in St Petersburg.
In Finland large scale vaccination against diphtheria was started in the early 1950s.
The most recent diphtheria epidemic was in 1947, and no case of endemic disease has occurred since 1961.
Since then, despite over 50,000 Finnish citizens travelling each year to tropical countries alone, only 1 case of imported pharyngeal diphtheria has been diagnosed and that was nearly 30 years ago (1965).
During the past 3 years, the incidence of diphtheria has increased sharply in Russia.
The situation is worst in St Petersburg, where 845 cases (17 cases/100,000 inhabitants) were reported in 1992, and 498 cases from January to April, 1993 (Dr Yuri Rikushin, Pasteur Institute of St Petersburg, personal communication).
To our knowledge this is the first case of imported diphtheria acquired by a western tourist on a short trip to Russia, although  each year over 400,000 Finnish citizens travel each year to Russia and about 200,000 Russians visit Finland.
The patient described here and the fellow traveller with a positive throat culture both had a contact with a local inhabitant, during which exchange of saliva occurred.
Although diphtheria is endemic in Russia, the risk for an average traveller is not very big.
However, a booster of diphtheria vaccine is recommended, even for a short trip to Russia, if the traveller is not vaccinated or has not had a booster dose within 10 years.
The risk of spread of diphtheria in Finland, where the coverage of childhood vaccination is good, is improbable.
However, in several countries of western Europe, surveys have shown that immunity to diphtheria is poor, especially among women over 40.
Male individuals are better protected because they are given an additional booster dose during military service.
There have been small outbreaks of diphtheria in Sweden and Denmark recently.
With increasing travel to and from countries endemic for diphtheria, booster vaccinations of the adult population in western European countries may have to be reconsidered; and health care workers, who are at greatest risk of exposure to the saliva of diphtheria patients, should be adequately immunised.
Falloposcopic instillation of prostaglandin in tubal pregnancy
SIR — Falloposcopy, the transcervical endoscopic examination of the fallopian tube by a linear everting catheter (LEC), was reported in 1990.
It has been used diagnostically in infertile patients.
Visualisation of the tubal lumen may assist in planning rational treatment of infertility, permitting surgery or tubal transfer of embryos or gametes if the mucosa is normal.
A further possible application is the diagnosis of tubal pregnancy.
The LEC may also be useful in the non-surgical treatment of unruptured tubal pregnancies.
We tried to manage an unruptured tubal pregnancy with high initial beta-human chorionic gonadotropin (-hCG) levels by prolonged prostaglandin (PG) F2 application via LEC.
Previously, non-surgical treatment of tubal pregnancy by local PGF2 instillation had been limited to cases with initial -hCG under 2500 mIU/mL serum; higher concentrations were correlated with unfavourable outcome.
In a 25-year-old patient with a 3-year history of infertility, tubal pregnancy was diagnosed by vaginal ultrasound. -hCG was 4506 mIU/mL.
Under general anaesthesia and laparoscopic control, the LEC was introduced into the relevant tube.
The tip of the catheter was positioned directly on the gestational sac.
After visualising the lumen of the fallopian tube and the tubal gestation, 10 mg PGF2 was injected.
Laparoscopy was ended with the LEC left in place.
3 hours later, a second dose of 5 mg was injected with no adverse effects and the catheter was removed.
The patient was discharged the next day.
Serial -hCG rapidly fell, and was undetctable after 4 weeks (figure).
Vaginal ultrasound revealed that the tubal pregnancy had disappeared.
Non-invasive antenatal diagnosis
SIR — Lo and colleagues (May 1, p 1147) attempt to determine the rhesus status of fetuses in utero by non-invasive means.
They admit that this method has not reached the precision required for routine application (because a 20% false negative and almost 20% false positive rate is too high), but it shows great promise.
Furthermore, it will contribute to a better understanding of normal fetomaternal physiology and to the quest for a non-invasive but definitive antenatal diagnostic procedure that would carry no risk to the fetus.
Research into the isolation and genetic analysis of fetal nucleated cells in the maternal circulation is gaining momentum, and three types of cell — trophoblast, lymphocytes, and nucleated fetal erythrocytes (NFEs)— are the main focus of attention.
For several reasons the first and last of these are the most likely candidates and each has its proponents.
Trophoblast is physiologically shed into the maternal circulation, whereas, in early pregnancy, NFEs are the most common type of nucleated fetal cell in the fetal circulation.
However, there is a growing opinion favouring NFEs.
Because Lo et al did not use any relative enrichment techniques, it is not known if they analysed one particular type of cell or a combination of types.
However, from their polymerase chain reaction results, it seems that in most cases there is 1 fetal cell for every 10–10 maternal cells.
These workers clearly demonstrate the ability to detect signals in patients with substantial antibody concentrations, but do not  state whether in these cases there was a rising titre of anti-D; this, however, can be construed because they went on to do cordocentesis in the patients with high values.
If the analysed cells were NFEs, then at 22 weeks' gestation (case 8) this should represent a transplacental haemorrhage of about 0.5 mL, which would certainly be sufficient to stimulate a secondary immune response in an isoimmunised mother.
If so, maternal antibodies would probably have mopped up the NFEs before their analysis, suggesting that the analysed cells are of a different type.
Little is known of fetomaternal cell transfer at much earlier stages of pregnancy, though there is growing evidence that NFEs occur in proportionately greater numbers in the maternal circulation at earlier gestations.
The question of which type of cell is most likely to give the desired result remains unanswered, but work such as theirs will help to elucidate this matter.
In combination with methods of selective cell enrichment and prospects such as fluorescence in-situ hybridisation, it may ultimately lead to routine definitive diagnoses of fetal genetic abnormalities from simple maternal blood tests.
Local treatment of thrombocytopenic mucosal haemorrhage
SIR — The management of patients with epistaxis or diffuse gut bleeding due to thrombocytopenia is difficult since platelet transfusion is often ineffective because of platelet antibodies.
We tried local application of platelet concentrates in 12 consecutive patients with leukaemia (9) or lymphoma (3) who had severe mucosal haemorrhages due to thrombocytopenia. 9 had epistaxis, 2 had a haemorrhagic gastritis, and 1 had diffuse colonic bleeding.
The local applications were random-donor concentrates which contained about 10 platelets in 50 mL plasma.
For patients with epistaxis, 10 mL of concentrate was applied to both nostrils in 1 mL doses with a syringe.
In the patient with diffuse gut bleeding, 50 mL of platelet concentrate was diluted in 500 mL 0.9% NaCl and administered as an enema.
The gastritis patients were given 50 mL of platelet concentrate at gastroscopy.
The haemorrhage was successfully treated and prolonged freedom from bleeding achieved in all patients except 1 whose haemorrhage was caused by diffuse gastric infiltration with chronic lymphocytic leukaemia and who had only short-term relief.
Local application of platelet concentrates shows potential in treating diffuse mucosal haemorrhage due to thrombocytopenia.
Halofantrine sensitivity
SIR — Brasseur and colleagues (April 3, p 901) investigated the in-vitro and in-vivo sensitivity of Plasmodium falciparum to halofantrine in the Congo and in Cameroon in 1992, and in the Congo recorded a high frequency of resistance.
In Gabon halofantrine came on the market three years ago and has since been widely used.
In 1992 we studied the response of P falciparum according to WHO criteria.
For the in-vitro studies halofantrine was precoated on microtitre plates, resulting in final concentrations of 0.3, 1, 3, and 10 nmol/L blood/medium mixture.
Isolates were obtained and successfully grown in vitro from 40 patients.
The EC 5 , EC 9 , and EC 9 9 were 0.4091, 0.9400, and 1.8523 nmol/L, respectively, indicating very good sensitivity of P falciparum to halofantrine in vitro.
In a clinical trial 30 children with uncomplicated falciparum malaria were included, distinct from those of the in-vitro study.
They had not received antimalarial treatment within the preceding 7 days and were 4–14 years old.
Their parents gave informed consent.
A micronised formulation of halofantrine with a two-fold increase in absorption was used.
The patients received three 8 mg/kg doses of halofantrine 6 hourly.
Parasitaemia on thick blood films and symptoms were recorded daily for the first week and subsequently every week until 28 days after therapy.
The median parasitaemia on admission was 21,000 asexual parasites per L (range 200–140,000).
The parasitaemia was cleared within 48 h in 4 patients and within 72 h in 26.
The patients were free of symptoms in 24–72 h.
Thick blood films of 28 patients remained free of parasites until day 28 of follow-up; in the other 2 patients symptomless parasitaemia of 15 and 50/L was detected on day 28.
Our findings indicate pronounced sensitivity of P falciparum to halofantrine in Gabon in 1992 despite its widespread use there over the 3 previous years.
This experience is very different from that of Brasseur and colleagues in neighbouring Congo.
The EC 5 in the Congo was 30 times higher.
This could be due to the notorious solubility problems with halofantrine which will be especially prominent in in-vitro systems with drug in solution.
Almost half the Congo patients still had detectable parasitaemia on day 4; in Gabon all patients had a negative thick blood film on day 3, and this may be explained by the use of a new formulation of halofantrine.
Authors' reply
SIR — The history of resistance to antimalarial drugs has been accompanied by a parallel move of resistance of some researchers to admit that it may be so.
This happened with early reports of resistance to chloroquine, quinine, and mefloquine, and it now seems to be happening with halofantrine (May 15, p 1282, and the above letter).
Gabon, the Congo, and Cameroon are neighbours.
We found P, falciparum isolates resistant to halofantrine in the Congo but not in Cameroon, by in vivo and in vitro methods, the results of which were in accord.
Wildling and colleagues report no resistance in Gabon, although in 2 cases parasites were still detectable at 28 days and could be classified as RI resistance.
The mechanism of drug resistance is not clear but earlier episodes suggest that a particular combination of events — pre-existing status of the various parasite clones, drug use habits or a particular combination of several drugs, the introduction of new parasites, transmission by mosquitoes — leads to the emergence of resistant isolates in restricted foci.
This combination cannot be expected to be homogeneous to start with.
The absence of resistance in a different country is thus not contradictory to our findings.
The Gabonese patients were treated with a micronised formulation (though drug concentrations are not presented).
Nor was any attempt made to measure drug pressure, and data on response to other antimalarials were not presented.
This may be important; geographical differences in the prevalence of parasites resistant to chloroquine, quinine, and mefloquine have been found in Gabon, and cross-resistance has been observed.
The quantitative comparison of EC 5 s is debatable since the two in-vitro assays are very different, especially in the proportion of red blood cells and the presence of leucocytes in which the drug concentrates preferentially.
Furthermore, differences in the EC 5 of up to 1000-fold have been reported for chloroquine between parasites from a single geographical region.
Carme and Basko (May 15, p 1282 and 1283) present results and arguments opposing our findings.
Since halofantrine was introduced in 1989 we do not see how data collected in 1990 go against our hypothesis that drug pressure led to the resistance we observed in 1992.
A large part of the population from the village we studied, located 15 km from the capital, work in Brazzaville (where drugs are readily available); and relatives go to the village at weekends and can carry drug-resistant parasites back and forth.
It is unclear to us why the interview of 600 mothers in 1989 would be a more reliable way to measure drug pressure than the total sales per annum of drug suppliers in 1991 and 1992 or the treatments given at an outpatient clinic.
In an effort to reject the existence of halofantrine resistance some point to poor absorption of the drug, or to poor solubility.
Both statements are correct.
Nevertheless they do not take into account the fact that with the same in-vivo and in-vitro protocols the 54 isolates we studied in neighbouring Cameroon were fully sensitive and thus serve as controls.
The same holds true about the plasma concentrations which, although they were determined by a new ELISA, were within the same range (70–350 ng/mL) in responsive patients from Cameroon and the Congo as in non-responsive patients from the Congo.
The monoclonal antibody that we used is as sensitive to halofantrine as to its main metabolite, whose half-life exceeds 3 days.
There may be ‘flaws’ in our study of which we are not aware but we cannot accept Basko and co-workers' statement that blood smears from day 0 to day 8, in-vitro sensitivity, and concentration studies were not done.
It is also argued that drug pressure could not be the origin of resistance because halofantrine was not used prophylactically and has a shorter half-life than chloroquine or mefloquine.
Clear-cut quinine resistance emerged in Asia in the early 1980s when this drug became more widely used, even though quinine has the shortest half-life of all antimalarials and is not given prophylactically.
The argument that acquired immunity may bias the measurement of parasite resistance or limit the progression of those parasites deserves comment.
This reasoning was put forward for two decades to support the idea that chloroquine resistance would not spread in Africa, where adults become immune.
Yet as soon as people harbour parasites (70% of sub-Saharan Africans) those parasites are potentially exposed to selection under drug pressure.
And, as we know, resistant parasites emerged and spread explosively throughout Africa in the 1980s.
The refractoriness of our malariological critics to the notion of resistant parasites leads them to a paradox; they refer to ‘the continuing efficacy of 4-aminoquinolines’ whereas the table derived from their own 1990 studies shows 41% resistance to these drugs.
Early diagnosis of Wilson's disease
SIR — The mechanism of abnormal copper metabolism in Wilson's disease remains unknown.
It has been thought that caeruloplasmin lacks para-phenylenediamine-oxidase activity because of loss of copper binding ability.
Early diagnosis and treatment prevents brain damage and liver cirrhosis.
However, it is difficult to detect early Wilson's disease, since caeruloplasmin activity is low in the newborn period.
We generated monoclonal antibodies against caeruloplasmin and selected two, ID1 and ID2; both can recognise caeruloplasmin protein, and ID2 neutralises caeruloplasmin oxidase activity.
A slightly modified, sandwich enzyme-linked immunosorbent assay was used to examine the oxidative activities of caeruloplasmin in normal subjects and patients with Wilson's disease.
Serum was incubated with ID1, which bound active and inactive caeruloplasmin.
Active caeruloplasmin was detected by ID2 (table).
The caeruloplasmin values detected by the combination of monoclonal antibodies were lower than those by polyclonal antibody in the patients with Wilson's disease, whereas normal subjects had similar values.
The same result was obtained from the combination of polyclonal and monoclonal antibodies (as a secondary antibody).
In this system, active and inactive caeruloplasmins were trapped with polyclonal antibody, caeruloplasmin protein was detected by ID1, and active caeruloplasmin by ID2 (data not shown).
These results indicate that inactive caeruloplasmin occurs in serum of the patients.
Because normal subjects had virtually identical values of active and total caeruloplasmin, we estimate that Wilson's  disease is caused by loss of oxidase activity rather than abnormal synthesis of caeruloplasmin.
Detection of active caeruloplasmin by our monoclonal antibodies will be a highly effective, early diagnostic tool for Wilson's disease.
Menorrhagia and endometrial resection
SIR — We share Coulter's concerns (May 8, p 1185) about the general lack of objective assessment of menstrual blood loss and think that there are two reasons behind this.
Many practitioners still feel that a normal menstrual blood loss measurement does not affect their management, provided that the patient's quality of life is affected, despite evidence that patients with normal measurements are prepared to forego any treatment.
Secondly, the alkaline haematin method is an unpleasant test and most laboratories are not prepared to offer it.
Under these circumstances, it is not surprising that objective measurements are not commonplace in clinical practice and remain largely a research tool.
However, there should be increased use of semi-objective methods such as pictorial blood-loss assessment charts.
These correlate well with the alkaline haematin method.
Our unit has run a menorrhagia clinic for 5 years and last year we introduced these charts for all our patients.
Normal values encourage some patients to opt for less drastic treatments.
Recently there has been much discussion about new treatments for menorrhagia.
The success of any treatment depends on good selection of patients; so more time and effort should be spent on assessing the severity of menstrual blood loss before surgery.
Primary hypothyroidism and I-MIBG therapy in neuroblastoma
SIR — In childhood advanced neuroblastoma, treatment is disappointing despite aggressive and multimodal strategies.
Because I-metaiodobenzylguanidine (I-MIBG) is selectively concentrated in adrenergic tissues, administration of this compound to irradiate neuroblastoma cells is a promising treatment associated with modest early toxicity.
Whether I-MIBG has long-term effects on thyroid function is unknown.
Between 1986 and 1992, 54 children with surgically unresectable or disseminated neuroblastoma, who had responded poorly to conventional chemotherapy, were treated with I-MIBG at 2.5–3.7 GBq (median 3.2).
To inhibit uptake or radioiodine by the thyroid, Lugol solution 2–3 mg/kg per day was given from 7 days before until 7 days after treatment.
4 patients also received high-dose chemotherapy and total body irradiation with autologous bone marrow transplantation (ABMT) before I-MIBG therapy.
Whilst most of these patients died within a year, 9 achieved tumour regression for at least 12 months (range 1–5 years) after two to four courses of I-MIBG.
In this group of long-term survivors, thyroid function tests every 3–6 months after I-MIBG administration disclosed primary hypothyroidism in 2 of 2 treated with I-MIBG and ABMT, and in 6 of 7 treated with I-MIBG only.
Serum thyroid-stimulating hormone was increased (mean [SD]32.7 [10.1]mU/L; range 4.4–75; normal <4) 3–39 months after the last dose of I-MIBG, and was associated with subnormal serum free thyroxine in 8 of 9 subjects and with overt clinical manifestations (myxoedema, growth deceleration) in 2.
Damage to the thyroid was permanent and thyroid ultrasonography (in 3 patients, none of whom had undergone ABMT) showed hypoplastic glands.
The usual preparation with Lugol may not prevent thyroid damage by I-MIBG.
Thyroid function in neuroblastoma patients treated with this compound should be closely monitored.
Linitis plastica after Hodgkin's disease
SIR — Of 892 adult patients with Hodgkin's disease treated at our institute from 1960 to 1984, 6 developed gastric carcinoma 7–29 years after completing treatment.
Compared with the general population (matched for age, gender, and year of occurrence), the observed to expected ratio was 9/1.
During the same period, 3 other patients not previously treated by us for their primary Hodgkin's disease were referred with secondary gastric carcinoma.
Linitis plastica was diagnosed in 6 of these 9 (67%), an unusually high proportion.
Among the 1200 patients with primary gastric carcinoma who were treated at our institution during 1969–92, 77 (6.4%) had linitis plastica (the diagnosis was made in all cases after review of the 1209 histological sections).
There were 3 females and 6 males; 5 had nodular sclerosis on histology and 4 mixed cellularity; 7 presented with stage I-IIA and 2 with IIIB, and all were treated with irradiation which included the para-aortic region to a total dose of 40 Gy.
Among the 439 patients who were given para-aortic irradiation, 6 developed secondary gastric carcinoma (2.4% 15-year cumulative incidence rate), by contrast with no gastric carcinomas in 453 patients not treated with such irradiation.
The radiation dose per fraction was 2.5 Gy in 7, 3.3 Gy in 1, and unspecified in 1.
4 were given mustine, vincristine, procarbazine, and prednisolone as initial treatment or at relapse.
The mean age at diagnosis of secondary gastric carcinoma was 38.8 (26–53).
According to International Union against Cancer classification, there were 1 with stage I and 8 with stage IV disease.
7 patients were treated with total gastrectomy and 2 with 5-fluorouracil, doxorubicin, and cisplatin.
The stage I patient died 8 years after diagnosis from an unknown cause.
The stage IV patients died 6–18 months after diagnosis from tumour  progression.
The increased risk of developing secondary gastric cancer after Hodgkin's disease has been reported.
No information is available about histological type.
In the present series, there was a low mean age (38.8) at diagnosis, compared with patients with primary linitis plastica (50.5); and all gastric cancers developed in fields previously irradiated with a high dose per fraction, which suggests that a high fractional dose should be regarded as a risk factor for secondary tumours.
Intestinal transplant for recurring mesenteric desmoid tumour
SIR — A 37-year-old man, who had been on total parenteral nutrition for 2½ years after excision of his small bowel for a mesenteric desmoid tumour, found this regimen unacceptable and sought a small bowel allograft.
The tumour had recurred and he had four previous operations, ending up with anastomosis of duodenum to mid-transverse colon.
He worked full time but had to spend most of the rest of his life feeding himself intravenously.
He took food by mouth but passed 5–8 motions a day and probably absorbed little of it.
On examination he appeared healthy but thin (46 kg).
There was a firm 6×4 cm swelling to the right of the midline scar in the adbomen which was presumed to be the recurrent tumour.
His feeding catheter was tunnelled under the skin and entered the right atrium via the right subclavain vein.
He was tissue typed (A2, A9/24; B5, B17; Bw4; DR3, DR7).
The whole of the small bowel with mesentery and mesenteric vessels were removed from a 15-year-old female accident victim who had been certified brain-stem dead.
Her tissue type was A1, A2; B7, B17, Bw4, Bw6; DR4, DR7.
Antibiotics were administered through the nasogastric tube and parenterally before organ removal.
The specimen was cooled externally by perfusion through the superior mesenteric artery with cold University of Wisconsin preservation solution.
The two ends of the bowel were closed with staples without irrigation.
The operation was done through the midline scar and the recurrent tumour was removed from the right upper rectus together with surrounding unaffected tissue.
Dissection was tedious because of multiple adhesions.
The colon distal to the duodenal anastomosis was dilated and thick-walled, having adapted to the flow of duodenal contents.
The liver appeared normal but access to the portal vein was difficult so the infrarenal inferior vena cava and aorta were prepared for anastomosis.
The duodenocolic anastomosis was clamped and divided, and the donor specimen was oriented anatomically.
A Carrel patch of donor aorta was anastomomosed to an anterior incision in the infrarenal recipient aorta, and the portal vein draining the superior mesenteric vein of the graft was anastomosed end-to-side to the anterior wall of the recipient inferior vena cava.
The arterial clamp was released, venting the initial flow through the splenic vein of the specimen which was then tied, and the caval clamp removed.
Perfusion was excellent and peristalsis was observed within a few minutes.
The proximal duodenum was anastomosed to the recipient duodenum end-to-end with a running all-coats layer and interrupted seromuscular sutures.
A similar technique was used to anastomose the side of the terminal ileum some 10 cm from its end to the end of the transverse colon.
The distal end of the ileum was brought out as an ileostomy and sutured to the skin with interrupted stitches (figure).
A large feeding tube was inserted into the stomach via a gastrotomy and threaded through the duodenum past the anastomosis.
The patient made a satisfactory recovery from surgery.
For immunosuppression we used a Pittsburgh type regimen with intravenous FK506 (5 mg/day intravenously over 24 h with epoprostenol at 8 ng/kg/min), azathioprine (5 mg daily), and prednisolone 200 mg daily reducing to 40 mg per day.
On day 6 the ileostomy became dusky red and biopsy revealed early cellular rejection without loss of epithelium.
There was strongly positive uptake by the entire graft on a labelled white cell scan.
A 10-day course of antithymocyte globulin reversed the changes completely, with rapid improvement seen on a repeat scan.
During the second postoperative week there was transient lymphocyte chimaerism in the peripheral blood and serum alkaline phosphatase rose, with no clinical evidence of graft-versus-host disease.
Two courses of ganciclovir were given for serological evidence of cytomegalovirus reactivation after 5 weeks and 4 months.
Serial intestinal permeability and absorption studies have shown no abnormal permeability to Cr-EDTA and a steady increase in C-labelled mannitol absorption to normal (>10% of the oral dose absorbed).
Barium follow through examination showed a normal mucosal pattern in the graft.
The patient was weaned off parenteral nutrition by the end of the second month and the stoma was closed after 5 months.
He is now in excellent health and back at work, maintained on 6 mg per day oral FK506 and 5 mg  prednisolone.
His current weight is 54 kg.
Experience with intestinal transplantation until recently has been disappointing, because of inability to control rejection with cyclosporin, mucosal necrosis, and graft-versus-host disease.
There were only two long-term survivors of intestinal transplant alone.
In 1990 Grant and co-workers started a programme of transplanting liver together with intestine, and three of their five patients are still alive (2 months, 2 years, and 3½ years); the other two patients died from lymphoma.
The Pittsburgh experience with FK506 as the main immunosuppressive drug is now fifty cases (fifteen small bowel alone and thirty-five with liver allografts).
The results after small bowel grafting alone have been encouraging.
Development of lymphoma in 15% of patients may indicate excessive immunosuppression.
Although our patient is only 6 months post-transplantation he has had a remarkably trouble free postoperative course, and immunosuppression with FK506 has probably contributed to this good early result.
Tumour inoculation during laparoscopic cholecystectomy
SIR — Early stages of gallbladder carcinoma (T1, T2) are usually diagnosed only at definite histological examination of cholecystectomy specimens.
Cancer frequency per total number of cholecystectomies ranges from 1 to 2%.
We report a patient who developed multiple metastases in the greater omentum from early stage adenocarcinoma of the gallbladder, presumably because of incidental intraoperative tumour inoculation at laparoscopic cholecystectomy.
This mishap may occur during laparoscopic surgery and has been described in a patient with advanced gallbladder cancer.
A 40-year-old man was admitted with right upper abdominal pain.
Ultrasonography revealed multiple gallstones, and laparoscopic cholecystectomy was done.
Dissection of the gallbladder was concluded uneventfully, but during removal through the umbilical incision the specimen ruptured.
Histological examination revealed adenocarcinoma (pT2).
Therefore, wedge resection of the gallbladder bed and regional lymphadenectomy were done two weeks later at our institution; all specimens turned out to be tumour free.
The tumour was thus classified as pT2 pN0 M0.
Surgery was followed by adjuvant chemotherapy with 5-fluorouracil 1000 mg/m weekly for five months, when the patient was readmitted with non-specific abdominal complaints.
Computed tomography revealed multiple intraperitoneal tumours with a diameter up to 10 cm.
During subsequent laparotomy all tumours in the greater omentum and one gallstone were removed from the abdominal cavity.
Histological examination confirmed diagnosis of metastases from adenocarcinoma of the gallbladder.
Eight months later, the patient is well and tumour free.
Metastases in the greater omentum, which otherwise would represent an unusual pathway of gallbladder carcinoma spread, lack of other regional or distant metastases, and presence of a biliary calculus in the abdominal cavity make intra-abdominal tumour inoculation during the laparoscopic procedure in this patient likely.
Our case shows that, with carcinoma of the gallbladder, rupture of the gallbladder during laparoscopic cholecystectomy must be avoided by all means, even if the umbilical incision has to be sufficiently extended to facilitate easy removal of an intact specimen.
Effects of intensive blood-glucose control
SIR — Wang and colleagues (May 22, p 1306) give a full description of the advantages of keeping close blood glucose control in insulin-dependent diabetics, but I do not think that they show a fully balanced picture about the frequency of disadvantages that diabetics encounter.
As they say, hypoglycaemia is the major problem.
It is not only that severe hypoglycaemic attacks cause distress for diabetics and their families — this distress can also be socially devastating.
In my practice, as a general practitioner, I am aware of several diabetics who have lost their jobs and their confidence in attending public events, such that they become socially isolated.
This isolation, of course, affects their family just as much.
One must look at the quality of life that diabetics and their carers have.
It is all very well to delay the onset of a retinopathy for a year or two, but to achieve this, is it justifiable that a diabetic has no social life and a poorer quality of life for 20 years?
If a patient dies because of hypoglycaemia, particularly a child, the upset and distress this will cause to many relatives is extreme.
All doctors should consider this possibility when instituting strict blood glucose control regimens.
SIR — Wang and colleagues' meta-analysis of intensive blood-glucose control found a significant reduction in late complications.
However, the incidence of hypoglycaemia and ketoacidosis increased.
At our hospital this form of intensified insulin therapy, as a base-rate bolus procedure, was introduced more than 5 years ago together with an intensive education programme.
72 patients have been treated with this form of insulin therapy since then and only 3 with continuous subcutaneous insulin, because our patients rarely accept it.
To evaluate the effect of these treatments on severe acute diabetic complications, on the hospital admission rate, and on medical care costs, we analysed the admission diagnosis of all adult diabetic patients treated at our hospital between Jan 1, 1989, and Dec 31, 1992.
The admission rate for diabetics fell by 26% over this four-year period, the number of patients admitted  with ketoacidosis decreased from 19 to 3 per year, and the number of cases of severe hypoglycaemia remained constant:
We suggest that the favourable results in respect of severe diabetic acute complications can be explained by the careful counselling of patients before the implementation of intensified insulin therapy and by the quality of glucose self-monitoring.
Age and prognosis in premenopausal breast cancer
SIR — de la Rochefordière and colleagues (April 24, p 1039) show that young age at diagnosis is an adverse prognostic factor for breast cancer survival and recurrence in premenopausal women.
They suggest that this factor may be related to genetic abnormality or more rapid progression of growth in younger women.
Another potential contributing factor may be age-associated variation in oestradiol levels.
We have measured oestradiol in daily saliva samples for one menstrual cycle in 53 healthy, regularly menstruating women aged 25–48, using a tritium-based radioimmunoassay.
Salivary steroid concentrations are believed to represent the free, biologically active hormone fraction.
Follicular and luteal phase oestradiol levels were highest in women aged 25–29 years, intermediate in women 30–34, and lowest in women 35 or over (figure), p=0.02 to 0.04 for follicular phase and 0.008 to 0.015 for luteal phase by repeated measures analysis of variance.
There were no significant differences in oestradiol by age from 4 days before menstruation to the first day.
Division of women aged 35 years or over into 5-year age groups did not show any significant differences in mean profiles.
Although average cumulative lifetime exposure to oestradiol would be higher for older women, our study suggests that levels of current exposure are higher for younger women.
In regularly menstruating women, salivary oestradiol stabilises after age 35.
This raises the question of whether exposure to higher oestradiol levels may contribute to more rapid development of disease in younger women.
Hormone replacement after breast cancer
SIR — Loba (May 22, p 1313) highlights the lack of information on the use of hormone replacement treatment (HRT) in women who have had breast cancer.
However, the management of severe menopausal symptoms in such patients is a problem, and we have not denied patients with breast cancer such treatment providing they have a clear understanding of the risks.
To avoid disease activation, we add tamoxifen on the basis that it has proven efficacy in the treatment of breast cancer in premenopausal women.
From our clinical records we have identified 35 patients who received HRT with conjugated oestrogens (Premarin or Prempak C 0.625 mg per day, Wyeth) with tamoxifen 20 mg per day (table), with hot flushes being the key symptom.
The dose of Premarin was increased to 1.25 mg in 5 patients because of failure to control symptoms.
To date, only 2 patients have relapsed. 1 developed an 8 mm recurrence in the conserved breast 68 months after primary treatment by wide surgical excision and radiotherapy followed by 6 months adjuvant chemotherapy.
She had started HRT and tamoxifen 24 months after primary surgery and continued for 44 months until relapse.
The other patient relapsed 27 months after primary diagnosis and neoadjuvant chemotherapy.
HRT and tamoxifen began 9 months after primary diagnosis.
Despite the   resolution of menopausal symptoms she stopped the HRT (and tamoxifen) after only 3 months.
12 months later she developed a pleural effusion and probable lung metastases.
Tamoxifen did not seem to compromise the effectiveness of HRT.
There is no evidence of rapid disease progression in these patients receiving HRT and tamoxifen.
Thus, it may be possible to design hormone replacement that retains the ability to control menopausal symptoms while protecting against breast cancer.
Nevertheless, there is an urgent need for randomised trials of HRT, with or without tamoxifen, before such untested treatments become common.
Mysterious respiratory disease in USA
SIR — Details of the mysterious respiratory illness mentioned by Nelson (June 12, p 1526) among Navajo Indians are at present sparse, but the symptoms are reminiscent of those described early in the course of an ‘acute nephritis’ which occurred with trench warfare in the First World War.
Although proteinuria and fluid retention was the norm, many soldiers presented with acute dyspnoea and probably low pressure pulmonary oedema, although death from the acute respiratory phase was rare.
The cause was never known but affected soldiers all had close contact with soil — which may explain why the cavalry and officers were rarely affected.
A similar illness was seen during trench warfare in the American Civil War.
In data quoted by Langdon Brown, the condition appears to have been more common in summer months, and in June–July, 1862, there were 148 deaths.
It has been proposed by several workers that trench nephritis was caused by hantavirus, although the classic descriptions from the First World War do not tally closely with the variants of hantavirus infection that have been described in the past 40 years.
QT interval, autonomic neuropathy, and alcoholic liver disease
SIR — The finding by Day and colleagues (June 5, p 1423) that prolonged QT interval in alcoholic liver disease is an independent prognostic indicator and associated with sudden death is of considerable interest.
Patients in their study initially had no clinical or echocardiographic evidence of heart disease and they suggest that the QT interval abnormalities may have been early indicators of alcohol-induced myocardial toxicity.
Another possibility is that these changes are related to an autonomic neuropathy, which may alter the length of the QT interval in the electrocardiogram.
A prolonged interval is seen in diabetic autonomic neuropathy where it is associated with sudden death, and in chronic alcoholics. 50% of patients with alcohol-related liver disease will have evidence of vagal neuropathy on standard cardiovascular reflex tests.
Kempler et al, noted a close correlation between prolonged QT interval and vagal neuropathy in both alcohol-related and non-alcohol-related liver disease.
Autonomic neuropathy could have contributed to the QT interval abnormalities and increased mortality noted by Day and colleagues.
Free radical production following vascular reperfusion
SIR — Grech and colleagues (April 17, p 990) show evidence of free radical production in a spin-trapping method after coronary recanalisation.
With an indirect method (the ratio between linoleic acid and its phospholipid esterfied conjugated diene isomer by high-performance liquid chromatography), we measured free radicals in a patient who had a liver transplant.
We were struck by the similarities between free radical signals that appeared after reperfusion and those noticed by Grech and colleagues (figure).
An increase 30 min after reperfusion of the graft was followed by a fall, then a second increase at 12 h.
We believe that the initial rise is related to increased levels of xanthine oxidase that accumulate during the ischaemic phase  after organ retrieval.
However, this explanation does not account for the second peak.
We agree with Grech and colleagues that neutrophil activation may be responsible.
A liver biopsy specimen taken from our patient shortly after reperfusion of the graft showed neutrophil infiltration.
Since neutrophils are known to have a membrane-associated nicotinamide adenine dinucleotide phosphateoxidase system that generates large numbers of free radicals, this seems a likely source for late free radical generation.
Anomalous acetylcholinesterase in CSF without clinical diagnosis of Alzheimer's disease
SIR — Navaratnam et al and Smith et al reported that an anomalous molecular form of acetylcholinesterase (AChE)(also called AChE-AD) could be found in postmortem and antimortem cerebrospinal fluid (CSF) of patients with histopathologically proven Alzheimer's disease (AD) but not in patients without AD.
We have found, however, that when more samples were investigated with the same methods, the abnormal AChE is not only associated with AD but also related to other neurological diseases that may cause dementia, although these patients are currently non-demented (Mini-Mental State examination):
The positive cases were aged 35–50 (M/F 2/3); the negative cases were aged 15–58 (13/11).
The biochemical and pathological process in AD takes years to develop senile plaques and neurofibrillary tangles (before dementia) and the plaques and tangles are not specific for AD.
The AChE is involved early in plaque and tangle formation.
Kokmen et al, in an investigation of clinical risk factors for AD, stated: ‘We are unable to rule out a similar increase in risk with a history of previous meningitis or encephalitis’, which indicates a possible relation between AD and our 5 positive cases.
Sotelo et al studied 753 patients with brain cysticercosis and found 16% with intellectual deterioration and 3% with disturbances of behaviour.
The fact that 3 of our patients had had brain cysticercosis might indicate that an abnormal AChE could be related to the intellectual deterioration in this disease, when a certain number of cysts are located in the regions responsible for memory and cognition.
Davis et al reviewed 157 necropsy-proven cases of multiple sclerosis and found 47% of patients with a mental disturbance.
In tuberculosis meningitis, Alvarez and McCabe reported 7 cases with abnormal behaviour or altered mental status in a total of 13 patients.
These findings, with the clinical risk study, might form a reasonable explanation for our 5 positive samples, and raise new insights that the CSF AChE-AD may not be specific for AD and may participate in the long biochemical and pathological abnormality of not only AD but also organic dementia.
An AChE-AD in the CSF of non-demented patients might mark a ‘brain at risk’, and treatment to clear the abnormal molecular form of AChE should be given with anti-cyst, anti-tuberculosis, and anti-multiple-sclerosis drugs.
A walking aid for Parkinsonian patients
SIR — Frequent falls are a common source of embarrassment and injury in patients with parkinsonism.
When postural dizziness, fainting, and tripping are excluded, many falls are caused by akinetic freezing — an inability to move the leading foot forward when the trunk starts in motion.
Physiologically, there is a failure to select the right agonist muscles in advance of rapid and precise movement; a delay in switching from one movement to another; and a defect in simultaneously activating different parts of the body.
Patients find visual proprioceptive cues very useful — eg, aiming at marks or patterns on the floor, trying to kick forward the leading leg, or turning up the toes inside the shoe.
A patient of mine has partly overcome his trips and freezing of gait by inventing a small horizontal metal bar, attached by a hinge to the bottom of his walking stick, 4 cm above the ground (figure).
If he glances at this, he is able to step quickly over it with a good length stride, without tripping or interrupting the rhythmical pattern of normal walking.
A friend fashioned this for him at negligible cost.