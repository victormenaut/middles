

PART TWO
A DATABASE APPROACH TO INFORMATION SYSTEMS DEVELOPMENT
Chapter 2
BUSINESS ANALYSIS
2.1 INTRODUCTION
Before we can carry out a detailed examination of the organisation in order to produce a data model which will then be implemented on a database, the analysts need to gain a general appreciation of the business (this term is interpreted broadly to include all types of organisation in the public and private sectors).
This background information will include an examination of the goals of the organisation, the company structure and the roles of key personnel.
Among other things, this will help the analysts to construct an interview plan.
Following preliminary interviews, the analysts will be able to construct models and attempt to find out in outline the information needs of the organisation.
This information will also be useful in deciding on the approach to adopt for information systems development.
However, business analysis may often suggest altering business practices rather than always reveal a need for computer information systems.
Some practitioners argue that a very detailed analysis of the organisation should be carted out at this time.
The danger here, however, is that analysing the working of the company to such a very detailed level early in the life of the project, might cause the analysts to be ‘biased’ towards the present methods of processing and, in general, to be superficial in the investigation and analysis stages.
The analysts may, as a consequence, address the wrong problem by, for example:
Mistaking a symptom of a problem for the problem itself
Accepting people's statements of the problem at face value
Posing questions that presuppose the nature of the problem
Moving too quickly into ‘solution mode’.
One of the main requirements of the analysts is that they view the organisation in a fresh and open way.
Even so, an overall impression of the organisation is still necessary.
An important ‘philosophical’ base of this text comes from systems thinking.
One of its ideas is that of holism — the whole is greater than the sum of the parts.
The inter-relationships between the parts are of crucial significance.
The systems approach focuses on the organisation as a whole and is concerned with the performance of the organisation and not on the specific requirements of any one department or application.
However, it is usually difficult to persuade employees to see this organisation-wide  perspective.
One of the advantages of the database approach is that it permits the sharing of data.
The database is viewed as a resource of the organisation as a whole.
In conventional data processing, on the other hand, files are seen as a department or single application resource.
Conventional files are not usually shared between applications.
The terms information and data represent different things.
Data elements represent facts concerning people, objects, events, and so on.
A person's date of birth or driving licence number are examples of data.
‘25786’, ‘78700199’ and ‘19873’are examples of data.
They represent a man's date of birth, a driving licence number and an identity number.
A date of birth (25/7/86) associated with a driving licence number (78700 199) and an identity number (19873) could be used to give the information that a person whose identity number is 19873 possesses a driving licence, even though that person is under the minimum legal age for driving motor vehicles.
The information comes from selecting data and presenting it in such a way that it is meaningful and useful to the user.
Sometimes, as in this case, its value comes from its rareness or unexpectedness.
The database will contain the facts of interest to the organisation.
Each item of data on its own may not be useful to the police officer studying the data.
Much has to do with the context in which the data is interpreted.
The information system should transform the data and present these facts:
Accurately (and consistently accurately)
Relevant to the appropriate recipient
In the correct level of detail
Completely
When required.
We have argued in Chapter 1 that computer databases offer advantages in storing and presenting the data over manual or computer file-based systems.
But the analyst needs to know how best to design the database and the information systems that use it.
Well designed information systems can give the information which will assist managers make better decisions, give their businesses competitive advantage or help them improve their cost structure or product base.
A major prerequisite to successful information systems is therefore finding out what the organisation is about, what direction it is going and what information is required to support these goals.
This issue is addressed in this chapter.
Although data does not appear at first sight to be a resource of the organisation, it certainly is a resource, as data is essential for the organisation to operate effectively.
If a company ‘lost’ its accounts data, it would represent a serious loss and one which would be difficult to recover from.
Some firms sell data, such as the names and addresses of their customers, which can be used by other companies for mailing lists.
Data is costly to collect, store and keep up-to-date and it is also costly to transform data  into information.
The information systems will access data from the database and alm to transform it into information which the organisation can use.
The value of information can be readily appreciated.
for example, accurate information about future weather conditions will help the manager of a shop to determine whether to buy stocks of ice cream or stocks of soup.
Here, information has the key role of reducing uncertainty.
Poor information is likely to prove very costly for the shop owner through unsold stocks or missed sales opportunities.
Such decisions, for example, as where and when to build a new warehouse, whether to buy out competitors or whether to change product structure, may have very long-term consequences and involve huge investments of money.
As can be seen from figure 2.1, information systems are designed to help managers make better decisions.
The model assumes that managers can specify their information requirements and that these can be predicted by the information system.
The system then has to transform these requirements for information into requirements for data.
The data will be retrieved from the database, assuming that it has already been collected and stored.
Otherwise a data collection exercise is necessary to capture the data and store it in the database.
Having retrieved the raw data, it needs to be transformed into the required information, perhaps by some preceding analysis work.
The information can then be presented to the manager who now has the opportunity to make good or at least better decisions because the information on which the decisions are based will be of a better quality and more timely.
The information system aims to support decision-making and the decisions ought to be directed towards achieving the goals of the organisation.
These are discussed in Section 2.2, along with a look at the universe in which the business is operating.
In Section 2.3 we look at the system of values, beliefs and understandings that are shared by members of the organisation, that is, its ‘culture’.
We then look at the structure of management in the organisation in Section 2.4.
A description of the roles of management (and of the users in general) and the information requirements supporting those roles is given in Section 2.5.
The role of the systems planning team is emphasised.
This will consist of the people involved in the technological change and they will play a major role in deciding what strategy to adopt and in following that strategy through.
Just as ‘the users’ need to be defined, so do ‘the analysts’.
The systems approach to looking at organisations suggests mixed teams and we describe the roles of the various personnel likely to be involved: users and management as well as the business and technical analysts.
There are a number of techniques that can be used to help in the business analysis phase, including interviewing strategies, information modelling, rich pictures and root definitions, and some of these are discussed in Section 2.6.
The various strategies for developing information systems are outlined in  Section 2.7.
The final section of this chapter stresses the importance of participation by managers and all users.
The more radical strategies will not be feasible unless the political climate of the organisation is conducive to major change.
Fig. 2.1 The basic information system.
2.2 GOALS OF THE ORGANISATION
The analysts cannot comprehend current or future information requirements unless the direction in which the organisation is going is identified.
Management needs to ensure that the expensive and long-term project has a good strategic pay-off.
The identification  of the goals of the organisation is therefore essential.
Texts used to talk of organisations having just one goal, frequently profit maximisation.
According to this view, the organisation was ‘tuned’ to maximise this one goal.
The truth is that businesses have a number of goals.
These could include:
Increasing the size of their market
Increasing the return on capital
Increasing turnover
Ensuring long term survival
Improving the welfare of their employees
Improving their public image
Increasing profits.
Obviously most firms would readily sacrifice some profit if it meant that they were likely to be in business for some time.
It could be that there are major goals which businesses fulfil, such as maximising the return on capital, whilst having shorter term aims, such as increasing turnover.
During recent times, the main goal of many firms may have been long term survival.
Much will depend on the general business climate and the particular business in which the organisation operates.
Some goals may conflict.
In a retail organisation one of the goals, to increase sales targets, may conflict with others, such as reduce staff levels or increase profit margins.
If sales levels do increase, it may be as a result of increasing sales staff or decreasing profit margins.
Many organisations do have a corporate mission statement which is a declaration of the company's ‘reason for being’.
It reveals the long-term vision of the organisation in terms of what it wants to be.
The mission statement of a health authority may be to promote the health and hygiene of the people in its catchment area.
Those of a church, hospital, business or charity are likely to be very different.
This type of information may well be gleaned from interviewing members of the directorate — the chief executive, vice presidents, general managers and, perhaps, divisional managers.
Other sources of information include the written company review and accounts, which is usually published annually as part of the statement to the shareholders or governors, and recruitment brochures.
They will tend to stress long term objectives such as growth in assets, profitability, the degree and nature of diversification, earnings per share and social responsibility, and may specify the mission statement.
Medium (say, a period of six months to two years) and short term (less than six months) objective setting is usually the concern of the individual managers.
They will be used as a basis for:
Allocating resources
Evaluating managers' performance 
Monitoring progress towards achieving long term goals
Establishing divisional and departmental priorities.
All this company background can help the analysts get a ‘feel’ for the organisation, particularly if they are from outside the company.
Information systems work is frequently carted out by external consultants or people recently recruited from outside, because they have had experience of such work elsewhere.
We will look into the techniques of information gathering and analysis in Section 2.6.
According to Porter (1980 and 1985), the most important element is the industry in which the business competes.
There are five competitive forces:
The threat of new entrants coming into the same market
The bargaining power of suppliers
The bargaining power of buyers
The availability of substitute products or services
Rivalry among competitors.
Such a study enables a better understanding of the business and promotes new ideas for information systems.
Figure 2.2 identifies where action needs to be taken either defensively or offensively to influence the forces in their favour in order to pursue the goals of that firm.
Fig. 2.2 Five forces of industry competition (adapted from Porter, 1980)
This is a useful and interesting view, even if it is rather narrow.
It is also useful to look at the broader environment of the organisation, because the actions of trade unions, financial institutions, stockholders and the government, for example, as well as customers, suppliers and competitors, can have far-reaching effects on the business.
Any information system needs to be sensitive to the present and future environment.
Management should observe and react to changes in economic, social, cultural, demographic, geographical, political, legal, governmental, technological and  competitive factors.
These factors can affect consumer demand, types of product developed, the services offered and choice of business to acquire or sell.
Although it is not always possible to record all this information in the modelling process, it is necessary to be at least aware of these factors.
figure 2.3 paints a sketch of the business universe having a number of major subsystems and environmental systems which relate to the business.
Fig. 2.3 The business universe
The analysis of the environment of an organisation, including the economic environment, needs to be supplemented by analysis of the political and cultural realities of the organisation, which may be at least as influential in the formation of the company goals and will be very important to the success of information systems.
For example, the individual organisations might explain why many firms opt for an expansionist policy in times of recession.
lt is important that the analyst detects the views and expectations of individuals and groups from within and around the organisation that exert power in it.
These may be related to the history and age of the organisation, leadership and management style and its structure and systems.
We will  look at these human and social issues in the next section.
2.3 CULTURE OF THE ORGANISATION
The success of the database venture will depend to a large extent on the culture of the organisation, that is, its people and their values, beliefs and understandings.
These will be shared by members of the organisation and set a pattern for its activities and actions.
This is likely to have had a great influence on the goals and, in particular, the mission statement of the organisation which were discussed in Section 2.2.
Essentially, the culture is a credible set of norms which employees, either consciously or unconsciously, use as guidance in meeting their specific job tasks.
Deal & Kennedy (1982) suggest that there are five elements of corporate culture:
Business environment: the environment in which the organisation operates will determine what it must do to be a success.
Values: these define what is seen as success by the organisation and establish standards of achievement within the organisation.
Heroes: these suggest role models for employees to follow.
Rites and rituals: the systematic routines in the company which define expected behaviour.
Cultural network: the informal means of communication.
The use of this network will be required in order to get things done.
A failure in an information systems project may be more due to a lack of recognition of or respect for the culture of the organisation than because of the technology itself.
On the other hand, failure may be caused by the culture.
Productivity gains expected may be seriously constrained because of ingrained attitudes which cause human behaviour to change slower than the potential rendered by technology or even go in a direction directly opposed to the requirements of the system.
Culture can cause an inward-looking view which may lead to missed opportunities in changing external conditions.
The analysts will have to adhere to the cultural norms of the organisation in order to be successful with their database project.
They have to deal with the attitudes to change found within the organisation, attempting to amplify positive attitudes and reduce negative ones.
Deal & Kennedy suggest the following in situations where the culture may go against successful implementation:
Position a hero in charge of the process
Recognise a real threat from outside
Make transition rituals the pivotal elements of change (for example, let people mourn old ways yet re-negotiate new values)
Insist on the importance of job security in the transitional stage.
As has already been highlighted, there is a great deal more to successful database and information systems projects than the ‘state-of-the-art’ technology, and the analysts explore further these factors in the business analysis stage.
Before the team of analysts engrosses itself into data analysis, which is partly carried out through interview and observation, it is necessary to consider the structure of the organisation and the roles played by members of the management team.
Without this knowledge, it is not possible to decide on the people to interview, in which order they should be interviewed, and the level and subject matter of the questions to ask.
We will look at these aspects in Sections 2.4 and 2.5.
Section 2.6 looks at some techniques that the analysts might use to analyse the information gathered.
2.4 STRUCTURE OF MANAGEMENT IN THE ORGANISATION
In attempting to understand the organisation it is essential to realise that there is a trade-off between being simplistic and being practical.
The management of organisations is very complex.
We will first view organisations as having three layers of management because this traditional view proves useful as a basis for analysis.
However, we should be aware of the greater complexity that really exists, and other structures, such as cell or matrix structures, have been proposed as being a more realistic reflection of management structure.
At the top of our three layer model, are the board of directors.
They are responsible for the long range planning activities of the firm and they will set the overall goals.
Middle management, typically heads of departments, will ensure that these policies are carried out and will act upon those conditions that veer from this norm (exceptional conditions).
The operations level will be responsible for the day-to-day operations of the organisation.
These people might include the chief clerks and foremen who control the dally ordering, production and distribution processes.
It is important to identify these individuals in the early stages of the project.
Let us consider a retail company which has a number of department stores and warehouses.
Strategic management for this business might include the directors of finance, trading, personnel and buying.
Middle management might include the heads of the branches, branch accountants and warehouse managers.
Operations management might include department managers, warehouse foremen and the head clerks.
This structure is shown in Figure 2.4.
Top management is clearly very important to the organisation, and managers are realising that management information systems can help them make better decisions.
Although the euphoria associated with automatic decision-making has largely gone, the more realistic concepts of decision-support systems and executive support systems have certainly not disappeared.
Management need these systems in order to help them establish sustainable goals and to provide information relating to the long-term decisions of the organisation.
Competitive advantage can be gained from exploiting information at the strategic level so as to increase business efficiency or to highlight areas in the organisation which are particularly strong or weak, and thus lead to new business strategies.
Fig. 2.4 The pyramid structure — layers of management for a retail company
Examples of the types of decisions that managers will make could include:
Where to site new factories
Whether to merge with other companies
Whether to drop a product range.
Information to help top management make these decisions could be provided by the decision-support system.
When the model of the organisation is implemented on a database, it will be possible for managers to make enquiries to the database.
The types of information required will tend to be unpredictable and unstructured so that database access will need to be flexible.
Such enquiries may be answered by getting data from a number of different areas in the database (for example sales, product and employee data to compare last year's with this year's performance).
Information systems should be integrated with the business plan at all three layers of the management hierarchy.
Figure 2.5 shows the types of decision, figure 2.6 examples of the decisions, Figure 2.7 the types of information required to support this decision-making, and figure 2.8 examples of such information.
A retail company with  a number of ‘High Street’ departmental stores will be used as an example.
As well as determining the information needed now and in the near future, some regard must be given to the difficult task of trying to foresee information needs in the longer term (Land, 1982).
Priorities need to be assigned to information needs, and these priorities have to be assigned by management.
Fig. 2.6 Examples of decision for a retail company
Middle management will ensure that the goals set by top management are being met.
Reports about performance will be produced regularly so that results can be compared to the targets set, and the facilities of the database are also likely to be made available to managers with the help of a query language.
The unusual, for example, products which are unavailable from stock, may also be reported on, so that managers may act on these exceptional conditions.
This will allow management by exception.
Much of the information required here will be predictable and internal.
for example, each week the manager might want a list of those customers who have not cleared their debts.
Decisions at the Operations management level can frequently be made automatically.
These decisions tend to be structured, rule-based and routine.
For example, a computer system can check stock levels and produce a supplier's order request automatically in good time, and this will ensure that there are always supplies of products in stock.
Fig. 2.7 Types of information required by retail company
Early in this section it was pointed out that not all organisations fit in easily to the three level structure.
Some smaller firms may have a centralised functional structure where tasks and activities are arranged by business function such as:
Production
Finance
Marketing
Research and development (R & D)
Personnel.
Some medium-sized firms will be decentralised and structured by divisions.
Within that type of structure there are four ways of organisation:
By product or service
By geographic area 
By customer
By process.
Functional activities will be performed centrally and in each division.
The headquarters will be responsible for planning and corporate performance, and divisional managers for profits.
Fig. 2.8 Examples of information required by retail company
Many large firms have a matrix structure which is the most complex, since it is dependent on vertical and horizontal flows of authority and communication, as opposed to the divisional and functional structures which depend on vertical flows of authority and communication.
Matrix structures exhibit dual lines of budget authority, dual sources of reward and punishment and dual reporting channels.
Much of the discussion related to the three layered model above applies to these situations, but the analyst needs to be aware of other frameworks and recognise them when they occur.
It is important that the information given by the system is relevant to the particular recipient and therefore in the correct level of detail.
This could be in summary form —management by summary.
It must also be accurate, or at least accurate enough for the recipient, and timely, as information provided too late for the particular p urpose is use it can respond to the changing needs of the user.
This means that the data analysis stage is as crucial as the implementation stage of the computer applications.
The most sophisticated technology is useless if the data is not appropriate; and the most accurate data collection exercise is useless if the technology does not support timely and adaptable systems.
2.5 ROLES OF KEY PERSONNEL AND SYSTEMS PLANNING TEAM
We have already discussed some of the roles of personnel, in particular the various levels of management.
The information needs of the individuals in an organisation should be identified.
The information requirements of some personnel may be inferred from their job description.
It may be easy to find out formal descriptions of the roles of personnel.
Many firms publish full job descriptions along with details of the reporting structure.
Equally important is the informal structure.
Important parts of the job could be carried out at break times or through telephone calls.
Official communication channels may be side-stepped.
The analysts therefore need a ‘feel’ for the informal as well as the formal system.
This exercise will also reveal the likely users of the information system.
One fact will be immediately obvious: the users of the information system will not all be trained computer experts.
Untrained computer users may make particular requests of the system or they might ‘browse’ through files.
This type of casual usage is likely to grow.
Therefore, although some access to the information system and therefore the database will be made by professional users through computer programs written in Cobol and other programming languages, some, perhaps most, access will be made by untrained users.
Some of these will be regular users , that is they may make daily or weekly access to particular parts of the system and they may be willing to train in the use of particular facilities.
Clerical staff who may be required to input data into the system will be expected to train in order to use the system.
Others will be casual users, and these users, frequently middle or top managers, might have had little knowledge and experience of computing.
Their use of the system will be varied.
Each day's enquiries could be different and based on different parts of the database.
It is therefore difficult to train such staff, even if they had the time and inclination to practise the skills necessary.
for this reason, there must be query languages available which make it easy for untrained users to use the system.
Olle (1991) suggests nine key roles to the successful development of an information system:
Executive responsible
Development coordinator
Business analyst
Designer
User's acceptor
User
Constructor's acceptor 
Constructor
Resource manager.
The executive responsible is a member of the Board assigned to ensure the successful progress and completion of the project.
We have already stressed the importance of senior managers demonstrating their commitment to the new system and this is therefore a key role.
The development coordinator is responsible for the day-to-day control of the project and will report back to the executive responsible.
The business analyst is responsible for the analysis of the business needs and the designer prepares the design of the information system.
The user's acceptor represents the users and approves the specifications from the users’ point of view before construction starts, and the user will use the operational system.
As we saw in the previous paragraph, there are many kinds of user.
The constructor's acceptor checks that the information systems design and ‘product’ conforms to the appropriate standards and the constructor writes the programs and other parts of the ‘package’that is the information system.
Finally, the resource manager ensures that the necessary resources are available for the project to develop smoothly.
Of course the actual roles and how many people in each role will depend on the particular organisation; indeed, we will discuss other roles, such as that of the database administrator, but this suggested structure is a useful starting point.
We now discuss the role of the systems planning team whose main function is to coordinate and control the information systems project and ensure information systems are closely aligned with the goals of the organisation.
With the growing emphasis on information technology for competitive advantage, systems planning may to some extent govern business plans and certainly influence them.
It is important that the systems planning team is not dominated by the technologists, although it will certainly include them.
Corporate experience, rather than technical expertise, should dominate.
The information systems project will affect the whole organisation and, as we have seen, it is necessary to include top management in the team.
This would suggest that the chairman of the systems planning team would be the ‘executive responsible’, who would carry the required status.
Other management representatives of the systems planning team could include the production manager, marketing manager and other department heads.
Such a high-powered team should ensure that the project has the prestige necessary to carry its proposals through the organisation.
It should also help to get the management commitment behind the project and this will significantly increase the likelihood of its success.
Top management will be seen to lead by example.
The information systems project will cause changes to the roles of employees and in working relationships.
The systems planning team ought to anticipate problems that may occur.
It is therefore necessary to include the personnel manager and a trade union  representative in the systems planning team.
As we shall see in Section 2.8, it is important to keep employees fully informed of the project.
Many systems may be excellent from a technological point of view, but fail because of a lack of consideration given to ‘people’ issues.
The new system may be seen as a threat to status and job.
Frequently staff will resist the change in ways which may be less dramatic than sabotage, but be equally effective.
Once the trust of the workforce has been lost, it is difficult to regain it, even if future change is perceived to be in the interest of employees.
Two of the most important representatives from the innovating group will be the database administrator and the chief analyst who is likely to be the project leader.
The chief analyst should have both organisational and technical skills and be seen as linking the users and the technical group, and needs to possess good personal skills.
This role is likely to be that of the business analysis rather than‘designer’, who will be concerned more about the technical aspects.
In some circumstances the database administrator will be the project leader.
The need for a database administrator is clearly recognised in the methodology, although the role is often filled late in the life of the project, perhaps when it is too late.
The database administrator can help the systems planning team decide on standards for communication, documentation, project development and evaluation, and help to implement these standards.
Frequently the systems planning team also includes an outside consultant.
This person will not have a background in the organisation, and a perceived lack of departmental bias will be useful when arbitrating on differences of opinion Another reason for ‘outsider’ expertise could be the lack of internal expertise in projects of this kind.
Figure 2.9 shows the possible membership of the systems planning team.
Fig. 2.9 The systems planning team
One of the most important tasks of the systems planning team is to determine the boundaries of the information system.
The decision may be implicit by the membership of the team; even so, the boundaries must be defined explicitly.
For a corporation like Unilever, which has a number of companies in its overall control, the boundary may be  defined to include one or a number of these companies.
With some organisations, the various subsystems may be so large that the boundary might be defined around accounts or personnel.
This may be particularly appropriate where subsystems do not naturally relate, that is, there is little data travelling between them or where one or two areas, such as production/supply and invoicing, in the case of the electricity supply industry, dominate the business.
Once the overall information system's boundary has been fixed, it will be itself divided into lower-level systems (sometimes known as subsystems) for separate development.
These arrangements will largely depend on the strategy adopted for the development of information systems.
The determination of this strategy, a positive plan for the development of information systems, is a particularly important role of the systems planning team and is discussed separately in the next section.
The data analyst has to know the boundary of the data analysis exercise and how much time to devote to it.
The systems planning team will ensure that the subsystems are developed according to the plan which allows for some form of integration later.
Each subsystem should be seen as a natural subsystem of the larger system.
The priority for developing these subsystems will depend on potential benefit, urgency of need, probability of success or natural precedence, which could be described as the next ‘piece of the jigsaw,.
Lederer & Mendelow (1989) suggest a number of guide-lines which should be considered when planning:
Develop a formal plan: set objectives and policies in relation to the achievement of organisational goals and thereby enable the effective and efficient deployment of resources.
Link the information systems plan to the corporate plan: provide an ‘optimal project mix’ which will be consistent with and link to the corporate plan, ideally over the same time period.
Plan for disaster: ensure that dependencies are identified and damage likelihood identified.
Audit new systems: evaluate present systems to identify mistakes and hence avoid their repetition and to identify areas where a small resource input might have led to a larger benefit.
Perform a cost-benefit analysis: identify intangible and tangible benefits and costs before putting in the required resources.
Develop staff: make use of and develop the skills of staff.
Be prepared to change: as relationships, structures and processes change.
Ensure information systems development satisfies user needs: understand the tasks and processes involved to establish the true user requirements.
Establish credibility through success: build up user confidence through previous  success, thereby promoting cooperation and lowering barriers.
The systems planning team will overview the development of the project although they will appoint a systems development team to control its day-to-day running.
The constituents of the systems development team will depend on the degree of participation adopted, although it is usual, if not always desirable, that it is biased towards the technologists.
The systems development team will include the chief analyst and the database administrator who are both likely to be members of the systems planning team.
The project leader, who will be one of these two representatives, is likely to act as chairman of the development team.
Other likely members of the systems development team will be data analysts who will be involved in developing the data model, and systems analysts, systems designers and programmers who will specify the needs of the users, and design and develop the various subsystems and programs (or choose and modify the application packages).
The final constituents of the systems development team will be the representatives from the user departments.
These are likely to include the department managers and possibly people who will use the system when it is implemented.
We will say more about user involvement in Section 2.8 and further in Section 6.4.
2.6 TECHNIQUES FOR INVESTIGATION AND ANALYSIS
In this section we look at some techniques for finding out information about the business and for analysing this information.
Some techniques could be described as formal and reasoned, and others as creative and informal.
As stated earlier in this chapter, at this stage we are not looking for solutions but an understanding of the organisation.
Some of these techniques stimulate the identification and representation of as many views and perspectives on the problem situation as possible.
The analyst will be asking such questions as:
What should be happening here?
Who or what are involved and how are they affected?
Who or what seems to be preventing the desired result?
Why might this be happening?
Are there associated problems that might be connected?
Could this be a symptom of a more fundamental and deeper problem?
Rickards (1974) discusses a variety of techniques which will enable the problem-solver to view the situation from many perspectives and to widen the horizon of the study:
Goal orientation: the idea here is to think of goals and obstacles.
Each situation should  be approached by enquiring, for example, what are the goals involved, that is , ‘what do I want?’, what are the boundaries to the situation, that is , ‘what constraints do I not want to challenge?’, and what are the barriers to the goal, that is , ‘what is blocking the path to where I want to be?’.
In this way, situations can be redefined in ways which may be more useful for analysis.
Successive abstractions: these define the situation in terms of higher and lower levels of abstraction.
If we consider the exercise-bicycle market shown in figure 2.10, for example, we could move to a higher level of abstraction to new fitness machines and to lower levels of abstraction which can reveal a whole range of possibilities.
Analogies and metaphors: these redefine the problem in an imaginative, non-literal fashion.
The original problem definition of ‘How to improve the sales team?’ could be re-expressed as ‘How to make the sales team perform like Newcastle United Football Club?’(an analogy which may suggest better team training or a new look at tactics) or ‘How to address corporate turbulence?’a metaphor which may prompt ideas of inter-departmental conflict.
Wishful thinking: the statement ‘If I could do anything I would implant the standards in his brain’ may spark off more realistic ideas such as running a course to explain the standards (perhaps in a country hotel so as to make it a pleasant experience).
Non-logical stimuli: the first step here is to list as many aspects as can be thought of connected with the problem situation and to consider a completely irrelevant, unconnected object and to see how many ways that the object could affect the situation.
for example, a production manager may be stuck about ideas on how to improve factory efficiency but looking at a potted plant on the desk might suggest an improved working environment or employees being given more wages (which may be seen as the equivalent of plant food).
Boundary examination: Here, the analyst writes a statement outlining the interpretation of boundaries and constraints which should then be critically examined for hidden assumptions and preconceptions.
Fig. 2.10 Successive abstractions
Having an overall view of the nature of the business, the people in it, and its information needs, it is possible to construct in information model.
This is largely a pictorial representation of the organisation in outline and it can be used as a basis for discussions with management.
It combines a process-oriented view of the organisation (supplier ordering, warehouse management and stuck control management) with a data-oriented view (goods to follow, stocks in warehouse and supplier orders).
As a result of the discussions described in this section, the information model may well change in content.
It will show the major application systems of the organisation and the flows of resources between them.
It is particularly useful in providing some boundaries and interfaces to the organisation.
Some logical files have also been included in the model.
These give the analyst ideas on what sort of data will be needed to support information systems.
But all this information is in outline.
It is not intended to suggest the detail of process nor detailed data storage facilities.
Figure 2.11 shows one iteration in the creation of an information model for a retail business.
The final version will be developed after several interviews with management personnel.
I have included in the model some of the names of the smaller subsystems, sometimes referred to as modules, in each of the larger systems areas.
These include stock control, supplier accounting, customer accounting and personnel.
Within stock control, for example, are supplier ordering, warehouse management and quality control.
The four major subsystems are carried out at different locations, by different personnel, and therefore formed ‘natural’ boundaries.
By the time the model has been constructed, the analyst has gained an appreciation of the business in outline.
With this knowledge it will be possible to agree an appropriate ‘strategy’ for the systems project.
This will be a decision of the systems planning team whose function will also be to oversee the development of the information systems project
Many of the factors discussed in this chapter are seen in the information model, and   these include interfaces, boundaries, subsystems and the control of resources.
However, some factors are excluded, for example, organisational structure, roles of personnel, organisational goals, employee needs, issues, problems and concerns.
Another technique is to construct a rich picture.
This is a pictorial caricature of an organisation and is an invaluable tool for helping to explain what the organisation is ‘about’.
It should be self-explanatory and easy to understand.
Fig. 2.11 Developing the information for a retail company
One may start to construct a rich picture by looking for elements of structure in the problem area.
This includes things like departmental boundaries, activity types, physical or geographical layout and product types.
Having looked for elements of structure, the next stage is to look for elements of process, that is, what is going on'.
These include the fast-changing aspects of the situation: the information flow, flow of goods and so on.
The relationship between structure and process represents the climate of the situation.
Very often an organisational problem can be tracked down to a mismatch between an established structure and new processes formed in response to new events and pressures.
The rich picture should include all the important hard ‘facts’ of the organisational situation, and the examples given have been of this nature.
However, these are not the only important facts.
There are many soft or subjective ‘facts’ which should also be represented, and the process of creating the rich picture serves to tease out the concerns of the people in the situation.
These soft facts include the sorts of things that the people in the problem area are worried about, the social roles which the people within the situation think are important, and the sort of behaviour which is expected of people in these roles.
Typically, a rich picture is constructed first by putting the name of the organisation that is the concern of the analyst into a large ‘bubble’, perhaps at the centre of the page.
Other symbols are sketched to represent the people and things that inter-relate within and outside that organisation.
Arrows are included to show these relationships.
Other important aspects of the human activity system can be incorporated.
Crossed-swords indicate conflict and the ‘think’ bubbles indicate the worries of the major characters.
Figure 2. 12 shows a rich picture for part of a district health authority.
Rich pictures are described fully in Avison & Wood-Harper (1990) as are associated techniques such as root definitions.
Root definitions can be used to define two things that are otherwise both vague and difficult.
These are problems and systems.
It is essential for the systems analysts to know precisely what human activity system they are to deal with and what problem they are to tackle.
This is a concept which is an aid to thinking about and understanding, in the most general way possible, the organisation in which the information system is to operate.
The root definition is a concise verbal description of the system, which captures its essential nature.
Each description will derive from a  particular view of reality.
To ensure that each root definition is well-formed , it is checked for the presence of six characteristics.
Put into plain English, these are who is doing what for whom, and to whom are they answerable, what assumptions are being made, and in what environment is this happening?
If these questions are answered carefully, they should tell us all we need to know.
Fig. 2.12 Rich picture for part of a district health authority (from Avison & Wood-Harper, 1990)
There are technical terms for each of the six parts, the first letter of each forming the mnemonic CATWOE.
We will change the order in which they appeared in our explanation to fit this mnemonic:
Customer is the ‘whom’
Actor is the ‘who’
Transformation is the ‘what’
Weltanschauung or world view is the ‘assumptions’
Owner is the ‘answerable’
Environment (or environmental constraints) is kept as the ‘environment’.
The first stage of creating the definition is to write down headings for each of the six CATWOE categories and try to fill them in.
This is not always easy because we often get caught up in activities without thinking about who is really supposed to benefit or who is actually ‘calling the tune’.
We may question our assumptions and look around the environment even more rarely.
Even so, the difficulty for the individual creating a root definition is less than the difficulty in getting all the individuals involved to agree on the definition to be used.
Only experience of such an exercise can reveal how different are the views of individuals about the situation in which they are working together.
Root definitions are particularly useful in exposing different views.
We will look at an information system for a hospital to illustrate this.
The different people involved in a hospital will look at the system from contrasting positions.
Furthermore, these viewpoints in this problem situation are very emotive as they have moral and political overtones.
In some situations this can lead to deliberate fudging of issues so as to avoid controversy.
This is likely to cause problems in the future.
Even if the differences cannot be resolved, it is useful to expose them.
Here are three different root definitions of a hospital system.
They all represent extreme positions.
In practice, anyone trying to start such a definition would make some attempt to encompass one or more of the other viewpoints, but any one of these could be used as the starting point for the analysis of the requirements of an information system in a hospital.
THE PATIENT
CUSTOMER
ACTOR The doctor
TRANSFORMATION Treatment
WELTANSCHAUUNG I've paid my taxes so I'm entitled to it
OWNER The system’ or maybe ‘the taxpayer’
ENVIRONMENT The hospital
This could be expressed as ‘A hospital is a place that I go to in order to get treated by a doctor.
I'm entitled to this because I am a taxpayer, and the system is there to make sure that taxpayers get the treatment they need.’
THE DOCTOR
CUSTOMER Patients 
ACTOR Me
TRANSFORMATION Treatment (probably by specialised equipment, services or nursing care)
WELTANSCHAUUNG I'll treat as many people as possible within a working week.
OWNER Hospital administrators
ENVIRONMENT National Health Service (NHS) versus private practice.
My work versus my private life.
‘A hospital is a system designed to enable me to treat as many patients as possible with the aid of specialised equipment, nursing care, etc.
Organisational decisions are made by the hospital administrators (who ought to try treating patients without the proper facilities) against a background of NHS politics and my vision is of a lucrative private practice and regular weekends off with my family’.
THE HOSPITAL ADMINISTRATOR
CUSTOMER Doctors
ACTOR Me
TRANSFORMATION To enable doctors to optimise waiting lists
WELTANSCHAUUNG Create a bigger hospital within cash limits
OWNER Department of Health and Social Security (DHSS)
ENVIRONMENT Politics
‘A hospital is an institution in which doctors (and other less expensive staff are enabled by administrators to provide a service which balances the need to avoid long waiting lists with that to avoid excessive government spending.
Ultimate responsibility rests with the DHSS and the environment is very political’.
All the techniques discussed in this section can be used to find out more about the organisation and to help determine the area for further work, the area for data analysis, database development and information systems development.
The decisions relating to this strategy will be discussed in the next section.
2.7 STRATEGY FOR INFORMATION SYSTEMS DEVELOPMENT
There are a number of alterative strategies that the systems planning team may decide to adopt.
It is essential that full consideration is given to the evaluation of which  strategy or combination of strategies is appropriate for the organisation, so that information systems will align with business needs.
Inadequate planning will lead to failure — however good the tools and techniques used in the database project.
The particular strategy appropriate to the organisation will depend to a large extent on the political and financial circumstances of the organisation.
Attempts in the past to implement change may have left scars, and the spirit of trust, confidence and cooperation necessary to implement radical change will be difficult to achieve.
The organisation may not be used to changes: both with regard to the roles of personnel and the technology used.
Blumenthal (1969) suggests that six strategies are possible and these are discussed first, although the meaning of each has been changed somewhat to take account of the technological and other developments that have taken place since that text was published.
These strategies should not he considered as mutually exclusive: usually the systems planning team decides that the best strategy for their organisation is one which is a combination of those discussed, and I have attempted to incorporate some of these possibilities.
One strategy for implementing information systems would proceed by breaking up the information systems project according to the departmental structure of the organisation.
This organisation chart approach would be carried out by implementing each department's system in turn.
In the retail organisation looked at earlier in this chapter, this could start at the sales office, then to accounting, warehousing, production, buying and personnel.
This has the particular advantage of avoiding possible political problems that may have occurred if a more radical strategy had been adopted, because the workplace and working relationships will probably remain intact, although the roles of the employees in it may change.
The management team is also likely to remain the same.
For the overall systems project to be successful, however, the design needs to allow for integration later.
An opportunity will be lost if systems in the various departments cannot ‘talk’ to each other and share the same database.
Thought has to be given to future integration at the initial design stage, otherwise subsequent integration will be very difficult and expensive.
There will be no ‘common language’ between the systems.
The systems planning team has a particularly important role to ensure a ‘design for integration’ and also to prevent any delay in this integration.
The longer the delay, the more difficult integration becomes.
Nevertheless, applications in many organisations have been built up piecemeal in an unorganised, often chaotic way.
They are often on different computers, with different capabilities, manufacturers, software and so on.
It is not only a difficult job to attempt to integrate these but also one that might have opposition from users who would prefer  to keep to a successful, if limited, system.
Much will also depend on the structure of organisations: the benefit from integrating the databases of separate departments may not be as great as the costs involved.
With many organisations, following the departmental boundaries of the firm will not be a good strategy.
Many processes, such as sales order processing, will cross departmental boundaries.
The sales order may first enter the sales office, then be processed by the warehouse, then the production department, then distribution, and finally the accounts department.
This reflects the following processes.
The order is received by the sales office; checks are made in the warehouse to see if the sub-assemblies are in stock; a schedule is made in the production department and the products manufactured; the goods are despatched by the distribution department; and the invoice typed up in accounts and sent to the customer.
lt is not surprising therefore that the strategy for information systems development which follows the existing departmental structure of the firm may be the easiest politically but not the one that gains the most from the database and informations systems project.
Even conventional processing is inter-departmental.
An alternative approach would break up the organisation according to some other criteria but implement each section in turn and allow for integration later.
Again, without this design for integration it will be very difficult to achieve the expected gains of databases and information systems.
This alternative design may be based on management needs, a sort of top-down approach.
This approach can be a good strategy where the information required by top management is fairly stable in terms of content, level of detail and frequency.
Designing the information systems project around management needs may, however, ignore the operational requirements.
To fulfil operational requirements as well may lead to data being collected, validated and stored more than once.
However, there are circumstances where management information cannot be derived from operational data.
Executive information systems, which are designed for strategic management, are usually of this type.
They combine information from both external and intemal sources.
There are databases which can be bought in from outside (provided by government, consultancy and other organisations) and are as important a source for information as that gained from internal databases.
Sophisticated analysis and model building tools enable the data to be converted into the information required to support decision-making by, for example, testing individual and organisational assumptions and values (discussed earlier in Section 2.3 on organisational culture).
In following the above approaches, there may be a tendency to design in terms of applications which mirror the present requirements of the business.
There is a danger that these needs will change.
The importance of developing the database without regard  to applications has already been stressed.
Applications requirements frequently change: data types and relationships are more stable.
The data collection approach stresses the importance of data collection and analysis without regard to functions.
The data is collected for an area of the organisation.
lt is then classified and the relationships between the data elements are recorded for use when required.
One of the criticisms of this approach is that data may be collected which is never used subsequently.
However, there is a two stage process: data modelling and mapping to the database.
Data which is unlikely to be used need not be transferred to the database.
The database approach need not necessarily be associated with data collection and classification.
Data analysis can be useful without databases.
The process of modelling the organisation should result in a better understanding of the business.
The data and data structures that have been identified can be mapped on to conventional computer files or clerical files and not necessarily databases.
Similarly, the database can be built up piecemeal without an organissed data analysis exercise.
However, the resulting database system may well not prove as flexible nor as long-lasting as one formed by a preceding data analysis exercise.
This text therefore proposes that data analysis is followed by database implementation.
The drawback associated with this approach is that data may be held indefinitely with no use being made of it.
It is an important role of the database administrator to control attempts to keep data on the database indefinitely and to ensure that data entering the database is likely to be of use in the near future.
The functions of the organisation should therefore be kept in mind in data analysis, even though in principle data analysis should attempt to look at the organisation independent of any processes.
Implicit in the discussion of this data analysis and database approach is that a total system is implemented.
By this is meant that design, development and implementation will be for the organisation as a whole.
The organisation is regarded as a ‘green field’, able and willing both to accept and adapt to a completely changed information systems environment.
However, rarely can the political, social and other forces existing in the organisation countenance such change.
Such an approach also assumes that management would be willing to pay for a large project, accept the risks involved and get little reward in terms of information systems for a number of years.
Obviously this is not a realistic assumption.
further, the greater the inter-dependence, then the greater the complexity.
Some of this complexity might be seen as unexpected side-effects.
The systems planning team will have to ensure that rewards come during and not only after the full information systems project is implemented.
The project must therefore be divided into chunks, and data analysis, database and information systems implemented in each of these divisions in turn, ensuring the success of each of these parts before the  next subsystem is developed.
Whilst doing so, the systems planning team must not lose sight of the overall plan, so that the benefits of the database approach, especially data sharing, can be achieved, and thus there can be better use of the information resource, greater flexibility in its use and an improved ability to meet new situations and applications.
The approach described in this book needs to be adapted flexibly as no single approach will work alone and compromises must be made.
Earl (1989) researches into the reasons that UK companies undertake information systems strategy formulation:
Sector exploitation of IT is posing strategic threats and opportunities
The need to align IT investment with business needs is recognised
The desire to gain some competitive advantage from IT is apparent
The desire to change the appearance of systems development from data processing to IT activities is apparent.
These reasons may conflict and their emphasis change over time.
No single approach will work alone and compromises must be made.
Strategic development plans are best managed as portfolios, that is, during the development period early successes must be seen to be delivered, thus ensuring a continuous flow of systems products to maintain interest and commitment to the system from all users.
We will look at this aspect in the next section.
2.8 MANAGEMENT AND USER INVOLVEMENT
The previous section has brought to light some of the difficulties of carrying out a radical approach to the development of information systems in many organisations.
The reasons for failure in these projects tend not to lie in the technical side, though the technology is complex, nor in the economic side, though the cost of these systems is very high.
The reasons for failure are more likely to be due to people problems, which may show themselves by the lack of cooperation when the information system is being developed and a resistance to the changes that occur when the application is implemented.
These aspects have been touched upon throughout this chapter, but are now highlighted to emphasise their importance.
People may regard the change negatively.
They may think that their jobs will be less secure, that they might lose the independence that they previously enjoyed, that their relationships with others will change for the worse and they might lose status, they may think the change unnecessary and they may simply fear change in itself.
In reality, the changes may be positive for the staff, but they may be perceived as negative (there is also the possibility that change will be negative and resistance to change is therefore a  positive thing).
During times of economic recession, especially, employees fear job losses and it is important to stress genuine reasons for the development of the database project which are not associated with cost savings.
Employees fear the unknown and therefore early training, progress reports and user involvement in the change process will help.
Managers and supervisors, who might fear a loss of status and authority, need also to be informed early and be involved in the training of their staff.
They may feel particularly threatened, especially if they think that their power and responsibility may be given to the specialists involved directly with the technology.
One possible move to counteract this is to ensure that the system proposer is a business analyst and not a technical analyst
Unless steps have been taken to ensure that the people of the organisation are fully informed of the changes that are proposed, that they support the changes, and see themselves as gaining from them, then there is no reason to assume that they will cooperate with the changes: indeed, there is good reason to assume that they will try to ensure that the changes do not work.
Their experience of the ways in which change has been effected in the past will also contribute to their present reactions.
If the trust and confidence has been ruptured previously, it will be difficult to regain.
The management climate within the organisation and the way in which grievances are heard and fears discussed are important.
Organisational learning , where the ‘organisation’ stores experiences in forms, procedures and rules, and uses them to teach new staff and retrain others, can encourage adaptiveness to change.
Such practices need to be established so that change becomes the norm, is expected, and is viewed positively.
The best strategy to counteract resistance is to ensure rational, self interest, where attempts are made to convince individuals that change is to their personal advantage.
Such inducements as salary, status and job interest, could be used to ensure that staff might enjoy their new roles.
The work force should be informed of the likely changes in good time.
Rumours about impending changes will occur anyway, and staff not fully informed are likely to fear the worst.
Those that can get jobs will leave, and these are likely to be the staff that the organisation wants to keep most.
further, resistance can be caused by fears which are not based on fact.
Education, about what computers can and cannot do, will be more important than training in the early stages of the project.
We may be requiring a change in the culture of the organisation.
As we have seen, it is a powerful influence on the actions and activities of the people in the organisation and will be one of the main influences on the attitude to change held by staff.
A culture that promotes and encourages change will greatly influence the attitude to change held by staff.
But changing the corporate culture is a very long term solution (up to ten years) and can be a very difficult period for all involved.
It is important to communicate to the user not only the potential advantages of the information systems approach, of relevant, timely, accurate, understandable, and up-to-date, information which is provided to the correct level of detail, highlighting critical factors which control the firm's success (critical success factors are discussed in Rockart, 1979), but also of potential pitfalls that should be avoided.
More information is not necessarily better: it may be too detailed for ‘digestion’ or irrelevant to the particular decision-maker.
Decision-making will not necessarily improve just because the information is available: it may be ignored as many managers will prefer to keep to the combination of intuition, experience and judgement to make decisions.
There should be wide discussion within the organisation of these aspects so that management is aware of the pitfalls (Ackoff, 1967).
The database project is very expensive and difficult to justify in tangible terms.
The major product is information, not tangible objects like stocks of goods which are easier to evaluate.
What is the value of information?
There are obvious advantages of the information which could include better costing, better cash flow, improved customer relations, and it should lead to better decisions being made by management, but it is difficult to put a monetary value on all this.
Yet it is important to convince management of these gains, otherwise it will be extremely unlikely that the information systems projects will be given the go-ahead, and even if this is achieved, they are unlikely to be successful.
It is also important that top management are ‘seen’ to support the system, are committed to it and that they participate in the change.
They should benefit most from the system, as they are the information ‘requirers’.
This commitment will encourage others to fall in with the change.
A general air of good communications and user participation will also help.
These users will include the clerical workers who will be concerned with the input of data to the system and the verification of its output.
It is very important that user groups do contribute to the change and that their suggestions are not merely dealt with by paying lip service.
Again, user involvement should help to lessen the likelihood of a general level of mistrust and a lack of confidence.
Earl (1989) argues that it is necessary to identify three issues in the business plan to help ensure the success of IT:
Clarification of business needs and strategy in terms of IT requirements (which requires a methodological approach and teamwork)
Evaluation of current systems, their provision and use (which requires involvement of users)
Innovation of new strategic opportunities afforded by IT (which requires creative thinking and the identification of people who have this ability and others who will champion IT and have access to funds).
Business analysis should be looked upon as a two-way exercise — an opportunity to inform, help and convince, as much as an opportunity to find out about the organisation.
It is therefore an important part of the methodology.
It has helped in identifying the concerns of the business, its goals and its information requirements.
It has identified the people concerned with information systems development.
It has also helped in looking at the functions of the business in outline.
This is enough to help the systems planning team draw limits to the data modelling exercise.
lt has set a strategy for the next stages in the development of the information systems and database project and, through the encouragement of user involvement and management participation, a ‘style’ likely to lead to its successful completion.
Chapter 3
The Conceptual Schema: Data Analysis and the Relational Model
3.1 INTRODUCTION
Conventional systems analysis procedures were applied to single applications that were the first to be computerised in the organisation.
When applications being developed are an integrated part of a total system, these techniques prove inadequate.
The most obvious situation which requires a different approach is the development of a database.
In a database environment, many applications share the same data.
The database is looked upon as a common asset
Data analysis techniques were largely developed to cater for the implementation of database systems, although that does not mean that they cannot be applied to non-database situations.
Data modelling may be carted out as a step in conventional file applications.
Data analysis can also be of interest to management as a way of understanding their organisation.
The methodology represents a significant change in the development of computing systems away from the technology, in particular, hardware and software (including programming techniques and algorithms), and towards data and the way it is structured.
The emphasis on programming in earlier systems was natural: firstly because computers demanded considerable programming skill, and secondly because the professional was interested in the possibility of solving complex programming problems using the basic instruction set of the computer.
The emphasis has moved towards data because, if it can be made available in the correct form for applications, programming presents much less of a problem.
Data is a very important resource of the business.
The algorithms required for most data processing are relatively simple.
As computer systems become more powerful, less stress needs to be paid to problems of implementation (availability of storage, for example) and more to problems independent of implementation.
One of the most important techniques of data analysis described in the text is entity modelling.
Just as an accountant might use a financial model, the analyst can develop an entity model.
The entity model is just another view of the organisation, but it is a particular perception of reality and it can be used to address a number of problems.
The model produced provides an excellent and novel way of viewing the business.
Systems  analysis in general— and data analysis is a branch of systems analysis — is an art, not an exact science.
There can be a number of ways to derive a reasonable model and there are a number of reasonable models (there are of course an infinite number of inadequate models).
A model represents something, usually in simplified form which highlights aspects which are of particular interest to the user, and is built so that it can be used for a specific purpose, for example, communication and testing.
As we saw in Chapter 2, many types of model are used in information systems work.
A model is a representation of real-world objects and events, and good entity models will be a fair representation of certain aspects of the ‘real world’.
The entity model is an abstract representation of the data within the organisation.
It can be looked on as a discussion document and its coincidence with the real world is verified in discussions with the various users.
However, the analyst should be aware that variances between the model and a particular user's view could be due to the narrow perception of that user.
The model should be a global view.
The size of that ‘globe’— a department, a number of departments, a company or an organisation — having been agreed in the business analysis phase (Section 2.7).
An entity-relationship model views the organisation as a set of data elements, known as entities, which are the things of interest to the organisation, and relationships between these entities.
This model enables the computer specialist to design appropriate computer systems for the organisation, but it also provides management with a unique tool for perceiving the business process.
The essence of rational scientific problem solving is to be able to perceive the complex, ‘messy’, real world in such a manner that the solution to any problem may be easier.
This model is ‘simple’ in that it is fairly easy to understand and to use.
Each entity can be represented diagrammatically by soft boxes (rectangles with rounded corners).
Relationships between the entities are shown by lines between the soft boxes.
A first approach to an entity model for an academic department of computer science is given in Figure 3.1.
The entity types are STUDENT, ACADEMIC STAFF, COURSE and NON-ACADEMIC STAFF.
The entity type STUDENT participates in a relationship with ACADEMIC STAFF and COURSE.
The relationships are not named but it might be that STUDENT takes COURSE and that STUDENT has as tutor ACADEMIC STAFF.
The reader will soon detect a number of important things of interest that have been omitted (room, examination, research and so on).
As the analysts find out more about the organisation, entity types and relationships will be added to the model.
A mistake frequently made at this stage is to define the entities to reflect the processes of the business, such as stock control, credit control or sales order processing.
This could be a valid model of the business but it is not an entity model and  cannot be used to produce the flexible database for the organisation that we require.
A database so created would be satisfactory for some specific applications, but would not be adequate for many applications.
Where data analysis differs from conventional systems analysis is that it separates the data structures from the applications which use them.
The objective of data analysis is to produce a flexible model which can be easily adapted as the requirements of the users change.
Although the applications will need to be changed, this will not necessarily be true of the data.
Fig. 3.1 Entity modelling — a first approach
The entity-relationship model is a data model, referred to in this book as the conceptual schema or conceptual model.
The model reflects data in the business, not processes.
It is concerned with the data that exists, not how it is used.
The entities are quantifiable and can be represented as special tables, known as relations.
These were introduced in Section 1.4 and typical relations were seen in Figure 1.8.
The data items (attributes) associated with each entity (or relation) can then be normalised.
Normalisation is a set of rules applied to the relations which simplifies the model.
These relations can then be mapped onto a database.
The relational model and normalisation are explained in detail in Sections 3.6, 3.7 and 3.8.
3.2 ENTITY-RELATIONSHIP APPROACH
Probably the most widespread technique of data analysis is that proposed by Chen (1976).
The major advances described in Chen's paper were helped by the preceding work of the Codasyl Committee (see Section 4.6) and Codd (1970).
The information algebra proposed in the Codasyl report contained two important concepts: that of an  entity as a thing that has reality, and that of joining records on equal values of keys.
For modelling reality, it is essential to distinguish between different objects in the real world and understand how they are related to each other.
The problem with the original Codasyl proposals was that there is no clear distinction made between the conceptual or user view and the physical or computer view.
This distinction is most important because of the inherent flexibility that results from the separation of the logical and physical views.
Codd's relational model is not dependent on any specific physical implementation, it is data independent.
In Chen's entity-relationship (E-R) model, the real world information is represented by entities and by relationships between entities.
In a typical business, the entities could include jobs, customers, departments, and suppliers.
The analyst identifies the entities and relationships before being immersed in the detail, in particular the work of identifying the attributes which define the properties of entities.
Fig. 3.2 An entity set relating to part of a hospital
Figure 3.2 relates to part of a hospital.
The entities described are DOCTOR, PATIENT and CLINICAL SESSION.
The relationships between the entities are also described.
That between DOCTOR and PATIENT and between DOCTOR and CLINICAL SESSION are one-to-many relationships.
In other words, one DOCTOR can have many PATIENTS, but a PATIENT is only assigned one DOCTOR.
Further, a DOCTOR can be responsible for many CLINICAL SESSIONS, but a CLINICAL SESSION is the responsibility of only one DOCTOR.
The other relationship is many-to-many.
In other words, a PATIENT can attend a number of CLINICAL SESSIONS and one CLINICAL SESSION can be attended by a number of PATIENTS.
The diagram also shows a few attributes of the entities.
The particular attribute or group of attributes that uniquely identifies an entity occurrence is  known as the key attribute or attributes.
The ‘employee number’(Emp.
No.) is the key attribute of the entity called DOCTOR.
The technique attempts to separate the data structure from the functions for which the data may be used.
This separation is a useful distinction, although it is often difficult to make in practice.
In any case, it is sometimes useful to bear in mind the functions of the data analysed.
A DOCTOR and a PATIENT are both people, but it is their role, that is what they do, that distinguishes the entities.
The distinction, formed because of a knowledge of functions, is a useful one to make.
However, too much regard to functions will produce a model biased towards particular applications or users.
Another practical problem is that ‘global’(organisation-wide) data analysis may be so costly and time-consuming that it is often preferable to carry out entity analysis at a ‘local’ level, such as the marketing area.
If a local entity analysis is carried out, the model can be mapped on to a database and applications applied to it before another local data analysis is started.
This is farmore likely to gain management approval because managers can see the expensive exercise paying dividends in a reasonable time scale.
An important preliminary step is therefore to define the area for analysis and break this up into distinct sub-areas which can be implemented on a database and merged later.
Again, the arrangement most suitable to the organisation should have been determined in the business analysis stage.
Local data analysis should also be carried out in phases.
The first phase is an overview which leads to the identification of the major things of interest in that area.
At the end of the overview phase, it is possible to draw up a second interview plan and the next, longer, phase aims to fill in the detail.
Although it is relatively easy to illustrate the process of modelling in a book or at a lecture, in real life there are problems in deciding how far one should go and what level of detail is appropriate.
The level of detail must serve two purposes:
1
) It must be capable of explaining that part of the organisation defined for these purposes by business analysis, and
2
) It must be capable of being translated into the physical model.
It is important to realise that there is no logical or natural point at which the level of detail stops.
This is a pragmatic decision.
Certainly design teams can put too much effort into the development of the model.
Some of the levels of detail that are capable of expression using the method outlined are perhaps best left to implementation.
An example of this could be entity occurrences of persons who are female, where they relate to:
1)
Patients in a hospital
2)
Students at university, and
3)
Readers in a library.
In the patient example, the fact that the person occurrence is female is important, so important that the patient entity may be split into two separate entities — male patients and female patients.
In the student example, the fact that the person is female may not be of great significance and therefore there could be an attribute ‘sex’ of the person entity.
In the reader example, the fact that the person is female may be of such insignificance that it is not even included as an attribute.
There is a danger here, however, as the analyst must ensure that it will not be significant in any application in the library.
Otherwise the data model will not be as useful as possible.
This type of debate can only be resolved when looking at the functions in some detail.
3.3 SOME DEFINITIONS
An entity is a thing of interest in the organisation, in other words it is anything about which we want to hold data.
It could include all the resources of the business, including the people of interest such as EMPLOYEE, and it can be extended to cover such things as a SALES-ORDER, INVOICE and PROFIT-CENTRE.
It covers concepts as well as objects.
A SCHEDULE or a PLAN are concepts which can be defined as entities.
An entity is not data itself, but something about which data should be kept.
It is something that can have an independent existence, that is, can be distinctly identified.
ln creating an entity model, the aim should be to define entities that enable one to describe the organisation.
Such entities as STOCK, SALES-ORDER and CUSTOMER are appropriate because they are quantifiable, whereas ‘stock control’, ‘order processing’ and ‘credit control’are not appropriate because they are functions: what the organisation does, and not things of interest which participate in functions.
Entities will normally be displayed in small capitals.
Entities can also be quantified — it is reasonable to ask ‘how many customers?’ or ‘how many orders per day?’, but not ‘how many credit controls?’.
An entity occurrence is a particular example of an entity which can be uniquely identified.
It will have a value, for example, ‘John Smith & Son’ and this will be a particular occurrence of the entity CUSTOMER.
There will be other occurrences, such as, ‘Plowmans PLC’ and ‘Tebbetts & Co’.
An attribute is a descriptive value associated with an entity.
It is a property of an entity.
At a certain stage in the analysis it becomes necessary not only to define each entity but also to record the relevant attributes of each entity.
A CUSTOMER entity may be defined and it will have a number of attributes associated with it, such as‘number’, ‘name’, ‘address’, ‘credit-limit’, ‘balance’ and so on .
Attributes will normally be displayed within single inverted commas.
The values of a set of attributes will distinguish one entity occurrence from another.
Attributes are frequently identified  during data analysis when entities are being identified, but most come later, particularly in detailed interviews with staff and in the analysis of documents.
Many are discovered when checking the entity model with users.
An entity may be uniquely identified by one or more of its attributes, the key attribute(s).
A customer number may identify an occurrence of the entity CUSTOMER.
A customer number and a product number may together form the key of entity SALES-ORDER.
The key attribute functionally determines other attributes, because once we know the customer number we know the name, address and other attributes of that customer.
key attributes will normally be underlined.
There often arises the problem of distinguishing between an entity and an attribute.
In many cases, things that can be defined as entities could also be defined as attributes, and vice versa.
We have discussed one example relating to the sex of people.
The entity should have importance in itself, otherwise it is an attribute.
In practice the problem is not as important as it may seem.
Most of these ambiguities are settled in the process of normalisation (Sections 3.7 and 3.8).
In any case, the analyst can change his model at a later stage, even when mapping the model onto a database.
Entities participate in functions of the organisation and the attributes are those data elements that are required to support the functions.
The best rule of thumb is to ask whether the data element has information about it, in other words does it have attributes?
Entities and attributes are further distinguished by their role in events (discussed below).
A relationship in an entity model normally represents an association between two entities.
A SUPPLIER entity has a relationship with the PRODUCT entity through the relationship supplies, that is, a SUPPLIER supplies PRODUCT.
There may be more than one relationship between two entities, for example, in this case because a PRODUCT is supplied by SUPPLIER.
Relationships will normally be displayed in italics.
The next stage in the development of an entity model, therefore, having defined the entities and ‘fleshed’ out the entities with attributes, is to associate related entities by relationships and thus put edges into the model.
A relationship normally arises because of:
1)
Association, for example‘CUSTOMER places ORDER’
2)
Structure, for example, ORDER consists of ORDER-LINE'.
The association between entities has to be meaningful, the relationship normally his an information content — CUSTOMER places ORDER.
The action places describes the relationship between CUSTOMER and ORDER.
The name given to the relationship also helps to make the model readable.
As will be seen, the relationship itself can have attributes.
The cardinality of the relationship could be one-to-one, one-to-many, or many-to-many.
A MEMBER-OF-PARLIAMENT can only represent one constituency, and one   CONSTITUENCY can have only one MEMBER-OF-PARLIAMENT.
A MEMBER-OF-PARLIAMENT represents a CONSTITUENCY.
This is an example of a one-to-one (1:1) relationship.
figure 3.3 shows various ways of representing relationships.
Very often, a one-to-one relationship can be better expressed as a single entity, with one of the entities forming attributes of the more significant entity.
For example, the entity above could be MEMBER-OF-PARLIAMENT, with CONSTITUENCY as one of the attributes.
The relationship between an entity CUSTOMER and another entity ORDER is usually of a degree one-to-many (1:m).
Each CUSTOMER can have a number of outstanding ORDERS, but an ORDER can refer to only one CUSTOMER: CUSTOMER places ORDER.
With a many-to-many (m:n) relationship, each entity can be related to one or more occurrences of the partner entity.
A SUPPLIER can supply many PRODUCTS; and one PRODUCT could be supplied by a number of SUPPLIERS (SUPPLIER supplies PRODUCT; PRODUCT is supplied by SUPPLIER).
In this last example (of a many-to-many relationship), entity occurrences of the SUPPLIER entity could be ‘Smith’, ‘Jones’ and ‘Wilson’, and they could supply a number of products each.
for example, Smith might supply wigits, ligits and sprigits; Jones might supply wigits and sprigits; and Wilson ligits and sprigits.
This can be shown in the spaghetti-like manner of Figure 3.4.
Fig. 3.4 Many-to-many relationship between SUPPLIER and PRODUCT
Frequently there is useful information associated with many-to-many relationships and it is better to split these into two one-to-many relationships, with a third entity created to link these together.
Again, this should only be done if the new entity has some meaning in itself.
The relationship between COURSE and LECTURER is many-to-many, that is, one LECTURER lectures on many COURSES and a COURSE is given by  many LECTURERS.
But a new entity, MODULE can be described which may only be given by one LECTURER and is part of only one COURSE.
Thus a LECTURER gives a number of MODULES and a COURSE consists of a number of MODULES.
But one MODULE is given by only one LECTURER and one MODULE is offered to only one COURSE (if these are the restrictions).
This is shown in figure 3.5.
Fig. 3.5 A many-to-many relationship represented as two one-to-many relationships
There are other distinctions and sophistications which are often included in the model.
Sometimes a 1:m or an m:n relationship is a fixed relationship.
The many-to-many relationship between the entity PARENT and the entity CHILD is 2:n (that is, each child has two parents); but a PARENT can beget more than one CHILD.
Whilst some relationships are manadatory , others are optional.
An entity MALE and an entity FEMALE may be joined together by the optional relationship married to.
Mandatory and optional relationships may be represented as shown in Figure 3.6.
Other structures include exclusivity , where participation in one relationship excludes participation in another or inclusivity , where participation in one relationship automatically includes participation in another.
Fig. 3.6 Representation of mandatory and optional relationships
A data structure may also be involuted where entity occurences relate to other occurrences of the same entity.
For an EMPLOYEE entity, for example, an EMPLOYEE entity occurrence who happens to be a manager manages other occurrences of the entity EMPLOYEE.
This can be shown diagrammatically an involuted loop, as in Figure 3.7.
Fig. 3.7 An involuted relationship
Any relationship is necessarily linked to at least one entity.
If it is linked to only one entity it is involuted, and such a relationship is also said to be reflexive.
Where a relationship is linked to two entities (as in the case of the examples in figure 3.3), it is said to be binary.
If a relationship is linked to three entities, as in figure 3.8, it is said to be ternary.
Otherwise it is n-ary , with the value of ‘n’ equalling the number of entities.
Fig 3.8 Example of a ternary relationship
Howe (1989) and Robinson (1989) discuss these variants and others, and their diagrammatic representation.
Readers should note that the terminology is not always consistent in the literature.
Howe, for example, in his excellent and thorough description of data analysis, uses the term ‘degree’ for what is referred to as ‘cardinality’in this text.
Up to now, we have considered the data-oriented aspects of data analysis, but functional considerations are made in order to check the model.
These are events and operations.
Entities have to support the events that occur in the enterprise.
Entities will take part in events and in the operations that follow events.
Attributes are those elements which supply data to support events.
‘Tom’ is an occurence of the entity EMPLOYEE.
Tom's pay rise or his leaving the company are events, and attributes of the entity EMPLOYEE will be referred to following these events.
Attributes such as‘pay-to-date’, ‘tax-to-date’, ‘employment status’, and ‘salary’ will be referred to.
Operations on attributes will be necessary following the event: an event triggers an operation or a series of operations.
An operation will change the state of the data.
The event Tom gets salary increase of 10% will require access to the entity occurrence ‘Tom’ and augmenting the attribute ‘salary’by 10%.
Figure 3.9 shows the entity EMPLOYEE expressed as a relation with attributes.
We have to check that the relation supports all the operations that follow the event mentioned.
Fig. 3.9 Event driven (functional) analysis
Some methodologies, for example, Merise (Quang & Chartier-Kastler, 1991) also define the synchronisation of an operation (Figure 3.10).
This is a condition affecting the events which trigger the operation and will enable the triggering of that operation.
This condition can relate to the value of the properties carried by the events and to the number of occurrences of the events.
For example, the operation ‘production of pay slips’ may be triggered by the event ‘date’when it equals ‘28th of the month’.
Some readers may be confused by the discussion of events (sometimes called transactions ), which function-oriented concepts, when data analysis is supposed to be function-independent.
The consideration of events and operations is of interest as a checking mechanism.
They are used to ensure that the entity model will support the functions that may use the data model.
This consideration of the events and operations may lead to a tuning of the model, an adjustment of the entitles and the attributes.
It will also be useful information for later stages of the methodology.
This event-driven analysis is frequently called functional analysis.
Another possible source of confusion is the similarity of the terms ‘relations’ and  ‘relationships’.
Whereas relationships express the association between two entities, relations are a tabular representation of an entity, complete with attributes.
Although the two names are similar, they represent two distinct concepts.
Fig. 3.10 Synchronisation of an operation
3.4 PHASES IN ENTITY ANALYSIS
We will now look at the stages of entity analysis which are:
Define the area for analysis
Define the entities and the relationships between them
Establish the key attribute(s) for each entity
Complete each entity with all the attributes
Normalise all the entitles to third normal form
Ensure all events and operations are supported by the model.
We have looked briefly at all these elements apart from normalisation which is discussed in Section 3.7.
The first stage of entity analysis requires the definition of the area for analysis.
This was discussed in Chapter 2 and is frequently referred to as the universe of discourse.
Sometimes this will be the organisation, but this is usually too ambitious for detailed study, and as we have seen the organisation will normally be divided into local areas for separate analysis.
Then we have the stages of entity-relationship modelling.
It is a top-down approach in that the entities are identified first, followed by the relationships between them, and then more detail is filled in as the attributes and key attribute(s) of each entity are identified.
For each local area, then, the entities are defined.
The obvious and major entities will be identified first.
The analyst will attempt to name the fundamental things of  interest to the organisation.
As the analyst is gathering these entities, the relationships between the entities can also be determined and named — sometimes they will be named in both directions.
Their cardinality can be one-to-one, one-to-many or many-to-many.
It may be possible to identify fixed relationships and those which are optional or mandatory.
The analyst will be able to begin to assemble the entity-relationship diagram.
The diagram will be rather sketchy, somewhat like a ‘doodle’, in the beginning, but it will soon be useful as a communication tool.
There are now computer software tools which can help draw up these diagrams and make alterations easily.
These are described in Chapter 11.
The key of each entity will also be determined.
The key attributes will uniquely identify any entity occurrence.
There may be alternative keys, in which case the most natural or concise is normally chosen.
The analyst has now constructed the model in outline and is in a position to fill in the detail.
This means establishing the attributes for each entity.
Each attribute will say something about the entity.
The analyst has to ensure that any synonyms and homonyms are detected.
A product could be called a part, product or finished product depending on the department.
These are all synonyms for ‘product’.
On the other hand, the term product may mean different things (homonyms), depending on the department.
It could mean a final saleable item in the marketing department or a sub-assembly in the production department.
These differences must be reconciled and recorded in the data dictionary (see Chapter 12).
The process of identifying attributes may itself reveal entities that have not been identified.
Any data element in the organisation must be defined as an entity, an attribute or a relationship and recorded in the data dictionary.
Entities and relationships will also be recorded in the entity-relationship diagram.
Each entity must be normalised to third normal form once the entity occurrences have been added to the model.
This process is described in Section 3.7.
Briefly, the rules of normalisation require that all entries in the entity must be completed (first normal form), all attributes of the entity must be dependent on all the key (second normal form), and all non-key attributes must be independent of one another(third normal form).
The normalisation process may well lead to an increase in the number of entities in the model.
This process puts the data model in a form more suitable for the next stage of the methodology.
The final stage of entity analysis will be to look at all the events within the area and the operations that need to be performed following an event, and ensure that the model supports these events and operations.
Events are frequently referred to as transactions.
For this part of the methodology, the analyst will identify the events associated with the organisation and examine the operations necessary on the trail of each of the events.
Events in many organisations could include ‘customer makes an order and ‘raw materials are purchased from supplier’ and ‘employee joins firm’.
If, say, a customer  makes an order, this event will be followed by a number of operations.
The operations will be carried out so that it is possible to find out how much the order will cost, whether the product is in stock, and whether the customer's credit limit is OK.
The entities such as PRODUCT (to look at the value of the attribute ‘stock’) and CUSTOMER (to look at the value of the attribute ‘credit limit’) must be examined (see figure 3.11).
These attribute values will need to be adjusted following the event.
You may notice that the ‘product price’ is not in either entity.
To support the event, therefore, ‘product price’ should be included in the PRODUCT entity, or in another entity which is brought into the model.
Fig. 3.11 Checking the entity model
Entity modelling has documentation aids like other methods of systems analysis.
It is possible to obtain forms on which to specify all the elements of the data analysis process.
The separate documents will enable the specification of entities, attributes, relationships, events and Operations.
These are shown as Figures 3.12, 3.13, 3.14, 3.15 and 3.16.
Some completed forms, of a slightly different design, are shown in Chapter 14.
These forms can be pre-drawn using software tools and their contents automatically added to the data dictionary.
As we have already stated, it may be possible to use completed documents directly as input to a data dictionary system so that the data is held in a readily-accessible computer format as well as on paper forms.
A description of data dictionary systems is given in Chapter 12.
Entity modelling can be used as an aid to communication as well as a technique for finding out information.
The forms discussed help as an aid to memory, that is, communication with oneself.
The entity-relationship diagrams, which are particularly useful in the initial analysis and as an overview of the data model, prove a good basis  for communication with managers and users.
They are much more understandable to non-computer people than the documents used in conventional data processing, although they are also a good communications aid between computer people.
They provide a pictorial description of the business in outline, showing what the business is, not what it does.
Managers and users can give ‘user feedback’ to the data analysts and this will also help to tune the model and ensure its accuracy.
A user may point out that an attribute is missing from an entity, or that a relationship between entities is one-to-many and not one-to-one as implied by the entity-relationship diagram.
The manager may not use this terminology, but the data analyst will be able to interpret the comments  made.
Data analysis is an iterative process: the final model will not be obtained until after a number of tries and this should not be seen as slowness, but care for accuracy.
If the entity model is inaccurate so will be the database and the applications that use it.
On the other hand, the process should not be too long or ‘diminishing returns’ will set in.
Fig. 3.12 Entity documentation
Fig. 3.13 Attribute documentation
Fig. 3.14 Relationship documentation
Fig. 3.15 Event documentation
The entity-relationship diagram given in Figure 3.17 shows the entities for part of a firm of wholesalers.
Included in the figure are the attributes of the entities.
The key attributes are underlined.
Perhaps you would like to verify that you can understand something of the organisation using this form of documentation.
It is a first sketch of the business, and you may also verify the relationships, add entities and relationships to  the model or attributes to the entities, so that the model is more appropriate for a typical firm of wholesalers. for example, I have not included payments in this interim model.
Fig. 3.16 Operation documentation
A more realistic example for an academic department is shown as figure 3.18 (discussed fully in Chapter 15).
Fig. 3.17 Entity-relationship diagram — a first approach for a wholesaler
3.5 ALTERNATIVE APPROACHES TO DATA ANALYSIS
The entity-relationship approach to data analysis is interview-driven , that is most information is obtained through interviewing members of staff.
It is also top-down , in that the entities are identified first and then more and more detail filled in, as where the attributes of the entities are identified.
An alternative approach is document-driven , where most of the information comes from a study of documents.
These documents could be the input forms used in the system and output reports, both hard copy and soft copy designs.
In order to make the model flexible, the investigation will include reports likely to be required in the future.
Though this approach includes a formal technique to derive the model, other information gathered in the investigation phase can be incorporated in the final model so derived.
The methodology has the following steps:
1)
Identify the documents that most typify the area under investigation.
Each of these is processed in turn, starting with the most significant.
These are usually the ones  that carry most data.
2)
Identify the data elements on these documents and attach names to each of these.
Ensure that synonyms and homonyms are recognised.
This may will necessitate  interviewing people in the department concerned.
Construct and develop the data
3)
Draw data usage diagrams for each of these documents.
These diagrams highlight the relationships that the documents reflect, in particular any hierarchical structure that the document shows.
4)
Separate each level of the diagram as a relation with attributes.
find the identifying (or key) attributes.
Modify the relations so that they conform to the rules of normalisation.
5)
Combine the relations obtained in the present document with the set obtained from previous documents.
This may imply adding new relations to the set, adding data elements (attributes) to one or more of the relations, or breaking up one relation into two or more relations.
As more and more documents are processed, the less likely will subsequent documents effect a change in the overall model.
Fig. 3.18 An entity model for an academic department (from Avison & Wood-Harper, 1990)
The methodology is best illustrated by an example which is taken from Avison (1981).
figure 3.19 shows an online enquiry and its associated data usage diagram (DUD).
The first level of the DUD relates to jobs and the second level to orders for parts, because for each occurrence of the job data, there can be many occurrences of order data.
The key of the second level consists of the key of the first level (job-number) plus the identifier of the second (part number).
Therefore the relations JOB and ORDER are as follows (with key attributes underlined):
JOB (Job-number , sales-area, representative-number)
ORDER (Job-number,part number , part-description, quantity-ordered)
Figure 3.20 gives another report to be analysed and its DUD.
This shows that for each sales area there may be a number of representatives.
The JOB relation will have to be modified and a SALES relation formed.
JOB (Job-number , sales-area)
SALES (Sales-area ,representative-number , sales amount)
The process is continued, developing and tuning the set of relations, until all the documents have been processed.
In practice, it is rarely possible to thoroughly analyse all the documents of the organisation in this way.
The process is too slow.
The first few documents will be used to create the initial model.
Later documents will be perused rather than analysed thoroughly in the hope that any new information is revealed in the process.
The major criticism of this approach is that it assumes that all information about the organisation is kept in documents.
For this reason it is best to consider the approach document ‘driven’.
In other words, whilst analysing documents, other information, such as that gained from interviews, can be incorporated into the model.
A knowledge of the firm could soon reveal, for example, that the ‘part-description’ in the  ORDER relation is dependent on the ‘part-number’, not the whole key.
This does not conform to the rules of normalisation (attributes must give facts about all the key).
There should therefore be another relation formed relating to PART, and the ORDER  relation modified.
PART (Part-number , part-description)
ORDER (Job-number ,part-number , quantity-ordered)
Rather than use the document-driven approach as the main approach to data analysis, I use this method to verify the model obtained from the entity-relationship approach.
A further approach, also a data collection approach, works by collecting the facts of interest to the enterprise.
The attributes are then combined into normalised relations.
Like the Data Collection Approach, but unlike the entity-relationship approach, it is a bottom-up procedure.
As we have seen, the entity-relationship approach looks at the major things of interest first.
This is a top-down technique.
Although bottom-up approaches work for simple cases, there usually proves to be too large a number of attributes to analyse in more realistic situations.
3.6 RELATIONAL MODELLING
In 1970 Ted Codd published his influential paper ‘A Relational Model for Large Shared Data Banks’.
This has had a profound effect on systems analysis and database design.
Most of the early work in data processing had been done pragmatically with developments taking place in the data processing applications environment rather than in the research and academic environments.
Little or no formal techniques were being used or developed: problems were solved by rule of thumb and guess-work.
However, by the 1970s the early pioneering days of computer data processing were over and more sophisticated integrated systems were being developed.
Large integrated files were needed.
Customer files, for example, were being developed which included data relevant to more than one system.
These could include sales order, sales ledger and bad debt processing.
Later came databases.
Codd's research was directed at a very real problem, that is the deletion, insertion and update of data on these very large files.
Being a mathematician, his approach to the problem was mathematical.
In describing the work in this text, however, it is intended to avoid a formal mathematical treatment.
The relational model is stressed in this text for a number of reasons:
The model is understandable by users as well as technologists
The model is transferable into other data models
The model is not computer-oriented
The model is not biased towards particular user requirements
The model is derived by the processes of data analysis 
Different users can see different views of the model.
It is a model which is understandable by the technologists, and also, more importantly, by managers and users.
They can far more readily appreciate the meaning of the data relationships illustrated in a tabular form than they could in the file and record specification forms of conventional systems analysis.
These are computer-oriented.
I have found more difficulty teaching the relational model to computer students (who want to think in terms of computer jargon and are looking for complications that do not exist) than to management students.
It is not necessary to know anything of computers or computer file structures to understand the relational model and to use it.
Everyone has used tables, and relations are tables.
Relations are transformable into other models, such as hierarchies or networks, which are other ways in which database management systems can view data structures.
Increasingly, however, relational database management systems are being used so that they are now by far the most common database management systems.
These systems expect data to be presented to them in terms of relations and normally present results to users in the form of relations.
The relational model is not computer-oriented.
It is not biased by any particular physical storage structure that may be used.
From a user point of view, the model stays the same whether the storage structures are held on magnetic tape, disk or main storage.
It is not biased towards the way that the data may be accessed from storage media.
If the target system is a computer system, it could be mainframe, minicomputer or microcomputer.
It does not show bias towards particular users , either particular user enquiries, enquiry types, or the enquiries from a particular (perhaps powerful) user.
Many systems make some questions easier to ask and answer, particularly where the structure of these matches the design of the database.
This is not true of the relational model itself.
The database administrator may choose to optimise the performance of some query type which is posed frequently, but these considerations are not made when building the data model.
The relational model can be derived from data analysis.
Most data analysis techniques derive their data model is a set of entities which can be represented as relations.
It maintains the truths of the universe it represents.
Thus it can represent that part of the real world that was modelled in the data analysis process.
It can also be adapted to reflect changes in the organisation and ‘readily adapted’, if the relations have been normalised (because normalisation leads to a much more flexible model).
Although the model represents the organisation as a whole, it can be adapted so that a particular part of the model can appear in a different form to different users.
There will be one overall model, the conceptual schema , but in the logical model, the  various user views can be represented as sub-schemas (particular user or external views).
Such flexibility allows users to see only that part of the data model that interests them and avoid an otherwise over-complicated view of the model, and supports other requirements such as privacy and security.
Fig. 3.21 The relation ‘SALES-ORDER'
As seen in Figure 3.21, a relation is a flat file.
This relation is called SALES-ORDER and it could show that Lee ordered 12 of ‘part number’ 25, Deene and Smith ordered 18 and 9 respectively of ‘part number’38, and Williams ordered 100 of ‘part number’87.
The entities and relationships identified in the entity-relationship model can both be represented as relations in the relational model.
The many-to-many relationship between COURSE and LECTURER in figure 3.22 can be represented as three relations: COURSE, LECTURER and TIMETABLE.
Here, COURSE and LECTURER come from the original entities and TIMETABLE stems from information about the coincidence of the two, that is, their relationship.
The key of the TIMETABLE relation is taken from the key of the COURSE relation (course-number) and that of the LECTURER relation (lecturer-number).
The processes involved in mapping these to relations are described more fully in Section 4.8.
We will now introduce some of the terminology associated with the relational model.
Each row in a relation is called a tuple.
The order of tuples is immaterial, although they will normally be shown in the text in a logical sequence so that it is easier to follow their contents.
No two tuples can be identical in the model.
A tuple will have a number of attributes , and in the SALES-ORDER relation of Figure 3.21, ‘name’, ‘part’ and ‘quantity’are attributes.
All items in a column come from the same domain — there are circumstances where the contents from two or more columns come from the same domain.
The relation ELECTION-RESULT (Figure 3.23) illustrates this possibility.
Two attributes come from the same domain of Political Parties.
The number of attributes in a relation is called the degree of the relation.
A relation with two attributes is known as a binary relation.
The number of tuples in a relation define its cardinality.
Each tuple in a relation is distinguished from another because one or more attributes in a relation are designated key attributes.
In the SALES-ORDER relation, the key is  ‘name’.
It might be better to allocate numbers to customers in case there are duplicate names.
If the customer may make orders for a number of parts, then ‘part’ must also be a key attribute as there will be several tuples with the same ‘Name’.
ln this case, the attributes ‘name’ and part' will make up the composite key of the ORDER relation.
What is the key for the ELECTION-RESULT relation?
On first sight, ‘election year, might seem appropriate, but in 1974 there were two elections in a year, and even if all three attributes were part of the key, there are still duplicate relations.
It is necessary to add another attribute or replace ‘year’ by ‘election date’to make each tuple unique (there will not be two elections of the same type on the same day).
Alteratively the composite key ‘election year’ and a new attribute ‘election number in year’will be adequate.
There may be more than one possible key.
These are known as candidate keys.
‘Customer name’ and ‘customer number’could be candidate keys in a CUSTOMER relation.
In this circumstance one of these is chosen as the primary key.
An attribute which is a primary key in one relation and not a primary key but included in another relation is called a foreign key in that second relation.
The structure of a relation is conventionally expressed as in the following examples:
SALES-ORDER (name, part quantity) and
ELECTION-RESULT (elect-year, elect-number, first-party, second-party)
Fig. 3.23 Relation ‘ELECTION RESULT'
3.7 NORMALISATION
The process of normalisation is the application of a number of rules to the relational model which will simplify the relations.
This set of rules proves to be a useful guideline because we are dealing with large sets of data and the relations formed by the normalisation process will make the data easier to understand and manipulate.
The model so formed is appropriate for the further stages in the methodology and the database will be shareable, a fundamental justification for the database approach.
On the other hand, normalisation can lead to an increase in the number of relations and to data duplication so that relations can be joined on the primary key and foreign key.
Codd developed three nested levels of normalisation, and the third and final stage is known as third normal form (TNF).
It is this level of normalisation that is usually used as the basis for the design of the data model, as an end result of data analysis, and for mapping onto a database (the logical schema).
There are a few instances, however, when even TNF needs further simplification, and these are looked at in Section 3.8 which may be skipped on first reading.
TNF is usually satisfactory in practice.
Normalisation is the process of transforming data into well formed or natural groupings such that one fact is in one place and the correct relationships between facts exist.
The relations formed by the normalisation process will make the data easier to understand and manipulate.
As well as simplifying the relations, normalisation also reduces anomalies which may otherwise occur when manipulating the relations in a  relational database.
Normalised data is stable and a good foundation for any future growth.
It is a mechanical process, indeed the technique has been automated (see Chapter 11), but the difficult part of it lies in understanding the meaning, that is, the semantics of the data, and this is only discovered by extensive and careful data analysis.
Automation of the normalisation process still requires this information to be supplied to the particular tool.
There are three stages of normalisation:
First normal form
Ensure that all the attributes are atomic (that is, in the smallest possible components).
This means that there is only one possible value for each domain and not a set of values.
This is often expressed as the fact that relations must not contain repeating groups.
Second normal form
Ensure that all non-key attributes are functionally dependent on (give facts about) all of the key.
If this is not the case, split off into a separate relation those attributes that are dependent on only part of the key.
Third normal form
Ensure that all non-key attributes are functionally independent of each other.
If this is not the case, create new relations which do not show any non-key dependence.
A rather flippant, but more memorable, definition of normalisation can be given as ‘the attributes in a relation must depend on the key, the whole key, and nothing but the key’.
This is an oversimplification, but it is essentially true and could be kept in mind as normalisation is developed.
A more detailed description of normalisation is now given.
A key concept is functional dependency , which is often referred to as determinacy.
This is defined by Cardenas (1985) as follows:
‘Given a relation R, the attribute B is said to be functionally dependent on attribute A if at every instant of time each value of A has no more than one value of B associated with it in the relation R.’
Thus, if we know a ‘customer-number’, we know the associated ‘customer-name’ and ‘customer-address’if they are functionally dependent on ‘customer-number’.
Functional dependency is frequently illustrated by an arrow.
The arrow will point from A to B in the functional dependency illustrated in the definition.
Thus, the value of A uniquely determines the value of B.
Before normalising the first relation given as figure 3.24, it is necessary to analyse  the meaning of the relation.
Knowledge of the application area gained from data analysis will provide this information.
It is possible to make assumptions about the inter-relationships between the data, but it is obviously better to base these assumptions on thorough analysis.
In the relation COURSE-DETAIL, there are two occurrences of ‘course’, one numbered B74 called computer science at the BSc.
level and the other B94 called computer applications at the MSc.
level.
Each of these course occurrences has a number of module occurrences associated with it.
Each ‘module’ is given a ‘name’, ‘status’and ‘unit-points’(which are allocated according to the status of the ‘module’).
Fig. 3.24 First normal form
First normal form:
The first stage of normalisation includes the filling in of details.
This is seen in the example in Figure 3.24(a) and is a trivial task.
You may note that in Figure 3.24(a), the order of the tuples in the unnormalised relation is significant.
Otherwise the content of the attributes not completed cannot be known.
As we have already stated, one of the principles of the relational model is that the order of the tuples should not be significant.
The relation seen in figure 3.24(b) could be in any order.
First normal form really converts unnormalised data or traditional file structures into  relations or tables.
The key of the relation of Figure 3.24(b) is ‘course-number’ and ‘module-number’together (a composite key) and the key attributes have been underlined.
A composite key is necessary because no single attribute will uniquely identify a tuple of this relation.
There were in fact a number of possible candidate keys, for example, ‘module-name’ and ‘course-name’, but we chose the primary key as above.
Further work would have been necessary if the following was presented as the unnormalised relation:
COURSE, COURSE-NAME, LEVEL, MODULE-DETAILS
‘Module-details’ has to be defined as a set of atomic attributes, not as a group item, thus it has to be broken down into its constituents of ‘module-name’, ‘status’and ‘unit-points’:
COURSE, COURSE-NAME, LEVEL, MODULE-NAME, STATUS, UNIT-POINTS will be the result in first normal form, with all the details filled out.
Second normal form:
This is achieved if the relations are in first normal form and all non-key attributes are fully functionally dependent on all the key.
The relation course-detail shown in Figure 3.24(b) is in first normal form.
However, the attributes ‘status’, ‘name’ and ‘unit-points’are functionally dependent on ‘module’.
In other words, they represent facts about ‘module’, which is only part of the key (known as partial dependency).
We may say that if the value of module is known, we can determine the value of ‘status’, ‘name’, and ‘unit-points’.
for example, if module is B743, then ‘status’ is basic, ‘name’is data proc 1, and ‘unit points’is 8.
They are not dependent on the other part of the key, ‘course’.
So as to comply with the requirements of second normal form, two relations will be formed from the relation and this is shown as figure 3.25.
But the relation course-module is still not in second normal form because the attributes ‘course-name’ and ‘level’are functionally dependent on ‘course’only, and not on the whole of the key.
A separate COURSE relation has been created in figure 3.26.
The course relation has only two tuples (there are only two courses), and all duplicates are removed.
Notice that we maintain the relation COURSE-MODULE.
This relation is all key, and there is nothing incorrect in this.
Attributes may possibly be added later which relate specifically to the course-module relationship.
The relation is required because information will be lost by not including it, that is, the modules which are included in a particular course and the courses which include specific modules.
The relations are now in second normal form.
Third normal form (TNF):
Second normal form may cause problems where non-key  attributes are functionally dependent on each other(a non-key attribute is dependent on another key attribute).
This is resolved by converting the relations into TNF.
In the relation module, the attribute ‘unit-points’ is functionally dependent on the ‘status’(or level) of the course, that is, given ‘status’, we know the value of ‘unit-points’.
So ‘unit-points’ is determined by ‘status’which is not a key.
We therefore create a new relation STATUS and delete ‘unit-points’ from the relation MODULE.
The third normal form is given in Figure 3.27.
Fig. 3.25 Towards second normal form: course-module needs further normalisation.
Sometimes the term transitive dependency is used in this context.
The dependency of the attribute ‘unit-points’ is transitive (via ‘status’) and not wholly dependent on the key attribute ‘module’.
This transitive dependency should not exist in third normal form.
The attribute ‘status’ is the primary key of the STATUS relation.
It is included as an attribute in the MODULE relation, but it is not a key.
This provides an example of a foreign key, that is, a non-key attribute of one relation which is a primary key of another.
This will be useful when processing the relations as ‘status’ can be used to  join the STATUS and MODULE relations to form a larger composite relation if this is required by the user.
But we are now drifting to the next stages of the methodology.
Fig. 3.26 Second normal form
Codd developed three nested levels of normalisation, and this third and final stage is known as TNF.
It is this level of normalisation that is usually used as the basis for the design of the data model, as an end result on data analysis.
As we shall see in Section 3.8, there are a few instances when even TNF needs further simplification, and Kent (1983) and Date (1990) describe these extensions.
TNF is usually satisfactory, however, and readers may avoid further extensions if they wish, that is, skip the next section.
Fig. 3.27 Third normal form
Relations are normalised because unnormalised relations prove difficult to use.
This  can be illustrated if we try to insert, delete, and update information from the relations not in TNF.
Say we have a new ‘module’ numbered B985 called Artificial Intelligence and which has a ‘status’in the intermediate category.
Looking at Figure 3.24, we cannot add this information in COURSE-DETAIL because there had been no allocation of this ‘module’ occurrence to any ‘course’.
Looking at Figure 3.25(b), it could be added to the MODULE relation, if we knew that the ‘status’ intermediate carried 11 unit-points.
This information is not necessary in the MODULE relation seen in Figure 3.27(c), the TNF version of this relation.
The TNF model is therefore much more convenient for adding this new information.
If we decided to introduce a new category in the ‘status’ attribute, called coursework, having a ‘unit-points’attached of 10, we cannot add it to the relation MODULE (Figure 3.25(b)) because we have not decided which ‘module’or modules to attach it to.
But we can include this information in the TNF model by adding a tuple to the STATUS relation (figure 3.27(d)).
Another problem occurs when updating.
Let us say that we decide to change the ‘unit-points’ allocated to the Basic category of ‘status’in the modules from 8 to 6, it becomes a simpler matter in the TNF module.
The single occurence of the tuple with the key Basic, needs to be changed from (Basic 8) to (Basic 6) in Figure 3.27(d).
With the unnormalised, first normal or second normal form relations, there will be a number of tuples to change.
It means searching through every tuple of the relation COURSE-DETAIL (figure 3.24(b)) or module (Figure 3.25(b)) looking for ‘status’ = Basic and updating the associated ‘unit-points’.
All tuples have to be searched, because in the relational model the order of the tuples is of no significance.
This increases the likelihood of inconsistencies and errors in the database.
We have ordered them in the text to make the normalisation process easier to follow.
Deleting information will also cause problems.
If it is decided to drop the B74 course, we may still wish to keep details of the modules which make up the course.
Information about modules might be used at another time when designing another course.
The information would be lost if we deleted the course B74 from COURSE-DETAIL (Figure 3.24(b)).
The information about these modules will be retained in the module relation in TNF.
The TNF relation COURSE will now consist only of one tuple relating to the ‘course’ B74 and the TNF relation COURSE-MODULE will consist of the THREE tuples relating to the COURSE B94.
3.8 FURTHER NORMALISATION
In Section 3.7 we regarded third normal form as the end of the normalisation process  and this is usually satisfactory.
Further, the process of normalisation to third normal form is neither difficult nor obscure.
However, much of the database literature discusses further levels of normalisation.
Boyce-Codd normal form (BCNF):
One criticism of third normal form is that by making reference to other normal forms, hidden dependencies may not be revealed, BCNF does not make reference to other normal forms.
In any relation there may be more than one combination of attributes which can be chosen as primary key, in other words, there are candidate keys.
BCNF requires that all attribute values are fully dependent on each candidate key and not only the primary key.
Put another way, it requires that each determinant (attribute or combination of attributes which determines the value of another attribute) must be a candidate key.
As any primary key will be a candidate key, all relations in BCNF will satisfy the rules of third normal form, but relations in TNF may not be in BCNF.
It is best explained by an example.
In fact, the third normal from relations in Figure 3.27 are also in BCNF, so we will extend the example used so far.
Assume that we have an additional relation which is also in TNF giving details about the students taking modules and the lecturers teaching on those modules.
Assume also that each module is taught by several lecturers; each lecturer teaches one module; each student takes several modules; and each student has only one lecturer for a given module.
This complex set of rules could produce the relation shown as Figure 3.28.
Fig. 3.28 Relation in TNF but not BCNF
Although this relation is in TNF because the ‘lecturer’ is dependent on all the key (both ‘student’and ‘module’determine the lecturer), it is not in BCNF because the attribute ‘lecturer’is a determinant but is not a candidate key.
There will be some update anomalies.
For example, if we wish to delete the information that Martin is studying B742, it cannot be done without deleting the information that Prof. Harris teaches the module B742.
As the attribute ‘lecturer’ is a determinant but not a candidate key, it is necessary to create a new table containing ‘lecturer’and its directly dependent attribute ‘module’.
This results in two relations as shown in Figure 3.29.
These are in BCNF.
Fig. 3.29 BCNF
Fourth normal form:
We will illustrate fourth normal form by looking at a relation which is in first normal form and which contains information about modules, lecturers and text books.
The example is based on that found in Date (1990).
Each tuple has a module name and a repeating group of text book names (there will be a number of texts recommended for each module).
The module can be taught by a number of lecturers, but each will recommend the same set of texts.
Fig. 3.30 Redundancy where relation is in BCNF but not in fourth normal form
The relation seen as Figure 3.30 is in BCNF (and therefore TNF) and yet it contains considerable redundancy.
If we wish to add the information that Prof. Harris can teach B742, three tuples need to be added to the relation.
The problem comes about because all three attributes form the composite key: there are no functional determinants apart from this combination of all three attributes.
The problem would be eased by forming from this relation the two all key relations shown as figure 3.31.
There is no loss of information, and there is not the evident redundancy found in figure 3.30.
Fig. 3.31 Fourth normal form
The transition to fourth normal form has been made because of multivalued dependencies (Fagin, 1977).
Although a module does not have one and only one lecturer, each module does have a pre-defined set of lecturers.
Similarly, each module also has a pre-defined set of texts.
Although these examples are valid, in that they do show relations which contain redundancy and yet are in TNF and BCNF respectively, the examples are somewhat contrived.
The reader will have seen that in both examples it was necessary to make a number of special assumptions.
The implication is that such problems will not be found frequently by analysts when carrying out data analysis and therefore that TNF will normally be a reasonable stopping point for normalisation.
Many academics take normalisation even further.
In order to provide an example of fifth normal form, Date has to bring in what he calls a ‘bizarre constraint’.
He goes on to suggest that ‘such relations are pathological cases and likely to be rare in practice’.
We will stop at fourth normal form!
Chapter 4
The Logical Schema: Relational, Hierarchical, Network and Object-Oriented Views
4.1 INTRODUCTION
Having formed an agreed conceptual schema, a model which represents the enterprise reasonably well and is independent of any type of physical system (computer or otherwise), it is possible to transfer or ‘map’ this model on to a database management system (DBMS).
The use of a DBMS is not necessary to develop information systems, but it is part of the methodology for implementing information systems discussed in this text.
Thus the entity model is mapped into a form required by the particular DBMS used.
This is the logical schema.
Most DBMS require the data structures to be presented in one of three ways: as relations, hierarchies, or networks, but more recently, object-oriented DBMS have begun to appear.
Although most DBMS fall into one of these design types, some do not.
This will be looked at in Chapter 7 where a number of commercial DBMS are investigated.
Some writers distinguish another type of database, the inverted file.
This distinction is more related to the physical schema (the next stage of developing the data model) and a discussion of inverted files is therefore given in Chapter 5.
It should be emphasised that the data model which we have called the conceptual schema was developed independently of both machine and software considerations.
It represents the integrated user views of the data and the data relationships in the area chosen.
This means that the best possible data model can be formulated with the knowledge that it can be mapped on to a DBMS.
Its conversion to a logical schema can then be made and this will be in the form required by the target DBMS.
Some DBMS accept more than one data model type, for example, both relations and hierarchies.
Whatever the form of the logical model, the accuracy of the mapping from conceptual schema to logical schema is crucial to the success of the database project: there should be no loss of information.
In order to put this mapping of conceptual to logical schema into perspective (and therefore the place of this chapter in the overall methodology), the next step (discussed in Chapter 5) is a further mapping to the storage level representation of data, the ways in which the data is stored on disk.
However, this mapping will be largely transparent to the user, in other words the DBMS will usually take care of physical mapping onto  storage media and file organisation.
This may be carried out following intervention by the database administrator, who will choose between the alternative file organisation methods offered by the DBMS (if any) as appropriate for the particular data and users.
Such alternative methods may include linked lists, indexes, inverted files, random access, B-trees or other methods of file organisation.
This is the concern of the physical schema.
DBMS therefore have two views of the data: the logical schema, which is the view given to them by the user, programmer or database administrator, and the physical schema.
The position of the DBMS, acting as a cushion between the logical views of the data structure and the physical schema, is shown in Figure 4.1.
Fig. 4.1 The logical schema and the database management system
DB2, IMS and IDMS are DBMS which are examples of different database types (we will look at object-oriented DBMS separately, they have only recently been available).
DB2 is a relational DBMS, that is data structures are presented to it in the form of tables; IMS views the data structures in terms of hierarchies; and IDMS in terms of networks.
This chapter discusses these forms of the logical schema and also shows how the data structures might be described to the database.
This description is usually called the Data Definition Language (DDL) of the particular DBMS.
An introduction to a DDL for each type of system is discussed below along with a description of the ways in which the data could be accessed and updated using the Data Manipulation Language (DML).
These features will be developed further in Chapter 7 when we look at particular commercial DBMS.
4.2 THE RELATIONAL APPROACH
The relational model was described in Chapter 3 and therefore it is not proposed to repeat this description here.
The model is an excellent means of describing the data structures in an enterprise, independently of the requirements of a particular DBMS or any other computer-oriented factor.
Many DBMS, such as Oracle, Rapport, Ingres, DB2 and dBaselV, are relational, and therefore can take advantage of the particular features of the relational structure.
This does not mean that the logical model need to be relational, but it is obviously convenient to use a relational DBMS when using the methodology described in the text.
We will look at database mapping in detail in Section 4.8, but in principle, when converting the conceptual schema to the relational model, entities become relations (entity occurrences being the tuples); the attributes are mapped directly, the key attributes of an entity will be the same as those of the relation; and the relationships also become relations, their keys being derived from the keys of the relations taking part in the relationship.
The result is a set of normalised relations mapped on to the target DBMS.
Although the conceptual schema will be a set of normalised relations, it may be efficient to have a logical design specified to the DBMS which has some non-normalised relations.
If a particular group item is always associated by users with a ration, it may be reasonable to specify a non-normalised relation.
This will be a decision of the database administrator.
Relations may also be stored which are derived from the primary set of relations.
Thus, from a basic EMPLOYEE relation
EMPLOYEE (emp-num, emp-name, dept-num) and a basic DEPARTMENT relation of
DEPARTMENT (dept-num, dept-name, no-of-staff, head) an EMPLOYEE-DETAILS relation consisting of
EMPLOYEE-DETAILS (emp-num, emp-name, dept-num, dept-name, dept-head) may be derived.
There may be good grounds, such as a requirement to frequently access this data, to include this unnormalised and derived relation on the database.
In most DBMS, tables are easy to create.
The DBMS may have a CREATE command, such :CREATE RELATION EMPLOYEE
(EMP-NUM, 10, C
EMP-NAME, 30, C
SALARY, 5, N)
This will set up the structure of a relation called EMPLOYEE which has three attributes.
The ‘employee number’ is 10 characters long, the ‘employee name’is 30 characters long, and the ‘salary’is a 5 digit number.
No links need be set up in a relational database between relations.
The links are set up temporarily by the DBMS at run time following a user request for this information.
This will be effected using the relational Data Manipulation Language (DML) of the DBMS.
The DML is the database language which is used to join relations together and pick out selected tuples or domains as requested by the user.
One of the most interesting aspects of relational databases are their data manipulation languages.
Codd (1970) developed a series of operators to manipulate relations.
These operators include those to merge relations and on separate out some columns or some rows.
The set of operators are known together as the Relational Algebra.
Other DMLs, such as the Relational Calculus and Query-by-Example (QBE), are usually translated into relational algebra by the DBMS before executing the request using the database.
The relational algebra is a procedural language.
The operators manipulate the relations using a series of steps to perform the user's requirements.
As we shall see, the relational calculus is less procedural, and the database system is expected to translate this into the procedural steps of the relational algebra.
In a relational system, the result of any operation on one or more relations is itself a relation.
This gives the relational model advantages over other models, The language reflects the design of the system.
The relational algebra is also relationally complete , that is, it is possible to perform any data manipulation on the relations required by the user using one or more of the relational algebra operators.
The language is also executed dynamically.
In other words the access paths for joining or separating relations are not set up with the data.
They are set up each time when the operation is required at run time.
This has particular advantages where the user requests are not predictable, but is less efficient when compared to hierarchical and network systems in situations where the access paths are predictable.
For this reason, many commercial relational DBMS enable links to be set up as an option and therefore they can be set up beforehand and executed each time they are required.
The database administrator (DBA) is given the opportunity to set up indexes at data creation time and this will increase the speed by which data is retrieved.
We now look at the relational algebra and the relational calculus in more detail.
Many DML implementations are based on one of these specifications.
4.3 RELATIONAL ALGEBRA
The relational algebra is a collection of operators on relations.
It includes the traditional set operators union, intersection, difference and product.
The UNION of two relations is the set of all tuples (rows) belonging to either or both of the relations.
The INTERSECTION of two relations is the set of all tuples belonging to both of the relations.
The DIFFERENCE of two relations is the set of all tuples belonging to one relation but not the other.
Thus A MINUS B is the set of tuples belonging to relation A and not B. The PRODUCT of two relations concatenates each pair of tuples on the original tables.
Thus, if the first table has R rows and the second table r rows, then the product of the  two relations will be R x r rows.
This operator is infrequently used because it is indiscriminate in the way it joins tuples together.
(Usually the JOIN operator is more appropriate, as we shall see later in this section.)
These are all traditional set operators and they require that all relations included in one operation have the same degree , that is, they have the same number of columns (referred to as union compatibility ).
Fig. 4.2 The relations associated with an MSc course
Fig. 4.3 Results of operations on relations
So as to illustrate the use of the various operators, we will consider the two four parts, three of which have a ten week compulsory unit and a choice of options.
The relation LECTURER-MODULE gives the module number (each compulsory unit and  option is termed a module) the name of the lecturer presenting the module and the number of weeks it lasts.
One course has no options, it runs the full 20 weeks.
finally, ATTENDANCE gives the student name, the modules that the student has chosen and the lecturer of that module.
Consider the ATTENDANCE relation.
If A is the set of students attending module 3, and B is the set of students attending module 2a (see figure 4.3(a)), then
A UNION B is the set of students tuples Figure 4.3(b)) who attend either module 3 or 2a (or both); and
A INTERSECT B will be the set of student tuples (Figure 4.3(c)) who attend both module 3 and 2a.
The DIFFERENCE between the two relations
A MINUS B will be the set of tuples (figure 4.3(d)) who attend module 3 but not module 2a.
Such operations may be useful for timetabling, classroom allocation and for making examination arrangements.
The results are in tabular form and are therefore understandable to the user.
The other basic operations in the relational algebra are not traditional set operators.
They are PROJECT, SELECT, JOIN and DIVIDE.
The SELECT operator in the relational algebra is different from the SELECT statement of SQL which is described in Chapter 8.
It allows a choice to be made of the tuples in a relation and takes a horizontal subset of an existing table.
For example:
SELECT ATTENDANCE WHERE MODULE = 3awill derive the relation shown as Figure 4.4.
The part of the statement ‘WHERE MODULE = 3a’, is called the conditional part of the statement.
SELECT LECTURER-MODULE WHERE NO-OF-WEEKS GREATER THAN 10 The statement:
SELECT LECTURER-MODULE WHERE NO-OF-WEEKS GREATER THAN 10
will give a one tuple relation shown as figure 4.5:
Fig. 4.4 Result of select
Fig. 4.5 Result of second select
The PROJECT operator will choose specified attributes of a relation and eliminate others.
It therefore forms a vertical subset of an existing table by extracting columns.
It also removes any duplicate tuples that may have been formed by the operation.
Hence:
PROJECT LECTURER-MODULE OVER MODULE, NO-OF-WEEKS
will give the relation shown in Figure 4.6(a).
The PROJECT operator may also be used to change the order of attributes written in the relation, for example:
PROJECT LECTURER-MODULE OVER NO-OF-WEEKs, MODULE
will give the relation shown in Figure 4.6(b).
Fig. 4.6 Result of projects
In using the relational algebra.
SELECT and PROJECT statements are important because many queries will require information from only some of the tuples or attributes in a relation.
But users may also have queries where the data comes from two or more relations.
THE JOIN operator will form a relation from others where there is the same value in a common domain.
This common domain might be a key in one relation and a non-key attribute (called a foreign key ) in another.
In order that tables can be joined, the value of the foreign key must exist as a primary key in another relation or be null (this is a rule of relational modelling known as referential integrity ).
Null  values are usually permitted because in some situations the value will not yet have been allocated.
Dealing with null values is one of the problems that extensions to Codd's original relational model do take into account.
The result of a JOIN operation is a new wider table in which each row is formed by concatenating the two rows that have the same value in the common domain.
For example:
JOIN LECTURER-MODULE AND ATTENDANCE OVER MODULE
will result in Figure 4.7 (the relationship between LECTURER-MODULE and ATTENDANCE is one-to-many).
Fig. 4.1 Result of join
In some relational systems, the operation will be written as:
JOIN LECTURER-MODULE AND ATTENDANCE WHERE
MODULE=MODULE
and some less refined systems repeat information in the relation that is derived from the operation so that the result is a relation consisting of the following columns:
MODULE LECTURER-NAME NO-WEEKS STUDENT-NAME MODULE
LECTURER-NAME
In its simplest form, DIVISION is defined as an operation between a binary relation (the dividend) and a unary relation (the divisor) which produces a unary relation (the quotient) as its result.
To form a binary relation consisting of student-name and module, we first carry out a projection:
PROJECT ATTENDANCE OVER STUDENT-NAME AND MODULE GIVING
DEND
which will produce Figure 4.8(a).
Let the divisor DDR be Figure 4.8(b), then:
DIVIDE DEND BY DDR OVER MODULE:
gives the single tuple of Figure 4.8(c).
There is only one student-name who takes module 2c.
If the divisor DDR was the two tuple relation of Figure 4.8(d), then:
DIVIDE DEND BY DDR OVER MODULE
will produce the relation shown as figure 4.8(e).
Atkins and Johnson both take modules 1 and 1a.
Fig. 4.8 Results of two divide operations
Retrieval requirements may necessitate a combination of operations in order to obtain the desired result.
For example, a typical retrieval request may be to find the names of students who attend any module given by Goodwin.
This can be achieved by the following:
SELECT ATTENDANCE WHERE LECTURER-NAME = ‘GOODWIN’ GIVING TEMP
followed by:
PROJECT TEMP OVER STUDENT-NAME GIVING RESULT.
Fig. 4.9 Two-stage retrieval
The temporary relation TEMP is shown as Figure 4.9(a) and RESULT as Figure 4.9(b).
To insert a new tuple into the relation, the operator UNION could be used.
If, for example, ‘Zed’ was the name of a new student in the first year, then we might add those details to the STUDENT relation by:
STUDENT UNION (‘ZED’, ‘1’) GIVING STUDENT and to delete a student:
STUDENT MINUS (‘ATKINS’, ‘1’) GIVING STUDENT
These procedures are rather awkward and most languages based on the relational algebra ‘cheat’ and have an ADD and DELETE operator as well as the standard operators discussed above, although strictly these are not necessary.
The relational algebra is fairly simple and powerful.
One criticism, particularly when compared to the relational calculus, is that it is somewhat procedural and the user will need some programming experience to be able to formulate a particular request.
Many requests may be very involved, with a number of individual statements.
For this reason, it should not be looked on as an end user language.
4.4 RELATIONAL CALCULUS
When compared to the relational algebra, the relational calculus is more oriented  towards expressing the user requests in a way that the user may construct.
Users specify their needs rather than having to construct the procedures to retrieve the required data.
‘Pure’ relational calculus is difficult for non-mathematicians, but more user-oriented languages which are based on the relational calculus are easier to use.
The QUEL data manipulation language of Ingres is a well used DML which is based on the relational calculus.
Ingres and QUEL are described in Chanter 14 in the context of a case study.
The original specification, Codd's Datu Sub Language called DSL ALPHA, has the general retrieval format:
GET INTO workspace (target list) option list where, workspace is the name of the area where the retrieved data is to be put; target list gives details of the relation; and option list gives the particular restrictions on the target list.
In some implementations of the relational calculus, GET INTO is replaced by RETRIEVE.
An example will make this easier to understand:
GET INTO W (ATTENDANCE-STUDENT-NAME)
WHERE (LECTURER-NAME= ‘CLARKE’)
Here we are putting into the workspace ‘W’ the names of students held in the relation called ‘ATTENDANCE’who are taught by the lecturer called ‘CLARKE’(see Figure 4.2).
The result in W will be is Figure 4.10.
Fig. 4.10 Result of GET statement
The option list can contain < or > (for less than or greater than) as well as = (for equal to).
If a particular relation is frequently referred to in a particular run, then the use of the ‘RANGE’ statement can reduce the work of the user.
For example:
RANGE ATTENDANCE A and
RANGE LECTURER-MODULE L
will allow the user to specify the ATTENDANCE relation using ‘A’ and LECTURER-MODULE using ‘L’as abbreviations.
For example:
GET INTO W (L.MODULE) WHERE (L.NO-OF-WEEKS 0) will give Figure 4.11:
(Module 4 is the only module which lasts longer than 10 weeks).
Fig. 4.11 Result of GET statement
It is possible to search through many relations, indeed the whole database, and the RANGE command can be used to restrict such access by only allowing users to refer to relations defined in a RANGE statement.
DSL ALPHA was specified as a host language system.
This means that the programs could be written in Cobol (the host language) and when data is to be retrieved from the database, the special DSL ALPHA commands will apply.
QUEL is not a host language system, but a self-contained query language.
It uses the RETRIEVE command rather than GET INTO, but this performs a similar function.
SQL, which stands for structured query language, is a calculus-based language used on a number of database systems, for example, DB2 and Oracle.
It has become a particularly popular query language for relational DBMS.
The American National Standards Institute have defined standards for the language.
Indeed, it has become so well used in relational DBMS that it is described in a separate chapter of this text (Chapter 8).
Queries are defined using the:
SELECT/FROM/WHERE
construct, so that a request:
LIST PARTS ISSUED IN 1985
would be formulated as:
SELECT
FROM
WHERE
PART NAME, QUANTITY (data items)
PARTSFILE (a relation)
YEAR=1985. (qualifiers)
Note that this SELECT is very different from (and more powerful than) the SELECT statement of the relational algebra.
The qualifications on the data retrieved can include ANDs and ORs.
As with QUEL, the user has to know the names of the relevant attributes and key them in explicitly.
The user has also to construct a path through the relations thus setting up the linkages required at run time.
SQL also provides a CREATE VIEW command which sets up alternative views of the data derived from other tables and selected rows and columns.
This is useful when setting up different sub-schemas (which adhere to the security and privacy requirements defined by the database administrator).
Lacroix and Pirotte (1977) have proposed an alternative calculus which is named the domain calculus in which the fundamental base is that of a domain variable.
Using this method, variables range over domains rather than over relations.
Languages using this principle include Query-by-Example (QBE).
This has gained popularity with the QMF (Ouery Management Facility) which is an option to DB2 (SQL is the standard interface).
QBE, proposed by Zloof in 1974 is designed for users with little or no programming experience.
This cannot be said of the relational algebra and it is more ‘user friendly’ than a straight relational calculus language such as QUEL or SCL.
The intention is that operations in the QBE language are analogous to the way people would naturally use tables.
Many databases which use SQL provide a QBE interface so that inexperienced users can formulate queries quickly (the data definition will usually have been carried out by a more experienced user using SQL and complex queries will also use SQL).
The QBE query is formulated by filling in templates of relations.
The user enters the name of the table and the system supplies the attribute names.
The system then details the attributes in which the user his indicated an interest.
Many systems are menu-based and it is not necessary to recall the relation names.
This means that minimal training is necessary.
However there are severe limitations to retain flexibility unless the database is not very complex.
Fig. 4.12 QBE example
A session could begin with the user requesting a template of the EMPLOYEE relation.
The template could be seen as Figure 4.12(a).
The user may then request  details of the tuple for employee whose employee number is 756.
The user also marks with a P (for print) any other entries for that employee that are required (Figure 4. 12(b)).
The system then supplies the information required (Figure 4. 12(c)).
The user can also specify retrievals to be made on a range of values, for example, all tuples where the salary is greater than 10000 and also use ANDs, ORs and so on .
Joins are effected by filling in fields from more than one relation.
We will look at QBE in greater detail in Section 7.4 in the context of the relational DBMS DB2.
4.5 THE HIERAECHICAL APPROACH
In the hierarchical model the data structures are represented by trees with the top entity referred to as the root (of which there can be only one per hierarchy).
Subordinate entities are connected to the root and further subordinates connected to them.
These are often referred to as parent/child relationships.
Fig. 4.13 Hierarchy with siblings
Figure 4.13 shows the basic structure.
The link between the entity types COURSE and LECTURER and that between COURSE and SUBJECT represents a one-to-many relationship.
For every occurence of the parent (which must be ordered), there can be any number of occurrences (including zero) of the child (which must also be ordered).
However, a child will have only one parent.
In Figure 4.13 LECTURER and SUBJECT are known as ‘siblings’ because they are of the same level but of different types.
If they were occurences of the same type, they are usually referred to as ‘twins’.
As the details of a parent are only held once, though it can have many children, these details can be integrated with the details of any subordinates as required, and this introduces redundancy.
The complexity of the model will depend on how the database administrator interprets the conceptual model, and increasing flexibility in use may lead to increases in complexity of the logical model and slower access times.
The connections or links are explicit in the hierarchical model.
Pointers are used to  explicitly relate elements of the model.
Trees are normally described using linked lists.
There are overheads when defining and manipulating hierarchies, and this is particularly apparent when comparisons are made with the relational approach.
This becomes worse when hierarchies are made realistically complex to reflect real-world data relationships.
Many real-world data structures fit into the hierarchical pattern, however, and they are also readily understood.
The speaker at many a company induction course will illustrate his organisation or departmental structure using a hierarchical diagram.
Many DBMS, including IMS, are based on the hierarchical approach.
Nevertheless, the approach has been included in this text with reservations.
The network approach, which will be discussed in Section 4.6 can deal with hierarchies as well as networks (many-to-many relationships), and these structures do occur frequently.
For example, a lecturer can teach on a number of courses and a course may use a number of lecturers.
This network cannot be expressed by one hierarchy.
It is possible to express many-to-many relationships using the hierarchical model, but only by creating two hierarchies.
Figure 4.14(a) shows that one course can be given by a number of lecturers and Figure 4.14(b) illustrates that one lecturer can lecture on a number of courses.
These form the many-to-many relationship shown in Figure 4.14(c).
This means that there is likely to be considerable data duplication when using a hierarchical database.
Fig 4.14 Many-to-many relationships expressed as hierarchies
The order of the tree is significant.
In Figure 4.15, the fact that LECTURER is placed to the left of SUBJECT, even though they are on the same level, means that the former will be processed first in any search.
In hierarchical model, each node represents a record type (it is an entity) and each entity occurrence of a record (for example, that for the lecturer J. Smith).
Of course there may be a number of levels in a hierarchy.
Figure 4.15 shows such a possibility.
Sometimes it is necessary to include intermediate nodes, which do not  actually represent records, but are included to maintain the correct hierarchical structure, so that they will be processed in the required order.
The inclusion of SENIOR LECTURER requires such an intermediate node on the right hand side so as to keep SUBJECT at the level of LECTURER who, along with SENIOR LECTURER, will be responsible for the teaching of the SUBJECT.
This is shown in Figure 4.16.
Fig. 4.16 Hierarchy with intermediate node
When manipulating data in hierarchies, it is necessary to specify to which level a request refers.
For example if the next record is requested, is it the next at that level or at a parent level or at some other level?
There is a convention to hierarchical sequence, the ways of visiting trees.
The routine is as follows:
VISIT
LEFT and call routine:
VISIT
LEFT and call routine, and so on RIGHT and call routine, and so on RIGHT and call routine, and so on.
Fig. 4.17 Hierarchical sequence
The routine will terminate when there are no more branches.
It is easiest to understand by following the example given in Figure 4.17.
The sequence here will be A (the root) then left (1), visit B then left (2), visit D, no more left, so back to B (3) and right (4), to visit E, no more left so back to B (5), back to A (6) to right (7) and visit C, back to A (8), no more right and, as it is the root, stop.
Thus A, B, D, E and C, is the natural hierarchical sequence in the above hierarchy.
Inserting and deleting records also presents a problem.
Insertions must be connected to a parent record.
Thus to insert a lecturer record, it is necessary to choose a senior lecturer to whom that lecturer reports.
Deleting a record may also present problems.
If the lecturer to be deleted is the only one that reports to a particular senior lecturer, then the information that the senior lecturer did supervise a lecturer is lost if the lecturer record is deleted.
One way to avoid losing this information, but perhaps not an ideal solution, is to keep an empty ‘lecturer’ record for that particular senior lecturer.
The relations developed by data analysis can be readily converted into hierarchies.
Each relation is an entity and if a domain is a key in one relation and an attribute or part  of a key in another, this would represent a one-to-many hierarchical relationship.
Where a many-to-many relationship is implied hierarchies would need to be created for both directions if the model is to represent the ‘real world’ accurately.
In IMS terminology (see Section 7.2), the entities are converted to segments, the attributes to fields, and the relationships are represented by hierarchies.
Each hierarchy is an IMS ‘database’.
For example, the following two relations in Figure 4.18, COURSE and SUBJECT, both have course as the key, but the latter has it as only part of the composite key with subject.
Thus COURSE to SUBJECT represents two record types having a one-to-many relationship.
COURSE Course, Lecturer-in-Charge
SUBJECT Course, Subject, Title, Lecturer, No-of-Hours
Fig. 4.18 Relationship between COURSE and SUBJECT
In other words, a particular course might have a number of subjects associated with it.
The addition of a LECTURER relation:
LECTURER Lecturer, Room-no and also a LEC-SUBJ relation:
LEC-SUBJ Lecturer, Course, No-of-times-given
would be implying, because Course is also part of the key, that a lecturer can teach a number of courses.
This also implies a many-to-many relationship between the hierarchical record types COURSE and LECTURER.
A DDL for setting up the user views or external schemas and a DML for the processing of hierarchies (applications using the data) is described in Section 7.2, in the context of IMS.
4.6 THE NETWORK APPROACH
As seen in figure 4.19, the relationship between COURSE and LECTURER is a many-to-many relationship and not a hierarchical one.
This can be represented as a network as  shown in Figure 4.20.
The days of the week in which the lecturer works and gives a course are given in link records which need to be set up in these diagrams.
In fact, the relationship between two record types is again one-to-many, but the construct of link records enables convenient representation of many-to-many relationships.
Implied in the diagram is that one LECTURER can give more than one COURSE (Fred, for example, gives business studies and computer science) and one COURSE is given by more than one LECTURER (maths by Tom and Dave, for example )— a many-to-many relationship.
Each link record is on two chains which connect the COURSE and LECTURER to it.
These link records, sometimes called connector records , are usually meaningful in themselves.
The link records in the above example could be called TIMETABLE and contain details of the particular class, for example, day, time, room number and so on .
Fig. 4.20 Network with link records
The occurrences of the records linked together have been represented by pointers.
In this approach, as with hierarchies but unlike the relational model, there are explicit links between related entity occurrences.
This is a clear way of showing the relationships, but it can be misleading as the linked list with pointers is not the only tool  that can be used to link record occurrences.
Indeed the means by which record occurrences are related physically is irrelevant to the logical schema.
An alternative, data structure diagram technique to relate records, is shown in the case study found in Chapter 14.
Although the network approach does answer some of the problems of the hierarchical approach, particularly in that it deals with many-to-many relationships, it is more complex.
Nevertheless there are a number of DBMS, possibly the most well known being IDMS, which are of this type.
The approach became particularly important after the publication of the Database Task Group (DBTG) report of the Codasyl Committee in April 1971.
Codasyl (COmmittee of DAta SYstems Languages) originally set up in 1959 is a group of commercial and government computer users in the United States, and representatives from the computer manufacturing world.
Its essential purpose is to set up standards for computing which reflect the users' requirements rather than manufacturers' convenience.
It was largely responsible for defining the business-oriented Cobol computer language.
One of the main uses of computers in commercial data processing is for file and record processing and the DBTG grew out of this concern.
A number of reports define a common set of standards for database systems and also feature an analysis of DBMS and distributed database technology (see Codasyl 1973, 1978a and 1981).
The Cobol host language interface is defined in Codasyl (1975 and 1978b).
In the terminology of Codasyl, the data item is equivalent to an attribute value, and these can be aggregated (a non-normalised group item) and referred to as a repeating group data aggregate.
A record is an ordered collection of data items and/or data aggregates, the equivalent of a tuple.
One of the main features of the 1971 report is the set concept.
A set consists of a one-to-many relationship between two record types (relations in the relational model).
If we assume that a LECTURER can only teach one COURSE, and many LECTURERS teach on one COURSE, then the ‘closed loop’ shown in Figure 4.21 represents such a set.
Each set type is named.
This set type is called CLEC.
Figure 4.21(a) is not acceptable from a Codasyl standard: it does not include a set name.
Figure 4.21(b) is Codasyl.
Set occurrences are shown in Figure 4.21(c) The set type has COURSE as the owner record type and LECTURER as the member record type.
There can be zero or more occurrences of a member for each occurrence of the owner.
Thus a set type can be defined as a named relationship between record types.
The latter is a form of entity such as COURSE, LECTURER, SUBJECT, and ROOM.
The rules of sets are as follows: a set is a collection of named record types and any number of sets can be defined to the database.
A set must have a single owner type.
A record type can be a member of one or more sets and can be both owner and member   but not of the same set.
A set must also have a specified order.
Fig. 4.22 Set occurrences at three levels
Fig. 4.23 Two set occurrences
The model can be developed showing more than one level, as in Figure 4.22 and further, by having more than one type of record at a particular level, as in Figure 4.23.
In Figure 4.22, LEC is a member of set CLEC and an owner of set TLEC.
In Figure 4.23, COURSE is a record type which is the owner of more than one set of the same type.
However, a record type cannot be a member of more than one set of the same type, and no record type can be both owner and member of the same set type.
Fig. 4.24 Use of link records — one record type at a number of levels
One of the problems that DBTG dealt with was the possibility of having one type of record at a number of levels.
Thus a LECTURER could be at a ‘head of  department, ‘course tutor’ and ‘tutor’level (we used the term involuted relationship in Chapter 3).
To cater for this possibility, link records need to be set up.
This is shown in Figure 4.24.
As we saw earlier, this technique of setting up link records is used to represent networks.
The link record is usually not redundant, that is, it contains useful information in itself (in the relational model, it may well have been specified in any case, as a separate relation formed from a relationship in the data analysis phase).
Although the network model is more flexible than the hierarchy, the overall structure is fairly complex to implement and requires extra storage space to handle the pointers.
As a first approach to converting the conceptual schema to the Codasyl model, the entities are mapped to records, attributes to data items, one-to-many relationships to sets, and many-to-many relationships to two sets connected by a link record type.
4.7 LANGUAGES FOR USING NETWORKS
The DBTG specification proposed a language for manipulating the files.
This data manipulation language (DML) is embodied in a host language.
The host language was originally Cobol but the DML extensions to other host languages have been specified.
The DBTG also specified a language to set up the logical schema, called the data description language (DDL) and a DDL to set up particular user views (sub-schemas) of the data structures.
The sub-schema DDL and the DML are extensions to the Cobol language, but even the schema DDL has a Cobol ‘flavour’, though it is supposed to be language-independent.
The sub-schema DDL has to be compatible with Cobol (or the chosen language) as it will be embedded in it.
The DBTG report did not suggest a standard for the Device and Media Control Language (DMCL), that is the language controlling the physical access to storage devices, because this would be dependent on the particular hardware used.
However the original Codasyl proposals were considered by many to have a ‘physical schema flavour’ as well as presenting a standard for the logical schema.
In the later report of 1978, a Data Storage Description Language (DSDL) was proposed and this included many of these physical (that is implementation) aspects.
It corresponds to a DMCL in part.
The Schema DDL is the language to set up the global data description.
The language is distinct and self-contained and it is not dependent on an established programming language.
It enables the specification of the various record types (including the individual data items such as‘lecturer’, ‘name’, and ‘age’ in the LECTURER record) and the specification of set types relating these records.
It is envisaged in the report that a  database administrator would be responsible for this overall schema data definition and at ANSI/X3/SPARC terminology, via the sub-schema DDL.
This was certainly perceptive, and most practitioners would argue strongly for strengthening the role of the DBA.
The concept of areas which is proposed in the Codasyl report is also an interesting one.
Although it is associated with the Schema DDL, it is an important consideration in the physical storage of the data.
It is a named portion of accessible storage space in which records are to be stored.
It may contain occurrences of records, or sets, or parts of sets.
This concept is one of the features of the original report which is most open to criticism because it is specifying ‘physical’ considerations at the logical level (the Schema DDL) and therefore may sacrifice some of the data independence of the model.
When assigning records to areas, the DBA has to reconcile a trade-off.
On the one hand, increasing the number of areas allows files to be distributed on a number of areas of storage.
This will optimise performance and increase integrity and security, so that concurrent access to areas can be prevented.
further, as back-up and restore procedures are carried out by areas, a program with exclusive access to a particular area will not exclude access to other areas.
The price to be paid for this is the consequent complexity of the procedures.
In view of these considerations, particularly its implementation aspect, it is not surprising that the area concept was removed in the later specification.
The sub-schema DDL allows the setting up of particular user views of part of the schema, such as the specification of privacy locks, and the renaming and redescription of data items and records.
Some sub-schemas could overlap.
The data manipulation language represents extensions to the Cobol language specification.
This allows data on the database to be manipulated.
It thus includes GET and FIND for reading records on the database (equivalent to the Cobol READ) and STORE and INSERT which are equivalent to the WRITE statement.
ERASE is similar to the Cobol DELETE statement.
It removes records from the database.
We will look at these languages in more detail in Section 7.3, which discusses IDMS, a Codasyl-oriented DBMS.
4.8 DATADASE MAPPING
The mapping or transformation rules to convert the conceptual model in the form of entities, attributes and relationships, to a logical model which could be relational, hierarchical or network has already been touched on in the relevant sections.
In this section we bring the basic rules together.
Although much of the mapping is relatively  straightforward to automate because it is rule based, there are exceptions and therefore the mapping in a real database application is not quite so straightforward as implied here.
In the case of mapping to a relational DBMS, the identifier or key of the entity remains the key, the properties or attributes of the entity remain as attributes, the entity becomes a relation and the relationship also becomes a relation.
The key of the relation in the latter case is made of the identifiers of the entities of the conceptual data model which the relationship links.
If the relationship is itself a carrier of properties, these become attributes of the relation.
There are slight variances in these generalities depending on whether the relationships are binary or n-ary and on the cardinality of the relationship (Quang & Chartier-Kastier, 1991, pp 103–107 and Elmasri & Navathe, 1989, pp 329–334 give details).
In the case of mapping to a Codasyl DBMS, each of the entity occurrences of the conceptual model becomes a record, the owner record in the set corresponds to the entity at the ‘one’ end of the relationship, with the other record as the member, the identifier or key of the entity remains the key of the records of the logical model, and the properties or attributes of the entity remain as data items or fields of the records.
The mapping of relationships also depends on whether the relationships are binary or n-ary and on the cardinality of the relationship.
In a one-to-many binary relationship, where the relationship linking the entities is the carter of attributes, these migrate to the member record and the relationship disappears.
In a many-to-many binary relationship, the relationship becomes a member record, its key is made up of the identifiers of the entities of the conceptual data model which the relationship was linking and if the entity is the carrier of attributes, these become fields of that record.
In the case of n-ary relationships, the relationship becomes a member record, each of the entities linked by the relationship becomes an owner, their keys are made up of the identifiers of the entities of the conceptual data model which the relationship was linking and the attributes of the entity become fields of the records generated.
Again, there are some exceptions (Quang & Chartier-Kastler, 1991, pp 111–116 and Elmasri & Navathe, 1989, pp 334–338).
In the case of the hierarchical data mapping, the one-to-many relationship types can be represented as parent-child relationships but many-to-many relationships are more difficult to represent and lead to duplication.
In this case the relationship is treated as if it were a one-to-many relationship, but the records on the ‘many side’ are duplicated because each record can be related to several parents.
There are a number of alternative ways of dealing with many-to-many relationships, all of which lead to either duplication or unnatural restrictions on the model (that is, the model does not represent the real-world accurately).
The various options are discussed in Elmasri & Navathe, 1989, pp 338–344).
4.9 OBJECT-ORIENTED APPROACH
The development of databases is sometimes perceived as having three generations:
1)
file management systems
2)
Hierarchical database systems
3)
Network and relational database systems to which can now be added a fourth:
4)
Object-oriented database systems.
This way of seeing the development of databases can be misleading because it should not be implied that either each new generation will replace the previous generation (all these systems will be used for some foreseeable time in the future) or that each new generation is necessarily better than the previous one (the appropriateness or otherwise of an approach will depend on the organisation and its applications).
Nevertheless, object-oriented database systems are an interesting newer development which is having considerable impact on the database world.
further, the object-oriented approach is likely to impact on data modelling, so that some database applications may be developed through object modelling (rather than entity-relationship modelling and relational modelling) mapped to object-oriented databases (rather than relational databases).
Nevertheless, it is perhaps premature to regard object-oriented design as a comprehensive methodology, though it has potential to become one.
An object-oriented DBMS (OODBMS) is one which supports an object model.
In an object-oriented database all data is represented as an object, though to support the richness of the data such DBMS need to support object-oriented programming language concepts such as complex objects, object identity and inheritance.
We will look at these concepts later in the section, but it should be noted that not all ‘object-oriented DBMS’ presently support all the concepts associated with object-orientation.
As well as distinct OODBMS now being available, some relational DBMS have object-oriented extensions.
According to Loomis (1990a), the appeal of OODBMS comes from two major factors:
The data model more closely matches some real-world entities.
Objects can be stored and manipulated directly: there is no need to transform application objects into tables.
Data types can be defined by the user, rather than being constrained to certain pre-defined types.
Thus, whereas in the relational model information about the object customer would be scattered over a number of relations (which could be: customer, holding customer name and address information; sales ledger, holding details about the debts of the customer; invoice, holding details of payments due), in the object-oriented model there would be one object, customer, reflecting all this information.
The database language can be integrated with an object-oriented programming language (in particular Smalltalk, but also Traits and Eiffel, and extensions of programming languages such as Object Pascal, Object Lisp, and C++), which means that the programmer deals with a single uniform model of objects.
It is not necessary to have different languages for database access (such as SQL) and a host language (such as Pascal) for the complex programming tasks using the data collected.
The first factor is important in that it brings some applications within the database world (for example, computer aided design, computer aided manufacture, architectural design, software design (Case) and office automation), previously ill-served by relational databases because of the performance of relational database systems and because the relational model itself is unsuitable For those domains.
Now engineers, for example, can also share information because it can be held on the object-oriented database.
The object model more naturally represents their real world (the relational model proves particularly suitable for data processing and accounting applications).
The correspondence between real-world and database objects means that the objects do not lose their integrity and identity, and therefore can be identified conveniently and operated upon.
The second factor is important because it should lead to improved productivity of application developers, as it is no longer necessary for them to know diverse languages, environments and command sequences.
As Khoshafian (1990) argues, by combining database functionality with object-oriented concepts, OODBMS become the ideal repository of information that is shared by multiple users, multiple products, and multiple applications on different platforms (hardware and software environments).
We have been aware for some years that the relational model and relational database systems have not been able to represent some real-world concepts and activities, and there have been many papers and conferences devoted to the study of extensions and refinements to the original model of Codd (1970).
The entity-relationship model of Chen (1976), Codd's own extended relational model (Codd, 1979), known as RM/T, and the functional data model of Shipman (1981) are three of the most well-known.
These extensions to the relational model are known as semantic data models (see, for example, Hull & King, 1987), and they represent the results of many efforts to enhance the richness of the relational model.
They aim to generalise from the relational model by capturing as much semantic information from an application domain as  possible.
(The role model of Bachman & Daya, 1977, attempts to extend the network model in a parallel way).
Although these models are able to represent more complex situations, to some extent this has been at the cost of reduced elegance and simplicity and only the entity-relationship model of Chen is widely used as a data modelling tool.
In the approach used in this text, the E-R model is used in conceptual modelling because it is much more semantically rich than the relational model, but this is mapped to a relational model because most commercial databases are relational and it is more convenient for logical mapping.
A more fundamental criticism of semantic data models has been that although there is increased support for data definition, there is no parallel increased support for data manipulation.
By specifying the operations that can be performed on each data object, the object-oriented approach attempts to model the dynamic as well as static aspects of the world being modelled.
The object-oriented approach increases both the semantic and manipulation capability of database systems and therefore offers more than semantic data models.
The uniform group of objects that the programmer has when using an object-oriented model, also reduces the sting of the ‘elegance and simplicity’ argument used against the proponents of semantic data modelling.
Therefore, even if complex real-world situations can only be represented by complex models, the proponents of the object-oriented model argue that it is only as it need be to represent these situations on computers and to manipulate those representations for the purpose of processing queries.
Indeed, many argue that its concepts allow even complex situations to be expressed easily and naturally and that the associated techniques and tools enable the construction of complex software systems for those applications.
In the object-oriented model, objects represent some aspect of interest in the application area: they could be physical entities, concepts, ideas or events.
Typically an object is a producer or consumer of information or an item of information.
This has its parallel with entities in the entity-relationship model, which will be represented as relations in the relational model.
However, there are major differences.
The concept of object identity provides one distinction.
Conventionally, a record or tuple is identified by one or more attributes that uniquely identify that occurrence.
This causes problems if, for example, there are two customers with the same name or the value of that identifier changes (a customer name change, for example ).
In the object model, the object's current value can change, but its identity is unique, system generated, never reused and identifies an object for its lifetime.
Any attribute of an object can be changed without destroying its identity.
Hence an object can be a dynamic structure, changing when being manipulated, indeed its form may not be known by the system in advance.
An object may contain any type of data: text, sound, photographs and video, and  multi-media systems (Section 11.9) may be based on objects.
Unlike alternative modelling approaches, the object model models data, data definitions and processes as objects, the same basic element of the model.
All this provides a flexibility not found in other models.
Another area of distinction is that the object model can represent an abstraction and generalisation (Smith & Smith, 1977).
An animal is an object and there are many kinds of animal (dog, cat, cow).
We could say that an animal is a group of things which are related in some way.
Using the terminology of object-orientation, animal is a class or abstract data type.
This is a set of objects with the same representation.
Within the class are sub-objects (birds, mammals).
These are also objects in their own right.
We can also bring similarities together by generalising.
In the object-oriented model, the features of a cat are added to the features of a mammal (to which sub-class cat belongs) and the features of a mammal include the features of all animals (the class to which mammal belongs).
This is the object-oriented concept of inheritance , that is, characteristics received from an ancestor in a class hierarchy.
They also specialise , by adding extra features to the object.
Thus objects include a much wider list of things than entities: indeed, a database schema or a data dictionary are themselves objects.
This generic type object gives the object-oriented approach advantages for data manipulation, as a number of related objects can share operations (called methods in object-oriented terminology).
Thus, objects can inherit operations, enabling code sharing and reusability among software modules, as well as characteristics.
A full description of the object-oriented model would be inappropriate here, but as Kim & Lochovsky (1989) and many others show, the approach provides a very rich language to model complex real-world objects.
There have been a number of approaches proposed to design and develop object-oriented systems.
These include object-oriented development (Booch, 1991), hierarchical object-oriented design (developed by the European spice agency) and object behaviour analysis (Gibson, 1990).
4.10 COMPARING APPROACHES TO LOGICAL DESIGN
Of the approaches to database design, relational databases are possibly the most flexible, as access paths can be defined as and when necessary using the JOIN operator.
This means that the exact nature of queries need not be predicted during database design.
further, there is less need for an experienced programmer or database administrator to access the database as there are languages available which are easier to use, such as QBE.
A full justification of the relational model was given in Section 3.6.
However, relational DBMS require the user to be aware of the relations that have been set up in order to obtain information from the database, including keys and foreign keys used to join relations.
The user also needs to know the operations that can be applied to the relations so as to carry out the desired retrievals.
Speed of access may also be worse, as the links between relations are set up at run time and not at data definition time.
Slow retrieval speed has led to a partial withdrawal of relational database systems in a few organisations as well as the late appearance of commercial relational DBMS which came many years after their research counterparts.
The incorporation of query optimisation techniques are an important feature of any relational DBMS.
Some organisations have database machines, dedicated to efficient database processing.
for this reason, pre-ordering of relations (strictly, tuples need not be in any pre-defined order in the relational model) and the use of indexes to increase speeds of particular accesses, are both options in many DBMS to improve their overall performance.
Storage space may also be greater (for example, for foreign keys enabling joins of relations), unless the other approaches use a large number of pointers to support database use.
One of the advantages of the relational model is its simplicity, and this makes relatively simple query languages, such as SQL, Quel and QBE, feasible.
On the other hand, this simplicity has its price in that the model proves unsuitable for some applications (hence, the drive towards semantic data models and object-oriented data models).
for example, attributes should be atomic (they cannot be divided into sub-attributes).
In a personnel system, for example, names will be divided into surname and first names, where searches such as ‘give me all people with surname Jones’ or ‘…first name David’are easy, but the request ‘give me all people with name David Jones’requires a connection between first name and surname which make queries more complex.
Further, repeating groups do occur in applications (values in a ledger, for instance), but the normalisation process does not permit representation as an array.
In the network and hierarchical DBMS the access paths are defined by the DBA before the database is set up and these are often referred to as navigational systems (Bachman, 1973).
Although these access paths can be changed at a later date, this can usually be achieved only with considerable difficulty.
It will probably necessitate reorganising part of the database and, therefore, navigational systems are rather inflexible once the design has been implemented.
Relational databases do not have this problem.
The hierarchical database only allows downward access paths, and a member at one level cannot be an owner of a record at a higher level.
The network system allows access paths to be defined at any level and thus allows much greater flexibility.
The  penalty paid for this is increased complexity when setting up the pointers and accessing and updating the database.
A more fundamental criticism is the requirement that databases should facilitate unplanned access as well as planned retrieval to the database.
In assessing DBMS there are other factors to bear in mind and not all DBMS of each ‘type’ follow the ideal.
Some have options which enable the user to correct many of the disadvantages of the type.
Object-oriented DBMS are much newer and appeal because of two major factors.
The first is that the data model more closely matches some real-world entities.
Objects can be stored and manipulated directly: there is no need to transform application objects into tables.
Generalisation and abstraction, which reflect real-world relationships where objects can inherit properties from their parents, are supported.
Secondly, the database language can be integrated with an object-oriented programming language (such as Smalltalk), which means that the programmer deals with a single uniform model of objects.
When object-oriented database systems become more widely available, there may be a number of application domains where their advantages make them a good choice.
However, at the time of writing, object-oriented DBMS are still in their infancy, with no system accepted as a standard (indeed present systems differ greatly in their exact data model, query language and mapping schema).
Commercial systems include Ontos, GemStone and OrionII.
They require navigation through the class hierarchy (rather like hierarchical and network DBMS) and though this may well be appropriate for some applications, it is not appropriate to all.
Chapter 5
The Physical Schema; File Access and Organisation
5.1 INTRODUCTION
The physical schema represents the various file organisation methods that are supported by computer systems.
It is sometimes referred to as the internal schema , for example, by ANSI/X3/SPARC (1975).
File organisation refers to the way that the records have been structured on disk, that is, record storage and how the records are linked together.
File access refers to the way that the records can be retrieved for the purpose of reading the data, changing it, adding to it and deleting it.
In common with most texts, this book uses the term file organisation to mean both the data storage structure and access methods.
As we can see from Figure 5.1, the physical schema represents a mapping from the logical view.
This mapping may be carried out automatically by the DBMS; indeed there may not be any options open to the user.
On the other hand, some DBMS enable alternative ways for the data to be stored and accessed, and the decisions will most likely be made by the database administrator, possibly with the help of optimising and efficiency information provided by the DBMS.
Figure 5.1 also illustrates that there is a further mapping to the machine view of the data (sometimes called the device and media model (see also Section 4.6)) in terms of blocks, pages, disks, tapes and so on.
This is not discussed in this text as it is dependent on the particular hardware chosen.
5.2 TRADITIONAL METHODS OF ORGANISING DATA ON BACKING STORAGE MEDIA
We will first look at traditional methods of file organisation.
This will enable us to understand the issues relating to file organisation, and in Sections 5.3, 5.4 and 5.5 we look at those structures most suitable for database work.
Serial files are the simplest form of file organisation.
The records are not stored in any particular sequence apart from that of first in, first out.
This is simple to create, but makes the retrieval of any particular record slow as the file has to be searched until the required record is found.
Because database systems require fast retrieval, this method  of organising data is unlikely to be appropriate, apart from, perhaps, the storage of historical or back-up files.
Fig. 5.1 The place of the physical schema
A second method of file organisation is to store records in a particular sequence.
A tile of lecturers may be stored in ‘lecturer number’ or ‘lecturer name’sequence.
The records need to be sorted in order of a particular (key) field before storing the data on to the file.
Sequential files are easy to use, they will support records of variable length and they use the backing storage media space efficiently.
Sequential organisation has the major advantage that the records are stored in a logical order, presumably that sequence to which the records are normally required for printing and for soft copy reports.
However, sequential ordering still does not cater for the particular requirements of most database uses.
It may be used for printing or processing information on a large number of records on the file (where there is a high hit rate ), but is inappropriate where the required hit rate in a run is low (say, less than 6%).
Particular records cannot be searched for directly.
A search has still to be carried out of the file until the particular  record required is found.
Tape files can only be accessed in this way (figure 5.2) and therefore tapes can only be used for serial or sequential files.
Fig.
Adding new records to a sequential file
If the file is stored on disk, however, there are possible answers to the problem.
One solution would be to store an index (Figure 5.3).
This will contain the key alongside a pointer , which is a field used to identify the location or address where the data is stored.
The record address can be retrieved following an index look-up operation.
With the address known, the record can be located.
This address could be an actual physical address, that is, the disk cylinder, track and block number where the data is held, or some sort of relative address: relative, that is , to the beginning of the file.
Fig. 5.3 Indexed files
The process of (a) searching the index to find the address of the required record, and then (b) going directly to that address, will be considerably faster than searching through all the records on a sequential file.
Sometimes the index is sorted, making access quicker, particularly when using the binary chop method of searching.
Here, the key of the record to be accessed is compared to the key of the middle record on the index.
If the desired key is higher than this, the key is compared to the third quartile record (if less, to the first quartile record).
The process continues until the matching key is found.
Figure 5.4 shows the method.
Four searches of the index table are required in order to find the address of record 30.
Serial searching will take eleven accesses of the index.
ln a large file, normally the case in database systems, the savings in time will be marked.
Fig. 5.4 Binary search (or binary chop)
The indexed sequential method of file organisation which is common in conventional files may be appropriate for some database uses.
This method is based on a combination of the sequential and the index methods described earlier.
The file is created in sequential field.
It can therefore be processed sequentially, and this may be best for some of the applications which use the file.
An index is created at the same time.
Unlike the examples of indexing discussed previously, in index to every record need not be held.
Instead there is a reference to only one record for each block of data.
The key of this record will be the highest in that block, which usually corresponds to a track on a disk.
Figure 5.5 shows an indexed sequential file of three blocks of data.
The index contains the keys of the records ending each block along with the address of the beginning of that block.
If the key requested is 24, then the index table will be searched to see in which block it is to be found.
The address of the beginning of the relevant (third) block can then be found, located, and searched through sequentially until the required record is located.
Thus index sequential files, even when processed by key, do not eliminate sequential processing, but limit it to one block.
With large files, there will frequently be a cylinder index added as a first level index.
The particular track index can then be located.
Even then, if a file is very large, it may spread over a number of disks.
In that case there will be a further preliminary search through a disk index.
Access speeds may still be slow for many purposes in a database environment.
Updating and deleting are straightforward.
The record is accessed (as previously described), changed or deleted (or marked ‘for deletion’).
The addition of a record to  the indexed sequential file can present difficulties, however.
Indeed, it will require complete reorganisation of the file unless precautions are taken, because the records should be retained in sequential order to permit sequential access.
One usual method is to leave gaps at the end of each block so that records can be inserted into a block (with only slight reorganisation of the records in that block to maintain the correct order).
figure 5.6 illustrates how a record with key number 5 can be inserted.
Problems will occur when a block becomes full.
Fig. 5.5 Index sequential file
Another possible way of allowing for addition is to set up a special block, called the overflow block (see Figure 5.7).
When a data block overflows, a pointer will be maintained to point to the address of the overflow block.
Eventually, however, reorganisation of the complete file is necessary to avoid poor performance.
A true direct access file is one where there is a direct relationship between the key field of a record and a unique address on disk.
Processing is therefore minimal and the records will be stored in sequence.
Keys have to be numeric and space has to be  reserved on disk to hold records of every key that can possibly exist in the defined range.
If many keys do not have records associated with them, then there is wastage in storage space.
Indeed, few files seem in practice to conform to this requirement.
Random access is more usual, and these files are often called (mistakenly) direct access files.
Fig. 5.7 Index sequential files — overflow block
Random access offers another way of organising data on disk for quick access.
The DBMS allocates space on disk and a record is allocated to a particular slot via a randomising (or hashing )algorithm which is designed to use the allocated space efficiently.
This translates the key of the record to a physical address on disk and the system places the record there.
The advantage of random access files lies in the speed of accessing a particular record via the algorithm (which will not produce files in sequential order).
There is a wide range of possible algorithms.
The designer has to choose one which is appropriate for the file.
Random access files can present problems mainly because an appropriate randomising formula is often difficult to construct.
A good formula will depend on the area on disk available and the particular spread of keys.
By ‘good’ we mean two things:
1)
The area on disk allocated for the file is well used — very critical for large files.
This is referred to as a high packing density.
2)
There are not many synonyms.
A synonym occurs when the application of the randomising formula to two or more keys gives the same address.
In this case, most systems look for the next available space when storing the data or the record is moved to an overflow area.
If synonyms occur frequently, it can slow down access speeds considerably, and therefore the advantage of random access is lost.
The file organisation methods so far discussed do not respond to all the desired features of database systems.
In particular, a database user may want to search a file in  a number of ways, according to a particular application.
A file of lecturers, for example, may be searched for according to ‘lecturer number’, ‘lecturer name’, ‘status’, or ‘qualification’.
A possible solution, but one which contains considerable data duplication and certainly does not conform to the aims of the database approach, would be to store the files in a number of ways.
In other words to have a file of lecturers in lecturer number order, another in name order, another in status order, and yet another one according to qualification.
Sorting records for each application would also be too slow.
This solution would be very inefficient in storage space and, if data changed, it would be necessary to undate a number of files.
Assuming that the DBMS permits a number of alternative file organisation techniques (which is normally the case), the actual organisation chosen for any particular data structure will depend on which is the most efficient for that particular application, and this decision is likely to be made by the database administrator.
The file organisation method chosen for some files could even be serial or sequential, particularly if the data is likely to be accessed in batch mode.
Most DBMS can process data both in real-time and batch mode.
If the use of the data changes, then its organisation should be capable of being changed without the users being aware of it (known as physical data independence ), apart, that is, from the gains in speed.
The systems of file organisation most suited to database management systems are B-trees, the various forms of list organisation and the inverted file.
These are discussed in the following sections.
5.3 B-TREES
The B-tree represents a development from the index sequential file with its multi-level indexes (for example, track, cylinder and disk).
As we saw, the necessity for overflow records presents problems with index sequential files.
B-trees are also multi-level indexes but they have a tree-structured index and perform much better (particularly in the way they can absorb new records).
There are many variants on the basic structure, the B+-tree being the most used variant of the standard B-tree.
Most DBMS support some form of B-tree and they represent the most common database storage structure.
Like index sequential files, but unlike random access files, B-trees support sequential access.
We will follow Date (1990) and illustrate B-trees through the variation proposed by Knuth (1973a) which is a B+-tree.
This is shown in Figure 5.8.
The basis of the B-tree is the multi-level index and the levels of index form the tree structure from which the file organisation method is part-named.
The ‘B’ stands for   balanced, meaning that all ‘leaves’(the sequence set) are the same distance from the root.
A non-leaf node that has x children will have x-1 keys (in the example, three children and two key values).
Fig. 5.8 Part of a B-tree (modified from Knuth, 1973a and Date, 1990)
The index has two parts, the sequence set and index set.
The former consists of an index to the data and, as can be seen from Figure 5.8, enables sequential processing through the data files.
The index set enables fast access to a particular part of the sequence set (and hence a particular data record).
It is the index set which represents the B-tree and with the sequence set forms the B+-tree.
In figure 5.8, each value is a value of the indexed field and those of the top node have two values (45 and 86) and three pointers.
Data records with index fields less than or equal to 45 can be found by following the left pointer; greater than 45 and less than or equal to 86 can be found by following the middle pointer; and records greater than 86 by following the third pointer.
The same principle applies at each level.
Having reached the sequence set, there is a pointer to every data record.
The leaf node therefore has a different structure to index nodes.
Leaf nodes contain data pointers and index nodes contain pointers down the tree.
As Date points out, this is a simplified version of a B-tree, because the nodes of a B-tree need not contain the same number of data values and they normally contain free space.
Additions and deletions can be made which always leave the tree balanced.
If we wish to add a new value, then we search for that node which is the lowest level of the index set in which it belongs.
If there is free space it is inserted at the correct node.
Otherwise, the node is split into two nodes containing the lowest and highest values respectively.
There may now be a place, but if not, this process can be repeated up the tree until a place is found for the new value.
In this way, the correct sequencing is maintained.
Thus there is no repeat of the search through overflow areas and/or frequent reorganisation that might be necessary to support index sequential processing.
It therefore has major advantages for some application domains: it enables quick access to individual records; it enables sequential access; and modifications are made easily.
5.4 LISTS
One of the obvious ways of organising the data on physical storage media is by using lists.
In fact the Codasyl DBTO started life as the List Processing Task Group but changed its name on investigating the wider issues of their task.
A list structure joins a sequence of data elements by including along with each data element a pointer, which is itself a field, relating to the next data element in the list.
This pointer could refer to the  actual address, for example, the cylinder, track and head number on a disk.
Usually it refers to a relative address, the location of the next record when compared to the address of the record being presently processed.
Alternatively, it may be a symbolic address, which is converted to an actual address via some form of key transformation scheme.
This logical identifier has to uniquely identify the record, and its actual address can also be obtained by looking up a table containing the addresses of all the identifiers.
Fig. 5.9 Linked lists
Figure 5.9 (a) shows a list of lecturers with details completed in Figure 5.9 (b).
‘Next’ represents a pointer to the address of the next record in the list.
The word ‘null’ marks the end of the list.
Obviously much of the data relating to each lecturer (such as department, salary, address, telephone number, courses taught) has been omitted.
These records need not be held in this ‘logical’ sequence (see Figure 5.9(c)), as long as the files are held on direct access storage devices.
One of the advantages of list processing is that data can be processed sequentially without the records themselves being stored sequentially.
Records are added by changing the pointer references (see Figure 5.9(d)).
If George Bloggs of number 5136 was added to the list then the pointer contained in B Dale's record should be changed to point to 5136 (and not to 5138) and the pointer in  Bloggs should contain 5138.
This maintains the required sequence.
Deletion requires the reverse of this exercise.
Fig. 5.10 Records with a number of pointers
To facilitate easier access it may also be necessary to add other pointers (Figure 5.10), such as a pointer to the beginning of the list or to the previous record as well as the next one.
It may be convenient for the last record to have a pointer which points to the first record in that list, in which case the list is a circular list, ring or chain (see Figure 5.11).
Fig. 5.11 Circular lists
For most DBMS applications, the lists required will be more complex.
For one thing, there are likely to be various levels.
These will support the tree structures (Figure 5.12) that databases frequently are required to represent.
DBMS are therefore expected to support multiple linked lists if they adopt the list processing method of file organisation.
Tree structures can be balanced or unbalanced.
Each node in the former will have the same number of ‘children’.
Binary trees are balanced, each node having two children and two pointers.
This can result in fast access if the application files lend themselves to this type of structure.
To implement a network, it is necessary to have a series of pointers at each record occurrence.
A realistic example would therefore have a considerable overhead in space for pointers, and the time necessary to access a record through a list may be slow.
Modifying the database will also be complex.
The above example could support a network.
Both the department head lists will hold the same pointers downward as the  lecturers hold upwards, that is, all types of access between the two ‘levels, are permitted.
Fig. 5.12 List processing — tree structure
The various records are likely to contain more data.
The lecturer records might contain the age, qualifications, years of experience, salary, and so on.
The list structure allows other fields within records to be linked (though this flexibility is paid for in terms of complexity and other overheads).
There may, for instance, be a requirement to join together lecturers of a similar qualification or age group.
Fig. 5.13 Records with a number of lists
In Figure 5.13 there is one list of four records joining the lecturers together.
There are also three lists joining lecturers within similar age groups.
The first of these (less than 10 years) has only one record, S. Smith.
The second list joins two records, that of B. Dale and P. Jones, who have less than 20 years experience.
The final list (over 20  years experience) contains only the one record, that of G. Bloggs.
Frequently there will be other pointers.
In a hierarchy, a pointer is likely to refer to the owner record — on each lecturer record there could be a pointer to its owner, the head of department.
This would be useful to support quick processing of enquiries.
The negative aspect of providing this flexibility is the added complexity, in particular to support insertions, deletions and updates.
Pointers have to be maintained for each of these operations, so time will be spent maintaining pointers ‘as much as updating data.
Furthermore, the software to support this requirement will also be fairly complex.
It is essential that links are not lost when maintaining pointers.
The database administrator has to think very carefully when setting up numerous lists as to whether their use will justify the overheads.
5.5 INVERTED FILES
In some applications the list structure has serious drawbacks.
One type of enquiry which is slow in list processing but which inverted files drill with much more efficiently is illustrated by the example: ‘Is there a lecturer with a name Perkins on the staff?’.
In a list file, this request requires access to all records on the ‘lecturer name’ list until the Perkins occurrence is found (if at all).
This simple enquiry, which a user may feel' ought to have an immediate response, might take some seconds to process.
With inverted files the records are held in a data area on disk, but there is also an index which contains values or a range of values for some of the data.
These will be the nominated key fields.
Along with these are associated pointers containing the addresses of the record(s) with those corresponding field values.
Normally the pointers are relative, rather than actual, addresses.
This allows for changes of physical location without changing the index, thus providing some level of data independence.
Another possible gain in efficiency is to hold the indexes of the inverted file in sequential order so that the binary search (discussed in Section 5.2) can be applied.
Many queries can be answered by simply accessing the index.
Fields which may be frequently accessed may be designated key fields by the database administrator and held on the index, to facilitate this easy access.
There are overheads in storing these indexes and this factor needs to be weighed with ease of access.
Record insertion, deletion and update may involve high costs.
Large files will have a number of inverted indexes.
Sometimes there is an overall key index, and pointers to the relative addresses of the particular key indexes are required to be searched.
A further possibility is to combine the list and inverted file.
The inverted file will have an index which points to lists of certain types of record.
Fig. 5.14 Inverted file
There may be a number of data items in a record that are chosen to be indexed.
In Figure 5.14, a lecturer's qualification and department are indexed but not name or date of birth.
Of course there is a price to pay.
The ‘ideal’ 100% inversion, where all data items are keys and therefore all items are contained in an index as well as a data record, will be very inefficient in many respects.
When deleting a record, not only has the data record to be deleted but all mentions of it in the indexes have also to be deleted.
for additions, the record is created and the index must include references to it where appropriate.
When updating, care has to be exercised when the field updated is one which is indexed.
Yet there are considerable advantages of such an approach.
Queries of the type ‘Are there?’ or ‘How many?’become simple if the information is held in the index.
for example: ‘Do any lecturers have a PhD?’ or ‘How many lecturers are in the Computer Science Department?’can be answered by accessing the index only.
If there are expected to be a number of queries of the type ‘How many lecturers are over 40 years of age?’, the database administrator may well consider using date-of-birth as a key for inclusion in the index.
More complex queries requiring ‘and’ and/or ‘or’can be described such as‘Is there a lecturer who has a PhD and is in the Business Department?’.
The answer can also be obtained by access to the index without recourse to the data storage area.
Fig. 5.15 Use of secondary indexes
When there are a large number of keys, it may be convenient to have a secondary index as shown in Figure 5.15.
5.6 CONCLUSION
If the database administrator has the choice of file organisation then that chosen will depend on the application.
Some database management systems support a number of file organisation methods and many will help the database administrator by simulating the requirements of the application on various file organisation types.
Some DBMS support a combination of the above approaches, such as, secondary indexes with chains.
Some DBMS, however, only support one file organisation technique.
Many will use compression techniques which will reduce the amount of data storage needed for any collection of stored data and thereby, and more importantly, reduce access times because the data requires less space and therefore fewer input-output operations.
Chapter 6
Implementing Applications
6.1 EXTERNAL VIEW OF DATADASE
As reference to Figure 6.1 (a modification of figure 5. 1) makes clear, the external view of the database is that which each user ‘sees’.
lt is derived from and is a subset of the logical schema (Chapter 4), which is the overall view of the database in a form required by the DBMS used, and which is itself a mapping from the conceptual schema (Chapter 3).
The external schema is the subset of the database which is relevant to the particular user, and though it may be a summarised and a very restricted subset, the user may think that it represents the whole view, because it is the whole view as far as the user is concerned.
Its format may also be different from the logical schema.
The presentation and sequence of the data will also suit the context in which it is presented.
The format will depend on the particular host language, query language, report writer or other software used.
Indeed, there may well be several external views, perhaps as much as one per application or user that accesses the database.
Different user languages will be described in the context of particular DBMS in Chapter 7.
But whatever the description given to the user, the underlying data described by the logical schema will be the same.
ln other words, there is program data independence , and the description of the data in the progams can change without the descriptions in the database changing, and vice versa.
Readers should note that the ANSI/X3/SPARC architecture described in Section 1.8 is slightly different to that shown in Figure 6.1 as in that architecture the external view is mapped onto a conceptual view.
There is no intermediary logical view as we have in our methodology (this text describes a methodology for designing databases and developing database applications, whereas the ANSI/X3/SPARC proposals describe an architecture to view database applications).
As we saw in Chapter 4, the logical schema proves to be a useful stage for database design and separates the conceptual model, which is independent of any hardware/software factors, from the physical model.
External views may be presented to the user through the use of host language programs or a query language but they may also be obtained through a dialogue, which approaches a natural language dialogue, or via a menu.
In figure 6.2, the users pass through a number of menus to indicate which system and which part of the system they require to use.
The options are provided in the menu: the user has only to select the   option required by pressing the appropriate key (following the question mark).
Fig. 6.2 Sequential menus
Fig. 6.3 Form processing
Alternative approaches which are also easy to use include a soft copy form presented to the users who ‘fill in the forms’ to state their requirements (see Figure 6.3) or icon/mouse systems, which use graphical symbols (icons) representing such requirements as filing (file cabinet), deleting (wastepaper basket) and so on.
The user specifies the option required by pointing the mouse to the icon and pressing a button on the mouse to activate that process.
In this chapter we look at the ways to develop the applications which use the database, including an analysis of data requirements, and in Chapter 7 we look at setting up different external views in the context of different DBMS.
Section 6.2 looks again at conventional applications development, but in the overall context of a database approach, and then we look at alternative ways of developing applications, again in a database environment.
In Section 6.3 we look at structured systems analysis and design, in Section 6.4 at participation and in Section 6.5 at prototyping.
Finally, in Section 6.6 we look at Multiview, a blended methodology for developing applications, using the methods, techniques and tools which are appropriate for a particular situation.
Although not necessarily used with a database, it proves suitable in that environment and in Chapter 15 a case study is described of an application developed with a database.
6.2 THE LIFE-CYCLE REVISITED
In a database environment, many applications are large operational systems, such as sales order processing, sales ledger, production control, stock control, and invoicing.
These systems have to be designed and implemented.
In Section 1.2 some critical comments were made of conventional systems analysis.
Some of these criticisms  applied because of their file orientation.
This type of criticism should not apply in a database environment.
Here there should be a separation of data aspects from processing aspects.
Sets of data which may be required in a number of applications need not be collected more than once.
The data will be shared on the database.
further, modifications to applications are likely to be much easier to make, because files are external to the program.
Changes to the data may only involve changing the relevant parts of the database, and the many programs that may use the data will not need to be changed.
There are a number of points in favour of the conventional approach.
The six stages (feasibility study, systems investigation, systems analysis, systems design, implementation, and review and maintenance) associated with conventional systems analysis have been well tried and tested.
At the end of each stage the analyst and management have an opportunity to review progress in the development of the application.
There are a number of documentation aids to help the analysis, design and implementation processes, particularly in helping to ensure that the work is thorough and can be communicated to interested parties.
Another argument in its support is that many of the developments in systems analysis, such as structured systems analysis, participation and prototyping which are discussed later in this chapter, can be incorporated into the conventional approach.
This can make the conventional approach perfectly adequate for some systems development work.
But as we saw in Section 1.3 there are limitations to the approach.
There is a danger that the new system merely ‘computerises’ the clerical system used previously as the analysis phase is often neglected.
Frequently it is designed quickly in order to solve a problem that has occurred with the old system.
A ‘quick and dirty’ solution may be politically expedient but may well cause considerable problems, particularly with regard to the maintenance workload.
The new design may be unambitious, not tested properly, and unlikely to take full account of the potential of the technology used.
The conventional methodology tends to pay lip service to user involvement.
There is no mechanism to enforce user involvement and it is therefore often ignored or at best lip-service paid to it.
This frequently results in the system being rejected by the users.
This rejection of the system may be a result of poor analysis: the system analyst has not found out the user requirements.
The users may not have been involved to the extent of being able to see the repercussions of the system design they had ‘agreed’ with the analyst.
The documentation provided is oriented towards computer people and it is difficult for users to translate many of these forms used in conventional systems documentation into their ‘language’.
If systems are developed without the involvement of the users, they may be unwilling to accept the new systems because they feel they  have been imposed on them.
Systems may therefore be rejected as soon as they are implemented.
The database approach is long term and costly, and therefore top management should not risk its failure by sticking to outdated methods of applications development.
The failure of one application could jeopardise all applications.
For this reason it is essential to pay regard to different methods of applications development.
Participation and prototyping could be used independently in some applications and in others it may be appropriate to use the conventional approach modified by incorporating these tools and techniques.
Similarly, the techniques and tools of structured systems analysis can be incorporated into the conventional approach or can be seen as forming part of an alternative methodology for applications development.
6.3 STRUCTURED SYSTEMS ANALYSIS AND DESIGN
The term ‘structured’ is very fashionable in computing.
It has been adopted by many authors and consultants and it seems to mean many things, depending on the author and context.
However, the essential feature of structured techniques is the breaking down of a complex problem into smaller manageable units in a systematic (disciplined) way.
The techniques may use a number of pictorial methods for presenting results and these can be understood by users and analysts alike.
Structured techniques are relevant to systems analysis and design.
Systems analysis could be described as ‘what is’ and ‘what is required’whilst systems design is ‘what could be’.
Using the technique of functional decomposition , a very complex problem can be broken down into a number of fairly complex parts and then further to less complex parts until, at the bottom level, all the parts are fairly trivial and therefore easy to understand.
As can be seen in Figure 6.4, ‘produce weekly payroll’ at the top level can be broken into ‘validate weekly return’, ‘calculate gross wage’, ‘calculate deductions’and ‘print wages slip’.
Each of these boxes is separate and, when at the program level, can be altered without affecting the other boxes.
Each of these steps can be further broken down, a process also referred to as stepwise refinement.
For example, ‘calculate deductions’ can be broken down into ‘calculate tax’, ‘calculate national insurance contribution’and ‘calculate loan repayments’.
Eventually this ‘top down’ approach can lead to the level of a few simple English statements or a small amount of programming code.
This technique has had a great deal of impact in computer programming where it  in systems analysis and design.
Books by Gane & Sarson (1979) and DeMarco (1979) have had considerable impact.
To some extent these books propagate a series of techniques.
These techniques are a great improvement on more established techniques, such as flowcharting, which typify conventional systems analysis.
Most of these newer techniques of systems analysis and design have in fact been with us for some time, but were not brought together until the publication of the above texts.
The techniques include data flow diagrams, data structure diagrams, decision trees, decision tables and structured English.
The improvements on conventional systems analysis resulting from the use of these techniques is gained both from the point of view of understanding the real-world processes that they represent and in communicating the knowledge acquired.
Structured analysis documentation includes documents describing the logical (real-world) analysis of the processes and not just their physical (implementation) level designs.
In other words, there is a clear distinction between application logic and the computer representation of that logic.
There will be a separation of any data that the system is likely to want to input, output, process, or store, and the physical record, that will be part of the computer database.
The analysis documentation will include the system inputs, outputs and data structures as well as the processing logic.
Data Flow Diagrams (DFD) are a particularly useful aid in communicating the analyst's understanding of the system.
They do so by partitioning the system into independent units of a size that enable the system to be easily understood.
The user, whether the operator of the system when it is operational or the manager of the department it is aimed to help, can readily check that the DFD is accurate because it is so graphical.
DFDs can be converted to computer procedures.
There are four parts to the DFD.
This is standard, though there are a number of different graphical conventions in showing them.
The convention used here is suggested by Gane and Sarson.
The arrow represents a data flow from one part of the system to another.
In figure 6.5 an example of a data flow is the delivery notes passing from one supplier to the process ‘check note against order’.
A second part of the diagram is represented by ‘bubbles’ or rounded boxes which are used to denote processes.
A third element is the data store, such as the store of purchase orders in figure 6.5.
Fig. 6.5 Data flow diagram — top level and more detailed second level
A fourth part of data flow diagrams is ‘sources’ and ‘sinks’, usually external, such as a customer, a supplier or the Inland Revenue.
We are not interested in them as such  (they are outside the boundary of the investigation), but we are interested in what they give.
For example, it is not the customers themselves which are important in the context of this particular data flow, but their orders.
Similarly it is not the warehouse that is important, but the drain on stock.
These are represented by boxes.
One of the main features of data flow diagrams is the ability to construct them in levels (reflecting functional decomposition of the process).
For example, the process contained in a soft box in one level could be a whole DFD in another.
This is illustrated in Figure 6.5.
Note that the process numbered 1 in the top part of the diagram is represented by 1.1, 1.2, and 1.3 in the more detailed version below.
Further examples are found in Chapter 15.
Data flow diagrams prove to be a good communication tool and only the major features of systems are drawn to help clarify.
In drawing a data flow diagram, the analyst will start by drawing a ‘doodle’ attempt of the whole system, and a number of lower level ‘doodles’.
These are then refined iteratively to the analyst's satisfaction.
This version is then presented to the users for their comment.
Further iterations of the diagram are therefore likely before the final versions are produced.
Another technique used in structured methodologies is the structure chart or diagram (DeMarco, 1979).
The basic function diagram, for example, Figure 6.4, shows the links as a function is decomposed into its constituent modules.
Structure charts or diagrams are more sophisticated.
Fig.6.6 Structure diagram
The structure chart is a series of boxes (representing processes or modules) and connecting lines (representing links to subordinate processes) which are arranges in a hierarchy.
The basic diagram is shown as Figure 6.6.
This structure chart shows that:
Module A can call module B and also module C. No sequencing for these calls nor whether they actually Occur is implied by the diagraming notation.
When the subordinate process terminates, control goes back to the calling processes.
When A calls B, it sends data of type D to B. When B terminates, it returns data of  type E to A. Similarly, A communicates with C using data of types E and G.
When C terminates, it sends a flag of type F to A. A flag is used as a flow of control data.
A number of examples of their use is seen in Chapter 15.
Structured English, decision trees and decision tables are alternative methods of describing process logic.
They are appropriate in different circumstances but each provides simple, clear, and unambiguous ways of describing the logic of what happens in a particular process.
Natural language is ambiguous and long-winded and these techniques are much superior.
Structured English , for example, is very like a ‘readable’ computer program.
It is not a programming language though it can be readily converted to a computer program, because it is a strict and logical form of English and the constructs reflect structured programming.
Structured English is a precise way of specifying a function and is readily understandable by the systems designer as well as being readily converted to a program.
An example is given in Figure 6.7.
Structured English uses only a limited subset of English and this vocabulary is exact.
This ensures less ambiguity in the use of natural language English by the analyst.
further, by the use of text indentation, the logic of the process can be shown more easily.
Structured English has an:
IF condition 1 (is true)
THEN action 2 (is to be carried out)
ELSE (not condition 1)
SO action 1 (to be carried out)
construct.
Conditions can include equal, not equal, greater than, less than, and so on.
There are alternative languages such as‘Pseudo Code’, and ‘Tight English’.
These vary on their nearness to the machine or readability to users.
Fig. 6.7 An example of structured English
A decision tree illustrates the actions to be taken at each decision point.
The actions follow each decision point via a branch of a tree whereas each condition will determine the particular branch to be followed.
The techniques of showing decisions and actions are graphical and easy to understand unless it becomes so large that it is difficult to follow.
An example of a decision tree is given in Figure 6.8.
They are constructed by first identifying the conditions, actions, unless/however/but structures from narrative.
Each sentence may form a mini decision tree and these will be joined together to form the version which will be verified by the users.
Decision trees prove to be a good method of showing the basics of a decision, that is, what are the possible actions that might be taken at a particular decision point and what set of values leads to each of these actions.
It is easy for the user to verify whether the analyst has understood the procedures.
Fig. 6.8 Decision tree
Decision tables are less graphical but are concise and have an in-built verification mechanism so that it is possible to check that all the conditions have been catered for.
An example of a decision table is shown as Figure 6.9.
Decision tables can be used as computer input, programs being produced directly from them, and are a number of packages available for this purpose.
Figure 6.9 shows the decisions that have to be made by drivers in the U.K. at traffic lights.
The table has four sections.
The condition stub has the possible conditions ‘red’, ‘amber’ and ‘green’.
Condition Entries are either Y For yes (this condition is satisfied) or N for no (this condition is not satisfied).
Having three conditions, there will be 2 to the power of 3 (2 x 2 x 2 = 8) columns.
The easiest way of proceeding is to have the first row as YYYYNNNN, the second row as YYNNYYNN and the final row as YNYNYNYN.
If there were four conditions, we would start by eight Ys and eight Ns and so on.
All the possible actions are listed in the Action Stub.
An X placed on a row/column coincidence in the Action Entry means that the action in the action stub should be taken.
A blank will mean that the action should not be taken.
Thus, if a driver is faced with Red (Y), Amber (Y) and Green (Y), the first column indicates that the driver should  stop and call the police.
All combinations, even invalid ones, should be considered.
The next column Red (Y), Amber (Y) and Green (N) informs the driver to stop.
Only the Red (N), Amber (N) and Green (Y) combination permits the driver to go with caution.
Fig. 6.9 Decision table
In systems analysis, there are likely to be requirements to specify actions where there are a large number of conditions.
A set of decision tables is appropriate here.
The first will have actions such as‘GO TO Decision Table 2’.
Each of these may themselves be reduced to a further level of decision tables.
The technique therefore lends itself to functional decomposition.
Structured methodologies use techniques most of which can be followed by analysts and users alike.
In the past, systems analysts were first and foremost computer people.
Now, to some extent at least, they are talking a language that the rest of the organisation can understand.
This is important in getting the willing participation of users.
Most of the documentation aids are graphic representations of the subject matter.
This is much easier to follow than text which is long-winded and frequently ambiguous or file designs or programming flowcharts which are understandable only to the computer professional.
Gane & Sarson and other writers on structured analysis also advocate the use of data dictionaries (discussed in Chapter 12 of this text) and other tools of data analysis, such as normalisation (discussed in Sections 3.7 and 3.8).
Forms for detailing the data elements and data stores are also provided.
In this text, we have attempted to separate these data-oriented aspects, particularly data analysis and databases, from the process-oriented  aspects.
In practice, the distinction is never clear-cut.
As well as improved communication tools, the methodology usually incorporates structured walkthroughs.
These are team meetings where the analysis and design specifications and other documentation are exposed to review by the members of the team.
It is usual that they represent meetings of peers and that ‘management’ are not involved.
This is to avoid the type of criticism which may have repercussions on the team member's status or salary.
A peer review is likely to reveal errors, omissions, ambiguities and weaknesses in style, and also help to motivate and train the staff involved.
Questions that the peer group are likely to ask of a design could include:
Can bespoke programs use library routines?
Is the user interface simple, understandable, consistent?
Does the design fulfil the specification fully?
Will it work?
Structured walkthroughs should avoid the late detection of errors and flaws in logic and hence greatly reduce the risk of failure when the system is running.
Structured systems analysis and design need not only be seen as an alternative to the conventional approach.
The different authors of the approach do not view structured analysis in the same way.
Some regard the techniques as a useful alternative to many techniques of the conventional approach, and which therefore should be incorporated in the conventional approach.
Other writers, for example Gane and Sarson, emphasise a ‘structured methodology’ which should replace the conventional approach.
DeMarco regards these techniques as in the realm of analysis only and considers design to be a separate process where the analyst uses his experience and imagination to ‘invent’ a new system.
Gane and Sarson suggest that the techniques can be used to specify the system design and help in the implementation process.
They discuss query types and their implication for physical design.
They also ask users to construct a ‘wish list’ defining their requirements and providing a menu of alternatives.
Writers in the structured school also emphasise different techniques, although some techniques are common to all.
It is functional decomposition which really separates the structured systems approach from other approaches.
6.4 PARTICIPATION
This is a practical philosophy aimed at providing solutions to user problems.
The belief here is that people have a basic right to control their own destinies and that, if they are allowed to participate in the analysis and design of the system that they will be using, then the implementation, acceptance and operation of the systems are more likely to be successful.
ln the conventional systems analysis methodology, the importance of user involvement is frequently stressed although it is not ‘built in’ to the methodology as are, for example, the various documentation requirements.
However, the computer specialist is the person who is making the real decisions.
ln practice participation may mean (a)‘doing a public relations job to placate the staff and (b)‘once that is done train them to use the system’.
This is not what we mean by participation.
This section is about ways to encourage participation in the systems development process.
Systems analysts are trained in and knowledgeable of the technological and economic aspects of computer applications but far more rarely on the human aspects which are at least as important.
The person who is going to use the system frequently feels resentment.
further, top management may do little more than pay lip-service to computing.
When the system is implemented, the systems analyst may be happy with the system.
This is of little significance if it does not meet the expectations of the users, who are the customers.
This may be due to poor analysis — the analyst may not have understood fully the present system or the users’ requirements.
On the other hand, many systems may ‘work’ in that they are technically viable, but fail because of‘people problems’.
Users may feel that the new system will make their job less secure, will make their relationship with others change for the worse, or will lead to a loss of the independence that they previously enjoyed.
As a result of these feelings, users may do their best to ensure that the computer system will not succeed.
This may show itself in attempts to ‘beat the system’.
Frequently it manifests itself when people blame the system for difficulties that may well be caused by other factors.
Some people may just want to have ‘nothing to do with the system’.
These reactions against a new computer system may stem from a number of reasons, largely historical, but they will have to be corrected or allowed for if future computer applications are going to succeed.
Users may regard the computer department as having too great powers, and controlling (by technology) other departments which were previously controlled by people within the user department.
Computer people seem to have great status in the eyes of top management and are not governed by the same codes as the rest of the company.
Pay scales seem to be much higher for computer staff, particularly when seen in the light of the length of time that computer staff have worked with the firm.
In any case, the track record of computer applications — missed cutover dates, greater costs, fewer benefits, and designs which seem to be very different from that promised — should have led to reduced salaries and status, not  the opposite.
Some of these arguments are valid, others less ‘objective’, but the poor communications between computer staff and others in the organisation, caused by the use of jargon or a lack of training and education, have not helped.
Somehow these barters have to be broken down if computer applications are really going to succeed.
One way to help this process is to involve all staff affected by computer systems in the computing process.
This includes the top management of the organisation as well as departmental staff.
In the past, top management has avoided much contact with computer systems.
Managers have probably sanctioned the purchase of computer hardware and software but have not involved themselves with their use.
They preferred to keep themselves at a ‘safe’ distance from computers.
This cannot lead to successful implementation of computer systems: managers need to participate in the change and this will motivate their subordinates.
Attitudes are changing, partly because managers can see that without their involvement computer systems are unlikely to succeed, and because they believe that computer systems should be able to help them in their decision-making.
Earlier computing concerned itself with the operations of the firm; modern systems concern themselves with decision-support as well.
Without decision-support systems, firms will lose out to their competitors.
Managers are now therefore far more likely to demand sophisticated computer applications and to play a leading role in their development and implementation.
Communications between computer and non-computer people within the organisation must also improve.
This should establish a more mutually trusting and cooperative atmosphere.
Training and education of all staff in the organisation about computers are therefore important for the success of the computing venture.
Of course data processing staff should also be aware of the various operating areas of the business.
This should bring down barriers caused by a lack of knowledge and by technical jargon and encourage users to become involved in the technological change.
Involvement should mean much more than agreeing to be interviewed by the analyst and working extra overtime hours as the operational date for the new system nears.
If the users participate more, perhaps being responsible for the design, then they are far more likely to be satisfied with and be committed to the system once it is implemented.
It is ‘their baby’ as well as that of the computer people.
This environment is one where the users and analysts work as a team rather than as expert and non-expert.
There is less likely to be a misunderstanding by the analyst which causes a poorly designed system.
The user will also know how the new system operates by the time it becomes operational, with the result that there are likely to be fewer ‘teething troubles’ with the new system.
Borovits et al(1990) look at some of the benefits of participation on the finished system.
Two important aspects of group work are identified: communication patterns and working procedures.
Traditional models consist of the transfer of information from one person to another, for example, from user to designer and from manager to designer.
Working independently, there is little opportunity for discussion and feedback.
In contrast, participation is characterised by communication amongst all members of the group working together towards a common goal, in a friendly, informative atmosphere with open discussion and the swapping of ideas.
This may be an idealised picture, but group participation is likely to be more successful.
It is argued that participation will result in:
More efficient use of resources
The elimination of poor ideas more quickly
More accurate performance feedback as it comes from a number of sources
A higher level of commitment.
The role of the computer analysts may be more of facilitator than designer, helping to realise the users' wishes.
This can be aided by the use of application packages which the users can try out and choose what is best for them.
Another possibility if for the user or the analyst (or both) to develop a ‘prototype’.
There are prototyping packages available which will set up screen layouts and bring in blocks of code for validating and presenting data.
The users can compare all these possibilities and develop a prototype before making up their minds on a final design.
This can then represent the specification for the new system.
Prototyping is discussed fully in Section 6.5.
A very interesting case study relating to a successful application of participation at Rolls Royce is given in Mumford & Henshall (1979).
The approach recognises that the clerks who will eventually use the system ought to have a major design role.
This includes the form of work structure into which the computer aspects of the system are embedded.
The final system design is evaluated on the basis of job satisfaction of those working on it as well as its efficiency.
A likely result of taking job enrichment into account is the reduction of monotony usually associated with the clerical tasks of computer systems.
It is particularly interesting, however, to discover that a small group of white collar workers at Rolls Royce did not want an intellectually taxing job and provisions were made for them in the final design.
Mumford distinguishes between three levels of participation.
Consultative participation leaves the main design tasks to the systems analysts, but tries to ensure that all staff in the user department are consulted about the change.
The systems analysts are encouraged to provide opportunity for increasing job satisfaction when redesigning the system.
It may be possible to organise the users into groups to discuss aspects of the new system and make suggestions to the analysts.
Most advocates of the  conventional approach to systems development would accept that there is a need for at least this level of participation.
Representative participation requires a higher level of involvement of user department staff.
Here, the ‘design group’ consists of user representatives and systems analysts.
No longer is it expected that technologists dictate the design of the system to the users.
They have equal say in the decision.
Representative participation does assume that the ‘representatives’ can represent the interests of all the users affected by the design decisions.
Finally,consensus participation attempts to involve all user department staff throughout the design and development of the system.
It may be more difficult to make quick decisions, but it has the additional merit of making the design decisions those of the staff as a whole.
This is the approach particularly favoured by Mumford, although it is not always appropriate.
Sometimes the sets of tasks in a system can be distinguished and those people involved in each task set make their own design decisions.
Frölich & Krieger (1990) report on a survey of participation in Europe following 4654 interviews.
This extends Mumford's three levels of participation to five:
No worker participation: management plans and implements new technology without any participation.
Information: management informs employees in writing or at joint meetings.
Consultation: joint committees are set up where users are given information and can state their opinions, and in the case of differing view, can expect management to explain its reasoning.
Negotiation: contractually binding conclusions are worked out in joint negotiation committees.
Full participation: decisions require the agreement of all parties.
In the survey, one of the two higher forms of participation was recorded by only 20% of those surveyed.
About half thought that their organisation used the second form of participation, that employees were kept informed.
Although not a high form of participation, employees were likely to be informed of the reasons for change, how and when it will be implemented, what the new system will mean to them and its benefits.
Even where the level of participation was highest, its emphasis was stressed at the implementation, operation and evaluation phases of the development of a project, very rarely in any earlier planning phase.
The report suggests that the time taken for decision-making and the quality of the decisions were largely unaffected by participation, but the utilisation of skills, the general feeling of empathy with the organisation, the working atmosphere and the employees' acceptance of technology all improved in general.
However, many organisations require a change in their culture (Section 2.3) to enable effective participation.
Although the human and social aspects of change are emphasised in this section, the technical aspects are also important.
As Hirschheim & Newman (1988) argue:
‘Resistance is more likely to occur in systems which are cumbersome, ‘unfriendly’, unreliable, lack functionality, and slow.
If users find the technical quality of the system to be low, they are unlikely to welcome it, with the result that they will be disinclined to use it.’
One approach which can support user participation is that of the information centre.
By giving users ready access to appropriate tools and guidance from the data processing department, user departments could, it is proposed, develop systems themselves.
A user request for a system may lead to standard data processing work — procedural language solutions, high volumes of data to be processed, critical response times…and a long development time.
But the user request may alternatively be followed by advice on how users can themselves make use of facilities available, such as microcomputers, terminal access, query languages, and other packages, such as report generators, graphics, spreadsheet and file management systems, so that users can design their own technical system.
One of the very positive aspects of the information centre is that is provides choice rather than immediate solutions (which may not match the requirements).
A number of alternatives are provided and the user exercises choice between these alternatives which will have social as well as technical repercussions.
User groups can discuss social issues such as retraining, new departmental structures and new job responsibilities as well as issues such as terminals and the particular package required.
In view of this, it should be perequisite for users to decide on the social/human and technical objectives before approaching the information centre.
Only then will the users be able to rank particular solutions.
The information centre needs to be a centre of education — what computers can and cannot do, the importance of good standards, security provision and so on— as well as a source of training, reference material and advice on particular problems or the appropriateness of particular hardware and software solutions.
Data processing professionals are much more facilitators in this environment, and they may be brought in to assist on tasks such as detailed debugging.
In the first chapter we discussed the problems of the application backlog, where user departments may have to wait years for the implementation of systems.
The information centre alternative can speed up the development of applications and maintenance (as the users should be able to maintain the systems as well as develop them).
On the other hand, care has to be taken to prevent the scope of applications being limited, as users cannot be expected to know all the possibilities that computers offer.
Furthermore, there is a danger that if testing, documentation, validation, back-up  and recovery procedures are not thorough, the quality of the final systems will be poor.
This will reduce the possibilities of sharing data, with consequent data duplication and the possibility of inconsistency.
The role of the facilitators is therefore crucial.
It should not be the intention of the data processing department to ‘wash their hands of the applications’ if the major role in decision-making related to their development rests with the users.
The participative approaches to information systems development discussed in this section are pragmatic but they require consensus on the part of the technologists and users.
Sometimes the technologists may ‘agree’ to the approach but resent it: they feel that they have lost their power in the firm.
Sometimes users resent the time taken from their usual work: they may ask why they should spend their time on ‘computers’ when they are employed to work on their application.
But it is surely generally accepted that some level of participation is necessary whatever approach to systems design and development is adopted.
Otherwise there is a very real risk that new systems will be rejected by the people who are expected to use it.