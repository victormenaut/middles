

ARTICLES
Immunogenicity of a supplemental dose of oral versus inactivated poliovirus vaccine
In many developing countries, the immunogenicity of three doses of live, attenuated, oral poliovirus vaccine (OPV) is lower than that in industrialised countries.
We evaluated serum neutralising antibody responses in 368 children aged 6 months and 346 children aged 9 months in Coôte d'Ivoire who had previously received three doses of OPV at 2, 3, and 4 months of age, and who were then randomised to receive a supplemental dose of OPV or enhanced-potency inactivated poliovirus vaccine (IPV) at the time of measles vaccination.
Although both vaccines increased seroconversion to all three poliovirus types, antibody responses were greater in the IPV group.
Among children with no detectable antibody at baseline, IPV was 2 to 14 times more likely than OPV to induce seroconversion (type 1, 80% vs 40% at 6 months [p < 0.001]and 81% vs 14% at 9 months [p < 0.001]; type 3, 76% vs 22% at 6 months [p < 0.001], and 67% vs 5% at 9 months [p < 0.001]).
Among children with detectable antibody at baseline, IPV was 1.4 to 7 times more likely than OPV to elicit 4-fold or more rises in antibody titre (p < 0.01).
Geometric mean titres (GMTs) to all three poliovirus types were also consistently higher among IPV recipients than in OPV recipients when measured 4–6 weeks and 13–17 months after vaccination.
Administration of a supplemental dose of IPV or OPV, which requires no additional visits or changes in the existing immunisation schedule, might improve protection against paralytic poliomyelitis in communities with suboptimum seroconversion rates after three doses of OPV.
Introduction
Widespread use of trivalent oral poliovirus vaccine (OPV) has almost eliminated paralytic poliomyelitis from industrialised countries.
However, the vaccine's effectiveness in developing countries, particularly in inducing serum neutralising antibodies against poliovirus types 1 and 3, has often been lower than expected.
There have also been poliomyelitis outbreaks despite high vaccine uptake, which suggests that strategies that rely exclusively on routine administration of OPV are inadequate to achieve global eradication of poliomyelitis by the year 2000.
Biannual national vaccination days and house-to-house visits to vaccinate children in high-risk areas have been successful in eliminating wild poliomyelitis infection from Latin America, and have become the cornerstone of the current global eradication strategy.
Nevertheless, supplements to the OPV regimen that might hasten eradication should also be evaluated.
One option would be the use of enhanced-potency, inactivated poliovirus vaccine (IPV), which induces detectable neutralising antibody in almost all recipients after three doses.
However, because of important limitations of IPV compared with OPV, which include low secretory antibody responses and absence of secondary spread to susceptible contacts, use of IPV alone in polio-endemic countries is thought unlikely to achieve eradication.
Combined regimens have greatly reduced the incidence of  poliomyelitis in the Israeli-occupied West Bank and Gaza, but the cost and complexities of this schedule have precluded use of both vaccines in other developing countries.
To assess the relative immunogenicity of one supplemental dose of IPV or OPV at the time of measles vaccination, we did a randomised trial in Abidjan, Coôte d'Ivoire, among children who had previously received three doses of OPV.
Subjects and methods
Population and study design
The study population consisted of 6-month-old and 9-month-old childen who, between October, 1990, and April, 1991, attended a large primary health-care centre for the poor in Abidjan. ‘9-month-old children’ entered the trial consecutively, at the time of their visit to the clinic for routine measles vaccination with standard-dose Schwarz measles vaccine.
These children were eligible to enter the trial if they were between 37 and 43 weeks of age and if their immunisation record indicated that they had received their first dose of OPV between 6 and 12 weeks of age and their second and third doses at 4 to 6 week intervals.
‘6-month-old children’ were eligible if they were between 24 and 30 weeks of age and had been immunised with OPV as above.
By use of a computer-generated list of codes, 400 9-month-olds and 406 6-month-olds were randomly assigned to receive one dose of IPV or OPV (6-month-old children also received Edmonston-Zagreb measles vaccine; results have been reported elsewhere).
Although the parents and the study staff were not blinded to the vaccine groups, parents gave informed consent before knowing which vaccine their child would receive.
Active follow-up of all children, which included home visits to the last known address, was done approximately 1 year later (March–April, 1992).
Vaccines
Single lots of OPV packaged in 10-dose vials and IPV packaged in single-dose syringes were used.
The potencies of each vaccine (as measured by an independent World Health Organization [WHO]collaborating centre) were 6.52, 5.50, and 5.93 log 1 50% tissue culture infectious dose (TCID 5 ) of poliovirus types 1, 2, and 3, respectively, per dose (OPV), and 37, 8, and 30 D-antigen units per dose of types 1, 2, and 3, respectively (IPV); these potencies were within the range recommended by WHO.
Cold-chain monitors were used during shipment and storage.
Vaccines were kept in a cold room in Abidjan, and limited supplies were kept at the clinic in a refrigerator that was used exclusively for this study.
Temperature was monitored and recorded twice daily at both storage sites, according to standard WHO procedures.
No breaks in the cold chain were documented at the central storage facility, although vaccines kept at the study site were discarded on two occasions after power failures that lasted several hours.
No more potency testing was done during or after the trial.
Specimen collection and testing
Blood specimens were obtained from each child by venepuncture at the time of vaccination and then 4 to 6 weeks and 13 to 17 months later.
Sera were separated by centrifugation within 8 h of collection, transferred to another container, and labelled with a unique identification number so that antibody assays could be done blind.
Samples were stored at -20°C until they were shipped on dry ice to the Centers for Disease Control and Prevention (CDC), Atlanta, for testing.
Neutralising antibody titres against poliovirus types 1, 2, and 3 were measured with a modified microneutralisation assay.
Approximately 80 to 100 TCID 5 of each vaccine virus and serial dilutions of serum (from 1:8 to 1:1024) were incubated at 35.8C for 2.5 h, and then 7.5×10 HeLa cells were added to each well.
After incubation for 3 days at 35.8°C, each microtitre plate was stained with 0.05% crystal violet in 25% ethyl alcohol.
Optical density in all wells was measured by an enzyme-linked immunosorbent assay reader to minimise variability in interpretation and to eliminate errors in transcribing results.
Each specimen was run in triplicate, and the final result was determined by the method of Spearman-Karber.
The sensitivity of reproducibility of the assay had been established previously by direct comparison with the standard microneutralisation test and with international reference sera.
Statistical analysis
Differences in antibody responses between the study groups were assessed by comparison of (i) rates of seroconversion (defined as a change in antibody titre from undetectable [<8]to detectable [8];(ii) rates of secondary 4-fold or greater rises in antibody titre in seropositive children for whom additional 4-fold rises could be reliably documented with a neutralisation upper limit of 1:1024 — ie, children with baseline titres of 8–256; and (iii) type-specific geometric mean titres (GMTs) among the total study population and among children who seroconverted or had secondary 4-fold or greater rises in antibody after vaccination.
Overall seroprevalence and GMTs 13–17 months after vaccination were extrapolated from titres measured in the 164 participants who could be located in March–April, 1992, in proportion to the serological state of children within each age group and vaccine subgroup at baseline and at 4–6 weeks after vaccination.
Chi-squared, Fisher's exact, Student's t, and Kruskal-Wallis tests were used to detect significant differences among vaccine groups; 95% confidence intervals (CI) for proportions were calculated on a logit scale and converted to the original scale, and 95% CIs for GMTs were calculated with the method of normal approximation.
Results
Study participants
Of the 806 children enrolled in the study, we obtained a second serum specimen from 714 (88.6%) 4–6 weeks after vaccination, and a third specimen from 164 (23%) 13–17 months later.
At the time of entry into the study, overall seroprevalences among 6-month-old and 9-month-old children, respectively, were 75% and 84% for poliovirus type 1 (p < 0.01), 92% in both age groups for type 2, and 62% and 75% for type 3 (p < 0.001).
Within each age group, however, there were no differences between OPV and IPV subgroups (table I).
The proportion of children who could be found 13–17 months after vaccination was also similar in  each subgroup (table I), with no significant differences in demographic characteristics when compared with children whom we could not locate (data not shown).
No paralyses or other serious adverse events after OPV or IPV were reported at the 4–6 week visit.
4–6 weeks after vaccination
Figs 1 and 2 illustrate the effect of supplemental IPV and OPV on the immune state of the whole study population, pre and post vaccination seroprevalences, and GMTs for all 714 children.
In both age groups, increases in the proportion of children with detectable antibody to poliovirus types 1, 2, and 3 were significantly greater after vaccination with IPV than with OPV, even after adjustment for minor differences in baseline seroprevalence.
In 6-month-old IPV recipients (fig 1), overall seroprevalences 4–6 weeks after vaccination increased from 73% to 94% (21%) for type 1, from 93% to 100% (7%) for type 2, and from 67% to 89% (22%) for type 3.
The OPV recipients' increases were from 76% to 85% (9%) for type 1, from 91% to 95% (4%) for type 2, and from 57% to 65% (8%) for type 3.
In 9-month-old IPV recipients (fig 2), overall seroprevalences 4–6 weeks after vaccination had increased from 85% to 97% (12%) for type 1, from 93% to 100% (7%) for type 2, and from 76% to 92% (16%) for type 3.
Increases in the OPV recipients were from 83% to 85% (2%) for type 1, from 91% to 94% for type 2 (3%), and no change (75%) for type 3.
Pre versus post vaccination GMTs at 4–6 weeks of all three poliovirus types also increased significantly among 6-month-old and 9-month-old children who received IPV, with smaller increases among those who received OPV (figs 1 and 2).
The responses to measles vaccine were equivalent in the two vaccine subgroups.
13–17 months after vaccination
In both age groups, the estimated (projected) proportion of children who remained seropositive 13–17 months after vaccination tended to be higher in the IPV group than in the OPV group, although few of these differences were statistically significant.
Among 6-month-olds (fig 1), estimated seroprevalences among children who received IPV were 96% for type 1, 100% for type 2, and 92% for type 3, compared with 89% (p < 0.01), 99% (p=NS), and  76% (p < 0.01) for types 1, 2, and 3, respectively, among children who received OPV.
Seroprevalences for types 1, 2, and 3 among 9-month-olds (fig 2), were 100%, 100%, and 91%, for IPV recipients, and 96%, 95%, and 90% among OPV recipients (p=NS for all comparisons).
The extent to which wild type 1 and 3 infections may have contributed to rapid increases in seroprevalences in OPV recipients between 4–6 weeks and 13–17 months after vaccination is uncertain, since clinical and virological surveillance for poliomyelitis was not done in Abidjan during the study.
Estimated (projected) GMTs of all three poliovirus types remained stable in both OPV and IPV recipients 13–17 months after vaccination (figs 1 and 2), although 2.4-fold reductions in GMTs to poliovirus types 1 and 3 were seen in 9-month-old children who had received IPV (fig 2).
Seroconversion in seronegative children
Of the 714 children, 301 (42.2%) had no detectable antibody against one or more poliovirus types at the time of vaccination.
As shown in table II, seronegative children who had received IPV were 2 to 14 times more likely to seroconvert than those who had received OPV; seroconversion rates in the IPV group were from 67% to 100%, compared with only 5% to 53% in the OPV group (p < 0.001).
GMTs 4–6 weeks after vaccination tended to be lower among IPV recipients for types 1 and 2, but were significantly higher for type 3, particularly among 6-month-olds (table II).
Most of the children who seroconverted and who could be located 13–17 months after vaccination remained seropositive (table III).
For those subgroups with numbers sufficient for comparison, there were no significant differences in seroprevalences or GMTs between children who received IPV or OPV (table III).
Antibody titre rises in selected seropositive children
As shown in table IV, rates of secondary 4-fold increases in antibody titre in children with baseline titres between 8 and 256 were significantly higher for all three poliovirus types among children who received IPV compared with children who took OPV.
IPV recipients also had significantly higher post-vaccination GMTs to poliovirus type 3 (p < 0.001), but not to types 1 and 2.
Discussion Although the production of type-specific secretory antibody has been associated with protection against poliovirus infection, large-scale efficacy trials and more limited case-control or cohort studies conducted during outbreaks of poliomyelitis indicate that detectable serum neutralising antibody is correlated with clinical protection against disease.
For this reason, the variable and often unsatisfactory seroconversion rates in developing countries after routine administration of OPV are of concern, especially in tropical areas where wild poliovirus infection remains endemic.
The percentages of children in the present study who had detectable neutralising antibody against poliovirus types 1, 2, and 3 after three doses of OPV are similar to percentages reported from other developing countries, and contribute to the increasing evidence that routine vaccination with OPV at 6, 10, and 14 weeks of age might leave many children susceptible to infection.
Because of the association between clinical vaccine efficacy and detectable serum neutralising antibody, children who remain seronegative after three doses of OPV are of great importance in the control and eventual eradication of poliomyelitis.
Compared with a supplemental dose of OPV, one dose of IPV administered at the time of measles vaccination at 6 or 9 months of age induced significantly higher seroconversion to one or more serotypes at the time of vaccination.
Seroconversion rates in these children (67% to 100%) were much higher than rates after one dose of IPV administered to newborn infants (20% to 43%), which may be because of maturation of the immune system, the priming effect of previous OPV administration, or a reduction in interference from maternal antibody when IPV is given to older children.
Although seroconversion rates after a supplemental (fourth) dose of OPV were, in contrast, somewhat lower than expected, rates of secondary 4-fold rises in antibody (‘booster’ responses) in seropositive children who received OPV were similar to those found in earlier studies, which suggests that the low response rates in seronegative children were not due to vaccine subpotency.
Lower seroconversion rates in 9-month-olds than in 6-month-olds could be due to a higher incidence of infections with non-polio enteroviruses or other enteric pathogens, which are known to interfere with antibody responses to OPV.
Administration of IPV resulted in substantial increases in the seroprevalence and GMTs 4–6 weeks after vaccination, but there were few differences between the study groups when antibody titres were measured 13–17 months later.
These findings are unlikely to be due to delayed antibody responses in the OPV group, since peak antibody concentrations are usually attained within 2–4 weeks after vaccination.
Besides secondary exposure to vaccine virus, an explanation for high seropositivity rates in the OPV group was a high incidence of wild poliovirus infection during the study period; this hypothesis is supported by differences in seroprevalences to types 1 and 3 between 6-month-old and 9-month-old children at the beginning of the study.
Natural infection may have also contributed to the high seroprevalences in the IPV group 13–17 months after vaccination, although evidence from previous studies in poliomyelitis-free areas suggests that this finding is more likely to be due to brisk secondary responses in children who had been primed with OPV and who later received poliovirus antigen parenterally in the form of IPV.
Even if antibody concentrations began to decline 4–6 weeks after vaccination with IPV, the high titres achieved initially would be likely to persist throughout much of the period of maximum risk of paralytic poliomyelitis, which in most developing countries occurs between 6 and 18 months of age.
An important question that could not be addressed in this study is whether a single dose of IPV would also reduce the risk of infection with wild poliovirus and subsequent spread to susceptible contacts.
Although the effect of combined schedules on wild virus circulation has not been established, large reductions in the incidence of paralytic disease were achieved in the Israeli-occupied territories of the West Bank and Gaza following routine administration of two doses of IPV and five doses of OPV during the first year of life.
In addition, Sabin observed that seronegative children who were previously immunised with OPV often develop secondary immune responses when challenged with IPV, and have low rates of virus excretion when rechallenged with vaccine virus.
Other investigators have also shown that the magnitude and duration of excretion that follows a challenge with vaccine virus is correlated with the concentration of serum neutralising antibody.
Because IPV recipients typically have higher post-vaccination antibody concentrations than OPV recipients, particularly for types 1 and 3, a combined schedule might be expected to reduce the risk of wild poliovirus infection and spread to susceptible contacts.
Nevertheless, until direct evidence of such an effect has been obtained through the supplementary use of IPV, combined schedules should not be regarded as a substitute for the proven strategy of mass vaccination campaigns for the global eradication programme.
Although changes in global policy cannot be made on the basis of the present study alone, the results suggest that a supplemental dose of IPV or OPV administered at the time of measles vaccination might increase a population's immunity against poliovirus types 1 and 3.
Such increases can be achieved without additional visits in the routine immunisation schedule, and would also provide an added incentive for programme managers to ensure high rates of compliance at the 6 or 9 month visit for measles vaccination.
The incremental cost of implementing either regimen (US$0.08 per dose for OPV and US$0.35–0.50 per dose for IPV) is similar to or lower than the cost of mass vaccination campaigns (US$0.68–3.60 per dose), and would also be less than expansion of the routine immunisation schedule to provide even more doses.
Although its effect on the circulation of wild polioviruses has not been established, the incorporation of a single supplemental dose of IPV into vaccination regimens could substantially reduce the number of paralyses in countries where immunisation coverage with 3 or more doses of OPV is high (90%), but where immunogenicity studies have revealed large gaps in immunity to poliovirus types 1 and 3.
Although reductions in the number of clinically apparent cases may mask the circulation of wild polioviruses in the absence of highly sensitive surveillance systems, and potentially delay implementation of control measures, further advances in laboratory techniques to detect wild poliovirus in the environment should increase the likelihood of detection of ‘silent’ spread of infection.
In countries that have not yet achieved high immunisation coverage with OPV, resources would be better spent in raising a population's immunity through a combination of improved routine services and the initiation of mass campaigns.
This study was supported in part by grant no AID 698-0421.81 from the US Agency for International Development to the Combatting Childhood Communicable Diseases Project.
The study protocol was reviewed and approved by the Direction Générale de la Santé Publique, Coôte d'Ivoire; by the Institutional Review Board of the CDC; and by the Secretariat Committee on Research Involving Human Subjects, WHO.
Vaccines were kindly donated by Smith Kline Beecham (OPV) and Pasteur-Merieux Serums et Vaccins (IPV).
Expression of endothelin-1 in lungs of patients withcryptogenic fibrosing alveolitis
The vasoconstrictor and mitogenic peptide endothelin-1 (ET-1) is believed to play a part in fibrosis and collagen production.
We examined expression of ET-1 in lung tissue from 52 patients with interstitial lung fibrosis, of whom 45 had cryptogenic fibrosing alveolitis (CFA), 10 had CFA and concomitant pulmonary hypertension, and 7 had non-specific focal fibrosis. 17 normal unused donor lungs were studied as controls.
Immunohistochemistry and in-situ hybridisation were done with polyclonal antisera to ET-1 and its precursor big ET-1, and complementary RNA probes for preproET-1.
Normal lung tissue and that from patients with focal fibrosis expressed very little ET-1.
By contrast, there was striking expression of ET-1 in lung tissue from patients with CFA.
Immunostains for ET-1 and big ET-1 and expression of ET-1 mRNA were most prominent in airway epithelium and type II pneumocytes, particularly those lining areas of young granulation tissue.
ET-1-like immunoreactivity and mRNA were also present in pulmonary vascular endothelial cells, particularly in specimens from patients with pulmonary hypertension.
In all patients, there was a significant correlation between ET-1-like immunoreactivity and histological parameters of disease activity (r=0.78, 95% CI 0.65–0.87, p < 0.001).
These findings suggest a possible role for cell-specific expression of ET-1 in the pathogenesis of CFA and associated pulmonary hypertension.
Introduction
Cryptogenic fibrosing alveolitis (CFA) is a fatal condition of unknown cause.
Common functional abnormalities of this condition include reductions in forced vital capacity (FVC), carbon monoxide diffusing capacity (DLCO), and oxygen partial pressure.
The pathology of CFA is characterised by inflammation, type II pneumocyte and fibroblastic proliferation, and collagen deposition.
Macrophages and other inflammatory cells produce several cytokines and growth factors that could contribute to the early pathogenesis and progression of CFA.
The pulmonary epithelium itself produces growth factors that may influence the proliferation of underlying mesenchymal cells in the alveolar wall.
A putative growth factor expressed in the respiratory epithelium is endothelin-1 (ET-1), a vasoconstrictor and mitogenic peptide.
ET-1 belongs to a family of peptides (endothelins 1, 2, and 3) that possess close structural and biological similarities.
ET-1 is produced by transcription and translation as a 212-residue precursor (prepro) molecule, which is subsequently processed to ‘big ET-1’ and finally cleaved by an endothelin-converting enzyme to ET-1.
Binding sites for ET-1 and its receptor mRNA are found in the airways and pulmonary vasculature of various species including man.
Apart from its vasoconstrictive and bronchoconstrictive characteristics, ET-1 is a mitogen for smooth muscle cells, fibroblasts, and other cell types.
ET-1 mRNA and mature peptide have been localised to the pulmonary epithelium of healthy subjects and those with asthma.
We have demonstrated expression of ET-1 in pulmonary endocrine cells, and reported that ET-1 expression is increased in the lung vasculature of patients with pulmonary hypertension, which may contribute to the medial hyperplasia and intimal fibrosis of this disorder.
In the latter study, we also noted prominent type II pneumocyte expression of ET-1 in a small number of patients with pulmonary hypertension and fibrosis.
In view of the biological and pharmacological activities of ET-1, we wondered whether production of ET-1 by respiratory epithelium may contribute to cellular proliferation and fibrosis in CFA.
We describe a study designed to determine the sites of ET-1 expression in patients with CFA, and to correlate these findings with histological changes in the disease.
Patients and methods
Patients
The study was approved by the ethics committees of the participating hospitals.
Lung-tissue specimens were collected from 52 patients who had undergone open lung biopsy or lung transplantation and had a clinical and/or histological diagnosis of interstitial lung fibrosis, and, as a control group, from 17 lungs intended for transplant but unused.
The patient population was divided into three groups: group A consisted of 35 patients with CFA only, group B of 10 patients with CFA and pulmonary hypertension, and group C of 7 patients with various lung disorders other than CFA or pulmonary hypertension (usually with focal non-specific fibrosis).
Patients in groups A and C had no clinical indications of pulmonary hypertension in their records and no flexiform lesions in histology sections.
Patients' final diagnoses were based on histological and clinical findings (table I).
Tissues were obtained at open lung biopsy or at the time of transplantation, and fixed in formalin for immunohistochemistry and in 4% paraformaldehyde in phosphate-buffered saline (PBS) for in-situ hybridisation.
Formalin-fixed tissues were embedded in paraffin.
Paraformaldehyde-fixed tissues were washed in a 15% solution of sucrose in PBS, placed in Tissue-Tek embedding medium, and stored in liquid nitrogen.
Immunohistochemistry
Multiple-step paraffin sections (4–5 m) were immunostained with two ET-1 antisera — one for the c-terminal pepetide of ET-1 and one for the c-terminal peptide of big ET-1 (big ET-122–38)— µby the avidin-biotin-peroxidase complex method.
Rabbit antiserum to von Willebrand factor VIII (Dakopatts, Glostrup, Denmark) was used as a marker for endothelial cells.
Sections were dewaxed in toluene, dehydrated in ethanol, washed in PBS, and immersed in a solution of 0.3% hydrogen peroxide in PBS to eliminate endogenous peroxidase activity.
Sections were washed three times for 5 min in PBS and incubated in 10% normal goat serum for 30 min.
The serum was drained and sections incubated with ET-1 antiserum overnight at 4°C.
After three 5 min washes in PBS, sections were incubated with biotinylated goat anti-rabbit IgG antiserum for 45 min, washed in PBS, and incubated with avidin-biotin-peroxidase complex (Vector Laboratories, Burlingame, California, USA) for 45 min.
Sites of immunoreaction were visualised by immersing sections in a solution of diaminobenzidine and hydrogen peroxide.
Sections were counterstained with haematoxylin, cleared, and mounted in Permount.
As a control, sections from all patient and control lungs were stained with ET-1 antiserum that had been immunoabsorbed with synthetic ET-1 overnight at 4°C, or incubated with normal goat serum instead of the primary antisera.
Additional lung sections were stained with haematoxylin and eosin for histological diagnosis and semi-quantitative grading, the grading being done by a modification of the system of Hyde et al.
Specifically, sections were graded from 0 to 4 for chronic inflammation, granulation tissue, type II pneumocyte hyperplasia, dense fibrosis, and pulmonary-vessel-wall changes.
Grade 1 included focal mild changes, whereas grades 2, 3, and 4 represented diffuse mild, moderate, and severe lesions, respectively.
The airway epithelium, type II pneumocytes, and vascular endothelium of ET-1-stained sections were also graded semiquantitatively from 0 to 4, with grade 0 representing no staining, grade 1 focal staining, and grades 2, 3, and 4 diffuse weak, moderate, and strong staining, respectively.
Grading was done and agreed upon by four pathologists (A. G., R. P. M., Q. H., M. S.) without knowledge of the patient's diagnosis.
In-situ hybridisation
In-situ hybridisation was by a modification of a method reported previously.
Briefly, cryostat sections were picked up on poly-L-lysone-coated slides and dried at 37°C for 2 h.
Sections were permeabilised in a solution of proteinase K and the reaction stopped by immersion in 4% paraformaldehyde.
To prevent non-specific signals, sections were washed in a solution of acetic anhydride and triethanolamine.
Sections were then dehydrated in ethanol and air dried.
Hybridisation of sections took place in a mixture containing 1×10 counts per min per section of S-labelled ET-1 complementary RNA probe for 14 h at 42°C.
Sections were then washed in decreasing concentrations of standard sodium citrate, and unhybridised probe was removed by incubation in a solution of RNAse A (20 mg/mL) in double-strength standard sodium citrate.
After dehydration in decreasing concentrations of alcohol, sections were air dried, dipped in autoradiographic emulsion (Amersham), and exposed in light-tight boxes for one week at 4°C.
Consecutive sections were stained with haematoxylin and eosin for histological evaluation of in-situ hybridisation results.
As a control, sections from patient and control lungs were hybridised with a probe with a sequence identical to that of the coding ET-1 mRNA (sense probe), or treated with RNAse A solution before hybridisation with the ET-1 complementary RNA probe.
Statistical analysis
Results were expressed as means (SE).
We used the Systat program (Systat, Evanston, USA) for statistical analyses by analysis of variance and the Tukey post-hoc test.
Correlation coefficients were calculated by Spearman rank correlation test and, in selected instances, the chi square test.
Statistical significance was taken to be p < 0.05.
Results
Most patients were hypoxaemic, had a reduced DLCO, forced expiratory volume in 1s (FEV 1 ), and FVC, and a high FEV 1 /FVC.
Patients in group B had high pulmonary arterial pressure and pulmonary vascular resistance (table I).
Patients in groups A and B showed moderate to severe interstitial fibrosis (fig 1).
The architecture of the lung parenchyma was disturbed with rearrangement and simplification of the airspaces, which were separated by wide bands of fibrous connective tissue with variable degrees of chronic inflammation.
The interstitium was thickened by either granulation tissue or dense fibrosis and in the worst cases showed honeycombing.
Pulmonary arteries showed an arteriopathy that varied in severity and extent, often demonstrating medial thickening and eccentric intimal proliferation.
Semi-quantitative grades for all histological parameters except chronic inflammation were higher for groups A and B than for group C (fig 1).
The only notable difference between groups A and B was the more severe arteriopathy in group B. When the histological parameters associated with active disease (ie, granulation tissue and type II pneumocyte hyperplasia) were summed, values were 4.6 (0.4) and 4.0 (0.6) in groups A and B, respectively, and 1.0 (0.5) in group C.
Endothelin-1-like immunoreactivity (ET-1-IR) was seen only occasionally in the pulmonary vascular endothelium and neuroendocrine cells of the control lungs, and there was no staining over other epithelial cells.
In patients with CFA (groups A and B), ET-1-IR localised predominantly to hyperplastic alveolar type II pneumocytes, undifferentiated epithelial cells, and small airway epithelial cells (fig 2, panels A–C).
Epithelial cells lining the airspaces adjacent to granulation tissue showed greater ET-1-IR (fig 2, panels A and B), whereas those adjacent to areas of dense fibrosis usually showed a weaker immunoreaction (fig 2, panel C).
ET-1-IR was also seen occasionally in inflammatory and neuroendocrine cells.
Variable ET-1-IR was seen in the vascular endothelium, this being particularly the case for Fig 2 — ET-1-IR in diseased human lungs.
ET-1-IR in airway epithelial cells (A) and type II pneumocytes (B, C) of patients with CFA; ET-1-IR in vascular endothelium and type II pneumocytes in a lung section from a patient with pulmonary hypertension secondary to CFA (D).
Only focal immunostaining for ET-1 was seen in lung sections from patients with focal non-specific fibrosis (E).
Panel F demonstrates expression of ET-1 mRNA in a lung section from a patient with CFA.
Magnifications: A, D, and F ×250; B, C, and E ×125. vessels with medial hypertrophy and intimal fibrosis (fig 2, panel D).
Specimens from patients with non-specific focal fibrosis (group C) usually showed weak focal staining in a few epithelial and endothelial cells (fig 2, panel E).
Most patients in group A had greater staining for big ET-1 than for mature ET-1.
No staining was evident in the control lung sections.
Semi-quantitative analyses of ET-1-IR in groups A and B showed similar magnitudes of epithelial-cell staining; however, vascular-endothelial-cell staining was greater in group B. Both groups with CFA had significantly more epithelial-cell staining than group C patients (fig 3).
We summed the ET-1-IR grades for all epithelial cell types (ie, airway epithelium and type II pneumocytes), and found  higher values in groups A (3.09 [0.4]) and B (4.0 [0.3]) than in group C (0.9 [0.5])(p < 0.01).
ET-1-IR in both airway epithelium and type II pneumocytes correlated significantly with young granulation tissue and type II cell proliferation (table II).
Arteriopathy grade correlated significantly with ET-1-IR in both epithelial and endothelial cells, whereas there was no significant correlation between either chronic inflammation or dense fibrosis and ET-1-IR in any cell type.
To analyse the correlations further, we summed the histology grades (young granulation tissue and type II pneumocyte proliferation) and correlated this with the summed airway epithelium and type II pneumocyte staining.
This correlation was highly significant by both Spearman rank correlation test (r=0.78, 95% CI 0.65–0.87, p < 0.001) and the chi square test ().
The distribution of ET-1 mRNA demonstrated by in-situ hybridisation was similar to the distribution of ET-1-IR.
ET-1 mRNA was expressed mainly in hyperplastic type II pneumocytes and airway epithelium (fig 2, panel F).
Comparison of hybridisation sections with consecutive sections stained with haematoxylin and eosin revealed clusters of silver grains deposited mainly over the epithelial cells adjacent to young granulation tissue.
By contrast, the density of silver-grain deposits over airway and alveolar epithelium underlined by dense fibrosis was only slightly higher than the background density.
Hybridisation signals were also detected in the vascular endothelium of pulmonary arteries, especially those with severe arteriopathy.
Only scattered signals were seen over sections from group C patients' lungs or normal lungs.
No hybridisation signals were seen over control sections.
Discussion
Our findings demonstrate for the first time increased expression of ET-1 in the lungs of patients with CFA compared with the lungs of subjects without CFA.
Alveolar type II pneumocytes and airways epithelial cells were the main sites of ET-1 expression, and the magnitude of immunostaining correlated well with the extent of type II pneumocyte proliferation and adjacent granulation tissue.
ET-1 was also expressed in the vascular endothelium of pulmonary vessels with severe morphological changes, this expression being particularly striking in lungs of patients with both CFA and pulmonary hypertension.
Therefore, increased expression of ET-1 in epithelial and endothelial cells may reflect a disease-specific activation of the cell types, which possibly contributes to the pathogenesis of CFA and associated pulmonary hypertension.
Semiquantitative analyses of ET-1 immunostaining in lung sections revealed significant differences between the control groups (group C and normal lungs) and patients with CFA (groups A and B).
Prominent staining was seen in the lungs of patients with CFA, which was localised predominantly to the airway epithelium and type II pneumocytes.
By contrast, only weak and infrequent staining was seen in group C or normal control lungs, a finding that is consistent with our previous results.
In addition, ET-1-IR grades for all patients correlated significantly with histological parameters of disease activity — ie, presence of granulation tissue and type II pneumocyte proliferation.
Previous studies have shown that ET-1 is a mitogen for fibroblasts and acts synergistically to amplify the response of these cells to other growth factors.
Furthermore, raised plasma ET-1 concentrations and ET-1-induced deposition of collagen by fibroblasts have been reported in patients with scleroderma and systemic sclerosis.
Indeed, ultrastructural features of CFA include, in the exudative phase, loss of type I pneumocytes and exposure of the underlying basement membrane, and, in the proliferative phase, increased contacts between epithelial cells and fibroblasts.
Moreover, Adamson et al found that in epithelial-cell-fibroblast cocultures, direct contact between these cells greatly increased the rate of fibroblast growth.
Thus, the proliferation stage of CFA may depend to large extent on mitogenic factors produced by the epithelium, including transforming growth factor, tumour necrosis factor, and ET-1, acting synergistically to stimulate fibroblast growth and extracellular production.
Increased ET-1 production by the epithelium could also have an autocrine role, by promoting proliferation of type II pneumocytes.
There are several possible mechanisms for increased ET-1 expression and for a role for this molecule in the pathogenesis of CFA.
Bronchioalveolar lavage samples and lung tissue from patients with CFA show increases in polymorphonuclear leucocytes and macrophages.
These cells release enzymes, lymphokines, cytokines, and growth factors that can act on parenchymal and mesenchymal cells.
Some of these products have been shown to induce ET-1 synthesis and release in various cell types.
Endo et al have shown that tumour necrosis factor, transforming growth factor, and interleukin-8 increase ET-1 synthesis in guineapig tracheal epithelial cell cultures.
Therefore, increased ET-1 expression in epithelial cells may be mediated, at least in part, by cytokines released from inflammatory cells.
Patients in group B had more severe arteriopathy than those in group A and greater staining of the endothelium.
These findings are in agreement with our previous observations of increased ET-1 expression in pulmonary arteries with severe morphological changes in patients with pulmonary hypertension.
In addition to its potent vasoconstrictive actions, ET-1 is also a mitogen for vascular smooth muscle cells.
Therefore, our data support the notion that ET-1 may contribute to the functional and morphological abnormalities of pulmonary vasculature associated with CFA.
To determine whether the ET-1-IR was the result of increased local synthesis, and not of uptake of ET-1 from the circulation, in-situ hydridisation with a complementary RNA probe was done.
Specific cellular localisation of silver  grains, indicating the presence of preproET-1 mRNA, was found over the same cells that demonstrated ET-1 immunostaining.
Most patients in group A showed greater immunostaining with big ET-1 than with ET-1 antisera, suggesting a predominance of precursor and providing evidence of increased local synthesis of ET-1 in the lung of patients with CFA.
The abundance of big ET-1 suggests that conversion to the mature peptide by endothelin-converting enzyme may be a rate-limiting step in the processing of ET-1 in lungs.
Alternatively, this abundance may be a feature of the cytokine-stimulation profile, since interleukins 1, 2, and 6 have been shown to increase the relative production of big ET-1 to ET-1 in cultured tracheal epithelial cells.
Our demonstration of increased expression of ET-1 mRNA and production of ET-1, particularly the propeptide, by alveolar type II pneumocytes and airway epithelial cells points to a pathogenic role for this peptide in the morphological changes characteristic of CFA.
Moreover, the finding that increased ET-1 expression is closely related to the active stage of the disease, which has the most favourable prognosis, suggests that ET-1 is a marker of disease activity.
With the availability of effective pharmacological antagonist of the endothelin system in the very near future, it may soon be possible to test directly the contribution of ET-1 to the progression of disease in patients with CFA.
Randomised controlled trial of nasal ventilation inacute ventilatory failure due to chronic obstructiveairways disease
Acute exacerbations of chronic obstructive airways disease (COAD) are a common cause of admission to hospital, and have a high mortality.
Nasal intermittent positive pressure ventilation (NIPPV) has been used successfully in patients with respiratory failure due to neuromuscular and skeletal disorders, but the outcome of treatment in patients with COAD is less well known.
We carried out a prospective randomised controlled trial of conventional treatment versus conventional treatment plus NIPPV, in 60 patients with acute ventilatory failure due to exacerbations of COAD.
For the NIPPV group there was a rise in pH, compared with a fall in the controls (mean difference of change between the groups 0.046 [95% CI 0.06–0.02, p < 0.001]), and a larger fall in PaCO 2 (mean difference in change between the groups 1.2 kPa [95% CI 0.45 to 2.03, p < 0.01]).
Median visual analogue scores over the first 3 days of admission showed less breathlessness in the NIPPV group (2.3 cm [range 0.1–5.5]) than in the control group (4.5 cm [range 0.9–8.8])(p < 0.025).
Survival rates at 30 days were compared for intention-to-treat and efficacy populations.
In the efficacy mortality comparison, mortality in the NIPPV group was reduced: 1/26 vs 9/30 (relative risk=0.13, CI=0.02–0.95, p=0.014).
This effect was less in the intention-to-treat analysis: 3/30 vs 9/30 (relative risk=0.33, CI=0.10–1.11, p=0.106).
In patients with acute ventilatory failure due to COAD who received NIPPV there was a significant rise in pH, a reduction in PaCO 2 and breathlessness, and reduced mortality.
Introduction
For patients with chronic obstructive airways disease (COAD) admitted to hospital in ventilatory failure, mortality ranges from 6% to 34%, the most reliable predictors of mortality being increased age and respiratory acidosis.
Nasal intermittent positive pressure ventilation (NIPPV) has been used in patients with acute and chronic respiratory failure due to neuromuscular disease, chest wall deformities, COAD, cystic fibrosis, and obstructive sleep apnoea.
In a small controlled study, Ahmed et al showed an improved outcome for patients with COAD treated with NIPPV compared with the respiratory stimulant doxapram.
We have compared NIPPV with conventional treatment in patients admitted to hospital with ventilatory failure due to acute exacerbations of COAD.
Patients and methods
After ethical committee approval, we did a randomised controlled trial at King's College Hospital, Southampton General Hospital, and the London Chest Hospital (centres A, B, and C).
Patients gave informed consent, or if they were too ill to cooperate, the consent of a relative was obtained.
Patients admitted with an acute exacerbation of COAD and who were aged 80 or less, had an arterial PaO 2 <7.5 kPa, and an arterial PaCO 2 >6 kPa, were eligible for inclusion.
Patients were excluded if they had severe disease not attributable to chronic respiratory disease, severe psychiatric disease, or if they used NIPPV at home.
Patients were randomly allocated to receive either conventional treatment alone (control group), or conventional treatment plus nasal intermittent positive pressure ventilation (NIPPV group).
Conventional treatment was that deemed appropriate by the clinicians responsible: oxygen at 24–28%; inhaled bronchodilators; and all, or a combination of, antibiotics, diuretics, respiratory stimulants, intravenous or oral corticosteroids, and bronchodilators.
Patients were assessed and treated as necessary by a physiotherapist.
Volume-cycled nasal positive pressure ventilation was started as soon as possible in the NIPPV group with either a Lifecare PLV 100 (Lifecare, Lafayette, Colorado, USA) in the assist-control mode, or a Brompton-PAC (PneuPAC Ltd, Luton, Beds, UK).
Ventilation was through a silicone nasal mask (Respironics Ltd, Murrysville, Pennsylvania, USA; UK distributors: Medicaid, Pagham, Sussex) with O 2 at 1–2 L/min.
The ventilated group were encouraged to use NIPPV for up to 16 h per day, including all night, with ventilation discontinued for eating, drinking, and moving around.
As the patients improved, the duration of NIPPV was reduced, at first during the day, and then at night.
Arterial blood gases were measured on admission, after 1 h on allocated treatment, on day 3, and day 7, while breathing room air (except at 1 h after admission when the patient was using either O 2 or NIPPV).
Visual analogue scores (VAS) for shortness of breath, well-being, and quality of sleep were obtained from the patients; and for nursing care requirements from a senior nurse, daily until day 3, and then on day 7.
FEV 1 , FVC, and peak expiratory flow rate were measured during the hospital stay.
Patients were followed-up for at least 30 days.
Ventilation was started and VAS measured by physiotherapists and medical researchers not otherwise involved in the management of the patients.
Statistics
Arterial blood gas measurements were compared in the two treatment groups with unpaired t-tests on differences, and between the three centres using analysis of variance.
The VAS were averaged over the first 3 days of admission (any patient with data missing was omitted from the analysis) and these means compared by Mann-Whitney U tests.
Proportions of patients surviving 30 days were compared with Fisher's exact test.
Results
Patients and treatments
60 patients were entered in the trial: approximately 10 in each group at each of the 3 centres.
The 2 groups had similar ages, arterial blood gases, and spirometry.
Patients at centre C were less acidotic (pH 7.369 [SD 0.043]), than the patients at A,(pH 7.340 [SD 0.065]), and B (pH 7.310 [SD 0.087]), p < 0.05.
Mean PaO 2 and PaCO 2 were no different.
Although patients at centre C had a somewhat lower PaCO 2 (mean 8.2 kPa [SD 1.24]), compared with centre A (8.7 kPa [SD 1.65]), and centre B (8.9 kPa [SD 1.67]).
Of the 30 patients randomised to NIPPV, 4 did not receive it: 2 because they were confused and could not cooperate, 1 because he was unable to breathe through his nose, and 1 because he requested the withdrawal of all active treatment.
5 patients allocated conventional treatment were ventilated on the decision of the clinician in charge: 3 Figure 2 — Mean visual analogue scores.received NIPPV, and 2 were intubated (1 of whom died).
All patients received controlled O 2 and nebulised bronchodilators, and about 75% of patients in both groups received corticosteroids and antibiotics.
In the NIPPV group, 18 (60%) were given diuretics, 7 (23.3%) methylxanthines, and 4 (13.3%) doxapram.
In the control group, 13 (43.3%) were given diuretics, 14 (46.7%) methylxanthines, and 13 (43.3%) doxapram.
The 26 NIPPV patients received 7.63 (1–23) h of ventilation day, over 6.0 (2–9) days, taking 1.86 (0.25–4) h to settle on ventilation.
Median stay in hospital was 9 (1–39) days for the control group and 9 (1–22) days for the NIPPV group.
Arterial blood gases
There was a mean rise in pH in the NIPPV group from 7.348 to 7.376, and a mean fall in pH in the control group from 7.331 to 7.313 after one hour of treatment.
This was a mean difference in change of 0.046 (CI=0.06 to 0.02, p < 0.001)(fig 1).
PaCO 2 in the NIPPV group fell from 8.6 kPa to 7.3 kPa; and in the control, from 8.6 kPa to 8.5 kPa.
This was a mean difference in change of 1.2 kPa (CI=0.45 to 2.03, p < 0.01).
There was no statistically significant difference in arterial blood gas measurements between the two groups during the rest of the admission, although the values of the control group tended to be more variable for both pH and PaCO 2 .
Symptoms and nursing care
Visual Analogue Scores over the first 3 days showed a significantly lower score for breathlessness for the NIPPV group, median 2.3 cm (range 0.1–5.5), than for the control group, median 4.5 cm (range 0.9–8.8), p < 0.025 (fig 2).
The VAS of quality of sleep and general well-being, and for the amount of nursing care required, were not statistically significantly different.
Survival
Survival rates at 30 days were compared for an intention-to-treat and an efficacy population.
In the NIPPV group, mortality within 30 days was less than with the conventional treatment group: 1/26 vs 9/30 (relative risk=0.13, CI=0.02 to 0.95, p=0.014).
However, when analysed on an intention-to-treat basis with inclusion of the 4 patients who were not treated with NIPPV, 2 of whom died, the survival advantage was lower: 3/30 vs 9/30 (relative risk=0.33, CI=0.10 to 1.11, p=0.106).
Short-term survival differed between the centres.
In the first 30 days there were no deaths at centre C, 5 at B, and 7 at A (centre C vs centre A and B: Fisher's exact test p=0.005).
One patient (centre B) continued with NIPPV at home and remains well.
Patients who subsequently died were more acidotic on admission than patients who survived: pH 7.310 (SD 0.078) vs pH 7.347 (SD 0.067)(p=0.01); and more hypercapnic: PaCO 2 9.4 kPa (SD 1.45) vs 8.4 kPa (SD 1.51 (p < 0.05).
Both groups were equally hypoxic, PaO 2 5.1 kPa (SD 0.93) and 5.3 kPa (SD 1.05).
Discussion
Blinding in this study was impossible, as ‘sham’ ventilation with NIPPV cannot be done.
To minimise potential bias, the study investigators set up the ventilators but were not involved in the clinical management of patients.
There was a rapid improvement in pH and PaCO 2 for the NIPPV group, and for the first 3 days they were less  short of breath than the control group.
Although many patients were unable to complete a VAS on the day of admission, it is unlikely that the NIPPV group were less breathless as the groups were otherwise well matched.
Sequential data were not available for the patients in the control group who were intubated and ventilated, nor for the NIPPV patients who could not be treated with nasal ventilation.
Comparison of arterial blood gas tensions and VAS necessarily, therefore, excluded the most sick patients in the conventionally treated group.
This may have masked further treatment benefits of NIPPV.
Comparison of the data for the 26 patients who received NIPPV with that of the 30 treated conventionally demonstrated improved survival (1 death in 26 vs 9 in 30).
Analysed on an intention-to-treat basis the mortality of the two groups (3/30 vs 9/30) was not significantly different.
The mortality in patients randomised to receive conventional treatment alone, 9/30 (30%), was higher than in some previous reports.
The criteria for inclusion in the study ensured that all patients had severe COAD with ventilatory failure, and so could be expected to have a substantial mortality.
Centre C had no deaths during the study.
The patients at this centre were less acidotic on admission, indicating that they were less critically ill at the start of the study.
This may reflect differences in hospitals rather than patients since centre C has no casualty department and most patients are admitted direct to the ward.
Jeffrey et al have suggested that doxapram should be given to all patients who are acidotic and hypercapnic, although Ahmed et al found a poor outcome in 5 patients treated with doxapram in a controlled study comparing doxapram with NIPPV.
Patients in our study fared better with NIPPV, and in the conventional treatment group (43% of whom received doxapram) some patients had to switch to NIPPV.
Although the application of NIPPV on a general ward has been described as a difficult and time-consuming procedure for nurses we did not find this to be the case.
We conclude that this treatment should be offered to patients with an acute exacerbation of COAD who are acidotic and hypercapnic, compliant, with no nasal abnormality and in whom conventional therapy does not produce a prompt response.
This study was supported by a grant from the British Lung Foundation.
A table of patients' results is available from the corresponding author 
Placebo-controlled trial of essential fatty acid supplementation in atopic dermatitis
Treatment of atopic dermatitis with essential fatty acids remains controversial.
A double-blind, placebo-controlled, parallel-group study was done to investigate the response of patients with atopic dermatitis to essential fatty acid supplements.
Patients with atopic dermatitis were randomised to receive evening primrose oil, evening primrose oil and fish oil, or placebo for 16 weeks.
Disease activity was monitored by clinical severity scores recorded by the investigator, topical steroid requirement, and symptom scores recorded by subjects.
Of 123 subjects recruited, 102 completed the treatment period.
No improvement with active treatment was demonstrated.
Our study, which avoided the methodological and analytical problems of previous studies, found no effect of essential fatty acid supplementation in atopic dermatitis.
Introduction
Evening primrose oil contains n6 series essential fatty acids (EFAs).
Epogam, a product containing this oil, is widely prescribed in the UK for treatment of atopic dermatitis (AD).
Clinical trials of evening primrose oil in AD have been inconclusive, and the use of fish oil, a source of n3 series EFAs, to treat AD has also yielded conflicting results.
The possible mechanisms by which EFAs might improve inflammatory skin disease include modification of eicosanoid metabolism so as to favour synthesis of relatively non-inflammatory prostaglandins and leukotrienes.
Since both n6 and n3 EFAs may possess this property, it is possible that giving both together will have a synergistic effect.
Our study was designed to investigate whether evening primrose oil was effective in AD, and whether any additional response might be gained from the simultaneous administration of fish oil.
Patients and methods
Patients
The study had a double-blind, randomised, parallel-group design with three treatment limbs.
Subjects were examined at baseline, at 4, 8, and 16 weeks on treatment, and again after an 8 week washout.
Outpatients attending the dermatology department for treatment of AD, of either sex and all ages, were recruited.
The diagnostic criteria of Hanifin and Rajka were used.
To avoid a disparity in age distribution between treatment groups, adults and children aged up to 12 years were randomised separately in blocks of three.
Equal numbers of adults and children were recruited.
All gave written informed consent.
Treatments
Patients were randomised to receive epogam, Efamol Marine, or matching placebo (Scotia Pharmaceuticals Guildford, UK).
Epogam capsules contained 500 mg of evening primrose oil, largely made up of the n6 series EFAs linoleic (321 mg) and gamma-linolenic acid (40 mg).
Efamol marine capsules contained 430 mg of evening primrose oil and 107 mg of marine fish oil.
This fish oil included the n3 series EFAs eicosapentaenoic acid (17 mg) and docosahexaenoic acid (11 mg).
Placebo capsules contained liquid paraffin for adults and olive oil for children.
Six capsules of each treatment were given twice daily for 16 weeks.
Capsules were cut open, if necessary, to administer the contents to children.
Compliance was monitored by questioning patients and counting remaining capsules at each outpatient visit.
Patients taking less than 70% of their capsules (50% for children) were defined as non-compliers.
Patients were allowed to use topical steroids and emollients as required.
They were advised to use topical steroids only on severely affected areas of skin, in accordance with our usual practice.
Apart from changes in the quantity applied, no changes in topical steroids were allowed during the trial or the preceding 4 weeks.
Treatment with sedative antihistamines was continued throughout the study if they were in use on entry.
Assessments of disease activity
The following parameters were used to monitor disease severity.
Clinical severity scores — The body was divided into 10 zones: face, neck, abdomen, back, elbows, antecubital fossae, dorsa of hands, palms and wrists, popliteal fossae, and feet.
Each zone was scored on a scale from 0 (absent) to 3 (severe) for: erythema, excoriation, dryness, cracking, and lichenification.
Total scores from each zone were added to give a maximum of 150.
We have termed this system the ‘Leicester score’.
Because we had an interest in comparing methodologies, a second, simpler scoring system for clinical severity, as described by Costa et al, was also used.
In this system, only the most severely affected site was assessed.
Erythema, oedema, vesiculation, crusting, excoriation scaling, lichenification, pigmentation, pruritus, and sleep disturbance were each graded on a 7 point scale.
This score is only briefly reported.
Percentage of skin affected — This was assessed by the ‘rule of nines’, as for burns injuries.
Topical steroid requirement — To allow monitoring, topical steroids were prescribed by the investigator.
Usage was assessed by weighing returned tubes of medication.
Some patients who used potent corticosteroids also used a mild compound for the face, in which case only the potent steroid was monitored.
Patient diaries — Every patient was given a diary containing a page of 10 cm visual analogue scales for baseline and each of the next 24 weeks.
Scales were provided for itch, dryness, scaling, redness, and overall impression.
All scales were labelled ‘none’ at the left-hand end and ‘worst ever’at the right.
Scores were added to give a maximum of 50 cm.
Statistical methods
We decided before starting the study that the primary response criterion would be mean change from baseline in Leicester score (the most comprehensive single assessment) at 16 weeks.
The study was designed to have 80% power to detect a treatment response of 20% with a standard deviation (derived from existing data) of 30%, at significance level 0.05.
We estimated that 37 subjects would be required in each treatment limb.
For each active treatment, mean changes in Leicester score between baseline and 16 weeks were compared with the placebo group by 2-tailed t tests.
Results were corroborated by non-parametric tests.
In addition, several secondary analyses were done.
The individual components of the Leicester score, Costa scores and components, percentage of skin affected, and diary scores and components were analysed separately with t tests.
As an overall test of response, improvements from baseline in Leicester scores, Costa scores, and percentage of skin affected, at all visits during treatment, were compared by repeated measures analysis of variance.
We used  BMDP statistical software 5V (BMDP Statistical Software Inc, Los Angeles, USA) for these analyses with the unstructured covariance matrix option.
We also did comparisons between the placebo group and the two actively treated groups combined, and separate analyses for adults and children (aged 12 or under).
To investigate the effect of withdrawals, we repeated t tests at the end of treatment using the last available assessment before withdrawal of treatment.
Results
133 patients were enrolled.
The three treatment groups were well matched at baseline for age, sex, and disease severity (table I).
At 4 weeks, 9 subjects had defaulted or been withdrawn, and this number increased to 14 by 8 weeks, 21 by 16 weeks, and 27 after the washout (table II).
4 subjects failed to attend for assessment at 4 weeks although they remained in the study.
Data from the patients who withdrew were included in the analysis up to the point of withdrawal.
Decisions to withdraw were made before unblinding.
There were no apparent differences at baseline between the patients who withdrew and those remaining in the study.
With the exception of patients withdrawn due to deterioration, there were no apparent differences in response to treatment.
At 16 weeks, the mean (SE; number of patients) improvements in Leicester scores were 8.48 (2.85; 33) for patients on epogam, 2.54 (2.89; 35) for patients on efamol marine, and 7.15 (2.88; 34) for those on placebo.
On neither active regimen was mean improvement significantly different from placebo at 16 weeks (p=0.74 for epogam, p=0.26 for efamol marine; fig 1).
Mean changes at 16 weeks in individual components of the Leicester score relative to placebo are shown in fig 2.
The only significant differences were in favour of placebo over efamol marine for responses of erythema (p=0.04) and cracking (p=0.05).
Overall, analysis of variance for improvements in total score on each treatment at all visits showed no significant treatment effect (p=0.17, n=114, 2 degrees of freedom).
Identical analyses of Costa scores showed no significant differences, with the exception of improvement in oedema at 16 weeks in favour of epogam over placebo (p=0.04).
The mean percentage of skin surface affected at 16 weeks fell 3.26% (4.49%; 33) on epogam and 0.11% (4.56%; 35) on efamol marine, and rose by 3.62% (3.52%; 34) on placebo.
No significant difference was found between either treatment and placebo.
Complete data on topical steroid requirement were available from 26 patients on epogam, 25 on efamol marine, and 30 on placebo.
Of these patients, 2 did not use any steroid, 9 were applying formulations classified in the British National Formulary as ‘mild’, 17 were using ‘moderately potent’ compounds, 49 were using ‘potent’compounds, and 4 were applying ‘very potent’topical steroids.
There was a reduction in steroid requirement in all three patient groups, with the largest reduction in the placebo group (fig 3).
Diaries were returned by 30 patients on epogam, 35 on efamol marine, and 32 on placebo.
The greatest mean overall reduction in visual analogue scales was seen in the placebo group (fig 4); however, there were no significant differences from placebo at 16 weeks in total score or in any component.
In particular, there was no significant difference in pruritus score.
When analyses were repeated with both active treatment groups combined, there was no significant difference from placebo in the mean improvement of any parameter (mean improvement in Leicester score at 16 weeks was 5.43 [2.05; 68]on treatment and 7.2 [2.88; 34]on placebo [p=0.63]).
Analysis of variance for improvements in total Leicester score at all visits again demonstrated no significant treatment effect (p=0.98, n=114, 1 degree of freedom).
Separate analyses for adults and children gave results similar to the overall analysis.
Analysis with the last available assessment did not indicate any bias resulting from withdrawals.
Discussion
Our study has demonstrated no response of AD to EFA supplementation, nor any evidence of an additional effect  when n3 series EFAs were used in combination with EFAs of the n6 series.
Since both treatments consisted predominantly of evening primrose oil, we felt it reasonable to perform an analysis with the two active treatment groups combined.
This analysis would be expected to provide the best chance of demonstrating a treatment effect, but no such response was found.
21 (17%) patients were withdrawn during treatment, presenting a potential source of bias.
A perceived lack of response may have been a factor in the withdrawal of these patients, but deterioration of eczema was the principal reason for withdrawal in only 8 patients, 7 of whom were assessed at least once on treatment.
Loss of data was therefore small, and analyses with the last available assessment for each subject did not indicate that any bias had been introduced.
The numbers withdrawn were similar in each treatment group.
It is, unlikely, therefore, that withdrawals influenced the outcome of the trial.
Our results are in agreement with those of Bamford et al from the largest study yet performed on evening primrose oil in AD.
These researchers recruited 154 adults and children into a placebo-controlled crossover trial and found no response to treatment.
The results have been called into question because plasma lipid assays raised the possibility that placebo and active treatments had been intermixed, but Bamford et al have no reason to believe that this occurred (personal communication).
A similar lack of response was seen in a study of 24 subjects by Rilliet et al, in which blackcurrant seed oil was used as a source of gamma-linolenic acid.
The differences between our results and those of some published studies may be partly explained by variations in design and analysis.
In a crossover study of evening primrose oil and placebo in 32 adults and children with atopic eczema, each treatment was given for just 3 weeks.
A significant improvement was found with active treatment but not with placebo.
A more meaningful analysis — µcomparison of the mean improvement on each treatment — µcan be done from the data provided, and this analysis shows that there was no significant difference between active treatment and placebo (t test).
Wright and Burton performed a crossover study of evening primrose oil and placebo in 99 adults and children with atopic eczema.
They reported a highly significant, dose-related response with active treatment, which reached a 43% overall improvement in patients receiving the dose used in our study.
However, there are slight differences in flavour, appearance, and, possibly, laxative action between active treatment and placebo, which may have unblinded the crossover format.
Furthermore, baseline comparability of treatment groups was not established.
As already pointed out, it is possible that the apparent difference in response between the two groups was due to greater severity at baseline in subjects receiving active treatment.
This group would then be likely to show greater spontaneous improvement.
A response to evening primrose oil was also reported by Schalin-Karrila et al in a parallel-group study of 12 weeks duration with 25 adult subjects.
They reported large and significant reductions (30–50%) in surface area affected and severity of signs and symptoms of atopic eczema.
The group on active treatment used 60% less topical steroid.
However, there are clear baseline differences in disease severity between groups and the analysis did not properly address differences in response between treatments.
A parallel-group study with 24 children given half the dose of evening primrose oil that we used for 4 weeks, demonstrated a significant difference between active and placebo groups, a finding that we cannot explain.
Three of the studies described above and five others have been the subject of a meta-analysis on the use of evening primrose oil in AD, which showed a dose-related response that increased progressively with treatment duration.
A particularly consistent response was claimed for pruritus.
It is important to establish baseline comparability of treatment groups in terms of disease severity in such an analysis but this issue was not addressed.
We believe that the results of a meta-analysis containing unpublished trials and inadequate baseline data must be regarded with caution.
Our study is the first to examine a combination of n6 series and n3 EFA supplements in AD.
However, n3 series EFAs have been investigated in two smaller placebo-controlled, parallel-group trials.
In the first trial, subjects' grading of symptom severity showed a significant response, although the investigators' assessments did not.
The second trial showed no significant response.
In conclusion, we believe that our study avoided the methodological and analytical problems of previous reports, which have given rise to a great deal of controversy over the efficacy of EFA supplementation in AD.
No therapeutic effect was demonstrated.
SHORT REPORT
Epidemiology of hepatitis E virusinfection in Turkey
The seroprevalence and risk factors for infection with hepatitis E virus (HEV) were analysed in five regions of Turkey, where one-third of acute hepatitis cases are non-A, non-B.
Antibodies to HEV (anti-HEV) were found in 80 (5.9%) of 1350 subjects.
Independent predictors of anti-HEV were age over 25 years, less than elementary education, antibodies to hepatitis C virus, and residence in the warmest region (Adana).
Whereas none of 105 subjects in the second decade of life were HEV seropositive, 17 (3.7%) of 464 and 28 (9.1%) of 308 of those in the third and fourth decades of life, respectively, had anti-HEV (p < 0.001).
These data demonstrate that in Turkey HEV is more prevalent in warmer regions and in adults, beginning in the third decade of life.
The major cause of non-parenteral non-A, non-B hepatitis has been identified and named the hepatitis E virus (HEV).
Serological evidence of infection with HEV (anti-HEV) has been found in 42% of Egyptian children with non-A, non-B hepatitis and in 71% of persons infected in a waterborne outbreak of non-A, non-B hepatitis in Kashmir, India.
However, no large population-based serosurveys of HEV have been reported, and in Turkey, one-third of cases of acute hepatitis were classified as non-A, non-B.
From 1990 to 1992, demographic information and serum samples were collected for a study of cardiovascular morbidity from over 8000 persons in five distinct regions of Turkey: Istanbul, a large urban centre in the northwest; Ayvalik, an Aegean coastal town; Aydin, a factory city in the southwest; Trabzon region, rural Black Sea villages in the northeast; and Adana, a warm southeastern city, with an agricultural-based economy.
All participants were volunteers responding to the advertised national heart study.
Demographic information was collected by medical students administering a questionnaire.
In July, 1992, these resources were used to estimate the prevalence of HEV and hepatitis C virus (HCV) in Turkey.
For this study, 300 persons were selected from each of the five regions with stratified random sampling based on gender, age, and income.
Serum samples were placed on dry ice and stored at -70°C.
Antibodies to HCV were screened with the second-generation Ortho HCV ELISA.
All specimens repeatedly positive by this assay were confirmed by second-generation Chiron RIBA HCV.
Antibodies to HEV were detected with Diagnostic Biotechnology's HEV ELISA (supplied by Genelabs Technologies).
In this assay two recombinant antigens, which correspond to the putative structural region of the ORF3 of HEV, are coated on polystyrene microplate wells to which serum is added.
These antigens correlate with the IgG and IgM response to HEV from strains as divergent as the Mexico and Burma isolates.
Specimens at or above the minimum positive value were repeated in duplicate.
Repeat positive specimens were considered positive.
51 persons were excluded from analysis because of missing data on age, gender, or obesity, and only 201 specimens from the region of Ayvalik were tested for anti-HEV, leaving 1350 persons for subsequent analysis.
Odds ratios for HEV were calculated for each covariate separately with or Fisher's tests.
Multivariate analysis was by logistic regression with anti-HEV as the dependent variable, starting with age, gender, and location as baseline covariates and with additional covariates added stepwise.
Antibodies to HEV were found in 80 (5.9%) of 1350 subjects.
Results of univariate analysis are shown in table I. 
After multivariate logistic regression, only older age, lower levels of education, HCV infection, and living in Adana region were independent predictors of HEV (table II).
To examine further the association of age and anti-HEV, prevalence rates were calculated for each decade.
Whereas none of 105 persons in the second decade of life was HEV seropositive, 17 (3.7%) of 464 and 28 (9.1%) of 308 of those in the third and fourth decades of life, respectively, had anti-HEV (p < 0.001).
The frequencies for the fifth, sixth, seventh, and eighth decades were 8/141 (5.7%), 14/161 (8.7%), 8/116 (6.9%), and 5/45 (11.1%), respectively.
No clusters of HEV occurred when seropositive persons were grouped by last name or street address.
Since, as with hepatitis A virus (HAV), HEV is generally transmitted by faecal-oral routes, the increased prevalence of HEV in those with less education was not unexpected.
Similar findings have been reported from large outbreaks where poor sanitation led to increased rates of HEV (enteric non-A, non-B hepatitis) in persons of lower socioeconomic strata.
HEV differs from HAV in that outbreaks of HEV occur exclusively in countries with warm climates.
This observation is consistent with the increased rates of HEV we observed in Adana and Aydin, warmer southern regions with similar population densities.
No outbreaks of non-A, non-B hepatitis have been recognised in Adana or Aydin.
HEV also differs with HAV in the increased frequency with which infection is observed in adults compared with children.
Similar to the situation in developing countries, most Turkish residents are infected with HAV by the second decade of life.
However, in our survey none of 105 persons in the second decade of life had anti-HEV.
By contrast, we detected anti-HEV in 3.7% and 9.1% of those in the third and fourth decades of life.
Although cases of HEV have been clinically and serologically documented in children, our findings are consistent with reports from several large outbreaks where high rates of HEV (enteric non-A, non-B) were reported in adults, beginning in the late second and third decades of life.
The increased detection of anti-HEV in adults in our study could reflect a cohort of Turkish residents infected with HEV in the past and a younger generation unexposed to virus, due perhaps to improvements in sanitation.
Alternatively, anti-HEV titres in subjects infected as children but not re-exposed as adults could have diminished beyond detection.
EDITORIAL
White elephants about town
The expanding urban population in many developing countries is causing a crisis in provision of public services.
Inadequate local health services for low-income groups and long queues of patients at hospitals have prompted health planners to explore the best ways of improving government health facilities.
The World Health Organization, in an attempt to assist health ministries in this process, has recommended the setting up of ‘reference centres’, a new tier in the health system between existing hospitals and health centres.
The aim of these centres is to provide first-contact medical services 24 hours a day, obstetric services, diagnostic facilities, and inpatient care.
Within their prescribed catchment area, a reference centre is responsible for the work of health centres, for community development, and for public health activities.
Thus in effect reference centres are indistinguishable from small district hospitals, and they are expected to relieve the perceived overload at existing specialist city hospitals.
At face value, decentralisation of services into communities seems eminently sensible, and reference centres have been effective in some countries.
Some of these successful schemes are linked to universities that teach community health care and therefore need model community services to train students.
The evidence that this approach will work more generally is patchy, and is furthermore based on a series of assumptions that have not been widely tested.
The danger in WHO's strong endorsement of reference centres is that governments and donors may well interpret this as a green light to invest capital in the new service tier.
Much more information and research within cities is needed before the architects are called in: the research that WHO are currently coordinating in several cities in Africa and Asia must examine the assumptions underlying their policy.
The WHO strategy is based on the belief that hospitals are often overloaded by patients with minor complaints who should be using cheaper and more basic services.
However, there is no firm evidence that hospital overuse is a consistent feature in all countries, and this perception might well reflect overstretched hospital management in certain areas.
Large patient throughput in hospitals can undoubtedly give the impression of excessive use if patients spend many hours waiting to be seen or queuing for drugs.
Before making global statements about overuse, one needs to examine ward throughput and outpatient flow data against the physical and professional resources.
Even if hospitals are shown to be overused, WHO policy assumes that the people using hospitals ‘should’ be using the health centres.
An alternative view is that hospitals provide a valuable primary health service to people excluded from health centres, which focus services on mothers and children.
Adolescents, men, the homeless, and people with sexually transmitted diseases may not feel comfortable in a health centre.
The opening of reference centres is no guarantee of an appropriate and accessible service for these population groups.
The reference centre policy also assumes that the average cost (ie, per patient seen) of hospital care is greater than the cost at a health centre, and that a reference centre is cheaper than a hospital.
Whilst ambulatory surgical care has been shown to be cheaper than hospital care in Cali, Colombia, such evidence cannot be taken as a general rule.
Health centres with a low throughput may have higher average costs than a busy hospital outpatient department.
A reference centre functioning as a small hospital with 24-hour cover needs many more staff than a health centre, and this will drive up recurrent costs.
Unless the patient throughput per doctor or nurse at a reference centre is higher than that in the hospital, economies of scale suggest that average costs at this new tier will be greater.
Few countries can afford increments in their recurrent budget.
Yet this additional tier requires staff, drugs, and managerial support.
Where should this come from?
With about 60–80% of government national health facility expenditure in developing countries absorbed by hospitals, an obvious source is existing hospital budgets.
If planners intend to trim hospital budgets and reallocate staff to smaller facilities, this strategy must be made explicit from the outset.
Although the reference centre initiative is an important stimulus for planners to evaluate their existing services, there may well be better uses for scarce resources.
Capital investment is rarely a solution to complex problems, and the recurrent cost implications may be detrimental to health ministries that are already over-committed.
Building a new tier in the health system is a simplistic solution to the broader problems of management and resource allocation: countries should not follow the global call blindly.
COMMENTARY
DRUG REACTIONS
Sumatriptan and chest pain
Clinical studies in over 12,000 patients have confirmed the efficacy of sumatriptan in migraine and cluster headaches.
This potent and selective 5HTID receptor agonist has predominantly cranial vasoconstrictor effects in animal and human studies and can be given subcutaneously (6 mg) or orally (100 mg).
Sensations of heaviness, pressure, and tightness at different sites, including chest and neck, recorded in 3–5% of patients suggest some extracranial vasoactivity.
During early clinical studies, 1 patient experienced cardiac ischaemic events, so a detailed safety programme was initiated (a) to assess the cardiovascular effects;(b) to analyse reports of possible cardiac complications; and (c) to conduct extensive electrocardiographic (ECG) monitoring during trials.
Extracranial cardiovascular activity was suggested by increases in blood pressure of 12/10 mm Hg observed 10 min after subcutaneous sumatriptan and lasting for 30–60 min.
A lesser effect (7/5 mm Hg) of longer duration was seen with oral therapy.
Detailed invasive haemodynamic investigations have now shown increased vascular resistance in both systemic and pulmonary circulations, with an increase in aortic systolic and diastolic pressures of 17–20% and 12–16%, respectively, and a relatively greater rise in pulmonary systolic and diastolic pressures of 40–50% and 33–77%.
The increase in pulmonary capillary wedge pressure by 90% points to additional venoconstrictive effects.
These results suggest either that 5HT receptors are more widely distributed than previously recognised or that sumatriptan is less specific in its agonist activity.
In-vitro studies in human epicardial arteries confirm 5HT-like receptor activity.
Maximum vasoconstriction following sumatriptan is only 21% of that seen with 5HT (serotonin).
The in-vivo effects were investigated in patients with chest pain but with less than critical coronary artery obstruction (stenosis <50%).
After intravenous sumatriptan, mean absolute diameter was reduced by 14 (SD 10) % vs a 16% reduction with subcutaneous administration.
There was no ECG evidence of myocardial ischaemia despite symptoms of chest tingling and tightness.
When serotonin was given into coronary arteries there was a 52% increase in cross-sectional area in normal arteries, a 64% reduction in patients with angina, and total occlusion in subjects with Prinzmetal angina.
The reduction in cross-sectional area is at least two-fold greater than with sumatriptan.
The ergot alkaloids given therapeutically in migraine have been used as a diagnostic test for coronary artery vasospasm, with a diffuse reduction in arterial diameter of about 30% evident in normal arteries.
The vasoconstrictor effect is accentuated with minor arteriosclerotic disease and total occlusion may occur.
In migraineurs, dihydroergotamine therapy has resulted in 18 reported cases of myocardial infarction, with death in 3. 2 patients had a history of cardiac disease.
75% were less than 50 years of age, 83% were women, and 78% had no risk factors for ischaemic heart disease.
Thus, the coronary vasoconstrictive effect of sumatriptan seems to be less than that of serotonin or the ergot alkaloid, which show additional 5HT and alpha-receptor stimulation.
Inman and Kubota observed patients with chest tightness and suggested that asthma might be induced by sumatriptan.
From the clinical database of 75 trials held by Glaxo, who manufacture sumatriptan, 375 asthmatics have been identified who had been treated for 1214 migraine episodes.
Only 1 of 6 observed episodes of asthma was believed by the clinical investigators to be related to therapy.
Chest tightness may indicate stimulation of pulmonary vascular receptors with pulmonary vasoconstriction rather than bronchoconstriction.
Coronary vasoconstriction has been suggested by several case-reports but full cardiac investigations have seldom been carried out.
Willett et al reported ST-T wave changes of Prinzmetal character 4 min after subcutaneous sumatriptan in a 47-year-old man.
This patient had a history of chest pain, although a negative exercise test had been recorded 7.5 months before the episode.
Underlying obstructive coronary artery disease was not excluded by angiography.
Curtin et al reported ventricular fibrillation in a 42-year-old woman within 3 min of injection, thought to have been induced by vasospasm.
Subsequent investigations confirmed a normal 24 h ECG recording but an exercise test suggested an ischaemic response and coronary arteriography showed a 40% obstructive lesion.
This patient had experienced several episodes of palpitations although she was otherwise well.
Ottervanger et al reported a myocardial infarction in a 47-year-old woman after subcutaneous sumatriptan.
The ECG showed changes of inferior infarction on admission but strangely no ST elevation; this pattern suggests a previous event or rapid spontaneous coronary artery reperfusion.
She had noted chest pain after two previous injections.
A subsequent exercise test showed possible ischaemic changes.
Of 6124 patients with 28,648 attacks, 2150 had ECGs within 4 h of oral or subcutaneous therapy and 5388 had ECGs at some stage (Glaxo data).
Although 4.6% had pressure symptoms, 99% had no ECG changes.
Non-specific changes were seen in 27 (0.5%); new changes of possible myocardial ischaemia occurred in 10 (0.2%) and 5 were believed to be related to sumatriptan (0.1%).
The ECG change was associated with angina in only 1 case.
In the subset of oral studies, ECGs were obtained in 1733 of 2786 patients with 24,000 attacks.
No irreversible ECG changes were seen, although pressure symptoms occurred in 4.4%.
These studies show that the frequency of ECG changes is very low.
Overall, there is a small risk of myocardial ischaemia following sumatriptan-induced vasoconstriction.
The drug is contraindicated in patients with symptomatic ischaemic heart disease with either angina or an earlier myocardial infarction, or if silent ischaemia or Prinzmetal angina has been documented previously.
In patients who are symptom free but at risk of atheroma — eg, postmenopausal women; men over 40; individuals with risk factors such as hypertension, hypercholesterolaemia, obesity, diabetes, or tobacco smoking; and those with a strong family history — one should consider giving the first dose of sumatriptan under medical supervision.
If sumatriptan induces symptoms suggestive of angina, nitrates should be given.
INFECTIOUS DISEASE
Antenatal interventions against sexually transmitted disease in Africa
In a recent review of the effectiveness of antenatal interventions, the World Health Organization gave high priority to identifying the best ways of ensuring the availability of measures to prevent syphilis and gonorrhoea in women.
The possible strategies included screening, treatment, follow-up, and health education.
Such measures are required urgently in Africa, where the prevalence and complications of sexually transmitted diseases (STDs) remain disturbingly high.
In Africa, the high overall incidence of STDs, fuelled by lack of effective STD control, means that antenatal interventions are unlikely to be very effective if they are not matched by equivalent or corresponding measures to control STDs in the general population.
Moreover, the low attendance by female patients at existing STD clinics raises several questions — eg, what proportion of STDs detected antenatally represent pre-existing symptomless infection (as a consequence of lack of partner notification) and what proportion are acquired during pregnancy?
Successful antenatal intervention largely depends on the proportion, frequency, and pattern of attendance at antenatal clinics, so there is need for strategies to overcome cultural, demographic, and socioeconomic barriers to early and continued attendance.
Nevertheless, the availability of reliable diagnostic tests and effective treatment for syphilis and gonorrhoea means that there is enormous potential and scope for reducing both their prevalence and their complications during pregnancy.
There are two main options:(a) routine antibiotic prophylaxis; and (b) case-detection through screening.
Although antibiotic prophylaxis may reduce complications of some genital infections in pregnancy, the benefits are not unqualified.
The data come mainly from industrialised countries where the pattern of STDs differs from that in Africa.
In addition, there are obstacles to routine antibiotic prophylaxis in Africa.
In view of the financial constraints, it would be hard to justify the cost of such prophylaxis for all pregnant women in order to cover 7–10%.
An important consideration is that multiple drug resistance has rendered many low-cost drugs useless for this purpose.
After a course of antibiotics in pregnancy, women will still be vulnerable to reinfection via sexual intercourse, while extension of prophylaxis to their spouses would double the cost without necessarily guaranteeing improved efficacy.
Furthermore, blind use of antibiotics on such a large scale and the attendant difficulties with compliance might well aggravate the problem of drug resistance.
Case-detection through antenatal screening may have logistic, financial, and manpower impediments to implementation but it remains the favoured strategy.
The inability of many African countries to sustain large-scale routine screening for gonorrhoea with conventional methods is the main obstacle to antenatal screening for the infection.
The search for cheap but reliable mass screening tests and accurate diagnostic algorithms has so far been fruitless, so case-finding based on clinical suspicion will continue to be the most important strategy.
By contrast, the availability of simple, rapid, reliable, and cheap screening tests for syphilis makes this STD amenable to routine antenatal screening.
A demonstration project in Zambia has shown that such screening is achievable and might significantly reduce adverse pregnancy outcomes.
Mandatory antenatal screening for syphilis could be the first major antenatal intervention against STDs; full integration of this policy into antenatal care could provide an avenue for research into pertinent questions raised in the recent WHO review and an opportunity to identify women who need evaluation for other STDs.
The costs of the programme could be controlled by coordinated use of facilities and personnel in antenatal and STD clinics and laboratories.
NEUROSURGERY
Surgical excision for single cerebral metastasis?
In a patient with a single symptomatic metastasis in an accessible area of the brain and an otherwise reasonable outlook, the neurosurgeon may contemplate surgical excision.
The usually good demarcation of tumour from normal brain allows a clean removal, quickly relieves symptoms of intracranial hypertension, and frequently improves of signs of focal brain dysfunction.
Patient outcome depends largely on the extracranial disease; the tumour seldom recurs at the site of a clean excision.
Computed tomography (CT) with contrast enhancement facilitates the diagnosis of single lesions, but in some cases will reveal multiple metastases, which are a contraindication to surgical treatment.
High-dose glucocorticoid therapy has made surgery much safer, both for tumour biopsy and for excision.
Before the introduction of steroid treatment, surgical biopsy without the decompression afforded by removal of the tumour carried a prohibitive morbidity and mortality.
The combination of steroids and CT-directed stereotactic brain biopsy has profoundly reduced the morbidity of the procedure: Thomas and Nouby reported mortality of 0.3% and morbidity of 4% in 300 cases, including tumours in the brainstem.
Thus all solitary mass lesions in the brain can be located and verified histologically and histochemically, as they should be, before a decision on treatment.
The increased safety of biopsy under steroid cover also means that one can contemplate radiotherapy as a safe treatment on its own — producing improvement in 60% of cases — rather than as an adjunct to surgical excision of the metastasis.
So, what is the best treatment for the patient with a solitary cerebral metastasis — biopsy and radiotherapy, or surgical excision and radiotherapy?
The answer has not proved easy.
The outcome of non-randomised studies has been influenced by the fact that the patients selected for surgery tend to be those in good overall condition with a favourable systemic prognosis.
Now we have evidence from two prospective randomised studies in which surgical excision followed by radiotherapy was compared with radiotherapy alone in similar groups of steroid-treated patients with a single histologically identified brain metastasis, reasonable neurological function, and a life-expectancy greater than 6 months.
In a study by Patchell et al, surgical excision and subsequent radiotherapy in 25 patients was followed by local recurrence in 5 cases vs 12 local recurrences in 23 patients treated by radiotherapy alone.
Median survival was 40 weeks and functional independent survival 38 weeks in the surgical group vs 15 and 8 weeks, respectively, in the non-surgical group.
Vecht et al report a randomised study of 63 patients and confirm significantly longer overall and functionally independent survival in the surgically treated patients.
The results of these latest trials indicate the continuing need for close cooperation between neurosurgeons, pathologists, and radiation oncologists in the management of cancers that metastasise to the brain, not only to provide biopsy material for histological diagnosis but also to carry out excisional surgery in appropriate cases.
ONCOLOGY
Chromosomal deletions in haematological malignancies
Haemopoiesis entails the differentiation of self-renewing pluripotential stem cells into a hierarchy of progenitor cells committed to one or more lineages.
These progenitors subsequently give rise to at least nine distinct differentiated cell types.
Haematological malignancies arise as a result of acquired mutations which disturb the normal balance between self-renewal and differentiation.
Characterisation of these mutations might be expected to tell us something about the normal regulation of haemopoiesis.
Two broad categories of tumorigenic genetic lesions are now well recognised:(a) activation of proto-oncogenes; and (b) loss or inactivation of tumour-suppressor genes.
Proto-oncogenes activated in tumours derived from a particular haemopoietic lineage are often functionally altered or ectopically expressed.
Their study, although greatly increasing our understanding of tumorigenesis, has provided little information about normal haemopoietic mechanisms.
By contrast, tumour-suppressor genes inactivated in tumours affecting a certain haemopoietic lineage are presumably expressed during the normal differentiation of the lineage, so their characterisation is likely to provide direct insight into the molecular mechanisms regulating such differentiation.
Tumour-suppressor genes have been implicated in an increasing number of solid cancers, and some of these genes have subsequently been found to be inactivated in haematological malignancies.
Thus, structural abnormalities of the retinoblastoma gene have been found in as many as 30% of some forms of leukaemia and lymphoma and mutations of the p53 gene have been reported as an infrequent event in myelodysplasia and acute myeloid leukaemia.
However cytogenetic deletions at other chromosomal sites are well recognised in several haematological malignancies and are thought to flag the site of novel tumour suppressor genes.
Deletions of two distinct regions of 6q have been identified in B-cell non-Hodgkin lymphoma but the target genes are not yet known.
Deletions of 9p occur in 7–13% of patients with acute lymphoblastic leukaemia.
This region of chromosome 9 contains the cluster of interferon- genes and the interferon-1 gene.
Diaz et al have shown homozygous or hemizygous loss of the interferon genes in 29% of patients with acute lymphoblastic leukaemia, but it is unclear whether the critical target gene is one of the interferon genes or another locus nearby.
Similarly, deletions of 13q14 in low-grade B-cell tumours may be associated with hemizygous or homozygous loss of the retinoblastoma gene.
However, recent evidence suggests that the crucial event is loss of an uncharacterised neighbouring locus.
Much attention has lately been focused on the 5q deletion associated with myelodysplasia and acute myeloid leukaemia.
Van den Berghe and colleagues originally drew attention to the deletion of 5q in elderly female patients with macrocytic anaemia, normal or raised platelet counts, hypolobulated megakaryocytes, and a low risk of leukaemic transformation.
This particular form of myelodysplasia was termed the ‘5q syndrome’.
Deletions of 5q and 7q are now know to be among the commonest chromosomal abnormalities found in myelodysplasia and secondary acute myeloid leukaemia.
Moreover, a whole battery of growth factors, cell surface receptors, and other tantalising genes have been mapped to the region encompassed by most 5q deletions.
In 1991, homozygous loss of the gene encoding the M-CSF receptor (also known as FMS or CSF-1 receptor) was shown in 4 of 10 patients with a cytogenetically detectable 5q deletion.
The remaining chromosome 5 appeared normal but presumably contained a submicroscopic microdeletion in a subpopulation of cells.
Could FMS itself be the target gene?
FMS is expressed in myeloid cells and FMS mutations are found in myelodysplasia.
However, there have not been any reports of point mutations in the remaining FMS allele in patients with a 5q deletion.
Even if FMS does not prove to be the critical target gene, characterisation of the microdeletions should considerably reduce the critical deleted region and provide a major step to identifying the real target gene.
The latest twist is provided by the identification of an alternative candidate gene.
The IRF-1 gene encodes a transcription factor that functions as a transcriptional activator of interferon-, interferon-, and other interferon inducible genes.
Willman and co-workers looked for loss of IRF-1 and several flanking genes in 12 patients with myelodysplasia or acute myeloid leukaemia associated with cytogenetic abnormalities of 5q31.
IRF-1 was the only one of these genes consistently deleted in all of these patients.
Furthermore, a rearrangement of the IRF-1 gene was identified in one additional patient with acute lymphoblastic leukaemia.
These researchers suggest that IRF-1 itself could be the target gene in 5q deletions and cite two further observations as indirect support for this concept.
First, interferon gene deletions have previously been associated with acute lymphoblastic leukaemia and it would, therefore, not be too surprising to find that abnormalities of interferon regulation or of the interferon signalling pathway were tumorigenic.
Second, in vitro experiments have shown that the balance between the transcriptional activator IRF-1 and its antagonistic repressor, IRF-2, plays a crucial part in controlling cell growth.
Overexpression of IRF-2 is tumorigenic, an effect that can be inhibited by increasing the expression of IRF-1.
These experiments provide functional evidence that IRF-1 has the potential to act as a tumour-suppressor gene.
Nevertheless, direct evidence implicating IRF-1 as the critical target gene in 5q deletions remains sparse.
The only structural abnormality of IRF-1 was found in a patient with acute lymphoblastic leukaemia, a disease not normally associated with 5q deletions.
The significance of this rearrangement for the pathogenesis of myeloid malignancies associated with 5q deletions remains unclear.
These results raise several questions.
Perhaps the most important is whether the FMS and IRF-1 data can be reconciled.
FMS and IRF-1 are separated by an estimated 5–12 megabases and microdeletions that include one are unlikely to include the other.
One possible explanation would be the existence of two distinct target genes on 5q, one in the region of IRF-1 and the other close to FMS.
This speculation would accord with the fact that some patients with myelodysplasia and a 5q deletion have a very low risk of transformation (eg, 5q syndrome) whereas in other myelodysplasia patients a 5q deletion is associated with poor prognosis (eg, secondary MDS).
These examples illustrate the central problem of characterising chromosomal deletions — how do you know when you have identified the target gene?
Searching for mutations in a candidate gene probably remains the best approach, albeit a labour-intensive one.
However, once identified, the target genes should be doubly interesting since they promise to shed light not only on the pathogenesis of haematological malignancies but also on the normal regulation of haemopoietic differentiation.
CLINICAL PRACTICE
Irritable bowel syndrome: pathogenesis and management
An international working team defined the irritable bowel syndrome as distinct from other functional bowel disorders.
Symptom criteria for the irritable bowel syndrome are known as the Rome criteria.
Since there is no pathophysiological marker for any of these syndromes we must rely on symptoms for their definition and classification.
The Rome nosology is a step towards better understanding of functional gastrointestinal disorders because the disparate syndromes are likely to have different causes and treatments.
Epidemiology
Studies in the UK, USA, France, New Zealand, and China indicate that irritable bowel syndrome (IBS) is present in 11–14% of adults.
Although most people with IBS in the community do not consult a physician for their symptoms; the treatment of those who do is time-consuming and expensive.
Management strategy must be developed with this in mind.
Pathogenesis
Is the IBS ‘…a qualitative, or merely quantitative departure from the psychophysiologic reactions of normal people?’— Thomas Almy, 1980
The cause of IBS has confounded physicians for almost two centuries.
Despite much research we cannot even today offer a convincing explanation.
Some declare it is caused by diet; others are convinced it is a motility disorder or due to altered perception, a psychological disorder, a psychophysiological phenomenon, or even abnormal illness behaviour.
Keeping Professor Almy's question in mind, a condition affecting at least 11% of a physically healthy population may be no disease at all.
A dietary disorder?
‘Eat what you want and let the food fight it out inside’— Mark Twain
Dennis Burkitt in East Africa noted that constipation and other ‘western’ bowel disorders were uncommon in natives consuming an indigenous high-fibre diet.
In 1972 a study of rural African and westernised populations noted that the greater the dietary fibre content, the greater the daily stool weight and the shorter the whole-gut transit time.
These observations led to the ‘fibre hypothesis’: the concept that many diseases of the colon and other organs, including IBS, result from the ingestion of a western, refined, low-fibre diet.
Subsequent studies confirmed that a high-fibre diet increases stool bulk and shortens gut transit.
The value of bran, psyllium, and other bulking materials is well established in the treatment of constipation.
The presence of IBS in individuals has not been linked to their fibre intake and its presence in countries such as China makes it unlikely that a single dietary factor such as fibre deficiency is at fault.
Dietary fibre supplements have not been successful in the treatment of IBS as measured by double-blind trials, although there is evidence that at least 30 g of dietary fibre a day will improve constipation and some other symptoms.
The fibre story is even more confused by the report that, among ‘healthy’ volunteers, an outgoing personality and positive self-image are associated with a larger stool output.
Some patients have no doubt that a food is the culprit.
This idea has spawned irrational diets that defy science, cause much inconvenience, and may even threaten nutrition.
True food allergy, such as to shellfish, affects systems beyond the gut and is more likely to cause vomiting and diarrhoea than IBS symptoms.
Certain non-IgE-mediated intolerances to wheat, dairy products, or beef are claimed to cause diarrhoea but not IBS as strictly defined.
Few patients with IBS can be confirmed by double-blind feedings to have a true food sensitivity.
Nevertheless, evidence for lactose intolerance, excessive caffeine intake, use of sorbitol-containing gum, or other drug or dietary habit that might affect the gut should be sought.
A motility disorder?
‘The bowels are at one time constipated, at another lax, in the same person…
How the disease has two such different symptoms I do not profess to explain…’— W. Cumming, London Medical Gazette, 1849.
Despite the evident gut malfunction in IBS the pathophysiology is obscure.
Colon motility studies in the 1960s suggested that in constipation, the motility index (frequency×amplitude of contraction) is increased, holding up passage of stool and causing abdominal pain.
Conversely, the motility index is decreased in those with diarrhoea, so that the sigmoid behaves like a sphincter.
The index has not proved to be a reliable feature of either constipation or diarrhoea.
The proximal colon and the small intestine are also thought to be dysfunctional in IBS.
In the 1970s, investigators reported that a 3 cycle/min colonic myoelectrical rhythm is commoner in IBS patients than in controls, although the specificity of this observation is now doubted.
European workers associated the recording of electrical short bursts in the colon with constipation and their absence with diarrhoea.
In the 1980s attention was drawn to the small bowel, where the secretory and motor responses to stress seem to be different in IBS patients.
None of these phenomena is sufficiently specific to permit its use as a diagnostic test of IBS nor do any of them offer an explanation of how symptoms are generated.
In a series of experiments, balloons inflated sequentially throughout the gut identified trigger points that reproduced the abdominal pain in most cases of IBS.
Even here we cannot be sure to what extent the pain of IBS is a normal perception of abnormal physiology or a normal perception of normal phenomena.
In IBS there is a tendency for both small and large bowel to over-react to various stimuli such as drugs, stress, balloon distension, and even eating, which may represent an exaggerated gastrocolonic response.
A perception disorder?
‘…a normal perception of abnormal motility or an abnormal perception of normal motility…’— M. J. Ford, 1986.
The failure of motility observations adequately to explain symptoms has led many to consider the sensory or afferent connections between the gut and the brain.
It has been shown, for example, that IBS patients experience pain at lower volumes of rectal distension than others.
The autonomic connections between the enteric and central nervous systems are well known, and the vagus and sympathetic nerves carry more afferent than efferent nerve fibres.
The experience of pain may be greatly influenced by emotion, memory, culture, and the psychosocial situation.
It is useful to distinguish acute from chronic pain.
The former is linked to tissue disease or organ dysfunction, and in the case of the gut is associated with eating, defaecation, and vomiting.
Chronic pain is continuous and unassociated with the physiological responses to acute pain such as sweating and tachycardia.
The patient with chronic pain is often mentally depressed and there may have been adverse experiences such as sexual abuse or threatening life events.
Acute, function-related pain is primarily sensory or peripheral, and may lend itself to treatment with analgesics directed to the involved organ.
Chronic pain, which is more influenced by central controlling activity, may require a different approach.
Thus, drugs such as tricyclic antidepressants enhance serotonin release and inhibit pain impulses, thereby closing the ‘gate’ on the perception of noxious stimuli.
They may also enhance endorphin release.
The use of benzodiazepines risks habituation and, via gamma-aminobutyric acid production, these drugs may inhibit serotonin and further open the ‘gate’.
Not only pain may be misperceived.
Patients often misinterpret their bowel action as well.
Although stool frequency in the population ranges from 3/week to 3/day, variations within this range may be considered abnormal, and frequent but fragmented or lumpy stool as diarrhoea.
There is no evidence that the occurrence of borborygmi, flatus, and distension differ from those in other people.
Lasser and Levitt found that patients complaining of ‘gas’ or ‘bloating’had no excess intestinal gas.
Altered perception alone does not explain IBS.
It does not explain the triggering sensation and altered function of the gut, nor does it seem important in those with IBS symptoms who do not seek medical care.
A psychological disorder?
The notion that the IBS is related to or even caused by the patient's psychological state is as old as the concept of IBS itself.
Many studies attest to the fact that anxiety, mental depression, and other types of psychological distress are more likely in IBS patients than in those with organic disease.
Such studies are of patients in a tertiary care setting who are unlikely to be representative of all persons experiencing IBS symptoms.
Whitehead found that the psychosocial make-up of patients with IBS in the community who do not see doctors is the same as that of normal people.
There are no studies of the psychological state of IBS patients seen in primary care, but it is reasonable to assume that they represent an intermediate population.
In one British clinic, chronic attenders with IBS were compared with newly referred patients.
Although psychological morbidity was similar in the two groups, the social consequences in the chronic attenders were more severe.
The foregoing data do not support the notion that psychopathology causes IBS.
Perhaps psychopathology elevates IBS symptoms to the status of a medical problem in a person's consciousness?
For the emotionally troubled person, IBS symptoms may provide a socially acceptable vehicle for care.
This argument is reinforced by observations that IBS patients consult more often than others after a stressful or threatening life event.
A person with IBS who has ignored the symptoms for years may become acutely aware of them when a close relative succumbs to cancer.
Treatment here is obvious.
Management of patients whose visits seem to be precipitated by depression, job loss, marriage break up, or other personal catastrophe may be much more complex.
Many tertiary care patients have sustained sexual or physical abuse.
In such a situation undue focus on the gut symptoms may be misplaced.
A psychophysiological disorder?
We all recognise that emotion affects gut function.
Who has not experienced some gut upset before an exam, a marriage, or other stressful emotional event? 40 years ago Almy showed that stress could alter gut function but that these changes were not specific to the stress.
One person may have ‘butterflies’, another diarrhoea, another vomiting, and yet another a migraine.
In IBS the relation of symptoms to stress is more subtle.
Some notice that symptoms improve during a crisis, only to return later.
In patients with IBS the gut seems to be more reactive to various stimuli when compared with controls.
Drugs, hormones, food, distension, and emotional stress elicit exaggerated motor responses.
Using radiotelemetry equipment, Valori et al observed that motility of the small bowel was altered in a different manner in patients with IBS than in controls.
He and his colleagues employed such  stressors as heavy-metal music, nocturnal arousal, or parking in London traffic to challenge the small bowel.
Are the above observations clues to the cause of IBS or merely epiphenomena?
A behavioural disorder?
A random telephone survey in Cincinnati compared people with peptic ulcer with those with IBS.
The latter had more somatic symptoms, viewed colds and influenza more seriously, and consulted physicians more frequently for minor complaints.
During childhood they also were more likely that others to have received gifts or remained home from school when ill.
This report concludes that people with IBS are prone to chronic illness behaviour and that this is learned.
The notion is gaining credence that health-care seeking in IBS, particularly to tertiary care centres, may have as much to do with a person's cultural and psychosocial state as with the IBS symptoms themselves.
How else can one explain why women are more likely than men to take their IBS symptoms to a doctor in western cultures, whereas the opposite is true in India?
Why do only a minority of those with IBS seek medical attention?
The concurrence of threatening life events and psychosocial distress may partly explain these phenomena.
Therapeutic approach
We must develop a strategy of management based upon the known facts.
Such a strategy should be based on 5 principles: a positive diagnosis, consideration of the patient's agenda, critical appraisal of drugs and placebos, continuing care, and a graded therapeutic response.
Positive diagnosis
‘…one who has seen many such cases can with tolerable certainty tell, without more minute examination what the nature of the complaint is.’— W. Cumming, London Medical Gazette, 1849.
Longitudinal studies indicate that the condition affects people over long periods of their lives yet there is no evidence that they are at risk of any serious organic disease.
Furthermore, a careful history and physical examination permit a confident diagnosis that stands up over time.
Because some physicians regard IBS as a diagnosis of exclusion they feel compelled to exclude all organic disease.
This approach is expensive.
The physician should attempt to establish a positive diagnosis at the first clinical encounter, based on the criteria in table II.
In a condition that affects up to 14% of the population, organic disease is bound to coexist in some.
The first step is to inquire after symptoms that suggest organic disease such as anaemia, bleeding, fever, weight-loss or a recent change in bowel habit.
If such symptoms and physical findings are absent, few investigations should be undertaken.
A sigmoidoscopy should be done but biopsy in search of microscopic colitis in the absence of continuous diarrhoea is unnecessary.
If the patient is over 40 or has risk factors for colon carcinoma, a barium enema is prudent.
Other tests should be avoided unless indicated.
An early, confident diagnosis permits tests to be kept to a minimum and reassures the patient that there is no mortal disease.
Such reassurance may be the physician's most effective therapeutic weapon.
Patient's agenda
We have seen that most people with IBS do not consult doctors and those who do may have reasons other than the symptoms themselves.
Severity of symptoms may be important but psychosocial factors must also affect the decision.
The answer to the question ‘Why has this patient come to see the doctor now?’ may be a good therapeutic clue.
Fear of serious disease should be met with firm reassurance that none exists.
Threatening life events should be discussed.
Some patients may require psychological help or stress management skills, but simple supportive psychotherapy by the attending physician can be salutary.
In a Swedish study, patients with IBS treated with eight sessions of ‘supportive psychotherapy which could be performed in any physician's office’ had fewer symptoms and psychological and physical disability compared with a control group when they were seen 3 months later.
This observation supports the notion that early, careful attention to the patient's psychosocial concerns is effective and has lasting benefit.
Such an approach is validated by a UK study which also showed that improvement is most likely if psychopathology is recognised and the pain is not constant.
Anxiety and depression must be treated on their own merits.
Although psychotherapeutic agents do not alter IBS symptoms, treatment of the psychological distress may help a patient to cope with them.
In some pain-dominant patients with IBS, tricyclic antidepressants may be helpful even if depression is not obvious.
These drugs have proved effective in other chronic pain syndromes.
They may act via central pathways that influence the perception of pain.
Drugs and placebos
‘…not a single study offers convincing evidence that any therapeutic agent is effective in treating the IBS symptom complex.’
— D. B. Klein, 1988.
Klein reviewed the so-called controlled trials of drugs in IBS over 20 years and found them all flawed.
The entry criteria were usually unclear, and the symptom criteria of Manning, Kruis, and Rome are recent innovations.
Many studies were too small, too short, or had too many drop-outs.
Others had inappropriate trial design or were marred by faulty statistical analysis.
No drug has been proven globally effective in IBS.
Physicians should discourage the chronic use of costly systemic drugs whose unwanted consequences may be more troublesome than IBS itself.
There is a place for the commonsense use of drugs.
If diarrhoea is the dominant symptom and the threat of incontinence an embarrassing social handicap, provided that the rectum is not impacted on examination, loperamide may be of some use.
This drug not only slows gut motility and decreases small bowel secretion, but also increases anal  sphincter strength.
Other examples of such targeted therapy include bran or psyllium for constipation, a 1-beta galactoside for excessive flatus, and amitriptyline for chronic abdominal pain.
An important feature of IBS symptoms is their tendency to improve with placebo: in clinical trials the placebo response ranges from 40 to 70%.
This shows the variability of the disease and supports the contention that no pharmacotherapy is generally acceptable for IBS patients without convincing demonstration of efficacy in defined-entry, placebo-controlled, double-blind clinical trials.
Placebos may be useful in certain circumstances.
It is said that if a placebo is to have a therapeutic effect, the patient must believe that it will.
Nevertheless, in a group of neurotic patients, a placebo was effective even when they knew the pills they were taking were inert.
It seems that the symbolic giving of medication has a therapeutic value.
Certain logical ‘placebos’ such as bran or simethicone, which have a plausible rationale, may exploit these phenomena.
The fourth and most important implication of the placebo response is that it reminds us of the beneficial effect of the successful physician-patient encounter.
Continuing care
For many troubled IBS patients seen by specialists, cure, or even acceptance of the diagnosis is an unrealistic goal.
Emotionally disturbed patients benefit from regular, brief visits.
These serve to reassure and control ‘doctor-shopping’ with inappropriate ordering of tests and treatments.
Through such visits the doctor remains vigilant for a change in symptoms.
Assistance may be sought from psychologists, psychiatrists, or other practitioners if needed.
Some patients may benefit from a stress management programme, while severe cases may require the multidisciplinary services of a pain clinic.
Although biofeedback and hypnotism seem to benefit some patients, such services are not often readily available.
If the physician seems to lose the confidence of a patient, referral to a colleague may help by confirming the diagnosis and reinforcing the management plan.
Graded therapeutic response
We have noted the differences in needs among patients with IBS.
The therapeutic response must be tailored to these needs if we are economically and effectively to use our resources.
Most of those with IBS do not seek medical attention.
Many of those who consult a primary care physician have acute symptoms that worry them and will respond to explanation and reassurance.
Those who return for the same symptoms or are referred may require more attention.
Those who chronically seek help from subspecialists are a small but costly subgroup in whom psychosocial factors may be more disabling than the gut symptoms themselves.
The primary care physician should emphasise the positive diagnosis, the chronic yet benign nature of the systems, the role of stress, and the ineffectiveness of drugs.
Bulk (eg, bran) improves constipation and is otherwise a safe, cheap placebo.
For non-responders, once the above items have been dealt with satisfactorily, supportive psychotherapy and drugs for specific indications may be added.
Overinvestigation or repeated testing without substantial indication undermines the patient's confidence in the doctor's conclusions.
Physiological scoring systems and audit
Scoring systems designed to rate the severity of an illness are being used for comparison of hospital units to identify different standards of care and to allocate resources.
One such scoring system is the Acute Physiology and Chronic Health Evaluation (APACHE) system which is designed to assess the severity of illness of patients in intensive care units (ICUs).
It is widely assumed that different ICUs can be compared by the ratio of actual mortality to that predicted by the APACHE score.
However, we suggest that the use of physiological data that can be influenced by medical and nursing intervention should not be used for audit.
For example, by good care a patient may be made less severely ill and, therefore, may have a lower actual mortality while, at the same time, accumulating only a low APACHE score with low predicted mortality.
This patient could have, therefore, the same mortality ratio as a patient treated inappropriately, who may have a higher actual mortality and a high APACHE score with greater predicted mortality.
Paradoxically, the very accuracy of these scoring systems for assessing the severity of illness precludes their use for comparison and audit.
Assessing standards and comparing outcomes are important in medical practice and allow comparison of different units and appropriate allocation of resources.
Patient numbers and mortality fail to take into account the influences of different demographic groups or diagnoses and so these simple data are being abandoned for more complex assessments that generate an overall score and tell us something about how ill the patient is.
It is widely assumed that these scoring systems can be used for comparisons.
Scoring systems can be simple such as the Glasgow coma scale or the Apgar score, or can require more formal data collection such as the Trauma Related Injury Severity Score (TRISS) or the Acute Physiology and Chronic Health Evaluation (APACHE) II score, but they often include physiological data generated from the patient such as heart rate, blood pressure, or respiratory rate.
We question the philosophy of using data that can be influenced by therapy for comparison of units.
APACHE II is widely practised in intensive care units (ICUs) and has become the ‘gold standard’ for rating the severity of illness of critically ill patients.
However, APACHE II was originally designed to compare the performance of ICUs.
The score is obtained by finding the most abnormal value for physiological indices in the 24 h after admission to ICU; additional points are given for age, chronic health status, and renal function.
The primary reason for admission to ICU is chosen from a number of diagnoses.
Each diagnosis carries a weighting, and the combination of this and the accumulated points gives a predicted hospital mortality.
It is supposed that the ratio between the actual and predicted hospital mortalities, a value known as the standardised mortality ratio (SMR), can be used to compare ICUs, but the following example illustrates the problem.
Assume that two identical patients are admitted to two ICUs.
The patients, being the same, have the same abnormalities of their physiological variables at the moment of admission, and also the same age and chronic health status.
In the first ICU the patient is correctly diagnosed and treated to correct the physiological variables.
In the second ICU the patient deteriorates, with worsening of physiological indices, before resuscitation eventually succeeds in restoring these indices towards normal.
In the second unit the patient will clearly score more APACHE II points and will, therefore, have a higher predicted mortality than the same patient in the first unit.
The second ICU, in allowing the patient to deteriorate, has performed less well.
A number of reasons such as underfunding or worse medical staff may be responsible.
A comparison of these ICUs could be very useful, but it is just this situation where the accumulation of points for physiological indices fails to allow such a comparison to be made.
The second ICU allows most of its patients to accumulate extra points for physiological data, and, therefore, has a higher average predicted mortality than the first.
It also has a higher actual mortality, because it looks after its patients less well, and so the SMR between the two ICUs will reveal little difference.
The higher APACHE II scores of patients in the second ICU indicate that the patients were more severely ill, as confirmed by the higher mortality rate.
However, what is missed is the fact that the patient is allowed to become, or is being made, more severely ill in the second ICU.
If a difference in SMR had been found, then one of the explanations for this might have been a difference in performance between the two ICUs.
Our example illustrates, however, that units that are performing badly and units that are performing well may have the same SMR.
There will be, therefore, a temptation to judge the units by cost criteria alone, and expensive, but possibly good, units will be penalised.
It could be argued that a poorly performing ICU is unlikely to occur in isolation.
Poor care is likely to be given throughout the inpatient stay thereby increasing the actual mortality and giving a higher SMR.
APACHE II scores on admission to ICUs cannot be used for comparison of performances of ICUs as they fail to take into account the influences that management prior to ICU admission, such as stabilisation in an operating theatre or ward, may have on physiological variables.
While useful for assessing the severity of illness of individual patients or groups of patients, physiological data that can be influenced by medical and nursing intervention, such as that obtained by the APACHE II score, cannot, paradoxically, be used to compare unit performances and must not be used for audit.
We submit that even scores such as the Glasgow coma scale and the Apgar score, which are assumed to be independent of medical management, may, in fact, be susceptible to a similar effect to that demonstrated with APACHE scoring.
In certain circumstances, medical and nursing intervention can change all these scores.
Therefore, to compare the severity of illness of patients, two options remain.
The first is to use data over which nursing and medical intervention have no influence.
The second is to add factors to the scoring system that take into account the interventions that have to be performed to achieve the physiological result.
Some systems, such as TRISS, come close to these criteria but inconsistencies in the timing of data recording will lead to the results being influenced by therapy.
We await the development of scoring systems that meet these criteria and allow comparisons of hospital units.
REVIEW ARTICLE
Are both aspirin and heparin justified as adjuncts tothrombolytic therapy for acute myocardialinfarction?
The use of aspirin and heparin as adjuncts to thrombolysis in the setting of acute myocardial infarction is controversial.
Because the two agents have independent inhibitory effects on platelet function and the coagulation cascade, it has been argued that both should be used.
Although the recommendation of combined use was based on rational pathophysiological principles, only randomised trial data can be used to assess the net risk-to-benefit ratio for the combination of these agents.
We therefore reviewed data from randomised trials of myocardial infarction which provide evidence on the clinical efficacy of aspirin and heparin, alone or in combination, as adjuncts to thrombolysis.
Evidence for aspirin as sole adjunctivetherapy to thrombolysis Aspirin, if given in oral or intravenous doses of at least 2 mg/kg (162.5 mg in a typical man), produces a rapid clinical antithrombotic effect due to immediate and near-total inhibition of thromboxane function and platelet aggregation.
The Second International Study of Infarct Survival (ISIS-2) showed conclusively the efficacy of aspirin both alone and in conjunction with thrombolytic therapy for the treatment of evolving acute myocardial infarction.
The trial was randomised, double blind, and placebo controlled with a 2×2 factorial design; 17,187 patients who presented within 24 h of symptom onset were assigned treatment with 162.5 mg aspirin daily by mouth, 1.5 million units of streptokinase intravenously over 60 min, both, or neither.
The primary endpoint, total vascular mortality, was 23% lower with aspirin alone than with neither treatment (95% CI -30 to -15, p < 0.00001), 25% lower with streptokinase alone (-32 to -18, p < 0.0001), and 42% lower with combined treatment (-50 to -34, p < 0.00001).
Thus, the effects of aspirin and streptokinase are largely additive.
For patients whose thrombolytic treatment was initiated within 6 h of symptom onset, the reduction in total vascular mortality was 30% with streptokinase alone and 53% with both aspirin and streptokinase.
The mortality benefit attributable to aspirin therapy in ISIS-2 was similar when the drug was started 0–4 h (25%), 5–12 h (21%), or 13–24 h (21%) after symptom onset.
With  aspirin there were also highly significant reductions in non-fatal reinfarction (49%) and non-fatal stroke (46%).
Similarly, there were fewer reinfarctions after aspirin plus streptokinase than after streptokinase alone.
As regards side-effects, for bleeds necessitating transfusion, there was no significant difference between the aspirin and placebo groups (both 0.4%), although there was a small significant absolute increase in minor bleeds among patients assigned aspirin (0.6%, p < 0.01).
There was no difference between the aspirin and placebo groups in the incidence of cerebral haemorrhage.
Thus, substantial mortality benefits can be achieved if at least 162.5 mg aspirin is given during the acute phase of evolving myocardial infarction.
Moreover, this benefit is largely additive to that of thrombolysis and can be obtained with a slightly increased risk of minor bleeding and no increased risk of cerebral haemorrhage.
Therefore, we believe that virtually all patients being treated for suspected acute myocardial infarction should receive a full dose of aspirin as soon as possible.
Unfortunately, aspirin use in acute myocardial infarction is not universal.
In a report on the treatment of acute myocardial infarction in US academic centres, only 58.7% of all post-infarct patients were taking daily aspirin 10 days after the index infarction.
The proportion receiving aspirin within the first 24 h may be substantially lower.
Evidence for heparin as sole adjunctivetherapy to thrombolysis
There have been no mortality trials of adequate sample size to assess the role of heparin as sole adjunctive therapy to thrombolysis.
Subgroup analyses from 433 patients in the SCATI trial provide the only available mortality data on the addition of heparin to thrombolysis in the absence of aspirin.
In the subgroup that received streptokinase with or without subcutaneous calcium heparin, there were 10 deaths among 218 patients randomised to heparin and 19 among 215 receiving placebo (not significant, 95% CI 0.22–1.02).
Furthermore, no significant differences between treatments were found for recurrent ischaemia or non-fatal reinfarction.
The ISIS-2 trial, although not designed to assess the effects of heparin, also provided subgroup data for patients who received subcutaneous or intravenous heparin at the discretion of the treating physician.
Mortality among streptokinase-treated patients was 8.3% for those who also received intravenous heparin and 9.0% for those receiving subcutaneous heparin.
Neither mortality rate was as low as that achieved by the combination of streptokinase and aspirin without heparin (8.0%).
We must emphasise that non-randomised comparisons of heparin and no heparin in ISIS-2 may be subject to substantial bias.
If the decision to use heparin was related to severity of infarction, mortality could be falsely high in the heparin-treated groups.
Conversely, because early death prevents the use of heparin altogether, mortality may be falsely high in the no-heparin groups.
Despite the paucity of randomised trial data showing a mortality benefit of heparin as adjunctive therapy to thrombolysis, routine treatment of acute myocardial infarction in many parts of the world includes immediate intravenous heparin.
This practice is based largely on the belief that intravenous heparin increases coronary artery patency rates.
However, patency data on heparin as sole adjunctive therapy to thrombolysis do not clearly support this view.
In fact, the two trials that directly compared intravenous heparin with no heparin as adjunctive therapy to thrombolysis in the absence of aspirin give contradictory results.
In Bleich and colleagues' study of tissue plasminogen activator (tPA) with or without intravenous heparin, the initial angiogram showed coronary patency in a higher proportion of patients who received heparin (71 vs 43%, p=0.015), but there was no significant effect on rates of recurrent ischaemia or reocclusion in the week after infarction.
By contrast, in the trial by Topol et al, addition of intravenous heparin to tPA had no effect on coronary patency rates measured at 90 min (79 vs 79%).
Two direct comparisons of the effect on patency of heparin and aspirin as adjunctive therapy to thrombolysis are equally difficult to interpret.
The Heparin Aspirin Reperfusion Trial (HART) directly compared tPA plus early intravenous heparin with tPA and 80 mg aspirin by mouth; coronary patency rates 7–24 h after therapy started were higher in the heparin than the aspirin group (82 vs 52%, p < 0.0001).
However, the 80 mg dose of aspirin used was inadequate to achieve a rapid clinical antithrombotic effect during the 7–24 h when angiography was done.
Several studies have shown that whereas full-dose aspirin achieves almost immediate platelet inhibition, doses of 80 mg take as long as 48 h to become maximally effective.
Coronary patency was also measured in the HART study at 1 week, when the low dose of aspirin would have achieved full effect; the patency rates in initially patent vessels were 88% in the heparin group and 95% in the aspirin group.
The other trial that compared heparin and aspirin plus thrombolysis is that of the National Heart Foundation; selected patients treated with tPA and 24 h intravenous heparin were then randomised to continued intravenous heparin or aspirin (300 mg) plus dipyridamole (300 mg) daily for a week.
The dose of aspirin was adequate to achieve rapid clinical antithrombotic effect.
The trial showed no difference in coronary artery patency between the continued heparin and aspirin plus dipyridamole groups (81.1 vs 80.2%).
Moreover, mortality and reinfarction rates were similar and left-ventricular ejection fractions at 1 month were virtually identical.
Thus, this trial shows that 24 h after tPA infusion, an adequate antiplatelet regimen is at least as effective as intravenous heparin in maintaining vessel patency, preserving left-ventricular function, and preventing reocclusion.
Thus no randomised trial with mortality as a pre-specified endpoint has shown a benefit of heparin as sole adjunctive therapy for thrombolysis.
Furthermore, patency studies provide inconsistent evidence of efficacy, and no trial has shown a benefit of intravenous heparin over aspirin in adequate doses.
Evidence for combining aspirin and heparinas adjunctive therapy
Data for full-dose aspirin plus heparin as adjuncts to thrombolysis are available from large mortality trials with delayed subcutaneous heparin and small patency studies with early intravenous heparin.
The Gruppo Italiano per lo Studio della Sopravvivenza nell'Infarcto Miocardico (GISSI-2), its international extension, and the Third International Study of Infarct Survival (ISIS-3) are large mortality trials that have provided reliable and consistent information on the risks and benefits of combined aspirin and subcutaneous heparin or aspirin alone as adjuncts to thrombolysis (table I).
GISSI-2 and its international extension compared directly the efficacy of two thrombolytic agents (streptokinase and tPA) and also tested whether there are additional net benefits of adding anticoagulation with delayed subcutaneous heparin to aspirin therapy plus thrombolysis.
All 20,891 patients received 300–325 mg aspirin daily by mouth, starting at the time of randomisation; thus a substantial antithrombotic effect would have occurred by the end of the thrombolytic infusion.
GISSI-2 found no differences in overall mortality between streptokinase and tPA, although streptokinase was associated with significantly fewer strokes.
The addition of delayed subcutaneous heparin to a regimen of full-dose aspirin and thrombolysis had no significant effect on in-hospital mortality (8.5 vs 9.0%, p=0.29) or total mortality at 35 days (9.3 vs 9.4%, p=0.82).
Because the subcutaneous heparin regimen in GISSI-2 was delayed, it could have no more than a moderate effect on activated partial thromboplastin time.
Even so, it was associated with a significantly higher rate of bleeding necessitating transfusion (table I).
This randomised comparison is consistent with subgroup data from ISIS-2.
ISIS-3 compared the efficacy of streptokinase, tPA, and anistreplase and of subcutaneous heparin versus no heparin among 41,299 patients with evolving myocardial infarction.
All patients received aspirin (162 mg daily) and the first dose was chewed for rapid and full antithrombotic effect.
ISIS-3 found no differences in overall mortality among the thrombolytic agents tested, although cerebral haemorrhage was significantly less frequent with streptokinase than with the other agents.
Nor was the addition of subcutaneous heparin to aspirin associated with any reduction in the prespecified endpoint of 35-day mortality (table I).
During the scheduled 7 days of heparin use there were slightly fewer deaths in the aspirin plus heparin group than in the aspirin alone group (7.4 vs 7.9%, p=0.06).
As in GISSI-2, the addition of delayed subcutaneous heparin to aspirin was associated with higher frequencies of important side-effects, including major non-cerebral haemorrhage and definite or probable cerebral haemorrhage (table I).
Reinfarction rates did not differ significantly between aspirin alone and aspirin plus subcutaneous heparin groups.
Taken together, the GISSI-2 and ISIS-3 trials provide data on more than 62,000 patients.
Overall, they show that treatment with subcutaneous heparin in addition to aspirin and thrombolysis has no significant effect on mortality but does result in an excess of major bleeding necessitating transfusions, as well as cerebral haemorrhage.
As well as large mortality trials assessing subcutaneous heparin, there have been three angiographic trials in which patients receiving thrombolysis and full-dose aspirin were randomly allocated intravenous heparin (table II).
Only one found any significant difference between heparin and no-heparin groups in coronary patency, and even that small increase was not associated with any significant improvement in enzymatic infarct size or recurrent ischaemic episodes.
Thus, despite the widespread use of intravenous heparin no patency data so far support the additional use of this agent once full-dose aspirin has been given with streptokinase or anistreplase.
For tPA, intravenous heparin added to aspirin seems to provide a small additional patency benefit.
However, this benefit must be weighed against the higher rates of haemorrhage associated with intravenous heparin as well as the higher rates of stroke with tPA than with other thrombolytics.
The routine addition of intravenous heparin to tPA therefore may not be clinically advisable, especially since tPA given with full-dose aspirin alone gives much better patency rates (75%) than tPA with no aspirin (43%) or inadequate aspirin (52%).
Despite a lack of consistent evidence from patency trials supporting intravenous heparin, the use of delayed subcutaneous heparin in the GISSI-2 and ISIS-3 trials has been widely criticised.
Sobel and Collen suggested that clinical decisions on intravenous heparin should await the outcome of unfinished trials.
However, no large-scale randomised trial has been done, is forthcoming, or is planned that assesses the addition of intravenous heparin to  full-dose aspirin in the setting of thrombolysis.
The GUSTO trial assessed the risks and benefits of immediate intravenous heparin or delayed subcutaneous heparin in addition to streptokinase (and found no mortality benefit), but did not make such comparisons for tPA or anistreplase nor provide any mortality comparison of intravenous heparin and no heparin.
Also, because the tPA regimen (front-loaded) used in GUSTO differed from that used in GISSI-2 and ISIS-3 we do not know whether differences in efficacy are due to the more aggressive heparin regimen or to the more aggressive thrombolysis.
At present, the only available data on the addition of intravenous heparin to full-dose aspirin plus thrombolysis are from non-randomised comparisons.
In ISIS-3, 4852 patients received intravenous heparin off-protocol.
Overall, their mortality rate was higher than that of patients who received subcutaneous heparin but lower than that of those receiving no heparin.
As discussed above, such non-randomised comparisons are likely to be biased.
Thus, although these subgroup data are compatible with the possibility that intravenous heparin adds a small benefit to that already achieved by aspirin and thrombolysis, they also raise concerns about excess risks of stroke (2.1% with intravenous heparin, 1.1% with subcutaneous heparin, 1.2% with none).
Net benefit-to-risk ratio of aspirin and heparin in the setting of thrombolysis
Despite the lack of some important information, we believe the currently available randomised trial data can guide clinicians weighing the benefits and risks of adding aspirin and heparin to thrombolysis (table III).
For every 1000 patients admitted to hospital with acute evolving myocardial infarction, about 100 can be expected to die within 35 days.
From the results of ISIS-2, the use of full-dose aspirin immediately would prevent 23 of these premature deaths and cause no increase in risk of cerebral haemorrhage.
From the results of all randomised placebo-controlled mortality trials of thrombolytic treatment, its use within 6 h would prevent another 27 premature deaths, although 2 or 3 treated patients would have non-fatal cerebral haemorrhage.
Since the effects of aspirin and thrombolysis are largely additive, nearly half of all expected deaths among 1000 acute infarctions can be avoided by their combined use.
Once aspirin and thrombolysis are given, however, the additional benefit of delayed subcutaneous heparin seems limited.
From GISSI-2 and ISIS-3, addition of subcutaneous heparin is unlikely to reduce mortality but will cause 1 or 2 further non-fatal cerebral haemorrhages and 3 other bleeds necessitating transfusion.
The risks and benefits of adding intravenous heparin to aspirin and thrombolysis are unknown and will remain so until trials are done to address this question adequately.
We believe therefore that the available evidence strongly supports the use of full-dose aspirin as adjunctive therapy to thrombolysis.
By contrast, the routine immediate use of heparin for all patients being treated with aspirin and thrombolysis still requires testing in large-scale randomised trials.
If designed appropriately, future studies on new thrombin inhibitors, such as hirudin and hirulog, can provide important and relevant information to this unanswered but crucial clinical question.
ARRHYTHMIA OCTET
Torsades de pointes and proarrhythmia
Definitions of torsades de pointes and longQT interval syndromes Torsades de pointes is a ventricular tachycardia characterised by QRS complexes of progressively changing amplitude and contour that seem to revolve around the isoelectric line.
The peaks of these complexes are successively located on one side of the baseline and then on the other, and this positive then negative polarity in the frontal plane of the ECG creates the typical twisting-about-a-point appearance (fig 1).
Rates are 180–250/min.
Long-short R-R interval cycle sequences commonly precede the onset of torsades de pointes.
Moderately late premature ventricular complexes can discharge during the termination of the long T wave, precipitating successive bursts of ventricular tachycardia (VT).
The VT is commonly non-sustained but sometimes persists long enough to provoke syncope, or even death if ventricular fibrillation supervenes.
The diagnosis of torsades de pointes is based on a characteristic VT and prolonged ventricular repolarisation time, with Q-T intervals usually exceeding 500 ms.
Additional features that are sometimes present include a prominent U wave, abnormal contour of T or TU waves, T wave alternans, a subnormal spontaneous sinus rate (especially in children), and sinus pauses.
Long QT interval syndrome (LQTS) can be either idiopathic (congenital) or acquired.
Idiopathic LQTS consists of three groups of patients who have a congenital disorder characterised by the above features:(a) the Jervell-Lange-Nielsen syndrome is transmitted as an autosomal recessive and features include congenital neural deafness;(b) patients with the autosomal dominant Romano-Ward syndrome have normal hearing; and (c) the sporadic form consists of a non-familial group with normal hearing.
Acquired LQTS can be produced by class 1 and class 3 antiarrhythmic agents and a host of other drugs, as noted below.
Pathophysiology: hypotheses
The electrophysiological mechanisms responsible for the long QT interval and torsades de pointes in patients with acquired and idiopathic LQTS are uncertain.
Based on the mode of onset of the arrhythmia, Jackman et al proposed a descriptive classification, labelling acquired LQTS ‘pause-dependent’ and the idiopathic forms ‘adrenergic-dependent’.
Experimental studies and clinical observations suggest that the cause of abnormal repolarisation may lie within the heart itself, perhaps in the form of an abnormal channel protein that reduces or blocks an outward repolarising potassium current or increases an inward depolarising calcium or sodium current.
The intracardiac abnormality results in afterdepolarisations (see Waldo and Wit, May 8, p 1189), probably early afterdepolarisations (EADs), that cause triggered activity and torsades de pointes.
Early afterdepolarisations are transient membrane oscillations triggered by a previous cardiac depolarisation that can be sustained in succeeding cycles by repeated triggering of the EAD to cause a tachyarrhythmia (fig 2).
Blockade of potassium channels or augmentation of the calcium current can produce EADs that lengthen repolarisation and cause triggered activity and ventricular tachyarrhythmias experimentally (fig 3).
EADs may be responsible for the prolonged repolarisation and the associated ventricular arrhythmias in both syndromes.
Slow heart rates generally increase the amplitude of EADs whereas fast heart rates suppress them.
This observation accords with the clinical finding that torsades de pointes often begins after a pause in the cardiac cycle and can be suppressed by pacing at a more rapid rate.
EADs have been documented from the endocardium in a patient with quinidine-induced long QT interval and torsades de pointes.
Intravenous magnesium sulphate promptly suppresses the acquired form of torsades de pointes and likewise suppresses caesium-induced EADs, prolonged QT interval, and triggered ventricular tachyarrhythmias in experimental studies.
This observation further supports the use of magnesium to treat torsades de pointes accompanying acquired LQTS and furnishes evidence for the hypothesis that EADs cause the ventricular tachyarrhythmias.
In patients with idiopathic LQTS, cardiac ‘sympathetic imbalance’ has also been proposed as an underlying mechanism.
According to this concept, reduced right cardiac sympathetic innervation, presumably on a congenital basis, results in a reflex increase in left cardiac sympathetic activity, thereby causing LQTS.
The experimental and clinical support for this hypothesis has lately been questioned.
Although the left stellate ganglion has a greater effect on arrhythmogenesis than the right in many instances, this is most likely due to quantitative rather than qualitative differences between the two ganglia.
Thus, the left ansae subclaviae innervates most of the ventricular myocardium, and stimulation of the left stellate ganglion results in a greater overflow of adrenaline in coronary sinus blood than does stimulation of the right stellate ganglion.
This quantitative difference may also account for the beneficial effects seen after surgical interruption of the left stellate ganglion.
Sympathetic stimulation (mainly left) caused by physical or emotional stress could periodically increase the amplitude of EADs that were present because of the intrinsic repolarisation abnormality, so that they reach threshold and provoke ventricular tachyarrhythmias.
Deflections consistent with EADs have in fact been recorded from the hearts of patients with idiopathic LQTS and their amplitude was increased by adrenaline administration.
Clinical study of sympathetic innervation patterns determined by the noradrenaline analogue, metaiodobenzylguanidine, and C-1 hydroxyephedrine scintigraphy in patients with idiopathic LQTS, have shown homogeneous uptake of the tracer in cardiac sympathetic nerve terminals in all regions of the heart.
These observations do not disprove the sympathetic imbalance hypothesis but certainly do not support it.
Keating and colleagues lately reported a link between a DNA marker at the Harvey ras-1 locus and LQTS.
This finding confirms the genetic basis of the disorder and localises this gene to the short arm of chromosome 11.
The protein encoded by the gene is one of the G proteins (guanine nucleotide binding proteins, see Steel, May 8, p 1187) and may help to control the passage of potassium ions through membrane channels; it is possible that the defect in LQTS lies in signal transduction at the level of these proteins.
Clinical features
Congenital long QT syndrome
Patients with congenital LQTS can present during childhood with syncope due to torsades de pointes, often precipitated by heightened sympathetic tone — eg, from exertion or fright or as a result of being startled.
Precipitating factors include loud noises such as alarm clocks and fire engine or police sirens; observation of blood, accidents, and so on; and sudden changes in body temperature such as when entering a swimming pool.
A typical story is of a youngster who has a syncopal episode while running across the school playground and is then sent for a neurological evaluation to exclude a seizure disorder.
Patients may have normal resting T and U waves that become enlarged and abnormal, both in contour and duration, during intense physical or emotional stress just before the onset of torsades de pointes.
Since sudden death can occur in this group of patients, the duration of the ventricular arrhythmia must become prolonged in some cases and can evolve into ventricular fibrillation.
However, unlike ventricular fibrillation caused by coronary artery  disease or other organic heart conditions, the tachyarrhythmia in LQTS subjects often reverts spontaneously to sinus rhythm, whereupon the patient regains consciousness.
The frequency with which arrhythmias and syncope occur varies considerably, from several times a day to less than once every several years.
Occasionally, syncopal episodes decrease in frequency and may even disappear with increasing age.
Sudden death can occur with the initial syncopal episode, but is more common after several such episodes.
In some individuals, hundreds of non-fatal syncopal attacks have been noted.
The frequency of sudden death may be as high as 80% in some families at great risk and less than 5% in others.
Patients at increased risk include those with family members who died suddenly at an early age and those who have experienced syncope.
Patients with Jervell-Lange-Nielsen syndrome may be more at risk than those with Romano-Ward syndrome.
The clinical manifestations of both genetic forms of LQTS are otherwise similar, except for the presence of deafness, as mentioned above.
Physical examination is normal in patients with LQTS, and there is no evidence of structural heart disease.
Chest radiographs and laboratory tests are likewise usually normal.
Haemodynamic and angiographic observations at cardiac catheterisation are unremarkable, although abnormal wall motion detected by echocardiography has been noted recently.
Electrophysiological studies with premature ventricular stimulation generally do not induce ventricular arrhythmias.
In some patients, EADs can be recorded from the right or left ventricular endocardium, especially after provocation with intravenous adrenaline.
ECGs should be obtained in all family members when the index case presents with symptoms.
The ECG manifestations may be more prominent in a sibling or a parent and can help to make the correct diagnosis.
Valsalva or stress testing can prolong the QT interval, produce T wave alternans, and reveal notched T waves, especially during the early recovery phase.
Patients should undergo prolonged ECG recordings, with various forms of provocation — eg, auditory stimuli, psychological stress, cold pressor stimulation, and exercise — designed to evoke ventricular arrhythmias.
Catecholamine infusions are helpful in some patients, but this challenge must be carried out cautiously, with resuscitation equipment and alpha and beta antagonists close at hand.
Spectral analysis of ambulatory ECG recordings from patients with congenital LQTS has shown an abnormal pattern of heart rate variability.
Acquired long QT syndrome
Acquired LQTS is largely an iatrogenic disease, most cases being related to drug therapy.
The agents and clinical situations that cause torsades de pointes also prolong repolarisation.
The principal manifestation of the syndrome is syncope or presyncope, often accompanied by palpitations.
Sudden death can occur.
Antiarrhythmic drugs — Among type 1a antiarrhythmic drugs, quinidine seems to have the greatest potential for causing torsades de pointes and is estimated to produce syncope in 0.5–4.0% of patients as a result of this tachyarrhythmia.
Quinidine prolongs the QT interval in most patients, whether or not ventricular arrhythmias occur, but significant QT prolongation (500–600 ms) is more often a characteristic of patients with quinidine syncope.
Many of these patients are also receiving digitalis or diuretics, and the latter increase the risk by producing hypokalaemia.
Syncope is unrelated to plasma concentrations of quinidine or duration of therapy, and torsades de pointes can occur at low (‘subtherapeutic’) plasma concentrations.
Procainamide and disopyramide likewise have the potential for causing torsades de pointes.
Many of the cases of torsades de pointes reported during procainamide therapy may be due to the actions of its active metabolite, N-acetylprocainamide, which often accumulates in patients with renal disease.
There is some evidence that patients who get torsades de pointes in response to one type of 1a antiarrhythmic drug have an increased incidence of this arrhythmia in response to other drugs in the same class.
Class 3 drugs such as amiodarone, sotalol, and sematilide can precipitate torsades de pointes.
Although class 1c agents such as encainide and flecainide can have pronounced proarrhythmic actions, torsades de pointes is uncommon.
Similarly, class 1b drugs do not produce torsades de pointes.
Non-cardiac drugs — Several pharmacological agents in addition to the antiarrhythmic drugs have been associated with LQTS and torsades de pointes — eg, phenothiazines, tricyclic and occasionally tetracyclic antidepressants, antihistamines of the H1 blocking type (astemizole and terfenadine), and some antibiotics, most notably erythromycin.
Phenothiazines have prominent electrophysiological effects, and moricizine, a phenothiazine derivative, is used to treat patients with ventricular tachyarrhythmias.
Thioridazine prolongs the QT interval, produces T wave changes and occasionally torsades de pointes, and has been implicated as a cause of sudden death in schizophrenics.
Other phenothiazines have less propensity to produce the arrhythmia.
Haloperidol has been implicated as a cause of torsades de pointes and death.
It is noteworthy that some apparently harmless drugs can, under the appropriate circumstances (eg, the combination of terfenadine and erythromycin), create life-threatening ventricular arrhythmias, and physicians can unwittingly expose their patients to risk.
Metabolic and electrolyte disorders — The relation between electrolyte abnormalities and cardiac arrhythmias is well known.
Hypokalaemia and hypomagnesaemia, when severe, can prolong repolarisation and precipitate torsades de pointes.
There seems to be a synergistic effect between these electrolyte disorders and therapy with class 1a antiarrhythmic drugs.
Liquid protein diets, and nutritional disorders such as starvation and anorexia nervosa, have likewise been associated with QT prolongation, torsades de pointes, and sudden death.
There are a few isolated reports of hypothyroidism as a cause of torsades de pointes, but in most of these cases other abnormalities were present.
Bradycardia — Bradycardia can beget tachycardia.
VT has long been recognised as a complication of severe bradyarrhythmias, and less commonly can be the cause of syncope and death in patients with complete atrioventricular block or sinus node dysfunction.
Central nervous system lesions — Intracranial disease, most notably subarachnoid haemorrhage but also intracerebral haemorrhage, cerebrovascular occlusive disease, trauma, and encephalitis, occasionally produces torsades de pointes, probably due to the influence of the autonomic nervous system on ventricular repolarisation.
Miscellaneous — Mitral valve prolapse or cardiac ganglionitis can be responsible for LQTS, and there may be an association between LQTS and sudden infant death syndrome.
Certain cardiac substrates — eg, left ventricular  hypertrophy — may be associated with a prolonged repolarisation time and a predisposition to acquired LQTS.
Proarrhythmia
Antiarrhythmic agents can worsen existing arrhythmias or cause new ones.
This effect of drug-induced aggravation or provocation of arrhythmias is known as ‘proarrhythmia’.
All types of bradyarrhythmia and tachyarrhythmia, whether supraventricular or ventricular, can be worsened or provoked by antiarrhythmic drugs.
The aggravation can be manifested as an increase in the frequency of arrhythmic episodes, in their duration, in the rate of the arrhythmia, or as an altered response of the arrhythmia to DC countershock.
Torsades de pointes is the prototype of the new ventricular tachyarrhythmias provoked by antiarrhythmic drugs.
Incessant monomorphic VT is another common form of proarrhythmia; this arrhythmia is often the result of class 1c antiarrhythmic agents and usually has a QRS complex with a very long duration so that it appears ‘sinusoidal’ in contour.
The arrhythmia is sustained and occurs more commonly in patients with a reduced ejection fraction who are receiving treatment for VT.
During therapy with class 1a drugs, the risk of developing proarrhythmia is increased by the presence of bradycardia, hypokalaemia, or hypomagnesaemia.
Atrial fibrillation, with its constantly varying RR intervals and varying action potential durations, also seems to increase the risk of proarrhythmia.
Although women comprise the minority of patients receiving antiarrhythmic drugs for ventricular arrhythmias, they represent almost half the reported cases of torsades de pointes due to class 1 drugs.
It is impossible to predict when QT prolongation will be beneficial, in that it represents a prolongation of ventricular refractoriness, and when it will be harmful.
The QT interval beyond which the risk of torsades de pointes increases abruptly has not yet been defined and most likely there is a continuum.
However, patients with quinidine-induced torsades de pointes usually have QT intervals exceeding 550 ms.
Patients at risk are probably those with pre-existing QT prolongation, bradycardia not corrected by a pacemaker, hypokalaemia or hypomagnesaemia, a history of developing torsades de pointes with similar drugs (because of cross-sensitivity), and genetic abnormalities in drug metabolism.
Perhaps an increase in regional QT interval dispersion — ie, refractory period prolongation in some areas of the ventricle but not others — during class 1a drug therapy can predict the occurrence of torsades de pointes.
Patients at risk of a proarrhythmic response also include those with a history of sustained VT and reduced ejection fraction.
In general, drug-induced ventricular tachyarrhythmias develop soon after drug therapy begins.
Onset of proarrhythmia after a period of stable, long-term drug treatment is probably due to an intervening event such as ischaemia, hypokalaemia, the addition of another drug, or change in drug dose.
During the CAST investigation, patients who had a proarrhythmia response during the early period of drug exposure were eliminated from the long-term study.
Subsequently, an apparently very different kind of proarrhythmic response occurred: patients died at a constant rate during the 10-month treatment period with flecainide and encainide, and the mortality paralleled the number of observed ischaemic episodes.
These data suggest an interaction between the antiarrhythmic agent and ischaemia — ie, the antiarrhythmic drug, which slows conduction as its usual mechanism of action, may have intensified the regional ventricular conduction delay accompanying ischaemia and caused ventricular fibrillation during ischaemic episodes.
Management
For patients with the acquired LQTS and torsades de pointes, the cause of the long QT should be determined and corrected if possible.
Avoidance of precipitating drugs is mandatory.
Intravenous magnesium and temporary atrial or ventricular pacing are initial choices of therapy.
Temporary pacing suppresses the VT, which often does not recur even after cessation of pacing.
Isoproterenol to increase heart rate can be tried cautiously until pacing is instituted.
Class 1b drugs can also be tried since they decrease action potential duration.
The vasodilators, pinacidil and cromakalim, decrease cardiac action potential duration by activating the ATP-dependent potassium current; these potassium channel activating drugs may become useful in the future to control arrhythmias related to prolonged repolarisation and EADs.
Prostaglandins are other therapeutic possibilities.
For patients with idiopathic LQTS who do not have syncope, complex ventricular arrhythmias, or a family history of sudden cardiac death, no therapy is required.
In symptom-free patients with complex ventricular arrhythmias or a family history of early sudden cardiac death, beta-adrenoceptor blockers should be given at maximally tolerated doses.
In patients with syncope, beta-blockers at maximally tolerated doses can be tried, perhaps in combination with a class 1b antiarrhythmic agent.
If the patient continues to have syncope despite maximum drug therapy, left-sided cervicothoracic sympathetic ganglionectomy that interrupts the stellate ganglion and the first three or four thoracic ganglia can be helpful.
The fact that surgical interruption of the left stellate ganglion reduces the incidence of syncope and sudden death after unsuccessful treatment with beta-blockers underscores the potential arrhythmogenic role of alpha-adrenoceptor stimulation in LQTS, because severing neural innervation provides alpha as well as beta adrenoceptor interruption.
There may prove to be groups of patients with LQTS in whom alpha rather than beta adrenoceptor stimulation is more arrhythmogenic and vice versa.
Results from specific automatic challenges might better direct therapy.
For example, patients who have induction of EADs and replication of their ventricular tachyarrhythmias during infusion of phenylephrine but not of isoproterenol might be better treated with alpha-blockers than with beta-blockers.
They may even represent the beta-blocker treatment failures.
The combination of beta-blockers and permanent pacing is effective for some symptomatic patients with idiopathic LQTS.
Implantation of an internal cardioverter-defibrillator is advisable in patients who have syncope despite sympathetic interruption.
BOOKSHELF
When Illness Strikes the Leader
Jerrold M. Post, Robert S. Robins.
London: Yale University Press.
1993.
Pp 243. £19.95.
ISBN 0-300056834.
In 1893, President Cleveland developed cancer of the hard palate.
For political reasons connected with the gold standard, it was decided no one outside his immediate circle should know.
His advisers hit on the idea of announcing that he was going on a cruise up the Hudson river where, away from prying eyes, he was propped up in a chair against the mast and anaesthetised.
Most of his upper jaw was removed and a prosthesis was fitted.
The official story was that the President had a dental extraction for severe toothache.
The true story remained unknown until 1928.
The case of Woodrow Wilson is well known.
In 1919, a stroke left him paralysed down the left side, confused, unable to read or dictate, and staring vacantly into space.
Impossible you might think, for this to be kept secret — but it was, from all except his doctor, his personal aide, and his wife who, in effect, ran the US government for seven months.
Churchill suffered from two strokes, the second leaving him completely unable to carry out his duties.
Once again the seriousness of the illness was known to only a small group who concocted an official bulletin stating the Prime Minister was exhausted and needed a prolonged rest.
There are many such instances where secrecy was maintained so that the leader or members of his entourage could remain in office, irrespective of potential or actual damage to government.
But the greatest danger occurs when a leader is plagued by paranoia.
King Ludwig of Bavaria and Idi Amin are two examples; President Macias Nguema of Equatorial Guinea became so obsessed with imaginary enemies in the 1970s that he butchered more than 10 per cent of his country's population, including almost everyone identified as an intellectual.
Such stories are enough to send cold shivers down the spine.
We learn that kings, presidents, and politicians have remained in office while either suffering behind a wall of secrecy from serious mental or physical illness or living under the influence of alcohol or drugs (Hitler was addicted to cocaine, President Kennedy and Anthony Eden to large and frequent doses of amphetamines).
If it has happened in the past, it can happen in the future, and may be occurring now for all we know.
Various authors have penetrated the veils of medical confidentiality and political secrecy to write  about the illness of world leaders: Hugh L'Etang, for example, in 1970 and 1980, and Bert Park in 1986.
The present authors have expanded the range to produce a huge compendium of ill, mad, or bad leaders, some famous, many unknown.
Several case histories are superficial or doubtful.
de Gaulle is squeezed in as suffering from ‘terminal grandiosity’.
What makes them think it was terminal?
The way the book is constructed has led to repetition, confusing sub-headings, a badly laid out text, and a notable absence of geographical or chronological structure.
In chapter 2, entitled ‘The Mad King: Mental Illness among the Mighty’, the cases of Thomas Eagleton, King Ludwig of Bavaria, King Talal of Jordan, Castlereagh, Churchill, Oliver Cromwell, Menachem Begin, Stalin, President Nguema, and Idi Amin are discussed in that order.
A similar bizarre arrangement occurs in all the chapters.
The authors, whose style is often banal and who favour such phrases as ‘the march of history’, end with a detailed discussion of how these evils might be avoided.
Since this subject is so important, it is a pity the book is so badly constructed that it lacks authority.
Nevertheless, if you are willing to clamber over the obstacles, you will be rewarded by a memorable collection of case histories which makes When Illness Strikes the Leader an entertaining and compelling read.
Sudden Death
Cardiac and other causes.
— Donald B. Hakel and Keith A. Reimer.
Carolina: Carolina Academic Press.
1993.
Pp 160. $75.
ISBN 0-890895465.
Hakel and Reimer make a book of a slender subject and somehow, despite 160 pages and 529 references, manage to slim it down to practically nothing.
Nothing, anyway, that helpfully addresses an important conundrum, often mentioned, seldom explored, and so far unsolved: when death occurs suddenly, and coronary atherosclerosis is found at necropsy (and nothing else that is evidently fatal), the certified cause of death will be given as ‘coronary artery disease’— or one of many synonyms — with no regard paid to the customary scientific standards of proving the relation between effect and cause.
It is not the pathologist's fault: he has to make a decision as to the cause of death and is not allowed to say he does not know.
The certified cause of death is of considerable importance because it is the usual end-point for epidemiological studies of ischaemic heart disease, and epidemiologists often assume that what is recorded on the death certificate is the cause of death.
It may not be, and Hakel and Reimer might have taken the opportunity to sow more seeds of doubt, or at least put the certified cause of death in an historical perspective: 100 years ago a recorded cause of death from heart disease (except for congenital malformations and valve defects) was barely known, yet ischaemic heart disease must surely have existed.
One might believe that, in common with advances made in the understanding of many other diseases, we now know better.
We do not.
The reason for many sudden deaths is still not known, but instead of saying so, we blame what is a commonplace incidental necropsy finding after the age of 25: sclerosed coronary arteries.
That objection apart, several other issues are skimped: the ‘café coronary’ and hypertrophic cardiomyopathy get a short paragraph apiece, while intracranial haemorrhage and the enduring mystery of pulmonary embolism are poorly covered.
This book is likely to prove useful only as a source of references.
Epidemiology and Control of Neural Tube Defects
J. M. Elwood, J. Little, J. H. Elwood.
Monographs in Epidemiology and Biostatistics, volume 20.
Oxford: Oxford University Press.
1992.
Pp 926. £85.
ISBN 0-192618849.
Neural tube defects — ie, craniorachisisis, anencephaly, and spina bifida aperta — are amongst the commonest congenital abnormalities.
The estimated annual number of births affected with a neural tube defect is about 400,000 world wide.
In several respects, control of neural tube defects can be considered an example of how research into congenital abnormalities has progressed more generally.
One can identify five phases.
Anencephalus is, of course, a lethal congenital abnormality.
Up to the late 1950s, few infants born with spina bifida aperta were treated actively and there was a wish, usually realised, that they would die.
During the second phase, in the 1960s, early surgery for the spinal lesion of spina bifida provided the best chance of preserving neuromuscular function.
This change led to an enormous effort to begin active treatment in the neonatal period for virtually all infants with this condition.
The result was a substantial increase in survival partly because of surgery and partly because of the greatly improved general care that infants received.
However, most survivors did not improve mentally or physically and large numbers remained with long-term disability.
Thus, the third phase of progress in the 1970s was characterised by selection for surgery of the most favourable cases with the expectation that those not selected for treatment would die.
This practice raised several serious ethical and legal issues.
Severe spina bifida was the first condition in which a policy of selective non-treatment was clearly enunciated.
Fortunately methods of antenatal diagnosis were developed and introduced in the 1980s, allowing parents to be offered termination of the abnormal fetus.
In the fourth phase of progress, population-based antenatal screening — maternal serum alpha fetoprotein and ultrasound screening, sometimes complemented by amniocentesis — has been proven to have a substantial effect.
Ultrasound can identify neural tube defects in anencephalic fetuses: the ultrasound detection rate for all studies evaluated in this book was 61% in open spina bifida.
Informed parents have to make a difficult choice.
However, I cannot imagine that the success of the ‘new’ medical genetics should be measured by the number of terminated fetuses.
New and appropriate alternative methods of genetic prevention are needed, and this will be the subject of the fifth phase in the neural tube defects story.
The primary prevention of neural tube defects by periconceptional multivitamin or folic acid supplementation seems to be appropriate for the reduction of both occurrence and recurrence.
Neural tube defects are the first common and lethal congenital abnormality for which a primary preventive action has been proposed and supported by the results of two large-scale randomised trials (Medical Research Council Vitamin Study and Hungarian Family Planning Programme).
The UK trial showed a dramatic  reduction in risk of recurrence of neural tube defects in births to high-risk women.
The mechanism of this effect remains unclear, although it could be viewed as a genetically determined vitamin dependent state rather than a vitamin-deficiency condition.
The history of the medical management of neural tube defects indicates that developmental abnormalities should not be considered an irreducible component of human disease.
Elwood, Little, and Elwood have produced an excellent and comprehensive account of the state of neural tube defect research up to 1991.
Cardiopulmonary Resuscitation
David V. Skinner, Richard Vincent.
Oxford: Oxford University Press.
1993.
Pp 214. £14.95.
ISBN 0-192619403.
The pièce de résistance of high drama in any hospital setting is a group of junior doctors rushing through the corridors clutching their crash bleeps on their way to a cardiopulmonary arrest.
Cardiopulmonary resuscitation (CPR) conjures up the image of a frenetic (often chaotic) and emotive scene where the dying are brought back to life.
Since the 1960s, with the help of modern technology, CPR has become an integral part of everyday hospital practice; but, in truth, its successes are few.
They are mostly limited to the accident and emergency department, the coronary care unit, and the intensive care unit; results from general wards are depressingly poor.
Yet there are those in the medical profession who do not have to witness this undignified event and who continue to believe that the impossible can be achieved in every case.
As a current member of a hospital crash team, I have become acutely aware of the problems arising from cardiac arrest calls.
Most medical and nursing staff are poorly trained, although performance has improved with the recent addition of resuscitation officers to most hospitals, and CPR is often instituted inappropriately.
Skinner and Vincent's handbook, which easily fits the pocket, provides information and advice about most aspects of CPR.
It is equipped with the latest UK Resuscitation Council guidelines and includes a section on paediatric arrests.
There are also helpful tips on troubleshooting.
For instance, what to do when the defibrillator requires intermittent cranking up, which is sadly often the case on wards off the beaten track.
There are important omissions, such as the 5 mg adrenaline ampoule on the crash trolley and aspirin as a main line treatment in myocardial infarction.
The text is clear and concise, although the need for extended detail on the pathogenesis of myocardial infarction is debatable.
Importantly, Skinner and Vincent have attempted to address difficult medical issues such as resistant arrhythmias and the ethical difficulty of the decision not to resuscitate.
Armed with Cardiopulmonary Resuscitation, one would have little excuse for not being able to run a calm and well-organised cardiac arrest.
I am sure that it will be a valuable addition to the shelves of any training resuscitation officer, but it may not win the battle for space in the already bulging pockets of the junior doctor's white coat.
Selected Books: Surgery
Operative Manual of Endoscopic Surgery.
— Edited by A. Cuschieri, G. Buess, J. Perissat.
Berlin: Springer-Verlag. 1992.
Pp 353.
DM296.
ISBN 3-540534865.
Review for Surgery: Scientific Principles and Practice.
— Lazar J. Greenfield, Michael W. Mulholland, Keith T. Oldham, Gerald B. Zelenock.
Philadelphia: Lippincott. 1992.
Pp 144. $29.95.
ISBN 0-397513135.
Atlas of Surgical Operations.
— 7th edn.
Zollinger & Zollinger.
New York: McGraw-Hill. 1993.
Pp 484. £105.
ISBN 0-071054170.
Surgery: Scientific Principles and Practice.
— Edited by L. J. Greenfield, M. W. Mulholland, K. T. Oldham, G. B. Zelenock.
Philadelphia: Lippincott. 1992.
Pp 1890. $92.
ISBN 0-397511213.
Operative Cancer Surgery: Volume 1 — Gastrointestinal Tract. µ — Edited by Ian Burn and James McK.
Wellwood.
London: Farrand. 1992.
Pp 227. £75.
ISBN 1-850830282.
Surgery for Skull Base Tumors. — Edited by Donlin M. Long.
Oxford: Blackwell Scientific.
1992.
Pp 271. £49.50.
ISBN 0-865420904.
NEWS
Tokyo Perspective
An apology for AIDS
Diffusion of responsibility occurs to such a degree in Japan that outright blame is not easily assigned.
This explains, in part, Japan's handling of the plight of haemophiliacs with AIDS.
The same problem in France precipitated a national scandal that destroyed careers and even contributed to the defeat of a government: senior public health officials were accused of knowingly allowing contaminated blood to be given to haemophiliacs, resulting in some 1200 infections with HIV and about 300 deaths from AIDS; some have been convicted and either jailed or given suspended sentences for fraud, criminal negligence, and failure to assist persons in danger.
As retrials begin, the scandal continues.
By comparison, a similar situation in Japan has fuelled little public outrage.
Yet haemophiliacs now comprise more than 60% of Japan's total AIDS population.
In April this year, 1685 of the 5000 haemophiliacs had tested positive for HIV and 363 had developed AIDS.
In addition to HIV infection nearly all haemophiliacs are believed to have contracted hepatitis.
Early this year the Ministry of Health and Welfare (MHW) announced that it would begin making regular payments to haemophiliacs infected with HIV.
The MHW decision followed a class action lawsuit filed by a group of 86 haemophiliacs (47 in Tokyo and 39 in Osaka) which contends that the spread of HIV among Japanese haemophiliacs was due to gross negligence on the part of MHW.
Also named in the suit are companies that marketed blood products — Baxter, Bayer, Green Cross, Kaketsuken, and Nippon Zoki.
Although no official court ruling has yet been made in this case (and under the Japanese legal system a final ruling is likely to be years in the making), MHW has decided to pay each infected haemophiliac the sum of 30,000 yen monthly (about $280) and has earmarked a total of 500 million yen in the current year's fiscal budget to meet payments.
The plaintiffs had sought about 1.3 billion yen in compensation.
In 1983, when the AIDS danger to the American blood supply was first identified, the US Food and Drug Administration acted decisively to approve heat treatment to purify blood products.
Risk to haemophiliacs was thereby sharply reduced.
At the first reports of possible contamination, Japanese haemophilia associations demanded that MHW halt imports of US blood products, which accounted for 90% of Japan's supply.
Unfortunately, the Japanese government continued to permit the import of unheated blood products.
The MHW eventually adopted purification procedures for haemophilia A patients in July, 1985, and for haemophilia B patients five months later.
Meanwhile haemophiliacs had been assured that blood products were free from HIV and therefore safe.
As a long-time Japan watcher I am continually surprised by the Japanese capacity for tolerance and forgiveness of both private and public infractions.
This tight-knit society functions much like a multi-celled organism whose individual cells must remain sensitive to their respective roles in ensuring the continuing health of the whole.
The effect of a single cell's suddenly stepping out of tandem could parallel that of HIV within the human body — an aggressively private agenda that compromises and ultimately destroys the entire host system.
Japanese and foreigners alike are fond of citing the Japanese axiom that the ‘nail that sticks out gets hammered down’.
Unwillingness to stand out has its roots in village life, where a community could be held accountable for the actions of one of its members.
Under certain conditions, conformity thus assumes an understandable significance.
Sadly, fear of non-conformity (along with typical fear of the unknown) is expressed in the stigma still attached to various genetic diseases, including haemophilia.
Then there is the Japanese interpretation of the law.
Mediation and compromise are the rule because relationships are paramount.
Views of all parties are taken into account; every angle is examined.
The aim is to restore, as far as possible, the harmony of the group.
Indeed, as several observers have noted, the avoidance of friction and preservation of peace in the community can often be more important than tackling the causes of wrong-doing.
What's more, among the ministries (such as MHW), layers of bureaucratic procedures and regulations serve to insulate government employees from outside criticism, and especially from outside punishment.
Where admonishment does occur, it is usually internal.
The MHW decision on haemophiliacs comes at a time when Japan is beginning to accept AIDS as a national health problem.
This year, MHW and the Ministry of Finance agreed on an AIDS budget that exceeds 10.1 billion yen, a five-fold increase over last year's spending.
MHW will now fund a comprehensive programme that incorporates AIDS drug research, vaccine development, global research collaboration, hospital testing, public health centre support, and nationwide AIDS awareness efforts.
In fact, a major impetus behind Japanese AIDS research is the attempt to delay the onset of full-blown AIDS in persons infected with HIV — another admission that haemophiliacs must in some way be helped.
Additionally, AIDS statistics have become increasingly worrisome.
During 1992 twice as many new carriers or AIDS patients were diagnosed as in 1991.
In April, official reports cited 554 AIDS cases, with more than 2600 persons infected with HIV.
MHW officials concede that the actual number of HIV infections may be closer to 8000, while non-government sources put the figure even higher.
Already more than 300 in Japan have died of AIDS.
By apologising and by approving some measure of financial compensation to haemophiliacs, the government is, in effect, initiating the Japanese process of forgiveness and acceptance that will restore harmony among the parties involved.
Haemophiliacs will feel that their plight has been acknowledged; the health ministry and pharmaceutical companies will admit accountability; structural integrity will be preserved.
Thereafter everyone will focus on practical aspects of dealing with an existing problem.
It may seem harsh and blatantly unfair to outsiders, but this is the way things work in Japan.
Washington Perspective
Where's health-care reform?
With the Clinton administration under the intensive care of political healer David Gergen, less and less is clear about the fate of health-care reform.
Many announced, leaked, and predicted dates for unveiling the Clinton plan came and went before the maestro of the Reagan image was improbably summoned to manage the case of Bill Clinton.
Still, as Gergen moved into the White House, nothing was on the table, although the task force created to draft the presidential plan had lived out its commission and disbanded.
The President has been widely advised to fall back from multi-front legislative conflicts in favour of one major fight at a time.
If that is the new strategy, health care must wait for settlement of the most contentious issues on the national agenda, taxes, and spending reductions.
Whether health care comes next is not known.
The President may indeed have settled on a programme of health benefits and how to finance them.
However, since nothing substantial has seeped out in this porous capital, that's doubtful.
Bits and scraps about policy and plans are often attributed to high sources, including the highest of all in health politics, Hillary Rodham Clinton.
But signs of indecision are plentiful.
For example, the mental-health establishment, behaving as though it has sniffed bad news, is publicly agitating for full-fledged inclusion in the benefits programme.
Vice-president Gore's wife, Tipper, active in mental-health affairs, is said to favour that.
The President's economists are said to be worried about the costs.
Mrs Clinton is on record as favouring mental-health services, but how bountiful a package is not clear.
Tension is reported between the two wives.
Many other issues appear to be unresolved.
Though solidly on record as a pro-choice champion, the President recently said that a decision hasn't been made about abortion services.
Mrs Clinton recently told a labour group that the President's health plan would put an end to financial ‘gouging’ by doctors.
For this she drew accusations of inciting class warfare, as well as being politically indiscreet when her husband's presidency is low on friends.
Perhaps in penance, she then told an audience at the Johns Hopkins medical school that over-specialisation is the real villain behind high medical costs.
More primary-care physicians are the remedy, said Mrs Clinton, echoing a popular theme in health-care deliberations.
Mrs Clinton also assured the Hopkins audience that the hospitals of major research institutions like theirs would be insulated against the dog-eat-dog, cost-cutting incentives inherent in managed competition, the administration's basic formula for reform.
Meanwhile, the repeated postponements and rising arguments over who gets what in a national health plan, and how it is to be paid for, have directed attention to the political calendar and the drooping vital signs of the Clinton presidency.
Auspicious moments for commencing the revolution in health care turn out to be surprisingly sparse.
Inaugural euphoria and some early successes in Congress created expectations that Clinton's domestic programme, with health care as the centrepiece, would move rapidly on Capitol Hill.
But that was before the botched appointments of senior officials, the $200 haircut episode, and the President's other poll-sapping pratfalls.
Senate Republicans have now tasted blood, and the President's own party members in Congress are expressing doubts about tying their political fates to the indecisive fledgling in the White House.
Even if Clinton sends a health-care bill to Congress soon, the 1993 legislative session is too far gone for the many lengthy hearings that must take place so that all can have their say.
Next year brings the mid-term Congressional elections.
Health reform will require additional taxes, but taxing is a serious risk factor in today's politics.
In the 100-member Senate, where a Republican minority of 43 (now 44 since the special Texas election) has stymied Clinton on tax and economic issues, the numbers do not favour the Democrats.
Of the 33 Senate seats at stake in 1994, 21 are held by Democrats.
One or two conservative Democrats often team up with the opposition.
The loss of just a handful of Democratic seats could create an effective Republican majority on specific issues, and could even put that chamber under Republican control.
The majority takes command of committee chairmanships, the bulk of staff positions, and legislative scheduling.
The next Senate could be a sinkhole for the final two years of Clinton's term.
But hopes persist.
As governor and presidential candidate, Clinton became an old hand at climbing out of the grave.
Furthermore, he was elected to serve for 48 months, of which barely 5 have elapsed.
Polls reveal the public to be far more forgiving of Clinton's debut difficulties than the press and fellow politicians, with his flops charitably attributed to unfamiliarity with the job.
Finally, wherever health-care reform may be on the stormy seas of the Clinton presidency, the public wants health-care reform.
It has all the marks of a winning issue.
Calling Dr Gergen.
Conference
Uncertain future for anti-HIV therapy
The IXth International Conference on AIDS held in Berlin last week provided a forum for the luminaries of HIV research — Luc Montagnier, Robert Gallo, and Anthony Fauci — to tantalise an audience of over 12,000 with their speculations about future treatment.
For those individuals with symptomless HIV infection, the apparent conflict between the results of the Anglo-French Concorde study and US data seems to be moving towards a resolution.
Zidovudine does consistently provide a time-limited benefit for up to 12–18 months in such patients.
The remaining disagreement is whether zidovudine monotherapy should begin early to take advantage of this short-lived benefit or whether one should wait until symptoms develop.
Arguments over toxicity and cost-benefit are finely balanced.
A consensus meeting between US and European investigators is planned for later this month, the aim being to devise more general guidelines for early intervention with zidovudine in symptom-free subjects.
One concern must be the risk of generating zidovudine-resistant HIV mutants.
For instance, a homosexual man recently became infected with a zidovudine-resistant strain of HIV-1, although he had not taken zidovudine himself.
He had most likely become infected through a partner who had been receiving zidovudine.
Early treatment might therefore risk selecting out mutant strains of HIV.
Such strains develop rapidly: in one in-vitro study (Larder BA, et al, Beckenham, UK) at time zero, 87.7% of HIV isolates were wild-type with only 6.6% mutants.
By 48 weeks in the presence of zidovudine, the proportion of mutants had increased to 53.9%.
The clinical importance of this observation is shown by viral resistance studies from ACTG 116B/117, a trial which compared zidovudine with didanosine (ddI).
Zidovudine resistance predicted subsequent treatment failure.
Data from combination therapy studies are also gradually becoming available.
Margaret Fischl (Miami, USA) presented results from ACTG 155 which showed that among symptomatic patients (CD4<300/L) who had taken zidovudine for a median of 18–19 months, the combination of zidovudine and zalcitabine (ddC) was no better than either zidovudine or zalcitabine alone (primary end points, first AIDS-defining event or death; mean follow-up, 17 months).
Convergent combination chemotherapy refers to the use of several antiretroviral drugs — nucleoside and non-nucleoside reverse transcriptase (RT) inhibitors — to damage the virus to such a degree that its replication is permanently impaired.
The efficacy of this approach with zidovudine, didanosine, and nevirapine has been reported in vitro and was further discussed in Berlin.
Over 400 patients have already been recruited into a trial to test this regimen.
Intriguing in-vitro studies also suggest that introduction of resistance mutations to one antiretroviral compound (didanosine) can diminish previously documented resistance to another agent (zidovudine).
However, Robert Gallo voiced concern about multidrug cocktails.
The more drugs that one gives, the more likely it is that damage to host DNA polymerase will take place.
The potential toxicity of these agents should not be underestimated.
Much emphasis has been placed on the theoretical value of tat and protease inhibitors.
The tat (transactivator) gene of HIV controls the rate of viral DNA replication.
In vitro, Ro-24-7429, a benzodiazepine derivative, inhibits RNA replication and viral protein production.
In a randomised blinded phase I/II trial comparing Ro-24-7429 with nucleoside RT inhibitors in 96 patients (CD4 50–500/L), Haubrich et al(California, USA) found that although the drug was well tolerated, it was ineffective in producing either a rise in CD4 count or a fall in p24 antigen activity.
Inhibition of HIV protease should slow maturation of virion core proteins and retroviral enzymes.
Nanomolar concentrations of protease inhibitors show highly specific antiretroviral activity.
An Abbot protease inhibitor (A77003) given intravenously produced no beneficial changes in CD4 count or p24 antigen in 22 HIV-positive patients (mean CD4 300/L).
In a few cases, there was a decline in quantitative HIV culture from plasma (but not from peripheral blood mononuclear cells).
Roche's oral protease inhibitor, Ro-31-8959, was tested in three phase I/II protocols: advanced disease with (France) and without (Italy) previous zidovudine treatment, and symptom-free or minimally symptomatic disease (UK).
Although well tolerated, the effects on CD4 count, p24 antigen, and HIV titres from mononuclear cells and plasma were weak and variable.
In the Italian study, the effects of Ro-31-8959 were comparable with zidovudine.
Clinical trials of antisense oligonucleotides that bind to and inhibit the HIV gag gene sequence will begin later this year with GEM 91 (genetic expression modulation) from Hybridon.
For the future, the application of dominant negative viral mutants (which, when mixed with wild-type virus, render it non-infectious), capsid-targeted virus inactivation (a similar process leading to viral degradation rather than inactivation), and virus-specific inhibition (for instance, by targeting the rev response element that is essential for normal rev protein function in virus assembly) was discussed by Max Essex (Harvard AIDS Institute, USA) and F. Wong-Staal (California, USA).
Where are the glimmers of hope?
Despite Anthony Fauci's assertion that ‘the virus is the major factor in HIV disease,’ it was clear that a shift in emphasis is taking place from direct antiretroviral strategies to immune-directed therapies.
HIV infection induces a state of immune activation leading to loss of follicular dendritic cells and normal lymph node architecture.
Interference with this process by targeting the activated T cell, apoptosis, or cytokine secretion might stabilise the immune system and control the course of infection.
Effective cell-mediated immunity is central to limiting viral damage.
Gallo noted that virus replicated faster in an activated cell where deoxyribonucleotides are in plentiful supply.
Hydroxyurea inhibits the rate-limiting enzyme supplying deoxyribonucleotides — ribonucleotide reductase — and substantially diminishes HIV replication.
Gallo also quoted some of the latest work of Daniel Zagury's team which has discovered a pentapeptide motif in gp120 which is also present in CD4 and which interferes with antigen activation possibly leading to a state of cell anergy and immunodeficiency.
The case for autoimmune damage in HIV infection was supported by Wilson et al(London, UK) who have found a 13 aminoacid sequence in gp120 that mimics human MHC molecules.
If autoimmune processes are important, T cell vaccination might be a suitable treatment strategy.
Jonas Salk's team in California, USA, reported the first results of phase I/II clinical trials with a gp 120-depleted treatment vaccine.
Both humoral and cell-mediated immune responses were generated and HIV copy number fell during the vaccination period compared with controls.
The promise of immunotherapeutic strategies to combat HIV infection was one strong and clear message to emerge from Berlin.
The next world AIDS conference will take place in Yokohama, Japan, in 1994 and subsequent meetings will be held biennially.
Medicine and the Law
Triazolam licensing in UK
Acting on the advice of the Committee on Safety of Medicines (CSM), the UK Department of Health suspended the product licences for triazolam (Halcion) on Sept 30, 1991.
On June 12, 1993, despite a hard-fought battle by the manufacturers, Upjohn Ltd — who have now exhausted all the statutory avenues of appeal provided by the Medicines Act 1968 — the Licensing Authority announced that the revocation of licences would be permanent.
Upjohn appealed to the CSM but its application was unsuccessful.
In May, 1992, the company took its case to the Medicines Commission, which advised revocation of licences for 0.25 mg products but thought that problems with 0.125 mg tablets could be dealt with otherwise.
The Licensing Authority, however, proposed on July 17, 1992, that licences for both dosages be revoked.
Since that determination went against advice from the Medicines Commission, Upjohn invoked the right, rarely exercised, to present its case to a panel appointed by the Licensing Authority under the 1968 Act.
The four-day hearing was in private but the panel's 37-page report is in the public domain.
Its findings were sympathetic to Upjohn's case — ie, ‘the data are generally reassuring but rare idiosyncratic episodes of violence in association with triazolam, as with other [benzodiazepines], cannot be ruled out’.
Notwithstanding the best efforts of the distinguished panel, the findings were arrived at after a one-sided presentation of evidence and argument.
Furthermore, much of the evidence cannot be independently assessed.
For instance, the finding on equivalent doses (which is at odds with the views of the CSM) is handled by reference to oral evidence, to published work, and to a transcript.
No attempt is made to summarise or to quote extensively or to append the reference documents.
As the panel notes, the statutory procedure makes no provision for a balanced argument; only Upjohn was able to present its case and its witnesses and evidence, though the Licensing Authority provided material and the panel ‘followed up leads from this material and from that presented by Upjohn’.
The panel notes that ‘it would be simpler and much more satisfactory if the procedure provided for the ‘contrary case’ to be presented''.
The panel is clearly dissatisified with its remit ‘only to hear the licence holder’ and, after an extensive examination of the evidence, to report to, but not be entitled to make recommendations to, the Licensing Authority.
The panel concludes that overall the benefits outweighed the risks of the drug at doses of 0.25 and 0.125 mg when provided with appropriate data sheets.
One ‘vital area’ was that of ‘equivalent dose’.
The panel notes difficulty in making exact comparisons but found the weight of evidence was ‘in favour of a dose of 0.25 mg triazolam being roughly equivalent to a dose of 30 mg flurazepam or 20 mg temazepam (UK formulation).
We accept this represents a major change from earlier views that the equivalent doses were triazolam 0.5 mg to 30 mg flurazepam or 20 mg temazepam’.
The Licensing Authority is unconvinced by this finding which is, in effect, that the triazolam doses in comparative trials were not equivalent to those of flurazepam.
Protocol 321 (in prison volunteers) was of ‘marginal value’ in assessing the safety of the drug in 1993, the panel felt, and Upjohn's explanation that the incomplete information on adverse effects was due to a transcribing error, is ‘not implausible, although…even in 1978, a properly organised company should have operated checks which eliminated such errors’.
One difficulty those advising the Licensing Authority had to contend with was the ‘relative lack of data about the efficacy and effects of triazolam at the licensed doses of 0.125 mg and 0.25 mg’ and at the beginning of the hearing that problem troubled the panel also.
‘…we find it unfortunate that Upjohn failed to respond at all readily to requests made by the Licensing Authority as early as 1990, that they should carry out a new post-marketing surveillance study’.
However, the panel was impressed (more so than the Minister) with a UK post-marketing surveillance study.
The panel felt that the starting dose for triazolam in the young and middle-aged ought to be 0.125 mg, increasing to 0.250 mg ‘only if necessary’.
The same logic suggests that the elderly should be started on 0.0625 mg, but no data are yet available (a US study is expected to be available this year).
Shortly before the publication of this report saw the successful appeal against the Legal Aid Board's refusal to allow potential Halcion claims to be investigated where applications for legal aid were made after the deadline of Oct 20, 1991.
This date was imposed by Mr Justice Kennedy, limiting the right of entry by legally aided litigants into the first group action in the multiplaintiff benzodiazepine litigation.
Some 580 claimants, many of whom came forward after the critical BBC Panorama television programme on triazolam in October, 1991, can expect to receive limited legal aid to allow for investigation of their claims.
The audit of Halcion claims along with some 3000 Ativan (lorazepam) claims already served, is due to be completed shortly.
Further judicial directions in the proceedings of the litigation are expected in July.
Upjohn has at all times contended that Halcion is safe and effective and that any errors in the presentation of its material to the Licensing Authority were unintended, and it is suing the BBC for libel.
The company is also suing Prof Ian Oswald, a psychiatrist expert in the field of sleep and hypnotics, and he is counterclaiming for libel.
The cases are due to be heard together before a judge in London without a jury, in January.
Noticeboard
1992–97 AIDS projections for England and Wales
Revised calculations from a working group chaired by Prof N. E. Day suggest that there will be between 1945 and 3215 new AIDS cases in 1997.
Projected annual incidences for 1997 in each of the main exposure categories are: homosexual males, 1350; injecting drug users, 165; and heterosexuals, 770.
Day and colleagues estimated that there were about 23,400 HIV-infected individuals in England and Wales by the end of 1991.
Although a plateau will develop in the overall incidence of AIDS, there are strikingly different predictions for each exposure category.
For homesexual males, the annual incidence of AIDS is expected to plateau in 1994 in line with a peak annual incidence of HIV infection in 1983–84.
Among injecting drug users, the peak in HIV incidence took place in 1985 and so the number of AIDS cases is still climbing.
For heterosexuals up to 1991, the peak of HIV infection had yet to be reached and so one can expect a continued rise in the annual incidence of AIDS cases for some time to come.
The pool of those with HIV infection acquired heterosexually via a partner who had also been infected heterosexually is slowly increasing.
The UK Department of Health claims that its AIDS prevention policy is responsible both for the reduction in projected new cases among injecting drug users and the greater certainty about heterosexual exposure.
According to Baroness Cumberlege, ‘the UK now has one of the lowest estimated HIV prevalence rates in western Europe’.
Day attributes the changes to more detailed and reliable data collection, together with better statistical techniques.
The report adopts a cautious note, reflecting concern about a possible recent increase in HIV transmission among homosexual males and the difficulty of guessing sexual mixing patterns between subsets of the population.
The latest figures replace 1990 estimates that have proved to be reasonably accurate except for an overestimate of the number of heterosexual exposures (355 vs 255) and an underestimate of cases among homosexual males (910 vs 1165).
Gases and glues
Data on deaths in the UK from volatile substances abuse (VSA) have been recorded by a team from the Department of Public Health Sciences, St George's Hospital Medical School, London, since 1971, and these data have been collected in a ‘stable and systematic’ manner since 1983.
The latest report from St George's, which covers the period up to the end of 1991, shows that there were 122 deaths from VSA in 1991 compared with 151 in 1990 and 113 in 1989.
Since 1983, there has been an average annual increase in VSA deaths of 5.4% per annum.
Young people aged 14–18 years are the age group most likely to die from VSA, accounting for 61.5% of all VSA deaths between 1971 and 1991, and the figures for 1991 show no significant change in this pattern.
87% of deaths from VSA in 1991 were in males, and again this percentage did not differ from the overall figure since 1971.
Between 1971 and 1991, the most popular substances for abuse were gas fuels (primarily cigarette-lighter refills and bottled domestic gases; 35.1% of deaths), aerosols (primarily deodorants/antiperspirants and pain-relief sprays; 20.9%), glues (mostly contact adhesives; 19.2%), and ‘other’(typewriter correction fluid and anaesthetic gases figure prominently in this group, which also included petrol, plastic remover, and dry-cleaning fluid; 20.7%).
With the exception of glue (13.9%), the figures for substances abused in 1991 differ little from the overall figures.
In 1991, VSA was most likely to occur in a public place (42%) or at home (39%).
Unsurprisingly, most deaths from VSA were in hospital or on the way to hospital (47%), and causes of death were recorded as direct toxic effects (49.2%), inhalation of vomit (14.8%), trauma — eg, hanging or drowning (11.5%), suffocation inside a plastic bag (7.4%), and ‘other/not known’(17.2%).
From February, 1992, the UK Department of Health sponsored the broadcast of television commercials designed to alert parents to the dangers of VSA.
The next report on VSA deaths should reveal what effect, if any, this campaign has had.
Financing health service
Until recently, most national governments muddled along with health-care systems derived from a mixture of charitable support, private practice, and voluntary or compulsory insurance.
Health care was seldom a major political issue, and inequalities in its provision were considered as inevitable as inequalities in wealth.
WHO turns its attention to the matter in its latest Technical Report, which notes the experience of representative nations from Algeria to Zimbabwe.
Just how diverse they are is shown by the proportion of health care provided privately.
Among the richer nations, in the USA 58% of health care is private, and in the UK, 13%.
In developing nations, the extremes are 80% in Uganda and 12% in Lesotho.
Within such diversity there is no common standard of achievable health care.
WHO suggests there should be, and, if not a common way of paying for it, at least an agreed means of approaching the difficulties involved.
In the UK, the tide is turning from all-out provision of health care by central government to a hybrid where private initiatives are encouraged to compete within state funding.
A more radical change may be on the way: a report by the UK Institute of Health Services Management concludes that ‘General taxation must be reconsidered as the major funding of the National Health Service if it continues to fail to deliver adequate levels of resources’.
Coincidentally, the UK Government's Department of Health launches a ‘Help us to help you’ campaign, wherein sick people are advised how not to waste the limited funds available for their treatment (‘…remember to cancel appointments if they cannot be kept…’) and reminded by the Secretary of State for Health that ‘Our primary care system is also the source of considerable envy abroad’, as I was reminded again during a recent trip to Russia.
Russia's envy is understandable when, under communist government, the state's doctors were so slightly rewarded as to make illegal private practice inevitable.
Whether Russia's change to a market economy will lead to the concept of free health care being abandoned, remains to be seen.
In the UK, this ideal has been eroding for some time and seems unlikely to survive the escalating costs of current treatments.
WHO and Yugoslavia
Dr Nils Rosdahl, former director of public health in Denmark, has taken up a WHO assignment as adviser to the UN High Commissioner for Refugees in former Yugoslavia.
Based in Zagreb, he succeeds Sir Donald Acheson, who went there in July last year on what was expected to be a two or three month assignment.
WHO now has some fifty staff in the area, half of whom are locally engaged.
On return from a ten-day visit to the five republics of former Yugoslavia, Dr Hiroshi Nakajima, WHO Director-General, was visibly appalled by what he had seen in Bosnia-Herzegovina particularly the destruction of some 260 hospitals and the gravity of the wounds caused by the high-velocity bullets.
Some children among Sarajevo's 330,000 inhabitants were showing signs of malnutrition.
Diarrhoea and dysentery outbreaks were increasing and tuberculosis and hepatitis were major threats.
State dispensaries no longer had even the most common antibiotics.
Dr Nakajima, accompanied on his tour by Dr Jo Asvall, head of WHO's European regional office in Copenhagen, said that he had been asked several times if Yugoslav funds frozen in the US and other countries could not be used for purchase of medical supplies.
He promised to pass this idea on.
Harmonising therapeutics teaching in Europe
The European Network of Therapeutics Teachers (ENOTT) has been formed to encourage the optimisation and harmonisation of undergraduate and postgraduate therapeutics teaching in Europe.
Some of their aims are that learning methods should be non-directive, interactive, and based on clinical cases and that teaching should foster the ability to evaluate critically published data on different treatments and new drugs.
They also believe that therapeutics should be examined as a separate subject in the final year of undergraduate study and that emphasis should be given to therapeutics throughout the whole medical curriculum, with a minimum of 100 hours being dedicated specifically to the subject.
Further information can be had from Prof A. Benetos, Hospital Broussais, Service de Medicine 1, 96 rue Didot, 75674 Paris Cedex 14, France.
Life expectancy and handedness
R. Peel, left arm slow with 102 test wickets against Australia, died aged 84 after an irregular, even accident-prone, life.
Not a classic illustration of the thesis of Aggleton and colleagues, who have been looking at the lifespans of cricketers and report a two-year gap in favour of right-handers, much of it ascribable to fewer deaths in accidents or in war.
Yorkshire cricket has been graced with world-class slow bowlers.
Rhodes, like Peel, lived to a great age; others did not; and one was killed in action.
But this study, from psychologists in Durham, is demography not anecdote.
Their source is Who's Who of Cricketers (1984) in which Aggleton et al found 3165 dead ones whose handedness could be determined.
This game is one where left-handedness has some advantage and considerable nuisance value so it is not surprising that as many as 18.5% of these first-class performers were left-handed.
The focus of the paper is the 147 cricketers known to have died from unnatural causes (excluding suicide).
28.6% were left-handed and this figure was much the same for those killed in action and for those killed in other ways.
With earlier hints from baseball these data point to an ergonomic cause — ie, machinery and systems designated by the right-handed for right-handers — rather than more elaborate explanations, such as the immunological.
NHS staff and the media
The National Health Service has issued its guidelines to staff on their right to speak out on concerns about health care (NHS executive letter EL[93]51).
Unauthorised disclosure to the media is no longer described as a ‘potentially serious breach of contract’(see Lancet Oct 24, 1992, p 1031, and Nov 21, p 1277 for draft guidance), but the new advice says that if such disclosure is judged to be unjustified it could result in disciplinary action.
Pharmacovigilance in Europe
A European Pharmacovigilance Research Group (EPRG) has been set up; it was established with support from the European Commission under the Biomed Programme to develop and validate methods of pharmacovigilance that are applicable to the divergent patterns of health care delivery across the European Community and that are compatible with national legal, cultural, and ethical customs.
As part of its work the EPRG is anxious to construct, and then make widely available to interested parties, an inventory of all case-control studies that are in progress across Europe and that are designed to investigate possible associations between particular conditions and prior exposure to medicinal products.
Investigators conducting such studies are invited to contact the EPRG Project Co-ordinator (Prof M. D. Rawlins, Wolfson Unit of Clinical Pharmacology, Department of Pharmacological Sciences, University of Newcastle, Newcastle upon Tyne NE2 4HH, UK) briefly outlining the study (in not more than 2 to 3 sentences), its start date, its planned date of termination, and the projected numbers of cases and controls to be recruited.
Update on SIDS
Although the number of cases of sudden infant death syndrome (SIDS) in the UK in 1992 was half that in 1991 (456 vs 912, respectively), SIDS remains the commonest cause of death in babies older than 1 week.
Last week the UK Department of Health confirmed its earlier recommendations that the risk of cot death is reduced if infants are laid to sleep on their backs, if they are not exposed to cigarette smoke before or after birth, and if they are not overwrapped or overheated, especially when feverish or unwell.
Preliminary results presented at a meeting organised by the Foundation for the Study of Infant Deaths may provide some leads toward the mechanisms involved in SIDS.
Dr Peter Fleming, Bristol Maternity Hospital, UK, reported that changes in the variability of respiration in response to alterations in room temperature have been found in some normal infants.
In addition, preliminary results of a postoperative study reported by Dr Stewart Petersen, University of Leicester, UK, and Dr Mike Wailoo, Leicester Royal Infirmary, suggest that changes in an infant's body temperature may precede (by up to 1 week) the development of other symptoms of infection that are recognised by parents.
These studies may provide some insight into the roles of overwrapping, overheating, and infections in SIDS.
However, other research suggests that a disturbance of maturation of the brain may be involved.
Pathological dendrite formation in neurons of the substantia nigra has been found in brains of infants who died of SIDS and also in infants with intrauterine growth retardation.
According to Professor Dick van Velzen, Royal Liverpool Children's Hospital, UK, these changes may be important in the aetiology of SIDS because they occur in an area involved in the control of breathing, heart rate, and temperature.
LETTERS TO THE EDITOR
Is PCR a viable option for population screening?
SIR,— Theoretically, the polymerase chain reaction (PCR) is the most sensitive test for the detection of infectious pathogens, and, in practice, its sensitivity exceeds that of animal transmission studies, which may represent the lower limit of clinical relevance.
For many infectious agents, hepatitis C virus (HCV) in particular, the specificity and extreme sensitivity of PCR makes it superior to standard serological tests in the determination of carrier state, and false-negative serological results have led to the suggestion that PCR should replace antibody testing for screening.
However, PCR is expensive and time consuming, and is generally regarded as unsuitable for the mass screening of clinical samples.
By arranging the samples to be tested in a two-dimensional array, either physically or conceptually, pooling the samples in both columns and rows (figure), and doing PCR on the pooled samples and those individual samples found to be positive by ‘grid referencing’, it is possible to increase the efficiency of screening.
Assuming one positive, and both negative reagent and sample controls, are run concurrently, and grid-reference-positive samples are not retested, the screening load of grid-referenced PCR (the number of samples to be assayed) increases at 2n+3, where n is the dimension of the array, while the screening rate (the number of samples about which information is obtained) increases at n (figure, a).
Thus, to screen a 20x20 array (400 samples) by grid-referenced PCR, a minimum of 43 individual PCR reactions would need to be run.
Assuming a single positive in the samples tested, the screening multiple (the ratio between the screening rate and the screening load) also increases exponentially (at a rate n/2n+3) and in a 2020 array is 400/43 or 9.3, meaning information is obtained on 9.3 samples for every one tested.
However, the wider application of this approach is limited by the introduction of ‘test false positives’ created by the grid-reference system (figure, b).
The number of false positives (FP) generated by grid referencing and requiring subsequent PCR is a function of the number of true positives (P), and of the distribution of those positives, but will maximally increase according to the equation FP=PP, meaning that the overall screening load increases at a rate 2n+3+ (PP), and the screening multiple by n/2n+3+ (PP).
The efficiency of grid-referenced PCR, reflected in the screening multiple, therefore declines with increasing prevalence of positives within the samples.
Nevertheless, at a positive prevalence of 1% in a 2020 array, a screening multiple of >6 can be achieved: useful information is obtained on 6 samples for every one assayed.
Sensitivity, a major concern when screening in a clinical setting, would decline as an inverse function of n, at rate 1/n (and is a function of Vap/Vp, where Vap is the volume of the pooled sample assayed and Vp the pooled volume), assuming DNA/RNA is extracted from the same volume of pooled serum as standard PCR assays.
Although the loss of sensitivity inherent in grid-referenced PCR needs to be addressed, it is possible that it will not be a major issue; even patients with chronic hepatitis C, who frequently have low levels of viraemia, generally have circulating levels of virus orders of magnitude greater than the lower limits of detection by PCR, and levels of hepatitis B virus DNA in chronic carriers are orders of magnitude greater still.
This methodology might be applicable to large-scale seroepidemiological studies of pathogens found at low prevalence within populations.
Immunoglobulin preparations, acutegraft-versus-host disease, and infectionafter marrow transplant
SIR,— Dr Blasczyk and colleagues (March 27, p 789) describe the variable content of soluble CD4, CD8, and HLA class I and II molecules in commercial preparations of intravenous immunoglobulin (IVIG).
Although the benefit of IVIG in terms of reduced infection and acute graft-versus-host disease (GVHD) has been shown among recipients of allogeneic marrow transplants, the mechanism of action, and the optimal dose and preparation, remain undefined.
At Georgetown University Hospital between 1987 and 1992, 41 patients undergoing HLA-genotypically-identical sibling donor transplant received high dose IVIG for infection and GVHD prophylaxis, and survived at least 30 days after the transplant.
Twenty-two patients received Gamimune N (Cutter/Miles) at a dose of 200–500 mg/kg per week beginning on day 8 until day +28, then every other week until day +100.
Subsequently, the toxicities and pharmacokinetics of continuous infusion (CI) IVIG of Gammagard (Baxter/Hyland) in 19 patients at a dose of 1000 mg/kg on day 8, then 500 mg/kg per week until day +100 were measured.
GVHD and antimicrobial prophylaxis were the same for all patients.
Although a lower incidence of bacterial and fungal infections was observed among the CI Gammagard-treated group, there was also an increased incidence of GVHD grade II or more (table).
Since the dose (and serum concentrations achieved) were higher with CI Gammagard, the decreased rate of infection might be explained by higher specific antibody concentrations.
The increase in acute GVHD in this group, however, was unexpected.
Although these data must be interpreted with caution given the non-randomised nature of the studies, and the heterogeneous group of diseases among the two patient groups, it is intriguing to speculate that non-specific immunosuppression might occur as a result of the increased concentration of soluble HLA molecules in Gamimune N relative to Gammagard.
As Blasczyk et al suggest, such molecules might interfere with the association between HLA molecules expressed on antigen-presenting cells and corresponding HLA-restricted T cells, leading to less severe GVHD.
Alternatively, the increased concentration of soluble CD4 and CD8 in Gammagard relative to Gamimune N might stimulate cells with the appropriate receptors, leading to more severe GVHD.
Future trials comparing the most commonly used IVIG preparations in marrow transplantation should measure not only differences in infection but in the incidence of GVHD, to further elucidate the mechanism of immune changes due to IVIG.
Determination of genetic sex by PCRamplification of Y-chromosome-specificsequences
SIR,— Gender verification of Olympic athletes has been a matter for discussion since the first analyses were done in Mexico in 1968.
Recently, the International Olympic Committee (IOC) decided to use a new technique, the polymerase chain reaction (PCR), to detect Y-chromosome-specific sequences (Y-PCR).
Since this decision has caused controversy, we thought it important to disclose the details and results of the analysis used at the Barcelona Olympic Games.
Several Y-specific genes have been used for sex typing.
Evidence from laboratory animals supports the idea that the testis determining factor is the product of the SRY gene.
For this reason, SRY and another Y-specific gene, DYZ, were chosen as targets.
As a first step, the presence of DYZ1 was determined with lysates of buccal smears from female competitors and two specific primers.
Details of the protocol will be published elsewhere.
As an internal control, co-amplification of a sequence specific for mitochondrial DNA was done.
In our previous studies, amplification of DYZ-specific sequences was detected in all 89 male samples and in none of 1629 female samples.
However, when samples from 2406 female competitors in Barcelona were analysed, 11 (0.46%) had a positive result.
In a second screening, SRY and DYZ1 were amplified from purified DNA from these 11 samples.
Amplification of the DYZ1-specific sequence was detected in 11 samples, as expected; however, only 5 contained SRY.
The IOC Medical Commission criteria state that a sample is positive only if both SRY and DYZ1 are found.
Only these 5 athletes were invited to attend for a medical examinaton.
Details have not been disclosed but morphological abnormalities were observed in 4, the other athlete refused (Dr Eduardo Hay, IOC Medical Commission, personal communication).
The findings are strictly confidential and we do not know if these athletes were allowed to compete.
In some cases, such as‘male pseudohermaphroditism’, genetic sex does not coincide with phenotypic or physiological sex.
The IOC Medical Commission was aware of this, and, for this reason, the presence of a Y chromosome was not a reason for exclusion; a complete physical examination was done before any decision about eligibility.
The advantages of Y-PCR over the test used in the past have been described.
We believe that Y-PCR testing, followed by a physical examination of positive cases, is a valid method of verifying the genetic sex of large numbers of people.
Several experts have proposed that the physical examination be carried out without previous screening.
We think, however, that most athletes would find a gynaecological examination purely for gender verification a humiliating experience.
Whatever method is chosen in the future, extreme care will have to be taken to preserve confidentiality before and after the final decision.
Non-toxigenic Corynebacterium diphtheriaevar gravis in England
SIR,— Major Sloss and Col Faithfull-Davies' report (April 17, p 1021) of non-toxigenic Corynebacterium diphtheriae var gravis among military personnel re-emphasises the importance of screening for C diphtheriae.
However, we point out that the Diphtheria Reference Laboratory is not part of the Communicable Disease Surveillance Centre, Colindale, as they suggest.
The correct designation is the Diphtheria (WHO) Reference Unit, Respiratory and Systemic Infection Laboratory, Central Public Health Laboratory.
The number of isolations of non-toxigenic C diphtheriae var gravis referred to the unit from laboratories that continue to screen all throat swabs for C diphtheriae has increased fivefold from 1990 to 1992.
Since January, 1992, 54% (76/140) of referrals have belonged to this biotype.
These isolates have mostly been referred from children and young adults (aged 5–35 years) with pharyngitis.
A recent report by Wilson and colleagues drew attention to the isolation of non-toxigenic C diphtheriae var gravis from patients attending a genitourinary medicine (GUM) clinic at a London hospital.
However, most isolates referred to the reference unit have no apparent association with GUM clinics.
A striking increase in this biotype has also been recorded elsewhere, including 7 cases of endocarditis in Australia.
The diphtheria laboratory has only examined one isolate of non-toxigenic C diphtheriae var gravis isolated from blood cultures of a patient in England.
This seems to be the first manifestation of invasive disease referred to the reference unit by a laboratory in the UK.
Molecular epidemiological studies have shown that geographical heterogeneity exists between isolates and that numerous clones (defined by restriction fragment-length polymorphism analysis of rRNA genes) seem to be circulating.
Toxin characterisation studies have demonstrated the absence of the structural gene for diphtheria toxin and also in-vitro and in-vivo expression of the protein.
There is no information on the carriage rates for this organism but the apparent increase is of concern, as is its potential to cause invasive disease in some patients.
Disappearance of antibodies to HPV 16 E7after treatment for cervical cancer
SIR,— Dr Jha and colleagues (May 1, p 1116) report antibodies against human papillomavirus (HPV) 16 E7 peptide (originally reported by Mueller et al) among women who had been diagnosed with cervical carcinoma on average 4 years before being contacted for donation of a serum sample.
The use of cases with a history of disease, rather than incident cases, is fraught with systematic biases.
The most obvious difficulty is whether antibody titres change after diagnosis and treatment, either as a result of curative treatment with removal of the antigenic stimulus, or of progression of disease and concomitant immunosuppression.
To examine these issues, Lehtinen and co-workers obtained serum samples from 28 patients with incident cervical cancer who were admitted to Helsinki University Hospital, Finland.
Serum samples were obtained both before treatment and at regular intervals during follow-up of up to 15 months.
These patients and their serological reactions to various HPV and herpes simplex virus type 2 (HSV-2) antigens have been reported in detail.
I analysed serum samples from these patients for antibodies against the HPV 16 E7 peptide used by Jha et al.
With the aim of achieving comparability of methods, the E7 peptide as well as positive and negative reference sera were obtained from Dr Mueller.
5/28 patients were strongly positive (enzyme-linked immunosorbent assay [ELISA]titre >1/600) for IgG to the E7 peptide.
In 3/5 patients, E7 antibody titres declined during follow-up, and in the other 2 patients the antibodies were undetectable after 1 year (figure).
The effect of disappearing seropositivity with time elapsed since diagnosis and treatment probably resulted in an underestimation of relative risks in Jha's study.
In addition, Jha and colleagues probably selectively excluded patients with progressive disease, since a high proportion of such patients will be dead within 4 years after diagnosis.
The selective exclusion of patients with progressive cancer introduces an additional systematic error, although it cannot a priori be said whether this would tend to underestimate or exaggerate relative risks.
Further, cervical cancer treatment will typically include radiotherapy, which will result in release of autoantigens and generation of autoantibodies that can cause background difficulties in various serological assays.
Thus, if a large proportion of cases, but no controls, have received radiotherapy, there will also be a systematic bias that will tend to exaggerate relative risks.
It is questionable whether the study of patients with a history of having been diagnosed with cervical cancer can form the basis for conclusions about the seroepidemiology of this disease.
It is unfortunate that Jha et al do not point out the disadvantages of not using incident cases, nor try to estimate the magnitude of the systematic errors thus introduced.
Cyclic guanosine monophosphate in heartfailure
SIR,— Dr Lerman and colleagues' report (May 1, p 1105) on circulating N-terminal atrial natriuretic peptide as a marker for symptomless left-ventricular dysfunction, and your accompanying commentary by Professor Barnett on the diagnosis of symptomless left-ventricular dysfunction, prompt us to draw attention to another promising analyte for diagnosing and monitoring heart failure — cyclic guanosine monophosphate (cGMP).
Once released from the heart, natriuretic peptides exert potent direct vasodilator and natriuretic actions by virtue of their ability to increase their intracellular second messenger cGMP.
Since cGMP is extruded from cells after interaction with atrial natriuretic peptide (ANP) or brain natriuretic peptide (BNP), it is a possible marker for natriuretic peptides.
In patients with heart failure, close correlations have been described between cGMP and ANP plasma concentrations.
Both ANP and cGMP correlate closely with the severity of congestive heart failure.
In addition, exogenously administered ANP raised plasma cGMP concentration in accordance with its physiological effects.
Because in plasma cGMP has a longer half-life (15 min) than ANP (1–2 min), cGMP can be regarded as a more sensitive marker for ANP release than ANP itself in many situations.
Routine laboratory use of ANP and other natriuretic peptides in heart failure hints at difficulties with storage, because these peptides are highly susceptible to degradation under most storage conditions.
By contrast, even at room temperature cGMP is stable for at least five days in plasma samples containing edetic acid.
Our results indicate that ethanol extraction of plasma samples before measurement of cGMP is not necessary (correlation of values with and without previous ethanol extraction: r=0.91, p=0.0001, n=52).
We could demonstrate that cGMP yields high sensitivity (94.2%) and specificity (93.7%) for cardiac diseases in the absence of severe renal functional impairment.
Symptom-limited ergometric exercise testing increases the diagnostic sensitivity of cGMP for symptomless heart dysfunction (see figure).
Before exercise, cGMP concentrations of New York Heart Association (NYHA) class I patients were only slightly raised and did not differ significantly from concentrations of healthy controls or patients with non-cardiac diseases.
By contrast, 30 min after finishing ergometric exercise testing, cGMP in NYHA I patients was significantly higher than in controls.
The high diagnostic efficiency of cGMP, its stability, and the possibility of direct cGMP determination in plasma samples lead us to conclude that in patients with heart failure measurement of cGMP is an alternative to measurement of natriuretic peptides for routine clinical laboratory medicine.
Misleading herbal Ayurvedic brand name
SIR,— Herbal medicines are being used by many patients in India and Europe.
Drugs from the ancient system of medicine Ayurveda are popular in India, and are believed to be safe because of their long history of use and their natural origin.
However, clinically important interactions between Ayurvedic medicines and modern medicines have been recorded.
I report another difficulty that can arise from a belief in safety of Ayurvedic medicines and misleading brand names.
Pain relieving medicines such as aspirin or ibuprofen are used for headache, pain, and arthritis.
Prolonged use of these medicines is known to harm the gastric mucosa.
If used by a person with pre-existing ulcer, the damage can be serious and cause severe bleeding.
Doctors warn patients and take necessary precautions.
I have noticed a preparation, Vendana Nigraha Rasa, which is recommended for pain relief by several lay people.
These words are Sanskrit (ancient Indian language), and vendana means pain, nigraha determination, and Rasa juice.
The Sanskrit words give a false sense of the product being Ayurvedic and hence herbal and safe.
Actually, its main ingredients are aspirin (530 mg) and paracetamol (100 mg).
Like other Ayurvedic preparations the drug is effective, which is not surprising since it contains almost double the quantity of aspirin used in normal aspirin tablets.
I am aware of one educated patient, with known duodenal ulcer, who had started taking this medicine for headache, believing it to be Ayurvedic and despite the facts that it is labelled as containing 530 mg aspirin and 100 mg paracetamol and that he had been instructed by his doctor to refrain from all pain-relieving medications like aspirin.
In a survey of pharmacies, I have not come across any other such misbranded medications.
Doctors, patients, and law-enforcing authorities should take a note of this warning and patients should ascertain that the Ayurvedic medicines they are taking do not contain anything other than Ayurvedic medicine.
Lack of association between myelin basicprotein gene microsatellite and multiplesclerosis
SIR,— Dr Rose and colleagues (May 8, p 1179) report that a tetranucleotide repeat polymorphism, 5 to the myelin basic protein (MBP) gene, is not genetically linked with familial multiple sclerosis (MS).
This finding is in contrast to previous population-based studies by Boylan et al and Tienari et al and their co-workers, in which this locus was shown to be associated with MS.
The Finnish study also demonstrated linkage in families said to have clinical MS.
However, both these studies used a low resolution technique to type a complex tetranucleotide repeat polymorphism, and thus alleles assigned the same fragment length may be genetically heterogeneous.
This difficulty was recognised by Boylan et al.
A much improved method for typing this polymorphism has since been reported.
This procedure enables accurate discrimination of allele sizes for a pair of tetranucleotide repeats at the 5 end of this complex repeat, and is the method used by Rose and colleagues.
We have also used this refined method to type alleles of the A and B loci of this MBP marker in 122 unrelated patients with clinically definite MS from neurology clinics in Belfast and Coleraine and 106 blood donor controls.
The table shows the allele frequencies for both the A and B loci in the healthy and MS populations.
The frequencies for all the alleles of both markers are very similar in the two populations and there is no allelic association with MS.
Analysis of genotype frequencies in both groups also failed to reveal any genotypic association with MS.
Thus this study, in the Northern Ireland MS population, confirms the work of Rose and colleagues, and infers that the myelin basic protein gene is not a candidate for conferring susceptibility to MS.
SIR,— In not accepting our explanation (May 15, p 1286) for the difference between the November, 1988, and January, 1990, forecasts for AIDS incidence in England and Wales — namely, a slowing down in the rate of transmission of HIV among homosexual men — Professor Stewart (p 1287) states that we are not reading the data correctly.
The data he cites are misleading or irrelevant and his line of argument is contradictory.
Stewart uses quarterly surveillance data to suggest that new reports of AIDS in homosexual men increased by 62% (from 785 to 1268) in late 1987 to early 1988, and that there was subsequently a further rise of 50%.
He arrived at these numbers by comparing reported AIDS cases in homosexual/bisexual males for the UK in 1986 and 1987 (785) with reports for 1988 and 1989 (1268) and for 1990 and 1991 (1907).
Grouping data in this way obscures the underlying trend.
The figure summarises data for England and Wales (excluding visitors).
Locally weighted scatter plot smoothing illustrates the slowing of the rate of increase in AIDS incidence in homosexual/bisexual males, beginning in 1987 and becoming more striking in 1988.
The trend in total AIDS incidence reflects this change.
Stewart also implies that our statement could be based upon the ‘impressive 30% drop in the number of homosexual men serotesting positively for HIV in late 1987 to early 1988’.
This drop is largely irrelevant because the long incubation period of 10 years or more between HIV-1 infection and AIDS onset results in a complex relation between the time of HIV-1 infection diagnosis and AIDS incubation period.
In a symptom-free person diagnosis of HIV-1 infection depends on factors such as risk recognition, willingness to have an HIV-1 test, and test accessibility, all of which changed during the 1980s.
The amount of HIV-1 testing, the proportion found infected when first having an HIV-1 test, and the numbers of HIV-1 infections diagnosed among homosexual/bisexual men between 1985 and 1989, are a poor guide to the underlying incidence of HIV-1 infections and should not be used to interpret the AIDS incidence curve directly.
We find a contradiction in Stewart's statement that the sharp rise in (AIDS) reports in 1987–88 was statistically merely a blip.
The effect of the introduction of the 1987 expanded AIDS case definition was much less in England and Wales than elsewhere and did not result in a sharp rise in incidence among homosexual/bisexual men (figure).
Stewart does not successfully refute the statement that the principal reason for the difference between the 1988 and 1990 forecasts was the reduction in HIV transmission among homosexual/bisexual men in the mid-1980s, the extent of which only became apparent between the reports.
Opportunistic infections and CD4lymphocytopenia with interferon treatmentin HIV-1 infected patients
SIR,— Dr Vento and colleagues (April 10, p 958) reported a decline of CD4 cells after interferon (IFN) treatment in HIV-1 infection.
We recently observed two cases of such cytopenia complicated by opportunistic infections.
Case 1 was a female intravenous drug user, 32, and seropositive since 1987.
IFN treatment (IFN-2a, Roferon, Roche) was started in August, 1992, for chronic viral hepatitis C. Alanine aminotransferase (ALT) was 69 IU/L, CD4 cells 310/L, and serum p24 antigen was negative.
Three months later, the patient had fever and headache which led to the diagnosis of cerebral toxoplasmosis with multiple cerebral abscesses.
CD4 cells were 130/L, and serum p24 antigen was strongly positive (242 pg/ml).
ALT was normal.
IFN treatment was stopped.
The patient is presently well and taking zidovudine, with a CD4 cell count of 90/mm.
Case 2 was a male intravenous drug user, 32, and seropositive since 1986.
IFN was started in February, 1993, for chronic viral hepatitis C. ALT was 180 IU/L, CD4 cells 600/L, and serum p24 antigen was negative.
Six weeks later, the patient was febrile, with perineal pain and oral candidosis.
Biopsy and culture of anal ulcers showed anorectal tuberculosis.
CD4 cells were 134/L and serum p24 antigen was negative.
ALT was 72 IU/L.
The patient is now well and taking four antituberculous antibiotics and zidovudine.
None of Vento's three patients developed AIDS during IFN therapy.
We report here 2 cases of opportunistic infections occurring after 3 months and 6 weeks, respectively, of IFN given for chronic viral hepatitis C in intravenous drug using seropositive patients.
Most HIV-positive intravenous drug users are also infected by hepatitis C virus.
IFN treatment in these patients must take into consideration the CD4 cell count and HLA haplotypes.
SIR,— Most commentators would describe ‘life-terminating acts without explicit request of patient’(LAWER) as non-voluntary euthanasia but the Dutch do not because euthanasia was defined for the Remmelink report only as ‘intentionally taking the life of a person upon his or her explicit request by someone else than the person concerned’.
The only legal category for LAWER in the Netherlands, as elsewhere, is murder.
The patients were incompetent, generally unconscious, and incapable of consent.
In 59% ‘some information’ was available about the patient's possible wishes; in 30% no medical consultation had taken place; in nearly 20% the family was not told what was happening; and in 40%, the killing was at the family's request.
The three illustrative cases call for comment.
In the woman with bone metastases the pain became ‘more and more difficult to relieve with opioids’, not surprisingly since such pain is better treated with other analgesics.
Consciousness deteriorated and, since the pain ‘could no longer be relieved’, it was decided to kill her with a large dose of opioids.
What effect on pain might, say, three-quarters of the lethal dose have had?
One reason for this action was ‘unbearable suffering’.
The patient was unconscious.
Whose suffering?
In the second case consciousness also deteriorated rapidly.
She had a respiratory infection and one reason for life termination was ‘the threat of asphyxia’— so she was given high-dose opioids, which ensured that she died of asphyxia.
The permanently unconscious man, who was not suffering, was killed with a lethal injection, in part because of his poor quality of life.
Whatever other reasons may have existed — and however unpleasant it may be to be reminded of it — this is the same reason used in Germany in the 1930s.
In the early 1970s, when the Dutch first got the legal go-ahead on medical killing, the British were developing palliative care — and so the two nations have continued.
Because there is no organised palliative care in the Netherlands, Dr Pijnenborg and her colleagues think ‘that a doctor's duty to preserve life and to alleviate suffering are in conflict’, and refer to the injustice that a patient must ‘suffer to the end’.
But there is only injustice when preventable suffering is allowed to persist to the point where a person would rather be dead.
The final irony comes with the observation that future safeguards ‘must include optimal palliative care’.
SIR,— Dr Pijnenborg and colleagues state that LAWER ‘complicates’ the debate but does not challenge the moral justifiability of euthanasia.
We disagree: LAWER accentuates the moral problems doctors face once euthanasia is accepted as normal medical practice.
There are methodological problems with this study.
9% of physicians refused to participate and only 322 of the 405 included did take part.
Is it correct to ignore the attitudes and practices of these other physicians?
Second, there are difficulties in interpreting ‘non-explicit requests’.
At best they are uncertain and may well reflect the impressions and attitudes of the attending physicians.
The physician ‘knew the patient’ but is this type of knowledge morally relevant when deciding on life-terminating acts?
One-third of the specialists ‘knew their patient less than a month’, hardly a reassuring foundation for insight into patients' non-explicit wishes.
Pijnenborg et al point out that ‘many doctors’ participate in and accept the practice and they claim that the practice has been stable over the years and assume that it may be less in future.
In the light of the methodological weaknesses and the fact that euthanasia without explicit requests was not permitted (but still yielded 1000 cases per year) we are not convinced that LAWER has not increased in frequency.
That many doctors condone this practice does not in itself indicate any moral justification.
The life-shortening effect in LAWER is claimed to be ‘smaller than that in euthanasia because the patients usually have only a few hours or days to live’.
Is this shorter time frame relevant morally?
Dutch investigators refer to poor quality of life and to the injustice of suffering.
Is it the responsibility of medicine to compensate for poor quality of life or to eliminate injustices by non-consenting life-terminating acts — or is death to be seen as a therapy?
We are told medical decisions at the end of life (MDEL) will increase in importance with demographic shifts towards older populations, with the increasing proportions of cancer and with the growth of life prolonging technologies''.
We agree that medical technology complicates end of life decisions, but do not see how old age and cancer should indicate an increased use of MDEL or LAWER.
SIR,— The details of the circumstances of LAWER lead Dr L. Pijnenborg and colleagues to disturbing conclusions.
First, LAWER is viewed as the caring response of doctor and relatives for the patient, whose wishes, they feel ‘confident’, would have been for life to have been ended.
What gives the doctor the certainty that he knows the mind of his patient?
Human beings reserve the right to alter and change views.
Moreover, if the answer is knowledge of the patient over time, what of the 2.3% of general practitioners and 31.3% of specialists who carried out LAWER on patients whom they knew for less than one month?
Second, should LAWER be viewed as an acceptable outcome of the doctor's responsibility towards well-known patients in great pain but unable to express wishes?
The present state of the hospice movement and continuing research on pain relief make us ask, ‘is optimum pain alleviation being sought?’
It is up to the medical profession to take stock of advances in this aspect of practice.
Yet, if pain remains, who would be so bold as to decide when the patient's limit of tolerance has been reached?
If there is no blanket strategy that can be applied to all, then more and better individualised pain care is needed.
Third, the logical ramifications of LAWER must be considered.
If it is acceptable to take away the lives of some, then we can no longer say that all human life is equally important and valued.
How can we then value life?
The quality of life that is appropriate and fitting for the neonate, young adult, handicapped man, and elderly woman are different.
Therefore, we must have a clear picture of the patients' beliefs and attitudes towards life.
What are we then to make of the 41% of LAWER that took place without discussion or the patient's previous wishes being made known?
Fourth, Pijnenborg and colleagues suggest that there is no evidence that the frequency of LAWER will increase, and thus it need not be the ‘thin edge of the wedge’.
If this is so, is it not strange that already 2% of physicians carrying out LAWER did not consult anyone?
Interviews with doctors who practise euthanasia have shown that they resort to LAWER when they have neither the courage nor cruelty to talk openly with patients, relatives, or colleagues.
Finally, it is fascinating to discover that Pijnenborg and colleagues are not able to define LAWER as murder or medical practice.
How then are we to view the procedure?
Furthermore, is it not rash to say that it is to be carried out only in the patients' best interests?
It does seem rather odd to say that death is for the good of the person involved.
It is far from clear how we can compare a situation of living with one of absence of sensation and life itself.
SIR,— Dr Pijnenborg and colleagues' description of LAWER resembles the dictionary definition of murder (‘unlawful killing of a human being on purpose’).
Their reasoning — that ‘when all safeguards are respected and the best interests of the patient are taken into account, it is certainly not murder’— parallels the apparent logic behind the Soviet cleansing of antisocialist elements, the Indian practice of murder/suicide of widows, and the infanticide that I observed among the Tsamai in south-west Ethiopia.
Dutch euthanasia practice is legalised and therefore becomes a collective responsibility but, as with war crimes, collective responsibility does not free the actor (the physician) from personal accountability.
Increased cerebral blood volume associatedwith pneumothorax in preterm infant
SIR,— Pneumothorax has been shown to be associated with intracranial haemorrhage in the newborn.
There is evidence from observational studies in newborn infants, and experiments in animals that the pathogenesis linking these two conditions is venous congestion.
An unexpected increase in cerebral blood volume (CBV) was observed during mechanical ventilation of a premature infant suffering from hyaline membrane disease.
The infant was monitored continously with near-infrared spectroscopy on days 1 and 2 of life.
Changes in CBV were recorded continuously by a cerebral oxygen monitor 105 Kritikon (Johnson & Johnson).
Absolute values of CBV were determined repetitively according to the method proposed by Wyatt.
On day 1, CBV remained stable whereas on day 2 it increased.
At the end of day 2, pneumothorax was diagnosed.
Heart rate, transcutaneous carbon dioxide tension, arterial saturation, and mean arterial blood pressure did not change.
This observation suggests that pneumothorax can influence cerebral haemodynamics without affecting heart rate, blood pressure, and blood gases.
Detection of changes in cerebral haemodynamics by near-infrared spectroscopy allows early intervention to prevent intracranial haemorrhage.
Limitations of pulse oximetry
SIR,— Dr John and Dr Peacock (April 24, p 1093) report delayed recognition of ventilatory failure in 3 cases of severe hypercarbia monitored by pulse oximetry (SpO).
We observed limitations of SpO in 6 patients treated after inhalation of carbon monoxide (CO), whose carboxyhaemoglobin (COHb) on admission ranged between 29% and 41%.
2 female (aged 87 and 71 years) and 1 male patients (70) were admitted after they complained of headache, dizziness, vertigo, and ataxia, and CO-poisoning was suspected.
1 h after admission (about 4 h after the start of symptoms) the male patient had a non-Q-wave myocardial infarction despite oxygen administration.
He was transferred for further treatment to a medical ward, and the other 2 patients were discharged.
2 further patients (1 man aged 60 and 1 woman of 61) were brought to our institution after exposure to CO at the home of an older relative who had died.
Their symptoms were headache and nausea.
Oxygen was given and both patients made an uneventful recovery.
The sixth patient was a 33-year-old man who was admitted after a suicide attempt by inhalation of car engine exhaust fumes and ingestion of about 20 mg of flunitrazepam.
On admission he was intubated, haemodynamically stable, and unresponsive to pain.
He was mechanically ventilated with 100% oxygen and eventually extubated after intravenous flumazenil.
After initiation of psychiatric care and normalisation of a moderate rhabdomyolysis (peak creatine kinase 4120 U/L) the patient was discharged.
Serial measurements of COHb (IL 482 CO-Oximeter, Instrumentation Laboratory, USA) were done hourly in all 6 patients until the index became normal, and arterial blood pressure, heart rate, and SpO were monitored (peripheral oxygen saturation by M1020 module, Hewlett Packard, USA).
The figure shows mean COHb values with corresponding SpO.
At all 18 measurements, the mean SpO value was above 91%, which would be readily accepted as sufficient oxygenation.
Decreasing COHb values led to a slight increase of SpO, as would be expected by the formula SpO= (OHb+0.9COHb) /total Hb100%, according to Tremper and Barker.
The increasing availability of pulse oximetry in intensive care units may lead to a false interpretation of oxygen transport capacity in cases of CO poisoning, especially if SpO is between 91% and 98%.
Physicians should be aware that the diagnosis of CO poisoning still depends on a high degree of clinical suspicion and direct measurement of CO.
Increased serum tumour necrosis factor during transient remission in acute leukaemia
SIR,— Spontaneous remission of acute leukaemia is unusual.
Most instances have been associated with infection, blood transfusion, or termination of pregnancy.
The mechanism is unknown, although it has been suggested that activation of tumour necrosis factor (TNF) may be responsible.
A 72-year-old woman had a transient remission of acute undifferentiated leukaemia coinciding with a pneumonia, during which raised concentrations of TNF were found.
She was admitted to hospital in September, 1991, because of fever, productive cough, and weakness.
She had a history of insulin-dependent diabetes and hypertension.
Physical examination was normal.
Blood values were: haemoglobin 105 g/L, mean corpuscular volume 89 m, white blood cells 1.410/L (12% neutrophils, 4% band forms, 80% lymphocytes, 4% eosinophils), and platelets 20210/L.
The bone marrow aspirate was hypercellular, with 71% of blast cells.
Cytochemistry for myeloperoxidase and esterases was negative.
Immunological marker studies of bone marrow (immunoalkaline phosphatase) showed CD45 and HLA-DR positivity in blast cells, whereas CD14, CD15, CD7, CD19, and CD20 were negative.
Cytogenetic analysis of the bone marrow showed 3n hyperdiploidy (15 evaluable metaphases).
Due to the age of the patient and the medical history, treatment with low-dose daunorubicin (55 mg/m per day for 2 days) and cytoarabine (100 mg/m per day in continous intravenous infusion for 5 days) was given.
A bone marrow aspirate 15 days after the end of chemotherapy was hypocellular with 62% blast cells.
The patient was followed up as an outpatient, and only supportive treatment with packed red cells given.
6 months later she was readmitted with pneumonia.
The blood, urine, and sputum cultures were negative.
Ceftazidime, aztreonam, erythromycin, and amphotericin B were given.
The pneumonia resolved in 18 days.
Staphylococcus epidermidis was isolated in two blood cultures.
A bone marrow aspirate showed 64% blast cells.
The patient was discharged without any further treatment.
After two weeks the general health of the patient improved, and her blood values were: haemoglobin 145 g/L, mean corpuscular volume 88 m, white blood cells 4.510/L (64% neutrophils, 1% basophil, 3% eosinophils, 32% lymphocytes), and platelets 23910/L.
A bone marrow aspirate was hypercellular.
Of the three haematopoietic cell lines, the granulocyte and megakaryocyte series were normal and the erythrocyte line was hyperplastic.
Only 9% of blast cells were seen.
In the cytogenetic study only 3 metaphases were evaluable, 2 of them being normal and the other 3n.
The patient remained symptom-free for 5 months, until her health worsened.
The blood showed haemoglobin 115 g/L, mean corpuscular volume 92 m, white blood cells 1.610/L, and platelets 13010/L.
The bone marrow aspirate was hypercellular, with 40% of blast cells.
A phenotype study of blast cells showed positivity for HLA-DR, tdT and CD45, CD68, and antimyeloperoxidase monoclonal antibodies.
TNF serum concentrations were retrospectively measured in samples taken during her illness.
At the time of diagnosis of leukaemia the TNF was 3 pg/mL; during her pneumonia, 40 pg/mL; 2 months after, 8 pg/mL; and 5 months after, 7 pg/mL.
The transient remission was not due to chemotherapy given 7 months before, without response, and coincided with pneumonia and S epidermidis bacteraemia.
Several cases of spontaneous remission in acute leukaemia have been described in association with bacterial or viral infections.
The most interesting feature in this patient was the inverse relationship between TNF and the number of bone marrow blast cells.
TNF is a cytokine that regulates cell proliferation and differentiation, and takes part in the immune response.
For this reason, TNF can act as an antineoplastic agent.
The relation between high activity of TNF and partial remission of leukemia in the patient reported suggests that TNF could have influenced the transient inhibition of blast cell proliferation.
Sudden infant death associated withdefective oxidative phosphorylation
SIR,— Sudden infant death preceeded by a fulminant hepatocellular disease has been described in children with inborn errors of mitochondrial fatty acid oxidation, such as medium-chain acylCoA dehydrogenase deficiency.
Until now there have been few reports on hepatic failure caused by disorders of mitochondrial oxidative phosphorylation.
The patient described by Boustany et al had a cytochrome c oxidase deficiency and a long illness before she eventually died of peritonitis.
The neonates described by Cormier et al, also with cytochrome c oxidase deficiency, had liver disease associated with repeated episodes of ketoacidotic coma.
We report a formerly healthy boy who suddenly died at 11 months after a Reye-like syndrome.
His older sister had died at the age of 6 months after an identical clinical course.
The boy was screened for inborn errors, with negative results.
His medical history was unremarkable until the day before admission.
Early in the afternoon, he became ill with fever and vomiting, and refused feeding.
A general practitioner diagnosed otitis media and prescribed nose-drops.
At 2300 h the boy was in a deep sleep and could hardly be awakened.
Next morning he was found comatose and taken to hospital.
There was severe hypoglycaemia (0.7 mmol/L), a moderately increased lactate (2.4 mmol/L), normal pyruvate (130 mol/L), and hyperammonaemia (240 mol/L).
Hypoglycaemia was corrected and artificial ventilation started, and he was brought to our clinic.
The liver was enlarged by 2–3 cm.
Neurological examination showed irritability, opisthotonos, and hypotonia.
Examination of the urine showed an abnormal, but transient excretion of dicarboxylic and hydroxy-dicarboxylic acids with only slightly increased ketone body concentrations.
Intravenous glucose administration (8 mg/kg per h) was continued and riboflavin also given.
During the next few hours, he had multiple convulsions.
Treatment was stopped after the electroencephalogram was found to be iso-electric.
Necropsy was done within an hour of death.
The liver showed marked steatosis.
Investigation of the fatty acid oxidative capacity in muscle and liver was normal.
Finally, we determined the activities of the respiratory chain enzymes in skeletal muscle.
The results were compatible with a mitochondrial oxidative phosphorylation disorders caused by a defect in the coenzyme Q region of the respiratory chain (table).
Most probably the boy's sister had the same biochemical defect.
At necropsy, she had the same abnormalities, although the activities of respiratory chain enzymes were not examined.
Routine metabolic screening of urine does not fully guarantee the absence of an oxidative phosphorylation disorder.
Antistreptokinase titres after topicalstreptokinase
SIR,— In 1990, Jalihal and Morris reported the results of a study showing a positive antibody response to a single intravenous dose of streptokinase.
The increased use of thrombolytic agents after myocardial infarction has resulted in numerous patients being exposed to streptokinase and antistreplase.
Re-exposure to these agents is not recommended within a cetain time because of the high antibody level and the risk of anaphylaxis.
To see if topical streptokinase, as in ‘Varidase’(streptokinase and streptodornase) could be absorbed from a wound site and produce a similar response I measured anti-streptokinase titres in patients with wounds.
The patients were monitored for any streptococcal infections during sampling, but no attempt was made to measure the amount of streptokinase absorbed.
The amount used per treatment, number of treatments, type, and condition of the wound are shown in the table.
Dr Jalihal did the antistreptokinase tests and the results are shown in the figure.
As can be seen, although the response is variable, all patients showed an increase in titre.
It would therefore seem prudent to restrict the use of topical streptokinase to patients not at risk of myocardial infarction.
Triazolam safety
SIR,— Dr Jonas of Upjohn (May 1, p 1150) claims that my summary of the FDA Psychopharmacologic Drugs Advisory Committee meeting on triazolam was highly selective.
The FDA did not ban the drug, but it did find the 0.5 mg dose excessive and had safety concerns about 0.25 mg.
All the FDA reservations and warnings occurred despite in-house differences of opinion about triazolam's safety.
The FDA Division of Epidemiology and Surveillance has consistently held over the past six years that triazolam produces many more and more severe psychiatric adverse reactions than comparable benzodiazepines.
The Division of Neuropharmacologic Drug Products disagree; but this group was responsible for originally approving the use of the 0.5 mg dose in the USA, over the objections of the FDA's own monitors.
In 1988, following the ban on the 0.5 mg dose form in France, Italy, and West Germany, the FDA allowed Upjohn to reduce their recommended starting dose to 0.25 mg.
In 1989, on the basis of data from its Division of Epidemiology and Surveillance, the FDA required stronger warnings, stating that triazolam presented a greater risk of anterograde amnesia than most other benzodiazepines.
In 1991 and 1992, the FDA emphasised the need for very short-term use and the fact that even 0.25 mg produced significantly more daytime anxiety than comparison drug or placebo.
By contrast, R. H. B. Conacher of Upjohn Ltd has stated that in clinical trials various behavioural side-effects such as nervousness ‘occurred in patients receiving placebo at a rate indistinguishable from that in patients receiving Halcion’.
Perhaps it is Upjohn that is being highly selective regarding evidence on serious psychiatric reactions to triazolam.
SIR,— Dr Jonas questions the documentation in our petition to the FDA arguing that triazolam's benefit-to-risk ratio is inadequate.
Of the 157 references in that petition, 108 are specifically cited to a document.
These include: 30 controlled studies, by sixteen different investigator groups; 13 controlled studies indicating limited efficacy for 0.25 mg of triazolam; 50 case-reports; 8 reports of post-marketing surveillance data; and 11 published reports of laboratory evidence on triazolam's safety problems.
These 108 references do not include 4 references in the petition that constitute meta-analyses or reviews of different aspects of triazolam's adverse reactions, to which Jonas refers misleadingly as ‘replications of the same study by Kales.’
Jonas also states that ‘equipotent doses of triazolam and other hypnotics show that triazolam does not produce more amnesia or behavioural medical events than other benzodiazepines.’
However, the controlled studies we cited compared doses of benzodiazepines that were equipotent both in regard to efficacy and to the starting recommended doses proposed by the respective manufacturer and approved by the FDA.
Upjohn's treatment, over the years as adverse reaction reports have accumulated, of the starting dosage recommendations for triazolam is instructive.
Upjohn first introduced triazolam in 1977 at a recommended starting dose of up to 1.0 mg.
After the Netherlands' 1980 ban, Upjohn lowered the starting dose to 0.5 mg.
When France, Italy, and West Germany banned the 0.5 mg tablet, Upjohn again reduced the dose worldwide to 0.25 mg.
Even this has proved unacceptable in several countries, including the UK (ban) and France and Spain (reduction to 0.125 mg).
There have been sixteen drug regulatory actions worldwide against triazolam since 1980 aimed at the manufacturer's recommended doses.
Every time Upjohn denied the safety concerns yet subsequently reduced the dose.
Stillbirth risk in Japan
SIR,— Of a total of 3715,762 infants delivered in Japan after 23 weeks of gestation over the three years 1989 to 1991, 22,531 were stillborn.
Stillbirths account for about two-thirds of the perinatal deaths in Japan.
We calculated the ‘prospective risk of stillbirth’ at week N, a newly proposed statistic, as:(number of stillbirths at or beyond gestational week N)(number of total births at or beyond gestational week N) 1000.
The figure shows the prospective risk of stillbirth at 23–42 weeks' gestation for all births in Japan and the cumulative percentage of stillbirths.
The risk at 23 weeks was about 6 per 1000, indicating that 1 in 165 pregnant women will subsequently deliver a stillborn infant.
The risk decreased by 39 weeks' gestation to about 1.1 per 1000 (1 in 909).
Thereafter the risk increased progressively reaching more than 3 per 1000 at or beyond 42 weeks.
A nearly constant fraction of fetuses that are alive at 23 weeks' gestation die each subsequent week, indicating that every fetus faces a random, but statistically predictable, risk of intrauterine death throughout pregnancy.
Despite antepartum fetal surveillance regimens, 28% of stillbirths (about 2000 per year) occur after 36 weeks of gestation.
More than three-quarters of all stillbirths after 36 weeks were linked to unfavourable antepartum or intrapartum conditions and were free from fetal malformations.
Consequently, the lives of more than 1500 mature and possibly healthy fetuses are lost in Japan each year.
Their lives may be saved by either an appropriate mode of delivery or by ending the pregnancy at an appropriate gestational age — the optimal time for delivery (the earliest date in a given pregnancy at which birth would carry a higher probability of healthy survival than would continued gestation).
Our figures on the risk of stillbirth by gestational age strengthen the concept that, especially from the fetuses' perspective, there is a biologically optimal time for delivery.
Unilateral pulmonary interstitial emphysemaand treatment with colfosceril palmitate
SIR,— Surfactant treatment is now widely used for the treatment of hyaline membrane disease (HMD) in the preterm infant.
Reduction in mortality and the incidence of pneumothorax and pulmonary interstitial emphysema (PIE) has been shown.
We report an unusual complication.
We have recently seen four cases of early onset, unilateral PIE after the use of colfosceril palmitate (Exosurf)(table).
All infants had HMD and satisfied the criteria for treatment used by the American Exosurf Neonatal Study Group.
The drug was given at the recommended dose via a correctly positioned endotracheal tube.
Early expected improvement in chest wall expansion and arterial blood gases did not occur, nor was there a reduction in peak inspiratory pressure (PIP)(table).
Radiologically severe unilateral PIE was noted in all babies by 33 h of age, and in case 4 as early as 12 h.
In cases 2 and 3, involving the right and left lung respectively, the changes were asymptomatic.
In case 1, a right-sided pneumothorax occurred requiring drainage, and in case 4 a right-sided pneumatocele resulted in multiple pneumothoraces.
This infant died at 34 days.
Generalised PIE is a well-recognised complication of mechanical ventilation in preterm infants.
The air collections in the extra-alveolar space are related to barotrauma to the immature mesenchyme.
Infants most at risk are those where high PIP (>26 mm Hg) is used within the first 24 h.
Because the need for high PIP remained unchanged, we believe that in these cases there was preferential distribution of surfactant into one lung exposing that side to increased barotrauma.
The manufacturer's leaflet does not mention the risk of maldistribution and we draw attention to this potential complication.
Artemether in cerebral malaria
SIR,— The possibility that observer bias may compromise an unblinded study must always be seriously considered.
Dr Anstry (April 17, p 1035) raises this concern about our study of intramuscular artemether versus intravenous quinine as treatment for severe paediatric malaria.
An unblinded design was necessary for this study, since artemether is commercially available only as an intramuscular injection, but intravenous quinine remains the treatment of choice for cerebral malaria in children.
The subjectivity of the assessment of appropriate cry is unlikely to have compromised our results, because recovery of an appropriate cry was usually achieved earlier than a full coma score — in only 9 patients (4 on quinine, 5 on artemether) would a different interpretation of normal cry have affected coma resolution time.
The mean time taken to achieve a normal cry, although shorter in the artemether group, was not significantly so, and the difference was not responsible for the significant difference in coma resolution times.
We have also noted that there is in practice little inter-observer variation in the scoring of verbal responses in these patients.
The analysis of much larger numbers of patients will soon be possible in this multicentre trial.
We will then also be able to establish whether artemether has a beneficial effect on the objective and unambiguous primary outcome measures (full recovery versus recovery with sequelae versus death).
Childbearing, oral contraceptive use, andbreast cancer
SIR,— Dr Beral and Dr Reeves (April 24, p 1102) support an interesting hypothesis about possible similarities in the role of pregnancy and of oral contraceptive (OC) use on breast cancer risk.
Their conclusion that apparent inconsistencies in this epidemiology might be resolved if the age of diagnosis of breast cancer is taken into account is well taken.
They anticipate that OC use will act like pregnancy on breast cancer risk and thus be associated with a reduction in risk at older ages, It would be a great relief to young women now if there was indeed such a decreasing risk.
Another crucial factor, not discussed by Beral and Reeves, is the age at which OCs are commonly used.
The average age of starting and the prevalence of OC use while young has changed strikingly in the past decades.
Prolonged early OC use (use before first pregnancy for instance) was not common until the middle 1970s.
Thus women who are 45 years old or more will not have been commonly exposed to OC use while young for long periods.
Hence the effect of early use on breast cancer risk at or above this age is difficult to determine.
The fairly consistent findings of an increased relative risk associated with OC use while young on breast cancer at young ages might portend a changing risk with age of diagnosis in any direction: increasing, level, or decreasing.
Conclusive evidence for any of these possibilities is lacking.
If early OC use affects the rate of early stage carcinogenic changes then current epidemiological studies will consistently underestimate the ultimate effect, even if the relative risk increases with age at diagnosis, as would then be expected.
In particular, an effect will only now be discernible for breast cancer at a young age, high doses of hormone will seem to have higher risk, and new studies will indicate stronger effects than those done a decade ago.
All these effects have been reported.
A recent overview, which includes all case control studies, reports a highly significant duration-related effect of early OC use on premenopausal breast cancer.
These results could still be seriously attenuated by insufficient follow-up, since they include many studies undertaken in the 1970s.
They also lack statistical power since most breast cancer cases included will be older women approaching 50, who will not have been commonly exposed.
So it seems premature to anticipate a reduction in risk by citing evidence from only one large case control study (the CASH study) which, it ought also to have been added, was completed in the early 1980s.
Then the past OC use of its subjects was very different from the past OC use patterns of women currently at risk of breast cancer and hence included little early OC use.
Unusual electronmicroscopic changes invalproate-associated liver failure
SIR,— Liver failure and acute pancreatitis may occur in children treated with valproate (VPA).
Fatal hepatotoxicity during VPA treatment has been reported rarely in adults, and only in patients with encephalopathy.
A 19-year-old previously healthy woman was admitted with generalised tonic-clonic seizure, followed by focal myoclonus of the right shoulder.
General examination was normal.
There was no mental retardation.
Laboratory studies, including skin and sural nerve biopsies, failed to show any metabolic disorder.
Magnetic resonance imaging showed a 5 mm high-signal area in the rolandic cortex, suggesting a small dysplastic lesion.
Both seizures and myoclonus were controlled with clonazepam 2 mg daily and VPA 1.5 g daily.
Myoclonic epilepsy reappeared with a low blood VPA (50 mg/L).
She was treated for 1 month with valproate 2 g daily (40 mg/kg), then nausea, vomiting, and abdominal pain occurred.
Laboratory findings included: severe hypoglycaemia, hyperammonaemia, amylasuria (2000 IU/L), and amylasaemia (200 IU/L); increased serum transaminases (AST 200 IU/L, ALT 280 IU/L), and normal -glutamyltranspeptidase and alkaline phosphatase; increased bilirubin (440 mmol/L); and impairment of coagulation.
VPA was discontinued.
Surgical exploration confirmed pancreatitis with peritoneal petechiae and a few areas of cytosteatonecrosis.
Despite carnitine treatment, the liver function declined continuously with development of jaundice, hepatoencephalopathy, and a bleeding diathesis.
Liver transplantation was proposed but the patient died from cardiovascular failure.
Liver examination showed prominent centrolobular necrosis with no evidence of regeneration.
There were large amounts of microvesicular fat, areas of intrahepatic cholestasis, balloon degeneration of hepatocytes, and periportal fibrosis.
Electronmicroscopy showed numerous lipid droplets, scarcity of cytoplasmic cells, and normal mitochondria.
The clinical features and light microscopic findings were similar to those described in children with liver failure due to valproate or in Reye's syndrome.
The association with pancreatitis has been reported but prognosis usually depends on the hepatic failure.
Electronmicroscopic changes were unusual since mitochondria appeared normal.
In children, mitochondrial swelling and biochemical abnormalities suggested that VPA hepatotoxicity was related to mitochondrial -oxidation impairment.
The mechanism of VPA hepatotoxicity may be different in adults.
Ro 40-7592, a COMT inhibitor, plus levodopain Parkinson's disease
SIR,— The treatment of Parkinson's disease complicated by motor fluctuations aims to stabilise plasma concentrations of levodopa.
Ro 40-7592, a selective peripheral and central catechol-O-methyltransferase (COMT) inhibitor, increases levodopa's half-life and bioavailability.
We investigated the clinical response to single oral doses of Ro 40-7592 plus Madopar (levodopa/benserazide) in parkinsonian patients.
After study approval by our hospital ethics committee, 11 patients with Parkinson's on-off fluctuations signed consent to participate in this double-blind placebo-controlled study.
There were 4 men and 7 women with a mean age of 67, a mean duration of disease of 12.5 years, and mean levodopa therapy lasting 11.6 years.
The patients received in a randomised order an acute challenge of Madopar (100 mg levodopa in 7 patients, 150 mg in 4) with 200 or 400 mg Ro 40-7592 or placebo on 3 test days.
There was 72 h between tests.
After an overnight fast, the drugs were given at least 1 h after waking.
Clinical response was assessed continuously until the patient switched off.
Every half-hour, blood pressure and heart rate, the motor examination part of the unified Parkinson's disease rating scale (UPDRS), a modified abnormal involuntary movement scale, and a hand tapping test were assessed.
Analyses were done with paired t tests with Bonferroni's correction.
1 patient withdrew because of symptomatic hypotension with visual illusions on the first test day with 200 mg Ro 40-7592, but the other 10 completed study.
Compared with placebo, 200 and 400 mg Ro 40-7592 significantly increased the duration of the on-phase by 65% and 77%, respectively (figure).
There were no significant differences between mean latencies to turn on, and between mean UPDRS and tapping test scores in the on-phase.
After 400 mg Ro 40-7592, mean dyskinesia score during on-phase increased by 20% compared with placebo (p < 0.05).
Neither dose altered disability related to dyskinesias.
Electrocardiograms and laboratory indices were unchanged.
200 and 400 mg Ro 40-7592 increased the duration of the motor effects induced by a single fasting oral dose of levodopa with minimum side-effects.
In healthy volunteers 200 mg Ro 40-7592 increased levodopa half-life by about 90% and the area under the curve by about 100%, without increasing the maximum plasma concentration.
These peripheral pharmacokinetic changes could account for the beneficial motor effects of Ro 40-7592 administered with levodopa.
However, since Ro 40-7592 can cross the blood-brain barrier and modify striatal dopamine metabolism, a central pharmacokinetic effect may also have a role.
Mefloquine
SIR,— I disagree with F. O. ter Kuile and colleagues' conclusions (April 24, p 1044) about the anti-malarial drug mefloquine, which is regarded as the most effective such medication at present.
These researchers believe that mefloquine is safe for people who have no history of neuropsychiatric disorders.
I fall into this category.
Two years ago I left on my third African consultancy — this one to propose an anti-AIDS programme for Uganda.
On previous trips I had taken chloroquine without noticeable side-effects.
By my first week in-country (my third on mefloquine) I had reduced sensation in my legs, was waking up with a start from severe nightmares which subjectively had lasted for hours (in reality for only 15 min), and occasionally wondering what it would be like to jump the eight floors from my hotel room.
Fortunately, my team-mates diagnosed a drug reaction.
Since they had all been longer than I in Kampala, they were taking what all expatriates (including all American Embassy staff) take: chloroquine plus proguanil hydrochloride (Paludrine, a British ICI-manufactured drug not yet approved by the American Food and Drug Administration).
My symptoms subsided immediately upon quitting mefloquine.
Later, when I consulted on another matter a British doctor who has been in Kampala some thirty years, he stated that he ‘never advises patients to take mefloquine.
It is a very dangerous drug’.
When I returned to Washington and told this story, I received several accounts of severe reactions, including psychosis, among healthy individuals who had taken this drug.
My physician has filed an adverse drug reaction report.
However, most people fail to do so unless they are admitted.
The cases reported to Hoffman-La Roche I believe are a tiny fraction of all reactions.
Incidentally, their researchers responded to my report to them by explaining away my reactions as being due to‘travel and change of climate, day time, and sleep patterns’.
I have worked around the world without so reacting to these factors.
As a demographer with a quarter of a century's experience I know that if I encounter finite numbers of a supposedly rare occurrence, the true rate is higher.
I also know to trust the observations of well-trained observers in the field.
My advice is: avoid mefloquine, advise patients to take chloroquine, and add Paludrine when they reach a country where it is obtainable.
Ultrasonic detection of cerebral emboli incarotid stenosis
SIR,— Dr Parmet and colleagues' (April 24, p 1057) demonstration of venous emboli detected during knee arthroplasty illustrates the use of a technique that may have much wider applications in the study of embolism.
It relies on emboli acting as ultrasound reflectors, resulting in high intensity signals that can be localised on a 2-dimensional image, as these researchers show.
However, emboli can also be detected in arteries, including the intracranial arteries, with doppler ultrasound alone without B-mode imaging, when they appear as short-duration high-intensity signals in the doppler spectrum (figure).
This technique has proved sensitive enough to detect pathological embolic materials such as thrombus, atheroma, and platelet aggregates, and the characteristics of the resulting signal provide information on the size of the embolus.
The ability to detect symptomless arterial cerebral emboli may have major benefits in the management of patients with cerebrovascular disease, as shown by the following case report.
A 44-year-old man, who presented with an abrupt onset of weakness in the right arm, was shown by computerised tomography to have a left lentiform nucleus infarct.
He recovered fully but had repeated transient ischaemic symptoms affecting the right arm lasting a few minutes despite aspirin therapy, with twenty episodes occurring during 6 months.
An angiogram showed only minimum irregularity of the wall contour at the right carotid bifurcation.
Aspirin therapy alone was continued; however, he continued to have transient ischaemic attacks.
With the use of a standard transcranial ultrasound machine the doppler signal was recorded from both middle cerebral arteries for 20 min each.
Recordings were stored on digital audio tape and visually analysed by an observer blinded to the side or clinical diagnosis (the recording was interspersed with recordings from control patients).
In the left middle cerebral artery, frequent high-intensity signals were seen (figure).
These embolic signals were noted at the very frequent rate of 4.8/min during the 20 min recording time.
They are very similar to those seen when emboli are introduced into experimental models, although they are of lower intensity (mean 5.61 dB, maximum 7.16 dB), suggesting a size much smaller than the 400 m or larger experimental emboli studied.
No embolic signals were seen on recording from the right middle cerebral artery.
On re-examination of the carotid bifurcation with carotid duplex ultrasound, a large smooth plaque was found in the bulb, causing a 30% stenosis, but parallel with the carotid bulb wall, explaining why the size of the plaque was not detected on angiography.
Warfarin was started and transcranial doppler recordings were repeated 1 month later; he had had only one transient ischaemic attack in the interim, when his international normalised ratio had been only 1.3.
By contrast with the previous recording, only one embolic signal was detected in the left middle cerebral artery during the whole 20 min; again none were detected on the right side.
This case illustrates the potential of this technique to detect symptomless emboli in the cerebral circulation.
It will prove useful in localising the embolic source in patients with stroke and may allow identification of subgroups of patients at risk of subsequent embolic stroke.
It may aid in monitoring therapy; in our patient the institution of warfarin therapy was followed by a striking reduction in the frequency of emboli.
This technique may also provide valuable information on the pathogenesis of cerebral embolism, and our case shows the large number of apparently symptomless emboli that may arise in such patients.
Was Aesop a chronobiologist?
SIR,— Among the best known of Aesop's fables is the story of the tortoise and the hare.
When the hare belittled the tortoise for his slow ways, the tortoise responded that he could nevertheless win a race between them.
At the start, the hare dashed off and built such a commanding lead that he decided to relax briefly and lay down for a nap.
By the time the hare awoke, the tortoise was already at the finish and even the hare's fastest running proved fruitless.
For over 2000 years, the moral — slow and steady wins the race — has lauded the tortoise's perserverance and dedication while condemning the hare's arrogance and laziness.
With modern knowledge, however, we suspect that the outcome of this historic race depended less on the character traits of these two species and more on differences in their innate 24 h activity cycles.
Tortoises are diurnally active, ambulating mainly over the course of the day.
Hares, on the other hand, are active in a crepuscular pattern — ie, they restrict their running between their feeding sites to dawn and dusk.
Aesop must have been a keen observer of natural animal rhythms.
Rather than agreeing to a daytime race, the hare should have countered the tortoise's challenge by proposing a sprint at twilight.
We believe the moral of the fable should be modified to reflect the importance of the internal biological clock that regulates such behavioural rhythmicity.
In man also, performance capabilities for various tasks vary with time of day, with some abilities peaking in the morning and others in the evening.
Ignoring these rhythmicities may be perilous, and some of the symptoms of jet lag, shiftwork, ageing, insomnia, and affective disorders may be due to their temporal disorganisation.
Thus, slow and steady wins the race, but timing is everything.