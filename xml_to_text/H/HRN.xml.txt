

Where does Russia head now?
Against some odds, President Yeltsin has won the part of the Russian referendum that he says matters most, but the really hard work is only just beginning.
Mr Boris Yeltsin, the president of Russia, may have won the battle, but he has not yet won the war he has been fighting for more than half a year with the Congress of People's Deputies.
that is how it seemed earlier this week, as people began making educated guesses about the outcome of the weekend's referendum.
It seems certain that Yeltsin will have won the trust of a handsome majority of those turning out to vote; consistently he has said that this is the only question he would take seriously.
On the other hand, he is like to have failed to win a majority of the whole electorate for an early reelection of the Congress.
That is bad news, and threatens a continuation of the debilitating battle between the government and the parliament that has dragged on since last November.
perpetuation of the pantomime of the past few months will serve nobody's purpose.
It is true, of course, that for the first time in 75 years, Russia now has in place the mechanisms of national democracy — a government whose president has been elected and a parliament able to enforce, by argument and voting, restraints on what the government does, as well as a constitutional court.
But the Congress in its present form dates back to Mikhail Gorbachev's; it was not elected by general suffrage, but (on behalf of the whole of what was still the Soviet Union) partly elected on a local basis and partly nominated by special interest groups.
(The All-Union association of philatelists had a separate influence, for example.)
The old Central Committee's placemen are still there.
No wonder that the government and the Congress have been at loggerheads.
Some of the consequences have been absurd.
Early on, the Congress won the right to control Russia's central bank, which has since kept on printing money with which to subsidize bankrupt enterprises, thereby undermining Yeltsin's efforts to engineer economic reform.
During the same period, it has voted to change the Russian constitution — itself new — on roughly 50 occasions.
Who can be surprised that ordinary people do not know where they stand?
And while irrational resentment abounds in Russia (and the Congress) about the way in which economic reform has created several battalions of dollar-millionaires, many of Yeltsin's problems stem from the sharp practices that have allowed these small  fortunes to accumulate, mostly unhindered by taxation.
So what should Yeltsin now do?
Forbidding though the prospect may be, he needs another talking shop — a commission whose function would be to draw up a definitive Russian constitution that could then be entrenched against the amending whims of the Congress.
(Amendment only by referendum would make sense.)
Legislating for seemly relationships between the government and the Congress may be the most urgent need; creating the circumstances in which the patchwork of titularly and hopefully autonomous regions that make up the Russian Federation have an incentive to hang together is, in the long run, more important.
Yeltsin could do his country a power of good by directing public attention to these issues.
Can he succeed?
There are two views of what has happened in Russia in the past few years.
One is that it has lapsed into chaos.
Another is that it has survived this far only because of a remarkable social cohesiveness (in which traditional fatalistic passivity may have played an important part).
Both views embody aspects of the truth.
On the whole, the durability of this remarkable society is the more impressive.
Yeltsin has that on his side.
But he must do more to suppress the air of lawlessness that now abounds (which would incidentally take the wind out of the Congress's sails) and persuade people that paying taxes is a social virtue (which would then help balance Russia's books).
No part of Russia's social fabric is more durable than its network of people with research at heart.
To be sure, many able people have now gone elsewhere.
Others have been forced out by impoverishment.
It is far from clear that the hope of the russian Academy of Sciences that industrial contracts can keep the rump of its establishments in being can be realized; experience in other countries in not encouraging.
Yet there remains a core of Russian research, not simply academic, that remains excellent and which seems certain to survive.
Yeltsin could help by ridding it of bureaucratic incubuses.
He will not in the long run profit from arrangements that turn the surviving research institutes into training grounds for emigrant specialists.
But would it matter all that much if Yeltsin failed?
Russia, after all, is not the only member of the Commonwealth of Independent States.
Do not the other eleven deserve at least as much attention, even compassion?
And what of the states of Central Europe, themselves also recently unshackled?
The answer should be, ‘Of course’.
There is indeed a serious danger that Western preoccupation with Russia's plight will induce neglect, even complacency, about the remainder of the ex-Soviet empire.
Yet what happens in Russia is distinctively important.
Its sheer size in one consideration, its huge stock of military equipment is another.
And Yeltsin's failure would probably presage a revival of the old nationalism that drove the tsars to the Pacific coast of Asia.
Who would benefit from that?
OPINION
Anglo-US concern
Both countries want industry to be more competitive, but differ in their recipes for using science and technology.
Do US president Bill Clinton and British prime minister John Major have more in common than would have been guessed from their brief meeting earlier this month?
Each is dusting off schemes for supporting innovative industry.
The Clinton administration is said to be looking again at the tangible objectives underlying the federal government's support for research (see page 776), while the British government, in the shape of the Department of Trade and Industry, is seeking a more direct (if a more avuncular than the American) role in fostering industrial competitiveness.
What can they hope to achieve?
These are occasions when warnings that civil servants are poor at ‘picking winners’ are again in order, misanthropic though their utterance may appear.
Even those with relatively short memories in either country should be aware of that.
In the 1960s, for example, the US administration was captivated by the doctrine of Research Applied to National Needs (or ‘Rann’), the most tangible benefit of which was the improvement of building standards for earthquake resistance.
(A decade later, the Pentagon's support for the large-scale integration of circuits on silicon surfaces almost surreptitiously created a new industry.)
In Britain a little earlier, Mr Harold Wilson's advocacy of ‘white-hot technology’ bequeathed to the country four uneconomic aluminium smelters.
There is a particular danger in the US inclination to regard the $14 billion a year the federal government now spends on basic research as a package of goal-directed programmes.
it has come to seem like that only by accident.
Recognizing that, when several agencies have a finger in the pie of, say, biotechnology, it is prudent that there should be a committee to coordinate their spending, the committee's budget then becomes an identifiable object in itself, to be judged against what may be called ‘results’.
The difficulty, of course, is that agencies' spending covers a diversity of objectives, from the practical to the basic.
Direct identification of benefits is difficult, but the substantial advantage to US industry is the small army of people skilled in the techniques of biomolecular manipulation that research produces.
If the administration now seeks a coherent shopping-list of objectives, it has better find a way of accounting for these hidden benefits.
Otherwise, not merely investigator-led research, but US industry as well, will be the losers.
British ambitions are mercifully less concrete.
Mr Michael Hesletine, the Secretary of State for Trade and Industry, seems to have determined on informed exhortation rather than explicit research support.
On the face of things, he has luck on his side.
There is some evidence that the long recession is ending early in Britain, while the Confederation of British Industry says that companies have begun spending more on innovation even when funds have been squeezed.
But, with Britain now ranked eighteenth among members of the Organization for Economic Cooperation and Development (OECD), he will need all the luck that he can get.
Too costly software?
Software piracy is wrong, but more flexible policies for selling it would help combat the crime.
STEALING software by illicit copying is wrong, of course, and in many places is a crime.
So it should be.
Like piracy of the copyright in any kind of work, theft deprives the originators of the rewards to which they are entitled.
But the software houses (as, a little pretentiously, they like to be called) are not quite the innocents they pretend.
In particular, they differ from those who sell other kinds of copyrighted products, book publishers for example , in their inflexibility over price.
Take, for example, the purchase of a wordprocessing program for a personal computer.
In the past decade, the market leaders in the field have been made enormously more sophisticated.
Some are almost indistinguishable from desktop publishing systems.
So what happens if a purchaser claims that he wishes to acquire only the basic wordprocessing part of the sophisticated package, perhaps because he or she wishes the output to be compatible with a machine on which all the bells and whistles have been legitimately installed?
Usually there is no legitimate solution.
Buy the whole package, or do without is what the salesman says.
Book publishers, many of whose products are topical, have a solution to this problem: first they sell a hardback edition and then, when the cream of the market has been satisfied, they sell a paperback at a fraction of the cost.
The analogy with computer software is not, of course, exact.
A computer program (like a dictionary) is a tool that may be repeatedly in use.
Even so, the idea that the only versions of the tool available for sale should be more elaborate ( and expensive) than many users need is a restraint of trade.
In the long run, it may even hurt the owners of the copyright by making illicit copying seem more acceptable.
If software programs were covered by patent rather than copyright legislation, there would be ways round these difficulties.
The owner of a patent does not have the unfettered right to make an invention available only on onerous terms.
Rather, a patent is an agreement between an inventor and a government that the former shall enjoy the exclusive right of exploitation of an invention for a prescribed period while the community at large shall have access to its benefits.
If the inventor of a novel mousetrap then decided that he would offer it for sale only when installed in a purpose-built house for which extra would be charged, most patents courts would probably be prepared to grant compulsory licences to those willing to sell the mousetrap without the house.
Copyright protection is, by contrast, absolute.
Even so, it is a question whether the interests of the software houses themselves would be better served if they submitted, perhaps voluntarily, to the unpackaging of their expensive packages (against payment of a royalty).
None of this says that software theft should be made legitimate.
NEWS
Germany stumbles on enacting plan to integrate eastern scientists
Berlin .
The fall of the Berlin Wall also destroyed the regime within which most scientists in East Germany had worked all their lives.
All research institutes were closed down, university professors were sacked and a new, smaller research system was created out of the ashes.
Now these scientists are being asked to bear another burden: nearly 2,000 who were promised university positions may now find themselves jobless because the universities do not want them.
Nearly all research in communist East Germany was carried out in the 57 institutes of the Academy of Sciences, with universities being given little chance to conduct serious research.
A new law established in October 1990 after reunification — the Higher Education Renewal Programme — was intended to mould East German research into a western system based on the unity of teaching and research.
But the transition has not been easy.
Academy institutes, which in 1989 employed 24,000 people, were grossly overstaffed.
Many employees were engineers who built scientific equipment that they were unable to buy from the West and who became superfluous when the two German economies were combined.
By the end of 1991, when the institutes were closed, migration, early retirement and mobility allowances reduced this number to 15,000 — still far too many.
Germany's science council, the Wissenschftsrat, was called in to evaluate the work of the institutions and recommended that 6,700 staff be retained in newly created institutes (in fact 7,200 are not employed).
It also suggested the integration by the end of 1992 of an additional 1,900 into eastern Germany's 10 universities and 400-odd institutes of higher education.
But the plan has hit serious problems because the universities are unwilling to offer the scientists contracts.
The reintegration programme, known as WIP (Wissenschaftler-Integrationsprogramm ), was originally given DM440 million (US$250 million) for four years.
Applicants were required to submit a research plan, and successful candidates (now known as WIPers) would be paid for a year of work at one of the new institutes.
They would use this time to seek a university job, helped by a coordinating body, KAI, set up for this purpose.
Once hired, the scientists would continue to be paid by the federal government, with regional government picking up an increasing share of the salary each year.
After three years, the WIPers would be taken on as normal university staff.
But problems soon arose.
Universities forced to reduce staff numbers by half after the science council concluded that they were grossly oversubscribed did not appreciate being asked to take in outsiders at a further cost to their own numbers.
At the same time, there was no place for the scientists to work because the universities had done virtually no research.
Debate continues on who should pay the rent for laboratories to be set up if the WIPers are taken on.
The universities are also concerned that in three years they may be obliged to give any available permanent position to a WIPer instead of advertising it for open competition, thereby reducing their ability to attract the best talent.
And they worry that they will be asked to make a contribution if the WIPers use up the DM600 million now allocated to the programme.
The programme also puts a strain on the Länder (regional governments).
Although the exact amount of money allocated to the programme is known, the proportion to be paid by the Länder , which are normally financially responsible for universities, is still being negotiated.
Two weeks ago, the federal and Länder governments agreed to a 75:25 split for 1994 and set individual salaries at 84 per cent of the rate for western Germany.
But the terms of funding for the last two years of the programme remain uncertain.
WIPers themselves have found the process difficult.
Accustomed to having a job for life, they had no experience of writing grant applications and working under the pressure of uncertain funding.
So far, only 1,234 of 1,920 have entered into negotiations with universities for permanent positions and only 85 have been given contracts.
Some blame the WIPers for the problems.
Last month, for example, the ministers of the Länder stated that ‘some WIPers need to make much more effort to integrate into the universities’.
But Hartmut Schulz of KAI says that such criticism is unfair.
There are many reasons why it has been difficult for researchers to form links with universities, he says, with some not even sure that particular departments would survive investigations into the competence and integrity of the professors.
All of these problems slowed negotiations to such an extent that the federal government was forced to double its initial one-year period of support.
But no more extensions are expected, and with most WIPers older than 40, those who do not negotiate a contract during this time fear that they may never work again as scientists.
Despite the problems, there is considerable optimism that most WIPers will, by hook or by crook, have been given contracts by the end of the year.
But the idea of true integration — that is, a normal university post — new seems little more than a pipedream.
For many the WIP programme is likely to be a stressful prelude to permanent unemployment.
Alison Abbott 
In addition to transferring researchers from research institutes to universities, the Higher Education Renewal Programme dictates a regional redistribution.
Those in the WIP programme, must be prepared to distribute themselves more evenly through the eastern Länder , and less than half will be able to stay in Berlin.
Clinton team wonders if FCCSET is broken
Washington .
The Clinton administration is slowly turning its attention to the $14 billion that the US government will spend this year on basic research, conducting a review of six multi-billion-dollar interagency initiatives and convening a panel of senior administrators to draw up programmes for future budgets that correspond to the president's domestic policy goals.
Six research themes in the 1994 budget
Despite the political rhetoric about change, the budget that President Bill Clinton proposed in 8 April for fiscal year 1994, which begins on 1 October 1993, makes only minor alterations in the way in which the government invests in basic research.
Even the economic stimulus package that Congress defeated last week, which contained nearly half a billion dollars for the National Science Foundation (NSF), the National Institutes of Standards and Technology and other federal research agencies (see right), would at best have strengthened existing programmes rather than charting new directions.
One sign that the adminstration is beginning to think seriously about basic research is its decision last week to release only partial information on six interagency initiatives that, taken together, will cost $12.5 billion.
Previously, each initiative — on advanced materials, manufacturing technology, biotechnology, high performance computing, global climate change and science education — was described in a pamphlet of a hundred pages or more that included detailed budgets of what each agency planned to spend.
This year, in contrast, the White House Office of Science and Technology Policy (OSTP) issued a slim volume containing a four-page summary of each programme.
John Gibbons, the president's science adviser and OSTP director, says that the six FCCSET (pronounced ‘fix-it’) initiatives, named after the Federal Coordinating Committee on Science, Engineering and Technology that supervises them are ‘a dynamic list of topics’ that are being reexamined for their contribution to national economic growth.
For example, he points out that the four-year global change programme began as an attempt to ‘understand the scientific processes.
Now we are trying to shift attention towards its potential impact on the economy.’
The search for practical results as well as for good science stems from the administration's desire to use technology to revive the US economy as well as from the fact that the initiatives have grown large enough to attract the attention of Congress.
‘In the past three years we have provided NSF with $600 million for the high-performance computing initiative’, says Kevin Kelly, an aide to Senator Barbara Mikulski (Democrat, Maryland), who is chair of the appropriations subcommittee that controls the budgets of NSF, NASA, the Environmental Protection Agency and several other agencies.
‘What have we bought with that money?
I think the NSF would have a hard time answering that question.’
The largest FCCSET initiative, some $4.3 billion for biotechnology, is also the most troubled.
Although the initiative includes programmes operated by the agriculture, energy and defence departments as well as by NSF and NASA, more than three-quarters of its budget is controlled by the National Institutes of Health (NIH), which is primarily interested in funding health-related basic research.
As a result, efforts to focus attention on environmental issues, for example bioremediation or bioprocessing, have net with limited success.
In addition, the administration has requested a 1994 budget for NIH that is essentially flat, leading one NIH official to point out that ‘if you don't get any more money, then it's superfluous to talk about new initiatives’.
Gibbons's predecessor, D. Allan Bromley, believed that the government could carry out no more than five or six major initiatives at once because of the time and resources required to coordinate activities among as many as a dozen agencies.
Although Gibbons has not said whether he agrees with that analysis, he has talked about ‘a rotation’ in which new programmes replace older initiatives.
One proposal, in the works for the past two years but delayed by the change in administration, would deal with coastal ocean zones.
Officials at the National Oceanic and Atmospheric Administration (NOAA) have already done an inventory of existing research programmes and begun to plan an interdisciplinary, global strategy to address issues ranging from non-point sources of pollution to sustainable development.
But a meeting last December was  cancelled by the outgoing Bush administration, and NOAA officials are waiting to hear if the Clinton administration wants to pursue the matter.
That decision — and many others involving the course of science over the next four years — may rest with a new group of  about 30 senior administrators convened two weeks ago by Gibbons and Bowman Cutter of the National Economic Council.
The so-called deputies' science and technology working group was asked to nominate people within their agencies to serve on panels that will develop policies for 15 topics ranging from specific issues such as medical technology to basic research itself.
The goal of the group, according to Tim Newell of OSTP, is to propose specific ways to carry out the president's technology initiative announced in February, but the group is also expected to make recommendations for programmes to be included in the budget for fiscal year 1995 that will be submitted to Congress next winter.
Part of the reason for the uncertainty surrounding the new administration's plans for science are the large number of unfilled positions at OSTP and the ambiguous relationship between it and the office of Vice President Al Gore, the leading figure within the administration on technology issues.
Gibbons is believed to be planning a realignment of portfolios, elevating the environment and lowering the life sciences in the OSTP hierarchy, but only one of his choices for the four vacant associate directorships has as yet been cleared by the White House and nominated by the president.
Jeffrey Mervis 
Research budgets suffer with defeat of stimulus plan
Washington .
The defeat of the economic stimulus package proposed by US President Bill Clinton means that the National Science Foundation (NSF) is unlikely to receive anything close to its 1994 budget request.
The stimulus plan contained $207 million for NSF in the current year, nearly half of the increase of 15 per cent being sought for fiscal year 1994, which begins on 1 October; without that running start, Congress is extremely unlikely to give NSF such a large increase in the face of pressure to reduce the federal deficit and to fund other domestic programmes.
At the same time, two other research programmes that stood to benefit handsomely from the stimulus plan — the $103 million proposed for the Advanced Technology Program (ATP) within the Department of Commerce and $47 million sought for cooperative research agreements between industry and the national laboratories in the Department of Energy — are expected to remain important priorities for the Clinton administration and to enjoy rising budgets in 1994 despite the temporary setback in Congress.
In the short run, ATP officials say they will cancel plans for another round of applications this summer to their programme, which funds proposals from individual companies and industry consortia, while the Energy Department expects to continue its emphasis on funding research projects within the laboratories that also serve the needs of industry.
J.M. 
Britain backs down over threat to run universities
London .
The British government has backed down under pressure from the House of Lords over a controversial proposal that would have given the Secretary of State for Education formal responsibility for the way in which universities are run (see Nature 362 , 275; 1993).
The government had argued that its proposal to include words expressing the minister's responsibility for further and higher education in a new bill would have had little impact, because the main thrust of the bill in question was changes in the administration of schools.
However in a parliamentary debate last week, members of the House of Lords, reflecting concerns expressed by many university vice-chancellors, claimed that the offending clause could be used to justify direct intervention by the government in university administration — and was, as such, an infringement on academic freedom.
Baroness Blatch, minister of state in the Department for Education, announced that the government had agreed to withdraw its proposal and that she would seek discussions with its critics on a more acceptable formulation of the secretary of state's responsibilities.
David Dickson 
Choice to head NIST defines shift in US technology policy
San Francisco & Washington .
The nomination of Arati Prabhakar as director of the US National Institutes of Standards and Technology (NIST) clearly shows the contrast between the views of President Bill Clinton and former President George Bush on industrial policy.
Three years ago, Craig Fields was forced out as director of the Defense Advanced Research Projects Agency (DARPA) after Prabhakar, then deputy director of DARPA's defence sciences office, made a $4 million investment in Gazelle Microcircuits Inc., a tiny Silicon Valley semiconductor company that makes high-speed gallium arsenide chips.
The agreement angered the Bush administration which was philosophically opposed to the government becoming a venture capitalist for fledgling companies.
Although the government eventually decided to accept royalties from sales of the chip rather than to take an equity share in the company, it was widely seen as the last straw in a conflict between Fields and the administration.
Now Prabhakar, a 34-year-old applied physicist, is poised to become head of NIST, to which the Clinton administration has given a leading role in stimulating the US economy.
Not surprisingly, her former boss is delighted by her promotion.
‘She is a very, very capable manager’, says Fields, chief executive officer of the industry-led Microelectronics Computer Consortium in Austin, Texas.
‘She's innovative and she understands new technology and how to apply it.’
Although Gazelle's technology was impressive, the company was unable to survive on its own.
In 1991, a year after the Pentagon's investment, Gazelle merged with two other chip makers, TriQuint Semiconductor of Beaverton, Oregon, and GigaBit Logic of Newbury Park, California.
TriQuint had been making the chips designed by Gazelle's researchers, and its name was retained by the new company, which last year had a turnover of about $30 million and showed a small profit.
The US government continues to receive royalties from the sale of chips designed under the original agreement with Gazelle, although TriQuint is no longer receiving any support from ARPA (the ‘D’ was dropped last month as part of the Clinton administration's promise to shift spending from the military to the civilian sectors).
Spencer Brown, TriQuint's executive vice president and chief financial officer, says that the federal contribution enabled Gazelle to continue improving its technology and that the company expects increased demand for its products if a national information superhighway becomes a reality (see Nature 362 , 582; 1993).
Fields is confident that Prabhakar, who now directs ARPA's $300-million microelectronics technology office, can run an agency whose budget is expected to quadruple to $1.4 billion in the next four years.
But he warns his former coworker that her every move will be scrutinized.
‘If one tries to help every industry, you spread yourself too thin’, says Fields.
‘But if you favor those with the best chance of succeeding, then you leave yourself open to political attacks from those left out.’
The Clinton administration hopes to prevent that from happening.
Speaking last week at a conference entitled ‘New Directions in Technology Policy’, Commerce secretary Ron Brown said that ‘a raging debate about the proper relationship between industry and the government’ is over and that the government is ‘ready to work hand-in-hand with industry to strengthen the US economy’.
If nothing else, Prabhakar's appointment seems to be proof of that commitment.
Frederic Golden & Jeffrey Mervis 
Arati Prabhakar was born in India and moved to the United States at the age of 3.
Trained as an electrical engineer, she received her PhD in applied physics from California Institute of Technology in Pasadena and moved to Washington in 1984 to become a congressional fellow at the Office of Technology Assessment.
In 1986 she joined the Defence Advanced Research Projects Agency (DARPA), first as programme manager of the electronics sciences division of the Defense Sciences Office and, later as deputy director of the office, where she negotiated the agreement with Gazelle (see left).
Two years ago, she was asked to head a new office in microelectronics technology, with an annual budget of $300 million.
Although Prabhakar has declined to talk to the media until after she is confirmed by the US Senate, three years ago she described her job at DARPA as one of ‘making sure that things happen.
My goal is to get the technology out of the lab and into the hands of others.’
That statement is also an apt description of what she will be asked to do at NIST.
F.G.& J.M. 
US government and utilities revive California solar project
Washington .
The US Department of Energy this week gave the go-ahead for a $39-million project to rebuild the core of one of the world's largest solar power plants with the goal of generating commercial quantities of electricity by 1998.
Going with the flow in California 
The Solar Two project at Barstow, California, is a successor to a 10 MW power plant that ceased operations in 1988.
Combined with the sharp increase in solar energy research proposed by US President Bill Clinton for fiscal year 1994, which begins on 1 October, the new agreement between the department and an industrial consortium is a powerful symbol of revived federal interest in renewable energy research.
The original demonstrator at Barstow used arrays of mirrors to focus the sun's rays on a central ‘power tower’ containing stream that was heated directly to generate electricity.
This approach was abandoned after five years because the tower's output was erratic, changing when clouds passed overhead, for example.
Now the moth-balled site is to be revived with extra mirrors — or heliostats, as solar researchers call them — and an energy cycle that will heat molten nitrate salt to 565° C and store it to generate a steady flow of stream for the existing turbo-generators.
The use of the molten salt cycle has been proved on a smaller scale at the department's Sandia National Laboratory at Albuquerque, New Mexico, which has already spent $39 million.
Significant technologies under development there includes improved stretched-membrane heliostats and pumps and valves to handle the molten salt.
The department will provide $19.5 million over the next three years to build Solar Two, a sum to be matched by the South California Edison Company and ten other industrial partners.
Construction will begin in the summer of 1994 for operations in 1996 intended to prove the technology can be used in commercial 100 MW solar power stations.
A proposed budget increase of $70 million for renewable energy, to a total of $327 million, is spread evenly across photovoltaic research, thermal solar energy work (including Solar Two), biofuels and wind power.
The department also intends to spend $16 million to find commercial applications for existing renewable technologies.
The strongest opposition is likely to come from supporters of nuclear power, which was cut sharply in the Clinton budget.
But advocates of renewable energy point out that the new budget still leaves the field far behind where it stood more than a decade ago, when President Jimmy Carter spent as much as $800 million a year on renewable research.
Colin Macilwain 
Framework budget finds its level
Strasbourg .
The council of research ministers for the European Communities (EC) today (29 April) will discuss a proposed budget for the fourth Framework programme that matches what member countries have said they can afford.
The budget proposed last week by EC research minister Antonio Ruberti for Europe-wide research requests ECU13.1 billion (US$15.8 billion) over the next four years.
It is the maximum acceptable level, at 1993 prices, agreed to by the council of ministers and, if accepted, would represent 4 per cent of the EC's total budget.
Ruberti's predecessor, Filippo Pandolfi, had asked for ECU14.7 billion, which the ministers rejected as too expensive at their last meeting in December.
The current Framework programme has a budget of ECU6.6 billion.
The expansion of the programme reflects the EC's commitment to research and to the mobility of research personnel.
It includes for the first time a modest amount of money for social science research, and emphasizes closer ties to international research laboratories such as CERN, the European Laboratory for Particle Physics in Geneva, and EMBL, the European Molecular Biology Laboratory in Heidelberg, as well as closer cooperation with national research organizations.
In particular, the Framework budget is divided into seven categories; information and communications technologies, 36 per cent ; energy, 23 per cent ; industrial technology, 16.5 per cent ; life sciences, 12 per cent ; environmental research, 9 per cent ; a European transportation policy, 2.5 per cent ; and socio-economic research, 1 per cent .
The European Commission, which is responsible for formulating the proposal in line with the views of the council of ministers, was criticized for the repeated delays in the adoption of the third Framework programme and hopes to stick to the intricate timetable for approval of the new programme, which must be accepted at three levels.
The council of research ministers is expected to reach a decision by the beginning of June.
At the same time, the European Parliament is considering the document and expected to finish its work by the beginning of July.
The goal is a political agreement in July between the council, commission and parliament — but acceptance rests with ratification this summer of the Maastricht Treaty.
Alison Abbott 
NEWS IN BRIEF Washington .
The US National Aeronautics and Space Administration (NASA) has expanded the length and scope of its mission in December to cope with the growing list of faults on the Hubble Space Telescope.
The new schedule calls for five spacewalks rather than the three originally planned for a mission lasting 11 days rather than eight.
Two pairs of astronauts from the mission's seven-member crew will alternately perform the spacewalks, record number that maybe further increased to seven.
NASA says that the extension of the mission would not add substantially to the cost.
The repairs to be carried out include the replacement of gyros, solar arrays and cameras and the installation of a device to correct the optics of the troubled telescope, launched in April 1990.
C.M. 
Paris .
France's Genetic Engineering Commission last week approved as safe the country's first gene therapy trial for cystic fibrosis.
The decision opens the way for the trial to begin this autumn pending approval from the French drug agency and the local ethics committee.
The trial, submitted by Transgène, a French biotechnology company, and supported by the French Cystic Fibrosis Association, will test the feasibility and tolerance of aerosol administration of the CFTR gene.
Gabriel Bellon of Lyon will carry out the trials on several adult volunteers.
The CF-modified adenovirus vector, developed by Transgène and Michel Perricaudet of the Centre National de la Recherche Scientifique viral genetics laboratory at Villejuif, is the same as that used earlier this month by Ronald Crystal of Cornell University in New York on a 23-year-old patient in the first CF gene therapy trial.
D.B. 
Clinton announces package of environmental reforms
Washington .
US President Bill Clinton used last week's celebration of Earth day to announce several initiatives that, despite their lack of detail, represent his strongest actions as yet on environmental protection.
As expected, Clinton said he would sign the international biodiversity treaty endorsed last summer by more than 160 nations in Rio de Janeiro (see Nature 362 , 577; 1993).
And he took the side of Vice President Al Gore against other cabinet members in committing the United States to lowering by 2000 its emission of greenhouse gases to 1990 levels, a provision the Bush administration had kept out of the global climate treaty signed at Rio.
How this will happen — whether, for example, the US programme will require tough new regulations or rely on incentives — is still to be determined; Clinton has asked for a ‘cost-effective’ plan by August.
Clinton also announced measures to clean up the government's own house, including a commitment that federal agencies will buy ozone-friendly products, energy-conserving computers, fuel-efficient vehicles and recycled products.
Clinton ordered agencies with facilities that release toxic pollutants, including military bases and research laboratories, to develop a plan to reduce their output by half by 1999, and to report them to the public as private companies are now required to do.
In his speech at the National Botanical Gardens in Washington, Clinton made only passing reference to efforts to elevate the Environmental Protection Agency (EPA) to a cabinet-level Department of the Environment, saying that he hoped it would happen ‘soon, by the grace of Congress’.
Environmental groups are concerned that the new department will be less capable than the White House Council on Environmental Quality, which Clinton wants to disband, of enforcing the National Environmental Protection Act and mediating disputes between federal agencies over environmental issues.
The Senate may vote this week on a bill to give the proposed department that authority, but the issue is expected to remain controversial when it moves to the House of  Representatives .
So far the EPA has taken a back seat to the Department of the Interior in shaping new environmental policy in the new administration; Interior Secretary Bruce Babbitt has already taken action on endangered species, land management issues and reorganizing its science programme.
In his Earth Day speech, Clinton officially announced the creation of a new national biological survey within the Interior Department (see Nature 361 , 574; 1993).
Clinton turns green on Earth Day.
The survey will also have an annual budget of $180 million, with more than $100 million coming from the US Fish and Wildlife Service, and draw on 1,600 employees seconded from within the department.
Its duties will include an inventory and monitoring of biological resources as well as providing independent scientific advice on ecological science to officials throughout the government.
The scientists reassigned to the survey possess expertise in areas such as population dynamics, physiology, animal  behavior , habitats and biodiversity.
The reorganization is expected to go into effect on 1 October pending congressional approval of the internal transfer of funds.
In the next few months, the Clinton administration expects to conduct a broad review of federally sponsored environmental science and research.
One intent of the review, says Nancy Maynard, an assistant director at the White House Office of Science and Technology Policy, is to eliminate duplication but it is not likely to lead to a wholesale restructuring of agency missions.
A proposal for a National Institute for the Environment to coordinate and fund scientific research on the environment, modelled on the National Institutes of Health, may soon gain the support of the National Academy of Sciences.
Although some believe that the institute is unnecessary, an academy panel is expected next month to endorse the basic concept and to propose a possible structure.
Representative Jim Saxton (Republican, New Jersey) is preparing legislation to create the institute.
Tony Reichhardt 
US-German council formed to strengthen international ties
Munich .
Germany and the United States have formed ‘an intellectual bridge across the Atlantic’ to shore up weakening links between scientists and scholars in the two countries.
The German-American Academic Council was set up this month on the initiative of Germany to support projects and exchanges in science, including social science, and technology.
The council will be sponsored by three major research organizations in Germany — the Deutsche Forschungsgemeinschaft (DFG), the Max Planck Society and the Alexander von Humboldt Foundation — and in the United States by the National Academy of Sciences, the American Council of Learned Societies, the American Academy of Arts and Sciences and the Social Sciences Research Council.
So far, Germany of footing the entire bill.
The German Ministry for Research and Technology has committed DM18.1 million (US$11.5 million) over the next four years.
US sponsors are being sought among the various private research foundations, but according to Reimar Lüst, president of the Humboldt Foundation, ‘there is no hurry, the council has enough money to get started’.
The idea, conceived by Chancellor Helmut Kohl and endorsed last month in talks between the German leader and US president Bill Clinton, is an attempt to revive the strong transatlantic ties that existed immediately after the Second World War but which have steadily declined as that post-war generation ages.
Kohl hopes that the council will become a way to revive these links at a time when Europe is very much looking inwards.
Lüst sees the council as ‘an intellectual bridge across the Atlantic…that can be an important vehicle for joint policy studies and interdisciplinary cooperation’ in such areas as industrial policy, technical education and the impact of immigration on the national economy.
A US-German committee that issued a preparatory report on the need for such a council sees it as a forerunner for an institution to promote cooperation between the United States and all of Europe.
The council will be composed of 30 members, 15 from each country, whose election should be complete within the next few weeks.
A main office will be set up in Bonn with a branch office in Washington.
The first council meeting is planned for early summer, when the first project grants, for up to 18 months, are expected to be made.
Applications for younger scientists will be given priority, as will projects in the humanities.
Alison Abbott 
Canadian study fuels move to limit rise in drug prices
Quebec .
The Canadian government body charged with holding down the cost of patented drugs is proposing amendments to its pricing guidelines at the same time a study shows that Canadian prices are often higher than the international median.
The Canadian parliament recently extended patent protection after pharmaceutical companies promised to control prices and to increase their spending on research into new drugs.
A new study by the Patented Medicine Prices Review Board (PMPRB) found that 56 per cent of the introductory prices of 124 new drugs in Canada were greater than the corresponding median international prices.
And the most popular drugs were more expensive in Canada than in any other country listed in the Patented Medicines Regulations except the United States and Germany.
The Canadian analysis confirmed the findings of a recent US General Accounting Office study that found prices for the same drug to be higher in the United States than in Canada.
The study found that the less useful the new product, the more the Canadian drug companies charged for it.
prices were higher in 21 per cent of cases involving drugs that provide moderate, little or no improvement over comparable medicines, but in only 9 per cent of cases involving therapeutic breakthroughs or drugs providing substantial improvement.
Among products providing new strengths or a comparable dosage of an existing medicine, only 5 per cent had higher Canadian prices.
The board found that 105 of 177 drug products sold in Canada were priced above the median international price.
In 42 cases the Canadian price was the highest in the world.
Patent drug prices have been under intense scrutiny in Canada since 1987 when parliament approved an extension to the patent act that critics warned would lead to higher drug prices.
Brand-name pharmaceutical companies denied that this would happen and promised greater investments in research in return for longer patent protection.
The price review board was set up as a result of the new patent legislation.
It has found that between 1987 and 1991, the price of existing patented drugs increased by 2.9 per cent compared with an increase of 4.7 per cent permitted by the guidelines, and that the annual increase has consistently remained below the rate of inflation.
Last month, the government passed legislation extending market exclusively for most patent drugs for another three years and modifying Canada's system of ‘compulsory licensing’, which makes it relatively easy for generic drug companies to copy brand-name products (see Nature 359 , 351; 1992).
In lobbying for the law, the brand-name manufactures announced that their research spending exceeded $400 million a year.
The revelation that Canadian prices for new drugs are relatively high internationally has become a political issue.
‘Something is wrong here’, says New Democratic Party leader Audrey McLaughlin.
But PMAC's financial officer, Robert Livingstone, says that drug prices depend on many factors, including the type of national health system through which they are marketed, and that meaningful comparisons are difficult.
‘We have not yet been able to review the data’, he says.
‘We are committed to fair and reasonable prices, but what is fair and reasonable?’
Last autumn, the price board proposed amendments that, among other things, would limit the price of a new drug that provides moderate or no improvement over existing drugs to the median international price or the price of all drugs in that category.
Hearings on the proposals were held in February, and the board is expected to make a final decision in June.
David Spurgeon 
15 drug companies to share data on AIDS therapies
Washington .
In a break with tradition, 15 pharmaceutical companies from the United States and Europe last week announced an agreement to work together on combination drug therapies to fight the human immuno-deficiency virus (HIV).
The participating companies will share information and supplies of drugs early in the drug development process to carry out independent testing of promising combination of new drugs.
The collaboration, which includes an effort to standardize preclinical testing procedures, does not extend to the search for an AIDS vaccine nor to basic research on the disease.
A recent survey by the US Pharmaceutical Manufacturers Association reported that more than 90 medicines were being developed to combat AIDS and AIDS-related conditions.
But progress has been hindered by problems with toxicity and with the ability of HIV to become resistant to individual drugs.
Earlier this month, preliminary results from an Anglo-French trial cast doubt on the ability of AZT (azidothymidine) to delay the onset of AIDS in patients with HIV (see Nature 362 , 483; 1993).
A year in the making, the agreement was conceived by P. Roy Vagelos, chairman and chief executive officer of Merck, and Edward Scolnick, president of Merck Research Laboratories, with strong support from Juergen Drews of Hoffmann-La Roche.
Participants in the Inter-Company Collaboration for AIDS Drug Development will share relevant information and supplies when a drug reaches phase II clinical testing, the point at which the drug's safety profile is known and preliminary tests are under way on efficacy.
Participants must be actively involved in AIDS antiviral research and have an investigational new drug in the early stages of drug development.
By dealing with work done before a product is ready for market, the collaboration hopes to avoid violating US antitrust laws and other problems associated with intellectual property rights and the sharing of royalties.
Although most of the important companies have signed up, two notable absentees are Abbott Laboratories and Upjohn.
A spokesperson for Upjohn says that the company prefers to maintain an independent anti-AIDS programme, which now receives more than a fifth of the corporate budget for drug discovery research.
The participating companies, nine of which are from the United States, include AB Astra, Boehringer Ingelheim, Bristol-Myers Squibb, Burroughs Welcome, Du Pont-Merck, Eli Lilly, Glaxo, Hoechst AG, Hoffman-La Roche, Merck, Miles, Pfizer, Sigma Tau, SmithKline Beecham and Syntex.
Diane Gershon 
Zeneca increases research spending
London .
The various business that will make up Zeneca, the new company being created out of the biosciences activities of Britain's Imperial Chemical Industries (ICI), spent £457 million (US $640) on research and development last year, according to figures released last week in preparation for the company's stock market launch in June.
This figure represents 11 per cent of the company's total sales and is an increase of 9.8 per cent over R&D spending in 1991.
In contrast, growth in R&D spending between 1990 and 1991 was only 5 per cent, largely as a result of a rationalization of research on agrochemicals.
Peter Doyle, formerly research and technology director of ICI who is to become an executive director of Zeneca, estimates that 1993 spending is likely to rise to ‘just under £500 million’.
D.D.
Japanese computer makers enlist scientists overseas
Tokyo .
Japanese computer makers are turning to scientists overseas to help them develop software and applications for massively parallel supercomputers.
Next month, NEC will ship a Cenju-2 parallel computer to the new Swiss Scientific Computing Center (CSCS) in Manno, while Fujitsu already has links with Australian scientists and others around the world.
The US computer industry has long used academics to develop software for new high-performance computers.
But such links are rare in Japan, and the interaction with foreign scientists is also a recent phenomenon.
In 1991, Fujitsu loaned at no cost a prototype massively parallel computer, the AP1000, to the Centre for Information Science Research at the Australian National University (ANU).
The centre, Australia's leading supercomputer facility, in 1989 began a five-year, $10-million agreement with Fujitsu for joint development of supercomputer software.
The company also has three AP1000 computers at its laboratory in Kawasaki that serve users in Switzerland, Britain and the United States.
Fujitsu says that the number of foreign users has tripled in recent months and that 21 groups outside Japan are linked to the computers.
There are also about 30 visiting foreign scientists at Kawasaki using the machines.
Outside scientists are developing software for generic problems in molecular dynamics, quantum physics as well as parallel software algorithms, computer graphics and circuit design and device simulation of more immediate use to Fujitsu.
Julian Clarke of the University of Manchester's Institute of Science and Technology in Britain uses on-line connections via Internet ‘several times a day’ to develop molecular level simulations of polymers and other amorphous materials.
‘We are able to perform scientific calculations that otherwise would not be feasible’, he says.
In return, Clarke provides Fujitsu with the computational chemistry software.
The new NEC High Performance Computing Software Development Center in Switzerland, which opened last month in the same building as CSCS, will by the end of this year give a limited number of outside users access to its Cenju-2 parallel supercomputer, says A. Scheidegger, director of CSCS.
The centre was established last autumn with SFr40 million (US $30 million) from the Swiss government and has an NEC SX-3 vector-processing supercomputer linked to the high-capacity Swiss education and research network (SWITCH) and to those carrying out joint research with Swiss government organizations.
Japanese companies have chosen Australia and Switzerland because they lack a domestic industry that might protest against the import of ‘free’ high-performance computers.
Furthermore, both countries have a good track record in software and have extensive computer networks.
The linkage with academics around the world is not confined to Japanese companies.
Cray Research Inc. in Minnesota, which has traditionally asked university scientists in the United States to help it to develop software, hopes to work with European scientists to develop specific software.
David Swinbanks 
UK companies report increased R&D spending in 1992
London .
Spending by British companies on research and development (R&D), which dropped sharply between 1989 and 1991 as a result of the economic recession, appears to be rebounding even before any overall recovery, according to survey released last week by the Confederation of British Industry (CBI).
The growth has been particularly strong in traditional manufacturing regions, such as the northwest of England and the east Midlands, as well as the high-technology areas of Scotland and East Anglia.
It has been lowest in the southeast and southwest of the country, where wealth is based more heavily on service industries.
‘The 1992 figures are the highest since the survey began in 1989, and are in marked contrast to 1991's figures, which were the lowest’, says Bryan Smithm of ICI engineering, and chairman of the CBI's research and innovation committee.
The CBI's conclusions are based on replies from 268 manufacturing companies and 151 non-manufacturing companies to a questionnaire completed at the end of last year.
These showed that half of the manufacturing companies increased their R&D spending in 1992 — and that almost as many intended to continue this trend in 1993.
In particular, CBI compared the number of companies increasing (or planning to increase) their R&D spending with those reducing it.
Earlier studies by the confederation's NatWest technology unit showed that, between 1989 and 1991, the excess of those increasing R&D spending on the new products fell from 40 to 16 per cent.
The downward trend was confirmed by figures released in January by the Central Statistical Office showing a 4 per cent drop in business spending on R&D between 1990 and 1991.
In sharp contrast, the returns for 1992 showed an excess of 42 per cent of companies were spending more rather than less on R&D — 9 per cent higher than had been anticipated on the basis of previous declaration by companies for their planned expenditure for the year.
(Figures for R&D on new processes showed a smaller increase, from a balance of 15 to 18 per cent; among non-manufacturing companies, the balance of companies spending more on product R&D increased from 10 to 23 per cent .
The fastest rate of growth in R&D spending was reported by manufacturing companies with between 500 and 1,000 employees, 67 per cent of which reported an increase last year over 1991.
In terms of regional differences, East Anglia reported the highest proportion of companies that had raised R&D spending, and the southwest of England the lowest.
David Dickson 
Simplify science?
I'll drink to that
London .
Can you describe what the Higgs boson is — and why you would like to find it — on single sheet of paper?
If so, William Waldegrave, Britain's cabinet minister for science, wants to hear from you.
Waldegrave threw out the challenge to the physics community last week at the annual conference of the Institute of Physics in Brighton.
Scientists, he said, must explain to the public what they are doing if they want to receive continued support.
More immediately, the winner will receive a bottle of vintage champagne.
Waldegrave said that the need to promote the public understanding of science would be addressed in the forthcoming White Paper (policy document) on science and technology and that he planned to launch a campaign to ‘evangelize’ science.
Hence his proposal for a competition to describe the significance of the Higgs boson — a target of the next generation of accelerators such as the proposed Large Hadron Collider at CERN in Geneva and Superconducting Super Collider under construction in Texas.
‘If you can make me understand that, I stand a better chance of helping to get you the money to find it,’ Waldegrave said.
Entries should be sent to him by 1 June at the Office of Public Service and Science, Cabinet Office,.
D.D.
CORRESPONDENCE
Degrees of national wealth
SIR —‘The South’ is now widely used as the collective term for non-industrialized nations, with the implication that there is either a consistent trend of a major discontinuity in the relationship between latitude and socio-economic development.
Within the Northern Hemisphere both these effects occur, with a marked decline in per capita gross national produce (GNP) between high and low latitudes.
National data for the United Nations Development Programme Human Development Index (HDI; based on life expectancy and adult literacy, as well as income) behave similarly.
However, a fully global analysis of these development indices shows that the geographic distribution of high living standards is bimodal, with a closer association to temperature than latitude (see figure).
Latitudinal distribution of 1991 (GNP) per capita(US$ per annum ), 1990 Human Development Index (HDI) and mean annual land surface temperature.
Land areas north of 65°N and south of 45°S are excluded from GNP and HDI analyses (that is, assumed to be uninhabited); temperature date also excludes land over 1,500 metres.
Similar patterns in the global distribution of per capita GNP have been found in previous analyses, of 1972 and 1988 data.
The latter study was based on GNP data for 141 countries and territories, ascribed to latitude bands according to the location of capital cities.
Because all countries were given equal weighting, regardless of population size, the importance of small, high-income nations (for example the oil-producing Gulf states) was accentuated.
In the current analysis, mean per capita GNP was determined from totals of GNP and population for 10° latitudinal intervals, assuming even distribution of 1991 national data (compiled from OECD, World Bank and national sources) across the latitudinal range individually occupied by 200 countries and territories.
Mean HDI values were determined similarly, from 1990 data for 150 countries and territories.
The HDI data excluded several non-UN countries, and gave a single value for the ex-Soviet Union (for GNP, separate data were used for the latter).
Not surprisingly, the global distribution of wealth is strongly influenced by the mid-to high-latitude location of the United States and Canada, Japan, the nations of Western Europe, and, in the Southern Hemisphere, Australia and New Zealand.
Nevertheless, the same pattern occurs when these countries are excluded; for example, when the analysis is limited to the poorest quartile of countries in each latitudinal interval.
For the 11 latitude bands considered, per capital GNP is more closely correlated with mean annual temperature  than with latitude per se .
Regression analysis for the former relationship indicates a decrease of around $400 per capita GNP for a 1°C increase in temperature.
However, such statistics are descriptive, nor predictive, and must be treated with caution.
Two provisos are particularly important.
First, that non-climatic influences (including the distribution of natural resources) are likely to dominate at the regional level, as evidenced by the 10–200 fold variation in national per capital GNP values within 10° latitude bands.
Second, the occurrence of a strong global correlation between GNP and temperature does not necessarily mean that a cooler, more seasonal climate inherently favours socio-economic development; it is arguable that such a relationship could be a fortuitous consequence of the high-latitude origins of modern industrialization, with that situation subsequently maintained by political and financial structures that have generally been disadvantageous to tropical nations.
An additional consideration is that the latitudinal pattern of agricultural productivity is closely similar to that for per capital GNP.
(R. W. Kates & Jen-hu Chang, personal communications), albeit crop yields in temperate regions are enhanced by ‘rich country’ inputs of fertilizer, pesticides, mechanization and advanced irrigation technology.
Resolution of such issues requires much more detailed demographic and environmental data, to elucidate the dynamic processes whereby climate interacts with national wealth and human development.
In this context, the current North-South terminology may be politically convenient; however, such a radical re-definition of global geography has dubious scientific merit.
Phillip Williamson Richard Moss 
Population control
SIR — In my experience and that of others, many women in societies with very high birth rates would like to start reproducing later, stop earlier and space their births at wider intervals.
If contraceptive services were available locally, cheaply, safely and privately, then many women would use them, even in cultures where most men are indifferent or opposed to contraception and religious teaching is generally against it.
It cannot be useful to talk about population growth (Nature 362 , 379; 1993) as a ‘threat to the global commons’ that may need to be ‘dealt with by coercion’before it becomes bothersome to ‘the rest of us’, which I assume means those of us lucky enough to live in countries where contraceptives are readily available.
This analysis is particularly misleading because ‘the rest of us’ use up more global commons, both per capita and in total, than any of the poverty-stricken countries where birth rates are high, in some cases by orders of magnitude.
If we want to save the fish in the sea, the trees in the rain forests and the ozone layer, the wealthy countries have to look to their own consumption habits.
Rapid population growth is a problem, because it increases poverty and ill-health in societies where it occurs.
We will have more success at slowing world population growth if we see it as problem of enabling women in poor countries to have control over their own reproduction, which some carefully targeted development aid could go a long way towards achieving.
Ruth Mace Department of Anthropology, University College London  
COMMENTARY
Forty years of molecular information Nicholas Short 
At UNESCO's celebration of the double helix's fortieth anniversary in Paris last week, nostalgia was overlaid by vigorous optimism about the future of molecular biology.
‘WHAT’, asked François Jacob (Institut Pasteur, Paris), opening the meeting in Paris on ‘40 years of molecular genetics’ at UNESCO's headquarters on 21–23 April, ‘will future historians remember of this century?’
Various wars, the rise and fall of fascism and communism, and the end of colonialism, perhaps, but surely also the fantastic development of science and industry.
In the first half of the century the focus was on physics, but in the second, the emphasis had shifted to biology, triggered in large part by the discovery that Watson and Crick published forty years ago.
The focus of the meeting, one of several being held this year (see, for example,(see Nature 362 , 105; 1993) was firmly on the intellectual background from which the discovery sprang.
First, Max Perutz (MRC, Cambridge) described the pioneering work of Oswald Avery, proving that genes are made from DNA.
Then Francis Crick (Salk) recalled how Avery's work, buttressed by that of Hodgekiss and of Hershey and Chase, had led him to muse on how to copy a three-dimensional structure; as with a sculpture, perhaps, by making a mould?
Or, more plausibly, by copying some linear array which had an intrinsic capacity to fold itself up?
Clearly, the structure of the molecule in which the cell stores information was crucial.
X-ray fibre diffraction data had been obtained for both the A and B forms of DNA by Rosalind Franklin and Maurice Wilkins at King's College, London, but they lacked the phase data necessary to solve the structure.
So, unlike Franklin, Crick and James Watson (Cold Spring Harbor) elected to build models and calculate the diffraction pattern each would produce.
It was only after they had worked out the correct tautomeric form of each base and found that adenine is exactly the right shape to pair with thymine (and guanine with cytosine) that they thought of Erwin Chargaff's discovery that DNA always contained equimolar amounts of bases in each of these pairs.
From then, the path to the double helix was clear.
Leslie Orgel (Salk) recalled that only two weeks after Watson and Crick's first paper appeared, the entire biochemistry department at Oxford travelled to Cambridge to view the structure and followed this up with a telegram: CONGRATULATIONS!(signed) GENE.
That the two paired strands formed a double helix was in Crick's original view a nuisance; they had to be unwound.
The key point was that the bases paired, making it possible for the first time to seem how a chemical entity might be able to store and copy information.
That was the theme that informed the entire meeting.
Of course, it was not always like that.
Before the first two helices had ever intertwined, biological information was very probably being copied and stored as RNA.
Indeed, that molecule is more versatile than DNA and, as Tom Cech (HHMI, Boulder, Colorado) demonstrated, can form enzymes of an efficiency and specificity as impressive as those of proteins.
But whether the storehouse of information is DNA or RNA, the crucial question is the error rate on copying.
For any biological text, explained Manfred Eigen (MPI, Göttingen), there is a single error threshold; above this, the error rate is too high, and information is lost on each round of replication, whereas below it, the error rate is too low, and not new variants can ever emerge.
By mapping the changes during the evolution of viruses, he has shown that the threshold varies in different codon positions and in different genes.
On a similar theme, the conference organizer, Giorgio Bernardi (Institut Jacques Monod, Paris), brooded on the significance of the many gene-rich segments of the genomes of mammals and birds termed isochores, whose G + C content has been highly conserved.
The exact fraction of G + C in these regions may have taxonomic importance, as the values in birds, mammals and cold-blooded vertebrates are distinct.
Information flow of a different kind was explored by Walter Gilbert (Harvard), asking whether genes or their introns came first.
Now that the full complement of introns in the ancestral triose phosphate isomerase gene has been mapped, he has been able to show that they are placed so that amino acids near each other in the protein's structure are encoded wherever possible by the same exon, bolstering the idea that the original form of the gene contained a full complement of introns and was produced by exon shuffling.
But the most spectacular example of biological information on the move stemmed from Takashi Gojobori (National Institute of Genetics, Mishima) 's project to map the MHC haplotypes of all the races of humanity.
Not only are many of the possible haplotypes confined to one race or area, but there is even a haplotype that is confined to the former ‘Silk Road’ between China and Europe.
If DNA is the repository of biological information, access to that information is controlled and directed by proteins.
In a sweeping review of the various domains by which proteins bind to DNA, Aaron Klug (MRC, Cambridge) revealed the diversity of structures that have evolved to meet the requirements of physical stability and evolutionary flexibility imposed by the need to recognize only a tiny fraction of the binding sites available in an entire genome.
Although most DNA is in the B form, whose structure was originally determined by Watson and Crick, there is no reason why the cell should not make use of other forms as well, and indeed Alex Rich (MIT) described a protein binding with high affinity to the left-handed helix of Z-DNA that he first described.
Most of this form of DNA is confined to actively transcribed genes, where Rich speculates that it may function to keep an appropriate distance between successive molecules of RNA polymerase.
Even without this complication, however, the need to allow the transcriptional machinery access to the DNA without permanently disrupting its protein packaging is a demanding one.
For many years, Andrei Mirzabekov (Engelhardt Institute, Moscow) has been probing the way in which vertebrate cells perform this feat using an increasingly sophisticated series of techniques for mapping protein-DNA contacts in vivo .
He finds that although the globular bodies of the histone molecules no longer contact the DNA of heavily transcribed genes, their charged tails still do, leading him to suggest that they remain tethered by their tails while the polymerase passes so that they can rapidly ‘snap back’ into their former structure.
Approaching this problem from the opposite direction, Gary Felsenfeld (NIH) described a series of experiments mapping the redistribution of nucleosomes caused by the passage of T7 RNA polymerase in vitro .
Under these circumstances also, the nucleosome is never forced to leave the DNA completely, but is instead shifted perhaps as much as 100 bases backwards from its original position.
How this movement is affected by the histone modifications and additional proteins present in vivo will doubtless provide fertile ground for future investigations.
One of the major insights offered by Watson and Crick's discovery was the basis  of the mechanism by which DNA was replicated, but how does the eukaryotic cell, with its thousands of replication sites, ensure that all the information is copied once and once only.
For a long time, workers on this topic have debated whether defined origins of replication are involved, as they are in bacteria, but only now count Arturo Falaschi (ICGB, Trieste) reported that in one area at least of the wild-type human genome replication can be shown to begin in a single defined phase.
Phenomena such as exon shuffling imply that genomes are constantly being rearranged, and are not mere static repositories of information.
The central contribution of extrachromosomal elements such as plasmids and episomes to this process was elegantly summarized by Stanley Cohen (Stanford).
Even in mammalian evolution, transposons and retroviruses have played an important role; Maxine Singer (Carnegie Institution) presented evidence that one of the 4,000 LINE-1 retrotransposons in the human genome still encodes an active reverse transcriptase.
On other occasions, the genome seems to rearrange itself for its own ends.
The most prominent example occurs in the immunoglobulin genes, where one of the many available gene segments encoding a variable region must be brought to a position next to the remainder of the gene before it can be expressed.
At the  locus this can entail either deletion or inversion of a DNA segment, depending on which of the 24 functional variable genes are used, explained Hans Zachau (Institut für Physiologische Chemie, Munich).
In all cases, however, the broken ends of the DNA on either side of the initial cut are apparently sealed so as to form hairpins, as Martin Gellert (NIH) showed, before they are nicked to form the final joint (a process reminiscent of the reaction mechanism employed by topisomerases).
How is the information stored in the DNA used to direct the development of a complete organism?
Intensive NMR studies in collaboration with Kurt Wüthrich have led Walter Gehring (Biozentrum, Basel) to conclude that the DNA binding specificity of the homeobox genes he discovered (which are transcription factors specifying segmental identity in metazoans from insects to humans) is determined not only by the ‘recognition helix’ that sits in the major groove of the DNA, but also by a flexible segment at the amino-terminal end of the homeobox which appears to wrap around the DNA so as to contact the minor groove.
Plants, too, can have homeotic mutations, noted Heinz Saedler (MPI, Cologne), although in the case of mutations affecting flower formation in Antirrhinum majus , at least, the proteins in question are related to the mammalian serum response factors, rather than containing homeoboxes.
In this case, however, mutations affecting the expression of any one factor invariably affect two adjacent parts of the flower.
Transformations of another sort are produced by mutations in the retinoic acid receptors.
Pierre Chambon (Institut de Chimie Biologique, Strasbourg) described an ambitious set of experiments aimed at elucidating their role by knocking out each of the eight isoforms in separate lines of transgenic mice.
This his initial astonishment, lines lacking the  and  receptor all appeared quite normal, and it was only when he had made lines lacking two isoforms at once that clear effects were seen (thank God, he added).
The apparent ability of these receptors to substitute for one another is strange in view of their strong evolutionary conservation; perhaps, mused Chambon, their absence results in a phenotype that cannot easily be measured in the laboratory.
Although the discovery of the double helix made it possible to understand how information might be stored in DNA, revolutionary advances in areas such as nucleotide sequencing and computing have been needed before people can contemplate recovering the stored information on a large scale.
One of the most exciting parts of the symposium was the account by Daniel Cohen (Centre d'Etudes du Polymorphisme Humain, Paris) and Jean Weissenbach (Institute Pasteur) of their recent progress towards a physical and genetic map of the entire human genome.
(‘French science in its grandest fashion’, remarked Watson later, adding that the audience should not believe what some ‘worse than third-rate authors’ had written elsewhere.)
Already, Cohen has isolated more than a thousand contigs, each containing an average of 15 yeast artificial chromosomes average of 15 yeast artificial chromosomes (YACs) and covering more than 70 per cent of the genome, while the number of markers on the map that Weissenbach and his colleagues reported last year has since been more than doubled and is set shortly to double again.
Although strategies to overcome problems with chimaeric YACs and marker-poor regions are still being worked out, it is clearly only a matter of time before a combined version of the two maps will be available to guide efforts to sequence the human genome.
Access to this flood of information about ourselves will eventually transform medical practice, and the first signs of this are already apparent.
Jean-Louis Mandel (Institute de Chimie Biologique, Strasbourg) described now diseases such as fragile X syndrome and myotonic dystrophy (recently joined by spinobulbar muscular atrophy and Huntington's disease) are caused by a progressive expansion in successive generations of a triplet repeat in the gene.
But although this discovery has made accurate diagnosis much easier, the way in which the expansion occurs remains to be understood.
Elsewhere, workers are trying to alter the body's response to disease by adding information to its cells.
Michael Blaese (NIH) described studies in which T cells transformed with a retrovirus encoding adenosine deaminase (ADA) were used to correct the crippling immunodeficiency produced by a lack of this enzyme.
Blaese and his colleagues have also shown that a mouse glioma can be completely ablated by treatment with acyclovir following a stereotactic implant of fibroblasts infected with a retrovirus copying the herpes virus gene for thymidine kinase.
Similarly Max Birnstiel (IMP, Vienna) described studies in which a newly developed and very efficient technique for gene transfer was used to induce tumour cells to synthesize high levels of interleukin 2, so as to make them more conspicuous to the immune system.
Mice immunized with two doses of such cells were protected against a subsequent lethal challenge with the tumour, and experiments in which the immunization is used to induce a response against an established tumour are under way.
But it was left to Watson to sum it all up.
We are now, he said, in the sixth phase of genetics, the phase of the human genome sequence, which had succeeded the ages of mendelian genetics, DNA, the genetic code, the control of genes, and recombinant DNA, respectively.
Although this ‘slightly megalomaniac project’ had been started on the strength of charitable donations, future progress should be funded by governments, if only because the project would otherwise be open to exploitation by private companies and their insatiable desire for secrecy.
The sixth phase would, he predicted, give way to a seventh age of genetic prediction.
This would make possible the routine diagnosis of vast numbers of genetic conditions, which should be eliminated where necessary by abortion; the world must shed the idea that this is evil, as it is an act of true moral cowardice to allow children to be born with known genetic defects.
Although some of these defects might eventually be treatable by gene therapy, this would never be possible for all of them, and prevention is in any case better than cure.
What of germline gene therapy?
The consensus is that the world is not yet ready for it, and it may in any case never be necessary.
Of course, it is possible to imagine evil-minded dictators abusing it to produce nightmares, but if it could be used to enhance our resistance to disease, or the survival of humanity depended on it, then it would be time to think again.
No principle is worth extinction.
Watson ended by firmly endorsing the power of genetics to improve the human condition; a lot of people will dislike it, he added, but when he was young a lot of people had disliked Roosevelt — so what?
Nicholas Short is Biology Editor of Nature .
GUIDE TO AUTHORS
PLEASE follow these guidelines so that your manuscript may be handled expeditiously.
Nature is an international journal covering all the sciences.
Contributors should therefore bear in mind those readers who work in other fields and those for whom English is a second language, and write clearly and simply, avoiding unnecessary technical terminology.
Space in the journal is limited, making competition for publication severe.
Brevity is highly valued.
One printed page of Nature , without interruptions, contains about 1,300 words.
Manuscripts are selected for publication according to editorial assessment of their suitability and reports from independent referees.
They can be sent to London or Washington and should be addressed to the Editor.
Manuscripts may be dealt with in wither office, depending on the subject matter, and will where necessary be sent between offices by overnight courier.
High priority cannot be given to presubmission enquiries; in urgent cases they can be made in the form of a one-page fax.
All manuscripts are acknowledged on receipt but fewer than half are sent for review.
Those that are not reviewed are returned as rapidly as possible so that they may be submitted elsewhere without delay.
Contributors may suggest reviewers; limited requests for the exclusion of specific reviewers are usually heeded.
Manuscripts are usually sent to two or three reviewers, chosen for their expertise rather than their geographical location.
Manuscripts accepted for publication are typeset from the London office.
Nature requests authors to deposit sequence and crystallographic data in the databases that exist for this purpose, and to mention availability of these data.
Once a manuscript is accepted for publication, contributors will receive proofs in about 4 weeks.
Nature 's staff will edit manuscripts with a view to brevity and clarity, so contributors should check proofs carefully.
Manuscripts are generally published 2–3 weeks after receipt of corrected proofs.
Nature does not exact page charges.
Contributors receive a reprint order form with their proofs; reprint orders are processed after the manuscript is published and payment received.
Categories of paper
Review articles survey recent developments in a field.
Most are commissioned but suggestions are welcome in the form of a one-page synopsis addressed to the Reviews Coordinator.
Length is negotiable in advance.
Articles are research reports whose conclusions are of general interest and which are sufficiently rounded to be a substantial advance in understanding, They should not have more than 3,000 words ow text (not including figure legends) or more than six display items and should not occupy more than five pages of Nature .
Articles start with a heading of 50–80 words written to advertise their content in general terms, to which editors will pay particular attention.
The heading does not usually contain numbers, abbreviations or measurements.
The introduction to the study is contained in the first two or three paragraphs of the article, which also briefly summarize its results and implications.
Articles have fewer than 50 references and may contain a few short subheadings.
Letters are short reports of outstanding novel findings whose implications are general and important enough to be of interest to those outside the field.
Letters should have 1,000 or fewer words of text and four or fewer display items.
The first paragraph describes, in not more than 150 words and without the use of abbreviations, the background, rationale and chief conclusions of the study for the particular benefit of non-specialist readers.
Letters do not have subheadings and should contain fewer than 30 references.
Commentary articles deal with issues in, or arising from, research that are also of interest to readers outside research.
Some are  commissioned but suggestions can be made to the Commentary Editor in the form of one-page synopsis.
News and Views articles inform non-specialist readers about new scientific advances, sometimes in the form of a conference report.
Most are commissioned but proposals can be made in advance to the News and Views Editor.
Scientific Correspondence is for discussion of topical scientific matters, including those published in Nature , and for miscellaneous contributions.
Priority is given to letters of fewer than 500 words.
Preparation of manuscripts
All manuscripts should be typed, double-spaced, on one side of the paper only.
An original and four copies are required, each accompanied by artwork.
If photographs are included, five sets of originals are required; for line drawings, one set of originals and four good-quality photocopies are acceptable.
Reference lists, figure legends and tables should all be on separate sheets, all of which should be double-spaced and numbered.
Three copies of relevant manuscripts in press or submitted for publication elsewhere should be included with submitted manuscripts, and clearly marked as such.
Five copies of revised and resubmitted manuscripts, labelled with their manuscript numbers are required, together with five copies of a letter detailing the changes made.
Titles are brief and simple.
Active verbs, numerical values, abbreviations and punctuation are to be avoided.
Titles should contain one or two key words for indexing purposes.
Artwork should be marked individually and clearly with the author's name and, when known, the manuscript number.
Ideally, no figure should be larger than 28 by 22 cm.
Figures with several parts are to be avoided and are permitted only if the parts are closely related, either experimentally or logically.
Unlettered originals of photographs should be provided.
Suggestions for cover illustrations, with captions and labelled with the manuscript number, are welcome.
Original artwork is returned when a manuscript cannot be published.
Protein/nucleotide sequences should ideally be in the three-letter and not the single-letter code for amino acids.
One column width of Nature can accommodate 20 amino acids or 60 base pairs.
Colour artwork .
A charge of £500 per page is made as a contribution towards the cost of reproducing colour figures.
Inability to pay these costs will not prevent publication of essential colour figures if the circumstances are explained.
Proofs of colour artwork may be sent to contributors under separate cover from their galley proofs.
Figure legends should not exceed 300 words and ideally should be shorter.
The figure is described first, then, briefly, the method.
Reference to a method published elsewhere is preferable to a full description.
Methods are described in the text.
References are numbered sequentially as they appear in the text, followed by those in tables and finally by those in figure legends.
Only papers published or in the press are numbered and included in the reference list.
All other forms of reference should be cited in the text as a personal communication, manuscript submitted or in preparation.
Text is not included in reference lists.
References are abbreviated according to the World List of Scientific Periodicals (Butterworths, London, 1963–65).
The first and last page numbers are included; reference to books should include publisher, place and date.
Abbreviations , symbols, units and greek letters should be identified the first time they are used.
Acronyms should be avoided whenever possible and, if used, defined.
Footnotes are not used in the text.
Acknowledgements are brief and appear after the reference list; grant and contribution numbers are not allowed.
Supplementary information is material relevant to Articles or Letters which cannot, for lack of space, be published in full, but which is available from Nature on request.
Submission .
Manuscripts can be sent to the Editor at  or at .
Manuscripts or proofs sent by air courier to London should be declared as ‘manuscripts’ and ‘value $5’to prevent the imposition of import duty and value-added-tax.
NEWS AND VIEWS
The case for the Higgs boson
Mr William Waldegrave, Britain's cabinet minister in charge of science, has offered a bottle of decent champagne to one who can tell him why the Higgs boson is worth finding (see page 781).
These notes may be helpful.
Do not mention invariance, non-Abelian or otherwise.
Nor should you begin with a reference to spontaneous symmetry breaking.
Of course, there are now plenty of neat proofs that the ground state of a quantum system does not necessarily enjoy the full symmetry of the system as a whole.
The obvious case is that of a ferromagnet cooled below the Curie temperature, when the rotational symmetry of the system may be lost by magnetization.
Waldegrave is both inquisitive and intelligent.
That he takes his job seriously is borne out by his willingness to worry about the Higgs boson.
Evidently the question has arisen in discussions over what should happen next at the European Laboratory for Particle Physics (CERN).
With a British director (Dr Christopher Llewellyn-Smith) now at Geneva.
Waldegrave cannot just walk away from the plan to install in the LEP tunnel the Large Hadron Collider, part of whose promise is success in the hunt for the Higgs boson.
It is uncharacteristically diligent of a minister to seek precisely to understand what money spent will accomplish.
But for all those virtues, it would be ridiculous to expect that he would be enlightened by an account of gauge invariance or symmetry-breaking.
Avoid both concepts.
Analogy is the only way to start, and the electron and the electromagnetic field the only place.
That approach has also the benefit of introducing several British names of which ministers other than Waldegrave will have heard.
(Faraday is now on the £20 banknote.)
So begin along these lines:With respect, Minister, it's to do with explaining action at a distance.
Newton worried about that when his theory of gravitation required apparently instantaneous interaction between two distant objects.
By Faraday's time, the problem was more acute; electrically charged objects were known to repel or attract each other, as did magnetic poles.
Faraday began talking of magnetic fields (and you'll remember the school experiments with iron filings around bar magnets).
Later, Faraday showed that moving charges made magnetic fields and so we had the electromagnetic field.
By the time of Maxwell, it was clear that the electromagnetic field was the means by which electrical charges and magnetic dipoles act on each other‘at a distance’.
Having begun in this historical vein, you had better stay with it.
If you consider that the recitation of the names of British heroes will advance your cause, you might even interpolate:
To begin with, people thought that electric charges could be arbitrarily big or small, but Faraday made the crucial step of showing that charged atoms, or ions, are always associated with a fixed amount of charge or some multiple thereof, suggesting atoms of electric charge.
Then J.J. Thomson showed that these atoms of charge exist; we call them electrons.
So far as we know, there are no corresponding atoms of magnetic charge.
It would be unwise to elaborate on the last point.
This is hardly the place for a dissertation of magnetic monopoles; indeed, with luck, you can hope to avoid a single mention of the grand Universal Theories (called GUTs) that predict them.
But you cannot avoid explaining how the coming of relativity and then of quantum mechanics influenced thinking on the electromagnetic field.
Try something like this:Relativity fitted naturally with Maxwell's electromagnetism, explaining for example why the electromagnetic field around a charge apparently at rest is purely electrostatic, but that around a moving charge (or an electric current) has magnetic components as well .
(Mention of H.A.
Lorentz at this point would be courteous, Hollander though he may have been.)
But the coming of quantum mechanics was the very devil.
For a decade until the late 1930s, people could do no better than to regard the electron as an empirical fact.
Only after Dirac had predicted that there must be positrons as well as electrons (verified in 1932) did he and others set out to see whether the charge and the mass of an electron can be derived from first principles.
He did not get far .
Now comes the difficult part.
The problem is how to explain the quantization of the electromagnetic field to one who cannot instinctively write down Maxwell's equations (which is no shame), let alone say what they mean.
Hand-waving is unavoidable.
A few references to the successes of quantum mechanics in the 1930s might be appropriate; in retrospect, Heitler's quantum theory of radiation is especially prescient.
But you may be running out of space.
So try this:
Dirac was defeated by the complexity of the problem.
The quantum theory of an atom requires that each of its components, its electrons for example, should be dealt with separately, but at worst there is only a finite number of them.
But the electromagnetic field has an infinite number of variables, one for each point in spacetime.
Finishing Dirac's problem was everybody's goal in the 1940s.
Feynman, Schwinger and Tomanaga won the prize.
The upshot was a picture in which electrons interact by exchanging photons with each other.
But they are not real photons at all, but virtual photons created out of the vacuum.
There's now a dilemma.
Do not raise awkward questions such as whether quantum electrodynamics, which calculates an electron mass by subtracting infinities by the technique of renormalization, attains Dirac's goal of deriving the properties of the electron from first principles.
Most think it does not.
In any case, you've almost used up all your space without mentioning the Higgs boson: best do so quickly:
The Higgs boson, Minister, plays much the same role as the photon, but in a different context.
Rutherford's school saw the difference between α and β radioactivity.
The second kind, in which nuclear particles give off electrons, forms a bridge between nuclear matter (neutrons for example) and non-nuclear matter such as electrons.
In other words, nuclear and non-nuclear matter act on each other.
It's action at a distance all over again.
So there's another field filling the whole of space that must be quantized.
What that exercise predicts is that there should be particles mediating the influence of that field just as the photon mediates the electromagnetic field.
They've been found, at CERN.
And then there's the last of them, the Higgs boson.
That's why its discovery is the last goal for the time being of particle physics .
That, sadly, is all you have room for on one side of a sheet of A4 paper.
Do not be downcast that you have been economical with the truth.
Not to have mentioned the strong interaction and the quarks and gluons thereby made necessary is barely forgivable, the omission of neutrinos and the lepton generations other than the electron (the  and  particles) is less so.
You may also cringe a little at your conclusion, knowing as you do that the correctness of the electro-weak theory has been essentially verified by the observation of neutral-current processes as well as by the discovery of the  and  particles.
The real interest of the hunt for the Higgs particle is that it may not be quite what is expected.
John Maddox 
CANCER
A death in the life of p53 D. P. Lane 
The papers on pages 847 and 849 of this issue establish the involvement of p53 in the induction of apoptosis by radiation and chemotherapeutic DNA-damaging drugs.
This discovery brings together two rapidly advancing fields by demonstrating that a tumour-suppressor gene plays an essential role in the induction of programmed cell death.
Apoptosis was first defined in the 1970s.
A complete molecular description of this process of programmed cell death initially proved to be elusive, but its central part in normal development has now been clearly established; the phenomenon is recognized to be widespread in multicellular organisms, and underlies such processes as organogenesis, tissue homoeostasis and the ‘editing’ of the immune system to remove autoreactive clones.
As well as being induced by developmental control processes, apoptosis can also be induced by toxic insults, and, in particular, by agents that damage DNA.
It is frequently seen in tumours and may play a key part in the kinetics of tumour growth.
Many genotoxic cancer treatments may also exert their effect through the enhanced induction of apoptosis.
Two independent pathways lead to apoptosis in thymocytes.
One, initiated by DNA damage, absolutely requires the product of the p53 gene; the other; initiated by glucocorticoids, calcium ionophores or ageing, has no such requirement.
Inactivation of the p53 pathway will result in the survival of cells exposed to mutagens, and may thus have a pivotal role in the development of cancer.
Elegant work on the nematode has identified a number of genes responsible for regulating the process, and in mammalian systems two proto-oncogene products have been shown to act as powerful regulators in the pathway.
The myc gene product, when over-expressed in cells deprived of growth factors, induces apoptosis, while over-expression of the bcl-2 gene makes cells resistant to it.
There was great interest in the observation that over-expression of wildtype p53 could induce apoptosis in a range of cultured cell systems.
But the interpretation of these findings was not straightforward, because high levels of p53 induce growth arrest without apoptosis in other cells.
The idea that p53 may be an essential component of the apoptotic pathway suffered a set-back when it was found that mice without any p53 gene were viable.
The animals develop normally, and are fertile and immunologically competent, which would seem to be at variance with the idea that p53 is involved in these apoptosis-associated processes.
But the demonstration that high levels of p53 are induced by DNA damage has led to the concept that p53 might be part of a damage-control pathway rather than the pathways that are at work in normal development.
The new studies provide compelling evidence in support of these concepts.
Lowe et al .
and Clarke et al .
decided to investigate the role of p53 in a classic system, the induction of apoptosis in thymocytes.
These immature T cells are very susceptible to apoptosis and are normally short-lived.
They can be induced to undergo apoptosis at an even more rapid rate in vivo and in vitro by exposure to low doses of ionizing radiation or other DNA-damaging treatments, or by treatment with glucocorticoids or by a calcium ionophore in the presence of phorbol ester.
Calcium ionophore treatment is thought to mimic the natural signal responsible for the editing out of self-active T cells in the thymus.
The two groups have repeated the work of Donehower and his colleagues and produced new strains of p53 knockout mice.
Thymocytes from the knockout mice show a normal apoptotic response to treatment with glucocorticoids but are extraordinarily resistant to the induction of the process by radiation, both in vitro and in vivo .
The p53 null thymocytes are resistant to more than 20 grays, while the control thymocytes show clear induction of apoptosis at doses of only 1 gray.
So p53 is essential for the apoptotic response to the radiation-induced signal, but has no part at all in the response to the glucocorticoid-induced signal.
These results are remarkable clean, and underscore the enormous power of the gene knock-out approach, and they show that there are at least two independent pathways to the induction of apoptosis.
Moreover, the knock-out mice will allow a crucial examination of the p53 pathway in the induction of apoptosis in other tissues and in response to a wide range of different stimuli (including agents used in cancer chemotherapy).
Indeed, Clarke et al .
show that p53 may play an important role in cell death resulting from treatment with the topoisomerase 2 inhibitor, etoposide, but that it appears to have no involvement in the spontaneous ‘ageing’ apoptosis associated with prolonged culture of these thymocytes.
Both groups are able to show a clear effect of gene dosage; that is, cells from heterozygous mice which contain only copy of the p53 gene are slightly more resistant to radiation-induced apoptosis than are homozygous normal mice with two copies (which are highly sensitive), while the homozygous null mice with no active copies are highly resistant.
A gene-dosage effect of this kind has important consequences for the treatment of people carrying germ-line mutations in the p53 gene, such as those affected by the Li-Fraumeni syndrome.
It also bears upon our understanding of tumour progression, because it implies that cells in which one p53 allele has been somatically mutated may have a survival advantage in the presence of genotoxic agents (for example ultraviolet exposure of skin) over those that have two intact copies of the gene.
This in turn could make them more likely to survive a second mutation at the p53 locus (which is frequently seen in cancer cells), or indeed at any other locus.
The mechanism by which p53 makes cells susceptible to radiation-induced apoptosis is not yet clear.
Certainly both groups find that p53 levels rise following radiation, and this could specifically activate transcription of apoptosis-inducing genes.
Alternatively, p53 may interact more directly with the repair machinery, perhaps increasing its fidelity but delaying repair so that lesion-induced  apoptotic signals are sustained.
Such an apparently paradoxical phenomenon is seen in cells acquiring resistance to alkylating agents by virtue of defective mismatch repair.
The induction of apoptosis by p53 following genotoxic insult may act as a defence mechanism to protect the organism from the propagation of cells that have sustained mutation.
The pathway will clearly be modulated by other genes that regulate apoptosis.
Abrogation of this p53 pathway is the most common specific alteration in human cancer, and may be central to the progression of the disease and to its response to treatment by radiation and chemotherapeutic drugs.
D. P. Lane is in the Cancer Research Campaign Laboratories, University of Dundee,.
VOLCANOLOGY
Geochemical hazard indicators David M. Pyle 
THE development of successful models for predicting volcanic eruptions hinges on answering two questions: how long do volcanoes live, and how much magna do they store?
On page 831 of this issue, Chen and colleagues report the discovery of a correlation between lava chemistry and eruption volume at Unzen volcano, Japan.
They find significant differences in the neodymium isotope compositions of small and large eruptive sequences over the past 300,000 years.
This suggests that the isotopic signature of lava from early in an eruption is a predictor of the volume still to be erupted.
In this case, with hindsight, this suggestion turns out to be true.
Lava has been erupting from Unzen since mid-1991, causing much death and destruction in the process.
Samples from early in this eruption have the isotopic signature of previous large eruptions.
Had this volume-composition correlation been known in May 1991, geologists could safely have predicted that the eruption would last for at least another few months, and would erupt at least twice as much magma again.
This is the first predictive tool to appear that is based on the chemistry of the erupted lavas.
The significance of the results extends beyond Unzen, as it reinforces a volume-composition pattern found across the western United States, and suggests that Nd-isotope tracers might be generally applied to continental effusive volcanism in the effort to minimize volcanic hazards.
There is a convincing explanation for this pattern.
Magma is believed to be stored in crustal reservoirs before eruption.
These reservoirs behave as imperfectly elastic containers, expanding and contracting to accommodate fluxes of melt.
They paly the role of a chemical filter, modifying the incoming melt by storing and mixing magmas before eruption, and allowing melts to interact with the chamber walls.
Fortunately, the incoming melts and the chamber walls often have significantly different isotopic compositions, so that their respective influences can be detected.
Whereas resupply with fresh melt produces erupted liquids with more ‘source-like’ compositions, prolonged storage and crystallization coupled with wall-rock assimilation generates liquids with more ‘crust-like’compositions.
The proposed explanation of the volume-composition relation is that it is controlled by the recharge rate: rapid recharge leaves little time for assimilation, and produces large volumes of mantle-melt melts.
With lower recharge rates, storage and assimilation become relatively more important, and the resulting eruptions are both smaller and more crust-like.
However, the maturity of the plumbing system and reservoir size may also play a role.
At some volcanoes, the most crust-like magmas are produced immediately after caldera collapse events, when the chamber roof collapses and melts that have failed to erupt are swamped with crustal contaminants.
Later magma, following the same route to the surface, encounters a barren conduit system, already stripped of contaminants.
There is also the possibility that as the volume of stored melt increases, and the ratio of volume to surface area in the reservoir increases, so wall-rock contamination becomes less efficient.
The other important questions are how long reservoirs survive, and how long they take to develop.
By a quirk of nature, many large-volume rhyolite systems harbour exceptional melt Rb/Sr ratios (in the range 100–1000) and consequently experience rapid changes in Sr-isotope composition as radioactive 87 Rb decays to 87 Sr. With routine measurements of Sr-isotope ratios possible to a precision of a few tens of parts per million, time differences of only a few thousand years between crystallization and eruption ages become detectable.
Studies of the 700-km 3 Bishop Tuff eruption and its precursors from Long Valley, California reveal distinct Sr-isotope differences between crystals and their host liquids and between melts erupted at different times.
Modelling leads to the startling conclusion that melts may have been stored for as long as 300,000–500,000 years before eruption.
Although this concurs wit suggestions that magma in caldera-forming systems is produced at rates of around 1km 3 per thousand years, it is not clear that this is a reasonable timescale.
Arguments turn on whether the whole chamber had to remain molten, and, if this was the case, on the plausibility of supplying an appropriate heat flux to offset cooling.
If Long Valley magma chamber was recharged, then to balance the heat losses of 10 8 watts requires a magna flux of 1 km 3 per hundred years sustained for the lifetime of the chamber.
This flux is comparable with the eruption rates of many continuously erupting basaltic and andesitic volcanoes, and is not obviously implausible.
Why the chamber should have decided to evacuate after half-a-million years stability remains a matter of conjecture.
At the other end of the eruption size-scale, time-series analysis of the rates of change of chemical signatures in lavas from continuously erupting volcanoes is now being used to reveal the storage times of magmas before eruption.
Results suggest that melt residence times beneath basaltic volcanoes are only of the order of tens of years, and stored melt volumes are consequently small (1 km 3 or less).
A similar approach has been applied to marine records of explosive eruptions in the Bay of Naples.
Eruptive and chemical cycles are correlated, and apparently both have periodicities of around 23,000 years.
This timescale is similar to the Earth's orbital precession period, which also appears in climate records, suggesting that external factors may modulate the ascent of magma to the surface.
Although these examples represent important advances in the application of chemical tools to tracing magma systems, the answer to what controls magma-chamber lifetimes is not yet clear.
If some continuously erupting, small-volume systems have short residence times, while other intermittent, large-volume systems are long-lived but continuously fed, this suggests that factors other than magma supply control eruptions.
However, the inference that at Unzen recharge rates do control eruption size, and the existence of well-documented examples where supply of fresh magma demonstrably caused eruption implies that if there is a general pattern to be discovered, it will not be a simple one.
David M. Pyle is in the Department of Earth Sciences, University of Cambridge,.
AGRICULTURE
Better cycling in China Peter D. Moore 
CAN the ecosystem concept be usefully applied in situations where human activity is dominant?
It certainly can, as shown by studies of energy flow and nutrient cycling ranging in scale from the mowing of lawns to the energetic balance of the entire Earth.
But in the Journal of Applied Ecology (30 ), 86–94; 1993) J. Y. Guo and A. D. Bradshaw now describe an unusually detailed and economically valuable application of the approach to a farming system in China, in which the interactions are complex and the management options are consequently varied.
Gone fishing (for bigger yields) in Jiangsu province.
The study of agricultural systems in ecosystem terms is usually limited in value because of the relative simplicity of the energy and nutrient flow patterns when compared with more natural habitats.
This simplicity is, of course, in many respects the precise object of their management, because energy and nutrients are thereby channelled as directly as possible into human consumption with few sidechains and reduction in the losses involved in transferring energy between trophic levels.
But such ecosystems are often open, in that they require heavy inputs of nutrients and energy to maintain populations and productivity of a few selected species of plants and animals.
Self-sustaining agriculture must move towards greater internal recycling, which means more nutrient and energy flow routes and more complex interactions within the system.
In this respect, the ancient farming systems of some areas of China are of especial interest as they have a high degree of self-sustainability and yet are able to support high densities of human population.
The site studied by Guo and Bradshaw is a village in Jiangsu province, having an area of 155 hectares (ha) and a population of 2,353 inhabitants which has increased by about 0.25% a year over the past 40 years.
This farming systems has proved self-sustainable for the past 2,000 years and has altered little except that summer (wet season) rice crops were once combined in the same fields with winter (dry season) legumes, whereas the winter crop is now wheat.
Artificial fertilizer application, associated with higher yielding wheat, is also now a feature of the system and is required to cope with the growth in population.
Of the land area in the village, about 45% consists of fields used for cropping, 21% of fishponds, and 8% of water channels, the remaining 26% being occupied by housing, roads and marginal areas.
Over 1,500 pigs are also raised annually, of which more than half are exported from the village.
Guo and Bradshaw carried out analyses of the nutrient (N, P and K) and energy flow patterns within the village and have conducted a variety of experiments to see how these patterns can be modified to result in a greater retention of nutrients and a more efficient return of energy to the human population.
Several improvements were tested experimentally, such as the harvesting of grasses from the banks of the fish ponds and feeding them to herbivorous fish.
In addition, fish ponds were deepened to find out if light penetrations was adequate to allow phytoplankton productivity to be increased.
In deepening the ponds from the traditional 0.5 m to 2.5 m, phytoplankton production occurred to depths of 1.6 m and annual production figures of 8.4 tonnes ha -1; were achieved, which is greater than the grain yield of rice for the same period.
The combination of these two modifications to the system resulted in increases if fish yields from 2.6 tonnes ha -1; yr -1; to 12.26 tonnes ha -1; yr -1;, an increase of 370%.
About one-third of the energy intake of the fish was derived from phytoplankton, so both of the management practices contributed to the boost in fish productivity.
The unproductive channels could also be planted with aquatic vegetation and the crop fed to the pigs.
A controlled experiment using the normal commercial pig feed and a mixture containing herbage from an alien aquatic plant,Justicia americana , common in the area, resulted in no significant change in pig growth rate.
But it did result in a 20% saving on commercial pig feed and also served to recover much of the lost nutrients in the ditches and to reduce eutrophication problems downstream.
More than 300 kg N, 80 kg P and 560 kg K were recovered per hectare per year by this method and most of this passed through the pigs and was used as fertilizer for the cropped fields.
The enhancement of internal recycling of nutrients via the pigs, as well as use of mud derived from the pond excavations and waste from animal consumption (fish and pig bones) as fertilizer additives, allowed the grain yields to be maintained while at the same time reducing the demand for external fertilizers by about 25%.
Against that, the process of recovering and spreading such wastes is labour intensive.
Overall, the modifications to the management of the system resulted in an increase of about 25% of the total agricultural output, with a change in emphasis towards fish production and export (which makes economic sense because of the high value of fish in the marketplace).
Use of marginal areas for grass production and channels for Justicia culture effectively enlarged the land base for the population, thus reducing population density from 34 to 19 persons per hectare of agricultural land.
More important, it left the people better fed.
These developments could provide a breathing space in which the pressure on resources is reduced, but in the long term the system can be sustainable only under conditions of stable population.
The ecosystem approach can provide a means of quantifying processes and predicting consequences even in such an inscrutable ecosystem as the Chinese village, but in the end there is a limit to the food that it can supply.
Peter D. Moore is in the Division of Life Sciences, King's College,.
ATOMIC CLOCKS
Limits to improvements?
Richard Thompson
EFFORTS to improve atomic clocks have come up against an unexpected hurdle.
The prospect has been that atoms cooled to microkelvin temperatures, with an arrangement called an atomic fountain, would allow orders-of-magnitude improvements in frequency standards.
But only last year, it was suggested that collisions between the chilled atoms might change the frequency standard by a small but significant amount.
Experiments by Gibble and Chu now confirm this, so that continuous recalibration through extrapolation back to collision-free conditions may be necessary.
It is now more than 25 years since the caesium atomic clock was first used to define the second, replacing the earlier definition based on astronomical observations of the mean solar day.
Since then the device has remained essentially unchanged in principle.
It consists of a beam of caesium atoms whose hyperfine transition frequency, at about 9.2 gigahertz, is probed by a microwave field using the ‘Ramsey technique’, where the interaction takes place in two spatially separated regions with a field-free region between them.
This scheme, for which N. F. Ramsey was awarded the Nobel prize in physics in 1989, enables the maximum resolution to be extracted without the need for a long and highly uniform interaction region.
The best caesium clocks achieve a fractional accuracy and stability of between 10 -13; and 10 -14;.
One of the serious limitations is the total interaction time with the microwaves, limited to a few milliseconds by the speed of the atomic beam and by the separation of the two Ramsey regions.
If the interaction time could be lengthened. the spectral linewidth obtained would then be reduced, allowing greater accuracy, if no systematic effects become significant.
One way of doing this would be to use slower atoms, and laser cooling is ideal for this purpose.
In this technique atoms have their velocities reduced by running head-on into a laser beam tuned to their transition frequency.
Another possibility, and perhaps a more elegant one, is to use a sample of atoms that have previously been cooled and trapped by laser beams, and to project them upwards against gravity.
They then interact with microwaves on their way up and again on the way down after gravity has turned them round.
Such an atomic fountain, first demonstrated by Kasevich et al .
in 1989, gives a total interaction time of a large fraction of a second, allowing the possibility of a linewidth some two orders of magnitude smaller than in the conventional arrangement.
It also has the advantage of reducing many systematic effects present in atomic clocks.
The unexplored hazard was the effect of collisions between the ultracold atoms, in particular the frequency shifts they introduce.
FIG. 1 The central portion of the resonance peak for the F=3 to F=4 transition studied by Gibble and Chu.
The oscillatory fringes are the product of the Ramsey technique which the authors use.
The study of ultracold collisions has become a fertile subject in itself because the physics of such collisions is fundamentally different from the physics of ‘conventional’ collisions.
The reason is that the de Broglie wavelength of a cold atom (typically 0.2 ) is greater than the scale of the atomic interaction, making the scattering intrinsically quantum-mechanical in nature.
It is only in the past few years that a good understanding of these types of collision has emerged, and it is only a year since Tiesinga et al .
made a calculation that suggested that, from the available evidence, the collisional frequency shifts might limit the improvement available by using a fountain set-up.
Collisions in general shift frequencies because of the way they perturb atomic structure, and the quantum effects under these conditions make the interactions significantly harder to calculate.
The calculations predicted an unexpectedly large shift of the order of 1 millihertz for an atomic density in the fountain of 10 9 atoms per cubic centimetre.
This might not sound very much, but it should be remembered that the current fractional accuracy is between 10 -13; and 10 -14;(between 1 and 0.1 mHz) and that the predicted accuracy available is of the order of 10 -16;(1  Hz).
So an experimental measurement of the shift became a very important priority.
Gibble and Chu's experiments proceed as follows.
First, a sample of 10 9 atoms is loaded into a magneto-optical trap, where they remain for just over 1 second.
Then the atoms are projected upwards with velocity of 2.5 m s -1; and cooled further to a temperature of 2.8  K. They are then optically pumped into the F=3 hyperfine level and pass through the microwave interaction region where they experience the first pulse of tunable microwaves centred close to the F=3 to F=4 transition at about 9.2 GHz.
After they have turned round under gravity and interacted a second time with the microwaves, the fraction of atoms in the F=4 level is detected and recorded.
The spectrum obtained this way (Fig. 1) reveals the linewidth to be just 1.4 Hz, a little over one part in 10 10 of the transition frequency.
FIG. 2 Frequency shift as a function of atomic density as measured by Gibble and Chu: green line, with all sublevels; red line, with most sublevels removed (see text).
Although the atomic beam pulses spread out during a particular measurement, owing to velocity dispersion, calculations show that the mean density quoted here is the important measure.
The centre of the resonance can be determined to 1.5 mHz (a thousandth of the linewidth) after just 100 launches.
To look for small changes in the transition frequency, it is sufficient to take measurements at two points, one each side of the central peak, roughly half way down the peak.
This is what Gibble and Chu did for different densities of the cloud of cold atoms, and the results are depicted in Fig. 2.
The find that the shift is about .
Gibble and Chu are not distressed by the discrepancy, because there is fair uncertainty in the interatomic potentials that are used in the calculation.
In fact, the experiment is sensitive to only one sublevel of the F states, although atoms in the other sublevels can contribute to the frequency shifts.
As they contribute nothing to the signal, it would seem to be advantageous to remove them from the fountain.
The authors achieve this with a small magnetic field (the redundant sublevels are magnetic), but in the event find that the shift is clearly larger, 15,8 mHz at the 10 9 cm -3; density already considered (Fig. 2).
The theoretical prediction for this case is only -2.2 mHz.
Evidently, ultracold collisions will have to be taken into account if atomic fountains are to be used in frequency standards.
A high atomic density is favoured to maximize the signal-to-noise ratio, but if it introduces frequency shifts, the shifts will either have to be well characterized, to allow an appropriate correction to be made, or else an extrapolation to zero atomic density will be necessary.
Gibble and Chu point out that for the latter option, it is not necessary to know the absolute density with high precision, but just the ratio of densities.
Indeed, they are able to predict the zero-density frequency to a precision of 4 x 10 -14; from the present data, and they outline several improvements that can be made to the present apparatus, which already has a stability better than many primary caesium standards.
So the ultracold collisions are unlikely to provide a fundamental limit on the fountain experiments, at least in the near future, although they will certainly need to be accounted for in some way.
In the meantime, the study of these collisions is certainly revealing much new and interesting physics.
For example, related experiments which measure elastic collision cross-sections of ultracold caesium atoms point to the feasibility of using evaporative cooling of magnetically trapped caesium to reach the temperatures required for the observation of the much-sought Bose-Einstein condensation of atoms.
That will open a whole new range of quantum-mechanical effects in cold vapours.
Richard Thompson is in the Department of Physics, Imperial College of Science, Technology and Medicine,.
EARTH SCIENCE
Bacteria forge a new link Lee Kump 
THE list of processes thought to occur abiotically at the Earth's surface has been shrinking on a yearly basis.
Increasingly, the involvement of organisms, especially microorganisms, in mediating these processes has become apparent, and Widdel and colleagues (page 834 of this issue) now come up with what looks to be a particularly remarkable example.
They suggest that one of the most spectacular of supposed abiotic events — the deposition of huge amounts of iron in the seas of Archaean and early Proterozoic times, 3.5–1.8 billion years ago — was in fact biological in origin.
The finger of suspicion points to anoxygenic, photosynthetic bacteria, newly discovered by Widdel et al .,
which use ferrous iron as the electron donor.
Banded iron formation near Ishpeming, Michigan, USA.
(Photograph by Preston Cloud, reproduced from Oasis in Space: Earth History from the Beginning ; Norton, 1988.)
That such an organism existed, or exists, has been the subject of much speculation by palaeobiologists and geochemists interested in the origin of the so-called banded iron formations (BIFs); these are sedimentary ores typically composed of alternating layers of silica- and iron-rich minerals that can often be traced as discrete units over hundreds of kilometres.
The reason for such speculation comes from the need for a mechanism of generating BIFs at a time in the history of the Earth when the atmosphere may have contained only minute amounts of free oxygen.
Current models for BIF deposition typically invoke an anoxic deep ocean, enriched in ferrous iron, which provided iron to the surface through upwelling processes.
At the surface, the iron was oxidized.
It then settled to the seafloor, below the level of wave action, where it accumulated as an enriched iron deposit.
Several pathways of oxidation have been proposed, including reaction in the ocean's surface with low concentrations of free oxygen produced by oxygenic photosynthesizers, and photochemical oxidation of ferrous iron.
Widdel and colleagues propose a third mechanism, the use of ferrous iron as the reductant in light-driven CO 2 fixation by anaerobic bacteria using only photosystem I. Such a pathway is energetically favoured, and its existence is now substantiated by Widdel and co-workers.
They obtained enriched cultures of purple non-sulphur bacteria from marine and freshwater muds; and, from stoichiometric arguments involving the amounts of cell mass growth and iron oxidized, they demonstrate that the bacteria do indeed use this pathway for carbon assimilation.
From a strictly biological point of view, this work is notable enough in that it establishes a new category of bacterial metabolism.
From a palaeoenvironmental perspective, the finding is highly provocative.
Although there have been proponents for a connection between microbial productivity and Precambrian iron deposition, the paucity of microfossils and low organic carbon content of BIFs has been seen by some to constitute strong evidence against biological involvement.
Widespread BIFs, such as those of the Hamersley Basin in Australia, would seem to have required massive and persistent microbial blooms to yield the tremendous quantity and uniformity of iron deposition within the basin.
Holland has estimated that the rate of iron deposition in this basin was of the order of 20 mg Fe .
Productive regions of the world's oceans today generate about 20–50 mg C cm , so the Hamersley Sea need only have been moderately productive to generate the iron deposits preserved there today.
But were such ecosystems able to generate iron deposits on this scale without preserving significant quantities of organic matter in the sediments?
The answer has to be ‘perhaps’.
Sediments underlying moderately productive regions of the open ocean today are typically low in organic carbon and enriched in minerals (in this instance, opal and calcite) produced by pelagic organisms.
And in the case of the upside-down Archaean biosphere proposed by Walker, a counterintuitive relationship between high productivity in the overlying water column and organic-poor sediments below arises, because the supply of oxidant (ferric iron) to the sediments exceeds that of reductant (organic matter).
Although the observations of Widdel et al .
are convincing, they are not conclusive.
Many questions must be tackled before anaerobic, iron-coupled photo-synthetic organisms will be accepted as the generators of the Precambrian BIFs.
Were these organisms pelagic or benthic?
How did they avoid coating themselves in iron oxides, which would quickly shut off the bacteria from the solar energy they demand?
Is it not likely that an efficient, anaerobic, heterotrophic pathway based on iron reduction would have been established?
(If so, then considerably higher rates of primary production would be required to sustain the iron deposition rates calculated for BIFs.)
What is a reasonable estimate for primary production in a system based on anaerobic photosynthesis and respiration?
Whatever the answers, the new discovery will fuel considerable discussion about the nature of the earliest ecosystems on the Earth, and the involvement of iron in microbial ecology today.
Lee Kump is in the Earth System Science Center and the Department of Geosciences, Pennsylvania State University,.
MANTLE RECYCLING
Isotopes and a smoking gun William M. White 
CYCLES characterize geochemical processes operating at the surface of the Earth.
The salts dissolved in the oceans do not reside there permanently, rather they cycle through them as they pass from crystalline to sedimentary rock.
CO 2 is continually cycled through the biosphere, atmosphere, hydrosphere, and soil and rock reservoirs.
Some geochemists have suspected that a much grander geochemical cycle, one encompassing the entire crust and mantle, is also at work in the planet.
According to this idea, oceanic crust and sediment are subducted into the deep mantle to rise again as mantle plumes hundreds of millions of years later.
On page 809 of this issue, Woodhead and colleagues report new isotope data on volcanic rocks from seamounts southeast of Pitcairn Island that seem dramatically to confirm this theory.
Although isotope studies have always been at the heart of the deep-mantle recycling hypothesis, previous analyses have focused on radiogenic isotope variations (those that result from radioactive decay, such as the isotopic composition of Sr and Nd); Woodhead et al .
combine such analyses with oxygen isotope analyses, and it is variations in the latter that are particularly significant.
Hotspots
Linear chains of oceanic volcanoes, such as the Hawaiian and Society Islands, that occur when lithospheric plates drift over a ‘hotspot’, are now widely thought to be the surface manifestation of mantle plumes, columns of hot rock that rise buoyantly from the deep mantle.
The volcanism results from decompressional melting as the plumes approach the surface.
From just how deep these plumes rise is still not resolved.
Both laboratory experiments and numerical simulations have demonstrated that plumes can arise only at a boundary between convective regimes, such as that between the Earth's core and mantle at a depth of 2,900 km.
In addition, recent calculations of the total plume heat flux show that it matches the expected heat flux from the Earth's core.
These and other observations have led to the obvious speculation that plumes arise at the core-mantle boundary.
If, however, the mantle itself is divided into two convecting layers, plumes might rise from the internal boundary between them, perhaps the seismic discontinuity at 660 km depth.
Magmas erupted at mid-ocean ridges are derived from the upper mantle.
Comparison of their chemistry with that of magmas erupted on oceanic islands demonstrates the plumes differ chemically from the upper mantle.
The cause of this difference has been a central question in solid Earth geochemistry for two decades.
Early on, it was thought that plumes came from a chemically unprocessed, or primitive, mantle reservoir.
However, plumes are generally enriched in those elements that are enriched in the Earth's crust, and studies of radiogenic isotopes showed that this enrichment must have occurred hundreds, if not thousands, of millions of years ago.
From these observations, Hofmann and I proposed that plumes consist in part of deeply subducted oceanic crust and sediment.
Subduction
Return of oceanic crust to the mantle is implicit in plate tectonic theory, and Armstrong recognized very early the potential implications for crust and mantle evolution if continent-derived sediments were subducted when plates converge.
Chemical and isotope studies of the magmas erupted along island arcs that occur above subduction zones subsequently established that sediment can be subducted to depths of at least 100 km.
Indeed, in last week's issue, Plank and Langmuir showed that there are global correlations between the flux of sediment into subduction zones and the composition of magmas erupted from the associated volcanoes.
As Plank and Langmuir point out, only a fraction of the sediment flux seems to be consumed in arc magma genesis, implying that the bulk is recycled into the deep mantle.
Nevertheless, unequivocal evidence for deeper subduction of sediment has proved elusive, and other explanations are possible.
In a hypothesis that bears some similarity to ours, McKenzie and O'Nions suggested that the lithosphere underlying continents might detach itself, sink to a convective boundary layer, and ultimately rise as mantle plumes.
Melts that had frozen within this subconscious lithosphere would enrich it in the same elements that are enriched in the crust, so it has not been possible to distinguish between these theories.
Also in last week's issue, Class and coworkers interpret radiogenic isotope date from the Kerguelen plume in just this way.
These two possibilities are not mutually exclusive.
There are three or four geochemical classes of plumes, and a different process may be responsible for each class.
The significance of the oxygen isotope  data reported by Woodhead and colleagues is that they appear to require an upper crustal or sedimentary component in the lavas of the Pitcairn seamounts.
The two principal isotopes of oxygen,, are chemically very nearly identical, but modest fractionation between them can occur when oxygen is partitioned between phases in which it is bound in very different ways.
This fractionation decreases with the square of the temperature, and should be insignificant at the high temperatures of the mantle.
Furthermore, the atomic environment encountered by oxygen in different mantle minerals and melts is similar.
Thus significant fractionation of O isotopes occurs only at the surface of the Earth.
Pitcairn seamount lavas have O isotope ratios up to a few parts per million higher than the mantle value, which leads to the virtually inescapable conclusion that they contain a component that was once at the surface of the Earth.
The only real alternatives to the Pitcairn plume containing recycled continental or sedimentary material are that the magmas have assimilated sediment in crustal magma chambers or they have exchanged oxygen during subsequent submarine weathering.
In this respect, it is somewhat disturbing that the highest, least mantle-like O isotope ratios occur in lavas that apparently experienced extensive fractional crystallization.
The extensive crystallization implies shallow magma storage and hence the opportunity to assimilate surrounding rock.
The nature of the correlations between O isotope ratios and Sr, Nd and Pb isotope ratios is not consistent with assimilation, and the correlation between O and Nd isotope ratios almost certainly could not result from weathering.
Nevertheless, the great potential significance of these results makes one hope that the authors will soon provide the types of data, such as H 2 O analyses and O isotope analyses of phenocrysts, that could completely rule out assimilation and weathering.
As Woodhead and colleagues point out, the strong correlations between radiogenic and O isotope ratios imply recent mixing between the sedimentary and mantle components.
This suggests that the subducted sediment somehow survived as a chemically and physically distinct region large enough to avoid obliteration by diffusion.
Another intriguing result is the sediment/mantle concentration ratios of Sr, Nd and Pb inferred from the curvature of the radiogenic-isotope-O-isotope arrays.
The sedimentary component appears to be richer in Sr, slightly richer in Nd, and poorer in Pb than modern clay-rich marine sediments.
Carbonate sediment would provide a closer, though still imperfect, match, a reminder that deep mantle recycling and mantle plumes may play a role in long-term climate regulation.
Mantle plumes add CO 2 to the atmosphere-hydrosphere system, while sediment subduction removes it.
If this sediment were carbonate-rich, it might imply a short cycle time because carbonate deep-sea sediment was rare before 600 million years ago.
But estimates of the timescale of deep mantle recycling, initially around 2,000 million years, have been growing shorter anyway.
Heating
The initial estimate was based partly on the time required for radioactive heating to cause the subducted oceanic crust and sediment to become buoyant and rise.
It now appears that radioactive heating would be less important than conductive heating from the core, allowing a faster cycle time.
If the seismically peculiar D’ layer at the base of the mantle is the source of plumes, as some geophysicists now suspect (for example, ref. 10), its volume is such that the present plume flux would exhaust it in a few hundred million years, implying that proto-plume material is quickly cycled through the layer.
Class et al .
have also concluded that the cycle time is short.
They argue that isotopic variations in lavas produced over the past 100 million years by the Kerguelen mantle plume require a cycle time of much less than a thousand million years.
Clearly, there are many issues surrounding the origin of mantle plumes and the fate of subducted sediment that remain to be resolved.
Although it might be too much to say that Woodhead and colleagues have found the proverbial smoking gun that settles the issue of deep mantle recycling, they do seem at least to have found a gun.
Whether it has been recently fired will only be shown with a few additional laboratory tests.
There will be some lingering doubts, but the work of Woodhead and others represents some of the strongest evidence yet for this grandest of geological cycles.
William M. White is in the Department of Geological Sciences, Cornell University,.
RÉSUMÉ
Twist or bulge?
HOW does a globular protein adapt to a mutation that forces a surplus residue into a helix?
J. S. Kavanaugh et al .
(Biochemistry 32 , 2509–2513; 1993) have looked at the structure  of the unstable mutant, haemoglobin Catonsville, which has a glutamate inserted into a highly conserved segment of helix (C helix).
One of two things could happen: the ends could untwist to admit the intruder or the helix could bulge outwards.
The C helix happens to have the 3 10 conformation (hydrogen-bonding to the third residue along, rather than the fourth, as in the α-helix), and the answer is that the glutamate generates a bulge; moreover the local helix geometry switches from 3 10 to α.
Kavanaugh et al .
suggest that such changes in helix geometry could constitute a general mechanism of adaptation to insertions or deletions.
Well spotted
IN astronomy, the boast is often that biggest is best.
But A. R. Patnaik et al .
from Jodrell Bank are proud to announce the discovery of the smallest gravitational lens yet (Mon.
Not.
R. astr.
Soc .
261 , 435–444; 1993).
They show that two specks, separated by just 335 milliseconds of arc in radio maps of part of the northern sky, are in fact duplicate images of the same distant radio source.
An intervening galaxy, acting as a weaker gravitational lens than any yet known, has bent the source's radiation to give the two spots and — the clincher in lensing studies — an ‘Einstein’ ring connecting them.
More than any other lens system, the pair should help cosmologists to evaluate the Hubble constant (giving the rate at which the Universe is growing) by directly tracing the geometry of space over vast distances.
Perversely, doing this will require the biggest and best of radio telescope arrays.
Roaring trade
PUT one way, K. McComb et al .
(Proc.
R. Soc.
B252 , 59–64; 1993) demonstrate a direct link between a mammalian discriminatory ability and a fitness benefit.
Put another, they find that lionesses with young can tell the difference between the roars of their cubs' fathers and those of unfamiliar males.
The experiments, carried out in the Serengeti National Park, took the form of playing recordings of known provenance to females with cubs.
The lionesses were unperturbed on hearing resident males of the pride, but instantly responded with protective behaviour when played the roars of unfamiliar males.
The fitness benefit of the lionesses' reaction is plain — males entering a new social group or area often kill the existing offspring, so to further their own reproductive ambitions.
STRUCTURAL BIOLOGY
Picture an enzyme at work Guy Riddihough 
OIL and water don't mix.
So how does pancreatic lipase, an enzyme that knocks around in the essentially aqueous environment of the small intestine, get to grips with fats and oils and break them down into the glycerol and fatty acids that are readily absorbed by the lining of the gut?
Several elements of the answer come together in the paper by H. van Tilbeurgh and colleagues on page 814; this is a sequel to the same group's report of last year (Nature 359 , 159–162; 1992), discussed in News and Views in the same issue (page 107).
The catalytic activity of pancreatic lipase, like that of a number of other lipases, is greatly when the enzyme comes into contact with a lipid/ water interface — this is the phenomenon known as interfacial activation.
van Tilbeurgh et al .
have now crystallized and determined the structure of a human pancreatic lipase-colipase complex in the presence of mixed phospholipid/bile salt micelles.
The results tell us a great deal about the mechanistic basis of interfacial activation.
The presence of the mixed micelles in the crystallization solution induces a series of conformational rearrangements in the protein complex which, in combination, create a binding site for the lipid substrate.
The most dramatic change is seen in the α-helical ‘lid’(yellow in the figure), which covers the active site of the enzyme and blocks access to the substrate in the ‘closed’, inactive, complex.
In the presence of micelles (green) the lid pops open.
This movement exposes the active site (pink), consisting of a catalytic triad like that of the serine proteinases, which lies at the bottom of a hydrophobic canyon in the catalytic amino-terminal domain of the protein.
Furthermore, in the structure there is a region of electron density which could be interpreted as a phospholipid molecule in the canyon, with its two acyl chains sitting comfortably in hydrophobic grooves and the scissile carbonyl carbon positioned in front of the active-site serine γ-oxygen.
A further consequence of the opening of the lid is the restructuring of the β5-loop (red in the figure).
In the closed complex, this loop makes van der Waals contacts exclusively with the lid.
When the lid opens these contacts are lost and the loop folds back onto the core of the protein.
This creates an electrophilic region, the ‘oxyanion hole’, around the active site serine; the oxyanion hole helps stabilise the transition-state intermediate formed during catalysis.
The movements of the lid and the β5-loop expose hydrophobic residues and bury hydrophilic ones in the active-site face of the complex, further increasing the affinity of the complex for the lipid substrate.
The activity of pancreatic lipase is inhibited by a number of components in the small intestine, among them bile salts, secreted by the gall bladder.
The inhibitory effect of these components is overcome by colipase, a small protein also secreted by the exocrine pancreas, which binds to the non-catalytic carboxy-terminal domain of the enzyme (purple in the figure).
Colipase has three hyudrophobic ‘fingers’ which point out from the protein complex on the same face as the active site of the lipase.
These fingers are thought to enhance binding of the complex to the surface of the micelles, overcoming the inhibitory effect of bile salts.
The new structure reveals a second important role for colipase.
Once bound, interfacial activation brings the colipase molecule close to the lid and three hydrogen bonds between the fingers and the lid stabilize the conformation of the open complex.
This interaction seems to be crucial for the activity of the enzyme.
Guy Riddihough is an assistant editor of Nature.
LAKE ACIDIFICATION
Acid rain not only to blame Donald Charles 
THE vision of pre-industrial society as ecologically neutral takes a knock on the head on page 824 of this issue, with evidence from Renberg and colleagues that prehistoric agricultural practices noticeably altered the pH of lakes in Sweden.
The changes were the opposite of those that are causing concern today — acidification — and were smaller.
But they demonstrate the role that land use can have in controlling lake chemistry.
As concern has grown over lake acidification in diverse parts of Europe and North America during the last 20 years, the influence of changes in land use has become a prominent part of the debate.
How important is land use compared with deposition of strong acids derived from the combustion of fossil fuels?
Several hypotheses have it that land-use change, primarily growth of forest or development of heathlands following fires, logging and abandonment of agriculture, could have caused the recent acidification of low-alkalinity surface waters.
To an extent, this is backed up by the work of Renberg et al .,
who provide evidence here and elsewhere of significant long-term regional effects on surface-water pH, including an increase in pH (increased alkalinity) following conversion of forest to agricultural land and a decline in pH (acidification) following abandonment of agriculture.
The full extent of the modern acidification, however, cannot be attributed to changed land use.
Renberg, Korsman and Birks use sediment composition to determine the effects of land-use change wrought by Iron Age peoples in 14 widely separated lakes in southern Sweden (where acid deposition is a severe problem today and a large proportion of lakes are acidified).
Sediment cores were dated using  and  vegetation and land use were determined from pollen; and lakewater pH was inferred from remains of diatoms.
The authors' ability to infer water chemistry quantitatively from sediment diatom assemblages is due to rapid developments over the past 5–10 years.
It is now possible to infer pH with a standard error of about 0.3 pH units.
This new tool has spurred many studies of acidification, and is being applied to other issues such as runaway algal growth and climate change.
Renberg et al .
reconstructed the lakes' history over their entire lifetime (more than 10,000 years), and they compare the relative importance of the factors potentially responsible for acidification of surface waters.
These factors include natural processes such as progressive leaching from the soil of base cations; removal of forest and replacement by agricultural activities during the Iron Age; regrowth of forest following abandonment of agricultural practices many centuries later; and acid deposition, beginning in the nineteenth century.
They find that pH declined from about 7.0 to 5.5 over the first few thousand years following deglaciation, owing to natural processes.
After deforestation and expansion of an agrarian economy during the Iron Age, pH increased by 0.5–1.4 units in 12 of the 14 lakes.
During the nineteenth and twentieth centuries, pH declined, in the past few decades to less than 5.0 in some lakes, a level toxic to many aquatic organisms.
Though it is not possible to determine the relative roles of revegetation and acidic precipitation from this work along, in conjunction with other studies Renberg et al .
argue that the pH decrease to below 5.5 is due to acid precipitation.
In this study, Renberg et al .
effectively perform natural experiments on lake acidification, with the advantage that the results are ‘controlled’ in that the same lakes are being examined as are recording the modern industrial assault.
The results also cover much longer periods than are possible with modern experimental field studies and they are based on direct evidence of intercomparable past conditions.
The role of land use in modifying lake acidity is important not only in an overall scientific understanding of the process, but more urgently because of the need to control sulphur and nitrogen emissions, primarily from power plants.
It is also relevant to management of individual lake-catchment systems.
If land-use changes on a regional scale play a larger role in surface-water acidification than allowed for in government assessments, less need would be perceived to control emissions.
On the other hand, if the role of land use is overestimated, then emission-reduction policies may be insufficient.
Donald Charles is in the Patrick Center for Environmental Research, Academy of Natural Sciences,.
DAEDALUS
King Coal, part II
COAL is an uncompetitive fuel.
Its high ratio of carbon to hydrogen gives less heat and more carbon dioxide than oil and gas; it burns well only in large elaborate furnaces; it produces ash and sulphur dioxide.
Worst of all, it has to be dug out of the ground by expensive skilled labour.
As a chemical feedstock, however, coal is still attractive.
Its high proportion to carbon is an advantage here, so is its complex, benzenoid, easily disrupted molecular structure.
It used to be the basis of the whole heavy organic chemical industry, and might be again.
Daedalus now proposes, not the underground gasification of coal, but its underground polymerization.
Coal is approximately .
It must have reached thermodynamic equilibrium over the years, and represents the most stable state for this composition.
But a bit of added hydrogen or oxygen could tip the balance.
The most stable state of  could well be polystyrene; that of  might well be polyester.
The copious benzene rings and potential carboxylic acid groups in coal suggest that these transformations might not be too difficult.
The methane of the coal seam could also be oxidized to formaldehyde, the basis of the acetal polymers, or even to higher alcohols — the other half of the polyester molecule.
All these thermoplastic polymers melt easily.
Once formed in place, they could be extracted by the Frasch process — pumping superheated stream down a borehole to blow them to the surface in molten form.
So DREADCO chemists are using all the cunning of modern catalytic chemistry to polymerize coal in situ .
They plan to pump a suspension of catalysts in steam and air or hydrogen, down into a coal seam.
They will launch intense microwaves down the borehole to start the reaction, and will them pump down more reagents to keep it going.
The seam will be converted into a vast bed of crude acetal/styrene/ester copolymer.
Thus coal will come into its own.
Oil and gas will provide our energy: they are good at that.
Coal will provide the raw material for the age of plastics.
The crude geopolymer will of course have to be cleaned, purified and fractionated; in the process; its sulphur and nitrogen will be extracted and put to good chemical use.
The miner, whose dangerous and unpleasant labour is so misguidedly romanticized, will be eliminated.
But coal will still be king.
David Jones.
SCIENTIFIC CORRESPONDENCE
Universal tree of life
SIR — Phylogenetic comparison of complete small-subunit ribosomal RNA (rRNA) sequences have changed well-established ideas about early events in the evolution of eukaryotes.
Nonetheless, the incongruence of rRNA-based phylogenies with molecular trees derived from elongation factors or DNA-dependent RNA polymerases presents a challenge to molecular evolutionists.
In rRNA phylogenies the earliest branches are nonphotosynthetic, amitochondriate taxa.
The; y are separated from the more recently diverged kingdoms of plants, animals and fungi by a series of independent protist branches, including Entamoeba.
Hasegawa and Hashimoto in Scientific Correspondence suggest that unusual G+C compositions erroneously place the diplomonad Giardia lamblia (74.7% G+C) and the microsporidian Vairimorpha necatrix (37.5% G+C) rather than Entamoeba histolytica (the earliest divergence in elongation factor and DNA-dependent RNA polymerase phylogenies) at the base of the eukaryotic tree.
We share their concern about potential misleading effects of biased nucleotide compositions of rRNAs used to infer evolutionary histories.
Because there is no accepted theoretical method for compensation for effects of biased nucleotide compositions in phylogenetic inferences, we have sequenced the small-subunit rDNA of the diplomonad Hexamita inflata (50% G+C) and the microsporidian Spraguea lophii (49% G+C).
Using distance, parsimony and maximum-likelihood methods the overall picture of eukaryote small-subunit rRNA phylogeny remains unchanged.
Diplomonads, trichomonads and microsporidians represent the earliest diverging lineages, but their relative branching order is influenced by the G+C compositions of prokaryote outgroups.
By contrast,Entamoeba consistently diverge higher in the tree after the separation of euglenoids/kinetoplasts, acellular slime moulds and amoeboflagellates.
Reconstruction of phylogenetic history from molecular sequence data is a probability exercise based on a specific model of genetic change.
When various models or different genes are used, statistical measurements can provide strong support for contradictory phylogenies.
Deciding between discordant branching patterns frequently reduces to arguments about the ‘correct model’ or ‘best molecular document’for inferring evolutionary history.
In the final analysis, conflicting molecular data sets can be judged by considering the biology of the considered organisms.
The fit between trees derived from the small-subunit rDNA data and morphological and ultrastructure data is unmatched by any other gene used to infer phylogenetic frameworks.
When measured by these criteria, the reliability of rDNA-based phylogenies is remarkable and unparalleled in phylogenetic reconstructions of the universal tree of life.
Mitchell L. Sogin Gregory Hinkle Detlef D. Leipe Marine Biological Laboratory .
SIR — Hasegawa and Hashimoto correctly point out that rRNA trees might be misleading in defining the evolutionary relationships between very distantly related organisms.
They suggest instead that protein trees should be used for this purpose.
Indeed, protein trees encompassing the three domains of life (archaebacteria, eubacteria and eukaryotes) are becoming more popular as the number of available protein sequences from archaebacteria increases.
In particular, several authors have rooted the universal tree of life in the eubacterial branch, using two composite trees of duplicated protein families (elongation factors and membrane ATPases α and β subunits).
Nevertheless, protein trees could be as misleading as rRNA trees for very distantly related organisms.
Well-known mistakes in extrapolating species trees from protein trees have arisen from lateral gene transfers, unrecognized paralogy (duplication of genes before separation of the lineages under investigation) and unequal rates of evolution.
We recently summarized all the data obtained so far by comparing archaebacterial housekeeping proteins with their homologues from eubacteria and eukaryotes at the sequence level.
As we expected, our analysis revealed contradictions between protein trees and the rRNA tree, and between protein trees themselves.
We also find that the composite ATPase and elongation factor trees used to root the tree of life are misleading.
First, the finding of V-type ATPases in two eubacteria suggested that V- and F0/F1-type ATPases are paralogous.
This was recently confirmed by the discovery of an F0/F1-type ATPase in an archaebacterium which already harbours a V-type enzyme.
As a consequence, the eubacterial rooting previously obtained from ATPase evolution is inconsistent as it was based on phylogenetic trees in which these two paralogous families were mixed in single trees.
Second, cladistic analysis suggest that the elongation factors EF1α (Tu) and EF2(G). are too divergent to root with confidence the composite tree of their two families, again invalidating the inferred eubacterial rooting.
The difficulty of rooting the universal tree of life using protein trees is also emphasized by our recent analysis of glutamate dehydrogenase phylogeny.
Trying to root one subfamily of glutamate dehydrogenase harbouring representatives of the three domains of life, using the paralogous subfamily as an outgroup, we obtained different roots according to the method of tree construction used.
Interestingly, the root was located either in the eukaryotic branch or in the archaebacterial domain, but never in the eubacterial branch.
Caution is therefore necessary in drawing definite conclusions from either rRNA or protein-tree analyses.
In particular, it is by no means clear that the problem of rooting the tree of life is now solved.
Patrick Forterre Nadia Benachenhou-Lafha Bernard Labedan Institut de Génétique et Microbiologie, Université Paris-Sud , %addr;.
Phytoplankton productivity?
SIR — Falkowski and Wilson elegantly demonstrate that historical Secchi disk measurements show no evidence of a significant increase in North Pacific phytoplankton biomass in this century.
Unfortunately, they also conclude that increased absorption of anthropogenic CO 2 by phytoplankton in the North Pacific is therefore equally unlikely.
This conclusion assumes that phytoplankton productivity can be usefully indexed by biomass levels, which is inappropriate when applied to the North Pacific.
Secchi disk measurements in the North Pacific, while clearly a surprisingly sensitive measure of phytoplankton biomass, cannot be used to infer changes in phytoplankton productivity.
Unlike the North Atlantic, phytoplankton biomass in the North Pacific is controlled by zooplankton grazing throughout the year, including the spring bloom.
Increased  phytoplankton productivity may therefore not be reflected in increased phytoplankton biomass levels, but may instead be rapidly transferred to higher trophic levels.
Changes in phytoplankton community structure could also result in higher rates of production per unit biomass.
Such changes could happen if, for example, the mean generation time of phytoplankton decreased as different groups of phytoplankton become more abundant; population growth rates, and therefore production, are directly proportional to generation time.
Several lines of evidence point to a change in North Pacific productivity, particularly at higher trophic levels.
As well as the doubling of chlorophyll in the central North Pacific gyre between 1965 and 1985 (ref. 4) mentioned by Falkowski and Wilson, zooplankton biomass in the Gulf of Alaska also doubled between the 1950s and 1980s (ref. 5).
A similar trend in zooplankton abundance is apparent in the western North Pacific — although perhaps not in the central North Pacific.
The productivity of many fish populations in the northeast Pacific has also shown sharp increases, beginning in the late 1970s (ref. 8).
Similar changes in a wide range of physical oceanographic variables have also occurred in the eastern Pacific.
These changes may be associated with the intensification of the Aleutian Low, a major meteorological system which dominates the weather of the North Pacific region.
The detailed reasons for these increases in productivity at higher trophic levels are unknown.
The could involve increased levels of wind-driven mixing of sub-surface nutrients through the thermocline, shifts in phytoplankton community structure towards more intrinsically productive species, or increased insolation.
However, the assumption that productivity must be directly related to biomass or chlorophyll is a fallacy.
Particularly in tightly coupled systems such as the North Pacific, increased phytoplankton production may not be reflected in increased phytoplankton biomass levels; production may instead be rapidly exported to higher trophic levels not readily indexed by Secchi disk measurements.
There have been striking changes in biological productivity at higher trophic levels in the North Pacific, the mechanisms for which have yet to be fully explained.
Nevertheless, the existence of these changes strongly suggests that Falkowski and Wilson's inference that open ocean phytoplankton productivity has remained roughly constant since the industrial revolution is premature, and may even by false.
The potential for the oceans to act as an enhanced sink for anthropogenic CO 2 remains, although the absolute increase in the rate of oceanic CO 2 sequestering possible may still be small relative to the global carbon budget.
David W. Welch Pacific Biological Station, Department of Fisheries and Oceans  .
FALKOWSKI AND WILSON REPLY —
Welch objects to our application of a simple algorithm relating phytoplankton chlorophyll to productivity, calling it a ‘fallacy’ in the North Pacific because of rapid export of production to higher trophic levels.
In doing so he is confusing carbon fixation with the fate of phytoplankton.
Obviously chlorophyll is a pool and primary production is a flux, but primary production (carbon fixation) can be quantitatively related to chlorophyll through light, nutrients and temperature, independent of the fate of the production.
We agree that the specific algorithm we used wold have been inappropriate if we were interested in examining seasonal or short-term changes in primary production, not because the algorithm does not include a grazing term but because it does not include terms for irradiance and quantum efficiency.
But unless there have been marked changes over the past 30 years in insolation, the quantum efficiency of phytoplankton photosynthesis, or rates of nutrient supply, interannual changes or lack thereof, in chlorophyll (which we inferred from interannual changes in Secchi depths) will reflect interannual variations in primary production in the North Pacific or any other ocean basin.
Welch implies that to account for the apparent increases in zooplankton biomass over the past 30 years in the North Pacific without a marked concomitant increase in phytoplankton biomass (as reflected by chlorophyll), phytoplankton productivity must have increased.
This would require that for the same amount of light energy (measured incident solar radiation has not varied by more than 0.5% since the beginning of the twentieth century) more carbon is fixed per unit chlorophyll (the quantum yield has changed).
Although the quantum yield of photosynthesis can vary in the ocean, a systematic, multi-decadal change in the quantum yield of twofold is highly implausible.
Moreover, if such an increase in chlorophyll-specific production had occurred and was responsible for the changes in zooplankton biomass, the hypothesized increase in primary productivity would have to reflect an increase in new production which, in turn, must be fuelled by a systematic increase in nutrient supply to the region.
If, for example, primary productivity in the region is limited by iron, and iron is becoming increasingly available, the iron stimulation would have to lead to a reduction in upper ocean nitrate and phosphate in the summer over the past 30 years.
There is not evidence of a systematic decrease in nutrients in the region.
Examination of the data showing the interannual variability in zooplankton biomass at Ocean Weather Station P (ref. 12) suggests there are decadal cycles, but the long-term systematic changes are not clear.
Finally, as we discussed previously, even if primary productivity were to have increased, unless the increase was supported by nutrients external to the ocean, or led to depletion of the excess nutrients in the upper ocean, the effect on the drawdown of anthropogenic CO 2 would be insignificant.
We stand by our conclusion that the effect of open ocean primary production on the drawdown of anthropogenic CO 2 appears to be extremely small in comparison with other sinks.
Paul G. Falkowski Cara Wilson Oceanographic and Atmospheric Sciences Division,.
BOOK REVIEWS
Soviet meltdown Zhores A. Medvedev 
No Breathing Room: The Aftermath of Chernobyl.
By Grigori Medvedev.
Basic Books: 1993.
Pp. 213 $20, £16.59.
UK distribution, Harper Collins .
Ablaze: The Story of Chernobyl.
By Piers Paul Read.
Random House/Secker and Warburg.
Pp. 416. $25, £18.50.
UK publication date, 26 May .
THE Chernobyl disaster was seven years ago this week (26 April 1986) is still treated as a growing menace in the Ukraine and Belarus.
The term ‘radiation genocide’ is commonly used in Kiev and Minsk newspapers to refer to the accident.
Before the collapse of the Soviet Union, the central Soviet budget met the cost of the decontamination, medical treatment and checkups of the several hundred thousand victims and liquidators, as well as the compensation payments to those still living in contaminated areas.
But with the Soviet government now gone, both the Ukraine and Belarus have been forced to create special ‘Ministries of Chernobyl’ and have introduced a 12 per cent‘Chernobyl Tax’.
Last Monday was not only the seventh anniversary of the accident.
It was also the closing date for entries to the international competition to design a new Chernobyl ‘sarcophagus’.
The previous one, hastily built to entomb the destroyed reactor with about 95 per cent of its original nuclear fuel, is now unstable.
The new project will convert the site into a long-term ecologically safe system and dismantle, process transport and bury the fuel and radioactive materials contained in the old sarcophagus.
The Ukraine, which announced the competition, apparently expects the international community to foot the bill for the construction of the winning project (at least 30 large companies from the United States, France, Canada, Italy and Germany have submitted entries).
It has become traditional in the West (as well as in the former Soviet Union) to move publication dates of new books on Chernobyl as close to the anniversary of the accident as possible.
This year is no exception.
But these two books do not contain new scientific or technical information or add anything to existing knowledge of the environment and medical consequences of Chernobyl.
They mostly supplement the picture of the Soviet political system, how it operated in different circumstances and how it created and controlled its nuclear-energy network.
Grigori Medvedev, a nuclear engineer and the former deputy head of the department of nuclear energy in the Ministry of Energy of the USSR, wrote a book about Chernobyl that was published in Russia in 1989.
One of the best on the subject in both substance and style, it proved to be extremely influential, appearing in English as The Truth About Chernobyl (Basic Books, 1991).
His latest book, written in 1990, is not about the aftermath of the disaster as its English subtitle suggests.
It is in fact about the problems Medvedev encountered in trying to publish the original book and some earlier stories about nuclear energy — that is, about his struggle against the complex system of Soviet censorship.
Life goes on, despite nuclear accidents and the collapse of the Soviet empire.
Taken from Chernobyl: Insight from the Inside by V. M. Chernousenko (Springer, 1991).
The Truth About Chernobyl was written in 1987 and submitted for publication to Novy Mir , one of the best Russian literary magazines.
Although the editors realized the importance of the work, they were unable to publish anything without higher approval.
This meant that the book not only had to pass general political censorship, commonly known as glavlit , the main obstacle for works of fiction; it also had to be approved by the relevant government institutions, the Ministry of Atomic Energy, whose senior officials were named by Medvedev as being responsible for the accident.
The descriptions of the author's visits from one bureaucrat after another take up the bulk of No Breathing Room .
The delay in publication of the original book would probably have been much longer had Medvedev not given the manuscript to Andrei Sakharov to read in October 1988.
Sakharov greatly admired the work and sent a personal letter of endorsement to Mikhail Gorbachev, then the omnipowerful Secretary General of the Communist Party and Chairman of the Supreme Soviet.
But Gorbachev was also unable to arrange publication and so he put the problem before the Politburo.
It was not solved there either: the Politburo wanted the opinion of the department of nuclear energy of the Central Committee of the Communist Party.
The situation seemed hopeless, particularly as the head of this department, Vladimir Marin, was portrayed unfavourably in the book.
Only in June 1989, when the highest authority of the land became the newly elected Congress of People's Deputies of the USSR and when the draft of the new law on the press provided for the abolition of censorship, did Novy Mir publish Medvedev's story for the benefit of its 1,600,000 subscribers.
But it was not an entirely happy ending.
In a comprehensive introduction to No Breathing Room , David R. Marples not only gives some hard facts about the Chernobyl disaster; he also indicates that the impoverishment of most of the population as a result of soaring inflation caused by ‘shock therapy’ economic policies, and the removal of subsidies to the publishing trade, made it difficult for high-quality literary magazines to cope, despite the so-called freedom of the press.
Many degenerated into sensational tabloids or nationalistic journals.
Novy Mir today barely survives with only 100,000 subscribers.
Medvedev argues that the Chernobyl disaster and the resulting struggle for truth were the main causes of the abolition  of censorship in the Soviet Union.
Ablaze , a brilliantly written story about Chernobyl that approaches the topic from many different angles, takes these arguments further.
The author tries to prove that the disaster was also the main cause of the collapse of communism and the Soviet Union.
Piers Paul Read begins with a short but comprehensive overview of the Soviet nuclear energy programme since the Second World War.
He explains how the Soviet research and development system worked at all levels, top to bottom, from Stalin to the millions of Gulag prisoners who built all 15 of the secret Soviet atomic towns that existed in 1953.
Subsequent leaders were also devoted to giving nuclear scientists and designers everything they wanted.
President Gorbachev wanted to double Soviet nuclear-generated electricity within 5 years — 50 per cent of this rapid growth was to be achieved by the construction of the Chernobyl-type graphite-moderated reactors RBMK-1000 and RBMK-1500.
But this programme was ruined by the Chernobyl disaster; and without it perestroika had no chance of success.
Powerful environmental mass movements emerged in the Ukraine, Belarus and the Baltic states; and they, in turn, were gradually transformed into nationalistic ‘people's fronts’, whose members won seats in local parliaments and advocated full independence from Moscow and the end of the communist empire.
This link between Chernobyl and the disintegration of the Soviet Union is obvious, but it was, of course, not the only factor in the fall of the superpower.
Newly independent states now enjoy sovereignty, but the; y still need electricity.
Sixty per cent of electricity in Lithuania is generated by two RBMK-1500 reactors; 40 per cent in the Ukraine is produced by nuclear power stations; and 50 per cent in Kiev is still supplied by the part of Chernobyl that remains in operation.
Armenia is begging Russia to recommission two old reactors that had been shut down for good in 1988 under popular pressure.
But after burning all the trees in Yerevan — even those in the botanical garden — as firewood, nuclear-generated electricity seems a better option than the cold.
Russia is now the only country among the 15 post-Soviet republics that can design, construct and operate nuclear power stations.
The recent decision of the Russian government to revive its nuclear-energy programme indicates that the Soviet common joint energy grid, once the largest in the world, may help Russia to restore at least some economic control over its former empire.
Zhores A. Medvedev is at .
Reaching for the stars R. Hanbury Brown 
Beyond Southern Skies: Radio Astronomy and the Parkes Telescope By Peter Robertson.
Cambridge University Press: 1993.
Pp. 357. £40, $75 .
OF the many delightful quotations in Peter Robertson's book, the one I like best tells us that ‘in creating the Universe God perversely located all the most interesting regions of our galaxy in the southern hemisphere but all the astronomers in the north’.
He certainly didn't put all the radioastronomers in the north: between 1946 and 1960, which Robertson calls ‘the golden age’ of Australian radioastronomy, the group at the Radiophysics Laboratory in Sydney was larger than any in the United Kingdom, the Netherlands or the United States and was remarkably productive.
Researchers had the southern sky all to themselves and radioastronomy was not yet a ‘big science’: one could still make an important discovery with simple equipment that one had built oneself.
Big dish, little dish — a 59-ft telescope was installed at Parkes in 1963 to form interferometer with the main dish.
The innovation brought only mixed success, and the small telescope has now fallen into disrepair.
Robertson begins by taking us through this golden age, dedicating the story to Joe Pawsey, to whose wise and expert leadership the Radiophysics Laboratory owed much of its early success.
With an agreeable mixture of personal and scientific detail, Robertson tells us about the early Australian work on radio emission from the Sun, the planets and the mysterious radio ‘stars’(point sources) and explains how the 21-cm line from interstellar hydrogen was used to map the spiral arms of our Galaxy; he also describes the development of the solar radio spectrograph by Paul Wild and of the high resolution ‘cross’ antennae by Bernard Mills and Wilbur Christiansen.
I particularly enjoyed reading about John Bolton's early work on radio stars at Dover Heights, and the very fair account of the heated cosmological argument between Martin Ryle and Mills over the number of radio sources.
Every one of these topics is skilfully presented in its proper worldwide context.
To be sure, there are a few mistakes in this richly detailed background.
I will mention two, neither of which affects the main story.
First, in the brief sketch of the early history of radar in the United Kingdom, the earliest work on the detection of aircraft was done not at Bawdsey but at Orford; Bowen's middle name was not Gordon, but George; and as a member of the Airborne Radar Group, I can assure the author that Bowen did not develop airborne radar ‘single handedly’, nor did we develop the plan-position indicator.
And second, the story of how Harry Messel came to create the astronomy department at the University of Sydney is in the wrong order; in fact the tragic death of Colin Gum occurred before, not after, the department was established.
The 210-ft telescope at Parkes was completed in 1961, some four years after the 250-ft Lovell telescope at Jodrell Bank, and in the second part of the book we are told how it was funded and built.
For most people such details might be rather boring, but Robertson makes the  narrative come alive through the personalities.
He dedicates this part of the story to Taffy Bowen, who was the prime mover.
With the help of Fred (later Sir Frederick) White, Bowen managed to keep the project out of the clutches of the Australian National University (at one stage he nearly took the whole setup to the United States), and he used friends in high places to raise large sums of money from the Carnegie and Rockefeller Foundations.
We are left in no doubt that fund-raising, and knowing the right people, are essential skills in scientific research.
The account of the design contract with Freeman Fox in London and the subsequent efforts to get the construction done by companies in the United Kingdom presents a depressing picture of British industry as seen from abroad.
The presence of Barnes Wallis as a consultant guaranteed an innovative approach, but it wasn't until the tendering was opened to competition outside the United Kingdom that things really began to move.
In the end, the main contractor was the West German company Maschinenfabrik Augsburg Nurnberg, which completed the job in roughly two years.
The building of the great dish was to split the staff of the Radiophysics Laboratory into two groups, one for and the other against its construction.
This division eventually led to the departure of some of the laboratory's senior staff, a loss that became the universities' gain.
The last section of the book is devoted to a description of what has been achieved with the telescope and is dedicated to John Bolton, who has been largely responsible for the dish's successful operation.
Faced with more than a thousand scientific papers, the author has chosen to give us the highlights: the measurement of the precise position of the source 3C273 by lunar occultation, which led to the discovery of the quasars; the fine work on mapping the southern sky; the detection of the presence of interstellar molecules; and the search for pulsars.
Finally, we learn how the telescope has been used to track spacecraft — there is an exciting account of how it came to be the main receiving station for the first moonwalk.
We should welcome this handsome book as a valuable history of Australian radioastronomy; it is well researched, easy to read and generously illustrated.
It left me wishing that someone would write something equally attractive about the work in the United Kingdom — golden ages in science are rare and should be recorded.
R. Hanbury Brown is at White College .
Chains of being Robert Foley 
Self-Made Man: And His Undoing.
By Jonathan Kingdon.
Simon and Schuster: 1993.
Pp. 369. £25, $30 .
BOOKS on human evolution fall along a spectrum from the weird to the technical.
The weird genre is one of the richest in science, including such gems as Von Daniken's Chariots of the Gods (extraterrestrial astronauts seeding the Earth with intelligent life) and Branko Bokun's Man the Fallen Ape (humans are the result of interbreeding between chimpanzees and orang-utans and have evolved from the laziest apes).
The technical are the lifeblood of palaeoanthropology, but seldom make a thrilling read.
In the middle of the spectrum are those rare books that are rooted in some sort of evidence while looking at humans as the fascinating evolutionary product that they are.
Jonathan Kingdon's Self-Made Man is one such book .
In a readable if somewhat diffuse and meandering style, Kingdon explores the history of our species.
He starts by looking at longer-term human evolution over the past five or more million years, showing rightly that this is not a simple progressive ladder but a series of adaptive radiations.
The central interest, though, is the last part of the story, the origins of modern humans and the subsequent diversification into geographically variable populations.
Kingdon accepts, probably too uncritically, that modern humans originated in Africa about 200,000 years ago and then dispersed across the globe.
His main focus is what this dispersal tells us about human variation and evolution.
As those who are familiar with Kingdon's other books on East African mammals would expect, the book is copiously illustrated with his own very distinctive line drawings.
Time's arrow — an example of Kingdon's distinctive art.
Two strong themes run through the book.
The first of these derives from Kingdon's background in biogeography.
People often think that evolution is a process that occurs solely through time.
It is extraordinarily refreshing to see Kingdon's emphasis on the geographical processes and patterns that must underlie any transformation through time.
Here we have, albeit speculatively (and more baldly than most specialists would dare), the distributions and dispersals of archaic populations and so a geographical history of human populations throughout the world.
The second theme is set by the title — the self-made man — which harks back to Gordon Childe's Man Makes Himself (1936).
Childe had in mind that human evolution and development were not just passive responses to an external environment, but were the result of humanity's own efforts.
Human evolution was progressive and driven by intellectual ingenuity as societies invented their own adaptive solution.
Kingdon's view is very different.
Humans are not self-made through any inner drive to progress, but through the ordinary processes of adaptation.
The do-it-yourself element comes from the fact that the capacity of human populations to use technology to adapt gives each population a unique set of circumstances, which in turn shapes both cultural development and biological characteristics.
Variation in skin colour, for example, reflects not only levels of ultraviolet radiation, but also the activities of the people involved; in the same radiant environments, seafarers are darker than terrestrial people because their dependence on tidal patterns constrains them from adopting the shade-seeking behaviour of their land  dwelling counterparts.
So technology that builds the boats leads directly to biological adaptation and evolution.
To talk of ‘self-made man’ in this way is perhaps misleading if it refers to any unique evolutionary mechanisms relating to technology, as Kingdon seems to suggest.
All evolutionary change is essentially the result of a tight feedback between the behaviour of the organism and the environments in which it finds itself.
What is unique about humans is the rate at which this has occurred and the consequences for other species.
Kingdon's ideas may seem like a recipe for environmental determinism, but the link drawn between a recent origin for humans and the fundamentally local nature of adaptation means that his book is also a powerful indictment of any attempt to rank human populations in terms of progress or to see the evolution of human races as anything other than short-term responses to particular problems.
Human diversity is the product of this process endlessly repeated and re-sorted over tens of thousands of years.
So the classic typological races disappear in the scientific reality of time, environment and geography.
Books on human evolution are often written by those who are not specialists in the subject and the results can be catastrophic or embarrassing.
But Kingdon's knowledge of the diversity of African animal life and environments enables him to cast a light on human evolution that many specialists would do well to consider seriously (despite reservations about details) and that many others will simply enjoy.
Robert Foley is in the Department of Biological Anthropology, University of Cambridge,.
What the British do best Kevin Burke 
Encyclopedia of the Solid Earth Sciences Editor-in-chief Philip Kearey.
Blackwell Scientific: 1993.
Pp. 713. £99.50, $179.95 .
THIS is a good book.
It is comprehensive and technical enough to meet the needs of professionals and is easy to use.
New to me is the mixing of three styles or article: short definitions, 200-word entries and extended essays of up to 1,500 words.
There are 2,700 entries, but the editors emphasize the importance of their alphabetical index with three times as many terms covering key words from the articles and synonyms.
The intention is that indexing and cross-referencing should ease the pursuit of a topic in depth.
I pursued ‘perovskite’.
The primary entry was a compositional definition, but two of seven further entries did take me to the deep mantle.
There is no text entry for ‘dinosaur’, but the index gives 11 references that together cover these creatures thoroughly, although some reveal the British distaste for an impact as the cause of the Cretaceous/Tertiary extinction.
Nearly all the 65 contributors work in the United Kingdom, a bias that has slightly influenced the selection of topics and viewpoints.
I do not think that ‘valley bulging’ would have had an entry in a similar work from any other country.
British strengths emerge.
Structural geology is covered well and, among applied topics, so too is engineering geology.
Mention of global change is hard to discern, but there is an excellent essay on the history of the Earth sciences.
References to books and articles are an asset.
Entries are initialled so that it is usually clear who wrote what.
Some articles, however, are misleading: for instance, in several places it is indicated that continental material has not been recycled into the mantle.
The illustrations, drawn specially or adapted, are large, clearly labelled and handsome.
The conodont animal looks at ease swimming alongside a collection of her teeth, but the accompanying article does not refer to the identification of dentine as a strong indicator of vertebrate affinity.
The few photographs are uneven.
A Landsat image of Tibet is magnificent but the cliffs of Moher are as black as the O'Loughlins who ruled them.
And the many short definitions for minerals seem out of place in an encyclopaedia (I prefer my American Geological Institute glossary, in which I can find ‘burkeite’).
Some hyperbole on the dust jacket says that the volume is ‘essential for professional earth scientists’.
I would not go so far, but this is a good place to start finding out about unfamiliar topics.
Students with limited time to complete an essay would do well to begin here, although they are likely to get a British perspective and, inevitably, some less than comprehensive coverage.
As a final thought, my enjoyment of the longer essays made me regret that we still do not have an Oxford Companion to the Earth Sciences .
Kevin Burke is in the Department of Geosciences, University of Houston,.
Untangling data and theory Deborah M Gordon
Spiders in Ecological Webs.
By D. H. Wise.
Cambridge University Press: 1993.
Pp. 328. £45, $79.95 .
COMMUNITY ecology is struggling to bring data and theory together.
Is it possible to establish any general principles of ecology?
Do we need more data or better theory?
This book contributes to the debate by looking at existing data on spiders.
Wise considers some basic problems of community ecology: how important competition is; whether niche partitioning structures communities; how much can be explained by food webs; and how to determine the effects of one population on the dynamics of another.
His discussion is based mainly on the results of field experiments, and he concludes that spiders are limited in numbers by food supplies, yet do not compete with each for food or limit the population sizes of their prey.
The tone of the book is upbeat, with the author arguing that more data of the right sort will resolve the outstanding questions of community ecology.
The empirical literature is carefully reviewed, with an emphasis on the evaluation of field methods, data analysis and experimental design.
At least to someone outside the field of spider ecology, Wise's discussion of his colleagues work seems fair and even-handed.
The writing is straightforward and generally very accessible, although it is sometimes prone to metaphorical entanglements in discussions of the big picture, Spider lovers will enjoy an amazing abundance of spidery puns; the title is only the beginning.
The book will be useful reading for anyone seeking to bridge the gap between data and theory in ecology.
It should make an interesting text for a graduate-level course in community ecology, help students to plan research and set any ecologist thinking about possible generalizations.
Deborah M. Gordon is in the Department of Biological Sciences, Stanford University,