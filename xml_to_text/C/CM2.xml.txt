

NATURALISM, FALLIBILISM AND EVOLUTIONARY EPISTEMOLOGY Christopher Hookway
In response to the failure of a number of attempts to provide foundations for certain knowledge of empirical facts, many epistemologists have tried to defend various forms of fallibilism.
They hold that we may, with justification, be certain of beliefs which, we admit, risk falsification, or that we may be justified in holding beliefs although there are none of which we are certain.
In all cases, a central theme is that the failure of the project of providing foundations for knowledge does not warrant scepticism.
Along with this has often gone a general rejection of the idea of a First Philosophy, more secure than, or prior to, the sciences, and an acceptance of the naturalistic doctrine that a philosophical account of knowledge should be permitted to use empirical information drawn from the sciences.
A more indirect borrowing from the sciences has been the use of evolutionary models, derived from the theory of natural selection, to describe or explain how fallible knowledge can grow.
Various aspects of these topics will be considered in this paper, which falls into three parts.
In the first, I discuss the relations between fallibilism and naturalism by examining the opposition to naturalism of the pragmatist, C. S. Peirce, and the way that he reconciles this with his fallibilism.
The second section examines the claims of Quine to have ‘naturalized epistemology’, and the third offers some brief critical remarks on the evolutionary epistemology of Donald Campbell.
1.
FALLIBILISM AND NATURALISM
The denial of naturalism carries with it the recognition of the possibility of an epistemology which is prior to all of the special sciences, and which can make no use of general or particular facts about nature.
There are questions about knowledge which may, or must, be answered using limited resources, although there is disagreement about what the available resources are.
For instance, if it is supposed that some of our perceptual judgements have a form of intrinsic credibility, and if a set of canons of rationality can be justified without recourse to the special sciences — they are analytic, self-evident, or a conventionally adopted linguistic framework — then it may  be possible to trace the credibility of all rational beliefs to that possessed by this foundation.
If a reductive programme of this kind is possible, we may be tempted to provide a rational reconstruction of our knowledge which shows how the credibility of derived claims can be traced to that of the foundation.
The epistemological importance of such an exercise is evident, as is the fact that care would have to be taken to avoid circularity, to ensure that we did not make the credibility of a claim to knowledge depend upon beliefs that were not in the foundation.
This suggests, perhaps, that the need for a prior epistemology depends, not on the character of the general questions which epistemology starts from, but rather from the kinds of solutions which, in this case, are offered — namely reductive ones.
If we are sceptical that an evidential base with intrinsic credibility and a isolatable and justifiable conception of rationality can be found, then we might try to tackle the same problems non-reductively, in which case anti-naturalist scruples seem out of place.
This suspicion is supported by the remarks of many fallibilist and pragmatist philosophers about scepticism.
It is only if we are in the grip of a mistaken conception of truth that the mere possibility of a belief being mistaken carries with it the threat of scepticism; it is an indication that the sceptic's conception of truth is not ours that his arguments do not prompt an epistemic crisis.
There are many things that we are certain of and, although it might be useful to try to doubt these certainties, we should not pretend to doubt what we do not doubt in our hearts.
Consequently, unless we are convinced of the possibility of a reductive account of knowledge, let us not tie our hands by refusing to make use of facts that we know to be true.
This style of argument is frequently found in Peirce's writings, so we should expect him to make use of materials from psychology and biology in his writings on knowledge.
However, perhaps the most distinctive feature of his philosophical system, dictating its somewhat Gothic structure, is, from his earliest work, a total repudiation of naturalism, and a defence of epistemology (‘Grammar’ and ‘Logic’) as a prior philosophy.
In this section, I want to examine how this ‘contrite fallibilist’ rejected naturalism with a view to seeing more clearly the bearing of naturalistic matters on philosophical questions about knowledge.
The opposition to naturalism shows in his ‘classifications of the sciences’: mathematics, the only subject needing no foundation, grounds phenomenology, which in turn grounds the general theory of value which issues in logic and epistemology.
Finally comes metaphysics which provides a bridge between logic and the special sciences.
He was only clear about the  classification as he worked through the foundations of his pragmatism and metaphysics in the 1890s, but it represents the systematization of a set of views that he had been groping towards since the 1860s.
It is reflected in two themes that he stressed then, in work reported in the influential set of three papers appearing in the Journal of Speculative Philosophy in the late 1860s.
The material in these papers is prefigured in several sets of lectures delivered in Cambridge, Mass., from 1865 on.
The first of the themes I want to mention — a resolute opposition to the psychologism of Mill and others — is stressed at the beginning of each series of lectures.
He groups himself with Hamilton, Boole, Herbart, etc. in holding that logic does not depend upon psychology — it studies the ‘products of reasoning’(terms, propositions, arguments) directly, and employs an objective notion of validity defined in terms of an objective notion of truth.
Believing, at that time, that the source of psychologism was the idea that no objective treatment of the logic of ampliative inference was possible — together with the desire for a logic which treats all forms of inference in a systematic fashion — he set out to provide an appropriate definition of truth, and of validity, and thus to offer a systematic objective foundation for logic.
Of course, this does not rule out the use of naturalistic facts so long as an objective notion of validity is employed — the validity of inductive inference could turn on features of the context in which it is carried out, or the perceptual apparatus of the reasoner.
(Peirce remarks that unless logic deals with a fact of psychology it is a mathematical game!
But he secures the connection by subordinating psychology to logic.
In ascribing psychological states to people we rationalize their behaviour, employing the standards of rationality explained by logic (see 5.266 — 82).)
The second theme that interests me is implicit in the opening paragraphs of the third of the published papers, ‘Grounds of Validity of the Laws of Logic’.
Concerned specifically with the issue why there is no circularity involved in a justification of deduction which employs deductive reasoning, he contrasts explaining the validity of a form of inference — for which purpose we can use any of our knowledge — with persuading someone of the correctness of a rule — which requires that we avoid circularity.
Our deductive practice needs no such justificatory shoring up and cannot be revised by rational argument, hence there is no circularity in deploying it in the explanation.
As his work developed, he became increasingly clear that our means for discovering the truth about reality stood in need of a persuasive justification.
Of course, that epistemology faces problems of persuasive justification does not, by itself, entail that no use can be made of empirical beliefs.
Demands for persuasive justification arise in contexts where an agent is faced with a choice.
The context determines what materials we can use in justifying a choice and the standards to which appeal will be made.
Circularity results only if information is used in justifying the choice which is available only if the choice is resolved in one way rather than another.
Use of naturalistic information is wholly disallowed only if there can be some settlable choice of cognitive methods where all of our naturalistic information is available only if we resolve the choice in one of the possible ways.
This appears to involve a form of transcendental standpoint.
Why does Peirce think that we are faced with such a choice?
How can his robust commonsensical fallibilism be reconciled with his recognizing such a demand for justification?
The reconciliation lies in Peirce's view of ‘science’ and its relation to common sense.
He sometimes speaks of the scientific method as one of the possible methods of responding to doubt, sometimes as a ‘way of life’, but he always seems to see it as optional though supremely rational.
It represents a way of life, it seems, that has only really become possible in the nineteenth century, although Kepler may have approximated it.
A philosophical conservative, he would have sympathized with the view of his father, a defender of natural theology, that the life of science could be understood as a sacred duty.
The scientific method is not identified as a set of methodological rules or strategies.
Rather, to adopt the method involves accepting that there is a ‘reality’, which is as it is independently of what anyone may think of it, but which suitably organized inquiry is fated to discover eventually; around the early 1870s Peirce speaks of reality as the final cause of inquiry.
The life of science involves dedicating one's life to the discovery of the nature of reality.
In 1878–9, Peirce published a six-part work in the Popular Science Monthly , entitled Illustrations of the Logic of Science .
In the first part of this, ‘Fixation of Belief, he outlines the project of grounding logical rules in the ‘presupposition’ of scientific inquiry — that there is a discoverable reality — and in subsequent papers attempts to do just that.
Adopting the ‘method of science’ involves resolving to use whatever rules can be justified by reference to the hypothesis of reality.
The suggestion that the concept of reality, or truth, is optional is not immediately compelling.
And plainly Peirce does not deny that we have some common sense grasp of it; but he holds that taking it seriously transforms the character of inquiry.
Ordinary inquiry is directed at the efficient settlement of belief, so that error or ignorance will not interfere  with our practical concerns.
Forming the intention to adopt beliefs only when they correspond to reality postpones the settlement of belief: we are forced to adopt rules which, while they are guaranteed to reveal reality to us in the long run, are not guaranteed to do so in the short run.
The inquirer does not believe current scientific results, but simply looks upon them as the current stage on the route to a final description of reality.
(He may believe them qua practical man, but not qua scientist.)
The individual sees himself as contributing to an indefinite process of inquiry by an unlimited community of inquirers, and he may well not believe that he will be around to see inquiry converge on the truth (5.589, 2.652ff.).
Inquiry has no bearing on the practical concerns of life, and science should not be valued for any (unscientific) practical applications that may be made of its current resting places (1.616ff.).
Yet this life of contributing to the progress towards knowledge of reality is rational, is a — perhaps the — form of human flourishing.
The conflicts and inconsistencies present in our commonsense and other mere primitive concepts of inquiry are resolved in a procedure which exhibited a harmony between presumption, aim, and method; it can be pursued in a totally self-conscious, reflective, self-controlled and rational fashion.
If we accept Peirce's characterization of science, we must acknowledge a discontinuity between ordinary commonsense procedures of inquiry, which all settle belief in the short run, and the life of science, although, of course, the latter may affect the former.
The inquirer is distanced from current results, and sees himself, not as finding things out but as contributing to the cognitive progress of a wider community.
The tasks of Peirce's philosophical epistemology become clearer now.
He seeks to provide a substantive characterization of the concept of reality which will enable him to do two things: first, to show how certain rules and procedures are logically correct, how employment of them will suffice to guide us towards knowledge of reality; secondly, to show how it can be rational for a self-controlled agent to seek to contribute to knowledge of reality.
I do not intend to say much about how he proposes to do this, or about the background in nineteenth-century intellectual history which accounts for his finding the picture attractive.
That it would be out of place to use scientific discoveries in the course of the investigation ought to be clear: if science is optional, it would be improper to use results deriving from that science in justifying it; and, if scientific results are held, tentatively, at arm's length anyway, they are not certain beliefs that would be appropriate  for philosophical argument.
Philosophy, although observational, rests upon observations which are open to all — denied only through slips, ignored only because of their obviousness.
It rests upon ‘acritical’ observations and inferences, which are not subject to self-control nor guided by an understanding of the concept of reality.
Agreement on philosophical issues does not risk endless postponement, as does agreement in the special sciences.
The justifications of procedures of inquiry are of two distinct kinds.
For Peirce, statistical sampling is the fundamental kind of ampliative inference, and for this he derives its ‘validity’ from his understanding of reality — its repeated use will take us to the truth in the long run.
Other methodological issues cannot be solved in this straightforward fashion: reality is not wholly determinate, so there are statements with the property that no long-run agreement on their truth or falsity can be expected, so when are we justified in continuing inquiry into a question which may, for all we know, have no answer?
And, since there is no limit to the number of hypotheses that can fit a given body of data, what reason have we to suppose that we are capable of producing, and finding plausible, an hypothesis that is on the right lines?
With respect to the second of these issues, Peirce holds that it is rational to suppose that there is, in any particular case, an affinity between our sense of plausibility and the nature of reality.
This supposition has the form of a ‘regulative hope’— both are adopted on the same basis that a card player bases his play on the hope of an improbable distribution of the cards if no other possible distribution gives him any chance of winning at all.
But this takes out a philosophical loan that must be repaid in the post-philosophical sciences which explain the affinity in question.
Here naturalism seems to slip back in, at least in the attempt to explain how knowledge is possible for us.
In fact, Peirce's explanation is metaphysical, resting on his panpsychist objective idealism: he rejects the use of natural selection in the explanation because the fact that a faculty was necessary for the commonsense inquiries which facilitate survival and reproduction is no guarantee that it will help us to describe reality.
Science has no survival value, and we have to rely upon our sense of plausibility in areas remote from the vital concerns of everyday practice (7.219; Hardwick 1977: 2o).
I shall now draw some general morals from this sketch of the structure of  Peirce's philosophical thought.
It is useful to distinguish two elements in his view of knowledge and its philosophical elucidation.
First, there is a demand for a clear and complete self-consciousness about what we are doing when we form beliefs about the world — a philosophical account of knowledge provides a perspicuous account of knowledge which means that we pursue inquiries knowing what we are doing, knowing why it is rational to proceed as we do, and understanding how we can obtain knowledge.
The second element is the picture of serious inquiry as optional, and as something from the results of which the inquirer is distanced.
It is plausible to see the first point as spelling out the root problem for a philosophical account of knowledge, and the second as linked to Peirce's anti-naturalism.
Whether the view of science led Peirce to look for a prior epistemology, or whether the desire for, or availability of, a prior epistemology led him to think of inquiry as he did, are questions I shall not pursue here.
The conception of serious inquiry as optional is required, it seems, if the demand that inquiry be made self-conscious and rational is to call for justification rather than explanation.
And, as yet, we have seen no reason to suppose that the need for explanation calls for a prior epistemology.
In the next section, I pursue this point by looking at Quine's programme for naturalizing epistemology.
2.
NATURALIZED EPISTEMOLOGY
Quine appears to hold that, since, regrettably, the project of providing a reconstruction of human knowledge which traces the credibility of all justified beliefs to the operations of canons of rationality upon intrinsically credible perceptual beliefs has failed, the only remaining motivation for avoiding the use of naturalistic materials in epistemology is an unwarranted desire that our knowledge be shown to be certain.
For the fallibilist, epistemology ‘or something like it, simply falls into place as a chapter of psychology and hence of natural science’, it ‘goes on, though in a new setting, and a clarified status’.
It is notable that he does not see himself abandoning epistemology, but as continuing it, doing it better than it previously had been done.
It studies a human subject who   Transcendental reflection is not required for dealing with what Quine takes to be the traditional concerns of epistemology.
It is useful, tentatively, to distinguish two elements in Quine's thinking here: his commitment to naturalism and rejection of the need for epistemology to exercise ‘scruples about circularity’; and his list of acceptable sciences, which excludes all of the social and human sciences, cognitive psychology, etc.
It is not obvious that the grounds for his naturalism would evaporate if he took seriously a wider variety of forms of discourse.
I want to take seriously the suggestion that the sort of inquiry that Quine has in mind could be the heir to traditional epistemology — although I shall not restrict the concerns of the latter to studying the relation of evidence to theory.
I shall, therefore, pursue two issues.
First, can Quine be forced to acknowledge that questions arise which demand transcendental reflection — does his resistance to such issues flow naturally from his philosophy, or docs it require a stubborn closing of the mind?
Secondly, can a naturalized epistemology provide the perspicuous self-conscious understanding of inquiry which we saw, in the last section, to lie behind some traditional theorizing — can he cope with normative issues and questions of justification?
His refusal to take seriously the possibility of a confirmation theory, for example, can easily lead one to believe that he is closing his eyes to important normative issues.
Notoriously, positivists such as Carnap enunciated a principle of meaningfulness which banned transcendental reflection, but which could only be justified by the sort of argument which it debarred.
Wittgenstein, early and late, constantly strained with the temptation to discuss what could not be discussed.
One reading of Quine is that his practice conforms to the Wittgensteinian teaching: only questions that can be resolved arise for him.
If the reflective clarity about knowledge that we desire can be obtained, it can be obtained naturalistically.
His ‘robust realism’ results from the fact that he cannot attain the standpoint of transcendental reflection from which he can notice what we take to be idealist tendencies in his work.
If he could adopt that standpoint, he might describe himself as an empirical realist but a transcendental idealist; but, since he cannot adopt that standpoint, he is just a ‘realist’.
The ‘idealism’ shows in his verificationism, and in the indeterminacy of translation, but it never reaches a formulation.
Of course, this is a one-sided and distorting reading, but it provides a perspective from which some central Quinean themes hang together.
Peirce's first philosophy requires that we can formulate substantive conceptions of truth and reference without recourse to the special sciences.
Employing those conceptions, we can ask how we can know that we have succeeded in referring to anything, and how we can know that we have come up with an accurate description of what we have referred to.
Unless  we can think about truth and reference from the standpoint of transcendental reflection — unless we have substantive conceptions of those notions — the questions we raise are not intelligible.
For Quine, our substantive conceptions do not have this independence of the special sciences.
When we raise questions about truth and reality, we carry with us presumptions derived from the special sciences, and thus cannot achieve transcendental reflection.
As is well brought out in a recent paper, the views of Carnap's which Quine opposed involved the claim that conventionally adopted analytic linguistic frameworks provided criteria of reality, which set up the standards according to which any question that might arise was settlable (Ricketts 1982).
By denying that we could empirically identify the linguistic framework employed by other agents (or, indeed by ourselves), Quine challenged the claim that we can have a substantive prior conception of truth which can be used to formulate questions for transcendental reflection.
But let us look directly at Quine's discussions of transcendental arguments and transcendental reflection.
In a recent paper, Stroud argues that Quine can be forced to acknowledge questions which cannot be answered by his naturalized epistemology (Stroud 1981).
Stroud grants that if we want to know how a third person can know the nature of reality, we might carry out a psychological investigation of his methods of information processing, and then compare the results of his reasoning with known facts.
Naturalistic epistemology may suffice.
The problems arise when we shift to the first person, asking how we come to have knowledge of the world, and asking how we are justified in dismissing the possibility that reality is wholly other than we take it to be.
When faced with these questions, he urges, we must bracket our ‘scientific beliefs’, and naturalistic epistemology will be of no use.
We do not have a conception of how reality is with which to compare our opinions.
Quine's response is instructive, although it may at first seem dismissive.
I can speculate about how I know about reality, but no bracketing of current empirical certainties is required.
Before asking what feature of Quine's position is supposed to make this response possible, we should note Quine's response to familiar sceptical  arguments, for example, those that rest upon evidence for perceptual error and illusion, and upon empirical evidence of delusion.
He acknowledges the ‘Humean predicament’; both induction and the hypothetico-deductive method are fallible, so that any of the beliefs which result from them could turn out to be in error.
He also grants that a sceptic might ‘use science to repudiate science’; our entire theory of external objects might be overthrown.
Quine's response to someone who believes that there is evidence to warrant this extreme measure is that the sceptic is ‘overreacting’, manifesting bizarre or deviant patterns of entrenchment of beliefs, different standards of plausibility and evidence.
There is no evidence that he thinks they can be argued out of their scepticism, but, since they have convinced neither him nor us, this is no cause for concern.
We might try to explain their eccentric trait.
What is at issue in calling this use of the sceptical argument ‘scientific’?
The intended contrast is between repudiating science because we cannot understand how to bring it into harmony with an antecedently given substantive conception of reality, and rejecting our most general scientific theories because they are constantly surprised by experience.
Quite what it would be like to give up material bodies I shall not consider.
However, what is behind Quine's refusal to admit a prior substantive concept of reality — a corollary of his rejection of the a priori — is that our broadest conception of reality is derived from science; ‘science identifies and describes reality’ and without science we have no way to think or talk about it at all.
Raising questions about whether we are able to know the nature of reality carries with it the scientific baggage involved in our substantive conception of truth.
Once we attain the transcendental standpoint, we have ceased to carry with us the substantive concept of truth required to raise epistemological questions.
Thus Quine can make use of psychological facts to inform someone such as Stroud that all we look for in inquiry is accurate prediction and control of ‘triggerings of our sensory receptors’, and can show no interest in further sceptical possibilities.
As is clear from ‘Facts of the Matter’,(Quine 1977), Quine's substantive conception of reality is taken primarily from physics.
Physicalism is adopted as a regulative principle because physical objects provide the most familiar examples of ‘things’; physics is permitted to provide our standards  of reality or thinghood.
It is because motives, intentions and historical events lack the kind of clear robust reality displayed by electrons, fields and quarks that Quine spurns the intentional; the indeterminacy of translation shows up, not a special character of a distinctive form of discourse, but the extent to which the realm of the intentional fails to meet standards of reality set by physics and physical objects.
We may here respond that Quine is overreacting.
His general point may survive even if we do not share his preferences for physics, or even science.
This is that we do not have an a priori conception of reality which allows for a range of possible universes, empirical inquiry determining just which of those possibilities is realized.
Rather, science (and maybe even social science and other forms of discourse, such as that embodied in ordinary talk of mind and morality) is the source of the substantive conceptions of reality which must be referred to in discussions of the possibility of knowledge.
We cannot, as Peirce would hope, hold science at arm's length and attempt to justify its procedures — unless, that is, we can frame an a priori conception of reality of the sort Peirce offers.
We should now turn to the second question that I raised, whether Quinean epistemology is, in fact, sufficiently continuous with traditional Epistemology to provide the self-consciousness about his practice desired by the reflective inquirer (see Putnam 1982).
Will it enable him to justify all that needs to be justified, and to explain the remaining features of his practice?
Once the aims of inquiry are specified in psychological terms, there is no obstacle to empirical investigations of efficient or possible means to achieving those ends.
If the ‘ultimate aim’ is specified as the securing of predictive control over the future run of experience, then either this can be justified as providing a means to achieving further practical ends, or, if it stands in no need of justification, it may be possible to explain why people have it without the explanation undermining its appeal.
We may ask for an explanation of how our perceptual apparatus is a reliable source of information about reality, or how our instinctive judgements of plausibility or similarity can be useful in forming hypotheses or making inductions.
Psychological explanations may be to the point here, or speculations about the evolutionary history of the faculties in question.
It will be recalled that Peirce made no use of natural selection for similar purposes, and the difference between the projects of the two epistemologists will be clarified if we note why Quine can do so.
That our perceptual apparatus is reliable and our abductive sense is sound do not, for the Quinean position, need arguing; we have ample evidence that they have guided us to the truth on many occasions.
We simply want to understand how they do so.
In that case, there is no reason to expect a general form of explanation linking our cognitive faculties to the truth, explaining the reliability of our natural  abductive sense.
If natural selection is employed in the explanations, we do not need a general linking of natural selection to the discovery of truth.
Rather, we can characterize particular features of our perceptual apparatus (which we know to be the source of their reliability) and ask for an explanation of those features.
Elements of the abductive sense may be instinctive, and the result of selection, others the product of training: selection may explain both the innate quality space and the flexibility which, we know, make possible the training which explains our current (reliable) sense of plausibility (Quine 1969c).
So far as I can see, even a partisan of naturalistic epistemology is not committed to finding general characterizations or explanations of justification or reliability.
It is an empirical issue whether such general reductive explanations are available, and hence whether general notions of justification have any application.
This becomes clearer when Quine's scientism is relaxed to the extent of recognizing the autonomy of forms of discourse — for instance, concerning moral or cognitive justification — which are plainly not ‘scientific’, let alone acceptable to a Quinean ‘physicalist’.
Self-conscious clarity about our epistemic methods, in that case, would require a perspicuous representation of the characteristics of the forms of discourse in question.
This does not prevent our wanting such reflective self-understanding also to involve understanding how our faculties can, in particular cases, lead us to the truth about an empirical subject matter; and naturalistic information has its place in meeting this demand.
So understood, Quine is unsympathetic to the Peircean doctrine that we hold our scientific ‘opinions’ at arm's length: at least some of our scientific beliefs must be fully accepted.
In fact, we may both be surprised that Peirce held that the scientific inquirer distanced himself from all scientific results, and suspect that Quine would grant that this attitude is appropriate for a range of them.
Peirce could probably allow the same: his position rests upon the belief that there is a logical guarantee that induction will take us to the truth in the long run, but that our confidence in the short-run efficacy of the method is simply an ‘acritical’ commonsense certainty which may be susceptible to scientific explanation.
Trusting induction in the short run cannot receive a philosophical justification.
However, there is a question about what a naturalized epistemology can offer by way of an explanation or justification of the contributions to knowledge that we make by contributing to an ongoing process of inquiry: we want to know, in such cases, not how we have arrived at the truth, but how what we do can be understood as contributing to the fact that someone else (or perhaps ourselves) can arrive at the truth at some time in the future.
In the next section, I offer some brief and critical remarks about a recent attempt to provide a general  descriptive account of how knowledge develops which would say something to that issue.
3.
EVOLUTIONARY EPISTEMOLOGY
The application of evolutionary ideas to the study of knowledge has recently been urged by a number of people, but I shall only discuss Campbell's evolutionary epistemology (Campbell 1974).
I shall concentrate upon the claim that there is a fruitful analogy between the development of species through natural selection and the growth of knowledge.
There are other elements in Campbell's thought: he seems to think that the most perspicuous model of the mind to employ in understanding human knowledge sees the mind as embodying a structured hierarchy of functionally characterized capacities, each of which works in a way analogous to natural selection; and he defends his view as part of a (reductive) explanation of how the capacities comprising human rationality have evolved.
Underlying the analogy is the thought that natural selection can itself be understood as a process whereby information is selected and transmitted, it applies a simple logical model to a natural phenomenon.
More important, natural selection provides a model of how there can be change towards a kind of fit or harmony, towards structured complexity, which is neither planned nor the result of goal-directed activity.
Variations occur within a population, explicable as genetic mutations or the results of mixing of genetic material.
Variations which are not suited to their niche tend not to survive to reproduce, and the characters of those that are successful are transmitted genetically to their offspring.
Traits which are advantageous tend to survive and spread in the population, those which are not, do not.
Neither in the production of variations nor in the elimination of disadvantageous variations is there any reference to an ‘end’ of producing ‘fit’or successful species: the probability of variations occurring, or of a particular variation occurring, is independent of the need for change to produce a better ‘fit’between organism and environment, or of that variation being successful.
The general claim that such a variation is disadvantageous has no role in the explanation of the fact that the individual displaying it fails to reproduce; selection does not involve ‘criteria’ which refer to the endpoint of the process.
The application of this picture to the growth of knowledge seems straightforward.
Thagard expresses it neatly:
In keeping with the analogy, Campbell holds that the variations are ‘blind’— that we come up with a particular hypothesis is not determined by our current experience, is independent of whether the hypothesis is in fact going to prove true or successful, and is not a correction of previous unsuccessful hypotheses.
Similarly, he must hold that theories are eliminated due to the clash of experience (for example), but not using criteria that are understood in terms of , or justified with reference to , the aims of inquiry.
Unless such points are maintained, the analogy becomes so strained that little that is distinctive about natural selection remains.
However, in that case, it does seem that the theory is straightforwardly false.
In proposing hypotheses, people are guided by knowledge of past failures, by analogies with theories that have been successful in handling related subject matters; Peirce's ‘affinity’ between mind and nature is an allusion to something which is required to explain the speed of the growth of knowledge; and testing theories is an intentional activity which appeals to general cognitive aims — to describe reality, anticipate experience, solve problems, produce elegant and simple formalisms, etc.
Thagard's excellent hatchet job spells out many more points of disanalogy.
If we allow that the analogy does not provide a useful general descriptive or explanatory account of the growth of knowledge, we may be surprised that it was found plausible at all.
Presumably it is an overreaction to a number of points that we have noted: that no logical guarantee of the soundness of our abductive sense can be provided; that our substantive conception of reality or of the aim of inquiry may itself be revisable; that while experience might lead us to abandon a theory with some confidence, proper acceptance should be postponed to the long run — true theories are survivors.
It may reflect the thought that although we do not aim at the ‘truth’— i.e. our immediate aims are for simple solutions to empirical problems — nevertheless, the truth is what we end up with.
Thus, even if inquiry is a goal-directed activity, the goals in terms of which it is directed do not represent a substantive conception of the ‘end’ to which the process tends.
But, while these points may be reasonable, and some of them may be true, this attempt to embed them in a general theory or schema seems unhelpful.
Looking backwards over the history of inquiry, from the security of our current views, we may see evidence of the cunning of reason bringing us gradually closer to the approximately true, but from that perspective we can offer descriptions and explanations of what was going on which show, for instance, that hypothesis selection was not ‘blind’.
Looking forward, and viewing ourselves as contributing to the development of provisionally held theories, it seems to offer no more than the hope that we might make a  contributions even if we do not really understand what, how, or to what.
As a theory, it has little to contribute to our reflective self-understanding of ourselves as agents of inquiry.
THE EVOLUTION OF ANIMAL INTELLIGENCE John Maynard Smith
The last twenty years has seen a major effort, theoretical and observational to understand the evolution of animal societies.
In this essay, I first discuss what level of intelligence is assumed in our theories, and what level is revealed by our observations.
I then ask what qualitative differences exist between animal and human societies, and in what ways these differences depend on human intelligence.
The two leading concepts in the analysis of animal societies are kin selection (Hamilton 1964) and evolutionary game theory (Maynard Smith & price 1973).
The central idea of kin selection is that a gene A, causing an animal to be more likely to perform an act X, may increase in frequency in a population even if act X reduces the individual fitness (expected number of offspring) of the animal itself, provided that the act increases the fitness of animals related to the actor.
Following Haldane (1932), biologists refer to such acts as ‘altruistic’.
It may require considerable skill to calculate in just what circumstances particular acts will evolve.
This has led some people to commit what may conveniently be called ‘Sahlins’ fallacy' (Sahlins 1976), and to suppose that the operation of kin selection requires that animals, or people, are able to perform the necessary calculations.
If this were so, kin selection could operate only in species of high intelligence.
But it is not so.
One of the clearest examples of kin selection occurs in a bacterial plasmid (Maynard Smith 1978); despite misunderstandings of the phrase ‘selfish gene’, no one supposes that plasmids think.
If, for example, members of a particular species have neighbours related to them by r = 1/13, say, then an act X which reduces an individual's fitness by 1 unit, and increases that of a neighbour by 14 units, will be favoured by selection.
There is no need for the animal to calculate r .
Of course, if an animal could distinguish close relatives from distant ones, then selection would favour a gene causing altruistic acts to be directed preferentially towards the former.
Can animals do this? parents certainly care for their own offspring: is there evidence of preferential care for other relatives?
There are several possible mechanisms.
First, an animal might direct altruistic acts towards individuals with which it had been raised; in  most cases this would have the effect of directing the acts towards relatives.
Second, an animal might direct altruistic acts towards genetic relatives of those with which it was raised.
This is known to happen in bees (Greenberg 1979) and isopods (Linsenmair 1972); it requires surprising powers of discrimination, and some memory.
A third possibility is that an animal might be able to recognise its own genetic relatives.
There is evidence that Rhesus monkeys in captivity can do this (Wu et al .
1980).
Animals do behave differently towards different conspecifics, both in cooperative interactions and in mate selection (Bateson 1980), and the criteria used in discrimination are correlated with actual genetic relationship.
There is, however, no reason to suppose that animals have a concept of genetic relationship.
In contrast, we do have such a concept.
Some anthropologists (notably Dickemann 1979) have interpreted human societies on the assumption that people act so as to maximise their inclusive fitness; i.e. that they behave as predicted by kin selection theory, with the added assumption that individuals know, at least approximately, their degree of relatedness to other members of their society.
I know of no very explicit discussion of how this could come about, in evolution or in individual development.
The hypothesis appears to be that we have inherited from our animal ancestors the habit of discrimination, but have added an additional criterion, namely the conscious calculation of relatedness, to the criteria of propinquity, and perhaps physical and biochemical similarity, used by animals.
I turn now to evolutionary game theory.
I first describe the basic ideas, to bring out the conceptual differences between classical and evolutionary game theory.
I then discuss where the boundary may lie between games animals play and those that only people can play.
Imagine two animals fighting over some resource.
Two ‘pure’ strategies are available to them — an aggressive strategy, Hawk (H), and a less risky one, Dove (D).
The ‘payoff matrix’ might then be 
In evolutionary game theory, we imagine a population of animals pairing off at random and playing this game.
Each animal has a strategy — pure H or pure D, or a ‘mixed’ strategy, ‘play H with probability p and D with probability 1 —p ’.
After playing each animal reproduces its kind, and dies; the number of offspring produced is equal to some initial constants, say + 10, modified by the payoff received.
Thus a Hawk which met a Hawk would produce 8 offspring, and one which met a Dove 12 offspring.
The population will thus evolve, and the relative frequencies of different  strategies will change.
The payoffs are interpreted as changes in fitness arising from the contest.
Evolutionary game theory is concerned with the trajectories of evolutionary change, and in particular in finding an ‘Evolutionarily Stable Strategy’, or ESS.
An ESS is a strategy such that, if all members of a population adopt it, no alternative, ‘mutant’, strategy can invade the population.
Applying this idea to the Hawk-Dove game, it is clear that H is not an ESS, because a population of Hawks would average — 2 per contest, whereas a Dove mutant would average o.
Similarly, Dove is not an ESS.
It turns out that the mixed strategy, ‘play H with probability 1/3; play D with probability 2/3’ is the only ESS of the matrix shown.
A population of individuals adopting this strategy could not be invaded by any mutant.
If only the pure strategists, H and D, were present, the population would evolve to a ‘genetically polymorphic’ equilibrium consisting of 1/3 H and 2/3 D.
Evolutionary game theory is a way of thinking about the evolution of phenotypes when fitnesses are frequency-dependent; i.e. when the best thing to do depends on what others are doing.
It does not require that contests be pairwise, and is not confined to fighting behaviour; it has been applied to the evolution of the sex ratio, of dispersal, of growth strategies in plants, and so on.
As I see it, the differences between classical and evolutionary game theory concern two main points: the meaning of a ‘payoff’, and the contrast between dynamics and rationality, and, arising from this, the meaning of a ‘solution’.
Both forms of game theory require that the possible outcome for a given player be ranked on a linear scale.
In the evolutionary version, the payoffs are changes in fitness; hence, although they may be difficult to measure, they do fall naturally on a linear scale.
In the classical version, some difficulty arises in arranging a set of qualitatively different outcomes — e.g. loss of money, reputation, or life — on a single scale.
The justification for a scale of ‘utility’ is that any two outcomes can always be ranked, because an individual must always have a preference between them.
I will leave to others how far this solves the difficulty; my point is that no comparable difficulty arises in evolutionary game theory.
More fundamental is the fact that evolutionary game theory is based on a well-defined dynamics — the evolution of the population — and the ‘solutions’ of the game are the stable stationary points of the dynamics.
In contrast, classical game theory supposes rational players, and seeks a solution in terms of how such players would behave.
Despite this conceptual difference, however, there is a close similarity between an ESS and a ‘Nash equilibrium’, which is the central equilibrium concept in classical game theory.
In a two-person game, if player 1 adopts strategy A and player  2 adopts B this constitutes a Nash equilibrium if neither player would gain by changing his strategy, so long as his opponent sticks to his.
An ESS differs in two respects.
First, an ESS of a ‘symmetric’ game such as the Hawk-Dove game (i.e. a game in which there is no external asymmetry conferring different roles on the two players) requires that both players adopt the same strategy.
Thus ‘player 1 plays H, player 2 plays D’ is a Nash equilibrium of the Hawk-Dove game, but it is not an ESS because, in the symmetric case, there is no way of distinguishing the players.
Second, the definition of an ESS contains a criterion for the stability of the equilibrium which is missing from the definition of Nash equilibrium.
Since ESS's arise from a dynamics, there is no assumption of rationality any more than there is in the case of kin selection.
However, some games (or, more precisely, some strategies) do require intelligence to play.
I now describe three games — the repeated prisoner's dilemma, the queuing game, and the social contract game — which are played both by men and animals, but in which the strategies available to men are more extensive than those available to animals.
An example of the prisoner's dilemma game is as follows:
The game is paradoxical for the following reason.
No matter what player 2 does, it pays player 1 to defect.
It also pays player 2 to defect.
So, rationally, both should defect, yet both would be better off if they cooperated.
Not surprisingly, the only ESS of the game is Defect.
Suppose, however, that the game was played between the same two opponents ten times.
Consider two strategies, Defect and Tit-for-Tat (TFT)— i.e. cooperate in the first game, and subsequently do as your opponent did last time.
The payoff matrix then becomes 
It is clear that TFT is an ESS; TFT strategists, when playing each other, get the benefits of cooperation.
However, Defect is also an ESS.
Hence there is a problem of how cooperation could evolve in the first place, although it would be stable once it had evolved.
In practice, the early stages probably require the operation of kin selection.
Trivers (1971) used essentially this argument to account for the evolution of ‘reciprocal altruism’, in which  animals cooperate only with those that cooperate with them.
More recently, Axelrod and Hamilton (1981) have shown (for a slightly altered model) that Tit-for-Tat is stable against any alternative strategy, and not just against Defect.
Trivers imagined that reciprocal altruism would evolve in species capable of recognising individuals and remembering how they behave, and of behaving differently towards different partners.
He did not have to suppose that an animal could foresee the consequences of its behaviour; still less did he have to suppose that an animal could imagine what it would do in its partner's place.
Packer (1977) has shown that baboons are capable of reciprocal altruism.
Baboons have a gesture for soliciting help from others: packer showed that those individuals which most frequently responded to solicitations from others are also those most likely to receive help when they solicit.
Axelrod and Hamilton point out that reciprocal altruism could evolve without the need for individual recognition in a sessile organism; in principle, it could evolve in a plant.
Thus, imagine a sessile species in which each individual has only one neighbour.
Then the strategy ‘cooperate with your neighbour if he cooperates; otherwise defect’ would be an ESS, although I have some difficulty in seeing how it would evolve in the first place.
To summarise on the prisoner's dilemma, we must distinguish between sessile and mobile animals.
For the former, since they play against only one or a few opponents, there is no need for learning; the genetically-determined strategy ‘cooperate with your neighbour if he cooperates; otherwise defect’ can be stable.
For reciprocal altruism in mobile animals, as demonstrated by packer in baboons, more is needed.
Since each baboon interacts with many others, and since there may be a long delay between action and reciprocation, stability requires that a baboon should recognise individuals, and remember how each has behaved, or, at the very least, associate with each individual a positive or negative sign, depending on how it has behaved.
We are not, however, forced to suppose that a baboon reasons, as a man would, that it will pay to be nice to X, because X is likely to reciprocate.
A queue is a sequence of individuals, arranged according to time of arrival, and not according to size or strength, such that the first in the queue has prior access to some resource.
Wiley (1981) reports that striped wrens form ‘age queues’, with the oldest male, at the head of the queue, taking over a breeding territory when the incumbent dies; such queues may be commoner than we have thought.
The stability problem is clear.
If a larger bird is low in the queue, why does it not displace the bird at the head?
We do not know the answer, although there are several possibilities.
My reason for mentioning queues, however, is that there are ways in  which a human queue might be stabilised which are unlikely to operate among animals.
Some possibilities are as follows:
(i)
A man stays in line because he has been taught that it is wrong not to.
(ii)
A man who jumps the queue will acquire a reputation which will damage him in later social contacts.
(iii)
A man who attempts to jump the queue will be restrained by the police.
(iv)
Any attempt to jump the queue will be resisted by all other members.
The first three of these possibilities explain stability by events external to the queue itself.
If we take reciprocal altruism seriously, method (ii) might conceivably operate in animals, although not in the case of age queues; methods (i) and (iii) could not.
Method (iv) is the most interesting.
Clearly, collective resistance could stabilise the queue; the problem is why individuals should join in collective resistance.
Let us consider animals first.
It is conceivable that collective resistance would be individually advantageous.
For example, suppose that, in the queue α — Β — γ, α is challenged by γ.
It might pay Β to help α to beat off the challenge, because if he does not the sequence might become γ — α — Β, and Β has dropped from second to third.
(It is harder to see why γ should help α to resist a challenge by Β) If the alternative strategies are ‘resist only challenges to oneself, and ‘resist any challenge’, the latter would be favoured by selection provided that it was usually advantageous.
It would not have to be advantageous in every possible case.
Thus I can imagine that animal queues are stabilised by collective resistance, without having to suppose that individual animals perform complex calculations.
However, there is no evidence that this is what happens.
In human beings, some individuals might perceive that it would be in the general interest if queue jumping was prevented, particularly if a complete breakdown of the queue made the resource unavailable (if the driver sees a fight at a bus stop, he doesn't stop).
If so, they might persuade the queue members to bind themselves to wait in line, and to punish transgressors.
Note that this is possible even if there are a minority of individuals (perhaps the strongest person in the queue) who do not benefit.
This brings me to my third game, the ‘social contract’ game.
Suppose that the payoff to all the members of a small group is greater if all cooperate than if all defect.
Then the members might agree to cooperate, and to join in punishing any member who defects.
Even if the act of punishing was cheap, and of being punished expensive, this would still not be sufficient to guarantee stability, because of the problem of the ‘free rider’.
Thus a member who cooperated, but did not join in punishing, would be better off than someone who cooperated and did join in punishing.
Hence a social contract can ensure stable cooperation only if it reads ‘I will  cooperate; I will join in punishing any defection; I will treat any member who does not join in punishing as a defector.’
Could an analogous behaviour occur in animals?
If, for some specific action X (e.g. jumping an age queue), animals (i) did not do X,(ii) drove out of the group any animal that did X, and (iii) drove out any animal that did not join in driving out an X-doer, then the behaviour would be evolutionarily stable.
The difficulty, of course, lies in imagining how such a complex behavioural syndrome, which is stable only when complete, could arise in the first place.
This illustrates Elster's (1979) point that natural selection is a hill-climbing process which can only reach local optima, whereas rational behaviour can reach a global optimum.
(Elster is, however, wrong in thinking that natural selection cannot reach the mixed ESS of the Hawk-Dove game.)
Despite the difficulty of imagining how a behaviour involving the three components outlined in the last paragraph could arise in the first place, I think it is quite possible that the explanation of stable age queues in animals may be of this kind.
A similar mechanism may perhaps account for the fact that some group-living animals drive sick or injured individuals out of the group.
To drive out a sick individual is sometimes advantageous, because the sickness may be infectious.
If, in the above specification, we replace ‘doing X’ by ‘being different from typical members of the group’, we have a mechanism that will explain this behaviour.
The crucial difference between men and animals, then, lies in the nature of the action X which is proscribed by the contract.
In animals, X would have to be genetically specified, although it might be specified merely as being different, in any way, from other members of the species.
In man, X could be a newly acquired possibility (e.g. human cloning or hang-gliding), perceived by some individuals as being socially undesirable, the perception being communicated to others linguistically.
Even language may not be enough to account for the agreement in a social contract not to do X, when
X is not genetically specified.
Thus, suppose that a member of the group recognises that he or she would be better off if no one did X. Before that member would embark on an attempt to persuade others, he would have to recognise that others might feel about X as he did.
To play the social contract game successfully, therefore, when the prescriptions of the contract are culturally rather than genetically specified, an animal would have to think of others as having motivations similar to its own, so that it could foresee their future behaviour, and it would have to communicate symbolically.
The game is therefore useful in illustrating the kinds of strategies animals cannot adopt.
However, I do not think it is a particularly appropriate model of human social interactions, for two reasons.
First, it treats all members of a social group as having the same set  of possible actions.
In fact, owners of land or factories can do things non-owners cannot, as can men with weapons compared to those without, or even men as compared to women.
Hence social contracts may bind, not all members of a society, but members of some group within society.
The problem is to explain how individuals come to identify their interest with that of a specific group, and why different societies tend to divide into groups along different lines, according to economic class, religion, race, etc.
The other weakness of the social contract model lies in its excessively rational and legalistic nature.
In practice, I suspect that ‘contracts’ are arrived at as much by religious and ideological persuasion as by rational discourse, and maintained more by the threat of social ostracism than by legal restraint.
The ideological nature of social contracts means that they need not always correspond to individual self-interest.
However, they cannot depart too far from it; men can be swayed by beliefs, but not too far.
Of course, one group in society may be able to impose its will on another.
Returning to my brief of animal intelligence, animals can and do act as members of a group against other conspecific groups.
The groups are usually composed of genetic kin, but not always.
The cohesion of the group may be cemented by joint display activities, as in the cacophony of a troop of howler monkeys.
However, I would not claim that these displays have any culturally mediated symbolic meaning, as do the myths and rituals which bind human groups.
Perhaps the main consequences of the lack of high intelligence in animals is that they are not as good as we are at fooling themselves.
INTENTIONALLY, SYNTACTIC STRUCTURE AND THE EVOLUTION OF LANGUAGE Neil Tennant
INTRODUCTION
I beg my reader's forbearance for beginning my own paper with such a long quotation from another.
It happens to mention almost every topic I shall be discussing here.
I hope to show that the phenomenon of recursive syntax may bear on the intentional ingredient in human communication.
I hope also to say something useful about the structure of primate brains as well as the mechanisms of linguistic evolution, both issues that Searle sets to one  side as irrelevant to the conceptual question he is pressing.
This paper accordingly falls into two not wholly unconnected parts — one philosophical, the other biological.
Language has obviously evolved.
But it has left no fossil record.
Deficiency of sufficiently diachronic facts ironically testifies to the tremendous selective advantage language must have conferred upon its users.
Evolutionary theorising is a matter of making inferences to the best explanation.
Today such inferences about the origins of language can draw on a vast assemblage of data and hypotheses in neighbouring disciplines.
Those who belittle the value of new integrative speculation are, in a phrase of Bennett's, dogmatically defeatist.
To be sure, the Linguistic Society of Paris in 1860 banned publication of the theorising about the origins of language then in vogue.
But in the century since we have come to understand better the structure of natural language, and have made some conceptual progress in the philosophy of language.
We have learned much about our own brains and vocal tracts, and those of our primate cousins.
Ethologists have studied many different systems of animal communication.
Darwinian theory has been synthesised with modern genetics.
We have attended closely to how children learn to speak.
In the light of all this, I do not think it premature to put forward an evolutionary scenario for natural language.
I
It is a philosophical commonplace that we cannot learn very much about human language from the singing of birds or the dancing of bees.
Systems of animal ‘communication’ are held to be mere signalling systems, systems for which there is some doubt as to the meaningfulness of the messages contained therein as opposed to their characteristic causal efficacy in evoking or triggering certain responses.
The dance of the bees has a repertoire of wiggles and tilts and speeds.
Certain features of their performance are found to be correlated with and hence to convey information about direction and quality and distance of food source.
But do the movements ‘convey information’ to fellow workers in the hive in any more semantic a fashion than, say, a bruise on a child's face conveys to his mother the information that he has had a nasty bump?
At first sight, it is unclear what the answer ought to be.
The bruise is a direct causal consequence of the bump.
Knowledge of human physiology would enable witnesses of the bump to predict the appearance of the bruise.
Similarly, detailed knowledge of bee physiology should enable one to predict that the bees' locating a food source will have as a causal consequence (even if only statistically) the subsequent pattern of wiggles and tilts  in the hive.
For bee dance is inflexible, and can be manipulated genetically (cf.
T. Eisner & E. O. Wilson 1977: 241).
So is it just a matter of degree?
Can one argue that the bees' dance is as much a causal consequence of this earlier experience as the bruise was of the bump?
Compare Armstrong:
If he is correct, then the ‘semantic’ status of apine dance, if we suppose the bruise to lack such status, is to be recovered from certain distinctive features of the underlying causal story.
Indeed this is a challenge that even the materialist should be willing to accept.
He can begin to analyse the distinctive features of communicative interactions while still using the language of the mentalist.
That is, he can attribute beliefs, desires, intentions, and so forth in giving a simpler account of a complex matter.
The complexity of the account will increase vastly when replaced by ultimate ‘physical’ talk, if this indeed be possible.
Nevertheless, the complexity imparted by the faithful physicalist version should not blind us to the features that distinguish the causal processes in the bees' brains or whatever from those involved in the swelling after the swipe.
The difficulty lies precisely in spelling out what these distinctive features are, and how they are pertinent to the contrast between conveying information on the one hand, and, on the other, being black and blue.
It may turn out that the distinctive features of a causal story associated with semantic content are precisely those that require the language of belief, desire and intention, in order for their isolation to have any point within the overall causal explanation of what happens when the speaker communicates with another.
Ironically, too, it could turn out that much less goes on physically when John sees Mary and tells Dick about it, than when John gives Mary a black eye that tells Dick of his blow.
So what is there to linguistic meaning, over and above mere causation?
One might offer the following reason why one could regard apine dance as having communicative significance lacking in the case of the bruise; that the dance is a somehow ‘arbitrary’ or ‘rule-governed’causal product of the sighting of food.
It is arbitrary in that the rules (consciously followed or not) in accordance with which this behaviour is produced,could have been otherwise , while yet serving the same communicative purpose.
That is, as a piece of adaptive behaviour, whether wholly instinctual or partly learned, it may very well now follow as a causal consequence of the sighting; but that precisely this sort of dance should have come to serve this purpose is, in a  phylogenetic perspective, quite accidental.
Selective pressures for a system of communication of this kind might have produced, through different mutations for new behaviour, a different dance or indeed wholly different behaviour types, serving the same function.
In fact, another species of bee that communicates about food sources (Apis florica ) does so in the same way except in so far as it wiggles in the horizontal rather than the vertical plane, and so can indicate directly the angle of the food source from the sun.
Apis mellifera (the common bee) has to have a convention as to which angle in the vertical plane corresponds to this latter angle.
Apis mellifera , if put on a horizontal honeycomb, dances just like Apis florica .
This ‘conventional’ behaviour is described with scare quotes because it is not clear that one would be justified in reading into it the constellation of reciprocal beliefs and intentions that someone like Lewis takes as constitutive of conventional behaviour.
The bees' behaviour has developed phylogenetically and of course may now be regarded as a more or less necessary causal consequence of their finding food.
The analysis is to hold whether or not the striking similarity between the dances of the two species is the result of their common descent from a dancing ancestral species, or the result of this sort of dance's being an optimal solution to the ‘evolutionary problem’ of efficient communal food use.
With enough genetic mutations at hand, the behaviour could perhaps have evolved independently in each species.
This phytogenetic perspective appears to separate nicely what is essential in the dance to its being a signal or message of some kind.
The dance constitutes a rudimentary system of communication, whether or not it is entirely instinctual, entirely learned or the result of a mixture of the two strategies.
But to see communicative significance as deriving from the phylogenetically arbitrary status of apine dance, as a solution to their co-ordination problem, is open to a serious objection.
This is that the dance might have been the only way, given apine physiology immediately before its evolutionary debut, to solve that problem.
If a certain behaviour pattern were, in some suitable sense, phylogenetically necessary, what would justify one in attributing content to it?
Now although in evolutionary terms, given the amounts of genetic variability usually at hand, it is likely that such behaviour has been arbitrary in the required sense (witness Apis mellifera v.
Apis florica ), one must concede the conceptual point here.
Armstrong even goes as far as asking ‘is it not conceivable that the whole of syntax and semantics should have been innate so that all mankind spoke the one, wired-in, nonconventional language?’(1971: 437).
What, then, justifies the attribution of content to exercises of evolved forms of behaviour?
A promising line of thought, due to McDowell, is that  all one requires is transfer of information simpliciter.
This holds regardless of whether the mechanism, behavioural, chemical or otherwise, might have evolved arbitrarily or might have been the only possible one phylogenetically.
The natural function of this communicative behaviour, whether wholly instinctual or not, is to impart to fellow creatures ‘cognitive stand-ins for the states of affairs they represent’.
Compare now the case of the bruise.
Bruising, it is reasonable to suppose, has never served the adaptive function of informing others that the body has been hurt.
It is rather the kind of causal concomitant of the blow that has wholly to do with internal processes of tissue regeneration.
So far, so good; we have separated bees from bruises.
What about us?
Is not a belief/intention ingredient essential to the notion of linguistic communication in the human case?
Certainly the Gricean analysis would have it so.
Grice himself might assimilate apine dance to the category of phenomena that have what he called natural meaning as opposed to the non-natural meaning of expressions of natural language.
The question that McDowell is pointing to, however, is whether, in the analysis of human communication, Grice's contribution concerns more what it is to be human than what it is to communicate.
Let us for the time being not take a stand on this issue but address ourselves to the whole phenomenon in its strong sense.
Whether a system with finitely many basic signals is a system of communication in the strong sense — that is, a linguistic system — depends on whether the characteristic behaviour within which and against which it  is interpreted is complex enough to sustain attributions of higher-order beliefs and intentions to the creatures concerned.
If one has to resort to such an intentional framework in order to make sense of the behaviour, in order to explain what the creatures are doing and why, and has so to resort after exhausting all possibilities of more austere, less anthropomorphic frameworks of explanation, then one is on the route to linguistic meanings.
Wittgenstein invites one to consider a primitive signalling system as the system of communication of a whole tribe.
We can  acquiesce in this thought experiment only because we implicitly assume that enough behavioural complexity will be discovered for us to regard tribe members as having beliefs and intentions, even if,ex hypothesi , their very simple language does not permit them to express these beliefs and intentions.
Provided a tribe member can recognise an intention that he should respond appropriately to a signal in the simple system, we are happy to regard the signal as linguistically meaningful to him, rather than simply causing or triggering certain behaviour — as was the case with the bees.
When radically interpreting the speech of a totally alien tribe (or species), one has to consider all the evidence there is concerning mental states.
Their attributions go hand in hand with, and must be adjusted to, our attributions of meaning to utterances.
One requires a background of considerable behavioural complexity before one is justified in attributing to any creature beliefs, intentions and so on.
Davidson (1975) would even maintain that this requires that they have mastery of a language; that it is wrong even to think in terms of quasi-beliefs or proto-beliefs when trying to explain even quite complex behaviour of languageless creatures.
This insistence, however, does not mesh well with an attempted evolutionary account of language, which it is my concern to sketch.
It appears unduly insistent on the primacy of certain conceptual connections between cognition and language.
Nevertheless, it is easy to underestimate just how complex behaviour has to be before attributions of attitude and meaning pass methodological muster.
Davidson rightly enjoins the radical interpreter to be nasty in thinking up as many competing interpretations of observed behaviour as possible.
A good example of how nasty one can be is to be had from the  chimpanzee language experiments.
A scholarly audience will forgive this quote from The Listener :
Let us be nasty towards Washoe.
The word ‘more’ has its meaning disclosed through combination with terms telling one what it is of which ‘more’is in question.
No doubt the word as used by infants will occur on its own, without the added term, and adults satisfying the need will work out the missing term from context.
But adults respond linguistically too, supplying the missing substantive with the wanted substance.
The child in due course in new contexts uses the modifier with substantives, thereby expressing his desires more clearly.
The process of learning words, learning their grammatical categories and acquiring them in correct combinations is very much a two-way affair.
It involves requests, solicitous enquiries, offerings, forbiddings and factual commentary.
But does Washoe use the hand signal, allegedly meaning ‘more’, in this way?
Does she offer more of things to her interlocutors when ‘asked’, as well as demand things from them?
Does she perform several different speech acts with the word, questioning, commanding, wishing, stating?
Watching the film of Washoe with Leakey's commentary, the nasty radical interpreter is struck by any number of competing interpretations of Washoe's signing.
Why not interpret her as expressing something tantamount to ‘I have an unfulfilled desire’, or, ‘make me feel nice!’?
Either of these — quite unconnected with ‘more’ of anything — would cohere with the facts just as well as the ‘more’hypothesis and explain the same range of behaviour.
If Washoe has an unfulfilled desire, one can rest assured that the eager and attentive experimenters will gather from context what it is and fulfil it if at all possible.
It is not unlikely either that the formation of the desire will be closely linked with current experiences and will therefore probably be fulfilled by providing more of whatever is salient in the context — porridge, or rides in a plastic tub, etc.
Likewise the hand sign might mean ‘continue’ or ‘would that things remain roughly as they are now, in lumps or in intervals.
We have so far been considering the extent to which the intentional ingredient in natural language distinguishes it from other systems of animal communication.
Another point that some have raised in this connection is that animals cannot — or do not — lie when using their communication systems.
The force of this objection depends, again, on the complexity of the attendant beliefs and intentions which analysis demands in the case of lying.
Let us disregard the moral overtones which are irrelevant anyway to  the concept of lying (what makes lying bad is what is bad about what makes it lying).
In communicating that ‘P’, one is lying just in case one believes ‘not P’ but intends one's audience to believe ‘p’.
A communicative act is a lie if by ascribing the appropriate belief and intention to produce in the audience the opposite belief, one achieves the best explanation — within one's theory of the agent's beliefs, desires, etc. — of his act.
Consider now the case of a bird ‘warning’ its flock members near a food source that a predator is nearby.
The other birds fly away leaving the warner with a temporary monopoly of the food.
Has this bird lied?
We must first enquire whether the bird believed there was no predator.
That he did not himself fly away is strong evidence for this belief, especially if predators prefer to attack lone birds, and we have further reason for thinking that the bird believes that .
Furthermore, we could investigate whether he behaves likewise when there is no food source to be monopolised.
Secondly, we must ask whether he intended his fellows to believe that there was a predator nearby.
To test for this intention we would have to establish whether he would act as he did in situations where he had no reason to believe that he could thereby induce the false belief in question.
But this is almost impossible to do, even by recourse to highly contrived situations not encountered in the wild.
To this extent, the status of the call as a lie is under-determined.
To the biologist, it may seem unnecessarily convoluted to eschew the simple hypotheses that the bird is lying, given its useful role in making broad predictions about the future.
And at least one philosopher (Routley 1981) is prepared to defend them in this view.
But for the radical interpreter, this would be to throw in the towel at a crucial conceptual conjuncture, imputing our intentional framework to another, dumb species.
Yet it is arguable that the child's first experiments with what adults would regard as linguistic deceit are on a causal and behavioural par with the bird's fraudulent warning cry.
Both might just by accident hit on the fraudulent behaviour eliciting the desired response, and subsequently adopt it as a strategy in such situations.
The same holds true of any other strategy adopted as a result of operant conditioning.
A significant difference might arise when the child learns about the possibility of detection and punishment after lying and its various moral implications.
The latter, though, might just be assimilated to the factors that the subject has to take into account under operant conditioning.
The trouble by this stage, however, is that an adequate account of the effects of  admonishments, threats and exhortations would enmesh one further in the semantic question posed by the original problem.
Some may see the bird's warning cry as falling short of lying for a further reason.
This is the absence of any higher intention, or evidence for such, that the receiver should believe something by recognising this very intention.
This extra Gricean ingredient in the conceptual analysis of linguistic meaning applies not just to cases of lying but to linguistic communication in general.
So far we have been discussing the intentional aspect of language in our comparison of human with other animal communication.
But we have not lost sight of the important role to be played by considerations concerning range of vocabulary items, methods of syntactic combination, discreteness of signals, and the associated semantic possibilities.
Among these are reference to spatio-temporally remote items, generality and tense — to name but a few.
They are among the most important semantic features of human language for which one would understandably be hard put to find correlates in other known systems of animal communication.
One would be hard put also to find grounds for the attribution of the beliefs thus expressible to creatures that might be thought to have but not express them:
Considerations concerning syntactic and semantic structure are not unconnected with those about the beliefs and intentions involved in linguistic communication — beliefs and intentions which McDowell suggests are quite secondary once one has accepted that communication in general(from simple instinctive types through to natural language) has first and foremost to do with the transfer of information, or the instilling of knowledge.
Let us put aside for the moment the misgiving that the latter phrase ushers in the intentional ingredient once more.
No doubt McDowell could make do with first-order ‘quasi'-knowledge in a theory of cognitive representations for ‘lower’ species (in something of the form suggested by Field 1978), stopping short of the special problems posed by the diagnosis of the higher-order beliefs and intentions involved in a Gricean or sub-Gricean mechanism (cf.
Bennett).
In doing so he might find himself in the company of evolutionary epistemologists such as Riedl (1979), whose over-arching theory of life as an ‘erkenntnisgewinnender prozess’ seems to require a unitary notion of knowledge or information, information that can be stored in a genome at  one end of the evolutionary spectrum, as well as be expressed, at the other end, by scientific theories that make the world a less strange place to live in.
But what I would wish to suggest is that other considerations of an evolutionary kind lend weight to the view McDowell is challenging.
To do so I must sketch the bare outlines of the opposing views on the conceptual issues, to see which one harmonises more with evolutionary conjectures about language.
(In this paper I cannot go very far into the available wealth of fact and conjecture in the neurophysiology of language, psycholinguistic theories of language acquisition, and comparative studies of man and other primates.
Fuller justice to these topics, as well as the ape language experiments and various glottogonic theories, is to be done in a longer study.)
The first evolutionary scenario, which places more emphasis on the intentional ingredient than McDowell would allow, is as follows.
Following Bennett, we regard it as admissible to frame theories of perception (or ‘registration’) and of ‘proto'-belief and desire to explain the actions of certain creatures, provided their behaviour is complex enough, and subject also to the usual caveats of holistic method.
But importantly (as behaviour proves more complex), the theory allows attribution of belief and intention at higher order , even in the absence of language.
Then, when higher-order beliefs and intentions of interactants engage in a suitable way — by instantiating a sub-Gricean mechanism, say — certain actions (token events) can have occasion meaning .
This is meaning in as rich an intentional sense as one is likely to get — far more than the mere instilling of (first-order) knowledge with which McDowell deals.
Now, says Bennett, we can move from meaningful event tokens to conventional meanings of event types.
For we are dealing,ex hypothesi , with creatures who can grasp the salience of certain features of past occasions of meaningful exchange.
Lewis's theory of convention is then applied to account for the acquisition of conventional meaning by certain action (or utterance)types in the developing linguistic life of the group.
But what is of crucial importance in the account thus far is that these action types are syntactically and semantically unstructured .
At this point in Bennett's exposition there is an interesting leap (between chapters 7 and 8).
Having accounted for how action types might acquire conventional but unstructured meanings, he advances straightaway to a discussion of how one might come into an alien community and find  evidence that their linguistic interactions are structured (syntactically and semantically).
Yet he offers no account of how the language of the group could have attained such complexity of structure from the humble (albeit highly intentional) beginnings he had been discussing earlier.
Let us call this the structural lacuna of the intentional scenario.
Compare now McDowell's account — in one way more austere, in another much more radical — which suggests a second scenario.
This opens the possibility (and one acknowledged in conversation) that communication could involve structured messages, perhaps even structure of the sort producing infinite generative capacity, without the creatures ever having passed the ‘Gricean hump’ that Bennett put them past even before occasion meaning had made its evolutionary debut.
It is worth pausing here for a moment to reflect on the weight of traditional thinking that McDowell is seeking to shift:
What McDowell is challenging is the notion thus unanimously upheld that the belief-expressive feature of assertions is essential to our understanding of the extra-linguistic purpose of statements of a natural language:
Is the italicised clause incidental or essential?
On McDowell's view it is not essential insofar as one is concerned just with explaining how language evolved to something like its present state of syntactic complexity, as selection favoured its communicative successes.
His view would be strengthened by scotching the appeal made by both Searle and Armstrong to Moore's paradox.
(This appears to be the only independent support, apart from introspection, that they adduce for belief-expression's being essential to assertion.)
But surely on the more austere view one can easily explain why it should be so peculiar to assert ‘p but I don't believe that p’.
The object language is already taken to contain attitude operators and the first person pronoun.
On McDowell's account, it would presumably be either as the language acquired such new structure, developing something like a pronoun system and (iterated) belief attributions or as, quite independently,behaviour became more complex, calling for higher-order indexical belief states for its explanation (cf.
Mellor 1980a) that consciousness dawned: in particular, consciousness of what was happening when one communicated .
With the evolutionarily intrusive possibility of linguistic deceit, as opposed to mere malfunction of communication, communicative exchanges would then come to be understood as having truth as their aim; and the Gricean template would come to be true of the pursuit of those aims.
So too would utterances of Moore's form acquire their paradoxical force.
It is only necessary, for the paradox to have its force, that an assertion that p provide grounds for believing (rather than:express ) that the speaker believes that p.
Now whenever I speak, this provides grounds for believing that I have vocal chords; but it does not express that fact.
Is it not just as strange for me to state out loud ‘p, but I don't have any vocal chords’?
If one believes the second  conjunct, one must invoke an unusual explanation for the sounds emanating from my mouth.
Likewise in the case of Moore's paradox.
What I wish to suggest is that filling the structural lacuna makes structured talk without higher-order thought look much less likely.
Even if one were to concede to McDowell that there could be traffic in simple message types before the Gricean hump, I wish to maintain that one would have to be over the hump (or at least capable of being in such states as are involved in the hump) before one's language could evolve syntactic structure of the kind yielding infinite generative capacity.
I stress ‘of the kind yielding infinite generative capacity’ because it is the peculiar combinatorial potency of expressions in natural language which, it seems to me, could not have emerged by any evolutionary mechanism unless the users of the system were capable of the sort of second-order intentional states involved in the Gricean mechanism.
I am quite willing to countenance the possibility that structure might usefully be discerned in signals whose segments thus descried were iteratively impotent.
Different temporal segments of a mating display, say, might convey information about different aspects of the physical prowess and motivational states of the performer.
Likewise different simultaneous aspects of a dance, such as wiggle and tilt with the bees, might separately convey information about different aspects of the world (distance, direction and quality of food source).
But expressions of a recursive language, I maintain, could probably not have acquired their categorial valency as it were, their powers of combination into new but readily understood messages, unless their users were on the second rung of the intentional ladder and therefore, potentially, past the Gricean hump.
To make this claim plausible — a claim which, it seems to me, lies in a grey zone between the conceptual and the empirical — I wish first to offer what seems to me the most likely account of the evolution of syntactic structure.
Even if my assessment of its implications concerning the relative order of emergence of the intentional ingredient and of syntactic structure were held to be incorrect, the mechanism of that evolution might be of independent interest, and be seen as bearing on other problems besides (especially in developmental psychology and theoretical linguistics).
Recently Sampson has challenged the status of many of the alleged linguistic ‘universals’ put forward by Chomskians.
He has argued that ‘independent explanation, more plausible a priori than nativism, is available for the universality of the trait’.
His attack is therefore two-pronged: to argue that there are far fewer genuine universals than had been thought, and to explain away the remaining ones in ways more plausible than Chomsky's.
The main syntactic universal upon whose existence both Chomsky and Sampson are agreed is that all sentences of natural language are  hierarchically structured .
It is with respect to this universal that we can illustrate the novelty of Sampson's contribution.
Any sentence can be broken down into immediate sub-units which themselves have reasonably independent status as coherent wholes, whose combination produces the original sentence.
Each of the sub-units may in turn be decomposable into sub-sub-units and so on, until non-decomposable words or morphemes are encountered.
Thus the revealed hierarchy of expressions is a tree-like structure with branchings determined in a definite way.
It was precisely this analysis of how certain expressions lower down in the tree thus fell within the scope of others higher up on a branch that launched modern logic on its powerful and sophisticated account of logical relations among sentences.
Chomsky and Sampson claim that this tree-like decomposition of a sentence into its parts is a feature common to all human language, and is indeed the basis upon which we determine the meaning of a sentence from the way it is built up out of its parts (an insight going back at least to Frege).
In a Chomskian grammar it is the base component, consisting of phrase structure rules, that generates the sort of revealed structure just explained.
According to Chomsky, we are genetically pre-programmed to search for just this sort of structure underlying the sentences that we hear as infants.
The strict sense in which such structures underlie the sentences is of course enshrined in the transformational component of the grammar.
All this imposes a powerful constraint on language acquisition from the allegedly scanty data available to any child.
The alleged deficiency is compensated for by a neurologically based preference for Chomskian grammars.
That is, the child is pre-programmed to arrive at this sort of representation of his mother tongue rather than any of the many other mathematically possible kinds.
The representation is, of course, implicit and we would not expect any child to be able to state explicitly the phrase structure rules and transformations generating the sentences of his language.
There is also considerable evidence (as marshalled by Ingram 1975) to suggest that up to the age of six years it is mainly short, simple but still hierarchically structured sentences that the child is able to produce.
Transformations that have to do with embedding one sentence into another (such as subordinate clause or relative clause formation) are only acquired between the ages of six and twelve.
This would imply at least that the early fragment of language poses the problem of hierarchical analysis in a reasonably pure form.
Sampson's attack on Chomsky's account of how the child overcomes this problem is ingenious.
He advances a quite different explanation of why hierarchicality is a feature to be found in all languages.
In so doing, he avoids any appeal to neurological pre-programming of a specifically linguistic kind as claimed by Chomsky.
Sampson applies an evolutionary model due to Simon, originally devised for quite different domains than the present one.
These include the evolution of human institutions, and certain kinds of pre-biotic evolution.
The idea is strikingly simple.
If one considers hierarchical assemblies in general, their evolution from their constituents is far more likely if they consist of relatively stable sub-assemblies which themselves are evolutionary products of an earlier period.
Evolution proceeds by the accidental combination of already existing stable sub-assemblies, thereby producing new stable assemblies of higher complexity.
A new stable assembly establishes its credentials as a useful whole, and is co-opted into the ever-growing network of items with burgeoning structure.
If we have a principled way of discerning the stable sub-assemblies, thereby distinguishing them from merely arbitrary collections of parts, then we can as it were see the evolutionary pedigree of a complex structure.
Let us apply this account to the syntactic structures of a natural language undergoing the process of evolution which it is only reasonable to suppose has taken place.
The systems of communication of our primate ancestors presumably consisted of words and short sentences (but what I have to say would hold even if only gestural sequences were involved).
It is reasonable to suppose that these language users might (even accidentally) hit on new combinations of phrases to produce slightly longer sentences than had hitherto been the rule: sentences, moreover, whose newly-coined significance derived from both the context of their first use and the pre-established significance of their components.
Now there are two ways that the new composite sentence token will be of a type that is eventually to acquire a constant significance.
(Note here that I am doing my best to avoid speaking of conventional meaning, for I do not wish to beg the question yet against the second scenario.)
The first is a highly fortuitous and austere way: namely that the compound might have the same effect — the transfer of some particular kind of information — sufficiently often and sufficiently advantageously for there to be some selective advantage in its coming standardly to possess that significance: that is, for it to acquire the biological function of transmitting just that kind of information.
In the case of instinctual communication the fortuitous nature of this process is only too apparent; for it requires the formation of new closed genetic programmes for both the production and processing of the complex signal.
Another observation is crucial here.
Remember we are concerned eventually to account for the repeatable contribution constituents make to the significance of signals involving them; this being of the essence when syntax is recursive, or creative.
Thus we want to see the representational role of the newly juxtaposed elements as recognisably preserved within, and  thereby helping to determine the new representational role of, the new signal in which they occur.
(I use the phrase ‘representational role’ rather than‘meaning’here to hew as closely as possible to the line of the second scenario.)
Now in a language of potentially unlimited generative capacity, I fail to see how this could be achieved for all expressions across all patterns of combination in the first way described above.
Insofar as any qualitative argument about improbability of emergence could have any force, surely this is one such case.
Compare now the vastly more plausible second way in which the new composite sentence token may be of a type that is to acquire constant significance.
And let us no longer worry about begging questions against the second scenario, and simply say ‘conventional meaning’ rather than‘constant significance’.
The second way makes the emergence of syntactic combinations seem much less fortuitous.
Consider an intriguing claim by Schmitt (1955).
A proto-language might contain one-word sentences used appropriately (in a variety of speech acts) for things and events.
There might be words for ‘man’, ‘seal’, ‘booty’, etc., and perhaps also words for events such as killings or harpoonings.
Bearing in mind constantly the radical interpreter at one's shoulder, one could regard these as mainly nominal in character.
Facial gesture and general demeanour could easily signal the force with which the utterance of such nouns was to be taken.
Truculent and threatening utterances would be demands, plaintive ones requests, excited but friendly ones declaratives, and so on.
Schmitt's claim is that even in present-day Eskimo the verb translated into English as ‘kill’ is a peripheral modification of the noun for ‘booty’.
One can imagine the first fortuitous utterance of the three word string
Man
seal
booty
as having the occasion meaning (via suitably diagnosed Gricean intentions etc.) of something roughly like ‘The man killed the seal’.
New strings like this, in a highly salient context, could readily be understood as new messages, of a new level of complexity, by an audience who already grasped the components from their isolated usages in the past.
We may expect new conventions governing syntactic combinations — in our example the Subject-Object-Verb complex — to establish themselves quickly in the evolving language of any group whose members are bright enough to tumble to the meanings of such innovations.
But this crucially involves their being past the Gricean hump.
The linguist Charles Li, writing in his introduction to a highly speculative volume on the mechanisms of syntactic change (Li 1977), claims that with a few exceptions the only documented  types of word order changes that are not due to language contact are SOV to (VSO) to SVO.
Being documented they are possible; and possible, despite word order change being, as Li himself puts it, ‘the most drastic and complex category of syntactic changes’.
As new syntactic categories settle down by innovative accretion along the lines suggested, the complex mesh of grammatical relations becomes the cloth of an ever-changing community coat.
By the time transformational rules (if they are really operating — which is theoretically contentious, as we shall see in due course) enter the picture, we can expect even more linguistic material to be available for reshaping, re-ordering and relocating.
In this way the surface output, being as it is so critically dependent on transformational pedigree, becomes a highly labile and volatile product.
If our account of innovative accretion is reasonable, we have a basic model within the second scenario of how the earliest linguistic structures were born.
A few salient juxtapositions would confer upon words a new relational potency over and above their semantic directedness to the external world.
They would acquire a certain potential for juxtapositions, a grammatical valency, which is now captured by the idea of the category of an expression, due to Frege and Ajdukiewicz.
Phrase structure rules would implicitly have been adopted, legitimating certain combinations that have successfully occurred, as well as others that had not yet occurred but might very well have, and with similar success, had the choice of words and message had to be different.
Thus even the earliest and most modest collection of phrase structure rules would have been pregnant with new output; and as the rules became entrenched so too would innovative effort and diagnostic insight become more relaxed and automatic, as words appeared newly combined in well-understood syntactic contexts.
Transformational rules also, presumably, emerged at this time — at first, perhaps, with mainly abbreviatory effect.
Later they could have wedded sentences in constructions such as causal conditionals, being both a spur to and the reflection of our ancestors' growing conceptual mastery of the world about them.
Premack's tests (Premack 1976) to establish that chimpanzees make rudimentary connections between ‘states of affairs’(in order not to beg the more refined question of objectuality within them) which to us appear closely connected by virtue of a cause and an effect, or by virtue of an implicit goal or problem and a means or stratagem, seem to the layman to support attributions to chimps of protean correlates of human categories of thought.
It is not too fanciful to see a connection here with a point Ingram has made about the acquisition of linguistic transformations by children (to dwell within the Chomskian paradigm a little longer).
It invites one to see Haeckel's principle at work once more, with ontogeny recapitulating phylogeny:
As Sampson observes, the phytogenetic account above of the simultaneous growth of phrase structure and transformational rules yields a prediction agreeing nicely with a principle formulated by the Chomskians.
This principle states that transformations apply cyclically, and that later transformations, in tinkering with the transforms already produced, deal with them as completed units undergoing at most peripheral changes in this later re-arrangement.
That is, later transformations tend not to interfere with or undo the effects of earlier ones in the generation of a surface from a deep structure.
This is precisely what one would expect if the above evolutionary account were true.
One would expect existing sentence patterns to be stabilised in use, and to be only minimally disturbed upon being combined to form more complex syntactic units.
According to Sampson, many other features of transformational grammar can be explained away in a similar fashion once one adopts this evolutionary perspective.
Although Sampson himself does not point to independent evidence or hypotheses about syntactic change, what he suggests coheres well with the speculations of Chung (1977).
From evidence concerning Pukapukan and Samoan, she forms the following conjectures.
Syntactic change affects simple sentences before it affects the action of superficial rules; and when transformational rules are affected, the more superficial ones are affected first, the major cyclic ones last.
Also worth noting here is that Hankamer (1977) has put forward the possibility of competing grammars for a body of linguistic data as a force inducing syntactic change.
The re-analysis of linguistic structure (once well developed) afforded by a different grammar may impel new structural developments and innovations.
Thus the old grammar could become obsolete and it might no longer be possible to extend it to deal with relatively simple sentences of the language resulting from the ‘actualisation of the re-analysis’.
But Hankamer's idea is more appropriate to the ‘evolution’ of fully fledged languages, a process of interest in its own right but somewhat peripheral to the project of accounting for how more complex linguistic structures might have evolved from simpler ones.
The structural lacuna in the intentional scenario is, I am suggesting, to be filled by something like Sampson's account of the evolution of hierarchical arrangements of stable sub-assemblies.
Innovative combinations become part of a familiar repertoire precisely because they offer a form of solution to recurring co-ordination problems whose salience the system users are quick to discern.
And the accompanying growth and refinement of categorial conventions is possible only because the system users have higher order beliefs and intentions.
A further suggestion that flows from this is that such evolutionary considerations lend support to a competing grammatical paradigm — that of Montague grammar.
Whereas in Chomskian grammar the basic approach is ‘top-down’, with transformation rules sometimes applying in ways that require one to consider syntactic environments beyond the immediate focus of application, the picture in Montague grammar is simpler, in a way more congenial to our evolutionary picture.
In Montague grammar various categories of expression are simultaneously defined in a recursive fashion.
One may think of similarly structured expressions within one category — differing only in lexical items — as stable sub-assemblies with an independent communicative value of their own.
This may derive not only from repeated occurrence within wider syntactical contexts, but also from frequent usage ‘in isolation’, as it were.
One thinks here, for example, of noun phrases being used in response to wh-questions.
Indeed, one empirical reason why a certain category within a Montague grammar should be basic may be precisely a high pragmatic probability of its isolated use in fruitful exchanges: here the grammar of conversation rather than of the sentence may become crucial.
One is inclined to ask here just how well argued is the frequent claim that ‘the unit of communication is the sentence’(Armstrong 1971: 428).
The way a sentence (or indeed any complex expression) is generated within a Montague grammar provides a possible synchronic fossil of how, diachronically, the language acquired the complexity making such a sentence possible.
For in Montague grammar one begins with lexical items (of known categories) and ‘assembles’ lower-level structures.
Within these one can still discern the ‘earlier’ items, whose syntactic combination in accordance with Montague's formation rules involves only relatively minor peripheral modifications (just what one would expect on the Sampson-Simon model).
The new lower-level structures are then themselves syntactically combined, again with peripheral modifications not obliterating their essential unity, into yet higher ones; and so on, until the sentence in question is produced.
Importantly, there are no transformations capable of wholesale disfiguration of the by-products of any stage of the generative process.
Montague generation, proceeding as it does through all categories of expression simultaneously, promises recognition of the conversational integrity of parts of speech in a way that sentence-focussed Chomskian grammar does not.
Moreover, there is a further independent reason for finding it attractive, apart from whatever success is to be had in the future in generating likely looking fragments of natural language.
This reason is that it is mathematically much less powerful than Chomskian grammar.
Peters and Ritchie (1971) have shown that every recursively enumerable set of strings is the language generated by some Chomskian grammar.
Montague grammars, by contrast, characterise at most context-sensitive languages, and therefore yield decision procedures for grammaticality.
This is especially desirable in the light of our manifest ability to parse — that is, to produce judgements as to grammaticality, and not just judgements that are grammatical.
It also means that whatever part of the brain it is that deals with matters grammatical can be regarded as relatively low in the so-called sub-recursive hierarchy of computing machines — putting it within easier reach, perhaps, of the evolution of cognitive capacities ‘from below’.
But these are technical considerations not to be dwelt on here.
On the account advanced by Sampson, especially with the substitution of Montague grammar for Chomskian, there is no need to appeal to innate linguistic abilities any more specialised than those required for general problem solving.
The Chomskian might advance the speculative thesis that any mutation causing children to search immediately in the right class of grammars would have a great selective advantage, and that such evolutionary change might well have taken place, producing human beings who are now pre-programmed to process linguistic data in a specific way.
Certainly the selective pressure for advanced linguistic competence would be very great — as witness the probable absence of any human languages in the intermediate range of the evolution that Sampson describes, that lacks, say, some of the syntactic and semantic functions and resources of known languages.
But whether genes for hierarchical supposition would confer a sharp selective edge on language learners carrying them depends very much on how satisfactory an account of language acquisition is to be had from a suitably sophisticated (non-behaviourist) learning theory.
Chomsky places faith in special neural mechanisms blossoming in the brain.
He is ungroundedly pessimistic over the prospect of developing a powerful enough learning theory to account for language acquisition.
His argument, if it counts as such, is a dogmatic admission of defeat, unsupported by quantitative evidence.
We simply do not yet know enough about the capacity of learning strategies that have been discovered or perhaps remain to be discovered.
2
So far we have considered how natural language might have developed that complexity which sets it off so dramatically from the signalling systems of other species.
What now of the claims that other primates can communicate with systems in all important respects as complicated as ours?
Several experimenters have maintained that chimpanzees and gorillas can be taught to communicate by means of artificial systems not involving speech.
For anatomical reasons, chimps and gorillas cannot produce a range of sounds sufficient for speech.
But some investigators claim that they have taught them to use systems approaching human language in their versatility and complexity.
The implicit claim to be considered is that apes are, qualitatively speaking, a quantum jump ahead of other animals in this respect (although admittedly not many others have been investigated): that they are ‘on our side of the divide’.
There has recently been some highly critical re-assessment of the claims initially made by ape language experimenters on behalf of their subjects.
The criticism is of various kinds.
Some concerns the basic methodological pitfalls surrounding any project of this kind — the lack of proper controls, over-readiness to read richer interpretations into bits of behaviour than a more rigorous viewpoint would endorse, even experimenters' proneness, given their wishful thinking, to be manipulated by their hairy charges into taking up certain attitudes not properly grounded in the available evidence.
Another set of criticisms concerns the basic approach, given the desired object of establishing communicative contact with and among the apes concerned.
Anyone can see that artificial keyboard or lexigram languages can be imparted, if at all, in only the most impoverished social situations — quite unlike the nexus of warm and intimate physical bonds that probably prevailed at the dawn of language millennia ago.
Normal communication is free and spontaneous, with parties taking turns, with an equal balance between initiative and response.
A human child quickly expands his vocabulary, and the mean length of his utterances increases accordingly.
He uses already mastered words in new combinations to which he has not hitherto been exposed, and responds to such combinations appropriately as well.
Other criticisms focus on massaging of the data, in the form of simplification of ape utterances in their reporting, so as (probably unwittingly) to make them look more like human utterances than the hodge-podges they really are; and re-analysis of unedited films of ‘discussions’ with the apes, showing just what a high proportion of the exchanges embody no more than unconscious cueing by the human experimenter, slavish imitation by the ape, or his redundant expansion and embroidery of their exchanges without  commensurate amplification of information conveyed.
More philosophical criticism is to be found along Davidsonian lines.
Apes may well have produced new two-sign combinations that their trainers have been inclined to interpret as appropriately invented for some feature in the context.
A celebrated example is Washoe's signing of ‘water bird’ in the presence of a swan.
But the radical interpreter could re-interpret the dyadic sign as ‘bird on water’ or ‘bird and water’or ‘wet feathers’or ‘reflection of bird’or in any one of many ways.
The dyadic sign need not be endowed with any unitary significance, or be regarded as a newly constructed compound of previously mastered components.
On this view it would be overcharitable to credit ape strings with syntactic structure when they may be nothing but sequences of single ‘word-sentences’.
One reviewer of the present state of the debate (J. L. Marx 1980) ably summarises all the points of failing in the collection of data and their interpretation, but makes so bold as to write:
Yet even this would be disputed by a Davidsonian who insisted on the primacy of a language of identity, reference and quantification before crediting anyone with even the general notion of an object.
One of the most important points of difference between language acquisition by human beings and the deficient parallel process in the case of the ape language experiments is, of course, that in the latter case one is not dealing with the acquisition by the young of a system employed already by conspecific elders.
The apes being taught are therefore without an evolutionarily conferred advantage that human children enjoy — that of employing learning techniques, and being initiated by their elders, in a way that has presumably been refined by selection pressures over a very long time.
Recent studies of language acquisition reveal that children are very attentive, and actively process evidence in ways perhaps not fully appreciated at the time when Chomsky was championing a theory of innate linguistic universals that depended heavily on an alleged gap between the scanty data available to the child and the rich system that he eventually masters in response thereto.
Children are highly motivated learners of language, a disposition no doubt by now‘wired in’ to our species.
In this respect they are probably quite unlike the apes, who have not yet been subjected to evolutionary pressures for rapid acquisition of symbol systems.
Children benefit also from more than usually grammatical speech from adults who address them in the early stages in a fashion tailored to their learning needs.
This tendency on the part of adults could no doubt also be selected for, once language was entrenched enough to exert the required pressures.
Parents repeat and reinforce their children's utterances, and produce slightly more complicated versions of things already mastered in a gentle advance up the ontogenetic slope.
These observations on human language learning, coupled with evolutionary speculations make one realise just what a formidable accomplishment it would be if apes could be taught the use of a communication system remotely approximating a natural language in creativity, recursiveness, and the extreme conventionality revealed in such matters as reference to spatio-temporally remote items or counterfactual conditionalising or universal generalisation about an unsurveyed domain.
They may well have certain  cerebral pre-adaptations that subtended vocalised speech linked with gesture, which responded in some ancestral line to pressures of selection for an ever more complex code of communication.
The ape's failure to acquire a ‘language’ from us (spoken or gestural or plastic-symbolic) in no way discredits the very reasonable evolutionary claim that they, or a common ancestor of ours, had the rudimentary cerebral beginnings that are now our speech centres.
The language faculty is closely tied up, both conceptually and empirically, to other cognitive faculties such as silent foresight, planning, anticipation, reasoned fear, etc.
It may be impossible for the apes to master a code that we would translate into our own terms.
Admittedly we can discern glimmerings of intelligent thought involving the location of sub-goals and the execution of sub-strategies, a  grasp of ‘causality’, perhaps even a grasp of the internal motivational states of others, as disclosed by their behaviour; and some experimenters have even looked for signs of a sense of self, and even of intimation of mortality.
But too much mist obscures the question what it is like to be a chimp for even the best-meaning efforts to make them make the best of meaning.
Our interpretation of the results of the chimp language experiments points to recursively structured language as a unique accomplishment of our own species.
Lenneberg (1966) has argued that a great deal of evidence about deafness in children, aphasia, environmental deprivation of various kinds, muscular debilities and so on supports the hypothesis that language acquisition by children follows a definite maturational path, passing certain milestones of achievement in a certain order.
Moreover these milestones correspond broadly with others in the course of sensory-motor development, being broadly in step with them even when the whole process is slowed down, as in the cases of mental retardation.
Both linguistic and sensory motor development are then co-ordinated further with the ‘brain maturation curve’.
This is a graph of the growth in degree of organisation and certain chemical concentrations, which gives a crude measure of growth.
There is no evidence from all this of cerebral ‘rubicons’ that correspond to the different stages of language acquisition, but one cannot avoid the impression that the linguistic skills acquired by the growing child are orchestrated by a physiological score; and one moreover that has its own peculiar, species-specific  crescendos .
In a survey of the evidence for the species-specificity of speech, Dingwall (1975) has reached broadly similar conclusions.
He concludes that ‘the ability to produce vocalisation which is articulated as opposed to holistic in nature, which is mediated by the neo-cortex as opposed to the limbic system, is unique to the genus: homo sapiens’.
Of course, he is concerned here with speech , and not with language in general , which may of course encompass gestural systems or systems such as those used by the ape language experimenters.
When two closely related species have similar behavioural patterns or capacities, and this similarity is due to their having a common origin, then the behaviours are said to be homologous .
By contrast, similarities (even across unrelated species) that are not due to common origin, but rather to force of environmental circumstance, to conditions which elicit one narrowly constrained adaptive response from the two species, are called analogous .
Dingwall concludes further from what he describes as ‘abundant evidence’ about structure and function, that human and ape vocalisations are not homologous.
As for other aspects of the communication systems, the ‘evolutionary, ontogenetic and neurological evidence currently available tends to support homology rather than[analogy]’.
Apes and monkeys employ a limited number of calls.
Some may serve to  warn fellows about very specific kinds of predator, and therefore have quite specialised communicative significance (Seyfarth et al .
1980).
But they are not formed into sequences having different significance according to their manner of combination and the basic items they contain.
There are some noteworthy differences in brain physiology, apart from the massive increase in brain size, as one passes up through the other primates to man.
Chimpanzees lack Broca's area (for muscular co-ordination in speech), as well as the neighbouring arcuate fasciculus.
The latter is the fibre bundle connecting the auditory area with Broca's area, which some believe to be crucially involved in the imitation of sounds.
It is little wonder then, given these handicaps of sheer physiology, that chimps are incapable of speech.
A more delicate question to assess, however, is whether one could reasonably maintain that they are pre-adapted for Language (not necessarily vocal, but symbolic and semantic in an appropriate sense).
We appear to share with chimpanzees a strong rooting of emotional cries in the limbic system — the so-called ‘emotional’ brain.
But for the rest of spoken language, our neo-cortex is heavily involved, with strong evidence furthermore of lateralisation for speech even as early as birth.
(By ‘lateralisation’ here we mean that the functions are based on one side — usually the left — rather than the other.)
Infants appear, on certain experimental evidence, to be ‘wired’ for the recognition of speech sounds as opposed to others.
And yet, paradoxically, the evidence for lateralisation of the ‘higher’ and more abstract processes of language apart from vocalisation, is less conclusive.
Geschwind once proposed a theory concerning association areas in the brain (Geschwind 1974).
According to him, language could only evolve once the auditory and visual association areas , that in monkeys as well as man convey impulses from the auditory and visual areas to the limbic system,themselves acquired suitable connections via yet another association area, thereby making possible so-called non-limbic, cross-modal associations.
This ‘association area for the association areas’ is the angular gyrus, and it is absent in monkeys.
The reason why its mediation of modes is so crucial is that learning and understanding names of objects probably requires the association of visual with sound images (assuming, of course, that we are dealing with names from a spoken language).
Of course, naming is not the whole of language, as the opening passage of Wittgenstein's Investigations makes us so well aware; yet it is no doubt a central part of language, and still an important and controversial topic in the philosophy of language.
Geschwind's express intention was to find physiological correlates of linguistic capacities, in order to bring language within the scope of scientific materialism.
His conjecture, then, is just one way of making sense of certain anatomical facts across species, evolutionary hypotheses, and observations  of impairments of linguistic functions on the part of human patients who had suffered different sorts of damage to their brains.
Surprisingly Dingwall states categorically that Geschwind's theory, ‘while ingenious, is most assuredly incorrect…’
He cites experiments that have shown that 
This is extremely puzzling argumentation.
Chimpanzees have angular gyri.
But given the copious evidence that chimps simply cannot get their lips and tongues around enough sounds, their cross-modal associations pose no threat to Geschwind's theory.
On the contrary, the very successes claimed by the chimp language experimenters, to which Dingwall himself is sympathetic, would bear out that theory even further.
For Geschwind had been careful enough to note that the angular gyrus is needed even for visual — visual (indeed, for any non-limbic) associations.
So perhaps it is because they have angular gyri that chimpanzees have been able to sign successfully.
I say ‘perhaps’ because rhesus monkeys have recently been claimed to be able to make cross-modal associations.
Nevertheless, so far only chimpanzees have displayed any ability to make visual — auditory associations.
And it remains to enquire to what extent other associative abilities experimentally revealed in other species approach those to be expected of a creature able to use linguistic symbols.
Von Glaserfeld has argued that 
In the same Lockean spirit, Davenport reaffirms the relevance of non-limbic associations when he asks  
Just as there appear to be no rubicons in a child's cerebral development, so also there were probably no such rubicons phylogenetically.
Evolutionary pressures would have gone to work on whatever genetic variability there was in the ability to make the relevant cross-modal associations underlying linguistic competence.
The importance of the former for the latter is not at all diminished by exhibiting languageless creatures that can associate across modes.
For, that one must be able to do X in order to do Y does not entail that if one can do X then one should be able to do Y.
Are the recursive resources of language an innate endowment of Homo sapiens alone?
Our earlier discussion inclines one to say so.
But we must not support the innateness claim with the wrong arguments.
Universality is no guarantee of innateness.
This is so even though it might be true that any infant, regardless of race or family niche, can acquire the tongue of any community.
This acquisition claim has never been rigorously tested.
Physiognomical differences between racial types might affect pronunciation of the foster mother tongue.
But setting purity of pronunciation aside, could any child master any first language so well that native speakers would regard him as fully competent in matters lexical and grammatical?
Should languages have diverged sufficiently radically in our evolutionary past, and should cerebral organization have adapted constantly to the need of parsing, it is quite possible that counterexamples to the claim of eventual competence might be found.
But this would not count in any way against language's being innate.
Indeed it is its very innateness, given sufficient divergence between different languages, that make such counterexamples seem possible.
Hewes (1977) counts twenty-four different theories concerning the origin of the first lexical items.
(perhaps ‘theory’ is too grand a label for these suggestions.)
He concludes plausibly that the best account will probably incorporate several different suggestions about the origin of words in different ‘lexical domains’.
Onomatopoeia and adult imitation of salient infant babbling are just two such suggestions.
But by far the most likely theory looks to gesture as the raw public material from which human communication grew.
Speech could have grafted onto gesture once the vocal tract had developed sufficiently, with selective advantages that  Darwin had remarked on.
The hands are free when one talks; and one can be heard in the dark and in all directions, under cover and so on.
Consistent with this theory is the synchronisation reported by MacNeill (1979) between hand movements and speech segments.
He observes also that gestures can extend and replace full imitation, while yet being sufficiently iconic to secure uptake.
Finally, although Neanderthal man had once been thought incapable of producing a reasonable range of vocal sounds, the fossil reconstruction on which this claim was based has been severely criticized recently (Du Brul 1979).
Complex sentences can be parsed .
In the case of natural language, natives' pre-formal agreement on the boundaries of grammatical division is the most important kind of evidence the linguist can glean as he searches for the grammatical recipes and ingredients of sentence meaning.
One attraction of the gestural theory of language origin is that significant structure does not have to await words.
Susan Goldin-Meadow's subjects were unacquainted deaf children; but they had normal parents who did not try to communicate with them by gesture, or at least not in sequences as the children did:
Of course, these children might have been interpreted according to overly lax canons — compare our discussion of chimp gesture above.
But prima facie the generous semantic account of their project is much more plausible, given that they are agents just like us except insofar as they cannot hear.
We should not regard their ‘invention’ of a structured gestural language as grounds for believing that earlier hominid handwaving could thus rapidly have attained parsable results.
After all, these children are presumably ‘wired’ for Language.
They will possess the same cognitive structures and whatever maturational schedules thereof that  underlie hearing children's language acquisition.
As we remarked earlier, their ‘problem’ was to find a channel of communication.
The problem facing our forebears was to discover communication.
Gestural messages, so it seems, can have significant structure.
Thus the structure of spoken sentences need not have derived entirely from combination of words that had replaced single gesture types.
Instead, structured gestural sequences, or syntagmata as MacNeill calls them, might already have had a rudimentary grammar before they were overlaid by speech.
Whatever the relative order of gesture, speech and structure thereof, my quasi-conceptual evolutionary thesis would still be pressing: iterable contribution to significant structure across the whole language, be it gestural or symbolic, is almost certain to have needed a shrewd intentional grasp by communicators of what they were up to.
Note that this claim would not be undermined by showing that infants ‘mindlessly’ master the communicative content of structured sentences before it dawns on them gradually that assertions express beliefs and that deceit is possible.
For selective pressures for linguistic ability could easily reverse in ontogeny the order I maintain would be needed in phylogeny.
Selection would have operated within the environment of an already structured code.
Mastery of that code by distant descendants could therefore become more ‘mindless’ in the early stages (even those involving significant structure) as brains were shaped by natural selection for more and more rapid language acquisition.
Interest in language origin theory, as the heavy volume edited by Harnad and others testifies (Harnad et al .
1976), is being revived after a long period of disrepute.
Hardly any writer has looked, linguistically, beyond unstructured lexical items.
My attention here has been held far more by the enigma of syntactic structure, and how its development might mesh with the conceptual analyses of meaning and intentionality offered by philosophers of language.
My thesis can best be summarised as follows:
There could be staccato talk without thought.
There had to be thought before structured talk.
Once established, structured talk could be mastered with less thought.
Once mastered, structured talk makes for more thought.
MACHINES AND CONSCIOUSNESS Yorick Wilks
INTRODUCTION
I have no strikingly original view of machines and consciousness to offer, nor shall I present a survey of the views of those in artificial intelligence (AI for short) who have discussed their relation, for the simple reason that there are none, at least in the sense of well-articulated philosophical views.
AI workers are, by and large, naive materialists and mechanists, and for them those are not positions to be justified, but simply assumptions that allow them to get on with the job of constructing mechanical analogues or simulations of ourselves, who are, in Minsky's memorable phrase, ‘meat machines’.
One could argue that such a strong assumption of underlying mechanism is only the normal situation in the sciences: one that allows experimental, as opposed to philosophical, work to proceed.
True enough but, as I shall argue below, AI is not an experimental science but an engineering technique or, if you want something more dignified, a practical task in the alchemical tradition, and what I have to say about its suggestiveness for investigating the nature of consciousness rests entirely on that fact.
Again, the ‘normal’ situation in the sciences is not a sure guide, as none of you needs to be reminded, when the brain and mind are the subjects of investigation, given the peculiar features that attach to the notion of consciousness and to its close relations, thought, feeling, privacy and so on.
In what follows, I shall play fast and loose with these words and the subtle distinctions between them.
That is to say, I shall not distinguish carefully such questions as:
Does X think?
Is X conscious?
Does X have essentially private inner processes?
Is X aware of a sensation of pain?
Is X aware of those trees in front of him?
Is X conscious he is giving an after dinner speech?
When I write of consciousness I shall mean what is often called self-consciousness, rather than awareness of sensations or perceptions.
Moreover, I shall not try to determine what kind of phenomenon consciousness is; the task here is rather to see whether the machine analogy can give us a way of talking about it, whatever it is.
The matter of the paper is to present those areas of AI (some would say that is too parochial and what I shall put forward belongs more generally to Computer Science) where a mechanical analogue of consciousness might be sought in the future, and to argue that they are not the obvious places, and have not been subjected to much philosophical investigation.
At the end, I shall turn your attention to three accounts of the nature of consciousness by philosophers acquainted with AI, and contrast what they have to say with my own account.
I shall present the following notions in turn:
(i)
modularity
(ii)
implementation independence
(iii)
program-level reduction
(iv)
program inference.
These are not really technical terms, but just convenient labels I shall use, though each is close to a well-understood technical concept.
They are, I believe, some of the places to start our search for features of modern machines that might be suggestive as models or analogues of consciousness.
Although I was brisk at the beginning about the philosophical attitudes of AI workers, I owe the reader some small sample of them, that I can refer back to later.
The best known of such views, very close to the theme of this paper, are Turing's speculations on whether machines could be said to think (Turing 1950).
These are both too well known and too widely misunderstood to introduce here in detail: Turing, it will be remembered, wanted to cut through philosophical discussion with a Wittgensteinian fervour.
He proposed a test, based on communication via teletypes with an unseen entity that was to be at first a human and then a computer, and if the human interlocutor, who had been told he was communicating on the subject of the differences between men and women, failed to notice that a machine had been substituted for the original human partner in the dialogue, then the machine was deemed to have passed the test, and Turing  suggested we might as well speak of such a successful machine as thinking by polite convention, just as, ‘instead of arguing continually…it is usual to have a polite convention that everybody thinks’.
A more recent sample, in very much the same tradition would be Minsky's:
Minsky is making an interesting and important point here and I shall refer back to it, but for the moment the quotation functions purely as a sample: what it has in common with Turing's classic paper is that it is not philosophical argument at all.
In Minsky's case, the philosophical question of whether there are hidden mechanisms that explain, etc., etc., never arises, because their existence is assumed from the outset, without discussion.
The passage will remind some readers of Chomsky's palmier days, in linguistics rather than AI, when he would begin arguments with ‘Obviously, everyone has internalized a grammar…’
(Chomsky 1965: 8).
(1) MODULARITY
Modern computer programs, especially those in AI, are written, and are intended to be thought of, as interconnecting sub-parts or modules, and not at all as seamless wholes.
Modules, in this sense, do not have access to the contents of other modules and, in Carl Hewitt of MIT's immortal words, ‘modules shouldn't be able to dicker around with the insides of their neighbours’.
In Winograd's (1971) well-known language understanding program, for example, there is a syntax analysis module and a semantics analysis module: these can demand answers from each other to specific questions about the structures of sentences but cannot ‘get at’ how the other one finds out whatever it does.
Herbert Simon has argued (1969: 115) that evolution will prefer structures that are decomposable in this way, and that modularity may be expected in ‘genetic programs’.
He uses the metaphor of the commercial viability of two watchmakers, one of whom puts watches together out of finished sub-assemblies which cannot fall apart, and another who assembles each watch from its basic parts and risks the whole thing falling to pieces if dropped.
It was such an idea of modularity that Minsky intended in the quoted passage above, and he has suggested at various times that an organism would be more efficient, in terms of its ability to survive, if it had, as a separate module, a model of itself, which might of course be totally false as to the facts of the self's reality: alcoholics who believe themselves to be merely social drinkers probably survive less well than those who believe themselves to be alcoholics.
An accessible model of the self is clearly one of the places that one might look for analogues of consciousness in a machine system.
More recently, Minsky has revived these notions, explicitly drawing analogies with the sorts of ‘modularity’ to be found in the writings of Freud (the three-way Ego, Id, Super-ego division) and Leibniz in his Monadology, by claiming that an appropriate machine analogue of the human individual should have, among its modules, a supreme organizer.
This module would, Minsky suggests, alone have access to the model (again possibly false, of course) of how it itself related to all the other, lower, modules, and it might be expected to have some property of the type we refer to as consciousness or self-consciousness.
This additional property, Minsky believes, could have a functional or evolutionary explanation, of the sort suggested for the property of modularity itself by Simon, in that the ‘conscious’ supreme organizing module would ipso facto be in a position to ‘debug’or repair the connections of the lower modules amongst themselves or to itself.
In order to preserve modularity, this power could not, of course, extend to repairing the modules themselves, for that is just the sort of tinkering that a principle of modularity would rule out.
In so far as I understand this view, and it is not yet a matter of close textual criticism of a published account, I find it difficult to connect the commonsense properties of consciousness (vague as they may be), with the notions of repair and debugging (fundamental as those are to any account of intelligent mechanisms).
It is reliably claimed, for example, that certain yogis are able to take control of their physiological functions (heart-beat rate, digestion, etc.), utterly inaccessible to most of us.
They can, if these claims are true as they seem to be, debug their ‘digestive program’, or slow their hearts considerably.
But would we want to say that these abilities, striking though they are, have any particular connection with consciousness?
If a yogi could tell us at any given moment what his digestive organs were doing, in chemical terms, and some constant monitoring apparatus attached to his intestines confirmed everything he said, then we might want  to say yes; for the performance would seem to show just that immediate awareness of goings-on that we think of as intuitively necessary for a Conscious process.
But that, of course, is not change, debugging or repair (the very things Minsky was referring to), for we would be impressed in this way by a yogi who was not able, also and in addition , to change his digestive process.
Conversely, we would be impressed by another yogi who could alter these processes in his intestines on a word of command but made no claims to know what was going on down there from moment to moment.
These are simply different powers, but only one of them seems connected with consciousness, and not the one Minsky opted for.
A more important point lurks here that I shall have cause to return to later: there is a tension in Minsky's accounts of these matters between an emphasis on hierarchical organization (the supreme organizer, or Simon's modular watchmaker would be paradigms of that), and another on heterarchical organization.
The latter notion has never been made very clear, but it was one much promoted by Minsky in the late 1960s (it enormously influenced Winograd's view of the organization of a language understanding system, for example); its essence was that there need be no permanent upper node of a system, as there always is in a hierarchical system, but that different nodes at different ‘levels’ could take control at different times.
This was a more democratic view of organization, both socially and metaphysically, and one closer to Aristotle and Leibniz, for whom every entity, however lowly, had its degree of consciousness.
Minsky's ‘supreme organizer’ view must be a hierarchical one, for that organizer alone has the model of its relation to other modules, and it must therefore always be in control, because no other module has the model of relationships that would allow it sometimes to be in control (in the way a heterarchical view requires).
Whether or not this control and its prerequisite knowledge remain as properties of a single ‘command’ module, or shift about heterarchically, both views are forms of what I shall later want to call a ‘light up’view of consciousness: as in a pinball machine different areas light up at different times depending on the state of the game.
It is a view of consciousness I shall want to question later, in the light of published views of Dennett.
(II) IMPLEMENTATION INDEPENDENCE
It is a reasonably well-known fact that the same computer program can be run on a number of machines, not only different tokens of machines but different types, where that extends to machines working with quite different physical processes.
This is what is referred to when one speaks of the implementation of a program being machine-independent, and it is part of  the conventional distinction between hardware (i.e. machines) and software (i.e. Programs).
That there is a conventional element in the distinction is shown by the fact that procedures expressed as programs can also be expressed by the hardware structure of machines: the principal programming language of AI is LISP, which has been in existence for about twenty years, but only recently has a ‘hard-wired’ LISP-machine been built, one in which the LISP programs are more straight forwardly isomorphic with the operations of the hardware.
All agree that hard-wired machines are faster, but what one loses with them is portability: the ability to run programs in many languages on a single machine, and a program in one language on many different sorts of machine.
It is this portability aspect of programs, and the conventional hardware — software distinction that goes with it, that has most interested those in AI who have concerned themselves with the relation of brains to minds: there has been an easy temptation to exploit the hardware — software distinction as a model of the brain — mind distinction.
That would lead, of course, to a portable notion of mind, one that many have always found independently attractive on theological grounds.
Support for it has come from the observation that both the brain and the conventional digital computer (i.e. the one hard-wired only for its machine code) seem to be surprisingly homogeneous in their internal structure, which led to remarks like Newell's (1973)‘…intelligent behaviour demands only a few very general features in the underlying mechanism’.
This mind — brain analogy will not be used here, in part because the shifting of the hardware/software boundary that has resulted from the  construction of new machines has made it less attractive.
Nonetheless, implementation independence has had a powerful effect on AI thinking about metaphysical problems, and has been behind McCarthy's insistence that AI must be defined as the study of intelligent mechanisms independent of their implementation in machines or brains, and hence to a general denial that AI is, in any strong sense, about machines.
The point can be seen best by contrast: in the uniform rejection, by anyone acquainted with the practice of programming, of Fodor's claim (1976) that the principal interpretations or models, in the logical sense, of programs are actual hardware items and states.
On this claim rests his whole theory of mental language, and yet it cannot be true for, if it were, there would be no serious portability of software, as between, say, machines of radically different architectures.
But there is (cf.
Johnson-Laird 1977 and Wilks 1982).
(III) PROGRAM-LEVEL REDUCTION
This phenomenon is related to the last, but concerns not the translation of procedures from programs/software to machines/hardware, but rather the  translation of procedures from one level of programming language to another.
In a digital computer the ‘lowest level’ of language is simply a string of binary digits 1 and 0 that is isomorphic to the states of the machine's registers.
At a level above that there is what is called machine code: one whose lines are normally instructions to add or subtract or shift the contents of whole registers (themselves strings of binary numbers).
At a level above that is assembly language whose commands normally translate into a set, of ten or perhaps a hundred, machine language commands.
This ascent up the levels of programming languages can go on without any natural limit: a language like LISP, when run, is normally translated through two or three levels before it becomes machine code.
But there are already languages, claimed to be of ‘higher’ levels than LISP, that must, in their turn, be translated into LISP before programs in them can run.
It is normally said that as one goes further up this ascent of languages, the code becomes progressively more like a natural language such as English.
This may be so, but is not necessarily so, and nothing particular hangs on the fact.
The notion of ‘translation’ just given covers a distinction that will be important for the purposes of this paper: a program at a given level can be either interpreted or compiled into a program at a lower level of language.
In the former case, one can think (without it being too misleading) of each statement of the higher-level program translated into a set of statements of lower-level program, and that is done when the program runs.
In the latter case, the higher-level program is translated as a whole into a lower-level program as a whole, and that process is carried out at a time before and separate from the time when the (compiled) program runs.
The latter is more efficient and can be thought of by analogy with the translation of, say, a whole poem in English by a whole poem in Chinese (perhaps by pound), with no line-to-line correspondence, but only an overall ‘sameness of meaning or function’; whereas, on an ‘interpretation’ view, a Chinese poem would be constructed by a line-by-line translation of the English one.
Both kinds of reduction ‘preserve meaning’ but in quite different ways.
Whenever a program runs, then, a sequence of such translations between levels of language is set up, but, and here is an important point, no particular lower-level translation is necessary for the program to achieve its purposes.
Yet more importantly, the upper levels of the program have no access to the levels below them: the programmer who writes at the topmost, or accessible, level has no need to know how his program is being translated, even though for certain purposes he might wish to find out.
This phenomenon is very suggestive of a feature of conscious experience: our lack of conscious access to how we perform the details of activities, both ‘mental’ and ‘physical’, if that distinction makes a rough sort of sense here.
When someone says ‘We went to a bar and ordered a drink’, plain men and  parsing specialists all agree that the sense of ‘bar’ in question is a drinking place and not a rod of iron.
Linguists, psychologists and AI workers have theories about what procedures might select the right sense on the basis of sensible rules and reject the wrong one.
However, it is plain that the speaker of English, although he performs this general task reliably thousands of times a day, has no idea whatever how he does it, and may well have a healthy scepticism about proffered accounts in terms of linguistic or other rules.
Yet how can we get a picture of the way in which it is possible to accept both that a task is done by rules (for it is certainly not done randomly) and yet the performer has no access whatever to the rules?
Wittgenstein pointed out that no account is satisfactory that assumes that a human somehow surveys the alternatives and chooses one: ‘It is as if I should say that the application of a word does not pass in one moment in front of my eye’(1964: 15).
Yet it is not too hard to imagine how a computer parsing program might achieve some such effect: if we ask an English speaker how he does it, we may well get some reply like ‘Well, I'm simply looking for a sense of the word related to drinking, aren't I?’
In program terms, we can imagine a high-level command:
GIVE ME A DRINKING SENSE OF THIS WORD IF THERE IS ONE
and the response to such a command could be to hand up some dictionary definition of a bar as a drinking place.
I am not advocating such a theory of parsing English into formal representations — I happen not to believe strongly in such ‘top — down’ theories, though there are several relatively successful ones in the literature.
The point is that there could be such a command and at the level of that command where, as it were, the answer would be received, no information would ever be received about the other senses of ‘bar’ that the system as a whole might happen to know about in its dictionary, and the procedures for surveying that range of senses would never be revealed.
To that very limited degree, such a program would avoid the puzzle posed by the Wittgensteinian remark, about how it is possible to locate the right sense without any ‘conscious’ access, even in principle, to all the uses of the word.
This situation might seem more difficult in the case of physical activities such as walking: I walk, with or without conscious effort, it may be said, but  have no access of any kind to the associated brain, nerve and muscle activities, even though we have perfectly good physiological evidence for the regular association of those activities with the act of walking.
As with the linguistic theories of how we understand ‘bar’, we may have theories or models or whatever of our own walking but they may be wildly false.
A lay person asked whether both his feet were off the ground simultaneously when he walked would probably be wrong as often as right.
Thus far, the analogy with the linguistic case holds.
However, how can this phenomenon be illuminated by means of the metaphor of levels of programming language, on which view the ‘lower activities’ are inscrutable at the ‘higher level’, given that, after a road accident, say, I can by conscious effort retrain myself to walk properly?
We are returning, by another route, to Minsky's suggestion that the evolutionary role of consciousness has been to give access to modules or levels, otherwise inaccessible, so as to debug, reprogram or retrain them.
I argued earlier that there was no necessary connection between the two families of notions (of access and repair), and the present example suggests, I would claim, that there is no factual connection either.
I have the ability to retrain my walk, say, precisely because the muscular sub-movements of walking (flexing toes and feet, moving calves, etc.) are neither in ‘another module’ of activity (i.e. from the ‘conscious’module, if there is one) nor at an inaccessible level of translation of commands.
After a tendon transfer operation, for example, one can relearn to walk by ‘moving’ one's foot outward instead of upwards (although, because of the transfer, it then actually moves upward, as required for normal walking).
I shall return later to the possibility of seeing the inaccessibility in terms of a limitation of access between levels (rather than between modules), but for the moment we can redescribe this retraining activity as the ability of the subject, at a given level (the uppermost, of intending to walk, in this case), to retranslate an activity into a lower level, in such a way that the translation later becomes compiled (in terms of the earlier distinction between compilation and translation) and ceases to be accessible from the ‘higher level’.
Thus, after a while, the one who had retrained his walk with great effort would have no more conscious access to his method of walking than one who had never had such an accident, or subsequent operation.
This intuitive account would require, in an analogue of consciousness for which program-level reduction could provide a necessary condition, but no more, that there be access from a higher to a lower level of programming language in certain, yet to be specified, circumstances.
Those might be, for example, early stages of the retraining of walking in which a subject could still access and change the order of muscle movements.
But this access  would be only to an interpretation translation at the lower level (one in which there remains a one — many mapping of commands) but not to an (inscrutable) compilation translation.
If this sort of account is true to the facts, then it is a consideration against any factual connection of the type Minsky envisaged between conscious access and ‘reprogramming or debugging’, at least it is if lower-level translations normally exist as (undebuggable) compilations.
For the provision of a new interpretation of walking (later to be compiled, as it were) precisely did not require any access to, or repair of, the structure of how the walking was done originally.
Empirical evidence against my point might arise if there was significant support for the claims now made by some surgeons that a patient after such an accident can both will, and have conscious access to, the process of nerve regrowth (as an alternative to the sort of superficial retraining of behaviour described above).
If that turned out to be possible, it might suggest there is access from a higher level to quite remote lower-level translations, unless one was prepared to redescribe such a phenomenon in terms of our (levels of programming language) metaphor as ‘This patient has found out how to access a highest level command GROW A NEW NERVE TO YOUR LEFT FOOT though without, of course, having any access to the translation of that command in terms of the nerve cell processes themselves.’
Though then, of course, the activity in question would no longer be walking but growing nerves, and the walking itself might again have to be relearned as described earlier.
(IV) PROGRAM INFERENCE
This concerns limitations on the ability to infer the highest level program given the lowest: if one stood in front of a large machine one would see banks of lights flashing, as in the conventional newspaper cartoons.
These lights are in one-to-one relationships with certain key registers of the machine and actually express the binary numbers in them (a light being off for 0 and on for 1).
Thus, if the whole internal process were slowed down enormously, one could actually see a representation of each command executed by the machine at the lowest possible level of representation.
If one were in possession of all those binary numbers in sequence, could one infer the highest level of program or, to put it another way, could one infer what the machine was actually up to in the sense of paying tax refunds to the citizens of London, as distinct from translating a book from English to Chinese?
And, if there are limitations on our ability to make such inferences, are they in any sense serious or just casual, in that they would require more effort than anyone is normally prepared to put in?
Such an ability would be of more than merely theoretical interest: there are specialists, detectives one might almost say, who can take enormous quantities of program in a lower-level language (not binary numbers, but normally machine code or something a little‘higher’) and make plausible guesses as to what they actually do at a higher level of description; or rather, given that they are told what the program was designed to do, work out how it accomplished the task and by what ‘higher-level’ steps.
This task is an important one because of the enormous amounts of program in the hands of organizations whose original writers have long since left without leaving any description of what they had done.
The US Government is reliably said to have invested large sums from its military budget on research in which impulses from computers (which might give you something like the binary number level of the program) were detected at a distance and the task was to see whether the highest level of program (expressing what the real purpose of the program was) could be reliably inferred.
The answer was ‘no’, roughly speaking, unless one knew not only the machine code but, most importantly, what the highest level language in question was, and the one from which the binary code had been obtained when the program ran.
The important point was that there seemed to be no way of determinately reversing this higher-to-lower translation unless the target high-level language was already known.
It is sometimes argued that an experienced programmer can detect the ‘general shape’ of a particular high-level language X from blocks of machine code, just by hunch and judgement, but this ignores the possibility that the code may have been written in language Y with the syntactic style of X precisely in order to create this confusion; just as one can murmur English with a German intonation and cause a distant listener to believe he is listening to unintelligible German.
In sum, then, there is no reason to believe that, in a complex machine, the real processes can be inferred reliably from any number of observations of internal behaviour (in the sense in which the changing of register contents expressed in binary numbers is internal behaviour), in the absence of knowledge of a quite different type: the language in which the processes have been expressed to the machine.
It would be dangerous to suggest that this impossibility is in any strong sense theoretical, i.e. open to mathematical proof, and I will assume it is no more than a strong empirical impossibility.
If such a program detective had descriptions of all the high-level languages ever written, and knew them to be all there were, then he could presumably work through them all in turn  (the obvious difference from the human brain case being that, even if it has a high-level programming language we have no idea what it could be like).
Suppose one grants some such upwards inscrutability, in terms of lower-to-higher language levels, does anything general or of interest for our discussion follow?
If it amounts to some sort of ‘machine privacy’, is that at all suggestive for the question of consciousness?
Before addressing those questions directly, let us return for a moment to the so-called ‘Turing test’(Turing 1950) mentioned earlier.
One aspect of performance that one might expect of any machine that was to pass the test (by behaving in such a way that the human interlocutor never even suspected a machine was present) would be to have the sort of final authority over what state it was in that we normally concede to humans: when Jones, on the neurosurgeon's table, insists that he is in pain, we tend to allow his authority even though the neurosurgeon says that, given the position of the brain probe at that moment, he should not be.
If a computer printed out that its memory was suffering from a certain kind of fault, we might be persuaded from past experience to go on examining its hardware for faults, even though we found none in the initially plausible places.
We might, to speak anthropomorphically, allow it to insist that its memory was going and, if we did, we would allow it the authority in question.
Closer perhaps to the title of this section, we might come to allow that the computer really was paying tax refunds (because it said it was), even though all detective work on its machine code program was consistent with it being occupied directing the trajectories of inter-continental ballistic missiles.
If we did come to allow such authority, or privacy, to the machine enormous consequences would follow, for its blueprints and machine programs would no longer be a safe guide to its future behaviour.
Yet, of course, a machine that was to pass the Turing test fully should have this authority, because we do, or appear to do.
To be plausible the authority should also be limited in the way ours is: a machine that appeared certain, in the teeth of all the evidence observable by us, that such and such a transistor was failing might well have given itself away  precisely because it would lack the ‘downwards’ inscrutability that our inner workings have for us.
To return to the question at issue: does all this even suggest anything insightful about the nature of consciousness?
Given that privacy is plausibly one of the necessary conditions for being an explication of consciousness (see below), it seems to me that any establishment of an interesting sense of machine privacy must be relevant.
At the very least, it would show that human beings do a fortiori have some sort of privacy.
An initial objection against any easy identification of such a privacy with consciousness is that, when driving, say, the highest level commands to turn the car, keep the accelerator depressed, etc., would be ‘inscrutable from below’ in the way discussed, but the level of those commands could never be identified with consciousness because, as Sartre liked to point out ad nauseam , we can drive ‘without thinking about it’and, moreover, at no danger to the public.
This is correct, but nothing I shall say, when I come to draw conclusions, will amount to any such identification, only to necessary, though not sufficient, conditions for an explication of consciousness.
So, if we are conscious in the sense of this analogy of levels, then it is of the uppermost level that we are conscious, but that does not require me to claim that we are always conscious of that level.
Nor am I assuming that the top level of language, in the sense of programming language, coincides always with the ‘top level of control’, though it will in general do so.
The driving-unconsciously-while-proving-a-theorem case illustrates that situation well enough.
More seriously, one might object that everything said here depends crucially on the assumption that there really is one activity, rather than another, that a given computer is performing.
One might go on to say that if there are two or more consistent interpretations of the lowest level code, then it makes no sense to say that the computer is in fact, say, paying tax refunds rather than doing something else because that can never be more than a matter of pragmatic interpretation by some human users of the thing.
This seems to me just false, although I find it hard to show it convincingly.
If a detective approaches someone and says ‘all your  activities of the last week are consistent with the interpretation that you are planning to rob a post office’ then, when the person replies, ‘but as a matter of fact, I just am shopping there and no more, and that's that’, one can either refer to intentionality or some such notion and allow the appeal or, with the detective, one can continue to keep one's eye open.
Not everyone will concede that there really is a single interpretation, nor that, if there is, the subject necessarily knows what he is up to.
I think the most commonsense response to this objection is as follows: whatever may be the theoretical case about competing interpretations of low-level code in the machine, we need to remind ourselves that, if this machine is ‘paying tax refunds’, as opposed to‘directing missiles’, then it is actually printing large rolls of cheques at its peripheral devices.
Any claim that, because there are competing interpretations, either is equally valid in the absence of knowledge of the programmer's intentions in the matter, is impossible to maintain in the face of the long rolls of encashable instruments pouring out of its printer.
CRITERIA FOR AN EXPLANATION OF CONSCIOUSNESS
I now want to draw together the four aspects of intelligent machines set out above and three prima facie features of consciousness: these three seem to me necessary criteria for any explication of consciousness, and I will suggest that the aspects of such machines already described are interestingly related to these facts.
They are not, of course, sufficient features and I shall make neither claims nor suggestions here as to the sufficiency of existing or possible machines for being conscious entities.
(a) Vacuity or ‘downwards opacity'
Dennett (see below) has in recent publications emphasized the relative emptiness of the contents of consciousness: all that is not there and which, for much of the workings of our bodies, cannot be brought into the contents of consciousness.
I would suggest that the two sorts of opacity in a computer that have been discussed here are at least potentially interesting explications of that fact: modularity and program-level reduction.
When discussing Dennett and Sloman below, I shall argue that the opacity of one level of programming language to another is a better preliminary model of consciousness than the inaccessibility of the contents of one module from another.
Different kinds of opacity within a program were discussed earlier and these would seem to have quite different correlates in the sphere of consciousness: the lower level of language is almost totally inaccessible from the higher level (unless special structural features are added to the language to make it accessible), in rather the way that the machine code of our brain, if there is one, is utterly inaccessible to me, thinking in English.
One might suggest at this point that the level and module metaphors are not really opposed, because there could be a module that did the translation between the uppermost level and the one below it, and so on downwards.
In that case, it would be argued, the levels are in one-to-one correspondence with the modules and the difference is only one terminology.
But one can see immediately that that is not so: the task of the modules is translation, and so the task of each translation module is therefore different (translating a poem from English to Chinese is not the same task as translating that poem from Chinese to French).
But the levels, in the original description, all have the same function.
The notion of difference of level between quite different programming languages can equally well be expressed within a single language: at a single level of language, say of the programming language LISP, one normally defines a function in terms of sub-functions, so that I might for example write a function WALK(x) (where x ranges over walkers) whose sub-functions (to be executed in order) might be some form of LIFT-RIGHT-LEG; FLEX-RIGHT-FOOT; and so on(I am not suggesting that sequence would be even remotely plausible in fact).
Normally the highest level of program, demanding the execution of WALK within a program called GO-SHOPPING(x), would have no access to WALK, nor a fortiori to the contents of WALK.
The difference of level now would be that of a hierarchy of functions and sub-functions within a single language, though one might still plausibly maintain that the above sequence beginning LIFT-RIGHT-LEG was the translation of WALK in that system.
One must be a little careful with the word ‘translation’ here: it is fairly innocuous in the present context of WALK, translated into the string of sub-functions, but much less so in the earlier use of the relation of a program in LISP, say into a lower-level program in machine code.
It is in that latter  role that Fodor (1976) has exploited the use of ‘translation’ and made the inference that the lower-level code must therefore mean the same as the higher-level code, thus arguing in effect against what I called ‘implementation independence’.
My earlier use of ‘translation’ to cover both interpretation and compilation into a lower-level language is not intended to carry any suggestion that the lower levels, including the lowest level of all, the machine's registers, can be an interesting formal semantics for the higher levels, let alone what the highest level ‘is about’.
Nor can one assume that two levels of program ‘have the same semantics’(as true translations normally would).
However, within the single language LISP, access could, with appropriate effort, be made to the translation of GO-SHOPPING(x) as a sequence beginning WALK(x), but the subsequent access from WALK(x) to the yet lower-level sequence beginning LIFT-RIGHT-LEG(x) is far more dubious, whatever the effort required, since that would normally have been compiled and so be inaccessible to the higher level in question, even though, as we saw, one can, in the human case, impose a new translation of WALK, in the place of the existing one.
A person's initial attempt, when recovering from such a supposed accident, to construct a sequence beginning LIFT-RIGHT-LEG(x), etc., may well draw on our observation of others, folk theories of walking, etc., and be a totally false — in the sense of ineffectual — theory of walking (i.e. be a wrong W in the terms of the original quotation from Minsky 1968).
The actual retraining of walking in physiotherapy may require that one has imposed on one a new sequence of movements, a sequence quite different from that of the false folk theory, and one that the patient might not be able to construct without the expert help of the physiotherapist.
All this, I argued, is at least suggestive about how we can have some form of access to the superficial form, say of our walking (i.e. to the gross sequence of movements, if not to the nerves), and can impose a new walking ‘strategy’ to which we will then in due course (after an analogue of compilation) again lose access.
The claim in this section that what I have called downwards opacity is a necessary feature of consciousness may seem open to the following reply: a person might claim that he was conscious, directly and permanently, of all aspects and details of his bodily functions: nerves, cells, blood vessels, etc., and medical evidence might confirm what he said.
Would we want to rule him out as being conscious, on the grounds that the above condition is a necessary one?
Logically, we would not, but the reply would have to be that given the processing mechanisms we seem to have, it would not be practically possible.
To say this is to adopt a quasi-evolutionary view, close to Minsky's, not in terms of debugging and repair, but in terms of attention  and processing load: to be able to concentrate on everything is not to be able to concentrate at all.
Or, to put it another way, there is an essentially privative (in the sense of ‘deprive’ not ‘private’) aspect of consciousness and this feature (a) is one way of capturing that.
(b) Upwards opacity
By this I refer to the acknowledged limitations on the ability to infer the contents of consciousness from any number of behavioural or physiological observations.
This is a deep but much discussed subject and I shall not go into it here but simply assume it.
As I argued, the phenomena associated with upwards program inference are a plausible explication of that.
(c) The unity of consciousness
This is another classic and difficult topic.
There is, to put it crudely, a firm intuition that the self we can identify with our immediate consciousness is a unity, in that we would not count as being that self any part or module of ourselves which was put forward as a candidate for being a conscious entity.
There are great difficulties about this notion, however crudely one expresses it, and however firm the intuition that it is true, and they come from well-known considerations to do with unconscious and sub-conscious aspects of the mind, as well as from the more rococo possibilities of more than one conscious individual within a single body, which has recently been connected with research on the status and relationship of the two hemispheres of the brain (cf.
Wilkes 1978).
Block (1980) has argued that one can construct a case where one would be forced to say that a conscious individual not only contained, but consisted of, other conscious individuals.
His ingenious situation concerns very small homunculi from space, of the order of magnitude of body cells, who enter a human being in large numbers and colonize him to such an extent that he is eventually made up of small conscious entities.
In some sense, we are then forced to say that he simply is them, physically speaking, and the problem, of course(for a view like the one put forward here), concerns the relationship of his consciousness to theirs.
To this I find no difficulty in replying briskly that the human's consciousness does not have the consciousnesses of the homunculi as parts any more than two Siamese Twins that share digestive organs have the unity and separateness of their respective consciousnesses compromised.
Again, we have to consider both modularity and program level as possible explications of this assumed unity of consciousness.
In the case of modularity, there seems a natural explication if we take seriously Minsky's  idea of a highest organizing module.
It will be remembered that his other, earlier, intuitions about heterarchy pulled in the opposite direction, towards a monadology of separate partial sub-consciousnesses.
I shall argue below, in connection with Dennett and Sloman, that our commonsense intuitions about the unity of consciousness are better preserved by analogy with program level than with program module .
Before turning to the work of Sayre, Dennett and Sloman, I should at least mention one major question that has been left unasked in this paper, and intentionally so: namely, what properties would a machine have to have in order to be sufficient for us to deem it conscious?
I simply do not know, at least not if the properties are to be non-trivial, and more than an acceptance of certain machines into the category of human beings by fiat or polite convention.
One might guess that a substantive discussion of this issue would centre on the question of linguistic performance: Danto (1960) suggested that we will do violence to the English language if machines achieve certain linguistic performances but we still refuse to deem them conscious.
However, such performances cannot be necessary conditions for such ascription, because of all those we would naturally deem conscious but who cannot, for various reasons, provide them.
A problem might arise here because the phenomena discussed in this paper — vacuity, upwards inference and so on— could only be established for entities capable of some degree of linguistic performance.
This difficulty could be fatal to the argument of this paper, if successful, for then the proposed necessary conditions would, in their turn, rest upon a sufficient condition, namely having language.
The first answer is to argue that the computer-science phenomena described here could be established by non-linguistic means (e.g. hardware readings of some sort).
The other, more plausible, avenue is to accept that the phenomena discussed would normally be established by linguistic means but that is in no way a necessary feature of them, and so no problem of principle need arise.
Sayre
I shall give a quotation from Sayre simply to show the difference between the approaches to such issues as consciousness in the AI tradition and in the cybernetics tradition; both, after all, are concerned with the implications of intelligent machines for consciousness, etc.
But this, of course, makes no serious claim at all about the notion of consciousness, in the sense in which the word is normally used: it simply sets up a definition of the term on which virtually any organism higher than an amoeba is conscious, as would be many existing computers.
He tells us at the beginning of his book that he is an Aristotelean in these matters, but the price he pays is that his definition simply fails to bear upon the intuitive criteria for an explication of consciousness, such as those set out above.
Dennett
In his Brainstorms, Dennett set out to: ‘…sketch a theory of consciousness that can be continuous with and help unify current cognitivist theories of perception, problem solving and language use’(Dennett 1979: 149).
His strategy is to argue from intuitive examples and psychological results that very little is in fact available to consciousness: that it is vacuous in the sense used above.
His most striking example is derived from an experiment  by Lackner and Garrett in which subjects heard sentences like ‘He put out the lantern to signal the attack’(which is ambiguous as between place outside and extinguish).
One group received just the sentence, while another group heard ‘disambiguating input’ through a channel they were not explicitly attending to (i.e. an additional sentence such as‘He extinguished the lantern’).
Naturally enough, the latter group interpreted the first sentence appropriately but were unable to report what they heard through the unattended channel.
In other words, they were not at all conscious of the perceptual information that had solved the problem for them.
The centrepiece of the paper is a flow chart, which I reproduce here (Dennett 1979: 155).
The heart of Dennett's case is that very little is available to consciousness: we just get the results.
We are conscious, in general, of some of our memory, and the results of speech generation, though in the latter case we are in much the same position as any other observer (cf.
E. M. Forster's ‘How do I know what I think till I see what I say’).
A principal feature of Dennett's case is that we posit features of our unconscious apparatus, which have no necessary connection with the actual thoughts we have, any more than had Hume's perceptions of causation with perceptions of causes.
This is an observation in line with his general theory of psychological predicates, and consistent with, for example, Minsky's notion of self-model that we discussed earlier.
Our folk theories of our contents of consciousness may be as wildly false to the facts as are our folk theories of grammar to the language that comes out of our mouths.
None of this shows, of course, that the folk theories, even if false, are not well and truly in consciousness; indeed if they were not one could not truly be said to be working with a false theory of the mind (unless theory became no more than a structure of, possibly unconscious, assumptions).
So the observations about folk theories of consciousness, though highly interesting, do not in themselves show consciousness to be more impoverished than we had earlier thought, for fictions are as good as fillers of consciousness as are truths.
No, the difficulty I have with Dennett's excellent paper, and the reason I have brought it into discussion here, has to do with the fact that he never actually says which parts of the diagram are in consciousness and his view is consistent with consciousness being (a) the ‘control box’,(b) sometimes one box and sometimes another (very like the ‘heterarchical aspect’ of Minsky's views, which I earlier called a ‘pinball machine’theory of consciousness), or (c) some elements of what passed down the communication channels, that is to say, the lines between the boxes.
The last view is suggested by his claims that we are conscious of some  memories (those brought to INTROSPECTION in the ‘control box’ for example) and some of the commands to say things (as in deliberately saying something, as distinct from finding ourselves saying it), which would be messages from the ‘control box’to PR (the ‘public relations box’, by analogy with the White House public relations chief, who is simply handed a piece of paper telling him what to say, but not to think about).
I have no quarrel with interpretation (c) and think it probably consistent with my suggestions in this paper that the notion of the ‘highest level of program’ is the best available explication of consciousness.
But one problem about showing that is that Dennett has opted (perhaps unnecessarily for his argument) for a flow-chart explication, which is inherently a static rather than a process one.
It is no accident that people in AI rarely draw flow-charts, and it is a cliché of programming in LISP that if one finds oneself drawing a flow-chart, one has not understood how LISP works.
Now merely saying that is not evidence of any kind (and may merely annoy those AI workers who program in languages other than LISP and do use flow-charts), but it does bring out something of the opposition between modules and levels that is the heart of the last part of this paper: flow-chart boxes are essentially separated from each other in ways like those that separate the modules of programs; but program levels are not like that.
The LIFT-RIGHT-LEG sub-function of the WALK function is not a part of it and in no way a module of it.
My argument here with Dennett is that if he intends interpretations (a) or (b)(a textual quotation here would be beside the point, since these distinctions are not made in his paper in that form), then those are inferior explications of consciousness to a process one in terms of level:(b) because it lacks the ‘unity’ requirement, and (a) because, although it could meet all the necessary conditions set out earlier, it seems in some way arbitrary, just as did Minsky's supreme organizing module.
After all, why should it be that particular one?
Given the richness of interconnections between the boxes, it could have been any one of them that was dubbed the conscious one, so we would need something more by way of justification for choosing any particular one.
In the case of levels, opting for the uppermost of a set of them seems less arbitrary.
I have not touched on the difficult question of localization here: it is a lively issue where the brain is concerned, but not for the digital computer because, depending on how you interpret its operations, information can be thought of as being anywhere at all in the machine (i.e. not localized in the sense of being associated with a particular place in it), or (and in some sense conversely) all operations of the machine can be thought of as going on in one very small and specific place.
Neither of these views is of much interest for brain scientists seeking suggestive computer models.
So, we can think of  different modules from Dennett's flow-chart as being stored and functioning in the same place in the machine, just as we can think of different levels of translation of programming language as being carried out in the same place in the machine.
Nonetheless, the flow-chart-cum-module model is much more open to interpretation in terms of localization (if only because both are normally explained and displayed spatially), and therefore any explication of consciousness in their terms will tend to be a localized model, or what I referred to as a pinball-machine view of consciousness, one that seems to me a priori implausible.
Sloman
Sloman (1978) straightforwardly argues for a conscious module or sub-part view, of just the sort I have opposed to a program-level view.
He describes, as I did, the lack of access between the interiors of modules, but accepts the view of consciousness that I associated with Minsky's views on heterarchy: that, roughly speaking, sometimes one module would be conscious and sometimes another, depending on circumstances:
In the terms established earlier this, like the Monadology itself, seems an unsatisfactory account of consciousness in that it ignores the unity principle, for what that is worth.
Viewed as an ‘independent robot’, I am happy to admit that a particular area of my brain controls my digestive processes and ‘I’ do not, but I certainly do not want to say that that brain area is ‘aware’of those processes.
Sloman prompts one to ask what it could be like to be an entity that controlled all existing (presumably conscious) human beings but had no access to what went on in their conscious minds at all, and of which,ex hypothesi , the humans themselves were not aware?
To use the argument brought forward against the related speculation of Block (i.e.  about the tiny homunculi), one might at the very least want to deny that the higher and lower consciousnesses could, in any sense, be the same consciousnesses, whether or not they could be said to be in a physical part — whole relationship.
The supreme organizer picture set out above, whose sub-modules are controlled but themselves conscious (though not of the control itself), is a well-known theological pattern and has been explored elsewhere in detail: a God who is omnipotent, conscious, but not omniscient in that he can look into human brains while being unable to see what they are thinking.
Whatever the heady consequences of that (and they have classically been found contradictory), Sloman's explication does not tackle the question of why it is we are conscious of certain things at some times but not at others.
In that sense it is, like Dennett's, a picture-cum-flow-chart view, rather than one in terms of processes.
I have done little to remedy that myself, except to fall back on the preferred notion of level, which at least can begin to explicate how things can be reached by effort at some times but not others.
However, it too is in need of a great deal of further explanation.
The concept of level, it must be said, is not an original metaphor to bring to bear on the topic of consciousness: mystics have always talked of levels of consciousness, and the nineteenth-century vitalists wrote at length of the emergence of a level of consciousness from a sufficiently complex lower level of organization.
What I believe a notion of computer language level can do, though I have not achieved it here, is to put some procedural flesh on those traditional bones.
ANIMAL PERCEPTION FROM AN ARTIFICIAL INTELLIGENCE VIEWPOINT Margaret A. Boden
I
Years ago, I saw in the pages of Punch a cartoon more memorable than most (I have redrawn it in Figure 1).
It showed a kingfisher sitting on a willow-branch, staring at a fish in the river below, and thinking to itself, ‘’.
This cartoon is no mere triviality, for it is a reminder of some deeply puzzling questions.
How does the kingfisher manage to catch the fish, no matter — within limits — where it is in the water?
(Unlike some birds, it does not dive vertically into the water, nor does it pursue the fish while under water; kingfishers are plunge-divers, who go rapidly straight to the target.)
Given that it has never heard of Snell's law, does it have to go through some alternative process of computation to adjust its angle-of-dive appropriately — and if so, what?
Or is its action based on a bodily skill whose exercise requires no computations?
How does the bird manage to identify part of the scene as a fish, or as food, in the first place, and how is it thereupon led to take appropriate action (that is, how does it know that it should dive, irrespective of how steep the dive should be)?
And, the most perplexing puzzle of all, just what is the phenomenal nature of the kingfisher's experience as it gazes into the water?
Similar puzzles arise with respect to all animal species.
In his seminal paper on ‘the invisible worlds of animals and men’, Jacob von Uexkull defined the task of comparative psychology as the articulation of the varied Umwelten of different creatures, by identifying what a given species can perceive and what it can do accordingly (von Uexkull 1957).
Von Uexkull illustrated this definition by his unforgettable pictures of the living-room as seen by fly, dog, or man, and of the fish and the boat as seen by a sea-urchin.
But these pictures are more charming than they are clear.
Not only do they fail to express what it is like to be a sea-urchin, fly, or dog, but they also fail to articulate the specific psychological functions involved.
Precisely what are the cognitive functions, or epistemological resources, of different animals?
Given that dogs can discriminate three-dimensional objects in living-rooms, and that kingfishers can catch fish viewed from above the water, how do they manage to do so?
And what are the constraints on possible perceptions: for instance, is it in principle possible that a creature able to perceive the motion of an individual object might be able to perceive the object's shape?
These questions concern the information being used by the animal, and the way in which the animal is using it, so the question arises whether artificial intelligence (AI) might help us solve them.
For AI provides a wide range of concepts defining the reception, storage, transformation, interpretation, and use of information by information-processing systems.
The systems that are the special concern of AI are systems employing symbolic representations, and their interpretative procedures are symbol-manipulating processes.
However, it is often regarded as problematic whether or not animals have mental representations, or use symbolic systems or languages.
Sometimes it is even stated categorically that they do not.
To some degree, these disputes turn on differences in the use of terms such as‘representation’ and ‘language’.
For example, some people define  ‘language’ in such a way that only a system of intentional communication between conspecifics could count as language, and some regard abstract features such as syntactic structure or individuating reference to past events as necessary to ‘language’.
But even setting aside such terminological differences, it remains true that whether or not any animals employ symbolic representations is widely regarded as doubtful.
Certainly, one cannot argue that all computations carried out by animals must be effected symbolically.
For example, hoverflies appear to compute their interception paths with conspecifics according to a simply specifiable rule, one which could plausibly be ‘hard-wired’ into the flies' brains (Collett & Land 1978).
Although this rule could be represented and applied within a symbolic system, it is reasonable to suggest that it has been ‘learnt’ by the evolutionary process and is embodied in the flies' neurophysiology.
It is significant, however, that the feedback involved is fairly simple, and dependent on only a few physical parameters, so that the computations concerned are relatively inflexible.
The fly in effect assumes that the size and velocity of the target are always those corresponding to hoverflies, and on this rigid (and fallible) basis the creature determines its angle of turn, when initiating its flight, according to the variable approach angle subtended by the target.
Moreover, the fly's path cannot be adjusted in mid-flight, there being no way in which the pursuer can be influenced by feedback from the (perhaps unpredictable) movement of the target animal.
Similarly rigid behaviour is fairly common in insects.
But the higher animals are capable of considerable flexibility in adjusting their behaviour to widely differing (and continuously changing) circumstances, where the relevant ‘parameters’ are structural features, rather than physical ones (such as angle-of-approach).
The freedom of much animal behaviour from physically specifiable constraints implies that many animals must have inner representations and symbolic language.
Only thus could they interpret stimulus-information sensibly in widely differing contexts and take appropriate action accordingly.
(Internal languages need not have formal features like those characteristic of spoken languages: we shall see that some representations hypothesized in AI are very different from verbal or mathematical strings.)
The more flexible the action, the more complex must be the computational resources for monitoring, planning, and scheduling different types of activity.
In particular, when the creature has to take account of a wide range of structural differences and similarities between distinct situations (as opposed to concentrating on only one or a few physical parameters), these structural features can only be represented symbolically — for, by hypothesis, they have no physical features in common .
This is true irrespective of whether the animal is able to communicate with its conspecifics — either by way of warning-cries, mating-calls, and the  like, or by means of a syntactically-structured language whose meaning is determined by social conventions rather than by fixed genetic mechanisms, The point is that even much non-communicative behaviour has to be understood in computational terms, such that internal symbolic processes must be attributed to the creature.
Indeed, the interpretation of audible or visible signs, words, or gestures as communications with a certain meaning presupposes the computational mechanisms involved in sensory perception in general.
This is why one philosopher deeply influenced by AI has referred to ‘the primacy of non-communicative language’(Sloman 1979).
What is meant by ‘the computational mechanisms involved in sensory perception in general’ will be clarified in the next section, where I discuss some AI ideas bearing on the puzzles about the kingfisher's perceptual competence that were listed above.
In Section III, I discuss the objection that the computational processes in a program may not be the same as the cognitive processes in the organism, and the view that AI can have no bearing on any phenomenological experience, in human or animal species.
2
Von Uexkull's assumption that all animal species have perceptual capacities functionally coordinated with their motor activity is shared by comparative psychologists in general.
For example, the behaviour of many species is assumed by ethologists to be dependent on motion-perception and object-concepts of some sort.
Just what sort, however, is usually unclear.
The perceptual side of the Umwelt has received less attention than the motor aspect, the study of animal perception (as opposed to sensory physiology or discriminative behaviour) having been inhabited by the explicitly anti-mentalistic bias of behaviourism.
Significantly, it has been held back also by the absence of concepts suited to expressing the psychological processes involved in perception.
The lack of suitable concepts has meant that even in the human case, where introspective reports are available, the psychological nature of motion-perception is not understood.
So, although we know that the phenomenology of motion-perception varies under differing conditions, we do not understand why our experience varies as it does.
For instance, if a person is shown two differing views successively, then one off our phenomenologically distinct perceptions may arise.
We may see an object (visible in the earlier view) disappearing, and being replaced by another one — as in a game of ‘peekaboo’.
We may see one and the same (rigid) object moving , perhaps involving a change in its appearance due to rotation.
We may sec one and the same object changing in shape so as to be transformed into something different — as the baby that Alice was holding  gradually turned into a pig before her eyes.
Finally, we may see an object moving and changing shape at the same time (as does a walking mammal).
The conditions under which these several perceptions are differentially elicited can be studied by the technique of ‘apparent movement’, wherein two stimuli or input-arrays are presented successively to the visual system and subjects are asked to report what they see.
This technique was originated by the Gestalt school, and since their early work experimental psychologists have amassed a great deal of information about which parameters influence the occurrence of apparent motion.
But the rationale for these facts is not well understood: why do these parameters affect motion-perception in the way that they do?
Some recent AI-based work done by Shimon Ullman poses computational questions and offers hypothetical answers that suggest how such phenomena are possible (Ullman 1979a, 1979b).
These suggestions are relevant not only to human vision (Ullman's prime focus), but to animal vision also.
For Ullman is concerned with types of perceptual process that can plausibly be attributed to many animal species.
Like the psychologist J.J. Gibson, Ullman attempts to show that many perceptual features can be recognized by relatively low-level psychophysiological mechanisms, whose functioning relies on the information available in the ambient light rather than on high-level concepts or cerebral schemata (Gibson 1950, 1966, 1979).
But unlike Gibson, who posits a ‘direct’ unanalysable perceptual process of ‘information pick-up’, Ullman views this functioning as a significantly complex process intelligible in computational terms.
(Ullman's work is thus closer to the spirit of Kant, and may be thought of as an attempt to articulate the intuitions and categories of space which structure and give meaning to the input stimulus.)
Ullman reminds us of the facts about the phenomenology of motion perception that were listed above.
His project is to discover the series of computations that the visual system performs on the input-pairs so as to arrive at an interpretation of the (2-D) array in terms of(3-D) replacement, motion, or change.
In particular, he asks whether (and how ) these distinct percepts can be differentially generated without assuming reliance on high-level concepts of specific 3-D objects (such as fish or sticklebacks), and even without assuming the prior recognition of a specific overall shape (such as a sort of narrow pointed ellipse with sharp projections on its upper surface).
Ullman thus attempts to follow Lloyd Morgan's Canon, asking what are the minimal computational processes that need to be posited to explain motion-perception.
As regards the visual interpretation of each array considered in isolation, Ullman relies on the work of David Marr, who studied the information  picked up from the ambient light by the retina, and the image-forming computations performed on it by peripheral levels of the visual system (Marr 1976, 1978, 1979).
The first stage of visual computation, according to Marr, results in descriptions of the scene in terms of features like shading-edge, extended-edge, line , and blob (which vary as to fuzziness, contrast, lightness, position, orientation, size , and termination points).
These epistemological primitives are the putative result of pre-processing of the original intensity array at the retinal level — that is, they are not computations performed by the visual cortex (still less, the cerebral cortex).
Marr defines further computations on these primitive descriptions, which group lines, points, and blobs together in various ways, resulting in the separation of figure and ground.
He stresses that these perceptual computations construct the image, which is a symbolic description (or articulated representation) of the scene based on the initial stimulus-array.
The computations are thus interpretative processes, carried out by the visual system considered as a symbol-manipulating system rather than simply as a physical transducer (though Marr attempts to ground his computational hypotheses in specific facts of visual psychophysiology).
Starting with Marr's basic meaningful units, Ullman defines further visual computations which would enable the system, presented with two differing views, to make a perceptual decision between replacement, motion, or change.
Ullman divides the computational problem faced by the visual system into two logically distinct parts, which he calls the correspondence and the interpretation problems.
(The latter term unfortunately obscures the fact that all these computations, including Marr's, are interpretative processes, carried out by the visual system in its role as a symbol-manipulating device.)
The correspondence problem is to identify specific portions of the changing image as representing the same object at different times.
This identity-computation must succeed if the final perception is to be that of a single object, whether in motion or in change.
Conversely, the perception of replacement presupposes that no such identity could be established at the correspondence stage.
The interpretation problem is to identify parts of the input arrays as representing objects, with certain 3-D shapes, and moving through 3-space (if they are moving) in a specific way.
In principle, correspondence- and interpretation-computations together can distinguish between the three types of perception in question.
And, if specific hypothetical examples of such computations are to be of any interest to students of biological organisms, they should be able to distinguish reliably (though not necessarily infallibly) between equivalent changes in the real-world environment.
This last point is relevant to the way in which Ullman defines specific  correspondence- and interpretation-algorithms.
In principle, any part of one 2-D view could correspond with (be an appearance of the same object as) many different parts of another; similarly, any 2-D view has indefinitely many possible 3-D interpretations.
(Anyone who doubts this should recall the images facing them in distorting mirrors at funfairs.)
Faced with this difficulty, Ullman makes specific assumptions about normal viewing conditions, and takes into account certain physical and geometrical properties of the real world, as well as(human) psychological evidence based on studies of apparent motion.
Accordingly, he formulates a hypothetical set of computational constraints which he claims will both assess the degree of match between two views so as to choose the better one, and typically force a 3-D interpretation which is both unique and veridical.
For instance, for the correspondence stage he defines ‘affinity functions’ that compute the degree of match between two points or short line-segments, depending on their distance, brightness, retinal position, inter-stimulus (time) interval, length, and orientation.
And for the interpretation stage, he defines a way of computing the shape and motion of a rigid object from three views of it, making his system assume that if such a computation succeeds then it is indeed faced with a rigid body in motion (as opposed to two different objects or one object changing its shape).
He justifies this by proving mathematically that, except in highly abnormal viewing conditions, three views of a rigid object can uniquely determine its shape and motion.
Given that Ullman's computations can indeed interpret correspondence, shape, and motion in a wide range of paired 2-D views (which has been tested by running his system in its programmed form on a computer provided with the relevant input), how is his work relevant to questions about perception in animals?
The first thing to notice is that Ullman builds implicit assumptions about the physics and geometry of the real world, and about biologically normal viewing conditions, into the computations carried out by the visual system.
It is plausible that many species may have evolved such implicit computational constraints.
That is, the animal's mind may implicitly embody knowledge about its external environment, which knowledge is used by it in its perceptual interpretations.
Something of the sort seems to be true for migratory birds, who have some practical grasp of the earth's magnetic field or of stellar constellations; and, as I shall suggest presently, the kingfisher may have some practical grasp of the refractive properties of water.
A ‘practical grasp’ of physical properties is not the same thing as the ability to articulate and reason about them by way of verbal symbols.
So I am here attributing to the kingfisher ‘knowledge how’ rather than‘knowledge that’.
(The view that the bird's ability to go straight to the fish is the  exercise of a ‘bodily skill’, supposed to be quite independent of any representation or computation, will be discussed in Section III.)
A creature has a practical grasp of a domain if its behaviour is, within limits, successfully adjusted to the underlying constraints of the domain.
If adjustment to these constraints appears (on the basis of psychological and/or physiological evidence) to be innate rather than learned, they may be said to be ‘built in’, or ‘hard-wired’.
However, that a constraint is built in does not mean that the animal must necessarily be (like the hoverfly) capable only of a rigid, predetermined response to any given value of the environmental parameter concerned.
It is in principle possible that (as in the perceptual systems posited — and simulated — by Ullman and Marr) the ability to construct symbolic representations of certain aspects of the environment is itself built in to the animal's perceptual system.
The terms ‘built in’ and ‘embodied’thus have rather different implications.
Knowledge is embodied if the functions in virtue of which the knowledge is attributed to the organism are at base carried out by bodily (physiological) mechanisms; knowledge is built in if its embodiment is determined genetically rather than experientially.
The knowledge of an empiricist's guardian angel would presumably be neither embodied nor built in.
What is ethologically implausible about Ullman's hypotheses is not that they involve some (unconscious) knowledge about material objects and normal viewing conditions, but rather that they assume the perception of rigid objects to be basic, while perception of non-rigid movement is taken to be a more complex special case.
Mathematically , of course, the perception of non-rigid motion is more complex; but this does not prove that it is biologically secondary to the perception of rigid objects.
At least in the higher animals, it is more likely that the visual perception of shape and motion have evolved in response to such biologically significant environmental features as the gait or stance of hunter or prey, or the facial grimaces and tail-waving of conspecifics.
The fact that human beings do not always perceive the correct (rigid) structure when presented with a mathematically adequate though impoverished stimulus, may be due not (as Ullman suggests) to their failing to pick up all of the mathematically necessary information in the stimulus, but rather to their using computational strategies evolved for the perception of non-rigid objects which —even when directed at rigid objects — need more information than is present in the experimental stimulus concerned.
Admittedly, a robot could be provided with an Ullmanesque capacity to perceive rigid objects in motion; but whether any creature on the phytogenetic scale employs such visual mechanisms is another question.
Our friend the kingfisher apparently possesses computational  mechanisms which can discover the real position of a fish at varying depths in the water.
Ullman's general approach suggests that these could well be relatively low-level processes, not requiring cerebral computations (as puzzling out the correct values of the variables in the formula for Snell's law presumably does).
For the visual computations algorithmically defined by Ullman do not depend on high-level processes capable of identifying (recognizing) objects as members of a specific class: the system does not need to know that an object is a fish, or even that it has the 3-D shape that it has, in order to know that it is an object.
Nor does it need any familiarity with the object; that is, it does not need to have experienced those two views in association beforehand.
Ullman therefore suggests (contra empiricists and Piaget) that a baby — or, one might add, a kingfisher — can see that two appearances are views of one and the same object even if it has never seen that sort of object before, and even if it has no tactile or manipulative evidence suggesting that they pertain to one and the same thing.
These conclusions follow from the fact that all of the correspondence-computation, and much of the interpretation-computation, is via low-level, autonomous processes that do not depend on recognition of the input as a familiar 3-D object.
The correspondence-computations match primitive elements (those defined by Marr) in successive views, and do not depend on computation of the overall shape as a whole.
It follows that creatures incapable of computing shape in any detail, or of recognizing different classes of physical object, may nonetheless be able to compute motion.
As the example of von Uexkull's sea-urchin suggests, this is no news to ethologists, who often have behavioural evidence that an animal can perceive motion though they doubt its ability to be aware of detailed shapes.
But Ullman's achievement is to have complemented this empirically-based intuition by a set of admirably clear hypotheses about precisely what visual computations may be involved, at least in the human case.
That some of his hypotheses are biologically dubious does not destroy the interest of his general approach.
If one accepts that comparable hypotheses may explain differential phenomenology (such as the different experiences of motion-perception previously described), then his work shows that it is in principle possible for a creature incapable of experiencing distinct shapes to be aware of motion and to ascribe it correctly to an individual object.
Ullman's work also casts some light on our kingfisher cartoon.
For if the general shape, the location, and the motion of objects can be computed in a low-level, autonomous fashion, then it is not impossible that a kingfisher may possess comparable perceptual mechanisms capable of computing the depth of a fish in water.
The refractive index of water would be implicitly  embodied in these computational mechanisms, perhaps in an unalterable fashion.
So a kingfisher experimentally required to dive into oil might starve to death, like newborn chicks provided with distorting goggles that shift the light five degrees to the right, who never learn to peck for grains of corn in the right place (Hess 1956).
This assumes (what is the case for the chicks) that the kingfisher utilizes an inborn visuomotor coordination, linking the perceptual active aspects of its Umwelt , a coordination that is not only innate but unalterable.
Psychological experiments on human beings, and comparable studies of chimps, show that these species by contrast can learn to adjust to some systematic distortions of the physics of the visual field (Stratton 1896, 1897; Kohler, 1962).
Our discussion so far implies that many animals are lay physicists, that they have implicit knowledge of real-world properties (such as the optics of 3-D objects viewed in air or water) that can be explicitly described by professional physicists.
Moreover, their physical knowledge contributes to their perception, so that they see something as an object, or as moving.
Presumably, many species perceive their environment in a way that is informed by a variety of concepts and inferential structures (innate and acquired) embodying everyday knowledge of the material world.
Diving animals need some grasp of the difference between solids and fluids , as well as of depth, movement , and distance .
A cat or monkey using its perceptual powers in leaping from wall to wall, or branch to branch, needs some representation of stability and support .
Newborn creatures who refuse to cross a ‘visual cliff’ apparently have some innate procedure for recognizing the absence of support , where the object to be supported is their own body.
It does not follow that they understand in any sense that the bottom bricks of a tower support the top ones — although this is something which a leaping animal living in a jungle or an untidy house may have to learn.
For leaping creatures who can recognize the potential for action in a pile of bricks,support has something to do with above .
In general, having a concept (whether linguistic or pre-linguistic) involves being able to draw inferences linking it usefully with other concepts.
Psychologists theorizing about animal Umwelten need to ask what such inferences might be, what is the perceptual evidence in which the animal's concepts are anchored, and what are the motor activities which test for them or which are carried out on the basis of conditional tests defined in terms of them.
In his ‘Naive physics Manifesto’, P.J. Hayes draws on AI-ideas about the representation of knowledge to ask comparable questions about the human being's everyday understanding of the physical world.
One might be tempted to dismiss such an enquiry as irrelevant to our discussion: human beings have Newton and Einstein, whereas animals do not, so  human knowledge of physics cannot be relevant to enquiries about chimps, beavers, or bees.
That this would be an inappropriate objection is evident from the fact that the Punch cartoon I mentioned earlier would have been almost as funny if it had figured a human fisherman rather than a kingfisher.
Not only do we not usually think of Snell's law when we try to net a fish or tickle a trout, but we could not use it to help us do so even if we did.
Our everyday perceptions interpret our environment in terms of pre-theoretical concepts such as weight, support, velocity, height, inside/outside, next to, boundary, path, entrance, obstacle, fluid , and cause (to name but a few).
It is this pre-theoretical knowledge which interests Hayes.
Like the understanding of animals, our earliest knowledge of naive physics is independent of language.
Hayes posits pre-linguistic conceptual networks as the basis of the infant's sensorimotor understanding.
But most of his detailed epistemological claims concern adult human perception, which is informed through and through by natural language.
The degree to which detailed descriptions of the human Umwelt are deemed relevant to animal Umwelten depends on one's view of philosophical semantics.
Hayes argues for a semantics from which it follows that, once natural language is acquired, the meaning of the more primitive core concepts is altered — not merely added to.
He claims that the meaning of a token in a representational system cannot in general be captured definitionally (in terms of semantic primitives, for instance), but depends upon the entire formalization of which the token is part.
On this view, a change to any part of the system can, in principle, alter the meaning of every other part of it.
So, even if we had a precise account of adult human knowledge of inside, support , and behind , we could not equate any part of this with a cat's or a chimp's knowledge simply by jettisoning those parts of it influenced by our linguistic representations.
Rather, we would need to be able to trace the development of our naive physical concepts, distinguishing their earlier, sensorimotor, forms from the later, linguistically-informed, semantic contents and inferential patterns.
Hayes makes some relevant remarks, but even more apposite here is the computationally-informed work of the psycholinguists G. A. Miller and P. N. Johnson-Laird, who have studied the basic perceptual procedures in which our linguistic abilities are grounded (Miller & Johnson-Laird 1976).
Miller and Johnson-Laird define a number of perceptual discriminations in detailed procedural terms, utilizing what is known about our sensorimotor equipment and development.
They then show how these discriminatory procedures could come to function as the semantic anchoring of our lexicon.
For example, perceptual predicates that can be procedurally defined include the following spatial descriptions:x is higher than y ; the distance from x toy is zero; x is in front of the moving object y; y is between x  and z; x has boundary y; x is convex;x is changing shape;x has the exterior surface y; x is included spatially in y; x, y , and z lie in a straight line; x travels along the path p .
They give both psychological and physiological evidence for the primacy of these notions, and they use them to define object-recognizing routines of increasing power.
Their sensitivity to computational issues leads them to ask not only which predicates are involved in a certain judgement, but when each predicate is applied in the judgemental process.
(For example, the logically equivalent ‘y over x ’ and ‘x under y ’are not psychologically equivalent: the first term in the relation should designate the thing whose location is to be determined, while the second should represent the immobile landmark that can be used to determine it.)
The perceptual routines they define as the meaning of words such as‘in’, ‘on’, ‘outside’, and ‘at’ are surprisingly complex.
Were a chimp to grasp the meaning of ‘in’ or ‘on’in Ameslan, therefore, this would presuppose extremely complex perceptual computations on the chimp's part.
And animals which, unlike chimps, have no great manipulative ability, would not be able to compute those perceptual discriminations requiring motor activities such as putting bananas inside boxes, so that their understanding of naive physics would be correspondingly impoverished.
General results in the theory of computation might throw light on animal perception, by showing that a given type of representation in principle could not express a certain type of information, or that it would he enormously less efficient than most other type.
Abstract considerations show that computational mechanisms of a certain type (such as a nervous net with no significant prior structure) simply cannot achieve specific kinds of spatial pattern recognition (Minsky & Papert 1969).
And certain mechanisms capable of performing some non-trivial computations are incapable of performing others which at first sight might appear to be within their range.
For example, a system that can compute convexity may be in principle incapable of computing spatial connectedness (Rosenblatt 1958).
So the prima facie plausible assumption that any creature able to perceive convexity would also be aware of connectedness is false.
Again, ‘analogical’ representations ma), be more computationally, efficient than ‘Fregean’ones (Sloman 1978a: 144–76).
An analogical representation is one in which there is some significant correspondence between the structure of the representation and the structure of the thing represented, By contrast, a Fregean representation need have no such correspondence, since the structure of the representation reflects not the structure of the thing itself, but the structure of the procedure (thought process) by which that thing is identified.
To understand a Fregean representation is to know how to interpret it so as to establish what it is referring to, basically by the method described by the logician Frege as applying functions to  arguments .
Analogical representations, however, are understood or interpreted by matching the two structures concerned (that is, of the representation itself and of the domain represented), and their associated inference-procedures, in a systematic way.
Applying this distinction to our kingfisher cartoon, for example, the formula expressing Snell's law is a Fregean representation, whereas the diagram itself (with the lines representing the paths of light and constructing the relevant angles) is an analogical representation.
The use — and usefulness — of analogical representation has been exemplified in a program that can reason from visual diagrams (Funt 1980).
The program's task, given a diagram like that of Figure 2, is to discover whether the arrangements of blocks depicted is stable,— if it is not — to predict the movements (falling, sliding, motion ended by contact with another block) and the final state of various blocks.
The answers to these questions are discovered from the diagram (given certain simple diagrammatic transformations carried out by the system, which are structurally analogous to changes that would happen in the real world), rather than being computed in terms of abstract mathematical equations and specific numerical values.
Much as it is ‘obvious’ to us from the diagram (though not from a verbal or mathematical description of the same sate-of-affairs) that B will hit D, that D will then tilt with its left half moving downwards, and that B will end up touching both A and D but not the ground, so it is easily discoverable by the program that this is what will happen.
the program ‘imagines’ gradual changes in the position of the blocks.
So for instance it imagines gradually moving an unstable block (such as B) downwards, pivoting on the relevant point of support.
It studies ‘snapshots’ of the successive positions, and so discovers specific points of contact with coincidentally present blocks (such as D) which will interrupt the fall that  would have been predicted by a theoretical physicist from equations and measurements describing A and B. As in this case, many detailed relations between blocks are implicit in the diagrammatic representation which could be explicitly stated only with the greatest difficulty.
Also, what space is initially empty, and what would remain empty after stabilization of the blocks, can be directly discovered from the diagram and the imagined snapshots.
This work is relevant to the topic of naive physics discussed earlier, for the physical knowledge exploited by the program is comparable to that of the lay person rather than the physicist.
Thus it has simple computational procedures, or ‘perceptual primitives’, which address the visual array in parallel so as to identity area, centre, point of contact, symmetry, and so on.
These spatial notions are likely to be useful in many different problem domains.
Also, the program has knowledge of qualitative physical principles relevant to its actual tasks, such as that if an object sticks out too far it will fall, and that it will pivot around the support point nearest to the centre of gravity.
Moreover, since it is able to discover the empty space, and also those spaces that would remain empty throughout stabilization changes, it possesses a type of knowledge that would be crucial to an animal looking for a pathway or for a safe space through which to move.
And leaping animals, at least those whose weight might cause significant changes in the terrain leapt upon, presumably have some understanding of support and can perceive potentially dangerous or unstable structures.
3
The Punch cartoonist recognized that, from the fact that if the kingfisher were consciously applying Snell's law its dives would be (as they are) appropriately placed, it does not follow that this is indeed the explanation of its diving ability.
Analogously, if one had produced a computational model whose performance mapped onto the kingfisher's behaviour one could not thereby be certain of having captured the bird's psychology.
For, as is often pointed out by critics of cognitive psychology (e.g. Heil 1981), there is always in principle more than one model capable of matching observed behaviour.
However, this caveat is a special case of the general truth that any scientific theory is necessarily underdetermined by the evidence.
That this underdetermination causes methodological problems is well known to every practising scientist.
Someone offering computational theories of animal perception would be no worse off on this account than any other psychologist faced with the task of testing theory against data.
The special difficulty is not how to choose between several alternative computational accounts, once we have got them, but how to arrive even at  one in the first place.
Psychologists and philosophers unfamiliar with AI typically underestimate the procedural-representational complexity of human and animal minds, and may not even realize that there are unsolved computational problems related to everyday psychological descriptions.
That is, descriptions of perception and action are assumed to be unproblematic which in fact are deeply puzzling.
Thus most ethologists take the existence of various interpretative and representational capacities for granted, and concentrate on asking which of these capacities are shared by which species.
Theorists sympathetic to AI, by contrast, are primarily interested in how such capacities are computationally possible.
For instance, the experimental psychologists Premack and Rumbaugh have asked whether chimps can perceive the world as humans can, and do things which we can do.
Can a chimp perceive a movie as representing a second individual trying to solve a problem, like reaching bananas or switching on a heater?
Can a chimp plan ahead of time, either on its own behalf or on behalf of its fellow?
Can two chimps cooperate in the solution of a task, perhaps using artificial symbols as publicly observable indicators of the tool that is required at a given stage of the problem?
And so on…
Computationally-inclined psychologists or epistemologists, however, are more likely to be interested in how these things can be done, irrespective of which species manage to do them.
How is it possible for a creature to form means — end plans for reaching a desired object, plans within which other objects are represented as instruments to the overall end?
How is it possible for an external symbol, as well as one in the internal representational medium of the creature's mind, to be employed by one animal and perceived by another as a request for a specific tool?
How is it possible for a creature to perceive apparent movement, or to distinguish visually between replacement, motion, and change?
It is this difference in theoretical focus which has led one computationally-inclined philosopher to acknowledge the fascination of these recent studies of chimps, and yet to complain that such studies are premature:
Ethologists may reply that they wish to discover which achievements are within the grasp of chimps, beavers, and bees.
This is indeed a legitimate question: natural history should include comparative psychology, an account of what different animal species can do.
Many such questions have remained unasked by professional ethologists, because of the inhibitory influence of behaviourism — and even of the founding fathers of ethology, who were anxious to avoid sentimental anthropomorphism.
But it remains true that a deeper understanding of animals' minds will require careful attention to the computational processes underlying their observed abilities.
However, perhaps there is a particular reason (over and above the a priori possibility of alternative theoretical models) for denying the relevance of AI ideas to animal psychology?
Even in the human case, it may be said (Dreyfus 1972), it is doubtful whether computations like those used in AI go on (except possibly during conscious logical or mathematical calculation).
And animal minds,a fortiori , do not engage in this sort of computation (which is why the Punch cartoon would have been less funny if it had shown a human fisherman).
Introspection does not reveal complex sequences of step-by-step formal reasoning.
If anything, it suggests that many unformulated ideas influence our experience simultaneously.
Perception in particular seems relatively immediate, and the notion that a bird's perceptions are laboriously constructed by strings of formal computations is absurd.
What is more, the vertebrate brain appears to be capable of parallel processing, so programs written for digital computers are of questionable relevance to human or animal psychology.
In short, the complexity of thought may be less than is assumed by AI-workers — and, moreover, may be of a very different type.
This objection is, in part, an appeal to ignorance.
From the fact that a mental process does not appear in introspection one cannot infer that it does not go on at non-conscious levels of the mind.
What is more, there is a great deal of empirical evidence (amassed over many years) suggesting that human perception is the result of a non-introspectible process of construction, a process that takes a measurable amount of time and that can be interfered with in specific ways.
One might, of course, argue that all talk of non-conscious mental processes is so philosophically problematic as to outlaw cognitive psychology in general (irrespective of whether it uses AI-ideas)(Malcolm 1971).
This extreme viewpoint would deny to theoretical psychologists rights of extended language-use that other scientists enjoy (Martin 1973).
Short of this position, one must admit the possibility of non-introspectible mental processes.
But what of the objection that, while such processes probably do exist, they may be very different from the processes posited by AI?
The first thing  to be said in reply is a familiar Popperian theme: a clearly articulated hypothesis, which fails to match the facts in certain specifiable ways, can be a crucial stage in the development of a more satisfactory scientific understanding.
So even if AI were incapable of modelling actual thought-processes, it would not follow that nothing of theoretical interest could have been learned from it.
The second point is related to the first: the scientific research-programme that is AI includes a number of significantly different approaches.
As the reference above to ‘parallel processing’ suggests, the logical-sequential approach is not the only possible form of a computational model.
Some very early work in AI attempted to model parallel processing, but the machines available were so primitive that little was learnt from this exercise.
Most research in the field has concentrated on modelling logical-sequential computation, which is well suited to general-purpose digital computers.
As a corollary, AI-workers have tended to play down the importance of neurophysiological knowledge about the brain.
This is partly due to the fact that many significant computational questions can be pursued quite independently of hardware-considerations, but is also due to the fact that a ‘general-purpose’ machine is precisely one whose hardware is capable of carrying out indefinitely many different types of computation.
In recent years, however, neurophysiological and psychophysiological evidence has been taken more seriously by some AI researches — notably by David Marr's group at MIT.
More recently still, hardware developments have made possible a radically new approach to computation, wherein parallel processing by dedicated (as opposed to general-purpose) hardware is used to compute properties previously assumed to require highly abstract sequential processing.
One such property is shape, and it is pertinent to our topic to note that the perception of shape is being modelled in this way (Hinton 1981).
On this approach, a shape can be recognized as a whole without the constituent parts being recognized as such (a part is represented in a radically different way if it is seen as a Gestalt in its own right).
We saw in Section II that shape-perception is not needed for the recognition of object-identity, so that if we were to attribute identity-perception to a creature (perhaps because of its ability to follow a moving target) we would not thereby be justified in attributing shape-perception also.
But, if we were to attribute shape-perception to a perceiver, what computational powers would we be crediting the subject with?
If shape-perception required the application of high-level concepts, it would be implausible to say, for example, that a kingfisher can see the shape of a stickleback (which I described earlier as ‘a sort of narrow pointed ellipse with sharp projections on its upper surface’, and which we could describe in many other ways).
But this very recent AI  work suggests that the bird might be able to perceive the shape of a stickleback despite being unable to represent it in terms of high-level concepts.
Further, it implies that the kingfisher need not be able to articulate the image of the stickleback into independently recognized component parts: it could perceive a fish without being able to perceive a fin.
A computational model of this type that was initially developed for shape-perception is now being applied to the control of bodily movement.
The bodily skill of smoothly moving one's arm requires delicate compensatory movements in the various joints, as well as subtle control of velocities at different stages of movement.
Earlier attempts to compute motor-control met with very limited success.
For example, roboticists relying on sequential processing were unable to write programs capable of computing the subtly balanced flexions of shoulder, elbow, and wrist that would be necessary for smooth movement of the robot-arm.
But these aspects of motor-control are now being modelled by these new AI techniques, with encouraging results.
Many problems remain; for example, it is not yet known how to compute a path that will avoid an obstacle placed between the arm and the end-point of movement.
But these new developments in AI should give pause to those philosophers who complain that AI can have nothing to say about bodily skills, and so is questionably relevant to human and animal forms of life.
But if simultaneous perceptual processing and bodily skills are not wholly intractable to a computational analysis, what of experience itself?
Can AI have anything useful to say about consciousness?
I remarked above that von Uexkull's pictures fail to express the phenomenological quality of what it is like to be a sea-urchin, fly, or dog — or, one might add, a bat (Nagel 1974).
Nor would different pictures have succeeded where these failed.
As Nagel points out, the problem in understanding what it is like to be a bat rests on the difficulty of matching different subjectivities.
Assuming (as we all do) that our experience is somehow intrinsically different from the bat's, how could we even conceive of what the bat's experience is really like — that is to say, what it is like for the bat?
No mere subtraction or addition of conceptualizable features could transform our own experience into the bat's.
Related points were made with respect to spatial perception, in the discussion of naive physics above.
Since it is implausible to suppose that a creature's understanding of ‘inside’ is independent of its manipulative abilities, a kingfisher's perceptual experience of containment would differ from a chimp's.
Similarly, no dog could perceive a bone to be inside a box in the way in which human adults can, because a baby's sensorimotor understanding of spatial concepts is radically transformed (not merely added to) by learning of language.
This difficulty in understanding different subjectivities casts doubt on the possibility of a theoretical phenomenology, and a fortiori seems to dash any hopes of a systematic comparative psychology concerned with the experience of animals.
But at the end of his paper Nagel hints at the possibility of an objective phenomenology.
Its goal would be to describe (at least in part) the subjective character of experiences in a form comprehensible to beings incapable of having those particular experiences.
Structural features of perception, he suggests, might be accessible to objective description even though qualitative aspects are not.
Unfortunately, Nagel gives no examples of what might be meant by ‘structural’ features of phenomenology.
Could AI ideas help to clarify these suggestive remarks?
Ullman's work is premised on the phenomenological fact that human beings can experience apparent movement in several different ways.
From a subjective viewpoint, these differences do not seem to depend on linguistically-represented concepts, and moreover are of such a general character that it is implausible to ascribe them uniquely to human perceivers.
That is, if a creature's phenomenology has any dimension comparable to visual experience as we know it, we can intelligibly ask whether (and when) distinctions such as these are perceived by it.
We can ask, for instance, whether it enjoys any or all of the experiences, ‘seeing the same thing moving’, ‘seeing one thing being replaced by another’, ‘seeing a thing of a particular shape’, and ‘seeing a thing being transformed into another’.
Indeed, that such questions are intelligible is largely what is meant by saying that its phenomenology ‘has a dimension comparable to visual experience as we know it’.
The relevance of Ullman's study is that he provides a theoretical account of differential (human) phenomenology that can be empirically investigated, and which if correct would explain how and why these distinct experiences arise when they do.
This account is couched in computational terms, which are objective rather than subjective but which relate to what one might call the ‘structure’ of phenomenology.
That is, phenomenological distinctions such as those just listed can be intelligibly related to hypothetical underlying computations (whether Ullman's hypotheses are correct is not of central interest here).
Moreover, ‘structural’ relations between them can be clarified, by showing for instance that this computation is or is not a necessary prerequisite or accompaniment of that one.
For example, Ullman's demonstration that computation of shape is not necessary for computation of identity gives theoretical support to the view that a creature might be able to experience identity without being able to recognize shape.
So evidence (whether behavioural or biological) of a creature's  ability or inability to perform certain computations could count as theoretical grounds for ascribing or denying experiences of certain types to it.
This is not to identify computation with consciousness.
We know from the example of ‘blindsight’ that ‘visual’computations can occur without any conscious phenomenology (Weiskrantz 1977).
Nor can one escape the difficulty that Nagel ascribes to all ‘reductive’ theories of the mental, that their truth is logically compatible with the absence of any subjective aspect whatever.
But Nagel himself is content to take for granted that other creatures do have experiences, and he does not require of an objective phenomenology that it provide a philosophical proof of this presupposition.
Rather, he asks for what one might term a systematic study of the structural constraints of ‘seeing-as’, a study which would illuminate our own subjective life as well as enabling us to say something about the experiences of alien creatures.
Again, Nagel gives no examples to show what sort of study this might be.
Possibly relevant are computationally-influenced investigations of visual imagery that have sought to explain introspectively obvious but intuitively mysterious facts about our visual experience.
For example, it has been found that some striking perceptual differences in viewing a wire-frame cube (including, for example , the ease with which certain mental images can be formed of it) depend on which alternative structural description of the object is assigned to it by the perceiver (Hinton 1979).
That is, an object seen as one sort of structure can be experienced (and imagined) in ways different from those made possible by seeing it as another sort of structure.
This approach to visual perception illuminates the nature and generation of our own experiences, and could in principle provide theoretical grounds for saying that a bat, or a Martian, who applied i specific (objectively definable) structural description to an object would be more likely to experience a specific type of imagery accordingly.
In sum, computational ideas are in principle relevant to the psychology of animals, and to their phenomenology too.
Counterintuitive though this may seem, AI might help us understand the perceptual experiences of kingfishers as well as kings.