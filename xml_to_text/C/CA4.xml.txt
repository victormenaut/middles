

Sensitivity Analysis
DISCRETE CHANGES
Simply obtaining the optimal solution to a linear programming problem is often far from the end of the story.
The data, particularly costs and resource availability, may be estimates and therefore subject to review.
Alternatively, these parameters may change over time, and solutions of essentially the same problem at different time periods may be required.
Yet again, the optimal solution may be felt to be unsatisfactory and further reflection reveal that the reason for this is that some aspect of the problem has not been modelled adequately.
In particular, it may be necessary to add or remove constraints.
In this chapter, we will study the effect of changes in the data on the optimal solution.
This is called sensitivity analysis .
Whole books have been devoted to the topic of sensitivity analysis in linear programming, and in one chapter we can do little more than indicate basic themes.
We shall start by examining discrete changes, in which the coefficients of the objective function or the RHSs of the constraints assume new values, or constraints are added or removed.
Other changes are possible, some of which are covered in the exercises.
If we wish to find the new optimal solution we could always solve the problem,ab initio , using the simplex method.
However, the art of sensitivity analysis is in finding methods of obtaining the optimal solution with less effort, in general, using the optimal tableau of the original problem as starting point.
For such an approach to be worth while, we would expect the changes in the data to be ‘small’ in some sense, so the new optimal tableau is close to the old one.
It is impossible to quantify these concepts precisely and the assumption that changes are small is more likely to be valid in large-scale problems (where one coefficient, for example, is less important in relation to the rest of the data) than in the simple examples used for illustration, but the underlying principles are independent of such considerations.
We will illustrate the techniques involved by solving examples but the general principles should emerge sufficiently clearly to be readily applied to any problem.
We will start by noting that the optimal tableau for problem P1 is  P1/T1, ignoring for the moment the extra rows (a),(b) and (b ‘), and the extra resource column (c).
How does this change if the objective function is replaced by  The general approach to discrete changes is to first find how the optimal tableau changes when the new data is used.
This usually results in a tableau which is primal or dual infeasible and the dual or standard simplex method is used to restore optimality.
The special methods of sensitivity analysis are only used in the former step so we shall concentrate on that.
Clearly, the new objective function will change the objective row of P1/T1.
The new objective row can be calculated in the usual way, using formulae (3.6), and the result is the objective row (a).
The tableau is no longer optimal, so we must pivot in the x 1 -column to restore dual feasibility.
The PRS rule selects r = 2 and two pivots are required to restore optimality, the optimal solution being .
Now let us return to P1 and ask what happens if the additional constraint  is included in P1.
This will obviously entail an extra row in the tableau and to determine the entries in this row we will consider how the general constraint, can be incorporated in a tableau.
To achieve this, we substitute the canonical equations into the additional constraint to obtain the equation in which the coefficients are given by  where  is the coefficient of  in the additional constraint.
This derivation is parallel to that used to obtain (3.6) and the reader is asked to supply the details.
Applying (6.1) to the additional constraint leads to the additional row (b) in P1/T1.
With this row added to the tableau, primal feasibility is violated and the dual simplex method is used to restore primal feasibility.
Pivoting initially in the s 1 -column of row (b), two dual simplex iterations are required to reach an optimal tableau.
The optimal solution is .
A slight modification of the procedure will enable us to add equality constraints.
For example, if we had desired to add  to P1, we could, instead, have added C, but then, as soon as became non-basic (after the first pivot), drop  and its column.
We have, therefore, treated  as if it were an artificial variable.
The optimal solution does not change but the optimal tableau has one less column.
If the equality constraint to be added had been  then just replacing the RHS of C with 4 would have changed the resource column entry in row (b) of P1/T1 to  no pivoting would take place.
In this case, rather than C, we add the constraint  which leads to the additional row (b’).
The new tableau is not primal feasible and pivoting in the x 1 -column of row (b ‘) gives an optimal tableau with .
The general procedure, given an equality constraint, is to add whichever inequality is not satisfied by the current solution.
This will ensure that the augmented tableau is not primal feasible and a pivot in the added row will make the slack variable non-basic.
This variable and its column are then dropped.
When the equality is exactly satisfied, either inequality can be used but the optimal solution will not change.
To remove an inequality constraint is straightforward if the slack variable for  that constraint is basic; we just drop the corresponding row from the tableau.
If the slack variable is non-basic, we can pivot to make it basic and then drop the corresponding row.
The choice of pivot row to achieve this can affect the computational effort required to restore optimality and is investigated in Exercise 1.
We will now consider the effect of changing the RHSs of the constraints of P1 and we must first see how to calculate the new resource column in P1/T1.
A general formula can be deduced on the assumption that all constraints are of ‘<’ form (as in problem P of Section 5.1), in which case one feasible solution is 
Let us now set  where  refer to the optimal tableau (or any other tableau under consideration).
This says that  is the RHS of the constraint in which  is slack variable or zero if there is no such constraint.
With this new notation,(6.2) can be written  and this is a feasible solution.
Substitution in the canonical equations gives  These results can also be obtained by applying the formula (3.6) to the dual tableau.
If the RHSs of the constraints of P1 are changed for 6, 8 and 3 respectively, the new resource column entries are, for example,
in row 1:
in row 3:
in row 0:
using (6.3).
The complete resource column is shown as (c) in P1/T1.
It is not primal feasible, but a dual simplex iteration, pivoting in the x 2 -row and s 1 -column, restores optimality and the optimal solution is .
When equality constraints are present, the formulae (6.3) cannot be used, since the assumption of inequality only constraints was made in deriving (6.3).
Indeed, no comparable formula can be found, since information is lost when the artificial variables introduced in Phase I are dropped as they become non-basic.
This presents no extra problem in the PFRS method since a forward transformation of the new RHSs will generate the new pivot column.
For small problems, solved by hand, an alternative technique is available.
To illustrate it, we will consider P2 whose optimal tableau is P2/T1.
P2: maximise  subject to.
What is the new optimal solution if the RHSs of the constraints are changed to 4 and 6 respectively?
We can find the new resource column by noting that  in P2/T1.
Hence  and  must satisfy .
This gives the new resource column shown in P2/T1.
Once the new resource column has been calculated, restoration of optimality is standard.
In particular, P2/T1 shows that the new problem is infeasible.
In general, we set all non-basic variables to zero and solve for the remainder.
This set of m equations in m variables (assuming m constraints) will always have a unique solution, determining the new resource column.
PARAMETRIC PROGRAMMING
In this section, we will consider how the optimal solution of an LP can be found when the data is a function of a single parameter,.
Of course, the optimal solution will also depend on θ but, in the case when θ appears only in the coefficients of the objective function or only on the RHSs of the constraints, the techniques of the preceding section can be modified to solve the problem.If one of the  depends on 0, the problem is much more complicated and we will not discuss it further.
When there is more than one parameter, other complications ensue.
We will not discuss the general multiparametric problem, but a special, though important and extendable, case will be treated in Chapter 9.
To justify the use of parametric methods we refer the reader to the LP for paper recycling  discussed in Section 1.1.
This involves a parameter λ (the proportion of available waste paper that is actually recycled).
Since the model is speculative, this parameter is unlikely to be known in advance, so RHS parametric programming could be used to see how the optimal objective function value and solution varied with λ.
We will start by finding the optimal solution for all values of θ when the objective function of P1 is changed to .
We must first calculate the new objective row and can use the formulae (3.6) for  to do this.
For example, we find .
For θ = 0, these values agree with the objective row of P1/T1 and we can augment P1/T1 with an extra row displaying the coefficients of 0.
The resulting tableau is P1/T2.
Tableau P1/T2 is optimal provided the objective row is non-negative: and these are all satisfied provided .
In general, if  denotes the entry in the θ-row and j th column, the tableau is optimal provided  or, and .
So, if we put  then the tableau is optimal for .
If θ (or ) is not defined because no  > 0 (or <0) then the upper (or lower) bound is absent.
Returning to our example, to find the optimal solution for all θ > 0, we imagine starting with θ = 0 and then increasing θ.
The BFS of P1/T2 is optimal for θ up to ⅕.
When θ = ⅕, ¼ -1¼ θ = 0, indicating an alternative optimal solution (Section 3.5), which can be found by pivoting in the s 1 -column.
The PRS rule selects the indicated pivot (s 3 -row) and pivoting results in tableau P1/T3.
Note that the θ-row is transformed by the usual rules since the entries are simply coefficients of 0.
Applying formulae (6.4) to P1/T3 gives θ = 5 and θ = 3.
The fact that θ for P1/T3 is the same as θ for P1/T2 reflects the fact that these are alternative optimal tableaux when θ = 5.
For θ = 3 in P1/T3, an alternative optimal tableau is found by pivoting in the s 2 -column.
Pivoting leads to tableau P1/T4.
Note that although these tableaux are different, the degeneracy of P1/T3 means that the basic solution has not changed.
In P1/T4–= 3, as expected, and there is no upper bound so that P1/T4 is optimal for all θ = 3.
There is no need for further tableaux and we see that  P1/T2 is optimal for θ up to ⅕, P1/T3 between ⅕ and 3 and P1/T4 for larger values of θ.
The general procedure for θ ≥ 0 should now be clear.
In each tableau we find θ using (6.4b)(θ is equal to θ for the previous tableau) and, assuming that there is a unique j (say j = k)which achieves the maximum in (6.4b), we pivot in column k and repeat the process.
(We will see how to proceed if the maximum is achieved at more than one j in Section 6.3.)
If there is no θ, there is no upper bound and we are finished.
If no pivot is possible because  for all j , then the LP is unbounded for all θ > θ (why?).
Very often we are only interested in θ satisfying θ ≤ U. In this case we would stop as soon as a tableau is reached in which θ ≥ U.
For θ <0, we apply a similar procedure, decreasing θ from zero and calculating θ in each tableau using (6.4a).
In P1/T2 this means  and we pivot in the x 1 -column (and x 2 -row by the PRS rule) to get P1/T5.
In P1/T5, there is no lower bound, so the tableau is optimal for all θ < -1.
To summarise the results, the optimal solution is  
We now turn to RHS parametrisation and ask for the optimal solution as θ varies when the RHSs of the constraints of P1 are 5, 9 + θ and 3.
The procedure  is dual to that used when the objective function is parametrised.
We first write down the new resource column in P1/T1 using the formula (6.3) for .
This gives P1/T6 in which the column contains the coefficients of θ in the resource column.
P1/T6 is optimal (primal feasible) provided .
More generally, if  denotes the θ -column, the tableau is optimal for , where .
Increasing θ from θ = 0, we find that when θ = 2, 1-½θ = 0 in the s 3 -row and pivoting in this row, using the dual-simplex pivot column selection rule to maintain dual feasibility, gives P1/T7 which is optimal for all θ > 2.
When θ = -⅔ in P1/T6, ½ + ¾ θ = 0 in the x 2 -row, giving the pivot indicated in that row and resulting in P1/T8, optimal for .
When , but the dual simplex pivot selection rule does not give a pivot column (because all entries are non-negative).
For any , and this observation implies infeasibility.
So the problem is infeasible for θ < -9 and, for θ > -9, the optimal solution is given by  
In our examples only one variable contained the parameter θ.
This is not essential.
If the objective function in P1 had been  we could have used the formulae (3.6) for  to generate the extra row  and then proceed exactly as before.
The reader should check that P1/T2 remains optimal for  and that we pivot in the x 1 -column when 0 = ⅙ and the s 1 -column when θ = -.
For problems with equality constraints subject to RHS parametrisation it may be necessary to use the trick of setting non-basic variables to zero described in Section 6.1.
For example, if the RHS of the constraints of P2 had been changed to 5 -2θ and 3 + θ respectively, we would have to solve .
With this new resource column we proceed exactly as above.
All of the problems considered in this section have had an optimal solution at θ = 0.
This need not always be the case.
Exercise 6 suggests a method for dealing with the case where the LP is unbounded or infeasible at θ = 0, but not for all values of 0.
FINITENESS OF PARAMETRIC PROGRAMMING
In the examples of parametric programming solved in Section 6.2, we had 0 > θ in every tableau.
Since θ for one tableau is equal to θ for the next, when increasing 0, this ensures that we cannot repeat a tableau.
This strict inequality need not always occur; as we shall see below, tableaux may arise with θ = 0.
This phenomenon raises the possibility of returning to a previous tableau, leading to the occurrence of cycling (cf. Chapter 3).
In this section, we shall describe a method of avoiding cycling, allowing us to conclude that parametric  programming problems can be solved in a finite number of steps.
Apart from its intrinsic interest, this result will have applications in other chapters.
We will start by looking at the parametric problem P3.
P3: maximise  subject to.
When θ = θ the initial tableau (x 1 = x 2 = x 3 = 0) is optimal.
Adding a θ-row gives the tableau P3/T1.
In P3/T1, we have no θ, and θ = 2.
The minimum in (6.4b) is achieved at j = l and 3 (x 1 — and x 3 -columns).
If we pivot in the first of these, as indicated in the tableau, we obtain P3/T2 in which θ = θ.
We will say that a tableau is optimal over a degenerate interval , if θ = θ.
However, we can still apply the rules of Section 6.2.
The minimum in (6.4b) is achieved at j = 3.
This gives the pivot indicated and leads to P3/T3 which is also optimal over a degenerate interval.
Following the same rules, we make the indicated pivot to arrive at P3/T4 in which θ = 5 > θ = 2.
From P3/T4, we proceed as in Section 6.2.
One more pivot, as indicated, is required to complete the solution.
In P3/T1 we had a choice of pivot column (first or third).
We will call these columns and, more generally, any column j for which  a critical column.
To be consistent with the procedure of Section 6.2, we must pivot in a critical column.
Since  by definition in a critical column, the objective row evaluated at  does not change under pivoting.
Now  is equal to θ in the new tableau and, if this tableau is optimal over a degenerate interval, θ is the same for both tableaux.
Hence, the set of critical columns will remain the same throughout a sequence of tableaux optimal over degenerate intervals.
It is readily verified that the first and third columns are critical in P3/T1, T2, and T3.
It follows from (6.4b) that there will always be a critical column j with , provided θ is defined.
It is possible that there is no pivot in this column because  for all .
But then, since , the problem is unbounded for θ > θ (see Section 3.4) and the solution is complete.
Otherwise, we can pivot in this column.
Thus, if  in all the columns which were critical in the preceding tableau, the current tableau must contain at least one new critical column, which means that the current tableau is optimal over a non-degenerate interval.
For example, in P3/T3, we have  (columns one and three are critical) and we can verify that 0 > θ in P3/T4 as asserted (only column two is now critical).
When applied to a sequence of tableaux optimal over a degenerate interval, this process can be interpreted as applying the simplex method restricted to the critical columns.
This suggests an effective rule, based on the pivot column selection rule of the simplex method, for choosing the pivot column: choose critical column k where  and the minimum is over critical columns.
If this results in a tie, an arbitrary choice may be made.
In P3/T1 we pivoted in the first column because 
The finiteness of the simplex method, established in Chapter 3, shows that in any sequence of tableaux optimal over degenerate intervals, we must eventually reach a tableau with  for all critical columns j , and consequently, at the next iteration, a tableau optimal over a non-degenerate interval.
Since simplex pivots can be chosen to avoid cycling and θ > θ ensures we will not reach the preceding tableau again, we will never repeat a tableau, when increasing θ.
Similar results apply to decreasing O (choose a column j with  maximising  over such columns).
Consequently, the parametric programming method is finite.
This result has indirect value as well as direct practical significance.
For example we can use it to show that it is possible to pivot from any primal feasible tableau (T1)corresponding to a set of constraints to any other primal feasible tableau (T2) for these constraints, maintaining primal feasibility .
We first observe that T1 is uniquely optimal for the objective function  because we would have .
This objective can be rewritten  in terms of the structural variables as  say.
Similarly, there is an objective function  which has T2 as its optimal tableau.
Now consider the parametric programming problem with objective function 
When θ = 0, T1 is optimal and, when θ = 1, T2 is optimal.
Consequently, we can increase θ from θ to 1 and the finiteness of the procedure proves the result.
This gives an a posteriori justification of the method used to solve Exercise 6 of Chapter 2.
A refined version of this argument will be used to validate a form of the simplex method for multiple objectives described in Chapter 9.
We have only dealt with a parametrised objective function so far in the section.
However, everything translates into RHS parametrisation.
In particular, when increasing θ, we pivot in row i , with , minimising  over such rows and, when decreasing 0, in row i with  maximising over such rows.
Bounded Variables
IMPLICIT CONSTRAINTS
One type of constraint, which occurs frequently enough to merit special consideration, is an upper bound on an individual variable.
In this chapter, we will see how such constraints can be incorporated implicitly in a tableau.
We will start by solving problem P1 by the simplex method.
maximise  subject to 
Writing σ 1 , σ 2 , σ 3 for the slack variables in the upper-bound constraints, the solution is given in tableaux P1/T1–T4.
A number of observations of the structure of tableaux P1/T1 T4 can be made.
Firstly, if one of σ J , x J is basic and one non-basic then the equation represented by the row containing the basic variable is  where U J is the upper bound on x J .
This follows from the form of the problem and is clearly true in general.
In tableau notation, it says that, if  is x J (or σ J ) and x/N, is its partner : a J (or x J ), then .
If σ J and x J are both basic, the sum of the corresponding rows is (7.1).
Once again this follows from the fact that (7.1) is just the expression in equality form of the constraint x J < U J .
In tableau terms, if  is x J and  t is σ J , then 
We cannot have both x J and σ J non-basic, assuming U J > 0, by (7.1).
These results show that, if x J (or σ J ) is basic we can deduce the corresponding row from the rest of the tableau.
For example, in P1/T1–T4 we could retain only the rows marked with an asterisk in addition to the objective row, without any loss of information.
In general, we will not explicitly include any rows for upper-bound constraints, but we will ensure that subsequent tableaux all contain one of x J or σ J , but not both, amongst the basic and non-basic variables.
Consequently, we will be able to deduce the implicit rows, corresponding to whichever of x J or U J is not contained in the resulting compact tableau.
When the pivot column has been selected, the entries in the implicit rows and pivot and resource columns can be deduced from (7.2) and (7.3).
The pivot row is determined by the PRS rule and it may be an explicit row, in which case we pivot as usual, or an implicit row, in which case we write down the complete row using (7.2) or (7.3) and pivot using this row.
In the latter case, after pivoting, implicit basic variable x J (or σ J ) becomes non-basic and the row in which its partner is basic, which may be the pivot or another row, is dropped, to ensure that only one of (U J and x J is retained in the next tableau.
The dropped row will always be of the form (7.2)(zeros and a one in the inner columns).
Compact tableaux can be used in Phase I as well as Phase II.
Since  only involves slack and artificial variables and, usually, only structural variables are subject to upper bounds, slack and artificial variables do not have associated  implicit rows.
Consequently the standard formula for the objective row in Phase I applies.
We will illustrate the method by solving P2.
P2: maximise  subject to 
The first two tableaux are P2/T1 and T2.
In P2/T1 the pivot row is the (implicit) σ 3 -row because  and in P2/T2 the pivot row is the s 2 -row because 
Pivoting in P2/T2 gives the first two rows of P2/T3.
Phase I is completed.
When calculating the objective row in P2/T3, we note that  and then use the usual formulae.
We can also obtain  by evaluating z at the
BFS of P2/T3:(x 1 , x 2 , x 3 ) = (0,1½, 2).
In P2/T3 the pivot row is the (implicit) σ 2 -row because  in that row is the only positive element in the x 1 -column.
Since u 2 becomes non-basic after pivoting, we drop the x 2 -row and substitute the x 1 -row (pivot row in P2/T3) to get P2/T4.
This is optimal and the optimal solution has x 1 = 3⅔, σ 2 = σ 3 = 0, which means (x 1 , x 2 , x 3 ) = (3⅔, 7, 2).
To show that pivoting in a compact tableau is just an application of the methods of Chapter 2, we wrote out the pivot row explicitly.
This imposes no real burden in small problems solved by hand.
However, it is unnecessary.
If the basic variable in an implicit row is σ J (or x J ) and its partner is non-basic, so we pivot on 1 (as in P2/T1), the pivot column k is simply multiplied by -1 and the resource column changes to .
If the partner of σ J (or x J ) is basic (as in P2/T3), it is readily verified (see Exercise 3) that the next tableau can also be obtained by pivoting in the row containing the partner of σ J (or x J ) and the same pivot column, and multiplying the new pivot column by -1.
The new non-basic variable must be labelled correctly: σ J (or x J ).
This is readily verified in P2/T3 using the x 2 -row and x 1 -column for pivoting.
With these modified rules it is easy to adapt the procedure of this section for use in the PFRS format (Exercise 3).
7 .2 SENSITIVITY ANALYSIS 
A particularly important use of compact tableaux occurs in the solution of integer programming problems (see Chapter l 1 ).
In the course of solving such problems we will frequently wish to alter upper bounds and to impose and alter lower bounds on variables.
It is therefore important to be able to perform discrete sensitivity analysis and to handle lower bounds.
In this section we will consider techniques for solving these problems and, in passing, illustrate how dual simplex iterations may be carried out using compact tableaux.
To illustrate the addition or alteration of upper bounds we will start by adding the constraint  to problem P2 of Section 7.1.
Since x 1 is basic in the optimal tableau P2/T4, we can imagine that x 1 acquires an implicit row and, since 2 < 3⅔, this means that σ 1 <0.
(Adding , where , does not change the solution.)
The σ 1 -row is written explicitly in P2/T5; its resource column entry is .
Since P2/T5 is primal infeasible, a dual simplex iteration is necessary to restore primal feasibility.
By the usual pivot column selection rule, we pivot in the σ 3 -column to obtain P2/T6 (ignore numbers in brackets,pro tem.).
Note  that in P2/T6 we would expect to have σ 3 basic in the second row.
However, for reasons which will appear later, we have replaced the σ 3 -row with the implicit x 3 -row (making the σ 3 -row implicit).
In general, whenever both x J and a J are basic, we will retain x J in the tableau.
To verify that P2/T6 really is primal feasible, we must check the resource column entries of implicit rows and, if any is negative, perform further dual simplex iterations.
In fact P2/T6 is primal feasible, so the new optimal solution is (x 1 , x 2 , x 3 ) = (2, 7, 3).
We will now consider the further effect of reducing the upper bound on x 2 by 4.
(The upper-bound constraints are now .
We must first calculate the effect of this change on the resource column.
This can be done by apply the formulae of Chapter 6 or, directly, by noting that replacing x 2 + σ 2 = 7 with x 2 + σ 2 = 3 can be effected by replacing σ 2 with σ 2 + 4, so that the equation corresponding to the first row: becomes  which is equivalent to replacing the RHS with 4.
In general, this shows that, if  is σ J and U J is reduced by , then  must be replaced with .
This gives the numbers in brackets in P2/T6.
Such a change renders the tableau primal infeasible, since  (because ).
A dual simplex iteration returns to P2/T5 except for the resource column, which has .
It is primal feasible and therefore optimal, with solution (x 1 , x 2 , x 3 ) = (l, 3, 2).
To impose a lower bound , we need only substitute , where , throughout the problem and solve in the usual way.
When upper and lower bounds are imposed on the same variable it is important to note that  means .
(We must have .)
To illustrate, we will add the bounds x 2 < 2 and x 3 > 1 to P2.
This means substituting x 3 + 1 for x 3 in P2/T6 and gives P2/T7.
The terms in brackets indicate changed values and variables.
Pivoting as shown in P2/T7 gives the optimal tableau P2/T8, in which x 2 = 6. x 3 = 0, σ 1 = 0 or (x 1 , x 2 , x 3 ) = (2, 6, 1).
If we now increase the lower bound on x 3 from 1 to 2 (meaning x 3 = 2, in view of the upper bound on x 3 ), this means we replace x 3 =x 3 + l with x 3 = x 3 + 2, in other words x 3 with x 3 + 1 This results in the resource column shown in brackets in P2/T8.
The tableau is no longer primal feasible and one further pivot as shown.
is required to achieve an optimal tableau in which .
Transhipment and Transportation Problems
TRANSHIPMENT PROBLEMS — THE NATURE OF THE BASIC FEASIBLE SOLUTIONS
A large retail organisation owns m sites, some of which may be warehouses and some may be shops.
For i = 1,…,m , we will write  for the excess of requirements over stocks held at site i of some commodity.
If the site is a warehouse, we may expect , whereas if the site is a shop, we may have  and there may be intermediate sites at which no stocks are held or required, so that .
Our aim is to transport goods from site to site so that requirements are fulfilled from the stocks available at other sites.
If we write  for the quantity of goods transferred from site i to site j (), we are faced with the constraints, for each i = 1,…,m ,.
since the first sum represents the flow of goods into i and the second is the flow out of i , so that the difference is the net flow into i , which must equal the net requirements.
If we sum these equations, we obtain .
so the  must sum to zero if the constraints are to be feasible and we will assume this to be true hereafter.
With this assumption, we can drop one of the m constraints without affecting the set of solutions, leaving only m-1 constraints.
Now suppose that the cost of transporting one unit from i to j () is  and our objective is to achieve the transfer of goods at minimal cost.
Then we must solve the transhipment problem (TRP).
Although the TRP is much more specialised than the LPs we have studied so far, a surprisingly wide range of problems can be modelled as TRPs including many examples which have nothing to do with shipping goods between sites.
The assignment problem of Section 8.5 and the inventory problem of Exercise 7 provide examples.
We shall therefore adopt the neutral term node instead of‘site’ from now on In Section 8.2 we will see that the simplex method can be performed very efficiently for TRPs if we take the structure of the problem into account and we will start by examining the nature of BFSs.
Any BFS must have m-1 basic variables and we will use these variables to establish a relationship between the nodes.
We will assign any node j for which  or  is basic to the first generation and say that the parent of j is 1, writing P(j) = 1.
For example, if m = 9 and the basic variables are x 19 , x 23 , x 26 .
x 29 , x 41 , x 59 , x 87 and x 97 , then nodes 4 and 9 are assigned to the first generation and P(4) = P(9) = 1.
Now for each node i assigned to the first generation, we assign any node j (–1), not already assigned, for which x i j or x j i is basic, to the second generation and set P(j) = i (i is the parent of j).
In our example, x 29 , x 59 and x 97 are basic and so nodes 2, 5 and 7 are assigned to the second generation and P(2) = P(5) = P(7) = 9.
No node has 4 as parent.
In general, for a node i of the rth generation, we assign any node j () not assigned to generations 1,…,r , for which  or  is basic, to generation r + 1 and set P(j) = i This means that all the remaining nodes in our example are assigned to the third generation and P(3) = P(6) = 2, P(8) = 7.
It is not clear a priori whether we will be able to assign a generation to every node except 1, but we will see subsequently that this is the case for any BFS.
A graphical representation of the relationships generated in our example is given in Fig. 8.1(a).
Node i is indicated by a number i in a circle and node i is joined to node j with an (unbroken) line or edge and an arrow directed from i to j , if x i j is basic.
(The broken line should be ignored, pro tem.)
Nodes belonging to the same generation share the same horizontal level.
It may prove helpful to view the figure (ignoring arrows) as a family tree in which the nodes correspond to male members of a family and node 1 is a patriarch.
Our earlier terminology ('generation', ‘parent’) was inspired by this interpretation.
For every node i –1 in Fig. 8.1(a) there is a unique path , or sequence of distinct nodes linked by edges, from i to 1 (corresponding to ‘a line of descent’ from 1 to i) and this is the defining characteristic of a tree .
The path can be determined by taking i , then P(i) , then P (P (i))and so on until 1 is reached.
With i = 6, this gives P(6) = 2,P(2) = 9,P(9) = 1 so the path is .
The complete tree (but not the directions of the arrows) can be generated from the  function P as can P from the tree.
The trees are a valuable visual aid when solving small problems by hand but ‘pointers’ such as P(i) are essential for handling problems of a realistic size on a computer.
Fig. 8.1 Trees and pivoting in TRPs.
We will now consider how pivoting may be carried out in the framework we have outlined.
Let us suppose that we have a BFS and an associated tree T and that we wish to insert , currently non-basic, into the basis.
Since  is non-basic there is currently no edge from I to J but we can always reach I from J in T because we can go from J to 1 and then from 1 to I. For example, if we take I = 3 and J = 7 in Fig. 8.1(a), the unique paths from 7 to 1 and 3 to 1 are  and  and we can go from 7 to 9 to 1 to 9 to 2 to 3.
This is not a path because node 9 is repeated but it can be shortened to the path  by leaving out the redundant section 9 to 1 to 9.
In general, we find paths from J to 1 and I to 1 and then look for the first node k at which the paths meet (k = 9 in the example).
In family-tree terminology,k is the most recent common ancestor of I and J. The path from J to I is then constructed by combining the paths from J to k and k to 1.
There are no other paths from J to I for otherwise there would be at least two paths from J to 1 (what are they?).
If we add an edge from I to J, we will create a cycle (a sequence of distinct edges, each having a node in common with its predecessor, and each node appearing in exactly two edges) by adding the edge I to J to the edges of the path from J to 1.
Furthermore, the cycle is unique because of the uniqueness of the path from J to 1.
If we traverse the cycle in the direction J to k to I to J, then some edges of the cycle will have their arrows directed in this direction and we will say that the corresponding variables are forward variables whereas the remaining edges will be directed against the direction of the cycle and we will  describe the corresponding variables as reverse variables.
In our example, we have added the edge from 3 to 7 as a dashed line in Fig. 8.1(a) and the resulting cycle consists of the edges from 7 to 9, 9 to 2, 2 to 3 and 3 to 7.
This makes x 29 and x 97  reverse variables and x 23 and x 37  forward variables.
In terms of the P function, we note that x I J is always a forward variable and that x pq  is a forward variable if p and q are linked by an edge, either on the path from J to k with P (p) = q , or on the path from I to k with P(q) = p .
The variables corresponding to the remaining edges on these paths will be reverse variables.
Now suppose we increase all forward variables by 0, decrease all reverse variables by θ and leave all other variables unchanged.
Then this new solution satisfies the constraints (8.1) for any 0.
To see this, we have only to verify that (8.1) is still satisfied when i is a node of the cycle (why?) so suppose that s to i and i to t are consecutive edges of the cycle.
There are four cases to consider.
(i)
and  are both forward variables;
(ii)
is a forward variable and  t is a reverse variable;
(iii)
is a reverse variable and  is a forward variable;
(iv)
and  are reverse variables.
These cases are illustrated in Fig. 8.2, together with the changes in the values of  and  t .
It is clear that the net change in  is zero.
So (8.1) is still satisfied.
Fig. 8.2 The effect of increasing forward variables and decreasing reverse variables by θ.
If we write  for the current value of  (it will be zero unless  is basic), we have seen that, when , we can still satisfy (8.1) with   where  for reverse, for forward and zero for all other variables.
Consequently, is the coefficient of  in the equation of the canonical form in which  is basic.
To apply the PRS rule, we must choose the smallest –for .
But  means that  and that  is a reverse variable, so  leaves the basis, where  is a reverse variable and .
Ties for minimum can be broken arbitrarily.
The new BFS is .
If, in our example, we have  (the other values are irrelevant at the moment), then .
since x 29 and x 97 are the reverse variables of the cycle created when edge 3 to 7 is added in Fig. 8.1(a).
Hence, x 29 leaves the basis.
Dropping  from the basis means deleting the edge from p to q from the tree, T, and adding I to J. The result is still a tree because, if the path from i to 1 in T is broken by dropping the edge from p to q , another path can be constructed by going instead from p to q round the cycle (using the edge I to J).
Since there is only one cycle created when I to J is added there will be no cycles left after the edge p to q is dropped and so there cannot be more than one path from i to l for any i .
The new tree T' is illustrated in Fig. 8.1(b) for the example studied above.
To handle the change of tree computationally, we must specify how the parent function P' of T' may be determined from P. Experimentation with a few examples will show that the following rules have the desired effect.
We have seen that, when pivoting from a BFS corresponding to a tree, the new BFS also corresponds to a tree.
In Section 6.3, we showed that it is possible  to pivot from any BFS to any other BFS through a sequence of BFSs.
Consequently, if we can exhibit one BFS corresponding to a tree,all BFSs will correspond to trees.
But,, if , and , if , for  is a BFS.
Indeed, if the ith equation of TRP is multiplied by -1 when , the constraints are in canonical form.
The tree corresponding to this basis has P(i) = 1 for all .
THE SIMPLEX METHOD FOR THE TRANSHIPMENT PROBLEM
In order to apply the simplex method to the TRP, we must introduce the objective function into the framework established in Section 8.1.
In particular, we need to calculate the objective-row coefficient of any non-basic variable.
To see how this can be done, we will start by observing that x i j occurs in the ith equation of (8.1) with coefficient -1 and in the jth equation with coefficient +1.
This means that the dual problem of TRP can be written in the form DP below.
DP: maximise  subject to.
The variables of DP are free because the constraints of TRP are equalities and  reflects the fact that the i = 1 equation of (8.1) has been dropped from the constraints of TRP.
We could have omitted Y 1 altogether, but the form DP is generally more convenient.
Now, if , so that  is a slack variable in DP, then , where we have swapped labels i and j in the final sum in the first line to obtain the second line.
Hence,.
For a given BFS, if we choose the  so that whenever  is basic, then (8.2) contains only the non-basic variables and we can deduce that  is the objective-row coefficient of .
This means that, if  for all i ,j (), then the BFS is optimal (the CS conditions) and if not, we make  basic where 
To apply this method, we must calculate  satisfying .
This is easily done by working down the tree.
Once all the  for the rth generation have been determined, we can write them down for the (r+1) th generation using (8.3).
If i is in the (r + l) th generation,
Fig. 8.3 Trees in the solution of P1.
For example, suppose m = 6 and the data is given in Table 8.1.
We will now give the complete solution of the problem P1 with data specified in Table 8.1 and references to trees T1–T4 in Fig. 8.3.
Following this example through shows that, when solving small examples by hand, it is usually easier to work with the tree rather than the function P  defining it.
However, it is important to recognise that all the operations required can be performed using P or, more easily, if G(i), the generation of i , is available for each i (Exercise 3), although this involves updating G when pivoting.
In the solution above, we calculated y ab initio at each iteration.
In fact, the calculation of the new y can be simplified by recognising that, in the notation of Section 8.1, if the edge p to q is deleted without adding the edge I to J, the tree T is broken up into two smaller sub-trees .
One of these, which we will call T A , contains node 1.
Thus, for p = 2,q = 9, the sub-tree T A in Fig. 8.1(a) is defined by the nodes 1, 9, 4, 5, 7 and 8.
It follows from (8.3) that Y i does not change for any i in T A .
If J is in T A , then I is in the other sub-tree, T B .
Since T A does not apply to T B (because 1 is not a node of T B ), if  is increased by , a solution of (8.3) is obtained by increasing , for all nodes of T B , by .
The increase in  is  and so we increase Y i for all nodes i of T B by .
By a similar argument, if I is in T A and J in T B , we decrease Y i for all nodes of T B by .
This can be verified in Fig. 8.3.
In order to apply this method to large-scale problems, we must find the tree T B using the pointers available.
Although this can be done with P (.)
and G (.),
it is not particularly efficient and may even be worse than solving (8.3) from scratch.
To make it easier to obtain sub-trees, the children of any node i (the nodes j for which P(j) = i) can be listed.
These lists must then be updated when pivoting.
More subtle and effective, but much more involved, ways of achieving the same end have been devised.
Assuming non-degeneracy, at each iteration the cost decreases by a positive amount  and so an optimal BFS is eventually reached.
Degeneracy can cause cycling even in TRPs but the possibility of this happening in practice is remote and can be ignored.
In Section 4.4, we showed that the pivot can always be chosen to avoid cycling and the procedure described in this section allows us to conclude that if b 1 ,…, b m  are all integers, then the optimal solution (indeed, any BFS)is integer-valued .
Apart from its intrinsic interest, this result sometimes allows one to formulate certain combinatorial problems, such as the assignment problem discussed in Section 8.4, as TRPs.
A simpler procedure than that of Section 3.6 is available for preventing cycling in TRPs.
It rests upon the fact that, if  is basic, then  where the sum is restricted to the nodes in either of the sub-trees resulting from the removal of the edge p to q from the tree corresponding to the BFS.
If we change every  to  and any one to , where e is small and n is the number of , then we can never have  for any subset of nodes.
Consequently, degeneracy cannot occur and, assuming b i ,…, b m are integers, it is only necessary to round down the optimal solution to the perturbed problem to obtain an optimal solution to the original problem.
TRANSPORTATION PROBLEMS
In some TRPs, many routes between sites will be impossible and so, for many i,j , will not be specified and we will expect to have x i j = 0.
Indeed, large problems are usually very sparse (most x i j s must be zero).
It is easy to cope with this in the TRP framework by setting  to some large number K (larger than the sum of the given  s will do).
This will force x i j to be zero in any optimal solution.
To capitalise on the computational advantage accruing from missing routes, we never evaluate  for any non-basic x i j for which  = K, since such x i j will never become basic.
However, such x i j could be basic in an initial BFS and, although they will eventually leave the basis, it is usually computationally advantageous to start with a BFS in which no basic variable x i j has  = K .
This can often be easily achieved when the problem has sufficient structure as we will now illustrate by turning to transportation problems.
A certain company owns M warehouses and N shops.
Warehouse i contains a i items of a certain product and shop j requires  items.
If it costs  to transport one item from i to j , how should the goods be transported in order to satisfy all requirements at minimum cost?
If we write x i j for the number of items sent from i to j , the problem can be written as an LP.
It is called the Transportation Problem TP.
TP: minimise  subject to.
For feasibility of TP, we require requirements and availabilities to match: and then drop the first equation.
It should be clear that TP is a special case of TRP with M + N sites, in which we have written  instead of whenever  (a warehouse).
We also put  = K except where i is a warehouse and j a shop, but we have omitted x i j when  = K , so K does not appear explicitly.
The solution procedure for TRPs can obviously be applied to TPs but certain special features arise from the extra structure.
We must first find an initial BFS for TP and at the same time we will generate the parent function defining its corresponding tree.
Indeed, the ability to obtain P (.)
corresponding to a feasible solution shows that this solution is indeed a BFS, since we explained in Section 8.1 how the canonical equations could be derived from the corresponding tree.
The general idea is to select i and j and then make x i j basic and as large as possible without violating the constraints of TP.
If  x i j is the first basic variable chosen, this means putting .
Then  and  must be reduced by this amount, which will leave one of them at zero.
The corresponding constraint is made unavailable for further selection and the process repeated until a BFS is obtained.
We can refine this process by allowing costs to influence our choice of i and j (small costs of basic variables being desirable), and one such refinement is now described.
Others, generally giving a better (smaller cost) initial BFS but at much greater computational cost, have been devised.
We will set out the data in a tableau with rows corresponding to warehouses and columns to shops.
Thus,i indexes rows and j indexes columns.
A line will mean a row or column.
When constructing the tree corresponding to a BFS, each line has a corresponding node and we will write R i for the node corresponding to row i and C j for the node corresponding to column j .
Initially, we will set  for all i and j , assume all lines are available and take the current line to be row l .
The general step is to find the smallest available cost in the current line, say , and put x i j = .
Then we set .
This step is repeated until  basic variables have been chosen.
Although complicated to describe, this method is intuitive and easy to apply, and we will illustrate it by considering problem P2 whose data is displayed in tableau T1 below.
The entry in row i and column j of T1 is .
Since M = 3,N = 4, there are 6 basic variables to be found.
The initial BFS produced by this method can also be used as an initial BFS for the TRP by identifying i with  as warehouses and i with  as shops and allowing the initial BFS to contain only x i j , where i is a warehouse and j a shop.
Applied to problem P1 of Section 8.2, it gives x 2 l = 7, x 4 l = 1, x 43 = 3, x 45 = 1 and x 46 = 4, which has a cost of 67 compared with 92 for the initial BFS given in that section.
The dual problem to TP has dual variables  and , and constraints .
So, for each basic x i j we have , and y i and  can be determined by working down the tree as in TRP.
Then we calculate  and proceed exactly as for the TRP.
The solution can be set out neatly in tabular form as in T2, T3, T4 where the circled entries are the values of basic variables and the remaining entries display  (which is zero for basic variables).
The BFS given in T2 is the initial solution obtained above.
Variables Y i and z ; are also shown in T2–T4 (in the first column and top row, respectively).
The trees corresponding to T2–T4 are T2, T3, T4 of Fig. 8.4.
It is clear that any basic variable x i j corresponds to an edge R i to C j so there is no need to put arrows on the edges.
Indeed, if the edge R 1 to C J is added, the path from C J to R 1 corresponds to an alternating sequence of reverse and forward variables, starting with a reverse variable.
This can be verified in T2 and T3.
In some circumstances we may wish to solve unbalanced problems ().
For example, suppose , so that some shops will receive less than their requirements.
Then the constraints can be written .
Fig. 8.4 Trees in the solution of P2.
If we now impose a penalty  per unit shortfall at shop j , the LP formulation becomes minimise  subject to which is just a TP with m + 1 warehouses, if we rewrite  as  and add the constraint .
A similar trick can be used if .
THE ASSIGNMENT PROBLEM
Suppose we have M individuals, each of whom can perform any of M tasks.
The cost of assigning individual i to task j is  and the Assignment Problem , AP, seeks to assign each individual to a different task at minimum total cost.
If we write x i j = 1 if individual i is assigned to task j and x i j = 0 otherwise, the problem can be written:
AP: minimise  subject to.
The first M constraints express the fact that each individual undertakes exactly one task and the second M constraints say that each task should be undertaken by one individual.
If we were to relax the requirement x i j = 0 or 1 to x i j > 0, we would have a TP, but any TP has an optimal solution in integers and, in view of the constraints, this means x i j = 0 or 1.
So the AP can be viewed as a TP.
However, it is a highly degenerate TP which, at worst, can mean that the problem cycles and always brings the danger that a very large number of iterations may be required.
Nevertheless, solving an AP as a TP works very well, if we restrict the solution to a special class of BFSs.
Any BFS of AP must have one I in each row and column.
This gives M I s ,but a basis must contain 2M-1 variables, so we need  among the basic variables.
We will describe a basis as alternating if the first row contains one basic variable (which must be a 1) and the remaining rows contain two basic variables (a 1 and a 0).
If  enters the basis, we will typically have a choice of variable to leave the basis.
To ensure that the new basis is alternating, we will make a basic variable in row I non-basic.
Clearly, exactly one of these basic variables, say , will correspond to an edge of the cycle created by adding the edge I to J to the tree and it can leave the basis provided it becomes θ in the new BFS.
This will certainly be true if  in the current BFS, because it is a reverse variable (why?) so  and its value does not change (a degenerate pivot).
If  is 1, we must show that θ = 1, so that  decreases to 0.
To see that this is true, we note that any path in a tree corresponding to a BFS of an AP consists of an alternating sequence of row and column vertices, and R i and C j adjacent in this sequence means  is basic.
Now suppose that the path from C j to R i ends:, and –Then , for otherwise there would be two i s in the Kth column.
Hence , since there is no other basic variable in row i .
(We cannot have i = 1, though I = 1 is possible.)
By continuing in this fashion, we see that the path from C J to RI corresponds to an alternating sequence of reverse variables with value 1 and forward variables with value 0.
This means that θ = 1.
To start the method, we must find an initial alternating basis.
This is done by first obtaining an initial assignment as follows.
We first look for the smallest , say , and set .
Then we declare row t and column u unavailable and search for the smallest  in the remaining available rows and columns, say , and set , declaring row v and column w unavailable.
This process is repeated until there is an x i j = 1 in every row i .
Applied to P3, which has costs displayed in tableau T5, it gives, in order, x 23 = 1, x 44 = 1, x 12 = 1, x 31 = 1.
To complete an alternating BFS, we must find a further basic variable (with value 0) for each row .
To do this, we choose x i j basic, where  for some  and .
Expressed verbally, we must choose a variable with a I above it in the same column.
From those available in row , the one with smallest cost is selected.
Applied to P3, this gives the BFS displayed in T6.
The parent function of the corresponding tree is given by putting  and , where .
For 16, the tree is shown as T6 in Fig. 8.5.
Note that , if .
Fig. 8.5 Trees in the solution of P3.
The complete solution is shown in tableaux T6–T8; the corresponding trees are T6 -T8 of Fig. 8.5.
Note that, in T6, we make x 1 1 basic (we could also have chosen x 21 ), but  must leave the basis (not x 23 or X 3 1) in order to maintain an alternating BFS.
Similarly, in T7, x 23 (not x 31 ) leaves the basis.
This method cannot cycle.
To see this, we first observe that, if θ = l the cost decreases () so the preceding BFS cannot recur.
When , which  means , as we saw above, we shall show that  increases, so the preceding BFS cannot recur in this case, either, since  and  are uniquely determined by the basis (and ).
To see that  increases, we first observe that this sum is just the sum of all the dual variables of the AP viewed as a TRP.
(We changed the sign of the dual variables corresponding to rows when going from TRP to TP.)
To prove that this sum increases we have only to show that, in the notation of Section 8.2, C J is in T A .
(This actually shows that some  increase by  and none decreases.)
But, by a similar argument to that used above, we can see that the path from  to  corresponds to an alternating sequence of forward variables with value 1 and reverse variables with value 0.
Hence, if  is the penultimate node on this path, is basic and –.
But there is only one variable, with these properties in row 1, so  and removing edge  to  breaks the path from  to .
This puts  in T B and  in T A .
CAPACITY CONSTRAINTS
In this section, we will impose the extra constraint on all, or some, of the variables of TRP.
We will see how such upper-bound constraints can be handled implicitly as in Chapter 7.
As in that chapter, we allow non-basic variables to be x i j or .
If  satisfies (8.3), then  is the coefficient of non-basic  so that  is the coefficient of .
We calculate   and, if these are both non-positive, the current BFS is optimal.
If  and , we make  basic.
This is done as in Section 8.2, by finding the cycle created when the edge I to J is added to the tree corresponding to the explicit basic variables of the current BFS.
We saw in Section 8.1 that the coefficient of  in the equation of the canonical form in which x i j is basic is , where  for reverse and  for forward variables.
This means that the coefficient of  in the equation in which  is basic is 1, if x i j is a forward basic variable, and the RHS of this equation is then .
Also, the coefficient of  is l in the equation in which  is basic and the RHS is .
(For further details, see Chapter 7.)
Thus, we define θ to be the minimum of 
The new variable values become  for forward x i j , for reverse x i j and remain at  for the remaining variables.
If the minimum occurs under case (a) for  we make  non-basic and, if the minimum occurs under case (b) for  we make  non-basic, so  becomes .
In either case, we add edge I to J to the tree and delete p to q .
The calculation of the new parent function is done as in Section 8.1, except that for case (b), we reverse the roles of p and q .
If the minimum occurs under case (c), we make  non-basic (instead of) and do not change the tree.
These rules can also be deduced by imagining that we try to send extra goods ‘round the cycle’ by increasing the flow of goods in edges corresponding to forward variables and reducing it in edges corresponding to reverse variables.
The requirement that no variable should exceed its upper bound or become negative leads to the results above.
If  and  in (i) and (ii) above, we make  basic.
This is done by adding an edge L to K , thus creating a cycle.
The direction of the cycle is chosen so that the edge L to K is traversed from L to K .
Then we define  as above, but with  replacing  in (c).
The rest of the procedure is the same as before (noting that  decreases by  except that, if the minimum occurs in case (c), we make  non-basic (instead of ).
An extra difficulty arises in problems with upper bounds in that the initial BFS may be infeasible because some x i j exceed their upper bounds.
In Chapter 3, we saw that, for general LPs, a two-phase method, maximising  in Phase I, could be used to tackle such problems.
The same approach can be adopted for TRPs since Phase I can itself be regarded as a TRP.
However, it proves  convenient to combine the two phases by replacing the cost  by , if x i j is a basic variable with –, where K is a large number.
Provided K is large enough and there is a feasible solution, no optimal solution, for any set of costs modified in this way, contains basic variables exceeding their upper bound.
Thus, we will eventually achieve an optimal feasible solution.
We will not specify K explicitly, but will leave it as a parameter.
Consequently, any  will be of the form  and to compare , we first compare, and, only if these are the same, compare .
This means that we are effectively using the two-phase method, except that, where a choice of variable to enter the basis occurs in Phase I (and such occurrences are very common), the choice is resolved by the true objective function.
While some basic variables remain infeasible, the rule for selection of the variable to leave the basis needs modification.
In fact, all that is required is that, when calculating θ as above, we restrict case (b) to feasible forward variables .
This must still yield a finite minimum, for otherwise we would have a solution satisfying (8.1) and  for all  and with cost decreasing (z increasing) in .
By choosing θ large enough, this would result in a negative cost, which is impossible, since  for all i ,j .
This modification can also be derived from the modified PRS rule of Section 3.3
We will now solve problem P1 of Section 8.2 again, but with the addition of upper bounds .
The remaining variables are unrestricted.
The corresponding trees are shown in Fig. 8.6 but it should be remembered that non-basic variables can also have a positive  We will use the same initial BFS as in Section 8.2, but the method of constructing an initial BFS described in Section 8.3 can be modified to produce a good (low cost) feasible, or nearly feasible (few infeasible variables), solution.
(See Exercise 10.)
Fig. 8.6 Trees in the solution of P1 with bounded variables (cont. overleaf)
Problems with Multiple Objectives — Efficiency
INTRODUCTION AND WEIGHTING FACTORS
Consider a manufacturer who can produce three products A, B and C. He has stocks of 250 units of input I and 210 units of input II.
Each unit of A has 7 units of I and 6 units of II,B uses 5 of I and 9 of 11 and C uses 6 of I and 5 of 11.
If x 1 , x 2 , x 3 are, respectively, the amounts of A, B and C produced, the manufacturer is faced with the constraints 
The selling prices of A, B and C are 1, 0.8 and 0.9 respectively so he would like to maximise his total revenue  subject to these constraints.
However, he also feels that total sales are important so he would also like to maximise 
The manufacturer also believes that there is a useful market to be exploited overseas.
He would therefore also like to maximise exports.
If 40 per cent of A, 60 per cent of B and 20 per cent of C produced are exported, this involves maximising 
These three objectives reflect aspects of the production problem and, ideally, the manufacturer would like a feasible solution maximising them all.
Unfortunately such a solution is rarely available and in this and the next chapter we shall examine ways of ‘solving’ such problems, which are typically grouped together under the title multiple-objective or multiple-criterion problems.
It is worth noting that multiple objectives also arise when several individuals have to make a joint decision, the objective functions modelling the aspirations of each individual.
For example, in the production problem above, the three objectives may be those of the managing director, the sales director and the export director.
We shall examine an extreme case of two decision-makers in Section 10.4.
In general, we will examine problems having p linear objective functions,, which we wish to maximise subject to linear constraints.
Typically, these objectives will conflict in that there is no feasible solution which simultaneously maximises them all.
In such a case, some form of conflict resolution must be adopted to arrive at a solution.
We will discuss some methods of achieving this in Chapter 10.
However, one of the simplest and most frequently used approaches will also play an important role in our development of efficiency and so we will introduce it here.
The idea is to multiply each objective by a weighting factor and then add the weighted objectives.
For example, if we choose weighting factors of 10 for revenue, 1 for sales and 5 for exports in the example described above, the sum of the weighted objectives is  and this is to be maximised subject to the constraints given above.
The optimal solution is (x 1 , x 2 , x 3 ) = (10, 0, 30).
The size of the weighting factor can be chosen to reflect the importance we attach to an objective, but the use of this linear form implicitly assumes that the weights should be independent of the solution and does not permit one to model, for example, the proposal that profits should be given higher priority relative to exports when profits are low than when they are high.
In general, if w k is the weight ascribed to the kth objective and the constraints are taken to be inequalities, we are faced with solving the problem
In Section 9.3 we will investigate the relationship between LP (w 1 ,…, w p ) and efficient solutions, introduced in the next section.
EFFICIENT SOLUTIONS
We have assumed that all objective functions are ‘desirable’ in that we prefer greater to lesser objective function values.
Consequently, if a feasible solution (x 1 ,…, x n )dominates (y 1 ,…, y n ) in the sense that   for all k and V for at least one k .
then we would prefer (x 1 ,…, x n ) to (y 1 ,…, y n ).
The strict inequality (9.2) is used to exclude the possibility of equality in (9.1) for every k .
For example, with the objectives and the points A:(2, 1, 1), B:(6, 3, 2) C:(6, 6, 3), D:(4, 1, 0), E:(6, 0, 2) have objective function values A:(2, 2), B:(5, 9), C:(3, 9); D:(3, 9), E:(8, 6), so that A is dominated by B, B dominates C, but C dominates neither of D or E.
We will say that a feasible solution is efficient if it is dominated by no other feasible solution.
Synonyms for ‘efficient’ include ‘admissible’, ‘Pareto optimal’and ‘non dominated’.
It is a consequence of the definition that it is impossible to increase any objective function at an efficient solution without decreasing another.
In the multi-person case this says that no individual can be made better off without making someone else worse off.
Concentrating on efficiency means that we do not need to select a method of conflict resolution (or at least may postpone the selection).
The compensating disadvantage is that when conflict exists there will be more than one efficient solution, but at least we would expect to have excluded those non-preferred solutions which could never be optimal.
When we are studying a multiple-objective problem with a view to calculating efficient points, we will speak of a vector maximisation problem (VMP) and display the data with the objective functions above the constraints as in the following example, P1.
P1: V max  subject to or, more generally, when the constraints are written in inequality form.
We would like to be able to test whether a feasible solution (x 1 ,.
.., x n ) is efficient.
As it stands, the strict inequality (9.2) is rather hard to handle, but (9.1) and (9.2) together are equivalent to (9.1) together with the inequality:(These expressions are the objective function values of LP (1, 1,…, 1).)
Consequently.
if we define then, either (x 1 ,…, x n ) is optimal, or any optimal solution dominates (x 1 ,…, x n ).
This means we can test (x 1 ,…, x n ) for efficiency in VMP by testing it for optimality in LP* (x 1 ,…, x n ).
This is most easily performed using the CS conditions.
For example, to test (2, 0, 0) for efficiency in P1, we must examine
LP* (2, 0, 0): maximise  subject to where we have written all the constraints in ‘<’ form.
The dual problem is DP.
DP: minimise  subject to where v k is the dual variable associated with the constraint of LP* (x 1 ,…, x n ) corresponding to the kth objective function and y i is associated with the ith constraint of the original VMP.
To apply the CS conditions we observe that  and , where s 1 and s 2 are the slack variables for the original inequalities of P1 which appear in LP* (2, 0, 0) as shown above.
The slack variables for the constraints of LP* (2, 0, 0) corresponding to objective functions of P1 are all zero (by definition).
If the CS conditions are to be satisfied, we must have y 1 = t 1 = 0, which means that (2, 0, 0) is optimal in LP* (2, 0, 0) if there is a non-negative solution of  where t j is the jth dual slack variable.
Unfortunately, we cannot simply solve these equations arid see if the resulting solution is non-negative, as in the examples of Chapter 5, because we have more variables than equations — a consequence of (2, 0, 0) being a degenerate solution of LP* (2, 0, 0)(by construction).
We must resort to Phase I of the two-phase method: add an artificial variable and minimise the infeasibility which, initially, is .
This yields the solution (v 1 , v 2 , v 3 ) = (0, 0, 1),(Y 1 , Y 2 ) = (0, 2) and shows that (2, 0, 0) is optimal in LP* (2, 0, 0) and is therefore efficient in P1.
To test (0, 1 5, 1 25) for efficiency in P1, we observe that x 2 , x 3 > 0, x 1 = s 1 = s 2 = 0 so that t 2 = t 3 = 0 and the constraints of the dual problem of LP* (0, 1 5, 1 52), of which we seek a non-negative solution, become
Applying Phase I shows that there is no non-negative feasible solution and, hence, that (0, 1 5, 1 25) is not efficient.
The optimal solution of LP* (0, 1 5, 1 25) is (35,o , 152) and, as we have proved and can easily be verified directly, it dominates (0, 15, 125).
In the next section, we will show that it is also efficient.
EFFICIENCY AND WEIGHTING FACTORS
In Section 9.1, we described the weighting-factor approach to conflict resolution.
It is natural to ask whether the optimal solution of LP (w 1 ,…, w p ) is efficient.
We can answer in the affirmative provided the weights are all positive.
To prove this, let us suppose that (x 1 *,…, x n ) is optimal in LP (w 1 ,…, w p ) and that w 1 ,…, w p > 0.
If this solution were not efficient we would have, for some (x 1 , x 2 ,…, x n ), and multiplying the first of these by W K  and the remainder by w k and summing gives  which would contradict the assumed optimality of (x 1 *,…, x n ).
So this solution must be efficient.
For example, in P1, the objective function of LP (2, 1, 3) is  and this gives the optimal solution (2, 0, 0) which must be efficient.
The condition of positive weighting factors is no real restriction, for the assumption that the objectives are desirable means that only positive weights make sense.
A similar argument to that described shows that an optimal solution of LP* (x 1 ,…, x n ) is efficient.
For, if (x 1 ,…, x n ) were optimal and dominated by the feasible (in VMP) solution (x 1 ,…, x n ), then  so that(x 1 ,…, x n ) is also feasible in LP* (x 1 ,…, x n ).
It is then easy to see, as above, that the dominance of (x 1 ,…, x n ) contradicts the optimality of (x 1 ,.
.., x n ).
We have seen that solving LP (w 1 ,…, w p ) yields efficient solutions and we can ask if all efficient solutions can be obtained in this way.
To show that this is true, we will suppose that (x 1 ,…, x n ) is feasible and efficient and exhibit W 1 ,…
, W p > 0 making (x 1 ,…, x n ) optimal in LP (W 1 ,…, w p ).
To achieve this we will compare the dual problem of LP* (x 1 ,…, x n ): and the dual problem of LP (w 1 ,…, w p ):.
Since (x 1 ,…, x n ) is optimal in LP* (x 1 ,…, x n ), the duality theorem of Chapter 5 states that D * has an optimal solution, say (v 1 ,.
.., v p , y 1 ,…, y m ) and the optimal objective function values are equal.
That is .
Now the constraints of D* can be rewritten as  which shows that (Y 1 ,…
, Y m ) is feasible in D (w 1 ,…, w p ), with .
In addition,(9.4) can be rewritten as  which says that the objective function values of LP (w 1 ,…, w p ) and D (w 1 ,…, w p ), its dual problem, are equal.
Hence (x 1 ,…, x n ) is optimal in LP (w 1 ,…, w p ).
This result has many useful consequences, as we shall see in subsequent sections.
We shall therefore refer to it as the Fundamental Theorem of Vector Programming .
To illustrate the theorem, we will see how it enables us to convert any VMP with two objective functions into a parametric programming problem.
Consider the problem P2: V max  subject to.
The objective function of any LP can be multiplied by any positive constant without affecting the optimal solution(s).
This means we can multiply the weighting factors of LP (w 1 ,…, w p ) by any positive scale factor.
Applying this to problems with p = 2, we can arrange for w 1 = 1 so we have to solve LP (1, w 2 ).
This means we can find all efficient solutions of VMP by solving the parametric programming problem LP (1, θ) for θ > 0, using the procedures of Chapter 6.
For P2, the objective function of LP (1, θ) is .
When θ = 1, the objective is  and maximising this subject to the constraints of P2 leads to the optimal tableau P2/T1 below.
However, we have replaced the objective row with the objective rows corresponding to the parametric objective of LP (1, θ).
Thus the efficient solutions of P2 can be expressed as   and .
We initiated the solution of P2 by solving LP (1, θ), but it is possible that this problem is unbounded whereas LP (1, θ) has a solution for some other values of θ.
In this case the technique of Exercise 6 of Chapter 6 can be used to find an initial value of θ (see Exercise 4).
FINDING AN EFFICIENT SOLUTION
The use of weighting factors provides a natural method for finding an efficient solution of a VMP.
We could set w k = 1 for all k and solve LP (1,…, 1) to obtain an efficient solution.
Unfortunately, this approach can fail, as the example P3 illustrates.
P3: V max  subject to.
The objective function of LP (1, 1, 1) is  which makes LP (1, 1,1) unbounded.
However, this does not mean that P3 has no efficient solutions, as we shall see below.
The difficulty we face is to select w k > θ making LP (w 1 ,…, w p ) bounded.
Now, boundedness of the primal problem is ensured if the dual problem is feasible (assuming primal feasibility).
Consequently, our first step is to find a feasible solution of the dual constraints, for some positive weights, if one exists.
In order to handle the requirement w k > 0, we use the freedom to multiply weights by positive scale factors to replace it with , which is equivalent to W k = 1 + v k , v k > θ.
In the case of P3, this means we must examine the constraints of the dual problem of LP (1 + v 1 , 1 + v 2 , I + V 3 ) which are
Using Phase I of the two-phase method, we obtain (Y 1 , Y 2 , Y 3 ) = (),(v 1 , v 2 , v 3 ) = (0, 0, 13) as a non-negative and feasible solution of (9.5), so LP (I, 1, 23), with objective function , is bounded.
Indeed, it has the optimal, and therefore efficient, solution (x 1 , x 2 ) = (0, 14),
If the third objective of P3 () is replaced by  to give problem P4, then the inequalities (9.5) are replaced, after rearrangement, by .
Using Phase I we can see that there is no feasible, non-negative solution of these inequalities.
The fundamental theorem then allows us to deduce that VMP has no efficient solutions.
(Otherwise, by the theorem, LP (w 1 ,…, w p ) would have an optimal solution for some set of positive weights and, therefore, the dual problem is feasible for these weights.)
It is possible that, having determined w k > θ for which the dual problem of LP (w 1 ,…, w p ) is feasible, we then discover that LP (w 1 ,…, w p ) itself is infeasible.
This means, of course, that VMP is infeasible and therefore has no efficient solutions.
Whatever happens, either we obtain an efficient solution or we can deduce that there are no efficient solutions.
This two-stage procedure is unnecessary if we can be sure,a priori,that LP (1,…, 1) is bounded.
This will certainly be the case if the constraints of VMP define a bounded feasible region.
However, testing for boundedness as a general procedure would not be worth while, in general.
Nevertheless, there are some cases where it is easy to recognise that the feasible region is bounded.
For example, if the problem includes a constraint of the form  where all  > 0, then the feasible region is bounded, since any feasible x j must satisfy x j < b j /.
This applies to problem P1 (either constraint) and so we can be sure that LP (w 1 , w 2 , W 3 ) has an optimal solution for any w 1 , w 2 , W 3 .
More generally, if the sum of some or all of the constraints is of the form (9.6) then the same conclusion can be drawn.
EFFICIENT TABLEAUX
In Section 9.4 we saw how to find an efficient solution.
However, the great majority of VMPs will possess more than one efficient solution and a complete resolution of the problem means finding all efficient solutions.
This can become very complicated, since efficient solutions which are not BFSs are typically involved.
Here, we will confine ourselves to obtaining all efficient BFSs and, as our first task, we will develop a test to determine whether the BFS corresponding to a given tableau is efficient.
To accommodate multiple objectives, we will extend the simplex tableau by including an additional objective row for each objective function.
For example, the initial tableau for problem P1 is P1/T1.
Each objective function relates to   one objective row in the tableau and, as usual, rows define equations, so the z 2 row represents .
Consequently, these additional rows are subject to the usual pivoting rules and the formulae (3.6) for generating the objective row from the objective function coefficients and the remaining rows are still valid.
For example, if we pivot as indicated in P1/TI, we obtain tableau P1/T2.
We would like to be able to recognise whether the BFS corresponding to P1/T2,(x 1 , x 2 , x 3 ) = (2, 0, 0), is efficient.
We shall see that this can be done by using a suitable generalisation of the single-objective criterion that the objective row coefficients should be non-negative.
We need some new notation to describe extra rows and will write  for the element in the jth column of the z k -row.
Thus, the z k -row can be written: where  is the objective row element in the resource column (=z k evaluated at the BFS).
Formula (3.6a) says that  where  is the coefficient of  in the kth objective function.
Our efficiency test will follow from the relationship between VMP and
LP (w 1 ,…, w p ).
Since these problems have the same constraints, we could write down a tableau for LP (w 1 ,…, w p ) having the same basic and non-basic variables (in the same order) as any given tableau of VMP.
If  is the element in the jth column of the objective row of LP (w 1 ,…, w p ), then applying (3.6a) to this tableau and using (9.7) gives .
If this tableau of LP (w 1 ,…, w p ) is optimal () for positive weights, then the BFS is optimal and hence efficient by the results of Section 9.3.
The requirement w k > θ can be translated, by scaling the weights, to w 1 or .
Therefore, the BFS is efficient if we can find a non-negative solution of  for all j .
In the case of tableau T2, these inequalities read:.
This has a solution (v 1 , v 2 , v 3 ) = (0, 0, 1), SO the BFS is efficient.
We established the same result in Section 9.2 by applying the CS conditions to LP* (2, 0, 0) and it is worth noting that the resulting inequalities typically involve more variables than if (9.8) is used, but (9.8) presumes a knowledge of the tableau as opposed to just knowing the BFS.
If a BFS is non-degenerate, it corresponds to only one tableau and the fundamental theorem allows us to deduce that, if the BFS is efficient, then (9.8) is satisfied.
For example, P1/T3 can be obtained by pivoting from P1/T1.
The inequalities (9.7) for P1/T3 are  and these have no non-negative solution.
Since the BFS is non-degenerate this can only mean that it is not efficient.
We will describe a tableau for which (9.8) has a non-negative solution as an efficient tableau.
We have seen that the efficiency of a non-degenerate BFS is equivalent to the corresponding tableau being efficient.
However, the fundamental theorem says that any efficient BFS is optimal in LP (w 1 ,…, w p ) for some w 1 ,…, w p > 0 and so it corresponds to some efficient tableau (Section 3.6).
Consequently, if we find all efficient tableaux and write down a list of corresponding BFSs (omitting duplicates, if necessary) we shall have found all efficient BFSs, whether or not degeneracy occurs.
In the next section we will describe a method for finding all efficient tableaux.
THE MULTIPLE-OBJECTIVE SIMPLEX METHOD
In Section 9.3, we saw how to find all efficient solutions for VMPs with two objectives.
In this section, we will describe a method for constructing all efficient tableaux for general VMPs.
The method starts from an efficient tableau and constructs others by pivoting.
We will illustrate the procedure by solving P4.
P4: V max  subject to.
To find an initial efficient tableau, we can use the technique of Section 9.4 to find w k > 0 making LP (w 1 ,…, w p ) bounded, solve this LP and then, in the optimal simplex tableau, we can replace the objective row with p new objective rows corresponding to the individual objective functions of VMP.
For example, in P4, LP (1, 1, 1) is bounded and the non-objective rows of the optimal tableau are given in the non-objective (first three) rows of P4/T1.
The entries in the z k -rows  can be calculated from formula (9.7) which is just a restatement of (3.6).
The result is P4/T1 and we know that this tableau is efficient.
If we make x 1 basic in P4/T1,s 3  leaves the basis and we can ask if the tableau obtained by making this pivot will be efficient.
We can offer a partial answer to this question by observing that, if we can find w k > O satisfying  and  for j = 2 and 3, then the BFS of P4/T1 is an alternative optimal solution of LP (w 1 , w 2 , w 3 ) and another optimal (efficient) solution can be found by pivoting in column 1 (x 1 -column).
This means that the new tableau will be efficient.
In terms of(9.8), if  denotes the jth slack variable in (9 .8), we are seeking a non-negative solution to (9.8) with .
We have written out inequalities (9.8) for P4/T1 in tableau format as a subsidiary tableau, P4/T1.1 below.
We can attempt to force  by maximising the infeasibility .
This is achieved in one simplex pivot in P4/T1.2.
So we see that the new tableau  obtained by making x 1 basic will be efficient.
The argument obviously generalises to show that,if there is a non-negative solution of (9.8) with ,then any new tableau obtained by pivoting in column j  is efficient .
Thus, we now seek to force  in P4/T1.1 or, since it also represents the same inequalities, P4/T1.2.
But the reader can check that P4/T1.2 is optimal for minimising , so no solution of (9.8) exists with .
In general, if  is basic in a subsidiary tableau and its row has no positive entries apart from a positive entry in the resource column, then there is no feasible solution with .
Hence, we cannot have  either.
Thus, only pivoting in the x 1 -column is guaranteed to result in an efficient tableau.
Making the indicated pivot gives P4/T2.
Making s 3  basic in P4/T2 would return us to P4/T1.
We therefore try to force  in the subsidiary tableau P4/T2.1.
Since P4/T2.1 is not primal feasible we can maximise  using Phase I rules.
Making the indicated pivot gives P4/T2.2.
In P4/T2.2, we have  and  (simultaneously, as it happens) and so pivoting in the second and third columns of P4/T2 gives efficient tableaux.
Making the indicated pivots gives P4/T3 and T4.
In P4/T3, the reader may verify that there is no feasible solution of (9.8) with .
Since making s 1 basic returns us to P4/T2 and making x 3 basic leads to P4/T4, no new efficient tableaux can be obtained by pivoting from P4/T3.
In P4/T4, there is no pivot in the s 3 -column, whilst making s 2  basic leads to P4/T3 and making s 1 basic returns us to P4/T2, so no new efficient tableaux can be obtained by pivoting from P4/T4.
We will see below that this means we have found all efficient tableaux.
Thus, the complete list of efficient BFSs is (x 1 , x 2 , x 3 ) = (0,1 ⅓,0),;(2,1,0),(⅔, ⅚,0),;(1,1,1).
At a general stage of the procedure we will have a list of efficient tableaux and ‘process’ one of them by examining the non-basic variables in turn.
We ignore those which cannot be made basic (no positive pivot) or whose introduction into the basis would lead to a tableau already on our list.
We then test columns containing the remaining non-basic variables by checking whether (9.8) has a non-negative solution with , for column j .
We then pivot in any column for which a solution is found and add the resulting efficient (unprocessed) tableau to the list.
This step is repeated by choosing a new  unprocessed tableau and processing it as above.
This continues until the list consists entirely of tableaux which have been processed, which must eventually happen since there are only finitely many possible tableaux.
We will now show that when all the tableaux in our list have been processed, the list contains all efficient tableaux.
To do this, we will start by describing two efficient tableaux reachable from each other by a single pivot as linked , if they are both optimal in LP (w 1 ,…, w p ) for the same positive weights.
This is equivalent to obtaining one tableau from the other by pivoting in the jth column while (9.8) has a non-negative solution with .
Thus, processing a tableau consists of finding new tableaux linked to the one being tested, and any tableau linked to a member of our list of processed tableaux will also be in the final list.
To prove that any efficient tableau is in this list, we will show that any pair of efficient tableaux can be connected by a sequence of linked tableaux.
Consequently, starting from any tableau on the list we can connect it to any other efficient tableau by a linked sequence, so the other efficient tableau must also be on the list.
Now, suppose that T and T are optimal tableaux and (9.8) is satisfied for (w 1 ,.
, Wp) and (w 1 ,…, w p ) respectively.
Consider the parametric problem LP (w 1 ,…, w p ) where .
When , T is optimal and, when θ = 1, T is optimal.
Using the parametric programming procedure of Section 6.3, we can connect T to T by a sequence of tableaux, each obtained by pivoting from its predecessor.
What is more, pairs of successive tableaux are alternative optima of LP (w 1 ,…, w p ) for w k satisfying (9.9).
But this means these tableaux are linked because  implies  for .
This justifies the assertion of the preceding paragraph.
Multiple-objective Problems — More Methods
COAL PROGRAMMING — DEVIATION VARIABLES
It can be argued that decision-makers' aims are often not to maximise profits (or sales etc.) but to achieve a satisfactory level (of profits).
This involves specifying a goal or target value for the objective function.
The decision-maker tries to maximise the objective up to the goal value but is not interested in values exceeding the goal.
For example, in the manufacturing problem described in Section 9.1, if a goal of 25 is specified for revenue, the manufacturer would seek to maximise revenue if it is below 25 but is not concerned with its value once it exceeds 25.
More precisely, we can say that the manufacturer wishes to maximise the minimum of revenue and goal:
The extremely dichotomous nature of this criterion, with its abrupt switch from maximising the objective to complete disinterest, is open to objection, but such criteria have found favour with some modellers.
However, we are still left with the problem of combining these modified criteria into a single objective.
In this section, we will use weighting factors, whereas in the next section we shall describe a different, and more radical, approach.
If, in the manufacturing problem of Section 9.1, we specify goals of 25 on revenue (as above), 30 for sales and 12 for exports and apply weighting factors w 1 , w 2 , w 3 > 0 to the resulting criteria, we arrive at P1: maximise  subject to.
P1 is not a LP, but we shall see that it can be transformed into one by observing that criterion (10.1) can be rewritten as and, if we define deviation variables d 1 ,e 1 , > 0 to satisfy  then, provided at least one of d 1 and e 1 is zero,(10.2) is equal to 
This suggests that P1 can be rewritten as P1 *.
P1 *: minimise  subject to.
Our formulation will be justified provided any optimal solution of P1 * has d k or e k = 0 (or both) for each k .
But, if we had a feasible solution with d k , e k > 0, we could reduce d k and e k by equal amounts without affecting feasibility and thus reduce the objective function, since w k > 0.
Hence, such a solution could not be optimal.
Note that this argument relies on positive weighting factors and fails if any w k is negative.
In some cases, the decision-maker's aim may be to force an objective as close to its goal as possible, both shortfall and excess being undesirable.
For example, in our manufacturing example, increasing exports reduces home sales and so we might use the criterion that exports should be as close as possible to their goal: 12.
This means that the objective function of P1 is changed to  where  is the absolute value of  (if  and  if ).
Now, if d 3 and e 3 satisfy the constraints of P1* and at most one of d 3 , e 3 is positive, then .
Consequently, we only have to change the objective function of P1 * to  (omitting constant terms).
By allowing e 3 and d 3 to have different (positive) coefficients, we could penalise under- and over-achievement differently.
GOAL PROGRAMMING — PRIORITIES
In this section we will assume that p objective functions are arranged in decreasing order of priority.
The exact implications of the ordering chosen will become clear below.
We will write  for the kth priority objective (k = 1 is the highest priority) and  for its goal.
To start, we consider the first (highest priority) objective and try to find a feasible solution (x 1 ,…, x n ) satisfying  and, if our search is successful, we impose this inequality as an extra constraint and then turn to the second objective.
If no such solution can be found, we impose the constraint that the first objective function should not drop below its optimal value before turning to the second objective.
If (x 1 ,…, x n ) is optimal for the first objective function, this implies that, if , then we impose the constraint .
Both cases can be covered by imposing the additional constraint  and then turning to the second objective.
A similar procedure is adopted for the second objective, imposing the constraint that it should not drop below whichever is the smaller of its optimal value and its goal.
We then proceed to the third objective and so on and continue until all objectives have been considered.
The additional constraint imposed on the kth objective before proceeding to the (k + l) th is that
Rewriting inequalities such as(10.3) in this form shows that by allowing a constant term in the objective we can effectively make the goal zero for all objectives.
The procedure outlined above can be re-expressed in terms of the criteria, such as(10.1) introduced in Section 10.1.
In that section, we combined the criteria additively using weighting factors.
Here, we sequentially optimise the criteria, starting with the highest priority objective and imposing (10.4), which says that we do not permit any reduction in the kth criterion, when passing from the kth to (k + l) st criterion.
More generally, we could use weighting factors  and priorities.
However, deviation variables always allow us to reformulate the problem in the form outlined above, which we shall refer to as a Priority Goal Programming Problem (PGP).
In the rest of this section we will show, by example, how PGPs may be solved by slightly modifying the simplex method We will start with problem P2 in which we write ‘Gmax’ to signal a PGP and list the objectives in order of priority (highest first).
Note that the goals are 16, -30 and 30, respectively.
P2: Gmax  subject to.
We commence by maximising the first objective function, using the simplex method, until we reach a tableau in which the objective function value is non-negative (so that the goal is achieved) or which is optimal.
The initial tableau is P2/T1 and, after two iterations, P2/T2 is reached.
In P2/T2 the goal has been achieved, so we impose the additional constraint .
This means we simply consider the z 1 -row as an extra constraint row and add a row for , using the formula (3.6a) for  (taking  to be ).
This gives P2/T3.
One further pivot results in P2/T4.
P2/T4 is optimal but Z 2 <0 so the goal has not been achieved.
The best we can do is Z 2 =-2.
Therefore, we add the constraint Z 2 > -2 or, equivalently, where .
Making this substitution in P2/T4 and adding a row for z 2 gives P2/T5.
Two further pivots give P2/T6.
From P2/T6 we see that the solution is (x 1 , x 2 , x 3 ) = (O, O, 2).
A potential difficulty arises if one of the objectives is unbounded as in problem P3.
P3: G max  subject to.
The initial tableau is P3/T1 and two iterations lead to P3/T2.
In P3/T2, we see z 1 is unbounded.
This means that there are feasible solutions with , but if we were to add z 1 a o as a constraint by treating the z 1 -row as a tableau row and incorporating a z 2 -row, the resulting tableau would not be primal feasible (and may also be dual infeasible).
The difficulty arises because there are no basic feasible solutions with .
To avoid this difficulty we can first pivot in the objective row and the unbounded (s 1 ) column.
This pivot must result in primal feasibility since, if i = 0 denotes the  z-row, we have , as the goal is not yet achieved and  for i > 1, where k is the pivot column, because of the unboundedness.
Hence,.
Apply this result to P3/T2, by pivoting as indicated; we obtain the non-objective rows of P3/T3.
Note that the goal is now achieved, since z 1 = 0, and we can complete P3/T3 by adding a z 2 -row.
One further pivot, as indicated, gives P3/T4.
In P3/T4, the goal is not achieved and so we should add the constraint .
However, the objective row of P3/T4 has only positive entries (apart from the objective function value) and thus a unique optimal solution.
This means that there is only one feasible solution satisfying  and, therefore, adding further objectives cannot lead to a new solution.
Consequently, we can assert that the solution of P3 is (x 1 , x 2 ) = (0, 6).
This would be true no matter what lower priority objectives were involved.
Quite generally, if we reach an optimal tableau, with a unique optimal solution, in which the goal is not achieved, the  for that tableau solves the PGP and there is no need to consider lower priority objectives.
MAXIMIN PROGRAMMING
One context in which multiple-objective problems arise is when the objective function can be one of several possibilities, which one being determined by external factors beyond the control of the decision-maker.
For example, the profits of the manufacturer introduced in Section 9.1 might be  depending on the state of the market in which he sells his goods.
This would arise if the prices of the three products were 1, 0.8, 0.9 in the first case, 1.3, 0.7, 0.5 in the second case and 0.7, 1.1, 0.9 in the third.
If sales and profits are ignored, the manufacturer is faced with combining the three objectives into one.
If he knew the probabilities of the three states of the market, he could compute and then maximise the expected profit.
With no knowledge of these probabilities, he might decide to assume the worst and maximise the lowest objective function, that is maximise min  subject to the constraints.
As it stands, this maximin problem is not an LP but it can be made into one by observing that it is equivalent to maximising  subject to min  and the constraints.
Furthermore, min  is equivalent to , so the problem becomes P4.
P4: maximise  subject to.
Discrete maximin problems are also of interest.
For example, we could imagine a firm (Firm I) deciding whether to launch a new product and faced with three possible actions.
Its profits will depend on the action chosen and the state of the market, which it has classified into four categories.
These profits are displayed in Table 10.1 in which rows represent actions and columns represent states of the market.
‘Assuming the worst’ means measuring each action by the smallest entry in the corresponding row.
Then each action has value 1 and all actions are equally good.
The pessimistic nature of the maximin approach is open to obvious objections.
If carried to extremes, no-one would cross the road because of the  outside possibility that they might stumble and be killed by a car.
However, in practice when using this approach, highly implausible cases are filtered out early and only outcomes with reasonably high probabilities are considered.
It may be felt that, when used with care, the maximin approach adequately reflects a conservative philosophy of decision-making.
Returning to the problem of Table 10.1, we have seen that the best value that can be achieved is 1.
However, if Firm I tosses a fair coin and uses action 1 if ‘heads’ results and action 2 otherwise, then the expected profits for each of the four states of the market are 1, 2, 2, 12.
The smallest of these is 12.
Since 12 > 1, we see that a pessimistic firm can improve its expected profits by using randomised strategies, where a randomised strategy specifies a probability P i that the ith action is chosen.
(In this context, we will sometimes refer to the original actions as pure strategies.)
The expected profit, if the first state of the market prevails, is P1 + 2P2 + P3 with similar expressions for other states of the market.
Hence, the maximin approach means maximising .
Since they are probabilities, p 1 , P2 and P3 must satisfy .
The trick used to obtain P4 allows us to rewrite the problem as problem P5.
P5: maximise  subject to 
It is clear that the maximum value of  in P5 is positive since, for example,P1 = P2 = P3 = 3 is feasible and gives .
This means we can add the constraint  without affecting the optimal solution.
Then, if we write x i for , the fifth constraint becomes .
Note also that maximising  is the same as the minimising .
Thus, dividing the remaining constraints by , P5 is transformed into P6 P6: minimise  subject to.
The constraint  is satisfied implicitly, since x 1 + x 2 + x 3 > 0 for any feasible solution of P6 and so it has been dropped.
The optimal solution of P6 is (x 1 , x 2 , x 3 ) = (,,)
Hence  and the optimal solution for Firm I is .
In general we consider a decision-maker having m actions available.
There are n states of nature and he receives a return of  if he takes action i and the state is j .
The pessimistic decision-maker prepared to adopt randomised strategies will have to solve the problem LPI*.
LPI*: maximise  subject to.
where P i is the probability of taking action i .
Provided we can guarantee that the optimal objective function value is positive, LPI* can be converted into LPI.
LPI: minimise  subject to.
If (x 1 *,…, x n ) is optimal in LPI,, the optimal value for Firm I, is  and the optimal solution is .
The reduction from LPI* to LPI required the assumption that the optimal objective function value of LPI* be positive.
In our example, we were able to deduce this because all the  s were positive.
The presence of  may prevent us from drawing such a conclusion.
In such a case, we may use the result that, if  for all i ,j , then   so that comparisons between strategies are unaffected.
This means that the optimal action(s) for a pessimistic decision-maker is (are) unaffected and the optimal value is increased by K .
If we choose K large enough to ensure that  > 0 for all i ,j , we can then use LPI to solve the problem.
In the example of Table 10.2, we can add 3 to all entries.
This gives Table 10.1, so the optimal solution is still  but the optimal value  is .
TWO-PERSON ZERO-SUM GAME THEORY
It is possible that the ‘state of the market’ faced by Firm I in the problem discussed in Section 10.3 is actually determined by a second firm, Firm II (duopoly).
Firm I's pessimism may be more readily justified if the competition between the firms is so intense that Firm II's choice of action is solely motivated by the desire to minimise the profits made by Firm I. This supposition is obviously too extreme to be realistic, but its relaxation involves further problems into which we shall not enter here.
More generally, we consider two players, I and II, with actions i (= 1,…, m) and j (= 1,…,n), respectively, available.
A decision problem in which I receives  and II receives -, if I plays i and II plays j , is called a two-person zero-sum game (ZSG —‘zero sum’ because the sum of the receipts of the two players is zero).
We have seen how I can solve the problem of choosing a pessimistic strategy, but the analysis assumed that one of the n states of nature will occur or, in game terms, that player II plays one of his pure strategies.
However, if I does well to use a randomised strategy, it seems sensible for II to do likewise and we must ask whether LPI* and LPI are still valid if II is using randomised strategies.
To analyse this case we note that I assigns a value to the randomised strategy (P1,…
, P n ) of   where  is the strategy adopted by II, min indicates the operation of minimising over  satisfying , and .
Now, for any randomised strategy , we have .
Conversely, by putting  and  for , we see that .
We have shown that  which implies that whether player II uses randomised or pure strategies is irrelevant to I's evaluation of his own strategies.
So LPI* and LPI are still valid.
There is a certain symmetry about ZSGs.
What goes for one player should also go for the other.
It therefore appears reasonable to perform a maximin analysis for player II.
Note that, since player II receives -, if he wishes to maximise his own expected receipts, he will aim to minimise the expected receipts of I. For example, in the problem of Section 10.3, we can re-interpret the columns of Table 10.1 as the actions available to Firm II, and Firm II seeks to choose the column with the smallest maximum entry.
In general, player II evaluates his randomised strategy (q 1 ,…, qm) pessimistically as  where  indicates maximisation over  satisfying .
He will try to choose  in order to minimise this evaluation and a similar analysis to that for player I shows that he can achieve this by solving LPII*.
LPII*: minimise  subject to and if the minimal value of  is positive, LPII* can be converted into LPII.
LPII: maximise  subject to where  for all j .
Note that, if  > 0 for all i ,j or a constant is added to all payoffs to ensure this result, then  for all feasible solutions so LPII* can be transformed into LPII.
For the problem of Section 10.3, Firm II must solve P7.
P7: maximise  subject to.
Now it is clear that P6 and P7 are dual problems and therefore we can read off the optimal solution of P7 from P6/T1, the optimal tableau of P6.
Using the idea of dual tableaux, expounded in Chapter 5, we conclude that the optimal solution of P7 is .
Hence , so that the optimal strategy for II is .
Quite generally, LPI and LPII are dual problems and the duality theorem applied to LPI and LPII says that the optimal objective function values are equal  and therefore , where  is the optimal value for player I (player II).
This can be rewritten as  in which form it is often known as the Fundamental Theorem of Game Theory .
The result applies even if we do not have all  > 0 since the device of adding a constant K to all terms simply adds K to  and .
Alternatively, we could have used the fact that LPI* and LPII* are dual problems.
Since our analysis assumes pessimistic behaviour, if player I plays , the optimal solution of LPI*, he will receive an expected return of at least .
If player II plays , the optimal solution of LPII*, his expected loss will be at most and therefore player I's expected return will be at most.
This says that, if both are playing pessimistically, player I is receiving  and cannot improve on this, so he has no incentive to change his strategy.
A similar conclusion can be drawn for player II.
We can describe the situation as stable in that neither player would gain from a unilateral change of strategy or, more graphically, that provided randomised strategies are used, pessimism is justified in ZSGs.
If we multiply the jth constraint of LPI* by q j  and sum over j , and the jth constraint of LPII* by P i and sum over i , where (q 1 ,…, q n ) and  are randomised strategies, we obtain .
Substituting (q 1 *,…, q n *) on the left and (P1*,…
, Pm) on the right gives  which again shows that neither player can improve his position by unilaterally adopting a different strategy.
Integer Programming
INTRODUCTION AND ROUNDING
If a linear programming problem is modified so that the variables are restricted to integer values, the result is called an integer linear programming problem (ILP).
An example is: P1: maximise  subject to.
For ILPs with two variables a graphical method can be devised (see Exercise 1).
This is of little use for problems with more than two variables.
Another possible technique is rounding , which starts from the optimal solution to the LP obtained by dropping the integer requirement on the variables.
We will call this LP the relaxation of the ILP.
For example, the optimal solution of the relaxation of P1 is (x 1 , x 2 , x 3 ) = (34, O, 18).
We can then round the non-integer components to integer values.
For example, if we decide to round down this gives (3, O, 1).
However, the objective function increases if we increase any variable.
So we might try rounding up to(4, O, 2), but this is not feasible.
A more general procedure would be to find all solutions obtained by rounding each non-integer variable in either direction (in the example this gives (3, 0,1),(3, O, 2),(4, O, 1),(4, O, 2)), select the feasible ones (in the example this means (3, 0,1) and (4, 0,1)), and choose whichever of these gives the greatest objective function value, in the example:(4, 0,1).
For large problems, a direct application of this method is impracticable.
For example, if the optimal solution of the LP has 40 non-integer components, there are 2&sup40; or about 10&sup12; solutions to test and this could well take years, even on a very fast computer.
Even after all this effort, the resulting solution may not be optimal.
For example,(4, O, 1) is not optimal in P1, as we shall see.
Even worse, there are ILPs in which none of the  rounded solutions is feasible (Exercise 1).
Nevertheless, provided their limitations are understood, certain heuristic methods, which are essentially sophisticated versions of rounding, can have some value for large-scale problems.
BRANCH-AND-BOUND METHOD
The apparently mild condition of integer-valued variables makes an LP much more difficult to solve.
One source of this difficulty is the absence of a duality theory, which means that there is no simple test of whether a feasible solution is optimal.
The method to be described, called Branch-and-Bound (B & B) has proved as effective as any in the solution of ILPs.
We will illustrate it by solving P1.
For reasons which will become clear subsequently we will refer to P1 as problem 0.
In the course of solving P1, we will create new ILPs by adding upper- and lower-bound constraints to P1.
The kth such problem created will be called problem k and the relaxation of problem k will be called LPk.
It is an elementary observation that, if the optimal solution of the relaxation of an ILP is integer-valued, it is also the optimal solution to the ILP.
So we start by solving LP0, the relaxation of P1, and hope that its solution is integer-valued.
The optimal tableau is P1/LP0 (ignore parts of the tableau in brackets and the σ 1 -row for the moment) and we see that our hope is not fulfilled.
Since x 1 = 34 in P1/LP0, the corresponding BFS does not solve problem 0.
However, any feasible integer solution must satisfy  or  so that if we create two new ILPs
(i)
by adding  to problem 0 to get problem 1 and
(ii)
by adding  to problem 0 to get problem 2,
then the optimal solution to problem 0 must also be optimal in problem 1 or problem 2.
Let us solve LP2 in the hope of obtaining an optimal solution which is integer-valued.
This involves adding the constraint  to P1/LP0.
(The choice of LP2, rather than LP1, was arbitrary.
We shall continue to make arbitrary choices in this selection but in the next section we will discuss how such choices may be resolved more sensibly.)
We will use the methods of Chapter 7 throughout this chapter, so we write  and the RHS of the x 1 -row of P1/LP0 changes to  as indicated in brackets.
Making the indicated pivot in the x 1 -row to restore primal feasibility gives P1/LP2.
LP2 does not have an integer solution because  so we can create two new ILPs, problems 3 and 4, by adding  to problem 2.
The solution process is displayed as an enumeration tree in Fig. 11.1, in which node k represents problem k .
The solution so far consists of nodes 0, l, 2, 3 and 4.
(The reader is advised to temporarily cover up the remaining nodes.)
Fig. 11.1 Enumeration tree for P1.
Nodes such as 1, 3 and 4 in the figure, which have not yet been developed (by solving the corresponding LP), are called active.
At each stage we select an active node and develop it.
Let us develop node 3, that is, solve LP3.
We can do this by making the indicated pivot in the row 03-row of P1/LP2 and the result is P1/LP3.
In P1/LP3, variable x 1 (and therefore ) is non-integral so we can grow two new branches to nodes 5 and 6 by adding the constraints  (equivalent to ) or  (equivalent to ) to problem 3.
This gives nodes 0, 1, 2,…, 6 of Fig. 11.1, and nodes 1, 4, 5 and 6 are currently active.
Developing node 5, by solving LP5, means pivoting as indicated in the σ 1 -row of  P1/LP3 () and yields the tableau P1/LP5.
This time we grow two new branches from node 5 to nodes 7 and 8 to give nodes 0, 1,…, 8 of Fig. 11.1.
Currently, nodes 1, 4, 6, 7 and 8 are active.
We now develop node 7 and find an optimal solution to LP7 by making the indicated pivot in the σ 2 -row in P1/LP5.
The reader can verify that this optimal solution is .
In terms of the original variables,(x,, x 2 , x 3 ) = (4, 0, 1).
Since this is integral it also solves problem 7.
There is therefore no need to grow new branches from node 7.
Furthermore,(4, 0, 1) is a feasible solution to P1 and therefore sets a lower bound,z =19 on the optimal objective function value of P1.
If we discover a better (larger objective function value) solution we will update the value of z .
The best feasible  solution so far discovered will be called the incumbent .
Before proceeding further we note that in P1/LP5 we have ,i .
e.
. Since all variables are non-negative this imposes the requirement .
This means that, since σ 1 is non-basic in P1/LP5, it can be eliminated and the σ 1 -column can be dropped, as marked by the asterisk.
Consequently, the pivot required to solve LP8 is in the x 2 -row and σ 3 -column, as indicated.
It is easy to see that, if this pivot is made, the resulting tableau is optimal for LP8 and has objective function value  Since the coefficients in the objective function are integral, the objective function value of any feasible solution of problem 8 is integral and not greater than 192.
Hence this value cannot exceed 19.
But z = 19, so no feasible solution of problem 8 can beat the incumbent.
We can therefore declare node 8 inactive and no further  branching is necessary.
In effect, we have pruned out those parts of the tree which could be grown from node 8.
This is the ‘bound’ part of the B & B method.
We are now left with active nodes 1, 4 and 6.
Developing node 6 means increasing the lower bound on x , from 4 to 5 in problem 3 and this is achieved by pivoting in the x 1 -row in P1/LP3.
The result is P1/LP6.
P1/LP6 is non-integral, so two new branches are grown with nodes 9 and 10 and the current enumeration tree is shown in Fig.11.1 if nodes 11 and 12 are ignored.
We now develop currently active nodes 9, 10 and 4.
The optimal objective function value of LP9 (pivot in the σ 3 -row of P1/LP6) is 19½ so node 9 can be declared inactive.
Adding the constraint  to problem 6 means that we would like to pivot in the x 3 -row of P1/LP6.
But there is no pivot available in this row so that LP10, and therefore problem 10, is infeasible.
Similarly, the absence of a pivot in the x 3 -row of P1/LP2 shows that LP4 is infeasible.
The infeasibility of problems 10 and 4 allows us to declare nodes 10 and 4 inactive.
This leaves node 1 as the only active vertex.
The optimal tableau is P1/LP1, which does not have an integer-valued BFS, so two new branches to nodes 11 and 12 are grown.
The current enumeration tree is shown in Fig. 11.1.
Pivoting in the a 3 -row of P1/LP1 gives, as optimal solution of LP11, with objective function value 20.
This exceeds the value of the current incumbent (z = 19), so z is updated to 20 and (3, 1, 1) becomes the new incumbent.
Since the optimal solution of LP12 cannot exceed that of LP1, which is 20 5, no solution of problem 12 can beat the incumbent solution and node 12 can be declared inactive.
No active nodes remain and we have explored the complete enumeration tree.
The optimal solution must be the incumbent solution:(3,1, 1).
The general procedure is clear.
We start with the single node 0, which is considered active, and set z large and negative.
At any future stage of the method an active node k is selected and developed by solving LPk.
We will write z k  for the optimal objective function value of LPk and [z k  ]for the greatest integer not exceeding z k  (so that).
There are four possibilities for LPk:(i) the optimal solution is not integral and ;(ii) the optimal solution is integral and ;(iii);(iv) LPk is infeasible.
(We have assumed that all objective function coefficients are integers.)
In case(i), two new branches are grown from k , giving two new active nodes.
In cases (ii),(iii) and (iv), node k is declared inactive.
In case(ii) the value of z is updated to [z k  ]and the incumbent solution replaced by (or defined to be, if there is no current incumbent) the optimal solution of LPk.
If there are no active nodes left, the problem is solved and the incumbent solution is optimal.
Note that in the early stages, when z is large and negative and there is no incumbent, the inequality does not need to be checked in cases (i) and (ii).
We have outlined only the bare bones of the B & B method.
For example, no mention has been made of which variable to base branching on, or which active node to develop next.
How such choices are resolved may have a considerable effect on the efficiency of the method.
We will discuss this fleshing-out in more detail in the next two sections.
PENALTIES
In this section we will describe some ways in which the choices involved in applying the B & B method may be resolved.
Unfortunately, there is no theory available which points to any particular approach being superior to others.
Plausible rules have been developed by studying aspects of ILPs but the ultimate arbiter is practicality: does the rule improve computational efficiency?
Such questions can only be answered by experimenting on a computer.
Choices can be made more easily if some of the consequences of such a choice are known.
We will therefore examine how bounds can be placed on the change in objective function value when a variable is chosen for branching.
We  first recall that new nodes are created by choosing a basic structural variable x r in row i of the optimal tableau of LPk which has a value  that is not integral.
We then create two new problems by adding the constraint ]or  1 to problem k .
We will call the first of these the down-problem at node k and the second the up-problem at node k .
The down-problem is solved by adding  to the optimal tableau of LPk and using the dual simplex method (in its upper-bound form) to restore optimality.
The constraint can be written  and involves adding the implicit row (or modifying an existing implicit row to) where  is the fractional part of .
The first dual-simplex pivot is in column J, where  and .
The new objective function value, after one pivot in the  -row and the Jth column, is .
This says that the decrease in objective function value after one dual-simplex is Dk (x r ) where  and Dk (x r ) is called the down-penalty .
At every stage of our solution of problem P1 in the preceding section, optimality was restored in a single dual-simplex pivot.
When this occurs, the optimal objective function value of the down-problem at node k is .
In general, several dual-simplex pivots will be required to restore optimality, in which case  is an upper bound on the optimal objective function value of the down-problem at node k .
Note that the formula for the down-penalty (and the up-penalty derived below) would still apply if the variable in row i had been x r .
We will now re-solve problem P1 of the preceding section.
The optimal tableau of the LP relaxation of P1 is, as we saw in Section 11.2, given in P1*/LP0, with the a 3 -row omitted.
(We will refer to P1 as P1* in this section so that tableaux are distinguished from those in Section 11.2.)
The down-penalties are therefore   
The up-problem involves adding  to the optimal tableau of
LPk.
This means that x r is replaced by  and  is changed to .
The pivot column is J, where  and .
This means that the decrease in objective function value after one dual-simplex pivot is , the up-penalty , where .
Once again is an upper bound on the objective function value of the up -problem.
In P1*/LP0, we find .
We have assumed that a pivot exists for both the up-problem and the down-problem (as was the case in P1*/LP0) so that both up- and down-penalties are defined.
We will consider the implications of the failure of this assumption later in the section.
Penalties are used in two ways.
Firstly, to choose which variable to branch on.
In this context penalties are implicitly treated as if they gave the exact decrease in objective function when a variable is branched on (as opposed to just a bound).
Secondly, the bounds provided by penalties can be used to prune the tree and avoid unnecessary pivots.
The use of penalties to choose the bounding variable is motivated by two considerations.
Firstly, the desire to find good integer solutions so that the enumeration tree can be severely pruned.
Secondly, the hope that branches not developed may be declared inactive by bounding.
An empirical rule based on these observations is to branch on the basic structural variable x r with largest penalty (either up or down).
Thus, for P1*/LP0, we would select x 3 (since  is greatest).
Two new branches are grown by adding  and   to LPk.
We then immediately develop the down-problem if  is the largest penalty and the up-problem if  is largest.
In P1*/LP0 this means growing new branches with  and  and developing the down-problem (since ) is greatest).
This is achieved by pivoting in the  -row of P1*/LP0 to get P1 /LP1.
Since , the down-problem at node l is developed by adding , which means pivoting in the  -row of P1*/LP1 and gives P1*/LP3.
The enumeration tree now looks as in Fig. 11.2 if nodes 5 and 6 are ignored.
Fig. 11.2 Enumeration tree for P1*.
The penalties indicate developing the up-problem at node 3 next.
We therefore solve LP6 by pivoting in the x 2 -row of P1*/LP3 and find an optimal tableau  with integer BFS (x 1 , x 2 , x 3 ) = (3, 1, 1) and objective function value 20.
This becomes the incumbent and we put z = 20.
Nodes 2, 4 and 5 are currently active and we can now use penalties to prune potential growth of the tree.
From the definition of penalties, we have  so node 5 can be declared inactive.
Similarly, and .
Hence  so nodes 4 and 2 can be declared inactive and the problem is solved.
The complete enumeration tree is shown in Fig. 11.2.
It may happen that the optimal tableau of LPk for some k has  for all  in a row i containing a structural variable x r and  is not integral.
Then Dk (x r ) is not defined and we conventionally put .
Adding the constraint  involves adding a row with no negative pivot, so the down-problem is infeasible.
In this case we can immediately develop the up-problem and declare the node on the down-branch inactive.
We will call this a forced move .
Note that the convention  (therefore the largest penalty) makes a forced move consistent with our earlier rule.
In the same way, if , we can write  and develop the down-problem, declaring the node on the up-branch inactive.
The remaining scope for choice surrounds the selection of an active node for development.
The rule we have used in our solutions so far is to develop one of the most recently created active nodes.
This rule says that, if a pair of new branches have been created, we immediately develop one of them.
Which one can be decided arbitrarily (as in the preceding chapter) or using penalties (as in this chapter).
If the nodes are numbered in order of creation, and a node has just been declared inactive (for any reason), the next one to be developed should be the highest numbered active (= most recently created) node.
This method is often called depth-first search or last-in, first-out (LIFO).
A contrasting rule is called frontier search .
Whenever the result of the development of node k is that two new branches are grown, the up- and down-nodes are assigned values of  and  (where x r is the branching variable) respectively.
This implies,inter alia , that nodes corresponding to infeasible problems are assigned the value  and can be declared inactive.
Then, at each stage, develop the active node with the largest assigned value.
This rule tends to reduce the number of nodes in the tree at the expense of storage requirements.
We will illustrate frontier search by solving a mixed integer linear problem (MILP) in which some, but not all, of the structural variables are required to be  integer-valued.
MILPs can be solved using our B & B techniques with two modifications.
Firstly, and obviously, we only branch on a variable required to be integer-valued.
Secondly, since the optimal objective function value is not necessarily an integer, we can declare a node k inactive by bounding only if  is no longer enough).
Practical problems are typically MILPs rather than ILPs.
We will now solve problem P2 which is problem P1 of Section 11.1 modified by requiring only x 1 and x 2 to be integers.
The optimal tableau of the LP relaxation of P2 is P1/LP0 (of Section 11.2).
However, x 3 is not now required to be an integer so we can only branch on x 1 .
The value assigned to the down-problem (node l) is  and to the up-problem (node 2) is .
Node I has the greatest assigned value, so we develop node 1, giving the optimal tableau P2/LP1.
In P2/LP1 we branch on x 2 to create (down) node 3 with  and (up) node 4 with , where AVk denotes the assigned value at node k .
The tree now looks like Fig. 11.3 and node 2 has the greatest assigned value of the active nodes (2, 3 and 4) so we solve LP2.
This has optimal solution (x 1 , x 2 , x 3 ) = (4, 0,14) in which x 1 and x 2 are integral and .
We  there-set and (4, 0, 14) becomes the incumbent.
Fig. 11.3 Enumeration tree for P2.
Nodes 3 and 4 remain active.
But  and  so 3 and 4 can be declared inactive.
The problem is solved and the incumbent solution (4,0, 1¼) is optimal.
When a choice of branching variable is necessary we can choose the variable which leads to a node having the largest assigned value.
This means choosing the variable with smallest penalty (either up or down).
STRATEGIES FOR LARGE-SCALE PROBLEMS
In this section we will offer some suggestions, all of which have proved valuable in practice, intended to render more efficient the implementation of the B & B method on large ILPs and, especially, large MILPs.
These suggestions are mainly concerned with ways of making the choices implicit in the B & B methods which were discussed in Section 11.3 and we shall start by considering the selection of a branching variable.
No numerical illustrations are given because it is the very nature of the techniques described that they are successful on large problems — too large to illustrate explicitly here.
Experience gained from solving and modelling with ILPs will often suggest that some integer variables in a particular problem are more ‘important’ than others, in that it is desirable to force the important variables to integer values early in the solution.
This is implemented by allowing the solver to specify priorities.
When the branching variable is selected, a high priority variable is preferred to one of low priority.
Another factor influencing this selection is the observation that variables which are nearly integer-valued often become integers, when branching on other variables.
For this reason it can prove profitable to branch whenever possible, only on variables which differ by more than some prespecified tolerance, 0.1 for example, from the nearest integer.
Even when these two methods are employed, some automatic selection of branching variables is useful.
In Section 11.3 we saw how penalties could be used.
The justification for selecting the variable with the largest penalty implicitly assumed that the penalty gave a useful estimate of the decrease in objective function value and not just a bound.
When only one dual simplex iteration is required to restore optimality, the bound is equal to the decrease, but in large problems, typically, several dual-simplex pivots are needed.
If z k  and ZD are the optimal objective function values of, respectively, LPk and the down-problem created by branching on x B i at node k , then we will define the down-pseudo-cost of  to be  where  is the fractional part of .
The up-pseudo-cost is  where  is the objective function value of the up-problem.
Computational  experience has shown that pseudo-costs do not vary too much from node to node (which is why we have not given them a ‘k’ subscript).
Consequently, pseudo-costs can be calculated early in the solution and used to estimate  and  at nodes further down the enumeration tree.
When several dual simplex iterations are required to restore optimality, pseudo-costs will generally give better estimates of these quantities than penalties.
In addition, pseudo-costs are cheaper to compute.
However, unlike penalties, they do not give guaranteed bounds, so penalties and pseudo-costs are not exact competitors.
Forcing x r to an integer value reduces the objective function by approximately  for the down-problem and  for the up-problem.
Hence, branching on  leads to a reduction of at least and selecting the variable which maximises (11.1) has proved a successful branching rule.
When solving the up- or down-problem at node k in preceding sections, we first modified the original tableau of LPk to allow for the new or changed, upper or lower bound.
Optimality was then restored using the dual simplex method with implicit upper bounds.
For large problems, parametric variation of the bound can prove more efficient.
Using this technique to change the constraint , for example, would mean solving the parametric problem  starting with θ = 0 and terminating at θ = 1.
This can be done easily, by combining the methods of Chapters 6 and 7.
Another area of choice in B & B concerns the development rules for exploring the enumeration tree.
In Section 11.3, we described two possibilities: depth-first and frontier search.
The first has the advantage that no time is wasted in finding a first feasible solution.
This means that substantial unprofitable parts of the enumeration tree are often pruned away by bounding.
In addition, depth-first search minimises storage requirements, but ignoring objective function values completely does not seem very sensible and this is borne out in practice.
Frontier search results in small trees but finds a feasible solution late in the process.
This usually means little pruning and, potentially even more serious, since large problems frequently have to be terminated before they have been completely solved to avoid excessive computation time, frontier search may provide no solution at all.
A hybrid procedure combining most of the merits of both methods is to develop immediately one of the up- or down-problems (This is depth-first search).
However, if at node k no new problems are created (because LPk has an integer solution, or is infeasible, or because of bounding), we develop the active node with the largest ‘value’.
We shall define this value more precisely below.
This rule (and similar variants) produces an optimal solution well before the enumeration tree is fully explored.
Since much of the computation time is used to establish that the solution is optimal, premature  termination of the procedure can often save time and yet still give the optimal solution, although its optimality cannot be guaranteed.
Development rules involving elements of frontier search need to assign a value to active node k .
Ideally we would set this value equal to the optimal objective function value of problem k , say .
The anticipated value used in Section 11.3 and based on penalties is unlikely to be very close to .
We may expect to generate a better estimate by using pseudo-costs.
If we make the (unrealistic) assumption that, in driving some variables to integer values, the other variable values do not change much, we can estimate the optimal objective function value of the integer down-problem created by branching on  at node k to be  where z k  is the optimal objective function value of LPk.
This expression uses formula (11.1) and a similar expression holds for the up-problem.
The development strategy can be further controlled by specifying a parameter  and temporarily removing nodes satisfying  from the list of active nodes.
We would then hope that, if  is well chosen, an incumbent will be found that enables node k to be removed permanently.
If our choice of  is over-optimistic, we may have to re-instate temporarily inactive nodes.
A related approach relaxes the requirement of finding an optimal solution to finding a solution with objective function value within ) of the optimum.
In the enumeration tree, this means that node k can be declared inactive if .
This often substantially reduces the size of the tree.
The resulting solution is called ε-optimal.
NON-LINEAR PROGRAMMING USING SPECIAL ORDERED SETS
The B & B approach described in preceding sections can be adapted to a much wider class of problems than integer programming, and in this and the next section we will offer two examples.
Both topics covered have been studied in much greater depth than we have space to discuss here, but our intention is only to illustrate the breadth of applicability of the B & B philosophy.
Problems in which the objective function or the constraints (or both) are nonlinear are called Non-linear Programming Problems (NLPs).
We will describe a procedure for solving certain NLPs.
Initially, we will illustrate the procedure by ‘solving’ a very simple, though non-trivial, example: P3 below.
P3: maximise  subject to.
Rather than solve P3 itself we will solve an approximate version of the problem, generated by approximating f .
To do this, we evaluate f at certain values of x 1 and draw straight lines between adjacent points.
This is illustrated in Fig. 11.4 where f has been evaluated at x 1 = 0, 1, 2, 3.
(There is no need to choose larger or smaller values as they would not be feasible in P3.)
Fig. 11.4 PWL approximation.
The approximating function , drawn dashed in the figure, is called a Piecewise-Linear (PWL)approximation and we will now define  rigorously.
For , let us write , where  (so that), and define  since f(0) = 2, f(1) = 3.
Similarly, for , we can write , where  and define  since f(2) = 2.
The definition for  should now be obvious.
As it stands, this definition of  is rather cumbersome, but it can be restated in a more  succinct form as follows (noting that f(3) = 5), where  and , and λ 1 ,…, λ 4 > 0, provided that λ 1 1,…, λ 4 constitute a Special Ordered Set (SOS).
This stipulates that either λ 1 , or λ 2 , or λ 3 , or λ 4 , or λ 1 and λ 2 , or λ 2 and λ 3 , or λ 3 and λ 4 is (are) positive.
The B & B scheme outlined for problems involving SOSs differs from that for
ILPs in one notable respect, namely it permits the possibility of the same solution being feasible for both branches created at a node.
In particular, if we have  on one branch and  on the other, then  for  is an SOS solution satisfying the constraints on both branches.
It is responsible, in the solution of P3*, for LP2 and LP4 having identical optimal solutions ().
Such an occurrence would not be possible when solving ILPs.
Although the existence of common solutions to distinct branches does not affect the validity of the B & B procedure, it could conceivably lead to increased computation time.
However, the remedies for avoiding common solutions, for example a positivity constraint on one branch, are likely to involve more additional computation time than their use would save.
It is simpler to learn to live with the proposed branching rule.
The choices noted in Sections 11.3 and 11.4, which must be resolved when implementing the B & B scheme for ILPs are also relevant when using SOSs.
Similar concepts can be employed in their resolution.
For example, pseudo-costs can be generalised.
In addition, the accuracy of the approximation must be considered, possibly within the B & B solution.
In our problem, we would look at how well P3* approximates P3.
For example, we can observe that, at the optimal solution (x 1 , x 2 ) = (2¾, 0) of P3*, z (in P3*) = 44.
whilst f(24) =3.55.
But this only gives information at one point.
Ideally we would like to know how good the approximation is over all x .
In addition, we could change the number of points in each SOS.
One obvious, and successful, method is to start with a course representation and subsequently make it finer around the optimal solution, using the accuracy of the approximation as a guide.
When pursued in detail, such procedures can become quite complicated.
Finally, we must consider what class of NLPs can be treated by PWL approximation using SOSs.
In the case of problems with two variables we can approximate the NLP: maximise  subject to by taking  and writing maximise  subject to.
This approximation obviously extends to problems with more than two variables provided the objective function and constraints are separable,that is, they are sums of functions of individual variables, for example ,.
Even separability can sometimes be forced on problems.
For example, a non-separable term such as can be written , where  are added as constraints, although the use of such tricks can adversely affect computation time.
However, the really important area of application is to problems which would be linear but for a few (separable) nonlinear terms and here it can be very effective.
THE TRAVELLING SALESMAN'S PROBLEM
Consider a salesman who has to visit each of n towns once, starting from and returning to town l .
We will call any such trip a tour .
The Travelling Salesman's Problem (TSP) is to find a minimal length tour, where  is the distance from town i to town j .
Apart from its direct interpretation, the TSP can also be applied to problems that have nothing to do with towns and salesmen.
For example, the problem of sequencing the production of n batches of paint of different colours on one machine, where  is the cost of cleaning the machine when colour j is to follow colour i , can be formulated as a TSP.
A great deal of research has been undertaken into the understanding of TSPs and our intention here is simply to illustrate an alternative branching scheme using the special structure of the problem.
We will do this by solving an example, leaving the (straightforward) generalisation of the method to the reader.
For any tour, we will write x i j = 1 if town j succeeds town i in the tour and x i j = 0 otherwise.
Then the length of the tour is  and this is to be minimised subject to representing a tour.
Any tour must leave town i , so  for each i , and arrive at town j , so  for each j .
These are the objective function and constraints of an AP, but TSP is not an AP.
To see this, consider the problem of Table 11.1.
Using the method of Section 8.4, we can find the optimal solution of AP0, the corresponding AP.
It is  and this is represented graphically in Fig. 11.5.
From this figure we see that the optimal solution of AP is not a tour so that AP is not TSP.
However, AP is a relaxation of TSP, but extra constraints eliminating subtours  (for example, 1 in Fig. 11.5) are required in TSP.
We will not write these down explicitly but will impose them successively in a B & B procedure.
Fig. 11.5 Subtours in the relaxation of a TSP
We can eliminate the subtour involving towns 1, 3 and 5 by specifying that the tour must go from town 1 to 2, 4 or 6 (problem 1) or from town 3 to 2, 4 or 6 (problem 2) or from town 5 to 2, 4 or 6 (problem 3).
In problem 1, we want  and this can be ensured by imposing .
In problem 2, we want , and in addition we don't need , since this is included in problem 1.
So problem 2 has  .
Similarly, problem 3 has  and .
To solve APk, the relaxation of problem k , we impose constraints such as x i j = 0 by changing  to K , where K is some large number.
Thus, to solve AP1, we change  and  to K and start from the optimal solution of AP0.
The optimal solution of AP1 is  and the reader can verify that this corresponds to the tour 1→2→6→4→3→5→1 with length 20.
The optimal solution of AP2 has length 24 and is eliminated by bounding.
(This is a minimisation problem.)
Similarly, the optimal solution of AP3 has length 20 and is also eliminated.
Thus, the enumeration tree is completed and the optimal solution of the TSP is the tour of length 20 given above.