

INTO THE TWENTIETH CENTURY
2.1 From the Eighteenth Century to the Present
The eighteenth century marks a turning point in British population history.
It begins just after Gregory King's population estimate in 1695 of 5.3 million (Glass 1965a); possibly no more than the medieval peak.
At its end, the first census of 1801 recorded the unprecedented population of 9.2 million; a decisive break with the past.
From about 1750, population increased yearly for the next 226 years (figure 2.1).
Britain became the most urbanized country in the world, with the fastest growing population, and the first to establish the primacy of the industrial mode of production.
By the end of the century industrialization uncoupled fertility and population growth from its Malthusian connection with the price of grain and real wages.
Fertility could remain high and mortality continue to decline without population growth being stifled by a fall in wages and increases in the cost of subsistence.
(figures 1.4, 2.2).
Substitution of manufactured exports for domestic grain imports permitted the population to be fed for a century, beyond the capacity of domestic agriculture, without a reduction in fertility or a rise in mortality.
The new factory system made earlier marriage easier by the end of the eighteenth century and larger family size correspondingly more common, although these trends were reversed by the  early nineteenth century.
New prosperity did not raise fertility to new heights.
Instead, by the mid-nineteenth century family limitation within marriage first appeared.
New sources of data
Compulsory civil registration of births and deaths replaced the defective parish registers from 1837 (OPCS 1987a).
It puts discussion of the nineteenth-century mortality and fertility trends on a different footing from the uncertainties of previous centuries, where perpetual controversy flourishes in the dark.
It included the medical certification of cause of death and the age, sex, and occupation of the deceased.
Parish registers of baptisms were increasingly incomplete, of declining value as proof of ancestry and statistically inadequate, a source of annoyance to officials and derision by foreigners.
At first, registration   of births was only about 95 per cent complete.
Mothers were not asked questions about their previous children and the date of their marriage until the Population (Statistics) Act of 1938, so earlier analysis is rather.
thin (Werner 1987).
After a contentious and abortive attempt to set up a population register in 1753, a Bill for a Census was finally passed without fuss in 1800 (Glass 1973).
The population base is known from 1801 (with initial under-enumeration, see Wrigley and Schofield 1981) and in more detail from the first household-based census of 1841.
The 1911 Census was the first to ask detailed questions about fertility, child survival, and occupation (Craig 1987) and to use a form of classification of social class (Szreter 1984).
However, individual census household schedules are kept secret by the hundred-year rule, and it is impossible to use civil registers to reconstruct individual life histories and family formation.
In these respects the demography of the nineteenth century is less easy to explore than that of the parish-register period.
Controversies
These new data have made for better informed controversies but have not resolved them all.
The reasons for the declines of  mortality and fertility are still not agreed; nor the relative impact on mortality decline of the conquest of smallpox, the agricultural revolution, or environmental improvement (Woods and Hinde 1987); why infant mortality fell in the eighteenth century but not in the nineteenth; economic versus cultural explanations for the decline in fertility.
There is yet no general explanation for the fertility decline (Cleland and Wilson 1987); it is now clear that the transition began within the space of a few decades throughout Western Europe, from Trieste to Tromso, in a wide variety of countries (Coale and Watkins 1986).
There is still argument about the primacy of trends in birth- or death-rates in generating population growth.
Malthus (1830) and Habakkuk (1953) argued that fertility change (regulated through marriage) was primary.
The population reconstructions of Wrigley and Schofield (1981) support them and indicate that from the sixteenth to the nineteenth centuries  fertility change was about twice as important as mortality in accounting for variations in population growth, and that declines in mortality were modest until the mid-nineteenth century.
McKeown (1976) and Hollingsworth (1982) insist that mortality response was the key to understanding the beginning of the demographic transition, particularly its continued decline despite the rapid population growth and urbanization of the nineteenth century, which might have been expected to raise the death rate.
2.2 The Decline of Mortality from the Eighteenth century
General trends
The pattern of mortality decline changes substantially across the two centuries.
Infant survival improved considerably in the eighteenth century, then showed no further improvement until the first decade of the twentieth.
Improvements in public sanitation and the consequent tall of cholera and typhoid, and also of tuberculosis, belong wholly to the later nineteenth century.
Mortality fell only erratically and modestly   during the eighteenth century.
The most significant development is the gradual disappearance of crisis mortality (Schofield 1972), clearly graphed in the burial totals (figure 1.4).
Family reconstitution work and later vital registration shows the improvement of expectation of life (figure 2.3), also noted at the time from early life insurance and annuity schemes.
The reports of the Select Committee on the Laws respecting Friendly Societies (1825–7) show improvements of 20–35 per cent from the early eighteenth to the early nineteenth century.
Assurance against death began in the late seventeenth century; properly funded schemes based on actuarial principles were available from 1762 (Ogborn 1962), a product of the advance of science and of financial services.
Improvements in expectation of life are slight in the eighteenth century.
It only increased from about 37 in 1801 to 42 by 1842 (Farr's First English Life Table based on census and vital registration).
About seven more years had been added to expectation of life up to 1901, after which the pace accelerated sharply.
Lower infant mortality may have contributed to the eighteenth-century fall.
From the 1840s child and young adult mortality contributed most of the decline.
Infant mortality remained unremittingly high at about 150/1,000 from 1840 until about 1900, although this apparent stability may have been in part an artefact of the extension of birth registration to the least healthy groups of the population, and of a shift in the social class composition of births.
Further declines in infant mortality and substantial declines in mortality of older adults had to await the twentieth century, where the former was substantially responsible for the improved trend in expectation of life (see Woods and Hinde 1987).
Improvements in the survival of the elderly were modest (figure 2.4).
The first national life tables (1842) already show that women enjoyed better survival than men at all ages, and their lead in expectation of life has expanded from two to six years.
Improvement in female mortality is particularly striking in young adult life as childbirth became safer — and, after 1870, less frequent.
Because tuberculosis affects women more than men, the gradual decline of that disease benefited female survival.
Female advantage in life expectation is not universal.
Where the status of women is low, female mortality may exceed that of males: in Pakistan and some states of North India today, and, because of the prevalence of TB, in Ireland up to the beginning of this century (Coleman forthcoming a).
Social and geographical contrasts
There are substantial social and geographical differences in mortality at this time.
By the early nineteenth century some areas and classes had   already achieved levels of survival not enjoyed by the country as a whole until about 1900.
The improvement of mortality from the nineteenth century is to a considerable degree the improvement of the risks in the worst favoured classes and areas (Woods and Hinde 1987).
National improvement in mortality faltered as a result of the mass movement of people to the unhealthy towns.
Perhaps the most remarkable aspect of mortality during early industrialization, accelerating population growth, and the Napoleonic Wars is that it did not worsen.
Instead expectation of life rose from 36.8 years in 1796–1800 to 40.2 in 1836–40 (Wrigley and Schofield 1981); these correspond to CDRs of 25.1 and 21.7 respectively.
Industrial towns and mining villages grew fast through migration and their own natural increase (table 2.1).
By 1850 half the population lived in cities of over 10,000 inhabitants, far ahead of any other European country.
Urbanization went hand in hand with a rapid change in the distribution of the work-force towards manufacturing (table 1.7).
Much of the housing rapidly thrown up to accommodate the workers was overcrowded and insanitary (see Ferguson 1964).
Contemporary criticism of these deplorable urban conditions (notably Chadwick's Report of 1842, see Flinn 1965) eventually led, after long delays, to national improvements in sewage and water systems, building regulations, and local government.
There were sharp differences between different areas and classes in the same towns (table 2.2).
In 1831–9, Farr (1840) estimated that the death-rate in the country areas was 18/1,000 compared to 26 in the towns.
A survey of Manchester in 1840 (Flinn 1965) shows that crude death-rates varied from 16 in the suburbs to 35 in the poor urban areas.
Professional families in Liverpool had about the same age at death as labourers in Wiltshire (table 2.2).
As late as 1861, expectation of life at birth for males in Liverpool was 26; in Okehampton, Devon, it was 57; as big a difference as the national difference between 1840 and 1960 (Woods 1982).
In general, expectation of life in rural areas in the south, south-west, and Wales   was oiler 50, and under 30 in urban Lancashire, Yorkshire, and the north-east.
Infant mortality in Leicester, Preston, Manchester, and other large towns in 1871–80 was over 200 per 1,000 births per year when nationally it was about 150 and under 100 in favoured rural areas (Woods and Hinde 1987).
Social and geographical differences in mortality are still with us and are still contentious issues (Chapter 8); but these very severe differences have gone, and the convergence of mortality is one of the major factors behind the faster improvement of mortality from the end of the nineteenth century.
Arguments about what caused the decline in mortality shift between eighteenth- and later nineteenth-century changes.
The slight improvements in the eighteenth century are important because they mark the beginning of the downward trend.
Because of the weakness of data it is difficult to come to definite conclusions.
Later causes of decline are clearly different, and do not relate to the same age-groups or causes of death, and there are better data to argue over.
Contending explanations include the adoption of new ways of caring for infants, the rise of medical science including the attack on smallpox, improvements in diet and general living standards, and improvements in urban sanitary conditions, especially drinking water and sewage disposal.
The first two are particularly relevant to the eighteenth century.
2.3 Infant Mortality and the New World of Children
In the parishes studied by Wrigley and Schofield, IMR declined from about 160 in the first half to about 130 in the latter half of the eighteenth century.
Mortality of the babies of aristocrats fell from about 160 to about 100 at the same time.
In a group of Shropshire  villages, infant mortality fell from about 200 in 1661–1710 to about 100 in 1761–1810 (Jones 1980).
Brownlee (1925) used the London Bills of Mortality to show that the death-rate of children under 2 fell by 40 per cent from 1730 to 1800.
Infant survival improved during the eighteenth century, but it is not obvious why.
Many historians believe that ‘there is a tremendous change in attitudes towards children from the 17th to the 18th centuries’(Plumb 1976).
Historians themselves have ‘rediscovered childhood’ in the last three decades (Aries 1962).
Before the eighteenth century, literature, diaries, monuments, art, and law indicate a coolness towards children (Stone 1977).
Many early child-rearing practices were barbarous by modern standards.
Children were swaddled shortly after birth, to keep them out of mother's way.
Medical opinion believed that early breast milk (antigen-rich colostrum) was harmful, and that feeding should be delayed until the passage of the first stools.
Early medications and purges were recommended, and unsuitable solids for early weaning, not to mention spirits and  laudanum to keep the infant quiet (Fildes 1980).
Medical advice, like wet-nursing, was only followed by the wealthy, and may explain their inferior infant mortality rates.
Breast-feeding
Breast-fed babies usually survive better than artificially fed babies (see Chapter 7)(Beaver 1973).
In modern societies living in a healthy environment, the difference is slight.
But in developing societies, as in Victorian times, the difference in death-rates can be up to threefold (Knodel and van de Walle 1967).
Furthermore, the contraceptive effect of lactation doubles the interval between babies in women not practising birth control (Bongaarts and Potter 1983), contributing further to child survival.
In the eighteenth century educated women became more likely to breast-feed their babies.
By the century's end it was fashionable to breast-feed, often in an ostentatious way, as now (Stone 1977).
The Dispensaries made breast-feeding one of the main planks in their platform for reform.
By the latter half of the century, the majority of books on child care — which were enjoying a tremendous popularity — strongly recommended breast-feeding and described it as the normal practice (Fildes 1980).
In the mid-seventeenth century no text recommended early breast-feeding with colostrum; by 1800 all did.
These apparently autonomous changes in attitude, with their powerful demographic consequences, are difficult to account for (Stone 1977).
But they were accompanied by other changes indicating greater interest in childhood as a unique stage in development.
More was spent on  children and produced for them.
Previously they had few books or toys.
By the eighteenth century toy windmills, printing presses, farms, alphabet cards, and jigsaw puzzles were to be found in the nurseries of the better-off.
After the 1740s children's books included part works for the children of the poor.
Foundlings and infanticide
But there was a darker side in attitudes to children.
Unwanted babies were frequently abandoned in the streets, particularly in the great towns.
Many but not all the mothers were unmarried: prostitutes and servant girls made pregnant by their masters.
Overlaying, often recorded in the Bills of Mortality, was a cover for infanticide.
Babies were often sent away to be nursed in rural parishes where they were ill fed, overcrowded, and quieted with opiates (see Fildes 1988).
Few returned, as their parents would well know.
In England the practice was not nearly as widespread as in France (Langer 1972), where ‘only the middling poor, probably a substantial proportion of rural society,…nursed all their own infants’(Flinn 1982).
Legislation to control baby farms in Britain was not introduced until 1872.
Thomas Coram's revulsion at the infant corpses in London gutters led him to campaign for a hospital for foundlings, established in 1741.
It proved impossible to find enough wet nurses for the thousands of babies left at the hospital.
Mortality rates at this and similar institutions were astonishingly high: only 4,400 of the 15,000 admitted in the first four years lived to adolescence.
This movement was more developed in other great European cities, especially in France where the mother of an illegitimate child had no legal recourse to the father and no access to poor relief (Fuchs 1987).
For example, the Hospice des Enfants-Trouvés in Paris, founded 1670, was admitting several thousand babies a year by the 1770s, equivalent to over 20 per cent of the yearly baptisms in Paris, although almost half came from outside the city, and 13 per cent into the nineteenth century.
Most of these babies, put out to wet nurses, failed to survive infancy.
In the country areas in England the Overseers of the Poor Law assigned foundlings to paid nurses.
Of about half a million foundlings christened in workhouses after 1728, only 40 per cent survived to their second birthday.
The adoption of contraception was one response: illegitimate births began to decline from about 1850 throughout Europe (Shorter et al .
1971).
Abortion was another; abortifacients, some of them potentially effective like ergot and savin, were advertised from the eighteenth century onwards in journals and handbills (Potts, Diggory, and Peel 1977).
Infanticide became a major and much discussed social problem, with about 1,000 recorded cases each year (Sauer 1978).
These pressures against infant survival can be seen as early indications that fertility was an increasing burden in a changing society.
2.4 The Irrelevant Rise of Scientific Medicine
In the enlightenment of the sixteenth and seventeenth centuries, scientific medicine finally struggled out from beneath the dead hand of classical authority.
Armed with the new empirical understanding of the human body and its function from Vesalius, Harvey, Malpighi, Hooke, Baillie, and others, respect for medicine grew and the profession expanded, although London and Edinburgh remained the only British medical schools.
Altogether twenty-five provincial hospitals were established in the eighteenth century, usually by individual benevolence, as in John Radcliffe's Oxford Infirmary of 1770, and several more in London to join the medieval foundations of Bart's and St. Thomas's and the specialist Bethlem and Bridewell.
In view of this heightened activity it is hardly surprising that medicine should be given credit for the decline in mortality (e.g. Griffith 1926).
But it has been long established (Habakkuk 1953, McKeown and Brown 1955) that medical knowledge of the time could not cure any important cause of death, could prevent only smallpox, and with a few exceptions remained impotent until the present century.
Hospitals and midwifery
McKeown and his colleagues blame the general hospitals for killing their patients more often than relieving them, at least until the 1875 public Health Act.
Fever patients were admitted to general wards.
Hospital nutrition was bad and standards of cleanliness worse: dysentery and louse-borne typhus, known as ‘hospital fever’ were common.
Nursing, without the continent's religious orders, was uninformed by training, devotion, or honesty until Miss Nightingale's reforms in the 1860s.
But in the eighteenth century McKeown's criticisms may be on less secure ground (Woodward 1974).
Some hospital charters forbade the admission of infectious diseases such as smallpox.
From the eighteenth to the early nineteenth century between 4 and 14 per cent of in-patients died in hospital (Cherry 1972, Woodward 1974).
90 per cent are listed as cured or relieved at York from 1740 to 1783 (Sigsworth 1972).
But these in-patients were a selected group; the treatment may not have brought the ‘relief’.
Institutional delivery of babies was  uncommon before the middle of the eighteenth century.
In 1749 the London Lying-in Hospital was founded, followed by Queen Charlotte's and three others in 1765, in response to poor standards of midwifery.
But hospitals without hygiene helped the transmission of puerperal sepsis (childbed fever, a streptococcal infection).
None the less, the London Bills of Mortality suggest that maternal mortality halved from 1700 to 1800.
Hospitals in the eighteenth century may not have made national mortality worse (Cherry 1980), but they were too few to improve the national health.
Surgery and drugs
Effective treatment was restricted to general nursing, setting simple fractures, lancing  abscesses (common in an age of poor nutrition and hygiene), amputations, trephining, operations for cataract, and ‘cutting for stone’(lithotomy), apparently more common in an age of high cheese consumption and stoneground bread.
Post-operative death varied from 4 to 14 per cent by the mid-nineteenth century.
But later in the nineteenth century operative technique greatly improved and the advent of anaesthesia (1800 for nitrous oxide, 1848 for chloroform) made surgeons more adventurous.
Despite brilliantly successful surgery, the patient still died.
Without antiseptic practice, post-operative mortality at University College Hospital in the 1870s was 35–50 per cent in general and 90 per cent fur some operations.
The eighteenth century pharmacopoeia was none too impressive.
Laudanum (opiates) was a painkiller then, as now, expensive and addictive.
Mercury, itself poisonous, was used against syphilis, which was never demographically important.
Digitalis, effective against congestive heart failure (dropsy) was no use against the major coronary diseases.
Cinchona, the rare and valuable ‘Jesuit bark’ from South America, is the source of quinine and a specific against malaria.
But malaria was only regionally important and the drug too expensive to give in sufficient dosage (McKeown and Brown 1955, Hobhouse 1985).
The attack on smallpox
Smallpox may be an exception.
Crude inoculation (immunization) was introduced against smallpox in the eighteenth century.
It may have helped reduce eighteenth-century mortality (see Chapter 1).
It can be effective where it does not kill the patient.
Material from a smallpox pustule is introduced into the body of the person to be protected, by a light scratch, The patient should then receive a light dose of smallpox, with only a dozen or so pox, and lifetime active immunity.
The process works (sometimes) because the immune system can cope with small numbers of viruses, although this was unknown in immunological terms at the time.
Lady Mary Wortley Montagu, wife of the Ambassador to the Sublime Porte in Constantinople, was so impressed with the results of this folk practice by the Sultan's Greek subjects that she had her children treated and introduced fashionable society to its advantages in 1721; as much for the protection of complexion as for the preservation of health.
But in the hands of the medical profession the original crude jab became elaborated into a less safe deep incision followed by expensive treatments (Razzell 1977).
This confined the treatment to the well-off and to demographic insignificance until in the 1760s the Sutton family popularized their cheaper method which was closer to the original simple practice and devoid of expensive medical encrustations.
The new method may have been widely used, especially is country areas, by doctors and by itinerant inoculators.
Smallpox mortality is claimed to have fallen correspondingly (Razzell 1977), a view support by the Bills of Mortality in provincial towns.
In the latter half of the eighteenth century parish vestries (the local ‘council’) were often prepared to order general inoculations when smallpox was abroad locally, and the Poor Law Overseers paid the fees of paupers (up to a third of the village).
This eighteenth-century controversy may be insoluble as it is so difficult to determine the extent of rural inoculation, and the proportion of deaths attributable to smallpox in country areas.
Edward Jenner was himself a keen inoculator, but he was impressed by the apparently safer prophylactic effects of the mild natural disease of cowpox; a zoonosis often caught by milkmaids which apparently protected them from smallpox, as he demonstrated in his paper of 1798.
The practice of vaccination spread fast at home and abroad.
It was made compulsory in Denmark, Norway, and Sweden around 1810.
We now know that the viruses of vaccinia (cowpox) and of variola (smallpox) are antigenically similar.
A fierce and often unscrupulous controversy arose over the relative merits of vaccination and inoculation, which persists to this day in demography journals.
The vaccinators triumphed, securing the legal prohibition of inoculation.
Vaccination was made free in 1840, made compulsory in 1851, and enforced in 1867.
By then the disease was no longer a major public health menace.
Similar measures (including inoculation) had also controlled smallpox in Western Europe and the USA (Mercer 1985).
The last epidemic in Britain was in 1900–5; 1935 was the first year without a smallpox death, by 1941 there were no cases.
Major smallpox may have become extinct  in Britain since the First World War, with all subsequent cases imported (Gale 1959).
Outside a few laboratories smallpox became extinct worldwide in 1977.
2.5 Agriculture, Diet, and the Death-Rate
Demographic and economic consequences of agricultural reform
Agriculture coped with the demands of the growing population with almost no recourse to imports.
Food output increased considerably from 1700 to 1800, accommodating the needs of a 60 per cent rise in population with, in most years, a small surplus to export.
Despite the increase in numbers ‘there was no very great change in the overall relationship between the consumption of cereals and the total population over the 18th century as a whole’(Deane and Coale 1969).
By the early nineteenth century the Malthusian links between rising population and fertility and rising domestic wages, food output, and prices were broken by industrialization (figure 2.1).
For the first time population could increase with a declining grain price and a less than commensurate food output, subsisting on grain imports paid for by manufactures; ‘It is one of the most striking ironies of intellectual history that Malthus should have fashioned his analysis just at the time when it was about to cease to be applicable to the country in which he lived’(Wrigley 1986).
The surpluses increasingly generated by eighteenth century agriculture were turned by the cash economy into increased consumption of non-agricultural produce, providing savings and capital for non-agricultural demand and investment, as Malthus observed (1836).
The good harvests of 1715–45 cheapened the costs of some of industry's raw materials and generated a surplus for consumption of new cheap textiles and cheap gin made from the grain surplus.
Later on, the relation was reversed — the growing town population increased the demand for grain and raised its price, putting a premium on agricultural efficiency and enclosure (Deane 1979).
Price as well as quality of food is of the essence; food was by far the biggest item in the wage-labourer's budget.
In 1790–6, labourers spent 40 per cent of their income on bread and flour, 20 per cent on animal products (e.g. bacon), 9 per cent on sugar, tea, beer — 70 per cent in all (Gilboy 1936).
Improved agriculture and enlightened landowners
Eighteenth-century agricultural improvement has been credited with beginning the downturn of the death-rate (McKeown and Record 1972, McKeown 1976b).
But the primacy given to it depends on the belief that other explanations (including smallpox) have been excluded.
It is difficult to show that individual diet, and consequently survival, had improved (Rotberg and Rabb 1985).
It is true that new crops — turnips, parsnips, swedes, mangelwurzels — diversified the landscape and the food supply, especially by keeping herds alive over winter.
The potato was not widely used directly as food in England until the nineteenth century, although its capacity to sustain a family on very little land has been credited with the rapid Irish population growth from the late eighteenth century (Connell 1950a, Langer 1975).
Clovers helped remove the fallow stage from cultivation, effectively bringing more land into use — as in the ‘Norfolk Four Course’(wheat, turnips, barley, and clover) advocated but not invented by Viscount Townshend of Raynham (‘Turnip’ Townshend, 1674–1738).
For the first time a technically competent farming literature was available to help the better educated farmers and landowners (e.g. Townshend and Thomas Coke of Holkham (1691–1755) who could drive their tenants to improve (Mathias 1972a).
It is not easy to evaluate the spread of improvement.
Turnips and clover, for example, may not have been really widely used until the nineteenth century, and the diffusion of knowledge across the countryside may not have exceeded a mile a year from the points of origin (Deane 1979).
Crop rotations and equipment could not be exploited in some areas until enclosure removed the remaining open fields and increased farm size.
Enclosure
Only half the arable land was still open fields in 1700.
By 1800 most had gone.
Enclosure turned small or fragmented holdings and commons suitable for little more than subsistence agriculture into consolidated holdings able to support the new farming and generate capital for improvement.
Half a million acres came newly under the plough between 1761 and 1792, one million more during the Napoleonic Wars.
High grain prices encouraged the pace in the later eighteenth century.
The holdings and rights of freeholders and tenants to common land were bought out after the survey and valuation of often hundreds of parcels of land and rearrangement of drainage, hedges, and roads (Rackham 1985), which increased demand for labour.
Private Acts of Parliament could be used to overcome opposition; 472 between 1770 and 1779, 2000 in all (Darby 1973b), although it seems that more land was enclosed in the seventeenth than in the eighteenth century.
The Enclosure Acts gave smaller farmers, freeholders, and copyholders consolidated allocations in return for lost common rights although they  would have to pay for hedging and ditching.
Many landless cottage labourers (cottagers) and rural squatters with only customary use of common land received compensation to buy small plots (Chambers 1953).
The diet of the rural poor may have deteriorated towards the end of the century because they had lost their pasturage and could no longer collect firewood or game from the common land (Taylor 1975).
In the first half of the nineteenth century there was a ‘second agricultural revolution’.
As a result, urban populations, economically emancipated from the land, could still enjoy cheap bread.
Potatoes became an important part of the national diet, grown on 40,000 acres in 1795 and 142,000 in 1851 (Salaman 1989).
Machinery was not widely introduced until the latter half of the century, as wages rose due to migration to the towns.
The new primacy of the urban interest was marked by the repeal of the Corn Laws in 1846, which reduced grain and bread prices.
Tinned meat from 1860, cheap refrigerated meat from New Zealand from 1880, fish, packed in ice and moved rapidly by the new railway system; all became a cheap commonplace of working-class diet.
Milk brought by rail from the country to the town enabled the atrociously insanitary urban dairies to be abolished from the 1870s.
Milk remained a health hazard until compulsory pasteurization eliminated tubercle and diphtheria bacilli from it in 1922 (McKeown and Turner 1975, Beaver 1973) with beneficial effects on infant mortality.
The common and often spectacular adulteration of food was not seriously tackled until the Food and Drugs Act 1875 (Burnett 1968).
The national diet
New techniques and organization of land may have led to a better diet.
But there is only scattered evidence that it did until the later nineteenth century.
The stature of children and age at menarche are fairly sensitive indicators of nutritional status and health (Tanner 1962, 1978).
The records of the Marine Society, a charity which prepared poor London boys for a career at sea, show a slight increase in stature up to about 1790, a decline until about 1800 then a sharp increase up to 1830, correlated with London labourers' wages.
These increases are attributed (Floud, Wachter, and Gregory 1990) to improvements in the diet of the London poor.
Later in the century there is a further trend towards greater height and weight among both adults and children (Fogel et al .
1983).
Age at menarche, almost the only indicator of sexual maturity of which we have any historical evidence, is also sensitive to nutritional  status.
It declines progressively from the high average of about 15 years in the middle of the century to about 12.5 today.
Some of these high estimates for the nineteenth century, which put mean age at menarche higher than in previous periods, have been questioned (Bullough 1981).
But most of the decline dates from the end of the nineteenth century.
None the less, even at the time of the Boer War (1899–1902) about 38 per cent of recruits to the army were rejected as undersized or unfit, especially that growing proportion that lived in cities (Oddy 1982).
This disturbing discovery encouraged further state intervention in child health and welfare in the following decade including school meals from 1906, medical inspection for schoolchildren from 1907 and the compulsory notification of tuberculosis in 1912.
Tea and sugar give clues about the ability of the working class to afford a reasonable diet.
Tea is nutritionally worthless, however vital.
Like sugar it was originally rare.
But as both are imports, there are data on them.
By the early nineteenth century both were becoming staples of working-class diet.
Sugar consumption fell from 31 lbs/head in 1805 to 18 in 1833, but it started to rise from 1845 as did consumption of tea.
By the 1750s, wheaten bread (as opposed to rye or barley) became almost universal, even in provincial towns.
By 1800 most of it was white bread, bought from a baker, possible because coal drove the cost of fuel down.
The food thesis is attractive but unproven.
Death directly by starvation in famines seems to have been rare in Britain, less so abroad (Walter and Schofield 1989).
Improved diet would have reduced death rates primarily because well fed people are better able to fight off infectious disease.
There is strong evidence that ill-fed people in modern industrial and Third World societies are more likely to develop some infections and succumb to them, primarily because their immune systems are less effective: the so-called ‘synergistic’ effect (Scrimshaw et al .
1968, Taylor 1983).
In the earlier part of the period much mortality seems to have been exogenous, with little respect for social or nutritional status (Livi Bacci 1983), and crisis mortality is not well correlated with grain prices (Wrigley and Schofield 1981).
However the departure of plague in the seventeenth century, the reduction of infant mortality, and the rise of tuberculosis, which is more sensitive to nutritional status, may have changed the situation.
By the early nineteenth century, and probably much earlier, substantial social and geographical gradients in infectious-disease mortality exist which are plausibly associated with nutritional status as well as environmental factors (Rotberg and Rabb 1985).
2.6 Major Causes of Death and their Control in the Nineteenth Century
The causes of death
Tuberculosis (TB) retained its dominance well into the century (table 2.3).
Because it affects primarily younger people, and in particular young women, it has a powerful effect upon expectation of life.
In the eighteenth century the rise of towns enabled it to increase.
The Bills of Mortality suggest that TB was at its maximum between 1780 and 1830; 30 per cent of deaths in London in 1796 (Brownlee 1916).
Nationwide, 17.6 per cent of all deaths in 1839 were due to TB according to the Registrar-General's first national analysis of causes of death.
From about 1850, it begins to decline at an increasing rate up to the present day (figure 2.5).
22 per cent of the overall decline in mortality from 1848 to 1971 have been attributed to the decline of TB in all its forms (McKeown 1976), and 45 per cent to 1900.
This decline — like that of most infectious diseases — took place despite the absence of any generally available cure or prevention.
Apart from smallpox it was the first major infectious disease to decline.
Koch identified the tubercle bacillus only in 1882.
Effective cure with drugs   (para-amino-salicylic acid or PAS, and isoniazid) and antibiotics (streptomycin) was not available until 1947.
Immunization by BCG (Bacille Calmette Guerin) was not introduced until the 1950s.
Pasteurization of milk, to destroy the bacteria of bovine TB and other parasites, was not introduced until 1922.
Sanatoria, popular by the end of the century, were only for the better-off, apart from some charitable foundations, and in any case had little effect.
There is no evidence of diminishing case-fatality rates among untreated patients, and therefore no evidence for the diminution of virulence favoured by some historians (see McKeown and Record 1963).
Instead the explanation may lie with better housing, which reduced the chance of transmission of the bacterium, and improved diet (discussed above).
A reduction in shared bathrooms and kitchens reduces the contact between individuals and families.
Adequate nutrition is particularly important because TB was an almost universal childhood infection and can be unusually long-lived.
Depending on nutritional status, the disease will be eliminated or reduced to dormancy by the body's defences, to erupt again if the antibody level falls.
As late as 1952, 56 per cent of Scottish 13-year-olds reacted positively to the tuberculin (Mantoux or Heaf) test, which indicates exposure.
Other important contributions to the fall in mortality up to 1900 arise from the decline in typhoid and typhus fevers (22 per cent of the total mortality reduction), scarlet fever (19 per cent), diarrhoea, dysentery, and cholera (8 per cent )(McKeown and Record 1962).
Diseases of infancy and childhood and their decline
Even at the end of the nineteenth century infant mortality still accounted for almost one death in five, Almost half of these were from diarrhoea and related gastro-enteric infections: dysentery, ‘colic’, ‘convulsions’, ‘griping in the guts’, infantile summer diarrhoea.
Most deaths from diarrhoea occurred in the summer and early autumn, and this ‘summer diarrhoea of infants’ was one of the most notorious child health problems in London (Finlay 1981) and even worse in the great Northern cities.
It rose to a peak in the latter years of the nineteenth century, the last survivor of ‘crisis’ mortality.
In the last great outbreak in 1911, 32,000 infants died of diarrhoea and the infant mortality rate climbed to 130.
The problem was particularly severe because of the inevitable fly-born contamination of food and babies' bottles in cities, poor households where food hygiene was difficult, and streets that received the dung from 250,000 horse-drawn vehicles.
Severe seasonal crises went with the advent of motor transport, but the pasteurization  of cows' milk fed to infants and the wider availability of clean drinking water were probably the main causes.
In 1921, the ratio in summer of diarrhoeal deaths to all infant deaths was 1: 6; in 1931 it was 1: 10 and later fell much further.
The epidemics of the hot, dry summers of the 1890s probably obscured an underlying decline in infant mortality which may already have begun by the 1880s.
The improvement of women's education, particularly marked in the late nineteenth century, and the decline in the birth rate from the 1870s emerge as two of the most prominent factors correlated with the decline of infant mortality in the 1900s, while the rise in real wages had a surprisingly weak effect (Woods et al .
1989).
Diphtheria, scarlet fever, measles, and whooping cough attacked older children just when they seemed clear of the terrible hazards of infancy.
Diphtheria is a congestive throat infection which kills by the effect of bacterial toxins upon the heart.
10,000 children died of it in 1859, and from the 1870s until the turn of the century there was little downward trend in deaths.
Accordingly it became a relatively more important cause of death than scarlet fever, which declined fast from the 1860s (the two diseases have some symptoms in common and were not separated in death certification until 1858).
As an infection carried on the air and in milk, diphtheria was not much affected by changes in living standards.
It flourished in the playgrounds of the newly schooled child population and was disseminated in unsterilized milk.
It is one of the few diseases which medicine could attack before the Second World War, by passive immunization.
Von Behring's anti-toxin was widely available after 1896, though incidence remained almost unchanged until the 1940s.
Scarlet fever is one of the few examples of a disease whose virulence has obviously declined.
From the 1830s it became the single most important cause of the death of children, peaking at four deaths per thousand children per year aged under 15; at worst, 30,000 deaths a year in the 1860s.
Then deaths declined steeply as mysteriously as they had increased.
Other streptococcal diseases such as erysipelas and puerperal sepsis (childbed fever) declined less dramatically.
In the nineteenth century, measles infected nearly all children, especially between 6 months and 2 years of age, and killed about 1 in 2,000 every year.
It is pre-eminently a ‘crowd disease’(Greenwood 1935): fast moving, short-lived, needing a large population size to remain endemic.
Its nineteenth-century decline in mortality went straight down from a peak in the 1860s to oblivion in the mid-1950s, although notifications remain high to the present day.
Measles is particularly sensitive to the nutritional status of the victim; which helps explain its decline and also its marked social class gradient; in 1931 mortality from    measles in social class V was 20 times that of children in social class I. The fall in family size probably helped the decline of this and other childhood diseases; fewer younger and vulnerable siblings would be infected at home by playground infections brought home by older children (Gale 1959).
Housing
The decline of tuberculosis mirrors the amelioration of living conditions in the later part of the nineteenth century, as its increase in the late eighteenth reflects the increasing proportion of the population moving to the insanitary towns.
Later in the century, income figures, data on the number of persons per room from the census after 1891 (Hole and Pountney 1971; Benjamin 1964), public health reports, and nutritional surveys (Burnett 1968, Drummond and Wilbraham 1938) all show trends which are ecologically unfavourable to the spread of tuberculosis, especially trends in overcrowding (figure 2.6).
National data on the proportion of households sharing kitchens, bathrooms, and other amenities have only been available from the census since 1951.
Overcrowding was not just an urban problem.
We should not glamorize the overcrowded, insanitary rural cottages — scarcely adequate as holiday homes for a two-child family now — in which parents tried to raise five children, which were so strikingly condemned in Chadwick's 1842 Report on the Sanitary Condition of the Labouring Classes (Flinn 1965).
In the towns, rapid urban growth (table 2.1) made it very difficult to build houses fast enough.
Between 1831 and 1841 Glasgow's population increased by 37 per cent, Manchester and Salford by 47 per cent from 1821 to 1831, West Bromwich's by 60 per cent , and Bradford by 78 per cent .
In the decade from 1831, the average occupancy per house in Liverpool increased from 6.4 to 6.9.
Some local areas were much worse.
Church Lane in St. Giles, London (a notorious ‘rookery’) had 655 people in its 27 houses, 4.9 to a room) according to a London Statistical Society survey of 1841, and in 1847, 1,095 or 8.1 to a room — an increase due partly to comprehensive redevelopment nearby, partly to renewed Irish immigration after the Famine of 1845–6.
Urban rents took 10–20 per cent of wages, compared to 5 per cent or so in the countryside.
Back-to-back houses usually in a terrace, with a front door on each side, separated by a transverse party wall, could be particularly cramped, having no back yard or through ventilation, although later versions, still to be seen in Leeds and other northern cities, were no worse in this respect than modern flats.
They were prohibited in the Metropolis by the series of Building Acts which had  improved the capital's housing since 1667, notably that of 1774.
But in Nottingham in 1840 7,000–8,000 of the 11,000 houses were of this type (see Burnett 1986).
Many people, especially the Irish immigrants in Liverpool, lived in damp and ill-ventilated cellars (22 per cent in 1841) despite the attempts of local authorities to fill cellars with sand and gravel (Ferguson 1964).
Evidence for the easing of overcrowding comes late in the century.
By that time, higher real wages allowed ordinary families to rent more spacious and better equipped accommodation (95 per cent of households rented until after the First World War) and encouraged builders to improve quality.
Almost all houses were equipped with their own privy or WC by the end of the century (outside was considered more hygienic), most from the medium size up had an internal bathroom.
In terms of these amenities, and of rooms per household, Britain was ahead of other European countries.
Welfare and the Poor Law
For many poorer people food and housing depended on welfare.
It had always been so (see Chapter 1).
Welfare was local, paid out of parish rates.
One system established by the magistrates of Speenhamland (Berkshire) in 1795 was widely copied until its abolition by the New Poor Law of 1834.
According to Malthus, its fiercest critic, it depressed the wages paid by farmers by compensating wherever they fell below a scale set by the bread price and the number of the labourer's children in relation to his wage (Marshall 1968).
It ensured that the unemployed earned as much as the labourer, reminiscent of the ‘poverty trap’ problems of today's welfare.
Malthus claimed that its introduction encouraged improvident early marriage, although evidence for this is difficult to find (Digby 1983).
The transfer payments of the Poor Law — finally abolished and transformed into the ‘Welfare State’ in 1948 — were considered by many to be an intolerable burden.
The indoor relief (workhouses) established by the New Poor Law was widely criticized, but the system as a whole provided a safety net for the poorest, and thereby helped mortality trends.
From 1834 little change was made in the welfare system until the first decade of the twentieth century.
Universal state old age pensions were introduced by the Old Age Pensions Act of 1908, workmen's insurance in 1911.
Their contribution to further falls in mortality is difficult to estimate, but income and mortality is considered further in Chapter 8.
Civil engineering, vital statistics, and local government
Intervention by national and local government to reduce environmental risks had dramatic effects on urban death-rates.
Typhoid and  cholera were eliminated in the nineteenth century by engineering, epidemiology, and bureaucracy.
Typhoid and cholera are different bacterial diseases which share a similar ecology; they are transmitted in food or water contaminated by the faeces of an infected person.
They were ideally suited to town life before the spread of hygienic knowledge, readily distributed by polluted rivers, reservoirs, and wells.
Their control merely requires the segregation of sewage from drinking water.
Ordinances had been passed since medieval times to remove rubbish from streets (Howe 1972) with further Nuisances Removals Acts in 1855, 1860, 1863.
But although closed sewers had been dug in larger towns in the eighteenth and earlier centuries to replace open ditches, because of their square cross-section and inadequate fall and water supply they endlessly blocked up.
By leaking, like cesspits, they contaminated ground water.
The water closet had been patented in 1775 but the widespread use of water carriage for sewage needed abundant water supply and good leakproof glazed drains.
In 1850 most nightsoil was still collected — expensively and with much fallout — by cart.
Drinking water was still taken from contaminated sources, so although pipes were laid to many individual houses and street standpipes by the 1840s, the water was impure (see Cruickshank and Burton 1990).
Water companies were required to stop taking their water from tidal (most polluted) sources only by the Metropolis Water Act of 1852.
The miasmatic theory of disease — favoured into the 1850s even by such luminaries as William Farr — hindered realization that the agents of disease might be present in water which looked quite drinkable.
The ‘Great Stink’ of 1858 (from untreated sewage in the Thames), which drove Parliament from the House of Commons, and the death of Albert, Prince Consort, in 1861 from typhoid contracted from Windsor's contaminated water, underlined the need for reform.
Asiatic cholera, a new enteric disease, provided the spur to action.
Endemic in Bengal for centuries, the first of four epidemics to attack the British population first appeared in Sunderland in 1831 (the others were in 1848, 1853, and 1866).
It spread fast in the contaminated water of the great cities, killing in all four epidemics about 100,000 people.
In 1854, Dr John Snow and William Farr assembled demographic statistics on the distribution of deaths from the new vital registration system to demonstrate beyond doubt, and contrary to received opinion, that the disease was water-borne and came from specific sources (Lewes 1983).
The disease had been efficiently delivered to the homes of victims through the water pipes of the Southwark and Vauxhall Water Company from the contaminated tidal Thames.
Customers of the rival Lambeth Water Company, which took its water from the upriver source of  Thames Ditton and which served houses in the same London streets, suffered only a sixth of their risk.
In London, and later in all major cities, new impervious trunk sewers were built on scientific principles to replace or intercept the old ducts and to ensure that all sewage was discharged downstream.
Cesspits were filled in and nightsoil collection ended.
Eventually sewage was treated before discharge in two enormous sites on each side of the lower Thames.
The London works, capable of disposing of 400 million gallons of sewage a day and still in use, were finished in 1865 under the direction of Bazalgette (Daley and Benjamin 1964).
By 1866, London's drinking water was taken from upstream or tributary sources, and later treated before distribution.
Chlorine was first used to treat water after the Maidstone epidemic of 1897–8 when hop-pickers contaminated spring water.
The incidence of typhoid collapsed.
Cholera never returned after 1866.
None of the subsequent pandemics, such as that of 1893, became established in Britain.
Precise information on deaths dates only from 1868; before that typhoid was confused with the symptomatically similar but epidemiologically different disease, louse-borne typhus.
Typhoid is more difficult to eliminate because symptomless ‘carriers’ can transmit the disease to others, although most of the few cases today are imported (see Chapter 7).
This was action by national government backed up by national statistical systems.
At the local level, sanitary reform was at first hampered by the chaotic diversity of different local bodies (Flinn 1969).
In response, the Public Health Act 1872 set up new local sanitary authorities to police legislation.
Local authorities assumed something like their present form after the Local Government Act of 1888, which gave them greater powers to raise funds and pass by-laws.
Local Medical Officers of Health had been established by the Public Health Act of 1845, but the Act was ineffectual and Chadwick's ideal was not approached until the 1875 Act.
The officers' activities were spurred by the contemporary knowledge of mortality differences between town and country, different towns, and different areas in the same town (Woods 1982).
Their role in the reduction of mortality was — and still is — to remove environmental risks to health, to encourage the building of proper sewage and water systems following the Metropolitan example, to enforce housing standards, to ensure that rubbish is collected, to organize health inspections of schoolchildren.
Such local action may be a neglected dimension in the late nineteenth century/early twentieth century mortality reduction (Szreter 1986).
Most of the discussions above have centred upon specific factors particular to Britain.
By the end of the nineteenth century, British  mortality was still more favourable than in most other Western countries, despite the very high level of urbanization.
Around 1900 expectation of life at birth for males was about 48 in Britain compared with 45 in Germany and France, 44 in Italy and 40 in Spain.
However Denmark and the Netherlands had both exceeded 51.
In Britain, babies were more likely to be breast fed, public health and sanitation were more advanced.
Real wages were still higher than in most other countries except the USA and Germany.
Nevertheless mortality had begun to fall in most Western countries only slightly later than in Britain, almost irrespective of the degree to which they had industrialized their economies, and with a similar diminution of particular causes of death (Preston 1975, 1976).
Common factors were widespread education, rising incomes, sanitary reform, and effective government.
Innovations such as vaccination could spread over Western Europe and North America within a decade; later the acceptance of the germ theory of disease spread even faster.
A specifically formulated general explanation which meets all these circumstances is still awaited.
This is true to an even greater extent of the fertility decline to which we now turn.
2.7 The Decline of Fertility
The pattern of fertility decline
Starting in the 1870s, average family size had declined from five or six children to today's two-child family within 60 years (figure 2.7).
This transition was without precedent in history.
It is almost certainly permanent.
It has had the most profound effects on family life, on population growth, age structure, and the economy.
By the late eighteenth century, in a lagged response to higher and more reliable wages from industrialization and more prosperous farming, median age at marriage had fallen to 23 for men, 22 for women, compared to 26 and 24 in the century before.
Only about 5 per cent of those generations never married — a very low proportion for Europe (see Chapter 1).
In response to the marriage boom, fertility reached an all-time high at around 1800.
It seemed that the industrial revolution was creating its own work-force.
A trend to higher fertility is a temporary response to the earlier phases of modernization in recent developing societies too, although for different reasons.
By the 1840s marriage returned to its former level and fertility consequently fell (figure 2.7).
The post-war fall in food prices, together with the larger cohorts seeking work, depressed agricultural wages, but new average real wage levels were dominated by industrial employment and continued to rise   until the end of the nineteenth century.
Throughout the nineteenth century fertility in Britain remained high.
By the marriage cohorts of 1870 a decline in national marital fertility became apparent, unrelated to any major retreat from marriage.
Unlike previous declines, it continued without interruption for sixty years, eventually embracing all sections of the population, until a completely new pattern of low fertility had been established by the 1930s, essentially that which we have today.
That is in itself a problem, for nothing in the arguments to explain the decline suggests why the fall in fertility should stop at about a two-child family, rather than‘overshoot’ and decline still further (Chapter 4).
Explaining fertility decline
This transformation from large to small family size, and the mortality decline which (in Britain) preceded it, is known as the demographic transition.
All industrial countries have experienced such a transition from high to low birth- and death-rates in the last century, usually (except in France) with an intervening period of moderate population growth when the death-rate falls before the birth-rate.
No simple  agreed explanation is available; it remains a major problem in demography
Proximate determinants
It is relatively easy to identify the immediate or ‘proximate’ reasons for the decline in births (Bongaarts and Potter 1983, see Chapter 4).
Women can only have fewer babies if they have intercourse less often (because of delays in marriage or less frequent sex); begin practising contraception or suffer from declining physiological ability to conceive (‘fecundity’); or prevent the gestation of conceptions by abortion.
As to these immediate causes, the choice is straightforward.
Contemporary observers were quite clear that the modest decline in marriage around 1870 could not account for more than 10 per cent of the fall in births (Yule 1906).
Setting aside fanciful notions that Western fecundity declined through a surfeit of meat eating or a failure of ‘racial vitality’(Soloway 1982), there is no evidence for declining fecundity.
The proximate cause of the decline in births was the adoption of contraceptive practices by the majority of married women and to a lesser extent (illegal) abortion.
This is amply confirmed by analysis of the 1911 census (Matras 1965), by the enquiries on contraception begun by Dr. E. Lewis-Faning in connection with the 1946 Family Census (Lewis-Faning 1949, Glass and Grebenik 1954), and by all other surveys since, as well as by contemporary accounts (Fryer 1965).
But families do not just decide, for the first time in history, to adopt contraception out of the blue.
Limitation of family size will not happen unless it can be done without trespassing on conscience and unless practicable means are available; and unless it is perceived to be in the families' interests to do so (Coale 1973).
Why and how this happened are the more difficult questions.
Models of the demographic transition
The ‘demographic transition model’, first put forward sixty years ago (Thompson 1929, Davis 1945, Notestein 1945) is an attempt to answer these questions.
At its least ambitious it is a description of a sequence of events in mortality and fertility decline in modernizing societies.
At its most comprehensive it proposes an explanation of historical population trends and a forecast of trends in the developing world.
Fertility falls because of the effects of industrialization and urbanization of the economy, and the concomitant modernization of society, upon the economic viability of large families and their support by traditional institutions and attitudes.
In recent years micro-economic interpretations of fertility transition have been dominant (e.g. the ‘New Home  Economics’ of Becker (1981) and others, which emphasize the increased costs of children, and the competition of demand for children with demand for consumer goods previously not available.
On this view high fertility is economically irrational, or at least is made so by the new circumstances.
Rational high fertility?
Under some circumstances high fertility may be rational (Caldwell 1982).
In rural, agricultural society children cost little.
They may not stop the wife's contribution to the household economy.
They can bring in returns of their own.
Large family size can help insure against risk of natural disaster or widowhood (Cain 1981; Smith 1986).
If women enjoy low status they may need sons for prestige as well as protection.
There may be no alternative institutional support.
Modernization eventually takes away children's ability to earn and turns them into a cost as they need education to earn as adults.
Only irrational ‘props’ to high fertility then remain; religion, tradition, and ignorance.
A more educated, literate, and rational society weakens their effect; contraceptive knowledge and practice spreads first in the more educated classes, then to poorer people.
This outline does not fit the European experience very well, let alone that of developing countries.
Even in developing societies children may bring little return or ‘insurance’(Vlassoff 1982, Nugent 1985).
Pre-transitional European fertility cannot be explained by the utility of children to their parents.
Where a familial, labour-intensive mode of production is predominant, among peasant farmers and cottage industries, children might provide useful labour.
This is less obviously true for wage-labourers; not at all for the middle class.
In practice in Europe, the returns seem to have been slight, of a kind which could be met by resident servants (Smith 1981), and did not include automatic old-age support.
With high mortality rates even a large family size only just replaces the parents.
The decline of infant mortality makes it easier to accept the idea of smaller family size.
But clear declines in infant mortality in Britain and in Belgium only become apparent some time after the beginning of the fertility decline, and in France mortality and fertility declines begin at about the same time.
Overall there is no correlation; both seem to he consequences of a more general modernization (van de Walle 1986).
International comparisons
European comparisons do not support the primacy of industrialization and urbanization in the fall in fertility.
In France, fertility decline  within marriage began a century earlier, in the 1780s (Wrigley 1983), without industrialization on the scale of Britain's. furthermore the decline began in many rural areas at the same time as in towns.
Migration and fertility decline are alternative escapes from population pressure in societies sufficiently developed for mortality to be declining (as in eighteenth century France) and can to some extent substitute for each other(Davis 1963, Friedlander 1983) to relieve population pressure (Woods and Smith 1983).
High fertility in Ireland (by modern standards) depended upon the institutionalized pattern of high emigration which prevented population rising from the 1840s until the 1950s.
But France had no industrial towns or suitable overseas colonies to migrate to (Grigg 1980).
Despite lower British fertility levels, migration out of England exceeded 100,000 per year by 1870; 3–5 per 1,000 population, representing up to a third of natural increase (Baines 1985).
Education and ideas
Almost all the countries of Europe except Spain and Ireland started their fertility decline just a few decades after Britain, before the First World War (Coale and Watkins 1986), even though some (e.g. Scandinavia) remained dependent on agriculture, fishing, forestry, and other primary production.
The modernization and the industrialization of a society need not be linked (Wrigley 1972).
The spread of literacy and of universal education may be more crucial in precipitating fertility decline, even if it cannot occur without the development of a more market oriented economy, less based upon family and subsistence production, and a perception of real income growth.
The spread of ideas and aspirations among literate and mobile populations is now given much more prominence as the key characteristic of ‘modernization’ which can explain fertility decline (Banks 1981, Cleland 1985, Cleland and Hobcraft 1987, Cleland and Wilson 1987).
New ideas and a ‘revolution of aspiration’ through higher real wages around the 1850s may have persuaded populations that economic advancement was possible away from subsistence agriculture into manufactures, services, and cash agriculture and that large families stood in the way of it (Lesthaeghe 1983).
Such changes must depend on the spread of education, which widened expectations and brought a more rational and secular mood inimical to the traditional values opposed to contraception.
People had become disinclined to believe without doubting what was revealed to them.
Such vague propositions need to rest on some testable factual basis.
In England, literacy and educational level increase, church attendance and Communion figures fall markedly at the end of the century (Woods 1987).
Throughout Europe, the rise of  divorce from negligible levels and the doubling of suicide rates from 1875 to 1914 suggest that fertility decline goes hand in hand with other radical changes in attitudes and behaviour.
Cultural and religious contrasts
Cultural and linguistic boundaries seem to be important in accounting for regional differences in the onset of fertility decline in different parts of Europe (Lesthaeghe 1983).
Areas which supported reformist social democrat political parties (as opposed to traditionalist Christian ones) had the fastest fertility decline.
Those with a local culture, under attack by central secular government and re-enforced by identification with the Church, preserved high fertility longest (Ireland and Brittany).
Catholic areas generally retained higher fertility than Protestant ones.
Secular attitudes go with a low proportion of population engaged in family farming and agriculture and a high proportion of women working (Lesthaeghe 1983).
Although difficult to measure, these differences cannot be accounted for simply by their accompanying economic differences.
The ideological dimension favoured in the explanation of Continental fertility differences seems less applicable to Britain, which lacked major foci of religious or social dissent.
The rural unrest of the early nineteenth century, the urban unrest of the Chartist and Gordon riots, the industrial turmoil of 1913 seem minor compared with the political and ideological upheavals and bloodshed abroad in the revolutions of 1789, 1815, 1830, 1848, and 1871.
Partly perhaps as a legacy of the risk-sharing of the old Poor Law (R. M. Smith 1986), British society preserved a more common interest and aspiration, without political parties embracing specific faiths or revolutionary ideals.
Particulars of the British fertility decline
In Britain, fertility fell surprisingly uniformly over the different regions of the country (Brass and Kabir 1978) with only a few exceptions (Woods 1987).
The prominent regional variations on the Continent revealed by the Princeton study (Coale and Watkins 1986) were minor in Britain, largely because regional linguistic and religious differences were minor.
However, fertility was higher where areas had high Irish or Roman Catholic populations (Teitelbaum 1984); some parts of Wales, the Highlands of Scotland, and Ulster had later fertility declines (the rest of Ireland much later) and Wales and Scotland also retained distinctive marriage patterns.
However, people of different social status began to limit their  families at quite different times.
These differentials give us important clues about the motivation and causes of the fertility decline.
Family limitation can first be inferred, primarily from the data of the 1911 census, among the families of professional men, particularly the clergy, doctors, and lawyers.
After them, families with heads in occupations which would now be placed in social class II: civil servants, school teachers, accountants, and the like (table 2.4).
In the middle classes, businessmen and farmers began to limit their families last (Innes 1938, Glass and Grebenik 1954, Matras 1965).
In general, working-class families reduced their families later than the middle class, but among them there were some important differences.
Skilled artisans adopted small family size before labourers; people in occupations where married women customarily worked (e.g. textiles) were also prompt to limit family size.
Agricultural labourers and coal miners, despite the skills and high income of the latter, were the last to follow, preserving high fertility into the twentieth century.
Many of the classic economic and social indicators of fertility decline had been present for a long time in nineteenth-century Britain.
In terms of the ‘demographic transition’ it is a great puzzle to explain why the response to them was so late in a relatively literate, urbanized, and industrialized society where returns from children do not seem to have been plausible for a long time (R. M. Smith 1981).
The closer the inspection, the less like a ‘classical’ demographic transition the British record seems to be, and the more difficult it is to attribute its timing directly to the development of an urban-industrial society (Woods 1987).
The question becomes; why did it take so long, not why did it happen at all.
Earlier — and still persuasive — analyses suggest that fertility decline was precipitated by a crisis in the ability to maintain rising standards of living (Banks 1954).
By mid-century living standards were perceptibly improving in all classes (Taylor 1975).
Large families began to threaten this prosperity because expectations of consumption kept rising while costs of children, especially for middle-class education, also rose.
The education which accompanied — or indeed may have preceded the new economic order made family planning thinkable (Banks 1981).
Marriage had always to be delayed until a suitable standing was achieved among middle-class and respectable poor (Macfarlane 1986).
The possibility of marrying on £300 a year and preserving any  pretensions to gentility was much discussed in agony columns (Banks 1954).
Servants were essential to run a large house and family, to free the middle-class mother for her prime role of managing her house, not    labouring in it, and caring for her children's development.
Domestic service was a commonplace of life before the First World War, even among the better paid working-class households.
In 1871, 1,106,000 men and women were employed as servants, the biggest single employment group outside agriculture.
Their wages increased by 30 per cent over the period through competition with industrial wages, to add to increases in costs from aspirations in housing, transport, social life, and many new conveniences, at a time when the general trend in prices was downwards (Deane and Cole 1969).
The ‘Great Depression’ after 1873 is alleged to have forced a decision between children and consumption, or at least between their number and the quality of their upbringing.
This ‘Depression’, unlike that of 1929, was a transient failure of the rate of growth.
An already antiquated industrial economy was overtaken by newer and more competitive ones in export and domestic markets, particularly by German manufactures (Barnett 1972, Wiener 1981).
Education and the costs of children
Children's costs rose because traditional middle-class occupations and positions in the services, professions, and the civil service (now much expanded) required some evidence of ability, following reforms of medical education (1858) and selection for the army (1871) and civil service (1870).
Education at a public school and university became essential (Banks 1981).
School fees were about £50-£100 per year; three years at Oxbridge might cost £600.
Girls' public schools began after 1870, and colleges after 1879.
Preparatory schools proliferated to replace tuition at home and to provide a middle-class alternative to the Board Schools following compulsory education in 1870.
Education was important too for the Pooters, if only to keep children out of manual work, even though the multitude of clerks' jobs were often less well paid — £70 per year — than much manual work.
Generally speaking, but with some local exceptions as in Sheffield, people in lower middle-class occupations (shopkeepers, clerks, junior teachers, and businessmen) limited their families later than the professional classes — possibly more because of the greater hold of traditional and religious influences on their behaviour than because their need was any the less (Woods and Smith 1983).
Early in the century the upbringing and education of working-class children was inadequate but cheap and their employment could at least contribute to their keep.
But the employment open to children and the age and the hours they could work were curtailed by legislation throughout the nineteenth century, beginning with the 1833 Factory Act.
Such legislation was a response to liberal humanitarianism but it  was made possible by the inevitable tendency of industrial processes to require more skilled labour.
Education was encouraged by the General Education Act of 1870; and made compulsory in 1876 up to age 10 and to age 14 according to the Act of 1880.
Until 1891 there was no state payment of the modest fees (see Smith 1931).
Whatever may have happened before, wealth flowed decisively from parents to children in all households in Britain from then on.
It is claimed that no society can maintain high fertility in the face of two generations of mass education, and that had been achieved in much of Western Europe by the end of the nineteenth century (Caldwell 1982).
But it would not be difficult to construct post hoc explanations for fertility decline occurring earlier in the nineteenth century as well.
2.8 Thinking the Unthinkable: The Rise of Birth Control
The spread of family planning in Britain is the story of the spread of information and the change of attitudes.
The supply of matériel was much less important.
Much of the fertility decline was achieved without mechanical contraception and initially without professional advice (see Chapter 4).
The beginnings of birth control
Written knowledge of contraception and abortion, some of it potentially effective like coitus interruptus and vaginal sponges and spermicides, goes back at least two  millennia (Himes 1936, McLaren 1990).
The account below of the struggle to establish the knowledge of family planning and its moral acceptability derives mostly from Fryer (1965), McLaren (1978), Leathard (1980) and Soloway (1982).
There is no good evidence for the widespread and effective use of any methods of family planning outside a few élite or urban groups until the end of the eighteenth century (Wilson 1984, Livi-Bacci 1986), and even then appliance methods do not become important for another century.
Robert Malthus brought the problems of excess fertility to the front of public discussion in 1798, yet he regarded contraception and abortion — which he alluded to only elliptically — as species of vice.
His argument that excess fertility impeded the solution of poverty inspired less squeamish reformers to resolve the problems of poverty he described with a new ‘neo-Malthusian’ prescription of contraception, not deferred marriage.
Utilitarians like Jeremy Bentham, uninhibited by Malthus's fear that contraception would remove a spur to industry, recommended contraceptive devices for the poor (sponges) in 1797.
The radical reformer Francis Place (1822, ed.
Himes 1967) was foremost in this movement (Fryer 1965).
His unsigned handbills addressed ‘to the married of both sexes’ both ‘in Genteel life’and ‘of the Working People’, distributed from London to Manchester in 1823, constitute the first known birth control publicity.
They specifically described and recommended the use of coitus interruptus and other contraceptive methods in marriage.
Richard Carlile followed with the first book on contraception in 1826 (Every Woman's Book ), followed by Moral Physiology by the American socialist Robert Dale Owen, 1830, and Dr Charles Knowlton's Fruits of Philosophy in New York in 1832.
The reformers accepted Malthus's analysis but not what they regarded as his negative attitude towards working-class behaviour.
But their radical and often anti-clerical politics hampered the spread of their own ideas among the respectable middle class, who distrusted practices so reeking of unwed vice and its connotations of disease.
The medical profession feared quasi-medical self-help.
Working-class leaders — influenced by Marxist hostility to Malthus — disliked these individualistic appeals, although many of the socialite socialists of the Fabian Society, notably the Webbs (1907) were enthusiasts.
Feminist opinion remained curiously muted, more concerned about defusing male sexuality and social dominance (Banks and Banks 1964).
In terms of the circulation of literature, birth control ideas made only modest progress before the 1870s.
But there is inferential evidence for the early adoption of family planning.
From the 1840s to 1900 the proportion of illegitimate births declined throughout Europe, from 7 per cent to less than 4 per cent, with no evidence that chastity outside marriage had become more popular.
Those most in need of contraception were finding out how to do it (Shorter, Knodel, and van de Walle 1971).
Indirect evidence from the 1911 census suggests that about 20 per cent of married women born in 1831–45 (and therefore marrying from the mid-1850s onwards) and over 40 per cent of those born 1861–70 were controlling their fertility (Matras 1965).
More direct if under-stated evidence comes from the retrospective questions posed in the first official birth control enquiry in 1946 (Lewis-Faning 1949)(see Chapter 5).
Heroes and heroines of the birth control movement
The public profile of birth control was changed radically by the celebrated Bradlaugh/Besant trial of 1877.
Charles Bradlaugh had founded the National Secular Society and edited the radical National Reformer , which often sought publicity for its views by courting prosecution.
In  1877 they republished Knowlton's old pamphlet The Fruits of Philosophy and were prosecuted for obscenity — in the end unsuccessfully.
But thanks to the newspapers, now circulating millions of copies in a population over 80 per cent literate, birth control was ‘brought onto every Englishman's breakfast table’(Banks and Banks 1954).
Sales of Knowlton's and other pamphlets soared, selling in millions over the next few years when previously they had sold in tens of thousands (Teitelbaum 1984).
Marie Stopes was the most prominent of the later campaigners.
She was untainted by the radical politics of earlier reformers and wisely avoided the tendentious macro-economics of the Neo-Malthusians.
An aggressive self-publicist, her inflamed prose brought her much notoriety.
Her books Married Love and Wise Parenthood , published with difficulty in 1917 and 1918, had sold a million copies by World War II.
Although alienating some support by her excessive regard for the physiologically and morally uplifting properties of semen, she emphasized the erotic, mystical, and fulfilling aspects of sexual relations and for these reasons rejected the use of coitus interruptus and the condom, recommending instead the use of the vaginal rubber cap and the quinine (spermicidal) pessary which she naïvely believed to be generally available in chemist's shops and which have never, in the event, proved to be very popular.
Under the banner of her Society for Constructive Birth Control and Racial Progress, Stopes's first ‘mothers’ clinic' in Holloway opened in 1921.
The Malthusian League's Women's Welfare Centre opened in Walworth later that year.
These pioneer clinics, which numbered only sixteen even by 1930, had treated only 21,000 women by that time.
The great majority of women limiting their families did not attend clinics at all but learnt from their neighbours, friends, or literature.
Family planning was in the air; newspapers and women's magazines were openly discussing family planning by the 1930s in a manner which would have been considered outrageous just ten years before.
Methods
Most of the fertility decline was achieved with coitus interruptus and the use of contraceptive vaginal sponges with spermicides such as vinegar, alum, or lemon, together with illegal abortion, whose practitioners were known to every working-class street.
Early condoms, intended to protect against disease, originally made of animal membranes and described by Johnson as a ‘cuirass against pleasure and a cobweb against infection’ were not really suitable.
Their large-scale use had to await  the vulcanization of rubber in the 1870s.
Once again, the First World War marked a turning point.
After a bitter debate between the rival proponents of cold baths and of condoms, and with the equivalent of two divisions out of action with venereal disease by 1917, the government eventually had to authorize the issue of condoms to the troops (Winter 1985).
The ‘Dutch Cap’ was not introduced by the German physician Mensinga until the 1880s and was only ever used by a minority of women, despite its promotion by clinics.
Manipulation of the womb by pessaries, stemmed and unstemmed, was a popular late nineteenth-century medical fad.
These intra-uterine devices would have had a contraceptive effect and were relied on by some women for that purpose.
Contraceptive IUDs as such were not developed until 1930 by Ohta in Japan, and were not made in a form suitable for mass use until after the Second World War.
Responses and reactions — religious, medical, and political
Individual need, private enterprise, advertising, and warfare effectively sidestepped medical and religious objections to contraceptive use.
Doctors and clergymen were among the earliest birth controllers in the middle class.
But their official bodies opposed family limitation.
The Church of England, which in its 1908 Lambeth Conference had condemned family planning, changed its mind when it was clear that the faithful were simply ignoring its precepts.
Although the 1920 Lambeth Conference took a hard line (not helped by Marie Stopes's literary, erotic, and religious excesses), the ‘safe period’ had been endorsed by 1913.
More and more Anglican bishops broke ranks before the next Lambeth Conference in 1930, which emphasized motives rather than method and gave grudging acceptance to ‘other methods’ than abstinence, noting that Church opposition was rooted in tradition not in Scripture.
The Church of Rome, on the other hand, hardened its attitude against birth control.
The refusal through contraception to reaffirm life during intercourse was — and is — regarded as a kind of Manichean heresy.
There was little conflict with congregations because, in England anyway, Catholic congregations (mostly poor) started to adopt birth control considerably later than others (Teitelbaum 1984).
The rhythm method had been approved by Catholic clergymen on the Birth-rate Commission as early as 1913 and in the 1930 Encyclical Casti Connubii .
But the general hostility of the Roman Catholic Church to all other forms of family limitation and abortion and contraceptive sterilization were reaffirmed in 1951, in the Encyclical Humanae Vitae in 1968, and  repeatedly by the present Pope John Paul II.
None the less, higher Roman Catholic fertility has now disappeared in Britain (Chapter 12) as in the USA and most Western countries.
Medical reaction to family planning
The medical profession ignored family planning in the first half of the century, only speaking to condemn it.
The British Medical Association refused to consider family planning at its meetings from 1912 to 1928, considering it to be tainted by association with vice and radicalism.
Dr Henry Allbut was struck off the register in 1880 for publishing The Wife's Handbook , popularizing contraceptive knowledge.
He claimed he was really being punished for making medical advice too cheap at 6d.
Some doctors recognized the medical need for relief from child-bearing, others were concerned about the eugenic aspects of differential birth-rates.
But most medical writing — even up to the 1920s — displays unscientific ignorance of sexual matters: dire warnings of physical and mental consequences of ‘Onanism’ or ‘conjugal masturbation’; the consequences to women deprived of ‘vital fluid’being especially severe.
Breast-feeding was recommended to space births and the rhythm method was approved.
Unfortunately, until well into the twentieth century, the safe period was assumed to be at mid-month.
But in 1921 the King's personal physician, Lord Dawson, stunned his profession alike and delighted the press by praising the morality of family limitation and warning Church leaders that they risked losing a whole generation by their unrealistic pronouncements; an example followed in 1925 when the next King's physician, Sir H. Rolliston, Regius Professor of Physick at Cambridge, became president of a new family-planning clinic.
The medical profession was about to be excluded by its ignorance from an almost universal practice.
There was no instruction on contraception in medical school; many doctors knew less than their patients.
Leadership had been abandoned to Marie Stopes, whose textbook Contraception appeared in 1923.
The Eugenics Society set up in 1926 the first medical study on contraceptive experience, which reported benefits to health.
In 1930 the Royal College of Public Health started the first family-planning courses for physicians.
The role of government
The government, unmoved by Continental fears of population decline (Teitelbaum and Winter 1985), did not follow the 1920 French example in making family-planning propaganda illegal (see Glass 1940).
But the public health and welfare aspects of family planning went unrecognized  by local or central government, despite pressure from some Medical Officers of Health.
The Ministry of Health insisted that it was not the business of maternity centres to offer family-planning advice, and the new Labour government in 1924 specifically forbade welfare centres from disseminating it.
The socialist position was hostile, an extension of Marxist anti-Malthusian analysis of poverty and its causes; and the Roman Catholic influence on the Labour Party was not negligible.
More Tory (54) than Labour (28) MPs supported a 1926 Private Members' bill to enable local authority welfare centres to give family-planning advice to married women.
The depression of the late 1920s and chronic unemployment appeared to confirm Malthusian pessimism.
Local authorities, many Labour, e.g. Stepney, Shoreditch, Sheffield, and Bootle, overwhelmed with welfare problems, challenged the Ministry of Health ruling.
Eventually a secretive 1930s Ministry memorandum enabled local authorities to give advice, initially on medical grounds only.
By 1939 280 local authorities were providing family planning help, together with almost seventy private clinics.
Most of the latter (except Stopes's five clinics) affiliated with the Family Planning Association which had been formed from the National Birth Control Association in 1939.
By 1949 there were 500 clinics helping 150,000 women every year.
Despite the Royal Commission's recommendations, their complete absorption by the National Health Service took a further twenty-six years.
2.9 Consequences: The Creation of a New Population
Population growth and structure
The gap between birth- and death-rates from the eighteenth century opened up an unparalleled new window for population growth.
Population had not previously exceeded five million and growth had seldom exceeded 0.5 per cent per year, which may have been the maximum that the underlying economy could sustain (Lee 1986).
Freed from previous restraints, British population reached 10.5 million by 1801 and 37 million by 1901 (figure 2.1).
Growth was most rapid in percentage terms around 1831 when it reached 1.55 per cent per year and in absolute terms around 1870.
This was small beer by the standards of today's Third World which (outside China) is growing at 2.5 per cent per year and in which some countries (Kenya, Zambia, Syria) approach 4 per cent.
But it was still enough to double population every fifty years.
From the beginning of the seventeenth century to the beginning of the nineteenth there had been relatively little change in the age-distribution   of the population.
The growth which began in the eighteenth century had made the population more youthful by 1801; the normal consequence of any decline in mortality and increase in fertility (figure 2.8).
The later fertility decline aged the population, a process already detectable by 1911 and marked by 1931 (figure 2.9)(Chapters 3 and 12).
Throughout the nineteenth century the proportion of the over-65s in the population remained below 5 per cent.
The enormous relative growth of older age-groups in the twentieth century is primarily due to the decline of fertility which began in the nineteenth (Chapters 6 and 13).
A new population map
The modernization and industrialization of nineteenth century Britain changed the population map.
During the nineteenth century Britain became the world's first modern urbanized society.
At the beginning of the century only about a third of the population lived in towns.
On the eve of the First World War, about three-quarters did, not far from the balance today (table 2.1).
Rural Britain had failed to hold onto its own natural increase, and had lost it to migration.
In the hundred years before 1851 rural population grew by only about 0.5 per cent per annum, compared with almost 2 per cent in the towns (Law 1967), despite the heavier urban mortality.
Numbers living in rural areas more or less stood still until mid-century, falling by just     half a million between 1831 and 1911.
Losses reached a peak during the decade 1881–91 when the net figure approached 85,000 per annum, just as the whole population was growing at its maximum absolute rate (350,000 per annum ).
Emigration abroad was also high; a net loss of about 1.2 million between 1841 and 1911.
But it was never so severe on the mainland as in Ireland (table 2.5).
Rural areas beyond immediate urban influence did not all behave similarly.
Loss rates may have varied according to the type of farming practised, depopulation being worse in heavier clay arable areas (Roxby 1912, Eversley 1907, Vince 1955).
Birthplace data from the 1841 census onwards show that only 40 per cent of those aged over 20 (the only people for whom data are available) had been born in the town they lived in.
Some rural parishes also recorded relatively high adventitious populations.
Over half of the population of Pinner, an agricultural village in 1851, were born elsewhere (Kirkman 1985).
Despite the view that southern England must have been emptied by trekkers to the coalfields of the north, migrants did not travel far.
South-easterners went to London; the coalfield towns were predominantly populated by nearby rural dwellers who brought with them high rates of natural increase.
Half of Liverpool's immigrants in 1851 came from Lancashire and most of the rest from Ireland, which in terms of travel costs was nearer than southern England.
New industrializing areas attracted migrants from the south to the north (and to South Wales) the reverse of ‘drift to the south’ which has preoccupied us this century.
But the growth of these areas was also substantially   due to their higher fertility (see Chapter 9).
Between 1801 and 1851 only limited areas were directly affected by urbanization, and high rates of increase still occurred in many rural areas, especially in the east and the south.
The map for the later period (figure 2.10) shows a very different picture with a broad band of population decline across the country from East Anglia to the South-West which is the area of maximum growth today.
The role of migration in bringing about these rural changes was decisive.
Growth from 1851 to 1911 was concentrated on the few urban industrial regions in which an increasing proportion of the population lived; these experienced both net immigration and natural increase.
Most rural areas lost population.
A few areas of early industrialization   also suffered losses, in north-east Lancashire and West Yorkshire, where cramped sites based on water power were no longer economic.
Only a few city regions and industrial areas gained constantly throughout the period (figure 2. 11).
The penetration of metropolitan influences into the Home Counties reversed early migration loss.
In the centres of conurbations, natural gains were already partly offset by net out-migration: higher incomes and better building regulations resolved problems of overcrowding.
Suburbanization became a powerful vehicle of population redistribution.
Merchants no longer needed to live close to their businesses (e.g. in Manchester, Girouard 1985) and the railway network permitted businessmen to commute to work from new high-status outer suburbs, notably around London, and later on to create Metroland.
Later in the century the development of ‘workmen's trains’ in London's unique network enabled many of London's workers to become commuters too.
More important, the repeal of the Corn Laws in 1846, following the triumph of Free Trade over Protection, reduced the price of grain with cheaper imports from North America, Russia, and later Australia (Deane and Cole 1969).
The ‘Golden Age’ of English agriculture was over; after the 1860s it entered a period of decline from which it was not to recover until the Second World War.
The introduction of labour-saving agricultural machinery to reduce wage costs began in earnest from the mid-nineteenth century.
The Great Reform Bill of 1832 enabled the growing urban demography, reflected in urban voting power, to challenge the agricultural interest in high food prices with an urban interest in low prices.
Temporary leadership in a world-wide free trade of manufactures enabled, for the first time, a population growing at between 200,000 and 350,000 people per year to be fed without dependence upon domestic food production.
But this required a free trade in food.
‘Those who had followed Peel with open eyes (to abolish protection and destroy his own party)…had done so because population was growing at the rate of 300,000 per annum.
It had been a question of time, a race between life and food.
To such men free trade was a need to be faced, not a treasure to be won’(Clapham 1932, cited in Grigg 1980).
THE CHANGING DISTRIBUTION OF POPULATION IN THE UK
3.1 Introduction
The last half century has seen the emergence of a number of trends in the changing distribution of the UK population.
These include the ‘drift to the south’, rural decline and subsequent revitalization, planned urban overspill and decentralization from cities, small town growth, and now perhaps a halt to the decline of the major metropolitan centres.
Two facts characterize these trends.
First, though there have been some marginal changes, most notably in the steady trickle away from the north and west to the south and east, the main features of the regional distribution of population remain the same.
Second, where change has occurred, particularly at the intraregional scale, migration has played the leading role.
The combination of census data with tried geographical techniques of cartographic analysis means we have been relatively well informed about the essentials of population distribution for over half a century.
The planning maps of the Ministry of Housing and Local Government showed which local authorities were gaining or losing people up to and including 1961.
A number of academics have used census data to plot details of changes in distribution in the inter-war and post-war periods (e.g. Willats and Newson 1953, Carter 1956, Moisley 1962, Champion 1976, Lawton 1977, Compton 1982).
From the 1966 census, but with analyses pushing back to 1961, several major studies have attempted to use a wide range of census data grouped in new and more functional ways to depict the spatial characteristics of the British population.
Most notable among these are the studies of Hall et al .
(1973) for 1966, Spence et al .
(1982) for 1971, and Champion et al .
(1987) for 1981.
More recently studies have combined decennial census data with annual population estimates supplied by the Registrar-General to enable more rapid updating of trends (Champion 1983, 1987).
Census atlases of individual conurbations have also become more common, such as those for the West Midlands (Rosing and Wood 1971) and London (Shepherd et al .
1974).
A corollary of these studies has been the classification of places in the UK according to their demographic, employment, and housing characteristics (Johnson, Salt, and Wood, 1974, Warnes and Law, 1984, Champion and Green, 1988).
This chapter will show the principal trends in population distribution since the inter-war period, focusing particularly on the last couple of decades.
It will attempt to assess the overall stability of the pattern and discuss the underlying forces which affect it.
3.2 Sources of Data
Study of the distribution of the British population has to be based largely on the census, an amazingly rich and flexible source of data but capable of providing us with only occasional, if regular, views of what is happening.
Filling in between times necessitates recourse to the mid-year estimates of the Registrar-General which provide details of the numbers of people in each local authority, together with a breakdown of annual changes by natural change and migration components.
The mid-year estimates are accurate for natural change, much less so for migration (see Chapter 10).
The only direct counts of migration fed into them relate to military personnel and to the residents of some institutions (mainly prisons).
The migration component has to be based on estimates gauged from a variety of sources like the Electoral Register, the NHS Central Register, the International Passenger Survey, and the migration patterns revealed by the last census.
There may also be problems of boundary changes resulting either from major local-government reorganization or from more local adjustments.
Despite these drawbacks the mid-year estimates undoubtedly provide an invaluable source and Champion (1987) concludes that for monitoring population change in England and Wales between 1971 and 1981 they may well be superior to the census; Scottish estimates, however, he considers less accurate because they were not revised in the light of 1981 census results.
For the period since 1981 both sets have similar problems but advances in methodology, especially in handling migration, make them more useful than in the past.
3.3 Where do the British Live?
The general distribution pattern
The UK is one of the most densely populated countries in the world, with an average density in 1981 of 242 per square kilometre.
There are, however, wide variations, ranging from densities of over 200 persons per hectare in some inner wards in London to virtually uninhabited    remote moorland areas in Scotland.
A feature of most high density areas is that there has been a long-term decline in population and density since before the Second World War (Craig 1986).
The high overall density reflects the urbanized nature of the population.
Even in 1911 over three-quarters of the British people lived in urban districts, the proportion being less in Northern Ireland where Belfast remains the only large industrial city.
The proportion of urban dwellers has changed little during the present century, though in reality over 90 per cent of the population now effectively lives an urban form of existence.
Few rural areas are beyond the ambit of a town or city and much of the increase of population in many parts of rural Britain reflects the underbounding of urban administrations.
The British people have chosen to dispose themselves unevenly across the national space.
The present map (figure 3.1) shows an axis of high density extending broadly south-eastwards from the southern Pennines, where it takes in the industrial conurbations of Lancashire, Merseyside, and South Yorkshire, through the Midlands to the London conurbation.
In the 1930s this zone was variously referred to as the ‘axial belt’, even the ‘coffin', though in truth there was always a low-density gap between London and Birmingham, so perhaps ‘doughnut’ would have been more appropriate.
It is interesting that the current growth of population in this empty middle is creating a proper axial belt for the first time.
Away from this are primary outliers of population concentration in the older industrial areas of South Wales, the North-East, and Central Scotland.
Smaller, secondary foci include the Belfast area, parts of the southern coast of England, especially in the vicinity of the Solent, the Norwich area, and the banks of the Humber.
Indeed, an almost cruciform pattern of high densities is emerging, with the North-West — South-East axis being crossed by one extending from South Wales, along the M4 corridor, passing to the north of London into East Anglia.
In contrast to these areas of high density large parts of the kingdom remain ‘empty’.
These are particularly the upland areas of Scotland, northern Britain, Wales, and South-West England; however, more rural parts of Northern Ireland and the East Midlands (including the Fens) are still lightly peopled.
Geographical variations in age structure
The beehive-shaped age structure of the UK as a whole is replicated by each of the main countries but one (figure 3.2).
The exception is Northern Ireland, whose age/sex pyramid is reminiscent of an earlier    phase of the demographic transition, with continuing high rates of fertility creating a much broader base.
In Northern Ireland, too, the booms and busts of earlier generations are much less apparent than in England, Scotland, and Wales.
For all countries the ten-year difference in the two sets of pyramids demonstrates how the excess births of one generation moves through the population, creating a successive need for new schools, jobs, houses and, ultimately, pensions.
The implications of this ground swell are returned to in Chapter 13.
There is a considerable degree of variability in the age structure of different parts of the UK, owing to the age-selective nature of the migration process (Chapter 10).
Particular interest attaches itself to the distribution of the elderly population, because of its implications for the provision of retirement accommodation and social services.
Law and Warnes (1976, Warnes and Law 1984) have shown that the elderly are peripherally located, especially in coastal areas (figure 3.3).
During the post-war period they have shown a quite stable distribution although in the 1970s the process of growing concentration in selected retirement areas, noticed in the 1960s, appeared to end.
Instead a new trend seems to have manifested itself.
This is for the proportions of the elderly to decline in the populations of some large, traditional retirement resorts, such as Bournemouth and Torquay, and to rise in adjacent and nearby local authorities.
The elderly are themselves decentralizing, it seems, in consequence of the growing attraction of large south-coast towns for the population at large.
This has pushed up housing costs and forced retirees with some capital but often on fixed incomes to seek out cheaper places.
The young, too, display their own distribution characteristics.
Figure 3.4 shows the massive concentration of 25–44-year-olds to the north and west of London, in precisely those counties which have recently had substantial population increases and which are regarded as some of the most prosperous places in the country (Champion et al .
1987; Champion and Green 1988).
The attractive power of this vibrant economic region for working-age people suggests its success will be long term.
3.4 Historical Patterns of Regional Change
Overall growth pattern
The basic regional distribution pattern has changed remarkably little since the beginning of the last century (table 3.1).
The South-East has increased its pre-eminence in both absolute numbers and share of the    national population but generally the regional batting order remains much as it was in the days of W. G. Grace.
The older industrial regions of the North, North-West, and Yorkshire and Humberside enhanced their population shares during the nineteenth century then fell back in the twentieth.
Scotland has steadily declined throughout; Northern Ireland fell steeply in the nineteenth century since when it has been relatively stable.
Wales's share of the British population has fluctuated little.
The more rural regions of East Anglia, the South-West and, to a lesser extent the East Midlands, lost out in the nineteenth century, but their fortunes have taken an upturn in recent decades.
Although there has been a generally stable long-term distribution pattern at the regional level, closer inspection reveals that since the industrial revolution growth has not been uniform across the country.
Between 1841 and 1911 the most rapidly growing regions were South Wales and north-east England; the big cities also grew rapidly, especially London and Manchester (Craig 1987).
Between 1911 and 1981 national growth was much slower and regional patterns differed from those earlier.
The most rapidly growing counties were all in the South-East, largely ones that had experienced below average growth rates in the nineteenth century.
Remote rural counties, such as those of mid and north Wales and Cornwall grew at below average rates over both periods until the middle 1970s, since when they have had above average growth.
This latest period has, in fact, seen its slowest growth rates in those counties which grew most rapidly during the Industrial Revolution.
Craig (1987) concludes that the trends are now back to the pre-industrialization pattern of population and that the ‘1911 type’ may eventually turn out to have been exceptional.
The absolute size of population gains and losses gives a slightly different picture of regional change.
From 1841 to 1911 the national increase was twenty million, over half of which took place in the metropolitan counties.
In contrast, from 1911 to 1981 the national increase was only thirteen million, the metropolitan counties in aggregate took under one-tenth of this and, from 1961, actually lost population.
Inter-war distribution of population
The inter-war period was one in which the distribution of population began to come to terms with a range of structural economic and social changes.
Competition from other industrial countries was already attacking the markets for traditional British manufactures before the turn of the century; the First World War meant that many overseas markets  were lost or severely curtailed.
At the same time new types of industry, demanding different locational requirements, were coming to the fore.
In particular the new ‘light’ industry regions no longer needed to be near to coal.
Electricity was flexible and clean, altogether a more congenial source of energy than steam.
The internal combustion engine created a new mobility, for people and goods alike.
New technologies meant that workers were no longer needed in such numbers, in many of the older industrial areas especially.
Already in the 1930s government was having to cope with this decline, and ventures like the Team Valley Trading Estate at Gateshead and the new iron and steel works at Ebbw Vale were testament to the need for regional support.
People's attitudes to where they wanted to live were also changing.
Ebenezer Howard's ‘garden city’ movement was one man's attempt to express the desire for a more suburban type of existence, although this was only ever fully developed in Welwyn Garden City and Letchworth.
The new pattern shunned the old industrial areas and the conurbation centres.
Employment and people alike began a southward drift, partially reversing the redistribution in favour of the coalfields that had characterized the Industrial Revolution.
The South-East was particularly vibrant, increasing its share of the national population by 2 percentage points between 1911 and 1951.
During the 1920s the loss of population from rural parts of England and Wales was relatively gradual, only exceeding 5 per cent in upland Wales, northern England, and parts of Wiltshire and Suffolk.
During the 1930s losses became more widespread and more acute in Wales, northern England, and Devon.
It was at this time that regional experiences diverged: the eastern part of the country, especially Lincolnshire and East Anglia, was recovering from its period of maximum depopulation, while in the west and the north the period of heaviest losses occurred (Willats and Newson 1953).
In general those rural areas experiencing persistent loss during the inter-war years were the uplands and other remote places (which in a generation or so would experience a surprising resurgence); in contrast, persistent increases occurred in rural places adjacent to the large urban areas.
In some parts of Wales, south-west, and northern England small town populations increased because of a local retreat from rural areas where there was little employment apart from agriculture, much of which was in a depressed state.
Regional population change in the 1950s and 1960s
Even before the Second World War the planning problems resulting from population growth in southern Britain were apparent.
Two major  Royal Commissions dealing with the issues produced reports in the early war years which were to provide the blueprints for post-war planning.
The Barlow Report (Royal Commission 1940) highlighted the strategic disadvantages of so much of the nation's industry and population being concentrated in South-East England (and within range of enemy bombers).
It recommended steps to prevent further growth there and to encourage dispersal of both people and jobs to other parts of the Kingdom.
The Scott Report (Ministry of Works and Planning 1942) was more concerned with the land use implications of growth, proposing a set of planning controls to deal with the spread of urban Britain.
Out of these two reports, and Abercrombie's proposals for the planning of London, came the dual strategy of containment (through Green Belts) and decentralization (through new and expanded towns) that have characterized post-war planning in metropolitan Britain.
In the regions where the drift to the south had its origins, new strategies were being adopted.
Regional policies designed to promote employment sought to maintain population levels in the less prosperous areas by curbing voluntary out-migration.
It may be argued that regional industrial policy in the UK in the late twentieth century has aimed to maintain — some might say fossilize — the population distribution created on the coalfields in the early nineteenth century.
Despite these strategies, applied in both depressed and prosperous Britain, it cannot be argued that there is or ever has been a clearly defined national population-distribution policy in the UK.
It is not hard to see why, since we are not in a position to evaluate the merits of different population distributions in any meaningful way.
It is in this light that we must view post-war regional population trends.
Despite attempts to diminish the drain of people from the depressed areas the flow to the south continued.
During the 1950s the population of the South-East went up by 1.13 million, with a net migration gain of 438,000.
East Anglia and the South-West also grew, by 110,000 and 189,000 respectively, some of this being overspill from their megalopolitan neighbour.
The West and East Midlands housed an additional 335,000 and 212,000 by the end of the decade.
The 1960s saw the beginnings of a change.
East Anglia and the South-West continued to grow, as did the South-East until 1966, then it experienced net loss owing to the massive haemorrhage from the capital.
Migration losses from the more peripheral areas were stemmed, no doubt in response to the government's development-area assistance (Eversley 1971).
The patterns of aggregate change described here reflect the interplay of natural change and migration which is not the same in all parts of the country.
Areas of high natural growth, like Merseyside, the coalfields  of northern England and South Wales, and Northern Ireland, have traditionally exported surpluses of population arising from their high birth-rates.
Many rural areas have acted in similar fashion.
Lawton (1977) has summarized the interaction between components of change for the 1950s and 1960s (figure 3.5).
Growth was widespread.
The fastest growing areas (types 2 and 3) experienced both natural increase and migration gain, and included large areas of central England, the Welsh borders, north-east Yorkshire, and parts of central Scotland — all regions combining the traditionally high fertility of rural areas with the immigration of overspill populations from the conurbations.
In more remote rural areas, such as northern England, parts of the Grampians, west Wales, and Northern Ireland, net migration loss was more than compensated for by high fertility (type 4).
In contrast, many retirement areas, such as the coasts of north Wales, north-west England, and Sussex, grew by virtue of continued in-migration exceeding natural decrease, itself a consequence of unbalanced age structure (type 1).
More widespread was natural increase exceeding migration loss (type 4), such areas including most of the conurbations and older industrial areas, together with some less remote rural regions in south Wales, northern England, and southern Scotland.
Areas of loss fell into two broad categories.
In Greater London (where decline has recently reversed) and in some of the older industrial areas, together with some remote rural areas in highland Scotland and Wales, long-standing out-migration created a population growing older but not yet suffering natural decrease (type 5).
Finally in north-east Lancashire net out-migration was compounded by natural de
3.5 Population and the Urban System
Decentralization in the 1950s and 1960s
The urban nature of the British population was recognized in 1951 with the designation of six conurbations in England and Wales (London, Merseyside, South-East Lancashire, Tyneside, West Yorkshire, and the West Midlands) accounting for 35 per cent of the population, together with Clydeside, which held 35 per cent of Scotland's people.
By 1971 the processes of urban decentralization and deconcentration had, despite some modest growth in absolute numbers, reduced these proportions to 33 per cent.
Such relative losses of population by the conurbations were paralleled in most large towns of over 100,000    people: during the 1960s their growth was only one-third of the national average (Champion 1976).
Much of this loss of population was from the inner, high density areas of the large cities.
Clearance and redevelopment took their toll in a planning system which decreed that the densities deemed acceptable by the Victorian house builders were not appropriate for the late twentieth century.
Many rehousing schemes were on the peripheries of the conurbations, often in adjacent local authorities, some still designated as rural districts.
Other inner-city inhabitants left for the suburbs of their own accord, trading a more pleasant environment for the often cramped, dirty, and increasing unsafe inner urban areas.
The exchange was not always beneficial; migrants from the old inner areas often regretted the life they had left behind in the move to a more open plan or high-rise environment (Young and Wilmott 1954).
They were encouraged by the first major decentralization of employment.
The Location of Offices Bureau showed the way in South-East England; distances moved from central London were often modest but they did have the effect of prising population outward.
Later it would become apparent that the decentralization of jobs was, in fact, following the drift outwards of the population and labour force.
Manufacturing industries, too, were finding existing sites, many dating from the end of the last century or even the 1930s, becoming cramped.
As their space requirements changed they sought more open greenfield sites away from the conurbation centres (Wood 1974).
Decentralization from the inner parts of cities accelerated from about 1960, affecting the largest places first and then spreading down through the urban hierarchy.
Analysis of this process can best be done by dividing urban areas into their functional zones.
The studies of Hall et al .
(1973) and Spence et al .
(1982) divided the urban system into Standard Metropolitan Labour Areas (SMLAs).
According to their scheme each SMLA consists of an urban core together with a metropolitan ring comprising the local authority areas from which at least 15 per cent of the workers commute to the urban core, while beyond the SMLA is an outer commuting ring from which at least 1 per cent of workers travel daily to the core.
The three areas together, core, ring, and outer ring, comprise a Metropolitan Economic Labour Area (MELA).
Between 1951 and 1971 Britain was urbanizing through an outward extension of commuter hinterlands, the populations of urban cores falling from 53.7 per cent of the national total to 47.9 per cent.
In the 1950s the decline of the cores was relative, absolute numbers of people living there continuing to increase, but in the 1960s absolute numbers declined as well.
In contrast, the metropolitan rings did well, as the   flight of population to them accelerated in both decades (Spence et al .
1982).
Migration was the vital component of population redistribution between urban zones (figure 3.6).
During the 1960s the cores lost about 2.5 million people from net out-migration.
In aggregate, substantial gross flows occurred between all zones, although an inflow to an individual ring was not necessarily from its own core or outer ring.
Especially in southern England, expansion in the metropolitan and outer rings of some MELAs originated from inflows from the cores and rings of other MELAs, frequently from other parts of the country (Kennett 1977).
The dynamism within the urban system is reflected in the performance of individual cities.
New and expanded towns experienced the highest rates of population growth in the 1950s and 1960s, but in absolute terms it was the medium-sized cities, like Bristol, Leicester, and Southampton, which made the biggest contribution to overall population increase.
Spence et al .
(1982) have run their own ‘British urban bumps race’ to show the changing fortunes of individual places (figure 3.7).
The new towns did well: Stevenage, for example, rose from 123rd in 1951 to 86th in 1971, Basildon from 122nd to 75th.
Other southern cities did well, Slough from 43rd to 32nd, Oxford from 27th to 21st.
But there were snakes as well as ladders.
Rhondda slumped from 71st to 110th, Burnley from 64th to 87th.
The decentralization process operated in association with the urban hierarchy, beginning with the largest place then moving successively downwards (table 3.2).
The capital was first in line.
Already in the 1950s the core was losing heavily, and this worsened in the next decade.
By the 1960s the London ring's increase had fallen and was no longer able to compensate for core losses, leading to a massive net loss in the SMLA as a whole.
In Manchester, Glasgow, and Newcastle similar patterns of change occurred; elsewhere decentralization only became absolute in the 1960s, for example in Birmingham, Sheffield, Leeds, and Nottingham, Smaller cities, such as Peterborough, Ipswich, and Bedford were centralizing, absolutely and relatively, during the 1950s, but then entered a phase of relative decentralization.
The mainland pattern was also occurring in Northern Ireland (Compton 1982).
Despite a natural increase of about 60,000, the Belfast Urban Area (population 612,000 in 1971) grew by only 3,000 between 1966 and 1971, indicating a high level of out-migration.
The beneficiaries of this movement were the new and expanded towns around the city — Bangor, Newtownards, Ballymena, Antrim, and Craigavon.
Within Belfast itself the inner city lost heavily, down from 417,000 to 340,000 between 1971 and 1978.
Decentralization in the 1970s
One major problems of ‘custom made’ urban functional regions like MELAs is that of comparability with time periods beyond the immediate study.
A similar analysis for 1971–81 has to be based upon a different set of regions, this time devised by the Centre for Urban and Regional Development Studies (CURDS) at the University of Newcastle.
The CURDS regions are again based on population and employment size and commuting patterns, but they use different thresholds and, unlike MELAs, they exhaust the UK space.
In consequence the definitions used in table 3.3 are not directly comparable with their procedures.
Despite decentralization, the British population remains highly concentrated in the urban cores (Champion et al .
1987).
The main built-up areas at the hearts of the 228 functional regions recognized still account for 61.6 per cent of the total population, despite losing nearly 1.5 million people during the preceding ten years.
Further evidence of the degree of concentration at this zonal scale is that 70 per cent of the rest of the population is housed in the rings.
This share changed little during 1971–81 because all three types of zone outside the cores gained population roughly in proportion to their original shares, each growing   by about 9–10 per cent.
The result is that in 1981 the cores and rings together accounted for nearly 9 out of 10 of the British population.
How the pattern changed among different types of urban area can be seen in table 3.4.
London's rate of population loss in the 1970s was virtually the same as the average for the next five cities (Birmingham, Glasgow, Liverpool, Manchester, and Newcastle), so the relative gap between them changed little.
In relation to the rest of the country, however, the share of the top six cities fell by 2.5 percentage points.
Further down the hierarchy the proportion of the population living in Other Dominant cities (cities at the core of the twenty major urban regions excluding the top six — e.g. Nottingham, Edinburgh, Coventry, Brighton) fell marginally, that of Subdominants (medium-sized cities   surrounding the Dominants) ruse by one percentage point and that of Freestanding cities went up by nearly two percentage points.
Hence, though change was taking place its pace has been slow, and it is unlikely there will be any fundamentally new pattern of population distribution in the urban hierarchy in the foreseeable future.
3.6 Regional Trends since 1971
The intercensal period of the 1970s was a decade of unprecedented decentralization in the UK, USA, and elsewhere, over more extensive geographical scales than hitherto (Champion 1983).
The widespread nature of the process took people by surprise; it was assumed that metropolitan growth was a permanent feature of modern society.
From the middle of the decade the idea developed that a ‘clean break’ with past patterns of urbanization had occurred (Berry 1976, Vining and Kontuly 1978).
Elsewhere, decentralization was seen less as a clean break, more as a part of the natural cycle of urban growth and change (Hall 1981).
Looking back from the late 1980s it seems that decentralization was not a clean break, nor was it a temporary aberration, since elements of both continue to exist side by side in the British settlement system.
Analysis of UK census data for 1981 do indeed show a country seemingly confirming the clean-break hypothesis (table 3.5).
In the preceding decade London and other large metropolitan cities recorded massive population losses; medium-sized cities either lost people or   experienced only slow growth.
In contrast, small towns and rural areas were consistent gainers, with remoter rural areas increasing their rates of population growth between the 1960s and 1970s.
What seemed to be happening was the spread of population growth out from the major urban centres where it had occurred in the nineteenth and early twentieth centuries progressively to the more rural periphery, leaving a population loss in its wake.
The picture we glean from a straight intercensal comparison is, however, misleading.
Using annual-change figures Champion (1983, 1987) showed that the main decentralization process peaked in the early 1970s, and that by the time of the 1981 census the former pattern had to some extent reasserted itself.
During the 1980s rural populations have grown more slowly then they did ten years earlier and some have declined; in contrast not only has there been a substantial slowdown in the rate of London's population decline (Britton 1986), but it has now gone into reverse in a small way.
The period since 1971 shows the cessation of the drift to the South-East, then its recovery.
During the first half of the 1970s all three of the most heavily urbanized regions (South-East, North-West, West Midlands) had population growth below the national rate, suggesting that the urban-rural dichotomy had taken over from the north-south divide as the main feature of UK population change.
The fastest growth rates during this time were in the three least urban regions of southern England: East Anglia, South-West, and East Midlands (table 3.6).
The South-East and the North-West were, in fact, doing worse than the traditional problem regions of Scotland, the North, and Wales, the last of which was doing very well.
By the early 1980s the situation had changed.
The South-East had improved considerably, its growth reasserted, and the north-south divide was re-established.
The scale of the region's recovery is shown by an analysis of percentage-point shifts in regional performance between 1971–4 and 1981–4 (table 3.6).
The South-East was the only region to record an upward shift in its growth rate during the period (Champion 1987).
The major part of these changes predate 1981.
The South-East's improvement was a feature of the second half of the 1970s, the 1980s being years of consolidation.
The relative deterioration experienced by other regions was likewise a child of the middle-late 1970s, especially in the cases of East Anglia and Wales.
Scotland and the South-West have been exceptions, moving against the national trend since 1981 with an upturn in their population fortunes.
The story implicit in this description of the general pattern of    regional change is reinforced by Champion's analysis at the more detailed county and district levels.
Counties with some of the highest growth rates up to 1978 were in the rural extremities of southern England, Wales, and Scotland, a most unusual situation for the twentieth century.
The Scottish Highlands went through something of a renaissance as English and Welsh moved there, often in semi-retirement, in search of a good and quieter life (Jones et al .
1986).
From the end of the 1970s modern normality was restored with a shift of population back to the South-East at the expense of more peripheral areas.
After 1981 the most rapid growth occurred in southern England, with the Wash-Severn axis emerging as a clear boundary.
The industrial and mining counties of the north performed badly, and many of the less industrialized parts of the north were also doing relatively worse than their experience in the earlier period.
Within the South-East divisions emerged, with western areas pulling ahead, along with adjacent parts of the South-West along the M4 corridor between London and Bristol.
Further north the impact of urbanization again began to be seen as a positive stimulus to growth, leading Champion (1987) to conclude that the most outstanding aspect of population change there was the strong upward shift recorded by the main conurbation areas, with the metropolitan counties of the West Midlands, Merseyside, Greater Manchester, Tyne and Wear, and Strathclyde registering the strongest improvements.
The significance of the degree of urbanization in accounting for these changes can be seen at district level.
Between 1971–8 and 1978–84 the largest upward shifts in rates of population change all occurred in Inner London boroughs.
In Inner London as a whole the average level of population loss fell from 63,000 per year in the early 1970s to 14,000 per year a decade later.
Absolute losses also fell in Outer London, from 26,000 to only 3,000 per year over the same period.
All of the other large cities had trends in the same direction, albeit on a less dramatic scale.
Southern coast seaside resorts also improved through their ability to attract a more balanced age structure.
In contrast, new and expanded towns experienced the greatest declines in growth rate, confirming that the new towns are mostly no longer new nor are they so fertile.
In non-metropolitan districts as a whole the rate of population growth in the first years of the 1980s was less than half what it had been a decade earlier, though again most of the fall preceded 1978.
Never the less such places, including remoter rural areas, continued to grow; decentralization may have slowed but is not yet completely out of steam.
Analysis of components of change again demonstrates the      importance of migration in modifying the population distribution map (tables 3.7, 3.8).
The recovery of births in the late 1970s helped London and other large cities to improve their population levels relative to smaller places.
This may have been due to the tendency of the former to attract in young people, leading to net gains by migration among 15–24-year-olds (Chapter 10).
Any subsequent slowdown in out-migration may thus lead to natural growth as this group remains in the city to start a family
3.7 Processes of Change
While no radical shift in the distribution of the British people at the macro-regional scale has occurred during the present century, the settlement pattern has displayed a dynamism that has proved difficult to forecast.
A number of writers have speculated on the underlying processes involved in counterurbanization (for example, Berry 1976, Hall and Hay 1980, Fielding 1982, Champion 1987).
Much discussion has focused on whether or not it is a general phenomenon, related to a ‘natural’ cycle of urban change that will in time affect all developed countries, or whether there are specific national and/or regional circumstances which mean different trajectories.
In the British case, Champion (1987) has argued that explanations for decentralization and counterurbanization should be sought chiefly in the 1960s and that the circumstances of the 1970s should be used to explain the resurgence of growth in the more urban regions in the 1980s.
At the same time he rightly argues that it is premature to conceive of a cycle of decentralization since that might ‘presuppose the existence of one single major engine behind the process and suggest the possibility of the recurrence of a similar round of developments in the future’(pp. 35–6).
We need to seek answers in the range of influences that guide migration decision making, particularly in those forces responsible for investment and thus employment, in changing residential preferences, and in the interaction between demographic change and the housing stock.
The role of investment
Massive changes have taken place in the location and scale of investment, especially in manufacturing, since the middle 1960s (Chapter 9).
It has been argued that a new spatial division of labour has been created which has changed the relative attractiveness of locations (Massey and Meegan 1982, Massey 1984).
At the same time the structure of corporate organization has changed with the growing  importance of the multi-locational, often transnational company.
Such employing organizations have sought out locations away from traditional ones in the large cities, seeking female and other non-unionized labour and sites with room for expansion.
Many of these new locations, particularly in the 1960s, were in areas attracting regional industrial assistance from government, further reducing costs.
Keeble (1980) and Fothergill and Gudgin (1982) have demonstrated the huge shift in manufacturing towards freestanding cities and smaller towns from the early 1960s onwards.
This trend has been reinforced by new, high technology industries with small-scale employment, and by the growth of self-employment.
Neither of these needs to be close to traditional large labour markets and their locations can pay greater attention to residential preferences.
Changes were taking place in rural economies too.
Government attempts to relocate small industries in rural areas through the COSIRA scheme had some success.
Other rural investments, in minerals (North Sea oil installations, for example), agriculture and service provision, had multiplier effects which increased the general desirability of living in more remote areas.
Residential preference
There is no doubt that the population at large has become more mobile in many ways.
Travel is easier and we are continuously bombarded with information about distant places.
For many, but not for all, there is a greater degree of choice about where we live, though, ironically, changing household lifestyles often introduces new constraints as more members go out to work.
It is in the USA that residential preference has been invoked most as an explanation for decentralization (Berry 1976), perhaps partly as a return to old ideas of the frontier and the rural idyll, as Champion (1987) suggests.
In the UK, a much smaller land, nowhere is very far from ‘civilization’ and most small towns have a reasonable range of services.
Retirement migration is the most commonly discussed example of movement away from large urban areas, and has increasingly affected younger retirees.
As will be discussed in Chapter 10, the first big generation of owner-occupiers began to retire from the 1960s onwards.
With capital to invest, the cheaper house prices and quietness to be found outside the large urban areas proved attractive to them.
Such virtues were also likely to appeal to many people of working age, especially if employment could be found in the growing service industries that had themselves decentralized.
Demographic structure and housing
Changes in family structures since 1960 have coincided with changes in housing stock.
The rise in birth-rate from the middle 1950s created new young households from the middle 1970s; higher divorce rates led to further fission; trends in retirement meant more elderly lived away from their families.
At the same time home ownership became easier and the norm.
Meanwhile the housing stock was being changed.
The search for an affordable home led many to leave the high priced housing markets of the cities (Salt and Flowerdew 1980).
Within cities older and cheaper houses were disappearing.
Slum clearance accelerated during the 1960s; elsewhere ‘gentrification’ removed the cheap end of the private rented sector.
Other forces were at work too, including new attitudes to urban redevelopment after the Ronan Point collapse in 1974, and the general slowdown in local-authority building that resulted from the growing awareness of the disasters of tower-block living.
Hence, at a time when growing numbers were seeking houses the capacity of the cities to cope was being tested.
It was not simply a question of equating total stock of houses with that of households, but with ensuring that individual aspirations could be satisfied.
Often the only way for that to happen was for those already in the cities to move out and for those contemplating a move in not to come.
FERTILITY TRENDS
4.1 Introduction
The new fertility regime
This chapter will describe the course of fertility since the First World War, relate its trend to social and economic change, and discuss some of the differences in fertility within British society.
Since the 1920s fertility in Britain has been a variation on a two-child family theme.
Since the 1970s, in common with the rest of the industrial world, it has fallen to its lowest level ever recorded.
Contraception within marriage is now nearly universal, so families can control the timing as well as the number of births to a degree new in history.
As a consequence, fertility has been highly volatile.
The annual number of births determines the age structure, and these twentieth-century fluctuations have created a new highly irregular outline (figures 2.9, 3.2).
The power to make effective decisions on fertility, reached independently but simultaneously by millions of families, has had startling aggregate effects on our population, society, and economy, the consequences of which are discussed further in Chapter 13.
The measurement of fertility trends
The Total Fertility Rate (TFR) is a useful way of expressing movements in current fertility (figure 4.1).
It is sometimes called the Total Period Fertility Rate (TPFR) or Total Fertility Ratio or Index, as it is not strictly a ‘rate’.
It shows the family size a woman would have if she experienced current age-specific fertility rates through her lifetime.
It cuts out the effects of the age structure of the population which can confound simpler measures such as the crude birth-rate.
But when changes in births are due to shifts in timing, the TFR exaggerates current trends in relation to the final future outcome of family size.
The completed family size is also shown in figure 4.1 for real birth cohorts of women.
Their family size is plotted on the graph at the year when they were aged 27 — the generation length, or age at which they gave birth to their median child.
As always with cohort measures, the data   cannot be taken up to the present day without a considerable element of projection (broken line) of the generation rate.
More satisfactory measures of total fertility can be based on estimates of the chances of women going on to have first, second, third etc. births (‘parity progression ratios’).
True ratios need completed cohorts.
But estimates can be made from period data e.g. the ‘Total Quantum Fertility Index’(Brass 1989).
Although it cannot be entirely free of period distortions, it shows more modest fluctuations in fertility than the TFR and in particular gives an estimate of fertility of over 2 children for most of the time since 1972, not the 1.8 or so given by the IFR.
However the data needed for this index, especially parity data on illegitimate births, are not routinely available, and its calculation demands a number of assumptions.
Overall fertility trends in the twentieth century
Fertility declined steadily from the 1870s to a new low in the 1930s.
Its downward trend was disturbed only by the uncertainty of the First World War and a sharp but transient post-war baby boom.
The 1930s nadir is usually regarded as the final maturity of the new trend towards small family size which began in the marriage cohorts of the 1870s.
In 1933 fertility was the lowest ever recorded until the 1970s.
In that year the TFR was 1.72, corresponding to a Net Reproduction Rate of 0.74.
None the less, women married for the first time in the 1930s actually had family sizes only just below replacement rate.
Average family size after thirty years of marriage fell below 2.1 among women married in 1928, reached a low of 2.01 among women married in 1936, and rose to exceed 2.1 among women married from 1943 onwards.
These figures exclude extra-marital births and births to remarried women.
Birth-rates recovered somewhat by the end of the 1930s (e.g. to 1.83 in 1939).
It seems reasonable to suppose that they had been particularly depressed by the economic conditions of the inter-war period.
Unemployment in the UK peaked at 17 per cent in 1932 but had fallen back to its 1920s level of 8 per cent by 1937.
Births fell and employment and real wages worsened in almost all industrial economies in the early 1930s.
It is important to remember that Britain fared less badly than the USA and some of its Continental neighbours, and that even adjusted for unemployment, real wages continued to increase on average until the early 1930s (figure 4.2, see Dimsdale 1984).
In the UK the unemployment was strongly concentrated in Scotland, Wales, Northern regions, and rust-belt industries.
Throughout Europe, these trends showed that population growth and even population replacement could no longer be taken for granted.
They provoked a renaissance of national interest and public debate about demography (Reddaway 1938, Hogben 1938, Glass 1936, Charles 1936), which is discussed further in Chapter 13.
The Census of 1911 had asked new, detailed questions on past births as well as present children, some of which were repeated in 1921.
The (private) National Birth Rate Commission was set up in 1913; the Eugenics Society established the Population Investigation Committee in 1935 to analyse the new trends.
The Population (Statistics) Act of 1938 enabled more data to be collected at the registration of birth.
The first and only Royal Commission on Population (1949), was set up in 1944 to see if Britain was indeed facing population decline and to   ‘consider what measures, if any, should be taken in the national interest to influence the future trend of population’.
But it led to no official population policy, nor to real changes in government policy towards family support (see Chapter 13).
Even before the Second World War births were increasing again: imperceptibly in France (Ogden and Huss 1982), slowly in Britain from 1933, and most dramatically in Germany.
It is only in Nazi Germany that population policies, general between the wars in Continental Europe, may have been effective in raising the birth-rate (Glass 1940, Bleuel 1976).
The Second World War itself had an erratic effect on the tempo of British fertility.
Wars generally suppress fertility, as families are separated, marriages delayed (after an initial precipitate rush), and the outlook for childbearing seems uncertain.
But this effect was weaker in the Second World War than in the First.
Unlike the earlier  conflict, British armies were expelled from Europe for four years.
This may be one reason why fertility recovered somewhat after its initial wartime decline, to reach a peak in 1944 (781,478 births), unmatched since 1923, only to fall again in late 1944 and 1945 as mass armies were once again able to operate overseas (see Registrar-General 1954).
Most recent wars have ended in a baby boom, even for the losers.
Despite the unpromising post-war British economic scene, births sharply increased from 1946 to 1948 as servicemen returned home.
The end of this demographic spike (smaller than that which followed the end of the First World War) brought a renewal of anxiety, expressed in the Royal Commission's Report (1949), lest renewed low fertility should revive the threat of population decline.
Fertility languished at a low level into the early 1950s, seemingly appropriate to the austere circumstances and declining real wages of the time.
But the flat or declining projections made then (Chapter 13) were soon confounded by a regular and sustained increase in fertility, shared with most Western countries.
From 1953 to 1964 the total fertility rate in England and Wales rose from a nadir of 2.14 in 1951 to its zenith of 2.94 in 1964.
The marriages of the late 1950s and early 1960s have produced an average of up to 2.4 babies.
This is the famous ‘baby boom’, shared more or less by all Western industrial countries (not Eastern Europe or Japan), which has now produced almost as many books as it has people.
Earlier childbearing by women born in the 1940s was responsible for most of the baby boom of the 1960s.
Absolute numbers of births increased from 667,811 in 1935 to 875,972 in 1964, an increase of 31 per cent.
The total fertility rate went up from 2.22 to 2.94 — an increase of 32 per cent.
But the family sizes of women married in 1945 and in 1954 after 10 years' marriage were 1.79 and 1.96 children respectively — a difference of only 10 per cent.
The most fertile birth cohort of women since the 1920s has been the women born around 1937, who would mostly have married by the early 1960s.
By age 40 they had produced 2.4 children on average; a statistic which seems to have become famous.
From then onwards fertility has declined and remained low, gradually until 1971 (TFR 2.38)— when population growth was still sufficient to alarm the environmentally conscious (Taylor 1970, Brooks 1974) —then much more severely.
Fertility fell below replacement level (TFR 2.1) in 1972; a new trough was reached in 1977 (TFR 1.7), a return to the position of fifty years earlier.
Fertility has oscillated around 1.8 ever since.
From the 1960s onwards, period fertility rates began to fall below the replacement level throughout Europe and the rest of the industrial   world, and have stayed there ever since.
The coincidence of the timing of these changes is remarkable, and even Southern European countries with more rural populations and traditionally higher fertility have followed since the early 1980s (figure 4.3)(see Council of Europe 1989, Davis, Bernstam, and Ricardo-Campbell 1986).
In fact Britain in 1989 had one of the highest fertility rates in Europe; 1.8, half a child higher than the astonishing 1.3 of Italy and West Germany, which are the lowest in world history.
In Britain it is later childbearing, rather than the abandonment of parenthood, which caused most of this fall in annual births and period fertility measures.
Recent cohorts will not match the 2.4 children per family of the baby boom years, but the total seems unlikely to fall much below 2 (table 4.1).
This is discussed further below.
Family size at longer durations of marriage has been rather stable in recent years; the similarity of family building of people born in the 1960s to that of the 1920s does not imply much reduction in family size.
However West Germany and its neighbouring countries may be heading for a different pattern.
Fertility recovered a little from its 1977 low (figure 4.4) but by 1980 this recovery was spent, at a level still below that needed to replace the population in the long run.
Up to 1989, TFR has not exceeded 1.8 in Britain.
This transient revival, shared with other Western nations, was probably a ‘technical rally’(Calot and Thompson 1981).
It is impossible to postpone childbearing indefinitely without children being cancelled altogether.
The baby bulge birth cohorts have been of an age to have children for some time.
But they have not produced the expected increase in births.
The postponement of first births by women born in the 1950s (mostly married in the early 70s) has made these births more ‘spread-out’ in time (Kiernan and Diamond 1982) and   contributed to the trough in births observed in 1975 — 9.
In the end, there is no guarantee that the ‘natural’ level of fertility in modern societies has to be at or above the replacement rate.
Indeed no one yet knows what a ‘natural’ or ‘equilibrium’level of fertility may be in such societies.
We will return to that problem in a later section.
4.2 Explaining Fertility Trends: ‘Proximate’ Causes
Sex and fecundity
Fertility rates are the outcome of complex processes affecting each couple which lead to millions of individual decisions, accidents, failures, and results.
In trying to account for overall trends it is helpful to start with the sequences of factors which control the immediate risks of fertility for the individual and without which births cannot occur: natural fecundity (the physiological capacity to conceive and bear children), exposure to intercourse within and outside marriage, and evasion of the consequences of intercourse through contraception and abortion.
Changes in these immediate precursors of child-bearing or ‘proximate’ variables provide one level of analysis and measurement (Bongaarts and Potter 1983) and point the way to the more remote social and economic factors, such as costs of children, desired family size, and their variation between social groups, which we will look at in section 4.3.
Fecundity and the frequency of intercourse are usually taken to be  constant factors.
Malthus assumed that the ‘passion between the sexes’ was constant; modern demographers seldom think of it at all in academic terms, leaving it to medicine and biology (Austin and Short 1980, Parkes 1976).
But this may assume too much.
For example, data from the United States suggests that the human sperm-count per ejaculation is about half its value of a quarter of a century ago (James 1980).
In a number of Western countries including Britain the relative frequency of fraternal (dizygotic, non-identical) twins approximately halved from the 1950s and 1970s.
In England and Wales, the sex ratio at birth has fallen from 106.3in 1971–5 to 104.8in 1988 (Shaw 1989).
As the frequency of such twins, and the sex ratio, are thought to depend partly on the levels of female sex hormone, this has been taken as evidence for a general decline in Western fecundity (James 1980, 1982).
But sexual intercourse seems to be becoming ever more popular.
US evidence suggests a 20 per cent increase in coital frequency at all ages from 1965 to 1975 (Trussell and Westoff 1980).
Sexual activity is certainly starting earlier (Mant et al .
1988).
Since the late 1960s most women have had sex before marriage — increasing from at least a third of women who were married in the late 1950s to three-quarters of women married in the early 1970s (Dunnell 1979).
Average fertility is so low today compared to the total number of children that might be conceived that factors affecting fecundity would be unlikely to influence final family size.
But they might affect birth spacing.
A certain proportion of women and men are sterile or have difficulty having children.
These difficulties can arise from sexual problems, infertility and pregnancy loss.
The overall prevalence of involuntary childlessness is difficult to estimate.
Demographic statistics include both the voluntarily and involuntarily childless.
Within marriage, fluctuations in the proportions without children are due to changes in choice about childlessness, not changes in fecundity.
In England and Wales the average proportion of each marriage cohort still without children after twenty years of marriage (all ages at marriage together) is between 10 and 15 per cent(e.g. 1951 13 per cent , 1956 10 per cent ).
The lowest proportion of any cohort of married women to remain childless by age 50 is about 7 per cent(see figure 4.10).
Eight per cent of all women aged 40–4 (married or otherwise) had no liveborn children according to the 1987 General Household Survey.
Apart from women sterilized for contraceptive purposes, 6 per cent of women aged 16–44 claimed it would be difficult or impossible for them to have children.
In the United States in 1982, 8 per cent gave the same answer in a similar survey (US Dept of Health and Human Services 1987).
In 1976, 10 per cent of US married couples with wives aged 15–44  had been unable to conceive after a year.
If married persons sterilized for non-contraceptive reasons are included, the proportion of the subfecund rises to 25 per cent(most non-contraceptive sterilizations are for conditions which would themselves cause sterility: McFalls and McFalls 1984).
The proportion of women married below age 20 still childless after 20 years was about 2 per cent for women married in 1951, 3 per cent for women married in 1956.
In 27 countries studied in the World Fertility Survey, the proportions of married women still childless by age 40–9 varied from 1.3 to 6.7 per cent, and historical data suggest that 3 per cent of couples are sterile from the beginning of reproductive life (Population Reports 1983, Bongaarts and Potter 1983).
The trend towards later marriage and childbearing, especially among career women, has sharpened interest in how far children can be deferred before running into seriously increasing risks of sterility.
Historical data from non-contracepting European populations reinforces a view from contemporary data that the risk rises gradually from youth until after age 35 — and then rises more steeply: 6 per cent of women aged 25 were sterile, 24 per cent at age 40,(Menken 1985, Trussell and Wilson 1985).
Median age at menopause in contemporary Western society is about 51.
Advances in medical science, surgery, and sex hormone therapy should tend to reduce involuntary childlessness.
Up to 2,500 births a year in Britain result from artificial insemination by donor (Teper and Symonds 1985) and since the birth of the first ‘test tube’ baby in 1978 there have been several hundred such births in Britain alone (see Edwards 1983).
Contraception and abortion
It seems obvious to blame the fall in fertility from the mid-1960s on the pill and legal abortion.
The contraceptive pill became available in Britain from about 1962.
Contraception became freely available through the National Health Service on social grounds, for unmarried as well as married women, by the National Health Service (Family Planning) Act 1967 (see Leathard 1980).
The grounds under which abortion could legally be carried out were greatly widened by the Abortion Act of 1967, although these stop short of the ‘abortion on demand’ available in several other countries.
These are tempting explanations for fertility decline.
The Germans have even called the post-1960s dent in their age structure ‘Der Pillenknick’.
But the implication is wrong.
This view assumes that people always want fewer babies than they actually have (because of inefficient family planning).
Hence any improvement in efficiency or availability of contraception will always  drive down the birth-rate.
Such a simple view cannot accommodate increases in fertility at times when contraceptive knowledge and technique were generally increasing, as in the late 1930s and the post-war baby boom.
Neither is it true that well-developed contraceptive techniques are necessary to achieve low fertility.
Severe reductions in fertility, to low levels similar to today's, were achieved in the past with quite rudimentary means of contraception.
The total fertility rate in the 1930s was reduced to 1.7 in 1933 from 2.4 ten years earlier (a 28 per cent reduction) and from 2.8 twenty years earlier (a 39 per cent reduction).
This was achieved solely through the use of the condom, the cap, coitus interruptus, abstinence, with the help of illegal abortion.
Better contraception does not necessarily mean lower fertility, although it should mean less unwanted fertility.
Planned pregnancies have declined much less than unplanned ones, especially in higher order births in Britain and in the USA (Westoff and Ryder 1977).
Better contraception should be regarded as the means whereby desired family size, and short-term accommodation to economic circumstances, can be more efficiently accomplished.
Fertility increases in Britain occurred in a modest way in the later 1930s and much more strikingly between the mid-1950s and 1960s.
But contraceptive techniques were improving throughout this time and successive marriage cohorts were more likely to use contraception (table 4.2) and use it more effectively than before (Rowntree and Pierce 1961, Langford 1976, Cartwright 1978, Dunnell 1979).
Sterilization is becoming one of the most important methods of contraception among women of childbearing years, and the normal method of contraception for couples where the woman is aged over 30.
In 1970 only 4 per cent of married women aged 18–44 were protected by contraceptive sterilization (i.e. either the husband or wife was sterilized).
By 1987, 30 per cent of married or cohabiting women aged 16–49 were.
It is not merely a final precaution taken by couples near the end of their fecund life.
In 1987, 10 per cent of such women aged 25–9 were protected by sterilization, 24 per cent of women aged 30–4, 46 per cent aged 35–9.
Eleven per cent were protected by sterilization after one birth, 35 per cent after two (GHS 1987).
Such terminal sterilization, common among all social classes, is likely to depress high parity births still further, although its relative irreversibility is already causing problems for remarrying couples.
‘Pill Scares’ and Fertility
Fear of the risks of contraception may turn women towards apparently safer but less reliable barrier methods, This may increase unwanted      fertility.
In the so-called ‘pill scare’ of 1977 attention was first drawn to the risks from thrombosis to older women on the pill (Vessey et al .
1977; Beral and Kay 1977).
These researches showed that with the pills then in use, mortality from circulatory diseases among pill users was five times that of non-users, and ten times higher among women who used the pill for 15 years or more.
Smokers had much higher risks.
Overall mortality of users was raised by 40 per cent(in age-groups where mortality is low).
Pill use declined, especially among older educated women, from 45 million courses in 1977 to 38 million in 1979.
Some of the less educated women in Askham's (1975) Aberdeen study had given up the pill as a result of such articles in popular newspapers and had become pregnant in consequence.
Births and abortion rates increased at about the same time.
However, the increase in fertility seems to precede the publicity about the pill, and the fertility increase has other explanations (Bone 1982, Ermisch 1983).
Fertility still remains low, despite lower pill use.
Although use of the IUD and condom has increased, especially among educated women, the rise of sterilization is much more important as it marks an almost certain end to childbearing (table 4.3).
Pill users have since been further worried by reports that early use of the pill can cause cancer of the breast (Pike et al .
1983) and that its prolonged use may increase the risk of cervical cancer (Vessey et al .
1983), although at the time the Medical Advisory Panel of the International Planned Parenthood Federation (IPPF) concluded that the evidence was insufficient to modify current medical practice.
Despite many investigations, the evidence for these cancer risks is still not conclusive, although it is certainly disturbing (Vessey et al .
1989).
Even a small increase in breast cancer is significant because it is already the most important cause of cancer death in women.
The pill's more certain beneficial effects on benign breast disease and on cancer of the ovaries and of the uterus are less widely known.
In the 1980s AIDS revived interest in the ancient protective properties of the condom.
Legal abortion
Legal abortion in Britain since 1967 came too late to explain the beginning of fertility decline.
Furthermore, there were many illegal abortions before the Act.
Estimates vary between 20,000 and 100,000 per year, with the most likely number probably around 60,000 (Leete 1976, James 1971, Goodheart 1973), Many illegal abortions which would have happened anyway could be performed legally after 1967.
Since then, observed levels of legal abortion cannot be responsible for more than a fifth of the decline in legitimate fertility, although its impact   of illegitimate births is greater.
In 1988 there were 168,298 legal abortions on residents of England and Wales (figure 4.5).
Legal abortions accounted for 39 per cent of all conceptions outside marriage in 1975, declining slightly to 37 per cent in 1955, and accounted for 8 to 7 per cent respectively of all conceptions known inside marriage from those two years (table 4.4).
The ratio of abortions per 1,000 live births was 229 in 1987, compared with 180 in 1977.
From 1967 to 1987 annual births dropped from 832,000 to 682,000.
The difference is double the increase in the number of abortions, and one abortion cannot be considered to ‘prevent’ one live birth — a ratio of 3:1 is often assumed.
From 1977 to 1987 births increased from 569,259 to 681,511, despite an increase of 53,514 in the annual total of abortions.
Legal abortion may have its greatest demographic impact in reducing pre-maritally conceived births and ‘forced’ marriages.
Overall, 40 per cent of conceptions outside marriage to women of all ages were terminated in 1976, and 36 per cent in 1986 (OPCS 1989c).
Unmarried teenagers and older unmarried women have the highest abortion rates, but the proportion changes little with age.
In 1986 38 per cent of the 103,000 conceptions outside marriage to teenagers (under age 20) were aborted, declining to 32 per cent of the 25,000 such conceptions to women aged 30–4 and rising to 56 per cent of the 2,000 conceptions to unmarried women aged 40 and over.
The abortion ratio for teenagers in England and Wales of about 500 per 1,000 live births is about the same as that of other industrial countries except for Japan, Denmark, and Sweden where the ratio is about 1,500 per 1,000 live births (Clearie et al .
1985).
However the actual number of teenage conceptions in Sweden is much lower than in Britain, thanks, it is believed, to comprehensive sex education.
Within marriage, abortions are uncommon: 7 per cent of all conceptions within marriage in 1986.
The number of abortions performed on married women has remained at about 40,000 per year for the last fifteen years, while the number performed on single women, and  divorced, separated, and widowed women has approximately doubled.
Most abortions within marriage are carried out on women over age 30 (56% per cent in 1988).
In 1986, only 4 per cent of conceptions to married women aged 20–4 were legally terminated compared to 43 per cent to married women aged over 40.
Some are unwanted high parity conceptions; in addition a high proportion of abortions at this age are carried out because a congenital malformation has been diagnosed in the foetus.
The risks of such abnormalities as Down's syndrome and spina bifida increase rapidly with maternal age from 1 in 2,700 births to mothers aged 25–9, to 1 in 170 to mothers aged over 45.
The proportion of these abortions to older married women is declining: 55 per cent of conceptions to married women over age 35 were aborted in 1975; 25 per cent in 1985.
Part of the decline in abortion at this age is due to the greater number of wanted pregnancies from remarried women.
The incidence of many congenital malformations has declined in the last decade thanks to advances in screening coverage (see Carter 1983, Royal College of Physicians 1989) and techniques, and wider knowledge of the availability of abortion.
But some malformations cannot be diagnosed until late in pregnancy.
Restriction of the gestation period within which abortion may legally be performed from the 28 weeks defined in the 1967 Act was frequently urged by abortion reformers, for example to 18 weeks in the failed 1988 Private Members Bill by David Alton MP.
Such changes would be likely to increase the incidence of some of these conditions, as well as of births to women who do not report, or even discover, their unwanted pregnancies until late in gestation (see Royal College of Obstetricians and Gynaecologists 1984).
These tend to be those least capable of caring for a child, or those bearing a foetus with malformations for which the diagnosis is time-consuming.
In practice, the professional bodies concerned, e.g. the Royal College of Obstetricians and Gynaecologists, had already agreed to a limit of 24 weeks and in 1990 a Bill was passed to reduce the legal limit to 24 weeks.
Attitudes to childbearing and family size
Attitudes are not a ‘proximate determinant’ in the technical sense, but it is useful to discuss the issues in the context of family planning.
A succession of opinion surveys strongly suggests that most people regard a family size of two children as ideal; this preference seems to have grown stronger over the years (Woolf 1971, Woolf and Pegden 1976, Dunnell 1979, General Household Survey, Jowell et al .
1986, 1987).
Two or three children remain the ideal of 80 per cent, only 6 per cent think one or none is ideal (Gallup 1986).
Average desired or intended family size is rather higher than the family size implicit in birth-rates since the 1970s.
TFR has been below 2.1 since 1972.
‘Ideal’ family size in 1967 was about 2.4 and has only declined marginally since.
‘Expected’ family size, recorded in the General Household Survey, has varied from 2.28 to 2.24 (table 4.5).
Women born before and during World War II expected (and duly produced) higher family size — 2.5 to 2.4 children.
Women born in 1945–59 expected 2.27 children; women born in 1960–4 expect 2.33 on average.
These are all substantially above replacement level and about a third greater than the fertility (1.8) implied by current rates.
This is one of the reasons why fertility is expected to rise a little in official projections, to 2.0 (OPCS 1989a), although there are reasons for believing that these high expectations will not be matched by performance (Shaw 1989).
Demographers look on surveys of family intentions with mixed feelings, although according to one analysis 80 per cent of the women married in 1959 achieved their intended family size (Barrett and Brass   1974).
While they may often be approximately right in aggregate, these forecasts are often wrong in the individual family.
Forecasts made by married couples of their personal family size seem to share many of the drawbacks of demographic forecasts made by demographers; especially too much dependence on current conditions (Westoff and Ryder 1977).
There may be a general tendency to exaggerate; couples seem to be more willing to abandon expectations of a third child than they are to opt for a third child after stating an earlier preference for two.
Accidental and planned fertility
In a modern society where just two children are the norm, any birth is a major event, every birth has major consequences for the mother's material circumstances and future.
This is especially true for the unmarried and the poor.
Why, then, are there still so many unwanted births among people with less favourable circumstances?
In some cases the birth, however objectively undesirable, may make circumstances less unfavourable.
Pregnancies of single mothers may have been made less unfavourable by housing legislation (Ineichen 1972).
Fewer single mothers are employed, contrary to the trend among married mothers and among single mothers overseas.
This may reflect the welfare benefits available to some single mothers compared to their low earning power (Campbell 1984).
Girls with unsatisfactory parenting may crave a baby of their own to provide the affection lacking in their own upbringing; but then themselves repeat the problems which they are trying to get away from(Kiernan 1980b).
Unemployed single teenagers are particularly likely to have children; it has been suggested as a ‘short cut’ to adult status (Penhale 1989).
Teenage fertility
Teenage fertility — both legitimate and illegitimate — brings together many of the problems of poor knowledge and planning, the harmful consequences of unwanted fertility, and its transmission across generations.
Teenage mothers are least able to cope with motherhood, being often themselves emotionally immature and sometimes physically immature (Russell 1981).
Often they possess no resources or accommodation of their own.
They are twice as likely to live in council housing, twice as likely to be supported by someone in class V (if at all) than in any other class (Werner 1984).
Their pregnancies are likely to be troublesome and repetitive.
In a Newcastle study, more than 70 per cent of first illegitimate pregnancies were followed by another.
The earlier the pregnancy, the worse the circumstances.
Girls who become pregnant by their 16th birthday are likely to have parents who  are divorced, separated, alcoholic, or in gaol, or to have been themselves in care or in a special school.
In one survey, 83 per cent of the younger girls never, used contraception or used it sporadically.
Only 51 per cent of the girls over 16 had attempted to avoid pregnancy; others thought that they could not become pregnant because they were too young or had sex too infrequently.
Although just over half of a sample of teenage mothers in 1979 — 80 had had some kind of family planning lesson at school, only a quarter had attempted to use contraception around the time they conceived.
Others replied: ‘it just happened’, ‘it didn't cross my mind’, ‘you don't think you are going to get caught’.
Only 30 per cent considered having an abortion.
Teenage mothers often copy their parents' experience (Madge 1988), especially ‘conventional’ teenage mothers of legitimate births who conceive within marriage (Kiernan 1980b).
Parents of teenage mothers usually married young themselves, had primary education only, were mostly in manual occupations, and came from large families.
The teenage mothers like their parents had little ambition and worse prospects for anything except further childbearing (Cartwright 1978).
Teenagers who conceived pre-maritally were five times as likely as the average to have been conceived pre-maritally themselves.
More than half teenage marriages are likely to end in divorce by twenty-five years' duration at risks current in 1979–82 (Haskey 1984).
Teenage marriage and pregnancy have comprehensively gloomy outcomes.
Their discouragement should be a prime aim of public policy.
Psychological aspects
Many births are unambiguously described as ‘unwanted’ by parents in response to survey questions.
It is a considerable puzzle to know why they are still so common.
Several studies on the differences between those who do and do not effectively plan their fertility emphasize the importance of belief in the possibility of control, and its connection with material or occupational circumstances (e.g. Askham's 1975 study in Aberdeen).
Unfavourable circumstances encouraged strong orientation towards the present, not the future: a feeling of lack of control over events and a tendency to accept them passively.
In a national survey, ‘forward planners’ in all classes had lower fertility (and family size clustered more tightly around an average of two children), and were more likely to use effective methods of contraception.
Considerably more women in the higher social classes had a ‘forward planning’ outlook which assumed control over their futures (Dunnell 1979).
These differences in attitude and personality are crucial for control of fertility.
It is not known if they are acquired through upbringing or  inheritance or acquired later through experience.
Their effects on life chances in general, not just fertility, are a prominent feature of classic sociological accounts of the British class system (Klein 1965).
The most important question seems to be how far individuals believe they are in control of their own environment and can control what happens to them.
Such problems are studied by psychologists under the general rubric of ‘locus of control’: whether control of life's events lies outside the individual or within his power.
‘Internal’ locus of control is associated with higher education and income, occupational position, and success.
Although the connection between these psychological ideas and sociological observations is highly suggestive, it has only occasionally been studied in the context of fertility and marriage (see Coleman 1990).
Other psychological attributes with a bearing on fertility trends, concerned more with ideas than with personal adequacy, are discussed below.
The ‘perfect contraceptive’ population
It has been estimated that in the UK in 1981 about 54 per cent of married women aged 15–44 wanted no more births.
About 42 per cent of unwanted pregnancies resulted in live births and there were about 15 unwanted births per year per 1,000 married women who wanted no further births; almost 10 per cent of the marital fertility rate.
A higher but unknown proportion of the 28 per cent of births now outside marriage are also likely to be unwanted.
A so-called ‘perfect contraceptive’ birth-rate, perhaps 15 per cent lower than now, and lower population growth would result if all births were wanted.
A wider use of existing methods of contraception among non-users — and a reduction in the use of the IUD, which has a high failure rate — would pull down the abortion rate 46 per cent.
Even if non-users started to use the least effective contraceptive methods the abortion rate could fall by 30 per cent(Westoff, Hammerslough, and Paul 1987).
Perfect contraceptive practice would be completely compatible with population growth, not decline, if desired family size remained at about its present level and was actually realized in achieved family size.
Marriage and illegitimacy
Over 70 per cent of births still occur within marriage, so the popularity of marriage and its timing remain important in the study of subsequent fertility.
For this reason, and because it is the basis of most households, marriage is considered separately in Chapter 5.
Where average family size is only about two children, it is perfectly possible for women to   delay marriage quite late — until their mid-30s — and still complete a family at or even above the average size.
But in general later marriages still produce fewer children (table 4.6).
Trends in marriage
In the late 1930s a new pattern of earlier marriage began to emerge which had not been seen in the West for centuries.
Hitherto, marriage in Britain had been late (average age at first marriage about 25 for women, 28 for men).
Up to 15 per cent of women and 10 per cent of   men avoided it altogether.
The advance in age at marriage, and in its popularity, accounted for about 30 per cent of the increase in annual fertility rates from the 1930s to the 1950s which comprised the ‘baby boom’.
As a later chapter will emphasize, birth-rates do not ‘blindly’ follow marriage.
The desire for children may determine the timing of marriage, rather than the desire for marriage determining the arrival of children.
Marriage is often a sign that a couple is prepared to accept children, so fertility intentions drive marriage rates as well as the other way round.
Illegitimacy
Over one in four births now occur outside marriage.
This is quite new.
Illegitimacy was relatively rare until the 1960s.
Illegitimate births declined from mid-Victorian times in most countries in Western Europe, and except for huge wartime increases (see Registrar-General 1954), remained relatively low until the 1950s (figure 4.6).
In the past, delayed marriage and low illegitimacy both followed the inhibition of sexual behaviour by strong social controls, including religion.
Then, pre-marital sexual intercourse tended to occur during courtship shortly before marriage.
Even if marriage was not intended, the almost  inevitable pregnancy would precipitate it.
Therefore, the later the marriage, and its preliminary courtship, the lower the rate of illegitimate births in relation to the larger number of unmarried women (Crafts 1982).
In the 1930s and the 1950s over 96 per cent of births were legitimate, even more than in the ‘good old days’ of Victorian Britain — and without its high levels of infanticide and unregistered births.
In retrospect this period seems to be the ‘golden age’ of respectable behaviour.
Illegitimacy started to increase again in the 1960s; later marriage since 1972 has been accompanied by higher, not lower, levels of illegitimacy.
As unwanted pregnancies can so easily be avoided by abortion and contraception, why is illegitimacy now so high?
There are still important  constraints on access and knowledge.
Abortion is still much easier in some parts of the country than others.
In 1987, 31 per cent of conceptions outside marriage in the North-West were terminated by legal abortion, compared with 45 per cent in the South-East.
Contraceptive techniques are still far from ideal, especially for romantic, ignorant, or embarrassed youth.
Sexually experienced young people often use effective contraception — 80 per cent of single women using contraception are on the pill.
But younger, inexperienced adolescents may not.
Effective sex education needs to be attuned to the realities of sexual behaviour when the average age at first intercourse is now between 16 and 17 (Mant et al .
1988).
The contrast between Sweden and the USA is instructive.
In Sweden intensive and early sex education is geared to family planning.
In the USA sex education is forbidden in many schools, despite the permissive attitudes of the society outside.
In Sweden, the ratio of abortions to births among teenagers is high but teenage conceptions and births are low and declining.
In the USA teenage births are the highest in the Western world despite a relatively high abortion ratio: 347 per 1,000 live births; 696 for girls aged 15–19 (National Center for Health Statistics 1989).
Even so, in England and Wales most illegitimate babies are born to women in their 20s, not to teenagers: in 1988, 44,642 illegitimate babies were born to teenagers, 68,151 to women aged 20–4, 38,168 to women aged 25–9, 26,391 to older women: overall (177,352) three times the number in 1978.
It is unmarried women in their late 20s who have the highest risk of illegitimate birth.
Accidental and unwanted births are a high proportion of this total.
But the picture is changing.
In the early 1970s the normal response to a pre-marital conception was marriage; abortion or illegitimate birth were less favoured alternatives.
In the 1980s abortion or illegitimate birth have been the most likely outcomes.
Most of the increase in illegitimate births has been to women who are   in some kind of informal union, not living on their own holding the baby.
This is inferred from the increase in the joint registration of illegitimate births by parents who give the same address, published since 1983.
Couples who register their births together are likely to be living together (Werner 1982, Haskey and Coleman 1986).
About 68 per cent of illegitimate births were jointly registered in the names of both parents in 1987 compared to 53 per cent in 1977, and in 1987, 70 per cent of such parents gave the same address.
Between 1977 and 1987 illegitimate births increased from 55,400 to 158,400; 76 per cent of this increase was in jointly registered births.
The increase is particularly striking among younger women (figure 4.7).
Eight per cent of women aged 18 — 49 were cohabiting in 1988, and over 20 per cent of those had never been married (see Chapter 5).
Cohabitation before marriage is becoming normal: 50 per cent before first marriage, 58 per cent before all marriages (Haskey and Kiernan 1989).
Partly because it is not routinely recorded, relatively little is known about cohabitation and the extent to which it may become an ordinary setting for childbearing before or instead of marriage.
It is discussed further in Chapter 5.
4.3 Explaining Fertility Trends: The Underlying Causes
It is easy to see how fertility has been kept low since the middle of the twentieth century, and how the control over its timing given by contraception could permit a new pattern of fertility fluctuation.
But the  means do not explain the motivation.
Two major problems dominate further enquiry into today's fertility patterns and trends in Britain and the whole industrial world.
First: why have there been such strong fluctuations in fertility in the last half-century, and are they likely to continue?
Second and more fundamental: is there nowadays any normal average level of fertility to which post-industrial societies are tending?
Is a two-child average here to stay?
Might the current TFR of 1.8, or even less, become the norm?
Or even no children at all?
At the moment there are no complete answers to these questions.
In modern society there are no economic advantages for the individual in having children, and the personal compensations may be less attractive than they were.
The apparent rise of voluntary childlessness suggests that many have reached this conclusion.
Prediction is a good scientific test of any quantified theory.
Since the Second World War, most theories of fertility have failed it (see Ryder 1979).
Classical Malthusian ideas
These questions are common to all developed countries.
Let us look first at the easier one — explaining variations in fertility.
Fertility used to march in step with the economy.
When real wages rose or unemployment fell, marriage and then fertility increased, and vice versa, roughly as Malthus claimed it would.
This seems to work both in short-term fluctuations and in long-term trends (Habakkuk 1971), although with an awkward time lag in the latter (Wrigley and Schofield 1981).
This connection between economic performance, marriage, and fertility, perhaps the best known empirical association in social science, then worked through the ‘valve’ of marriage: delaying marriage or avoiding it was the only practical way of controlling births.
When times were good, men married earlier and some were able to marry who otherwise could not have married at all— and vice versa .
This system depends on most fertility being marital fertility and on marriage being late, avoidable, and variable in age — until recently the unique property of Western European societies.
Well into the twentieth century there was a simple correlation between economic indicators, such as unemployment, and marriage rates (Glass 1938, Galbraith and Thomas 1941, Kirk 1942), even though by 1870 the birth-rate had begun its decline within marriage.
The great increase in the popularity of marriage and the baby boom which it partly caused accompanied the post-war economic growth of the 1950s.
This may be regarded as the last fling of this relationship.
The British and most other Western economies sustained acceptable  growth rates in their economies and real wages into the 1970s (see figure 4.2) and beyond.
Yet fertility throughout Europe and the industrial world began its post-war decline from the mid-60s.
By the time Western economies suffered the severe shock of the quadrupling of oil prices in 1973, and subsequent further increases in 1979, the baby boom was already long over, although marriage continued to become more popular until 1972.
In the UK the depression of the early 1980s was mainly a failure of growth of individual incomes of those who remained in employment.
Incomes are ‘sticky downwards’; most of the decline in income has been borne by the unemployed.
But fertility has not fallen accordingly — its nadir was in 1977.
Since 1982 real incomes in the UK have been growing fast, about 4 — 5 per cent per year, and from 1986 to 1989 unemployment fell.
None of this has produced a rise in fertility.
Economic facts since the 1960s no longer fit the demographic trends expected from the traditional theory, which predicts a positive relationship between birth-rates and the performance of the economy.
Cohort size and population cycles
Conventional ideas about prosperity and fertility fit the upward slope of the baby boom but not its decline.
Many new circumstances might account for the new relationship — the movement of married women into the work-force, unprecedented economic security, the decline of religious feeling.
These new circumstances require a new model, but there is still disagreement on its form.
The American economist and demographer, Richard Easterlin, has suggested (1961, 1968, 1980) that population and economy are linked in a perpetual series of fluctuations, powered by income expectations, alternations in cohort size, and autonomous cycles in the economy itself.
There is nothing new in the idea of population cycles, or in the suggestion that they may influence the business cycle (Losch 1937).
Easterlin's ideas emphasize the importance of cohort size on life chances.
His model consists of a set of hypotheses.
First, that economic aspirations of cohorts of adults are fixed in adolescence through their experience of the household in which they were brought up.
Second, that fertility of these cohorts depends on the extent to which they can match these aspirations in their subsequent adult life, any shortfall in ‘relative income’ being met with responses to limit fertility through deferred marriage or smaller family size.
Third, that the economic opportunities available to large and small cohorts of births are quite different.
Members of larger cohorts experience fiercer competition throughout their lives for places in schools, university, employment, and  promotion.
Their incomes and standards of living relative (with allowances for rising expectations) to those of their parents will seem unsatisfactory.
To preserve expected standards as far as possible, marriage will be delayed and family size kept small.
Wage growth will be low, unemployment high, with consequent feedback effects on the economy.
But their children, born into small cohorts, will themselves experience comparatively favourable opportunities for promotion, recruitment, and wage increase.
Their income relative to their parents will be correspondingly higher, their fertility will be high, and so the cycle will continue.
As a result of these disadvantages, members of large cohorts are also claimed to suffer — and generate — higher crime rates and other indicators of social discontent (Easterlin 1978).
The last two components of the model are classical Malthusian approaches, applied to cohorts rather than to whole populations.
In the USA, income in the largest post-war cohorts is indeed about 15 per cent less than in smaller cohorts (Easterlin 1980) and their scholastic education scores have been lower.
In part, the model is excellent.
But it is difficult to get numerical estimates for ‘relative income’, a concept much attacked by critics.
Relative cohort size (the ratio of men 35–64 to those below 34) may be used (Easterlin and Conron 1976), to see if high ratios are associated, as expected, with high birth rates.
The model fits variations in US fertility trends reasonably well, although not necessarily much better than conventional economic variables (D. P. Smith 1981).
It fails to predict the downturn in fertility in the UK and in West Germany (Ermisch 1979, Eversley and Köllmann 1982).
Furthermore, cohort studies in Britain show that the relation between a woman's family size and the family size in which she is brought up is positive, although not very strong: childless women are more likely to be single children themselves, mothers of large families tend to come from larger than average families themselves, even when other influences are controlled for statistically (Kiernan 1989a).
Women's work and the ‘New Home Economics'
The problems endured and created by large cohorts seem considerable and there is no reason why Easterlin effects should not be recognized and incorporated into other models.
But a more promising line of enquiry emphasizes the demographic importance of working wives, which Easterlin's original model neglects (Oppenheimer 1976).
There are now two kinds of household: a minority where the wife does not work, and may have no intention of returning to work, to which the old expectations apply, and a rising majority of households with   working wives for which the economics of childbearing have been reversed.
For most women, working and childbearing are alternatives, at least for a few years.
When a wife's income forms an important part of the family income, rising wage levels make children a less attractive option.
The movement into the work-force by married women has been one of the most startling and far-reaching social changes since the Second World War.
In 1911 just 9 per cent of married women worked, by 1966 38 per cent, and in 198866 per cent of married women aged 16–59 were working (figure 4.8).
At recent rates about 80 per cent of married women can expect to work at some time in their married lifetime (Martin and Roberts 1984).
By contrast, the proportion of single women and of divorced women in the work-force has remained more constant over that time; about 70 and 55 per cent respectively.
In recent years an increasing proportion of married women even with pre-school children have taken on part-time work.
This matters because married women who work have fewer children than married women who do not (Dunnell 1979, Jones 1981).
Employed married women tend to delay their first and second child, compress their childbearing into their late 20s and the second five years of their married life, and avoid third and subsequent births (compared to housewives)(ni Bhrölchain 1986b).
This revolution in the labour force has affected almost all industrial  economies.
It has contributed much more to labour force growth than immigration, and will be the main source of its growth for the rest of this century (DE 1987).
In Eastern European countries, where labour productivity is chronically low, women's labour force participation is correspondingly even higher — 89 per cent in the Soviet Union.
It is not hard to find reasons why married women now go out to work.
In fact it is more of a puzzle to work out why it took so many decades to become established (Hatton 1986).
In part, it is a consequence of the completion of the demographic transition.
Since the 1920s women have been free of the burden of high fertility which previously would have given them young children to cope with for more than two decades of their adult life.
Two World Wars have helped to break traditional restrictions and change attitudes and laws concerning the work that women could do.
So has the parallel progress of women's education, giving them at least in theory the same job opportunities as men.
Recent legislation in most industrial countries has helped to turn this into reality.
In Britain the Equal Pay Act became law in 1970.
In 1984 it had to be amended further to follow the Euro-notion of ‘equal pay for work of equal value’ whereby the pay of women occupying jobs without immediate male comparison is set equal to men's through the opinion of a judge rather than the operation of market forces.
The demand for labour from the 1950s and, later, rising relative wages compared to men has tempted many married women to stay in the work-force or to return to it.
The shake-out of labour after 1979 affected women's employment less than men's, both here and abroad.
The advantages of work appear to keep women in the work-force even when circumstances subsequently become less favourable (Joshi and Owen 1985).
High divorce rates (and public awareness of them) encourage women to preserve their independence both financially and socially (Willmot 1976, Ermisch 1981).
These changes in society seem irreversible, especially when the household's standard of living comes to depend more on the wife's income in dual earner households (60 per cent of all households with two adults under retirement age in 1987, compared with 52 per cent in 1973).
The ‘New Home Economics’ model relates family finance and decision making to individual behaviour, especially family formation and employment.
It represents a revival of interest in the factors of the family budget and their effect upon life chances such as cohabitation, marriage, childbearing, and divorce (see Becker 1981, Willis 1973).
In the past, other things being equal, improvement in a man's income removed obstacles to marriage.
With the rise of family limitation, it would also remove the need to delay children within marriage.
But the relation of fertility to women's employment and income is quite  different.
Babies deprive women of the opportunity to earn money through the expanding opportunities for full- and part-time work now available.
These market and legal changes have imposed an ‘opportunity cost’ on childbearing which previously did not exist.
The cost in foregone earnings of bearing two children has been estimated at £119,000 if they are born when the mother is in her early 20s to £121,000 if they are born when the mother is in her mid-30s (Joshi 1987a).
Other calculations based on similar data yield an estimate of £135,000 (Roll 1986).
Discounting the cost at present value, to give a more realistic estimate of the loss at the time decisions are made, emphasizes the importance of deferring births: earlier childbearing leads to a loss of £84,000; having the two children later loses £62,000 (quite apart from the career implications).
These estimates assume eight years out of work altogether and twelve years' part-time work subsequently.
Lost hours of work, and lower rates of pay from missed experience, contribute approximately equally to loss of income (Joshi 1987a).
So if the overriding aim is to minimize loss of earnings, it is economically rational for working women to delay or avoid marriage when it is no longer necessary for their financial security (Ermisch 1981).
On economic grounds it is also prudent for both husband and wife to delay and limit childbearing within marriage, and compress the period devoted to childbearing.
For many couples children clearly remain the first consideration.
In 1987, 36 per cent of married women of working age were not at work.
But enough couples respond to economic motivations for the theory to describe ‘average’ behaviour quite well since the 1960s (ni Bhrölchain 1986b).
Data from other countries point to a similar penalty on childbearing.
In the USA in the 1970s a middle-class child raised through four years at college cost $74,000 and $145,000 if the wife's opportunity costs were included as well(Espenshade 1980).
These US estimates tend to emphasize direct rather than opportunity costs, partly reflecting the lower level of welfare support especially for the higher education of children.
The opportunity costs of two children to a white mother in the 1970s have been estimated at $50,000 (Calhoun and Espenshade 1988).
The correlation between fertility trends and relative wages and work-force participation works well over time within particular countries.
But there is considerable variation between countries and the differences do not correlate well with current fertility.
For example, West Germany and the Netherlands both have lower female work-force participation than Britain but also have considerably lower fertility; while Japan, with a period fertility level about the same as Britain's has a relatively low work-force participation by married women.
Such a model assumes that the fertility in families with and without employed wives responds to wage changes quite differently.
The crucial factors are the relative opportunity costs and the ratio of men's to women's earnings.
These increases in women's relative wage rates, from about 65 per cent of men's in the 1960s to 75 per cent by 1975, can be shown to have substantially reduced the number of births during the 1970s — by 130,000 — and made their timing more volatile (de Cooman, Ermisch, and Joshi 1987, 1988).
However the effect is not always negative.
The anticipation of future employment by women may bring births forward.
In the 1950s and 1960s the movement of women into the work-force, or its anticipation, appears to have accelerated the pace of childbearing temporarily, especially of first and second births, reinforcing the positive economic effect through men's income (ni Bhrölchain 1986a, 1986b).
When applied to British fertility trends, the correlation (elasticity) between fertility and women's real wages increases over time because of the rise of the real wages of women and their increasing tendency to work.
The model correctly predicts a downturn in fertility around 1964.
Further change reinforced by acceleration of women's real wage growth led to a further decline after 1970.
The effect of the sharp rise of the ratio of women's to men's wages in 1972 is evident in the graph (Ermisch 1979).
Housing may also be an important component as its relative cost increased in Britain from 1955 to 1975.
In the public sector, scarcity is more important than price.
In the 1950s and 1960s supply improved as a result of policies to build large numbers of council houses, especially in Scotland and major cities in the North of England.
Since 1979, public sector housing has been severely curtailed.
It is discussed further below.
All this has upset the applecart of the relation of fertility to prosperity.
It may even have reversed it (Butz and Ward 1979) so that the short-term effects of economic growth in modern societies may be further to reduce fertility, not increase it.
As wage rates rise, more women will enter the work-force, so the opportunity costs of child-bearing will increase and fertility decline — and vice versa.
But in the relatively short-run experience so far, women have not in fact left the work-force when conditions have been less favourable.
Forecasting fertility
These changes are so recent that it is difficult to know if we are beginning a series of cyclical changes or a once-for-all transition to a new, low fertility regime.
The latter seems more likely.
Mass movement of  women into the work-force dates only from the Second World War — especially from 1960 — and the improvement of women's wage rates relative to men's happened only since 1970.
Women have been attracted into the work-force by wages.
They have been enabled to enter the work-force by their prior control over fertility.
There are many reasons for supposing that the small family is here to stay.
Families may become dependent for their living standards upon two incomes.
This dependence may increase, not decline, if wage rates deteriorate in real terms.
Unemployment may force the husband, not the wife, out of work.
High fluctuations in fertility, characteristic of the period from the 1920s to the 1960s, are therefore likely to be a thing of the past.
On this analysis, the 1950s-60s fertility boom can be regarded as a once-for-all anachronism.
Established birth control and resulting low fertility permitted a unique advance in marriage at a time when married women were not yet committed to the work-force.
That permitted a last Malthusian response to the then unprecedented economic growth and prosperity which followed the Second World War.
A relaxed attitude to family size and the timing of births then cost little.
But the economic trends which helped created the baby boom have subsequently ensured that it would have no successors, by drawing married women permanently into the work-force.
Forecasting future fertility is the most difficult task in demography, and seldom successfully attempted.
The econometric models described above can derive current fertility from current socio-economic indicators.
Whether they can predict the future remains to be seen, Short-range time-series projections of fertility are reported to be promising (de Cooman, Ermisch, and Joshi 1988).
But it is notoriously difficult to predict the economic trends by which the models estimate fertility.
Other factors may also be important: the mild upturn in fertility and its rapid failure evident after 1977 were forecast on other grounds: the impossibility of deferring children indefinitely; the new contribution to fertility from remarriages.
None the less, some things do seem clear: the unlikelihood of more baby booms, except for weaker echoes of the present one, and a generally lower level of fertility.
The balance of influence between husband's and wife's income, in aggregate, keeps fertility low.
But each has different effects at different ages so the balance is sensitive to changes in age structure (de Cooman, Ermisch, and Joshi 1987, 1988).
Analysis of current fertility by parity, and the responses to attitude surveys, have persuaded official statisticians and some demographers (OPCS 1989, Shaw 1989, Brass 1989) that fertility in Britain is likely to return to a higher, if not replacement level.
At what level, if any, will it stabilize?
Is there any reason to suppose  that individual decisions on family size will result in an average which happens to be the replacement rate?
The models described above relate economic change to fertility change.
They do not tell us why the level is set at its current average rather than some much higher or lower figure.
There are good family economic reasons for believing that fertility will not become high.
There are none which tell us why it should not become zero.
For that we must rely more on the intentions and attitudes of potential parents, and the importance of children for parents, and cast our ideas rather wider, beyond economic trends and family finance.
Fertility, intentions, and ideology
Parents in modern society seldom receive material benefits from their children and do not, except in Japan, usually live with their children in old age.
Instead, they receive benefits from their own parents.
But as family size may not be tending to zero, it is evident that income and material benefits are not the only considerations in determining family size.
The difficulty of accounting for the historical demographic transition by conventional economic measures (Cleland and Wilson 1987), and the fall of fertility even in some Third World countries with little economic progress today (Cleland 1985), has revived interest in non-material motivations for childbearing.
Children can be regarded as a ‘consumption good’ bringing non-material satisfactions.
The belief that they do so, and compete for household spending with the costs of other ‘consumer durables’ until an acceptable living standard is achieved, is one of the underlying assumptions of the ‘New Home Economics’approach to the economic analysis of family size variation, although one which has been strongly attacked (Blake 1968).
The analysis of these non-material motivations, to explain why family size is not zero in modern societies, is one of the most difficult problems confronting demography.
Prestige, honour, loyalty to norms of society or religion, or apathy, may transcend material calculations, especially when the chances of improving material comforts are weak or unimaginable.
Such considerations are far from the traditional pre-occupations of demography.
But if they can be measured they may help us relate fertility decisions to the values, meanings, or signs which people use to make sense of the world and which motivate or justify their actions.
Childbearing can be regarded as a confirmation that the parents feel at one with the nation and culture in which they live (Simons 1986a).
Statements from couples concerning family size and childbearing often have a strong normative or quasi-religious quality stressing niceness, decency, responsibility, and so on(Busfield and Paddon 1977, Askham 1975).
The lack of reaffirmation of their society thought to be implicit in the low fertility current in the Federal Republic of Germany is worrying some members of that society, quite apart from the obvious material consequences.
British attitudes to fertility, judged by actual behaviour in relation to fertility and marriage, and by the values stated in surveys, seem more conservative (Simons 1986b).
In some societies (e.g. Hutterites and other fundamentalists, Shi'ite Islam and the Wahhabi school of Sunni Islam, Puritan sects, Mormons, the Irish form of ultramontane Catholicism) reproductive behaviour has a ‘sacred’ role, whatever the cost, manifested either as an obligation to have as many children as possible or to avoid the means of limiting them (Simons 1982).
Some ethnic minorities emphasize the duty of procreation and in-marriage to preserve their society from assimilation, alongside a generally rigid control over behaviour (Siegel 1970).
Most religions take a more pragmatic attitude (e.g. French Catholicism, the Church of England and other mainline Christian Churches, and less fundamentalist (Sunni, Ismaeli) Islamic sects).
These admit various ways of accommodating the sacred to the profane; i.e. behaving more or less like a rational economic man.
This contrast between ‘fundamentalist’ and ‘pragmatic’attitudes is not specifically religious but can be generalized to describe the range of attitudes within a society and the direction in which they are changing.
According to this notion, fertility decline since the war can be interpreted partly in terms of concomitant swings in public attitude, irrespective of formal religious affiliation, between the fundamentalist and the pragmatic pole (Simons 1982), On this view, by the end of the 1960s parents raised under the more fundamental, disciplined 1930s had been replaced in the childbearing years by those brought up with more material security and freedom, both moral and legal, from traditional restrictions.
The trends in number of Easter Communicants, of conversion to the Roman Catholic Church, or the National Secular Society, are regarded as indicators of a switching of ideological allegiance.
There is some similarity in these trends with those of fertility from 1900 to 1975.
This model has much in common with the dichotomy between ‘post-material’ versus ‘traditional’attitudes much debated on the Continent (van de Kaa 1987).
‘Post-material’ attitudes emphasize egalitarian, non-material hedonistic attitudes which welcome novelty and ignore the past, contrasting with more ‘bourgeois’values which emphasize the  importance of family, country, traditions, security, duty, and altruism.
The former values tolerate childlessness, cohabitation, and liberal sexual attitudes.
They tend to depose the ‘child king’(Aries 1983) and lead to a weaker level of ‘familism’: that is, supportive feelings towards marriage and childbearing (Lesthaeghe and Meekers 1986, Lesthaeghe and Surkyn 1988).
International differences in these attitudes can be correlated with differences in the birth-rate.
The high fertility countries (UK, France, Ireland) tend to be more conservative or nationalistic compared to their lower-fertility neighbours (Simons 1986b).
It must be said that continental discussions of some of these ideas read rather strangely, at least to the authors of this book.
They present as a major force in society attitudes and tastes which would be regarded as rather eccentric and marginal in Britain.
One of the most important unsolved questions here is the extent to which changes in ideas and attitudes are really autonomous.
For example, responses to questions on attitudes change with economic conditions — the worse the inflation, the more traditional the responses (van de Kaa 1987).
Ideas and values seem to be rather fragile props for the permanence of any particular level of fertility.
Other societies have adopted, or are adopting, different behaviour and values; we may do the same.
Voluntary childlessness
Perhaps 5 per cent of married couples choose to be childless.
There has been much speculation whether this proportion is likely to grow substantially in industrial societies.
The trend in successive cohorts suggests it is.
The proportion of women still childless by age 25–9 doubled from 20 per cent in 1961 to 40 per cent in 1981.
In Britain 34 per cent of women of that age were childless in 1964, and 47 per cent in 1984; levels not seen since early this century.
Many of these women may still start families in their late 20s or 30s.
Only 10 per cent of the 1940–4 birth cohort of women (now aged 45–50) remained childless; this figure is expected to rise to at least 15 per cent and possibly 20 per cent for the 1960–4 birth cohort (Werner 1986, OPCS 1989a, figure 4.9).
Such high levels of childlessness do not fit the consistent response to questions in the General Household Survey that only about 5–7 per cent of women intend to remain childless; but up to 20 per cent of women have failed to reply to this question, although fewer in more recent surveys.
This uncertainty makes it particularly interesting to know whether intentionally childless women are different in personality or attitudes from those who wish to have children, and whether these attitudes could readily become more widespread.
The research on this subject so far gives a mixed answer (Campbell 1985, Baum and Cope 1980, Kiernan    1989a).
Childlessness increased strongly with age at first marriage, even at ages where fecundity is usually still high.
Motives for remaining childless were mixed: hedonistic, ideological, and medical.
No particular personality type seemed to predominate.
The intentionally childless married woman was likely to be well educated, employed in a high status occupation, married to a husband in a professional or managerial job.
Childless men, especially those with a broken marriage, were more likely to be ambitious, highly educated professionals.
While most of those who were delaying children cited economic reasons first (70 per cent), only 41 per cent of those who intended to remain childless mentioned the costs.
The most important consideration was freedom of action; an understandable response, as childbearing may be regarded as equivalent to a fifteen-year sentence of partial house arrest, without remission for good behaviour.
So far these problems have hardly been explored from the viewpoint of psychology or sociobiology.
The interpersonal dynamics of a two-child family, especially two children of opposite sexes, may be especially rewarding for parents and insufficiently increased by the prospect of a third child to warrant further disruption to domestic economy.
Biological explanations of fertility based on principles of maximizing a broadly defined reproductive success (Wilson 1975) have been useful in non-human species and have been applied to human populations with natural fertility (Borgerhoff-Mulder 1987), in an attempt to relate reproductive success with the resources available to women.
But such models seem to have little to offer in the elucidation of the behaviour of complex societies practising family limitation.
If there is any long-term tendency to produce a family size of about two, then the explanation may lie in social psychology or sociobiology rather than in the realm of economics.
4.4 Family Formation
To make sense of the trends in overall fertility, and to attempt to answer questions about future family size, we need to know more of the ‘components’ of the changes in annual births.
Do they depend upon changes in the popularity of families of different sizes (‘quantum’) or the pace at which parents produce their children (‘tempo’)?
Has fertility changed mostly because successive cohorts each grow up with their own characteristic attitude to childbearing (‘cohort’ effects), or do families respond in a more opportunistic way to the economic and social opportunities or problems of the moment (‘period’effects)?
With enough data, fertility trends can be separated into the  components due to period and to cohort effects (Hobcraft, Menken, and Preston 1982, Ryder 1980).
In general, period effects seem to be more important than used to be.supposed in determining fertility (Brass 1974).
Almost 60 per cent of the annual fertility rate change in the upturn and in the downturn of the US baby boom were due to tempo changes rather than changes in final family size (Ryder 1980).
Final family size is strongly affected by the timing of the first birth (Brass 1974).
Women who start having babies early end up with more babies than average, and this is the single most important statistical determinant of final family size (Kiernan 1987).
For example, in the 1946 birth cohort, women who had their first child at age 15–19 went on to produce an average of 2.8 by age 36, compared to 1.9 children in all for mothers who had their first child between ages 28–30.
So the socio-economic factors affecting the timing of the first birth (see Thompson 1980) are important determinants of future fertility.
Fertility and age at maternity
In Victorian times, women frequently continued producing their large families into their 30s and even early 40s.
Average age at last birth is about 40 years in populations not practising birth control.
The fall in family size made childbearing a more youthful activity and the reduction in age at marriage from the late 1930s reduced the average age at first birth even further so that it reached a minimum of 23.8 from 1968 to 1970.
Earlier marriage meant a great increase in teenage births in the 1960s, up to the time when average age at marriage increased (after 1972), when legitimate births to teenagers began to fall as well.
Women giving birth at over age 30 began to be regarded by the medical profession as something of a curiosity.
When marriage trends went into reverse from 1972, so did the pattern of fertility by age.
Childbearing after age 40 is now almost extinct (1.31 per cent of births in 1988).
But the 30s have become once again more popular for motherhood (figure 4.10).
Several factors have encouraged the fertility of women over age 30.
Most married women now work.
Professional women in particular need to establish their career before having children; a disproportionate number of mothers aged over 30 are in social class I or II.
One in four marriages of women are remarriages, and these women (and their new husbands) often wish to start a family of their own.
In 1988 only 8 per cent of legitimate children were born to remarried women, but they contributed 13 per cent of births to women aged 30–4 and 22 per cent of births to women aged 35–9.
As well as marrying later, women have been starting their families later in marriage.
Family size has been ‘squeezed’ from both extremes of the age range: at older ages as a continuation of long-term trends away from large families; and more recently at younger ages as women have married, and started families, later in life.
From the 1930s to about 1970 the median interval from marriage to first birth remained 20 months.
In 1978 it had risen to 31 months, from which it has now declined a little, to 27 months in 1986 (see Werner 1988).
The result has been a compression of childbearing into ages 22–30 to a degree never seen before; about two-thirds of all births (figure 4.10).
Couples married in the 1930s took six years from the birth of their first child to the birth of their second, and almost ten years from the birth of their first to the birth of their third.
Couples married in the early 1950s produced the same family sizes in 4 years and 7.4 years respectively (table 4.7).
Consequently, childbearing is concentrated within the first decade of married life (84 per cent of births within marriage in 1956–60, 92 per cent in 1976), and especially in the second half of the decade — 33 per cent of births occurred in the fifth to the ninth years of marriage in 1976 compared to 26 per cent in 1951–5 (figure 4.11).
The most popular time for a first child is the third year of marriage (41 per cent).
Only 5    per cent of women want a child within the first year, and 12 per cent prefer to wait until after the fifth year of marriage.
So there is still scope for further delay.
Pre-marital conceptions
Births in the first seven months of marriage are assumed to be pre-maritally conceived.
Pre-marital conception is nothing new.
Perhaps a third of first births in Victorian times were so conceived.
In 1967, 26 per cent of first births were premaritally conceived.
By 1976 this had fallen to 16 per cent where it remained without significant change up to 1986.
Conceptions which in the 1960s might have precipitated a ‘forced’ marriage now either lead to abortion or, increasingly, to births in non-marital unions, concerning which more will be said later.
More babies conceived pre-maritally are also being born to remarried women.
Looking now at all legitimate births, not just first births, remarried women contributed 7 per cent of the 40,231 pre-maritally conceived births in 1976, 18 per cent of the 41,250 in 1986.
Over the last twenty years, trends in pre-marital conceptions, and the fate of those conceptions, have been complex.
In the 1960s the number of pre-maritally conceived live births increased — not surprisingly at a time when sexual inhibitions were being cast off (Bone 1986).
From 1964 to their peak in 1968 pre-marital conceptions increased 10 per cent(to 74,531) to comprise more than 10 per cent of legitimate births, even though total births were declining.
The Abortion Act of 1967 changed that pattern and made it more useful to consider all the known conceptions outside marriage to observe their changing fate (figure 4.12).
In 1970 there were 185,900 conceptions outside marriage — 21.2 per cent of the total known.
Then the most likely outcome (40 per cent) was a legitimate birth following the marriage of the parents.
Twenty-four per cent were legally aborted, 36 per cent led to an illegitimate birth (OPCS 1984a).
Since then extra-marital conceptions have increased sharply while all conceptions, and all births, have fallen.
In 1987 40 per cent(337,100) of all conceptions occurred outside marriage.
The proportion of such conceptions ending in illegitimate births (especially jointly registered) has greatly increased; up from 36 per cent in 1977 to about half (51 per cent) in 1987.
Thirty-six per cent ended in legal abortion — less than in 1977.
Only a minority (12 per cent) ended in legitimate birth (Werner 1982) following the marriage of the parents (OPCS 1987b)— down from 25 per cent in 1975.
Pre-marital conception leads to larger than average family size, even when other factors are taken into consideration: 2.6 children compared with 2.3 in the 1946 cohort.
Parity and the distribution of family size
In the long run, the replacement or growth of population depends on the number of women who go on to have third or further babies.
There is therefore considerable interest in the decision to have a third child.
But final family size cannot be known until women are in their late 30s or well into their second decade of marriage.
Seventy-four per cent of women who were married in 1900–9 who reached two children went on to have a third or more.
By the marriages of 1925 — 9, this figure was halved.
Birth-rates by the number of previous children (parity) are not routinely available because the appropriate denominators — the number of women by age and number of previous children — are not known without appropriate questions in the census (as in 1911, 1951, 1961, and 1971).
But much can be inferred from statistics of births in any year to women who already have 0, 1, 2, etc. children (figure 4.13).
The number of births to married women each year according to their parity is known, but not for unmarried women.
The chances of going on to an additional baby from a given family size (‘parity progression ratios’) can be calculated from past data for women who have completed their families.
They can only be estimated, with difficulty, from current data, as described in an earlier section (see Brass 1989).
Second babies have been least affected by the squeeze on fertility — the two-child family is more popular than ever.
Between 1966 and 1976 34 per cent fewer babies were born.
Births to women with 4 or 5 previous children fell by a precipitous 77 per cent and even third and further births were down by almost half (49 per cent).
First births declined by less than average — 26 per cent, while second births fell least of all, just 19 per cent .
The large family becomes ever scarcer.
But once families are committed to at least one child and all the disruptions of career and household which inevitably follow, they are then inclined to have the second child that most expect, and are expected, to produce.
The modest increase in births from 1977 to the late 1980s (11 per cent overall) has favoured first births and births of higher order (parity).
Second births, having declined little, also recovered little, increasing by just 6 per cent.
But third births increased by almost a quarter, and higher order births by 18 per cent.
This indicates that some of the previous reduction was a slowing in tempo: a postponement, rather than an absolute decline, in family size.
Many women have been delaying births for years.
But births cannot be delayed forever — postponed births will eventually become cancellations.
Some women decided  to have their babies before they or their marriage became too old to begin or continue childbearing, as Eversley (1980) correctly forecast.
Remarried women contributed a third to that increase in births.
In 1977 13 per cent of births to women married once only were third births, but among remarried women 24 per cent of births were third births.
Because of the increase in low parity births to remarried women, these proportions had converged by 1987, to 15 per cent and 16 per cent respectively.
The high fertility of mothers born in the New Commonwealth also contributes particularly to higher order births (see Chapter 12).
These trends have concentrated family size on two children.
In 1938 66 per cent of births were first or second births and 80 per cent were first to third.
In the peak fertility years of 1961–5 the figures were very similar: 67 and 83 per cent respectively, even though 1938 was a period of ‘low’ fertility, 1961–5 of ‘high’fertility.
This shows the importance of timing of births in year-to-year marital fertility change rather than the final number.
But by 1977 the shift towards lower order births was much more apparent; 81 per cent were first or second births, 94 per cent including third births.
That year was the high point of the concentration of marital fertility on first and second births.
Despite the two-child family becoming the average this century, most women married since 1930 have not had two children: three-quarters had at least one child more or less than the average.
This was particularly true of marriages earlier in the century (Glass and Grebenik 1954).
Even in 1971, with one-child and large families more scarce, only 44 per cent of families were two-child families.
It is only now, in the late 1980s, that half of all completed families will be two-child families (table 4.8).
By the time they had reached age 36, with their fertility almost complete, 52 per cent of the mothers in the 1946 cohort of births had two children, 16 per cent just one, and 32 per cent three or more (Kiernan 1989c).
4.5 Socio-Economic Differentials and Trends in Fertility Historical Background
So far we have discussed average fertility, ignoring differences of social class, education, and race.
Social differentials in fertility which have emerged in the last century have powerful implications for population growth, social welfare, and political power.
Ethnic differences are so striking that they are described separately in Chapter 12, although there are a few references in the text below.
The censuses of 1911, 1951, 1961, and 1971 are basic sources on social class patterns of fertility.
More recently, information from the registration of births can be related to estimates of the social class distribution in the population derived from the General Household Survey and the Labour Force Survey.
Before the rise of the West European marriage pattern the aristocracy married earlier and therefore had more children than others.
But little is known about differentials within the commons.
In many (but not all) traditional societies outside Western Europe where family planning is not practised, for example Bangladesh (Stoeckel and Chowdhury 1980) higher status rural women have more children than others.
The underlying pattern in modern societies may be of this kind too (Becker 1981).
According to this view, the preponderance of large families among the poor and ill-educated only persists today because their demographic transition is not yet complete, their higher fertility being due to unwanted births from poor family planning.
Social class and family planning
The early adoption of family planning by some groups of society before others in the nineteenth century created a gradient of family size by social class whereby lower social groups had higher average family size.
This has come to be regarded as normal but it is now looking rather transient.
Before the First World War there were big differences between social classes in the proportions of those who planned their families (see Chapter 2).
But they were disappearing even before the    Second World War.
Family planning is now general throughout society, although there are marked social differences in the popularity of different methods (table 4.9).
Fertility is still not uniform.
There remain some class differences in the proportion who take a more casual or a more planned approach to family building, and in the reliability of the methods which they use, and therefore in the proportion of unintended pregnancies.
On top of that, there are class differences in desired, as well as in actual family size.
By the late 1960s 56 per cent of managerial and 52 per cent of other non-manual workers' wives then used reliable methods of contraception, while only 45 per cent of skilled manual workers' wives and 38 per cent of other manual workers' wives did so.
20 per cent of the wives of non-manual workers used less reliable methods (mostly withdrawal but also douching, the use of cap, or spermicide, only) compared with 26–30 per cent of the wives of manual workers.
Since the 1960s pill use has changed radically.
In 1967 only 13 per cent of the wives of unskilled workers used the pill.
By 1973 this proportion had risen to 43 per cent.
But over the same time the proportion of women in professional families using it remained steady.
So by the 1970s the most effective method was used more by working-class wives (47 per cent of wives of skilled manual workers, 41 per cent of unskilled workers) than by the wives of non-manual workers (32 per cent of professionals, 44 per cent of skilled non-manual workers)(Cartwright 1978).
By then many women in social class I (45 per cent) had given up the pill, compared to 39 per cent in class II and 28 per cent of the others.
They were more likely to be aware of, or be deterred by, the health risks discussed earlier.
Barrier methods such as the sheath came to be used more, not less, by wives with husbands in the professional and inter-mediate class (34 and 32 per cent) compared to skilled and unskilled manual (25–21 per cent ).
Of the other contraceptive methods, the cap has remained almost exclusively a middle-class contraceptive, withdrawal predominantly a working-class practice, although now little used.
The IUD and safe period are about equally unpopular in all classes.
In Britain, as elsewhere, sterilization is becoming the most important means of family limitation among all classes.
Male sterilization is more normal among middle-class couples; among working-class couples it is more the wives' responsibility (General Household Survey 1984).
Despite this growing equality of method of contraception, in the 1970s about 20 per cent of the wives of non-manual workers, 27 per cent of the wives of skilled manual workers, and 47 per cent of the wives of unskilled manual workers said their last pregnancy was unintended (Dunnell 1979).
More recent data of this type are not available, as Dunnell's was the most recent major survey of family formation.
Askham's study in Aberdeen (1975) showed that much of the high fertility of parents of larger than average families (four plus children) was unwanted.
The higher the social class (from V to IIIM) the more likely the extra children were to be described as being unwanted.
The parents of small families knew much more about sex, contraception, and family building and had talked much more about them before marriage.
The large families in class V were born of ignorance and fear of sex and contraception, and of reluctance to discuss them by parents.
Disapproval of contraception too, and not just ignorance, was a powerful factor.
Attitudes were different: high fertility families were less concerned for the future, had little faith in their ability to control their own life, and little ambition.
Material circumstances seemed to condition these responses.
High fertility families in social class V enjoyed much less financial security than the low fertility families in class III.
They had less secure and frequently changing employment, more marital problems, and less adequate accommodation.
The low fertility families in social class III had courted for a long time, decided carefully about marriage, moved house seldom, and tended to be upwardly mobile in their jobs.
The present pattern of family size by social class
For some decades now the lowest average fertility has been found, not at the top of the social scale, but among families where the husband is in social class IIIN (clerical workers, clerks, salesmen).
On each side of the class scale, fertility then rises.
Even in the census of 1951 average family size where the husband was in social class I was 1.57 children compared to 1.48 for social class II.
But 1961 social class II fertility was almost as high as that of class III, while that of IV and V — semi-skilled and unskilled workers — had fallen by a tenth or more.
By 1971 this J-shaped curve was long established (table 4.10), with fertility lowest in social class IIIN and still highest in social class V. As might be expected, fertility is related to social class based on women's own occupation in more linear fashion.
Average family size at age 36 among the 1946 birth cohort of women was 2.0 among women in social class I increasing to 2.7 among women in social class V. During the declining fertility of the 1970s, social classes I and V moved even closer together, as fertility of the latter fell faster (Pearce and Britton 1977, Werner 1985).
In the aggregate, social class differences in fertility are   diminishing.
In the 1946 birth cohort, their effects were mediated mostly through differences in timing of first birth, the significance of which was underlined above (Kiernan 1987).
It is important to remember that we are talking about averages; families of all sizes can be found in all social classes, but in different proportions.
The shape of families is determined by parents' decisions to start, space, and stop their childbearing.
Patterns of starting and spacing, and of the distribution of family size, differ more by social class than does the simple average.
Despite lower average fertility and a later start, fewer women in social class I and II end up childless than in class IV and V, and one-child families are least common in these two highest classes (figure 4.14).
Higher fertility in class I and II than in IIIN is therefore due to the relative rarity of families with just one child or none.
Family size in class I and II is concentrated on two- and three-child families more than in any other class.
The two-child ‘norm’ is a middle-class norm.
Families above three children, and less than two, are more frequent in the manual social classes, especially class V. Manual workers have higher average fertility because they are more likely to have big families, not because they avoid having really small ones.
Forty-one per cent of wives of manual workers married in 1955–60 have at least four children compared to 29 per cent in the non-manual group.
Social class differences in age at marriage and timing of births
Middle-class couples marry 2–3 years later than working-class couples (see Chapter 5).
They also start their families two years later after marriage.
As a consequence wives in social class I and II in 1986 produced their first child when they were more than four years older than wives in social class IV and V. From the 1930s until the 1970s the average couple's first baby arrived about twenty months after their marriage.
But from 1971 to 1979 the gap widened to thirty months.
Brides with husbands in social class I had their first baby on average thirty-two months after marriage in 1971, but waited a full year later (to forty-four months) in 1979, a delay of 38 per cent(table 4.11).
In 1975, women with husbands in social class V gave birth for the first time a mere nine months after marriage, as at most times in the past.
A high proportion of these births were pre-maritally conceived, also as at most times in the past.
Now this interval has increased to thirteen months.
Much of this shift is due to the reduction in pre-marital conceptions, which used to be a major precipitating factor in marriage (Britton 1980).
In 1971 47 per cent of first births in social class V, compared with   7 per cent in class I, were pre-maritally conceived.
This difference is now smaller — in 1988 10.2 per cent of first marital births to women with fathers in class I and II were pre-maritally conceived, compared with 24.1 per cent in class IV and V. Ten per cent of all births in 1988 in classes I and II, and 30 per cent to classes IV and V occurred illegitimately but within informal unions (as inferred from the joint registration of their birth)(see Werner 1985).
If pre-marital conceptions are eliminated from the calculation, the social class gradient in the duration of time from marriage to first birth is only half as steep as before, from forty-five months in social class I to twenty-four months in class V. In   these respects, as in many others, there seems to be a sharp difference between the behaviour of social class V and the rest of the population.
Young age at marriage powerfully increases the fertility of working-class women, but it has less effect on the fertility of middle-class women.
In 1971 social class I women who married under age 20 had on average 1.96 children, declining to 1.23 (63 per cent of the average for teenage marriage) for women married at age 30–44.
Outside class I and II the fertility gradient with age at marriage is steeper; more like a natural fertility decline.
In class IIIM the equivalent decline by age is from 2.33 to 1.00 (43 per cent), and in class V 2.69 to 1.11 (41 per cent ).
Social class differences in fertility are little evident in women who marry between ages 25 to 29.
Among women married over age 30, women in class I and II have more babies than the average.
In the 1980s, age-specific fertility rates in class I over age 25 were the highest of any group (figure 4.15).
The average number of children being born to middle-class couples has held up much more than in working-class families, and these children are now also born consistently later in marriage.
Women in middle-class occupations have more qualifications to acquire and more of a  career structure to protect than women in manual or routine jobs.
It is more important for them to defer their childbearing and to compress it into a short space of time in their married life.
This means that although fewer remain childless in the end, more non-manual couples are still childless compared to manual workers even after ten years of marriage.
The social origins of children
Victorian eugenists and biometricians such as Francis Galton and Karl Pearson expressed concern about higher working-class fertility and its supposed consequences for social welfare and national average ability.
The Royal Commission (1949, Chapter 15) while uncertain that the trend of ‘national intelligence’ really was downwards, felt that ‘it is clearly undesirable for the welfare and cultural standards of the nation that our social arrangements should be such as to induce those in the higher income groups and the better educated and more intelligent within each income group to keep their families not only below replacement level but below the level of others’.
Arguments on the measurement and the inheritance of ability and its social distribution (see Mascie-Taylor 1990) remained hot issues in the 1950s and 1960s.
Sir Keith Joseph, when Secretary of State for the Social Services, revived the controversy in 1972 by attributing the persistence of some social problems to the high fertility of least educated girls.
At least one of the trends behind the argument is now reversed.
Annual data on the social class origin of births, routinely available since 1970, have highlighted a startling shift in the relative contribution which each social class has made to each birth cohort and therefore to each future generation (table 4.12).
In 1970 only 22 per cent of births within marriage occurred in social class I or II families.
By 1982, the proportion had reached 32 per cent, in 1988 it was the same (32 per cent ).
This trend has been driven both by shifts in fertility and in the distribution of occupations.
There has been a long-term move away from manual and less skilled occupations towards skilled, service, and professional jobs (Werner 1985, Boston 1984, Routh 1987).
Its implications are explored further in Chapter 13.
During the fall in fertility in the 1970s, all legitimate births fell by 24 per cent between 1970 and 1975, and a further 3.4 per cent by 1985.
Births in manual workers' families, particularly to women in class IV and V, fell by a third.
Births to routine non-manual workers families (IIIN) fell by roughly the national average.
But women with husbands in class I and II did not share in this decline at all.
This shift in the distribution of births between classes ceased in   the 1980s.
But it had already made a difference in the balance of class origins of future generations.
In 1971, for example, there were about 224,000 births to fathers in non-manual groups, compared to 469,000 to ‘manual’ fathers — a ratio of 1: 2. 1.
But by 1980, ‘non-manual births’ remained almost the same — 226,000; ‘manual’births had fallen to 333,000 — a ratio of 1: 1.5.
In 1988 the respective numbers were 222,500 and 269,000 — a ratio of 1: 1.2 (OPCS 1990).
It must be remembered that these statistics refer to legitimate births and omit the 26 per cent(in 1988) of births which are illegitimate.
In a high proportion of these cases the occupation of father or mother's chief economic supporter, where known, is manual (particularly social class V).
In 1983, 76 per cent of the 61,000 illegitimate births which were jointly registered by both parents had fathers in manual occupations; 34 per cent were in social class V (see Werner 1985).
If allowance is made for such births, the proportion of births in 1986 with fathers in social classes I and II falls to 29.6 per cent.
Births registered by the mother only cannot be assigned to a social class, but the Longitudinal Study showed that 73  per cent of teenage illegitimate births in 1970–4 occurred to women where the ‘chief economic supporter’(usually father) was a manual worker; 13 per cent in social class V (Werner 1989).
The shifts in legitimate fertility by social class match shifts in family intentions.
In the 1940s and 1950s working-class women expected to have more babies than middle-class women.
These preferences and expectations are now reversed (table 4.4), and seem to offer further evidence in support of Becker's view that the better-off will eventually have more children than others.
Social class patterns of ‘ideal’ family size were rather uniform in the 1960s (Woolf 1971).
But when couples were asked how many children they were likely to have in reality, bigger social class differences became apparent.
Manual workers' expectations of family size had fallen, managerial workers' expectations had risen since 1945.
Women born in 1935–9 with ‘non-manual’ fathers expected 2.10 children compared to 2.52 for women with ‘manual’fathers.
Women born in 1955–9 with non-manual fathers expected rather more children (2.44 to 2.12) than women from manual backgrounds, and the new difference is maintained in the most recent cohorts (General Household Survey).
Married women at work
Aid with child care may be essential to help women combine work with childrearing.
Access to it varies by social class.
Pre-school facilities in general are mostly used by children of middle-class parents.
The cheap care provided by childminders is mostly patronized by poorer mothers, especially African and West Indian mothers.
In a sample of 5-year-olds in 1975, only 8 per cent of children with fathers in class I had no pre-school experience, compared to 43 per cent of children with fathers in class V. In 1980 there were places in all these institutions, plus childminders, for only 4 per cent of the 0 — 4 age-group.
Up to a third of mothers using such facilities have no job, except for day nurseries where 60–70 per cent of the mothers are employed (Osborn, Butler, and Morris 1984).
Only the relatively few places at these institutions provide full-time care, particularly for single mothers.
Full-time employment may be less of a deterrent to childbearing if mothers can pay for personal child care.
Only women in class II and (especially) class I are likely to have the accommodation, income, and confidence to employ any kind of residential domestic service: au pairs, housekeepers, or nannies.
Their services may not be essential to the ability of school teachers to combine children with employment; they may be to business women and barristers, and more so to divorced or  single working women.
At high income levels, the ability to buy child care may make women more, not less likely to have more children as their income rises still further (Ermisch 1988).
Whatever the truth of it, mean family size by class of employed women varies more than men's.
Women who are themselves in professional jobs (class I) have lower fertility than women with jobs in class II, whereas the wives of men in social class I have higher fertility than the wives of men in social class II.
Perhaps the remarkable thing is that married women who are professionals in their own right have any children at all; a high proportion certainly stay unmarried.
The general importance of child care in supporting the fertility of married women is recognized by those countries which have policies to encourage fertility.
In Britain the government has allowed tax relief for child care provided by employers, to encourage women to return to the work-force to compensate for the missing teenagers of the 1990s (Chapter 13).
Education and fertility
In general, people with more education have fewer children, partly because they marry later.
The two censuses of 1951 and 1961 both asked the age at which education had been completed (‘terminal age of education’).
This question was not repeated in 1971; it has the draw-back that most of the population give the minimum age, which itself has changed over time.
But since 1961 a question has been asked to list all educational qualifications, and the General Household Survey and the cohort studies now provide additional information.
As might be expected women who left school at the minimum age had the largest families, but the best educated were next in line.
Furthermore, between 1951 and 1961 the fertility of better educated women increased relative to the less educated, amidst a general decline of family size, in parallel with the social class changes.
The trend according to educational qualifications from 1961 to 1971 shows a  similar pattern.
In the 1970s the average family size of graduate women at the time of interview (1.53) was considerably lower than the fertility of women with no further education (2.04)(Cartwright 1978) and they were likely to be childless.
But when graduate women do marry, beyond age 20 their fertility is higher than that of other women — in 1971 5 per cent higher among graduate women married at age 20–2½ rising irregularly to 41 per cent higher for graduates married at age 35–9.
Graduates are least likely to have just one child or four children and over.
Graduate husbands are less likely to be childless in their marriages than are graduate wives.
This apparent paradox arises because 52 per cent of graduate wives have graduate husbands, while only 31 per cent of graduate husbands have graduate wives.
Dual graduate couples have the highest fertility of all, if family size up to four children only is considered.
As with social class, the effects of the wife's education predominate over those of the husband (OPCS 1983a).
The cohort studies underline how far qualifications, ability, and family history determine the age at which childbearing starts (Kiernan and Diamond 1982).
In the 1946 cohort the less able (measured at age 11) and women with no qualifications start having babies earlier and are also more likely to go on to have third and higher order births and have them at shorter intervals.
At the other extreme, women still childless at age 32 were more likely to be from a professional background with few brothers or sisters, and with mothers who themselves married late, and to have more than average personal ambition and parental interest.
Educated women have fewer unwanted babies; they are more likely to use family planning methods.
Even by the 1970s, fewer university educated women took the contraceptive pill — just 21 per cent compared to 43 per cent of other women.
Forty-eight per cent of university educated women used the sheath compared to 27 per cent of the rest.
Graduates were better aware of the health hazards associated with the pill — even though the alternatives they had taken up were less reliable.
Fertility and housing tenure
The fertility difference between council tenants and owner-occupiers is one of the biggest of any between major social groups (table 4.13).
In the 1976 Family Formation survey, family size of council tenants was almost one child (0.84) more than owner-occupiers (Murphy and Sullivan 1985).
The distribution of the fertility of council tenants was different: three times as many — almost a third in all — had four children or more, higher than the average manual working-class pattern.
A higher proportion also had only one child, but that is primarily due to the very high proportion of divorced women in this tenure (in the 1946 birth cohort, 32 per cent of women who were council tenants were divorced by age 36).
Almost half the girls married in their teens and who were occupying council houses in their mid-20s had conceived their first birth before marriage (47 per cent), compared to 30 per cent of owner-occupiers, 14 per cent renting privately, and 9 per cent who were still living with parents (Kiernan 1980b).
In that cohort, council tenants have their first child much sooner after marriage than   owner occupiers (median interval thirteen months compared with thirty months) and also have their third births much sooner after their second.
It is not just a question of social class.
All social classes are to be found in all tenure groups, although to a very unequal degree, and tenure differences still hold within social classes and categories of  education (Murphy and Sullivan 1985).
Owner-occupiers in different social classes have similar achieved family size and intended family size, although among council tenants, family size differs more according to social class (Cartwright 1976).
Tenure brings together many influences under one roof.
It combines together the effects of both parents' social class, occupation, and attitudes, in a common physical and social environment.
It divides the population into just three substantial, although unequal sections — in 1989 66 per cent of households were owner-occupiers, 24 per cent rented from a local authority, 6 per cent rented privately, 2 per cent from a housing association.
Many tenure effects are selective — many council tenants have no other choice of home.
The elderly, unemployed, the less skilled, single-parent families, larger families are all concentrated in council tenure because it is the only form of tenure to which they may have easy access.
Partners who divorce are more likely to end up as local authority tenants even if they were previously owner-occupiers (Holmans, Nandy, and Brown 1987).
However, the concentration of people with particular demographic characteristics is clearly not just a selective effect.
For example, fertility differences persist within each tenure after the initial selection effects, partly independently of social class (Fox 1982).
Council tenants are also more likely to become divorced.
The housing itself is unlikely to provide the reason.
There are more physical constraints in council housing than in owner-occupied housing in terms of bedrooms and size, but these are minor compared to the restrictions in the private rented sector.
The physical difference between average (purpose built) local authority houses (not flats) and owner-occupied houses is small; private rented accommodation is much more likely, and owner-occupied housing slightly more likely, to be sub-standard.
Twenty per cent of council properties (mostly flats) lack gardens and a high proportion are three floors or more from the ground, which is rare in private accommodation.
But most of these factors might be expected to depress, not to enhance, fertility.
More council households have no earner at all, or three or more earners.
There are many fewer two-earner households, so the ‘housewife effect’ may be important.
Clustering of high fertility families, and geographical and social isolation of some estates, may be important too.
The ethnic factor was relatively unimportant as until recently immigrants have been under-represented in local authority accommodation.
West Indians now comprise an increasing proportion of tenants in urban estates.
Instead, the strategies suitable for obtaining housing of different  tenures appear to encourage differences in family formation, especially early fertility or its postponement.
Owner-occupiers tend to delay childbearing because they face heavy housing costs at the beginning of their marriage.
They may have to save before they marry, and both may need to stay in work for as long as possible.
Council tenants (or intending council tenants) do not face such a cost problem because rents are low (1988 average with rates £20) and may be paid in part or whole by Housing Benefit (65 per cent of tenants).
Instead they face a rationing problem, where priority is awarded according to assessment of ‘need’.
Accordingly they may marry early and begin a family quickly; that will increase their chances of being allocated a council house through the points system, even though it reduces their chances of buying or renting privately (Ineichen 1979a).
The 1977 Housing (Homeless persons) Act (now consolidated in the 1985 Housing Act) responds to pregnancy of single women by giving them priority for rehousing by the local authority.
Previously those living with their parents would have been expected to continue living there.
Fourteen per cent of those accepted as statutorily homeless in 1989 were pregnant women (DoE 1990).
The proportion of unmarried mothers living with their parents declined from 49 per cent in 1973–5 to 30 per cent in 1983–4, with a corresponding increase in those living in their own home from 36 to 59 per cent over the same period.
This is believed to be a consequence of the legislation (Kiernan 1989b).
Some small-scale surveys suggest that some young girls do indeed become or remain pregnant with this in mind, but the impact at the national level is not known (see Murphy 1989).
Tenure differences may also reflect differences in lifestyle and attitudes.
Family ‘clans’ could develop on old-established estates (Young and Willmott 1954) and help with mutual support with children and a consolidation of lifestyle.
These close contacts in urban working-class life may have been a twentieth-century creation of local authority tenure (Anderson 1983).
It has often been claimed that especially in large, newer urban estates, residents have little ability to control their personal environment, the state of repair of their flat or its surroundings (Power 1988).
Some urban estates, partly as a result of allocation policies, are effectively one-class areas with few examples of success, clearly labelled by their architecture and appearance is distinct from the rest of society.
Their residents are heavily dependent upon others in most aspects of their lives and unaccustomed to taking action to change their own condition or to the belief that it is possible.
Some communal aspects of the design make these problems worse (Coleman 1985).
The importance of apathy and fatalism behind high fertility has  been mentioned earlier, but in the absence of specific research, any connection between attitudes on council estates and fertility must remain speculative.
Tenure differences may become sharper in time as the proportion of households in each tenure changes.
The privately rented sector is declining fast, marginalized after seventy-five years of rent controls and other restrictions, although the present government seeks to change and revive it through partial deregulation.
Just before the First World War over 90 per cent of households rented privately.
In 1939 58 per cent were still doing so.
By 1945 the figure was just 8 per cent.
Council tenure, almost non-existent in 1914, grew to a maximum or 34 per cent of households in 1980 and has since declined to 24 per cent, plus 2 per cent of households which rent from housing associations.
Government policy since 1979 has encouraged the sale of council homes to more than a million tenants (Daunton 1987).
If these policies continue council tenure may decline to 20 per cent, or less if present government policy to transfer whole estates to other landlords or to their tenants succeeds.
Seventy per cent home ownership by the end of the century seems inevitable (Morgan Grenfell Economics 1987).
Councils may eventually cease to be major landlords (Coleman 1989a) and this dimension of fertility differences will have to be analysed in new ways.
If the tenure effects are in part causal, not just selective, and if the new housing association or private landlords operate a different rent and allocation regime, then family formation patterns among tenants may change in consequence.