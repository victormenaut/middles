

How to read this book
I hope it won't be considered too presumptuous if I offer some advice to the reader who finds himself, perhaps for the first time, left to work on his own.
To most of us, reading a mathematics book is a lot harder than reading a novel.
A certain amount of application is required.
Begin by getting up out of that armchair and, with pencil and paper, find yourself a nice clear desk and a comfortable but upright seat.
Perhaps the first thing to realise is that learning is not a linear business.
I have known students who feel obliged to know pages 1–65 almost by heart before they dare turn to page 66.
This is silly.
If your conscience allows you to say that you really are stuck at some point don't be afraid to pass on to the next paragraph.
Things can often become clear with hindsight.
If, however, you find yourself carrying three or four such problems it seems clear that there is nothing for it but to go back and attack the first difficulties again.
Incidentally, one way to motivate yourself if things do get sticky is to imagine that you have to explain the subject to the class the next day.
(Think of all the unpleasant questions your classmates might ask you and make sure you can answer them!)
Definitions are sometimes hard to grasp on first reading.
One method I've found helpful is to learn the definition by heart.
Once you have control over the words you can concentrate all your efforts on their meaning.
Perhaps the best approach to reading a proof for the first time is positively to disbelieve each assertion made.
This commits you to examining each word and can be quite tiring.
On the other hand the exhilaration following the successful completion of such a task is well worth the effort.
Moreover having mastered the proof as it were "locally" it should now not be too difficult to master it "globally" by identifying the key points that make the proof work.
Explain the method — not the details — to yourself.
Regarding exercises: before attempting to answer a question do make sure you know the meaning of all the words in it!
It may seem obvious but some appear to forget that if you don't understand what a question asks you have little chance of answering it!
If a problem (or a proof) seems too difficult as it stands one can try looking at a special or simpler case.
This may point the way in regard to the original problem.
In any event, if you can't manage a simpler case you are surely not going to succeed with the more difficult one.
The message here is: Don't be afraid to examine lots of concrete examples.
Drawing pictures, as I do in this text, can also be helpful.
One more (obvious) point.
Don't expect the various parts of solutions to problems to occur to you in the "right" order.
(I can assure you that this book was not written straight through from beginning to end!)
Having got a rough draft of a solution, now write it out neatly and in a logically developed manner so that you will be able to read and understand your solution in 6 months time.
Finally may I say that I hope that (most days) you enjoy reading this book as much as I (most days) enjoyed writing it.
Prologue
In this prologue we discuss some of the questions which few beginners seem to have the courage to ask and yet to which they would surely like some kind of answer.
The questions considered are (i) what is algebra?(ii) what is its history? and (iii) what is it good for?
We urge the reader to dip frequently into the historical outline below.
The discoveries mentioned there of some of the world's best mathematicians should whet the appetite for, and also place in some kind of perspective, the mathematics covered in Chapters 1 through 6.
What is algebra?
The word algebra derives from the word al-jabr which appears in the title of a book written in the 9th Century by the Persian mathematician Mohammed Al-Khowarizimi (from whose name comes the word algorithm).
This book, in a Latin translation, had great influence in Europe.
Its concern with problems equivalent to those of solving polynomial equations, especially those of degree 2, led to the word algebra eventually becoming synonymous with the science of equations.
This state of affairs persisted into the 19th Century, Serret, in 1849, observing that "Algebra is, properly speaking, the analysis of equations." 
One possible definition of algebra [116]is that it is the study of operations, of rules of computation.
This is slightly unfair since, as we observe in Chapter 2, not all operations are interesting.
In any case the word algebra is nowadays often prefixed, to indicate different stages of development, by adjectives such as classical, modern and abstract.
Whilst there does not appear to be any universal agreement on the precise meaning to be attached to these prefixes (compare the definitions of modern algebra in [71, p. 669]and [81, p. 702]) we can fairly safely say that classical algebra is the present synonym for the theory of equations, a theory in which are manipulated symbols which invariably represent numbers, be they complex, real or rational.
The term modern algebra can then be used to describe that subsequent algebra, some of it arising from more detailed investigations within the classical theory, in which the symbols manipulated are no longer restricted to representing numerical quantities.
(Cauchy, in 1815, had defined multiplication of permutations, whilst Gauss, in 1801, had combined pairs of binary quadratic forms and also integers "mod p" .)
Finally, we give the name abstract algebra to that  generalisation of modern algebra in which the main object is the study of algebraic systems defined solely by postulates or axioms (usually chosen not arbitrarily but with several concrete instances in mind), no particular meaning being attributed to the symbols being manipulated.
Abstraction and axiomatisation
The introduction of the concepts of abstractions and axiomatisation is attributable to the Greek mathematician/philosophers of the period 600–300 BC.
Thus, neither is a recent invention.
The method of abstraction has several interconnected uses.
By stripping concrete objects of their less essential features, they become less involved and hence more amenable to mathematical treatment.
Also one increases the chance of revealing similarities between superficially distinct objects, so that a theorem from one area of mathematics may suggest an analogue in another area.
Further, several concrete theories can be studied simultaneously under the umbrella of one general theory of which the concrete theories are special cases.
Finally, one penetrates to the real reasons for the success (or failure!) of a theory.
In this way a deeper understanding should result.
As a simple illustration of these remarks we note that the Babylonians posed many problems [71, p. 34]of the form: "Find the side of a square if the area less the side is 870." 
The verbal solution proceeds: "Take half of 1, that is ½; multiply ½ by ½, that is ¼.
Add this to 870 to get 870¼.
This is the square of 29½.
Add ½ to 29½ to get 30, the side of the square."
Of course this same procedure is applicable to other problems of a like nature.
But how much deeper in content and how much easier to understand is the observation that infinitely many such problems can be dealt with all at once, as it were , by replacing numerical coefficients by literal coefficients of no initial specific numerical value.
In short:  (In the above problem a = 1, b = 1, c = 870.)
The axiomatic method which rejects proofs based on intuition involves deducing by logical argument alone, from initial statements assumed without question (the axioms), other statements (the theorems).
The most widely quoted example of the use of this method is Euclid's Elements (c. 300 BC) i which the author supposedly sought to give a consistent foundation to geometry.
(For a different point of view see [106].)
It is argued in [110]that Euclid's fifth axiom (The whole is greater than the part) was included specifically to eliminate from mathematics the paradoxes (c. 450 BC) of the infinite introduced by Zeno .
Furthermore, the "crisis" brought about by the discovery that  is not a rational number clearly called for rigorous investigation: after all, the intuitively "true" had been proved false!
(See problem 3 below.)
Thus the axiomatic method was called upon to help confirm the basic consistency of mathematics.
For the next 2000 years the abstract/axiomatic approach was with a few exceptions replaced by a more concrete intuitive approach.
However, in the headlong rush to develop the newborn calculus of Leibniz and Newton, absurdities arising from the free use of intuitive geometrical arguments (see problem 1) led to the so-called second great crisis and a call for analysis to be made more rigorous; in other words based on arithmetic (whose foundations were obviously (!) secure).
The publication by Lobachevsky in 1829 of a consistent (non-Euclidean) geometry, in which Euclid's parallel postulate is denied, should perhaps have turned mathematicians' attention back to a study of axioms, especially as it had long been appreciated that Euclid's use of the axiomatic method was, to say the least, inconsistent ([106]contains stronger views.)
However, this work apparently attracted little attention for several years.
In due course the "arithmetisation" of analysis got under way: Dedekind insisted that "what is provable should be proved" , observing that even the equality  had not yet (1858) been satisfactorily established.
Eventually the desire continually to express concepts in terms of yet more "fundamental" ones led to Peano setting down in 1889 his symbolic and axiomatic description of the set of integers (in terms of the undefined concepts: set, belongs to, zero, number, successor of).
Furthermore the whole numbers themselves were shown to be definable entirely in terms of Cantor's new notion of set (Section 0.1).
Unfortunately Cantor's definition was too wide-ranging: intuition had failed once again at the most basic level (see Russell's Paradox in Section 0.2) and the third great foundational crisis was at hand.
One result was Zermelo's attempt (1908) to build a formal set theory on an axiomatic basis.
One of the facets of 19th Century algebra, the ever-increasing number of concrete structures of distinct outward form but with similar underlying properties, encouraged their cataloguing and comparison by abstracting common features.
Indeed Weber's book [32]of 1893 talks not just of (groups of) permutations, matrices, etc., but of (groups of) "things" (Dingen, in German).
Basing his proposals on the more commonly used properties shared by permutations, matrices, etc., he postulated that his "things" be subject to similar rules (i.e. axioms).
It would then follow (as Boole had said in 1847) that"the validity…does not depend on the interpretation of the symbols…
Every system of interpretations…is equally admissible."
Note that whilst the axiomatic method permitted, in theory, the assuming of arbitrarily chosen sets of axioms, those adopted were not chosen at random, the aim being to reflect properties of concrete systems already deemed important.
Thus the abstract/axiomatic method has a role to play in classification and reorganisation; in making special results more intelligible by identifying common features.
The method thus supplies greater transparency and insight and leads to a unified approach offering progress along a wide front.
It also helps us avoid making intuitively obvious but unfounded assertions and hence "proving" false theorems.
A final remark.
Since Euclid's time the concept of axiom has changed somewhat.
To Euclid axioms were unshakeable truths (e.g. every two distinct points uniquely determine a line).
Today we interpret the word axiom  differently.
We do not ask whether or not an axiom is "true" — just as we don't ask if the rules of chess are "true" .
(One can of course question whether or not the axioms are appropriate if one is trying to model a concrete example.)
The Greek word axioma originally meant "request" ; the reader is requested to accept the axioms unquestioningly as the rules of the game.
All he can (and should) question is whether or not the asserted conclusions follow logically from the axioms.
Historical development of algebra
As stated above, the algebra now called classical concerned itself with (polynomial) equations, in particular with attempts to supply formulae for the roots of equations of degrees 3, 4, 5, etc.
The solutions  of the general quadratic equation  were known to the Babylonians via the process of completing the square (although only positive solutions were found acceptable).
A long search ensued for similar formulae in terms of radicals (that is, formulae involving  and the coefficients of the given equation) for the roots of equations of higher degree, but none appeared until the 16th Century when a formula for the cubic was found by the Italian Niccolo Fontana (more commonly known as Tartaglia, "the stammerer" , because of a speech impediment brought about by injury in childhood).
There is no need hereto go into the rivalries of the various mathematicians involved in the search for the cubic formula — however it makes for fascinating reading ([71],[64],[75]).
Not long afterwards the solution of the general quartic (or biquadratic) equation was found by the Italian Ferrari and in 1545 both the cubic and quartic formulas were published in the Ars Magna of Jerome Cardan.
After this double success hope must have been high that a solution in the case of the general quintic would soon be forthcoming.
New ways of solving the quartic equation were discovered but still the quintic equation resisted attack.
In 1770/71 Lagrange set about analysing the various methods then known for dealing with the general equations of degrees 2, 3, 4 and he found that they all depended on the same general principle (see Section 5.2).
In particular Lagrange showed cause for finding the number of formally distinct "values" taken by a function, for example xy + zt, on permuting the symbols x, y, z, t, in all possible ways.
Although Lagrange seemed to cherish hopes that his work would show the way to the solution of the general quintic the results obtained indicated a distinct possibility that there might be no corresponding formula in the case of equations of degree greater than 4, and a supposed proof of this was given by Ruffini in 1799.
It was apparently difficult to see if Ruffini's proof was complete and despite a further attempt by him at the  problem, in 1813, the credit for supplying the first generally accepted proof of impossibility goes to Abel (pronounced "Arbel" ) in 1824.
Thus it was proven that no universal radical formula for obtaining all the roots of every quintic was available.
Yet it was undeniable that such formulae were available for certain special quintic equations (for instance the five roots of  are each radically expressible; see exercise 4.6.6).
In 1832 Evariste Galois described, by associating with each equation a finite group, exactly which equations were treatable.
This result is, incidentally, a mere corollary to a much more general theory, still a subject of research, called Galois Theory.
Galois, it is usually said, coined the word group at this time and introduced the concept of normal subgroup.
He was also the first to investigate fields with finitely many,  elements, where p is a prime and .
At the turn of the century other ideas later to be seen as part of algebra were coming from the pen of Carl Friedrich Gauss, one of the greatest mathematicians who ever lived.
Before he was 19, Gauss had constructed, by straightedge and compass, a regular 17-gon, the first "new" constructible regular polygon for 2000 years, and in 1799 he gave the first satisfactory proof (where Newton, Euler and Lagrange had failed) of the fundamental theorem of algebra (4.8.1).
Gauss' most influential contribution is probably his Disquisitiones Arithmeticae (1801), a work in which appear his 17-gon (see Section 4.6), his introduction of the notation of congruence (see Section 2.2), his proof that all the roots of  are expressible in radicals, and a proof of the quadratic reciprocity law.
(For an integer a and a prime r not dividing a, define  to be 1 or -1 according as the congruence  (mod r) can be solved for x or not.
The reciprocity law states that, for distinct odd primes, .)
Despite the fact that at the turn of the century mathematicians were happily (in most cases) using complex numbers, there was some residual disquiet relating to them.
Firstly, although Wessel (1797) and Argand (1806) had tried to make complex numbers a little more respectable by showing how to interpret them, their addition and multiplication geometrically, there remained, possibly because of doubts concerning the intuitive use of geometrical arguments, the desire to put them on a firmer basis.
Furthermore there was still the problem "What exactly is ?" (see section 4.4).
Suffice it here to say that this problem was finally dispensed with in 1833, by banishing  (!):
Hamilton replaced the objectionable a + ib by the ordered pair (a, b) of real numbers (see Section 4.4.), thus duplicating an earlier (unpublished) work of Gauss.
Secondly unease was caused by the continued manipulation of letter symbols as if they were positive integers in situations in which  they clearly were not.
For instance, the equality (a — b)(c — d) = ac + bd -ad -bc, acceptable to everyone whenever a, b, c, d were whole numbers with b less than a and d less than c, appeared to remain valid when particular irrational or complex numbers were substituted for the letters.
In addition, due to the efforts of Woodhouse, Babbage, Herschel and Peacock from 1803 onwards, Leibniz"notation and methods in the calculus had gradually replaced Newton's in England and as a consequence much of continental analysis had become available to the English.
In particular the calculus of operations became in English preserve [96].
In this calculus one combines functions as if one is dealing with algebraic quantities.
Thus in the differential equation  one separates the "operator"  from the operand y and even factorises it as .
(Indeed, later (1843), Boole applied the method of partial fractions writing .)
So there was obviously raised the question as to the validity of doing this sort of thing.
It is in fact the case that whereas not all the early investigators even tried to validate their reasoning, several, including Cauchy, Servois and Boole, certainly did.
And it was in this connection that Servois, in 1815, introduced the notions of functions which are "distributive" and "commutative" , terms still used today (see Section 1.2).
It was in this atmosphere that Peacock, his friends Babbage and Herschel having worked in the calculus of operations, introduced (1830, 1833, 1842) his two concepts of algebra: arithmetic algebra and symbolic algebra.
Arithmetic algebra, declares Peacock, concerns the use of symbols representing positive integers so that, for instance , the expression a — b is meaningless if .
Symbolic algebra is the same as arithmetic algebra except that its operations are universally application: a — b has meaning for all a, b.
The rules for operating in symbolic algebra are to be just those of arithmetic algebra.
A consequence of this is Peacock's Principle of Permanence of Equivalent Forms which essentially says "Whatever the occasion, a.b = b. a" .
It was against this background, namely that it seemed preposterous to suppose that a.b could ever be other than equal to b.a in any consistent algebraic setting, that Hamilton looked for more than 10 years for an extension of complex numbers suitable for application to the physics of 3-dimensional space.
At last, in 1943, he broke through the psychological barrier by founding a consistent set of 4-dimensional "hypernumbers" in which all the usual laws of arithmetic hold with the exception of  in general .
He had invented the quaternions.
(The reader who fails to see how any system of entities in which the laws  both fail can have any practical value has only to recall the familiar 3-dimensional vector calculus, denoting vector multiplication.
Subtraction on the set of integers is another example!)
At this very time the German H Grassmann was generalising complex numbers still further by inventing an unlimited number of arithmetics of n-tuples.
Unfortunately his exposition was not easy to read and it was Hamilton's quaternions which gained the more attention.
Hamilton had great hopes for quaternions but they were not quite what the physicists wanted.
It is true that the greater mathematical physicist Maxwell referred to them when making use (separately) of the "scalar" and "vector" parts of a quaternion, but the vector calculus of Gibbs and Heaviside (in the 1880s) seemed to suit the physicists better.
For a time a battle raged between the supporters in each camp.
Gibbs-Heaviside eventually won as regards applicability but the quaternions have the honour of being the first to demonstrate the existence of consistent number systems not satisfying the commutative law of multiplication.
During the 1840s, the quaternions soon inspired the manufacture of other consistent number systems which violated the most obvious laws of arithmetic.
In 1845 Cayley described his (8-dimensional) octonions, still called the Cayley numbers.
Here not only is  not universally valid, the associative law  is also broken.
In 1854 Hamilton invented biquaternions (quaternions with complex coefficients which turn out to be 2 x 2 complex matrices in disguise.)
Here  is possible even when  and .
That is, the biquaternions possess divisors of zero.
The "smaller" systems of hypernumbers introduced were put into some kind of order by the American B Peirce in a paper published posthumously in 1881.
Peirce had set himself the task of methodically classifying, and looking for applications of, the n-dimensional systems for all .
Did Hamilton have to go to 4 dimensions to find his algebra?
It appears that he did search for 3-dimensional examples but could find none without divisors of zero.
Indeed Hamilton believed that all 3-dimensional systems had to have divisors of zero and considered the fact that the quaternions had no divisors of zero to be one of its chief merits.
In 1861 Weierstrass proved (essentially) that the only finite dimensional extensions of the real numbers in which all the usual laws of arithmetic hold are the real numbers themselves and the complex numbers.
In 1878 Frobenius showed that relinquishing the commutative law of multiplication adds only the quaternions to the list, and using algebraic topology Bott, Milnor and Kervaire showed, in 1957, that relinquishing in addition the associative law adds only the Cayley numbers.
For more on this see the article by C W Curtis in [2].
Returning to 1847, Boole invented another sort of algebra, since called Boolean algebra in order to put logic on a symbolic mathematical basis.
After his death his wife Mary wrote that the idea of symbolising logic had occurred  to him at the age of 17 (Leibniz had had similar but less developed ideas as early as 1666), but several subsequent writers (see [96, p. 235]) have indicated that Boole's work on the calculus of operations in the early 1840s must have at least influenced his approach if not actually initiated it.
In this book we have insufficient space to be able to do justice to Boole's ideas by indicating applications to logic, probability and computer design.
Fortunately there are several introductory books on the subject;[65]and [66]are just two of them.
Looking at the 19th Century development of number theory we return to Gauss and his reciprocity law.
He was able to extend his law to cubic and biquadratic residues, but to state his results elegantly he found it helpful to introduce numbers of the form a + pb and a + ib respectively where a, b are integers, p is a complex cube root of unity and, of course, i is the usual square root of -1.
In this work he needed to know that these numbers factorised uniquely into primes (3.7.13) just as do the ordinary integers.
Kummer endeavoured to study higher residues and considered, for the purpose, numbers of the form  where the  are integers and  a prime.
These numbers are also relevant for attempts to solve Fermat's Conjecture (se Section 3.5) and indeed the FC would be solved if only the uniqueness of factorisation theorem valid for numbers of the form a + pb and a + ib extended to them.
Unfortunately, as Kummer knew, p = 23 provided the first instance (of infinitely many) of the failure of unique factorisation.
To try and get round the problem Kummer introduced extra "ideal" numbers (Section 3.9) to help him regain uniqueness of factorisation in many cases.
His analysis showed that the FC is indeed true for all prime exponents , except for "irregular" primes p = 37, 59, 67, which cases he dealt with later.
Starting in 1871 Dedekind extended Kummer's ideas further.
Any complex number which is a root of an equation  where  are integers will, said Dedekind, be called an algebraic number.
(see exercise 3.2.14.)
Those for which, in addition,  will be termed algebraic integers  is one such (!) since it is a root of .
Dedekind introduced the term number field (Zahlkörper, in German) to denote a collection of complex numbers satisfying the field axioms to be found in Definition 3.2.2(10).
One can prove that the algebraic numbers form a field, but not so the algebraic integers: given (algebraic) integers α, β we find that α/; β need not be an integer.
The concept introduced here, then, needs a name.
It was called number ring (Zahlring, by Hilbert, 1897).
Our present concept of field grew out of Dedekind's work and also that of Kronecker, the basic notions being present already in the work of Abel and Galois.
Dedekind looked at collections of numbers gathered into a completed whole, this concept being essential in his construction (Section 4.4) of the real numbers on the basis of the rational numbers.
Kronecker would have none of this.
He insisted  that every entity asserted to exist in mathematics must be shown to exist by a finite set of instructions whose applications yield the entity.
Even the statement that "obviously any polynomial in x with integer coefficients must be expressible (by using a" degree "argument) as a product of polynomials which cannot be further decomposed" was of little value to him unless it be accompanied by a method which in each instance would supply the indecomposable factors.
He supplied such a method (see exercise 1.11.11).
Since Dedekind's algebraic numbers extend Kummer's concepts, his algebraic integers also lack uniqueness of factorisation.
He reinterpreted Kummer's concept of ideal number in terms of collections of already existing numbers, called these collections "ideals" (see Sections 3.4 and 3.9), and showed how every ideal could be expressed uniquely as a product of prime ideals (see Section 3.9).
In 1983 H Weber gave an account of Galois' theory which is "applicable to every case…from function theory…to number theory" using the field concept "without reference to any numerical interpretation" .
Weber can thus be regarded as the founder of abstract field theory.
One further item which should be mentioned here since it has strong connections with present day ring theory is the subject of algebraic invariants.
Algebraic quantities remaining essentially unchanged under a change in coordinates are important in coordinatised geometry since they correspond to intrinsic geometric properties.
In number theory too there ws interest in representing whole numbers by, amongst other things, binary quadratic forms  and in quantities such as  which remain unchanged when in  are replaced by  where α, β, γ, δ are integers and αδ -βγ = 1.
Motivated by an 1841 paper of Boole, Cayley, who was interested at the same time in algebraic aspects of projective geometry, began seeking invariants of homogeneous forms of degree n in two and more variables.
Cayley attracted his friend Sylvester into studying invariants (the term "invariant" is due to Sylvester) and these two did so much research on the subject that they were named the Invariant Twins.
Cayley's work soon encouraged many mathematicians to invariant theory, described by Sylvester as "the essence of modern algebra" , and so many invariants were found that, to bring some order to the subject, minimal systems of invariants were sought in terms of which all others could be expressed.
Gordan (1868) proved that to each binary form f (x, y) there is such a finite system.
The proof is hard.
The whole subject was brought to a sudden climax when Hilbert in 1888 showed that for a form of any degree in any number of variables a finite "basis" always exists — and this without giving any indication of how to find such a set in any particular instance!
What Kronecker said doesn't seem to have been recorded; Gordan called it "Theology, not mathematics" .
Hilbert's theorem in its ring-theoretic form is stated in Section 3.4.
Though it is sometimes said that Hilbert's theorem killed invariant theory, this is not entirely correct.
Invariant theory continued — albeit at a reduced  level of intensity — and in recent years activity has begun to pick up again.
(For more on the history see [90].)
A subject of which invariant theory formed a considerable part was that of algebraic geometry.
Algebraic geometry which, very loosely speaking, is concerned with curves and surfaces in n-dimensional space which are defined by algebraic equations, is a meeting ground for several mathematical disciplines including geometry, complex analysis, topology and number theory as well as algebra.
Around 1900 many deep results were obtained especially by the Italian geometers although the validity of some of their methods was not always apparent.
The subject was set upon firm foundations around 1930 by Emmy Noether and van der Waerden using an abstract algebraic approach.
Along with algebraic number theory algebraic geometry can claim to be one of the main motivating factors behind an autonomous theory of current interest, that of commutative rings.
What about the development of group theory from Galois' time up to 1900?
Firstly one should note that it was not until about 1846 when Liouville published two of his papers that Galois' work became better known.
As suggested in [95, p. 94], it was possibly in connection with the announcement of publication, made as early as 1843, that in the period 1844–6 A L Cauchy was especially active in developing the theory of permutations.
Adding to his work of 30 years before he proved Galois' assertion that if a finite group G contains pm elements, p being a prime, when G contains a subgroup of order p.
One should note however that Cauchy did not use the word group but talked rather of "a system of conjugate substitutions" .
Although Galois had introduced the word group he had used it inconsistently.
In 1854 Cayley explicitly defined the term group.
He thought only in terms of finite groups but since he specifically insisted on the associative law being satisfied his ideas were similar to those of today.
In this paper Cayley's concern was with systems of elements satisfying the equation  and for both n = 4 and n = 6 he showed that there is essentially just one other system besides the set of complex nth roots of unity.
Up to about 1867 only finite groups were considered.
Furthermore the term group was not generally employed and the main interest was in finding the number of formally distinct values a function of n variables takes when the variables were permuted.
In 1867 Jordan, motivated by earlier studies of crystal structure by the physicist Bravais, considered groups with infinitely many elements — in particular groups of movements.
In 1870 Jordan produced his Trait  e des substitiones et des equations algebriques.
This work organised the known theory of permutation groups and its relationship with Galois Theory.
It also introduced many new results and the concept of homomorphism (5.10.1) as well as providing the atmosphere for the eventual finding by Fedorov and Schonflies around 1890 of the 230 crystallographic space groups (see Section 5.12).
In 1872 the Norwegian L Sylow extended the Galois/Cauchy result mentioned above by replacing  in hypothesis and conclusion (see 6.2.8 and 6.2.12).
In the same year Felix Klein, in his famous inaugural address at the University of Erlangen, stated his aim of using group theory to bring a unity to the various classical geometries that had been found since the announcement of the first non-Euclidean one by Lobachevsky in 1829.
Thus geometries would be classified by groups of transformations which left certain geometrical aspects invariant.
(The concept of invariance was definitely the "in" subject at the time and it was later to provide a central idea in the theory of relativity.)
Since rotations and translations of the plane can be arbitrarily small the notion arises of an (infinite) continuous group.
In 1874 and 1883 Sophus Lie (pronounced "Lee" , Norwegian) used the idea to attempt a classification and simplification of the solutions to certain differential equations.
In studying his continuous transformation groups (groups whose elements depend upon a system of continuously varying parameters satisfying certain differentiability conditions) Lie was led naturally to study some non-commutative, non-associative algebras subsequently named after him: Lie Algebras.
(In a Lie algebra multiplication and addition satisfy  Lie groups and Lie algebras form a major component of the present day theoretical physicists' armoury.
We note in passing that initially there was no universal agreement about what exactly constituted a group.
For example, whereas Cayley, in 1854, specifically demanded that the associative law should be satisfied, Lie and Klein, in their earlier work, did not feel obliged explicitly to mention the requirement; in all cases of interest to them the condition was automatically satisfied!
As the group concept became yet more prominent it became increasingly desirable to standardise terminology.
In 1882 H Weber gave a set of postulates for abstract groups of finite order.
These postulates are essentially those in use today.
Two other directions taken in the 19th Century by the theory of groups should perhaps be mentioned.
One is Dyck's concentration on systems of generators for a group and on relations satisfied by these generators.
These concepts came to be of prime importance with the introduction of non-abelian groups into topology, specifically via the fundamental group of a topological space.
The second is the introduction of group representation theory in which groups are represented (via homomorphisms) by groups of matrices with complex number entities.
Matrices have the advantage that they can be added together and multiplied by scalar quantities; further the concepts of determinant and trace are available to aid computation.
This theory, developed by Frobenius, Molien, Schur and Burnside (see [92]) is of vital importance today [92],[28, Chapter 12],[53]) in the theory of finite groups and also in representing certain groups which arise naturally from symmetry considerations in chemistry and physics.
Possibly inspired by Hilbert's (1899) full axiomatisation of Euclidean geometry the 20th Century began with many attempts to find independent sets of axioms for fields and for groups, the main worker being E V Huntington around 1902–5.
In 1905 J H M Wedderburn proved that every finite division ring is a field, a result which provides the only known proof that in a finite  projective plane Desargue's theorem implies that of Pappus [13].
In 1907 Wedderburn proved one of the fundamental theorems of modern non-commutative ring theory.
This theorem, proved for algebras over an arbitrary field, was extended by Artin in 1927 to more general rings.
It describes, just as does the Fundamental Theorem for Finite Abelian Groups (6.4.4), the exact structure, in easy terms, of a wide class of rings (the so-called semisimple ones).
In 1908 K Hensel introduced for number-theoretic purposes a new type of number — the p-adic numbers.
Inspired by Hensel's construction, E Steinitz, in a 140 page paper in 1910, undertook a classification of all fields, several apparently unrelated sorts of which had now come into existence.
He proved that all fields fall into two categories: those whose unique minimal (so-called prime) subfield is essentially the same as the rational numbers and the others where it is essentially the (finite) Galois field  (see Section 3.10).
Further, every field can be obtained from its prime subfield by successively "adjoining" elements.
1914 saw the first axiomatic declaration of exactly what constitutes a ring.
(It is less general than the current one.)
This axiomatising process was carried further in the 1920s and 1930s by Emmy Noether, the greatest of all women algebraists.
Following her doctoral thesis which, supervised by Gordan, closed with a complete list of 331 covariant forms for a given ternary quintic, her strong inclination to unify, organise and generalise via axiomatisation took over.
In 1921 she investigated differential operators in quantum mechanics by abstracting their essential properties, taking these properties as axioms and building all consequences thereupon.
This approach she adopted in all her subsequent work thereby introducing a revolutionary style of attack on problems of algebra.
In the period 1920–6 she brought within her general theory of ideals several theories which until that time had been seen as independent.
Her methods also greatly simplified many of the earlier proofs, a number of them being very important in algebraic geometry.
The rings she and her school investigated, in one sense dual to those of Artin mentioned above, are now called, naturally enough, Noetherian rings.
Penetrating theorems about these rings have been obtained as recently as 1960 by A W Goldie.
Amongst the major achievements of the century in group theory one must mention the modular representation theory of groups by matrices over finite fields as pioneered by Richard Brauer and the subsequent use of this theory in investigations into finite simple groups (Section 6.6).
In the theory of infinite groups various conjectures of Burnside have been established by the Russian mathematicians Golod, Kostrikhin, Novikov and Adjan.
We refer the reader to [22]for precise statements.
The group-theoretic result of Golod (1964) is a consequence of joint work with Shafarevich (also 1964) in which was solved affirmatively an old problem of field theory closely related to the work described here in Sections 3.6, 3.7 and 3.9 (see [43, p. 125]for brief details).
Some uses of algebra
It is impossible for the author to know exactly what the reader will accept as being a "proper" application of abstract algebra.
Applications within pure mathematics other than those found in the text are far too numerous and diverse to mention here.
Even excluding these the following represents only a small sample.
Firstly algebra offers its basic notations and concepts as a most convenient means for expressing, mathematically, certain concrete ideas.
For instance, professional mathematicians often prefer the phrase "ring of integers" to "collection of all integers" , even when no use is to be made of the ring structure: the algebraic terminology gives a more complete, a more faithful picture.
(In this text we make a similar use of set theory.
That is, we use its more elementary concepts and notations as a language in which to express, succinctly, our algebraic notions.)
Of course there is no obligation to use the language of algebra in this way any more than there is to use the symbol x in solving problems involving quadratic equations: as mentioned earlier the Babylonians readily solved such problems writing down everything in longhand.
It's just that the new terminologies and notations offer extra insight and clarity of expression and enable more powerful methods to be developed.
Regarding the more specialised applications which seem to use the algebra (of the type presented here) in an essential way, the theory of groups, being (in a rather wide sense) the mathematical formulation of symmetry, is naturally widely employed in physics and chemistry — for instance in applications to crystallography, spectroscopy, general relativity, molecular vibrations, molecular orbitals, solid state physics and especially in the modern theory of elementary particles.
(In February 1964 the Omega minus particle, which group theory had previously predicted should exist, was first identified.)
Ring theory is not quite as old a subject as group theory and direct "practical" applications seem to be somewhat limited in number.
This ceases to be the case when one considers rings which have extra structure, as for instance, when the ring is an algebra or a field.
In particular algebras of matrices and of polynomials occur frequently: and in quantum mechanics use is made of the algebra of all polynomials in two (non-commuting) letters x, y between which the relationship xy = yx + 1 is assumed to hold.
(Researches in ring theory are still investigating this ring!)
In other applications rings (algebras) also arise from groups.
Both finite and infinite multiplicative groups give rise to so-called group algebras via attempts to "represent" the group elements by matrices, and in studying a Lie group an investigation of the associated Lie algebra is invaluable.
The theory of fields naturally underlies all appearances of the fields of rational, real and complex numbers — but also finite fields (as well as finite groups, polynomial rings and power series rings) are put to practical use in  the construction of efficient (from the point of view of cost!) error detecting and correcting codes in the area of data communications.
Finite fields are also of importance in statistics via their association with (sets of orthogonal) Latin squares.
Finally the algebra introduced by Boole to model logic mathematically has found application to the design of computers and telephone switching circuits, again via the very real problem of reducing construction costs.
The following problems are not algebraic in content.
They are placed here mainly for your enjoyment and for subsequent discussion with friends and teachers.
However, it is intended that they should extract from the reader that kind of critical attitude with which he should read this book from Chapter 1 onwards.
Problems
1 Sketch the graph of .
Clearly this function is continuous everywhere and fails to be differentiable only at the origin (i.e. x = 0 is the point at which there is no tangent).
Invent a function which is continuous for all x and yet not differentiable whenever x is a whole number.
Can there exist a continuous function which is not differentiable for any x?
(Try sketching such a function and then use your intuition.
Finally, ask your teacher.)
2 Euclid defined a point as "That which has no part" .
Criticise this definition.
3 Discuss: Given any two straight line segments, say , it is obvious that there must exist some (perhaps very small) unit of length in terms of which the lengths of the above lines are m and n units respectively, m and n being whole numbers.
4 Draw a circle C. Call the interior of C "the plane" , each point inside C "a point" and each chord of C (except for its end points) "a straight line" .
Defining two "straight lines" to be "parallel" if they do not meet in a "point" (i.e. inside C) show that: Given a "straight line" L and a "point" P, not on L, in the "plane" , it is possible to draw through P infinitely many "straight lines" which are "parallel" to L. 
Elementary set theory and methods of proof
0.1 Introduction
In 1895 at the beginning of his work Beiträge zur Begründung der transfiniten Mengenlehre Georg Cantor* made the following definition: By a set we understand any collection M of definite, distinct objects m of our perception or of our thought (which will be called the elements of M) into a whole.
Thus examples of sets are: the set Z of all whole numbers, here called the integers; the sets Q, R and C of all rational, all real and all complex numbers respectively; the set M comprising all moons of Mars; and even the set H of all ten-legged octopodes which visited Archangel last 1 April.
Cantor's need for such a definition had arisen around 1872 from his investigations concerning the possible uniqueness of representation of functions by trigonometric series.
In due course it became apparent that all of mathematics could be made to rest upon a set-theoretic base.
In particular Cantor and Richard Dedekind, in his Stetigkeit und irrationale Zahlen (1872), shows how the somewhat intangible irrational numbers (that is, those elements of R which are not in Q) could, using the set concept, be made respectable in terms of Q (see Section 4.4) and Gottlob Frege (1884) demonstrated how the natural numbers 0, 1, 2, 3,…(on which Z and ultimately Q can be based — see exercise 4.4.17 and 3.10.5(iii) could be defined in set-theoretic terms.
In addition, the concept of function can also be defined set-theoretically (see Section 2.6).
It is therefore not surprising that the notations, terminology and simpler notions of set theory now form an essential part of the language in which contemporary mathematical discussions are conducted.
The next two sections introduce the simple set-theoretic ideas useful in this book.
0.2 Sets
We shall consider the words set, collection, aggregate as synonymous.
The elements of a set we shall sometimes call its members.
If A is a set and if an object a is an element of A we write .
One usually reads the symbolism  as "a belong to A" .
If a is not a member of A we write  and say  "a does not belong to A" .
Thus  Phobos εM, Isaac Newton ε H.
Sets can be described by listing their members between pairs of curly brackets (also called braces).
For instance M may be written alternatively as M ={Phobos, Deimos}.
It is of course impossible to describe sets containing infinitely many elements this way.
On such occasions we might be tempted to write{ x: P(x)}where P is a property characterising those and only those elements of the set in question, P(a) indicating that the object a has property P and hence lies in the set.
Thus the set Z+ of all positive integers may be written .
(This symbolism is read as "z plus is the set of all x such that x belongs to z and x is greater than 0" ).
However, the notation  where the three dots indicate the vague expression "and so on" , is sometimes used.
Small finite sets can be exhibited in several ways.
We have, for instance, M ={x: x is Phobos or x is Deimos]or again M ={x: x is a Martian moon}.
If A and B are sets and if each element of A belongs to the set B we say that A is a subset of B or that A is contained in B, and we write  (or , the latter being read also as "B contains A" ).
In particular  for each set A and if  and  then .
Given , if we know (and if we care!) that B contains elements not in A then we will write  (or ) or even ) if extra emphasis is required.
A is then called a proper subset of B. Thus we write  according to the emphasis required.
If A is not a subset of B we write .
Note that  when an only when A contains at least one element which is not in B. We says sets A and B are equal, and write A = B, when and only when they contain precisely the same members.
Thus A = B when and only when both  and  hold simultaneously.
It follows that a useful way to establish the equality of two sets A and B is to prove both  and .
Warning: Try not to confuse ε and .
Somewhat roughly stated: ε is used in relating a set to its elements;  is used in relating a set to its subsets.
To illustrate this, suppose .
Thus A is a set with five elements, namely  Hence   (incidentally, sets like{ 2}which contain exactly one element are called singletons.)
Now consider the sets F =  and H as mentioned earlier.
It appears that each contains no members at all.
A set with no members  is called an empty set.
Despite the differences in definition of F, G and H we can show that there is only one empty set.
To prove this and more, let  be an empty set and let A be any set.
Then , for otherwise  and so  would contain an element not to be found in A. But this is silly since  has no elements.
Now suppose  is another empty set.
Since  is empty .
Since  is empty .
These two inequalities, taken together, imply .
Thus we can talk of the (unique) empty set.
Problem 1 What is wrong with the following argument showing ?
To prove  we must show that every element of B is also contained in A. Since  has no elements we cannot show .
Hence 
Problem 2 In connection with the footnote on p. 2, can you think of a set A for which A ε A?
0.3 New sets from old
New sets can be made from old in several ways, one of which we saw above in defining Z+ in terms of Z.
Let A and B be sets.
Then the sets  and  are called, respectively, the intersection and the union of A and B. Thus if  then  whilst .
It follows immediately that for sets A and B we have 
Intersection and union of sets can be thought of in terms of the shaded regions in Fig. 0.1; such figures are called Venn diagrams.
If we introduce a third set C and shade the region common to  and C (Fig. 0.2) it appears that the sets  are equal.
This can be checked (exercise 5(b)) by an argument not depending upon pictures.
[Pictures can be deceptive.
What value has this pictorial proof if, say  or ?
See [109].]
There is no difficulty in extending the definition of union and intersection to larger finite or even infinite collections of sets.
For example, if for each  we define    then the set of elements common to all the Sn is denoted by .
In fact , the set containing the one real number 0.
Here we have "indexed" the various S with the elements of the set Z+.
We can also use the elements of R+ (see exercise 6) or indeed the elements of any set (see 3.4.5(F)) as an indexing set.
Another way of producing a new set from two old ones A and B is to define their difference .
In particular the set of all non-zero real numbers is then denoted by .
The next definition is suggested by the way coordinates are introduced into the real plane.
Points in the plane are made to correspond to pair of  real numbers and vice versa in such a way that if points P and Q are given coordinates  and  respectively then P and Q coincide when and only when .
In particular(1, 2) and (2,1) correspond to distinct points.
Since the only distinction between (1, 2) and (2, 1) is the order in which the numbers 1 and 2 are written down we refer to such pairs of numbers as ordered pairs.
Much more generally we make Definition 0.3.1 Let A and B be sets.
Then A x B denotes the set  of ordered pairs.
A x B is called the Cartesian product of A and B. The word "ordered" implies that elements (a, b),(c, d) in A x B are defined to be equal when and only when both a = c and b = d.
Notes 0.3.2 (i) The concept of ordered pair can be described in a purely set-theoretic manner.
See exercise 8.
(ii) In a like manner one can define the set A x B x C of ordered triples (a, b, c) where .
One can even form the Cartesian product of a collection of infinitely many sets (see exercise 6.3.20).
Example 0.3.3 If  Note that A x B and B x A each have 6 (= 2.3 = 3.2) elements but that .
[Why not?]
One final word on set production: Definition 0.3.4 Let A by any set.
By  we denote the set of all subsets of  is called the power set of A.
Example 0.3.5 If  then 
Note that A has 3 elements whilst  has .
0.4 Some methods of proof
From the beginning of Chapter 1 onwards the majority of this book is taken up with assertions described as theorems (or lemmas or corollaries) followed by explanations purporting to be "proofs" .
Just as we have adopted an intuitive approach to the concept of set so we shall allow our intuition to guide us in the matter of whether or not an explanation is logically acceptable: to formalise the notion of acceptability would take us too far afield, into symbolic logic.
(The reader keen to see how this can be achieved is referred, for instance, to [48],[50].)
On the other hand there are a few techniques, frequently used  in constructing proofs, to which it is perhaps useful to draw attention as they have been known to give students the occasional difficulty.
We do this via some concrete examples, simple enough to allow the logical principle involved to be easily seen.
Consider, for instance, how you would prove the following assertion:(i) If x is an odd integer then x2 is an odd integer.
You would probably say: "Assuming x is odd, we may write x = 2n + 1 for some suitable n ε Z. Then  which is clearly odd" .
Here we go directly from the hypothesis that x is odd to the conclusion that x2 is odd.
Such a proof is called a direct proof.
For a second example consider the following assertion about the integers x and y:(ii) If  then  (1).
A direct proof is possible.
From  we deduce that .
It follows that  and hence .
However, the proof most of you would probably have given is: If x = y then x2 = y2 (2), a contradiction.
Hence the result.
This approach, using the method of indirect proof, conceals a number of logical points.
Two we wish to draw attention to are most easily discussed in symbolic terms.
Let A and B stand for  and  respectively.
Further let  denote their negations or denials.
Thus  is  and  is  Then (1) is the assertion: If A then B. We write this briefly as  and read it as "A implies B" .
On the other hand (2), apparently logically equivalent to (1), is the assertion .
In fact it is shown, in courses on elementary logic, that the assertions  and  are equivalent as is the pair  and , What are clearly not (in general) equivalent are the assertions  and .
[In terms of our concrete examples this latter assertion reads , which is blatantly false, whereas (1) is (blatantly) true.]
The assertion  is called the converse of the assertion .
To summarise: to prove that an asserted conclusion B follows from a hypothesis A one can proceed directly or prove, equivalently, that  is deducible from hypothesis .
This kind of indirect proof is called proof by contraposition,  being the contrapositive to .
Another frequently adopted method of indirect proof is that of reductio ad absurdum or proof by contradiction.
Here the negation of the hoped for conclusion is shown to lead to a contradiction (or absurdity).
Possibly the first ever use of this method was in a proof of: Theorem 0.4.1 If  and  then .
In other words,  is not a rational number.
Proof We suppose  so that we may write x = m/n where m, n ε Z. Clearly we may assume m, n have no common divisor ; such a common  divisor can be eliminated before proceeding.
Then from x = m/n we deduce that  that is .
It follows that m2, and hence* m, is even.
Writing  we find ; that is .
But then n2, and hence n, is even.
Thus our assumption, namely that  has led to the contradiction (absurdity) that numbers m, n with no common divisor  are both even.
Consequently .
Now consider the following two assertions.
(i) Every positive integer is the sum of four (non-negative) integer squares.
(ii) Every positive integer is the sum of eight non-negative integer cubes.
It is not very difficult to check that (i) is certainly true for each positive .
For example .
You should note however that even if you check (i) for all positive integers up to, say, ,(i) would still not be established for every positive integer n; indeed, for all you then know,(i) might fail for .
No, to prove (i) you must devise a proof which covers all positive integers n.
In fact (i) is true; it was first proved by Lagrange in 1770.
On the other hand (ii) is false: not all positive integers can be expressed as asserted.
For instance 239 cannot so be expressed.
239 is thus a counterexample to assertion (ii), Actually there is only one other positive integer which is not expressible as a sum of eight cubes.
Can you find it?
It is less than 50.
The point is that even though only two out of infinitely many positive integers fail to satisfy (ii) the existence of even one such "nasty" integer is sufficient to destroy (ii) 's claim to be a valid assertion.
In particular, finding the other "nasty" number changes nothing — the challenge was set just for fun.
There are other principles of reasoning we could mention.
For instance in showing that 2.5.1 followed from 2.5.1"we split the proof into two cases (one when p divided n and one when it does not) and establish each case separately.
This kind of proof is called proof by cases.
Its use in the instance quoted is so straightforward that no special attention needed to be drawn to it.
We refer the reader interested in a detailed account of different methods of proof to [50, pp. 30–46].
We close with one or two remarks on terminology and notation.
We have already observed that the assertion "If A then B" is written  and read "A implies B" .
We also say "A is a sufficient condition for B" (since A is enough, all by itself, to allow us to conclude B) or that "B is a necessary condition for A" (since B necessarily follows from A — whether you like it or not).
Alternatively mathematicians say "B if A" or "A only if B" .
If we know  and  we say "A if and only if B" , an assertion we write briefly as  or A iff B. Technically all definitions should be in "iff" form.
If, for instance, in 1.3.1 we had defined a to be a divisor of b if b = ac this would have left open the question of whether or not we are to call a divisor of b if no such c existed.
Thus, use of "iff" indicates that a will be called a divisor of b when and only when the required c exists.
(Cf.
Definition 1.3.3.)
In symbolic logic much use is made of the signs  (there exists) and  (for all), although we don't often employ these here.
As an example, note that one of the properties of the equality relationship on Z used in the proof of 1.2.1(i) may be stated succinctly as , whilst axiom A3 in Section 1.2 includes the statement that .
Finally note that the negation of , that is , is  and that, similarly, .
1 Numbers and polynomials
1.1 Introduction
As implied in the Prologue, one of the central concepts of modern algebra is that of "ring" , two of the most fundamental examples being the ring Z of all integers (Section 1.2) and the ring Q[x] of all polynomials in the "indeterminate" x with coefficients in the field Q of rational numbers (Sections 1.6 to 1.11).
The terms ring and field will be defined formally in Chapter 3.
The main objectives of this chapter are as follows.
(1) To present some of the simpler properties of the sets Z and Q[x] in such a manner as to emphasise their similarities (and their differences!).
Thus we shall have to hand important concrete examples and theorems which will help motivate, and can act as test-cases in, our later development.
(2) To introduce some terminology and notation in common use in the following chapters.
One advantage of doing this at this stage is that the reader will probably feel able to devote a little extra effort to learning this terminology as the statements of the theorems themselves will take little remembering — many of them should be fairly familiar to him already.
Finally and by far the most important is:(3) To introduce into this familiar setting a few notes of caution.
Here we hope to develop the reader's critical faculties by showing him, especially in relation to Q[x], that not everything is quite as straightforward as he might have thought (see in particular Section 1.6).
We hope the reader will examine proofs of theorems with one question continually in mind, namely "Why can he say that" ?
As promised earlier, many of the proofs in this chapter are written in expansive style with square brackets indicating those portions of proofs that could, without great loss, be omitted.
The threefold purpose of these brackets is detailed in the preface.
1.2 The basic axioms.
Mathematical induction
We start, then, by looking at the set Z of all integers.
As noted in Chapter 0 a definition of integer can be given in set-theoretic terms (see, for example[49]).
The algebraist is, however, little interested in what the integers are; he  is mainly interested in the fact that these integers, whatever they may be, are added and multiplied together, two at a time, according to the following axioms (where we use the symbol "." 
to denote multiplication).
For every three integers a, b, c (distinct or not) we have: 
Remarks (i) The axiom A1 is called the commutative law of addition in Z: M1 is the commutative law of multiplication.
A2 and M2 are the associative laws of addition and multiplication respectively.
A3 and M3 announce the existence of additive and multiplicative identity (or neutral) elements.
A4 asserts each integer has an additive inverse.
D lists the distributive laws.
(ii) The reader will probably have recognised N as having properties usually ascribed to the set of positive integers and I as being the principle of mathematical induction.
(iii) Despite the fact that the algebraist is not interested in the nature of the integers themselves he certainly gets joy out of the fact, to be proved later (see Section 3.12), that there is essentially only one system of objects satisfying the above axioms A1 through to I. (The idea of two algebraic systems being "essentially the same" will first be defined formally in 3.10.1.
Before that, part (ii) of the Remark in Section 1.8 might prove helpful.)
(iv) It is notable that for the set Z there is no multiplicative analogue of A4.
However, as is well known, the sets Q, R, C do all satisfy a near analogue of A4, viz:  unique number, namely  (also written ) such that 
Problem 1 Assuming, for the moment, the truth of the assertion of uniqueness made in Remark (iii) above, one deduces that, amongst the axioms listed for Z, there must be at least one which cannot be satisfied by Q. Can you identify which axiom(s) from A1 through to I are not satisfied if one attempts to apply them to Q instead of Z?
The reader can no doubt think of other properties usually ascribed to Z which we have so far failed to mention.
For instance, we are all familiar with 
These properties might be called the zero-divisor law, the cancellation law and the mysterious law (see [4, p. 5]) respectively.
Why do we not add them to the list of axioms given above?
The answer is that we can, without using intuition, speculation or hearsay concerning Z, prove that Z, C and M are logical consequences of the axioms A1 through to I. Now whilst algebraists do not regard it as their prime duty to reduce all such sets of axioms to a minimum size, it is part of an algebraist's function to investigate consequences of axioms such as those just referred to.
In this manner the algebraist can hope to discover which features of a given system are essential and which only incidental.
We shall perform a number of consequences-seeking calculations of this type in a more abstract setting from Chapter 3 onwards.
Let us content ourselves for the moment with finding out exactly which of the axioms given earlier are required to establish property M. First, then, we prove, giving all the details, Lemma 1.2.1 
Comment If these results seems rather too trivial to bother about, let's see if we can make them more impressive (and less "obvious" ?) by stating their conclusions in words:(i) The product of the additive identity with any integer always yields the additive identity.
(ii) The additive inverse of a product [that is -(a.b)]is equal to the product of the additive inverse of the first [that is -a]with the second [namely b].
(iii) The additive inverse of the additive inverse of a given integer is equal to the given integer.
Proof (i)[From the property of 0 as stated in A3 we have, on setting a = 0,]
[Multiplying each side by c we have]
Hence 
[Now using the second axiom in D on the left-hand side of this equation]
Consequently 
[By A4 the element -(0.
c) certainly exists: we add it to each side.
Reflecting the fact that on the left-hand side it is being added to the element 0.
c+0.c we write]and so 
[Now using the associative law on the left-hand side]
It follows that 
[And replacing  by 0 on using A4 once on each side]
[Finally using A3 on the left-hand side] as required.
(ii) Given a we obtain successively 
(iii) Given  we have 
[Now the unique additive inverse of any element x is denoted by -x.
Hence the unique additive inverse of -c is denoted by -(-c).
But c is this additive inverse.]
Hence -(-c) = c.
Theorem 1.2.2 For all 
Remarks (i) If the reader feels we have gone to a lot of trouble to establish a result which is "obvious" we ask him on what grounds he bases his belief i this result?
Is it merely "experience" ?
Or has he had it "n good authority" that it is true?
What we have done is to show that the semi-mystical assertion that "minus times minus is plus" is deducible as a consequence of other ( "more obvious" ) axioms of arithmetic.
In later chapters we shall prove several results of the above type where the symbols used will not necessarily stand for integers.
Then, assertions of the above type will certainly be far from "obvious" , since we will lack the appropriate "experience" .
(ii) We were trying to find out exactly what we needed to assume in order to prove 1.2.2.
Including the (necessary) Lemma 1.2.1 it appears that our proof of 1.2.2 depends on various properties of the = symbol together with several applications of A4, A3 and D together with just one application of A2.
Notice that A1, M1, M2 and M3 were not called upon.
Note 1.2.3 One other thing 1.2.2 does for us is to prove that Z can contain only one subset N with the properties listed in axiom P. (If M is a subset of  Z satisfying P then either 1 or -1 belongs to M. It follows from 1.2.2 that  and then from axiom P(ii) that, in any case, 1 ε M. Similarly 1 ε N and hence .
If, now  then  since both M and N satisfy axiom P(ii).
But then  since Z satisfies axiom I. This means that .
Clearly  is impossible [why?].
The reader who is beginning to doubt the author's sanity should note that there is something needing proof here.
The innocuous looking set S =  can support two distinct such Ns, namely the subset N1 of all  which are positive real numbers in the usual sense and the set .
To deduce the familiar properties of ordering amongst the integers we make: Definition 1.2.4 The (unique) subset N of Z described in axioms P and I will be called the set of positive integers; the subset -N the set of negative integers.
If a, b ε Z we say that a is less than b and write  If we only know (or care) that  we write  (b — a is the shorthand notation for the more accurate B + (-a).
P(i) then says that each integer is either positive or zero or negative (and never has two of these properties simultaneously).
P(ii) says that the sum and the product of positive integers are both positive.
I says that any subset of positive integers which contains 1 and which contains a + 1 whenever it contains a is precisely the set of all positive integers.
One can use axiom P and 1.2.4 to establish all the familiar properties of the  sign.
We treat just a couple, place some in the exercises and leave the rest for the reader to prove for himself or look up in, for example,[24].
Theorem 1.2.5 If a, b c ε Z are such that 
Proof [To say ]according to 1.2.4,[to say that]b — a ε N. [To say , similarly,[to say that]c — b ε N. [But]from P(ii) we [can then]deduce [that b — a + c — b ε N, that is,]that c — a ε N. Thus [using 1.2.4 again this simply says], as required.
Theorem 1.2.6 
Proof (Briefly) We are given that b — a and c are both in N. Hence by axiom P(ii) (b-a) c ε N. It follows* that bc — ac ε N, that is .
Now seems a fairly appropriate time to make Definition 1.2.7 Let a ε Z. We define  by 
Thus Example 1.2.8 
We first use this concept in Theorem 1.4.5.
We concentrate for a while on I, the principle of induction.
Let us suppose that to each positive integer n there corresponds a statement which we denote by S(n).
For example, S(n) might be .
Let us further suppose that:(i) We can show S(1) to be a true statement; and that (ii) for each positive integer k we can prove, under the assumption that S(k) is a true statement, that S(k+1) is also a true statement.
Then I tells us that for every positive integer n the statement S(n) is true.
For: Let U denote the subset of N comprising all those positive integers n for which the statement S(n) is true.
That is, U ={n:n ε N and S(n) is true}.
Then 1 ε U [since we are supposing that we can prove S(1) to be true]and, whenever k ε U [that is, whenever S(k) is true]then k+1 ε U [since we are supposing that we can then prove S(k+1) true].
By principle I we see that U = N. Thus for each n ε N we have n ε U; that is S(n) is true, as required.
After talking about proofs by induction we should also mention the technique of definition by induction.
Consider for example the famous Fibonacci sequence  where  and (ii) for each integer , is defined not directly but relative to previously defined terms of the sequence by the formula .
One is tempted to say that the Fibonacci sequence is defined by (i) and (ii) but there are some logical subtleties associated with this temptation.
We shall not discuss them here as they can be overcome.
The reader who is sufficiently intrigued by this warning is referred to a very readable article [94]by L Henkin.
We now look at two variants of I. Consider the following assertions: 
W: Every non-empty set of positive integers contains a least member.
That is, if  then T contains an element t such that  for any other z ε T.
Theorem 1.2.9 The statements  are equivalent.
That is, each implies the other two.
Remarks (i) On the face of it, it looks as if we have six results to prove, namely: .
In fact we only need to prove  which we can write in abbreviated form as  (Compare this with the deduction of a = b = c given that  and .
(ii) W is also expressed by saying "The positive integers" taken in their natural order, are well-ordered".
W is called the well-ordering principle — hence the letter W!
W is manifestly untrue when applied to Q and R. [Why?
And what about C?]
We shall use W a lot in this chapter in particular and call upon I1 in 1.10.1.
Note 1.2.10 Despite the remarks above on economy of effort we shall prove here only the equivalence of I and W. We leave the reader to prove that 
Proof of Theorem 1.2.9  [Here we wish to show that every non-empty set of positive integers has a least member so…]assume [to the contrary]that there is a non-empty set T, say, of positive integers such that T has no least member.
Then  [since 1 is the smallest positive integer.
Which axiom(s) prove this for us?
See exercise 19].
Define the subset V of N by V = .
Then  otherwise  for some t ε T so that[since  and since no integer lies between k and k+1 (why not?)]t = k +1 is the least integer in T [contradicting the assumption that T has no such least integer).
Thus, by principle I, V = N whence .
This manifest contradiction  tells us that our one assumption is not tenable.
Thus no non-empty set T exists and the required result is thereby proved.
 Let U be a subset of N such that 1 ε U and a + 1 ε U whenever a ε U. Suppose .
Then the set  is non-empty and so [since we are assuming W holds we can conclude that]T has a least element t, say.
Thus  and so  [is t — 1 a positive integer?].
Then [by the given property of U]t — 1 + 1 ε U. That is t ε U. But t ε T [we chose it as the least element of T]and so  a contradiction.
Thus [once again]our one supposition [this time that ]must be invalid, so that U = N, as required.
    
Divisibility, irreducibles and primes in 
We move now to the central concept in this chapter by making Definition 1.3.1 Let a, b .
We say that a divides b (or that a is a divisor of b) and we write  there exists  such that .
If a does not divide b we write .
Remark If we wish to emphasise that we are demanding  we might say a divides b in .
We shall have to be careful over this point in Section 3.8.
(See also the paragraph following 1.9.15.)
As trivial examples we offer Examples 1.3.2  [is this one correct?
I might be teasing to see if you are awake!], .
Note that .
Definition 1.3.4 If  are such that  then d is called a common divisor of a and b.
The following result is easy but important.
Lemma 1.3.4 If 
Proof Since 
The next definition, which we formulate from a desire to get at the fundamental building blocks as far as multiplication in Z is concerned, is intentionally unconventional, introducing, as it does, a familiar concept in an unfamiliar way.
The reader will recall that one of the author's main aims in this chapter is to encourage a critical attitude on the part of the reader to statements made in this book.
The main reason for adopting this definition will reveal itself in Sections 3.6 and 3.7.
Definition 1.3.5 (i) If u ε Z is such that  (in Z) then u is called a unit.
(ii) If a ε Z is neither 0 nor a unit we say that a is irreducible iff, whenever a is expressed as a product, a = bc with b, c ε Z, it follows that either b or c is a unit.
[Both can't be units.
Why not?]
(iii) If a ε Z is neither 0 nor a unit we say that a is prime iff, whenever a divides a product, that is, a\bc where b c ε Z it follows that a\b or a\c (or both).
(iv) If a, c ε Z are such that a = bu where u is a unit then a and b are associates.
Examples 1.3.6 1, -1 are the only units in Z (exercise 1.2.20); 3, -7 are irreducibles; 3, -7 are also primes; 9, -8 is a pair of associates.
Remarks The reader's first comments on the above definition might well include:(1) What is the point of making 1.3.5(i) when the only two integers satisfying the property are 1 and -1?
(2) I thought the concept of primeness was defined by (ii) and that (iii) describes a well-known property of prime numbers.
(3) What is the point of introducing the two definitions, namely (ii) and (iii), when they express precisely the same concept?
(4) I can see why you exclude a = 0 from (ii) and (iii) but why debar a from being 1 and -1?
(5) I find from (iii) that -3 is a prime.
Surely you don't allow negative numbers to be primes?
Points (1),(2) and (4) will be commented on later (see Remarks (i) after 1.9.3 Theorem 1.4.10 and the Remark following 1.5.1 respectively).
For the moment we dismiss (5) with the answer "Yes we do" !
Regarding (3) we offer the reader the Challenge: Are you absolutely sure these concepts are the same?
If you are so sure, a proof shouldn't be hard to come by.
So, go to it before reading on!
We now reveal that the concepts of irreducibility and primeness do coincide in Z. It is, however, precisely because of the fact that these concepts do not coincide in every kind of number system we shall meet (see exercise 6) that the celebrated conjecture of Fermat remains unproven to this day (See Section 3.5.)
Returning to Z we see that to prove the above assertion of coincidence we must show:(i) that every irreducible element of Z is necessarily a prime element; and (ii) that every prime element of Z is necessarily irreducible.
The latter we can do immediately; the former will take a little longer (Theorem 1.4.10).
Theorem 1.3.7 Every prime element in Z is necessarily an irreducible one.
The strategy of this particular proof is to head directly from hypothesis to conclusion as follows: Let a be any prime element of Z. We wish to prove that a is irreducible.
This will (essentially) be achieved if we can show that, whenever we write a as a product, that is a = bc with b, c ε Z, then one of b, c is a unit.
Proof Let a be a prime in Z. Then [by 1.3.5(iii)] and a is not a unit,[and it only remains to establish the property described in 1.3.5(ii)].
[Thus]suppose a = bc where b, c ε Z. Then certainly a\bc.
[But a is given to be prime and so from 1.3.5(iii),]we deduce a\b or a\c (or both).
WLOG we suppose a\b.
Then [by definition]there exists in Z an element s, say, such that as = b.
It follows that asc = bc.
But bc = and so asc = a = a.1.
Since  we may deduce from property C (Section 1.2) that sc = 1.
Thus [since cs = sc = 1]c is a unit — as required.
Remarks (i) Note that under the assumption that a\b we have shown that c is a unit.
If you think that we should now give another proof of the same length to show that the possibility that a\c leads to the conclusion that b is a unit, you have cheated yourself in that you have accepted the statement "WLOG we can suppose a\b" above as a valid one when you don't even understand what it says!
If you've been caught out here please do not be so careless again.
(ii) The word "irreducible" is a good one for elements with the property listed in 1.3.5(ii) since according to that definition an irreducible element is one which cannot be represented as a product of two properly "smaller" elements.
Thus irreducible elements are seen to be the fundamental building blocks with respect to multiplication for the system Z.
Let us suppose for the moment that we have shown that in Z irreducibles and primes are the same thing.
A natural question, to which you probably know the answer (but not a proof? is Question 1.3.8 Are there infinitely many primes (i.e. irreducibles)?
To answer this we use: Lemma 1.3.9 Let a be an integer such that .
Then a can be expressed as a product of finitely many positive irreducibles (i.e. primes).
Remarks (i) We extend the usual meaning of product to include the case of single numbers standing alone.
Thus whilst 2.3 and 3.2 are the two ways of writing 6 as a product of (positive) irreducibles the expression 7 is regarded as the required product decomposition for the integer 7.
(ii) It follows immediately from 1.3.9 that if a ε Z and if  then a is expressible as a product of primes, one of which is negative.
Proof of Lemma 1.3.9 [We use the principle W.]Let S be the collection of all those integers (greater than 1), if any, which are not expressible in the desired form.
[If, maths.
the required result is immediate — there are no "nasty" integers.]
If  then [S is a non-empty set of positive integers and]principle W asserts that S has a least member.
Let this member be called m [m for minimum?].
Then m cannot itself be irreducible [since such an m would have a product decomposition of the required kind, namely m itself].
Thus m can [definition of irreducible]be expressed as a product, say m = .
But [since m is the least element of S we have].
Thus both m1 and m2 can be expressed as products of (positive) irreducibles,  say.
It follows that  [which shows that m is expressible as a product of irreducibles after all].
Thus m ε S, contradicting the assumption that m ε S. Hence (this assumption is wrong and].
Remark Whilst this proof uses the "contradiction method" it gets to this contradiction by supposing the existence of a counterexample, hence, by principle W, a smallest counterexample and thence the contradiction.
Accordingly this type of proof might be called proof by minimum counterexample.
We can now answer Question 1.3.8, giving a proof essentially due to Euclid.
Theorem 1.3.10 There are infinitely many primes.
The proof is yet another "by contradiction" .
Recall that we are temporarily assuming that irreducibles and primes are one and the same thing in Z.
Proof Suppose there is only a finite number, n, say, of positive primes.
Let them be listed in increasing order as  [so that  etc.].
Now form the integer .
By 1.3.9, Nn can be written as a product of finitely many primes [i.e. irreducibles] say, where each ti must be one of the primes from the complete list .
Suppose ti = pm.
Then .
hence [by 1.3.4, with s = 1 and t = -1, we see that].
That is .
[But this is absurd  since by definition a prime cannot be a unit.]
This absurdity completes the proof.
As an entertaining diversion and one on which you can again get your hands dirty we consider a small point which naturally arises here, namely: Is it in fact the case that each of the numbers Nn is itself a prime?
After all with , etc. we get, successively  all of which are primes.
We make a Conjecture If we set  then, for each positive integer n, Nn is itself a prime.
Can you amend the proof of 1.3.10 to obtain a proof of this conjecture — or can you supply a single counterexample which will kill off the conjecture?
Lemma 1.3.9 raises, and the remarks following it answer, an obvious question, namely: Is the way of expressing any integer (other than -1, 0 and 1) as a product of irreducibles unique?
Clearly not, since we may write, for example, not only .
Rather more to the point are questions like "Are  equal?" 
Disregarding variations of ordering and sign as in the decompositions of 6 as given above one can establish a uniqueness-type theorem (1.5.1 below).
We first prove 1.5.1 by employing the standard proof involving definite use of the concepts of irreducibility and primeness.
Then for comparison, entertainment and for later examination we offer a more novel approach which apparently uses only irreducibility.
David Hilbert (23 January 1862 — 14 February 1943) Hilbert was born in Königsberg (now Kaliningrad), the son and the grandson of judges.
In 1885 he obtained his doctorate with at thesis on the theory of invariants.
After becoming a professor at Königsberg in 1892 he obtained the chair at Göttingen in 1895, a position held until his retirement in 1930.
Hilbert's mathematical interests ranged widely, encompassing the theory of invariants, algebraic number theory, foundations of geometry, analysis and relativity theory.
His outstanding longer works include the 370 page Zahlbericht (1895–7), in which he rewrote much of algebraic number theory, and his axiomatic approach to Euclid's geometry (1899).
At the International Congress of Mathematicians in 1900 Hilbert presented his famous list of 23 problems to which he believed mathematicians should address themselves.
Several of these still remain unsolved.
After his work in geometry perhaps Hilbert's greatest wish was to prove the consistency of arithmetic and thereby to resolve the "foundations-crisis" that attracted philosophers such as Bertrand Russell.
Some rejected his proposed method of procedure and in 1931 Kurt Gödel dashed all hopes by proving that in a consistent system formalising the natural numbers there is a theorem A such that neither A nor not-A can be proved within the system.
Around 1903 Hilbert, to help deal with a problem on integral equations, introduced that infinite dimensional extension of Euclidean space now called Hilbert Space.
1.4 GCDs
To get to the standard proof of 1.5.1 will take a while.
We begin with Definition 1.4.1 Let a, b ε Z. An integer c ε Z is termed a greatest common divisor (gcd) or highest common factor (hcf) of a and b iff (i) c\a and c\b and (ii) if d\a and d\b then d\c.
Remark We talk of a gcd rather than the gcd of a and b since 1.4.1 says nothing about uniqueness (nor even about existence!).
In anticipation of proving existence of a positive gcd we introduce Notation 1.4.1 The positive gcd of a, b (not both of which are zero) is denoted by (a, b).
Thus Examples 1.4.3 (i)(60, 24) = 12;(17, -42) = 1.
(ii) For a, b ε Z (not both zero) we have (a, b) = (b, a) = (\a\, \b\).
Thus we only ever need consider pairs of non-negative integers (see exercise 3 following).
Problem 2 It seems rather obvious that any pair a, b of integers (not both zero) must possess a unique positive gcd.
Surely one simply takes from the set of all positive common divisors d of a and b the largest?
It should be easy to prove that d is the required gcd.
You may not even think there is a problem at all since the words "greatest" and "largest" are synonymous in the English language.
Notice, however, that for us "largest" is to be interpreted in the sense of ordering whereas "greatest" (as used in "greatest common divisor" ) is used in the sense of division.
Bearing in mind these remarks you might try to prove here and now that the numerically largest common divisor d of a and b, as described above, is indeed their (positive) gcd.
(Warning: your proof had better not apply to the H-numbers introduced above (see exercise 15 below) nor to some of the number systems introduced in Section 3.6 (see exercise 3.6.12) where this assertion on gcds is false!)
Incidentally there is no problem in proving the uniqueness of the positive gcd — given that a gcd actually exists at all.
The main difficulty seems to be with establishing existence.
A theorem which is very important for us and which proves not only that any two integers a, b (not both zero) do have a gcd, but much more besides is Theorem 1.4.4 Any two integers a, b (not both zero) have a unique positive gcd.
Further, if this gcd is denoted by c, we can find s, t ε Z such that c = sa + tb.
To help prove this theorem we shall need to call upon the following result, which is known as the Division Algorithm.
Theorem 1.4.5 (The Division Algorithm) Let a, b ε Z with .
Then there exist unique m, r ε Z such that a = mb + r where .
[The letters m, r are chosen as they are the initial letters of the words multiple and remainder respectively.
A careful choice of notation can often be very helpful.]
Examples 1.4.6 (i) With a = 17 and b = -5 we find m = -3 and r = 2 so that 17 = (-2)(-5) +2.
Note that m ε Z, r ε Z and .
(ii) With a = -19 and b = -12 we find m = 2 and r = 5 so that -19 = 1(-12)+5.
Our intuition and experience tells us that the division algorithm is clearly true.
After all we have been working examples like this since age seven or so.
However, let us note that the verification of 1.4.5 in as many as 20 million cases by no mens establishes the truth of the theorem for all possible cases (cf. the remarks in Section 0.4 on sums of four squares).
Furthermore, intuition is not always a good guide, as problem 3 in the Prologue demonstrates.
One difficulty the beginner might find on being asked to prove 1.4.5 is that of deciding exactly what to write down.
It seems difficult to explain so obvious an assertion in simpler terms.
A proof using Q may occur to the reader: Choose M ε Z such that m is the greatest integer not exceeding .
Then trivially  so that  (assuming for the moment that .
Setting a — mb = r we see that a = mb + r where .
This proof suffers from a slight defect in that one of our later aims is to show how to construct Q on the basis of Z so that officially Q does not yet exist.
Another defect is  raised by the question "How do you know that the stated m exists?" 
(Exercise 3.12.10 gives an example not so unlike Q in which no such m can be found.)
We base a proof on principle W.
Assuming for the moment that  we give Proof of Theorem 1.4.5 Let S be the set of all non-negative integers belonging to .
Then S is not empty since a ε S. By the obvious extension of principle W from , S has a least member, which we shall denote by r.
Thus  for some .
We claim: .
For otherwise .
It then follows that  is an element of S smaller than the smallest element r of S. This absurdity leads us to conclude that .
Thus m1 and r are such that 
If  we can, by the above, find m, r such that  Then  where .
But then  Clearly  (If r = 0 there is nothing to prove since than , immediately.)
The proof of 1.4.5, in the case where  and of the assertions of uniqueness we leave to exercises 8 and 9.
We can at last give the Proof of Theorem 1.4.4 Let S denote the set of all positive integers of the form ma + nb where m, n are free to range over the whole of Z. Then clearly at least one of a, -a, b, -b belong to S. [Why?]
Thus S is not empty.
Invoking principle W we see that S contains a least member — let us call it c.
We claim: c is the required (unique) positive gcd of a and b.
In proving that c is a common divisor of a and b we in fact prove rather more, namely: if w ε S then c\w.
Indeed, let w ε S and use the division algorithm to write w = kc + r where K, r ε Z and .
Noting that if r = 0 we have nothing left to prove, we may assume that .
Since w and c can be expressed in the forms ua + vb and  respectively, where  we see that r = .
Thus r [being greater than zero and of the correct form]belongs to S. But this contradicts the choice of c [as the least member of S; this contradiction shows that the assumption  just cannot hold].
Thus r = 0 and hence c\w, as required.
It follows readily that c\a and c\b.
Finally, c is a gcd for a and b.
For, if d\a and d\b then  [Lemma 1.3.4].
The uniqueness of c is left to the exercises.
Remarks (i) The author would like to think that the reader got some enjoyment out of reading over that proof.
It is a rather tidy kind of argument with no loose ends.
(ii) Whilst being a very agreeable proof of the universal existence of gcds the above proof is not very helpful in determining (a, b) for any particular  pair of integers a and b.
To do this we call upon the procedure known as the Euclidean Algorithm.
In order to clarify the procedure we work the "general case" first and only then a couple of concrete examples.
Suppose, then, that a, b ε Z are given and that, to avoid trivialities, neither a nor b is zero.
We use the division algorithm repeatedly as follows:  and in general At step (t) we find  such that .
Since  and all the ri are non-negative we must eventually reach a first integer l for which .
Thus the lth step in the above then reads: At step (l) we find  such that  This last equality implies  and as a consequence .
Now it is not difficult to see that (a, b) = b, r1)(exercise 13) and that similarly .
Consequently, we see that  which can (except in the case where b\a, that is where rl = 0) be described as "the last non-zero remainder in the above process" .
We offer two concrete examples where the gcds are perhaps not entirely obvious in advance!
Examples 1.4.7 (i) Find (10 113, 21 671).
We apply the above procedure to the pair 21 671, 10 113.
Successively we get 
Thus (10 113, 21 671) = 1.
(Note that in fact neither of the given numbers is itself prime, since 10 113 = 3.
3371 and 21 671 = 13.
1667.
(ii) Find (30 031,— 16 579).
We apply the procedure to the pair 30 031, 16 579.
Successively we get 
Thus (30 031,— 16 579) = 59, a result achieved without factorising either number!
(Incidentally if the number 30 031 has no special significance for you, I think I can safely claim that you have not properly settled the conjecture following 1.3.10.)
1.4.4 asserts that there exists s, t ε Z such that 1 = 10 113 s + 21 671 t.
To find these we simply read the steps of the Euclidean Algorithm in 1.4.7(i) backwards, as follows.
 as required.
Thus we may choose s = 10 828 and t = — 5053.
Of course these coefficients are by o means uniquely determined since we can also write  where k is an integer.
On occasions it might prove convenient to compute the gcd and s and t simultaneously by putting the two procedures described above into one table, as shown in Table 1.
l.
We leave the reader to work out exactly how the various rows in the table are arrived at.
Remark The above working of the Euclidean Algorithm in the general case is easily seen to provide another proof of the existence of and the formula for the positive gcd of any two integers (not both zero).
However, I am sure everyone would agree that the proof of 1.4.4 is much, much sweeter, the proof based on the Euclidean Algorithm being somewhat ungainly.
Thus we have the interesting situation of having two proofs concerning gcds.
The first offers a beautiful proof of existence but is fairly useless for calculations.
The   second is perfect for specific calculations and also gives a proof of existence.
On the other hand this proof seems to have little aesthetic merit.
Finally in this section we prove (at last!) that every irreducible element in Z is necessarily a prime element so that the concepts of primeness and irreducibility coincide in Z. We need a definition and a trivial consequence.
Definition 1.4.8 Two integers a, b are said to be relatively prime (or coprime) iff (a, b) = 1.
Combining this definition with 1.4.4 we obtain immediately Theorem 1.4.9 Let a, b ε Z. Then a and b are relatively prime iff there exist, in Z, integers s and t such that sa + tb = 1.
At last we have Theorem 1.4.10 Let a ε Z be an irreducible element.
Then a is a prime element.
Proof Suppose b, c ε Z and a\bc.
[We want to show that a\b or a\c (or both).]
If a\b we are finished so suppose .
It follows that (a, b) = 1.
[Since a is irreducible its only divisors are a, -a, 1, -1, and since  the only divisors a can have in common with those of b are 1 and -1.]
Thus, by 1.4.4 there exists s, t ε Z such that sa + tb = 1.
But then sac + tbc = c.
Now a\sac [you can see that it does]and a\tbc [because…why?]
Thus a\c, as required.
[In summary: we have proved that either a\b or, failing that, a\c.
Thus by 1.3.5 (iii), a is a prime element in Z.]
Remark As already noted in the system H of exercise 1.3.6, irreducible elements are not necessarily prime ones.
Of course H is intended only as an easy illustration of what might go wrong — and go wrong it does in some of the important generalisations of Z that arise in Section 3.6.
 
1.5 The unique factorisation theorem (two proofs)
Even though we have just proved the equivalence of the concepts of primeness and of irreducibility in Z we ask the reader to note that in the following theorem the concept of irreducibility is associated with the existence of a decomposition of the asserted kind whereas the property of primeness is used to establish the uniqueness of this decomposition.
Theorem 1.5.1 (The Unique Factorisation Theorem for Z; also called the Fundamental Theorem of Arithmetic) Let a be a non-zero element of Z. Then either a is a unit or a can be expressed as a product of a unit and finitely may positive primes.
Further, if  where u, v are units and  are positive primes then u = v, r = s and the pi and the qi can be paired off in such a manner that paired primes are equal.
Proof Half of the theorem has been proved already; Lemma 1.3.9 and Remark (ii) following it show that every integer greater than 1 (respectively, less than -1) can be expressed as a product of (respectively, -1 times a product of) finitely many positive irreducibles (which we now know to be primes).
Now suppose there exists an integer a with decompositions as above but in which the pi and qi do not pair off.
If a is a negative integer then  is a positive integer with the same nasty property, and the set S of all positive nasty as is non-empty.
Thus S contains a smallest member.
WLOG let it be  Now  and clearly .
By exercise 1.3.3 we deduce [because pi is prime]that  for some i. but qi is irreducible and so .
Since p1 is not a unit and since p1 and 1i are both positive we are forced to conclude that p1 = qi.
Thus the above equality reduced to .
[Why are we allowed to cancel?]
But this integer is clearly smaller than  and so it does not lie in S. That is, unique factorisation does apply to this smaller integer and we can deduce that the remaining  pair off (in particular r -1 = s — 1 so that r = s) in the manner described in the statement of the theorem.
Since p1 and qi have already been paired off in the appropriate manner (i.e. they are equal!) we find that  is not a nasty integer after all.
That is, S is empty, and this proves the theorem.
Remark If we admitted 1 and -1 as primes we would lose the uniqueness we've just obtained since, for instance, 7 = 1.7 = 1.1.7.
Hence the restriction on primes and irreducibles being non-units.
Before closing this section we offer a delightful second proof of this last theorem in which (the reader is invited to verify) no use is made of the concept of primeness.
The reader is also invited to ponder which of the two proofs gives him the greater personal satisfaction.
(The author's preference should be obvious!)
This time we shall work throughout with decompositions of integers  into products of positive irreducibles.
Second proof of Theorem 1.5.1 Once again we quote 1.3.9 (which makes no use of the primeness property) to establish the existence of a decomposition (into irreducibles) in each case.
To establish the uniqueness of this decomposition we proceed as follows.
If the assertion relating to uniqueness is incorrect then there exists a smallest positive integer  c, say, such that c is expressible in two essentially distinct ways.
Suppose indeed that  where the pi and qi are (positive) irreducibles.
Sine [by the minimality of c]no pi can be equal to any qi, we may assume [WLOG]that .
Now  is the decomposition of q1 — p1 into irreducibles.
(If q1 — p1 = 1 we get c — d =  Now .
hence .
This means that c — d has a decomposition [into irreducibles]of the form  where  is a decomposition of (c — d) p1 into irreducibles.
Thus c — d has two essentially distinct decompositions into products of irreducibles.
For: p1 occurs in one decomposition but not in the other since (i) p1 is not equal to any of  and (ii) p1 cannot be equal to any  (or else  whence  which is impossible — why?).
Since  it appears that our assumption that there exists a smallest counterexample is untenable and so the theorem is proved.
Remark In Section 3.6 we shall discuss number systems in which the concepts of prime and irreducible definitely do not coincide and in exercise 3.7.16 we shall ask the reader to criticise the application of the above proof (since it leads to a manifest contradiction!) to these systems.
See also exercise 2 below and exercise 3.11.1.
1.6 Polynomials — what are they?
We now try to duplicate for polynomials some of the basic definitions and theorems set down earlier when investigating Z. We shall find out that, despite the obvious differences in appearance between the elements of Z and those of Q[x], the underlying structures of these two systems are quite similar.
Before beginning this attempted duplication we ought perhaps to remind ourselves exactly what polynomials are.
This is easy.
A polynomial in x with, let us say, rational number coefficients is simply an expression of the form  for some m ε Z and some ai ε Q. Further we define addition and multiplication of polynomials in the usual way.
That is we define addition by  and multiplication by 
At this point the author would like to clarify a few apparently trivial points.
(i) Are we to consider  as being of "the same form" as  or not?
(ii) What actually is x?
(iii) In the above definition of addition it seems that the + sign which is being defined (that is, the one between the brackets) is being defined in terms of itself!
(Witness the same + signs inside the brackets.)
Am I right?
Is this fair?
It would appear then that not everything in the garden is quite as rosy as might have been suspected.
Certainly one would like the answer to (i) to be "yes" ; and in fact that's easily arranged.
And (iii) causes no real problem.
One just changes the sign between the brackets to  and proceeds a little more carefully.
Point (ii) however is a bit more problematical.
To say that x is an "indeterminate" is no good.
(What on earth is an indeterminate?)
No more can x be "something which stands for a number" since one could equally well imagine it standing for a matrix or even the differential operator  
[Actually the same sort of problem arises in the definition of a complex number as "a number of the form a + bi" .
Questioner: What is "i" ?
Responder: It is .
But what is that?
Is it a number?
Well…yes.
It can't be a real number since its square is negative.
Of course.
So on what grounds do you assert that, for instance, …
No reply!
We shall deal with these questions in Section 4.4.]
There's an easy way to avoid embarrassing questions about x (just as there is about i).
Dispense with x!
Notice that the symbols  are really nothing more than place markers.
This is especially noticeable (now you know to look for it!) when multiplying two polynomials in the usual manner.
Taking into account the problem raised in (i) above we make the formal Definition 1.6.1 (i) A polynomial with coefficients in Q is simply an infinite sequence  in which the ai all belong to Q and in which all the ai are equal to 0 from some point onwards.
(more formally: all the ai belong to Q and there exists N ε Z such that  for all n such that .
Of course N is allowed to vary from one sequence to another.)
The set of all polynomials with coefficients in Q will, for reasons which will emerge in Section 1.8, be denoted, as usual, by Q[x].
(ii) The polynomial  and the above polynomial  are said to be equal iff ai = bi for all on-negative integers i.
(iii) Addition is defined by  where, for each non-negative integer 
(iv) Multiplication is defined by  where, for each non-negative integer  That is, we put 
Remarks (i) The symbols  (the first of which is often called "hot cross plus" !) are used in preference to + and.
to distinguish them from the signs by which we  are already combining elements of Q. The new symbols are sufficiently unfamiliar to remind us of their defining role but sufficiently similar to + and.
to help us remember their meaning with a minimum of effort.
(ii) It is probably clear to the reader that, although we are trying to work formally and to assume nothing other than that given, we do not choose our definitions of  and  in a perfectly random fashion, but rather make them reflect what we want to happen.
Remember the aim here is to get rid of the problems concerning x, not to get rid of the concept of polynomial itself!
Exercises 2 and 3 will convince doubters that our definitions of  and  are the right ones!
(iii) According to the above definitions, the sum and product of two given polynomials are again polynomials.
Clearly each sum and product is a sequence so the crux of the matter is to show that, in each case, it is a sequence with only finitely many non-zero terms (exercise 6 below).
 
1.8 The "new" notation
Definition 1.6.1 is all very well in that it gets rid of any nasty problems concerning x, by getting rid of x!
Further, exercises 1.6.2 and 1.6.3 indicate that these sequence-type polynomials do appear to behave as we wanted them to.
(This is scarcely surprising since we defined  and  so that they would!)
But exercises 1.6.2 and 1.6.3 and perhaps, more especially, exercise 1.6.5 emphasise that sequence-type polynomials are a little difficult to handle.
Why is this?
The answer is simple.
Unfamiliarity.
We can work using the x-form faster than we can the sequence form simply because we've been using the former for years.
Let's see how we can recover the familiar x-form from the sequence-type definition.
To begin with, consider the subset  comprising those polynomials (i.e. sequences) whose every term, excepting possibly the first, is zero.
Thus we are looking at polynomials such as  For two such polynomials (a, 0,…) and (b, 0,…) we note that, according to 1.6.1, their sum and product are respectively (a + b, 0,…) and (a.b, 0,…).
We see, then, that the brackets, commas, zeros and dots are just so much dead wood in that, if we write (a, 0,0,…,) more briefly as  then the above addition and multiplication can be written much more briefly as 
We achieve even more economy by giving the name "x" to the polynomial (0,1,0,…).
For then we observe that  and in general  in our new notation.
Finally we observe that  and so, if we replace the  sign by +, if we simply drop the  sign and if we then agree to write  we establish a notational way of writing polynomials that we might just find a little easier to work with!
That is, we are now proposed to write () more intelligibly as .
It appears that the bars over the as are also superfluous since (on using the new notations + rather than  etc.) .
So we drop the bars too and find that with the conventions described above we have shown that every polynomial can be expressed in the form .
Note that the ways of adding and multiplying polynomials in this new notation are precisely those you have always used.
Remarks (i) The reader who feels both exhausted and cheated at this outcome (if any such reader there be) will perhaps comment: "What a waste of time.
All that just to get back to where we started."
In fact we haven't quite got back to where we started.
We have shown that polynomials can be thought of in the way we have always thought of them, secure in the knowledge that uncomfortable questions about x can be circumvented.
(ii) In recognising that the sets Q acted upon by + and.
and Q acted upon by  and  are essentially the same, we are meeting for the first time a concept of prime importance in algebra, namely that of isomorphism.
This concept can perhaps be more readily appreciated by looking at Fig. 1.1 which also clearly indicates that whilst the elements of Q are totally distinct from  those of Q[x], nonetheless Q[x] contains the subsystem Q which to all intents and purposes is the "same" as Q so that, if it proves convenient, Q may be identified with it.
Such identification of the elements of Q with those of Q is in exactly the same spirit as the identification of the elements of Z with certain elements of Q (see Fig. 1.2), an identification to which you've probably never previously given much thought.
(For more on this see Sections 3.10 and 5.9.)
(iii) At this point we should like to draw the reader's attention to a second aspect of polynomial algebra which is sometimes badly dealt with.
The question arises: What are we to make of the equality ?
Since the left- and hence the right-hand sides of this equality are polynomials they are simply shorthand for (2, -3, 1, 0,…) and (0,0…) respectively.
Hence 1.6.1(ii) tells us to deduce that 2 = 0, -3 = 0 and 1 = 0 in Q!
We leave the reader to think about this until we reach Section 1.11.
Note however that the assertion  is certainly one which is meaningful — and also happens to be true!
These remarks indicate that, if nothing else, we ought to proceed with circumspection.
1.9 Divisibility, irreducibles and primes in Q[x]
The fact that both Z and Q[x] satisfy, with respect to addition and multiplication, most of the same axioms (both failing to pass M4) leads us to ask if other concepts defined for Z have analogues in Q[x].
(Of course, even if they do, it will not necessarily mean that these concepts will have the same importance for Q[x] as for Z.)
Following Section 1.3 we being with Definition 1.9.1 (cf 1.3.1) Let f, g ε Q[x].
We say that f divides g (or that f is a divisor of g) and we write f\g iff there exists h ε Q[x] such that g = fh.
If f does not divide g we write .
Examples 1.9.2 
Definition 1.9.3 (cf. 1.3.5)(i) If u ε q[x] is such that u\1 then u is called a unit.
(ii) If f ε Q[x] is neither the zero polynomial 0 nor a unit we say that f is irreducible iff, whenever f is expressed as a product, f = gh with g, h ε Q[x], it follows that either g or h is a unit.
A non-zero non-unit polynomial f will be called reducible iff it is not irreducible.
(iii) If f ε Q[x] is neither 0 nor a unit we say that f is prime iff, whenever f divides a product (that is f\gh where g, h ε Q[x]) it follows that f\g or f\h (or both).
(iv) if f, g ε Q[x] are such that g = gu, where u is a unit, then f and g are associates.
Remarks (i) We emphasise that the requirement u\1 in 1.9.3(i) demands that we find an element v in Q[x] such that uv = 1.
Thus according to 1.7.2 the units in  Q[x] are precisely the non-zero constant polynomials — and there are infinitely many of them.
It follows that each element of Q[x] has infinitely many associates.
(ii) Instead of Q[x] we could equally well have considered Z[x], R[x], C[x], each defined in the obvious way.
Note that in Z[x] the only units are the polynomials 1 and -1 and that whilst  in Z[x].
(iii) Whether or not a polynomial is irreducible depends upon which set of polynomials it is considered as belonging to.
For example, the polynomial  is irreducible in Q[x] and R[x], but reducible in Z[x] and in C[x].
Can you see why in each case?
Note 1.9.4
(i) In Section 1.5 we proved that each non-zero non-unit element of Z factorises into a product of irreducibles in an essentially unique way.
We naturally enquire whether or not the corresponding result holds in Q[x].
Analysis of 1.5.1 shows that it does if Q[ [x]satisfies analogues of 1.3.9, 1.4.10, exercise 1.3.3 and property C of Section 1.2.
Now 1.5.10 calls upon 1.4.4 which itself uses 1.4.5 which…!
The energetic reader is invited to supply the details.
To give it maximum prominence the statement concerning factorisation in Q[x] is placed at the end of this section (1.9.18).
However, instead of checking this chain of results for every concrete example where we seek to prove uniqueness of factorisation, it would be better if we could isolate the essentials behind these theorems leaving us in each specific case only these essentials to verify.
This we shall do in Section 3.7.
(ii) Because (the analogue of) property C can be proved in Q[x], the analogue of 1.3.7 is easily seen to hold there.
Together the analogues of 1.4.10 and 1.3.7 show that, in Q[x], an element is prime if and only if it is irreducible.
Thus appears a very minor reason for our introducing the prime versus irreducible battle in 1.3.5 and 1.9.3: primes (i.e. irreducibles) in Z are usually called primes!
Primes (i.e. irreducibles in Q[x] are usually called irreducibles!
Thus any difference implied by use of different words in Z and Q[x] is illusory.
(iii) One final point.
The proofs of 1.3.9, 1.4.4 and in particular the Division Algorithm 1.4.5 make use of the fact that property W holds in Z. Since, despite exercise 1.7.4, Q[x] does not seem to satisfy W (try it!), we are fortunate that the appropriate analogue for Q[x] of 1.4.5 depends upon Z, rather than upon Q[x], satisfying W. We obtain this analogue in 1.10.1.
Somewhat more perplexing however is the following.
In proving in Z and in Q[x] that all irreducibles are primes we make essential use of the appropriate division algorithm.
However it is easily seen (exercise 1.10.2) that there can be no analogue of 1.10.1 in Z[x].
This in itself does not immediately imply that there are irreducibles in Z[x] which are not primes.
Of course if there are, the analogue for Z[x] of 1.5.1 could well fail since the proof as given certainly wouldn't then go over to Z[x].
And yet surely factorisation in Z[x]  is unique?
What do you think?
We pose Problem 3 Are all irreducibles in Z[x] also primes?
Get your hands dirty by doing a bit of practical experimentation.
You might even stumble upon a theorem or two in your researches.
We now ask the obvious question: Which polynomials in Q[x] are irreducible?
The best reply we can give is: There is a test (see exercise 1.11.11) due to Kronecker which will always tell in a finite number of steps (which may easily be large enough to require a computer) whether or not a given element of Q[x] is irreducible.
Otherwise there is a criterion (1.9.16) which is easy to apply in practice and which describes infinitely many (but not all) polynomials irreducible over Q[x].
Other tests are given by the remark following 1.11.7 and 4.2.10(ii).
To establish 1.9.16 we make some definitions and proofs relating to Z[x].
For ease of recognition we denote polynomials in Z[x] by capital letters.
Definition 1.9.5 A polynomial  is called primitive iff the positive gcd in Z of all the zi is equal to 1.
Example 1.9.6  is primitive,  is not.
Notation 1.9.7 Extending the notation of exercise 1.4.6 we denote the positive gcd of  (not all zero) by .
Such a gcd always exists and is unique.
A word for word copy of the proof of 1.4.4 shows this gcd is equal to the smallest positive integer in the set 
A passage from Q[x] to Z[x] is provided by Lemma 1.9.8 If f ε Q[x] then there exists in Z[x] an associate F = αf of f (where α ε Q is chosen to be positive) such that F is primitive.
Proof I'll leave you to outline this.
The following example shows you the way.
Example 1.9.9. 
The next result is due to Gauss (see p. 68).
Theorem 1.9.10 If F =  and G =  Z[x] are both primitive then so is FG.
Proof Let FG = .
If FG is not primitive then  have a common divisor (other than 1 and -1) in Z and hence  some common prime divisor p, say.
Now p doesn't divide all the yi nor all the zi.
[Why not?]
Let s and t be the least suffices for which  Consider  now  [since  and p is a prime].
On the other hand [by choice of s, t]p does divide every other term in the above equality including  itself.
[This manifest contradiction shows that the assumption that FG is not primitive is untenable.]
Thus FG is primitive, as required.
For the next theorem we need two definitions.
Firstly Definition 1.9.11 Let 
The gcd  is called the content of F.
Example 1.9.12 The content of 
Secondly Definition 1.9.13 Let f =  Then n is called the degree of f.
We write deg f = n.
(We do not attribute a degree to the zero polynomial f = 0).
We can prove the important Theorem 1.9.14 (Gauss) If F ε Z[x] and if we can write F = gh where g, h ε Q[x] then we can write F = GH where G, H ε Z[x], deg G = deg g and deg H = deg h.
Remark The real meat in this theorem lies in the equalities of the degrees of G and g and of H and h.
Without these equalities the theorem is true but absolutely trivial.
Proof (i) We first suppose that F is primitive and write  where a, b, c, d ε Z+ are such that (a, v) = 1 (c, d), where G, H ε Z[x] and are primitive and where deg g = deg G and deg h = deg H.
Thus  both primitive polynomials [by assumption and by 1.9.10 respectively]and so the content of bdF is bd [why?]whilst the content of acGH is ac.
[Since the polynomials bdF and acGH are equal their contents must be.]
Thus ac = bd and F = GH follows [from the equality above]as required.
(ii) If now F is not primitive, write F = zF1 where z is the content of F and where F1 is primitive.
Then, from F = gh we get .
Hence, by (i),  and consequently  where .
As an immediate corollary we obtain a result you might have stumbled on in your researches into Problem 3.
Corollary 1.9.15 If F ε Z[x] and if one can't write F as a product of polynomials, each of smaller degree and with integer coefficients, then one still can't write F as a product of polynomials each of smaller degree even if one is prepared to use polynomials with rational coefficients.
Equivalently: If F ε Z[x] is reducible in Q[x] then it is already reducible in Z[x].
We now give the solution to Problem 3.
We take care in stating exactly where each division is taking place.
(Recall that 2\3 is false in Z but true in Q.)
Solution to Problem 3 Let F ε Z[x] be irreducible (in Z[x]).
Then F is prime (in Z[x]).
Proof Suppose F\GH where G, H ε Z[x].
[We must show that F\G in Z[x] or F\H in Z[x].
Now F is irreducible in Z[x].
Thus if deg  is primitive and is also irreducible in Q[x] [by 1.9.15].
Anticipating the proof (see 3.7.2 (ii) and 3.7.8) that in Q[x] irreducibles are necessarily primes we see that F is prime in Q[x].
Hence [by the definition of primeness]either F\G in Q[x] or F\H in Q[x].
Suppose WLOG the former.
Then there exists s ε Q[x] such that Fs = G. Writing  where  and S ε Z[x] is primitive, we obtain aFS = bG.
Equating contents [noting that FS is primitive]we see that a = b.
(content of G).
Thus  whence s ε Z[x].
This proves F\G in Z[x].
If deg F = 0 then F is irreducible, hence prime in Z and hence prime as an element of Z[x].
[Three assertions have just been made.
Do your understand them?
Do you believe them?
Can you prove them?
Cf. exercise 5.]
We now come to the first of the simpler tests for irreducibility over Q[x].
This one is generally ascribed to Eisenstein.
As implied by 1.9.8 we can do all our working in Z[x].
Theorem 1.9.16 (Eisenstein's Test) Let  If there exists a prime p in Z such that(i)   (ii)  then F is irreducible in Q[x].
Proof Supposing F is reducible in Q[x], we may assume [which theorem?]that F = GH where  and H =  and where  and .
We have  etc.
Since  we know that  or  [since p is prime), but this time not both since .
Suppose WLOG that  and .
Now not all the bi are divisible by p or else all the ai would be, contrary to .
Let k be the smallest suffix for which .
Then .
Now  and  since .
Also p divides all the terms  except the last [since  and .
This contradiction established the theorem.
Example 1.9.17 (i)  is irreducible in Q[x].
For: We write f = .
Noting that 3\3, 3\45,…,  and  we see that Eisenstein's theorem tells us that the polynomial in brackets is irreducible in Z[x].
Consequently so is f, in Q[x].
The following type of polynomial plays an important role in Gauss' theory of constructible regular polygons (Section 4.6) and in the number theory described in Sections 3.5 and 3.9.
(ii) Let p be a prime in Z+.
Then the pth cyclotomic (circle dividing) polynomial  is irreducible in Q[x].
For: If C = fg in Q[x] then  in Q[x] where, in each case, the polynomial ha is defined as that obtained from h by replacing x by a + x.
In particular .
This is most easily simplified by noting that C can be written formally as  and consequently 
Here, each binomial coefficient displayed (except ) is divisible by p (exercise 1.4.17) and so Eisenstein's test says that C+1 is irreducible in Q[x].
Thus there are no such f, g ε Q[x] (apart from trivial ones) whose product is  C+1.
Hence there are no f-1, g-1 ε Q[x] (except trivial ones) whose product is C. Thus C is irreducible, as required.
[All right?]
Remarks (i) One might try to employ this sort of manoeuvre (exercise 10(c)) on any given polynomial one suspected of being irreducible.
One trouble is that one wouldn't know for which integers k, if any, the replacement of x by x + k would change the given polynomial into Eisenstein form.
(ii) Eisenstein's criterion shows that there exist in Q[x] infinitely many prime (i.e. irreducible) polynomials of each degree .
(iii) As we did with the integers (1.3.9) we can use induction (but this time on the degree) to show that every non-zero non-unit polynomial in Q[x] can be expressed as a product of a finite number of irreducible ones.
As implied in note 1.9.4(i) an analogue of 1.5.1 can be established in Q[x].
This analogue is: Theorem 1.9.18 (Unique Factorisation Theorem for Polynomials) Let f be a non-zero element of Q[x].
Then either f is a unit or f can be expressed as a product of a unit and finitely many monic* irreducible polynomials.
Further, if  where u, v are units and  are monic irreducible polynomials, then u = v and the gi and the hi can be paired off in such a manner that paired polynomials are equal.
This result will finally be established in 3.7.13.
1.11 Roots and the remainder theorem
Towards the end of Section 1.8 we noted that the only interpretation one could put upon the equality  between the two polynomials mentioned is that 1 = 0, -3 = 0 and 2 = 0 in Q. Thus the above equality is meaningful but blatantly false.
It is because such equalities are therefore not worth investigation (they are all false!) that a totally different interpretation is usually attached to the statement .
This interpretation is expected to elicit from the reader the response: "That's easy.
x = 2 and x = 1 are the only solutions to this equation."
Any feeling of insecurity in the reader's mind concerning this dual interpretation of  is not so much  due to the perversity of the author but is rather due to the dual role that the public at large expects x to play.
In popular language these two roles are those of "indeterminate" and "variable" respectively.
This is perhaps best illustrated by considering the assertion that "one way to show that x — 2 is a factor of  is to put  ad show that the result is 0" .
We can't put x = 2 or anything else for that matter: x is the infinite sequence (0,1,0,…).
[Nor can we apparently "substitute 2 for x" since the result, namely , is meaningless.
On the other hand we can get round these difficulties, so that we can talk (loosely) as above without fear of any difficulties arising, by doing something which looks very much like substitution.
In what follows we shall let J stand for any one of Z, Q, R or C.
Definition 1.11.1 (i) Let us, for brevity, denote the polynomial  by f.
For each c ε J we define the value of f at c to be the element  We denote it briefly by f(c) and refer to the process of obtaining f(c) from f in this way by the ill-chosen but universally familiar expression "substitution of c for x in f" since on the face of it we have just replaced every occurrence of x in f by c.
(ii) If t ε J is such that f(t) = 0 ε J we call t a root or zero of f in J.
(iii) The expression "Solve the equation  = 0 in J" means: "Find all elements c ε J such that f(c) = 0 in J." 
Notes 1.11.2 (i) What we are actually doing in 1.11.1(i) is to associate with the sequence  and the element c ε J the element  in J.
(ii) In the expression  the ai are abbreviated notations for infinite sequences; in  the ai are actual elements of J.
(iii) The assertions that one can solve  = 0 in Z and that one cannot solve  = 0 in Q should now be completely unambiguous (and both are true).
We can now prove a result which is often stated and proved in a rather casual manner.
In order not to obliterate its intuitive meaning, and yet satisfy ourselves that we are not falling into a subtle trap, we denote the constant polynomial (a, 0,0,…) in J[x] once again by .
It might be quite instructive for the reader to select an algebra text at random and see how many, if any, holes he can pick in the proof of the following theorem given therein.
Theorem 1.11.3 (The Remainder Theorem)   Proof By the division algorithm we can surely find polynomials m, r ε J[x] such that f = m(x-a) +r.
Further, either  [the zero polynomial]or .
In this case deg r = 0 and r is (again) a constant polynomial.
Thus set  where b ε J. If we now, according to 1.11.1 substitute a for x in each side of the above equality we obtain  from which one easily sees [if one thinks carefully what the symbols mean]that f(a) = b.
Hence  as required.
We leave the second part of the theorem to exercise 1.
Remark If in 1.11.3 we adopt the notation finally settled on in Section 1.8, that is we drop the bars, we obtain the remainder theorem in its usual guise.
Notice that then the symbol a is being asked to play two roles simultaneously, namely as an element of J and as a polynomial in J[x] — unless (see Fig. 1.1) one is prepared to regard J and  not just as isomorphic but as being one and the same (that is, identical) so that J is being regarded as a subset of J[x].
Now we've shown that the sloppy approach can be put on a firm footing we shall, because of its greater familiarity, lapse into it!
There now follows immediately Theorem 1.11.4 Let J be any one of R, r, C and let f ε J[x] such that deg .
Then f has at most n roots in J.
Proof We proceed by induction on n.
If f has degree 1 then f has exactly one root in J. Now suppose f is a polynomial of degree K + 1.
If f has no root in J. then since  the result holds.
Otherwise suppose b is a root of f in J. Then by 1.11.3 we may write f = (x-b) m where by exercise 1.9.9 deg m = k exactly.
By the induction assumption m has at most k roots in J. Since m is uniquely determined by f and x — b (so that no other polynomial m1, say, can supply up to k more roots) the assertion follows.
Note 1.11.5 Even if we count repeated roots according to their multiplicities (and it is usual to) the theorem still holds.
For example the quintic  has three roots (namely 1, 1, 2) in R.
A second practical irreducibility test referred to in Sections 1.9 and 1.10 follows from Theorem 1.11.6 (The Rational Root Test) If r/s is a rational root of the polynomial 
Proof If r/s is a root then .
Multiplying through by  Clearly s then divides  and hence  by exercise 1.5.5.
In a similar manner one proved .
Example 1.11.7 If  has a rational root r/s with (r, s) = 1 then .
Hence r/s has four possible values, namely 1, -1, 2 and -2.
Substituting each of these in turn in the given polynomial never gives the value 0.
Hence  has no rational root.
Remark With regard to irreducibility, 1.11.6 is chiefly of use in showing certain cubic polynomials in Q[x] irreducible, because if a cubic polynomial is reducible at least one of the facts must have degree 1.
hence such a cubic polynomial has a root in Q. The theorem is not of immediate use in checking polynomials of degree 4 or more for irreducibility.
Indeed the above quartic which has no root in Q nonetheless factorises as  in Q[x].
Whilst talking about reducibility of polynomials we mention a very important result which we shall, using just a tiny bit of analysis, prove in full later (Theorem 4.8.1).
Theorem 1.11.8 (The Fundamental theorem of Algebra) Let f ε C[x] be such that deg .
Then f has a root in C. It follows that f factorises completely into linear factors in C[x] so that if deg f = n then f has, including repeats, n roots in C.
Applying this to R[x] we get Theorem 1.11.9 If f ε R[x] then f can be expressed as a product of polynomials of degrees at most 2 in R[x].
Proof By exercise 8 below we se that if a + ib is any complex root of f, then a — ib is also a root of f.
Thus non-real roots of f (if there are any) occur in complex conjugate pairs.
The observation that  then suffices.
And as a corollary, Corollary 1.11.10 An irreducible polynomial in R[x] is either of degree 1 or of degree 2.
Remark Given a polynomial f in R[x], 1.11.10 restricts possible irreducible factors to having degree 1 or 2.
This knowledge is unfortunately of little help in actually factorising f and in general we have to be content with approximation methods.
Investigations along these lines naturally belong to numerical analysis.
 
2 Binary relations and binary operations
2.1 Introduction
If we let X stand for any one of the sets X, Q, R, C, Z[x], etc. mentioned in Chapter 1, then the operations of addition and multiplication defined on X may be described as binary operations on X in that, to each pair of elements of X, both + and.
produce a unique entity which is again an element of X.
As remarked in the prologue, present-day algebra might be defined as the study of (n-ary) operations on sets (n = 2, but also 0, 1, 3, 4,…etc.).
This would, however, be a little unfortunate since only relatively few n-ary operations are either interesting or important.
In later chapters we shall investigate in depth some of those binary operations which have, by their repeated occurrence and usage, shown their importance.
(For the sake of illustration we shall also introduce some which, to say the least, are of little significance!)
The general concept of binary operation on a set is defined in terms of that of "function" .
No doubt the reader can give what he feels is an adequate definition of "function" .
(Indeed we invite each reader, here and now, to make such a definition and compare his proposal with ours, given later.)
However, in keeping with the somewhat critical approach adopted in Chapter 1, and because we also wish to indicate how our studies might be put upon a set-theoretical base (the soundness of which is the concern of set-theorists) we propose definitions rather more formal than many a reader might expect.
Any reader worried by the prospect of such formality will be pleased to see how, as with polynomials in Section 1.8, the more informal notation to which he is more accustomed is soon restored.
The formal approach requires that we start with a study of binary relations.
2.2 Congruence mod n.
Binary relations
We begin with some simple number theory which will help us feel our way and also lead us to some new and fascinating algebraic structures.
Let n be a positive integer, any one you wish, but fixed once chosen.
Given two integers a and b, it is the case that either (i) n\a -b or (ii) .
[Equivalently: a and b (i) do have or (ii) do not have the same remainder on division by n.]
In the first case we shall write a = b (mod n), in the latter  (mod n).
These we shall read as a is (is not) congruent to b modulo [more  briefly, mod]n.
Thus 7 = -3 (mod 5) whilst  (mod 5).
Working modulo 5, then, we see that certain pairs of integers are related (by being congruent mod 5) whilst other pairs are not so related.
There are other more familiar ways in which certain parts of integers appear to be related whilst other pairs are not.
For example, since  the ordered pair (3,7) is replaced by  whereas the ordered pair (7.3) is not.
The symbol "=" yields another relation on Z, two integers being related this time if and only if they are equal.
As an example of doubtful mathematical value consider, on the set P of all human beings above at, say, 10.05 a.m.
(GMT) on 7 April 1978, the relationship "is greatgrandfather of" .
Still thinking aloud it appears that the relationships , and "is greatgrandfather of" can be interpreted as subsets of Z x Z, Z x Z, Z x Z and P x P respectively.
For example,  can be identified with the subset of Z x Z comprising all ordered pairs (a, b) for which .
Similarly = can be identified with the subset{ (a, a): a ε Z]of Z x Z and  (mod 5) with the subset of all those (a, b) for which  (mod 5).
That these identifications are circular (equality in Z being identified with the set of all pairs,(a, b) for which a and b are equal!) is unimportant at the moment since we are still informally trying to feel our way.
Now, being formal, but motivated by the above, we make Definition 2.2.1 A binary relation on a set A is a subset of A x A.
One easily solved and one unsolved problem in number theory are included in Examples 2.2.2 (i) The binary relation  is a finite set comprising just the two elements (2,4) and (4,2).
[Challenge: Prove it!]
(ii) The relation  and x, y both primes}.
Thus R2 contains the pairs (3,5),(5,7),(11,13),…, ,
[It is an unsolved problem of number theory as to whether or not R2 is a finite set.
Each pair{ x, y}is called a pair of twin primes.]
(iii) The relation  and x is greatgrandfather of y].
It seems unlikely that many readers will find themselves as first member (that is, in the x-place) of any element of R3.
(The author would certainly be pleased to hear from any such greatgrandfather!)
Having shown (in 2.2.1) how we can give an unambiguous definition of binary relation in set-theoretic terms we make the notation more readable.
Thus Notation 2.2.3 Let R be a binary relation on a set A. If  we write  and say that a1 is related to a2 under (or by) R. Otherwise we write  and speak accordingly.
On any given set few (if any!) binary relations will have mathematical significance.
Indeed those binary relations occurring most frequently in practice possess special properties including some or all of: Properties 2.2.4 
Definition 2.2.5 If R satisfies (r) we say that R is a reflexive binary relation.
If R satisfies (s) we say that R is a symmetric binary relation.
If R satisfies (t) we say that R is a transitive binary relation.
If R satisfies (a) we say that R is an antisymmetric binary relation.
If R satisfies (r),(s) and (t) we say that R is an equivalence relation.
If R satisfies (r),(t) and (a) we say that R is an order relation.
Note 2.2.6 To remember these latter two definitions the author uses the mnemonics rEst and rOta.
You will, of course, remember your own mnemonics all the better.
Examples 2.2.7 (i) Since for all x, y, z ε Z we have: .
we see that  is an equivalence relation on Z.  does not satisfy (a).
(ii) The relation  on Z satisfies (r),(t) and (a) and thus is an order relation.
[This basic example motivates the use of the words "order relation" in 2.2.5.]
(iii) The relationship "is brother of" on the set of all male human beings satisfies (s) but, in normal parlance, not (r).
What about (t) and (a)?
(iv) The relation  on the set{ a, b, c}satisfies only (r).
(v) On the set of all triangles in the plane the relationships of similarity and of congruence are equivalence relations.
(vi) On  the relation R defined by (a, b) R (c, d) iff ad = bc is an equivalence relation of a kind which we shall meet again in the proof of 3.10.3.
2.3 Equivalence relations and partitions
Equivalence relations are often more readily comprehended and also constructed in terms of the concept of partition.
Definition 2.3.1 Let A be any non-empty set and let  be any collection of non-empty subsets of A such that(i) the union of the sets in  is A and (ii) each distinct pair of sets in  has empty intersection.
Then  is called a partition of A.
Examples 2.3.3 (i) the subsets of even integers (include 0) and of odd integers define a partition of Z.
(ii) The sets of all males, of all females and of all joggers do not form a partition of the human race.
(iii) The concepts of similarity and congruence define partitions on the set T of all triangles in the plane.
Partitions give rise in a natural way to equivalence relations because of Theorem 2.3.3 Let A be a non-empty set and let  be a partition of A. Define a relation R on A by setting, for a, b, ε A, aRb iff a lies in the same element of  as b.
Then R is an equivalence relation on A.
Proof (r) Each a in A lies in some subset of A belonging to .
Then aRa follows immediately [since a lies in the same element of  as does a!].
(s) is also immediate.
[If a belongs to the same element of  as b then b belongs to the same element of  as a.]
Thus .
(t) If a belongs to the same member of  as b and if b belongs to the same member of  as c then…[can you finish it?].
That is, aRb and bRc .
On the other hand, each equivalence relation (e.r.
for short) gives rise naturally to a partition.
Theorem 2.3.4 Let R be an e.r.
on the non-empty set A. To each a ε A define â to be the subset{ x:x ε A and xRA}of A. [Thus â is the subset of A comprising all elements x of A which are related to a under R.]Then the set  is a partition of A.
We have thus shown that if two elements of  have non-trivial intersection then they are actually equal.
In other words, distinct members of  have pairwise empty intersection.
In particular if y ε â then â = ŷ, even though a and y might not be equal (cf. 2.3.7).
There is a small point you might care to ponder.
Problem 2 We've seen that every partition on a set A gives rise to an e.r.
on A and vice versa, each in a natural way.
Suppose I give you a partition, you construct the corresponding e.r.
and then I construct the partition corresponding to your e.r.
Do I obtain the partition I began with?
Terminology 2.3.5 Given an e.r.
R on a set A the members of the corresponding partition of A are called the equivalence classes of R.
Examples 2.3.6 (i) The relation R defined on the plane by setting  is an e.r.
The equivalence classes are the circles with centre the origin.
(ii) The relation R defined on Q[x] by setting fRg iff f(0) = g(0) is an e.r.
on Q[x].
Each class comprises all those polynomials with a particular constant term.
We now return to our most important example, that of congruence mod n on Z.
Example 2.3.7 for each  is an e.r.
In particular, when n = 5 there are five equivalence classes, namely   Note that (i) two integers a, b lie in the same (equivalence) class iff  (mod 5), that is iff 5\a — b or, again, iff a = b + 5k for some k ε Z; and that (ii) although the notation  is most natural, these equivalence classes can also be denoted, respectively, by, for instance ,  and .
2.4 Zn
Let n be any positive integer.
Form the set of equivalence classes determined by the e.r.
.
We call this set Zn.
We now show how to manufacture very interesting "number" systems by introducing a kind of addition and multiplication into Zn.
These number systems are not mere curiosities.
Indeed certain of them have found application in coding theory and in statistics as well as being important in geometry and in algebra itself.
)[6],[52],[19]and 4.2.10.)
Using the notation of 2.3.4 let s and t be two elements of Zn.
Thus s and t are equivalence classes of integers mod n.
We define their "sum"   "product"  Definitions 2.4.1.  Note that, whatever else,  and  are certainly elements of .
Taking the specific example of n=7 we see that  whilst .
Information such as this is conveniently stored in the form of addition and multiplication tables as follows.
Here the values of  are exhibited at the intersection of the  row and the  column in the appropriate table.
Notes 2.4.2
(i) Our notation s is bad (but customary).
It's bad because it does not indicate which Zn one is looking at.
However, the context usually makes a more explicit notation (such as s n) unnecessary.
(ii) Corresponding tables for any other (fairly small) positive moduli n are easily constructed.
What has perhaps escaped your attention is that our definitions of  and  might not make sense!
The problem is this: In Z7, say, we have seen that .
Now 4 and 6 also go under the names 39 and -925, for instance.
Thus, according to our definitions,  whilst  and so, unless  and  are just other names for 3 and 1 w are in the intolerable situation that a sum or product of two classes depends not only on which the classes are but also on what we choose to call them!
Fortunately everything works out all right(2.4.3), a situation we describe by saying that  and  are well defined.
Thus Theorem 2.4.3 The  and  of 2.4.1 are well defined.
Proof Suppose  Then a = c + un, b = d + vn for suitable u, v ε Z. It follows that  whilst  as required.
Problem 3 We could have avoided the well-definedness problem as follows.
Denoting the elements of  we define sum and product unambiguously by  and  where, using 1.4.5, x + y =  and  with  and where  and .
However, one rarely gets something for nothing in mathematics and another small difficulty now replaces the well-definedness one.
Can you see what it is?
[Hint: Try proving .]
We now see which of the (analogues of the) axioms A1, A2,…,
M3, M4, and property Z, listed in Section 1.2, hold for  and  on Zn.
(We have axioms D, P and properties C, M of Section 1.2 to exercise 11.)
For A1 we take any two  and ask: Is ?
Indeed it is.
For  [by definition]whereas .
Since a + b = b + a in Z we have  in Zn as required.
The proof of M1 is similar.
Let us offer a streamlined proof that M2 holds.
We leave to the reader the explanation as to why each of the asserted equalities holds.
For all a, b, c ε Zn we have  The proof of A2 is similar.
Note 2.4.4 The rather pedantic observation that (ab) c = a(bc) is included here only to emphasise that the associativity of  depends heavily on that of.
in Z. Otherwise such pedantry relating to Z was left behind as long ago as 1.2.5.
Since for all â ε Zn we have  and  we see that the analogues of A3 and M3 hold in Zn.
So does A4.
For, given â ε Zn it is clear that .
We move on to M4 and Z. A quick glance at the  table for Z7 shows that both hold in Z7.
Consider however the  table for Z6.
It is, omitting circumflexes just for simplicity,
It is clear that 0 and 1 are again the (unique) elements required by axioms A3 and M3.
Further:(i)  and (ii) there is no element  for which .
Thus both M4 and Z fail in Z6.
What about M4 and Z for other Zns?
It is pretty clear that the reason Z fails in Z6 is that 6 is a composite integer.
It is equally clear that Z fails in Zn whenever n is composite.
Further, if  it is immediate that there can be no x ε Zn such that .
We have, therefore, Theorem 2.4.5 Let n be a compositive positive integer.
Then M4 and Z both fail in Zn.
On the other hand we can prove Theorem 2.4.6 Let p be a positive prime.
Then Zp satisfies both M4 and Z.
Proof (a)  (b) 
Problem 4 You may, even at first reading, have an uneasy feeling that the proof of (b) was not quite what you were expecting and that a more direct proof should be available.
Indeed there is such a proof.
Can you find it?
(The reason for giving the present proof will appear in exercise 3.3.5.)
Remarks We have just seen that in some ways the number systems Zp, where p is a prime, are more like Q, R and C than in Z itself in that all of A1, A2,…,
M3, M4, D, Z hold in each of them whereas M4 does not hold in Z. Indeed Zp, Q, R, C are all examples of fields (see 3.2.2(10)) whereas Z is not.
Carl Friedrich Gauss (30 April 1777 — 23 February 1855) Gauss, called by his contemporaries the Prince of Mathematicians, was one of the greatest scientists ever.
Not only did he do stupendous work in many areas of pure mathematics but he also devoted much time to probability, theory of errors, geodesy, mechanics, electromagnetism, optics and even actuarial science!
He was born in Braunschweig (Brunswick) Germany, to a poor family.
His mother was intelligent but not fully literate, his father at various times a gardener, general labourer and merchant's assistant.
It is said that aged three Gauss corrected an error in a wages list, whilst aged eight (some say ten) he wrote down in moments the answer to the following problem set in class: add together all the integers from 1 to 100.
(One must presume he saw the answer had to be one half of 100 x 101.)
Various sources claim that before age 20 he had amongst other things, rediscovered and proved the law of quadratic reciprocity, discovered the double periodicity of elliptic functions, proved that every positive integer is a sum of 3 triangular numbers (these are integers of the form  where n ε Z+), formulated the principle of least squares and conjectured the prime number theorem (itself not proved until 1896).
But what made him choose to devote himself to mathematics was his finding in 1795 of the criterion for a regular n-gon to be constructible using only compass and straightedge.
Then for his doctoral thesis (1799) he gave the first ever proof of the Fundamental Theorem of Algebra (Section 4.8).
In 1801 he became famous in the eyes of the general public by locating the exact position of the newly discovered (and subsequently lost!) planet Ceres, this from very little information and where the best astronomers had failed.
In 1807 he was appointed professor of astronomy and director of the Göttingen Observatory, a post he held until his death.
1801 also saw the publication of his classic book Disquisitiones Arithmeticae.
As one might expect, Gauss didn't collaborate much with others.
One exception was a collaboration with Wilhelm Weber which produced in 1833 the first operating electric telegraph.
In his unpublished notes Gauss anticipated major work of several mathematicians.
In particular he considered non-Euclidean geometry before  Lobachevsky and Bolyai, quaternions before Hamilton, elliptic functions before Abel and Jacobi as well as much of Cauchy's complex variable theory.
His revelations of priority were painful and, in the case of Bolyai, nearly disastrous.
It is reported in [118]that Gauss's friends found him cold, uncommunicative, ambitious for security and fame, and very conservative.
It is said that he tried to dissuade his sons from studying mathematics so that the name of Gauss would remain synonymous with excellence.
There is a story that at the 50th anniversary celebrations of the award of Gauss' doctorate, Gauss was about to light his pipe with a page from the original Disquisitiones.
Dirichlet, appalled by this, grabbed the page from Gauss and treasured it for the rest of his life.
2.5 Some deeper number-theoretic results concerning congruences
In this section we present (essentially) two number-theoretic results of which much use is made in many areas of mathematics.
The second (2.5.4) may even surprise and delight you if you've not seen it before.
We use 2.5.4 later (see 3.8.1) and 2.5.3 to solve exercises 2 and 3 in such a way as to show the power generated by a mere change in notation from n\a — b to .
It was Gauss who first disclosed this power in his Disquisitiones Arithmeticae.
The notation  was suggested to him because of the similarity of the properties of the relations of congruence and equality.
The reader might care to recast all congruences in terms of divisibility and see if the results we obtain are then so obvious and their proofs so transparent.
In particular he may care to flex his muscles by proving, without the use of congruences, that (i)  (ii)  (iii) the equation  has no solutions in integers x, y, z.
We begin by stating explicitly the famous Little Theorem of Fermat announced (but not proved) in a letter dated 1640.
Euler proved it in 1736 by the method of exercise 1.4.17 and later generalised it.
Apparently Leibniz was also in possession of a proof around 1683.
Theorem 1.5.1 (Fermat's Little Theorem) Let p be a (positive) prime and n a positive integer.
Then ; in Gauss' notation .
 Yet another proof accredited to Euler is found in exercise 9.
In 1770 the English mathematician Edward Waring published his Meditationes Algebraicae.
In it he announced, without proof, several new  results.
One, which asserts that to each s ε Z+ there corresponds k(s) ε Z+ such that each positive integer is expressible as a sum of at most k(s) positive integral sth powers, was first proved by Hilbert (in 1909!).
There is a nice article on this in the American Mathematical Monthly, Vol. 78, 1971, pp. 10–36.
We mentioned in Section 0.4 that Lagrange proved k(2) = 4 in 1770.
In 1771 he proved another of Waring's assertions suggested to Waring by one of his students, John (later, Sir John) Wilson.
Theorem 2.5.4.
(Wilson's Theorem) Let p be a positive prime.
Then .
In other words (p — 1)! 
To make the proof easier to follow we first prove it in the case p = 11.
In Z11 we have  and, of course,  Hence  It follows that 10!  (mod 11) as required.
The method of proof in general should now be clear.
Proof of 2.5.4 The cases p = 2, 3 are easily dealt with directly.
So we may assume  We consider the p — 3 elements of the set  Zp and we prove (i) if a ε S then there exists b ε S such that  and (ii) if ab — ac — 1 then b = c.
That is, the element of b of (i) is unique.
We may then deduce that the elements of S resolve themselves into  pairs, whose product in each case is 1.
And since  in Zp we shall be finished.
So let us prove (i) and (ii).
(i) Given a ε S we know from 2.4.6(a) that there exists b ε Zp such that ab = 1.
Clearly  and  [why not?]and so b ε S. Further .
For suppose it were.
We should then have aa — 1, that is  or, again .
Thus p\a + 1 or p\a — 1 in Z. Recalling that  we see that the alleged divisions are impossible.
This contradiction ensures .
 
2.6 Functions
We have so far met several examples of what the reader would probably regard as a "function" , but we have as yet given no definition, formal or otherwise, of the concept.
The reader may feel he can get along quite nicely without any definition!
If so, we should first remind him that we shall be interested here in functions other than those between two sets of numbers.
Further, he might find it interesting (and  salutary ) to read (see, for example,[71],[115]) how such great mathematicians as Euler, d'Alembert and Daniel Bernoulli came, around 1750, to arguing about their respective solutions to the "vibrating string" problem essentially because their ideas as to what constituted a function did not coincide.
In 1755 Euler wrote, "If some quantities depend upon others in such a way as to undergo change when the latter re changed then the former are called functions of the latter." 
For Euler the quantities were numbers: today we have need to discuss functions between more general sets.
The study of functions was originally an offshoot of the study of properties of curves, geometrically defined, so it is interesting to see how a modern definition of function, expressed in terms of the set concept, is equivalent to what you would, in the case of a real valued function of one real variable, naturally think of as the graph of the function.
One definition of function often found in present-day texts is: "A function is a rule which associates with each element of some set A a single element of a second set B.
You might care to discuss with your friends:(i) Is the word "rule" any more self-explanatory than the word "function" ?
(ii) Can different rules lead to the same function?
We thus eschew the word "rule" and opt for a more precise definition of the function concept.
Definition 2.6.1 A function (also called map, mapping, or transformation) from a non-empty set A to a non-empty set B is a non-empty subset f of A x B such that for each a ε A there exists exactly one b ε B for which (a, b)) ε f.
Remarks (i) The words map, mapping and transformation reflect the geometric origin of the function concept.
(ii) The words map and mapping seem to have gained greater favour in algebra than the word function; from Chapter 3 on we shall almost invariably use one of these two words.
Examples 2.6.2 
Several technicalities arising from 2.6.1 will be needed throughout the text.
We do not try to motivate them but simply gather them together for ease of reference.
Notations 2.6.2 (i) If  is a function from A to B we emphasise this by writing  (ii) Given (a, b) ε f the uniquely determined b is most frequently denoted by either f(a) or af.
Notes 2.6.4
(i) Which of the notations introduced in 2.6.3(ii) is used is merely a matter of personal difference.
On the whole, algebraists seem to have a slight preference for the latter whilst analysts tend to prefer the former, perhaps because of tradition.
The latter has an advantage when discussing compositions of functions (see 2.6.7) but as both are in common use we shall encourage the reader to use both by using both ourselves!
We invite the reader, in his  verbal communications, to refer to the symbols f(a) and af as the value of f at a or, simply f of a and the image of a under f respectively.
(ii) It then becomes rather palatable to describe f as the function given by  or again as the function given by af = b, for all a ε A.
(iii) 2.6.3(ii) emphasises the difference between the symbol f denoting the function and the symbol f(x) denoting the value of the function f at the element x of A.
Definitions 2.6.5 Let f:  be given.
A is called the domain of f.
If  we write Sf for the set{ sf: s ε S}and call this set the image of S under f.
The subset Af of B is often called the range of f.
(Note that  is permitted.)
If Af = B we frequently say that f is onto B. If we don't know (or if we know but don't particularly care) that f is onto B, we often just say that f maps A into B. (Thus "into" is just another word for "to" .
Please note that "into" does not mean "not onto" .)
The map f:  is one to one (briefly 1-1) iff from  it follows that .
That is, f is 1-1 iff distinct elements of A map to distinct elements of B. In the contrary case where there exist  such that  and yet  we sometimes say f is many-one.
Two functions f:  and g:  are equal iff the sets f and g of ordered pairs coincide.
That is, f = g iff A = C and af = ag for each a ε A (= C).
Note f = g is possible even if .
Let f:  and let S be any non-empty subset of A. The subset  is the subset of f and is a function from S to B. (Informally fs is just f except that the action of f on elements of A lying outside S is ignored.)
It is called the restriction of f to S and is commonly denoted by f\S.
We shall, incorrectly, write fs as f.
No confusion will result.
If  is such that af = bo for all elements a in A we say that f is a constant function.
If  and if f:  is such that af = a for all a in A then f is called the inclusion function of A in B. If f:  is such that af = a for all a ε A then f is called the identity function on A.
Again let f .
If  we write  for the set{ s: sf ε T}.
[Remember this definition by "equating" sf ε T with  in your mind.}
 is called the inverse image of T under f.
In case f is 1-1 and onto B then, for each  comprises a singleton{ a}, say, where a is the unique element of A such that af = b.
The subset{ (af, a): a ε A}of B x A is a 1-1 function from B onto A called the inverse of f.
We denote it (there is only one such inverse; see exercise 12) by .
If f:  is 1-1 and onto B, f is sometimes called a 1-1 correspondence.
A 1-1 correspondence f:  is called a permutation on A.
Note 2.6.6 The terms surjection and injection are sometimes used to describe maps which are respectively onto and 1-1.
If f:  is both 1-1 and onto, f is then called a bijection.
All examples relating to 2.6.5 are relegated to the exercises.
Now suppose f:  and g:  are functions.
We define a new function from A to C which we denote by  to indicate its dependence on f and g by: for all a ε A,  the image of af under g.
This may be represented pictorially by Fig. 2.1.
We have Definition 2.6.7 The function  is called the composite of f and g.
The operation o combining f and g in this way is called composition.
Example 2.6.8 Let  and  be the functions given by  whilst  Thus .
It follows that  since, in particular .
Note 2.6.9 We now see one advantage of placing the function symbol to the right of the element it operates on, namely that the combined effect of f followed by g is naturally denoted by .
Putting the function symbol on the left leads to writing g (f (a)) as  so that  denotes, once again, the action of f followed by g.
This "backwards" notation seems somewhat unnatural when dealing with permutations (see 5.3.6).
 
2.7 Binary operations
At last we arrive at Definition 2.7.1 Let A be a non-empty set.
A mapping  is called a binary operation on A. If  and if  we say that S is closed under (or, with respect to) ρ.
Examples 2.7.2 (i) Addition, multiplication and subtraction on each of Z, Q, R, C, Z[x], Q[x], Zn, etc. are binary operations.
Division is not a binary operation on any of these sets.
[Why not?]
Regarding Z+, Q+, R+, as subsets of C, each is closed under the addition and multiplication on C but none is closed under subtraction.
Notation 2.7.3 (i) According to 2.7.1 and 2.6.3(ii) we should find ourselves writing (3,5) + = 8 instead of the more usual 3 + 5 = 8.
Since, due to familiarity, we are more at home with the second notation than the first we shall adopt it for all binary operations.
That is, given ρ as in 2.7.1 we shall, from now on , write aρb rather than (a, b) ρ.
(ii) If, as in 1.7.1 S is closed under ρ it follows that ρ\S x S is a binary operation on S. For psychological reasons and to stop pedantry getting out of hand, we replace ρ\S x S by the technically incorrect but intuitively more  palatable symbol ρ and speak accordingly of ρ as being a binary operation on S. Thus for s and t in S we write sρt whether s and t are regarded as being in S or in A.
Examples 2.7.2. 
The following chapters of this book deal with a variety of sets upon which are defined either one or two binary operations.
In the most important cases these binary operations will satisfy several of the analogues of the axioms postulated for Z in Section 1.2.
To ease the reader into the following chapters but, more importantly, to establish 2.7.6 and 2.7.7 we introduce, for general binary operations, Definition 2.7.4 Let ρ be a binary operation on a set A. If for all a, b ε A aρb = bρa we say that ρ is commutative.
If for all a, b, c ε A we have (aρb) ρc = aρ (bρc) we say that ρ is associative.
If there exists in A an element e, say, such that aρe — eρa = a for all a ε A we say e is an identity (or neutral) element for ρ.
Examples 2.7.5 In examples 2.7.2 parts (ii) to (iv), ρ is commutative in parts (iv) and (vi), is associative in (iv),(v) and (vi) and possesses identity elements in (iv),(v) and (vi).
For example in (vi)  (Thus is 7 an identity element for ρ.)
Much more important is the verification that o (in 1.7.2(v)) is associative.
Theorem 2.7.6 Composition of functions, when defined, is associative.
Proof 
In a like manner  definition (see 2.6.5) of equality of functions.
That o of 2.7.2(v) is associative follows on setting X = Y = Z = T.
Next a remark about associative binary operations.
These comments have already been made in exercise 1.2.1 in connection with the binary operation + on Z.
Let ρ be a binary operation on a set A. For each ordered triple  of elements A we can evaluate their product in that order either as  or .
If ρ is associative then, by definition, these two evaluations are equal and we can unambiguously denote the result by .
In the case of four elements there are five ways of evaluating their product, all of which yield the same result (copy the argument of exercise 1.2.).
We now prove generally Theorem 2.7.7 (The Generalised Associative Law) If ρ is an associative binary operation on the set A then every way of evaluating the product of the ordered n-tuple of elements  of A taken in that order leads to the same result.
Proof We proceed by induction on n starting with the trivial case n = 3.
Now suppose the desired result proved for all n such that  and let  be elements of A. The final step in any evaluation of a k-fold product is the evaluation of a product of the form  for some i such that  (see exercise 5).
Because of the induction hypothesis, the products inside the brackets are unambiguous.
Thus to prove the theorem we only need to show that for each i, j such that  we have 
If i = j there is no problem.
If  and .
Now x, y, z are unambiguous by induction hypothesis.
Thus 2.7.8 is valid provided that .
But this follows from the associativity of ρ, and the proof is complete.
Thus we have shown that for each m the product of n elements  can be defined unambiguously and can be denoted by  without brackets.
Finally we establish the so-called laws of exponents for positive integral powers.
Writing  instead of  etc. 2.7.8. leads to Theorem 2.7.9 Let ρ be an associative binary operation on a set A and let a ε A. Setting  to be the (unambiguously defined) n-fold product  we have, for all r, s ε Z+,  and 
Notes 2.7.10
(i) Of course, when A = Z and ρ is multiplication, the above merely repeats results we've known and used for a long time.
The same is true when ρ is taken as addition except that, of course ,  is then written na.
(ii) 2.7.9 will in general fail if the hypothesis of associativity is withheld.
An example, already known to you from examples 2.7.2 is given in the following exercises.
