

Criteria for science in the courts
The US Supreme Court may produce new criteria for the admissibility of scientific evidence in the courts on the basis of a suit now being heard.
THE question of what constitutes valid scientific data, suitable for admission as evidence in court, has plagued judges for decades.
Generally unschooled in the scientific method, judges have the legal duty of deciding what may or may not be presented to a jury.
For years, US judges have relied on the standard, dating back to 1923 and too often honoured in the breach, defining admissible evidence as that which derives from methods of inquiry that are ‘generally accepted’ by the scientific community.
Frequently, courts have interpreted that to mean ‘published in the peer-reviewed literature’.
A more lenient standard, set out on legislation in 1975, permits judges (at their discretion) to admit as evidence almost any opinion from an ‘expert witness’, defined as someone who is qualified ‘by knowledge, skill, experience, training or education’ to speak to a given subject.
Each standard is in some way deficient.
First, it is (or should be) well known that the peer review system is not infallible and, further, that the best journals openly acknowledge that editorial judgement on the importance of a paper and its estimated interest to readers play an important role in deciding which papers to publish and which to reject.
And even this journal has rejected papers that subsequently proved to be of exceptional significance.
Thus to bar from the courts data that have not appeared in a peer reviewed journal could be foolhardy.
But it is also well known that the so-called expert witness in court may be a hired gun, willing to testify to anything for a fee, or a crackpot whose unsupportable ideas are masked by an advanced degree — often from a respectable university.
The issue of standards of evidence arises now because of a case just argued before the US Supreme Court over whether data do or do not support the allegation that a drug called Bendectin, once widely prescribed to prevent morning sickness in pregnant women, causes limb deformities in newborn babies.
The manufacturer and the defendant in the case, Merrell Dow (now Marion Merrell Dow of Kansas City), has consistently won its case in some 200 lawsuits brought by parents who claim that Bendectin is a teratogen.
The company can cite more than 25 published epidemiological studies indicating no correlation between Bendectin (which was taken by more than 30 million women worldwide) and limb deformities.
(Nevertheless, because of the high cost of litigation, the company withdrew Bendectin from the market in 1983, leaving women to rely on old-fashioned remedies to prevent what is, in some instances, a serious complication of pregnancy.)
The plaintiffs in earlier cases and that now before the Supreme Court, known as Daubert v.
Merrell Down Pharmaceuticals , have relied largely on the testimony of expert witnesses, some of whom have reached conclusions by analogy rather than direct experiment.
Most of the time, the courts have ruled their testimony inadmissible.
The issue has been cast in scientific circles and the press as a clash between ‘good’ science and what is scornfully described as ‘junk’science because it fails to meet tests of scientific legitimacy.
For instance, much of the case against Bendectin in Daubert rests on testimony by a Berkeley-trained epidemiologist, now affiliated with the California sate health department, who claims that her ‘reanalysis’ of the published epidemiological data shows a one in 1,000 incidence of limb deformities caused by Bendectin.
She has not written up her data for publication.
What should the court do?
Reflecting a befuddlement judges often express when dealing with science (and revealing again that science is not yet part of the mainstream of education) one of the justices said: ‘There are Harvard law professors on both sides of this case; I had hoped you could get together and lead us out of the wilderness.’
But it is not really a wilderness, as many of the ‘friend of the court’ briefs filed by scientific bodies suggest.
One in particular(from the not-for-profit Carnegie Commission on Science, Technology and Government) offers a clear way out.
The commission urges the justices to adopt a new standard for evidence that would require judges not to resolve scientific controversy but only to ask three pertinent questions in weighing admissibility of evidence: is the claim testable?
Has it been tested?
And is the methodology sound?
Courts should not exclude evidence just because it is not accepted wisdom; nor should they allow plaintiffs to be held liable on the basis of mere hypothesis or speculation.
While it is true that the speculation is an essential part of science, and true that new ideas may have a hard time gaining acceptance, it does not follow that untested science belongs in court.
That would be bad public policy.
An influential fellow
The death two weeks ago of Lord Zuckerman will leave a sad gap in public life in Britain and elsewhere.
(as even his enemies called him), more formally , OM, was an iconoclast by  temperament, a Pooh Bah by habit and a mandarin by instinct.
Over nearly half a century, he was a powerful influence on several aspects of British public life, but mostly behind the scenes.
Only in the past two decades (he was 88 when he died last week) has he spoken out in public, and in the liberal persuasion, on matters such as disarmament in Britain's policy on nuclear weapons, matters that occupied a great deal of his professional life in government.
One of has last articles appeared earlier this year in Nature (361 , 392; 4 February 1993) but the British government has not yet deigned to answer the awkward questions he raised in it.
, Jewish and proud of it, and South African by origin, was an iconoclast in the limited sense that he saw it has his duty within the government and outside to tell people, officials and politicians when he considered that they were wrong.
(But in the 1930s, while a teacher of anatomy at the University of Birmingham and investigating hormonal influences in reproduction, he had already been reproved for obscenity by the Archbishop of Canterbury — the executive head of the Anglican Church — over his first book,The Social Life of Monkeys and Apes .)
His continued scepticism about  's various proposals for innovation in nuclear weaponry and strategy became a legend; was a friend he never made.
But  had a knack, a blend of humour and guile, that allowed him to tell his political masters they were mistaken without giving permanent offence.
It was in that spirit that he persuaded  in the early 1960s to take Pugwash seriously.
On another occasion, he dissuaded  from listening to a hare-brained proposal for a military take-over of the government.
was not an institutional iconoclast, but sometimes the opposite.
His appointment in 1955 as Secretary of the Zoological Society of London, for example, was marked by his affection for the zoo, but also by ructions as he fought the old guard for his sensible proposals for change.
(Recent events have sadly shown that they were not radical enough; 's regret a few weeks ago was that he was not still in charge.)
His capacity for doing several jobs, and simultaneously, seems to have been formed during this period; he was essentially full-time executive of the London Zoo and its appurtenances while moving from the Ministry of Defence to the Cabinet Office as the most senior and confidential adviser in sight, whence the reference to Pooh Bah.
was a mandarin (the British name for the head of a government department) in the antique Chinese sense; he conveyed the sense of having no power personally, but that there was a powerful emperor over his shoulder.
That, of course, is a recipe for survival, but there is no doubt that  enjoyed the influence this gave him, outside government as within.
He was a great manipulator.
Even so, it is a pity that he did not accept the challenge offered by , 1964's new prime Minister, of a post in the Foreign Office with responsibility for disarmament.
His most fitting memorial would be that his friends should press for an answer to the awkward questions on nuclear policy the British government has not yet answered.
Horse-racing hoax
The British, once a proud nation, have now lost even the trick of organizing horse races, preferring hoaxes instead.
ONCE upon a time, the British used to boast of the battles they had won — Agincourt, Trafalgar and Waterloo (with less justice) for example.
Then they boasted about technological innovations — steam engines, railway locomotives, steam turbines and so on.
Latterly, they have been reduced to boasting of the spectacles they organize — the Highland Games (in Scotland in the summer), the Oxbridge boat race on the Thames and the horse raced called the Grand National, which should have taken place last Saturday.
As is widely known, the race never happened.
Instead, the organizers staged a hoax on the 39 horses and their jockeys, together with their trainers, owners and other hangers-on, the few tens of thousands of people waiting in the rain at Aintree (near Liverpool), those who had wagered £75 million on the outcome of the race in Britain alone and the score of broadcasting organizations elsewhere that had brought the right to show the event on television.
It is possible that the organizers had misread the calendar, believing that Saturday was the first of April, not the third?
The centrepiece of the hoax was a mechanical device a traditional way of arranging when horses in a race should begin to run.
At most racetracks, horses are put in separate boxes which all open at the front when the race begins.
Not so at the Grand National, where the horses are assembled behind a fibre rope (call a tape) some 70 metres long, whose ends are hoisted when the race is signalled to begin.
The technology of the starting mechanism is deceptively simple.
Like any rope suspended from its ends, the Aintree tape must be disposed in catenary form.
To allow the horses and their appended jockeys through, all points of the tape must rise by about 1 metre in the time it takes a horse to travel about the same distance from rest, perhaps much less than 0.1 seconds.
Whether this will happen will be a function of the wave velocity along the tape, whose mass per unit length will have been increased and wave velocity reduced, by the drenching rain last Saturday.
On the first false start, the tape caught round some horses' necks.
On the second, some jockeys were similarly incommoded.
The traditional technology for recalling horses after a false start is equally sophisticated: the official at the starting line waves a red flag if a start is false, and then a second official 200 metres down the track waves another.
(Jockeys do not often look backwards at the beginning of a race.)
Those who believe that klaxon horns or traffic lights (as used elsewhere), would more securely given warnings overlook the long tradition of the British flagman, one of whom by law carried a red flag before the early railway locomotives and then the first motorcars.
Aintree says puckishly that this arrangement is reliable because it depends on people's judgement, but it is also a hoaxer's charter.
On Saturday, the device did not work the second time.
It remains to be told who played hoax, and why.
No other explanation is charitable.
AIDS administrators plan crisis meeting over future of AZT
London & Washington .
Senior research administrators from both sides of the Atlantic plan to meet before the end of the month to discuss the implications to Anglo-French trial results which last week cast doubt on the usefulness of the drug AZT in treating HIV.
According to the preliminary results published in The Lancet , the three-year trial, organized jointly by Britain's Medical Research Council and the French National AIDS Research Agency, has shown no evidence that treatment with AZT delays the onset of AIDS in patients known to be carrying the HIV virus.
The study was based on 1,749 patients.
Some of these were given AZT from the start; the others were given a placebo until they developed symptoms of either AIDS or AIDS-related complex (ARC), when they were then switched to AZT.
There was no significant difference in the rate at which AIDS progressed in the two groups; about 8 per cent in each group had died within the three years.
Scientists on both sides of the Atlantic have said that the so-called ‘Concorde’ trial confirms what many had felt for a long time, namely that AZT taken on its own is likely to be limited in delaying the onset of AIDS.
They hope that a combination of drugs — including AZT — will prove more effective, impelling mutations of the virus in inconsistent directions.
But differences of opinion have emerged on the regulatory significance of the Concorde trial.
Some European scientists have critcized the decision of the US Food and Drug Administration, under heavy pressure from AIDS activists, to endorse the widespread use of AZT in pre-symptomatic AIDS patients on the basis of early results from four US studies showing the drug's apparent effectiveness.
The European critics feel that the results of the latest trials justify their initial scepticism about some of the early claims made as a result of the US studies.
, professor of immunology at St Bartholomew's Hospital Medical College in London, acknowledges that the US studies showed a ‘possible benefit’ to those infected with HIV.
But he argues that the benefits were ‘too small and too short-term for us to know what they meant in the long-term’.
The results of the Concorde trial came as ‘a considerable shock’, he says, particularly to those who had been expecting some positive outcome, however small.
In the United States, senior health administrators stressed that their advice to doctors and researchers to continue working with AZT in the immediate wake of the Concorde results did not imply any criticism of the european study.
‘I have every reason to believe that this is a first rate study’, says , AIDS director of the National Institute for Allergy and Infectious Diseases (NIAID).
‘the results need not be inconsistent with other trials.
In the end we all agree that AZT is an effective drug, but only for a short period.’
, who called the upcoming meeting with the Concorde group as soon as the results were published, reacted angrily to UK press criticism that previous US AZT trials had been rushed, pointing out that the trials were curtailed only because regulators judged, once AZT appeared to be inhibiting AIDS, that their continuation was unfair to patients on the placebo side of the trial.
Wellcome Foundation, which currently sells more than £200 million ($300 million) of AZT a year, dismissed the use of AZT on its own as ‘old science’, but also claimed that the Concorde conclusions were not as black-and-white as they appeared.
But  of the Treatment Action Group, an activist organization in New York, said that large numbers of patients newly identified as HIV carriers were still being prescribed AZT on its own.
One matter to be discussed when the research administrators meet will be the Concorde trial's demonstration that the level of CD4 white blood cells rose significantly in patients who had been receiving AZT, even though there was no subsequent clinical benefit.
US regulatory officials have been using the level of CD4 (T-helper) cells as a surrogate measure of the progress (and alleviation) of AIDS symptoms.
However, there are many in Britain who feel that the US approach, which has relied heavily on the CD4 measure in licensing other potential anti-AIDS drugs, is wrong, and that the decision to put a patient on AZT should be more broadly based, using a number of factors — including the best current estimate of likely long-term risks and benefits — rather than a simple formula.
and 
Rules eased for US field tests
San Francisco .
The US Department of Agriculture (USDA), ending years of debate, has relaxed regulatory supervision of field tests of six important genetically engineered crops.
A new rule published on 31 March in the Federal Register exempts the crops from the long process required for a permit and allows biotechnology companies to carry out experiments with corn, cotton, tomatoes, soybeans, tobacco and potatoes after notifying USDA in a letter and waiting 30 days.
The new rule would have allowed 85 per cent of the 365 permits issued for field tests and 724 sites to have gone forward under the simpler notification.
Biotechnology executives see the plan as a substantial improvement on the present system, in which the government analyses every proposed planting in a process that can take four months.
The new rule is also a clear victory for the environmentalists against a proposal by the Bush administration that would have eliminated any waiting period.
, chief of the agriculture department's biotechnology section, points out that the rule is inconsistent with  's intent to balance economic stimulation with environmental safety.
The preamble to the rule advises the industry to expect an additional review of genetically engineered pesticides and the commercialization and marketing of foods.
The Food and Drug Administration has also hinted that it may revise a policy that does not at present require special safety testing or labelling for most biotechnology foods.
The new field-testing rule requires regulators to tell researchers within a month whether they can proceed.
Plants used to produce drugs are not eligible for the one-month notification.
The new rule also allows the federal government to consult state officials on any regional scientific concerns.
A company my request a one-time exemption for a crop not on the list or it may ask the government to add its crop to a list of those permanently exempted form the permitting requirement.
The rule abandons an earlier plan that would have allowed safety boards or state officials to help researchers make such decisions on their own.
Medley said that his agency received 84 comments from universities, states, public-interest groups and others on the proposed rule.
The Industrial Biotechnology Association says that the 30-day plan reduces the chances that an increasing number of applications will clog the system.
At the same time, it said, the new rule may help to build public trust by retaining clear federal supervision and by providing enough time for local officials and public interest groups to react to any proposed field test.
British aerospace industry wants more government support
London .
Britain's aerospace manufacturers, hoping to persuade government officials of the need to support key technologies to counter growing US competition, have asked the Department of Trade and Industry (DTI) to back a national research programme in advanced civil aeronautics research.
In particular, they want the DTI to increase its support for such research by a factor of four, from £26 million (US$; 38 million) to £100 million a year.
The proposal, drawn up by an advisory group of industrialists and academics known as the DTI's aviation committee, was originally put to the department last November, but has only just been made public.
It argues that a ‘national strategic technology acquisition plan’ is needed to provide a boost to the industry's own research efforts comparable to what US companies receive from the National Aeronautics and Space Administration (NASA), which spends about $900 million a year in the field.
Britain has historically provided substantial direct aid for civil aviation research, most recently for the development of Airbus, but that support has ended as the programme approaches commercial viability.
‘We want a plan formulated jointly by government, industry, research establishments and the academic community, which will prevent us all going off in different directions’, says , professor of aeronautics at the Cranfield Institute of Technology and chairman of the committee.
According to & name , there are many areas of aviation technology, ranging from aero-engines to advanced avionic systems, in which Britain is a world leader.
‘If we are to maintain this type of lead, we need realistic support similar to our competitors’, he says.
So far, the DTI has shown little enthusiasm for the idea, and the budget for civilian aerospace research is scheduled to decline next year by 15 per cent, to £22 million.
Aerospace officials, however, are hoping to change the government's mind by pointing to the new activist policies of  and by citing evidence that the US government is spending considerably more than the three per cent of turnover accepted as a limit under a trade agreement signed with the European Commission last July.
Last year's expenditures by NASA of $902 million barely exceeds three per cent of the US civil aviation industry's sales last year of $29; million.
But  has proposed that NASA spend an additional $550 million over the next four years on civil aviation research, raising the government's level of support beyond the limit if, as seems likely, sales remain stagnant.
At present, the aeronautics and pharmaceutical industries make the largest positive contributions to Britain's balance of payments, capturing almost 12 per cent of the world's market for its products.
But the committee warns that the industry's long-term future is threatened by a squeeze in defence spending and reduced civilian orders, and that its survival depends upon increased spending on long-term research.
Additional government spending is particularly important in such areas as advanced wing design, air traffic management systems and low-cost manufacturing.
Aerospace officials say that support is growing for a national strategic plan in which increased government spending would be matched by the industry's own resources.
‘Our message to the Treasury is that they have to look at public spending on research and development in aeronautics as an investment, not just an expenditure’, says an official of the Society of British Aerospace Companies.
‘Our members feel that failure to sustain adequate R&D investment to counter international competition could mean an inevitable erosion of the UK's capability — and once lost, the industry's leading position could not be regained.’
ESA is wary about impact of redesigned US station
Munich .
The European Space Agency (ESA), whose pressurized space module Columbus is an ECU2.4-billion (US$; 3.1 billion) part of the $30-billion US space station Freedom, is considering other alliances in the wake of yet another redesign of the US station and amid fears that the US Congress may reject even a smaller, cheaper version.
ESA's contribution to the US station has been withheld until concrete proposals emerge from the redesign ordered last month by US president , who instructed the National Aeronautics and Space Administration (NASA) to scale down the station and cut its operational costs in half (see Nature 361 , 195; 1993).
The fortunes of manned space travel have had a rough ride in the past few years, with the United States finally joining Europe in questioning the costs of such programmes in times of recession.
ESA abandoned its own plans for independent manned missions last year, after much internal strife, but its commitment to Columbus was reaffirmed last November despite a funding shortfall of 10 per cent.
ESA has already spent millions of dollars on Columbus, and it now seems likely that the redesign of Freedom will require a corresponding change in its space laboratory.
But at least ESA will now have a voice.
Europe and Japan were dismayed and angered last month by comments from the US government that ignored their significant roles in the programme (Japan is contributing a similar module).
But since then NASA has promised to protect their interests (around 20 per cent of the programme's total cost), and both Europe and Japan are now represented in the discussions.
The deadline is very short.
wants to see plans for the modified programme in early June, and a first draft is expected by mid-May.
An outside advisory group, led by Charles Vest, president of the Massachusetts Institute of Technology, will provide a second layer of review.
In the meantime, ESA and its member countries are considering back-up plans for Columbus should the United States abandon Freedom completely.
ESA acknowledges its disappointment that the programme is being redesigned ‘at a point when we were almost ready to cut iron with our contribution’, according to a spokesman.
‘Redesigning our part will cost us more money.’
But Germany, which led the campaign to reduce spending on space to help pay for the soaring cost of reunification, sees the situation as an opportunity of reduce the cost of Columbus as well.
Germany's new minister for science and technology,, thinks that another way to save money is to merge Russia's MIR 2 programme with NASA's Freedom.
But experts say such a proposal would not be technically feasible.
It might, however, be possible to combine the Japanese and European modules.
But such an alliance is unlikely given each country's desire to retain control over its own module.
Despite the behind-the-scenes politics, the most likely outcome of a redesigned Freedom is a slowing of ESA's programme to spread the costs over a longer period.
European labs begin parallel processing computer project
Munich .
The European particle physics laboratory CERN and the European centre for mathematics in Toulouse, France, which analyses  meteorlogical data, are leading a three-year project in massively parallel processing as part of an ESPRIT programme to promote information technology in Europe.
Funds of ECU15 million (US$; 21.8 million) will be used to provide hardware and develop software for a technology that competes with standard vector computers.
Each laboratory, which is obliged to match the grant from the European Commission (EC) with money from its own budget, will be installing a scalable CS-2 parallel computing machine built by the British company Meiko.
The computers will be used to develop software to analyse vast amounts of particle physics and meteorological data, which are currently analysed by Cray vector supercomputers.
Parallel processing is capable of higher overall performance compared to vector computers, particularly important for specialist scientific, high-input high-output data analysis such as that generated at CERN.
‘We are not a typical vector user’, says , project manager at CERN, ‘because our work is as much computer intensive as data intensive’.
Massively parallel computers have the added advantage, for all users, of being much cheaper to build and to run than vectors.
The EC hopes that the project, which began last month, will increase user confidence in the potential of the European massively parallel computers and will create a report on high-performance computing in Europe, compiled by CERN director  for the EC, concludes that Europe can compete effectively with US companies, traditionally the leaders in the supercomputing field, provided that the support is given to overcome the perennial problem of turning basic research results into competitive products.
The EC hopes to build upon Europe's acknowledged strength in software development.
Japan, although further behind in the race, is forming collaborations to close the gap.
Last month NEC signed an agreement with the Swiss scientific computing centre (CSCS) to develop applications for massively parallel computers in a new, NEC-funded high performance computing software development centre on the CSCS site.
Meiko is part of a three-way consortium of suppliers that will benefit from the EC project; its partners are the British company that Parsys and the French company Telmat.
New, greener ethos bolsters bill to protect California desert
Washington .
A 17-year effort to protect some seven million acres of federally owned desert in California seems to be on the brink of success, the result of a new president and changing attitudes in Congress.
A campaign by environmental groups to shield the fragile Mojave and Sonoran deserts from the impact of mining, ranching, military and off-road use has long been opposed by the Reagan and Bush administrations and by the lobbying interests of the people who work and play in the desert.
But Interior Secretary  has pledged that preserving California's desert will be the first land conservation goal of the  administration.
's announcement came a month after California's two newly elected Democratic senators introduced a bill to preserve one-third of the federally owned desert land in their state, a measure that is expected to become law by this summer.
The Mojave and Sonoran deserts cover 40,000 square miles of remarkably diverse terrain.
Sand dunes, dry lakes, extinct volcanoes and 90 mountain ranges dot the landscape, and there are 760 animal and 1,300 plant species.
The California Desert Protection Act would create a 1.5 million-acre Mojave National Monument in the East Mojave, which houses the world's largest Joshua tree groves as well as the greatest population of the threatened desert tortoise.
It would transfer supervision of the areas from the Bureau of Land Management (BLM) to the National Parks Service and would bring to an end the existing cattle ranching, mining activity and off-road vehicle use.
The bill would also add 1.3 million acres to Death Valley and 200,000 acres to Joshua Tree national monuments, transforming both into more tightly controlled national parks.
‘We've all seen this initiative stymied far too long’, Senator  said about her bill, co-sponsored by Senator .
‘Now the important thing is to get the ball rolling and protect what's left of the desert.’
The federal government owns about half of the California's entire desert wilderness.
Of that land, about 15 per cent is used by the military as training grounds for flight, bombing and land warfare exercises.
On any given day sonic booms rattle the land near Edwards Air Force Base, while soldiers practising tank battles at Fort Irwin leave dust clouds visible more than a hundred miles away.
The most controversial aspect of the Mojave; some 80 per cent would be reserved as wilderness under the legislation.
The bill's effect on mining, the desert's biggest industry, would be even more dramatic.
The ending of mining rights would lead to a loss of some 20,000 jobs and $2 billion in revenue over the next seven years.
‘If concessions aren't made for those who live in the desert, the results would be economically disastrous’, says US Representative Jerry Lewis (Republican, California), who represents most of the rural Mojave.
Most of the federally owned land in the area is run by BLM, which has apportioned it for extensive commercial mining and ranching as well as for recreational use.
Each year, mining levels mountain peaks and kills off plant and animal species that have adapted to the habitat over thousands of years.
Equally threatening are the dozens of federally subsidized cattle ranches that have depleted underground water sources used by antelope and bighorn sheep.
The BLM has also set aside 1.5 million acres of formerly pristine wilderness for unlimited shooting ranges and for use by jeeps, dune buggies and dirt bikes.
More recently, the BLM has leased large tracts of wilderness to be used as garbage dumps for cities in the Los Angeles area.
And later this year, the agency plans to sell land near the Colorado River for the nation's largest unlined low-level radioactive waste dump.
The desert tortoise is one of many species in the protected area.
Cost overruns, management problems found in British Antarctic projects
London .
The British Antarctic Survey (BAS) received a public rebuke last week from the government's financial watchdog for its handling of several capital projects during the past few years.
The National Audit Office (NAO) complimented BAS on the three of four projects on the general quality of its research.
But it criticized some of the procedures used, including those for the construction of a new research and supply vessel, the RRS  , and for an airstrip and berthing jetty at its Rothera station.
The total cost has been several million pounds more than budgeted.
The budget of BAS, which is part of the Natural Environment Research Council, rose nearly fourfold, to £52.4 million (US$; 75 million) in 1990–91, in the wake of the 1982 Falklands War following a decision by the then Prime Minister Margaret Thatcher to raise Britain's profile in the region.
As a result, Antarctic science was one of the few research disciplines to receive a significant funding boost in the 1980s.
A large proportion of the new money has been spent to improve access for scientists.
In addition to building the RRS  and the Rothera airstrip, BAS replaced its research station Halley, located on a fast moving ice-shelf, and bought a De Vailland Dash 7 aircraft, which was converted for Antarctic operation.
There were cost overruns on each project and also, in the last two cases, delays in completion.
Design specifications for the research ship, for example, were still being finalized even after the design contract had been signed.
Originally estimated to cost £30 million, the ship was budgeted at £40.8 million and wound up costing £42.7 million.
Similarly, inadequate information about the siting of the jetty at Rothera meant that it had to be redesigned and repositioned at a relatively late stage, at an additional cost of £2 million.
BAS spent £9.4 million, £3.4 million more than planned to rebuild Halley, and the project took two years longer than expected.
In each case, according to the audit office, BAS devoted insufficient resources to project management and failed to train and support the project managers adequately.
The project officer for the RRS  , for example, lacked knowledge of ship construction and experience with large projects.
BAS director  is withholding detailed comments on the criticisms until after parliament's public accounts committee has discussed the NAO's report next month.
But he believes that ‘the report clearly indicates that, as far as taxpayers are concerned — and given the difficulties of operating in one of the most hostile parts of the planet — they have had value for money’.
says that the problems with contract management were ‘second-order issues’ and that the cost overruns have had little overall impact on the scientific programmes.
However, he says that the points raised in the report have been ‘taken seriously’ by BAS, whose experience should help other groups faced with similar challenges.
Meanwhile, BASE is due to announce this week how it intends to spend an extra £4 million awarded by the Office of Science and Technology over the next three years.
A significant proportion of this sum will be used for clean-up operations required under the terms of the Antarctic Treaty signed in Madrid in 1991.
Part of the money will also be used to refurbish the Signy research station.
The money is considerably less than the £12 million that BAS is reported to have requested and has raised fears that some research projects may be terminated.
says that BAS intends to maintain its current level of scientific activity and that there are no plans for redundancies among BAS staff.
Ice was not the only obstacle to building the RRS .
Magnetic money
London .
The ever decreasing value of coinage is the common currency of taxi driver and bar talk, but anew cut-price penny with surprising magnetic properties is the sole interest of an international Magic Penny Society, founded by , professor of Biochemistry at Brunel University.
Last September the Royal Mint stopped making penny and two-penny coins from bronze and started instead to electroplate steel disks with copper.
first knew of this when one of the coins stuck to his magnetic key-retainer — the steel core is a soft magnet that becomes magnetized in an applied field.
Fields minimize their energy by funnelling through the coins, which consequently stick together.
Initially, used the effect to entertain children at the Leukaemia unit in St Bartholomew's hospital, but when he found that scientists also were taking interest, he founded the Magic Penny Society, partly because he sees the new pennies as an excellent educational toy to excite children's interest in science.
Already featured at the British Science Museum, the Bristol Exploratory, in Singapore (where one onlooker built the penny pagoda, above), and even with one of Faraday's original electromagnets at the Royal Institution, the clever currency will next appear at the Edinburgh Science Festival next week.
Carnegie completes review of science — government ties
Washington .
Participants in a five-year effort by the Carnegie Corporation to bring science and government closer together believe that they have improved that relationship somewhat and hold out the promise of even greater success.
But critics say that the project failed to recognize the diversity of the US scientific community and that it is very difficult for any non-governmental group to bring about significant and lasting change.
Last week, the Carnegie Commission on Science, Technology and Government issued a summary report on the results of a $10.7-million project begun in 1988.
The commission drew on the knowledge of some 160 senior figures and science and public policy, including former US presidents  and , and distributed 208,000 copies of 19 separate reports.
, co-chair of the commission, says that the group's work will attract ‘more recognition in the twenty-first century’ than it does today.
Even so, he says, the commission can claim credit for several short-term victories, including a larger role for the president's science adviser and improved liaison between some government departments and related federal agencies.
Critics say, however, that the commission failed to obtain the views of those outside the academic and political establishment and that any progress has come about independently of its work.
, whose science policy roots go back to the presidency of , retorts: ‘Paternity is always to prove’.
The commission's first report,Science, Technology and the President , was issued shortly before the election of President .
It called for the appointment of a science adviser at cabinet level and early enough in the new president's term to help him fill other scientific posts, an expanded Office of Science and Technology Policy (OSTP) and access to expert outside advice.
fulfilled all those requests except for promptness; that was met by  when he nominated  on Christmas Eve, a month before taking office himself.
There have been other successes, according to the commission's executive director,.
The group's proposal for a joint office between the National Science Foundation and Department of Education has been partially met by the recent signing of a formal ‘memorandum of understanding’ between the two.
Robinson also cited the work of the commission's judicial task force in recommending and helping to set up a scientific organization inside the Federal Judicial Center that will provide guidance for judges on the question of interpreting scientific evidence in court (see page 481).
& name , speaking last week at the unveiling of the commission's final summary report, said he was struck by how many of those involved are now — like himself — part of the new administration.
The commission's defence task force, which called for radical changes in military procurement practice, was led by & name , the new deputy secretary of defence.
‘The difficulties of integrating civil and military technology are even greater than envisaged’, said last week, ‘but we will persevere’.
Responding to criticism that the commission had said next to nothing about the role of university scientists,, president emeritus of the Rockefeller University in New York and co-chairman of the commission, said that any such comments would have been dismissed as special pleading.
But more telling criticisms may be the restricted nature and the lack of any formal mechanism to follow through on its reports.
‘They should get credit for the informal aspects of what they did, and their staff work was superb’, says , a senior associate with the congressional Office of Technology Assessment.
‘But the process was pretty closed.
With any organization that consists almost entirely of white males over 50, you are talking about one particular view of the world, and that worries me.’
Nearly half of the commission's 17 panels contained no women, for example, and fewer than half a dozen minorities participated in the exercise.
Commission officials say that they sought out the most talented people in each field, regardless of gender or race.
As for follow-through, & name will continue to run a small office at the corporation to monitor implementation of the commission's proposals over the next two years.
South Africa tightens spending for universities
Cape Town .
The South African budget for this year gives university researchers less than they need to keep pace with inflation and allocates most of the additional spending on science to applied work in government laboratories, dashing hopes of a significant increase in university funding.
Not only does the increase of 6 per cent in the science budget fall short of a general increase in spending of 9 per cent and an inflation rate of 10 per cent, but four-fifths of the additional $13 million will be allocated to the Council for Scientific and Industrial Research (CSIR), which carries out in-house research on applied science and technology.
The allocation runs counter to the recommendations of a report last year that asked for greater spending on universities (see Nature 356 , 9: 1992).
the impact of the report appears to have been confined to a reduction of 6 per cent, to $68 million, in the allocation to the Agricultural Research Council (ARC).
The two largest councils, the ARC and the CSIR, together get 61 per cent of the science budget.
, deputy vice-chancellor (research) at the University of Witwatersrand, says that the share of the budget going to universities ‘remains dangerously inadequate by international standards’.
,
a former director of the CSIR's Laboratory for Molecular and Cell Biology now at the University of Cape Town, interprets the allocation as an attempt by the government to rescue the CSIR in times of recession and a continuing decline in military contracts.
Since it was restructured five years ago (see Nature 332 , 197; 1988), the CSIR has tried to obtain a larger proportion of its income from both the private and public sectors for consulting.
In an article co-authored by , writes in the current issue of South African Journal of Science that ‘it would probably be true to say that science as a thriving, innovative, intellectual enterprise within the CSIR is dead’.
The reduction in the ARC's subsidy is a belated acknowledgement that agricultural research is overfunded (see Nature 362 , 386; 1993).
The allocations to the other councils are:
CSIR, $75 million (up 16 per cent);
Human Sciences Research council, $24 million (up 6 per cent);
Medical Research Council, $14; million (up to 7 per cent);
Council for Mineral Technology, $18 million (up 7 per cent);
Foundation for Research Development, $37 million (up 11 per cent).
Joshua Lederberg (left) and William Golden
US court gives original owners the right to deep-sea treasure
Washington .
High-technology adventurers may think twice before seeking sunken treasure after the US Supreme Court let stand a ruling that insurance companies have a claim on the booty.
The possibility of extended legal battles, salvagers say, may stifle the technological and scientific progress that accompanies the recovery of shipwrecks.
The supreme court declined to hear a case involving a ton of gold recovered so far from the SS Central America , resting in 8,000 feet of water off the coast of South Carolina after being sunk by a hurricane in 1857.
Their action let stand a decision by the Fourth Circuit Court of Appeals in Richmond, Virginia, that property lost at sea can be abandoned only through the failure of insurers to appear in court or through a written statement relinquishing title.
Several groups have tried to locate the wreck in the past 20 years since technology made possible the recovery of its contents.
They discussed purchasing the rights to the treasure with the insurance companies, but no contracts were signed.
The district court originally gave the gold to the successful salvage team, the Columbus-America Discovery Group of Columbus, Ohio, by applying a finders-keepers rule known as the law of finds.
It said that the Atlantic Mutual Insurance Company and eight other insurers had abandoned their stake by destroying or losing all records of paying the claim.
The appellate court instructed the district court to apply the law of salvage instead, determining the share of salvagers and insurers according to the cost of recovery and the care taken by the salvagers to preserve the historical and archaeological value of the wreck.
As an incentive to preserving wrecks, court awards to conscientious salvagers often approach the total value of the treasure, according to, an expert on marine policy for the Woods Hole (Massachusetts) Oceanographic Institute.
But the lawyer for Columbus-America,, says that the law of salvage undermines the financial incentives to develop the necessary technology for a high-risk venture that requires large amounts of capital.
Columbus-America investors spent more than $12 million, in large part to develop a sophisticated unmanned submersible, called Nemo, for a return that may be worth as much as $1 billion.
Columbus-America designed Nemo to remain at depths of 1.5 miles for several days.
In addition, the group developed robotic equipment, using three-dimensional video monitors, that is sensitive enough to pick up single gold coins and to lift heavy beams.
Biologists have studied the nutrient-rich environment created from the wood and iron in the shipwreck in an otherwise barren area by using hundreds of hours of tape from Nemo's cameras.
‘Having three-dimensional viewing has almost taken away my desire to go down there myself’, says , director of the group's science projects.
The new equipment has also enabled them to recover biological samples, including a crab holding eggs tucked under its tail and sponges that are being tested for medically active chemicals.
Gold coins and bars lie amid soft coral stalks covering timbers from the SS Central America.
Genome project ‘to be done by 1994’
Washington .
The human genome project, which aims to identify the make-up of all human genes, could be completed in a purpose-built US laboratory by the middle of next year, according to a leading participant in the work.
, the former NIH official who set up the $70 million Institute for Genomic Research in Maryland specifically to pursue the project, told a Nature Genetics conference in Washington last week that he hopes to have ‘a nearly complete description of the human genome, in the freezer and on computer, within twelve to eighteen months’.
, head of gene sequencing at the institute, said that the latest estimate compared with a three-to five-year estimate when work started last summer, thanks partly to more resources being devoted to the project and partly to better methods for finding new genes quickly and without redundant effort.
says that his facility, which employs some 50 robotic DNA sequencers, is finding between 500 and 1,000 new gene sequences a day, including partial sequences.
He estimates that the project is already 20–25 per cent complete, and that the total number of human genes may be around 75,000.
‘We'll know the number for sure in a few months as we get closer to completion’, he says.
The institute is responding to concern about the wider social and medical consequences of the project by establishing an ethics division, and by releasing groups of its gene sequences for general research use.
Catalonia plans for synchrotron
Barcelona .
The autonomous region of Catalonia has decided to build a synchrotron facility near Barcelona, following rejection last December by CERN, the international particle physics laboratory in Geneva, of a request by Spain for support of a facility to produce large numbers of tau and charm particles.
Pure and applied research in microelectronics, biochemistry and material sciences, areas of particular economic importance to Catalonia, will be conducted by a large permanent staff as well as by visiting scientists from the rest of Spain, Portugal and southern France.
The synchrotron should also offer an entry point into Europe for scientists from Latin America.
The Spanish government has promised to pay some of the Pts 11 billion (US$95 million) cost of building the Barcelona synchrotron, and the European Communities have been asked to provide half of the money.
But Catalonia says that it will bear the entire cost if necessary.
An international advisory group headed by Catalonian physicist , a director at the Max Planck Institute for Solid State Research in Stuttgart, Germany, is developing specifications.
The 2.5 GEV synchrotron, which will have a perimeter of 250 metres and 20 workstations, is to be built 20 km from Barcelona.
The site is near the Autonomous University of Barcelona and the Vallés Technological Park and close to two large centres for micro-electronics and material sciences.
Construction is expected to begin next year and the first experiments could be under way as early as 1998.
Greek university law defended
Sir — I wish to comment on the letter from  (Nature 360 , 704; 1992) on the new university law recently adopted by the Greek parliament.
The letter is one-sided and misleading.
Before the law was introduced in the Greek parliament, there was an extremely long procedure during which the minister of education visited each Greek campus and held lengthy discussions with the universities' senates on the basis of written replies to a questionnaire distributed by the ministry to each member of the university community in Greece.
It is therefore misleading to suggest that there has been ‘barely any debate’.
Either by accident or design, has also failed to mention several of the chief features of the new law.
Thus it includes provisions for allowing the emergence of new institutions, one of which is the new open university aimed at providing extramural education for particular groups in Greek society and the foundation of a centre for the Greek language to coordinate the maintenance and dissemination of Greek in the European environment.
Similarly the law has made possible regulations to encourage the mobility of Greek university students within Europe.
The arrangements for the accreditation of Greek universities is a new procedure aimed at rationalizing the distribution of funds according to objective criteria.
The new law also provides a simple and straightforward legal framework for the foundation of postgraduate courses, which have so far been weak and virtually  non-existent .
Universities can now apply for funds to found postgraduate departments as well as to create academic research institutes linking universities with the industrial and business world.
At the same time, the new law provides that the previously free distribution of textbooks to all students be confined to needy students only.
The funds set aside in this way will be used exclusively for departmental and central university libraries.
might have also mentioned that the number of scholarships and the funds available for them have been increased by 50 per cent and 500 per cent respectively.
is also concerned with the status of existing academic personnel.
But neither the old nor the new law provides tenure for the two lower academic grades.
may be right in saying that in Greece there is ‘very little employment in the private sector’, but he fails to mention that only under the new law people who fail to get tenure at universities are now eligible for employment in the public sector.
Moreover, it is not true that under the new law there is a freeze on promotions.
Rather, the new law requires that academic positions be filled by responses to open advertisements and that the disciplines concerned be determined by the universities and not by the ministry.
The ministry's power consists only of agreeing the number of places to be advertised, for which of course it has to pay the bill.
's hope that the universities would have been overtaken by protests and that Greek scholars would have boycotted the examinations is wishful thinking.
The universities are working normally under the new law.
's account of the ‘private universities’ is also inaccurate, as they are illegal under the Greek constitution.
His comments on the absence of the minister from European meetings leaves no doubt of his one-sided and political approach to the problems of higher education in Greece.
Copyright scam
Sir — I am concerned at the cavalier way in which the copyrights of scientific papers are handed over to commercial publishers.
Increasingly the status and pay of scientists are determined by what they publish and by ratings in the Science Citation Index .
Indeed they have to publish or perish (‘Sure he was a great teacher, but what did he publish?’)
But in which other profession are the products of one's labour given away for nothing to people who then sell them?
I have been making a collection of the agreements that scientific authors are required to sign to get their papers published.
I find some of them quite horrifying.
Not only copyright, but all rights, are often signed away.
Twice recently, papers submitted to a journal have later appeared as sections of a book.
The situation appears to be very different from that in the literary world, where, for example, copyrights of minor works by  are objects of immense value.
Copyright in chapters for a scientific textbook are often bought for sums that would hardly cover the cost of typing.
Dangerous conditions are sometimes attached.
One publisher requires that authors warrant that the article does not infringe any copyright, trademark or patent, that is not libellous and so on(matters that are beyond the competence of the author to judge) and the author is required to indemnify the publishers ‘against any costs expenses, or damages which [the publisher]may incur or for which [the publisher]may become liable as a result of any breach of these warranties’.
The publisher has formed a limited liability company to protect himself from such contingencies but nevertheless has tried to pass this liability to the private individual.
Whether an item is libellous or whatever depends on where it is circulated and this is under the control of the publisher and not of the author.
I think that authors requiring protection should assign copyrights to their institutions, or to the research council supporting the work, and inform the publisher that this body will give the publisher an exclusive licence to the material for perhaps two years.
I have tried this a couple of times and have got away with it.
Publishers have to make an exception for work produced by US government employees, where copyright is not transferred to the publishers, and this causes, I believe, no trouble at all.
With increasing commercialism, research councils and universities have a duty, I consider, to help to protect their authors in this matter.
Tobacco code
SIR — Because direct advertising of cigarettes is forbidden in Italy, it seems that other insidious strategies for cigarette promotion are being developed.
But to my knowledge, never in the past has a Nobel prizewinner in medicine signed a promotion for Philip Morris Companies Inc.
This year, however, new year greetings from Philip Morris, specifically addressed to the younger generation, occupied a whole page of the Italian newspaper La Republica on 31 December 1992, and included a message singed by the Nobel prizewinner  under the heading ‘Culture of the modern time’.
As a physician of Italian origin, I can only express a sense of surprise and shame which, I hope, is shared by the whole Italian biomedical community.
addr;
Opinion and fact and UK energy
SIR — and  have firmly held opinions on nuclear power (Nature 361 , 677–680; 1993), but they should not be construed as facts.
Opinion: ‘New nuclear stations were uneconomic when the discount rate was raised to 8% in 1988…
The breakthrough with nuclear capital costs has not been achieved…‘
Fact: Sizewell B is ahead of schedule, within the £2.03 billion budget which includes all first-of-a-kind costs, will be profitable and will contribute significantly to Nuclear Electric's future cash flow.
A twin pressurized water reactor (PWR) Sizewell C, replicating the design, has been costed in detail.
It could be built, operated for 40 years, fully decommissioned — and still produce at well under 3 pence per unit at an 8 per cent rate of return, competitive with new coal, new gas and French nuclear stations.
The design is exciting international interest; Westinghouse has asked Nuclear Electric to join it in a joint bid for Taiwan's new nuclear station on precisely this design.
Opinion: ‘…it will not be known until Sizewell B has been operating for some years whether the PWR will become the established nuclear option that has been sought for 30 years.’
Fact: With more than 150 operating worldwide over 30 years, the PWR already is the established international option.
The Sizewell B design is based on the Westinghouse reactors operating for the past eight years at Wolf Creek and Callaway in the United States.
Both stations routinely appear at the top of the performance league, with load factors well in excess of 80 per cent.
Sizewell B, with enhanced diversity in control and monitoring systems, will certainly produce at over 85 per cent load factor.
Opinion: ‘The total liability for nuclear decommissioning costs now stands at £18–20 billion…£5.4 billion for the Magnox stations and £4.2 billion for the AGRS…‘
Fact: The authors have chosen here to quote undiscounted and out-of-date figures although they make much play of discount rates elsewhere in their article and must be aware that the decommissioning costs concerned arise over a very long period of time stretching to the end of the twenty-first century and beyond.
As our evidence to the Select Committee showed, we have made full provision for these costs in our accounts.
Their present value is £2.5 billion using a very conservative discount rate of 2 per cent.
The Select Committee suggested that these costs might actually be too high and it is true that if our ‘Safestore’ strategy is adopted we will be able to reduce the liability by at least a further £500 million.
It is completely irrelevant to quote the decommissioning costs associated with Ministry of Defence activities in what purports to be an article about civil nuclear power.
Opinion: ‘Nuclear objectives proved inherently more difficult to achieve…‘
Fact: Yes — until 1989.
Since then, Nuclear Electric is achieving them.
In less than three years, output is up 25 per cent to record levels, productivity is up 50 per cent, unit costs are down by almost a third, and Sizewell B is ahead of schedule and within budget.
Opinion: ‘The levy…which finances more than half of nuclear generating costs.’
Fact: No it doesn't.
Nuclear Electric inherited liabilities from CEGB operations of £10.5 billion, without any of the provisions to recover them.
The levy, which reduces in real terms year on year, will raise a total of £9.1 billion.
However you do the sums, the levy is clearly there to pay for history; the Select Committee said it is ‘indefensible that so much of the burden of discharging the pre-April 1990 liabilities should be placed on electricity consumers within one eight-year period’.
But to suggest it's an inevitable part of our continuing generation costs is simply wrong.
It has nothing to do with the commercial future of nuclear power.
Still, there's one overriding opinion on which we can all agree.
As  and  put it: ‘Whatever decisions flow from the government's policy review, it is important to understand the background…
Throughout, there was no formal framework of energy policy.’
Let's get it right this time.
Patent problems
SIR — Your leading article (Nature 361 , 192; 1993) on the patenting of transgenic plants illustrates some of the complexities being faced by the European Patent Office from applications to patent transgenic agricultural plants and animals.
You highlight the problems likely to become apparent should animal breeds or plant varieties be patented; there would be established a double system of monopoly rights and subsequent confusion over ownership.
No doubt lawyers would be happy with this but conventional breeding programmes could well suffer.
Article 53 (b) of the European Patent Convention (EPC) currently excludes plant varieties from patentability.
The response of appellant patentee lawyers has been to claim exclusive rights to ‘plants’ which will, in fact, include all varieties of plants containing the gene in question.
Beside being a linguistic contortion to avoid the use of the phrase ‘plant variety’, this all-embracing course of action seems merely to exacerbate the problems.
Of equal significance to biologists is another part of Article 53 (b) which prohibits the patenting of ‘essentially biological processes’.
This was included, no doubt, because those who drafted the convention wished to avoid the patenting of living matter, although an exception was made for microbiolgoical processes.
Applicants for patents on transgenic organisms are now arguing that where the inserted genes come from microbiological sources, the organisms being patented have suddenly become microbiological.
Second, they argue that because human intervention is involved in the production of transgenic plants or animals, they have somehow ceased to be biological and thus patentable.
Such assertions run contrary to the organizational and integrative principles of biology.
It is crucial that more biologists now join this debate on the patenting of life forms to forestall the conveyance of their subject into a legalistic and linguistic Wonderland.
A patent is also meant to be granted in the public interest and the exclusion clause concerning public order and morality is to ensure that patents fulfil this obligation.
The granting of patents on plants rewards a technology that could lead to the genetic pollution of nature, an outcome that is clearly not in the public interest.
The wording of the EPC, however, is difficult to apply to morality issues surrounding genetically engineered organisms for exactly the same reasons that difficulties arise over ‘essentially biological processes’— the drafters of the convention did not envisage patenting of plants and animals.
The European Patent Office has got itself into a mess over these issues because it has proceeded to patent life forms without first examining properly whether the EPC, when interpreted as it was originally intended, actually allows the patents the patent office has granted.
It has allowed itself to be pushed into juggling words to fit industries' interests and has ignored the legitimate moral concerns of many.
What it should do is to stop issuing patents for plants and animals and await the outcome of proper international debate.
The shadow of genetic injustice
The effects of the inevitable discoveries emerging from the Human Genome project will be catastrophic for some.
Now is the time for preventative action to be taken.
Many knowledgeable people have a deep dislike of the Human Genome Project.
If you ask them why, some may say that the money would be better spent on different, smaller projects.
A few fear that the outcome could be harmful; they foresee a new underclass, unable to get jobs or insurance.
These pessimists of course overlook that this underclass already exists: 37 million people have no health insurance of any kind in the United States.
Could things become worse?
Proponents of the Human Genome Project imagine great progress in diagnosis of diseases and in their therapy.
Some predict that eventually everyone will carry all the sequences of his genes on a compact disk.
Others even claim that the present knowledge of man and his mind is nil and that real culture will arise only out of the knowledge of the sequence of human DNA.
This, I think is unlikely.
As pure scientists, these enthusiasts often do not imagine the consequences for society as a whole.
My own view is that the project will be truly enlightening about the molecular structure of the human body and brain, and that this knowledge alone is exciting enough to justify the programme.
Little is known in molecular biology and many unsuspected medical applications may emerge from this endeavour.
As a molecular geneticist, I have always defended the Human Genome Project as scientifically sound and important, and I continue to do so.
But I have also tried to spread the knowledge of the past criminal misuse of human genetics in Nazi Germany.
Until now I have abstained from commenting on the possible effects of the Human Genome Project on society in future.
But science after all consists of making and testing, predictions.
So I have decided to stick my neck out and to make some predictions for the next 30 years.
There is no doubt that the main medical progress will be in the diagnostics.
Therapeutics may lag behind, possibly for decades, so I will abstain from discussing these aspects here.
What will be the main diagnostic results of the Human Genome Project that are relevant to society?
I think there are two different areas: genetic ailments of the body and those of the mind.
There are many severe ailments of the body for which there are genetic predispositions.
Most of these conditions are extremely rare: only a very few are as frequent as one per cent.
A the moment it is in most cases just about possible to say that, according to a family history, a certain risk exists.
The Human Genome Project, however, will lead to the identification of DNA defects in the relevant genes.
Then, accurate predictions can be made from the blood (DNA): for example, one will be able to predict that a healthy child will die in his or her forties from Huntington's chorea or from familial Alzheimer's disease.
In Huntington's chorea, the time-point of the outbreak of the first symptoms will vary considerable.
In other diseases it may vary even more.
The predictions will thus not be accurate in the sense that the severeness of the symptoms can be predicted for a particular time, but they will be statistically accurate.
This concept is difficult to explain to patients and clients.
There are some similarities between this situation and that in the late nineteenth century, when infections were random and where no treatment was available.
Then, many had the bad luck to be infected whereas here we are talking about only a few people.
But the late nineteenth century was the time that general health insurance was introduced in countries like Germany.
Bismarck's reform in that country had the effect of increasing the industrial output of Germany.
Perhaps the time of molecular revelation of rare diseases would be the proper time to introduce general health insurance to the United States?
To continue my predictions for the future.
Imagine four potential cases of genetic disablement.
(1)
When a haemophiliac wants to become a butcher it makes sense to discourage him.
(2)
When a colour-blind man wants to become a truck driver, he fails at a colour test: his DNA does not need to be tested.
(3)
When a healthy young man asks for employment and his employer wants to know whether his genotype indicates that he may die in his thirties or forties from Huntingdon's disease, this is unfair.
(4)
When a healthy black person seeks employment and is tested for the sickle-cell anaemia gene, then is refused employment because there may be an extra risk under certain conditions, this is unfair.
Thus, in my view, practical ability tests are acceptable, but genetic tests for disabilities which may turn up later in life should not be made available to employers.
Moreover, DNA testing for employment purposes should not be performed if a large ethnic group would suffer.
But it goes without saying that a person presenting a potential employer or insurance company with unsolicited evidence that he does not carry the genes predisposing for various ailments will have a definite advantage.
One can imagine the development of protest movements among those who have decided not to reveal their genetic identity, and there will be a fundamentalists who will never have themselves tested because they themselves do not want to know.
All these groups will suffer grave social disadvantages.
One can also imagine clubs for the ‘healthy’ that will demand proof of ‘genetic fitness’.
The turning point may come when the US supreme court decides that genetic injustice is so immense that it is within the law to show an employer or the insurance falsified genetic identity.
(In Germany the supreme court has just decided that pregnant women may lie about the pregnancy to an employer.)
From then on, the interest of employers and insurance companies in the genetic identity of their employees or clients will wane.
I think all these developments will be painful but could come to a good end: the genetic bad luck of a very few may, eventually, be helped by many in that general health insurance and tough anti-discrimination laws could become reality.
It is, of course, also possible that the genetically handicapped will be simply too few ever to influence the majority to come to their help.
The situation will be different with the genetic ailments of the mind, which I shall now discuss.
Here, the percentage of the population involved is much larger and cannot be easily overlooked.
Any current textbook of human genetics will contain the assertion that the most differences of the human mind and soul (to use an old-fashioned word) are inherited.
But in almost all these cases the respective genes have not been located or isolated, and the DNA defects are thus totally unknown.
It is claimed that psychiatric diseases such as manic depression or schizophrenia, and also psychological disablements such as low intelligence, are somehow inherited.
But the truth is that the extent of family influence or other environmental factors is unclear.
Proof that these ailments are essentially genetically determined can come only through the knowledge of the DNA sequences of these genes and their variants.
Knowledge of the DNA sequences of a person will then allow one to make statistically accurate predictions about that person's intelligence and mental stability, and thus about their possible fate.
In earlier times, such predictions were the business of charlatans.
But belief increases the likelihood of a predicted outcome: placebos against psychic ailments work astonishingly well.
The scientific prediction of a person's limitations, and thus his possible fate, has a very dangerous component in that it may lead the individual to inaction and despair.
It may also lead the population at large to believe that, as there is no real chance, money should not be wasted to counteract genetic limitations.
It could be forgotten that these limitations are also set by environmental factors.
Many medical scientists would like the fame of having isolated one of these genes.
Some succumb to the pressure and claim they have obtained evidence where in fact there is little or none.
The ‘discovery’ of genes determining alcoholism, schizophrenia, manic depression and Alzheimer's disease have been peer-reviewed published in leading scientific journals.
They were hailed in the media as breakthroughs — and then they were shown to be wrong either by their own authors or by others.
These errors do not mean that the main qualities of the mind and the soul are not biologically inherited: they show simply that scientists are ordinary people.
One can predict safely that many more such non-discoveries will be made.
This will be confusing for scientists and painful for the particular patients involved.
But eventually the truth will emerge in areas where now we can only make guesses.
Finally, I would like to imagine what may happen if it does turn out that mental health can be shown to have a genetic basis.
Let us assume as an example that a gene is isolated which in several mutated forms predisposes for schizophrenia.
It will take by then a matter of weeks to determine the complete DNA sequence of such a gene.
Will the sequence reveal anything about schizophrenia?
For a molecular biologist, knowledge of a sequence means that the function of the protein specified by that sequence is known — for example the protein may form an ion channel or be an enzyme metabolizing a particular molecule.
Does this mean that we will understand schizophrenia if we know that it often occurs in people when a certain ion channel or a certain enzyme is damaged?
Many scientists or doctors would answer in the affirmative, but I would like to say, emphatically, that the answer is no.
Understanding a biochemical defect brings us no nearer to understanding the thoughts and actions of the schizophrenic.
Those opposed to my view would argue that knowledge of the gene will allow us to identify which lifestyles are dangerous for such a person.
But we do not need any knowledge of the gene or its product to do this.
Pharmacologists, on the other hand, will point out that they now can design rational drugs which may influence precisely the functioning of the relevant gene product.
But it is not true that if we know which drug to prescribe, we know all that is necessary to know about schizophrenia.
Although the molecular-genetic approach will certainly lead to a frenzy of new drugs on the market, in the end the suffering patients will be helped only partially.
It is so much easier to prescribe a pill than to change the social conditions that may be responsible for the severity of the symptoms.
I have little faith in the notion that treatment of mental diseases will truly benefit from knowledge of the culprit genes and gene products.
But I have no doubt that diagnosis will flourish.
Cheap tests will be developed which will allow everyone to be tested for the variants of genes determining psychiatric ailments or psychic qualities outside the doctor's office.
Those carrying the disabling genes of schizophrenia, manic depression and low intelligence may constitute ten more per cent of the US or European population.
Medical scientists will test anonymously all possible ethnic and social groups for these genes, and doubtless some ethnic differences will be found.
Suddenly, genetic racial injustice will be a fact.
The isolation of the first gene involved in determining ‘intelligence’(whatever that is) will be a turning point in human history.
Will governments endorse the view of the eugenicists of the 1930s that the carriers of such genes are ‘bad and inferior’ and that they and their followers are ‘good and superior’?
Or will they stress privacy and cleverly leave the necessary selection to market forces?
Or will they resort to legal measures to speed up the process of the ‘physical disappearance of the unwanted’?
And what will the geneticists themselves say?
Perhaps they will simply be relieved to find that their own mental genotypes are ‘healthy’.
But will some of them propose ways to eliminate the ‘bad’ genes from others?
Anticipating such conflicts, many may conclude that we do not need or want this genetic knowledge.
I disagree.
The knowledge will simply unveil reality, emphasizing the injustice of the world.
It is certainly not enough to face reality bluntly if the future develops as I describe it.
It is not enough simply that the right of privacy is acknowledged.
If those who have this right have no education, health insurance or jobs, the right is not enough.
Laws are necessary to protect the genetically disadvantaged.
Social justice has to recompense genetic  injustice .
The details of such legislation can be spelled out only when the genetic facts are known.
Deep changes in attitudes are also required.
All we can do now is to be prepared.
Progress may be painfully fast or slow, and will be full of contradictions.
To master it demands firm values.
At the extremes, people will have to choose between the values of the Nazis and those of Moses — that is, racism or an appreciation of equal human rights.
The choice for politicians of the world's governments will be between international fascism or, if science and justice are combined, a fundamentally improved social structure throughout the world.
I would like to conclude by citing a sentence form Science and the Future , published in 1924, written by the optimist .
‘I think that the tendency of applied science is to magnify injustices until they become too intolerable to be borne, and the average man whom all the prophets and poets could not move, turns at last and extinguishes the evil at its source’.
predictive powers — a DNA sequence of the HL-A gene cluster from chromosome 6, being analysed as part of the Human Genome Project.
The next step in AIDS treatment
The apparently well-founded discovery that AZT is not a useful drug for the treatment of people infected with HIV does not mean that it is useless in the treatment of those with AIDS.
The drug AZT, an inhibitor of DNA synthesis, has been for several years the only treatment for those with symptoms of overt AIDS.
Nobody has claimed that it can clear the infectious organism, HIV, from a person's system.
Rather, it has been used as a retardant of the progression of AIDS, largely on the basis that it seems to sustain the subpopulation of T cells that appears especially vulnerable to HIV in the overt stage of the disease.
Later, beginning in the United States, AZT was also used in people infected with HIV but not yet showing symptoms of overt disease, again on the basis that administration of the drug appeared to abate the decline of vulnerable T-cell counts.
But now a paper in The Lancet (341 , 889; 1993 appears to some to have thrown a spanner in the works: certainly it has caused a furore of misinterpretation by newspapers and others.
The new development is a preliminary account of an Anglo-French controlled trial of the use of AZT (proprietary name Zidovudine) in the treatment of symptomless people.
It is a substantial study, including more than 1,700 people infected with HIV and divided into matching placebo and treatment groups.
Although the CD4 counts of the 877 people given AZT were consistently greater than those of patients receiving only placebo, the first three years of follow-up have shown that the proportions of people in the two groups progressing to overt AIDS or even to death were not significantly different at roughly 18 per cent.
The conclusions are that AZT is not an effective prophylactic of AIDS in those infected with HIV and that the CD4 cell count may not be a reliable proxy for the progression to AIDS in infected people.
Of the two conclusions, the second is the more soundly based.
The first, that AZT may be ineffective in postponing the development of overt AIDS, would be more persuasive if the length of the follow-up reported in this preliminary study had been greater.
It is, for example, possible that those in the treated group who have succumbed are a particularly susceptible group within the whole; whether that is likely will be more easily assessed when the data so far gathered by the study have been analysed to show the fate of subgroups, for example those with different CD cell counts at their recruitment.
The new data are, in any case, consistent with the conclusion of the Veterans' Affairs Cooperative Study, published a year ago (New Engl.
J. Med. 326 , 437; 1992).
In a comparison of the administration of AZT to infected people before and after the appearance of AIDS, early treatment was found not to affect the eventual outcome, but merely to postpone the rapid decline of CD4 cells in the blood.
The implications of these findings are not of course, encouraging.
On the face of things, the use of AZT as a prophylactic of the emergence of AIDS in infected people is ineffective; probably it will now be abandoned.
But nothing is implied by the new study of the utility of AZT in the treatment of those in whom symptoms have already appeared; there is no case for abandoning that treatment, at least on the evidence now available.
It is much more disconcerting that the CD4 count has been shown to be an unreliable indicator of the effectiveness of drug treatment in HIV infection.
That, so far, has been the standard test.
How quickly can those concerned with the search for new drugs switch to examination of lymph-node biopsy material for this assessment?
Two recent articles in Nature by  et all.
and  et all (362, 355 & 359; 1993 showed that these organs (among others) are sites at which the virus multiplies during the often long latency period in AIDS, but more will have to be known of this process before it can be the basis of a routine assay.
The reaction to these developments has been marked by a note of hysteria, at least in the British press.
Variously, headlines have proclaimed a ‘bleak shadow over’ or a ‘setback for’AIDS research.
Those are justifiable, if only just.
But another, in the Sunday Times , announces a page-long article under the banner ‘The cure that failed’, goes on to boast of ‘How we broke the story’(in 1989) that treating AIDS patients with AZT might not be valid and then went on to complain that ‘doctors, en masse, have taken up [AIDS]as a crusade, at the expense of science’.
‘Perhaps’, it continues, ‘the worst feature of the medical and scientific professions’ behaviour has been the enormous dominance of a single focus — HIV — for research, prevention and therapeutic efforts, to the exclusion of other approaches.’
Three issues are raised by the nature of these reactions, one of which is the extremes to which criticism of both professions will be carried while there remains no cure for AIDS.
It is an atavistic reaction, born partly of disappointment that decades of believing that infectious diseases are a danger past and partly from the underlying despair of those infected with HIV and the anger of the groups that represent them.
It is also a dangerous reaction, if only because (in the United States at least) it is likely to mould programmes of AIDS research in directions that will not yield the benefits expected of them.
Needless to say, those infected with HIV are unlikely to be much comforted by intemperate comment of this kind.
There is also a regulatory issue.
AZT was approved in the United States in 1987 for use after symptoms of AIDS have appeared, after a controlled trial was brought prematurely to an end when it emerged that the treated group was doing better than the placebo groups.
Similarly, the use of AZT in early AIDS was quickly approved after the appearance of a report on a controlled trial in April 1990 (New Engl.J.Med. 322,941; 1990).
Those who now complain that these approvals were given too quickly overlook the sense of crisis which the spread of AIDS has engendered over the past decade, not to mention the eagerness of all concerned to match the pathos of those infected with HIV with anything that might be an effective medicine.
The position of the Wellcome Foundation, the chief manufacturer of AZT, is unenviable.
Over the past few years, AZT has been the chief source of optimism about the company's fortunes.
Just as the price of its shares rose dramatically at the outset, so it has now fallen.
Yet there is no reason for anybody to believe that the company has behaved dishonourably.
It is right now to insist that nothing in the Anglo-French study shows AZT to be ineffective in the late treatment of AIDS; it is to be hoped that point will be tested by a new controlled study.
But the company is probably right also to say that several drugs in combination are likely to be involved in more effective drug therapy.
Some of the justification for that may be the recent argument of Y.-K.
Chow et all.
Nature 361 , 650; 18 February 1993).
The pattern of AIDS research is also likely to be profoundly shaped by the events of the past few weeks, but more importantly by the articles by Fauci and Haase and their colleagues than by the preliminary report of the Anglo-French study.
The first and obvious need is to turn the knowledge that HIV is alive and all too well, from the outset of infection, into a workable assay of the progress of disease.
The general application of such an assay will probably in itself provide a better understanding of the pathogenesis of AIDS.
But the way in which the lymph nodes (like some other organs of the immune system) sequester replicating HIV particles is so unexpected that it may well point to unexpected and more effective means for the treatment of AIDS.
All that, of course, would be based on what the sceptics insist on calling the ‘HIV hypothesis’: there is no other, and thus no choice.
Orientation detectors in insects


INSECTS perform highly sophisticated visual tasks with a tiny brain.
This implausible conjunction has often led people to suppose that insects analyse visual patterns in ways which avoid the need for heavy computation, and that mechanisms involved differ sharply from those operating in mammals.
But from the two paper on pages 539 and 541 of this issue it seems that, in at least one important respect, the mammalian visual cortex and the insect optic lobe operate upon visual patterns in a similar way.
Insects, like mammals, extract the orientation of edges within the retinal image.
Bees readily learn to distinguish a pattern of stationary horizontal stripes from a similar pattern of vertical stripes.
, and  have asked how bees do so.
One possibility is that they are blind to the spatial properties of the stimuli and that they can distinguish horizontal from vertical only because the two stripe-patterns activate different classes of motion detectors; that is, detectors of the vertical motion will be stimulated best when a bee approaches or moves past horizontal stripes, whereas the signature of vertical stripes will be a discharge from detectors of horizontal motion.
This possibility should be taken seriously, for a frequent assertion is that insects are better at analysing motion patterns than spatial ones.
Indeed, the optic lobe contains a wealth of neurons which respond specifically to elaborate patterns of motion, such as the optic flow fields generated on a retina when an insect turns around or flies over the ground, but there have been disappointingly few descriptions of neurons with interesting spatial properties.
Spatial cues
et all.
now show conclusively that bees do parse visual scenes for the orientation of edges by means of purely spatial cues.
Their evidence is of two kinds.
First, bees can discriminate stripe orientation when patterns are presented in brief, 2-millisecond exposures, and so must be virtually stationary on the retina.
Second, bees that are trained to approach horizontal stripes do not come to prefer vertically moving random-dot patterns over a similar pattern moving horizontally.
On the other hand, they do favour a horizontal row of dots moving horizontally over a vertical row of dots moving vertically.
So movement neither substitutes for spatial information nor disrupts it.
A feature of the stimuli is that the widths of the stripes within each pattern are random and that the arrangement of stripes is changed from trial to trial.
This means that the stimuli can be discriminated only by differences in their orientation and implies that insects have neural mechanisms for abstracting this parameter.
In the second paper, describes the discovery of a new class of neuron in the optic lobe of the dragonfly.
These neurons resemble simple cells of the mammalian visual cortex in responding optimally to elongated bars in a specific orientation.
Their orientation tuning is broad, with the response dropping to half of its maximum when the orientation changes by 90°.
Although it is tempting to suggest that analogous neurons are involved in the orientation discrimination of bees, this temptation should be resisted — at least for the time being.
For one thing, the neurons in the dragonfly require moving bars to activate them, whereas bees are able to recognize stationary patterns.
What is anatomical basis of this orientation selectivity?(personal communication) suggests an answer which is reminiscent of that proposed by  to account for the ability of octopuses to recognize spatial patterns.
An octopus can be taught to discriminate between vertical and horizontal bars, but it is unable to learn the difference between the bars at +45° and at -45°.
This finding has an anatomical correlate: the dendritic fields of cells in the octopus optic lobe tend to be elliptical with their major axis oriented predominantly horizontally or vertically.
So it seemed plausible that these elliptical fields are stimulated best by bars orientated parallel to their long axis, and that the lack of obliquely oriented cells explained the difficulty the octopus has in distinguishing between the two diagonals.
But it was hard then to perform the necessary physiological experiments to test the idea.
Oriented dendritic fields are also seen in the optic lobes of insects, and  has now been able to correlate physiology and anatomy.
He finds that the dendritic processes of the orientationally sensitive neurons cover an elliptical fields.
The major axis is parallel to the bar-orientation which elicits the greatest response, while the length of the minor axis equals the projection of the width of the visual receptive field.
What is the normal function of insect orientation detectors?
One possibility is that they provide inputs for very specific visual reflexes.
For instance, houseflies fixed so that they can only roll about their long axis, orient themselves to keep edges aligned vertically in their frontal visual field.
Under normal conditions, therefore, they may rely on plant stems and trees to remain upright.
The input for such a reflex could come from a single class of broadly tuned vertical orientation detectors.
Orientation channels
A more interesting function is that performed by orientation detectors in the primate visual cortex.
Here each patch of visual space is analysed by about 18 different orientation channels.
This enables the cortex to encode precisely and economically the orientation of an edge within the patch.
The number of orientation channels in the dragonfly optic lobe is unknown, but work by , and  (personal communication) implies that bees have at least three — the minimum required for coding orientation unambiguously.
When bees have been trained to approach horizontal rather than vertical stripes, they prefer the horizontal pattern over a uniform grey one and they prefer a grey stimulus over vertical stripes.
Bees, thus, actively approach horizontal stripes and avoid vertical ones, suggesting that there are two or more independent channels.
Unlike octopuses, they can also be trained to distinguish between +45° stripes and -45° stripes, bringing the total to at least three.
It would be intriguing if orientation detectors play the same fundamental role in the pattern perception of mammals and of insects.
Such similarities would not have surprised the great anatomist Cajal, who, overwhelmed by his first view of the insect optic lobe, wrote:
In comparing the visual ganglia from a bee or a dragonfly with those from a fish or an amphibian one is extremely surprised.
The quality of the psychic machine does not increase with the zoological hierarchy…
It is as if we are attempting to equate the qualities of great wall clock with those of a miniature watch…[the insect's optic lobe]is a marvel of detail, delicacy and precision.
Flip-flop end to last ice age
Richard G. Fairbanks
The most celebrated finding in ice-core research is the record of atmospheric  concentration spanning the past 200,000 years.
No doubt, future historians will debate the extent to which this record catalysed international interest in global warming forecasts.
On page 527 of this issue, Richard Alley and co-workers report ice-core evidence of a different, but equally profound nature.
From measurements of annual ice-layer thickness over the past 15,000 years, the authors find that Greenland's climate, emerging from the last ice age, twice shifted from glacial to inter-glacial conditions over an astonishingly quick 3–5 years.
This result apparently supports earlier claims that the Earth's climate system has several stable modes, between which it flips.
At issue is whether these apparent climate shifts are driven by internal or external forces.
Specifically, environmental scientists have posed the politically charged question: are the catastrophic climate changes measured in this and previous work at the crest of Greenland the result of climate boundary conditions unique to a glaciated North America, or could they recur tomorrow?et all.
contribute towards unravelling this problem in two ways.
First, they establish the exact age and duration of these climate shifts.
Second, they use ice accumulation rate as a simple index of climate change.
Knowing the rate and timing of the shifts helps eliminate several postulated mechanisms.
The US Greenland Ice Sheet Project II (GISP2) will complete drilling through the Greenland ice cap sometime this summer.
One objective of the GISP2 programme, and the parallel European effort GRIP (Greenland Icecore Project), is to date accurately the apparent shifts from glacial to interglacial conditions that have been observed in previous Greenland ice cores.  et all.
examine the most recent shift to inter-glacial conditions (Younger Dryas/Preboreal shift) and an earlier aborted shift (Older Dryas/Allerod).
Armed with sophisticated sensors to detect subtle chemical and microparticle changes that are known to occur annually, the GISP2 team set out to measure annual layers in the Summit ice core one at a time from the surface to the last glacial period some 18,000 years ago.
Compression of the ice and diffusion of the chemical species were expected to obliterate the annual signal beyond this period.
Much to the scientists' surprise, summer conditions left visible annual strata.
Bleary-eyed after counting and recounting 15,000 layers, they discovered that the climate shifts occurred in less than a decade and that the shifts occurred approximately 1,000 years earlier than previously estimated from the ice cores. although such layer counting has a credibility problem, at the Summit site  et all .
were able to circumvent difficulties that plagued other ice core sites.
The  timescale over the deglacial period has been calibrated recently using  measured in corals and extended  tree-ring chronologies.
This calibration gives the basis to compare the  dated records of the climate shifts to the calendar years as measured by ice-core layering.
The match between  and colleagues' and independent age estimates of the same shifts seen in lake cores and deep-sea sediments is strong support for their quality.
The ice accumulation rate doubled during the first aborted Older Dryas/Allerod shift and during the final successful Younger Dryas/preboreal shift from glacial to interglacial conditions.
Using modern calibrations, & name et all .
estimate that air temperature warmed by 7°C during these shifts, a value consistent with independent estimates based on the oxygen isotope ratio of the ice.
Comparable oxygen isotope shifts have been measured in sediments from Swiss and Polish lakes at precisely the same times.
One additional piece of the puzzle falls into place with the new ice-core timescale.
The two successive episodes of Northern Hemisphere ice-sheet collapse began at precisely the same time as the climate shifts that are recorded in the Greenland ice core (within the cited uncertainty).
On the face of it, this seems like a happy ending that ties up the loose ends.
However, that will require identifying a mechanism for a 7°C climate warming that reached from North America, across Greenland, the North Atlantic and into central Europe.
Analogies to the North Atlantic Oscillation, the much cited ocean-circulation link to climate, cannot apply over such distances.
The prevailing theory that these climate shifts were triggered by the switching on of ‘thermohaline’ density-driven vertical circulation is very appealing; however, it advocates have a difficult, but not impossible, task in explaining mode changes that last only three years.
Although areas of deep-water convection are very localized, significant heat release from the ocean to the atmosphere, the control mechanism, requires large areas of ice-free water.
How could sea ice come and go so quickly?
In fact, an example of such rapid change did occur in the Southern Ocean in the 1970s.
More difficult to explain is how the heat release from the thermohaline circulation (down wind of North America) contributed to the North American ice-sheet collapse.
We can appeal to non-linearities in the climate thresholds, but glaciologists offer an alternative.
The GISP2 ice-cores ages align the shifts to warmer climates precisely with the two periods of catastrophic ice-sheet collapse, as documented by the well-dated changes in sea level recorded in Barbados corals.
Glaciologists have argued that the two intervals of ice-sheet collapse were more probably driven by some extreme nonlinear response to slowly changing climate than by rapid climate change.
Simple diffusion models of the Atlantic Ocean yield surface salinity drops in the North Atlantic of 3–10 per cent during the meltwater pulses associated with the collapse of the ice sheets.
This creates a thin surface layer with relatively low thermal heat capacity which allows very rapid summer heating even at subpolar latitudes.
With few exceptions, the proxy records of the two climate shifts can be explained by changes in seasonality ascribed to this mechanism.
The strong, rapid shifts described by  et all .
occurred when the climate was sensitized by a long-term shift from glacial to the current interglacial conditions, but whether we should take comfort from the fact and the past 8,000 years of stability is debatable.
Midnight sunlight shining through fogs gives the GISP2 drill tower a halo on summer nights.
The frozen hoar frost from the summer fog delimits annual ice bands in the core.
SOLAR PHYSICS
Yohkoh basks in sunlight
Len Culhane
EIGHTEEN months after the launch of the Japanese Yohkoh (‘Sunlight’) satellite to study the Sun's corona, astronomers gathered in Sagamihara to review its principal initial results (What Have We Learned from Yohkoh Observations ?, 23–25 February 1993).
The importance of complicated magnetic field patterns, particularly the ‘reconnection’ of field lines, for releasing energy in the solar atmosphere was especially evident.
The spacecraft was built by the Institute of Space and Astronautical Science and launched on 30 August 1991.
It carries a battery of X-ray instruments provided by Japanese, US and UK research groups.
Two telescopes image the Sun, one in soft and the other in hard X-rays between 0.2 and 100 keV, with angular resolutions of 2.5 and 5 arcseconds, respectively.
High-resolution Bragg spectrometers study selected emission lines to measure the emissives, temperatures and velocities of plasma from solar flares.
Broad-band detectors measure the spectra and variability of flare X- and γ-rays up to 10 MeV.
The Sun produces energy by nuclear fusion reactions in a core of temperature 15 million K. Although energy is transferred outwards initially by radiation, the final transfer to the surface is by convection.
Together with differential rotation, this causes a large amount of mechanical energy and magnetic flux to pass through the surface.
The resulting magnetic field structures control the state of the corona, a hot, tenuous outer atmosphere where the temperature of the plasma reaches 1–2 million K in large, quiet magnetic structures, 3–5 million K in more compact structures associated with active regions, and up to 50 million K during solar flares.
Although the role of magnetic field structures both as channels for energy supply and for containing hot plasma has long been recognised, it seems that Yohkoh may allow a detailed understanding of some processes involved.
An example of magnetic field reconnection supplying energy in large-scale coronal structures was found (, High Altitude Laboratory) by comparing Yohkoh Soft X-ray Telescope (SXT) images with visible-light observations in the Hα line and from a coronameter at Mauna Loa in Hawaii.
Observations from 22 to 26 January 1992 showed a prominence extending above the south-west limb of the Sun on 22 and 23 January.
At this time the Yohkoh images showed faint X-ray emission from a large closed magnetic field structure above the prominence.
This was also seen by the coronameter, which registers the faint visible light scattered from coronal electrons, using an occulting disk to blank off the string visible emission from the surface of the Sun.
However, between 08:07 and 09:20  on 24 January, Yohkoh observations showed that a major change had occurred.
Figure 1 shows a large helmet streamer in X-rays (red) and visible light (green/yellow).
These images were both obtained at 20:12 .
The underlying prominence was found to have disappeared, but subsequent observations on 25 and 26 January showed the newly formed X-ray feature rising at about 1 km .
The disappearance of the prominence indicates that a coronal mass ejection, which blew open the previously closed magnetic field structure, took place before) 9:20  on 24 January.
The re-formation, X-ray brightness and subsequent evolution of the new helmet streamer show that the corona is being heated by reconnection of the disrupted magnetic field.
The observations confirm a model for coronal mass ejections in which magnetic reconnection proceeds outwards along a current sheet that is formed following the prominence eruption.
This also shows that the ejections are not necessarily triggered by solar flares, a view that has recently been gaining support.
On a smaller scale, an example of magnetic reconnection was observed with the SXT following the intersection of two active-region magnetic loops (, US Naval Research Laboratory).
In an interval of 20 minutes, a pair of loops intersected, brightened and then re-formed as two parallel structures with the release of about  joules as heated thermal plasma.
A converse example was also seen in which two separate loops brightened and merged.
The SXT has also discovered a new phenomenon associated with active regions, namely the frequent occurrence of transient brightenings (& name , Tokyo University).
These happen typically every three minutes in regions of high activity; in relatively ‘quiet’ active regions, the frequency falls to about one an hour.
They occur mostly in the form of single or multiple loops, although some are pointlike.
The plasma temperature increases from below 5 to around 7 million K; the associated thermal energies are in the range  joules.
The brightening of the loops starts either at their footpoints or at sites where several loops are in contact.
The footpoint brightenings are suggestive of material being supplied from the chromosphere.
Yet another discovery from the SXT is the observation of X-ray jets in the corona.
These were first seen in full-frame images with time resolution from minutes to an hour (National Astronomical Observatory of Japan).
They are 5,000–500,000 km long, their tips move at 30–300 km  and their kinetic energies are between  and  joules.
They often originate as flares in bright points and emerging flux regions while a void sometimes appears at the footpoint of the jet after ejection.
The example in Fig. 2 is of an undulating jet where the material may be expanding along helically twisted magnetic field lines.
et all .
propose a number of magnetic field reconnection configurations which can supply energy to drive jets and feed them with material from the chromosphere.
The study of coronal bright points, first observed extensively during the Skylab mission, is also being significantly advanced.
The magnetic bipolar nature of the features is now firmly established.
In addition a comparison with ground-based magnetograph data shows that magnetic flux decreased in both poles at the time a flare occurred (, Solar Physics Research Corporation).
The principal new result to come from this work is that bright-point flares are often associated with increased emission in much larger structures.
One example presented involved the production of a jet that moves at  km  and eventually connected with another footpoint 400,000 km away.
This site then flared a day later.
Here again there is good evidence that reconnection supplies the energy and that interaction of emerging flux with existing coronal magnetic structures is usually involved.
For solar flares there is evidence that magnetic reconnection at the tops of loops is often involved in supplying energy particularly for larger structures (, Tokyo University).
The reconnection is very similar to that described above for the helmet streamer, although time and size scales are smaller.
The energy release is followed by an upflow of plasma from the chromosphere.
In an event of 16 December 1991, data from the Bragg spectrometer and the Hard X-ray Telescope (HXT) show that plasma flows up following the impulsive deposition of  joules in the chromosphere by non-thermal electrons (, Tokyo University).
Alternatively, for the early phase of the flare of 15 November 1991, data from both telescopes, the Bragg spectrometer and ground-based  observations indicate that upward flows were driven by thermal conduction (, Hawaii University).
However,(Hawaii University) pointed out that both energy transport mechanisms can operate depending on the amount of energy involved.
Although Yohkoh has not sewn up all of solar physics (no surprise, given the subject's complexity) it is showing where explanations to many of the spectacular coronal phenomena may be found.
FIG. 1 Superimposed images of the solar corona from the Yohkoh Soft X-ray Telescope (red) and the Mauna Loa Observatory coronameter (green).
The low-lying helmet streamer, lower right, was formed after a prominence eruption and coronal mass ejection.
The yellow (red + green) of the outer part of the streamer shows it to be bright in X-rays.
FIG 2.
The development of a twisted jet observed by the SXT between 05:07 and 06:24  on 3 October 1991.
The jet originated at the site of a compact flare and reached an extension of 220,000 km.
The velocity of the top was about 200 km  during extension.
Pharmacology
ET touches down in Houston
Timothy D. Warner and John R. Vane
The three endothelins are highly active peptides — they are the most potent in constructing blood vessels and raising blood pressure, but may also influence such activities a neurotransmitter release, maintenance of electrolyte balance, cell proliferation and hormone production.
Snake venom peptides (the sarafotoxins) also belong to the same family.
How goes the research into working out just what the endothelins do and how they do it?
Such was the subject for examination at a conference on endothelin, held earlier this year.
The news was predominantly heartening, as it became plain that investigation of endothelin action is entering a new phase with characterization of the receptors concerned and the advent of selective, potent and orally active receptor antagonists.
Conversely, progress in identifying the elusive ‘endothelin-converting enzyme’ has been frustratingly slow.
Two endothelin receptors are known, the selective  receptor (endothelin-1=endothelin-2>endothelin-3), and the non-selective  receptor.
Both are members of the rhodopsin superfamily with the receptor selectivity being determined by changes in the loop structures, including the location of cysteine residues in loops I and II ().
Although it was thought that  receptors mediate the contractile effects of the endothelins whereas, for instance vasodilator effects are mediated by , it is now clear that both receptors can mediate vaso-constriction.
For instance, constrictions in the rat aorta, dog coronary artery and saphenous vein, and the iliac artery and saphenous vein of the cyomologus monkey are mediated by  receptors, where the  type mediates constrictions of the rabbit pulmonary artery and saphenous vein, and the dog saphenous vein ().
Similarly, it is  receptors that predominantly mediate constrictions of the renal vasculature of the rabbit (), but  receptors are more important in the rat kidney.
So a central issue at the meeting was whether a man is more like a dog, a rat, a pig or some other animal in terms of the distribution of endothelin receptors.
Immunohistochemistry provides some clues, but caution must be exercised in drawing conclusions.
For instance, in the dog kidney although 80 per cent of the receptors are of  subtype vaso-constriction is largely mediated by  receptors (,).
This observation also implies that endothelines have other renal effects, such as regulating the responses to vasopressin ().
The roles of the endothelins have been clarified by the development of antagonists such as BQ-123 (receptor-selective) and PD 145065 (non-selective)().
Most notably & name :() reported the development of Ro 46–2005, a non-peptide, orally active, non-receptor-selective antagonist which ameliorates the consequences of severely impaired kidney blood flow and blood clots on the brain.
We can also expect increasingly selective antagonists to be developed, for there are effects, such as the stimulation of nitric oxide release from the endothelium, that seem to be mediated by additional receptor subtypes.
So much for the bright side, against which is failure yet to isolate the endothelin-converting enzyme which cleaves the inactive big endothelines (38–41 amino acids) in their active forms (21 amino acids).
We do know, for instance that its minimum substrate is big endothelin (19–34) of which the region of 27–34 is most important in determining the velocity of conversion ().
However there has still been no definitive characterization of the enzyme.
Clearly this could be very important for it is analogous to the development of inhibitors of  angiotensin-converting enzyme, such as captopril, that stop the production of another potent pressor peptide, angiotension II.
This class of drugs has become widely used in treating high blood pressure and congestive heart failure.
Interestingly, in the rat uterus big endothelin-1 is only one-seventh as active as endothelin-1 (), suggesting that this tissue is a particularly active and/or rich enzyme source, for in isolated vascular preparations there is usually some 100–300-fold difference in potency
The endothelins are potent releasers of other locally acting agents, such as nitric oxide and prostanoids.
It is now evident in increasingly complex systems, such as the dog kidney () or even in the human hand (), that the release of these mediators modulates the actions of endothelin.
There was further evidence of the proliferative effects of the endothelins.
For instance, endothelin-1 infusion greatly increases the hyperplasia that follows from balloon-catheter injury of arterial vessels ().
So the endothelins may have a major role in atherosclerosis and the reocclusion that often follows surgical opening of occluded arteries.
Of course endothelin should also have its good side, and treatment with endothelin-1 reverses the hypotension associated with endotoxin shock while still maintaining renal function ().
The next international meeting on endothelins will be held in London in two years' time.
What developments can we hope for before then? top of many people's lists is the characterization of the endothelin-converting enzyme and of the pathway leading to the formation of the endothelins.
Although first found as a product of the endothelium, endothelin-immunoreactivity has been detected in a range of other cell types and tissues, which may or may not contain similar processing enzymes.
The existence of other endothelin receptors also seems to be increasingly likely; indeed, it may well be difficult to imagine that the actions of three different peptides are mediated by just two receptors.
With this in mind it is also important to define better the roles for endothelin-2 and endothelin-3.
Do they have physiological or pathological functions.
These questions will find answers with further development of endothelin receptor antagonists for use in chronic, as well as the more usual acute, studies..
Chemical Dynamics
Chemists make it snappy
Ian W. M. Smith
Over the past decade, lasers able to generate ultra-short pulses have moved out of the laser physicist's laboratory and onto the chemist's bench.
With them, chemists have been able to fulfil a long-standing dream: the direct observation of the motions of atoms as molecules undergo reactions.
These advances were celebrated last month in Berlin in what will surely be seen as a landmark conference on the femtosecond chemistry.
The ‘new frontiers’ are opening up not only in molecular science, but also in studies of fast processes in condensed phases and at interfaces.
At first, 's uncertainty principle might lead one to suppose that high-resolution is sacrificed if one works on such short timescales.
But this is not so.
Experiments typically start with the excitation of an ensemble of molecules by a femtosecond laser pulse.
But because this initial excitation is coherent, high time resolution replaces, in a complementary way, the high frequency resolution of conventional high-resolution spectroscopy.
These aspects of femtosecond spectroscopy are brought out most clearly in respect of molecular vibrations and rotations ().
The technique of rotational coherence spectroscopy illustrates particularly well the advantages of working in the time, rather than the frequency, domain — even if some of its most useful applications move us away from the femtosecond world.
Large molecules, dimers and clusters have large moments of inertia, and consequently their rotational energy levels are so closely spaced that high-resolution spectroscopy is defeated.
But the rotational periods are correspondingly long and for all states are quite simply related to the rotational constants of the molecules.
With coherent, polarized light, one can prepare excited species and observe their recurrence times (at which the excited molecules become realigned) and hence measure their rotational periods.
From their rotational constants, the structures of the species can be deduced.
In the case of isolated molecules, ions and clusters, must emphasis has been placed on the investigation of unimolecular photodissociation and photoionization.
Once a laser has put molecules into a repulsive, electronically excited state, it can take 100–1,000 femoseconds for the species to dissociate into independent fragments.
Clearly, spectroscopic methods based on femtosecond pulses are an appropriate way to observe the unstable dissociating (or transition) states that exist fleetingly as the molecules fall apart.
Transient absorption spectroscopy using femtosecond pulses has been applied very successfully to the photodissociation and predissociation of ICN and NaC1.
In Berlin, Y. Chen (University of California, Berkeley) reported the application of stimulated-emission spectroscopy to photodissociation of the very weak visible spectrum of .
The experiments employ a white-light probe to stimulate the emission and record a full spectrum at several pump-pulse-probe-pulse delays.
This method could probably be applied widely, as it is not limited to strong photodissociating systems and does not, like absorption, require a third state to which excitation is caused in the probe step.
Others recalled the role that emission spectroscopy without this high time resolution can have in uncovering photodissociation.
In a similar, but elegant form of these latter experiments (), conventional, 10–20 nanosecond laser pulses can be used.
With experimental care, an extremely weak emission spectrum of the dissociating molecules can be observed, and from this spectrum and how it varies with the excitation frequency, one can deduce the dynamics of the dissociation process.
Of course, the quantum yields for such emissions are extremely low, and the experiments require species with inherently strong spectra (less of a limitation with the new stimulated-emission methods).
Chemists are interested not only in isolated and dissociating species but also in colliding molecules and their reactions.
But even in highly compressed gases, the time between collisions far exceeds the duration of the collisions themselves, making the interaction details  hard to discern.
Of course, such difficulties are not present in condensed phases.
One can argue that the simplest condensed phase is a van der Waals dimer.
The latest application of such species for the investigation of quasi-bimolecular dynamics was described for the reaction between H atoms and  ().
When , bound in a van der Waals complex with & formula , is photolysed and the H atom impelled into the & formula , the delay time for appearance of the OH reaction product varies between 0.25 and 1.5 picoseconds.
This delay, and its dependence on the energy imparted to the H atom in the photolysis step, indicate the role played in this reaction by short-lived HOCO species.
New and exciting results were also reported for systems on surfaces, in large clusters and in condensed phases.
One such highlight was the study by femtosecond techniques of reorganization at the surface of the semiconductor GaAs ().
A sequence of processes could be identified starting with the initial excitation of photons and consequent production of free electrons and changes in the semi-conductor's electronic band structure.
The resultant destabilization of covalent bonds brings about structural rearrangements on a timescale of 1–10 picoseconds, which under some circumstances can be followed by material leaving the surface.
The meeting was privileged to be given  an historical perspective, as well as a view of biological applications of the latest techniques, by George Porter.
He charted the progress of time-resolved photochemical methods, from the millisecond flash-photolysis experiments of the 1950s, which he helped pioneer, to the femtosecond pulses of the present day.
His lecture posed and promoted the question of whether we have reached the end of a road — can we, need we, go faster?
To follow the motion of atoms in molecules it seems that the time resolution of current techniques may be sufficient, although the development of femtosecond electron diffraction (Zewail) should extend ultrafast measurements to molecules not observable by spectroscopy.
The motions of electrons in molecules are characterized by sub-femtosecond timescales.
However, light pulses that short will have energy spreads comparable to the strength of chemical bonds, and information that can be acquired is unlikely to throw light on chemical processes.
OBITUARY
Albert B. Sabin (1906– 1993)
THE heroic era of poliomyelitis research started with the isolation of the polio virus in monkeys by  and  in 1908, and culminated in the development of vaccines protecting against this paralytic disease in the 1950s. , who died on 3 March, in Washington, was one of the heroes of this era.
His interest in polio is said to have originated during the polio epidemic in New York City in 1931.
This was the year  got his MD degree from New York University Medical School.
's enthusiasm for virology went back to his postdoctoral years at The Rockefeller Institute where he was working under the tutelage of .
isolated and characterized the herpes B virus which caused the death of scientists bitten by a virus-carrying monkey.
With , he also studied the factors involved in the neuroinvasiveness of viruses.
During the Second World War, as a lieutenant colonel in the US Army, isolated the virus of sandfly fever and was successful in controlling the infection through the elimination of virus-carrying insects.
During the war he was also instrumental in developing vaccines against dengue and Japanese B encephalitis.
At one point in his career he became interested in toxoplasmosis and shortly thereafter developed a diagnostic dye test for the disease.
These were significant scientific contributions.
But, undoubtedly, 's major achievements were in the field of vaccination against polio.
In March 1951, I reported the first immunization of children with live attenuated polio type II virus at a meeting of the National Foundation for Infantile Paralysis in Hershey, Pennsylvania.
At that time, was experimenting with the immunization of monkeys with vaccines made with killed polio virus.
, on the other hand, agreed with me during our discussion at the meeting that a live oral vaccine is a preferable method of immunization against polio.
When Sabin developed his attenuated strains of polio he energetically pursued his goal of making them widely accepted as vaccine strains.
The most convincing evidence of the innocuousness of the strains and their effectiveness was the vaccination of millions of subjects in 1958 and 1959 in the Soviet Union.
This episode, more than anything else, finally convinced the authorities to license a live polio vaccine for immunization in the United States.
Following worldwide use of the oral vaccine, 's time was largely taken up in visiting various countries around the world and action as an adviser to the polio vaccination programme.
At one point he was involved in research on the possible viral aetiology of human cancer.
But he soon became convinced that the herpes virus, which was thought for a while to be associated with cancer, had in reality nothing to do with the disease.
relentlessly pursued his interest in vaccines.
He immunized children against measles by aerosol application, and wrote about the varicella-zoster vaccine.
He was also concerned about the influenza epidemics and studied the virus involved.
He continued writing numerous papers about the oral polio vaccine, with particular emphasis on vaccination problems in the Third World tropical countries, and in 1984 he published a description for ‘Strategies of elimination of polio-myelitis virus in different parts of the world with the use of oral polio virus vaccine’.
The last case of paralytic polio in the Western Hemisphere was reported earlier this year, from Peru from 1992, a testament to his work.
In the last years of his life, became interested in AIDS.
He was convinced that HIV is transmitted only by mucosal route and doubted whether any vaccine that fails to convey mucosal immunity will protect against the disease.
A letter on this topic, a response to some critics, appeared only last month (Nature , 362,212; 1993 ).
That his views on AIDS carried great weight was a result not only of his experience with research on vaccines, but also because he was so articulate and incisive.
was a champion of the debating society at New York University where he graduated.
This skill never left him.
He was the most eloquent and persuasive participant at meetings and was hard to defeat in scientific argument — not least because he often interpreted data more correctly than the actual presenter.
At one time, and I became adversaries over the selection of polio virus strains to be used as oral vaccines.
This did not affect our long-lasting friendship and mutual respect.
In a letter to me written just over a year ago, reviewing a paper speculating that AIDS started with polo vaccination in the Belgian Congo, expressed his opinion that this was ‘a most irresponsible and uncritical communication’.
Courageous and wise.
This is how I see him.
I will miss him sorely.
Parasitology
The evolution of virulence
Andrew F. Read and Paul H. Harvey
How virulent should we expect animal parasites to be?
The answer depends, in part, on whether increased damage to the host improves the reproductive success of the parasite.
Biologists have developed a theoretical framework within which the evolution of virulence can be analysed, and it has been demonstrated experimentally that virulence can evolve in response to alternative modes of parasite transmission.
Now, in an elegant comparative study published in Science , shows how differences in the likely mode of transmission correspond with expected differences in virulence in natural populations.
For parasites whose offspring infect the offspring of their host (vertical transmission), any behaviour by the parasite that decreases the fecundity of its host (and thereby that of the parasite itself) is unlikely to be favoured by natural selection.
In contrast, parasites whose offspring are not dependent on those of their host (horizontal transmission) are expected to become more virulent if that results in the production of an increased number of parasite young.
In a test of this idea,et all .
propagated populations of phage-infected Eschericia coli in two ways.
In one, phage replication was wholly dependent on host reproduction.
In the other, phage could be transmitted vertically and horizontally.
The two selection regimes had the expected effects on host fitness: infected bacteria in the vertically transmitted lines increased in density much more quickly than those in the lines where horizontal transmission could occur.
Genetic changes in both host and parasite were responsible for the evolved differences in virulence.
The generality of conclusions from well controlled experiments can often be tested by comparative studies.
For example, has argued that variation in virulence among a variety of human diseases may be accounted for by differences in the relationship between virulence and transmission.
In his view, directly transmitted diseases such as common colds generally have lower virulence than vector-borne diseases such as malaria because transmission rates in the former are higher if infective hosts continue to come into contact with other individuals.
In contrast, host immobility is beneficial to vector-borne diseases because vector-avoidance behaviour is minimized.  's surveys of human diseases generally fit this pattern.
But such broad surveys necessarily include a wide range of parasites, which differ not only in transmission routes but in many other aspects of their natural history.
As one of us wrote only two years ago, ‘comparisons of the virulence of parasite taxa utilizing different transmission routes from similar hosts are not yet possible’.
's study shows that they now are.
examined a group of direct life-cycle nematodes (Parasitodiplogaster spp.) each specific to one of eleven Panamanian fig wasp species (Pegoscapus spp.,Tetrapus sp.).
Because of the peculiar natural history of these hosts, the relative importance of horizontal and vertical transmission differs among parasite species.
Gravid female fig wasps enter figs, lay eggs and die.
Their progeny mate inside the fig before the young females disperse to new figs.
The nematode generations are intimately connected with those of their hosts.
For part of their life cycle, nematodes consume the body of the adult female fig wasp.
After the death of an infected foundress in a fig, about six or seven namatode adults emerge from the carcass, mate and lay eggs in the same fig.
The eggs hatch synchronously with the emergence of the wasp larvae, and the namatode larvae infect young female fig wasps before they disperse.
Because the wasps lay all their eggs in a single fig, estimates namatode virulence as the relative lifetime reproductive success of the infected compared with uninfected wasps, when each lays on its own in a fig.
A consequence of fig-wasp life-cycles is that the number of female wasps laying in a fig determines the opportunity for horizontal transmission by the nematodes.
If only one female is present, a nematode's offspring must infect the offspring of that host.
So selection should favour benign parasites that limit the destruction of their hosts.
But where there are more foundresses per fig, parasite reproductive success depends less on the fecundity of its host because other wasps are laying potential hosts.
It seems reasonable to assume that greater consumption of host resources by nematodes causes reductions in host fecundity, and that, as in other parasitic nematodes, bigger worms produce more eggs.
As a consequence, greater virulence should be favoured if enough offspring of other wasps can be infected to more than compensate for the subsequent loss of extra offspring from the current host.
shows that this is indeed the  case.
In those species where almost all broods are initiated by one foundress, he found that parasitism had no detectable effect on fitness.
But in species where less than a third of broods are founded by a single wasp, parasites reduced the reproductive success of their hosts by up to 16 per cent.
The relationship between virulence and the frequency of single-foundress broods were roughly linear.
Thus, the wasp-nematode relationships ranged from commensal to clearly parasitic, in accord with their different transmission routes.
Comparisons of virulence of closely related parasites in hosts with which they have co-evolved automatically control for many third variables which potentially confound such analyses.
Comparisons of the same host-parasite system when transmission is likely to vary, either between populations or over time, may also prove profitable.
For example, and  have suggested that fluctuations in the virulence of gerbil leishmaniasis (an animal model of the human disease) are a consequence of seasonal variation in transmission dynamics.
Now that observations are beginning to support theories about the relationship between virulence and transmissibility in the evolution of pathogenicity, the way is open for further theoretical developments.
Many of the same theoretical issues arise in the study of the evolution of sex ration on group-structured populations, a topic which has spawned some of the most impressive fits between quantitative predictions of evolutionary theory and field data.
It is no accident that the curious natural history of fig wasps played an important part in that work as well.
A twist in the tail —Parasitodiplogaster obtunema , one of the direct life-cycle nematodes, parasitic on fig wasps, central to the work described here.
(Photo,)
Palaeoanthropology
Secrets of the Pit of the Bones
Chris Stringer
Some 300,000 years ago, the remains of at least 24 people somehow found their way into the Sima de los Huesos, a small chamber deep within a cave in the Sierra de Atapuerca, northern Spain.
There they became fossilized.
Although the mechanism of accumulation of the humbled bones presents a puzzle, the discoveries of the latest in a long series of excavations by Spanish workers, reported on page 534 of this issue, seem not only to have settled the question of the affinities of the Atapeuerca hominids, but also promise to clarify our understanding of the evolution of humans in Europe.
Debate about the European fossil hominid record has concentrated on the Late Pleistocene interface between Neanderthals and early modern humans in the period between 40,000 and 30,000 years ago (40–30 kyr).
This emphasis has tended to eclipse developments in analysis of the Middle Pleistocene record, which begins with the Mauer (Heidelberg) mandible (estimated age about 500 kyr) and ends at about 130 kry, after which time undoubted Neanderthals were present in Europe.
A possible chronological sequence for some of the import important hominid finds is shown in Table 1, together with proposals as to their taxonomy.
There are two main issues in the study of the Middle Pleistocene sequence.
Can the species Homo erectus be recognized in the European fossil record?
And how far back can the Neanderthal lineage be traced?
Other questions that follow are whether other species (for instance H. heidelbergensis ) should be recognized to fill any taxonomic void between H. erectus and Neanderthals, and the extent to which internal, rather than between-taxon, variation is responsible for differences between the specimens.
On this last point, I have argued that differences between large specimens (such as the Petralona cranium) and small specimens (such as the Steinheim cranium) do warrant a degree of taxonomic separations, whereas others consider that the differences are due more to internal population variation such as sexual dimorphism.
The Atapeuerca skeletal sample is large by the standards of any other Middle Pleistocene hominid site, so it provides an unprecedented opportunity to examine the internal morphological and metrical variation of what is assumed the represent a penecontemporaneous sample of individuals of different ages and sex.
This is important for questions of taxonomy, because we can observe the relative frequency of H. erectus and Neanderthal characters in the sample, and for phylogeny, because questions of character polarity may be resolved (for some parts of the skeleton there were no Middle Pleistocene data before the discovery of the bones at Atapuerca).
The new paper and Table 2 list the extent to which the Atapuerca sample shows some of the characters typical of H. erectus (late European), Neanderthals, and modern H. sapiens .
The absence of most of the distinctive H. erectus features in the Atapuerca sample is evident.
There are more Neanderthal features present, although some postcranial characters may well turn out to be more widespread plesiomorphies when data from other, non-Neanderthal, samples are known.
But the presence of characters such as midfacial projection and an incipient suprainiac fossa does seem to point to a significant phylogenetic link with the Neanderthals, aligning the Atapuerca hominids with that clade (or species, if H. neanderthalensis is recognised).
What of variation in Middle Pleistocene specimens generally?
My reasons for distinguishing the Petralona and Steinheim crania included their contrasting facial morphologies and very large size difference.
However, the small atapuerca cranium 5 shows clear facial resemblances to Petralona, while a larger facial fragment (AT-404) displays a resemblance to the cheek region of Steinheim.
Size differences between crania 4 and 5 also approach those between Petralona and Steinheim.
So although considerable differences remain between the Petralona and Steinheim crania in features such as vault thickness and occipital form, it seems feasible that they can both be regarded as extension of the variation shown in the early Neanderthal population(s) sampled at Atapuerca.
In turn, more fragmentary finds such as those from Vértesszöllös (Hungary) and Bilzingsleben (Germany), regarded by some as representing H. erectus , may also be linked morphologically to the Petralona-Atapuerca group.
This would lead to the further weakening of claims for the existence of H. erectus in Europe in Europe (Table 1,a ).
The new data also bear upon the concept of a separate species,H. heidelbergensis , in the Middle Pleistocene of Europe and Africa (Table 1,c ).
This concept sprang from the resemblances between European specimens such as Petralona and African crania such as Broken Hill, but it depended not only on a clear demarcation of the specimens such as Petralona and African crania such as Broken Hill, but it depended not only on a clear demarcation of the specimens from H. erectus and modern H. sapiens (still very defensible), but also on a demarcation from the Neanderthals.
In the case of the European Middle Pleistocene representatives of H. heidelbergensis , this second distinction will be much more difficult to maintain following consideration of the variation present in the Atapuerca sample.
It is difficult to envisage a suite of characters that could consistently differentiate specimens such as Mauer, Arago and Vértesszöllös from the primitive Neanderthals whose remains are concentrated in the ‘Pit of the Bones’ at Atapuerca.
The Neanderthal lineage seems to have its roots deep in the Middle Pleistocene.
For those such as myself, who believe the Neanderthal lineage was distinct from our own, this would mean that the origin of the H. sapiens clade was similarly ancient.
DAEDALUS
A bid for pregnancy
IMPORTANT biological objects, as  remarked, come in pairs.
For example, a woman has two ovaries.
Yet every month, only one of them releases an ovum; the other holds back.
How do they decide which of them is to do the job that month?
One guess is that they work in strict alternation, perhaps in obedience to a monthly nerve signal from the brain.
This seems unlikely; nature is not usually so digital.
In any case, reproductive physiology tends to use hormonal rather than nervous signals.
reckons that the ovaries compete for the monthly honour.
Each sends out some sort of hormonal signal, while reading the corresponding signal sent out by the other.
Like two poker players, each raises its level of hormonal bidding, until one of them feels outbid, and folds.
The victor then provides the ovum for that month.
This mechanism suggests a novel form of contraception.
By introducing false signals into the system, it should be possible to trick each ovary into thinking that the other has won the bidding.
So DREADCO endocrinologists are taking a regular sequence of blood samples from a panel of female volunteers.
They are looking for rapid fluctuations of usually neglected minor steroids during the build-up to ovulation.
In these signals they hope to read the bidding and counter bidding of the two ovaries.
The results could be quite complex and subtle.
Each ovary may well prepare several potential ova each month, like a card player considering which card to play in the light of signals from the opposition.
Once the team has cracked this bidding code, they should be able to subvert it.
DREADCO's ‘Anova’ contraceptive will slip a false hormonal bid into the system.
It will convince each ovary that the other has won the trick.
Both will then fold in defeat, and their owner will not become pregnant that month.
A single monthly pill or injection of Anova should work neatly, with no side-effects.
By preventing ovulation it may ben suppress menstruation — a useful advantage, once the user has realized that it is not a sign of disastrous contraceptive failure.
Furthermore, the same biochemistry might be adapted not to eliminate the chance of pregnancy, but to increase it.
Four would-be mothers, Deadalus hopes to develop ‘Binova’.
This inverse hormonal cocktail will be a sort of biochemical misère bid.
It will convince each ovary that the other has folded.
Both will therefore produce an ovum that month.
This will double the subject's chance of pregnancy, and also give her a useful chance of bearing non-identical twins.
David Jones 
SCIENTIFIC CORRESPONDENCE
Yttrium carbide in nanotubes
SIR — Filling the hollow core of carbon nanoclusters with foreign material has added a new aspect to the promise of this family of novel materials.
Lead has been inserted into nanotubes by capillary suction through open ends, and micro-crystals of  have been encapsulated into polyhedral cages.
We have now used the general approach in refs 2 and 3 to place yttrium carbine into nanotubes.
We prepared samples in a computer-controlled reaction chambers.
The composite graphite anode had a centre hold packed with paste of yttrium oxide powder mixed with isopropanol.
Both materials were removed in a 550-torr helium arc discharge sustained by 28 V at 70 amps d.c.
The material deposited at the cathode was ground into powder for examination by a transmission electron microscope and an X-ray diffractometer.
Figure 1 shows an electron-microscope image of a 10-nm diameter tube contained in the deposit, filled with material in the larger of two internal cavities.
The irregular appearance of this material does not necessarily mean that it is amorphous, as lattice images depend on the direction of the electron beam.
Apparently, the process conditions promote deposition of the enclosed material in the larger cavity, but prohibit it in the smaller.
The inserted material does not wet the internal cavity surface completely.
Filling extends for 100 nm along the length of the tube without overlapping the edges of the innermost tube.
This would not happen by chance for material deposited on the outside of the tube.
The tube tips remain closed, and show no sign of damage.
Electron energy-loss spectroscopy places the electron beam of nanometer size entirely on the filling material.
The M edge of yttrium and the K edge of carbon is observed with no oxygen signal detected (Fig. 2).
Yttrium metal peaks are absent in X-ray power-diffraction patterns.
Both results suggest that the nanotubes are filled with yttrium carbide crystals, and contain no yttrium metal or yttrium oxide.
Low-magnification micrographs show that about 20% of the nanotubes are 50% of the nanoparticles are filled.
The fraction apparently depends on particle shape and dimension, indicating an influence of the internal topography of the cavity.
Inclusion in the large diameter cavity but not in the smaller establishes size and shape as factors influencing the ease of filling.
Control of the inclusion through variation of the processing parameters will be a critical factor in developing technological applications.
FIG. 1 High-resolution transmission electron-microscope image of a nanotube with two different sizes of inner cores.
Only the larger core is partially filled; the smaller one is empty.
Scale bar, 3 nm.
FIG. 2 Electron energy-loss spectra of a nanotube filled with foreign material.
Spectrum after removing the background is shown in the inset.
Chicxulub — K/T melt complexities
SIR — and  attempted to strengthen the link between the Chicxulub structure and the Cretaceous/Tertiary (K/T) boundary by comparing their reconstructed Chicxulub Y6N17 groundmass compositions with typical igneous fractionation trends and impact glasses from the K/T boundary in Haiti.
They argue that Y6N17 could not be easily produced by volcanism, and was probably formed by the same impact event as the Haiti glasses.
They reconstructed their Y6N17 compositions from averages of microprobe data and modal analyses of three groundmass minerals, including albite (see figure), asserting that this technique allowed them to avoid the contaminating effects of xenoliths and secondary mineralization that pervade the sample.
Regardless of its origin, however, a silicate liquid crystallizing augitic pyroxene in equilibrium with plagioclase of andesine composition would not co-crystallize albite.
Typically, albite of this purity results from secondary alteration processes such as hydrothermal metasomatism or authigenic growth during burial diagenesis.
In hydrothermally altered igneous rock, pseudomorphic replacement of calcic plagioclase by albite is common, often in cases where relict primary textures are preserved.
Corresponding changes in whole rock major-element chemistry (for example) can cause substantial shifts away from cotectic compositions.
Data in Table 1 (of ref. 1) reveal that about 27% of  and & name 's average Y6N17 groundmass is albite.
Consequently, it is possible that alteration is responsible, at least in part, for these unusual groundmass compositions.
Whether impact melting is necessary or sufficient to produce these non-cotectic compositions remains to be demonstrated.
and  also state that their Fig. 1 b (ref. 1) shows a ‘coherent mixing trend’ defined by slight variations in their reconstructed ground-mass compositions (A, B, C and D), and suggest further that the Y6N17 groundmass lies along the same mixing trend as that previously noted for the Haiti glasses.
The notion that the Haiti glass compositions can be modelled as a mixing line is based on a published range of glass compositions that is much larger than that used in ref. 1.
When additional data reflecting this more extensive range are considered, a simple collinear relationship between the Y6N17 groundmass and the Haiti glasses is not observed (part a in the figure).
In addition, if a mixing relationship exists between the ground mass trend and the Haiti glass trend, it would be evident in any depiction of this projection scheme.
However, when these data are projected from olivine (part b in the figure) rather than plagioclase, it is readily apparent that the ‘coherent (groundmass) mixing trend’(ref. 1) is oblique to the Haiti glass trend.
It is possible that the groundmass ‘trend’ noted by  and  is  an artefact?
Any composition derived by their technique is constrained to lie within the narrow triangular region of the plate delimited by the three minerals used in their modal reconstruction.
It is conceivable that the modest groundmass variations (A–D) derived by  and , on the scale of a thin section, reflect uncertainties inherent in their technique, in which case a ‘trend’ would represent only scatter about an average composition.
Furthermore, Y6N17 AVG (see figure) departs significantly not only from the Haiti glass trend, but also from other samples of Chicxulub melt rock, some of which show no petrographic sign of alteration or unmelted casts (see C1N10 in the figure).
Consequently, the apparent collinearity of ‘trends’ in their illustration (Fig. 1 b in ref. 1), while intriguing, does not provide compelling evidence for a link between Chicxulub and Haiti glasses.
The fact that this Y6N17 groundmass composition does not lie on the Haiti glass trend does not preclude a genetic association.
The Y6N17 melt rock contains abundant partially assimilated casts of target lithologies.
Primary variations in whole rock (melt + casts) composition within a suite of impact melt rocks could reflect variations in the relative proportions of incorporated target lithologies dependent on distance and depth from the impact point.
Variations in melt matrix or groundmass composition may evolve through selective mechanical disintegration of casts and non-modal partial melting of minerals to various degrees.
Secondary processes that may overprint these primary compositional variations include hydro-thermal metamorphism and low temperature alteration over time; large volumes of impact melt may even undergo chemical differentiation analogous to magmatic systems.
In contrast to impact melt and melt breccias within and proximal to an impact crater, tektite-like impact glasses are early-formed total melts that are rapidly quenched, and thus more likely to show a less complicated mixing relationship.
Given the complexities involved in the formation and evolution of impact melt rocks, there is no reason to expect that tektite-like impact glasses and melt rocks within the crater will necessarily bear a simple linear mixing relationship.
There is compelling evidence for a link between Chicxulub and the Haiti impact glasses based on Ar/Ar age determinations.
The disparity between groundmass compositions reconstructed by  and  and the Haiti glass trend suggests that the selective clast melting and alteration are important effects that may not be easily circumvented when attempting to decipher impact melt petrogenesis.
Properties of an HIV ‘vaccine’
SIR —et all .
raise two points in their Scientific Correspondence.
On is that a recombinant gp160 envelope glycoprotein precursor from HIV-1 produced by MicroGeneSys in a baculovirus expression system does not bind strongly to the CD4 receptor; and the other is that this recombinant gp160 does not stimulate the same antibodies as the HIV-1 virus does in natural infection.
We consider these properties to be advantages of our vaccine rather than disadvantages, as it is just these properties that distinguish the MicroGeneSys vaccine from the other candidates.
Vaccination with recombinant gp160 in patients infected with HIV-1 broadens HIV-specific envelope-directed immune responses, including crossreactive antibodies to gp160 epitopes and  and  cytotoxic T-cell responses.
HIV is a progressive disease which the immune response ultimately fails to control.
Therefore, vaccines that present HIV-1 envelope glycoproteins (gp120 or gp 160) in their native form would be unlikely to be of clinical benefit to an HIV-infected individual.
By design, we never intended our gp160 molecule to be identical to the native protein.
Antibody responses against native HIV-1 proteins, including the types described by Moore et all .,
exist in nearly all AIDS patients but do not prevent progression of HIV disease.
Also, the binding of gp120 or gp120-antibody complexes to CD4 has been shown to interfere with antigen-specific activation of CD4 cells and trigger programmed cell death in vitro , which may contribute to the pathogenesis of HIV infection (including immune function deterioration and decline of CD4 T lymphocytes).
The absence of CD4 binding by the MicroGeneSys gp160 vaccine  may therefore be looked on as an added safety feature.
It is known which specific immune responses are required for therapeutic benefit, so we have proceeded cautiously.
More than 1,100 HIV-positive patients are enrolled in five independent phase I trials and three independent phase II trials with gp160 with up to 4 years of follow-up.
The phase I studied have shown evidence of stable CD4 cell counts, stimulation of cytotoxic T cells and the suggestion of restoration of immune function.
Some of these results have also been confirmed in phase II studies.
On the basis of these and other clinical results, MicroGeneSys gp160 was chosen by scientists at the Karolinska Institute, the National Bacteriological Laboratory and South Hospital in Sweden for the first phase III vaccine therapy studies.
SIR — Differences in opinion may exist as to whether the principles of therapeutic vaccines have been solidly established.
It is clear, however, that the immune response in preventive and therapeutic vaccines differs in fundamental ways.
Preventive vaccines are introduced into an immunologically naive system, whereas therapeutic vaccines encounter a system already immunologically primed.
In the present discussion on the gp160 protein from MicroGeneSys this fact has been largely ignored.
The concept of ‘original antigenic sin’ was first shown by studies on the antibody response against influenza virus.
A first encounter with a particular virus strain led subsequent contacts with other influenza strains to induce antibody responses strongly biased against the first ‘original’ strain.
Subsequent research has documented that this effect occurs at the B- and T-cell levels.
The fact that gp160 expressed in baculovirus with the sequence of HIV strain IIIB (LAI) will induce different immune responses when used de novo in previously unimmunized individuals as compared to already infected individuals is thus highly logical.
Whereas non-infected individuals respond with largely type-specific responses, infected individuals respond with a profile distinctly different from the normal individual.
A broad antibody response, including not only IIIB but MN and even autologous isolates with regard to neutralization, is thus induced by vaccinating the infected individual.
Similarly, there is significant enhancement of T-cell responses against HIV- env — protein-derived peptides.
Our extensive placebo double-blind study of HIV-1-infected individuals from the Nordic countries and Switzerland that is now starting has two basic aims:
(1)
To verify or disprove that therapeutic vaccines can induce an anti-HIV immune response of such a kind that it has clinical, positive consequences; and
(2)
to carry out a double-blind trial that could lead to clinical use of the product.
It has been shown that a broadened serological response occurs after immunization of HIV-infected individuals and that T-cell responses increase drastically.
Further, increased affinity to unrelated crossreactive MN HIV peptides increases after immunization with rgp160 of the LAI strain (B. W. et all .,
unpublished results).
These observations suggest that an anamnestic responses to the patient's own virus might become reactive by the foreign, but still related, protein.
Certainly other candidates may also induce such reactivities.
Both CD4 and CD8 cytotoxic T lymphocytes have, perhaps unexpectedly, been shown specifically to increase during a series of six immunizations of previously HIV-infected subjects.
MOORE ET ALL .
REPLY — and  claim that MicroGeneSys, Inc. has ‘designed’ its gp160 to be denatured.
Oral presentations by company scientists indicate that the design feature involves extraction of gp160 from insect cells in the presence of urea, followed by enrichment of the glycoprotein fraction on lentil lectin columns.
The conversation of MicroGeneSys to virtues of denatured immunogens is one of the Damascene proportions.
Can this be the same company whose scientist's opinion was only 2 years ago that ‘…native structure is required for an HIV-1 envelope glycoprotein to be a successful vaccine antigen’?
This was at a time when the same gp160 product was being vigorously touted by MicroGeneSys as a prophylactic vaccine candidate; a time when, as now, being able to mimic as closely as possible the natural viral antigens was held to be important for such a vaccine.
It is significant that MicroGeneSys gp 160 has been excluded from recent trials of prophylactic vaccines.
It is cynical to suppose that, far from ‘designing’ its gp160 to be denatured, MicroGeneSys has rewritten history and grasped the concept of vaccine immunotherapy as a way to exploit their denatured product?
Post hoc ergo propter hoc .
A rationale for using denatured gp160 as a therapeutic agent in HIV-infected people is to boost the immune response by raising antibodies against novel gp160 epitopes, as  and , and & name :et all .,
describe above.
This concept, originally promoted by  and , is not entirely devoid merit.
We believe, however, that to achieve a relevant alteration of the immune response will require an immunogen substantially more sophisticated than the MicroGeneSys gp160.
It would be shame if a superficially reasonable idea was destroyed by a premature focus on poorly designed reagents.  et all .,
also justify the use of MicroGeneSys gp160 as a recall antigen, which boosts the production of antibodies raised originally against native viral antigens earlier in infection.
But it is precisely those antibodies which  and  dismiss to cavalierly as unimportant for prevention of progression of HIV disease!
Which view prevails?
and  cite reports that binding of gp120 to CD4 in vitro can perturb immune cell functions.
To date, however, there is no evidence from clinical trials that native gp120 vaccines harm the immune systems of naive or HIV-infected recipients in vivo .
But  and  fail to point out that the region of retroviral envelope glycoproteins (including HIV-1) most commonly found to have immunosuppressive effects in vitro is located in the ectodomain of gp41 (ref. 14), and is therefore present in MicroGeneSys gp 160 but absent from gp120.
Furthermore, gp160 has the potential for raising unwanted autoimmune responses because of a region similar to MHC class II present in the gp41 moiety but not in gp 120.
Perhaps these features should also have been ‘designed’ out by MicroGeneSys?
The importance under in vivo conditions of any of these in vitro observations remains to be determined.
et all .
 advertise that they are testing MicroGeneSys gp160 in a large-scale clinical trial in Sweden.
Data from controlled phase II trials of MicroGeneSys gp160 by the US Army should be available later this year.
These will reveal whether or not et all .
were wise to expand the scope of their experiment before knowing the results of previous ones.
We did not suggest in our previous Scientific Correspondence that such trials should not take place; indeed one of us (J. R.) will soon be involved in a clinical trial of MicroGeneSys gp160 in pregnant women.
We restricted our comments to the design of pending trials in the United States that are a matter of considerable debate, and suggested that such trials should be comparative precisely because, as  and  say, ‘it is not known which specific  immune responses are required for therapeutic benefit’.
If only a single product is tested, how are we ever going to know?
See Nature 362 , 277; 25 March 1993 for an account of MicroGeneSys's withdrawal of its vaccine from the US Army's trial.
Mulluscan shell growth and loss
SIR — Shell growth rings are the principal source of information on the age and growth of mulluscs.
A growth annulus is assumed to be deposited each year, and microscopic rings are thought to record the length of lunar, daily and tidal cycles.
The assumption that shell material, once laid down, is never removed, has not been adequately tested.
Some studies have used marks on shells that would disappear if shells decrease in size; it is known that molluscs can remain living for long periods without detectable growth; and some studies have recorded negative growth but dismissed it as apparent error.
If molluscan shells can decrease in size, then growth rates of mulluscs could be overestimated and growth annuli would not reliably estimate age.
We investigated changes in shell size in populations of the common freshwater mussels Anodonta grandis grandis and Lampsilis radiata siliquoidea at two sites in a well buffered, oligotrophic lake.
The rate of shell growth was determined by measuring and marking the edge of the shell at a recorded time, returning the mussels to their natural habitat, then remeasuring the shell dimensions after 1–5 years of in situ growth.
Seventy-three Lampsilis and 56 Anodonta were measured and marked during August 1986 and 1988.
We removed animals from the lake briefly, used non-toxic dental cement to glue a pointed, rigid, plastic label at the posterior-ventral margin of one valve of each mussel, and returned animals to the water within 1 hour of collection.
Shell dimensions were determined independently by W. L. D. and an experienced field assistant for each mussel relocated during August of 1987–91.
The initial height of each mussel and the height in subsequent years were measured exactly perpendicular to the shell hinge at the umbo using a digital caliper (±0.01 mm).
Mussels were handled gently and kept at lake temperature in ambient lake water during measurement.
Measurement error found in blind repeated trials was less than 1 mm in 95% of the 600 trials (see figure).
More than 35% of marked mussels decreased in size.
Decreases were commonly 10% and sometimes as much as 20% of total shell height.
Although Lampsilis showed the most frequent and radical rates of shrinkage,Anodonta shells also decreased significantly in size.
Decreases in shell size are not due to our marking method or the general water quality in this lake, but vary among sites and populations.
For example,Lampsilis at the sandbar site grew at rates (average 2.6 mm per yr) similar to those found for the same species in other lakes, whereas less than 1 km away, in the same basin of the same lake, the average rate of growth of marked Lampsilis was -0.3 mm per yr.
Changes in shell size were significantly greater than measurement errors (see figure for the example of Lampsilis ).
Most negative measurement errors were between 0 and -1.1 mm; the greatest found in 600 trials was -2.5 mm.
At the sandbar site, all Lampsilis showed growth, but at the bay site shell height in Lampsilis frequently decreased at rates greater than 5 mm per yr.
A  comparison of the 52 estimates of annual lampsilis growth that were  with the 247 estimates of measurement error  shows that shell shrinkage is much greater than would be expected from measurement errors alone (n=299, P<0.0001).
Shrinkage was not caused by exterior shell erosion because the outside periostracum was largely unbroken in shrinking shells and the nacre and crystalline layers inside had been removed, leaving the excess periostracum.
The erasure of shell growth we found has not previously been reported in the longstanding literature on molluscan growth.
We hope that people trying to read the shell-growth record will now be aware that part of it may have been erased.
Cyberscience
William H. Press
Supercomputing and the Transformation of Science.
By William J. Kaufmann and Larry L. Smarr.
W. H. Freeman (Scientific American Books): 1993.
Pp. 238. –18.95, $32.95.
In their preface,(who has previously written several excellent books) and  (an astrophysicist who now directs the National Center for Supercomputing Applications in Illinois) invoke the image of a global ‘cyberspace’, a construct of pure information, parallel to, but distinct from, physical reality.
Increasingly, we are seeing all sorts of societal activities migrate into cyberspace.
The financial sector's move is almost entire.
Entertainment and the arts are beginning the transition, not only with digital storage and transmission media (compact disks, high-definition television), but also with the creation of purely computational art, ranging from computer music to the popular ‘morphing’ of film images.
The intersection of cyberspace with science, variously called computational science, simulation science or (as these authors term it) supercomputing, is a world where physical laws are known or posited, as in theoretical science, but where complex physical systems can be seen (within the computer) evolving in their own only partially controlled manner, very much as in an experimental or observational science.
Supercomputing, according to the authors, is not just faster computing.
It is marked by the crossing of a distinct threshold in the complexity and realism with which systems of interest are modelled, an abandoning of the simplify-and-idealize tenets of theoretical science in favour of a much richer (though, to some, less pristinely elegant) framework.
A trivial, and at the same time profound, corollary is that the results of supercomputing cannot usefully be displayed as graphs or tables of numbers.
Computational science requires, and uses, the most advanced computational techniques of colour graphics, computer visualization and animation.
(As part of the Scientific American Library series, this book is lavishly illustrated in full colour.
Alas, no accompanying video!)
The first third of the book (three chapters) is an ambitious and generally successful attempt to explain, at a level appropriate to the educated nonscientist, how computational science works, specifically how the continuum (or quantum mechanical) equations that describe physical reality can be accurately approximated by discrete computational techniques such as finite difference, finite element, Monte Carlo or N-body particle models.
The rest of the book surveys the different scientific disciplines and highlights specific projects, mostly work done in the united States at one or another of the national computer centres sponsored by the National Science Foundation.
Topics include quark phase transitions, fullerenes, quantum-well devices, chemical kinetics, protein folding, blood flow, brain function, casting and rolling steel, automobile crash testing, mantle convection, ocean circulation, weather and climate prediction, turbulence, astrophysical jets, black holes and large-scale cosmological structure.
Like the rest of the scientific American Library series, the book is nominally addressed to the adult reader.
Notwithstanding, it is an ideal volume for a bright 12–14-year-old, especially one who is (or shows signs of being) addicted to computers.
Scientists and science educators might do well to worry about the attractiveness of computers and the sense of power they unlock for brilliant young people today.
We in the ‘traditional’ sciences are perhaps losing too many of the best young minds to interests and careers in computer science and other information technologies.
Most of us can name a few particular books, read in the pre-college years, that influenced our choice of science as a life's work.
The best of these often give a sense of both the science and the doing of science.
The younger reader is thus able to get some kind of tentative vision of what his or her own life in science might be.
A senior generation of scientists was influenced by  's Microbe Hunters (1926) or  's The Universe Around Us (1929).
I recall with pleasure  's The World of Science (1958), along with the delightful books of .
In recent years, the books of  have inspired many.
and  's book could (one might say, should) be similarly influential to the present generation of youthful computer enthusiasts.
Like the books mentioned above, its specific content will be superseded in a few years; but in meantime the book may be able to rescue some future Nobel laureates from becoming otherwise lost in cyberspace.
That is one point of view; another is that these future Nobel laureates may in fact earn their prizes by bringing more supercomputing to science, and thus moving more of science into cyberspace.
Good posture The traditional (below) and more recently proposed (right) stance of carnosaurs, here Tyrannosaurs rex , both based on the posture of birds.
There is still, however, no consensus regarding the stance of these dinosaurs.
The drawings are taken from The Dinosauria edited by , and , a comprehensive compendium of information on dinosaur anatomy, taxonomy, diversity and distribution.
For a review, see Nature 348, 686; 1990.
University of California Press, $47.50, £27.50 (pbk).
Fads and friction
Jacqueline Reynolds
The Culture of Science in France, 1700–1900.
By Robert Fox.
Variorum : 1992.
No pagination. 47.50, $89.95.
SOCIETY IN FRANCE during the eighteenth and nineteenth centuries was characterized by enormous upheavals, leaps forward and painful retreats.
History records the end of the reign of the ‘Sun King’ Louis XIV, the ineffectual rule of his successor, the brutal singularity of the Revolution, the rise of Napoleon, the restoration of the monarchy, the formation of the Third Republic.
And all the whole, French scientists continued to do just what they had always done — make new discoveries and expand our understanding of the natural world.
This was the period of great excitement at the Jardin du Roi with , and , and of that famous duo in Arcueil, and .
set the foundation for modern chemistry: published his monumental treatise on electricity and magnetism; Carnot established the second law of thermodynamics; demonstrated the wave nature of light.
What a record of achievement!
in his fascination with ‘the diversity of the contexts in which the French pursued science’ has put together 12 of his own papers (all published elsewhere between 1968 and 1985) that describe the changing nature of government influence, scientific institutions and the approach to scientific discovery between 1700 and 1900.
There is inevitably a problem with continuity as well as some mildly annoying repetition, but such is the nature of collected studies written so far apart in time.
The first half of the book centres on society and institutions.
addresses the attitudes of successive governments towards scientific endeavour, ranging from active encouragement during the Napoleonic days to indifference in the post-Restoration era.
We are shown how scientists themselves evolved through these periods both in their approach to research and in their personal attitudes towards fame and fortune.
These individual differences, rather than government interventions (or the lack thereof), were in the author's view the real causes of the changing face of French science.
The rise of numerous sociétés savants is one of the most interesting aspects of this period, with their accommodation of not just professional scientists but anyone who could be induced to participate and part with the fee charged for annual membership.
Inevitably, these sociétés became ever more specialized and less accessible intellectually to the layman, so that by the end of the nineteenth century the universal sauvant had effectively disappeared.
Most of  's societal framework refers to the provinces and not the better known institutions centralized in Paris, an approach that makes a refreshing change.
In fact, one of the most fascinating chapters deals with the history of Mulhouse and the meeting between science and industry in the region of shifting political allegiance far from the centralist powers in Paris.
But all is not politics and institutions.
As  said, ‘The history of science is science itself’ and, appropriately, Fox uses the history of the study of heat in eighteenth and nineteenth century France to demonstrate the changing aspirations and preoccupations of the participating scientists.
there is  's ‘particles of fire’ theory of heat set in the context of the arguments that raged at the time: that is, heat as a fluid (caloric) versus heat as motion (kinetic).
and  appear with their study of specific heats, the results of which dealt a severe blow to the Laplacian school that supported the caloric theory.
The two grand old men of Arcueil, and , and the entire cult of French physics that surrounded them are entertainingly dissected through the period when French physics moved from uncompromising support of newtonianism to its near rejection with the wave theory of light.
(yes, there were fashions in science even then.)
And, finally,, and  appear on the road to the second law of thermodynamics.
Here, endears himself to the working scientist who tires of the seemingly endless efforts by historians to dig up some obscure individual who ‘did it first’.
In his words, ‘I believe that, in  's case, it is possible to place too much emphasis on the identification of traditions and precursors.
As a result of recent research, we can now trace possible sources for most of the individual elements in his theory, but the originality of the way in which he synthesised these elements remains his unique achievement’.
writes for an audience far wider than the historian of science, and although these essays may present some challenge for the uninitiated, today's working scientist (looking beyond the arguments about supercolliders or the effects of retroviruses on the human population) could well profit from a dip into this book.
Misinformation
Hermann Haken
Information Theory and Molecular Biology.
By Hubert P. Yockey.
Cambridge University Press: 1992.
Pp. 408. £55, $69.95.
INFORMATION theory and molecular biology are both large and important fields.
In this book, tries to link the two.
Quite often the word ‘information’ is used with different meanings, but from the very beginning he sticks to a single interpretation — Shannon information.
This information does not carry meaning; rather it is a measure of the scarcity of an event or message.
Yockey's starting point is laudable — but how far does it get him?
He begins with a sound presentation of basic principles of probability theory and then discusses the concept of entropy, including that of genetic sequences.
There follows a short chapter of the principle of maximum entropy, which regrettably lacks a discussion of the crucial role of constraints.
An outline of codes and coding theory provides the main basis for the author's later conclusions.
In the chapter on source, transmission and reception of information, he emphasizes the analogies between information transfer and the genetic code.
It is in the second part of the book that Yockey extends this theory to problems in molecular biology.
He begins by presenting the information content of several protein families and goes on to deal with a variety of subjects, such as the evolution of the genetic code, neurobiology, the primaeval soup, black holes and cosmology.
In several places the author states that although molecular biology must be consistent with chemical and physical processes, biological principles cannot be derived from physics and chemistry alone.
This statement holds still more for the relationship between mathematics and molecular biology (or any other branch of science).
But Yockey repeatedly conveys the impression that the laws of molecular biology can be derived from mathematics.
In addition, information theory by itself does not contain any dynamics: in order to understand the genetic code, the study of the underlying dynamics of its self-organization is essential.
So the author's polemic against the seminal work of , who clearly recognized and formulated the fundamental role of these dynamics, is not only unfounded but misleading.
Undoubtedly, information theory has its uses in molecular biology, in particular by giving insights into the way genes code for proteins.
But instead of sticking to this problem, which is interesting in itself, Yockey makes wild extrapolations in a futile attempt to show how the classical concept of information can be applied to problems in generating information.
He would have been wiser to have published only the first part of this book.
Elusive concept
Michael E. Akam
Evolutionary Developmental Biology.
By Brian K. Hall, Chapman and Hall; 1992, Pp. 275 –29.95, £47.50
WHEN walking across the upland fells of Britain, it is not uncommon to find the whitened jawbone of a long-dead sheep, teeth rattling in its sockets.
Food for thought on a wet day for both the developmental and evolutionary biologist.
What mechanisms built the jaw's distinctive pattern, each tooth unique, the bone an asymmetric array of lumps and bumps?
What mix of selective forces and developmental constraints moulded the mandible, honed the teeth?
Here,par excellence , is the meeting of the ways for developmental and evolutionary biology.
The mandible is a favourite subject for Hall.
His discussion of the forces that shape it is one of the best parts of his book.
He approaches his theme — the interplay of developmental and evolutionary biology — first as historian, then reviewer and finally, in the context of the mammalian mandible, as modeller.
As an annotated bibliography, the book is superb.
There are nearly a thousand references, spanning the literature from palaeontology to molecular biology, much of it recent, but with good coverage of the eighteen and nineteenth centuries (12 references for !) and, perhaps most valuable, a survey of the extensive but scattered literature of the twentieth century.
Here you will find pointers to classic studies, still pertinent, but not accessible from compact disc databases or minireviews: on developmental canalization, on cyclomorphosis, and  on kidney development.
But the book aims to be more.
It aims to re-examine key concepts — hierarchy, epigenesis, archetype, homology.
As a development geneticist, I hoped for a personal synthesis of the subject viewed from a different perspective.
For the first 160 pages I was disappointed.
's views, 's definitions, 's ideas, yes; but only glimpses of a personal perspective.
These chapters are not easy to read.
Sentences running to 70 or so words don't help.
The ideas shine out most clearly from the frequent and well chosen quotations:—‘Homologue…
The same organ in different animals under every variety of form and function.
Analogue…
A part or organ in one animal that has the same function as another part or organ in a different animal’(1843);(on late nineteenth century biology)—‘Morphology was studied because it was the material believed to be the most favourable for elucidation of the problems of evolution, and we all thought that in embryology the quintessence of morphological truth was most palpably presented’(1922);(on developmental constraint)—‘But if organisation and the laws of development exclude some lines of variation and favour others, there is certainly nothing supernatural in this, and nothing which is incompatible with natural selection’(1919).
Pithy stuff.
But elsewhere, if the original author is not quoted, the ideas often fail to jump off the page.
It is too easy to criticize a book of this scope for errors of detail.
But in places, fails to do justice to the very logic of the subject.
Under the heading ‘Key innovations as single gene mutations’ we find a discussion of torsion and gastropods.
showed in 1923 that the handedness of coiling in snails is controlled by a single pair of genes.
Hall seems to endorse a proposal made some years ago by  that, on the basis of this simple genetic control, ‘Torsion, and with it the class Gastropoda, arose through a single gene mutation.’
It is a profound error to equate a mutation that changes the symmetry of torsion with a mutation that invents a mechanism of torsion itself.
This is like saying that all the genetic differences between male and female, oogenesis and spermatogenesis, are specified by a single gene on the Y chromosome.
This may be true at the population genetic level, but it tells us absolutely nothing about the developmental complexity of the process itself.
This error is common enough in the literature, but it is disturbing here to find even a hint of support for such thinking.
The book becomes gripping once  allows his own interests and opinions to come closer to the surface.
Does the idea of homology apply only to morphological patterns, he asks, or can it be applied to developmental processes?
He follows the topic from  and  to .
For once, we are  left in no doubt where the author's sympathies lie, and why —‘Homology is a statement about pattern, and should not be conflated with a concept about processes and mechanisms’.
Before I read the book, I thought this was exactly the opposite of the truth.
Now I am not so sure.
I appreciate more clearly that, to quote another of  's quotes, ‘Among evolutionary biologists, homology has a firm reputation as an elusive concept’(, 1989).
For anyone with an interest in the subject, 's book is a valuable guide to who thought what, and where to find it.
But don't use it as a primer.
Time warp
Oliver Penrose
Time's Arrow: The Origins of Thermodynamic Behaviour.
by Michael C. Mackey.
Springer: 1992.
Pp. 175.
DM98, £40, $49.
THE second law of thermodynamics always arouses interest because it is the only widely applicable law of physics that is not symmetric under time reversal, and so singles out a particular direction of time.
The law states that if a physical system is thermally isolated (that is, if no heat enters or leaves the system), the thermodynamic quantity known as entropy must increase or stay constant.
The law therefore singles out the time direction of irreversible physical processes as the direction of increasing entropy.
For example, imagine two cylinders of gas joined by a narrow pipe containing a valve that is at first closed, then open for a short time, then closed again.
Somebody measures how the gas is distributed between the cylinders before the valve opens and after it closes, writes the results on two cards, and asks you to tell which card describes the earlier state and which the later.
One way of solving the puzzle would be to use the second law.
It is not difficult to show that (other things being equal) the more evenly the gas is distributed between the two containers, the greater the entropy; so the state with the gas more evenly distributed between the two cylinders is the one that occurred later.
When the second law was originally put forward by the founders of thermodynamics, it must have seemed completely mysterious.
Unlike the other concepts of thermodynamics such as energy, temperature, pressure and so on, entropy had no counterpart in mechanics or everyday experience.
But in 1872  discovered one of the jewels of nineteenth-century physics, a precise quantitative relation giving the entropy of a gas in terms of mechanical quantities, namely the distribution of positions and velocities of its molecules.
He showed, in his famous ‘H theorem’, that if the gas is not in equilibrium then collisions will increase the entropy until finally a state of equilibrium is reached.
Around the turn of the century, discovered another important formula connecting entropy with mechanical quantities.
Unlike Boltzmann's formula for the entropy of a gas, that of Gibbs applies to liquids and solids as well; but it pays for this greater generality by being applicable unambiguously only to systems that are in equilibrium.
In real life — or even in the physics laboratory — we frequently encounter objects that are neither gases nor in equilibrium.
One would like to have a ‘time's arrow’ covering this more general case too — that is, a formula for the entropy of a non-gaseous non-equilibrium system that can be shown to have the property of non-decrease in time if the system is thermally isolated.
Neither of the formulas so far mentioned will do; the Gibbs formula gives an entropy that for an isolated system of molecules is easily shown to stay constant in time rather than to increase, and the Boltzmann formula also does not have the desired non-decrease property if the system is not a gas.
The title of this book leads one to expect a contribution to the solution of this problem, but unfortunately these hopes are soon dashed.
The discussion is based on what the author calls the ‘Boltzmann-Gibbs’ entropy, but in the terminology used above it is just the Gibbs entropy.
His claim in Chapter 1 to provide that this is essentially the only possible definition is belied by the existence of Boltzmann's definition, which is certainly different.
The author notes that the entropy, as he has defined it, is constant for systems whose time evolution is governed by differential equations such as the laws of newtonian mechanics.
Because he wants the entropy to increase with time, the obvious conclusion would be that the wrong definition of entropy has been chosen and that it should be changed, but he decides instead to change the law of time evolution.
The change he makes is to assume that the laws of motion obeyed by the particles constituting an isolated system are non-invertible: that is, that the same dynamical state at a given time can be reached from two or more different dynamical states at some earlier time.
Before the generally accepted laws of motion for such a system of particles do not have this property, his assumption is a departure from what we know about physical reality.
The rest of the book is an exploration of the consequences of this unphysical assumption.
The hypothetical world where this assumption holds contains, indeed, many interesting things, and the author guides us around it skilfully.
Unlike some mathematical authors, he does not write as though his prime objective is to show how clever he is; he provides plenty of examples to illustrate his theorems and definitions, and clear summaries at the end of each chapter.
All the book needs to make it worthy of recommendation as an introduction to this enjoyable branch of mathematics is a health warning like the one on the tobacco advertisements: ‘Reading this book can damage your understanding of physical reality’.
New in paperback
Understanding the Present: Science and the Soul of Modern Man by Bryan Appleyard.
This book was the cause of considerable controversy when first published in the United Kingdom last year.
The author's central argument is that ‘Science is not a neutral or innocent commodity…a convenience.
Rather, it is spiritually corrosive, burning away ancient authorities and traditions’.
Nature 's commentators were not convinced — for reviews see Nature 356 , 729 and 357 , 29; 1992.
Picador, £6.99.
(US hardback edition recently published by Doubleday, $23.50.)
Vital Circuits: On Pumps, Pipes and the Workings of Circulatory Systems by .
In a review in Nature 358 , 720 (1992), wrote that ‘[Vogel]leads us through the functions and regulation of the heart and circulation with extraordinary clarity and lightness of touch.
It is difficult to find fault with this eccentric and lively account’.
Oxford University Press, $12.95.
Spring Books
Next week's issue (15 April) contains Nature 's Spring Books supplement.
Among featured reviewers will be  on the psychology of interrogations, confessions and testimony; on the works of  and ; on polar mythology; on Vincent van Gogh, chemicals and creativity; and  on chimpanzees and humans.