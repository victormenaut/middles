

Tap Water: Nitrates, Lead and Aluminium
While seas and rivers may be dirty, British tapwater is supposed to be pure, and its pollution causes more concern than any other.
In Stanley Kubrick's film Dr Strangelove , US Air Force General Jack Ripper sends his B52s to bomb the USSR because he fears the Russians have polluted the drinking water of the West.
‘As human beings,’ he tells his RAF liaison officer Group Captain Lionel Mandrake, ‘you and I need fresh pure water to replace our precious bodily fluids.’
While marines machine gun his East Anglian air base, the deranged commander observes that, ‘You never see a Commie drink a glass of water — and with good reason.’
The fictional General Ripper was obsessed with fluoridation.
A crank in the 1960s, his worries over the wholesomeness of water would put him in much better company today.
Today it is not so much fluoridation as nitrates and pesticides which generate international conflict over water quality.
In October 1989, after four years of dispute, the European Commission finally began its legal attack on the UK for allowing excess nitrate in the water supply of Norwich and Redbridge, and for illegal levels of bacteria and aluminium elsewhere.
As Geoffrey Lean and Fred Pearce wrote in a survey reported in the Observer in August 1989,
‘Millions of Britons drink water contaminated above international safety limits with poisons.
Well over two million people are at risk from excessive concentrations of lead, which damages children's brains.
Two million people drink water containing more than the European Commission's Maximum Admissible Concentration (MAC) of aluminium, which is increasingly believed to cause Alzheimer's disease (incurable dementia).
At least 1.7 million people consume water that breaks European limits for nitrate, suspected by some scientists as a cause of cancer…
‘Friends of the Earth identified 298 water supplies exceeding MACs for dangerous pesticides.
A tenth of Britain's groundwater supplies are thought to contain cancer-causing solvents above limits drawn up by the World Health Organisation.
And nobody even knows the full extent of contamination by trihalomethanes — other suspected carcinogens — except that it is widespread.
Nitrates
In the sandy soils under General Ripper's airbase, as in aquifers throughout  the agricultural Europe, the ‘time bomb’ of nitrate pollution has been ticking away since the 1950s.
Nitrates take a decade or more to reach the level of boreholes.
The source of nitrate pollution is now undisputed: changes in agricultural practices, including the ploughing up of permanent pasture but, most importantly, the enormous growth in the use of artificial fertilisers.
Pasture, especially natural or long established pasture, acts like a biological sponge, holding large amounts of nitrate.
When pasture is ploughed up or overloaded by nitrogen input, organic nitrogen is oxidised and the highly soluble nitrate ion is washed quickly through the soil and into rivers or groundwater.
When arable land replaced pasture over much of Britain between the 1950s and 1980s, there was little in the soil ecosystem to hold nitrate back (only rarely is it all used by the crops).
So nitrate from the enormous growth in artificial fertiliser applications is rapidly leached away.
British farmers now put 1.6 million tonnes of nitrate fertiliser on the land each year, an eightfold increase since the 1940s.
The countries of the European Community used 1.7 million tonnes of fertiliser in 1953 but this grew to 8.9 million tonnes by 1982.
As a result, significant nitrate problems now exist in almost every European country.
Six were surveyed by David Baldock of the Institute for European Environmental Policy (IEEP) in 1989 (see table).
Nitrate in Drinking Water in Six Countries
1.
Denmark
The most affected areas are rural pans of areas in western Jutland. 6 per cent of the population (300,000 people) receive water with a nitrate content above 50 mg NO&sub3; /l, mostly from private wells.
2.
Federal Republic of Germany
Worst affected are areas of Schleswig-Holstein, the lower Rhine, western Westphalia, Lower Bavaria, and smaller areas under horticulture and vines, such as the Upper Rhine, the Palatinate, parts of Unster-Mittelfranken and Wurttemberg.
Approximately 5 per cent of all those consumers on public supplies receive water with nitrate above 50 mg/litre.
About 25 per cent of the population receive drinking water with more than 25 mg NO&sub3; /litre.
In addition, there are some private supplies affected.
As in France and the UK, the number of consumers affected by water supplies with high nitrate levels is falling, but not the number of contaminated sources which is still rising in some regions.
3.
Spain
Most affected are Valencia, Murcia, Catalonia and the Mediterranean coast.
An estimated 1 per cent of the population receives water above the limit (approximately 350,000 people).
This number is increasing, due to fertiliser pollution.
4.
France
Most affected areas: the regions of Brittany, Centre, Poitou — Charente, Champagne-Ardenne, Bourgogne, Picardie, Ile-de-France and Nord-Pas-de-Calais.
Some 860,000 people receive water with an average nitrate concentration above 50  mg/litre.
Approximately 1,725,000 people receive water with a maximum concentration of above 50 mg/litre.
5.
Netherlands
Worst affected areas are sandy soils in the eastern, central and southern parts of the country where intensive livestock units are concentrated.
Over 99 per cent of the population is connected to the public water supply receiving water with less than 50 mg/litre nitrate.
An estimated 100–200,000 use private wells, many with high nitrate levels.
6.
United Kingdom
The most affected regions are in East Anglia, Lincolnshire, the East Midlands and West Midlands, especially in arable areas.
Some 850,000 people are supplied with water with a nitrate concentration above 50 mg/litre.
Nitrate concentrations are still rising in many groundwater sources.
Source : responses to IEEP questionnaire to national ministries in early 1989, preparatory to a meeting at Toulouse, April 1989, supplemented by other data.
Nitrogen and Health
Although the existence of nitrate pollution and its health implications had been known for decades, in the UK it was not thought to be much of a problem until 1984.
High levels of nitrate can cause ‘blue baby syndrome’(infantile methaemoglobinaemia), which is potentially fatal (see below: Government Advice), but there had only been 3,000 cases worldwide since 1945, and 14 in the UK since 1950.
Nigel Haigh of the IEEP points out that in 1979 the Royal Commission on Environmental Pollution reported that evidence for the other major health worry — the potential for nitrite, which is formed from nitrate in certain conditions, to create ‘nitrosamines’ and cause stomach cancer — was only ‘weak and equivocal’.
In 1970, the World Health Organisation (WHO) took the health threat seriously enough to recommend a European standard for drinking water of 50–100 mg/litre.
US authorities adopted a 45–50 mg/litre standard as a precautionary measure.
From 1973 the Health Protection and Environment Directorates of the European Commission had also been working on measures to control wastes and ‘trace’ substances in water: they adopted a 50mg/litre nitrate limit for the 1980 Directive Relating to the Quality of Water Intended for Human Consumption (80/778/EEC), which also controls pesticides and metals.
Countries had to meet the standards, unless ‘derogations’ were allowed, by 17 July 1985.
They were not met by the UK, which also granted itself invalid derogations, and the country ended up in the European Court (see below).
Despite the reassuring noises of the Royal Commission in 1979, by 1984 the UK's Standing Technical Advisory Committee on Water Quality (STACWQ) reported that, if present trends of fertiliser use were to continue, mean nitrogen levels in surface waters would increase by 2–3 mg/litre (as nitrogen) by 1994–2004 and this would cause widespread breaches of the 11.3 mg/litre nitrogen level (50 mg/litre nitrate).
Supplies to millions of people would be  affected.
The Department of the Environment (DoE) had foreseen difficulties with the Directive, and now asked the Department of Health for advice on nitrate levels, most probably so that the UK could argue for laxer standards.
Unlike the US agencies and the EC, the DHSS/DoE Joint Committee on Medical Aspects of Water Quality proposed in 1984 that it would be acceptable to relax the EC standard by 60 per cent, from 50 to 80 mg/litre.
The Department of Health issued a ‘guidance note’ to Chief Environmental Health Officers in 1985 which, in the words of David Wheeler of Surrey University's Robens Institute, ‘dismissed the possibility of a link between nitrates in water and stomach cancer in the UK’.
This policy may have seemed ‘reasonable’ to the civil service, but it had two major flaws.
First, it was illegal.
The Commission took a far stricter view of what the Drinking Water Directive meant than what the UK felt it ought to mean.
Second, raising the limit to 80 mg/litre was an administrative judgement not a medical one.
In 1988, the government's Chief Medical Officer, Sir Donald Acheson, told the DoE that the level should remain at 50 mg/litre, mainly because, if a cancer risk existed, it would be a result of long-term exposure.
He was apparently repeating old advice.
The Government Advice on Nitrates and Health
When the DoE asked the Chief Medical Officer and his Committee on Medical Aspects of the Contamination of Air, Soil and Water for an opinion of the health risks of nitrate, the chief points were:
The concentration of nitrate in public water supply should not exceed 100 mg/litre;
Water undertakings (now the companies and plcs) should ‘nevertheless continue to make every effort to keep the concentrations below 50 mg/litre’;
Monitoring for infantile methaemoglobinaemia should be carried out where the values are 50–100 mg/litre and babies should get bottled water at or above 100 mg/litre.
The UK government has since tried to use Acheson's opinion to argue for an upwards revision of the 50 mg/litre limit.
However, the Medical Officer wrote: ‘A preferred standard of 50 mg/litre is the result of a long established consensus amongst scientists in the United Kingdom and internationally,’ although ‘transient excursions’above that level would have no importance ‘in relation to the postulated role of nitrate in causation of cancer, since in this context the long-term average content is the significant figure.
‘I must emphasise,’ said Acheson, ‘that the present scientific evidence on the ill effects of nitrate on human health does not point unarguably to a precise standard.
The consequences of permitting the distribution of water containing average nitrate concentrations in the range 50–100 mg/litre cannot be placed beyond doubt.
Indeed, even the current standard for nitrate in water does not  totally guarantee the absence of ill effects.
One can only say that, with any increases in exposure, the likelihood of ill effects occurring and the extent of ill effects which might occur would be expected to increase…
‘I must emphasise that the present nitrate standard of 50mg/litre remains, in our view, the appropriate preferred standard.
Nothing of substance has emerged recently on which to question it.’
Acheson added: ‘I would accept [using]the average concentration [rather than a maximum]in relation to methaemoglobinaemia,’ which goes less than half way to the justification that the DoE was seeking for a relaxation of the 50 mg/litre standard.
‘I could not agree,’ he said, ‘to any interpretation of this advice that meant accepting avoidable delay in taking appropriate action in the most affected areas.’
Cancer
Acheson's advice says:’ A variable proportion of ingested nitrate is converted to nitrite in the body.
Nitrite can then react, mainly in the stomach, with a wide variety of compounds which are ubiquitous components of the diet, to form N-nitroso compounds.
The evidence that such a reaction does actually occur in humans, most markedly in individuals with low stomach acidity, has strengthened in the past few years.
Some dietary components, including vitamin C, tend to inhibit the reaction.’
‘If nitrate does, indirectly, cause cancer, it will be the long-term average that matters and the effect of any increase in cancer rates would be expected to take many years to become apparent…
The epidemiological evidence shows that nitrate cannot be having a major effect on cancer in the general population in the UK.
Nevertheless, the inherent limitations of such evidence mean that we cannot absolutely exclude a small risk.’
He adds: ‘It should be noted that much of the evidence deals only with cancer of the stomach, because this was the subject of the original claims.
Extrapolation from animal studies predicts that nitrosamines, and hence nitrates, would be as likely to cause other types of cancer in humans.
‘There is no proof,’ said the medical advice, ‘that nitrate leads to human cancer but, nevertheless, experts in the UK consider that the theoretical likelihood of an effect is sufficient for them to advise government to seek ways of restricting the total intake of nitrate.’
They observed that for someone with an average British diet, although preserved meat and vegetables contain nitrates, it is water, once the level of nitrate reaches 50 mg/litre, that will be the main source.
Since vegetables are an important source of vitamin C, the medical experts favour controlling nitrate intake via water rather than food.
(The average daily intake of nitrate is about 60 mg a day, 75 per cent attributable to vegetables.
Some people take in 110–120 mg, and water contributes 10–80 mg depending on pollution.)
Infantile Methaemoglobinaemia
The government's advisers say that this is a rare condition ‘in which the ability of the blood to carry oxygen is decreased, characteristically causing a grey/blue tint to the skin’.
Drugs aside, nitrate over 100 mg/litre is the principal  cause.
The last UK case was in 1972.
Babies in high nitrate areas are given bottled water.
The EC Drinking Water Directive
This Directive has been a major source of conflict between the UK and the European Commission.
It sets a guide value (maximum) for nitrate of 50 mg/litre but permits member states to apply for permission to grant ‘derogations’(exemptions) where the levels arise from geological factors (’ the nature and structure of the ground’), or ‘exceptional meteorological conditions’.
The date by which all supplies should have met the Directive's standard was July 1985, but little was done in the UK as the government hoped to negotiate a laxer standard.
Instead, in November 1985, with the backing of the farming lobby, which feared that controls or taxes would be imposed on nitrogen fertiliser, the DoE granted water suppliers no fewer than 57 derogations, 48 of them for contamination by nitrates.
At the same time the government took it upon itself to interpret ‘Maximum Admissible Concentration’(MAC) as meaning an average over three months and not an absolute limit.
Specifically, the DoE ‘granted’ derogations for supplies where nitrate exceeded 50 mg/litre but did not exceed a three monthly average of 80 mg/litre and a maximum of 100 mg/litre ‘except in exceptional and transitory circumstances’.
The DoE's claimed justification for this last step was medical advice from the DHSS, although, as we have seen, this interpretation owes at least as much to administrative convenience as to scientific advice.
The nitrate derogations affected 2 per cent of the population.
Aware that removing nitrate from water supplies could be a very expensive business, that curbs on the use of nitrogen would probably enrage the agricultural lobby, and with privatisation on the horizon, the issue became progressively more important for the DoE.
As the STACWQ had made clear in 1984, the nitrate problem would go on getting worse for decades into the future.
This was hardly a welcome dowry for the investors that the government wanted to attract into a newly privatised industry.
Everything the government did from this date onwards had privatisation in mind.
In fact, as early as 1.983 ENDS magazine had warned that the government was legally wrong about the Directive.
Friends of the Earth and other environmental organisations now challenged the government's interpretation of the Directive on two grounds.
First, they argued that a Maximum Admissible Concentration should be treated as an absolute limit not to be breached, and not the basis of an average.
Second, FoE pointed out that Article 9 allowing derogations for nitrate levels arising from the ‘nature and structure’ of the ground was intended to cover exemptions for water from nitrate-bearing rocks and soils, whereas most of the nitrate found in UK groundwater stems from highly unnatural levels of fertiliser.
In 1985, the government set up a Nitrate Coordination Group (NCG) including DoE, the Ministry of Agriculture, Fisheries and food (MAFF), the Fertiliser Manufacturers' Association and the National Farmers' Union, but  excluding consumers, environment or health groups.
After various delays it produced a report,Nitrate in Water , in December 1986.
This showed that 1 million East Anglian and Midlands consumers regularly drank water above the 50 mg/litre standard, and another 3.8 million drank water almost as polluted during 1984/5.
‘However,’ says David Baldock of the Institute for European Environment Policy, which has advised several governments on the issue, ‘in twenty years’time five million people could be drinking water well over the 50 mg/litre limit.
In the long term seven people in ten will be drinking water with more nitrate than the European limit in the Anglian region and half those in, Severn Trent.’
Nitrate concentrations in groundwater, noted the report, are rising steadily and will reach equilibrium at over 100mg/litre.
The NCG dismissed the use of slow release fertilisers, taxes or quotas, favouring education for farmers and avoiding ‘unacceptable economic consequences’ for the agricultural industry.
lo ensure that everyone's nitrate levels stayed under 50 mg/litre the NCG estimated that immediate capital spending of £50 million would be needed, and £150 million would be required within 20 years.
On 11 August 1987 the European Commission began its formal moves against the UK for unlawful levels of nitrate in drinking water in Tower Hamlets, in Bassetlaw District, Scotland, and at five supplies in Norfolk.
Several months later, on 2 December, the government conceded that an average was not a maximum after all.
The Secretary of State for the Environment told the House of Commons that ‘after taking legal advice’ he had decided that ‘the term ‘maximum admissible concentration’in the European Community Drinking Water Directive should relate to individual samples and not to averages over a period’.
In its reply to the Commission on 11 December, the UK said it would now withdraw the derogations (which happened on 10 April 1988).
The withdrawal of derogations was disclosed to Parliament by Junior Minister Colin Moynihan on 20 January the following year.
A major row now blew up between the Ministry of Agriculture and the Department of the Environment.
ENDS magazine wrote in January 1988, ‘The government's retreat in its interpretation of the 1980 EEC Directive on drinking water quality in the face of legal challenges by the European Commission turned into a rout this month when it withdrew permission for 48 water supplies to exceed the EEC limit.
But a decision whether nitrate contamination of water is to be reduced by curbs on land use or by treatment with untried technology is being held up by a conflict in Whitehall on whether farmers should be compensated for income losses resulting from land use controls’.
The climbdown over averages was communicated to all chief executives of Water Authorities and Companies on 10 March and the derogations were formally rescinded in April.
By now, almost eight years had elapsed since the birth of the Directive, and three years had gone by since the nitrate limits were supposed to have been complied with.
The Environment Commissioner Carlo Ripa di Meana later recalled:
‘the UK…drew attention to the practical steps being taken to deal with
the nitrate problem, such as the taking out of service of several sources, the development of experimental denitrification processes and advice and publicity to farmers…
The UK nevertheless drew attention to the fact that denitrification processes are in an early stage of development and that further research is necessary and that the protection of water sources may take years to have the desired effect…’
This set the new tone of the UK's argument: everything possible was being done and therefore any breaches of European law should be disregarded while ‘things were brought up to scratch’.
Many people found this a strange attitude from a government committed to law and order.
Muggers who decided to phase out mugging by 1993 could hardly expect to be let off, yet the UK expected to go on breaking the law with impunity.
The Commission noted that another letter from the UK dated 5 January 1989 had tried to make the case that ‘all supplies which presently exceed the nitrate admissible concentration will be brought into progressive compliance between now and 1995, all but two supplies complying by 1993’.
But a promise of jam tomorrow wasn't enough to satisfy the Commission, which issued a Reasoned Opinion against the UK's implementation of the Drinking Water Directive in April.
The UK's measures, said the Commission, were ‘not being achieved with sufficient rapidity’.
Except where a delay or derogation was allowed under the Directive — and the UK had now also accepted that its grounds for derogation had all along been invalid —‘all supplies of drinking water in the United Kingdom were required to be in compliance with the Directive by 20 July’.
‘In view of the long-standing nature of this infringement,’ the Commission concluded that it could not ‘accept that any further delay in remedying the infringement should be permitted.’
By now the British government was plunged headlong into preparations for selling off the water industry.
Urgent talks began in London and Brussels as the UK tried yet again to negotiate a way round the EC rules.
As the autumn flotation of the water industry neared, each additional future commitment to spend money on water treatment became a political liability.
Shareholders, after all, were interested in a short term return on their money.
In June, the Observer Business pages warned that the Commission was ‘poised’ to prosecute Britain in the European Court early in 1990 and this could ‘effectively wreck the Government's controversial water privatisation plans’.
Water Minister Michael Howard was to meet Commissioner Carlo Ripa di Meana ‘to extend the framework by six years’, while the government was preparing (successfully) to throw out a Lords amendment to the Water Bill which required compliance with EC rules by 1993.
In July,The Times reported that in Brussels talks had again broken down.
Signor Ripa di Meana said, ‘If Britain cannot meet our demands in two months, it shows a lack of goodwill, and we will definitely go to the courts.’
At home the UK government launched a desperate news management campaign to ‘frame the debate’ along its own lines.
An essential component of any pollution argument is backing from ‘independent scientific’ opinion to  undermine the views of opponents.
On 20 July, the Daily Telegraph published an article entitled ‘Is our water really worse than theirs?’ by Con Coughlin, which purported to explain the ‘science and politics behind the EEC's verdict’.
If this was not a government-inspired and briefed story, it must have warmed the cockles of the Ministry's heart.
‘Nitrate,’ it informed readers, ‘as most gardeners know, is produced whenever there is intensive cultivation of the soil.
Thus in agricultural areas high levels of nitrate are produced which gradually make their way into the water table and ultimately into our water supply.’
Did fertilisers play no role?
Apparently not.
‘Such worthy bodies as the World Health Organisation,’ the article sneered or just condescended, depending how you read it, had inspired the European Commissioners to set nitrate limits, because of blue baby syndrome and a fear of stomach cancer.
The first factor, the newspaper explained, was not important as no cases had occurred since 1972, while ‘the claim that high levels of nitrate cause cancer has yet to be proved.’
Having dismissed the basis of the EC Directive, the article now went on, as the government itself would like to imply, that, while others had muddled about wrongly trying to control nitrates, Britain had been engaged on work of an altogether higher order, ‘in the interests of good water’.
This, it turned out, was the elimination of bacteria.
‘When it comes to the purity of our water, we have virtually no traces of bacteria in the finished product, which is more than can be said for some other Common Market countries,’ Dr Derek Miller, assistant director of the soon-to-be-privatised Water Research Centre told the newspaper.
The bacterial breaches of the Directive (see below) and trihalomethanes produced by chlorinating sewage contaminated supplies were not mentioned.
And then, the Daily Telegraph discovered ‘the truth’: ‘privatisation of Britain's water industry…runs against the grain of Mr Delors’ social strategy.’
Here was a socialist plot that would have launched General Ripper into a pre-emptive strike against Brussels.
Almost apologetically, Mr Coughlin conceded that ‘Britain also has been less diligent than some of its neighbours in setting out a timetable for reducing nitrate in water’.
Elsewhere in the Daily Telegraph that day, Mr Michael Howard, Water Minister, was in a bullish mood.
‘Minister Warns EEC Not To Prosecute on Water’, declared the headline.
‘Mr Howard: We will hear no more of the matter’, ran the caption under his photograph.
Stout stuff.
So perhaps Britain was about to send a gunboat to shell the Perrier works after all?
Employing one of those supremely disingenuous somersaults of logic that only long training in double-speak and the official brand of British arrogance can confer, Mr Howard told a Westminster audience of backbenchers that ‘If the Commission were to take us to the European Court I can think of few things more calculated to bring the Commission into disrepute’
While other European countries (not to mention the United States) were struggling to comply with a 50 mg/litre nitrate limit, the minister revealed that Britain had once again discovered unique drawbacks to the proposed solution to pollution, which made the cure, as it were, worse than the disease.
Denitrification could create a risk to health.
‘I'm very confident that when the Commission learns at first hand that it is not technically possible, they will see  the overwhelming good sense of the British government and we will hear no more of the matter,’ declared Mr Howard.
So it was not after all, the pressure from the farmers, or the cost, or the private discussions with the fertiliser manufacturers which had motivated the UK to drag its feet over nitrates, but a uniquely shrewd insight into the better interests of public health.
On 24 July it was the turn of The Times .
It reported that the difference of view between the government and other countries had now led the Royal Commission to announce an inquiry into the evidence of the health effects of nitrate in groundwater.
The Commission's two-year inquiry, said The Times , ‘may produce new evidence exonerating the water supply’.
Quoting Lord Lewis of Cambridge University as being ‘sceptical of claims made by Mr Di Meana about nitrate concentrations in British water’, the newspaper ventured the opinion that ‘Backing for Lord Lewis may emerge today when the Royal Society is due to hold an open seminar on scientific evidence about the purity of the water supply.
The Royal Society note on the seminar indicates that ‘less fashionable’ chemicals than nitrates might be worth attention.’
While the Royal Commission has never been accused of acting as an arm of government, its inquiries are not conducted with the same open-eyed approach to political manipulation as are those of the Select Committees of the House of Lords or House of Commons.
The Times story is interesting because it links the report on the Commission's activities with the Royal Society, a body which had become embroiled in official political-scientific propaganda exercises before, such as the five-year study of acid rain financed by the Central Electricity Generating Board.
At the same time as backbench morale was being boosted and the government's view was being channelled to the voters, a lid was being kept on the facts about water privatisation and pollution cost.
On 20 July, the generally anti-government Guardian reported that ‘scientists at water authorities have been threatened with criminal proceedings if they talk about the costs of dealing with nitrate pollution.’
After the Water Bill was enacted, said the newspaper, ‘all staff at the Anglian Water Authority were warned that statements about water quality must be cleared with the organisation's lawyers and merchant bankers.
They were also told the penalty for non-compliance was up to seven years in prison and an unlimited fine under Section 77 of the financial Services Act.
Mr Ken Hipwood of Anglian Water said that the costs of dealing with the nitrate problem had been calculated but they were being kept secret.’
However, the UK failed in its attempt to head off the EC Commission, even though it made a last-minute offer to bring forward compliance of five water supplies contaminated with nitrate, from 1995 to 1991–4, ‘a shift,’ according to ENDS , ‘which, until then, the UK had insisted would be impractical.’
On 18 September, Environment Minister Chris Patten flew to Brussels for talks with Environment Commissioner Carlo Ripa di Meana, according to The Times still ‘optimistic that he could head off threatened prosecution of Britain by the European Court’ by showing that ‘Britain was doing everything it could to comply with the European drinking water legislation.’
But on 20 September the Commission announced that for the first time it would take Britain before the European Court of Justice.
Proceedings were  also begun against France and Belgium.
The slow legal process of ‘transposing’ the Directive into national law, especially in Northern Ireland, was one of the Commission's grounds for prosecution.
Another was the level of nitrates in water: up to 100 UK supplies may have breached the 50 mg/litre level between 1985 and 1989.
The last cause was lead pollution (see below), which breached the Directive's rules in 17 Scottish supplies.
Toxic Algae in the Reservoirs
Just as the European Commission was preparing to issue its legal summonses, embarrassing evidence of a major new problem of eutrophication in the nation's water system was coming to light, as rivers and reservoirs used for drinking water fill up with nitrate and phosphate.
On 17 August the public were warned to stay away from Kings Mill Reservoir near Mansfield, Nottingham, ‘after it turned bright green’.
Severn Trent blamed algae ‘caused by hot weather and lack of rain’.
Then, as Michael McCarthy reported in The Times on 9 September 1989, ‘Europe's biggest reservoir was closed to people and animals after the discovery of a possibly poisonous bloom of algae in the water.’
Windsurfers, walkers and fishermen were banned from the 123 billion litre Rutland Water reservoir, which serves 700,000 people around Northampton, Milton Keynes, Peterborough and Daventry.
The cause was microcystis , a type of blue-green alga which produces microcystin, a toxin to both people and animals.
The closure order was prompted by the deaths of a dozen sheep and as many dogs.
Nevertheless, the director of quality for the new Anglian Water plc, Peter Matthews, told the press, ‘I can assure all our consumers that the water we are providing from the reservoir is absolutely safe to drink.’
By 15 September two Cornish reservoirs and one in Devon were also closed to the public owing to blue-green algae.
By now a total of 26 reservoirs had been sealed off throughout the country mainly owing to blooms of blue-green algae.
The underlying reason, said Andrew Lees of FoE, was over-enrichment with nitrates and phosphates.
Gwynne Lyons of FoE says that Anglian Water will in future have special equipment to remove phosphate from the ‘raw’ water entering some of their reservoirs.
Brian Moss of Liverpool University has estimated that phosphate levels in rivers of lowland England are 100 times the natural background, and nitrogen levels are 10 times background.
Altogether 63 waters containing toxic algal blooms were identified by the National Rivers Authority, 37 of them in Anglian Region and 10 in the South West.
By the end of the century, the nitrate now seeping through soil and rocks will put rivers such as the Thames, Great Ouse and the Avon ‘over the limit’ of 50 mg/litre.
In April 1990, Anglian Water admitted that 1.25 million customers had drunk water probably contaminated with microcystin .
Anglian Water and Water Research Centre scientists were accused by an Australian expert on microcystin and by US scientists of ‘playing down’ the findings.
The Ecological Impact of Nitrates
Two rivers illustrate the impact of nitrate pollution.
One is the river Tay in Scotland: a salmon river, still famous as the ‘silvery Tay’ and flowing unpolluted down from Ben Lawers, out of the Grampian Highlands.
It has an abundant wildlife and a nitrate average of 2.8 mg/litre.
The other is the river Great Eau in Lincolnshire, which has an average nitrate level over ten times as high: 37.6 mg/litre.
No nitrate-sensitive plants or animals could survive here.
The Nature Conservancy Council has officially advised the DoE that nitrate levels in rivers ‘of special interest’ for conservation should not exceed 15 mg/litre and in lakes should not exceed 7.5 mg/litre.
‘Of special interest’ in effect means almost any unpolluted river.
Low-nitrate rivers are now so rare, thanks to sewage discharges, fertilisers, slurry and silage, that each one is important for conservation in itself.
A few, like the river Wye which flows from Plynlimon in mid Wales down to the Severn, are designated ‘Sites of Special Scientific Interest’ and support salmonoid fish such as trout, otters, a wide range of invertebrates such as crayfish and dragonflies, along with many plants that are sensitive to high nitrate levels and which have disappeared over much of the country in recent years.
The NCC agrees that in nearly all British freshwaters it is phosphate rather than nitrate which is the limiting factor in controlling eutrophication and that a phosphate directive is required to deal more effectively with it.
‘However,’ the Council said in its evidence to the House of Lords Committee examining the EC's proposal for a Directive on nitrate in vulnerable waters, ‘the nitrate pollution that occurs is caused by the application of chemical fertilisers and animal slurries, which contain a mixture of phosphate and nitrate.
Control of these substances, albeit in zones designated for drinking water protection, will have an indirect benefit for wildlife.’
The NCC also identified more direct benefits to wildlife.
Eutrophication is nitrate limited in the Norfolk Broads during the summer, as it is in Loch Leven and the lakes of Cumbria.
Eutrophication of coastal waters is usually nitrate limited (see Chapter 1) and many species are adversely affected by nitrate long before eutrophication runs its full course (and thus the ‘limiting factor’ argument becomes significant).
Although nitrate pollution of the Broads is well known, recent studies are reported to show that streams in areas such as the Cotswolds are now equally polluted.
The NCC says that the normal nitrogen:phosphorus ratio for many aquatic plants is around 10–20:1.
In waters such as Hickling Broad and Hoveton Great Broad in Norfolk, and in Loch Leven, the supply of nitrogen becomes limiting in July and August when agricultural crops grow rapidly, rainfall is light and nitrate applications are low.
‘A few blue-green algae can store nitrogen, others can use atmospheric nitrogen, so algal communities can change from the normally dominant green to blue-green algae at this time,’ says the Council.
Fish-fry populations and reed swamp development are both damaged by nitrate.
The NCC states that loadings of nitrate in rivers ‘have been sufficiently high at biologically critical times of year (e.g. early spring) to cause environmental damage’.
Studies by the German researchers Sukopp and Schindler  have shown that the stems of reed mace Typha are weakened and tend to break at even 11–20 mg/litre nitrate, while, in the Broads, an extensive dieback of the elegant Norfolk reed Phragmites is attributed to nitrate pollution.
The Sunday Times interviewed a Norfolk fisherman, Frank Wright, in 1989.
Thirty-four years before, Mr Wright earned a paltry 20th position in a fishing competition on the River Ant with a 20lb catch of bream.
Now the River Ant contains up to 60 mg/litre of nitrate and ‘days could pass without a bite’.
‘The truth is,’ said Wright with slight exaggeration, ‘the Broads are dead.’
The NCC's regional officer, Dr Martin George, was also reported as saying that 95 per cent of the Broads were affected by nitrate and phosphate pollution from fertilisers and sewage.
To remove excess phosphate from all the Broads would cost an estimated £20 million.
The benefits of reducing nitrate applications have already become apparent in the ‘environmentally sensitive area’(ESA) of the Brecks in Norfolk and Suffolk.
Here, on highly permeable sandy soils, the ESA provides money to persuade farmers to return to more traditional methods and farm less intensively.
As a result, some long-lost annual plants once characteristic of the area are starting to return.
In Denmark, the authorities encourage and finance the re-establishment of fringing plant communities and marshes alongside rivers which were once canalised.
The plants provide shelter for animals, trap silt and draw nutrients from the water.
Bacteria in mud and in the root mass naturally denitrify the water.
The NCC proposes that farmers should now leave uncultivated strips of land alongside watercourses as ‘soaks’, acting as wildlife habitat and fitting in well with MAFF plans for set-aside of land to help curb over-production of food.
The Draft Directive on Nitrates in Ground and Surface Water
Directly linked to the tapwater issue is the EC's proposal to control nitrates entering water liable to suffer from eutrophication (see the discussion in Chapter 1).
The Draft Directive has adopted the same 50 mg/litre nitrate standard as the Drinking Water Directive and proposes the designation of ‘zones’ that are ‘vulnerable’to being polluted with nitrate up to 50 mg/litre, either directly or indirectly.
The UK government has opposed it.
The National Farmers' Union has attacked it as ‘unreasonable, unworkable and far too sweeping’.
33
Once all water which contains or could on present trends contain, 50 mg/litre nitrate is designated ‘vulnerable’, steps would have to be taken to reduce nitrate pollution.
Limits might be set for the amount of animal manure put on the land, and for inorganic fertiliser application, based on crop and soil types.
In some months fertiliser would probably be banned, and a range of optional measures such as training, use of catch crops and set-aside of land (for example to be used for forestry or nature conservation) would follow.
The draft also proposes that sewage treatment works serving a permanent population of 5,000 or more should have treatment facilities to cut nitrogen in effluent to 10 mg/litre (equivalent to 48 mg/litre nitrate) where the discharge is into a vulnerable zone.
The NCC points out that this would not be enough to  protect many waters, and nitrogen removal equipment would have to be operated to achieve a lower level.
In a joint memorandum addressed to a House of Lords Select Committee, the DoE and MAFF detailed a series of objections to the plan.
The Parliamentary Under Secretary of State, Mrs Virginia Bottomley, reiterated the familiar British line that while the government was ‘already considering the scope of agricultural restrictions,’ it ‘believes that wherever possible, they should in the first event be on a voluntary basis, with compulsory powers being retained as a fall-back.’
The ‘polluter pays’ principle had also been abandoned.
‘The government also believes,’ said the Minister, that ‘where farmers restrict their activities beyond the degree which could be regarded as good agricultural practice, they should be compensated.’
Coming from an administration whose leader laid claim to effective action on global issues such as the hole in the ozone layer and the greenhouse effect (see Chapter 11), this revealed a remarkable weakness in the position of the DoE.
Its attempt to make farmers pay for the pollution they caused had been overturned by the environmentally primitive MAFF.
Not only that, but, unlike other industries, farmers would actually now have to be paid to stop polluting the environment.
The final ignominy was that it was the Environment Department not MAFF that now had to provide public justification.
The UK's objection to the proposed Directive was that it was ‘unreasonably restrictive’ to designate large river catchments ‘simply because, for a short period of time in the autumn, the water may temporarily exceed 50 mg/litre although throughout the rest of the year it will be well below that figure.’
Such ‘temporary exceedances’, said DoE, could be dealt with by natural denitrification in storage reservoirs, by blending or by using other sources.
On the surface, there is something in the government's view.
But once again it overlooks or rejects the underlying philosophy and purpose of a Commission's proposal.
The UK view would be logical if nitrate pollution was not a problem at all below 50 mg/litre, and if it was in equilibrium.
But that is not the situation.
Nitrate levels are increasing, there is a large ‘slug’ of pollution working through the groundwater system and the ecological effects and cancer risk do not disappear below 50 mg/litre.
In trying to protect sources of water for drinking, the EC proposal is a step towards environmental purity rather than a sticking-plaster solution which does the minimum needed to avoid the worst nitrate pollution.
In many of the large catchments to which the DoE refers, impact of nitrate may now be over the limit for only a week each year, but in future it may be for two weeks, then three, then a month or more: when does the UK propose to take action?
And what does the UK mean by a ‘large’ catchment?
The UK makes no proposals for a definition.
Instead it proposes that the 50 mg/litre limit should be treated as an average not a fixed limit.
The MAFF-DoE memorandum also contains several contradictions which illustrate the confusion and conflict in government circles.
‘It is important to note,’ says the document, that ‘the UK draws a higher proportion (70 per cent) of water from surface sources than any other member state except Eire.’
Thus (presumably unfairly) the UK ‘would be more affected by this part of the  Directive than other states.’
Why this should be an objection to the control of pollution from nitrates is not clear, and it also overlooks the government's own study commissioned from Sir William Halcrow and Partners.
The Halcrow report showed that, while overall some 30 per cent of UK drinking water comes from aquifers, this varies from 6 per cent in Wales to 74 per cent in Southern Water's region.
Would this mean, then, that the government would accept curbs on nitrates in the rivers of Kent but not of Wales?
The UK memo claims to assess the ‘scientific basis’ of the Commission's proposals and notes haughtily, ‘There seems to be an underlying assumption that the nitrate problem is concerned with the quantity of fertiliser and manure applied, whereas studies in this country, and elsewhere, have established that the problem is much more complicated than this, reflecting, for example, other aspects of farming practices.’
Although it fails to name them, these ‘other aspects’ include the ploughing-up of old pasture and the number of stock kept on grassland.
A study by the Water Research Centre (WRC) showed that nitrate losses of 30–40 kg per hectare often occur under winter cereals and are worse under potatoes, rape and other vegetables, while only 2–5 kg/ha are leached out under grass, even fertilised with 200 kg/ha.
But, with heavier stocking, even grass leaches more nitrate.
To meet the EC 50 mg/litre standard the WRC believes the leaching rate must be (at most) 20 kg/ha in eastern England and 30 kg/ha in the wetter west.
40 On this basis, nitrates might have to be cut by more than 25 per cent to achieve a real reduction in leaching.
The DoE and MAFF memorandum also claimed that, ‘in common with current UK policy,’ the Draft Directive shows ‘a belief that measures must be taken to protect water sources from contamination by nitrate’.
But it then goes on to criticise the Draft Directive because it ‘fails to recognise that a different mix of measures including water treatment or blending may be required depending on local factors, such as geology, rainfall and farming practice.’
In other words.
the focus is again on treatment rather than prevention.
The UK proposed to argue for more ‘discretion’ for individual governments, for example‘aiming’to reduce nitrate levels while being allowed to set its own rates of application of manure.
From past experience with UK rules that allow ‘discretion’, it is not very hard to imagine that the practice might bear little resemblance to the ‘aim’ of the policy.
If the UK has its way, then the Directive should have little impact, for it notes, ‘We do not believe that it will be necessary to apply measures in the UK to protect waters from eutrophication due to nitrate.’
In May, the UK government produced counterproposals in the shape of a consultation paper from MAFF.
This proposed setting up ‘nitrate sensitive areas’(NSAs) in which sources are at or over 50 mg/litre, to be identified by the National Rivers Authority.
Proposals to reduce nitrate would then be put to MAFF, which would in turn consult interested parties.
On the basis of the MAFF-DoE memo this would seem likely to include the farming and fertiliser industries, and would give the Ministry of Agriculture almost total control over the policy.
The NSAs, said MAFF, would be designated on a ‘pilot basis’.
According to the magazine ENDS , it seems likely that ‘at least five years will elapse before further NSAs are [then]designated unless the government's hand is forced by EEC legislation.’
Controls on farmers' use of nitrates would just be voluntary to start with, and legal limits would be imposed only if the voluntary controls turned out to be ineffective.
Conversion of cereals to grassland would probably involve compensation, although avoidance of autumn fertiliser applications would not.
Farmers would be invited to enter into contracts for low nitrogen fanning.
Only if a legal system was eventually brought in would a breach of contract become a criminal breach of the law.
Otherwise, as the NRA later pointed out, the only sanction against polluters would be the recovery of compensation.
The National Farmers' Union welcomed the proposal while awaiting details of how much money farmers would get.
FoE termed it ‘pathetic’.
In August, the Agriculture Minister, John Selwyn Gummer, announced plans for 12 experimental water protection zones including: Branston Booth, Lincs; Sleaford, Lincs; Chalford, Oxford; Egford, Somerset; Millington Springs, Humberside; Ogbourne St George, Wilts; Boughton, Notts; Tom Hill, Staffs; Wildmoor, Hereford and Worcester; Wellings, Staffs and Shropshire; Milton, Derbyshire.
In an area restricted to just 15,200 hectares farmers would be offered compensation to reduce fertiliser inputs.
Farmers over another 22,400 hectares, would merely be given advice and asked to use less fertiliser; there would be no money for them, and no other sanctions (The Swells, Gloucestershire; Bircham and Fring, Norfolk; Sedgeford, Norfolk; Fowlmere, Cambs; Far Baulker, Notts; Dotton and Colaton, Devon; Cringle Brook, Lincs and Leics; and Bourne Brook, Warwicks).
It now emerged that the new National Rivers Authority had warned MAFF that the voluntary approach was ‘an insufficiently secure base’.
‘A single ploughing may be all that is needed to negate five years of land use control, and the nitrate locked in the root system could be irretrievably lost,’ said the Rivers Authority.
Voluntary controls could, it warned, end up as a ‘costly failure’.
At the time of writing it is not clear what will happen to the Draft Directive.
As has been seen above (Chapter 1), the same measure will call for controls to prevent eutrophication in coastal water and Britain has been fighting the battle-of-the-maps at the Paris Commission.
As to inland waters, the UK apparently intends to argue that the controlling factor in eutrophication is not nitrate but phosphate.
‘Where there is a danger of eutrophication in this country,’ says the government, ‘usually the limiting factor is phosphate, and not nitrate; if confined this would mean that zones would not therefore need to be designated to prevent eutrophication.’
For scientific assessment of a subtle ecological issue such as the causes of eutrophication, it is important that the information used to decide policy is openly available for scientific debate.
In this case, it seems that the DoE will decide behind closed doors.
The DoE wrote to Water Authorities ‘seeking from them assessments of the zones that would need to be designated’ if the Directive came into being.
It also has information on nitrate levels held as part of the Harmonised Monitoring Programme and ‘separate information on  farming practices’.
It seems this will be used by the DoE to ‘assess’ the EC proposals although none of the facts will be available to the public.
The UK also hopes that the need for unanimity in the Council under Article 1305 will enable it to exercise a veto on the proposal.
But should the UK fail to deflect the Commission and the Directive goes ahead, then its impact could be considerable.
In the ‘worst case scenario,’ say MAFF and DoE, ‘it is possible that most of the Anglian region would have to be declared a water protection zone as well as substantial parts of Severn Trent and other areas, accounting in total for the great bulk of the UK arable protection area.’
The Soil Survey and Land Research Centre at Silsoe has identified 15 per cent of soils in the Midlands, eastern and southern England as lying over chalk, limestone or sandstone, and so vulnerable to nitrate leaching.
Would this be the worst case scenario or the best case scenario?
After all, almost the whole of industrial Britain became a controlled, smoke free area under the Clean Air Acts, of which the current government is so proud some 30 years later.
It is a fact that the water supplies of the Anglian region, and others besides, are seriously polluted with nitrate and that on present trends this pollution will get much worse.
There seems a prima facie case for prompt national action.
If the ‘polluter pays’ and preventative principles were genuinely applied, there would, as the memo fears, be ‘major agricultural and related consequences’.
But these would be for the general benefit, not for the worse.
The government, it seems, is still thinking short term.
Indeed it illustrates its ‘worst case, scenario with the observation that the ‘major changes’ in farming would ‘benefit water quality little if at all in the short to medium term’.
As an example it gives the ‘extreme case’ of chalk areas once downland but converted to cereals mainly in the 1960s and 1970s which, if returned to unfertilised grassland, would reduce nitrate to 50 mg/litre only after 2040.
A return to downland would be popular with walkers, with Britain's 3 million paid-up members of nature conservation organisations, its 10 or 20 million ‘green consumers’, with local people who now suffer spraydrift, and with consumers of water.
A few hundred or a few thousand farmers might oppose a return to downland because it would mean lower profits.
But it would save the country money in not producing surplus cereals.
And much of the land in question is not naturally good for cereal growing.
Without heavy use of fertilisers there would be no cereals on it.
In the year 2040 our children and our descendants might think it was an ‘extreme case’ not to have begun to take action to reduce nitrate pollution half a century earlier.
Nitrate Solutions
According to the ‘polluter pays’ principle, the costs of dealing with pollution should be borne by the people who create it.
But who is this in the case of nitrates: is it the farmers, the consumers who buy high nitrate-produced food, or the fertiliser manufacturers?
The consumer has had little or no choice in the matter: without organic food widely available it has not been possible to choose to avoid creating nitrate  pollution by avoiding conventionally grown crops (in any case, organic farming can still create some nitrate pollution, see also below).
The blame must lie with the fertiliser manufacturers, who undoubtedly have done their best to encourage increased consumption by heavy advertising to farmers, and who in many cases also own farms themselves and supply seed of varieties specially bred to respond to high levels of nitrogen.
The farmers who have undoubtedly profited from producing heavier crops of higher-yielding varieties that rely on nitrates also share the blame.
The government, too, has a responsibility, as through the Common Agricultural Policy of price support system it has guaranteed a market for cereals and encouraged intensive production.
The Thatcher government has opposed planning controls over agriculture that could have stopped the spread of intensive arable farming.
And at one time it was UK policy for fertiliser use to be encouraged by direct grants and subsidies.
A MORI Poll conducted in November 1988 showed that of a hypothetical £100 to be spent on cleaning up nitrates and pesticides in the public water supply, people felt industry should pay £39, farmers £19, the government £31 and the consumer £11.
(Asking if the ‘government’ should pay of course begs the question, where does the government get the cash from if not from the consumer?
In practice, as we have seen, it is British policy to subsidise the polluting farmers rather than make them pay to clean up the pollution.)
The precautionary principle suggests that, as the future damage done by pollution is often more costly than the extra expense of avoiding it in the first place and in any case it is often unacceptable, even if a money cost can't be put on it, then prevention is better than cure.
In the case of nitrates, it is increasingly evident that stopping the pollution at source is going to be far cheaper than trying to deal with the consequences afterwards.
Prevention can take several forms.
Nitrate application can be banned or severely controlled in areas where bore-holes will be particularly affected in the short term.
It can be limited across the whole country or across wide areas liable to suffer in the longer term (for example, on all permeable soils such as on chalk, limestone, sand or sandstone).
Quotas can be introduced, which might or might not be tradeable between farmers (David Baldock of the IEEP suggests that within the National Farmers' Union a debate over quotas ended with victory for those in favour of no such regulation).
Taxes can be imposed either to raise funds for pollution control or to discourage over-use of nitrates, or both.
Lastly you might choose to rely on advice to farmers and fertiliser manufacturers.
This ‘voluntary’ approach, as we have seen, is the one traditionally favoured in the UK.
Its success is almost invariably limited to the few cases where free advice enables a businessman to make immediate cost savings.
According to the DoE, since 1979 government advice to cereal farmers has reduced autumn applications of nitrate by 60 per cent.
At the same time, more cereals have been sown in the autumn rather than the spring and sowing has been earlier (e.g. in September not October).
Both factors have reduced nitrate loss from arable land and some ‘fast responding’ aquifers have shown slight decreases in nitrate pollution as a result, says the DoE.
But there are other reasons for farmers switching to autumn cereals than a desire to curb nitrate  pollution, and this is likely to be the limit of what the ‘voluntary approach’ can achieve.
The two water regions of Britain with the greatest nitrate problems are Anglian and Severn Trent.
Following the government retreat over the terms of the Directive in 1988 it was estimated that Anglian would have to spend £60-£70 million over the following 10 years to clean up the water supplies of Bedford and Norwich, along with a total 25 supplies that had been subject to derogations.
‘Remedial measures,’ said the magazine ENDS , ‘will be needed at 42 sites in the Anglian region.
Ion exchange denitrification plants will be installed at 23 locations.
Another four will employ denitrification technology.
A combination of blending and source replacement will be used for the other 15 locations.’
Within the next 20–25 years Anglian expects another nine supplies to go over the limit.
In 1988, Severn Trent had already begun a £7 million capital programme to bring 17 ‘derogated’ supplies below 50 mg/litre by 1991 by blending and taking sources out of commission.
ENDS noted that around 6 per cent of Severn Trent's abstracted groundwater supplies exceeded 50 mg/litre of nitrate but ‘with a continuation of present land use practices the figure will rise to 20–25 per cent by 2011’.
This, said ENDS , was ‘the issue causing the water industry most concern’.
It could still be averted but only ‘if action to reduce nitrogen leaching at source is taken immediately’.
No such action has been taken: the pilot projects apply to just a handful of catchments.
Other countries are further advanced.
Since 1985 the German Drinking Water Ordinance has specified ‘protection zones’ of 100–500 metres around bore-holes, and the UK Nitrate Coordination Group noted in 1986 that the Germans were giving ‘consideration…to stricter catchment area regulations laying down maximum nitrate fertiliser application rates, banning its use at certain times of the year and requiring changes in the land use’.
In Lower Saxony, the spreading of manure and slurry was already controlled.
In Denmark.
farms with more than 20 head of cattle must have access to stores for slurry and effluent with at least six months' capacity.
Liquid manure cannot be spread on the fields from harvest until 1 November (unless it is onto growing vegetation or crops for the next winter).
Nor can it be spread on frozen ground.
Similarly, Danish farmers must comply with a limit of two head of cattle per hectare, equivalent to an input of about 170 kg of fertiliser nitrogen on each hectare each year.
To keep more animals, the farmers must possess a written long-term agreement to deliver surplus manure to other farms, or to common storage plants or biogas plants.
No such system is proposed for the UK.
Dutch manure and fertiliser rates are due to be reduced until a 50 mg/litre soil water concentration is achieved at 2m depth, later to be cut to 25 mg/litre.
Under the National Environmental Policy Plan, the Netherlands aims to balance its national nitrogen and phosphorus budget by the year 2000.
New pig and poultry farms are banned and grassland applications are limited.
In theory, Section 31 of the 1974 Control of Pollution Act gave Water Authorities powers to restrict farming activities in protection zones, but the  powers were never used.
The question of introducing nitrate protection zones got serious consideration only through the Nitrate Coordination Group in 1987.
In 1988, the DoE published a report The Nitrate Issue: A Study of the Economic and Other Consequences of Various Local Options for Limiting Nitrate Concentration in Drinking Water .
This studied the economics of reducing groundwater pollution in ten catchments overlying chalk, sand or limestone aquifers.
In five of them the nitrate level already exceeded 50 mg/litre and it was due to exceed that level in the others by 1991–2004.
The study compared the costs of prevention through land use controls and of treatment by removing the nitrates from abstracted water.
In eight out of ten catchments the report's authors calculated that the immediate cost of water treatment or blending was less than the ‘local cost’ of farming measures, but, once Exchequer savings from reduced agricultural production were included (i.e. savings in public grants and subsidies paid to farmers to produce crops), in most cases it was cheaper to establish protection zones with curbs on farming than to treat the polluted water.
While changing agricultural prices and subsidies will have a marked effect on which options look most economic, as ENDS remarked, ‘controls based on a combination of land use and water measures’ are probably the best option.
It is also significant that the DoE study did not consider the costs of several ‘uncostable’ factors such as the increased corrosion and organic contamination which denitrification can cause but which would not be incurred by preventative methods.
Lead
Lead is a poison which impairs the development of the nerves and the brain, especially in children, and which will poison anyone if enough is ingested.
These days acute lead poisoning is a rarity, but children are still at risk from chewing flakes of lead-based paints, which taste sweet.
As early as 1943 American researchers R. K. Byers and E. E. Lord had published a paper Late effects of lead poisoning on mental development .
Research in the 1970s and 1980s showed a link between lead entering the body and the state of health.
Pollution consultant Brian Price, who campaigned with the pressure group CLEAR (Campaign for Lead Free Air), highlights a crucial study by Dr Robert Needham in Boston, USA.
Needham examined the IQ of children and the quantity of lead found in their (shed) milk teeth.
‘He found, after allowing for some 37 factors not related to lead,’ says Price, ‘that those children having high lead levels in their teeth did significantly less well in tests than did those with low levels.’
As Price points out, the importance of his finding was that such ‘high’ lead levels were common in city children in Britain.
Dr Robert Stephens of Birmingham University says Price concluded that 20 per cent of urban children in Britain could be suffering ‘some form of  lead-induced mental impairment’.
Lead dissolves in acid water.
In naturally acid areas, or where acid rain has made the water more acid, lead pipes are a serious hazard to health because the water strips the metal from the pipe walls and it ends up coming out of the tap.
The Romans were fond of lead vessels for food and drink and used lead-based glazes on pottery: lead poisoning was common and has even been proposed as a cause of the madness among high-ranking Romans which contributed to the fall of the Roman Empire.
The palliative for drinking water supplies is to dose the water supply with calcium carbonate (lime) to raise the pH (that is, reduce the acidity) and stop the lead dissolving.
The only cure, however, is to remove lead from the water system altogether, by replacing old pipes and tanks.
Although lead in water would be an important factor in high blood and body lead in some areas, high levels were also observed in people living in areas with lime-rich ‘hard’ water with low lead levels.
Campaigners and many scientists believed lead from the tetra-ethyl lead added as an anti-knock agent in petrol had to be the cause.
The government, under pressure from companies such as Associated Octel, disagreed.
In 1980 the government published a report Lead and Health , which, says Price is ‘now notorious’.
This report, also known as the Lawther Report after its chairman Professor Patrick Lawther, played down dust and airborne lead as sources of contamination.
Robert Stephens showed in 1982 that a 2-year-old child in a high-density traffic area could ingest 54 per cent of its lead via the air (some directly, but most from dust picked up on fingers and food), while 46 per cent would be accounted for in food and drink.
While nationally the proportion of lead taken in by people from water is 6 per cent, in areas with high lead levels in water this can rise to more than 50 per cent .
In a vigorous and ultimately successful seven-year campaign, CLEAR forced the government to admit both the exposure to lead due to lead in petrol, and the health hazard posed by lead as such.
The problem of lead in water, however, remained.
A 1975/6 random daytime survey for the Department of the Environment revealed that over 7 per cent of the tapwater in England exceeded the EC limit while in Scotland the figure was over 34 per cent.
In its ninth Report,Lead in the Environment , published in 1983, the Royal Commission on Environmental Pollution called for more vigorous action to reduce the hazard from lead in drinking water.
Although a voluntary programme of treatment (water hardening) and replacement of lead plumbing was begun, it was not enough.
Grant-aid was available, but only in houses with rates below a certain level.
And these grants were later affected by cuts in government expenditure.
The result is that many homes still have lead pipes.
As of 1982 more than 300,000 households still had water with more than 100ug/l lead in their tapwater.
Studies in Glasgow showed that mothers with higher than normal levels of lead in their bodies had more stillbirths, and babies born small.
Lead passes freely across the placenta into the unborn child from the mother's blood.
Dr Michael Moore of Glasgow University showed that mothers who drank water with a high lead level were twice as likely to  have mentally retarded babies; 61 per cent of mothers involved in stillbirths or whose foetuses were abnormal had placental lead levels over 1.5 ppm whereas only 7 per cent of those with normal babies had such high lead levels.
The most recent study in Scotland involved more than 800 6-9-year-old schoolchildren in Edinburgh.
Children with higher blood lead levels did less well in IQ tests, number skills and reading ability.
Dr Robin Russel-Jones, at one time chairman of CLEAR, stated in 1989 that ‘A later stage of this study also showed a clear link between blood lead level and aggressive/anti-social and hyperactive behaviour in children.
These effects occurred at what were previously considered very low levels of lead in blood.
There was not only clear evidence of a dose-response relationship, but no indication of a threshold below which effects would cease to occur’.
In 1989, it was estimated that over 2 million people in the UK still drank water contaminated with lead.
Many of them (but not all — see below) were in Labour-controlled Scotland, and many of these were in old, poor tenement housing.
It is hard to escape the feeling that this proven hazard would not have been allowed to continue, poisoning children and adults, if the victims had been residents of Weybridge or Sloane Square rather than the Gorbals.
In his book Acid Rain , Fred Pearce describes what happened to a report on the severe lead pollution in Scotland.
‘The most devastating evidence about the scale of the problem was ‘presented to Parliament’ in 1980.
Actually, a report was placed in the House of Commons library late one afternoon just before Christmas.
The study, by the Greater Glasgow Health Board, revealed that more than a tenth of all newborn babies in Glasgow entered the world with more lead in their blood than is considered safe for adults…
Five percent of the mothers in the survey had more than the international limit of 350 micrograms of lead in every litre of blood in their bodies.
Eleven percent of their newborn babies exceeded the limit.’
The EC Drinking Water Directive set a Maximum Admissible Concentration (MAC) of 50 ug/litre of lead in tapwater.
The formal date for compliance was July 1985 but, as Nigel Haigh notes in EEC Environmental Policy and Britain , in 1983 the UK government set itself a target of complying with the lead limit only by December 1989.
By March 1987 the UK had applied to the Commission for a delay until 1989 for all areas where, on a random survey, more than 2.5 per cent of the properties showed the MAC was exceeded.
Between 38 per cent and 57 per cent of houses in the UK have lead pipework, while around 27 per cent are in areas with ‘plumbosolvent’(lead dissolving) water, and half of these homes have lead in the connection pipe leading to the tap.
In November 1988 the UK sent plans to the Commission, indicating the ‘completion of necessary works’ by 1992.
In April 1989 the European Commission sent Britain its Reasoned Opinion alleging an infraction of the EC Drinking Water Directive and citing the UK's record on eliminating the hazard from lead.
‘From the information submitted by the United Kingdom, the Commission is of the opinion that little had been done by way of systematic work on the problem of lead in drinking water in Scotland between the adoption  of the Directive and 20 July 1985.
The Commission indicated, by a letter of 9 August 1988, that it required a commitment that all Scottish water supplies would be in conformity with the lead parameter of the Directive by the end of 1989.’ 1992 was better, but not good enough, said Brussels.
A striking example of the opposed thinking in Brussels and Westminster now emerged.
The Commission said, ‘In view of the long-standing and widespread nature of this infringement of a toxic parameter of the Directive, the Commission cannot accept any delay beyond 1989.’
Contrast this with the words of a House of Lords Select Committee considering exactly the same problem: ‘because of the high proportion [of homes with polluted tapwater]and the long-standing nature of the problem, the Committee believe that the two years for compliance with the Directive is wholly unrealistic.’
The Commission also focused on the way the UK was measuring lead in tapwater.
Lead levels from a system with lead piping are much higher in the first few pints that come from a tap that has been left turned off than after the water has run for a few minutes, as this flushes out much of the dissolved lead.
The UK government measures the lead in tapwater that has been running for several minutes, as it would be if you had a bath in it, and not the first few pints or cupfuls, as you would use for cooking or making a cup of tea.
Quite understandably, the Commission objected to this procedure, which made UK tapwater appear to have less lead in it than the consumer experiences.
As of 1989, 70 water sources in Scotland had water which needed chemical treatment to make it safe for putting through lead pipes, including supplies in Fort William, Edinburgh, Glasgow and Peterhead.
Although the Scottish Office would not say how far supplies exceeded MACs for lead, at least 17 schemes were in progress to reduce lead in water that would not be completed until as late as 1992.
The government is now faced with the realisation that treatment alone will not get lead levels down to the 50 ug/litre required by the Directive.
When Friends of the Earth researched the tapwater survey of England and Wales run in the Observer in 1989, they found that lead exceeded the legal limit in a far larger and more widely distributed number of supplies than had previously been supposed.
Water suppliers' own records for 1987–9 showed that such surprising areas as Bath, Brighton, Huntingdon, Braintree and Waveney had lead pollution of drinking water above the MAC.
A chunk of East Anglia from north Bedfordshire up to Hunstanton was affected, but the worst areas were in north Wales, Lancashire, Greater Manchester, Merseyside, Yorkshire and the southwest.
Reservoirs in the Lake District, for example, supply acid lead-stripping water to Greater Manchester.
At Dilworth near Preston, a tapwater sample contained 3,600 ug/litre lead, that is, 72 times the 50 ug/litre limit.
Another from Blackburn contained twice as much: 7,750 ug/litre lead or 155 times the EC limit and 77 times the action level.
Even Hythe in Kent had 147 ug/litre while a sample in the area of Huntingdon went 9 times over the limit at 450 ug/litre.
Altogether they found 122 supply areas with lead above the MAC.
Lead can also get into water from the solder in copper pipes.
Research by the Water Research Centre in 1981 showed that long runs of copper piping  with lead solder could produce high levels of lead in tapwater.
The WRC suggested it should be banned, but the government has taken no action.
Schools, of which no survey has been published, may have a particularly large number of lead pipes, despite the risk to children.
Dr Robin Russel-Jones pointed out in September 1989 that, although the UK planned to achieve the EC 50 ug/litre level, which conformed to the prevailing WHO level, the WHO itself was planning a revision downward to 25 ug/litre or below.
In February 1990, a MAFF study discovered that children and pregnant women were receiving more lead than had been thought, and recommended a level of 10 ug/litre, as applies in the USA.
The cost of replacing all the pipework that poses a danger of lead contamination has been put at £1,000 million by the WRC and a committee apparently advised the DoE that the cost could be £405-£2,000 million.
Aluminium
Unlike lead, aluminium is a relatively recent arrival in the pollution debate.
It gets into water supplies in two ways: either from acid soils, where it becomes soluble at low pH (there are huge amounts in the soil and these are washed out by acid rain where there is little organic matter to bind the aluminium), or by being deliberately added to peaty water to remove the suspended organic matter and make the water clear, by a similar chemical process.
We also take in a considerable quantity of aluminium in certain foods, but this is normally in a ‘non-bioavailable’ form.
Since the 1960s there have been growing suspicions that Alzheimer's disease (a form of senile dementia) is associated with high levels of aluminium.
The disease is growing in importance as the population of many western countries ages: it kills 120,000 a year in the USA.
Scientists at the Medical Research Council (MRC) reported that a link might exist as long ago as 1968.
People living in Guam and west New Guinea suffer unusually high rates of Alzheimer's disease and live in areas with high levels of aluminium in the soil.
Since 1986 Norwegian research has also suggested that in the southern, most acidified parts of the country, there may be a link between high rates of Alzheimer's disease and aluminium in water.
The characteristic signs of Alzheimer's disease visible with a microscope are tangled clumps of nerve cell fibres in the brain and ‘senile plaques, knobby patches of dying nerve fibres’.
These are thought by researchers to be the result of low calcium and magnesium and high aluminium in the brain.
Kidney dialysis patients are particularly vulnerable to aluminium in water supplies as their bodies are continually ‘flushed’ with large volumes of water.
In England Sheila Brayford from Staffordshire died from brain damage in 1981 when she absorbed about 100 times the normal level of aluminium in dialysis.
An MRC group reported in January 1989 that people receiving drinking water with high levels of aluminium stand a 50 per cent greater chance of suffering from Alzheimer's disease.
The study encompassed 1,203 dementia patients in 88 districts.
For those with raised aluminium levels in the water  supply (of 110 ug/litre), the risk was found to be 35–50 per cent higher in people under 70.
The Council's research showed highest aluminium concentrations in Northumberland, Durham, Tyne and Wear, Devon and Cornwall.
The lowest were in Suffolk, Cambridgeshire, Hampshire, Nottinghamshire, Derbyshire, and Norfolk.
Camelford
Aluminium poisoning became famous in Britain when on Wednesday 6 July 1988 a 20-tonne lorry-load of aluminium sulphate was poured into the wrong tank at Lowermoor treatment works and polluted the water supply of Camelford in Cornwall.
The water became acrid and bitter.
It curdled milk in tea, was just drinkable in coffee, and, when people washed in it, the water turned blue.
The treatment plant, where the aluminium sulphate was routinely used to clear the water by removing organic matter, serves 7,000 homes and 20,000 people.
The lorry driver, a relief worker from ISC Chemicals in Bristol, came on the wrong day and found nobody at the plant.
He put the chemical in what he thought was the storage tank but wasn't.
Edward Pilkington writing in the Guardian later commented, ‘What at first glance appears to be no more than a simple case of human error becomes, at closer examination, as much a story about bungling management and an inadequate safety system born of years of cuts.
John Lewis, the district manager in charge of Lowermoor, had been voicing his concerns about the safety of the treatment works for some years before the accident.
As early as August 1986 he wrote to South West Water's headquarters in Exeter complaining that ‘we seem to move from one crisis to another.
Proper pre-planning is becoming more and more a dream.’
Staff cuts and reductions in overtime, both introduced as part of the Authority's attempts to increase book profits, were blamed by Lewis in more than one confidential memo for procedures which cut corners and increased the risk of accidents.
He had asked for a new security system for Lowermoor but was refused on cost grounds.
The number of safety advisers working in the field in SWWA's region was reduced from six to two after a visit from the government's ‘flying accountants’, brought in to see that the Authority met its ‘performance aims’ after financial cuts in 1980.
The same accountants apparently proposed getting rid of river wardens and people in pollution control.
Len Hill, South West Water chairman until 1987, was one of those who protested that government cuts meant a ‘greater likelihood of disruptions and deteriorations in service to our customers and possible health and safety implications…
Experienced operators on site are able to detect failures but these procedures are not followed due to decreased manpower.’
Mr Hill was replaced by Ken Court, from Blue Circle Cement.
The reaction of the SWWA to the incident was a mixture of panic and secrecy.
The aluminium sulphate was added to the ‘contact tank’, a bad place because it was the last stage in the treatment process before the water left for the town.
That was at 4.30 p.m.
Not long after the lorry driver left, the water became more and more acid and alarms went off.
By early evening, emergency  staff reached the plant.
But at first they didn't realise what had happened and, instead of stopping the supply and cleaning out the tank, they flushed out the system, which sent the acid down the mains into the homes of the area.
The Authority's early explanations mentioned lime dosing equipment and spoke of ‘a slight acidity problem’.
This was later altered on 14 July to ‘high acidity’.
It was not until Friday morning, after a day and a half had elapsed, that the real fault was diagnosed.
By this time the rivers Camel and Allen had been polluted with aluminium, killing 60,000 trout and salmon.
Pilkington talked to Dr Richard Newman, who was out on an early morning call, on the Thursday.
He watched in disbelief as water gushed from the fire hydrants and flowed down the road into the river, where the fish were dying.
The SWWA had opened the hydrants at night to try and clear out the system before the town woke up.
The attempt failed and no warning was given to the public.
Pilkington recounts how when Winifred Harper turned on her tap, ‘out came a yellowy, stinking acerbic fluid which curdled the milk’.
She, like others, telephoned the Water Authority and was reassured that the water was safe to drink.
Still, it tasted so bad she had to make sugary coffee instead of tea.
‘For the first few days,’ wrote Pilkington, ‘it was all quite humorous.
People laughed about their pink towels turning blue, the result of copper leached from the hot water pipes by the acidic water.
The press, slow to grasp the significance of what had happened, treated it as a light hearted story about bleached hair turning green.’
Postmaster and chairman of the Camelford Chamber of Commerce Walter Roberts noticed that his solar panels had become corroded.
Clive and Anne Ahrens, who had a holiday cottage in Helstone village, found a white sludge at the bottom of a glass of water poured from the tap.
The SWWA told them it was safe to drink.
On one farm 1,300 chickens died, and on another 10 lambs out of 40 (the rest being made ill).
The Ahrens' 40 ducklings also died within a fortnight.
By Friday 8 July residents of Camelford were being supplied with water at up to 500 times the Maximum Admissible Concentration under the EC Drinking Water Directive of 200 ug/litre (0.2 mg/litre).
Independent tests put the levels at up to 3,000 times the limit.
Considerable efforts were then made to clean out the system but not to inform the public.
Indeed, the Authority orchestrated a positive attempt at secrecy.
A SWWA document obtained by the Guardian stated that, ‘If public reaction escalates to a major extent over the weekend the Department of the Environment should be informed.’
The emergency staff had realised at 5 a.m. on Thursday 7 July that dialysis patients were at special risk from the aluminium, but the house of Robert Hill, a local man with kidney failure and a dialysis machine, was not contacted until the next day.
The Authority blamed Dr Grainger of the District Health Authority, who, ‘since he had received no complaints’, ‘took no positive action’.
Fortunately, Mr Hill was in Truro Hospital at the time, out of the area.
Otherwise he might have suffered the same fate as Sheila Brayford.
John Lewis, the man who had complained about the effect of financial cuts on safety at Lowermoor, was the only person to be sacked as a result of the incident.
He told the Guardian that he believed senior management took deliberate  steps to hush up the dangers.
As soon as the contamination was confirmed, he was told to treat the information as confidential.
His views were later confirmed by SWWA's own non-executive director John Lawrence who commented in his own report: ‘There seems to be a culture in which the public are told as little as possible and expected to trust the Authority to look after their interests.’
Even Ken Court admitted that, with hindsight, the secrecy was ‘mistaken’.
But the health effects were only just beginning.
People began to complain about feelings of sickness, vomiting, diarrhoea and headaches.
It seems likely that these were connected not just with aluminium but with copper, lead and zinc stripped from the insides of pipes, solder and fittings by the acid.
Similar problems occur in Sweden where many private supplies are highly acidified as a result of acid rain.
People presented symptoms to Dr Newman such as skin rashes, lip blisters and, in particular, mouth ulcers.
He estimated that there were more than 300 cases.
According to Pilkington, ‘One child he examined had such a lacerated tongue he compared it to sago pudding.’
Other people suffered arthritis and aching joints and muscles.
Dr Newman was seeing five or six people each surgery with the same symptoms, and ‘the pharmacist reported a flood of people asking for remedies’.
A year later, some still had the symptoms.
Flautist Tim Wheater, well known for playing with the group The Eurythmics, suffered an outbreak of lip blisters and ulcers which destroyed his ability to control the flute, and lost him a year's income.
Winifred Harper was diagnosed as suffering from Parkinson's disease, which both she and Dr Newman suspected to be connected with the water poisoning.
‘Anybody can make a mistake.
To err, it's only human.
I can forgive them for that.
But I will never forgive them for covering it up,’ she told Edward Pilkington.
Dr Grainger and some of the local doctors dismissed the ulcers and sore throats as ‘common’, saying they ‘could well have occurred as a coincidence’.
According to the Sunday Times , ‘A Government Minister has implied that their [the people of the area]problems are imaginary or psychosomatic and a senior Whitehall official has suggested that the whole thing was a ‘hoax’, devised to thwart the Government's privatisation plans.’
Many of the indigenous Cornish people of the area suffered symptoms but have accepted compensation from the Water Authority.
‘It's too big for us to tackle,’ said one, Keith Hill, his wife adding, ‘We keep quiet because we see ourselves as second best — so we don't make demands when perhaps we should.’
But the newcomers were more belligerent and less respectful of authorities of all kinds.
A number were still pursuing court cases against SWWA at the time of writing.
If it had not been for an alliance of Dr Newman, consultant ecologist Doug Cross and Walter Roberts — three local residents who formed the Camelford Scientific Advisory Panel and conducted their own investigations — and for John Lewis, who lost his job, it is doubtful whether very much would ever have come out about the incident.
On 11 July Cross had taken a sample of his own tapwater into the local North Cornwall District Council offices and asked for it to be analysed, but the Council refused.
The Sunday Times reporter Peter  Gillman recounts how the Panel went to the Trading Standards Office at Bodmin but was told that there was nothing to be done because water was not covered by the Food Act.
The Office referred them to the Office of Fair Trading.
Here they were told to call the Water Research Centre.
The WRC said not to worry as copper was not particularly toxic.
Eventually they learnt from Dr Grainger that there was some contamination with alum (aluminium sulphate) but that this was ‘not particularly dangerous’.
On 19 July Roberts and Cross wrote to Dr John Lawrence at SWWA suggesting that the public had been exposed to a more serious hazard than had been admitted: they were told that they could meet Lawrence but he would give them no information.
Then on 22 July someone rang Roberts anonymously and told him that a lorry-load of aluminium sulphate had been tipped into the water supply.
Roberts called a meeting of the Town Council and he and Cross asked the police to start an investigation.
The police were’ sympathetic’but according to Cross were ‘even more in the dark than we were’.
On 3 August, Roberts and Cross met with Lawrence but he only said that the lorry story was ‘hearsay’.
Not until the anonymous informant had rung again on 4 August and there had been more calls to Lawrence, and they had finally made contact with John Lewis, did the Panel begin to find out what had really happened.
On 22 July, the Water Authority had admitted for the first time that aluminium had been involved by placing a small advertisement in the local press saying ‘that the water was no more acidic than lemon juice’.
From 14 August onwards the Water Authority’ went public’and issued numerous statements and explanations.
On 18 August it met with the Health Authority's officials and asked them to conduct a public health survey which it would pay for, mainly to reassure the public that, although the EC limits for aluminium had been exceeded over 500 times, and those for sulphate, copper, zinc and lead had also been broken, there was no long-term harm.
A number of people who were previously healthy now reported difficulties with their memory, digestion, concentration and muscle control.
Some said they were sensitive to the water, their symptoms leaving them when they drank bottled water.
It was also discovered that the Poisons Unit at Guys Hospital and a DHSS toxicologist had wanted to visit the area to do tests but had been ‘warned off’.
On 31 October 1988, Minister Colin Moynihan told the House of Commons that there would be ‘no adverse or long-term effects’.
Peter Gillman suggests that it was not until TV South West made a film ‘A Trust Betrayed’, and consulted Dr Neil Ward of Surrey University, that a plausible explanation for the conflicting evidence of poisoning and the official view that it could not happen came to light.
Ward believed that the Water and Health Authorities had misunderstood the chemistry of aluminium.
Whereas there is a lot of aluminium in substances like antacid tablets and toothpaste, this remains insoluble because acidity is low.
This makes it highly soluble and capable of being absorbed by the body.
Mixed with water, aluminium sulphate becomes increasingly acid.
The cocktail of metals that resulted from the high acidity ‘fitted the textbook on metal poisoning,’ said Ward.
The Health Authorities at this time maintained that the maximum aluminium level had been 109 mg/litre, but the Ahrenses still had their jar of water and when it was analysed at Somerset Council's laboratories it was found to contain 620 mg/litre, 3,000 times the EC limit.
Government minister Robert Freeman now had a meeting with Roberts and promised a new inquiry.
This time it was a team from Southampton University.
At first they said it wouldn't be necessary to go to Camelford, then they said it would.
The resulting Clayton Report said there would be no long-term health effects.
In June 1989, Neville Hodgkinson and Peter Gillman of the Sunday Times reported that Clive Ahrens had been found to have aluminium in his hip bone, laid down as a band (as is common in people living in areas of high lead in water).
The discovery was made not by the local health officials or the DHSS but at Ahrens' own instigation, using the services of a London specialist and Manchester University's rheumatology department which studies aluminium uptake in dialysis patients.
Dr Ward has spoken of the potential effects on children who were unborn at the time of the incident.
Mr John McGarry, a consultant obstetrician, is attempting to monitor all children from the area born after the 6 July incident.
In November 1989, following police investigations, it was announced that SWWA would be prosecuted.
In February 1990, Dr Tom McMillan, a clinical psychologist at Atkinson Morley Hospital in London, said most of his 11 Camelford patients still had their lives disrupted by the poisoning.
French holidaymakers in the area at the time of the accident still had up to four times the normal level of aluminium in their bodies almost two years later.
The ‘not invented here’ or ‘not discovered by us’syndrome runs strongly in British official circles and official inquiries are more than capable of requiring standards of rigour and a quality of proof from external or ‘independent’evidence which are never applied to the government's own sources.
In the Camelford case, the muddled official response has done little to engender confidence in the abilities of the DoE, DHSS or their local networks to deal effectively with any similar incident in future.
When in August 1988 the overstretched Health Authority belatedly sent out a questionnaire enquiring about symptoms that might have been suffered as a result of drinking the water, one went to 450 ‘occupants’ of a graveyard in St Tudy, which happened to have a tap in one corner.
According to the Guardian : ‘the rector of St Tudy filled in the questionnaire for them.
Under ‘symptoms experienced’ he wrote: ‘General symptoms of claustrophobia and inability to walk.
At times we feel as if everything is getting on top of us’.’
Whether or not the Camelford aluminium case is ever proved to have led to long-term damage to the health of people living in the area, it was certainly a major pollution incident.
The Authority reacted wrongly and secretively.
The Health Authorities and the DoE were confused and ineffectual.
In May 1989 a similar accident occurred in SWWA's region, albeit on a much smaller scale.
At Jennetts in Bideford, north Devon, on 15 May 2,300 litres of aluminium sulphate spilled when a hose came loose from a tank.
A nearby stream was polluted for 1,000 metres and 20 fish died.
And at Watercombe on 5 May the River Yealm was found to have two and a half times  the permitted level of aluminium.
After the Jennetts incident, John Lawrence, who had written the critical report on the Authority's attitude, declared his disappointment that increased safety and greater openness had not been implemented since the Camelford incident.
‘This is an unfortunate and extremely regrettable incident,’ he told a newspaper.
‘Obviously the message has not got through to some parts of the organisation.’
In June 1989 aluminium sulphate also entered a water supply intended for tens of thousands of people at Fossany Bane Works near Newry in Northern Ireland.
The works was unmanned at the weekend.
Somehow the chemical got from a storage tank into water supplies.
Under a third of the 23,000 litres stored was thought to have got into the supply system.
In August 1989 it was reported that 13,600 people in Dumfries and Galloway had been supplied with water containing up to 1,000 times the EC limit for aluminium, starting in March 1989.
‘After aluminium blocked filters at a treatment plant at Penwhirn Reservoir near Stranraer,’ wrote the Sunday Times , ‘the authority decided not to tell its customers and then issued safety advice that proved to be incorrect and inadequate.’
Friends of the Earth contacted the European Commission asking for legal action in relation to the Drinking Water Directive, because from 5 to 21 March the plant produced water with excess aluminium levels.
After 8 April the aluminium returned to tapwater when a mains burst: sludge was found at the bottom of a glass of water but when it was sent to the authority for analysis it was discarded as ‘too sludgy’.
Strangely, another sample of water divided in two and analysed both independently at the Glasgow Institute of Biochemistry and at the Water Board's Dumfries laboratory showed two very different results.
The Institute found 92 mg/litre and the Authority only 12.5 mg/litre.
But ‘there was no cover up,’ the Director of Water and Sewage was reported as saying in the Sunday Times .
People in Wigtown suffered diarrhoea, sickness and mouth ulcers.
Local doctors were said to be sceptical because the water suppliers dosed the water with a large quantity of lime to prevent soluble forms of aluminium being formed as had happened at Camelford.
However they were said to be ‘furious’ that the Scottish Office, the local community health specialist and the water suppliers had reassured people that there was no risk to health.
The 1987–9, FoE- Observer tapwater survey found 154 water supply areas with aluminium at above the EC level of 200 ug/litre (0.2 mg/litre).
Most of the aluminium contamination is due to aluminium sulphate added to water to remove cloudiness.
As a result of the ‘evidence of a causal link’ between high levels of dementia and raised aluminium (even at around half the EC limit), the Thames Water Authority has now decided to cease using aluminium in water treatment.
Dr Paul Altmann of the Kidney Unit at the London Hospital in Whitechapel is reported as saying: ‘It is when you are absorbing aluminium over many years that it is potentially damaging.
The low-level, continuous exposure arising from aluminium sulphate in water purification is very worrying.
The practice should be stopped.’
Between 1987 and 1989 London aluminium levels broke EC limits in  Barnet, Merton, Sutton, Croydon, Enfield and Epsom and Ewell supply districts.
In the Calder Valley district, the Yorkshire Water Authority's data showed the EC limits were met less than half the time in 1988.
Eccup water treatment works supplying Leeds failed ‘regularly’ and only 38 per cent of the works supplying Wales met the standard.
In Hertfordshire, Hatfield was being supplied with aluminium at four times the EC limit (800 ug/litre) at one time in 1988.
Egham reached 850, Bracknell 710 and Staines 642 ug/litre.
On 20 November 1989, the same treatment works at Camelford again polluted water with twice the EC limit for aluminium.
Other Contaminants
There are many other ‘trace’ contaminants in our tapwater besides nitrate, lead and aluminium.
Trihalomethanes, for example, are created by the reaction of chlorine, added to kill bacteria, and a wide range of organic matter, from sewage to peat.
One such substance is chloroform.
The EC has set a limit of 100 ug/litre but a US study suggests this should be significantly reduced because of a ‘highly significant relationship’ between chloroform and cancers of the bladder, colon and rectum.
West Germany has already set a limit of 25 ug/litre.
The FoE- Observer tapwater survey found that 84 areas had drinking water contaminated with trihalomethanes beyond the EC limit of 100 ug/litre, including Broxbourne, Enfield, Epping Forest and Haringey in north London, most of Devon and a large tract of the Midlands from Lincoln to Montgomery.
In December 1989, in water supplied to 500,000 people on Tyneside, phenol pollution combined with chlorine to create foul-tasting chlorophenols.
Chlorophenols are potential carcinogens.
In a similar 1984 incident, phenol polluted 2 million peoples' water on Merseyside, and although the public were told it was safe, a subsequent study found 40 per cent of consumers had at least one symptom of poisoning.
Antibiotics and hormones used in human drugs have been detected in drinking water and our rivers.
Vinyl chloride, a known carcinogen, can leach from PVC pipes into water, as can epoxy resins, asbestos from asbestos cement and PAHs (poly aromatic hydrocarbons) from the bitumen used in lining old cast iron mains pipes.
According to studies by the Water Research Centre, PAHs leached from the insides of old London water mains are present at up to five times the EC limit of 0.2 ug/litre.
PAHs are some of the same group of cancer-causing substances that are found in cigarette smoke, oil and vehicle exhaust fumes.
Replacing the old steel and iron pipes lined with coal tar would cost £1,000 million, David Wheeler of WRC told the Consultative Conference on Water Privatisation in October 1988.
Coal tar linings were banned from the 1970s onwards after advice from WRC but, said a TWA spokesman, someone would now have to foot the bill for replacing the old linings ‘and it looks like the ratepayer’.
Trichlorethylene and tetrachlorethylene used in dry cleaning and degreasing also pose threats to health, especially through contamination of groundwater, and have been detected in tapwater.
The Halcrow Report commissioned by the  DoE pointed out in 1988 that up to 8,000 people around Luton and Dunstable were drinking water with these solvents at four times the World Health Organisation's maximum recommended level, while about 80,000 drank water with trichlorethylene at 150 per cent of the limit.
In 1989, carbon tetrachloride and chloroform were found in a bore hole near the government Harwell Laboratory in Oxfordshire, affecting supplies to 3,500 homes.
Conclusions
Water privatisation, commented the Observer in August 1989, ‘has crystallised public concern about the state of our drinking water.
Most people cannot believe that private companies, with a primary duty to their shareholders, will put public health before their profits.’
The newspaper pointed out that, while Nicholas Ridley had created the National Rivers Authority to oversee the state of river water, he had refused to create a similar body to monitor tapwater and bring prosecution of polluters.
‘The new private companies,’ the Observer concluded, ‘will, in effect, police themselves.’
The prospects are not encouraging.
In 1989 Nicholas Ridley himself denounced European standards for water as ‘ridiculous, extravagant and unnecessary’.
And it soon emerged that the government had systematically arranged breaches of the Drinking Water Directive that would stretch into the next century.
During the switch from publicly owned Water Authorities to privately owned water companies, the government extracted ‘pre-privatisation undertakings’ from the Authorities.
The details of these became apparent in October 1989 after the new Act became law.
These included exemptions granted to the new water companies by the DoE, so that they did not have to comply with EC rules.
Time-limited exemptions for high aluminium, iron and manganese were granted to Northumbrian and Southern Water under the ‘nature and structure of the ground’ clause.
As these metals came from natural sources, such a move was perfectly legal.
However, it also emerged that the DoE had agreed similar exemptions for lead, trihalomethanes, coliform bacteria and pesticides.
These could well be illegal.
The UK could find itself in greatest difficulty with exemptions granted for pesticides in Southern Water's area, namely for the herbicides Atrazine, Simazine and Propazine, where a treatment facility is not planned until 1997 or 1998.
‘Under the Water Act 1989,’ commented the magazine ENDS , ‘the NRA's main powers to prevent the entry of these pesticides into water sources from diffuse discharges will only be exercisable if the Secretary of State chooses to designate water protection zones in the areas concerned.
There is little sign that this ranks prominently among the DoE's present priorities’
A few weeks later, FoE obtained a government document showing that some water supplies would now be in breach of the Drinking Water Directive until 31 December 2000, and in some cases perhaps even longer.
Again, agreements had been secretly made as ‘undertakings’ in the course of privatisation.
Thames Water was allowed ‘illegal’ pesticide levels until the last day of 2000, while in Environment Minister Chris Patten's own constituency Wessex Water  had the same plan.
No date had been set for pesticides to meet the MACs in Severn Trent.
Aluminium would be in breach of EC limits for an unknown number of years in Northumbrian Water's region.
The water supply areas where such ‘undertakings’ have sanctioned breaches of the EEC Directive are shown in the Appendix.
Previously, Chris Patten had promised EC Environment Minister Carlo Ripa di Meana that Britain would have almost all its supplies cleaned up by 1995.
One of Minister Patten's officials was now quoted by the Observer as fearing that ‘Ripa will hit the roof when he sees these documents.
Britain's apparently gratuitous flouting of the law is a very serious matter indeed.’
FoE planned to take the matter to the High Court.
Interviewed on BBC Radio 4, Andrew Lees of Friends of the Earth declared, ‘I have been given clearance by my Board to seek a judicial review…we are very serious about this issue and are going to go for it.’
So far the evidence for drinking water prosecutions has had to be wrung from the Water Authorities themselves.
Data should still be available from the new privatised water companies, but whose responsibility will monitoring be?
It seems a fair bet that the water companies will be less than enthusiastic about providing the public with evidence that will provide for their own prosecution.
Lean and Pearce wrote in August 1988, ‘In theory Local Authorities could fill some of this gap by taking their own samples of drinking water.
In practice, however, they are short of resources and manpower, squeezed by government curbs on spending.
And the water privatisation legislation weakens them even further.
It repeals a duty conferred on them by the existing regime to monitor the wholesomeness of water in their areas.
And a little known provision may enable the Environment Secretary to prohibit them from checking water quality altogether.’
To date it has been the keepers of the government's cheque books rather than those who conduct the nation's health checks who have dictated policy over drinking water.
Whereas the government publicly estimated the cost of meeting EC water standards at £3–4 billion, City sources put it at £22–32 billion over 11 years.
City analysts UBS-Philips and Drew suggested in a report released in August 1989 that the cost of meeting just the lead levels in the Directive would be £2,500 million.
It is the fear of frightening off investors that has stopped the government investing in water quality.
As the Financial Times pointed out, there are rational arguments for the government to borrow the money, instead of trying to hold down investment in order to raise the funds from private investors and short term price rises for consumers of 7 per cent above inflation or more.
‘It would be wrong for water users in the next five to ten years to be asked to finance the whole cost of projects which have been neglected in the past and which may be expected to last for 30, 50 or even 100 years.
The cost to consumers should more appropriately be spread over future generations by borrowing,’ commented the ET .
In 1987 the House of Commons Environment Committee urged the creation of water protection zones and commented that there was ‘no place for extended discussion’ between the DoE and MAFF.
Unfortunately it seems that it has  been the agricultural ministry which has won the debate, and Britain is merely tinkering with the problem.
John Mather, chief geochemist at the British Geological Survey, warns that a third of our tapwater could exceed nitrate levels within a few decades.
Nitrogen deposition from air pollution (which starts out as NOx or ammonia) is now also sufficient in parts of East Anglia to put groundwater over the EC limit.
It has been pointed out by various ‘consumer experts’ and journalists, who feel they have stumbled on a minor scandal that deserves an exposé, that many mineral waters contain significant amounts of metals and salts.
These are, after all, what gives the water its characteristic taste.
Some are hazardous: for example, nitrates and radon, which is said to have been regarded as an invigorating tonic in some Italian mineral water.
But at least the mineral waters are analysed and labelled.
You can choose whether to buy them or not, and, coming from groundwater sources, their content is pretty constant.
Tapwater is not like that.
Apart from the 1 per cent who have their own private supplies, the great majority of the UK population rely upon the decisions of the water suppliers to ensure that the great bulk of the water we drink and cook with is ‘wholesome’.
Yet, as we have seen, there are some very unwholesome things in our drinking water supplies, from pesticides to nitrates, metals and cancer-causing residues of plastic and bitumen.
As tapwater comes with no warnings, and no analysis and varies from week to week, our reliance on the judgement of the suppliers is total.
Fortunately the vague concept of wholesomeness has been overtaken by internationally agreed standards set by experts, in our case drawn from the European Community countries.
Unfortunately, the government is still taking the cheap option, not the best.
Despite new evidence that the brain damage caused by childhood exposure to lead as low as half the EC limit can affect people well into adulthood, in June 1990, the UK dropped plans to pull out old lead pipes, in favour of the cheaper but less reliable chemical dosing method.
The UK has flouted the EC's Directive and disagreed with the standards it originally agreed to.
Central government and the Water Authorities have deliberately hidden the truth from the water-consuming public by failing to conduct surveys (for example, not collating data on pesticides), using misleading types of measurement (for example , lead from tapwater running for several minutes), and simply withholding information (as at Camelford and in Dumfries and Galloway).
The result has been a loss of confidence in the purity of our tapwater.
Like the air we breathe, water is a basic element: if you can't trust what comes out of the tap, then, most people feel, something is wrong.
APPENDIX
Areas where Future Breaches of the EC Drinking Water Directive were Sanctioned by the Government in ‘Undertakings’ to New Water companies in 1989.
Thames Water
Pesticide levels will be allowed to exceed the EC limits until at least 31 December 2000.
Even by then, Thames need only check the effectiveness of current water clean-up equipment, and not actually meet the EC pesticide targets.
Areas affected: Amwell, Ardley, Ashenden, Beacon Hill, Beckley, Bedwyn, Bladon, Boars Hill, Brasenose, Bromley, Cheshunt, Chessington, Cleeve, Croydon, Culham, Dartford, Datchet, East London, Edmonton, Enfield, Epping Forest, Eynsham, Frith Hill, Greenwich, Grimsbury, Guildford, Hampstead, Headington, Henley, Henley Knapp, High Wycombe, Horspath, Hounslow, Islington, Kensington, Knockholt, Lambeth, North Swindon, Over Norton, Reading, Richmond, Sheeplands, Shotover, Slough, Tower Hamlets, Upshire, Wash Common, Watlington, Windsor, Witney, Woodstock.
Wessex Water
Pesticide levels will be allowed to exceed EC limits until 31 December 1999.
Water clean-up equipment must be installed at plants ‘where appropriate’ by that date.
Areas affected: Bath, Malmesbury, Colerne, Bowden, Devizes, Warminster, Poldens, Bridgwater, Weymouth, Purbeck, Blandford, Salisbury, Amesbury, Yeovil, Crewkerne, Chard, Wincanton, Sherborne.
Southern Water
Pesticide levels may exceed EC limits until clean-up equipment installed by 31 March 1998.
Areas affected: Andover, deadline 31 March 1996; Allhallows, Borstal, Colewood, Singlewell East, Westfield, Woolmans Wood, 31 March 1997; Testwood, 31 March 1998.
Severn Trent Water
‘Occasional’ breaches of pesticide levels allowed while improvements to clean-up equipment investigated.
No date set for completing investigation or meeting EC limits.
Areas affected: Atherstone, Bedworth, Kenilworth, Leamington, Nottingham, Nuneaton, Polesworth, Stratford, Warwick.
In other parts of the region, equipment to combat high pesticide levels will be installed by 1997.
Areas affected: Blakeney, Cinderford, Drybrook, Lydney, Mitcheldean, Quedgeley, Ruardean, Stroud, parts of Wolverhampton.
Equipment is to be installed for the Loughborough and Leicester supply area by December 1996.
At other works, water clean-up filters have to be  installed by December 1997 or the works closed.
Areas affected: Allesborough, Dunchurch, Hillmorton, Kempsey, Napton and Priors Marston, Pershore, Pirton, Rugby, Stonehall, Worcester.
Northumbrian Water
Aluminium, iron and manganese levels exceed EC limits throughout Northumberland, including Durham, Darlington and Middleton, because of distribution system faults.
A mains clean-up will be completed in unnamed priority areas by 1994, but no date is given for meeting EC limits throughout the region.
Part II
INTRODUCTION Air Pollution: Exhalations of Industry, Power and Transport
Just as Britain tried to ‘deal with’ water pollution with longer pipes and the principle of dilute and disperse, its reliance on exactly the same policy has drawn it into conflicts over air pollution.
Since the Middle Ages, British authorities have ordered chimneys to be raised in order to dilute ground-level pollution.
London's long-standing reputation for ‘fog’ was based on smoke-filled fog, or ‘smog’, in which thousands of people choked to death.
After 4,000 perished in 1952 the government introduced the Clean Air Acts, with 2,000 local Clean Air Zones.
Low-level smoke and soot were banished, as households switched from burning coal to heating with coke and electricity (and, later, North Sea gas).
In the 1960s the Central Electricity Generating Board (CEGB) replaced urban power stations with short chimneys, which directly polluted the surrounding area, with a new generation of massive power stations with very tall chimneys.
This ‘tall stack’ policy was intended to disperse pollution and dilute it to ‘harmless levels’, while smoke was to be trapped at source by simple electrostatic filters.
Problems such as London smog would disappear.
That was the theory, and it worked.
Smog of the old London type vanished, but the result was acid rain.
For 20 years, from 1968 to 1988, Britain rejected Scandinavian claims that action should be taken to reduce the UK's upwind pollution.
Britain, led by its electricity industry, waged an unprecedented war of scientific propaganda against researchers and politicians in first Norway and Sweden and later Germany and the rest of the European Community.
Even today it is trying to back out of commitments to fit pollution controls.
From the early 1980s a new type of forest decline was discovered, first in West Germany and central Europe, then in the UK, Scandinavia, North America and Japan.
Britain managed to isolate itself in international fora — such as the 35 Nation Convention on Long Range Transboundary Air Pollution convened by the United Nations Economic Commission for Europe (UN ECE)— by rejecting the scientific consensus that forest decline was linked to air pollution from industry, power stations and vehicles.
As the 1990s began, almost every western industrialised country was investing heavily in new rail links and traffic controls to combat growing levels of air pollution from traffic.
Britain, however, had fought to impede European  agreements requiring catalytic converters on cars and even as London's traffic slowed to pre-First World War speeds, planned to build vast new roads and to cut rail services.
Just as officialdom saw the legendary return of salmon to the Thames as evidence that Britain's water pollution was cured, so it gloried in the Clean Air Acts of the 1950s and turned a ‘blind eye’ to dying lakes, moribund trees and the links between car exhaust and human health that became apparent in the 1980s.
On current plans, Britain will probably remain the largest exporter of air pollution in Western Europe.
Acid Rain: Power Stations and Air Pollution
The First Acid Thursday
On the blustery, damp morning of Thursday 15 December 1983, a motley collection of protesters and student groups converged on Sudbury House, headquarters of the Central Electricity Generating Board (CEGB).
There were Friends of the Earth, Greenpeace, the Ecology Party (now Green Party), and the Young Liberals.
It was the first protest that the capital had seen about acid rain, an environmental issue which had stimulated political curiosity but very little action.
‘Biggest Acid Rain Polluter in Western Europe’ read one 60-foot message stretched across the CEGB's entrance.
Baffled office workers opened their windows and leaned out to get a better view.
‘Killer in the Sky’ said a placard, held helpfully flat so that the Board's staff above could get a better look.
The CEGB was part of Harold Wilson's ‘white heat’ of British technology.
For those times it was an environmentally sensitive organisation.
Nature reserves were set up around new power stations to mollify local conservationists.
The CEGB even had its own environmental department: ‘We invented the term ‘environment’,’ CEGB employee John Clarke remembered some years later.
By the early 1980s air pollution had been a dead issue for more than a decade.
Roses and conifers, long impossible to grow in London because of sulphur dioxide and soot, once again appeared in the capital's streets and gardens.
Pollution ‘experts’ joked merrily that the only drawback of the clean air policy was that, with less sulphur pollution, mildew had also returned.
Agricultural students learnt that the dilute rain of sulphur from power stations was just useful extra fertiliser.
Environmentalists paid it little heed: they spent the decade prior to 1983 worrying about issues such as the 1981 Wildlife and Countryside Act, the third London airport, motorways and nuclear power.
The CEGB had no pollution controls on its power stations, and produced most of the sulphur dioxide, a major component of acid rain, in Britain.
Indeed, the Board's power stations produced more pollution than any in Austria, Bulgaria, Denmark, Finland, Greece, Hungary, Ireland, Holland, Norway, Portugal, Spain, Sweden or Switzerland.
Putting air pollution, especially invisible, smokeless pollution, back onto the  political agenda, particularly in the teeth of determined opposition from the CEGB, was bound to be difficult.
The people running the CEGB came from a generation who believed in the power of technology to solve problems, in motorways and in growth of consumption.
Mrs Thatcher, a 1950s chemist, got on well with Sir Walter Marshall, a 1950s physicist and the CEGB chairman.
Both were devotees of nuclear power.
There was an inbuilt resistance to the idea that a technical fix which had solved one pollution problem had actually created a bigger one.
Yet, seemingly from nowhere, acid rain had become the world's most dramatic environmental issue.
On Environment Day 1983, Mostafa Tolba, executive director of the UN Environment Programme, declared:
‘In northern Europe, Canada and the northeast of the United States, the rain is turning rivers, lakes and ponds acidic, killing fish and decimating other water life.
It assaults buildings and…costs millions of dollars every year.
It may even threaten human health, mainly by contaminating water.
It is a particularly modern, post-industrial form of ruination and is as widespread and careless of its victims and of international boundaries as the wind that disperses it.’
Despite its apocalyptic international profile, acid rain wasn't much of an issue in Britain.
British environmental groups had only recently been roused by appeals from Sweden and Norway.
Now, like a blindfolded child trying to pin the tail on a pretend fairground donkey, the environment movement was attempting to attach the blame to the CEGB.
The CEGB, however, had other ideas.
Carrying the banner of the nuclear industry, it was well aware of what it meant to be a target for the new generation of environmentalists.
Walter Marshall had a natural flair for public relations, During the long-running Sizewell Inquiry on a Pressurised Water Reactor (PWR), he had upstaged Friends of the Earth by donating his tie to a campaign jumble sale.
Later, when serious-minded Scandinavians handed in a dead salmon, Sir Walter would rise to the occasion by sending down his chef to collect it.
When the 1983 demonstration arrived outside the CEGB, it was witnessed by just a handful of press: foreign TV, the Guardian , the Morning Star , and local radio.
Still uncertain of what tone to adopt, the campaigners brought six people dressed as Santa Claus, including, incognito behind his cottonwool whiskers, one Greenpeace Santa.
‘Ring us when acid rain burns babies,’ the Daily Express told activist Steve Elsworth.
Advice from the Daily Telegraph
In the mid-1980s the Daily Telegraph probably still spoke for the great bulk of Middle England, when, after Britain had yet again resisted international calls to reduce pollution, it announced:
‘Good news…
Britain is not going to let itself be rushed into any costly action, that would have little chance of success, to solve the problem of ‘acid rain’…
Indeed the whole theory of ‘killer acid’ from distant power
station chimneys falling in the rain where it damages trees and kills fish seems to us more like second-rate science fiction than a description of real events…
Forest ecologists increasingly believe that the source of most acid is natural, not industrial.’
The Telegraph added, inadvertently creating a spectacular hostage-to-fortune:
It is vital to avoid any repetition of the ‘ozone scare’ of the seventies when many aerosol products were banned through mistaken fears that they would deplete the earth's protective ozone layer.
This policy is said to have cost the United States economy 8,700 jobs and $1,520 m in lost profits.
We must beware of hysterical solutions to complex problems urged by people whose real motive is often hatred of industry and capitalism.
The Telegraph 's argument was wrong then and everyone knows it is wrong now.
But the Telegraph was not taking its line from saloon bar advisers.
Its editorial was almost certainly inspired by Sir Walter.
The CEGB versus Scandinavia
The CEGB was not just briefing the Daily Telegraph .
Since the 1970s it had been fending off pressure from Scandinavia.
The link between emissions of sulphur and nitrogen in industrial countries, its upwards transmission over great distances by the weather, and the consequent acidification of lakes and rivers, had first been made by a Swede, Svante Oden, in 1968.
Oden found that Sweden imported far more pollution than it exported because of the prevailing winds.
In 1972, the Swedish government hosted the United Nations Stockholm Conference on the Environment and called for international action to reduce transboundary movement of air pollution.
Britain and West Germany rejected this in favour of more research.
The CEGB dealt with the early complaints by talking-down the problem and querying the evidence.
Scandinavia was, after all, not a very important trading partner now Britain had joined the EC.
The CEGB devoted its considerable resources to submitting the claims of any acid rain damage to the greatest possible scientific scrutiny.
But this went much further than healthy scepticism.
The CEGB drew no line between science and politics, and began to act as a lobby, using science as a political weapon.
‘Scandinavians now accept that there is no evidence of damage to forests but they claim that fish lakes and rivers suffer from the additional acidity,’ said its Annual Report for 1976/7.
In 1980 the Electricity Council anticipated forthcoming battles in its Medium Term Development Plan: ‘We are aware that environmentalist pressure could lead to arbitrary restrictions on emissions.’
As to laws to reduce pollution, the Council warned, ‘The industry will strive to influence legislation before enactment.’
It added: ‘Long range drift from chimney emissions…is being studied in order to avoid arbitrary restrictions on atmospheric discharges…
Initial research suggests that at best only a very marginal decrease in the long distance deposit of acid rain would result from a major reduction of sulphur emissions from the UK.’
By picking away at every factual link in the chain from furnace to fishless lake, Britain's electricity industry hoped to avoid any restrictions on atmospheric discharges, whether ‘arbitrary’ or not.
But having been once rebuffed, the Scandinavians set about collecting more evidence.
A major Norwegian study was launched in 1972.
By 1975, Norwegian rivers such as the Tovdal, famous for salmon fishing within living memory, were turning acid and losing their fish and other life.
By 1982, the Norwegians calculated that 2,000 trout fisheries had been lost, while 5,000 lakes across some 33,000 km&sup2; had turned acid, 1,750 of them losing all their fish.
Throughout the 1980s Britain waged a war of scientific words with its neighbours.
As Nigel Haigh of the Institute for European Environmental Policy (IEEP) later remarked of electricity privatisation, one of its ‘great advantages…is that the CEGB will no longer be making not only British air pollution policy but also some of its foreign policy’.
As David Clarke MP put it, that policy had involved ‘chemical warfare’ in the form of dumping pollution on downwind countries.
It did much to establish Britain's reputation as the ‘Dirty Man of Europe’.
With Sir Walter — soon to be Lord Marshall of Goring and known also to environmentalists as ‘The Dark Lord’— leading the scientific battles of claim and counter-claim, at times the acid rain issue seemed to follow a pantomime script full of cries of ‘oh yes you did’ and ‘oh no we didn't’.
Unfortunately it was more serious than that.
What Is Acid Rain?
The main gases contributing to ‘acid rain’, and which often end up in rain or fine dry particles, are sulphur dioxide (SO&sub2;), nitric oxide (NO) and nitrogen dioxide (NO&sub2;).
Together NO and NO&sub2; are known as NOx.
Ammonia, ozone and hydrocarbons also play important roles in creating ‘acid rain’ damage.
SO&sub2; is formed when coal, oil or other sulphur bearing material is burnt.
NO is formed from fuel and air at high temperatures, such as in furnaces, power stations and car engines, In air, NO is converted to NO&sub2;, With sunlight and hydrocarbons, this forms ozone, part of photochemical smog.
But at high NO concentrations, ozone levels are low as it reacts with NO to form NO&sub2;.
Because of this ‘back reaction’, photochemical pollution is worse away from the actual pollution source itself.
SO&sub2; in air mainly becomes sulphuric acid in raindrops, sulphate or ammonium sulphate (by reaction with ammonia).
The longer it stays in the air, the more likely it is to be oxidised acid, the process speeded up by pollutants such as ozone and hydrogen peroxide.
NOx can become nitric acid.
The result is wet acid deposition (in rain), occult acid deposition (mist), and acid aerosols (the haze formed partly of ammonium sulphate particles).
About half Britain's sulphur rains down as wet deposition and half is dry fallout.
Part of the ‘acid rain’ pollution nexus is photochemical smog, a dry chemical soup including hydrogen peroxide, hydroxyl radicals, ozone, PAN (peroxy  acetyl nitrate) and acid sulphate.
Acid Rain: How Much Falls and Where
Since the 1950s a huge area of acid fallout has settled over Europe.
Britain is part of this industrial shroud.
1 In 1870 the average pH of rural rain in Britain was about 5.0. (pH is a logarithmic measure of hydrogen ion acidity: below 7 is acid, above 7 is alkaline.
Unpolluted rain is normally in the range pH 5.5 — 6.)
By 1980 acidity had increased sixfold to pH 4.3.
Nitrogen emissions had risen fourfold and sulphur three to sixfold.
Today rain falling over Denmark, West Germany, Sweden and the East Midlands of the UK is often pH 4 or less: over 40 times the natural acidity.
In terms of gas concentrations and what comes down in the rain, rural parts of central and eastern England are similar to northern Germany.
Rural parts of northern England, Scotland and Wales have similar conditions to Scandinavia.
The southeast of England has moderate concentrations of SO&sub2; and NOx and relatively frequent episodes of ozone.
Average rainfall acidity of rain in west Britain is three times that of natural rain.
In the east it is up to 16 times more acid.
However, areas such as the Lake District and Wales receive the largest total acidity because of their high rainfall.
The S. Pennines, the Lake District, Cumbria and Galloway receive as much acid in a year as the badly acidified parts of southern Norway.
The UK's highest sulphur pollution as dry deposition and gas is in the power station belts of the East Midlands, South Yorkshire and north Kent.
Much of the Midlands has SO&sub2; above the 7 parts per billion (ppb) average identified by the UN ECE as critical in preventing harm to vegetation.
The equivalent NOx levels are exceeded throughout central and southeast England, with particularly high levels around London, Birmingham and Manchester.
Cloud and mist speed up conversion of SO&sub2;, NOx or ammonia to acids.
At Great Dun Fell in Cumbria, which is under cloud for part of 250 days a year, cloud is on average four times as acid as rain at the same spot, with more than twice the ammonium, nitrate and sulphur.
The UK has 10 networks for monitoring air pollution.
SO&sub2; and soot are measured at 418 sites (the descendants of thousands set up for the Clean Air Acts) but, prior to criticism from the House of Commons Environment Committee in 1984, knowledge of other pollutants was threadbare.
By 1990 the Department of the Environment had nine ‘primary’ sites (assessing a dozen ions, NO&sub2;, SO&sub2;, and particulate sulphate) and 50 ‘secondary’ones sampling acidity in rainfall.
Ozone has been monitored at 17 places since 1985.
A NOx and ammonia network covers 60 sites and ‘rural’ sulphur dioxide 27.
Hydrocarbons are being checked in a ‘pilot’ scheme at one urban, one rural and one ‘remote’site.
Ground-level ozone, which has almost no contact with the ‘ozone layer; of the stratosphere, forms in’ peaks’or ‘episodes’, strongly influenced by  sunshine.
Levels of 60 or more can affect plants.
British ozone concentrations reflect traffic, industry (the source of NOx and hydrocarbons) and ozone clouds imported from the continent.
Ozone formation is so influenced by sunshine that even the west of Ireland and northern Scotland experience summers with ozone damage.
In most years, levels in southern England exceed the US National Ambient Quality Standard (120 ppb for one hour).
During 1976, when the weather was warm and dry, record-breaking ozone levels were detected, reaching over 200 ppb and exceeding 60 ppb on half the summer days.
Ozone occurs more frequently at higher altitudes, so uplands may be particularly at risk.
Despite the fall in low-level sulphur dioxide due to the Clean Air Acts, urban rain is still more acid than rural rain because fumes from traffic and oil-fired central heating add to pollution from more distant sources.
For example, Norwich rain has been found to be twice as acid as rain at a rural site nearby.
In February, 1985 SO&sub2; levels in London jumped spectacularly to over 1,000 ug/m&sup3; as the plume of a large power station crossed the city.
On average a fifth of the sulphur in London and 45 per cent of that in Lincoln and York comes in from rural power stations.
Trends in Emissions
Sulphur dioxide and smoke
Britain's SO&sub2; emissions peaked in 1970 at over 6 million tonnes a year.
A downward trend after 1979 was caused by industrial recession and use of North Sea gas, reaching 3.8 million tonnes by 1987.
Emissions from power stations, which account for about 80 per cent of the total, hardly fell at all.
Whereas urban SO&sub2; concentrations fell fivefold from 1968 to 1988, rural concentrations fell by only half.
By 1985, average urban concentrations were 38 ug/m&sup3; and rural concentrations were 28 ug/m&sup3;.
Without a major desulphurisation programme, any increase in industrial activity will stimulate demand for electricity and so increase SO&sub2; emissions from power stations.
Today, emissions are again on the rise.
Oxides of nitrogen (NOx)
Each year the UK emits some 2.48 million tonnes of NOx (measured as NO&sub2;).
Power station NOx pollution (32 per cent of the total) fell from 859,000 to 792,000 tonnes between 1978 and 1988, while emissions by traffic increased from 886,000 tonnes in 1977 to 1.11 million in 1988 (45 per cent).
The Department of Energy estimates future NO&sub2; emissions at up to 2.1 million tonnes by 2000.
If the targets of the EC Large Plant Directive are followed (see below), the UK will be Western Europe's largest NOx polluter in this category between 1993 and 1998.
Ammonia
Almost nobody studied ammonia emissions until recently, but estimates range from 370 kilotonnes for Great Britain to 405 kilotonnes for the UK.
Dr Helen ApSimon of Imperial College estimates that British ammonia emissions have increased by 50 per cent since 1950, mainly owing to more intensive livestock production and use of agricultural fertilisers.
Hydrocarbons
In the UK, emissions of man-made hydrocarbons or VOCs (volatile organic compounds) increased 12 per cent in the ten years to 1985, largely owing to increased road traffic.
The official Photochemical-Oxidant Review Group believes levels have doubled over the last 100 years, as a result of industrialisation.
It seems likely that with traffic growth and the possible warming of Britain's climate that they will increase.
The Pollution Export Business
On balance, Britain is a major pollution exporter.
‘From the Scandinavian countries Britain is seen as the dirty man of the acid rain affair,’ wrote the Guardian's Anthony Tucker in September 1984.
Not only was the UK the largest emitter of sulphur dioxide in Western Europe, it also exported 77 per cent of it, mainly on westerly winds to Europe.
By the late 1980s, only 19 per cent of the sulphur falling in the UK was estimated to come from other countries, the lowest percentage in Western Europe.
For example, 69 per cent of sulphur falling in Denmark, 53 per cent of that in Belgium and West Germany, 74 per cent of that in the Netherlands, and 89 per cent of that in Switzerland, came from abroad.
In 1988, EMEP (the UN ECE Convention's European Monitoring and Evaluation Programme) calculated that the UK contributed 7–11 per cent of sulphur deposited in Norway and 4–7 per cent of that deposited in Sweden.
This, however, only accounts for pollution of a ‘known origin’.
In both countries, a significant fraction is of ‘unknown’ origin or ‘background’, in the air long enough to be difficult to assign to any one source.
Of the pollution with a known origin, Britain contributes 17 per cent of the sulphur falling in Norway, around 18,500 tonnes a year.
Moreover, Norway and Sweden are large countries running from south to north, with acidification problems only in the polluted south.
In these problem areas, the UK contributed around 20 per cent of the sulphur, over half as much again as any other country.
While sulphur pollution decreased throughout most of northern Europe in the period 1978–1988 and is expected to reduce by 25–50 per cent by 1995, nitrogen pollution is increasing.
In the 1980s nitrate doubled in the lakes and rivers of southern Norway and ammonium increased.
Dr Alan Apling of the DoE has estimated that the UK contribution to NOx pollution in southern Norway could be ‘more than 50 per cent in some areas’.
By 1985, the UK was the largest exporter of NOx in northwestern Europe and the biggest contributor to NOx pollution at sites in Iceland, Ireland, Norway, Sweden, Belgium, the Netherlands and Denmark.
The Origins of a Fine Mess: The Alkali Inspectorate
From the 1880s to the 1980s, Britain's thinking on air pollution developed hardly at all.
In 1862,The Times wrote: ‘Whole tracts of country, once as fertile as the fields of Devonshire, have been swept by deadly blights till they are as barren as the Great Salt Lake.’
At St Helen's in Lancashire the newspaper reported a ‘scene of desolation.
You might look around and not see a tree with any foliage on it whatever’.
The cause was the ‘alkali works’ of the industrial revolution.
These works made soda by the Leblanc process.
The product was strongly alkaline but the by-product was acid.
Moist air burned holes in people's vegetables and landowners complained of damage to trees.
This was ‘acid rain’ in the simplest sense.
In due course, the government's response was to pass a law and appoint an Alkali Inspector named Angus Smith.
Ten years later he coined the term ‘acid rain’.
It was the start of Britain's acid rain debate and the birth of its ‘modern air pollution policy.
The approved solution under the Alkali Acts was to build taller chimneys and to remove one of the acids (hydrochloric acid) from the flue gases.
Smith advised the manufacturers on how to reduce emissions to better than the 5 per cent permitted under the new Act.
At first everyone was happy.
The Times noted ‘fruit trees which had begun to blossom after having long ceased’.
But, within 15 years, air pollution was as bad as ever.
The Act had failed to control smoke or sulphuric or nitric acid, and it set only a percentage reduction for the pollutant it did cover, without a total ceiling on pollution.
Here were the seeds of the twentieth century problem of acid rain and other crises besides.
Smith's successor transferred the concept of ‘best practicable means’ used in local legislation into national policy.
Manufacturers would be required to use these to reduce emissions, but it was never very clear how the ‘best practicable means’ would be defined.
Discussions between the inspectors and manufacturers were a trade secret, but the Inspectorate tried to avoid placing ‘an undue burden on manufacturing industry’.
Lord Derby, who steered the Act through Parliament, would have felt quite at home with the parliamentary debate which took place after the 1952 smog some 90 years later.
MPs demanded action but the government was not keen.
It pointed to the 1936 Public Health Act which gave powers to local authorities, although these were patently ineffective.
Industry lobbied against stringent restrictions.
Prime Minister Harold Macmillan told cabinet colleagues, ‘We cannot do very much, but we can seem to be very busy — and that is half the battle nowadays.’
The impression of ‘busyness’ was achieved by setting up the 1953 Beaver Committee.
Its Report was pre-empted by MP Gerald Nabarro, who proposed a Clean Air Bill.
The government then produced its own, which became the 1956 Clean Air Act, forbidding ‘dark smoke’ as defined on an objective scale.
The Beaver Committee defined 294 areas of England ‘black’, requiring smoke control, and by 1974 all but fourteen local authorities had taken some steps towards implementing the Act.
The Failure of ‘Best Practicable Means’
The Act was a success, although the ‘particulates’(mainly soot) raining down over Britain's cities had anyway been gradually decreasing since the 1920s.
By 1980, soot had fallen 80 per cent.
Unfortunately nothing was done about sulphur.
In Acid Rain Fred Pearce records that the Beaver Report had also recommended the use of flue gas desulphurisation (FGD).
It was not as if Britain was unaware of FGD — German engineers had visited a pilot scheme in Manchester in 1880.30
But the Alkali Inspectorate decided that FGD was not ‘practicable’.
Instead it opted to ‘render SO&sub2; harmless’ by ‘dilution and dispersal’from ‘tall stacks’.
A few FGD schemes were installed: at North Wilford, Battersea, Bankside and Fulham.
They were all later closed down.
Without FGD, rapid growth in electricity use led to an uncontrolled increase in emissions of sulphur dioxide.
By 1960, power stations had become the largest single source.
‘Dilution’ did not ‘render harmless’sulphur emissions.
First, it was assumed that no long-range problems would occur; that is, that the acidifying or toxic power of the gases would be neutralised long before they got to Scandinavia or anywhere else.
Second, it was assumed that the gas would be evenly mixed in the air, so that no dangerous concentrations would occur.
Unfortunately, the Alkali Inspectorate was wrong on both counts.
Even official sources now accept that tall stacks tend to increase long-range transport of pollution.
In smoothly flowing medium speed winds, plumes from power stations can travel in a concentrated stream for hundreds of miles.
Tall stacks (Drax is 259 metres tall) can take emissions above a layer of air trapped near the ground into smooth air streams.
On a steady southwesterly, a plume from Eggborough power station, ‘labelled’ with sodium hexafluoride, was tracked all the way across the North Sea to Denmark.
Such episodes are commoner than the planners of tall stacks assumed.
Pollutants can also be actively deposited by local weather effects such as mist.
Meteorological Office researchers followed Eggborough's plume crossing the Welsh mountains.
In clear skies sulphur was being slowly oxidised and deposited.
Once it entered low mountain cloud, production of acids increased 14-fold and fallout increased over 200 per cent.
The total removal rate shot up to 27 per cent an hour, depositing the sulphur in a concentrated drizzle.
Early Signs: Britain as Reluctant Witness
Acidified lakes and rivers were known to British scientists from the 1970s.
The River Solway Purification Board and the Ministry of Agriculture conducted a ten-year study into Galloway's declining fish stocks in the 1970s.
The North West Water Authority blamed acid rain for a fish kill on the River Esk in the Lake District.
In Wales, the Cammdwr and Upper Twyi became acidified affecting salmon and trout fishing.
The Forestry Commission started to lime Galloway lakes in 1982.
At first, acid from rotting conifer needles was blamed, but the average  acidity of rain in the Scottish Grampians had increased tenfold from 1962 to 1975 36 and the suspicion grew that the strong acids of pollution were responsible.
(The greater surface area presented by plantations means they may ‘scrub’ 60 per cent more pollution from the air than open ground.)
At the Pitlochry Freshwater Fisheries Laboratory, for example, researchers had measured rain of pH 2.4: more acid than vinegar.
The CEGB, nevertheless, favoured the idea that land use and not pollution was the factor responsible for the increased acidity and low calcium of Scottish and Welsh lakes.
Britain Emerges as a Pollution Exporter: Stockholm 1982
By the time the Scandinavian countries invited Britain back to Stockholm for the 1982 Conference on Acidification, the sums had been done for exchange of acid pollution between Europe's nations, showing Britain to be the biggest producer and exporter of sulphur pollution in Western Europe.
Armed with a decade of research results, Sweden and Norway proposed that the weight of sulphur falling in rain or dust on each square metre should be kept at less than half a gram (0.5 g/S/m&sup2; or 5 kg per hectare).
Outside areas with such pollution, lakes were not dying.
Within them, lakes and rivers had become acid.
West Germany, prompted by the discovery of dying forests, now supported controls.
The CEGB said a 0.5 g/S/m&sup2; target would imply ‘an impossible’ 80 per cent cut in sulphur emissions in the UK and ‘require a capital investment exceeding £4,000m’.
Giles Shaw, Britain's junior minister, joined American calls for more research and played down acidification as ‘a problem of great complexity’ affecting ‘certain limited areas of the world’.
Britain now grew increasingly isolated from its European ‘partners’.
Clinging to the US axis might be good for morale but did nothing for Britain's environmental reputation.
A German diplomat commented, ‘It is very sad.
The British have proved once more that the Channel is wider than the Atlantic.’
Britain had four arguments to justify doing nothing.
First, a lot had been achieved (other countries were only now catching up with the Clean Air Act).
Second, a large reduction in UK emissions might not create a large reduction in the pollution deposited in foreign countries.
Third, the increasing use of nuclear power would make expensive control technologies like FGD unnecessary.
Fourth, it might be more effective to reduce other pollutants, for example those from West German cars.
The Conference ended with no apparent change.
The new German position encouraged the European Community to ratify the UN ECE Convention on Long Range Transboundary Air Pollution.
Enough countries ratified the Convention to bring it into force.
This achieved little, as it asked countries only to try ‘so far as possible’ to limit and ‘gradually reduce and prevent air pollution using the ‘best available technology which is economically feasible’.
Back in Britain, Shaw reassured the House of Commons that ‘the acid rain problem’ was ‘not nearly as severe as the media make out’.
Britain now opposed a European Community bloc which included West Germany, the Netherlands and Denmark, backing controls on motor exhausts and FGD for power stations.
The ‘victims’ of acid rain had also begun to  form an alliance — the ‘30 per cent Club’, a group committed to reducing 1980 sulphur emissions 30 per cent by 1993.
It included Sweden, Norway, Denmark, Finland, West Germany, Austria, Switzerland, the Netherlands and Canada.
The 30 per cent Club: Britain Does Not join
By 1983 Britain was effectively alone in Western Europe as a large unrepentant pollution exporter.
In June 1983, the Nordic countries proposed a 30 per cent Sulphur Protocol to the UN ECE Convention.
The UK and the USA opposed it.
In June 1984, when West Germany hosted an Environmental Conference, the ‘30 per cent Club’ grew to 18.
Britain again declined to join.
‘In a speech described as brusque and arrogant by West German sources,’ reported Anna Tomforde in the Guardian , ‘British representative Dr Martin Holdgate, the chief scientist at the Department of the Environment’said, ‘We see no point in making heroic efforts at great cost, to control one out of many factors unless there is a reasonable expectation that such control will lead to real improvement in the environment.’
At Helsinki in 1985, the ‘30 per cent Club’ became official with a 30 per cent Sulphur Protocol to the Convention.
Now Britain took issue with the base year of 1970.
But, as Nigel Haigh of the IEEP has pointed out, even if 1970 (the year of peak emissions) was taken as a baseline, the UK would achieve only a 44 per cent reduction as against 80 per cent for the Netherlands, 56 per cent for West Germany and 50 per cent for Belgium.
The Weakness of the Department of the Environment
Fred Pearce says that DoE scientists returned from Stockholm in 1982 ‘convinced that Norwegian claims about acid rain were largely right and the righteous warnings from the generating board about scientific uncertainties were largely prevarication.’
The civil servants determined to convince ministers ‘that acid rain poses a real threat to forests and woodlands throughout Britain.’
In William Waldegrave the DoE had acquired an intelligent minister with a genuine sympathy for the environment.
But the DoE found the Whitehall struggle more difficult than it had supposed.
The CEGB could rely on the Treasury to oppose the expenditure to reduce pollution which did most damage in remote Welsh hills, Scottish lochs and other countries.
The Department of Trade and Industry was lobbied by the Confederation of British Industry (CBI), which feared higher electricity prices and controls on factories.
Although the Foreign Office was embarrassed at the offence caused to Britain's old ally Norway, it carried relatively little weight.
‘A modest desulphurisation programme’ was shelved owing to lobbying from the Treasury and the CEGB early in 1983.
A crucial meeting took place at Chequers on 26 and 27 May 1984.
The DoE believed it had the decision to join the 30 per cent Club ‘in the bag’ and briefed journalists to that effect.
The meeting pitched Martin Holdgate and Sir Hans Kornberg of the Royal  Commission on Environmental Pollution against Walter Marshall and Dr Peter Chester of the CEGB.
The CEGB won.
Just prior to the 1985 Helsinki conference, the DoE was once again making some progress and, at the final meeting, it was the Foreign Office which enjoyed the casting vote.
As Fred Pearce put it: ‘The Foreign Secretary, Geoffrey Howe, had the deciding vote.
His departmental brief was to assuage the diplomatic pressure from Norway and vote for joining the club.
This he did not do.
Sober mandarins from the environment department insist that the generating board engineered the debacle through Lord Marshall's ‘close contacts’ with the prime minister.’
Others have it that Sir Geoffrey missed the crucial part of the meeting because he was playing golf.
Foreign Office texts prepared for Scandinavia were hastily withdrawn.
The DoE's difficulty was made worse by its maladroit handling of the press and contacts with Britain's scientists and environmental groups.
It failed to build the essential trust needed to form an effective lobby.
Waldegrave had no seat at the cabinet table and his superiors were more interested in the privatisation of council houses.
The gentlemanly DoE lost its battles behind closed doors.
Holdgate (now at the International Union for the Conservation of Nature) is a relatively cerebral individual, ill suited to match the vigorous propaganda of the CEGB.
The CEGB's Propaganda War
In 1983 the CEGB joined the National Coal Board (NCB) in a set piece attempt to ‘frame the debate’ and fix the timetable for any decision on acid rain by creating the Surface Water Acidification Project (SWAP).
The NCB had a chairman, Mr Ian McGregor, of whom the Prime Minister approved.
It produced the coal for 80 per cent of Britain's electricity, so, like the CEGB, it had a direct interest in denying the effects of acid rain, or at least delaying action as long as possible.
SWAP operated under the aegis of the Royal Society, giving it credibility in the eyes of the press and the collaborating Norwegian Academy of Science and Letters and the Royal Swedish Academy of Sciences.
The Boards provided £5 million for research, which was not due to be completed until 1988.
Environmentalists such as Steve Elsworth warned that the real purpose of the project was to muddy scientific waters and ensure delay.
Marshall confirmed that ‘it would be unwise to take any action on retrofitting desulphurisation equipment before the results of the scientific investigation have been published.’
Compared with £1.5 billion for FGD to achieve a significant cut in Britain's sulphur emissions, it was a cheap way of buying time: a year's grace for each £1 million, about an hour's expenditure for the CEGB.
The ostensible objectives of SWAP were scientific but they also allowed for mischief.
Quantifying ‘other factors than acidity’ enabled the CEGB to push its theories at SWAP's Royal Society launch, which even DoE officials described as a ‘red herring’.
‘It has all been carefully stage-managed,’ said one disappointed Norwegian.
‘It was a sham.’
The NCB was keen to play down  existing evidence, calling it ‘conjecture and speculation’.
The CEGB extended its influence into the non-government sector, funding research by the Cathedral Advisory Commission and the Royal Society for the Protection of Birds.
The Royal Society for Nature Conservation's junior branch ‘WATCH’ undertook an educational ‘acid drops’project.
The CEGB paid for advertisements in its magazine and funded WWF publications.
The CEGB used its scientific budget to ‘shadow’ work done by others, such as the Nature Conservancy Council (NCC).
‘Every time we started doing a piece of research,’ said an NCC officer, ‘we'd find the CEGB then commissioned another research team to investigate the same thing.’
CEGB scientists became assiduous attenders of conferences.
In 1984, at the Scottish Wildlife Trust's self-consciously neutral ‘Acid Rain Inquiry’ in Edinburgh, Board researcher Ron Scriven showed a series of misleading diagrams suggesting that hydrogen ion acidity at Eskdalemuir in Scotland came ‘mostly from the Irish Sea’.
The audience of conservation managers, journalists and the public found this puzzling.
This must surely mean that acid rain was natural and the acidification of lochs had nothing to do with power stations.
This would have remained the impression if Scriven had not been followed by Dr David Fowler of the Institute of Terrestrial Ecology (ITE).
Fowler had similar basic data but presented in a quite different way.
Instead of a simple wind diagram, he had ‘back-projections’ of the actual path of the rain-bearing wind over the previous 48 hours.
The ‘acid rain’ had passed over industrial areas of the UK, France or West Germany.
Scriven had neglected to mention this in his presentation.
The CEGB was very concerned to block any suggestion of a direct link between power station emissions and acid fallout in rain.
Such a link was made when on 20 February 1984 a heavy fall of black snow took place in the Cairngorms in Scotland.
The winds that carried it there had passed directly over the power stations of South Yorkshire and the ‘sulphur valley’ area around the Trent, where there are 12 large power stations.
On the same day, fish farmers in the West Highlands where rain fell, reported the death of trout.
The black snow, full of oily soot, had a pH of 3, several hundred times more acid than normal.
News of the black snow did not leak out until Scots journalist George Rosie reported it in the Sunday Times on 19 September.
There were rumours that a team from the University of East Anglia who were researching the acidity of the Cairngorm snow pack had run into difficulty in getting their work published because CEGB staff were used as anonymous ‘referees’ to vet papers submitted to several scientific journals.
Eventually it was published in Nature .
Trevor Davies of the UEA team estimated that at least 20 tonnes of soot fell on the Cairngorms that night, forming a layer of snow at least 5 cm thick and affecting an area of over 200 km&sup2;.
In 1985 the CEGB spent £200,000 to produce an ‘educational’ video on acid rain.
It featured a Norwegian soil scientist Rosenqvist, who was almost alone in his belief that land use processes not air pollution were causing the  acidification of lakes.
It also included an interview with fisheries biologist Hans Hultberg, which had been edited to make it seem as if liming was a perfect answer to acidified salmon rivers.
‘It is obvious that I have been exploited for propaganda purposes in a film in which I should never have allowed myself to appear,’ he bitterly commented afterwards.
Mrs Brundtland, Norway's Prime Minister, complained about it to Mrs Thatcher and New Scientist reported that even the CEGB's own scientists ‘discreetly let it be known that they were annoyed by the final version of the video.’
In the film, one of them, Dr Tony Kallend, appeared to explain that acid rain was nothing to be worried about as it was only as acid as Coca Cola.
By Christmas 1985 the DoE publicly sought to distance itself from the CEGB video, acknowledging that ‘the film attempts to minimise the British contribution to acid deposition in Norway when it is much the largest.’
Confirmation Denied: CEGB Ignores Battarbee Research
In 1981 the CEGB had commissioned Dr Rick Battarbee, a geographer at University College London (UCL), to study diatom microfossils in acid lochs.
Silica-rich skeletons of these single-celled planktonic plants sink to form layers in the lake mud.
As the diatom species differ in their sensitivity to pH, it is possible to ‘read’ the layers to reconstruct the lake's history.
The CEGB's video did not mention this study, which had effectively quashed any scientific doubt over what caused acidification.
 Batterbee and his team tested six hypotheses with unimpeachable rigour: the first was that Scotland's lakes were naturally acid and had not changed; the second was that acidification was slow and pre-industrial; the third blamed changes in burning or grazing; the fourth implicated just conifer planting, and the fifth, decreased agricultural liming; only the sixth implicated acid deposition from the combustion of fossil fuels.
In Galloway, Battarbee and his team found clear evidence of a recent and dramatic change in pH.
They studied six lakes, including some with afforested and some with treeless catchments, all lying on granite bedrock with little buffering capacity.
Five had become significantly more acid in the previous 130 years (0.5 — 1.2 pH units).
Round Loch of Glenhead showed an eightfold increase in acidity.
Scientifically, the evidence was devastatingly comprehensive.
The diatom analysis was sensitive enough to pick up land use changes: lake records detected pollen change, showing variation in vegetation, but it was the coincidence of the onset of acidification with the industrial revolution, its rapid acceleration after the Second World War when the consumption of fossil fuels leapt up, and the appearance of industrial metals, which made the conclusive link.
Finally, there were the microscopic soot spheres, produced by high temperature combustion, such as in the boilers of power stations.
Only Dr Peter Chester of the CEGB still seemed to find the evidence wanting.
Interviewed by Central TV's documentary ‘The Acid Test’, Chester declared that ‘You cannot possibly, by observing diatoms, be able to say it is definitely due to power stations or to any other cause.
What you can say is that  there's been acidification.
Of course you'd expect to find fly ash at any period since people began burning coal in quantity.
You cannot deduce from the fact that it was happening at the same time that it was a cause.
That you cannot do!’
In the CEGB's tangle of arguments, this was perhaps the most bizarre twist of all.
When the CEGB agreed the hypotheses to be tested with the UCL geographers, every conceivable explanation had been included.
Yet now here was the CEGB disregarding the logic of its own research plan.
Exaggerating the Costs of Action
Confronted with evidence of damage, the CEGB tried to frighten industrialists with visions of enormously inflated electricity bills as a result of FGD, and to impose a ‘Judgement of Solomon’ on environmentalists which meant ‘accept either acid rain or nuclear power’.
The industry publication Electricity Supply and the Environment cited an ‘undoubtedly huge expenditure’.
The Board gave FGD costs as the increased cost of generating electricity at particular stations and not as that delivered to the consumer in electricity prices.
The Electricity Council stated: ‘the cost benefit ratio of installing FGD…would be very high as it is anticipated that the generating costs of a power station fitted would rise 25–30 per cent…’
In fact, electricity bills include many things other than actual generating costs — wages for example.
And not all electricity comes from power stations requiring FGD.
When these factors were accounted for, the increase in electricity prices shrank to 3–6 per cent over ten years (i.e. 0.3–0.6 per cent a year).
This the Board failed to make clear.
So, for example, on 18 March 1984 Sunday Times journalist Brian Silcock unwittingly wrote: ‘according to the CEGB, even a 50 per cent reduction in its own sulphur emissions would add 15 per cent to electricity prices.’
Following correspondence with Steve Elsworth, Silcock wrote, ‘I feel…that the CEGB was a bit disingenuous in giving increases in generating costs.
It was all too easy, as I did, to equate those with electricity prices.’
The CEGB did its best (for example in its 1984 Acid Rain ) to promote nuclear power as ‘the answer’ to acid rain.
It fully expected a massive expansion.
A cabinet minute for Tuesday 22 October 1979 records how Mrs Thatcher approved a plan for 15 one giga watt (1 GW) nuclear power stations to be built from 1982.
At a rate of more than one a year, Britain was to get 15 Pressurised Water Reactors.
As it turned out, by 1989 Britain got only the half-built Sizewell B.
But work by Earth Resources Research for FoE showed that, as a means of reducing sulphur pollution, building a nuclear power station would be ten times as expensive as fitting FGD equipment to an existing coal-fired power station.
Furthermore, building enough to replace sufficient coal-fired stations to meet the draft EC Directive meant an impossible construction programme.
Nevertheless, the Board enjoyed a generally good press.
The Air Pollution Inspectorate and the CEGB
The evidence didn't fade away, and Britain's legacy of Victorian pollution controls came back to haunt it.
The Alkali and Clean Air Inspectorate spelt out its wobbly rationale for taking no action in its 1981 Report, saying that its powers were ‘intended to protect the population and environment of England and Wales’, so ‘it is unlikely that action could be taken under the legislation solely to protect the environment of other countries.’
However ‘new fossil fuel generating plant’ might warrant controls.
When Marshall and his CEGB colleagues appeared before the 1984 Commons Environment Committee they were able to point to the vague Alkali Laws to justify their position.
‘The CEGB is required to use ‘the best practicable means’,’ he noted, but for power stations the Inspectorate specified no FGD technology.
‘As drafted,’ the law made no mention of cost.
‘It implies only that if a control measure is deemed to be practicable’ then it must be adopted.
But it was ‘well known’ the Board said, that the ‘financial impact’was considered in defining ‘practicable’.
What is less clear is the extent to which a demonstrable need must be proven before they insist on the adoption of otherwise practicable control measures.
The Inspectorate is quite likely to take a precautionary attitude, where the cost/benefit ratio of a proposed control measure is still obscure but where the costs, in its estimation, ‘do not represent an undue financial burden.’
In other words, so long as it was cheap, the precautionary principle might be applied.
But, if the measure was not cheap, the policy had nothing to say.
Here was a classic British fudge.
The administration of vague rules and guidelines effectively removed control of pollution policy from the politicians who had framed the laws, and put a stop to any parliamentary scrutiny.
The secretive negotiations between administrators and polluters meant there was no public accountability.
The word ‘practicable’ was made of plasticine.
The CEGB revealed that’ all fossil fuelled stations built since the formation of the CEGB were subject to the condition that space must be left for the later provision of sulphur removal equipment should the Secretary of State require this to be fitted.
In over thirty years, this condition had never been invoked.’
So, although either the Air Pollution Inspectorate or the Secretary of State for Energy could require FGD to be fitted, this had never happened.
Leslie Reed, then Chief Inspector, told a House of Lords Select Committee in 1982 that the Inspectorate would ‘almost certainly’ require methods to reduce sulphur at new power stations but this had no immediate effect.
There could be no doubt that FGD was practical: it was widely employed in Japan, the USA had 88 units and 40 more under construction some two years earlier, and West Germany had announced plans for a retrofit of 200 power plants and other large boilers in 1983 giving over 80 per cent FGD on power plants by 1988.
Altogether FGD was fitted in more than 1,000 power plants all over the world, some with equipment made in Britain.
‘Practicable’, however, didn't just mean that it could be done.
The DoE said  ‘best practicable means’ meant ‘both technical and economic feasibility’.
The Inspectorate applied these principles in two steps, said the DoE.
First it would try to prevent emissions, second to render them harmless if prevention was not practicable.
‘In the case of SO&sub2; and NOx emissions from combustion processes,’ said the Department, ‘prevention, or partial abatement, has hitherto been regarded as impractical on grounds of cost, and operators are therefore required to render ground-level concentrations of the emissions harmless by dispersal from high stacks and dilution in the air.’
But acid rain and sulphur dioxide episodes both occurred.
Emissions were not harmless.
The government could and should have required FGD on all its old power stations but it didn't.
However, the EC soon had other ideas.
The EC Large Combustion Plants Directive
The EC's Draft Directive on Large Combustion Plant was first published in December 1983.
The initial proposal was for a 60 per cent cut in sulphur dioxide and a 40 per cent cut in dust and NOx, on 1980 levels, by 1995.
In the UK, the category was more than 80 per cent accounted for by power stations.
The Draft Directive implied an active clean-up of 12 existing UK power stations, not just new ones.
It was inspired by the FGD programme that West Germany had imposed on its own industry and power suppliers.
Nigel Haigh of IEEP notes that Britain soon became ‘the most consistent opponent of significant early reductions’.
Environment and parliamentary groups pressed repeatedly for the UK to drop its opposition to the Directive.
The 1984 Report of the Commons Environment Committee, chaired by Conservative Sir Hugh Rossi MP, prompted the first full-scale public and political debate on acid rain.
Britain, said the MPs, should immediately join the 30 per cent Club and the CEGB should fit FGD to meet the Draft Directive on Large Plant.
‘We firmly believe that the Government's present position pays too little heed to the weight of scientific evidence in Britain and in Europe that sulphur dioxide, nitrous [sic]oxides and hydrocarbon emissions are separately and in conjunction destructive to many natural and built environments.’
They were ‘deeply disturbed’ by Britain's position on acid rain as a whole.
Small signs of a Conservative Party ‘turquoise tendency’ had emerged from the Bow Group which criticised the UK for ‘trailing lamely along at the back of the international pack’and proposed a new Clean Air Act to halve the output of SO&sub2;.
It felt this would have ‘electoral as well as environmental’ advantages.
The all-party House of Lords Select Committee on the European Communities issued a similar report to Rossi's.
‘It would be foolish and dangerous for no action to be taken to combat air pollution,’ said their Lordships, advocating that Britain should reduce sulphur dioxide emissions by 30 per cent.
The government rejected all these advances.
Instead, in December 1984 it announced that a 30 per cent cut was becoming ‘an aim of policy’.
What, many people wondered, was an ‘aim of policy’?
It was a wish or a hope but not a policy that would be implemented.
After much bitter debate, in March 1986 Britain quietly announced that it  would be, reviewing’ policy following scientific advances made in the SWAP programme.
In July, the CEGB announced that it proposed to retrofit three 2000 MW power stations with FGD between 1988 and 1997.
It would not be enough to join the 30 per cent Club, but it was a U-turn.
In September, Marshall admitted that the CEGB was partly to blame for acidification of Scandinavia.
He accepted that evidence of deep soil acidification and the discovery of a large soil reservoir of sulphate meant pollution had caused wholesale ecological change.
Unfortunately, the EC Directive negotiations were being separately conducted from the UN ECE negotiations and the UK's talks with Scandinavian countries.
In 1986, the Dutch had the EC Presidency and proposed a cut of at least 50 per cent for West Germany and others, with 40 per cent for Italy and the UK.
‘The Commission and West Germany thought this did not go far enough — others thought it went too far,’ says Haigh.
Britain was one of those who thought it went too far.
It was rejected.
In 1987, Denmark proposed even more favourable terms to the UK, but still no agreement was reached.
The Board was in no hurry to cut pollution despite the UK's U-turn.
Marshall suggested that a quarter of all the acid rain which had fallen on Norway in the previous century had come from Britain.
But even if all the power stations were shut down immediately, acid and sulphur would continue to bleed from the soil for years to come.
The weary Norwegians rejoined that, even if the rate of deposition was falling, any extra sulphate entering soils already depleted in calcium would make acidification worse.
In 1988 West Germany proposed tighter limits for the EC directive.
These, said the DoE, were ‘a serious step backward’, The DoE claimed that the low sulphur coal (under I per cent) which would be needed was ‘not readily available’.
Finally, in June 1988 there was yet another version of the Directive.
By now, the negotiations had been a long and tedious process.
The result, just as a camel is a horse designed by a committee, was a Directive with a separate emission programme for each country.
After five years' argument, the UK agreed terms.
The Directive gave the UK something of a ‘soft landing’, with only a 20 per cent cut on 1980 emissions required by 1993, and just 60 per cent by 2003.
The UK's intransigence had therefore won an eight-year delay.
In 1982, well-informed observers had believed the CEGB's scientific propaganda was intended to win breathing space until more nuclear plants came on stream at the end of the century.
Old coal stations would be pensioned off, and any new ones would have FGD put in more cheaply.
So, despite seeming to lose every argument, the CEGB probably came close to achieving its original goal.
In the end, the environmentalists had almost no impact on real action.
Spain aside (in second place), Britain's sulphur output from ‘Large Plant’ in the year 2000 will now dwarf that of the other Western European countries.
It will be over a quarter of that for the entire EC, more than twice West Germany's and equivalent to those of France, Belgium, Denmark, Luxembourg, Greece, Ireland and Portugal added together.
Even though the Directive was agreed, Britain's policies have still caused controversy.
Britain was granted a specially lenient deal because Environment Minister Lord Caithness argued that it could not fit FGD in the required timescale, and that it had domestic high sulphur coal which it ‘had to use’ whereas other countries relied mainly on imported low sulphur coal.
At the same time, Caithness argued that the impact on the coal industry would be ‘devastating’ if low-sulphur coal had to be imported to meet the stricter emission limits being proposed by West Germany.
Meanwhile West Germany, Denmark, the Netherlands, France and Belgium have to reach progressive reduction targets of 30 per cent, 60 per cent and 70 per cent , even though the UK's total emissions are already much higher and, unlike countries such as West Germany, it had invested in no controls to date.
As late as December 1989 Patten's junior Environment Minister David Trippier said that ‘at least 12,000 MW’ of power station capacity would receive FGD to meet the Directive.
But as ENDS magazine pointed out, by February 1990 the electricity industry had committed itself to only one third of this: 4,000 MW of FGD.
On 20 February, the Secretary of State for Energy, John Wakeham, downplayed the role of FGD in meeting the sulphur reductions, telling the House of Commons that FGD was ‘an important means’ of cutting SO&sub2; but ‘there were other means too’.
He also said: ‘The essence of the Directive is a commitment by member states to reduce their emissions of acid gases.
It is not a commitment to any specific form of abatement technique.’
The government implied that it would allow the privatised electricity industry to meet the reductions in the Directive by importing low-sulphur coal and by building gas-fired power stations to replace coal capacity.
Robert Malpas, chairman of the privatised power company Powergen, was quoted as saying that he would like to ‘get away’ with fitting FGD at just one of his power stations.
This, however, was not the basis of the negotiations on which Britain was granted its special deal.
The government now denied that it was granted the terms it received under the Large Combustion Plant Directive on the basis that it would be fitting FGD so it could continue to burn high-sulphur coal.
In a letter to Greenpeace acid rain campaigner Andy Tickle, D. Aspinwall of the Air Quality Division in the Department of the Environment wrote on 20 March 1990: ‘You suggest that the Government has changed its plans for implementing the large combustion plants Directive.
As you are probably aware the Directive does not specify how emissions from existing plants should be reduced.
The Government has always envisaged that a range of measures would be needed.’
In other words, the plans had changed, but the government wanted to hide the fact.
Not only environment groups and analysts such as Nigel Haigh of IEEP are sceptical of the government's claims.
The Financial Times Survey of the Electricity Industry noted: ‘The British government and the generators are  retreating from their commitment to the European Commission to put sulphur-scrubbing flue gas desulphurisation (FGD) equipment on 12,000MW of coal-fired plant in England and Wales at a cost of £2bn.’
The Financial Times identified the government's desire to oil the progress of the electricity privatisation by reducing investment in pollution control.
‘The Government is warned that privatisation proceeds would be severely reduced if the full FGD commitment were repeated in the prospectuses of National Power and PowerGen,’ said the newspaper on 29 March 1990.
National Power subsequently indicated that low-sulphur technology would be installed at Didcot, Tilbury and Ironbridge.
West Thurrock and Ironbridge have already been converted by National Power.
Powergen has converted Kingsnorth, but cancelled plans to install FGD at Fiddlers Ferry.
It might fit FGD at Ferrybridge and Ratcliffe on Soar.
The use of low-sulphur coal reduces sulphur output by only about 50 per cent, compared with the 90 per cent removal that can be achieved with FGD.
Using low-sulphur coal is therefore only a short-term economic palliative and will not reduce UK sulphur pollution to below critical levels.
Reneging on the sulphur agreement is likely to perpetuate Britain's bad environmental reputation in Europe.
Nigel Haigh commented in a letter to the Independent on 20 February 1990:
‘If Britain now decides to meet the legally binding percentage reductions by importing low sulphur coal, or by burning gas, it will be accused of having twisted the other member states.
There are two courses open to the British government if it is to maintain its reputation, on which it prides itself, of honouring its commitments.
Either it must achieve the reductions it agreed to by using technological means to remove the sulphur from domestic coal, or, if it chooses to see low-sulphur coal being imported, or gas burned in power stations, it must endure reductions in emissions closer to those agreed by West Germany, the Netherlands, France, Belgium and Denmark.’
Following the announcement, the government began a news-management campaign designed to talk away the problem.
The Prime Minister stated in March 1990 that it was privatisation that had enabled the sulphur cuts to be made in other ways.
This comment would be bound to confuse, as the government was well aware of impending privatisation when she made her previous statement that the UK would fight FGD.
She told an audience at the Royal Society, ‘Let me confirm unequivocally…the United Kingdom will meet the commitments which it has solemnly accepted to reduce acid emissions.’
She did not mention the basis of the deal.
Which was strange, as she had spelt it out very clearly less than a year before, in a BBC TV interview.
Mrs Thatcher told the BBC's Michael Buerk:
‘Now the quickest way to diminish acid rain would be to say, ‘Right, we will not use any British coal, we will use coal from overseas which has no sulphur.’
Now I hardly think that would have been acceptable, either to the people of Britain, let alone to the mining areas, or to some of those
European Commissioners who are critical.
So we didn't go about it in that way.
We said we will try to extract that sulphur…so we are spending a fortune, well over a billion in the first instance, and then another £600 million after that, to try to take the sulphur out.’
Could it be that the Prime Minister didn't want to admit to a U-turn, to reneging on a commitment to European partners, ‘some of those European Commissioners’, ‘the mining areas’ and ‘the people of Britain’?
Relying heavily on a low sulphur coal strategy in place of FGD, may create problems which could yet undermine even the short-term economic case for such a policy.
It could create dependence on unreliable imports (as the world market for low sulphur coal is extremely susceptible to sharp increases in demand and expensive because of its popularity in the USA), adversely affect the balance of payments, be vulnerable to changes in the value of the pound, increase unemployment costs from the mining industry, and lead to the geological abandonment of UK mines, thus making it a difficult policy to reverse.
Reducing Sulphur and Nitrogen Emissions
Over 180 different systems of FGD exist worldwide, at least 30 of which are used commercially.
Following privatisation it is probable that Powergen and National Power will fit the limestone/gypsum process to 12,000 MW of capacity (6 large stations).
This mixes wet limestone with the hot gases.
Calcium sulphate (gypsum) is formed, removing up to 90 per cent of sulphur.
Other systems do not need large amounts of limestone and produce usable pure sulphur or acid rather than waste.
Gypsum is widely used in the building trade as ‘Plaster of Paris’.
The UK market of about 3 million tonnes could be supplied by six FGD-fitted large power stations.
Power station NOx can be reduced with ‘Low NOx’ burners (now being fitted at 12 of the largest coal-fired power stations), which have a different type of flame and can reduce NOx by 30–40 per cent.
The cost is low.
For NOx reductions of 80 per cent or more, the USA, Japan and West Germany have opted for SCR (selective catalytic reduction), in which ammonia is injected into the superheated region of a boiler or outside it, in a catalyst bed at around half the cost of FGD.
One process known as ‘NOx Out’ uses urea and achieves up to 80 per cent reduction at a quarter the cost of SCR.
It is fitted in West Germany and the USA but not in the UK.
As of 1988 the UN ECE Convention included a NOx Protocol, under which UK emissions must not rise above 1987 levels: this will probably require power station controls as well as limits on traffic sources.
Pollution Targets and Critical Loads
Critical loads mark the limit of nature's ability to tolerate acid rain.
Before they were defined, there were no ecological targets for pollution control, only political ones.
The 30 per cent Club was an unashamedly political target, a ‘first step’ initiative.
‘Critical loads’ and ‘critical levels’provide a rational basis for  pollution control, if targets are set to meet them.
The 0.5 g/S/m&sup2; Stockholm target was an early definition of a critical load.
Jan Nilsson, one of the originators of the concept, has defined a critical load as ‘The highest load that will not cause chemical changes leading to long-term harmful effects on the most sensitive ecological systems.’
Critical loads have been calculated for nitrogen, sulphur and hydrogen ion acidity (taking into account rainfall and soil type) by international teams of scientists.
Soils high in carbonate (e.g. chalk, limestone) have high ‘alkali production’, so acids will be mostly neutralised and will have a high critical load.
But, in Britain as in much of Europe, soils change markedly over a few miles or tens of miles, so targets must be set to protect the most sensitive soils, such as those based on sand or overlying hard, naturally ‘base-poor’ rocks.
The target reductions required to bring sulphur pollution below the critical loads will vary according to present pollution.
In southwestern Scandinavia it will be 80–90 per cent, while in some parts of Europe the deposition is four times as great, indicating a reduction above 90 per cent .
In the UK, sulphur and nitrogen will need to be reduced 90 per cent to safeguard all sensitive ecosystems.
For freshwater the key criterion is pH.
Once it drops below 5.3, increased solubility of aluminium and other chemical effects lead progressively to the decline and loss of fish populations (see below).
For the most delicate lake systems of Sweden and Norway even a 100 per cent reduction in deposition is not expected to return them to life; 3–7 per cent of the lakes in Norway's 1,000 lake study are in this super-sensitive category with a ‘zero alkalinity’ at 100 per cent reduction, and will require liming.
High rainfall, coniferous vegetation, steep slopes, and coarse, sandy, free-drained, shallow soil with few chemical bases (e.g. calcium, magnesium) and a low capacity to adsorb sulphate all make soils more sensitive to acid and give a low critical load.
Critical loads for forest soils agreed by the UN ECE's expert working group in 1989 range from 3kg/S/ha (O.3g/S/m&sup2;) on granite, through 8–16 kg/S/ha for moderately easily weathered soils, to over 32kg/S/ha for limestone or chalk.
Large areas of the English Midlands receive several times the critical loads for even moderately sensitive soils.
Only northeast Scotland, parts of the southern Uplands and a few areas in the southwest receive less than 10kg/S/ha a year, and in the 1970s deposition was higher.
When soils receive more than the critical load, the natural capacity to neutralize acidity is overwhelmed, cations (e.g. calcium) are leached out, phosphorus is retained and aluminium and trace metals appear in the soil water.
The result may be nutrient deficiencies, toxic levels of metals, and a long-term drop in pH.
For British lakes with limited alkalinity, critical loads of sulphur are put at 3–6 kg/S/ha, a level exceeded over most of the country.
The load is set so as to maintain a pH of at least 5.3, the minimum for a bicarbonate rather than an aluminium buffering system, with aluminium levels of 30 ug/litre or less.
In Wales, Plynlimon receives up to 16kg/S/ha and the Berwyns 31kg/S/ha, that is six times the critical load.
The highest values have been recorded at Holme  Moss in the Pennines (43kg/S/ha, eight times the level) and at a site in Galloway with 31.4kg/S/ha.
Unpolluted ecosystems release only small quantities of nitrate in streams.
At high deposition, nitrate is found leaching away because the soil processes and capacity of plants to take up nitrogen are exceeded.
Low-productivity ecosystems such as heaths, peat bogs, chalk grasslands and many ancient woodlands have a lower critical load than productive systems such as commercial forests or reed swamps.
(The acidifying effects of nitrogen and sulphur are additive, so high nitrogen and sulphur deposition should really be considered together.)
Previously unpolluted ecosystems begin to change almost as soon as extra nitrogen arrives because the plants and animals living there are adapted to low-nitrogen conditions.
Most rare and threatened flowers and other plants cannot tolerate high nitrogen.
Many of the biological effects involve direct toxicity, and are levels as much as loads.
Nitrogen pollution is now high enough to affect ecosystems throughout England and Wales.
Only part of central northern Scotland may be below the critical load for sensitive plant communities such as those around some lakes and peat bogs.
Unless nitrogen pollution is significantly reduced, many of the rare plants for which conservation sites were designated or nature reserves were set up will disappear.
Research in the Netherlands, West Germany and Denmark has shown that at 20kg/N/ha heather gives way to grassland.
Forest decline may be a similar process involving trees.
The southern Pennines has the highest calculated load of nitrogen at 25 — 55kg/N/ha, John Lee from Manchester University and Sarah Woodin of the NCC have shown that sphagnum peat bogs first killed off by sulphur dioxide now cannot recover because of high levels of nitrogen.
Tessa Robertson of WWF comments that critical loads present ‘a stark picture for some of Britain's most valuable natural habitats’.
Soil Acidification
Traditionally, soils were thought to have such enormous chemical reservoirs that they could buffer any realistic pollution impact but recent evidence shows they are much more vulnerable.
In 1984 90 soil profiles sampled by Professor C. O. Tamm in 1927 at Tonnersjoheden Forest in Sweden were re-examined by his son.
Acidity had increased up to 0.9 pH units in the upper level and 0.7 units at a metre depth.
In some places the pH dropped 1.5 units.
Goran Persson of the Swedish Environment Protection Board says such ‘acidification…was not even considered conceivable’ in the 1970s.
Scientists from Aberdeen University have now found that sulphur pollution caused deep soil acidification between 1948 and 1988 at Balmoral, even though the area is remote from industrial centres.
‘The same workers,’ says the Nature Conservancy Council, ‘have demonstrated that acid deposition has caused widespread acidification of upland peat.’
An experiment near Bristol showed pollution downwind of a coking works  could increase acidity tenfold in just 30 months.
Even soil with a ‘fairly high clay content’ became acidified at depth.
Dry deposition of sulphur dioxide may, says a DoE review, also ‘cause rapid surface soil acidification (over months rather than years)’.
Ammonium sulphate is also a powerful soil acidifier.
In the mid 1980s it was also discovered that the naturally leached soil of most of upland Britain is vulnerable to ‘sulphate saturation’.
Eventually this encourages displacement of neutralising substances and any further sulphate input, as in rain, causes calcium or other cations to flow out and, when these are exhausted, toxic aluminium.
A striking Norwegian experiment, in which a glass roof has been put over a whole catchment, shows the degree of such ecosystem damage.
When polluted rain was substituted by clean rain, the ‘bleeding’ of sulphate from the soil reduced 50 per cent but after 36 months it still continued at up to seven times the rate of input.
In an unpolluted area, over 80 per cent of the added sulphate was retained in the soil.
In Britain, a DoE group has now estimated that to return Britain's soil and freshwaters to a pre-pollution state, a 90 per cent reduction in sulphur is required.
Damage to Freshwaters
The controlling factor of acidification is pH but the main toxic substance is aluminium.
Its solubility and hence biological impact is strongly influenced by pH.
Permanently acid lakes have a pH below 5.6 and alkalinity close to zero because calcium and magnesium have been washed from the system.
Many invertebrates, such as crayfish, cannot survive without a certain level of calcium, and are completely absent.
High levels of aluminium — toxic to fish in several chemical forms — are often present.
Other vulnerable lakes endure episodes of acidity in which pH drops below 5.6 when a rainstorm or snowmelt flushes acid, sulphate or aluminium into the catchment.
Such episodes kill fish.
Aluminium hydroxide forms on their gills, mucus is secreted and the fish choke to death.
Fish eggs and invertebrate larvae may be more affected by episodes of acidity than adult organisms.
In particularly acid seasons, no young may be recruited.
It may then take decades for a long-lived species like trout to die out completely.
The third category of river or lake is never acid because of high alkalinity. pH is usually above 6 and the catchment has well buffered soils.
Below pH 4.5 the main form of aluminium present is the Al ³+; ion.
At pH 3 — 4, aluminium combines strongly with organic matter — such as humus or peat — so that it is less of a hazard in peaty waters.
Fish deaths in ‘brown’(peaty) rivers are usually due to hydrogen ion acidity.
Fisheries' records show that 22 Welsh salmon rivers declined between the periods 1966–75 and 1976–85.
Of these, 20 had become acid.
Fish deaths resulting from acid surges have also been observed on the rivers Esk and Duddon in Cumbria, and Glaslyn in Snowdonia.
But a declining fish population may become increasingly dominated by young fish or, more often in lakes, a few large old fish.
Anglers paying only occasional visits are unlikely to realise that anything is wrong until all the fish disappear.
It seems likely that, if Prince Charles hadn't made it famous through his children's book The Old Man of Lochnagar , nobody might have known that, by 1984, only 13 trout survived in this highly acidified loch.
A surer way of assessing how far lakes have turned acid is ‘diatom analysis’ as used by Dr Rick Battarbee and his colleagues at UCL.
The UCL Studies
After showing how Galloway Lochs had become acidified, Battarbee has now produced similar evidence for 17 Scottish lochs, 5 English and 8 Welsh lakes.
Many show more than a tenfold increase in acidity since 1850.
The recent sediments contain soot characteristic of fuel-oil, which has been extensively used only since 1940, and magnetic particles which, says Battarbee, ‘are predominantly spherical and can be identified as power station fly-ash.’
The fly-ash particles only appear in sediments after 1945, rising steeply in the 1970s.
At Lochnagar, a nature reserve at 785 metres in the Cairngorms, fly-ash appears separately from fuel-oil soot from the 1960s onwards.
This, says Battarbee, ‘might, therefore, be an independent measure of the impact of large, tall-chimney, coal fired power stations built in the United Kingdom since 1960.’
Today, Lochnagar has a pH of 5.
In 1800 the pH would have been 5.9.
The nineteenth-century microfossil record shows a decline in plant plankton such as flourish in neutral conditions and an increase in acid-tolerant species.
At Loch Laidon, which lies in Rannoch Moor National Nature Reserve, the whole catchment is designated a Site of Special Scientific Interest.
Here, contamination with metals, says Battarbee, occurs ‘about ten years before the first evidence of acidification’, which suggests that it took only ten years to exhaust the lake's capacity to buffer pollution.
Acidified lakes also occur throughout mid Wales and Snowdonia.
Llyn y Bi is a small 3 metre deep lake which lies at 445 metres in the Rhinogs of Snowdonia National Park, the most acidified mountain range in Wales.
Pre-war records show that this little lake held good stocks of the rare arctic char (a fish of the salmon family) and brown trout.
The remains of an old boat house stand on the shore.
But now, says Battarbee, it ‘is virtually fishless’.
The lean ecology here means the entire 150 years of diatom layers are just 5 cm deep.
In this shallow bed of microfossils, Battarbee reads a progressive acidification from 1870.
After 1900 acidity increased tenfold.
As at other sites, the trace metal contamination — first lead, then zinc — predated the onset of acidification by a few decades.
Fuel-oil traces and power station fly-ash appear in the late twentieth century.
Investigators are now focusing on ponds of ‘decalcified’ sandstones, including  Pinkworthy on Exmoor and Woolmer in Hampshire, where the acid-sensitive and rare natterjack toad has disappeared.
Only one part of the UK appears to have so far escaped: the northwest Highlands and Islands.
Disappearing Life: Acidification and Conservation
Diatoms are not the only plants affected by acid water.
Certain species of sphagnum and the acid-loving bulbous rush may become abundant.
Rare species such as awlwort, typical of the margins of delicate mountain lakes, the pipewort (restricted to Skye and western Ireland), Isoetes echinospora, the rare waterwort, and its relative Elatine hydropiper, and the various-leaved pondweed, are all likely to vanish.
As waters become acid, animal life too diminishes.
Mayflies and several varieties of caddis disappear from acid streams, and the freshwater shrimp becomes scarce in soft water or below pH 5.7.
In lakes, crustaceans, molluscs, leeches and mayflies dwindle, although predators remain more numerous, feeding on alternative prey.
Dragonflies and stoneflies tend to be more common in acid water.
Toads, frogs and newts are strongly affected.
Exposure to water below pH 4 kills embryos in spawn.
At slightly higher pH, embryos suffer spine deformities, and hatching enzymes are impaired.
It seems that increased acidity has caused widespread deaths of tadpoles in southwest Scotland and the retreat of the common frog in northwest England.
Toad tadpoles put into acid water develop severe leg abnormalities.
One of the few British birds studied in relation to acidification is the dipper, a small black and white bird which feeds on invertebrates along rivers.
Stephen Ormerod, Stephanie Tyler and others from the RSPB and University of Wales have shown that dippers declined 80–90 per cent on the River Irfon at a time of acidification and afforestation.
Along streams with high aluminium and low pH, dippers show a reduced ability to feed their young.
Acid stress and calcium deficiency may both be significant.
Of the apparently suitable streams in North Wales, 72 per cent are now without dippers.
Ospreys do not nest in the Galloway lochs, although they do ‘prospect’ the area regularly: it seems likely that it is the lack of fish that keeps them away.
Unique varieties of the arctic char in Lochs Grannoch, Doon and Dungeon are threatened with extinction owing to a tenfold increase in acidity since the 1970s.
In Wales, the otter may not be recovering from declines caused by persecution and dieldrin, because of a lack of fish due to acidification.
In conservation terms, Britain is internationally significant for its peat bogs, both the lowland raised bogs and the blanket bogs and mires of the Flow Country and much of the upland Pennines and Welsh mountains.
This heritage, together with its birdlife and scarce plants, is now threatened by acidification and nitrogen eutrophication.
Lesser twayblade and heather have already declined on Peak District bogs, while the acid-tolerant cotton-grass and bilberry are now more common.
Another British speciality in world conservation terms is the rich flora of  mosses and lichens.
More than 1,500 lichens, 680 mosses and 380 of the related liverworts occur in Britain (as opposed to only 1,700 flowering plants).
Lichens were wiped out over large areas owing to sulphur dioxide emissions.
Today, Britain has 70 per cent of the total number of mosses and liverworts in Europe and holds nearly one-third of its threatened lichens.
From the nineteenth century onwards, biologists reported ‘lichen deserts’ around towns.
A 1970s survey showed that much of England was devoid of all but one lichen  (Lecanora conizaoides)which actually prospers in sulphurous conditions.
Leicestershire for example, lost 89 per cent of the vulnerable species in its lichen flora and 47 per cent overall.
The west of Ireland still enjoys rich ‘mossy’ and lichen-covered woods (and even a lichen-eating and spotted Kerry slug), not just because areas such as Killarney have a moist climate but because they have so far escaped most of Europe's pollution.
In the early 1980s, lichens were observed ‘reinvading’ urban areas such as Greater London as SO&sub2; declined.
But close examination showed that species intolerant of acid conditions or high nitrogen levels were not reappearing.
From remote rural areas came evidence of continuing decline because of increased acidity of rainfall.
Lungwort (Lobaria)lichens became gradually extinct on 20 oaks studied at Monk Wood in Northumberland between 1969 and 1985.
The oak bark had become acidified.
Only a nearby ash tree, which had better buffered bark, retained the lichen.
One of the few lichens that are spreading is Parmeliopsis ambigua , known to lichenologists as the ‘yellow peril’.
Previously restricted to the acid substrate of debarked pine twigs, this lichen is now appearing on a variety of surfaces.
Strangely, Britain's botanists have taken almost no interest in acidification, although plants such as cowslips, heather and many woodland flowers are also known to be affected by sulphur and nitrogen deposition.
In Sweden, British species such as dog rose, cowslip and ladies' bedstraw are thought to have been lost from polluted woods, where the nitrogen-loving nettle and acid-tolerant wood sorrel now occur.
Heathlands may be highly vulnerable.
Dutch scientists have found that acid-loving rush and sphagnum species have taken over from Lobelia and Littorella (reduced from 500 to 30 sites) and 20 other rare plants of heathland pools.
Changes in nitrification caused by ammonium and nitrate deposition are blamed, and heathland soils have become richer in nitrogen and more acid.
Conclusions on Acid Rain
The acid rain problem may have disappeared from the headlines but it is far from being solved.
Britain remains a significant polluter of Scandinavia.
In Sweden in 1988 it was estimated that some 15,000 lakes were acidified, just over 1,800 of which were ‘seriously’ acidified; 100,000 km of rivers and streams were also affected.
In Norway, a re-survey of 1,000 lakes studied in the 1970s showed that many lakes have actually lost more of their neutralising capacity in recent  years, although sulphate levels are falling.
The salmon has ‘essentially disappeared’ from seven major rivers in southern Norway.
Lakes across more than 13,000 km&sup2; are ‘practically devoid of fish’ and over another 20,000 km&sup2; the stocks are reduced.
But Britain, too, has been quietly ravaged by acid rain.
In the UK, lakes in Snowdonia National Park, most of mid Wales, the Lake District, Cairngorms, Pennines and even on the Surrey heaths, are dead or dying from acidity.
Peatlands for which the UK is internationally important, and probably heaths, are being changed beyond recognition.
Britain remains the Dirty Man of Europe so far as SO&sub2; and NOx pollution is concerned, and will do so even when the EC Large Plant Directive is implemented by 2003.
And Britain lags behind in taking action.
Britain still talks of anything more than a 15 per cent cut in the CEGB's emissions within ten years as being ‘impracticable’.
But the power industry in West Germany cut its sulphur emissions 75 per cent between 1983 and 1988.
The net emissions of Germany's power stations were reduced from 1.6 million tonnes to 0.5 million tonnes within five years, enabling it to go far beyond the 30 per cent Club target.
In Sweden, sulphur dioxide emissions will have been reduced 65 per cent between 1980 and 1995, 45 per cent by 1985.
The UK's policies, note the Swedes, cause damage in other countries and are ‘not in agreement with the principles of the Geneva Convention nor the Polluter Pays Principle’.
Britain of course protests that ‘no country is doing more’ to reduce sulphur — but then no country in Western Europe creates so much pollution and has done so little (nothing) in the past.
Yet even this calculated white lie will not be a particularly helpful excuse if the limited FGD programme agreed by the CEGB continues to lag behind schedule.
In December 1989 Greenpeace consultants suggested that by 1993 power station emissions would again reach 1980 levels and overall emissions would fall not 22 per cent as the government predicted, but only 10 per cent.
Two years earlier the CEGB was vigorously rewriting history.
‘Our approach then,’ Derek Davis, a member of the Board, told the Commons Environment Committee as he sought to explained its role back in 1984, ‘was ‘agnosticism’rather than‘scepticism’.
Peter Chester informed the same Committee that ‘there certainly have been changes in knowledge.
I think the word ‘evolution’ may be the more accurate word to use’.
Chester was still trying to persuade the Committee that there was no urgency about reducing sulphur emissions.
Given the CEGB's assertion that it might take ‘decades or even centuries’ for soils to recover, Robert Jones MP wanted to know, ‘is that not all the more reason for getting on with it as quickly as possible and on as wide a scale as possible?,
‘Emissions will have essentially vanished’ by the year 2020 when the CEGB had a completely new generation of plant in place, said Chester.
Bringing down emissions faster might take £2 billion.
If calcium recovery in soils was slow, we would have ‘gained very little’ by such pollution control.
Under the Environmental Protection Bill the DoE will control sulphur pollution according to a national reduction plan administered by HMIP.
This much  is an advance, but whether they have the stomach to take on the power industry and the Department of Energy remains to be seen.
Forest Decline
Waldsterben
In the higher reaches of the Black Forest near the spa resort of Baden-Baden there are some very stately forests managed under the system known as ‘Plenderwald’.
Here foresters maintain an irregular mixture of species, ages and sizes of trees, quite unlike the cabbage-field plantation methods employed in the UK.
Many of the Plenderwald trees are massive silver firs, the species that was the original ‘Christmas tree’ even in England, before being replaced by the spruce.
These forests contain many species of trees (dominated by a mix of beech, spruce and fir), hundreds of wild flowers, deep carpets of moss and many lichens.
They have their own bird fauna including the hazel hen.
At least until the 1970s, the Plenderwald forests were West Germany's traditional playground, more important to the citizens of the Federal Republic for weekending or walking holidays than Britain's New Forest or the South Downs, with a greater role in the national heritage of myth, folklore and literature than the Lake District to the English or the Great Glen to the Scots.
Until 1970 the foresters here say they could rely on a good sale of fir trimmings — used in Christmas decorations — to pay for the routine management of the rest of the forest.
Then in the 1970s (nobody can put an exact date to it) the foliage began to change, becoming sickly yellow or brown instead of shining grey and green.
The needles began to fall and gradually families stopped coming.
Foresters had known of such things before.
The fir is a native of the European alps and periodic declines or Tannesterben (fir-dying) had in the past been attributed to years of drought or warm winters.
This seemed to be an unusually severe Tannesterben .
By autumn 1980 the decline started to affect spruce, concentrated in old trees on gravelly plains round Munich and in the central Bavarian forest.
This was put down to a ‘Fichtesterben ’, or progressive spruce death, but a year later it appeared in other species too, such as beech, oak and the mountain ash, and foresters grew alarmed.
The decline was particularly marked in the Black Forest and in the Bavarian National Park in southern West Germany, an alpine region close to Austria and Switzerland, where fir decline may have slightly predated that in the Black Forest.
All the German-speaking foresters are in close touch with each other and by  1982 the decline had been identified in Switzerland and Austria, together with the Netherlands where Scots pine and beech were affected.
The decline was also unprecedented, affecting many species with no sign of a new disease organism (not that one was known which would sweep through many different types of tree at once), and no obvious climatic or soil factor involved.
The blame fell on pollution; not the clouds of smoke and occasional waves of sulphur dioxide which were known to drift up from the industrial valleys of the Ruhr and kill off the sensitive firs, but the more insidious long-term changes implied by acid rain.
Enormous political and scientific concern was stimulated by the realisation that such a decline had been predicted by Professor Bernard Ulrich of Gottingen University.
Ulrich proposed that acid industrial emissions, principally the old enemy sulphur, were poisoning the forests through the release of aluminium in the soil.
Der Spiegel magazine made Waldsterben a front-page issue in November 1981.
Bitter scientific disputes soon developed.
By the mid-1980s well over a hundred hypotheses had been put forward to explain the forest decline.
A recent one involves ‘simple chlorinated solvents’.
These halocarbons, such as tri-chlorethylene, are used as degreasants and in dry cleaning.
As well as being a threat to the ozone layer and human health, it is now thought that they are especially attracted to the wax coatings on conifer needles, reaching 100 times the concentration in the air.
In the presence of high ultraviolet light, they break down to release radicals which destroy photosynthetic pigments and may form TCA (trichloroacetic acid), itself used as a herbicide.
In 1986, one scientific reviewer noted: ‘Sorting out the causal agents is among the most challenging scientific detective tasks of the century.
Given the severity of the Waldsterben syndrome there is no time to lose in grappling with its causes and formulating effective control strategies’.
Inexplicable, unprecedented and catastrophic,Waldsterben took on the character of an apocalyptic plague.
‘A  specter is haunting Europe and North America — the  specter of widespread forest decline,’ wrote a respected US Institute as United States researchers reported that, in a survey of 17 states from Maine to Missouri, several species showed ‘a systematic, regionally scaled and sustained decline in growth’over the previous 20 — 25 years.
The political Greens took it as confirmation that industrial society was indeed not sustainable.
Even highly conservative scientists became concerned that some large-scale change to ecosystems in industrialised countries seemed to be under way.
As experimenters came under pressure to test their laboratory theories in the forests themselves, it became increasingly clear that whatever was killing the forests, it was not a single known pollutant.
Professor Peter Schutt of Munich University proposed a ‘multiple stress’ hypothesis.
He pointed out that a cubic metre of air over the affected forests had been found to contain 400 mainly unknown but man-made organic compounds.
Trees were exposed to mixtures of heavy metals, acids, over-doses of nutrients and soil stresses including the leaching of important ions such as calcium and magnesium.
Supporters of Schutt's multiple-stresses hypothesis pointed to studies of tree rings in West Germany, Switzerland and other countries, which showed a long-term slowing in growth well before the year in which visible signs of decline  had developed.
They suggested that pollution stresses combined to weaken the trees so that, when a ‘normal’ stress such as frost or drought came along, the tree died or went into a rapid decline, before succumbing to competition from neighbours or invasion by fungal pests or other disease organisms.
The multiple-stress hypothesis explains the observed facts, although it does not confirm the exact pathways or relative importance of individual pollution factors.
It is generally accepted by most researchers in the field (although there is now increasing concern over drought and climate change, see below), as is the three-step concept of forest damage, which distinguishes ‘predisposing’, ‘inciting’ and ‘contributing’factors.
The first, such as poor climate, dry conditions, the genetic make-up of the tree, the nutrient status of the soil, background pollution and the degree of competition from neighbours, will predispose a tree to succumb to stress.
They are chronic weakening agents.
The ‘inciting’ factors are triggering episodes and many include frost, insect attack, drought, mechanical injury or an episode of high pollution.
The ‘contributing’ factors hasten death.
They include bark beetles, fungi, viruses and competition.
While the complex of possible causes was being investigated, foresters began comparing notes on the common features of the forest decline, the ‘symptoms’ of Waldsterben .
The visible signs are of two main sorts.
The first involve loss of leaf area, through leaves and buds not developing fully, and leaves falling early.
This is particularly noticeable in conifers such as yew, spruce or fir, where it makes the canopy increasingly lace-like or transparent.
The second involve changes to the branching structure of the tree.
In conifers this often involves ‘fear shoots’ or ‘compensation shoots’sprouting from the upper sides of branches, and affected beech trees show a characteristic hooking of the short side-shoots in the top of the canopy, creating a ‘fingered’rather than a fully-formed rounded tree outline.
Many other changes take place in the declining forests.
The death of plants growing under the canopy, where rainfall drips onto them, is also quite widely reported.
It is often difficult to disentangle cause and effect.
From 1985 the United Nations Economic Commission for Europe (UN ECE) began coordinating international surveys of forest decline, and has adapted the original German system to classify damage or health according to four categories, largely dependent on the loss of leaves and the density of the canopy.
These are as follows:
Damage Categories Used in European Surveys
Damage is assessed visually, and many countries use the illustrated Swiss guide Sanasilva which shows the various stages of decline in all the commoner  native British and European trees in colour photographs.
The first national West German survey took place in 1983 and included spruce, pine, fir, beech and oak.
It showed 34 per cent of all trees were damaged to some degree.
A year later damage had increased to 50 per cent and by 1985 it stood at 51 per cent.
Some species were very severely affected: over three quarters of the silver fir were damaged, with 49 per cent in categories 2, 3 and 4 and 67 per cent in these groups by 1985.
In many areas, by the late 1980s, up to 80 per cent of the mature silver fir had died.
While 66 per cent of fir trees in 27 plots established in 1980 had been considered healthy, by 1982 just 1 per cent remained so.
In the same period, the number of ‘moderately damaged’ firs increased 12 per cent to 79 per cent.
The spruce decline was even faster: in autumn 1981, 94 per cent were still ‘healthy’ but a year later this had dropped to 6 per cent.
Preceding the national survey, regional studies in 1981 and 1982 had suggested that under 10 per cent of German trees were affected.
The discovery of a national catastrophe led the forest-loving Germans into a maelstrom of surveys, scientific investigations and conferences.
Besides a national flue — gas desulphurisation (FGD) programme on power stations, West Germany introduced its own laws to require catalytic converters on cars (devices containing a platinum-coated honeycomb which treat exhaust gases, transforming up to 90 per cent of the NOx, hydrocarbons and carbon monoxide).
By 1983 surveys had also begun in the Netherlands, France, Austria, Switzerland, Luxembourg and Italy, as well as in many Eastern European countries where short-range pollution involving sulphur dioxide and NOx was still a major problem.
And not just the foresters and the ecologists but the public, and hence the politicians of Europe were galvanised by Waldsterben unlike any environmental issue that had gone before.
From Vienna, where the famous woods were now dying, came reports that 53 per cent of the country's population were more worried about the death of the nation's trees than any other factor, personal or economic.
In Switzerland, too, forest decline became a national obsession, overwhelming concern at death from nuclear war, terrorism, drugs, gold smuggling or any of the other traditional Swiss worries.
In areas such as the central swiss canton of Uri on the St Gothard pass, the death of protection-forests conserved to prevent avalanches, was soon threatening the homes of 150,000 people.
In Britain, things took a rather different turn.
The British Response
The Forestry Commission — A Recipe for Confusion
Just as the DoE lost its early struggles to get the Electricity Board to accept pollution controls, it fell to the Forestry Commission (FC) to pronounce on the state of the nation's forests, although environment scientists declared themselves convinced of the risk as early as 1982.
The Forestry Commission has two official roles in national life.
It is the ‘forest enterprise’ which plants trees, and the ‘forest authority’which does a  little research, provides technical and policy advice to government and private foresters, and hands out grants to the private sector.
By the early 1980s the Commission had been coming under increasingly severe criticism from conservationists and others.
Members of a House of Lords Select Committee on Science and Technology had criticised the quality of Commission research.
Richard Grove of Cambridge University was one of many conservationists who attacked the Commission's policies of converting deciduous, often ancient, woods to conifers.
The Royal Society for the Protection of Birds (RSPB) and the Nature Conservancy Council (NCC) assailed plans to afforest the unique peatlands of Caithness and Sutherland Flow Country, where the Commission was grant-aiding the private sector.
Friends of the Earth found itself fighting alongside left and right-wing members of the House of Commons Public Accounts Committee to overturn the tax system which benefited investors in plantation forestry at the expense of wildlife and the taxpayer.
The same critics pointed to the obsolete Victorian rationale of the Commission — which was established to plant trees for wooden ships and pit-props in a war in which timber would be a strategic material — and wondered aloud if the organisation as well as its activities should not be reformed or abolished.
Free marketeers even talked of privatisation.
So, as the Germans put Britain under pressure to reduce air pollution to protect forests, all the ingredients were in place for conflict and confusion.
The Commission — and the private foresters with which it was closely associated — was strongly disinclined to believe anything that environmentalists said.
It had experienced air pollution as a problem solved, not a new one as did the rural foresters in West Germany and Sweden.
It was ambivalent about acid rain because of its experiences with lakes.
It managed very few semi-natural woodlands and understood them very little, and had routinely fertilised, ploughed and drained its own forest soils so that long-term changes such as those feared on the continent would be masked by its own management.
If long-term changes were taking place, the Commission's forests were not the place to look for them.
Even acknowledging these differences was an institutional difficulty.
There were, for example, very few old Sitka spruce in the Commission's forests because the rotation of the plantation was only some 35–55 years.
As its critic Richard Grove had pointed out, the original plan was for the trees to live much longer (and become more valuable timber), only the Commission had badly underestimated how windy it was in the British Isles, and the Sitka spruce suffered badly from ‘wind-blow’, which stressed the timber and made sure that the trees would never reach the grand dimensions they do in their native temperate rainforests of the Pacific Northwest.
In Europe, it was the older trees — over 60 or even 100 years old — that were first and most severely affected by the new Waldsterben .
‘An Element of Neurosis’
In 1982 the Forestry Commission's Head of Site Studies and Head of  Pathology paid a short trip to West Germany to see forest decline for themselves.
They were unimpressed: ‘There is undoubtedly an element of neurosis involved in the readiness of many foresters and some workers to attribute any decline without adequate critical investigation,’ they wrote in a report published in 1983.
‘German research,’ they added,’appears to suffer from lack of co-ordination…insufficient information is being paid to biological experimentation…’
It seemed, said the Commission, ‘unlikely that the Sitka spruce, Lodgepole and Scots pine and larch forests in the uplands of Britain are in immediate danger.
Utterances like these, which rubbished German claims and made reassuring noises about the UK, were no doubt music to the ears of government, which needed to justify no action over pollution from cars or power stations.
Perhaps significantly, the Commission said nothing about more natural and lowland woodlands such as the New Forest or Epping Forest, where some pollution effects (on lichens) were already known.
Nor was anything said about unsolved mysteries such as the widespread dieback of ash trees, which the Commission itself had studied since 1960.
Originally concentrated in the East Midlands, ash dieback is a term for a general thinning of foliage, and death of branches from the tree top down, often accompanied by massive production of ‘keys’(ash seeds), very much the changes reported as Waldsterben .
‘It is hard to turn a blind eye to something so obvious and widespread,’ wrote the author of a forestry text in 1962.
In 1983, Dr R G Pawsey from the Commonwealth Forestry Institute reported an ash dieback survey which showed that 80–90 per cent of the trees in areas such as north Buckinghamshire (near Milton Keynes) were now affected.
No biological or physical cause could be found and the studies were abandoned.
Dr J N Gibbs, a Forestry Commission pathologist, later pointed out that Pawsey's survey found no correlation with pollution, although Pawsey in any case had no detailed pollution data.
It was generally assumed that deep-ploughing, straw burning, fertilisers or sprays might be the cause of ash decline in arable areas, but no detailed chemical and ecological investigation was made.
As ash trees are not ‘forest trees’ in its plantations, the Commission had little interest in them.
(In 1989 a DoE/Forestry Commission survey of hedgerow trees found a correlation between ash dieback, acidity and sulphur, suggesting, says Andrew Tickle of Greenpeace, ‘some synergism between air pollutants and agricultural factors’.)
It was also rumoured that in the late 1970s or early 1980s the Commission had sold Mark Wood and Latton Wood in Essex to the then Harlow New Town Corporation, because of slow growth and dieback attributed to air pollution.
By April 1984 international pressure on Britain was building and the Commission obliged government by supplying further arguments for inaction while taking part in ‘more research’.
Although the FC had yet to conduct a survey, it stated ‘there is little evidence of damage to trees or forests in Britain from direct pollution and from acid rain.’
The Commission devoted 11 pages to a sceptical treatment of the chemistry of pollution and Waldsterben in Europe.
Remarkably, its arguments began to sound like those of the Central Electricity Generating Board, as it sought to  cast doubt on the very existence of an acid rain problem.
‘It must also be noted that the Scandinavian interpretation of the water chemistry, in particular the role of acidity and the loss of fish, has been challenged,’ noted the Commission.
The challenge, of course, had come from the CEGB.
The Commission's cited source for this information was Dr P F Chester of the CEGB, while those thanked for ‘helpful suggestions during drafting’ included Dr R A Skeffington, a researcher with the CEGB who at this time was also showing pictures of perfectly healthy trees from the Black Forest to British conferences.
Whether or not the Commission had now fallen under the influence of the CEGB, it seemed taken by surprise when the Scottish Friends of the Earth invited Dr Joachim Puhe from Professor Ulrich's team at Gottingen University to survey British sites ‘likely’ to show forest decline.
Over a few days in May 1984 Puhe accompanied Andrew Kerr of FoE Scotland and other campaigners in a tour of Scotland, England and Wales which took in 47 sites, at 31 of which beech, Douglas fir and spruce were found with the same symptoms as those in West Germany.
Trees outside the Commission's own Lake District Information Centre at Whinlatter Pass were among those listed as ‘damaged’.
A few weeks later, Arnold Grayson, the Commission's research director, was before the House of Commons Select Committee on the Environment.
He rejected the FoE Scotland study as ‘alarmist’, down-played damage to forests in West Germany as ‘very small’ and stated that ‘no damage’had been seen in the UK.
‘We have not seen anything of this kind in Britain,’ added the Commission's Head of Site Studies.
The Commons Committee made field visits to the areas highlighted by FoE, and reported that in Cumbria's Whinlatter Pass ‘Forestry Commission officials have observed extensive dieback of shoots on Scots Pine, evidenced by browning of needles…
Some damage was also observed on Norway Spruce and Sitka Spruce.
A minority were badly enough damaged to be at risk of dying.
Officials said similar damage had been observed in the past, but not on anything like a comparable scale.’
Although the Commission found it ‘disturbing’, it asserted that the damage was not the same as that seen abroad and it blamed the winter of 1983/4.
Conflicting signals now began to emerge from the Commission.
At the Scottish Wildlife Trust's Acid Rain Inquiry of September 1984, Dr Redfern of its Northern Research Station talked of the ‘striking similarities’ between the damage which FoE Scotland had first discovered in Cumbria and northwest Scotland.
This now affected Scots and Lodgepole pine, Douglas and grand fir, western hemlock and Norway spruce.
Redfern said that ‘typical damage is death of shoots below the leading part of the tree; we term that ‘sub top dying’.
Damage is most severe in older trees and at higher elevations.’
In Cumbria, said Redfern, the ‘height increment [growth]does appear to have fallen off markedly since about 1975/6…down as much as a quarter in recent years.’
To environmentalists — and an increasing number of foreign foresters and ecologists — it seemed that Britain too had all the signs of forest decline.
The Commission was in a difficult position.
The overall government line was to stonewall international pressure for pollution curbs, pending more  research.
Much of the debate centred around evidence of damage.
In summer 1984 a Commission scientist wrote in Coal and Energy Quarterly , a journal funded by and used as a mouthpiece for the National Coal Board, that ‘there is no damage [in British conifers]that can be defined as new’ and ‘the chances of similar damage [to that in West Germany]occurring here is considered low.’
The article was vigorously publicised by the Coal Board's public relations section.
As the argument began to centre on what was or was not ‘damage’, the Commission pinned its definition to the yellowing of conifer needles.
The ‘crucial differences’ between the British and German trees, said Redfern at the Edinburgh meeting, was that there was ‘no needle yellowing’and especially no magnesium deficiency.
Professor Peter Schutt from Munich was also in the audience.
He disagreed, saying that yellowing was typical only of high altitude damage in foggy conditions, whereas green needles were falling off trees over wide areas at lower altitudes.
Whatever its internal opinions, in responding to the House of Commons Select Committee's report Acid Rain , the DoE peddled the Forestry Commission line that British tree damage bore only a ‘superficial resemblance’ to ‘that associated with air pollution in West Germany’.
However, prompted by the embarrassing visit of Puhe, in 1984 the Forestry Commission began its own ‘Air Pollution and Tree Health Survey’.
1985: Attacks of Blindness
The results of the Commission's 1984 survey were announced in a press release on 5 March 1985.
It announced that the survey had been stimulated by ‘fears’ that forest decline of the European type ‘may be occurring in Britain’.
While ‘appreciable crown thinning’ had been discovered in spruce and Scots pine, this was ‘well within the usual range of our experience’.
The results were ‘very reassuring’ with ‘no sign whatever of the damage seen in West Germany or any unexpected abnormalities’.
In particular, there was ‘no sign whatever of the yellowing of needles which characterises the damage occurring in German forests’.
Given the discoveries of Puhe and its own observations, this was a strangely definitive denial.
As it turned out, it seems to have been inspired by a desire to bolster government policy rather than to make an objective assessment of the state of trees.
Conservation groups pointed out that the Commission was studying the wrong forests (its own and its younger plantations) and had deliberately excluded the trees which, on the basis of continental experience, would be most at risk.
Those at the edges of a stand or in an exposed position were excluded.
It was, as it were, a survey designed not to find a problem, rather than one to check on the most vulnerable forests.
Even so, when the report was actually published in August 1985, the Commission's own figures proved to be remarkably similar to those reported from West Germany.
Like many scientists, campaigners at Friends of the Earth in London (including the author) had assumed that the absence of strong evidence for  forest decline in Britain might be due to a combination of climate (moist conditions good for growing trees) coupled with the possibility that magnesium in rain coming in from the sea might counteract leaching by acid rain.
However, by 1984 a stream of continental ecologists were visiting FoE's headquarters and appearing at conferences to remark that, so far as they were concerned, the trees looked much the same in Britain as they did in West Germany.
Following the Commission's strange response to FoE Scotland's survey, other British campaigners made visits to the Black Forest and the Netherlands.
They returned convinced that Britain's trees — and especially its beech trees — were showing ‘acid rain’ or Waldsterben symptoms as bad as, if not worse than, those on the continent.
On the Homisgrinde, a mountain top half way along the spine of the Black Forest and littered with dying fir and spruce, the author and a colleague asked a German ecologist how something so dramatic had apparently started unnoticed.
‘What,’ they asked, ‘had been the first sign that the forests were in decline?’
The German scientist appeared to consider this carefully.
Then with a sardonic smile he replied: ‘I think the first sign, the very first symptom of Waldsterben , was an attack of blindness in foresters.’
With grant-aid from the World Wide Fund for Nature (WWF), FoE now involved over 500 volunteers in its own survey of yew and beech trees.
These species were chosen as native, easily identifiable trees growing on a range of soils throughout most of Britain, which show clear symptoms.
Declining yews lose needles and develop in the same pattern as other conifers, and affected beech show specific changes to their branching structure.
The FoE study, published in October 1985 as The Final Report: Tree Dieback Survey , was based on 1,638 beech trees and 1,546 yews at over 700 sites.
The results showed that 69 per cent of all beech and 78 per cent of yew had some form of dieback.
FoE concluded: ‘Typical symptoms of acid rain damage to trees are now widespread in Britain.’
It discovered severely deformed and dying beech at Denny Wood in the New Forest, a major conservation site in an area run by the Forestry Commission, which objected to television filming of the dying trees.
From Snowdonia, the National Park Officer wrote to say that, ‘We see little point in completing the beech survey since so many beeches in North Wales are showing the exact symptoms you mention — i.e. small leaves, reduced leaf number (dieback) cluster twigs and leaf curl.
In general many beeches, particularly those on the western edge of the Park show a general lack of vitality…
We have also noted: 1.
Unusual numbers of dead holly trees with senescent branches or main trunks; 2.
Ash trees with thin leaf canopies.
Also ash and hornbeam showing very large numbers of keys (often apparently infertile)…3.
Sycamores with reduced leaf canopies and 10–15 year old saplings with yellowed necrotic leaves; 4.
The odd Scots Pine with reduced needles; 5.
Rowan showing reduced leaves and early leaf fall…’
Such symptoms — repeated at places up and down the country — were exactly those to be seen throughout northern and central Europe.
Hollies were dying on the granite outcrop of Charnwood Forest in Leicestershire.
At Llyn Brianne in mid Wales, Lodgepole pine showed extensive yellowing; on the acid sands of Surrey, beech were as deformed as those on the sandy soils of  Holland and in the worst affected areas of the Black Forest.
And beech and lime showed dieback in Richmond Park.
In Stoke Poges churchyard, whose large yew tree was immortalised in Thomas Gray's Elegy Written in a Country Churchyard , the ‘yew tree's shade’ was poorer than the poet had found it, as its needles were yellowing, branches drooping and canopy transparent.
Generally, said FoE, the growth scars of slow-growing beech twigs dated the onset of decline at 1965–70, while the ‘fear twigs’ on spruce trees had appeared from 1978 onwards.
Damage was worst on exposed sites, on high ground and in rural rather than urban locations.
Dieback was found on all soil types, but trees on acid sands were ‘consistently bad’.
During the survey, FoE had also brought over Swedish ecologist and authority on beech trees, Professor Bengt Nilghard from Lund University in Sweden.
British spruce, said Nilghard, were often ‘in really bad condition’, with symptoms of gas damage on older needles.
Near Tintern, Sitka spruce growing on soil with a pH of 3.1 showed slight magnesium deficiency, while ‘dead branches, early yellowing, brownish leaf-edges and early leaf-fall were also seen on many other tree species, eg ash, lime, chestnut and maple…’
’ Summing up,’said Nilghard, ‘I must say that the only place where I have seen correspondingly bad situations in Central Europe before is around the biggest airports in West Germany.’
The British forestry establishment reacted with fury and not a little venom.
After an article in New Scientist which summarised the findings of FoE and Nilghard, R G Pawsey wrote from the Department of Forestry in Aberdeen University that ‘there is no evidence that acid rain or any other pollution factor is significantly affecting the general health of forest and other trees in this country.
Dr Nilghard's proclamations,’ declared Pawsey, were ‘entirely spurious.
The Forestry Commission's Director of Research, Arnold Grayson, wrote to New Scientist : ‘To imply that we are deliberately trying to cover up evidence that trees do die back is obvious nonsense.
Our concern is to see whether any new mechanism of decline is at work.’
Professor Hugh Miller, also from Aberdeen University and who had advised on the Commission's 1984 paper which questioned the link between acidity and fish losses, wrote that ‘there is at present no evidence of damage’ and ‘When worried about the health of our trees it would seem preferable to heed qualified forest pathologists rather than respond to assorted ecologists and conservationists, no matter what their nationality.’
Here, as in the pollution debates over the hazards of sewage or acidification of lakes, the Commission and its supporters were applying one line of reasoning whereas environmentalists and scientists in other countries were applying another.
The view that almost everyone except the Commission subscribed to was summed up a few years later by the UN Economic Commission for Europe when it wrote in a report on damage surveys in many countries: ‘Research results obtained so far indicate that air pollution is an essential, causal factor in the destabilisation of forests or even in the breakdown of some forest ecosystems.’
FoE argued that, lacking any other explanation and given abundant circumstantial  evidence that pollution was bad for trees, a combination of natural and pollution stresses was the best explanation for the observed symptoms.
The Commission seemed willing to call on practically any explanation except pollution.
Yet the Commission and its supporters were increasingly inconsistent.
Sometimes there was no dieback or decline, at other times there was dieback but it was not connected to pollution, on occasions trees which had lost a third or more of their vegetation were normal.
At Kew Gardens, an official-minded institution administered via the Ministry of Agriculture, a spokesperson neatly captured the illogical Commission view by telling a journalist who enquired about dieback of Kew's cedars, beech and yews, that ‘there wasn't any dieback’, and, if there was, ‘it wasn't due to acid rain’.
Just like the CEGB and ‘proof’ of a link between pollution and damage to lakes, the Commission put up one condition after another as a ‘test’of whether Britain's trees were showing decline, and then replaced it with a new argument if it was satisfied.
This proved nothing in itself, but it made a mockery of the Commission's repeated claims to be particularly scientific in its approach.
1986: Britain's Trees Show Official Yellowing
In June 1986 the Commission released its own 1985 survey, which now found a ‘high degree of yellowing’ in some Scots pine, as well as yellowing on Norway and Sitka spruce.
But whereas the year before the absence of yellowing was proof that Britain did not have the same forest decline as in Europe, its discovery did not show that Britain did have the same forest decline.
Yellowing, said the Commission, was ‘difficult to interpret’ and might be connected to fungi or the weather.
Not, this time, the 1975/6 droughts but now the coldest February in 40 years.
The official press release explained that the survey showed that ‘Britain's trees are in good health’.
The report itself had no conclusions, so came to no conclusion about the yellowing.
In December the Commission reported on its 1986 survey.
Now the Commission had changed the point of its survey.
No longer was it to look for signs of forest decline, it was to investigate a link between pollution and forest decline.
Although the survey found a ‘worsening of tree condition’ and conifer health was suddenly ‘only moderate’, there was, said Mr Grayson, ‘No regional pattern of crown condition…and it is therefore difficult to ascribe the findings to a cause such as air pollution’
Yet the Commission's survey was a singularly unsuitable test of such a link.
Its sites were on variable, disturbed and fertilised soils and mainly included only the less vulnerable trees.
The survey was not detailed, comprehensive or controlled enough to investigate a geographical link to air pollution even if the right pollution data were available.
In the rest of Europe, as Sally Power, Katy Ling and Mike Ashmore of Imperial College later pointed out, surveys ‘have made little attempt to link tree health to variations in site factors, stand types or pollution climate.
Instead emphasis has been placed on monitoring temporal and spatial changes in tree health.
In Britain the Forestry Commission has  attempted to relate variations in tree health to…slope, ruggedness of the site, altitude, and also to derived climatic and pollution data.
No account has been taken of, amongst other things, the role of soil factors, of disturbance, or canopy density.’
The body of the Commission's report, which was written by recently recruited geographer Dr John Innes, took a less bullish line than the press release.
The report admitted that the survey ‘does in fact show that in comparison with this year's results from West Germany, we have appreciably more trees in the slightly (11–25 per cent) and moderately (26–60 per cent ) damaged categories.’
While the report was acknowledging considerable dieback, the press release which accompanied it three days before Christmas (a traditional moment for government departments to ‘bury’ embarrassing statistics) spoke only of ‘no sign…of the forest decline which is of such great concern in central Europe’.
At the same time as the Commission put out its 1986 study of conifers, it released its 1985 survey of beech trees.
This study was organised by Commission pathologist David Lonsdale.
Lonsdale found the results ‘alarming’, but the official press release reassured the press and public that there was ‘no sign’ of forest decline among beech trees.
Not surprisingly, many people found the whole issue thoroughly confusing.
In 1985, William Waldegrave, minister at the DoE, intervened to urge cooperation between the Commission and the Institute for Terrestrial Ecology (ITE).
He didn't think FoE's study was a ‘particularly good piece of scientific work’ but ‘didn't want to rubbish it’.
In new surveys he said, a ‘multi-stress’ approach would be needed, while ITE's deputy director, Fred Last, commented, ‘I don't agree with some of Friends of the Earth's conclusions.
But many trees today have characteristics that are not healthy.’
FoE's surveyors, said Last, were ‘entitled to more rigorous answers’ than they had been given.
1987: Half a U-turn
By 1987 the European Commission had joined the UN ECE in the forest survey business and required all member states to undertake a standard grid survey, reporting on a set number of trees at regular 16 km intervals, using a common method.
The Forestry Commission did the survey in Britain.
When the results for beech trees were disclosed, they were almost identical to those of the much criticised FoE survey of 1985.
The grid survey showed that the majority of Britain's deciduous trees, and 60 per cent of all oaks, were in the ‘moderately damaged’ categories, having lost 26–60 per cent of their leaves.
As to its own survey, which was now harmonized with those of 15 other countries contributing to the UN ECE study, the Commission's John Innes stated in its Bulletin 74 that: ‘This report, together with those from previous years, indicates that both crown thinning and the yellowing of needles, the two symptoms most frequently associated with forest decline on the continent, are present in Britain.
The extent of crown thinning and  discolouration in Britain is similar to that in West Germany.’
So forest decline had officially arrived in Britain?
Well, not quite.
Just as Bulletin 74 was being published, a Forestry Commission booklet Air Pollution and Forests stated ‘there is no sign of the type of damage seen in West Germany, occurring in Britain at the moment’.
Although some of its own experiments were now showing that air pollution in southern England damaged leaves and made them fall early from trees, it could not bring itself to say as much.
Instead, the Commission indulged in some verbal gymnastics with: ‘This work is beginning to suggest that some types of tree grow better in air that has most of the pollution removed from it.’
Nevertheless, whichever way the statistics were presented, surveys showed that Britain's trees had as many deformations, lost leaves, stunted branches and thin and discoloured canopies as those in the supposedly worst-affected continental countries.
1988: A Faint Connection
In February 1988 the House of Commons Environment Committee returned to the topic of acid rain in its Air Pollution inquiry.
By now, the Commission had retreated on the existence of symptoms, and adapted its surveys to include older and more exposed trees of a greater range of species.
It had also begun open-top chamber experiments in which pollutants are filtered from the air so that tree growth can be compared in ‘ambient’(polluted) and clean conditions.
The Committee chairman, Sir Hugh Rossi MP, asked the Forestry Commission's research director, Arnold Grayson, ‘Are you coming to the conclusion that there may be some connection in the United Kingdom between the condition of trees and pollutants?’
’ Very faintly,’replied Grayson, referring to experimental evidence that the fine roots of Scots pine grow less and the leaves of poplars fall earlier in ambient than in unpolluted air.
As to other evidence, despite the fact that other countries and other scientists believe a more detailed approach is needed, the Commission persisted in using survey data to look for proof or a lack of proof that pollution was the cause of leaf-loss and canopy thinning.
There was, said Grayson, ‘no such indication’.
Rossi pointed out that the Commission's denial of a link between forest decline and pollution was ‘quite contrary’ to evidence from others and ‘the Swedes, Norwegians and Germans who fear that there is a very, very direct connection’.
Pointing to the Commission's admission that the key symptoms in Britain were now ‘similar’ to those in West Germany, Rossi expressed surprise that the Commission was ‘apparently no further forward than in 1984’.
Why did the Commission's own publications catalogue the effects that NOx, SO&sub2; and ozone could have on trees, until ‘one could scarcely but come to the conclusion that there is a strong connection between atmospheric pollution and damage,’ yet ‘we are still talking about not being satisfied.’
Grayson cited ‘fungi, insects and so forth’.
His colleague Dr Freer-Smith said, ‘in many areas of West Germany there is evidence to link specific pollutants as a major causal factor; but I also think the Germans would accept that there are both natural and pollutant stresses interacting — the idea that has been  referred to as the multiple stress hypothesis.’
Not surprisingly the Germans would accept this, they had proposed it when the Commission had by turns been denying the existence of damage in West Germany and demanding proof based on a single pollutant (rather than multiple stress).
Freer-Smith added, ‘The problem with surveys is that you cannot establish a causal relationship.’
Unfortunately, in its public statements, the Commission had never shown such scientific rectitude, preferring such comments as ‘these results give Britain's forests a clean bill of health’.
And only with extreme reluctance would the Commission agree that pollution should be reduced.
FGD would bring only ‘small’ benefits in terms of forest growth said the Commission, ‘in relation to the expenditure involved’.
When, asked Henry Bellingham MP, ‘did the Forestry Commission first urge the government to reduce emissions of both NOx and SO&sub2;?’
The answer: ‘we have not done so.’
Taking up evidence from Greenpeace which simply drew figures from the national forest surveys coordinated by the UN ECE and EC, Sir Hugh wanted to know if the Forestry Commission accepted that ‘the beech forests of Britain are among the worst affected in Europe’.
Mr Grayson replied: ‘If we can assume that the same degree of care is taken in all countries showing — and I think it is true, certainly, of Switzerland and West Germany — beech observations, then you could certainly say that the health of beech trees reflected in crown condition must be borne out by observation.’
The Commission persisted in arguing that, as it was impossible to put a precise figure on what was a ‘normal’ and an ‘abnormal’amount of leaf loss, it was impossible to conclude whether or not thin tree crowns were abnormal.
‘It could be argued,’ commented MP Andrew Hunter, ‘that wherever there could be uncertainty, you are hiding behind it.’
In its 1988 survey, the Commission found that ‘the condition of trees is poor’ but ‘no trend can be established’, so it was ‘not possible to determine…whether the trees in the United Kingdom are in a state of decline’.
There was, however, ‘increasing concern [in Europe]about the role of soil acidification’(it was not mentioned that this has been caused by air pollution).
The report ended enigmatically: ‘a potential problem exists…it is clear that air pollution, particularly in the form of acidic deposition, is affecting many aspects of forest ecosystems in Britain and there is an obvious need for continued detailed monitoring.’
cash is available to look for effects in the field.