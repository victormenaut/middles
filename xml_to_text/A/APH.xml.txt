

Latent inhibition as reduced associability
Habituation and latent inhibition
In a typical experiment on latent inhibition, a stimulus that has been repeatedly presented on its own is used as the CS (conditioned stimulus) in a conditioning procedure and the course of acquisition of the CR (conditioned response) is compared with that shown when a novel stimulus is used as the CS.
The procedure can thus be viewed as being an indirect way of assessing the effects of habituation training.
Certainly, the habituation process described in Chapter 2 will be engaged during the first stage of a latent inhibition experiment and might be expected to play some part in determining the outcome.
What we need to determine now is what further processes, if any, must be assumed to operate during the habituation procedure if the results of studies of latent inhibition are to be explained.
The suggestion that pre-exposure to a stimulus establishes an exact representation of it, for instance, is not in itself enough to explain latent inhibition; indeed it seems capable of predicting quite the wrong result.
The ability of the CS to evoke the CR after conditioning trials have been given will depend, in part, upon that stimulus being identified with the one that was presented during conditioning.
To the extent that this identification is more likely to be made when the stimulus in question is already capable of activating a detailed representation, the likelihood of a vigorous CR may be increased.
What we have already seen, however,(p. 24) is that pre-exposure to the to-be-CS results in retarded development of the CR.
Clearly, this aspect of our account of habituation is not enough in itself to explain the latent inhibition effect.
But, whatever its theoretical explanation, the habituation procedure has direct behavioural consequences and their role needs to be considered before we are obliged to accept the suggestion that latent inhibition implies the operation of mechanisms other than those involved in habituation.
The role of the UR
The most obvious possibility is that the waning of the overt unconditioned response (UR) produced by pre-exposure to the to-be-CS might be directly responsible, in whole or in part, for some instances of latent inhibition.
Some conditioning procedures (in particular, those that use the suppression of ongoing behaviour as their measure) engender the development of a CR that is similar in form to the UR evoked by the CS when it is first presented.
The summation of these two response tendencies (conditioned and unconditioned) in control subjects, for whom the CS is novel, might be enough to ensure a bigger net response than that seen in pre-exposed subjects, for whom the unconditioned component will be absent.
There can be no doubt, however, that there is more to latent inhibition than this because the effect is routinely observed both when the stimulus evokes no very marked UR (e.g. Carlton and Vogel 1967; Domjan and Siegel 1971) and also when the CR required is quite different from the UR.
The experiment by Channell and Hall (1983), mentioned in Chapter 1, provides an instance.
Here, as fig. 1.8 shows, the CS when it was first presented to the control subjects tended to suppress responding; but thereafter, these subjects acquired the CR (which involved an increase in responding) more rapidly than did pre-exposed subjects.
A somewhat more subtle interpretation of latent inhibition in terms of the known facts of habituation comes from considering the role of orienting responses (ORs).
Pavlov (1927) expressed the view that the OR evoked by a novel stimulus constituted an ‘obstacle’ to conditioning; and indeed, if conditioning is seen as a process in which the OR comes to be replaced by the CR, it might be expected that pre-exposure to the CS would facilitate this process.
That it does not, requires a different interpretation, and one has been offered by other Eastern European workers, notably by Sokolov (e.g. 1963), who suggested that the functional significance of the OR is that it facilitates the uptake and processing of environmental information.
Conditioning with an habituated stimulus as the CS requires, it is suggested, that the OR be re-established so that the processing necessary for the formation of an associative link can occur.
Certainly, those experiments in which the occurrence of an overt component of the OR has been monitored have demonstrated that the response that has been habituated during pre-exposure tends to reappear when the CS-US pairings are begun.
An example is shown in fig. 3.1.
This experiment, by Kaye and Pearce (1984)(see also Pearce, Kaye, and Mall 1982), included a group of rats given a series of presentations of a light, a stimulus that initially evoked a behavioural OR.
After habituation had occurred, the light was used as a CS signalling the availability of food.
The subjects showed latent inhibition, developing the CR (the response of approaching the site or food delivery when the light came on) less readily than a control group that had not received pre-exposure (Fig. 3.1, lower panel).
And, as the upper panel of the figure shows, the introduction of the conditioning procedure was accompanied by a rapid and dramatic increase in the occurrence of the OR.
(It is interesting to note that the frequency of occurrence of the OR actually declined during conditioning for the control subjects.
This observation, along with the implications of the performance shown by group Partial, will be taken up later, p.100.)
Interpretation of the results presented in Fig.3.1 is not straightforward.
It   is difficult to distinguish cause and effect; and even if we could be sure that the low level of orienting in the pre-exposed subjects was responsible for their retarded conditioning, it would still be necessary to explain why it is that the OR should return when reinforced training begins.
None the less, it seems plausible to suppose that the loss of the OR in the pre-exposed group might retard the formation of a light-food association simply because it  reduces the likelihood that these subjects will see the light at the start of conditioning.
In this case, latent inhibition and habituation would indeed reduce to essentially the same thing.
But, in fact, latent inhibition is routinely obtained with stimuli that are diffuse and non-localized and thus seem certain to impinge on the subject's sense organs even in the absence of an overt OR.
Stimuli of this sort may still evoke an OR that habituates during pre-exposure but there is no reason to suppose that the loss of the overt components of this response will prevent the subjects from seeing or hearing the stimulus.
Siegel (1972) reports that pre-exposure to a tone in rabbits leads to a loss of the OR (evident as a decline in the likelihood of occurrence of an eye-opening response evoked by the novel tone) but the latent inhibition that was also found cannot be directly attributed to the loss of this aspect of the OR.
Mean percentage of observations on which rats were recorded as orienting to a light (OR) or showing the conditioned response (CR).
The pre-exposed and control groups received continuously reinforced conditioning in the second stage; pre-exposed subjects experienced non-reinforced presentations of the light in stage one.
The partial group received pre-exposure in which the light was followed by the reinforcer on half of the trials.
(Adapted from Kaye and Pearce 1984.)
In order to explain latent inhibition in terms of the habituation of the OR it is clearly necessary to interpret the latter as being some central response, the evocation of which is necessary for speedy conditioning.
(An overt OR might serve as an index of this central change but would not, in itself, be directly responsible for most cases of latent inhibition.)
Another way of putting this is to say that habituation training produces a loss of effectiveness by the to-be-CS (a reduction in ‘conditionability’ or ‘associability’) and that this loss is determined by the same mechanism as that controlling the loss of overt URs.
We need to examine, therefore, those experiments that try to determine if the conditions necessary for stimulus exposure to produce latent inhibition are the same as those that produce the habituation of a UR.
Should it turn out that latent inhibition and habituation can be dissociated, two possibilities will emerge.
One is that latent inhibition should not be interpreted as being the result of a loss of stimulus associability; Chapter 4 will discuss the most widely considered alternative, ‘interference’ theory.
The other is that latent inhibition does indeed result from a loss of stimulus associability but that the mechanism responsible for this loss is not that that underlies habituation.
Later sections of the present chapter discuss this possibility.
Empirical dissociations of habituation and latent inhibition
(a) Cross-experiment comparisons
If latent inhibition and habituation reflect the operation of a common mechanism, then procedural variations that modify the magnitude of one of these phenomena should have a similar effect on the other.
Often this has proved to be so.
Thus, the extent of habituation increases with the number of exposures to the stimulus and so does the magnitude of latent inhibition (e.g. Lubow 1965; Lubow, Markman, and Allen 1968; Lantz 1973).
Again, testing with a stimulus somewhat different from that used for pre-exposure produces generalization decrement both in habituation and in  latent inhibition (e.g. Carlton and Vogel 1967; Siegel 1969; Dawley 1979).
There are, however, some seeming discrepancies.
One of Thompson and Spencer's (1966) list of ‘criteria’ for habituation is that habituation proceeds most readily with closely spaced trials; but latent inhibition appears to be more profound when the pre-exposure trials are relatively widely spaced (Lantz 1973; Schnur and Lubow 1976; but see also Crowell and Anderson 1972).
Again, Thompson and Spencer suggest that intense stimuli habituate only slowly but there is some evidence that they might acquire latent inhibition especially readily,(Crowell and Anderson 1972; Lantz 1976; Schnur and Lubow 1976).
A further feature of habituation is that it is supposed to show spontaneous recovery after a retention interval; but although some studies of latent inhibition have revealed an equivalent effect (e.g. Best and Gemberling 1977; McIntosh and Tarpy 1977; Westbrook, Provost, and Homewood 1982), others have reported perfect retention of latent inhibition (Siegel 1970; James 1971; Crowell and Anderson 1972).
It might be objected that when the comparison of habituation and latent inhibition gives rise to a discrepancy, it does so because we have failed to compare like with like.
In particular, Thompson and Spencer's list of the features of habituation was derived from studies of short-term examples of the phenomenon; it may therefore be unreasonable to expect these features to be found also in latent inhibition (see Lubow, Weiner, and Schnur 1981).
There is some reason to doubt that all the discrepancies can be eliminated by taking account of the short-term/long-term distinction: Lantz's (1973) demonstration of superior latent inhibition with spaced trials came from a procedure in which the first conditioning trial followed the last trial of pre-exposure; James' (1971) demonstration of perfect retention used, in training, the interstimulus intervals typical of studies of short-term habituation.
None the less, it is surely true that it will be difficult to reach firm conclusions from comparing experiments which, however similar they may be formally, use quite different subjects, stimuli, and training procedures.
We need to find examples of experiments in which the habituation of the UR and the development of latent inhibition have been assessed in animals subjected to identical conditions of training.
We should acknowledge at the outset that there may be many cases (like that illustrated in Fig. 3.1) in which presentation of a given stimulus has been shown to result in both latent inhibition and the habituation of a UR.
Our concern must be, however, to seek out instances in which one effect has been observed in the absence of the other.
(b) Physiological Intervention
One of the earliest claims to have demonstrated a dissociation of latent inhibition and habituation was made by Weiss, Friedman, and McGregor (1974), who studied the phenomena in rats having surgically produced lesions of the septal area of the limbic system.
When presented with a series of tones, these rats showed perfectly normal habituation of the UR (the suppression of a  water-licking response); but equivalent pre-exposure produced no latent inhibition when the subjects were required to learn a shock-avoidance task with the tone as the warning signal — animals with septal lesions learned readily whether they were familiar with the tone or not.
More recent corroborative evidence comes from Gallagher, Meagher, and Bostock (1987), who investigated the effects of infusions of the opiate agonist levorphanol into the septal region of rabbits.
Animals that received infusions during pre-exposure sessions showed normal habituation of the UR (heart-rate deceleration) evoked by the tone used as the stimulus.
The response remained habituated on a test session given 24 h after the last pre-exposure session; but when the tone was then paired with shock, conditioned responding developed readily, that is, there was no sign or latent inhibition.
(c) Generalized effects of flavour pre-exposure
Pre-exposure to a novel flavour normally results in both a reduction of neophobia and in a reduction of the ability of that flavour to serve as a CS in flavour-aversion conditioning.
If the initial exposure is given to some flavour other than that used for the test phase, then both the habituation of neophobia (Siegel 1974) and latent inhibition (Tarpy and McIntosh 1977) are attenuated but may still occur to some extent — the degree of generalization will presumably depend upon the extent to which the pre-exposed flavour and the test flavour are perceived as similar.
With certain flavours, however, it has been found that habituation can generalize perfectly well when latent inhibition fails to do so.
The effect was first shown in an experiment by Braveman and Jarvis (1978)(see also Miller and Holzman 1981; Gilley and Franchina 1985; Franchina and Gilley 1986).
They used a saline solution as the test flavour and gave different groups of subjects pre-exposure either to saline itself or to some other flavour or flavours.
(The flavours used are listed in Fig. 3.2.)
Half the subjects received a test in which their consumption of saline was measured.
As panel A of Fig. 3.2 shows, only the subjects pre-exposed to water showed neophobia; pre-exposure to any of the other flavours produced generalized habituation of the neophobic response to saline.
But the latent inhibition effect did not generalize.
Panel B of Fig 3.2 shows the results for a further set of subjects that received aversion conditioning with saline as the CS before the test session.
It is evident that only pre-exposure to saline itself produced latent inhibition and that an aversion was established perfectly readily after all other conditions of pre-exposure.
On the basis of their findings, Braveman and Jarvis (1978) put forward the suggestion that latent inhibition derives from a loss of effectiveness by the specific cues that characterize the CS (and thus requires pre-exposure to that very stimulus), whereas neophobia is taken to be a reaction to the aversive properties of novelty per se and can be attenuated by prior exposure to any other novel event (see also Braveman 1978).
Support for this interpretation   has been sought in the effects of a procedure in which subjects are given pre-exposure to a variety of flavours.
This procedure, it might be supposed, would ensure repeated experience of novelty and should thus be especially effective in reducing neophobia to a test flavour not previously experienced.
It should not, however, generate latent inhibition to this test flavour.
Figure 3.2 shows such a dissociation between neophobia and latent inhibition, result subsequently confirmed by Miller and Holzman (1981)(fig.3.3).
*
Fig. 3.2 Amount of a saline solution consumed by rats pre-exposed to water (W), saccharin (S), Lemon (L), coffee (C), almond (A), sodium chloride (Na), or to a variety of these flavours.
In the neophobia test (panel A) the subjects were allowed access to the saline solution for 10 min.
Panel B shows the results of a similar test given after a conditioning trial on which consumption of saline was followed by an injection of LiCl.
(From Braveman and Jarvis 1978.)
However suggestive these results may be, they are all open to a rather trivial explanation.
As Fig. 3.3 shows, the subjects exposed to a variety of flavours acquired the aversion readily; that is, showed no latent inhibition.
But, as the figure also shows, the dose of LiCl used was sufficient to produce an almost total aversion in the subjects naive to the CS, and thus a difference between the groups might have been obscured by a ‘floor effect’— by a lack of sensitivity in the test (see also Misanin, Blatt, and Hindenliter 1985).
And indeed, Tarpy and McIntosh (1977), using a procedure likely to be more sensitive (involving, among other things, a weaker US and prolonged testing), were able to demonstrate substantial latent inhibition in rats given exposure to a variety of flavours before conditioning.
Miller and Holzman (1981) were aware of this possible problem with their procedure and conducted a further study in which a smaller dose of LiCl was used as the US.
Although they again found no difference between subjects given no pre-exposure and   those exposed to a variety of flavours, interpretation of this finding is rendered equivocal by the fact that the two groups drank different amounts of the CS flavour on the conditioning trial.
Differences in the amount consumed are capable, in themselves, of determining the magnitude of a conditioned flavour aversion (e.g. Bond and DiGiusto 1975).
Once it has been allowed that the procedure used in these various studies of flavour pre-exposure might have used insensitive measures of conditioning, it becomes inappropriate to place much reliance on them as demonstrating a dissociation of habituation and latent inhibition.
Indeed, Braveman and Jarvis (1978), having argued that their results imply separate mechanisms for the two phenomena, go on to acknowledge the possibility that their results might simply reflect the use of a test procedure that was less sensitive as a measure of conditioning than as a measure of neophobia.
And, although it arises in a particularly acute form in these flavour-aversion studies, the same difficulty of interpretation vexes all the experiments cited here.
(d) Effects of changing the context
Figure 3.4 shows the results of an experiment by Channell and Hall (1983) in which rats received appetitive conditioning with a light CS — some subjects      in a context to which they had been pre-exposed, others in a novel context, differing in its smell and the level of a background noise from that with which they were familiar.
Half of the subjects in each training condition had received presentations of the light during the pre-exposure phase.
Those that received pre-exposure and conditioning in the same context acquired the CR slowly (i.e. showed latent inhibition); those that experienced a change of context from exposure to conditioning learned as readily as the control subjects.
This failure of latent inhibition to transfer from one context to another is of some theoretical significance, being uniquely predicted by Wagner's (1976, 1981) theory (see below, p. 81), and it has subsequently been sought and found in a variety of other training procedures (e.g. taste aversion, Hall and Channell (1986); conditioned suppression, Hall and Minor (1984), Lovibond, Preston, and Mackintosh (1984), Swartzentruber and Bouton (1986)).
The evidence discussed in Chapter 2 showed that, at least when the test context is familiar, a change of context does not restore an habituated response.
Does latent inhibition continue to show context specificity when tested under equivalent conditions?
The experiment by Hall and Channell (1985b), described in Chapter 2 (p. 40), was designed to allow an answer to this question.
Recall that the experimental group (E group in Fig. 2.5(b)) in this experiment received exposure to two distinctive contexts (those used by Channell and Hall 1983), with presentations of a light occurring in only one of them (context A in the figure).
The habituated OR to the light did not return when this stimulus was presented in the other context (context B in  the figure), that is, habituation transferred across contexts.
The status of latent inhibition was assessed in a final stage of training that immediately followed the habituation test.
All subjects received two sessions per day of appetitive conditioning with the light as the CS, one session in context A and one in context B. As Fig. 3.5 shows, control subjects, for whom the light was novel, learned readily in both contexts.
The experimental subjects acquired the CR only slowly when trained in context A, the context in which the light had been presented during the first stage of habituation training.
But no latent inhibition was evident when conditioning was assessed in context B.
Essentially identical results have come from an experiment by Hall and Honey (1989 a ), which used quite different procedures.
Two stimuli were used, the offset of a light and the presentation of a clicker, both of which tended on first presentation to evoke a suppression of ongoing behaviour.
All subjects received presentations of both stimuli, the light in one distinctive context and the click in another.
On the test session, subjects in one group continued with this same arrangement but for subjects in the critical experimental condition the stimuli were presented in the contexts in which they had not previously occurred.
As Fig. 3.6 shows, the change of context for the latter group did nothing to restore the ability of the stimuli to evoke the UR.
(Bouton (1990) reports an experiment by Bouton, Okun, and Swartzentruber which used closely similar training procedures and generated essentially identical results.)
But the change of context did attenuate      latent inhibition.
Figure 3.6 also shows the results of a final conditioning phase in which presentations of the stimuli continued as in the test session but were followed on each occasion by electric shock.
Conditioned suppression was acquired readily only by those subjects (group D in the figure) that received training with the stimuli presented in the ‘wrong’ context.
(e) Effects of a retention interval
In all the examples cited so far the dissociation of habituation and latent inhibition has taken the form of showing that certain procedures abolish the latter while leaving the former intact.
There is, however, one procedure that has been found to produce apparently the reverse effect — Hall and Schachtman (1987) have demonstrated that leaving a long interval between the last session of pre-exposure to the stimulus and the first test session will result in the restoration of an habituated UR while leaving latent inhibition unaffected.
In this experiment, which used the same general procedures as those employed by Hall and Channell (1985 b ), rats received 16 days of pre-exposure to presentation of a light, enough for their tendency to orient to habituate.
They then remained undisturbed in their home cages for a further 16 days before experiencing the light again.
The frequency of occurrence of the OR returned to its original level.
But when light-food pairings were given in the test phase, latent inhibition was observed; subjects given this treatment acquired the CR no more rapidly than control subjects that went  straight from pre-exposure to conditioning without an intervening retention interval.
The dissociation produced by this procedure constitutes something of a theoretical puzzle.
The account of habituation offered in Chapter 2 held that dishabituation would occur after a retention interval when the input failed to match the (partly forgotten) representation of the stimulus — when the subject failed to recognize the test stimulus.
But if subjects failed to recognize the stimulus in the Hall and Schachtman (1987) study (as indicated by the occurrence of dishabituation), how could they show latent inhibition to that stimulus?
The suggestion put forward by Hall and Schachtman was that the dishabituation observed in their experiment was not the consequence of a failure of input and representation to match, but rather depended on a change in level of arousal.
They suggested that the likelihood of an OR might be determined not only by the specific state of habituation of the target stimulus but also by the extent to which the context is generally arousing (cf. the dual-process theory of Groves and Thompson 1970).
It might plausibly be assumed that one of the effects of a long retention interval is to restore the lost arousing properties of a familiar context.
If so, the restoration of the OR seen in these circumstances need not necessarily imply a change in the specific ability of the target stimulus to evoke its UR; rather it may mean that even a weak tendency to emit this UR can be amplified substantially by a high level of arousal.
(The further implication is that the change in the effectiveness of the stimulus that is responsible for latent inhibition is not susceptible to the effects of heightened arousal.)
Table 3.1 presents the design and results of a study (reported by Hall and Honey 1989 b ) that provides support for this dual-process interpretation.
Two groups of rats received 14 sessions of habituation training in context A with a light as the target stimulus.
By the end of this training, ORs were occurring on no more than 20 per cent of trials.
Both groups received a test session in which the light was presented in a different context (B) but one with which they were familiar in that all subjects had received 14 sessions of exposure to it.
For one group (A-L/B in the table) these sessions of exposure to B followed habituation training in A, whereas for the other group (B/A-L) they preceded habituation training, which itself immediately preceded the test.
Table 3.1 reveals that the OR was not restored after the retention interval when exposure to the test context filled the interval — habituation of the specific response to the light must therefore have survived the retention interval.
On the other hand, the effects of pre-exposure to the test context did not survive the 14-day retention interval during which the light was presented in another context — subjects in group B/A-L showed a restoration of the OR similar to that seen in subjects transferred to an entirely new context.
It seems that the arousing properties of a context will be re-established over a retention interval and will boost the frequency of occurrence of an habituated response that is otherwise unlikely to occur.
(f) Conclusions
On close inspection, several of the demonstrations of a dissociation between latent inhibition and habituation turn out to be of less theoretical significance than they first seemed.
They consist, for the most part, of demonstrations that a certain procedure can abolish latent inhibition but will leave habituation intact.
This finding is open to the criticism that it reflects no more than a difference between the two phenomena in their sensitivity — that the procedure in question disrupts some process common to both phenomena and that the latent inhibition procedure provides a more sensitive measure of this disruption than does the habituation procedure.
This criticism not easy to meet but perhaps the best argument available comes from the experiment described last.
Here it was the habituated OR that was sensitive to a change in arousal level whereas latent inhibition was not.
To accept the implication of this finding makes it difficult to explain away the dissociation obtained by Hall and Channell (1985b)(the observation that latent inhibition is context dependent when habituation is not), which came from an experiment using the same response measures, stimuli, and procedures as were used by Hall and Schachtman (1987).
Taken together, these two experiments constitute a double dissociation between habituation and latent inhibition.
The force of the argument just presented may not be overwhelming but it is enough to prompt the tentative conclusion that, in addition to the habituation process invoked by exposure to a stimulus, some other process comes into play and is responsible for latent inhibition.
The mechanism responsible for latent inhibition may still be a loss of stimulus associability but we need some new account of how this loss comes about.
A range of rival theories is considered next.
An important feature of a successful theory will be an ability to explain (since this is where the habituation theory fell down) why latent inhibition should be context-specific.
One of the most persuasive interpretations of this fact, and that to be considered next, derives from  Wagner's (1976, 1981) theories.
Ironically this account was originally presented as a theory of habituation.
CS predictability and latent inhibition
Wagner's theory
Wagner's (1976, 1981) theory of habituation, discussed at length in Chapter 2, is also a theory of latent inhibition.
It will be recalled that, according to this theory, a stimulus is held to be fully effective only when it is able to generate the Al (primary activation) state in the node that constitutes its central representation.
A node that is in the A2 (secondary activation) state (either because its stimulus has only recently been presented or because it has been activated internally by means of an excitatory associative link) will not be able to move into Al.
Accordingly, a stimulus will be relatively ineffective both when it has itself recently occurred and also when it is presented along with cues that have previously signalled its occurrence.
(In the case of habituation training, these cues will normally be those that constitute the context in which the stimulus has been presented).
So far we have considered only the extent to which changes in stimulus effectiveness might be revealed by changes in the ability of a stimulus to evoke its UR; but, according to the theory, a loss of effectiveness will have other effects.
In particular, a stimulus can come to acquire associative strength as a CS only when it is represented in the A1 state in company with another active node — excitatory conditioning will occur when the US node is in A1; inhibitory conditioning when the US node is in A2 (see Chapter 1).
Thus the conditions of training that generate habituation of the UR (i.e. those that put a node into the A2 state) will also result in the stimulus acting as only an inadequate CS — that is, they will also produce latent inhibition.
Like habituation, therefore, latent inhibition can be the result of both associative and non-associative mechanisms, since the A2 state can be established both directly (as the after-effect of presentation of the stimulus itself) and associatively (by presentation of contextual cues).
In its latter aspect the theory can be seen as a formalization, in associative terms, of the very reasonable suggestion that a stimulus is likely to receive a full measure of processing only when its occurrence is unexpected (i.e. not predictable on the basis of other, usually contextual, cues).
The extensive discussion of the topic in Chapter 2 led to rejection of the account offered by Wagner for habituation, but this may be just as well as far as his theory of latent inhibition is concerned.
For the empirical evidence discussed in the first section of this chapter led to the conclusion that habituation and latent inhibition are subserved by different mechanisms; it follows that a theory based on the assumption of a common mechanism must be wrong in one way or another; the fact that Wagner's theory is inadequate  as an account of habituation provides no reason to reject its explanation for latent inhibition.
I turn next, therefore, to an examination of its two chief predictions about latent inhibition.
They are, first, that it should be possible to distinguish different forms of latent inhibition based on associative and non-associative mechanisms, the former being short-lived and the latter more nearly permanent.
Second, long-term latent inhibition depends upon the target stimulus being predicted by other cues and should be disrupted when the stimulus occurs in the absence of these cues (in particular, in a new context).
Short-term latent inhibition
Most studies of latent inhibition use procedures in which there are many widely spaced presentations of the target stimulus and there is a long interval (frequently of 24 h) between the last pre-exposure trial and the first trial of conditioning.
Such procedures are right for generating the phenomenon in Wagner's associative form — a series of widely spaced exposure trials will promote the formation of a context-stimulus association; and the after-effects of presentation of the target stimulus could not be expected to survive a 24-h retention interval.
But it should also be possible to generate a short-lived form of latent inhibition in which the A2 state is an after-effect of the occurrence of the target stimulus.
When the interval between exposure and the start of conditioning is short enough to ensure that this after-effect has not dissipated, latent inhibition should be particularly marked having contributions both from the short-term and the associative mechanisms.
The size of the latent inhibition effect should decline as the interval is increased and the short-term effect is lost; and when the exposure conditions are poor for association formation (e.g. when there is just a single, brief exposure) there may be only a short-term effect and no latent inhibition at all at the longer interval.
(a) Effect of the interval between exposure and test
In order to test these predictions we need to compare the size of the latent inhibition effect found after a long exposure — test interval (which we may take to be 24 h or more) with that found after a short interval (when conditioning follows exposure immediately or after a few minutes).
The matter was first investigated by Siegel (1970)(but see also Lubow et al .
1968) who found, using an auditory stimulus and aversive conditioning, that latent inhibition was as substantial with a 24-h interval as when there was no delay between exposure and conditioning.
Siegel's result has been confirmed by others using similar training procedures (James 1971; Crowell and Anderson 1972).
But evidence consistent with Wagner's account has come from experiments using the flavour-aversion procedure.
Figure 3.7 summarizes the design and results of an experiment by Best and Gemberling (1977)(see also   
Best and Barker 1977) in which rats were given access to 5 ml of a distinctively flavoured solution at various intervals before a conditioning trial on which consumption of this solution was followed by an injection of LiCl.
It is apparent that the aversion developed only poorly with an exposure-test interval of 3.5 h but that conditioning proceeded more readily (latent inhibition was attenuated although not abolished) when the interval was increased.
*
An attenuation of latent inhibition with a long interval between exposure and conditioning has not always been found, even in flavour-aversion learning — Nachman and Jones (1974) and Siegel (1974) have found the latent inhibition effect to persist in strength at intervals of much more than 3.5 h.
Bond and Westbrook (1982) suggest that the source of the discrepancy can     be found in the amount of the flavoured substance experienced by the subjects on the pre-exposure trial — Best and Gemberling (1977) gave only brief exposure whereas the other experimenters gave rather more.
Direct tests of this suggestion have been conducted in experiments by Westbrook, Bond, and Feyer (1981), by Bond and Westbrook (1982), and by Westbrook, Provost, and Homewood (1982), in which the duration of the initial pre-exposure trial was varied.
All three studies confirmed the Best and Gemberling (1977) effect when the exposure duration was brief but also found that more prolonged initial exposure generated latent inhibition both when the exposure-conditioning interval was short (3–4 h) and when it was long (24 h or more).
Figure 3.8(a) summarizes the effect seen in the experiment by Westbrook et al .
(1981).
* Westbrook and his collaborators interpreted their results as supporting Wagner's account of latent inhibition, arguing that prolonged experience of the stimulus in the context might be necessary for the   formation of the association on which long-term latent inhibition is held to depend.
A brief exposure will, therefore, produce only the short-lived effect.
Longer exposure will produce both this effect and the associative version, making it much more difficult to see any decline in latent inhibition at the longer intervals.
It may be noted that the experiments by Siegel (1970), James (1971), and Crowell and Anderson (1972), which failed to find any loss of latent inhibition at longer intervals, all gave repeated presentations of the target stimulus in pre-exposure; that is, they gave training likely to establish a context — target association.
Although these results appear to be consistent with the short-term/long-term distinction inherent in Wagner's theory, they do not require us to make the distinction.
We can just as readily conclude that there is just one type of latent inhibition; that the change induced by exposure to the stimulus always tends to dissipate during the exposure — test interval; and that the size of the interval required for such a loss of latent inhibition to become apparent grows longer as the strength of the effect induced by the initial exposure phase is increased.
An implication of the last point is that even when the initial exposure to the stimulus has been prolonged or has involved repeated presentations, it should be possible to detect some loss of the latent inhibition effect over time, provided the interval before the start of conditioning is long enough.
I have already said that a 24-h interval is not enough for such a loss to be seen.
But what will be the effect of extending the interval to a period of days or even weeks?
Experiments using the flavour-aversion techniques, comparing intervals of 1 day versus 24 days (McIntosh and Tarpy 1977), and of 1 day versus 21 days (Kraemer and Roberts 1984), have found that latent inhibition is diminished at the longer interval.
And equivalent results have come from at least some studies using the conditioned suppression procedure (e.g. Baker and Mercier (1982b), who compared intervals of 1 day and 5 days; Hall and Minor (1984), who compared 1 day and 8 days; but see also Crowell and Anderson (1972)).
(b) Contextual manipulations
Thus, the observation that brief exposure to a flavour produces latent inhibition only at short retention intervals (say 3–4 h) whereas more prolonged exposure produces latent inhibition at intervals of 24 h or more is not in itself proof of the suggestion that different mechanisms underlie the two cases.
What is needed is evidence that the phenomena supposed to reflect different short-term and long-term processes respond differently to various procedural manipulations.
Such evidence has been sought by Westbrook et al .
(1981), with mixed results.
They argue that prolonged exposure (to an odour in their experiments) will allow the formation of a context-stimulus association, wholly responsible for the latent inhibition seen at 24 h and partly responsible for that seen at 4 h.
A change of context between the exposure phase and the conditioning phase should, therefore, eliminate  latent inhibition at the 24-h interval and reduce that seen at the 4-h interval.
Westbrook et al .
found this to be so, and demonstrated also that the latent inhibition produced by brief exposure to the odour was unaffected by contextual change.
They argue further that it should be possible to eliminate the long-term effect by a procedure designed to extinguish a context — stimulus association — the procedure they used was that of giving a period of exposure to the context alone between initial exposure to the target and the conditioning phase.
(See below, p. 87, for a fuller discussion of the implications of this procedure.)
And indeed, they were able to show (see Fig. 3.8(b)) that such a procedure dramatically attenuated the latent inhibition produced by lengthy initial exposure both at4 h and 24 h but had no effect on the latent inhibition (at 4 h) produced by brief initial exposure.
Interpretation of these findings is not straightforward.
The sensitivity to contextual manipulations of the latent inhibition produced by lengthy exposure is certainly consistent with the suggestion that a context-stimulus association is responsible for the effect seen at 24 h and contributes to that seen at 4 h.
But an obvious implication of this last point is not confirmed by the observations.
As the latent inhibition effect seen at 4 h after lengthy pre-exposure is supposed to enjoy the benefit of contributions from both the short-term mechanism and from the context — stimulus association, this effect should be especially strong.
But no such result is round.
Westbrook et al .
(1981) conducted several experiments (Fig. 3.8(a) constitutes an example) that allowed the relevant comparison and all agreed in showing that the latent inhibition seen at 4 h was just as profound after brief exposure as after lengthy exposure.
They have no explanation to offer apart from the speculation that there might be an incompatibility between the two sources of latent inhibition allowed by Wagner's account, with the short-term version being developed only in conditions that preclude the development of the associative version.
There is no evidence bearing on this suggestion and the matter must remain unresolved for the time being.
We can conclude only that unequivocal evidence for the existence of the special short-term version of latent inhibition posited by Wagner's theory has yet to emerge.
Such evidence as there is derives from the studies by Westbrook et al .
(1981) in which contextual cues were manipulated.
We need now to examine the role of contextual factors directly and in more detail.
The context-specificity of latent inhibition
The evidence, already discussed in this chapter (pp. 75–8), demonstrating that latent inhibition shows context-specificity constitutes powerful support for Wagner's (1976, 1981) associative interpretation of the phenomenon.
Prior to the development of Wagner's (1976) theory, the role of contextual factors had received little attention (but see Anderson, O'Farrell, Formica, and Caponigri 1969; Anderson, Wolf, and Sullivan 1969; Dexter and Merill  1969).
The prediction of the theory, that latent inhibition should be diminished when pre-exposure and conditioning take place in different contexts, thus provoked a new line of experimental work and, as we have seen, the prediction has been convincingly confirmed.
But, however suggestive it may be, the fact that a given phenomenon is successfully predicted by a theory does not prove the theory to be correct.
Alternative accounts of context-specificity are available and they will be discussed in Chapter 4.
For the time being we shall concentrate on empirical studies concerned with specific further predictions made by Wagner's theory.
(a) Exposure to the context alone
According to Wagner's theory, a period of exposure to the context alone, given either before or after exposure to the target stimulus in that context, should attenuate latent inhibition.
Prior exposure to the context will produce latent inhibition of the contextual stimuli themselves (they will come to be reliably predicted by those cues that signal the start of an experimental session) and so the context — stimulus association will form only slowly, if at all, when presentations of the target stimulus are begun.
Exposure to the context alone given after latent inhibition training will, according to the theory, allow extinction of the context-stimulus association that has already been formed.
As latent inhibition is held to depend on the integrity of the context — stimulus association, both these procedures should alleviate the retarding effect of pre-exposure to the stimulus.
Investigation of these predictions has yielded mixed results.
We have already noted that the experiment summarized in fig. 3.8 (Westbrook et al .
1981) produced results suggesting that the latent inhibition consequent on relatively prolonged exposure to an odour can be abolished by extinction of the context.
But experiments using the conditioned suppression procedure have generated doubts about the generality of this conclusion.
Baker and Mercier (1982b) found evidence of the extinction effect in only two of their six experiments and were able to offer no very convincing account of why these two studies (which used a somewhat different test procedure from that used in the others) should have generated the result they did.
And a further set of six experiments by Hall and Minor (1984), which used procedures very similar to those of Baker and Mercier (1982b), found no sign of any loss of the latent inhibition effect when exposure to the context intervened between exposure and conditioning.
Hall and Channell (1985c) used the same general training procedures as had been employed by Hall and Minor (1984) but they reversed the order of the treatments in the critical condition; that is, they gave exposure to the context alone before the latent inhibition training.
Here, as Fig. 3.9 shows, a clear effect was observed.
Pre-exposure to the stimulus (a tone) produced latent inhibition but the effect was especially marked (i.e. the acquisition of conditioned suppression was particularly retarded) in subjects given prior    exposure to the context.
Thus, far from attenuating the latent inhibition effect as Wagner's associative theory requires, prior exposure to the context acted to enhance latent inhibition.
Hall and Channell (1985 c ) interpreted their context pre-exposure effect as being a special case of the more basic phenomenon of context-specificity.
They argued, first, that pre-exposure to a context will reduce the extent to which a subject is likely to learn about it (see Balaz, Capra, Kasprow, and Miller 1982); second, that presenting reinforcers (shocks in this experiment) in a context in which they had not previously occurred is functionally equivalent to transferring the animal to a physically different context.
The first suggestion implies that context pre-exposure should make subsequently acquired latent inhibition less likely to be context-specific — subjects need to be prepared to learn about the context if context-specificity is to occur.
Adding this to the second suggestion, it follows that the change of context produced by introducing reinforcers in the conditioning stage will have little effect on subjects given pre-exposure to the context but will limit the degree to which latent inhibition is shown by subjects not given pre-exposure.
What remains to be explained, of course, if this account is accepted, is why latent inhibition should show context specificity at all .
Although explicable as an instance of context-specificity the results of this experiment have  apparently put paid to the only explanation for the phenomenon that we have so far considered.
An alternative associative account
Although we may want to reject Wagner's (1976, 1981) specific formulation, it remains possible that other versions of associative theory might be able to explain the findings on context-specificity.
This is what has been claimed for an alternative associative account put forward by McLaren, Kaye, and Mackintosh (1989).
This theory develops the convergence (also noted by others, e.g. Sutton and Barto 1981; Gluck and Bower 1988) between standard associative models and connectionist systems of parallel distributed processing (e.g. Rumelhart, Hinton, and McClelland 1986) and it is intended to apply to a range of phenomena in perceptual learning.
For our present purposes, however, the important feature of the model is that it develops the implications of the widely accepted notion that even the simplest stimulus will consist of a set (probably a large number) of elements and that only some of these will be sampled (will activate their central representations) on a given exposure (cf.
Estes 1950; Neimark and Estes 1967).
Latent inhibition is held to be a consequence of the formation of associations among these elements.
More specifically, the model makes the usual associative assumption that excitatory links will be established between representations (of elements) that are activated concurrently.
Repeated presentation of a given stimulus, therefore, will allow a network of links to be established among the elements that go to make it up (particularly strong links being formed among those elements that tend to be sampled frequently).
After such training a given element will, when the stimulus is applied, be the target both of external input and of internal input by way of the associative links that connect it to other elements.
The model assumes the existence of a mechanism (a ‘modulator’) that detects any discrepancy between internal and external inputs, and, when the external input exceeds the internal, boosts the associability of the element to a degree proportional to the discrepancy.
A novel stimulus will lack internal inputs, will have a large discrepancy between external and internal inputs, and thus its elements will enter readily into associations.
For a familiar stimulus there will be little discrepancy and associability will be low — that is, latent inhibition will occur.
Contextual cues are given no special status in this scheme.
The context consists of a set of elements some of which will be sampled along with elements of the target stimulus on each exposure.
The associative network formed will thus include links among contextual elements and between these and elements of the target.
These latter, as in Wagner's theory, will play their part in lowering the associability of the target stimulus.
It will be apparent that this model can readily generate the prediction that  latent inhibition will be (to some extent) context-specific.
Presenting the target stimulus in a new context will eliminate some sources of internal input to the stimulus.
Furthermore, as context-stimulus associations are not the sole source of latent inhibition, the model need not be embarrassed by the observation that exposure to the context alone (either before or after exposure to the target stimulus) does not abolish the effect.
What remains a problem is to explain why exposure to the context alone given after exposure to the stimulus can be quite without influence on the magnitude of latent inhibition (Hall and Minor 1984) and why pre-exposure to the context should actually enhance the magnitude of the effect (Hall and Channell 1985c).
McLaren et al .
(1989) acknowledge the reality of these problems for their theory and present additional features of the connectionist model that might allow it to accommodate the findings.
One suggestion is that pre-exposure to the context, by preventing the subsequent formation of context-target associations, might enhance latent inhibition because it ensures that particularly strong associations will be formed among the elements of the target itself.
A further suggestion is that exposure to the context alone given after exposure to the target stimulus will result not only in extinction of context-stimulus associations but may also serve to attenuate the rate with which associations are forgotten.
The latter process will tend to counteract the loss of latent inhibition produced by the former.
These other features of the model have yet to be fully developed and experimental evidence that might allow us to evaluate them is sparse.
We can do no more for the time being, then, than acknowledge that a refined version of associative theory might be capable of dealing with features of latent inhibition that constitute problems for a theory which relies solely on the context — stimulus association for its explanation.
It may be noted, however, that the connectionist theory is in principle identical to that of Wagner (1976, 1981).
Both theories have as their basis the suggestion that a stimulus (or stimulus element) will be low in associability to the extent that it is the subject of excitatory associative influence — to the extent that it is predicted by other stimuli or stimulus elements.
Other theories of latent inhibition have taken a quite different view of the phenomenon.
They have suggested that the loss of associability suffered by a pre-exposed stimulus is determined not by the extent to which it is predicted by its antecedents but by the relationship it bears to subsequent events.
I consider these theories next.
Stimulus consequences and latent inhibition
Conditioned attention theory
(a) An outline
The theory proposed by Lubow and his collaborators (e.g. Lubow, Schnur, and Rifkin 1976; Lubow, Weiner, and Schnur 1981) starts by assuming that a  novel stimulus will evoke an ‘attentional response’.
This response is not directly equated with the OR or any component of it; rather it is an hypothetical construct evidenced in behaviour by its effect on the rate of conditioning — the associability of a stimulus is determined by the magnitude of the attentional response it evokes.
Repeated presentation of a stimulus is held to bring about a decline in the size of the attentional response.
This decline is postulated to be a classically conditioned decrement, with the consistent absence of any effective event following the target stimulus being viewed as the US that supports the conditioning of inattention (see Lubow et al .
1981, p. 5).
The assumptions about classical conditioning that are implied by this notion must be rather different from those embodied in the standard model.
It is necessary to assume, for instance, that the complete absence of an event can function as a US; that some mechanism exists whereby whatever association is formed during conditioning brings about a reduction in the magnitude of the unconditioned response (i.e. the attentional response) initially evoked by the stimulus that is trained as the CS.
Lubow et al .
(1981, p. 8), however, deny the need to specify a theory of conditioning and concern themselves with the empirical implications of the view that changes in attention will be governed by the known laws of classical conditioning.
We need to examine, therefore, whether procedures known to influence associative strength in orthodox conditioning will also influence the acquisition of inattention during stimulus exposure.
(b) Overshadowing, blocking, and related phenomena
The considerations just advanced led Lubow et al .
(1981, p. 14) to maintain that the development of latent inhibition, like the acquisition of associative strength, will be subject to overshadowing and blocking.
Overshadowing (the observation that the associative strength acquired by a target stimulus A is reduced when another event, B, is also present on reinforced trials) and blocking (the observation that prior reinforced training with B can effectively eliminate acquisition to A when AB trials are given) are primary characteristics of conditioning, found in all training procedures and in almost all organisms capable of classical conditioning.
It is critical for Lubow's general approach, therefore, to demonstrate their presence in latent inhibition.
In order to investigate this matter, Honey and Hall (1988) gave rats pre-exposure to a compound stimulus AB (a mixture of two flavours) followed by aversion conditioning with A alone as the CS.
Less profound latent inhibition than that shown by subjects given pre-exposure to A alone would constitute evidence for overshadowing of latent inhibition.
Other subjects (in a separate experiment) received pre-exposure to B alone before AB exposure.
If the acquisition of latent inhibition by B blocks that by A, then these subjects should show more rapid conditioning to A than those given    no experience of B before their AB trials.
The results, summarized in Fig. 3.10, show an overshadowing effect (group AB conditions more readily than group A) but far from producing blocking, pre-exposure to B alone allows AB exposure to generate a particularly marked latent inhibition effect.
This pattern of results is enough to make us doubtful about the interpretation of latent inhibition offered by Lubow et al .
(1981); but it remains to explain why latent inhibition procedures should apparently be capable of producing overshadowing but not blocking.
Honey and Hall (1988) offered the following.
Latent inhibition will be fully evident only when the subject is able to identify the stimulus used as the CS as being that presented during pre-exposure.
If the two are perceived as being different in some way (and with compound, AB, pre-exposure the presence of B might interact with A at a sensory or perceptual level making it discriminably different from the A used in conditioning), then generalization decrement would occur and the transfer of latent inhibition would be incomplete.
Thus the overshadowing effect can be explained without recourse to the mechanisms usually supposed to be responsible for it in associative learning (but see Pearce 1987).
And the notion of generalization decrement can also explain the effects of prior exposure to B alone if it is allowed that such training will make the B stimulus less effective and thus relatively unlikely to interfere with the perception of A when the two are presented in compound.
Pre-exposure to B, therefore, will attenuate or abolish the overshadowing effect.
Evidence in favour of Honey and Hall's interpretation comes from examining the conditions that are necessary for the overshadowing effect to emerge in latent inhibition.
In particular, earlier studies using the conditioned suppression procedure have provided little evidence for the effect
(Rudy, Krauter, and Gaffuri 1976; Baker and Mercier 1982a; Mercier and Baker 1985)(but see also Mackintosh 1973).
Thus, Mercier and Baker report a series of experiments in which rats pre-exposed to a compound of a clicker and a light acquired latent inhibition to the click just as readily as subjects pre-exposed to the click alone, and this in spite of the fact that the light was a salient event that was certainly noticed and processed by the animals.
Everyday experience tells us that a flavour mixed with some other will be perceived differently from that flavour presented on its own, but a click and a light (perhaps because they come from different modalities) are less likely to interact in this way.
If so, a click pre-exposed in compound with a light will be perceived as being identical to the click presented alone, and compound pre-exposure will thus be perfectly effective in producing latent inhibition.
In other words, considerations of generalization decrement give reason to expect an overshadowing effect in flavour-aversion experiments but not in the conditioned suppression experiments.
Evidence to support this conclusion comes from a further set of studies by Honey and Hall (1989b), who examined the effects of compound pre-exposure in the conditioned suppression paradigm using two auditory stimuli, a tone and a click.
Sharing a common modality may be enough to produce generalization decrement and thus, according to the account presented above, an overshadowing effect might be expected with these stimuli.
This is just what Honey and Hall (1989 b ) found — compound exposure attenuated the acquisition of latent inhibition by a tone when the other member of the compound was a click; but (as in the Mercier and Baker (1985) study) a salient light was ineffective as an overshadowing stimulus.
*
A further implication of this analysis concerns the related case in which the AB compound is used as the CS and pre-exposure is given to the A and B elements presented separately.
If A and B interact, then the components of the compound will not necessarily be perceived as being the same as the stimuli encountered during pre-exposure and latent inhibition will be attenuated.
Exposure to the AB compound itself will be much more likely to generate latent inhibition.
Conditioned attention theory appears to make the     opposite prediction.
To the extent that a given CR, separately acquired to each of two CSs, is likely to be evoked with particular vigour when the two CSs are presented together so the conditioned response of inattention will be especially strong when A and B have separately undergone latent inhibition training.
These rival predictions have been tested in a series of experiments by Baker, Haskins, and Hall (1990).
Figure 3.11 shows a representative set of results from an experiment using the flavour-aversion paradigm.
The stimuli and procedures were identical to those employed by Honey and Hall (1988; see Fig. 3.10) except, of course, the CS was the AB compound and pre-exposure was either with AB itself or with A and B presented separately.
As the figure shows, both pre-exposed groups showed latent inhibition (were more ready to consume the AB compound on test than subjects not given pre-exposure) but the size of the effect was much diminished in the group that experienced A and B separately.
Thus the interaction between these stimuli is just what would be expected from considerations of generalization decrement and is not that predicted by conditioned attention theory.
*  
(c) Exposure to a serial compound
Although, according to conditioned attention theory, exposure to a stimulus will inevitably lead eventually to a decline in its power to evoke an attentional response, this decline can be postponed (even for a time reversed) if the target stimulus (S1) is followed by some other (S2) during initial exposure.
The rationale for this suggestion is that, since S2 will itself initially evoke an attentional response, classical conditioning will ensure that this response comes to be evoked by S1, summating with the attentional response evoked by S1 itself.
With sufficient training, however, both stimuli will lose the ability to evoke attention.
It follows that after a moderate number of trials the amount of latent inhibition accruing to S1 when presented as part of the serial compound S1-S2 is likely to be less than that produced by training with S1 alone.
It may be noted that Wagner's (1976) theory makes the same prediction.
The presentation of S2 will, according to the theory, act as a distractor, limiting the subject's ability to process the target S1 and thus the development of the context — S1 association supposedly responsible for latent inhibition.
Experimental investigations of this prediction have produced a mixed set of results.
The left-hand panel of Fig. 3.10 (p. 92) contains an example of a study of the S1-S2 procedure (by Honey and Hall 1988) in which the stimuli 1120 were distinctive flavours.
Subjects in the A→B condition received pre-exposure to the target stimulus A followed by access to another flavoured substance (B).
The result in the figure show that this procedure generated just as much latent inhibition as did exposure to A alone.
This failure to find any effect of presenting the S2 during pre-exposure confirms the findings of an earlier set of experiments by Westbrook et al .
(1982), which, using very similar stimuli and training procedures, similarly found no effect.
It is something of a puzzle, therefore, that others using the flavour-aversion procedure (Best, Gemberling, and Johnson 1979; Kaye, Swietalski, and Mackintosh 1988 b ) should have been able to demonstrate an attenuation of latent inhibition.
The results of one of the experiments by Kaye et al .
(1988 b ) are summarized in Fig. 3.12.
They show that animals pre-exposed once to vinegar before a conditioning trial with this flavour consumed more vinegar on the test trials than did control subjects not given pre-exposure.
But subjects given vinegar followed by sucrose (VS) during pre-exposure showed no latent inhibition, consuming no more vinegar on test than did control subjects.
Although these results are compatible with conditioned attention theory (and with Wagner's theory), Kaye et al ,(1988 b ) prefer an explanation in terms of generalization decrement.
We have already acknowledged that latent inhibition can be expected only when the CS is identified as being the same as the pre-exposed stimulus.
If we accept (and the case has been argued in detail with respect to habituation; Chapter 2, pp. 50–3) that the    presentation of S2 modify the representation of S1 that is formed, then generalization decrement will ensure that little latent inhibition will be evident after S1 — S2 pre-exposure.
Kaye et al .
(1988 b ) support this interpretation by demonstrating that the VS pre-exposure treatment will produce perfectly good latent inhibition when the CS used in the conditioning phase is itself vinegar followed by sucrose, an arrangement that should ensure that the same representation of vinegar is activated on both occasions.
They further point out that the likelihood of such generalization-decrement effects will depend upon the exact nature of the stimuli (some pairs will interact more than others) and that not all combinations of S1 and S2 can be expected to generate an attenuation of latent inhibition.
Certainly, experiments using more orthodox conditioning stimuli (tones, lights and so on) have produced only scanty evidence that the presence of S2 during can influence latent inhibition.
Mercier and Baker (1985)(see also Baker and Mercier 1982a) report the results of an extensive and thorough series of experiments (using the conditioned suppression procedure and a clicker and a light as the stimuli), none of which yielded any indication that the S1-S2 procedure diminished the magnitude of latent inhibition.
On the other hand, Lubow et al .
(1976) have found that pre-exposure to a tone and a light presented serially generates less latent inhibition than pre-exposure to the target stimulus presented alone (a result also reported by Szakmary,(1977) in a study using a noise and a light as the stimuli).
Oddly, however, the S1-S2 procedure used by Lubow et al .
(1976) did not generate reliably  less latent inhibition than a control procedure in which the subjects received exposure to S1 and S2 presented uncorrelated with one another.
Why this control procedure should have attenuated latent inhibition is not clear, but evidently the presence of S2 can have this effect even when it is not (as is required by current theories) presented immediately after S1.
The source of the effect seen in subjects given the S1-S2 treatment is accordingly difficult to interpret.
The issue is made yet more obscure by the results reported by Matzel, Schachtman, and Miller (1988, experiment 1A).
Although their procedures were almost identical to those employed by Mercier and Baker (1985), they were able to demonstrate a total abolition of latent inhibition in subjects given S1-S2 pre-exposure.
I can only speculate about the reasons for their success.
One possibility concerns the nature of the stimuli they used — a tone as S1 and a click train as S2.
I have already suggested (p. 93) that tone and clicker (at least when presented simultaneously) might interact perceptually in a way that an auditory and a visual stimulus do not.
If the same applies with serial presentation, then the generalization-decrement argument offered by Kaye et al .
(1988 b ) for their own results might be applicable in this case too.
A second distinctive feature of the experiment by Matzel et al .
(1988) was that their S2 varied from one pre-exposure trial to the next.
S2 was always a 3-s train of clicks but the frequency of the train was varied being 3 Hz on some trials and 12 Hz on others.
There is a hint here that latent inhibition might be determined by the extent to which the target stimulus has consistent consequences — a notion inherent to the theory (proposed by Pearce and Hall (1980)) that will be discussed next.
(d) Conclusion
Lubow et al .
(1981) list 15 predictions that they derive from their conditioned attention theory and point out that many of these have received empirical confirmation.
If 1 have concentrated on just a few of these predictions this is, in part, because several of those neglected are not unique to the conditioned attention theory and are readily accommodated by its rivals.
The predictions that I have concerned myself with are rather more critical.
Lubow et al .
(1981) themselves place great emphasis on the S1-S2 procedure.
From their point of view the results are rather disappointing.
There is some sign that the presence of an S2 during pre-exposure might, in some circumstances, attenuate latent inhibition but little to suggest that the mechanism responsible is that envisaged by conditioned attention theory.
Even more worrying is the fact that latent inhibition does not suffer from overshadowing and blocking in the way that normal conditioning does.
This is enough in itself to make us want to reject conditioned attention theory as it is presently formulated.
The Pearce-Hall model
(a) An outline of the model
Perhaps the earliest statement of the view that latent inhibition depends upon the relationship between the target stimulus and its consequences was that offered by Mackintosh (1973), who argued that the phenomenon was one aspect of a general ability of animals to learn to ignore stimuli that predicted no change in reinforcement.
This notion was formalized in the model proposed by Mackintosh in 1975.
Recall that the standard associative model (see equation 1, p. 6) makes use of α, a learning rate parameter determined by the properties of the CS.
Mackintosh's suggestion was that the value of α (i.e. the associability of the CS) might be determined not only by the intrinsic qualities of the stimulus (such as its intensity) but also by the animal's past experience with the stimulus.
This notion was taken up in the model proposed by Pearce and Hall (1980)(and developed by Hall and Pearce (1982), and by Pearce, Kaye, and Hall (1982)), which was presented originally as an account of classical conditioning.
The central assumption of the model is that the α-value of a novel stimulus tends to be high (its exact value being determined by stimulus salience) but will decline with training according to an equation of the following sort: where α N represents the associability of a given CS after trial n the, magnitude of the US presented on the preceding trial, and  the associative strengths of stimuli having associations with the US representation.
In a simple conditioning procedure the value of V will grow from trial to trial as a result of CS-US pairings and in consequence the value of a will fall.
When a reaches zero (when λ, the outcome of the CS, matches V , what is expected on the basis of the CS), no further conditioning will be possible.
The general principles just outlined apply to latent inhibition training as to conditioning.
In such training the values of λ and V will be zero, and α will accordingly fall to zero.
Subsequent learning about the pre-exposed stimulus will then be impossible until a discrepancy between the values of λ and V is established (as will happen when the US is introduced at the start of the conditioning phase of a latent inhibition experiment) so that the value of α will be increased for the next trial.
It should be acknowledged that eq 3.1 must be viewed as only an approximation to the true state of affairs.
In particular, it is clear that associability is not (as eq 3.1 implies) determined solely by events occurring on the preceding trial.
Associability does not fall to zero after just one exposure trial and is unlikely to be fully restored by a single reinforced conditioning trial.
Rather, α needs to be determined by some sort of average of the values for a number of preceding trials as in the following equation:(3.2)
The value of the parameter γ (which will lie between 0 and 1) will determine the extent to which the outcome is weighted in favour of events occurring on the immediately preceding trial.
Returning to the more general picture, the essential notion that this formalization tries to capture is that the associability of a stimulus will be determined by how well the consequences of that stimulus are predicted.
A stimulus that is reliably followed by a given consequence (whether this be a US or no event at all) can be ignored.
Animals need to attend to and learn about a stimulus only when its implications for the future are uncertain.
The unique implication of this view is that associability will not be lost when a stimulus is followed by inconsistent consequences.
This implication has been subjected to a variety of experimental tests.
(b) Inconsistent reinforcement and transfer
A standard latent inhibition experiment investigates transfer from a pre-exposure phase in which the target stimulus is consistently associated with no subsequent event to a test phase consisting of reinforced training.
An experiment reported by Pearce et al .
(1982) examined the transfer effects produced by a pre-exposure procedure in which the outcome of the target stimulus was inconsistent.
Rats were given extensive pre-exposure to a tone which was followed on some randomly chosen trials by a mild electric shock and on other trials by no event at all(i.e. they received a partial reinforcement schedule).
It may be evident that eqns 3.1 and 3.2 above imply that a variation in the value of λ (i.e. in the magnitude of the reinforcer) from one trial to another will tend to maintain the value of α.
In a second stage of training the tone was followed consistently by a stronger shock and the acquisition of conditioned suppression was monitored.
The results, summarized in Fig. 3.13, reveal some evidence of latent inhibition in that these subjects acquired suppression less readily than control subjects that had received initial exposure to a stimulus other than the tone.
But the latent inhibition effect was not profound — rats given partial reinforcement during pre-exposure learned much more readily during the test phase than a group of subjects that had experienced the tone followed consistently by the mild shock during pre-exposure.
The slow learning of the group given consistently reinforced pre-exposure can be taken as being a special case of latent inhibition — of associability being lost by a stimulus followed by a consistent consequence.
The results for the partially reinforced group show that this latent inhibition effect will be attenuated when the reinforcement presented during pre-exposure to the tone is inconsistent.
It will be recalled (see p. 97) that the most convincing demonstration of an  effect in latent inhibition came from the experiment by   
Matzel et al .
(1988), and that these experiences used an  that changed (being a train of clicks that could vary in frequency) from one trial to the next.
The findings of Matzel et al .
can be accommodated by the Pearce-Hall equations if it is accepted (as seems reasonable) that the different event used as  differed in qualities that determine the value of λ used in these equations.
(c) The orienting response in rats
A transfer test is an appropriate way of assessing the value of α since the Pearce-Hall (1980) model assumes (as do similar models) that the value of this parameter determines the readiness with which a CS will enter into associations.
It is possible (although not necessary), however, that other aspects of an animal's behaviour toward a stimulus might be determined by the value of α.
One possibility advanced by Kaye and Pearce (1984) is that the behavioural orienting response (OR) shown by rats to a localized stimulus might serve as a direct index of α.
Certainly the OR shows some of the right properties — it has a high likelihood of occurrence when the stimulus is first presented and tends to decline with repeated presentation (see Fig. 3.1, p. 70).
But such an effect is predicted, of course, by any theory of habituation.
More significant, therefore, are the other results shown in Fig. 3.1.
One group of subjects (labelled ‘Control’ in the figure) experienced only reinforced conditioning trials.
Although it occurred more slowly than for subjects given non-reinforced pre-exposure, loss of the OR occurred in control subjects too, a result consistent with the view that α will decline when the CS predicts a consistent consequence and that the OR reflects the value of α.
An effect of this sort was reported some years ago by Grastyan (1961), who monitored an electrophysiological correlate (hippocampal slow waves) of the overt OR during conditioning and found that the electrical response diminished as the CR emerged.
He concluded that, with conditioning, ‘the significance of the stimulus becomes ‘certain’ and as a consequence the ‘what-is-it reflex’loses its biological usefulness’(Grastyan 1961, p. 245).
Öhman (1983) reaches almost exactly this conclusion on the basis of formally equivalent experiments investigating conditioning of the human skin conductance response, as do Svendsrød and Ursin (1974) in their analysis of the acquisition of a conditioned emotional response by rats.
More novel are the results produced by a third group of subjects (‘Partial’ in the figure).
These received exposure in which the stimulus was followed on a random 50 per cent of occasions by the presentation of a reinforcer.
In these subjects the OR continued to occur at a high level throughout training, a result suggesting that a stimulus that fails to predict its consequences reliably will continue to receive some form of processing.
The essential feature of these results, that partial reinforcement tends to maintain orienting to the light, has been sought in a series of further experiments by Pearce and his collaborators and has been amply confirmed (e.g. Pearce, Wilson, and Kaye 1988).
Swan and Pearce (1988) have attempted to establish the generality of the principle that the strength of the OR will be inversely related to the predictive accuracy of the stimulus by using a procedure other than partial reinforcement.
In one of their experiments (the results of which are presented in Fig. 3.14) rats were given a serial conditioning procedure in which presentations of a light were followed, after an interval of either 10 s or 30 s, by food.
For subjects in the critical experimental condition the long interval was filled by one auditory stimulus (say a clicker) and the short interval was filled by a different stimulus (such as a tone).
Both these auditory cues should become associated with food and acquire conditioned reinforcing properties, but by virtue of its onset occurring in relatively close proximity to the food, the tone is likely to acquire somewhat more power than the clicker.
The animals received, therefore, some trials on which the light was followed immediately by a weak conditioned reinforcer and others on which it was followed by a more powerful conditioned reinforcer.
This effective variation in reinforcer magnitude should, according to equations 1 and 2 above, maintain the level of α.
The results displayed in Fig. 3.14 show that these subjects continued to orient to the light over the course of 16 ten-trial sessions.
Control subjects that experienced training in which the immediate consequences of the light did not differ in their reinforcing value from one trial to another (for these the interval between the offset of the light and food delivery was fixed and thus did not depend on whether it contained a tone or a clicker) showed a steady decline in the frequency of the OR.
Using the behavioural OR as an index of the value of α should not be taken to imply a belief that α is the sole determinant of this OR.
I have already argued in this chapter (p. 79) that the rat's level of arousal can help    determine the vigour of the OR; and, theoretically more important, in this and in previous chapters I have argued that the decline of this investigatory response represents the operation of a process of habituation.
The relation between the effects of habituation and those implied by the Pearce — Hall model can perhaps be made clear if we adopt the elaboration offered by Liddell (1950) of Pavlov's notion of the orienting reflex.
Liddell suggested that in addition to Pavlov's, what-is-it?, reflex, the presentation of a novel stimulus will also evoke a ‘what-happens-next?’ reflex.
The first of these reflexes will be subject to the influence of habituation and will decline as the subject becomes familiar with the stimulus, forms an accurate representation of it, or whatever.
The second will be evoked when the consequences of the stimulus are uncertain; it is this reflex that tracks the value of α. *
The behavioural OR (or investigatory response) shown by rats in the experiments just described must be presumed to reflect the state of both these reflexes.
The initial loss of this response produced by presenting the stimulus repeatedly alone will occur both because of habituation and because of a decline in the value of α, each of these changes influencing one   of the reflexes that contributes to the observed behaviour.
But once habituation has occurred, there is no reason to suppose that the ‘what-is-it?’ reflex will play a further role in the experiments discussed here.
These can be unambiguously interpreted in terms of the effect of predictive accuracy on the ‘what-happens-next?’ reflex.
(d) Partial reinforcement in pigeon autoshaping
In the autoshaping procedure, pigeons are confronted from time to time with presentations of an illuminated disc (a response key) signalling the delivery of food.
Classical conditioning principles suggest, what is indeed the case, that the bird will come to peck at the lit key.
It is well established (e.g. Gibbon, Farrell, Locurto, Duncan, and Terrace 1980) that partial reinforcement generates a higher rate of autoshaped responding than does continuous reinforcement.
Collins and Pearce (1985) have offered an interpretation of this effect in terms of the Pearce-Hall model for associability change.
They suggest that in pigeon autoshaping the level of associability of a stimulus might determine, in part, the likelihood that it will be responded to.
When pigeons attend to a discrete stimulus, it is suggested, they are also likely to peck at it.
Partial reinforcement, by maintaining associability (attention), will generate pecks that constitute components of the OR and these, by adding in to the CRs produced by the associative strength of the CS, ensure a high rate of response.
Evidence to support this interpretation comes from a series of studies of partial reinforcement in autoshaping by Collins, Young, Davies, and Pearce (1983), Collins and Pearce (1985), and Pearce, Kaye and Collins (1985).
They investigated the rate of autoshaped pecking controlled by the first of two keylights presented serially and were able to show that the rate was elevated when the first element was an inaccurate predictor of its consequences.
In a typical experiment the target stimulus (A) was followed, on different trials, by different stimuli (keylights X and Y).
For one group of subjects the A-X serial compound was followed by food and the A-Y sequence was non-reinforced; for another group the A-X and A-Y compounds were each equally often reinforced and non-reinforced.
Both groups thus experienced equivalent schedules of partial primary reinforcement.
But for the first group, stimulus A was sometimes followed by an event (X) having high associative strength and sometimes by one having no strength (Y).
For the second group, A was always followed by events having comparable strength (X and Y were both reinforced on 50 per cent of occasions).
The former condition generated a higher rate of response.
Honey, Schachtman, and Hall (1987) have confirmed this finding and have devised an explanation for it that follows directly from the Pearce-Hall model.
They point out that the model predicts that a stimulus will lose associability if it is consistently followed by a reinforcer of a given size but that if the size of the reinforcer varies from trial to trial then associability will be maintained.
The extreme  case is, of course, a partial reinforcement schedule in which food is sometimes presented and sometimes not.
In the serial conditioning procedure we must take account of the fact that stimuli possessing associative strength (and thus conditioned reinforcing power) follow the target stimulus.
Thus, for one group of subjects, A is followed on some trials by both primary and conditioned reinforcement whereas on other trials there is no reinforcer at all; for the other group, stimulus A is always followed by a reinforcer of some sort.
Accordingly the associability of A (and its tendency to evoke the OR) will be higher in the former case because the differences in reinforcer magnitude that follow the stimulus are particularly marked.
*
(e) Conclusion
The notion that the development of latent inhibition might depend (directly or indirectly) upon what follows the target stimulus during pre-exposure is to be found in several theoretical accounts of the phenomenon.
What sets the Pearce-Hall (1980) model apart is its prediction that a decline in associability occurs when a stimulus accurately predicts its consequences and that inconsistent consequences will attenuate or prevent the loss.
And, as we have seen, there is plentiful evidence from a variety of experimental procedures to encourage the conclusion that this prediction is well founded, The theories of latent inhibition that I considered earlier in this chapter (e.g. that proposed by Wagner) were concerned with phenomena that suggested that latent inhibition depends upon how well the target stimulus is predicted by its antecedents.
Clearly, theories of this sort must be extended in some way if they are to accommodate the fact that latent inhibition depends in part on what the target stimulus predicts.
But by the same token, theories like that proposed by Pearce and Hall must come to terms with the evidence that has been taken to show that what the stimulus is predicted by can also play a role.
The role of contextual factors
However much we may criticize the details of the explanation they offer (above, pp. 86–9), the clear fact that latent inhibition shows context-specificity (i.e. is most likely to occur when the context predicts the occurrence of the target stimulus) constitutes good support for theories that emphasize the role of stimulus predictability.
Theories that suppose latent inhibition to be a function of what the stimulus itself predicts have been   compelled to introduce special explanations to deal with the fact of context — specificity — and, as we shall see, they have done so with rather little success.
(a) Context in conditioned attention theory
Lubow, Rifkin, and Alek (1976) have explicitly investigated the role of the context in a series of experiments concerned with the effects of pre-exposure on discrimination learning in both children and in rats.
The procedure used with the rats was as follows.
Subjects were exposed to a distinctive odour in one or other of two discriminably different cages.
They were then trained to approach the source of an odour to obtain food.
Some subjects received a novel odour at this stage, others the pre-exposed odour.
Of these two main groups, half the subjects received training in the context with which they were familiar (that used for pre-exposure) and half received training in a novel context.
It was found that when the test environment was the same as that used for pre-exposure, the rats learned only with difficulty to approach the source of the familiar odour — that is, latent inhibition occurred.
This effect showed context — specificity in that when the familiar odour was presented in a novel test environment, learning proceeded more rapidly.
In fact the combination of an old stimulus in a new environment produced more rapid learning than any other arrangement.
In order to explain these results, Lubow, Rifkin, and Alek (1976) accept that the loss of the attentional response produced by pre-exposure must be modified by contextual factors.
They put forward a number of suggestions.
They suggest (see also Lubow et al .
1981) that a novel environment might serve as a dishabituator causing the original attentional response to be reinstated; in addition the novel environment might, quite independently of the stimuli presented in it, have an arousing effect that helps learning.
The mechanisms that might be responsible for these effects are in need of fuller specification but the effort would perhaps not be worthwhile since in fact they do not account for the results to be explained.
In particular, Lubow, Rifkin, and Alek (1976) themselves have shown that animals pre-exposed in one environment and tested in another can learn more readily than a control group exposed neither to the stimuli nor to the test apparatus.
For both these groups the attentional response should be present and the test environment arousing, and thus the considerations introduced by Lubow, Rifkin, and Alek (1976) give no grounds for predicting the result observed.
Accordingly it becomes necessary to consider the further possibility that learning might be exceptionally rapid when there is a contrast in novelty between the critical stimulus and the environment in which it is presented.
I have already discussed the notion of relative novelty in the course of an analysis of habituation (Chapter 2, pp. 44–5) and failed to come up with hard evidence that might require us to accept its reality.
More critical for our present concern (which is the need to explain the context-specificity of latent inhibition) is the fact that latent inhibition can be abolished by a change of context  even when the test context is fully familiar, provided that pre-exposure to the test context and the target stimulus have taken place separately.
With such a procedure there is no contrast in novelty between stimulus and context.
(b) The Pearce-Hall model
The Pearce-Hall (1980) model faces a further problem in attempting to deal with contextual effects.
The extra dimension comes from the fact that the model is based, in part, on experiments that use the likelihood of occurrence of a behavioural OR as a measure of the level of associability of a stimulus.
But as we have already seen (pp. 75–8), the OR and associability (as indexed by latent inhibition) can be dissociated.
In particular, a change of context will attenuate latent inhibition whether the test context be novel or familiar but will restore the OR only when a novel test context is used.
The results for the OR are just what the model expects.
There is no reason why a change of context should restore α but given that a novel context is likely to be arousing, some increase in the likelihood of the OR might occur.
No increase is to be expected (and none is found) when the test context is itself familiar.
The problem for the model is, therefore, to explain why latent inhibition should be attenuated when the value of α apparently remains low.
A possible solution to this problem was proposed by Hall and Honey (1989 b ).
They pointed out that the conditioning phase of a latent inhibition experiment involves a treatment (a change in the outcome of the target stimulus — the presentation of a US) that, according to the theory, should produce a change in the value of α.
A simple habituation test would produce no change in α.
Now the results of experiments on habituation of the OR have been taken as supporting the latter assertion — dishabituation fails to occur if the changed context is familiar.
And the latent inhibition results imply that α is very rapidly restored by contextual change.
This outcome can be predicted if it is allowed that a change of context causes changes in α to be determined solely or largely by events on the immediately preceding trial (i.e. if contextual change produced an increase in the value of γ in equation 3.2 above).
This interpretation amounts to saying that an animal will still recognize an habituated stimulus when that stimulus is presented in a new context but will be prepared to change very quickly its assessment of that stimulus as being of no significance.
However plausible this suggestion, empirical investigation has lent it no support.
In a recent unpublished experiment, Honey and Hall gave rats extensive experience of two contexts but presented the target stimulus in just one of them.
Subjects in the critical experimental condition then experienced a single conditioning trial in which the target stimulus was presented in the unaccustomed context and was followed by shock.
(See also Hall and Minor 1984.)
On this single trial the α-value of the target must be assumed to be at the (low) level produced by pre-exposure — however rapidly α tends to change in a new context, the change will require at least one trial to occur.
Accordingly, this single-trial procedure should eliminate the tendency of the contextual change to attenuate latent inhibition.
The results of the test trials (Fig. 3.15) in which the CS was presented again (in the conditioning context) show no such thing.
Latent inhibition was still found to be attenuated by the change of context.
It will be evident that the Pearce-Hall theory finds it difficult to deal with the effects of contextual factors.
But this difficulty is not enough to make us abandon the theory.
The weight of evidence on the effects of predictive accuracy makes it essential to accept some version of the notion that stimulus consequences help determine the level of α.
What seems inescapable, since a change of context does not seem to restore α, is the conclusion that a change in the level of α is not the sole determinant of the retardation of subsequent learning produced by prior experience of the stimulus.
The next chapter evaluates a range of further possible sources of latent inhibition.
4.
Latent inhibition as associative interference
The term proactive interference is used to refer to the fact that prior learning can interfere with the acquisition, retention, or use of new information.
At the operational level, therefore, latent inhibition is a good example of the phenomenon, and any theory of latent inhibition can be seen as proposing a mechanism by which proactive interference occurs.
The theories discussed on the previous chapter were united in arguing that the source of this interference is to be found in a loss of associability (conditionability, effectiveness, attention-attracting power) by the critical stimulus.
But in this context the term ‘interference’ is commonly used more narrowly to designate those theories that try to explain latent inhibition in terms of the interaction of standard (usually associative) processes of learning or performance and without recourse to attentional constructs of the sort employed by the theories discussed in Chapter 3.
In this Chapter I shall begin by briefly considering the (rather wide) range of interference theories that have been applied to latent inhibition.
Having identified the most likely candidates I shall then consider the critical experimental evidence (which again derives from studies of the role of contextual factors).
Finally, I shall attempt to devise a synthesis that can incorporate the successful features of several theoretical approaches (including aspects of the attentional theories discussed in Chapter 3).
Interference theories
Interference with conditioning
The theories to be discussed in this section of the chapter all suppose that something is learned during the pre-exposure phase of a latent inhibition experiment that interferes with the formation of the CS-US (conditioned-unconditioned stimulus) association during the conditioning phase of the experiment.
They differ from the theories described in Chapter 3, therefore, only in that they do not ascribe the poor acquisition to a change in the value of some attention-like process.
Interference theories have taken a variety of forms.
(a) Interfering responses
A possibility originally considered by Lubow and Moore (1959) as an explanation for their newly discovered latent inhibition effect was that during pre-exposure the subject might come to perform some response to the stimulus that interfered with the response monitored during conditioning.
The    obvious inadequacy of this explanation is that it begs the question of why exposure to a stimulus should allow the acquisition of a new response — we usually suppose only that existing unconditioned responses (URs) will habituate.
Furthermore, there are several empirical results (some provided by Lubow and Moore (1959) themselves) that speak against it.
Perhaps the clearest instance is that reported by Halgren (1974) and summarized in Fig. 4.1.
Rats that had been pre-exposed to a tone were trained on a task in which presses on a lever in the presence of the tone yielded reward but responses in the absence of the tone did not.
As the figure shows, the subjects came to respond only when the tone was on, but they formed the discrimination less rapidly than control subjects that had not received pre-exposure.
Their slow learning cannot be explained by assuming that the pre-exposed tone tended to evoke a response that interfered with lever pressing.
Figure 4.1 shows that the performance of another group of subjects that, after pre-exposure, was required to lever press in the absence of the tone and to refrain from pressing in its presence.
These subjects too learned less readily than controls in spite of the fact that the postulated ‘interfering’ response should actually have proved facilitatory in this case.
(b) Inhibition
According to the theories of associative learning presented in Chapter 1, certain conditions of training (roughly, those in which the CS predicts the  omission of a US that might otherwise be expected to occur) allow the formation of an inhibitory CS-US link.
A consequence of the existence of such a link is that excitatory conditioning will be retarded when an inhibitory CS is used to signal the occurrence of the US.
Our use of the term ‘latent inhibition’ is a remnant of the suggestion (Lubow and Moore 1959) that simple pre-exposure might allow a stimulus to acquire inhibitory properties.
This suggestion has proved unpopular, in part because simple non-reinforced pre-exposure is not a procedure that, according to most current theories, should be capable of generating inhibition.
It has also often been argued that there is empirical evidence that rules the suggestion quite out of court by demonstrating that a pre-exposed stimulus quite lacks the properties that have been taken as defining for an inhibitory CS (Rescorla 1969).
But in fact the experimental results turn out to allow of more ambiguity than has commonly been supposed.
One set of results concerns the transfer from latent inhibition training to inhibitory conditioning in which the pre-exposed stimulus is explicitly trained as a CS-, a stimulus that signals the omission of the reinforcer.
If pre-exposure endows a stimulus with inhibitory properties then such a stimulus should readily come to serve as a CS- since the inhibition already acquired should give a head start.
And as we have already seen, Halgren's (1974) experiment shows that pre-exposure results in retarded conditioning, both when the stimulus is used as a CS+ (signalling reinforcement) and when it is used as a CS-.
(See also Rescorla 1971.)
Although unfortunate for the view that latent inhibition can be equated with conditioned inhibition, these results do not in fact constitute a death blow.
Hall, Kaye, and Pearce (1985) have pointed out that just as latent inhibition can occur during the CS-US pairings of excitatory training (producing a fully trained CS+ that will form further associations only slowly), so latent inhibition can also be expected to occur during inhibitory training.
Hall et al .
(1985) presented evidence that inhibitory training procedures produce a CS- that is subsequently learned about rather slowly even when the test procedure was one that required further inhibitory learning.
They interpreted this finding as showing that associability lost by the stimulus during the first stage of training could outweigh the transfer deriving from the fact that both stage of the study involved inhibitory learning.
Whatever the merits of this interpretation, the fact remains that pre-training a stimulus as a CS- has been found, on occasion, to retard further inhibitory learning.
Accordingly, the demonstration that latent inhibition training will also retard subsequent inhibitory conditioning can hardly be taken as evidence that latent inhibition is quite different from conditioned inhibition.
A further feature of an inhibitory CS is that it will reduce the ability of a separately trained excitatory CS to evoke its CR when the two stimuli are presented as a compound.
There is some doubt as to whether a latent inhibitor  can produce this effect.
The experimental results are mixed.
Rescorla (1971) found that the CR remained undiminished when a latent inhibitor was added to an excitatory CS whereas experiments by Kremer (1972) and by Reiss and Wagner (1972, experiment 2) have both found that the compound tends to evoke less conditioned responding than the excitatory CS alone.
The proper interpretation of the latter finding is unclear.
It is certainly what would be expected if the latent inhibitor functions like a CS- but it could just as easily be a consequence of generalization decrement — adding another stimulus to the excitatory CS might modify the way in which the latter is perceived and thus reduce its ability to evoke the CR.
The role of generalization decrement in these experiments can be investigated by comparing the effects on excitatory responding of an added stimulus that has had little or no pre-exposure with those produced by a stimulus that has undergone latent inhibition training.
The ability of the added stimulus to disrupt normal perception of the excitatory CS should undergo habituation and thus the ‘inhibitory’ power of the pre-exposed stimulus might be less than that of a novel one.
This apparently decisive comparison turns out, unfortunately, to be nothing of the sort.
In the first place, there is a discrepancy in the experimental results — Reiss and Wagner (1972) found that a novel stimulus was more effective than a familiar one in disrupting the CR whereas Kremer (1972) found the opposite.
And worse, each result is susceptible to an alternative explanation.
That from the Reiss and Wagner study may imply only that, in their procedure, generalization decrement completely masks inhibitory effects; the result does not require us to deny the existence of the latter.
And the Kremer result does not require us to accept that a pre-exposed stimulus inhibits a CR in the way that a CS- does.
His experiment used the conditioned suppression procedure and his critical experimental finding was that a novel stimulus added to a pre-trained excitatory CS evokes more suppression than the compound of CS plus familiar stimulus.
An obvious explanation for such a result is that the former compound evokes more suppression simply because the UR (of suppression) evoked by the novel stimulus combines with the CR elicited by the CS+.
For a pre-exposed stimulus there will be no such effect since the tendency to evoke unconditioned suppression will have habituated.
Although these experimental results are indecisive, the theoretical point remains — the latent inhibition procedure is not that used to train a conditioned inhibitor.
Accordingly it would be surprising if animals learned the same thing about the stimulus in the two cases.
At the very least, conditioned inhibition training is likely to differ from latent inhibition training in that the former is likely to convey the information that a given event (a given US) will not occur whereas the latter could only convey that no event will occur.
Evidence that illustrates this point come from investigations aimed at the notion of ‘learned safety’.
(c) Learned safety
Workers studying the effects of CS pre-exposure on flavour-aversion learning have independently generated an account of latent inhibition that is formally exactly equivalent to the inhibitory notion just discussed.
Kalat and Rozin (1973) suggested that subjects given exposure to a novel flavour are capable of learning that the flavour predicts no aversive consequence, that the taste is ‘safe’.
Such learning would then interfere with a subsequent attempt to teach the subject a taste-poison association.
This hypothesis prompted a series of experiments by Best (1975) in which a given flavour was established as a conditioned inhibitor (or a signal for safety) by an explicit discrimination training procedure in which this flavour was presented along with a previously established CS+ but in the absence of the US.
The effect of giving non-reinforced pre-exposure to the target flavour was to reduce the readiness with which it came to function as a safety signal — that is, the outcome was the opposite of that predicted by the suggestion that the pre-exposed stimulus might already have acquired some of the properties of a safety signal.
The argument made here is that already applied to Halgren's (1974) results.
It is thus, of course, open to the objection that was raised previously — that a loss of associability incurred during pre-exposure might obscure the transfer effect generated by such inhibition as was also acquired.
But Best's experiment included another feature that helps establish the distinction between latent and conditioned inhibition.
His procedure for testing the properties of the target flavour was one in which the rats were given a choice between this flavour and some other.
He found (see also Batson and Best (1981)) that animals given his discrimination training procedure showed an enhanced preference for the safe flavour (the conditioned inhibitor).
Subjects given simple pre-exposure to the target flavour showed no such preference.
We must conclude then that, at the very least, a conditioned inhibitor is a stimulus having motivational properties that a latent inhibitor lacks; only in a very restricted sense, therefore, can the latter be regarded as an instance of the former.
(d) Learned non-correlation
Acknowledging the problems inherent in the learned safety hypothesis, Kalat (1977) suggested a revision which he referred to as ‘learned non-correlation’.
Rather than learning that ‘nothing bad’ follows a non-reinforced stimulus, the animal might learn that the stimulus predicts nothing at all, that the stimulus is not correlated with another event.
Weiss and Brown (1974) have put forward a similar proposal with their suggestion that latent inhibition is a consequence of the subject's learning a ‘zero correlation, between the target stimulus and other events.
It is, of course, necessary for these theorists to specify how the learning of  a zero correlation produces the effects it does.
At one level of analysis, this task poses no special problems.
As Weiss and Brown (1974) point out, conditioning occurs when correlations are arranged between pairs of events: excitatory conditioning when there is a positive correlation with the CS predicting the US; inhibitory conditioning when the correlation is negative so that the CS predicts the omission of the US (see, e.g. Rescorla 1967, 1968).
An animal that has previously learned that a stimulus is not correlated with other events will have learned something quite incompatible with what is true during conditioning and so negative transfer can be expected.
All this will be perfectly satisfactory to those who are willing to stop their analysis of conditioning at this level and who do not concern themselves with the detailed mechanisms that underlie the subject, s sensitivity to these various correlations.
Indeed, to some, the account offered by Weiss and Brown (1974) may seem to be over-elaborate.
Testa and Ternes (1977) reject even the notion of learned non-correlation.
Conditioning, they argue, is determined by the conditional probability of occurrence of the US given the CS, and of the US given no CS.
Exposure to the CS prior to the conditioning trials will mean that the conditional probability of US given CS (when assessed over all trials) will be lowered and the likelihood of the CSs being able to evoke a CR will be reduced.
(Baker and Mercier (1982 a ) offer a very similar interpretation.)
The mechanism by which animals might integrate these various events, spread out as they usually are over several sessions of training, and by which the critical conditional probabilities might be computed, are not considered.
Kalat (1977), by contrast, makes an attempt to express the notion of learned non-correlation in more traditional, associative terms.
He refers to latent inhibition training as producing a CS-US association, the US being described as consisting of ‘no event’(see also Hall et al .
1985).
Odd as this may sound, the idea that an association might be formed between a CS and the absence of some event is not without precedent in associative theorizing.
Konorski's (1967) well-developed and influential theory of conditioning deals with inhibitory learning by assuming that an inhibitory CS is one that activates a ‘no-US’ representation.
(See also Pearce and Hall (1980).)
The inhibitory CS is supposed to owe some of its behavioural effects to the fact that activity in the ‘no-US centre’ inhibits that in the ‘US centre’.
Something of the sort could apply in latent inhibition.
Our usual accounts of excitatory conditioning refer to associations between events referred to as CS and US, but in doing so they are guilty of over-simplification.
Each of these stimuli has many attributes.
An electric shock US, for instance, is an event occurring at a particular time, with a certain duration and intensity, that impinges on a particular part of the animal, and so on .
Instead of postulating a single link between CS and US we should perhaps imagine a whole collection of them (the exact number depending on which attributes of the stimuli are noticed  and encoded by the animal), all of which play some part in generating the CR.
One of these may be a link between the CS and a representation of ‘some event’.
If so, latent inhibition training that establishes a CS-no event association would retard the course of conditioning.
However elaborate (indeed, contrived) this theorizing may be, it is still not wholly adequate for the task in hand.
Applying to the inhibitory case the analysis of conditioning presented above leads to the suggestion that one of the associative links likely to be formed will be between the CS and a representation of no event.
The hypothesis that, for a pre-exposed stimulus, this association will already have been formed before the start of conditioning leads to the prediction that pre-exposure should facilitate inhibitory learning.
That it does not means that this theory, like the other versions of interference theory already discussed, needs to include some further process (such as a loss of associability) to explain the results.
(e) Concurrent interference
Revusky (1971, 1977) has developed an account of the role of interference effects in associative learning that has proved applicable to a wide range of phenomena.
The essence of ‘concurrent interference’ is the postulate that the formation of an association between the target event and some other will reduce the ability of the target to enter into other associations.
The particular case of greatest interest to Revusky is that of conditioning with some interfering event (X) presented in the interval between CS and US.
The concurrent formation of CS-X and X-US associations is held to interfere with the formation of the target, CS-US, association.
As the theory is explicitly concerned with concurrent interference, its application to latent inhibition (an instance of proactive interference) has not been fully worked out.
But Revusky (1971) offers the following possibility in discussing flavour-aversion learning.
During pre-exposure, the consumption of a flavoured substance will be followed by certain consequences with which an association could be formed; if the flavour is presented as a fluid, for instance, consumption will be followed by an alleviation of thirst.
But the exact nature of the associate does not matter — the mere existence of a CS-X association will interfere with the ability of the pre-exposed flavour to form a new association as is required in both excitatory and inhibitory conditioning.
Although Revusky (1971) does not discuss the matter, there is a second mechanism by which concurrent interference theory might predict latent inhibition.
In discussing the effect produced when event X precedes the CS-US pairing of a conditioning trial, Revusky lays emphasis on the interference that results from the formation of an X-US association; but it might also be allowed that the formation on an X-CS association plays a role.
If so, an equivalent effect can be expected in latent inhibition.
During initial exposure, experience of the flavour will be accompanied by a consistent set  of antecedents comprising those cues that characterize the presentation of a container of fluid.
The X-CS association that results would then interfere with subsequent attempts to establish the CS-US association.
Other interference  theories attribute latent inhibition to the effects of an association between the pre-exposed stimulus and its consequences.
But concurrent interference theory can also allow that an association between the target stimulus and its antecedents might also contribute to the effect.
To this extent the theory can be seen as incorporating the central feature of Wagner's (1976, 1981) interpretation of the phenomenon — the suggestion that further learning proceeds slowly about a stimulus that has formed associations with its antecedents.
The similarity to Wagner's account becomes more marked when we consider the application of concurrent interference theory to procedures other than flavour pre-exposure.
Latent inhibition occurs readily with stimuli like lights and tones that have no obvious antecedents or consequences in the way that consumption of a flavoured fluid does.
We can suppose, however, that associations might be formed between the stimulus and the context in which it is presented.
The source of interference with formation of the CS-US association might thus be the previously established context-CS and CS-context associations.
The mechanism by which the context-CS association has its effect is different but the principle is just that adopted by Wagner (1976, 1981).
What follows from this analysis, of course, is that concurrent interference as an explanation of latent inhibition is open to the same objections as were raised to Wagner's theory.
The evidence in Chapter 3 that was taken as counting against Wagner's analysis of latent inhibition is just as damaging for the interference account developed here.
Although direct associations between the context and the target stimulus can play little part in the effect, it is still possible that associative interference might be responsible, in whole or in part, for latent inhibition.
A further possible source of latent inhibition emerges as soon as we accept that any stimulus can be construed as consisting of a set of elements.
Stimulus elements that are experienced together will become linked.
Latent inhibition training can be expected, therefore, to establish a network of associations among the component parts that go to make up the stimulus.
McLaren, Kaye, and Mackintosh (1989) made use of this suggestion in developing their account of latent inhibition.
Their proposal (see Chapter 3) was that the associability of an element activated by external stimulation will be enhanced when this element lacks associative connections.
The analysis offered by interference theory is similar, but rather simpler — it is that the existence of these (within   stimulus) associations will interfere with the ability of the elements to form new ones.
(f) Conclusions
Several of these attempts to devise an interference theory for latent inhibition fail to explain important features of the data and must be rejected.
The notion of a learned non-correlation fails for a rather different reason — it can accommodate the facts but largely because it attempts little more than a redescription of the facts to be explained.
Two accounts remain viable: the suggestion that pre-exposure allows the formation of a stimulus-no event association; and the suggestion that within-stimulus associations interfere with new associative learning involving the same stimulus elements.
Neither of these alternatives excludes the possibility that a change in associability might also play a part on generating latent inhibition.
The first of these accounts actually requires the additional assumption that associability declines during pre-exposure.
Without this the theory would find it difficult to explain why latent inhibition should be seen with inhibitory conditioning in the test phase.
The concurrent interference account does not require this assumption but can perfectly well allow that changes in associability might accompany the formation of within-stimulus links.
We have seen how McLaren et al .
(1989) makes use of this possibility but the mechanism involved need not be the one they describe.
It could just as easily be that proposed by Pearce and Hall (1980).
According to this interpretation, the associability of each stimulus element might be expected to decline as it becomes associated with others, just as a CS as a whole is thought to lose associability as it becomes linked with a US.
Retrieval failure
All the accounts of latent inhibition discussed so far have assumed that what is learned during pre-exposure interferes with the formation of the CS-US association.
Revusky (1971) call his theory a concurrent interference theory to emphasize just this point and to distinguish it from another commonly held interpretation of proactive interference.
According to this alternative view, the target association may be formed perfectly normally but since, on a subsequent test trial, both this and any previously learned association will become activated when the CS is presented, the subjects may fail to retrieve the ‘correct’ one and the likelihood of the CR will be reduced.
Indeed, the effects of retrieval failure may also be evident during the course of conditioning itself because any trial after the first can be viewed as being a test trial on which the subject is required to retrieve information acquired on earlier trials.
Clearly the retrieval-failure hypothesis will make many of the same predictions as other theories of latent inhibition.
But if interference effects do indeed operate at retrieval rather than on the formation of the CS-US association, it should be possible to show that an apparently weak association formed after conditioning with a pre-exposed stimulus can be revealed as being perfectly strong in appropriate conditions of testing.
(a) Reminder treatments
Miller and his colleagues (e.g. Miller and Springer 1973; Miller and Schachtman 1985) have shown that certain ‘reminder’ treatments are capable of restoring lost behaviour, thus demonstrating that the information necessary for the behaviour in question must have been present all along but was merely unavailable to the mechanisms responsible for controlling behaviour.
For example, rats given electroconvulsive shock (ECS) immediately after learning a simple avoidance response fail to show this response on a subsequent test, a result that has been interpreted as showing that the ECS destroys the original memory trace.
But this interpretation is challenged by the observation that a reminder treatment (foot-shock administered in a different apparatus from that used to train the avoidance response) will allow the avoidance response to appear on a subsequent retention test.
Rather than destroying the original trace, it is argued, the effect of the ECS is to make retrieval more difficult.
Kasprow, Catterson, Schachtman, and Miller (1984) have applied this logic to latent inhibition.
In their experiment, rats that have been pre-exposed to a noise prior to noise-shock conditioning trials showed latent inhibition in that, on a final test session, the noise elicited only a weak CR.
(The results of the test session are shown in Fig. 4.2.)
But subjects that experienced a reminder treatment (unsignalled foot-shocks given in a different apparatus) between the conditioning trials and the test session showed almost as much conditioned responding as control subjects that had not been pre-exposed to the noise.
Kasprow et al .
are not able to specify how the reminder treatment produces its effect but they are quite at liberty to use the unexplained phenomenon to make their central point — that the deficit in performance see in the subjects given pre-exposure stems from a failure of retrieval rather than a failure of initial acquisition.
These results certainly require us to take seriously the notion that retrieval process play a part in determining the outcome of a latent inhibition experiment.
We must acknowledge, however, that another interpretation is possible.
Even those who hold that the latent inhibition treatment interferes with initial acquisition would allow that some association between the CS and US (albeit a weak one) will be formed.
It is assumed that so-called reminder treatments act in some way to increase the likelihood that associations (especially weak ones that will be most in need of such help) will be effective in generating overt conditioned responding, then the pattern of results observed can be accommodated.
This assumption is arbitrary, but no more so than the assertion that reminder treatments somehow make available information that could not otherwise be retrieved.
(b) Effects of a retention interval
Kraemer and Roberts (1984)(see also Kraemer and Ossenkopp 1986; Kraemer, Hoggman, and Spear 1988) have invoked results from a rather different experimental procedure in seeking support for the retrieval-failure account of latent inhibition.
They conducted flavour-aversion conditioning with rats given prior exposure to the substance used as the CS (chocolate milk).
The strength of the aversion was then tested; on the day after conditioning for some rats, and 21 days later for others.
Although those tested after one day showed only a weak aversion (i.e. latent inhibition was observed in this condition), more of an aversion was evident in those given the delayed test.
Apparently the CS-US association must have been formed relatively normally, even in subjects given pre-exposure to the CS, because appropriate conditions of testing (i.e. the use of a long retention interval) were able to reveal the existence of a strong aversion.
Kraemer and Roberts (1984) interpreted their results as implying that the pre-exposure procedure and the conditioning procedure establish independent memories and the ability of the former to interfere with retrieval of the latter declines over the retention interval.
We should note that this analysis involves an element of special pleading.
Theories of proactive interference (see, e.g. Spear 1978) usually assume that the capacity of the first-formed memory to interfere with that formed  second tends to increase as the retention interval is extended.
But in this cases it is necessary to assume the opposite in order to explain the results — to assume that what is learned during pre-exposure interferes less after 21 days than after one day.
Kraemer and Roberts (1984) offer as justification for this assumption the suggestion that ‘important’ memories retain the ability to be retrieved even after a long retention interval whereas less important memories do not.
If the memory of the association of a given flavour with illness can be taken to be more important than the memory that the flavour has also been experienced without harmful consequences, then the latter memory would interfere after a short but not after a long retention interval.
This suggestion is not implausible but it remains to specify the mechanism by which the content of a memory might act to determine its retrievability.
A  further problem arises from the fact that the experimental results of critical importance are far from clear-cut.
Although Kraemer and Roberts (1984) were able to demonstrate an apparent loss of latent inhibition over a long retention interval using chocolate milk as the CS, no effect was apparent when saccharin was used.
And Kraemer and Ossenkopp (1986), in an experiment formally equivalent to that reported by Kraemer and Roberts, were able to find only a small and statistically unreliable effect using milk as the flavour (see Fig. 4.3).
Their results did, however, confirm another effect evident in the original Kraemer and Roberts study — that the effects of a long retention interval can be very apparent when pre-exposure is given to a stimulus different from that used for conditioning and the test.
Figure 4.3 includes the test results for a pair of groups given pre-exposure to saccharin before being conditioned and tested with milk.
Such pre-exposure is apparently capable of generating latent inhibition since the group tested one day after conditioning (SM-1 in the figure) showed little evidence of having acquired an aversion, consuming just as much of the test substance as those subjects actually given latent inhibition training with this substance (group MM-1 in the figure).
When tested after 21 days, however, the latent inhibition effect produced by pre-exposure to saccharin has disappeared.
Subjects given this test (group SM-21) now show a strong aversion.
* It is not clear why the effect of the retention interval should be especially marked when the pre-exposure flavour is different from that used in the subsequent phases of the study.
One possibility is that generalized latent inhibition is likely to be weaker than that produced by pre-exposure to the CS itself and thus is more likely to be susceptible to the effect of the long interval.
This suggestion is, unfortunately, no more than speculation at this stage.
(c) Conclusions
The evidence summarized above will not be enough to convert adherents of other theories to the view that latent inhibition is a consequence of the subject's failing to retrieve the relevant information about the CS.
The evidence is suggestive but not conclusive.
It should be noted, however, that to accept the retrieval-failure account of latent inhibition does not necessarily imply a rejection of all other theories of the phenomenon.
Those who have espoused the notion of retrieval failure have not usually been much concerned with specifying what it is that is learned during pre-exposure; rather, their experiments have been designed to show that, whatever the source on interference may be, it acts at retrieval rather than on the formation of the CS-US association.
The hypotheses considered in the preceding section of this chapter (that pre-exposure allows the formation of a stimulus — no event association, and so on) still remain viable provided it is allowed that the associations they envisage can still be assumed to interfere with retrieval.
What is more, the experimental results that have been taken to demonstrate that retrieval plays a part in latent inhibition cannot demonstrate this failure to be the sole  source of the effect — these experiments show that a CS-US association is indeed formed after latent inhibition training and can be revealed if the conditions of testing are appropriate; they do not show convincingly that the association is just as strong as that formed in subjects given no pre-exposure to the target stimulus.
In other words, pre-exposure to the stimulus may both retard the acquisition of the CS-US association and act to interfere with the retrieval of the information embodied in this association when it comes to a test trial.
Discussion of these various possibilities must remain the merest speculation, however, in the absence of convincing evidence demonstrating competition between associations at retrieval.
I have already noted that the experimental results presented so far have not been conclusive.
There is, however, evidence from a quite different experimental procedure that can be taken to confirm that retrieval mechanisms play an important role in the phenomena considered here.
Evaluation of this evidence requires us to consider once again(see Chapter 3) the role of contextual factors.
Contextual factors in retrieval
Students of human memory have long recognized that performance on a retrieval task tends to be superior when the test context is similar to that experienced during initial training.
This effect, sometimes referred to as ‘encoding specificity’(e.g. Tulving and Thomson 1973; Tulving 1983) has been subject to a range of interpretations but what concerns us here is the general possibility that contextual factors might determine test performance by modulating the ease with which stored information is retrieved.
This notion has been adopted by a number of animal learning theorists (e.g. Spear 1973, 1981; Hirsh 1980).
Its relevance to  our present concern is as follows.
Latent inhibition shows clear context-specificity — that is, the retardation of conditioning produced by pre-exposure to the stimulus is attenuated when pre-exposure and conditioning occur in different contexts (see Chapter 3, pp. 75–8).
Such an effect is readily interpreted in terms of the assumption that the presence of the original set of contextual cues is necessary for the subject to retrieve, during conditioning, information about the stimulus that was acquired during pre-exposure.
In other words, the context-specificity of latent inhibition might constitute evidence in favour of the retrieval theories of the phenomenon discussed in the preceding section of this chapter.
In order to establish the retrieval interpretation of latent inhibition it is necessary to demonstrate, therefore, that contextual cues do indeed have their effect by fostering the retrieval of previously acquired information.
Most of the relevant experimental evidence on this issue comes not from studies of latent inhibition but from investigations of conditioning itself.
Accordingly, I consider next the phenomenon of context-specificity in Pavlovian  conditioning.
Does conditioned responding fail to transfer from one context to another?
If it does fail to transfer, can such context-specificity be established by means of the training procedures typically employed in studies of the context-specificity of latent inhibition?
And need a failure of transfer necessarily imply the failure of a contextually mediated retrieval process?
Context-specificity in conditioning
(a) Explicit and non-explicit training
It is well established that explicit discrimination training can render the effects of a conditioning procedure context-dependent.
By explicit training is meant a procedure in which the animal is given pairings of a CS and a US in one distinctive context, and presentations of the same CS along with some other event in a different context.
The animal can come to emit different CRs in the two contexts.
Thus, for example, Preston, Dickinson, and Mackintosh (1986) have demonstrated that rats given alternate sessions in two contexts can come to respond appropriately to a tone that signals the occurrence of shock when it occurs in one of them and the occurrence of food when it occurs in the other.
They can also discriminate appropriately when the stimulus signals food in one context and extinction in the other, a result amply confirmed in experiments by Bouton and Swartzentruber (1986).
These latter note the parallel with ‘occasion-setting’(cf.
Holland 1983; Rescorla 1985), a term used to describe the finding that subjects can come to respond appropriately to a CS that is sometimes reinforced and sometimes not according to whether some other event (the occasion setter) accompanies the target CS.
Boulton and Swartzentruber urge the acceptance of a common explanation for both phenomena, arguing that contextual cues and explicit occasion setters act not by direct association with other events but by modulating the influence of CS-US associations.
The occasion-setting procedure involves explicit discrimination training; in the procedure known as feature-positive training, for instance, the subject experiences reinforced trials in the presence of the occasion setter and non-reinforced trials in its absence.
It is usually assumed (see e.g. LoLordo and Ross 1987; Ross and LoLordo 1987) that such explicit training is necessary for a stimulus to acquire modulatory, occasion-setting properties (but see also Bonardi (1989, 1991)).
But what we need to demonstrate is that the context can acquire modulatory properties even in the absence of explicit training — we are seeking to explain the context-specificity shown by latent inhibition and this is evident without any form of initial discrimination training.
Latent inhibition will show context-specificity when the only training given has been non-reinforced presentations of the stimulus in a given context.
Will conditioning show context-specificity in comparable circumstances?
Will a    subject given reinforced trials in one context fail to respond when the CS is presented in a different context without having had previous experience of non-reinforced trials in the second context?
Bouton and his collaborators have conducted a series of experiments (for reviews see Bouton (1988, 1990)) using the same stimuli and general procedures as those employed by Bouton and  Swarzentruber (1986) but differing in that no explicit training was given.
They found that conditioned responding transferred readily from one context to another.
The results of a typical experiment (by Bouton and King 1983) are shown in Fig. 4.4.
After acquiring conditioned suppression to a CS in context A, rats received non-reinforced presentation of this CS in either A or B. It is apparent from the figure that conditioned responding was equally likely in both test contexts — that there was no evidence in this phase of training for context-specificity.
Evidence for a form of context-specificity came from a final test stage of the experiment in which all the subjects experience the CS again in the original context (A).
Those that had received extinction trials in A continued to show little responding; but those given extinction in B showed a recovery of suppression.
Bouton and King (1983) interpret their results as showing that context-specificity will be evident only when there is some ambiguity about the CS that the contextual cues help to resolve.
In this case the CS is at one  time associated with a US and at another time is not, and for one group of subjects different contextual cues are correlated with these arrangements.
Another way of interpreting these results is as showing that explicit training is indeed necessary for context-specificity — in this case the two conditions are presented separately (i.e. there is a block of reinforced trials followed by a block of non-reinforced trials) but the arrangement is formally equivalent to the explicit discrimination procedure used by Bouton and Swartzentruber (1986).
Although Bouton and his collaborators have failed to establish context-specificity after simple conditioning, they would not want to claim that such an effect can never be seen — there is ample evidence from experiments using rather different training procedures that a change of context can produce a performance deficit.
In flavour-aversion learning, for instance, Archer and his collaborators (e.g. Archer, Sjödén, Nilsson, and Carter 1979; Archer, Sjödén, and Nilsson 1985) have routinely found that an aversion established in one context is far less marked when the flavour is presented in a different context.
Avoidance learning similarly shows a clear decrement (e.g. Gordon, McCracken, Dess-Beech, and Mowrer 1981) as do certain food-rewarded discrimination tasks (e.g. Chiszar and Spear 1969).
There are even experiments using the condition suppression procedure (as used in Bouton's own experiments) that have succeeded in showing a loss of the CR with a change of context (e.g. Balaz, Capra, Hartl, and Miller 1982; Lovibond, Preston, and Mackintosh 1984, experiment 1B).
The question that needs to be asked about these results is do they demonstrate the function of the context as a retrieval cue or can they be explained away in other terms?
(b) Alternative explanations
The observation that a CR may be less likely when the CS is presented in a context other than that used for training is open to a number of explanations, some of them of little theoretical interest.
Perhaps the least interesting is the possibility that the change of context modifies the way in which the CS is perceived, either because of some change in the physical nature of the signal (the properties of an auditory cue, for instance, are likely to change according to the shape of the space in which it is presented) or because there is some change in the way the cue impinges on the animal (if different contexts promote different patterns of orienting behaviour, the same cue, defined physically, may be experienced differently in the contexts).
It may even be possible for background, contextual stimuli to interact with the pattern of nervous activity evoked by the target stimulus in the early stages of sensory or perceptual processing (Hull's (1943) notion of ‘afferent neural interaction’).
Whatever their source, such generalization decrement effects would mean that a change of context would produce a diminution of the CR.
It seems very likely that the context-dependency reported by Balaz et al .
(1981) and Balaz, Capra, Kasprow, and Miller (1982) is, in part, a consequence  of generalization decrement as is that described by Archer et al .
(1985)(see the discussion of this issue in Chapter 2, p, 42).
A related possibility makes use of the notion of configural conditioning.
The stimulus that the experimenter regards as the CS might interact with features of the context in which training is given to form a unique cue that constitutes the effective CS (see Rescorla 1972, 1973).
Presenting the nominal CS in come other context would generate some different configural cue and so, in the absence of the trained CS, no CR could be expected.
Configural conditioning has been proposed as the likely explanation for the context-specificity observed by Preston et al .
(1986, experiment 2).
Finally, it proves possible to generate an associative explanation for context-specificity.
We may assume that during conditioning associations will be formed between contextual cues and the individual events (the CS and the US) that occur in their presence.
Presenting the CS in a test context that lacks those associative links could have several consequences.
One possibility that follows from Wagner's (1981) theory is that the magnitude of the CR might actually be enhanced.
In the context used for training, contextual cues will be able to establish an A2)(secondary activation) representation of the CS thereby limiting the ability of the stimulus itself to evoke Al (primary activation).
In a different context, however, the CS will be able to evoke Al and since the Al state is better than A2 at activating an associative link, the CS will be better able to elicit its CR when the context is changed.
There may be other associative effects, however, and these allow the possibility that a change of context might reduce the likelihood of the CR.
In particular, associations will also be formed between the context and the US, allowing the former to come to elicit, at least to some degree, the CR evoked by the nominal CS.
If the CR observed in the original context depends on a summation of its own excitatory strength with that controlled by the context, then a decline in the vigour of the CR is to be expected in a test context lacking excitatory strength.
Bouton (e.g. 1988, 1990) has argued against this summation notion, pointing out that in his own experiments the size of the CR seems to be uninfluenced by the associative value of background cues.
But for other experiments the interpretation seems very plausible.
Lovibond et al .
(1984, experiment 1b) gave rats tone-shock pairings in one context, intermixed with sessions of exposure to a second context; the subjects were thus fully familiar with the second context but had no experience of conditioning procedures in it.
Presenting the tone in this second context produced a distinct loss of the CR of suppression.
But Lovibond et al .
conducted a further study (experiment 1 c) that was almost identical to the first but which failed to yield any evidence for context-specificity.
This second study differed only in that during initial training the subjects received conditioning trials with a different stimulus (a light) in the context that was to be used for the test.
Lovibond et al .
point out that this change in procedure will tend to    equalize the associative statutes of the two contexts and suggest that the context-specificity they observed in their first experiment was solely a result of their failure to equalize the two contexts in this respect.
(c) Eliminating the alternatives
In order to provide evidence for the view that the context can act as a retrieval cue (or exert conditional control over an association), it is necessary, as a first step, to demonstrate context-specificity using the control procedures recommended by Lovibond et al .
(1984).
Hall and Honey (1989 a ) report a study doing just this.
Rats received initial training in which presentations of an auditory cue in one context were associated with food, as were presentations of a visual cue in different context.
The subjects developed the CR of approaching the site of food delivery in the presence of each of these cues.
On the single test day, subjects in the critical group (group D in Fig. 4.5(a)) received sessions in which the cues were presented in ‘wrong’ context.
Group S continued with the usual arrangement.
As Fig. 4.5 shows, the change of context experienced by group D produced a substantial loss of vigour in the CR, evident from the very first trial of the test.
This result encourages the conclusion that appropriate contextual cues must be present for satisfactory retrieval of the effects of the effects of initial conditioning; but it also poses a number of further questions.
Foremost among these is why the experiment summarized in Fig. 4.5(a)  should have revealed context-specificity when formally equivalent experiments (e.g. by Lovibond et al .
1984) have failed to do so.
The most obvious distinguishing feature of the experiment by Hall and Honey is that an appetitive reinforcer was used rather than the electric shock of the conditioned suppression procedure.
There is some evidence to suggest that this difference is critical.
Hall and Honey (1989a) report a further study (experiment 3) in which the procedures were exactly the same as those used in their appetitive conditioning experiment except for the use of shock as the reinforcer and conditioned suppression as the measure of behaviour.
The results are shown in Fig. 4.5. (b).
Groups S and D performed identically during the test; using an aversive reinforcer eliminates context-specificity in this preparation and allows the result of Lovibond et al .
(1984) to be replicated.
The null result of fig. 4.5(b) is of theoretical significance as it helps rule out several possible explanations for the effect seen in fig. 4.5(a).
Recall that among the possible reasons for a loss of conditioned responding with a change of context was the generalization decrement that might be expected when the event designated by the experimenter as the CS is presented in a different context.
Evidently no generalization decrement occurred with the stimuli and procedures used here because there was no loss of conditioned responding in group D. But the stimuli and procedures used in the aversive conditioning experiment of Fig. 4.5(b) were just the same as those used in the appetitive conditioning experiment of Fig. 4.5(a).
Accordingly, generalization decrement cannot be held responsible for the context-specificity of the CR seen in the latter case.
(d) factors determining context-specificity
Having ruled out the alternatives we are left with the conclusion that the context-specificity seen in fig. 4.5(a) depends in some way on the context functioning as a retrieval cue.
It now becomes necessary to determine why such a form of conditional learning should be evident in this procedure and not in others.
The evidence just presented seems to suggest that the use of a food-reinforced procedure makes context-specificity more likely, and certainly other successful demonstrations of the effect have used appetitive conditioning (e.g. Honey, Willis, and Hall (1990), who studied autoshaping in pigeons; Peck and Bouton (1990), experiment 2, who measured the ‘head jerk’ CR of rats trained with a food reinforcer).
But the use of a food-reinforced procedure is not in itself enough — Peck and Bouton's (1990) result was not replicated by Bouton and Peck (1989); and Kaye and Mackintosh (1990) who, apart from giving very prolonged initial training, used procedures similar to those of Hall and Honey (1989a), were unable to find the effect.
Nor can we say that context-specificity will never appear when aversive conditioning is used.
Bonardi, Honey, and Hall (1991) found the effect using flavour-aversion learning; and Hall and Honey (1990) were able to demonstrate context-specificity in a conditioned suppression study    that differed from the experiment reported by Hall and Honey (1989 a ) only in that conditioning consisted of a single reinforced trial with each CS (Fig. 4.6).
In attempting to explain this last finding, Hall and Honey (1990) offered a suggestion based on the observation (made by many but see, e.g. Schachtman, Channell, and Hall (1987) that, with prolonged conditioned suppression training, the CR grows up to a point and then begins to decline in magnitude.
Apparently a shock that has been experienced often will lose effectiveness as a reinforcer.
This effect, Honey and Hall (1990) suggested, might derive from the action of some opponent process that comes into play with the repeated presentation of the shock.
In terms of the opponent-process theory of Solomon and Corbit (1974), repeated presentation allows the development of a ‘slave process’(‘relief’) that opposes the state of fear evoked initially by the shock.
Whether or not we accept the particular characterization offered by opponent-process theory, it seems that conditioned suppression training is likely to involve more than the formation of a CS-shock association.
This association will be formed in the early stages of training but, as training proceeds and the shock loses effectiveness, some form of inhibitory learning can be expected to occur.
The magnitude of the overt CR will depend on the interaction of these two forms of learning and can thus be expected to diminish as the contribution of the latter increases.
The relevance of this analysis for out present purposes is that if contextual cues are helpful in retrieving associative information quite generally, then  both of the associations formed during conditioned suppression training can be expected to suffer a loss when the context is changed.
The effect of the change on the observed level of suppression will depend on the relative contributions of the two associations and any difference between them in their susceptibility to contextual factors.
The suggestion put forward by Kraemer and Roberts (1984)(see above, p. 119) that the retrieval of more ‘important, associations will be less dependent on contextual cues implies that a change of context might produce a greater loss of effectiveness in the inhibitory than in the excitatory association.
What follows from this account is that the effect produced by a change of context in the conditioned suppression paradigm will vary according to the amount of initial conditioning.
In order to see context-dependence in conditioned suppression it will be necessary to devise a procedure in which the contribution of the inhibitory association will be negligible.
The one-trial conditioning procedure achieves this since acquisition is over before the shock has a chance to lose effectiveness.
The change of context can influence only an excitatory association and thus the result of Fig. 4.6, a loss of the CR, can be expected.
With more training, however, the inhibitory association will begin to form.
At some point in training, then, the loss of effectiveness of the inhibitory association occasioned by a change of context will counteract the reduced effectiveness of the excitatory association more or less exactly and the outcome will be little or no net change in the observed CR.
Thus the null result of Fig. 4.5(b) could occur in spite of the fact that aversive conditioning is context-specific.
With yet further training, however, the contribution of the inhibitory association will continue to increase and the effect of its loss when the context is changed is unlikely to match exactly the reduction suffered by the excitatory association.
We can expect that a change of context after prolonged training would produce an actual increase in the magnitude of the observed CR.
It is gratifying to note that Kaye and Mackintosh (1990) have recently reported a series of conditioned suppression experiments in which initial conditioning was prolonged (the subjects received a minimum of 80 shock-reinforced trials in each context during acquisition) showing just such ‘supertransfer’ when the context is changed.
At first sight the failure of conditioned suppression to show context-specificity (as in Fig. 4.5(b)) looks like evidence against the notion that contextually mediated retrieval is a factor of general importance in associative learning and performance.
But this result becomes understandable once it is appreciated that in order to predict the outcome of a change of context it is necessary to know what associations have been formed, how each affects behaviour, and how susceptible each is to the contextual change.
By taking all these into account it is possible to explain the varied pattern of context-dependence shown by conditioned suppression after varying amounts of training.
The failure of this CR sometimes to show a loss with change of context (and the fact that sometimes the CR is enhanced by such a change) turns out to be evidence in favour of the proposal that appropriate contextual cues can foster the retrieval of associative information.
The same general analysis goes some way toward explaining why context-specificity should be more easily demonstrable in appetitive conditioning.
Loss of effectiveness in a shock US is amply demonstrated, for instance by the loss of the CR that occurs with prolonged training.
But such effects are much less evident with food reinforcers, implying that our normal procedures for appetitive conditioning will not usually involve an inhibitory component.
An appetitive CR must usually be expected to suffer a loss, therefore, when the context is changed.
This is not to say that food reinforcers are quite immune from the operation of a mechanism like that supposed in Solomon and Corbit's opponent-process theory.
But it may mean that an apparent loss of context-specificity might be found only after very protracted training indeed.
(e) Conclusions
The experimental results described in this section of the chapter allow a number of conclusions.
first, conditioned responding can show context-dependence and will do so even without explicit training in which the significance of the target CS is different in different contexts.
Second, context-specificity can be observed in conditions that preclude an important role for generalization decrement or other factors that might act by influencing the way in which the CS is perceived.
It is also observed in procedures that control for a possible contribution from direct  associations between the context and the US.
(It may be added that Wagner's (1981) proposal that direct associations between contextual cues and the CS might limit the ability of the latter to evoke its CR can be rejected with some confidence on the basis of the present results — this hypothesis implies that the CR should always be enhanced when the CS is presented in a different context.)
What remains is the suggestion that contextual cues present during associative learning acquire a conditional function allowing the use or retrieval of the information encoded in associative links.
Acknowledging that more than one sort of link may be formed as a result of even simple conditioning procedures helps explain the way in which these effects show in behaviour.
And although no precise mechanism for contextual retrieval has been given, the parallel with occasion setting has been noted and forms a basis on which we may build.
Finally it is appropriate to say something at this point about the fact, discussed in detail in Chapter 2, that habituation does not show context dependence.
In that chapter I treated the habituation experiment as a form of recognition memory task in which subjects compare current sensory input with representations of previous inputs.
When the test stimulus is recognized as being the same as one experienced during training (when there is a match between input and representation), no UR is evoked; when the test  stimulus is not (or not fully) recognized, dishabituation occurs.
The important difference between habituation and conditioning when it comes to context specificity appears to be that the latter involves the recall of associative information whereas habituation requires no more than the recognition of the stimulus as being familiar.
Evidence that recognition of an event can survive a change of context when other forms of learning do not comes from a variety of studies of human memory.
An extreme instance is provided by Godden and Baddeley (1975, 1980), who found that the ability of divers to recall a list of words learned either under water or on the beach was much better when the test took place in the same conditions as had prevailed during original learning.
But the ability to recognize a word as coming from the original list was uninfluenced by the change of context.
An example rather closer to the common experience of most of us comes from studies of face recognition.
Young, Hay, and Ellis (1985) asked 22 people to keep records over an eight-week period of difficulties they experienced in recognizing people.
In many of these (233 cases) it turned out that a person was recognized perfectly well as being familiar but the subjects reported a difficulty in recalling other details, such as the person's name.
Incidents of this type, Young et al .
report, tend to occur when the person is encountered in an unusual context — that is, efficient recall of semantic information appears to require the presence of appropriate contextual cues whereas recognition does not.
Retrieval and context-specificity in latent inhibition
The experimental work described in Chapter 3 established clearly that latent inhibition is context-dependent; it also pointed, if a little less clearly, to two further conclusions.
First, it seems that preliminary exposure to the training context can enhance the magnitude of the latent inhibition effect, perhaps because it retards the development of context-specificity (Hall and Channell 1985 c ); second, giving animals extensive exposure to the training context alone after they have experienced presentations of the target stimulus in that context does nothing to diminish the size of the latent inhibition effect (e.g. Hall and Minor 1984).
Associative theorizing (e.g. Wagner 1976) can deal well enough with the basic fact of context-specificity but is less comfortable with the other phenomena (see also McLaren et al .
1989).
If, however, we accept that the effects of non-reinforced exposure can become context-dependent in the same way as associative learning itself can, then the entire pattern of results can be accommodated.
First, the effects of pre-exposure will not transfer readily from one context to another because the original contextual cues will be necessary if the information acquired during pre-exposure is to be fully available.
Next, there is no reason to suppose that the loss of effectiveness suffered by a cue as a result of non-reinforced pre-exposure will be restricted to the case in which  that cue is subsequently required to function as a CS; no doubt the development of an occasion-setting function will also be retarded by exposure to the cue in question.
Accordingly, pre-exposure to the context can be expected to attenuate the extent to which latent inhibition will develop context-specificity.
And, finally, this interpretation leads to the expectation that exposure to the context after latent inhibition training might be without effect.
Certainly it has been argued for a discrete stimulus trained as an occasion setter that subsequently presenting it alone will not produce a loss of occasion-setting power (e.g. Rescorla 1986)(but see also Ross 1983; Holland and Gory 1986).
In order to ‘extinguish’ a cue that signals that a CS and US are associated it may be necessary for the animal to experience uncorrelated presentations of CS and US in the presence of the cue.
If the same holds true for contextual cues in latent inhibition, then giving an animal experience of the context alone (i.e. without any presentations of the target stimulus) should not eliminate the ability of the context to activate the associative information about the target stimulus that was established during pre-exposure (e.g. the information that the target is followed by no significant event).
In summary, then, the basic facts of context-specificity in latent inhibition encourage acceptance of the view that the role of the context is to function as a retrieval cue for information acquired during exposure to the target.
But before expanding on this interpretation it is necessary to consider some evidence that is apparently directly contradictory.
(a) A dissociation of latent inhibition and associative interference?
Kaye, Preston, Szabo, Druiff, and Mackintosh (1987) report an experiment which makes use of a contextual manipulation in an attempt to demonstrate that latent inhibition is not to be interpreted as a form of associative interference.
In the test phase of their study (the results of which are summarized in Fig. 4.7) all the subjects, thirsty rats, received presentations of a CS followed by access to water.
Subjects given pre-exposure to the CS in the test context (group LI same in the figure) acquired the CR slowly compared with the control group for whom the CS was novel and with a group of subjects (LI diff) that had experienced the CS previously in a different context.
This training procedure evidently generates context-specific latent inhibition, a result that I have interpreted as implying that the effects of exposure to the CS are less likely to interfere with new learning when the context is changed.
The challenge to this interpretation comes from the performance of the two other groups shown in the figure.
These had received pre-training in which the CS had been used to signal electric shock — pre-training designed to interfere with the final stage of appetitive conditioning.
An interference effect was indeed apparent at the start of the final stage (Fig. 4.7) and did not differ in magnitude between subjects that had received their aversive training in the same context as that used for the test and subjects that had received aversive training in a different context.
Associative interference is evidently    not attenuated by a change of context, so Kaye et al .
(1987) argue, and hence interference cannot be the basis of the (context-dependent) latent inhibition effect.
A recent experiment by Peck and Bouton (1990) failed to replicated the null result of Kaye et al .
(1987)— that is, Peck and Bouton were able to demonstrate context-specificity in aversive-to-appetitive transfer.
Whatever the source of the differing outcomes of these two experiments, it remains the case that the specific procedures used by Kaye et al .
did achieve an apparent dissociation of associative interference and latent inhibition.
These results still constitute, therefore, a direct contradiction of the thesis being developed here.
It may already be obvious, however, that even the results summarized in Fig. 4.7 do not require us to conclude that the mechanism responsible for latent inhibition is quite different from that responsible for orthodox associative interference effects.
The critical results presented by Kaye et al .
constitute a demonstration that an aversive CS will interfere with further (appetitive) conditioning, whether the context is changed or stays the same.
But a difference between the two conditions (the two INT groups of the  figure) could only be found if the effects of aversive conditioning were context-specific; and, as we have already seen, aversive conditioning (at least with a moderate level of initial training and a mild shock as the US such as Kaye et al .
employed) will transfer from one context to another.
The results of Kaye et al .
constitute a challenge to interference theory only to the extent that this theory is constrained to predict that the aversive CR acquired in the first stage of training should fail to transfer fully to a new test context.
But, as we have just seen (p. 127), there are good reasons for thinking that the overt CR evoked by a CS for shock may fail to show context-specificity in spite of the fact that contextual cues can help the retrieval of associative information.
(b) At what stage does interference occur?
The results presented so far imply that the information acquired during non-reinforced exposure to a stimulus, like that acquired as a consequence of other conditioning procedures, requires the presence of appropriate contextual cues if it is to be fully retrieved.
To this extent the context-specificity of latent inhibition is to be interpreted as a retrieval phenomenon.
What these results do not show, however, is that latent inhibition depends on interference during retrieval in the way postulated by Miller and his colleagues (see above, p. 117).
Their version of retrieval theory supposes that information acquired during pre-exposure (that the target stimulus is followed by no event, say) can coexist with information acquired during conditioning (that the stimulus is associated with a US) and will compete with it on a retrieval test.
Demonstrations of context-specificity do not in themselves establish the correctness of this view as they can be readily interpreted as showing simply that the effects of pre-exposure interfere less with acquisition when conditioning takes place in different context.
What is required is an experiment in which, after non-reinforced exposure in one context and conditioning in a different context, the subjects are tested with the target stimulus back in the original context.
Conditioned responding should develop perfectly readily given the context-specificity of latent inhibition; but will it be evident on the final test?
If retrieval of the CS-US association depends upon the presence of the cues of the conditioning context whereas the cues of the pre-exposure context allow retrieval of the information that the target stimulus is not followed by any event, then no conditioned responding can be expected when the CS is presented in the latter context.
Experiments aimed at investigating this issue have been reported by many workers (e.g. Anderson, O'Farrell, Formica, and Caponigri 1969; Anderson, Wolf, and Sullivan 1969; Dexter and Merrill 1969; Bouton and Bolles 1979; Lovibond et al .
1984; Wright and Gustavson 1986; Wright, Skala, and Peuser 1986).
Most of these studies have produced results consistent with the suggestion that interference occurs at retrieval, finding that a CS that has undergone latent inhibition training will be relatively ineffective in evoking   its CR when it is presented in the context used for pre-exposure.
But, as Bouton and Swartzentruber (1989) point out, many of these experiments obtain the effect only in rather poorly controlled conditions, making unambiguous interpretation difficult.
The experiment reported by Bouton and Swartzentruber (1989, experiment 3) is more satisfactory.
Table 4.1 shows the design and results.
Rats were given repeated exposure to presentations of a light in context A and were also familiarized with a different context (B) in which no stimuli were presented.
They then received conditioning trials with the light as the CS in a third context (C).
for subjects subsequently presented with the light in A, the suppression controlled by this CS at the end of conditioning in C was much attenuated.
To some extent a loss of conditioned responding is to be expected purely on the grounds that conditioning tends to be context-specific.
But this is not the sole source of the loss because other subjects that received the final test with the light in context B maintained suppression to the light.
A change of context is not enough; what is required is that the subjects receive their test in a context in which they have previously undergone latent inhibition training with the target stimulus.
* Bouton and Swartzentruber (1989) conclude that representations of both the latent inhibition treatment and the conditioning treatment are available in the subject's memory at the end of training and that which controls performance depends on retrieval by the appropriate context.
The context-specificity of latent inhibition is not be explained (or at least, not entirely) in terms of interference effects that go on during the conditioning phase of the procedure.
Conclusions
It is now time to attempt to draw together not only the material presented in this chapter but also that considered in Chapter 3.
It will no doubt already be apparent that no simple account to latent inhibition is likely to emerge in spite of the seeming simplicity of the basic experimental procedure.
None of the theories proposed for the phenomenon is wholly satisfactory but there is evidence that requires us to accept at least some aspects of several of them.
Some sort of hybrid theory is likely to result.
The chief conclusion to emerge from the work discussed in this chapter is that retrieval processes contribute to the hybrid — that the latent inhibition effect occurs, at least in part, because subjects tend to retrieve information acquired during pre-exposure to the target stimulus rather than information acquired during conditioning.
One advantage of accepting this conclusion is that it provides, with its assumption that the context can act as a retrieval cue, a ready account of the various contextual effects discussed in this chapter and in Chapter 3.
The qualification ‘at least in part’, is necessary because, as I have already noted, the evidence available on the role of interference at retrieval can show only that this process contributes to the effects observed, not that it is the sole source of these effects.
It is quite possible that pre-exposure to a stimulus both retards new learning about that stimulus and also establishes a memory trace of its own that interferes with any subsequently formed when recall is tested.
I shall take up this point shortly.
To endorse the suggestion that information acquired during pre-exposure interferes with the use of that acquired during conditioning requires an attempt to specify the nature of the information acquired in the first phase.
The earlier discussion of the various forms of interference theory left us with two main candidates (above, p. 116) and both still remain possible.
The first, and perhaps the simplest, was that the formation of within-stimulus associations might interfere with the acquisition of subsequent associations involving these stimulus elements (and, I must now add, might interfere with the ability of subsequently formed associations to influence behaviour).
The second candidate — that latent inhibition training might establish stimulus-no event association — is perhaps a little more complex in that this hypothesis requires the addition of a supplementary mechanism to explain why pre-exposure should retard inhibitory as well as excitatory conditioning.
The suggestion offered earlier in this chapter was that pre-exposure to a stimulus, as well as allowing certain associations to be formed, might also result in a loss of associability.
If this were so, the strengthening of the various associations generated by the inhibitory conditioning procedure would proceed only slowly for a pre-exposed stimulus and this effect could well outweigh any advantage that the existence of a stimulus-no event association might bestow.
The extra complexity of the second hypothesis may seem a good reason  for preferring the first, but in fact any theory of latent inhibition will need to find some place for the idea that associability can change if it is to accommodate the chief conclusions to emerge from Chapter 3.
Although current theories stressing the role played by stimulus consequence in latent inhibition are inadequate (they are unable to deal with the role of context), there remains a set of empirical findings that such theories alone can accommodate — the various experiments (see pp. 99–104) suggesting that the associability of a stimulus depends on its predictive accuracy.
The explanation for these findings offered by Pearce and Hall (1980) was that the associability of a stimulus is determined by how well the stimulus predicts its consequences, declining to zero as the CS — consequence association rises to asymptote.
(Inconsistent consequences, such as occur with partial reinforcement schedule, act to maintain associability.)
In its original form the Pearce-Hall (1980) model applied this analysis only to the loss of associability that occurs with CS-US pairings, but it is a straightforward matter to deal with orthodox latent inhibition in the same way.
Simple pre-exposure may be assumed to be an effective method for reducing associability because it allows the rapid formation either of a strong stimulus — no event association or of associations among stimulus elements, just as consistent reinforcement allows the rapid formation of a strong CS-US association.
According to this account the formation of associations during pre-exposure and the loss of associability are not to be thought of as rival interpretations of latent inhibition.
Rather they are complementary, with the latter process being a consequence of the former.
To sum up the hybrid theory that has emerged: it is that non-reinforced pre-exposure to a stimulus will allow the formation of potentially interfering associations and bring about a loss of associability.
The associative learning that goes on during such pre-exposure will be dependent upon the context in which training occurs, and to this extent latent inhibition will be attenuated by a change of context.
The loss of associability suffered by the stimulus will be depend on the extent to which the stimulus, or each of its constituent elements, is able to form strong associations (and will thus be restricted when the consequences of the stimulus change from trial to trial).
Both these factors (interference and low associability) may hinder the formation of the association required during conditioning but this new association, when it is eventually formed, will not totally obliterate those formed during pre-exposure and there will be competition between old and new during a retrieval test.
It may seem extravagant to invoke such a range of processes in order to explain the effects of what is perhaps the simplest of training procedures;, but in fact the list is as yet incomplete.
In addition to the processes just described, repeated presentation of a stimulus also brings about those changes responsible for habituation that I discussed in Chapter 2 — changes that were characterized as resulting in the formation of a representation of the  stimulus.
Given that our primary concern is with perceptual learning it might be argued that this last process is what should command attention and that the processes involved in latent inhibition (apart perhaps from the attentional change it involves) are of marginal importance only.
Indeed in subsequent chapters I shall again be concerned almost entirely with the way in which stimulus representations are modified by experience.
But a detailed investigation of latent inhibition will not be wasted.
Latent inhibition goes on in all experiments aimed at revealing the nature of stimulus representations and often acts to mask the effects under investigation.
Only by knowing the mechanisms responsible for latent inhibition will we be able to eliminate its effects allowing other forms of perceptual learning to be revealed.
5.
Acquired distinctiveness: mediation and differentiation
In Chapter 1 I distinguished two basic training procedures, one involving discrimination training, the other mere exposure to stimuli, that have been thought to reveal perceptual learning effects.
In the exposure learning procedure the subjects are simply exposed to a stimulus (or sometimes to more than one stimulus) without there being any explicit consequences arranged by the experimenter.
The effects of this exposure treatment can be assessed in a variety of ways and three were specified in Chapter 1.
The first of these, the habituation of the unconditioned response consequent on repeated exposure, was discussed in Chapter 2; the second, the way in which exposure modifies the ability of an event to serve as a classically conditioned stimulus, was discussed in Chapters 3 and 4.
The third method for assessing the effects of exposure is one in which the subjects are required to learn a discrimination between pre-exposed stimuli (or between the pre-exposed stimulus and some other).
Discussion of the outcome of experiments that have used this method will be postponed until Chapter 7 as a proper interpretation of the results of such experiments requires consideration of the processes revealed by the second of the two basic training procedures outlined in Chapter 1.
The next two chapters, accordingly, are concerned with evaluating the effects of reinforced training — in particular, with the suggestion that giving subjects experience of different stimuli associated with different schedules can modify the way in which these stimuli are perceived.
It is common ground that discrimination training will establish associative links that play an important part in generating the changes in behaviour that are observed.
Following one stimulus rather than another with food, for example, is likely to establish associations that will cause the subject to approach the former stimulus rather than the latter when both are available.
What we need to consider now is the possibility that discrimination training also allows processes to operate that modify the distinctiveness of the stimuli themselves.
Changes in stimulus distinctiveness might, indeed, be in part responsible for the development of different patterns of overt behaviour to the different training stimuli, but the mere fact that discrimination learning occurs cannot prove this to be so.
That is, the acquisition of a simple discrimination can be adequately explained solely in terms of the formation of stimulus-reinforcer associations and without recourse to any concept of perceptual learning (see Chapter 1, pp. 13–16).
Attempts to demonstrate acquired distinctiveness have, therefore, usually  made use of a two-stage procedure in which the effects of an initial stage of discrimination training are assessed in some form of transfer test, often one that requires the learning of a further discrimination.
If the stimuli used in initial training have acquired distinctiveness, then this might be revealed in positive transfer to a second task involving these same stimuli.
Interpretation of the results of these studies has been an important battleground for theoretical disputes over the nature of perceptual learning.
The notion of acquired distinctiveness was originally devised by theorists wedded to an associationist  interpretation of all forms of learning (Miller and Dollard (1941), drawing heavily on the S-R theory of Hull (e.g. 1939)), who were thus seen as offering an associative account of perceptual learning.
Gibson and Gibson (1955) responded vigorously to this suggestion, arguing that there were no clear cases in which it was necessary to conclude that the associations formed by cues during discrimination training were responsible for changes in the distinctiveness of those cues.
Instead of an associative mechanism (what they refer to as ‘enrichment’), Gibson and Gibson (1955)(see also Gibson 1969) advocate an interpretation in terms of their differentiation theory whereby subjects become able to respond to aspects of the cues that were present from the outset but were not initially responded to.
This account provoked an immediate riposte from Postman (1955), who objected that no mechanism had been specified to explain the process by which differentiation occurs and argued that associative processes constituted a possibility.
Consideration of these two theoretical positions — the non-associative notion of differentiation and the associative account (to be referred to as ‘mediation,)— will occupy the latter part of this chapter.
It is already clear, however, that the notion of acquired distinctiveness is of central importance in discussions of perceptual learning.
We need now, therefore, to consider the empirical work that has been done on the topic of acquired distinction (and on acquired non-distinctiveness or equivalence).
* The aim of the review that constitutes the next two major sections of this chapter is largely methodological.
Its purpose is to examine in detail the various experimental procedures that have been said to demonstrate acquired distinctiveness and to determine which generate reliable effects that are not to be explained away in other terms.
It is against the results produced by these procedures that the theories can be assessed.
5.1 Experimental studies with human subjects
The first explicit investigation of acquired distinctiveness was that conducted by Lawrence (1949), using rats as subjects.
(Lawrence was a student of Miller who, with Dollard, was responsible for formalizing and popularizing the notion of acquired distinctiveness: Miller and Dollard (1941); Miller (1948); Dollard and Miller (1950).
Most of the experiments on this topic have, however, been conducted with human subjects and I shall begin by reviewing this work before turning to an analysis of the relatively few further studies that have used animal subject to pursue the phenomenon revealed by Lawrence.
Previous reviews of some of this material have been supplied by Spiker (1956 b ), Arnoult (1957), Vanderplas (1958), Cantor (1965), and Gibson (1969).
Transfer between verbal and motor tasks
(a) Preliminary demonstrations
The first publication reporting an explicit test of the Miller-Dollard hypothesis with human subjects was by Rossman and Goss (1951), although their procedure and findings were anticipated by Gagné and Baker (1950) in a near-identical experiment conducted for slightly different reasons.
In both experiments, subjects were trained initially to respond with different verbal labels to a set of visual stimuli and then received training on a motor task with these same stimuli.
In the Gagné and Baker task, the subjects had to learn to respond with a particular letter of the alphabet when a light of a given colour in a given position was presented; there were four different lights, each to be associated with a different letter.
In the test the subjects were required to learn to press a given switch out of four available in response to a given light.
Subjects given extensive pre-training in applying labels to the stimuli learned the motor task more readily than others given little or no pretraining.
They quickly learned to respond more rapidly and pressed the incorrect switch relatively infrequently (Fig 5.1).
The superiority of subjects given preliminary training in applying labels over control subjects given no pre-training has been repeatedly confirmed by experimenters using procedures closely similar to those just described (Goss 1953; Smith and Goss 1955; Battig 1956; Holton and Goss 1956; Goss and Greenfield 1958).
(Arnoult's (1953, experiment 2), failure to find an effect was probably a consequence of giving too little preliminary training.)
The rationale for this experimental design is identical to that underlying Lawrence's (1949) experiment.
An initial phase of discrimination training will establish a tendency to make certain (different) responses to the critical    stimuli, but the change in the nature of the task for the second phase should mean that the specific response patterns acquired in the first phase will be irrelevant.
It would be premature to conclude, however, that the transfer seen in such an experiment depends on the acquisition of distinctiveness by the critical cues as a consequence of phase-one discrimination training.
Results of the sort shown in fig. 5.1 could arise simply because the initial training produces general facilitatory effects (by allowing the subject to become accustomed to the situation, and so on).
They do not require an appeal to effects specific to the discriminative stimuli.
An essential first step is to demonstrate that pre-training must be given with the same stimuli as are used in the test phase.
Some experimenters, therefore, have made the comparison with a control group given initial training in applying labels but to stimuli different from those used in the motor task.
There have been some exceptions (again, perhaps because insufficient phase-one training was given)(Arnoult 1953; McAllister 1953; McCormack 1958) but for the most part the test performance of control subjects has turned out to be inferior to that of subjects pre-trained with the relevant stimuli (G. Cantor 1955; J. Cantor 1955; Cantor and Hottel 1957; Smith and Means 1961; Hendrickson and Muehl 1962).
Experiments by Murdock (1958) and Reese (1961) have confirmed these findings using a within-subject design.
They showed that subjects given verbal pre-training with one set of stimuli learn the appropriate motor response more rapidly to these stimuli than to a further set introduced for the first time in the test phase.
The experiments just mentioned not only introduce a much-needed control procedure but they also extend the generality of the effect.
Cantor and Hottel (1957) obtained their results with mentally retarded subjects;
G. Cantor (1955) and Reese (1961) used young children.
Stimuli other than the onset of a light have been used along with more meaningful verbal labels — G. Cantor (1955), for example, used line drawings of faces (versions of those shown in fig. 5.2) with the labels being names (Jack and Pete).
This study also introduced a different test task using not a successive but a simultaneous discrimination in which the subjects had to choose (for the reward of a marble) one of the two faces.
The experiment by Hendrickson and Muehl (1962) constitutes an even more radical departure in that, uniquely, the order of the training phases was reversed.
The children in this experiment were pre-trained on a motor task (they had to push different levers in response to two stimuli, the letters ‘b’ and ‘d’) before being tested on a verbal paired-associate task involving these stimuli.
Here too, such pre-training produced better test performance than did training on an equivalent motor task with quite different stimuli.
(b) further control procedures
I have established that the initial phase of training must be given with the stimuli that are to be relevant in the test phase.
It remains to show, however, that this training must be discriminative, with the different stimuli being associated with different events.
Gibson's (1969) account of these effects  suggests that discrimination training may not be necessary but the same conclusion can be derived from more prosaic considerations — it might be argued, for instance, that the results described so far reflect no more than a disruption in control subjects confronted by novel stimuli at the start of the test discrimination.
In an attempt to deal with this issue, Goss (1953) introduced a control procedure in which subjects were simply allowed to observe the stimuli (in one case being instructed to attend to them, in another given no special instructions) during the first phase of training.
It was found that these control subjects performed just as well in the test as did subjects trained initially to make different verbal responses to the stimuli.
Subsequent experiments using these control procedures have produced inconsistent results.
Some (e.g. Smith and Goss 1955; Holton and Goss 1956; Vanderplas, Sanderson, and Vanderplas 1964) confirmed that subjects made to observe and attend differed little from experimental subjects.
But others (G. Cantor 1955; Goss and Greenfield 1958; Katz 1963; Ellis and Muller 1964, experiment 2), found that such controls showed inferior performance.
Pfafflin (1960) was able to generate both results by changing details of the stimuli used and the nature of the labels supplied to the experimental subjects.
A close examination of these various experiments might reveal the reason for their discrepant findings.
But rather than undertaking such an analysis it will be enough to demonstrate that this control procedure is in principle inadequate for our present needs.
first, there is the possibility, acknowledged by several of the experimenters just cited, that subjects told merely to observe the stimuli might actually come to attach their own labels to them.
Such subjects, as a consequence, might not differ in their test performance from those who were explicitly trained to apply labels.
This outcome could not, of course, justify the suggestion that discrimination training is unnecessary for positive transfer to occur, as the control subjects would have supplied for themselves what may be the critical feature of such training.
The other possible result of an experiment of this design is equally ambiguous.
However strict the instructions about attending to the stimuli, subjects in the control condition might fail to do so.
In the limiting case, therefore, this control condition reduces to that in which no pre-training is given.
We have already seen that subjects given no pre-training do less well in the test phase than those given initial discrimination training and we have acknowledged that unambiguous interpretation of this difference is impossible.
What we need to arrange is that control subjects perform some sort of task in the first phase — not one, of course, that requires them to attach different labels to the critical stimuli, but one that guarantees that attention to these stimuli is maintained.
Ideally the control task would be one that required some sort of discrimination learning, thus ensuring that both groups came to the test phase having had the experience of learning one previous discrimination problem.
Unfortunately, although not surprisingly, it has proved difficult to devise a suitable control discrimination.
Some use has been made of a control suggested by Kurtz (1955), in which the subjects received pre-training on a same — different task.
The stimuli to be used in the test phase were presented two at a time and the subjects were asked to describe the pair as ‘same’ or ‘different’as appropriate.
Such training, Kurtz argued, means that the subjects learn a discrimination and must attend to the stimuli although they are not required to label them differently.
Kurtz's own experiment demonstrated positive transfer (to a verbal paired-associate task) in subjects given same — different pre-training, but unfortunately the comparison was made only with a group given no pre-training at all.
But Norcross and Spiker (1957)(see also Spiker 1956 b ) found, using stimuli like those shown in Fig. 5.2 and a simultaneous discrimination test, that children given training in applying different names to the two faces were superior in their test performance to controls pre-trained on a same — different task.
This finding has been confirmed by Spiker and Norcross (1962).
It constitutes the best evidence we have come across so far that training in which the critical stimuli become linked to different events generates a unique source of transfer to further discrimination learning.
But even here a sceptic might find reason to challenge this conclusion.
The problem is that the same — different task and the labelling task are unlikely to be matched in the demands they make on the subjects.
A difference in the difficulty of the pre-training tasks might be enough in itself to generate a difference between the groups in the ease with which the test problem is learned, and there would then be no need to suppose that the learning of specific distinctive labels is the critical factor.
A similar objection can be raised to the procedure used by Katz (1963).
Here the control group learned an initial discrimination in which one verbal response was required for each of two stimuli (A and B), whereas a different response was required for C and D. Experimental subjects were required to learn a different label for each of the four stimuli.
The superiority of this latter group on a test task which involved choice between A and B could well depend on their having learned different labels (and on the control group having learned the same label) for these stimuli.
But it could equally well be a consequence of a difference in difficulty between the two pre-training tasks.
(c) Within-subject procedures
Perhaps the most satisfactory solution to the problem of devising an adequate control procedure is to make use of a within-subject experimental design of the sort employed by Norcross (1958) and Reese (1972).
The design and results of the critical aspects of Reese's experiment are summarized as Fig. 5.3.
All the subjects (young children) were trained with three ‘nonsense’ figures as the stimuli, learning to apply one verbal label to two of them and a different label to the third.
There was no question, therefore, of different groups receiving different amounts of exposure    to the stimuli or receiving training on discrimination differing on difficulty.
The children were divided into two groups for the test phase in which they received successive discrimination training, learning to press one button in response to one of the pre-trained cues and a second button in response to another.
For subjects in an ‘acquired distinctiveness’ group, the two stimuli had been given different labels in pre-training; for those in the ‘acquired equivalence’group the two stimuli used were those that had been given the same label.
As the figure shows, subjects in the distinctiveness condition learned the test discrimination more readily than those in the equivalence condition.
The experiment by Norcross (1958), which used a slightly different version of this procedure, produced essentially the same outcome.
Four stimuli were used: two of these were given very different labels, thus approximating to the A and B stimuli of Fig. 5.3.
Norcross found that the acquisition of a button-pressing task proceeded more rapidly with the first pair of stimuli than with the second.
The obvious problem with the designs of these experiments is that it is impossible to know whether the difference observed in the test depends on positive transfer in the distinctiveness group, negative transfer in the equivalence group, or both of these.
This uncertainty seems a small price to pay for the comfort of knowing that at least one of these effects must be a reality.
Transfer to a generalization test
Traditionally, acquired distinctiveness effects have been assessed by requiring the subjects to learn a test discrimination between the stimuli experienced in the first phase of training.
Typically the various stimuli are presented concurrently, each associated with a different outcome or requiring a different response.
An alternative and, in some ways, simpler procedure is to establish some new response to one of the pre-trained stimuli and then, in a separate test phase, to present the other pre-trained stimuli.
( Simplified representations of versions of this experimental design are displayed in Table 5.1.).
If there is little generalization of the new response to the test stimuli, we may conclude that the stimulus used in acquisition and that used in the test are readily discriminated.
Malloy and Ellis (1970) conducted a study which included the conditions shown in Table 5.1 as involving between-subjects comparisons.
The critical (A and B) stimuli were six-pointed geometrical shapes; X and Y were nonsense syllables learned as responses to these shapes.
In the acquisition phase, subjects learned to say a particular noun (R) to shape A. The test showed that training in the acquired equivalence (AE) condition was more likely to produce generalization of this response to a B stimulus than was acquired distinctiveness (AD) training.
Wickens and Briggs (1951) report the same result from an experiment using a quite different procedure.
Stimuli A and B were a tone and a light; X was the verbal response of saying ‘now’ when one or the other was turned on, and Y that of saying nothing.
Training on a task in which the subjects had to learn to make a finger movement in order to avoid shock generalized readily to B for the AE group but poorly for the AD group.
These experiments confirm the results obtained from studies using a discrimination task in the test phase, and, like those previous studies, they are unable to discriminate between acquired equivalence and distinctiveness effects.
Malloy and Ellis (1970) included in their experiment control procedures of the sort I have already discussed (groups given no pre-training, allowed simply to observe the stimuli, and so on) but the proper interpretation of the results from such control procedures is no more secure in this experiment than in any other.
It is also true of this experimental procedure, as of the use of a discrimination task, that it can be strengthened by a modification to allow a within-subject comparison.
If the AE and AD groups differ in the ease with which they acquire the target response to A (as they well might), then differences in test performance might reflect only that there was more conditioned responding to generalize in one case than in the other.
The within-subjects designs shown in Table 5.1 avoid this problem.
The first of these designs was used by Jeffrey (1953).
Children learned to make the same response (X) to the visual stimuli A and B, and a different response (Y) to stimulus C. (for some subjects X and Y were different verbal labels, for others they were different hand movements.)
All subjects then learned to push a handle (R1) in response to A and to pull it (R2) in response to B. The test phase showed that stimulus C tended to evoke R2, that is, to evoke the response acquired to the training stimulus that had received equivalent pre-training.
What is perhaps a simpler within-subjects design, that shown last in Table 5.1, has been employed by Eisman (1955) and in a series of experiments by Grice and his colleagues (Grice and Davis 1958, 1960; Grice and Hunter 1963)(see also Grice 1965).
Eisman's subjects were children, the stimuli (A, B, and C) were coloured blocks for which verbal labels (X and Y) had to be learned.
After being rewarded for choosing A in the middle stage of training, the children were asked which of B and C they thought might bring reward.
They chose B, the block that had shared a common label with A. Grice's experiments, using adult subjects and a classical conditioning procedure, generated equivalent results.
In one set of studies stimulus A was the onset of a light and the target response acquired to it in the intermediate phase of    training was a conditioned eye-blink reinforced by an air-puff unconditioned stimulus.
The final phase tested generalization to two auditory stimuli, tones differing in frequency.
In the first stage of training the subjects had received instructions to emit a given response (a key-press in one study, a word in another) to one of the tones and to the light, but to emit a different response to the other tone.
The result of Fig. 5.4 show, especially for the manual-response case, that generalization from the light CS (conditioned stimulus) occurs more readily to the same-response tone than to the tone pre-trained with a different response.
The advantage of the design of these experiments is that the critical comparison is made within-subjects.
Generalization to both test stimuli (to B and to C) has its origin in a common source (the responding governed by A), thus eliminating the problem identified with the between-subjects design of Table 5.1.
As with similar studies using a discrimination test, however, the results do not distinguish acquired distinctiveness from acquired equivalence.
The response conditioned to A might generalize more to B as a result of the first stage of training; or this training might reduce the extent to which generalization occurs to C; or both of these effects could be occurring.
Transfer from verbal to perceptual tasks
The study by Arnoult (1953), mentioned in the preceding section of this chapter, included a condition in which the subjects were tested not on a discriminative motor task but on a task which required them to make a judgement  about whether two stimuli presented together were the same as or different from one another.
Initial training in applying labels to these stimuli did not influence test performance, and this prompted Robinson (1955) to suggest that pre-training might be ineffective when the test task is ‘perceptual, rather than‘motor’.
Recognition memory tests
The grounds for the same — different tasks being categorized as ‘perceptual’ appear to be that, unlike the ‘motor’tasks discussed so far, it seems not to require further associative learning.
However this may be, Robinson (1955), in common with most subsequent investigators of transfer from verbal pretraining to a perceptual task, made use of a quite different task procedure — a test of recognition memory in which the subject had to respond in one way to stimuli that had been presented in pre-training and in a different way to novel stimuli.
* What is of importance here is not so much whether or not the label ‘perceptual’ is appropriate as whether this task is susceptible to the effects of pre-training in the same way as the motor tasks discussed earlier.
Robinson's (1955) results seemed to suggest that it is not.
In Robinson's (1955) experiment the stimuli used were 10 fingerprints.
Subject in the distinctiveness condition learned separate names for each of them (‘Duke’, ‘Slim’, and so on); subjects in the equivalence condition learned to call five of them ‘cops’ and five of them ‘robbers’.
The test required subjects to identify pre-trained stimuli when they were presented among new but similar stimuli.
Both groups were superior to a control given no pre-training but they did not differ from each other.
Using stimuli of the sort depicted in Fig. 5.5, Vanderplas and Garvin (1959) similarly found no effect on a recognition test of having learned distinctive labels.
These results led Ellis and Muller (1964)(see also Ratz 1963; Vanderplas et al .
1964) to make a direct comparison between a recognition memory test task and one involving the learning of a motor response.
For six-pointed shapes of the type shown in Fig 5.5, Ellis and Muller found that subjects who had learned labels for the various shapes out-performed controls (who had simply observed the stimuli in the first phase of training) on a motor task that required pressing one switch rather than some other in response to these stimuli.
There was no such difference on a recognition memory task where, if anything, the control group did a little better than the group given distinctiveness training.
We should be cautious about using these results to argue that verbal pretraining does not influence performance on a recognition memory test.
first, the failure of Robinson's (1955) study to reveal an effect was not necessarily a consequence of the test procedure as DeRivera (1959), who used stimuli and a pre-training procedure closely similar to those used by Robinson, was similarly unable to find an effect on test in spite of using a test procedure that required further associative learning.
Second, although Ellis and Muller found no difference between their distinctiveness and observation training conditions, there was evidence for a superiority of the distinctiveness group over one given equivalence training in which they had to apply one label to half of the shapes and another label to the rest.
The difference in recognition memory-test performance between subjects given these two forms of verbal pre-training has been amply confirmed in several other experiments (e.g. Ellis, Bessemer, Devine and Tafton 1962; Ellis and Feuge, Long and Pegram 1964).
Some experimenters indeed (including Ellis and Muller (1964) themselves when they made use of the more complex 24-pointed shapes as their stimuli) have been able to demonstrate a superiority of distinctiveness training  over the observe-only control condition (see also Kurtz and Hovland 1953; Ranken 1963; Ellis and Feuge 1966; Ellis and Homan 1968).
Why some of these experiments should be more successful than others at revealing group differences has been the subject of some experimental investigation.
There is some evidence to suggest that among the important determinants should be numbered the complexity of the stimuli (Ellis and Muller 1964), whether or not the stimuli look like recognizable objects (Ellis, Muller, and Tosti 1966), and the extent to which the labels match critical features of the stimuli (Segal 1964).
I need not pursue these issues further here, however, as the results already cited are enough to establish that, given appropriate circumstances, verbal pre-training can influence performance on a recognition task just as it can on a motor task involving discrimination among the pre-trained stimuli.
The experiments reviewed in this section do little to advance out understanding of the mechanisms involved (often important control conditions are lacking) but they do allow the conclusion that there is no fundamental difference between the so-called perceptual and motor test tasks in their susceptibility to acquired equivalence/distinctiveness effects.
It will not be necessary, therefore, to devise separate theoretical interpretations for the results of the two test procedures.
Studies with non-human subjects
I have already considered in some detail the starting point for these studies — Lawrence's (1949) experiment.
I begin here, therefore, with a discussion of the relatively few reports of attempts to replicate and extend his findings.
Transfer in discrimination learning
Studies which like Lawrence's (1949) experiment allow investigation of the transfer between simultaneous and successive discriminations involving the same stimuli have been reported by Mackintosh and Holgate (1967, experiment 1), Siegel (1967), and Winefield and Jeeves (1971, experiment 1).
The last of these produced negative rather than positive transfer but, in that study, the shift was from a very difficult successive task to a simultaneous discrimination, and comparison was made with a control group that received no prior training.
It seems possible that the extensive exposure to the stimuli experienced by the experimental subjects in the first stage of training might have resulted in a latent inhibition effect that would obscure any other source of transfer.
Positive transfer was, however, observed by Mackintosh and Holgate and by Siegel, the comparison being made in each case with control subjects that received initial training on a discrimination between stimuli other than those that were relevant in the transfer task but with the latter stimuli being present but irrelevant.
Other experimenters have varied the procedure, following Lawrence in  training subjects on two discrimination tasks involving the same stimuli but changing the response requirement not by changing from the simultaneous to the successive arrangement but in some other way.
Thus Mackintosh (1964) trained rats on an, absolute' brightness discrimination in which choice was between black and grey on some trials and between white and grey on other trials.
For some subjects, grey was the positive, for others grey was the negative, but neither type of training should establish a difference in associative strength between black and white.
The positive transfer that occurred to a subsequent, simultaneous black-white tasks must, therefore, derive from some other source such as an increase in the distinctiveness of the cues.
In a pair of experiments by Jaynes (1950), rats were required in the first-stage task to lift one of a pair of flaps bearing distinctive cues in order to obtain food.
Different groups had to lift the positive flap just a little(half an inch) or rather more (up to seven inches); thus the groups differed not in whether they had received pre-training with the cues but in the magnitude of the response acquired.
When required to learn a further discrimination involving these same cues but a different response (for example, a reversal in which a simple push to the previously non-rewarded cue now yielded reward) the animals pre-trained with the larger magnitude response learned more readily.
Jaynes attributed the transfer to an increase in the distinctiveness of the cues, an increase that was directly dependent on the magnitude of the response first learned to them.
These relatively complex experimental designs are certainly open to more than one interpretation.
Unfortunately, the same has proved to be true of experiments using Lawrence's original design.
In particular, Siegel (1967) found that a close examination of his results led to the conclusion (see also Riley 1968) that it was unwarranted to assume that responses learned in the first stage could not be the source of the transfer seen in the second.
He noted that his rats tended to adopt a strategy for sampling the stimuli in the first stage of (simultaneous) training in which they  consistently looked into one arm of his T-maze, withdrawing and turning to the other arm only if confronted with the negative stimulus.
This strategy was transferred to the successive discrimination, and for at least some of the animals allowed immediate solution of that test — an animal that had learned to turn away from white in the left arm, say, would already be equipped to solve a successive discrimination in which it was rewarded for turning left when both arms were black and right when both were white.
In this case, at least, there was no need to assume that the transfer obtained depended on changes in the distinctiveness of the cues.
We may doubt that Siegel's is a complete theory.
It needs elaboration, for instance, to deal with the case in which the response strategy learned in the first stage does not accord with the behaviour required in the second.
And although Siegel's results demonstrate convincingly that response strategies can indeed transfer from one discrimination to another, they cannot show  that such strategies are the sole source of the transfer effects seen in his experiment.
Nor can they show that response strategies are responsible for the results reported by others.
Indeed, as Sutherland and Mackintosh (1971) have pointed out, Siegels's training apparatus was specifically arranged so as to ensure that the rats would adopt response strategies of the sort they did.
Other experimenters have used different forms of apparatus with layouts that seem very unlikely, as far as one can tell, to induce such strategies.
Nonetheless, Siegel's arguments have undermined the belief that simultaneous to successive transfer can allow an unambiguous demonstration of an acquired distinctiveness effect.
Perhaps better evidence for such an effect comes from experiments using procedures quite different from those employed by Lawrence (1949), procedures in which response strategies of the sort considered by Siegel (1967) could play no role.
Other procedures
(a) Reinforcement history and generalization
I have already discussed (see Table 5.1) how acquired distinctiveness effects can revealed by means of a generalization test.
One of the pre-exposed stimuli receives reinforcement as a separate phase of training prior to a test phase in which the other is presented.
Lack of generalization to the test stimulus indicates that the two stimuli are readily discriminated from one another.
Honey and Hall (1989 c ) investigated the influence of an initial phase of discrimination training on such a generalization test, using rats as the subjects and a version of the between-subjects design shown in Table 5.1.
In order to distinguish the conditioned response (CR) of the test stage from that acquired in the first stage of discrimination training, the nature of the reinforcer was changed, from appetitive to aversive.
Two groups of rats received initial Pavlovian training with two auditory stimuli, A (a burst of white noise) and B (e.g. a tone).
For one group, both of these cues signalled the presentation of food; the second group received discrimination training in which B was followed by food and A was not.
The effect of this preliminary training on generalization between A and B was examined by establishing A as a CS for some new response and then testing the ability of B to evoke this response.
Thus, A was next trained as a signal for shock in both groups.
The generalized ability of B to evoke the newly trained CR (of suppression) is displayed in Fig. 5.6.
Little generalized suppression was shown by the group given initial discrimination training; that is, these subjects showed a superior ability to discriminate B from A.
In a series of experiments using infant rats as the subjects, Spear and his colleagues (e.g. Spear and Molina 1987; Spear, Kraemer, Molina and Smoller 1988) have investigated acquired equivalence using a design formally identical to that of Honey and Hall (1989 c ).
In one of their paradigms, subjects in    the experimental condition were exposed on separate occasions to an odour and to a tactile cue (the texture of a flooring).
In the presence of each they received an infusion of a sucrose solution.
Control subjects experienced the critical cues but not the sucrose.
The odour was then paired with shock for all subjects.
The effects of this training were found to generalize to the texture cue in experimental subjects, who tended to avoid contact with that particular flooring whereas control subject showed something of a liking for it.
A problem with the design used by Spear is that there is no guarantee that the two groups learned equally readily during aversive training with the odour cue.
Accordingly, the difference between the groups on the generalization test might reflect only a difference in the amount of associative strength acquired in the aversive conditioning phase.
The same issue arises with respect to the critical comparison shown in Fig. 5.6, as this depends on the assumption that stimulus A acquired associative strength equally readily in the two groups during the aversive conditioning phase.
Honey and Hall (1989c) conducted a direct test of the conditioned suppression controlled by stimulus A and found no difference between the groups.
But in order to avoid having to rely on a null result, Honey and Hall (1989 c ) conducted a further study which attempted to eliminate problems of this sort by making use of the second of the within-subjects designs presented in Table 5.1.
Two groups of rats received initial training in which presentations of each of three auditory stimuli occurred.
For both groups, B and C (a tone and a clicker) had different consequences, one being consistently followed by food    and one not.
The groups differed in the treatment given to the third stimulus, A (a noise).
For one group the noise was also reinforced (symbolized as A+B+C- in Fig. 5.7) whereas for the second group (A-B+C-) it was not.
Thus the first of these groups learned a discrimination between A and C but not between A and B; the second learned a discrimination between A and B but C and A received the same treatments.
All subjects then received A-shock pairings followed by a test session assessing the generalization of conditioned suppression to both B and C. Suppression on this test was not profound but, as Fig 5.7 shows, each group showed more suppression to the stimulus that had received stage-one training equivalent to that given for A.
As I have noted before, an apparent disadvantage of this design is that there is no obvious way of distinguishing acquired equivalence from acquired distinctiveness.
That is, the results shown in Fig. 5.7 may occur because rats generalize readily between stimuli that have had the same consequence in prior training; but equally it may be that they generalize less readily between stimuli that have had differing consequences; or both processes  may be operating.
The advantage of the design is that it allows us to exclude other explanations.
The critical comparison made in this experiment (between test responding to B and to C) is made within subjects.
Generalization to both test stimuli thus has its origin in a common source (the associative strength acquired by stimulus A during aversive conditioning), eliminating the possibility, inherent in the between-group comparison made for the results in Fig. 5.6, that differences in the associative strength of stimulus A might be responsible for the outcome.
Comparison of the results of the two groups shown in Fig. 5.7 also allows us to reject an explanation in terms of changes in associability or in terms of the transfer of some specific response that directly modifies test performance.
The Pearce-Hall (1980) theory (see Chapter 3) allows that the associability of A, B, and C will change during pre-exposure and that the magnitude of this change might depend on whether or not the stimulus was associated with food in stage one (Pearce and Hall 1980).
Further, the tendency of a stimulus to evoke an appetitively conditioned response might directly influence the level of responding shown to this stimulus during conditioned suppression training and testing.
But the results of Fig. 5.2 show that the degree of suppression evoked by a test stimulus does not depend upon whether or not it was appetitively reinforced during the first stage.
What determined the suppression shown to a test stimulus was whether or not its stage-one treatment matched that given to stimulus A.
(b) The differential outcomes effect
All the experiments discussed so far have made use of a two-stage procedure — an initial phase of discrimination training intended to render the cues distinctive, followed by a test in which the nature of the task is changed but which still requires discrimination between these same cues.
Experiments investigating the ‘differential outcomes effect’(DOE; Peterson and Trapold 1980) can be seen as collapsing these two stages.
First, consider this design for a two-stage experiment with rats.
In initial discrimination training a tone signals the occurrence of another event (the onset of a light for 5 s), whereas no event follows presentations of a clicker.
Control subjects receive training in which the light is equally likely after the tone and the clicker.
In the test phase all subjects are required to learn a new task in which presentation of the tone signals that the left-hand of two levers will yield food and presentation of the clicker signals that a response to the right-hand lever will do so.
A difference between the groups on this test (with the subjects given initial discrimination training being superior) would constitute a classic demonstration of acquired distinctiveness/equivalence.
Friedman and Carlson (1973) report just such a difference in a one-stage experiment employing the equivalent of this design.
Rats were tested on a food-rewarded (conditional go left/go right) discrimination between tone and clicker.
Those in a correlated condition experienced the light only after a    correct response in the presence of one of the cues (and thus, as the task was learned, increasingly often along with this cue); those in an uncorrelated condition received the light after 50 per cent of rewarded responses whether these were in made in the presence of the tone or of the clicker.
As fig. 5.8 shows, accurate choice performance developed more readily in the correlated group.
The difference between the groups shown in Fig. 5.8 has been called a differential outcomes effect because the superior performance of the correlated group depends on the fact that the outcome of a correct response is reliably different for the two trial types (tone trials and clicker trials).
Although in this case the two outcomes are differentiated by the presence or the absence of a neutral event (see also Fedorchak and Bolles 1986), there are several other ways in which such a differentiation can be arranged.
Results almost identical to those shown in Fig. 5.8 have been produced by experiments in which the two reinforcers differ in their nature (Trapold (1970) used sucrose pellets and standard food pellets), in their timing (Carlson and Wielkiewicz. (1972) used immediate versus delayed reinforcement), or in their magnitude (one food pellet versus five, Carlson and Wielkiewicz (1976)).
DOEs have also been obtained in experiments investigating matching to sample (MTS) in pigeons.
In MTS, the bird is first presented with one stimulus, say a red light, as the sample; two ‘comparison’ stimuli, say red and green, are presented (often after a delay) and response to the matching stimulus is rewarded.
This procedure is formally equivalent to the conditional  choice procedure use with rats.
It differs in that the alternatives offered for choice include one that is physically identical to the conditional cue (the sample).
But this is not an essential feature of the procedure.
In ‘symbolic’ MTS, the bird might have to choose between horizontal and vertical as the comparison stimuli, choice of one being rewarded after a red sample, and choice of the other after green.
Brodigan and Peterson (1976)(see also Peterson, Wheeler and Armstrong 1978) report just such an experiment in which two types of reward (food and water) were available.
Subjects for whom trial-type (i.e. whether the sample was red or green) and reward-type were correlated learned more readily than subjects for whom there was no consistent relation between type of trial.
A similar DOE has been found in experiments by Edwards, Jagielo, Zentall and Hogan (1982), who used different types of grain to differentiate the outcomes, and by Peterson and Trapold (1984) who presented (in the correlated condition) food after a correct response for one trial-type and just a feedback tone for a correct response for the other type of trial.
The DOE has been interpreted as showing that expectancies can act as effective cues in the control of choice responding (e.g. Peterson and Trapold 1980).
The correlated procedure allows each sample (or conditional) cue to become associated with a different outcome.
The different expectancies generated by these cues can then, it is supposed, themselves serve as cues.
One reason why the ability of the conditional cues to evoke such expectancies might facilitate learning comes from theories of animal working memory.
Some accounts (e.g. Honig 1981) assume that a reinforcer expectancy is more likely to persist in memory than is a representation of the sample stimulus itself.
Support for this interpretation can be derived from studies of delayed MTS in pigeons, which have found that the magnitude of the DOE diminishes as the length of the delay is reduced, there being no effect when there is no delay (Brodigan and Peterson 1976; Edwards et al .
1982)(see also DeLong and Wasserman 1981).
It may be doubted that memory mechanisms form the sole basis of these effects, however, given that formally equivalent conditional discriminations using rats as the subjects have regularly found a DOE using a procedure which makes no demand on memory because the conditional cue remains present until the choice response is made.
Studies of working memory in animals may seem to be rather remote from   the standard, acquired distinctiveness procedure.
It is appropriate to restate, therefore, the formal parallel between DOE studies and acquired distinctiveness experiments (a parallel hitherto ignored by all but Edwards et al .
(1981)) and to note the possibility (to be taken up shortly) that the mechanisms responsible for the DOE may in fact be responsible for the effects that have been observed in orthodox studies of acquired distinctiveness.
Conclusions
Although Lawrence's original (1949) experiment has not been replicated exactly, the work of his successors (conducted both with animal subjects and with humans) has confirmed the validity of his central contention — discrimination training, in which each of a pair of cues becomes associated with a different outcome, enhances performance on a further discriminative task involving the same cues.
This is not to say, necessarily, that the trained cues acquire distinctiveness or that discrimination training is the source of the effect.
My conclusions come from experiments in which comparison is made between subjects given discrimination training and subjects given a control procedure of some sort (or sometimes, in the studies done with human subjects, no pre-training at all).
Many of these comparisons produce results that can be explained without appealing to the idea that the pre-trained cues become more distinctive.
There are, however, several studies (some conducted with human subjects, others with animals) that allow a comparison between the effects of distinctiveness training and those of equivalence training.
The difference found in these is difficult to explain away and I shall accept the conclusion that it derives from a change in the distinctiveness of the pre-trained cues.
What these experiments cannot reveal is whether both the equivalence and distinctiveness treatments are effective in producing changes in the discriminability of the cues — clearly the observed difference could be obtained if just one of these treatments worked.
This is a pity for, as we shall soon see, the two main theoretical interpretations of the effect differ on this issue.
The mediation theory is founded on the notion of acquired equivalence and only with difficulty can predict an acquired distinctiveness effect.
Differentiation theory, on the other hand, was constructed as an account of distinctiveness and finds no place for equivalence.
But both theories can accommodate the basic effect and we shall need to give both full consideration in the next part of the chapter.
Theoretical interpretations
Mediation theory
(a) Background
James (1890) was perhaps the first to offer what amounts to an associative account of the acquired distinctiveness of cues.
This account can be illustrated  by considering his analysis of how discrimination training might increase the ease with which one can distinguish burgundy from claret.
When we first drank claret we heard it called by that name, we were eating such and such a dinner etc.
Next time we drink it, a dim reminder of all these things chimes through us as we get a taste of the wine.
When we try burgundy our first impression is that is it a kind of claret; but something falls short of full identification and presently we hear it called burgundy.
During the next few experiences the discrimination may still be uncertain —‘which’, we ask ourselves, ‘of the two wines is this present specimen?’
But, at last the claret-flavor recalls pretty distinctly its own name, ‘claret’, ‘that wine I drank at so-and-so's table’, etc.; and the burgundy-flavor recalls the name ‘burgundy’ and someone else's table...
After a while... the adhesion of each wine with its own name becomes more and more inveterate, and at last each flavour suggests instantly and certainly its own name and nothing else.
The names differ far more than the flavors, and help to stretch these latter further apart.
James 1890, p.511.
James does not discuss the matter, but the process he postulates could also, in some circumstances, make stimuli more difficult to discriminate.
If two initially rather different cues are made to adhere to the same name or to closely similar names, then rather than being stretched apart the two cues might be forced closer together.
This was the notion taken up by Miller and Dollard (1941) as the acquired equivalence of cues.
Miller and Dollard make only brief mention of the idea (on which James concentrates) of acquired distinctiveness; but Miller (1948) explicitly acknowledges both possibilities, expressing them in terms of S-R theory.
According to stimulus-response theory, learning to respond with highly distinctive names to similar stimulus situations should tend to lessen the generalization of other responses from one of these situations to another since the stimuli produced by responding with the distinctive name will tend to increase the differences in the stimulus patterns of the two situations.
Increased differentiation based on this mechanism has been called the acquired distinctiveness of cues.
On the other hand, if the individual learns to respond to two quite different situations with the same verbal response, the stimuli produced by this response will be a common element mediating an increased amount of generalization from one stimulus to the other.
This has been called acquired equivalence of cues, or secondary generalization.
Miller 1948, p. 174.
Although both James and Miller concern themselves with the effects of attaching verbal responses to the cues to be discriminated, there is no reason to restrict the analysis they offer to such responses.
It can be applied just as readily to cases in which the associates are non-verbal responses or (as may happen with classical conditioning) not necessarily responses at all but representations of other stimuli.
(b) The acquired equivalence mechanism
Although at first sight the quotations reproduced above may seem to have said all that needs to be said, it will be worthwhile trying to specify exactly    the mechanisms by which associative mediation theory generates the effects that need to be explained.
The mechanism supposed by Miller (1948) to underlie acquired equivalence is that introduced by Hull (1939) with his notion of secondary generalization.
Primary generalization will occur between stimuli that are similar, presumably because they have features or elements in common.
But secondary or learned similarities can mediate generalization between otherwise quite dissimilar stimuli.
Hull's S-R version of this idea is presented in Fig. 5.9.
The case considered is that in which a subject is trained to make the same response () to each of two different stimuli, and .
When this response occurs, it will elicit a set of feedback cues () and these will thus be evoked whenever either  or  subsequently occurs.
Training the subject to make some new response (in the figure) will allow the formation of an  association.
which, like  is capable of evoking , will thus be capable of eliciting  in spite of the fact that the  link has not been trained directly.
This analysis applies directly to generalization tests for acquired equivalence of the sort depicted in Table 5.1.
And a tendency for the responding acquired to  to generalize to  will also explain why discrimination learning which requires the subject to make different responses to the two stimuli should be retarded.
Although the S-R terminology seems well fitted to the case in which  is a verbal label, it is possible to dispense with the reliance on the notion of response-produced cues that is central to S-R theory.
As we have seen, modern theories of Pavlovian conditioning assume that stimuli can activate    the representations of other events.
Such theories can therefore allow that an associatively activated representation may support generalization between stimuli that share the ability to elicit activity in this representation.
Thus both A and B could become directly linked with a representation of stimulus X as shown in Fig 5.10(a).
Subsequent training with A followed by an unconditioned stimulus (US) might allow the formation of an association between the associatively activated representation of X and the US.
Thus B would become capable of activating the US representation (and of evoking a CR) by way of its ability to activate the X representation.
A and B might well have features in common from the outset (the c elements in the figure) and these will produce primary generalization — training on A will give associative strength to stimulus elements that are present also in B. The X representation functions in just the same way as the c elements in producing generalization except for the fact that the ability of A and B to activate X is based on prior conditioning.
It should be acknowledged that not all theories of Pavlovian learning are entirely happy with the suggestion that an associatively activated representation can form an excitatory link with the (directly activated) representation of some other event.
In particular, Wagner's (1981) theory (see Chapter 1) asserts that the state of activation produced by an associative link (A2) is different from that produced by the stimulus itself (the A1 state).
When one  representation (node in Wagner's terminology) is in A2 and another in A1, the latter may form an inhibitory link with the former.
It is not supposed, however, that an excitatory connection will form between the two.
There are experimental findings to suggest that Wagner's theory needs to be modified in this respect.
Direct evidence that an associatively activated representation (such as X) can acquire associative strength when its CS is associated with a US has been produced by Holland (1981).
Rats were given orthodox conditioning with a tone as the CS and a distinctively flavoured food as the US.
In a second phase of training presentation of the tone preceded a nausea-inducing injection of LiCl.
In a final test the rats, although hungry, proved unwilling to consume the food.
Holland's interpretation was that in the second phase of training an association was formed between the illness produced by the injection and a CS-evoked representation of the food that preceded it.
(See also Holland and Forbes (1982 a ,b ), who provide further evidence to suggest that the CS-evoked representation of an event can act as a substitute for the event itself.)
It is appropriate to add a few words at this point that might help avert the possibility of becoming involved in any extensive debate about whether associative changes of the sort described here should really be thought of as instances of perceptual learning, producing changes in the perceived similarity of the stimuli.
Certainly these associative processes can just as readily be interpreted as constituting an associative mechanism for categorization or concept formation in that they allow physically different stimuli to come to be treated in the same way.
Indeed, some (Bhatt and Wasserman 1989) have argued that the ability of an animal to respond in the same way to a range of different stimuli should be ascribed to the operation of a conceptual category only when mediated generalization can be demonstrated among these stimuli.
Following Lea (1984), they suggest that true conceptual categories involve equivalence classes of stimuli that are not tied together by perceptual similarity, Mediated generalization is clearly not to be regarded, according to this view, as making the stimuli perceptually more similar.
Should acquired equivalence, then, be interpreted as concept formation rather than perceptual learning?
Fortunately it turns out to be impossible to give a simple answer to this question and possible to give a more complex answer allowing both possibilities.
Stimuli A and B may be regarded as physically similar to the extent that they share common,c , elements.
The mechanism responsible for mediated generalization does nothing to increase the number or proportion of such elements and thus those wanting to cite mediated generalization as a characteristic of concept formation can assert that, in a very real sense, the similarity of the stimuli has not been changed.
But when it comes to the issue of how a given physical stimulus is actually perceived, we have chosen to represent this in terms of the central representational elements that it activates.
The percept initially evoked by A, therefore, must be equated with the  a and c elements that it activates unconditionally, After training, however, presentation of A activates not only these but also elements that in other circumstances are unconditionally activated by stimulus X. The pattern of central representation (and thus, by one definition, the percept) is changed by experience, making the phenomenon an instance of perceptual learning.
(c) Acquired distinctiveness mechanisms
It might seem that a simple complement of the analysis offered above for acquired equivalence would supply us with a mechanism for acquired distinctiveness.
The two cues to be discriminated, A and B, are associated in pre-training with different events represented by X and Y in Fig. 5.10.
(X and Y would be response-produced cues in the S-R version of the theory.)
Subsequent discrimination training thus occurs between the compound of A and its associate X and of B and its associate Y. If the events used as X and Y differ from each other more than do A and B (see Fig. 5.10(b)) then it might be supposed that the compounds would be discriminated more readily than would an untrained A and B. Certainly most proponents of an associative account for acquired distinctiveness effects have taken their analysis no further, implying that the phenomenon follows directly from what has just been said.
In fact, closer inspection reveals that further mechanisms must be introduced if the associative structure shown in Fig. 5.10(b) is to generate acquired distinctiveness.
Consider why it should be that A and B are difficult to discriminate in the first place.
The interpretation offered by Fig. 5.10 is that they hold many elements (c elements) in common.
Training with A will allow both its unique a elements and the c elements to acquire associative strength.
Stimulus B will then be able to evoke the response because it too possesses c elements.
The only way in which the discrimination between A and B can be enhanced is by some process that reduces the role of the c elements in producing generalization from A to B. It is not apparent that establishing associations between A and X and between B and Y will do this, even though X and Y themselves hold rather few (z) elements in common.
The existence of the A-X association may mean that training with A will allow the associatively activated representation of X to gain associative strength; but as long as the c elements still gain strength, generalization to B will still occur and discrimination will not be improved.
Indeed, to the extent that X is perceived as being similar to Y (i.e. to the extent that these events hold elements in common), the acquired equivalence mechanism will operate, with these common elements (the z elements) mediating generalization from A to B.
Consideration of the phenomenon of overshadowing provides one possible way by which the associative structure shown in Fig 5.10(b) can be made to predict acquired distinctiveness.
If animals are given Pavlovian conditioning with a compound CS consisting of separable components, the amount of strength gained by one of these components after a given number of training  trials will be less than would be acquired if that component had been trained alone.
The presence of the added component ‘overshadows’ learning about the other.
This effect is well established empirically and is dealt with by all modern versions of the standard associative model.
Thus the Rescorla-Wagner (1972) model (see Chapter 1) envisages conditioning as being a process in which stimuli present on a conditioning trial compete with each other for a limited amount of associative strength.
To the extent that one component of a compound gains strength, the other will not.
Other theorists (e.g. Mackintosh and Reese 1979)(see also Wagner 1981) attribute the effect to competition between stimuli at a perceptual level, suggesting that because of some limited capacity mechanism, attention can be paid to one component only at the expense of that paid to the other.
Now, one consequence of establishing the A-X association shown in Fig. 5.10(b) is that subsequent training with A presented alone is supposed to allow the associatively activated X representation to gain strength.
It follows that X should to some extent overshadow A, restricting the amount of associative strength gained both by its a elements and its c elements.
This last point is critical — if the associative strength of the c elements is low there will be little generalization from A to B, that is, discrimination between A and B will be enhanced.
A second possible way in which the existence of A-X and B-Y associations might facilitate the discrimination between A and B follows if it can be allowed that different associations might differ in the ease with which they are remembered.
We have supposed that once the A-X association has been established, further training with A will endow both A itself and the associatively activated X with associative strength.
Now suppose that the strength acquired by A tends, for some reason, to be lost over a retention interval whereas that controlled by X is retained.
On a subsequent test, A will still be able to evoke the conditioned response, but only or chiefly by virtue of its ability to evoke the X representation.
There will be little generalization to B, however; there is no reason for generalization to occur between X and B (they have no elements in common, Fig. 5.10(b)), and the c elements of B itself will, we are assuming, have lost their strength over the retention interval.
Compared with the case in which A and B have not been pre-trained, the consequence of A-X and B-Y associations having been pre-established would be that a new response could be acquired very readily by stimulus A without any increase in the extent to which generalization occurs to stimulus B.
It may seem wholly arbitrary to assume that the association acquired by A should lose its effectiveness when that acquired by the X representation does not.
There are, however, circumstances in which this assumption looks secure, or at least justifiable.
If X were much more salient than A, for instance, then the required outcome can be anticipated.
A version of this suggestion has already been met with as a possible explanation for some  DOEs when it was suggested (p. 159) that an expectancy of a reinforcer evoked by a sample stimulus will be more likely to persist in memory across the delay of a delayed MTS experiment than will a representation of the sample stimulus itself.
for experiments with human subjects when X is a verbal label for stimulus A, the important feature could well be not that X is very much more salient than A but that the mechanisms of human memory are especially adept at maintaining verbal information.
Thus, when in G. Cantor's (1955) experiment the subjects had to learn over a series of trials a simultaneous discrimination between faces of the sort shown in fig. 5.2, the task required them to carry over information from one trial to the next.
Subjects given appropriate pre-training would be able to bridge the interval by rehearsing some version of the information ‘jack is to be chosen’, perhaps an easier task than retaining information about some direct representation of the visual cue itself.
A third possibility that we should consider makes use of the notion of generalization decrement.
It is accepted that there may be interactions between supposedly distinct stimuli when they are presented simultaneously.
These interactions (which may be peripheral or central) will modify the way in which the stimulus is perceived, producing generalization decrement when a stimulus trained in compound with another is presented on its own, or vice versa.
It is a small step to suppose that such effects might also operate when a stimulus is presented in compound not with another that is physically present but with the associatively activated representation of such an event.
An untrained stimulus A may be perceived in one way; one that evokes the representation of X in a different way; and one that evokes a representation of Y in a different way yet again.
Establishing A-X and B-Y associations will mean, therefore, that the perception of A will be changed in one way and that of B in a different way.
These effects will presumably operate both on unique features of the stimulus and also on those features that, for untrained subjects, are perceived as being common to both stimuli.
This last point is critical since it means that the number of common elements will be reduced and accordingly that discrimination between A and B will be enhanced.
There is no reason why these various possible mechanisms for acquired distinctiveness — the memory hypothesis, the overshadowing hypothesis, and generalization decrement — should be seen as rivals.
Activation of the representation of an X cue might overshadow learning about A, might reduce the number of elements perceived as being common to A and B, and might be especially good at controlling behaviour after a retention interval.
It is quite possible that all three mechanisms might operate.
And, I should add, it is also possible that none does so.
All three suggestions follow directly from, and indeed seem to be required by, our current understanding of the mechanisms of associative learning.
But it might well be that an associatively activated X is simply not salient enough to produce effects of a  measurable size.
As the best evidence for the effects under consideration comes from experiments demonstrating a difference between the acquired distinctiveness and acquired equivalence procedures, it is not essential for the theorist to devise a mechanism for both.
The whole of the difference could be a consequence of the operation of the acquired equivalence mechanism, with the acquired distinctiveness procedure constituting no more than a neutral control procedure.
It would also be open to the theorist, of course, to devise a mechanism for acquired distinctiveness and to attribute the whole of the effect to this, denying the reality of acquired equivalence.
This is just the approach taken by the second theory to be considered.
Differentiation theory
(a) Background
In an analysis of verbal learning, Gibson (1940) argued that much of what must be learned in a paired associate task involves establishing discrimination among the items.
The improvement in performance that occurs with training was held to depend not just on the strengthening of the association between the stimulus — word and the response — word but also on a reduction in the extent to which the various words tend to be confused.
This process of stimulus differentiation was equated by Gibson with a steepening of the gradient of generalization surrounding each stimulus.
Gibson (1940) also assumed that stimulus differentiation, once it had occurred, would transfer to and facilitate performance on a new task involving the same stimuli.
And although Gibson's initial concern was with verbal learning, subsequent experimental investigations of ‘stimulus pre-differentiation’, as it came to be called, were conducted in a range of transfer paradigms.
One of the earliest was the study by Gagné and Baker (1950) with which I began this chapter (fig. 5.1), this study being designed to show that training in applying a different verbal label to each of the various stimulus lights would produce differentiation among them that would transfer to a subsequent motor task.
(b) Mechanisms
Gibson's (1940) account of differentiation in paired-associate learning was expressed in part in the S-R terminology then current.
Training with a list of stimulus words was regarded as an instance of discrimination learning in which each S comes to evoke an R different from those evoked by other stimuli.
The sharpening of the generalization gradient around a stimulus was in some way a consequence of the processes responsible for the formation of the S-R links but was not thought to be itself associative in nature.
The non-associative nature of the phenomenon was emphasized by Gibson and Gibson (1955), who made a firm distinction between their own account and the associative interpretation, which they referred to as ‘enrichment’.
Associative theories of acquired distinctiveness (and of perceptual learning more generally) assume, they argued, that ‘percepts change over time by acquiring progressively more memory images’(p. 34), thereby becoming enriched.
(Stimulus A of fig. 5.10 might be said to be enriched, if only a little, by virtue of its ability to evoke the image of X.) The differentiation theory, in contrast, holds that ‘percepts change over time by progressive elaboration of qualities, features and dimensions of variation’(Gibson and Gibson 1955, p. 34), that is , by an elaboration of aspects of the stimulus that are present in it from the outset.
‘Perceptual learning, then, consists of responding to variables of the physical stimulation not previously responded to’(Gibson and Gibson 1955, p. 34).
In particular, with practice and exposure, a subject will become better able to detect the distinctive features that distinguish one environmental event from another, and also become better able to detect the invariant features that a given event displays from one occurrence to the next.
Gibson (1969) expands on these notions at some length but is not in fact very forthcoming about the mechanisms involved.
She emphasizes again their non-associative nature and puts forward three possibilities.
The first is a process of ‘abstraction’, the process responsible for discovering invariants.
Little is said about how this process works apart from the suggestion that a number of presentations of a given object or event may be needed if invariant aspects are to be distinguished from incidental features that may vary from one occasion to another.
The next two processes discussed by Gibson may be considered together — they are central and peripheral mechanisms of attention.
These mechanisms allow a subject to ignore irrelevant aspects of stimuli, irrelevant aspects being those that fail to distinguish one stimulus from others.
Again, no formal theory of attention is developed and indeed it is unclear that some mechanism other than that involved in abstraction is necessary — the distinctive features of an object or event are likely to be the invariants and the irrelevant aspects those that vary from presentation to presentation.
In the original exposition of the theory, Gibson (1940) assumed that explicit training, which allowed the formation of associations between stimuli and different distinctive consequences, might be necessary for differentiation to occur.
This notion was soon abandoned, along with any use of the S-R framework (see, e.g. Gibson and Gibson 1955), to be replaced by the suggestion that perceptual learning can go on quite independently of any associative processes.
This is not to say that the procedure employed in studies of discriminative learning will be quite without effect.
Rather the assertion is that explicit training is not necessary to produce discrimination, that the need to extract information from the environment is enough to do so, independently of any externally imposed rewards or punishments or of knowledge of results more generally.
It follows that this account finds no place for a notion of acquired equivalence.
All the forms of training used in the experiments    described earlier in this chapter will produce stimulus differentiation.
We cannot suppose, however, that discrimination training is quite without effect — to do so would eliminate any possibility of explaining why acquired distinctiveness training should be superior to acquired equivalence training in producing differentiation.
We must assume, therefore, that explicit discrimination training will enhance the size of the effect, presumably because it will compel the subject to attend to distinctive and invariant aspects of each stimulus, and to ignore irrelevant aspects.
Leaving the question of what mechanisms produce differentiation, we may turn now to the separable issue of the mechanisms by which discriminative performance is enhanced once differentiation has been achieved.
The central question in terms of the diagram used in Fig. 5.11(a) (see also Fig. 5.10) is why experience of A and B should reduce the effectiveness of the c elements in producing generalization between these stimuli.
The critical aspect of differentiation theory in this context is its assertion that exposure to A and B will lead to an increase in the number of distinctive features that can be detected.
This must mean, in terms of the figure, an increase in the number of a and b elements activated by the stimuli, something that can be represented diagrammatically in more than one way.
Figure 5.11(b) shows an increase in the areas a and b with a consequent reduction in the proportion (although not the absolute number) of c elements activated by a stimulus.
figure 5.11(c) presents a slightly different possibility in which the sizes of  the A and B circles have not been changed.
There is no net increase in the number of elements activated by a given stimulus but the circles have been moved apart, indicating not only an increase in the number of unique elements activated by a stimulus but a corresponding decrease in the number of common elements activated.
It is not clear if either of these possibilities accurately represents the full implications of Gibson's (1969) differentiation theory, but both are capable of predicting the required result.
* Comparing the scheme shown in fig. 5.11(b) with that of Fig. 5.11(a), we might expect that the presence of the extra active a elements would produce more effective overshadowing of the c elements during training with A. Generalization to B would therefore be reduced.
Generalization would also be reduced given the scheme shown in fig. 5.11(c).
There would again be an increase in the ability of a elements to overshadow c elements, but in addition the reduction of the number of c elements activated by A would mean that relatively few of them would be capable of acquiring associative strength in the first place.
Evaluation of the theories
The interpretations of acquired distinctiveness/equivalence effects offered by mediation theory and differentiation theory differ markedly on two (related) issues.
first, they differ about the role of explicit discrimination training.
The account of acquired distinctiveness supplied by mediation theory (Fig. 5.10) requires that each target stimulus become associated with some different event, and a reliable correlation between each stimulus and its outcome is needed for this to be established.
Differentiation theory, by contrast, asserts that merely observing target stimuli will be enough to produce some effect.
Second, differentiation theory as developed here envisages no mechanism for generating acquired equivalence, whereas secondary generalization between two stimuli on the basis of their both having associations with some other, common event is at the heart of the mediation theory.
However clear-cut the differences between the theories may be, direct tests of these issues lead to no very clear conclusions.
As we have already seen, the results of experiments comparing the effects of mere observation of the stimuli with those produced by explicit discrimination training have turned out to be inconsistent and open to a range of interpretations.
And attempts to demonstrate empirically the operation of an acquired equivalence   mechanism distinct from that responsible for acquired distinctiveness have proved unsuccessful.
It is difficult to devise the neutral control condition that such a demonstration would require.
By far the most satisfactory demonstrations of changes in the discriminability of cues with training come from experiments in which the comparison is made between acquired distinctiveness and acquired equivalence conditions; a difference between these conditions could, of course, be entirely the consequence of an acquired distinctiveness effect.
Given the failure of these direct tests we need to turn to some slightly less obvious empirical implications of the differences the theories noted above.
(a) Varying the difference between the associates
In acquired distinctiveness training, subjects experience target stimulus A, along with one outcome, X, and target stimulus B along with a different outcome, Y (Fig. 5.10).
Mediation theory requires that X and Y be rather different from each other if the formation of A-X and B-Y associations is to lead to an increase in the discriminability of A and B. 1f X and Y are made more similar, the effect will be reduced (the limiting case, when X and Y are identical, being that in which acquired equivalence is held to occur).
Differentiation theory does not predict that the magnitude of the acquired distinctiveness effect will depend on the similarity of X and Y. This theory might suppose that some form of discrimination training will be superior to none if only because such training will ensure that the subjects attend to relevant aspect of the situation.
But there is no reason to suppose that the effectiveness of discrimination training in maintaining attention will differ according to the similarity of the events used as X and Y.
Such results as are available tend to support the associative view.
In the experiment by Norcross (1958), briefly mentioned above, young children were shown four pictures of different faces, two boys and two girls, and were taught a different name for each.
For one pair of faces (say, the boys) the names were rather similar (‘zim’ and ‘zam’) whereas for the other pair they were dissimilar (‘wug’and ‘kos’).
Acquisition of a button-pressing task (a separate button was assigned to each of the four faces) proceeded significantly more rapidly for members of the pair given dissimilar names than for members of the pair given similar names.
The experiment by Jaynes (1950)(see above, p. 153) can be taken as demonstrating an equivalent effect in rats.
Recall that transfer was good for subjects trained in a simultaneous discrimination to raise a flap bearing the positive stimulus through seven inches but was less good for subjects trained to raise the flap only half an inch.
Given that the subjects were trained not to raise the negative flap at all, we may say that the difference between associates established to the two stimuli was greater in the case in which the more extensive motor response was required — that is , in the condition that produced  superior transfer to a test  discrimination involving the same stimuli.
The interpretation put forward here is not fundamentally different from that offered by Jaynes when he suggested that a tendency to emit (a fractional version of) the response acquired in the first stage would serve as a mediating process in the second, and that the salience of the mediator would depend on the magnitude of the initial response of which it was a fraction.
(b) Disrupting the associations formed during distinctiveness training
According to mediation theory, discrimination training produces distinctiveness because it establishes A-X and B-Y associations.
Differentiation theory, on the other hand, while allowing that these associations are likely to be formed during training, gives them no special role in acquired distinctiveness.
An implication of the latter view is that the enhanced discriminability of A and B should survive the introduction of procedures that act to eliminate or disrupt the associations formed during training.
A phase of extinction training, for instance, in which A and B are presented in the absence of X and Y, or of reversal training in A-Y and B-X pairings are introduced, should not influence the differentiation established by an initial phase of  discrimination training.
The associative theory makes a different prediction — to the extent that these procedures (extinction and reversal) mean that A and B will no longer be firmly associated with distinctively different events, this theory must expect that the consequence will be an attenuation of the acquired distinctiveness effect.
There are no published experiments that allow a direct assessment of the effects of interpolating a phase of extinction or reversal between the normal training and test phases of an acquired distinctiveness experiment.
However, a study of the DOE by Carlson (1974) can supply the information we require.
Recall (see p. 157) that I characterized the DOE procedure as collapsing the two stages of the orthodox, acquired distinctiveness design.
The test discrimination between tone and clicker (say) is aided by the fact that the two stimuli evoke different expectancies (are associated with different reinforcers), these expectancies not being pre-trained but being formed during the course of the discrimination itself.
Carlson (1974), however, reverted to a two-stage design.
Rats in one condition received initial Pavlovian discrimination training in which the tone was used to signal food and the clicker to signal a compound reinforcer consisting of food plus a mild electric shock.
They were then shifted to an instrumental discrimination in which presentation of the clicker signalled that choice of one lever would be reinforced (with food) and presentation of the tone signalled that choice of the other lever would be reinforced (with food plus shock).
The Pavlovian training produced positive transfer to the test discrimination.
As fig. 5.12 shows, these subjects learned the test more readily than did control subjects that were trained on the same task but had received initial Pavlovian training in which the tone and clicker were uncorrelated with reinforcer type.
The    comparison between these groups amounts to a comparison between subjects given acquired distinctiveness training and controls given a form of acquired equivalence training and confirms the result found elsewhere.
The novel finding, of central importance here, comes from the behaviour shown by a third group of subjects.
Carlson's third group of subjects received initial Pavlovian training in which the auditory cues were correlated with reinforcer type, but the relationship between cue and reinforcer was the reverse of that in effect during the test stage.
In initial training the tone was established as a CS for food, but in the test task a correct response in the presence of this cue yielded food plus shock; the clicker was initially trained with food plus shock in the first stage, but in the test a correct response in the presence of this cue yielded just food.
This reversal of the relationship between cue and reinforcer produced poor learning of the test discrimination, performance being retarded with respect to that of both the other groups (Fig. 5.12).
Associative theory has, in principle, no problem in accommodating these results.
The solution of this, rather difficult, test task is critically dependent on the rat forming associations between the cues and their differential outcomes.
These associations serve to promote discrimination in one or other of the way described above.
Clearly, training that allows the subjects to come to the task with these associations ready formed will be of help; and equally clearly, reversed pre-training will have just the opposite effect.
But these results run counter to the differentiation theory.
According to this theory, initial discrimination training should produce differentiation of the stimuli and the fact that the associations formed during this stage of training do not accord with those likely to be formed during the test phase should be of no consequence.
The stimuli having undergone differentiation should be, and remain more, discriminable, regardless of the fate of the associations they may have acquired.
(c) Absence of positive transfer after distinctiveness training
Differentiation theory in its basic form seems constrained to predict that distinctiveness training should produce transfer to any test task that requires discrimination between the differentiated stimuli.
But the associative theory can predict that the mechanisms that normally produce acquired distinctiveness might fail to do so, or might even produce negative transfer with certain sorts of test task.
An experiment by Tetewsky and Garner (1986) can be interpreted as demonstrating such a failure.
Two groups of undergraduate subjects were compared, a group that had learned to read Hebrew and a group that had not.
Familiarity with Hebrew was taken to approximate to having had distinctiveness training with this alphabet.
Hebrew letters might, therefore, have undergone differentiation for the former group and would certainly have become involved in some associations.
In particular, the symbol for gimmel would have become associated with some name or sound equivalent to that associated with the Roman ‘G’; the mem symbol and the Roman ‘M’ would also have acquired a common associate.
All subjects were tested on tasks in which they had to sort cards bearing these letters in various ways.
In one,gimmel and G had to go in one pile and mem and M in another; a further task required the subjects to put mem and G together and to put gimmel with M.
If the symbols for mem and gimmel have become differentiated for Hebrew readers during the course of their previous experience with this alphabet, these subjects should be at an advantage on both test tasks — an enhanced ability to discriminate between the two Hebrew symbols should be helpful in both cases.
The associative theory, on the other hand, predicts an  advantage only on the first task, the associations being of no help, or perhaps even hindering, when symbols having a common associate require different responses.
The outcome was that predicted by associative theory.
Those who knew Hebrew took significantly less time to sort the pack than those who did not in the condition that required symbols having the same name to be put together.
The groups did not differ in their card-sorting times on the other task.
(d) The specificity of the effects of discrimination training
In experiments using the within-subjects designs of fig. 5.3 or of Table 5.1 (e.g. Grice 1965; Reese 1972; Honey and Hall 1989c experiment 3), initial training involves three stimuli, two, A and B, being treated in the same way  and the third, C, being associated with a different outcome.
Subsequent tests show discrimination between A and C to be superior to that between A and B.
The explanation for this result offered by mediation theory depends on the fact that the initial phase of discrimination training establishes links between both A and B and a common associate (X in Table 5.1), and between C and a different associate (Y).
According to my account of differentiation theory, however, the effect of discrimination training depends not on the associations it creates but on the fact that it concentrates the subject, s attention on distinctive features of the stimuli.
It is difficult to see how this latter analysis can predict the results obtained.
Consider the experiment by Honey and Hall (1989 c ) in which X was the delivery of food and Y its absence.
In order to establish the appropriate conditioned responses (or expectancies) during the first stage of training, the subjects would have to attend to and discriminate those features that distinguish A from C and those that distinguish B from C. Any plausible mechanism capable of allowing a subject to do this would also endow the subject with an enhanced ability to discriminate between A and B because it would involve the animal in coming to respond to the distinctive features of each of the three stimuli.
Thus, discrimination training of the sort considered here must be predicted to have general effects, enhancing the discriminability of all the stimuli.
But the results show, as we have seen, that the effects are specifically determined by the nature of the associations formed.
It is worth adding, as something of an aside, that the experimental result just described not only gives reasons for preferring the associative account but also seems to demand an explanation in terms of acquired equivalence rather than acquired distinctiveness.
At the empirical level, all we can observe is a difference between the acquired equivalence condition (good generalization from A to B) and the acquired distinctiveness condition (poorer generalization from A to C).
But, given that we must interpret this difference in terms of the mechanisms postulated by mediation theory, certain conclusions follow.
First, good generalization from A to B can be readily explained in terms of mediation by the associate.
Conditioning with a pre-trained A allows the associatively activated representation of X to acquire associative strength and thus B will be able to evoke responding by virtue of its association with the X representation.
C, lacking an association with X, will be able to evoke the test response only to the extent that, without any special training, it holds elements in common with A. Now, for acquired distinctiveness to occur, for the generalization between A and C to be reduced, it is necessary for the training procedures to reduce the extent to which these common elements acquire strength.
I allowed in discussion of the mechanisms of acquired distinctiveness that the acquisition of strength by a representation of strength by a representation of X might restrict the strength acquired by A itself during acquisition of the test response.
There is  no reason, however, why the restriction should apply only to the elements that A holds in common with C and not those that it holds in common with B. That is, the overshadowing mechanism, if it operates, should reduce generalization both from A to C and from A to B. It cannot, therefore, be responsible for the result obtained.
This is not to say that the overshadowing mechanism is not operating here or to deny that such a mechanism may be responsible in other cases for observed acquired distinctiveness effects.
We can say, however, that whatever may be true of acquired distinctiveness, the reality of acquired equivalence must be accepted.
(e) Conclusions
The evidence discussed in this final section of the chapter has been uniformly in favour of the associative account of acquired equivalence and distinctiveness.
We can conclude that an associative mechanism is required in order to explain these findings.
(In some cases we can be more specific and assert that the associative mechanism in question must be that responsible for acquired equivalence.
This is not to deny the possibility that an associative acquired-distinctiveness effect may also operate in other cases).
There is no evidence that requires us to accept differentiation theory.
What remains possible, however, is that differentiation occurs alongside the associative processes.
In standard acquired-distinctiveness experiments, of course, the two mechanisms will work toward the same outcome.
The various experiments just discussed use procedures specifically chosen to ensure that differentiation effects and mediation effects tend toward different outcomes.
The fact that in these experiments there is no evidence for differentiation may mean only that differentiation produces less powerful effects than does mediation.
A subject trained with both A and B each followed by event X may come to perceive the distinctive features of A and B, something that might reduce generalization between them.
But to accept this does not make it necessary to deny that A-X and B-X associations may be formed and that X could thus act to mediate generalization between A and B, perhaps outweighing the effects of differentiation.
Whatever the theoretical possibilities, it is clearly more parsimonious to explain acquired distinctiveness and equivalence solely in terms of associative mechanisms.
Is there any positive evidence that might prompt us to adopt the more complex position that differentiation occurs as well?
Three lines of argument merit our attention.
First, it may be recalled that in Chapter 2, I concluded discussion of habitation by allowing the possibility that simple exposure to a stimulus might result in the formation of an increasingly detailed and well-specified representation of it.
This possibility is, of course, an important aspect of what has been described in this chapter as stimulus differentiation.
However different they may be from those involved in habituation, the training procedures used in studies of acquired distinctiveness necessarily involve the    subjects in receiving repeated exposure to the critical stimuli.
The processes underlying habituation training will presumably go on during distinctiveness training and thus, if we accept the interpretation of habituation discussed in Chapter 2, we must also accept the possibility that stimulus differentiation plays a part in acquired distinctiveness.
The second argument is based on the conclusion derived in Chapter 4 that associative learning tends to be context-dependent.
In contrast, acquired equivalence/distinctiveness effects seem to transfer quite readily from one context to another.
In many of the experiments described in the earlier parts of this chapter, the context was changed substantially from pre-training to the test.
Perhaps the most dramatic example is provided by Lawrence (1949) himself.
Figure 5.13 shows the apparatus used by Lawrence in each phase of his study.
The stimuli may have remained the same but almost all other aspects of the training situation were radically altered in the shift from the simultaneous to the successive procedure.
If acquired distinctiveness depends on associative mechanisms, we might expect it to be attenuated or abolished by a change of context that renders these associations less effective.
The tact that the effect survives a contextual change so readily encourages acknowledgement of the possibility that a non-associative process (such as differentiation is presumed to be) may be in part responsible for it.
Finally, we may return to the fundamental difference between the mediation and differentiation theories of acquired distinctiveness — for the former the associations formed during distinctiveness training play a critical role, whereas for the latter these associations are essentially irrelevant.
An implication of differentiation theory but not of the associative theory, therefore, is that mere exposure to a pair of stimuli without their being followed by any special consequences should be enough to enhance their discriminability.
Discussion of the various phenomena of ‘exposure learning’ in Chapter 1 revealed just such an effect — rats given prolonged exposure to a pair of stimuli presented in their home cages learned a subsequent simultaneous discrimination between these stimuli more readily than subjects for whom the test stimuli were novel (Gibson and Walk 1956).
This single observation is not enough in itself to make us accept the assertions of differentiation theory — there are, it turns out, a number of possible interpretations.
But it is certainly enough to make us want to persist with our exploration of this theory and to examine in some detail (in Chapter 7) the various possible explanations for the effect.