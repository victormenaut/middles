

The Ninth British Library Research Lecture
Computers and the humanities
Anthony Kenny
The British Library 1992
Scholars in the humanities, like researchers in other subjects, make use of computers for word-processing, for cataloguing the books in their departmental libraries, and also for desk-top publishing.
This generic computer use is not my topic.
I want to talk about the use of computers in actual research, and the features of that use which derive from the special characteristics of the humanities.
University lecturers in the humanities are making increasing use of computers for teaching purposes; but this too lies outside my scope.
The uses of the computer in humanities research can be divided into three main kinds, depending on the nature of the objects studied.
A humanities researcher may use a computer to study strings (as in natural language processing or the analysis of literary texts) or to study numbers (of discrete items or events, as in social and economic history) or to study icons, whether visual (as in art history), or acoustic (as in musicology).
Of course individual humanities disciplines make use of more than one of these types of computer study: the student of literary style will make statistical studies of the frequency of the lexical strings and grammatical features identified in his chosen text; the historian may have occasion to search through and analyse the texts of his primary sources in machine-readable form.
In art history a large part of electronic data processing takes the form of cataloguing works of art, and only a small proportion of research concerns pattern recognition or digitization of images; in musicology it has only recently become possible to study scores directly input to the computer without the mediation of alphanumeric code.
In all humanities disciplines the computer is used in an endeavour to replace intuition with quantification.
Linguists, students of literature, art historians and musicologists all seek to identify styles, and the similarity and differences between them.
Common to all these disciplines is the question: what statistics define a style?
How far can a style be identified with an aggregate of statistical features?
 Similarly , a historian will make use of statistical techniques to give objective substance to intuitively sensed historical processes or trends.
In all humanities disciplines the use of computers seems to have passed through three stages, each corresponding roughly to a decade.
The first stage was the period of pioneering work: the abortive first attempts at machine translation; the earliest computer concordances; the early authorship studies of Morton, Ellegard, Mosteller and Wallace; the first conferences on computing in the humanities (1964–7) and the more specialised ones in history and anthropology (from 1962) and in literature and linguistics (from 1970).
During this period concordances to many major works of literature were produced by computer, but published in traditional hard copy.
Many of these early projects were the work of interdisciplinary research teams, including not only humanities scholars but senior information scientists, responsible for custom-designed software.
This period lasted until about the end of the sixties.
The second period lasted through the seventies until the early eighties.
Humanities scholars learned how to write their own programs, as programming languages became easier to master.
But more significantly the development and widespread availability of pre-designed packages, such as OCP (the Oxford Concordance Package) for concordance construction, and SPSS (Statistical Package for Social Sciences) for statistical analysis, meant that it was possible for the humanist to become more independent of the computing experts, and to operate with at most low-level assistance.
In history, in particular, the use of computers to provide material for use in monographs, once a matter to be trumpeted in the text, became a commonplace to be noted without elaboration in footnotes.
Humanities computing centres were set up in Oxford, Liege, Tubingen and other places.
While the availability of systems such as SPSS meant that there was a temptation for humanists to fit their special data to the software, rather than to design software for the data, on the other hand there began to be developed systems for source-oriented data entry, of which the most famous was CLIO.
Throughout the seventies and the early eighties, humanities scholars, despite their growing independence, still worked in collaboration with, and usually in proximity to, their colleagues in computing centres.
With the advent of cheap microcomputers in the mid-eighties, the typical humanities scholar was more likely to be found doing research at home on a personal computer, attached no doubt to a network, but engaged much less interactively with  colleagues in the computing centre.
As a result of the microcomputer revolution, electronic data processing has become a familiar feature of research in most humanities disciplines.
Nowadays, I am told, in Germany most doctoral students in history make use of computers, not just for word processing, at some stage of their studies.
No doubt this will shortly be the case, where it is not so already, in other European countries.
The building of large databases, whether unstructured (like text-archives) or structured (like many historical databases) has been a major preoccupation of this period.
The development of CD-ROM techniques has been of particular  significance for students of texts: classics students can search the complete database of ancient literature by using the IBYCUS implementation of the Thesaurus Linguae Graecae; students of English can use the CD-ROM version of the Oxford English Dictionary to conduct searches which would take lifetimes if conducted on the twenty volumes of hard copy.
In archaeology, I understand, it can now be said that computers are automatically considered for any project, however small.
Throughout these stages of development, the research conducted by humanities scholars has taken place at three levels.
The lowest, most general, and most unambiguously useful level consists in the employment of a computer to perform a humdrum task in less time than would be possible for an unaided human: the search through a text for a quotation, the determination of a word frequency, the construction and analysis of a contingency table, the classification of iconic data.
This kind of activity will leave little visible trace in the published work of the researcher.
At the other end of the scale there are ambitious research projects undertaken for methodical demonstration purposes.
The results are sought not so much to enrich the domain of research with fundamentally new findings as to demonstrate the validity of some new form of automatic processing.
Two examples of such projects are discussed in a recent paper by Manfred Thaller.
In France Gian Piero Zarri has developed a system known as RESEDA to allow Artificial Intelligence techniques to be applied to historical source material (concerning the Hundred Years War).
The project is well known in AI circles; but it seems to have left virtually no trace in the historical literature.
This is because the system is so complicated that no historian could use it without explicit support from the AI project group.
Again, in 1981–7 the Volkswagenstiftung funded on a large scale the ARCOS project, a system which photographs a three-dimensional archaeological object, provides a drawing and converts it into data which lend themselves easily to methods like Cluster Analysis.
The prototype was in the vanguard of technical development.
Unfortunately the price of the prototype was so high that it was out of the financial reach of the archaeological community.
Projects of this kind win reclaim in the literature of AI, but pass almost without remark in the parent humanities disciplines such as history and archaeology.
Where, in fact, are we to look for the major effects of the use of computers in humanities research?
The most spectacular is the creation of the whole new topic of research, the new discipline of computational linguistics.
The most widely felt effect has been the perfecting of traditional tools of research, data-collections, concordances, dictionaries, and corpora.
New tools of research, and new testbeds for theories become possible.
Perhaps the most dramatic opportunity being developed for linguistic studies is the British National Corpus.
This is a collaborative venture, involving three publishers (OUP, Longman, Chambers), the British Library, and two universities (Oxford and Lancaster).
The aim is to provide a corpus of 100 million words of contemporary spoken and written English to provide raw data for the empirical study of language by a wide variety of disciplines.
The project started in January 1991 with a grant from the DTI and the SERC.
When complete it should provide an unparalleled resource for lexicographers, and information technologists, and a test-bed for theoretical linguists.
But in spite of the multiplication of new basic research tools in the humanities, it is surprisingly difficult to point, in specific areas, to solid, uncontroverted gains to scholarship which could not have been achieved without the new technology.
The high hopes which some computer enthusiasts held out that the computer would revolutionise humanistic study have been proved, over and over again, to be unrealistic.
Sometimes the initial claims made were much exaggerated: as the claim made in New Testament studies that the computer would for the first time reveal the true lineaments of the Bible that we had ignorantly worshipped.
But even in areas where there was no hubris in the initial claims, the results delivered have often been disappointing.
Between humdrum research and showpiece research, what the humanities scholarly community is really anxious to see is work which is both (a) respected as an original scholarly contribution within its own discipline and (b) could clearly not have been done without a computer.
In the last three decades the number of such contributions is sadly small.
No doubt in each discipline my colleagues could single out a handful of seminal projects.
What I want to do, in this second part of my lecture, is to illustrate in the field in which I have been myself involved — literature and stylistics — the kind of projects which have been attempted and the limited success which has been achieved.
From the outset, some scholars have been highly suspicious of the use of numerical methods in literary studies.
This suspicion is particularly strong among biblical scholars, and one of them, Principal T.M. Knox, said more than twenty years ago ‘The spirit moveth where it listeth and is not to be reduced to the numerical terms with which alone a computer can cope’.
But such hostility to numerical methods seems irrational, and often inconsistent.
Biblical scholars have long noticed, and based arguments on, the occurrence of hapax legomena, or words which occur once only in a given text or author.
But if one is to allow counting at all in a literary context, it seems perverse to insist that counting should stop at the number one and go no further.
Since long before the invention of computers, students of literary texts have made extensive use of concordances.
Concordances are often more helpful than dictionaries for determining the precise sense of a word in an author's vocabulary, and they enable favourite phrases and tricks of style to be identified and catalogued.
To make a concordance of a  lengthy text by hand calls for months or years of tedious labour.
With the aid of a computer, concordances of many kinds can be made with comparatively little difficulty.
Students of literature who used concordance packages on mainframes commonly asked the computer to produce for them a complete print-out of their chosen concordance, and then took it home to consult in the traditional manner, turning the pages of a sheaf of paper.
Producing several hundred pages of hard copy with a desk-top computer and printer would be a long and tedious business.
It is much more common now to use the facilities provided by a package such as micro-OCP to generate a particular concordance entry on the screen of a PC directly from the text-base whenever it is needed.
Thus, if you want to study the use of the word ‘true’ in Arthur Hugh Clough, rather than generate a massive concordance containing all the words in Clough's poems, and then leafing through it to find the word ‘true’, you use the concordance package commands to generate a concordance which contains all and only the uses of ‘true’.
You then consult that on the screen and print it out only if you think you are going to want to look up the same word again.
Concordances produced by computer may differ from traditional hand-made concordances in several ways.
One is that they will be much more complete: they will contain all the tiny function words which were too frequent, and regarded as too unimportant, to figure in a work like Cruden's Concordance to the Scriptures.
More interestingly, the items which provide the focus to the concordance need not be dictionary headings or even vocabulary words at all.
A particular grammatical feature, rather than a particular lexical item, may provide the focus of the concordance; thus, you may look up, in a Virgil concordance, not the occurrences of ‘bellum’, but the  occurrences of the second declension neuter singular nominative noun.
Concordances of this kind to the New Testament have been produced on the basis of the Analytical New Testament compiled by Barbara and Timothy Friberg (Baker Books, 1981).
What is the literary scholar to do with all the information to be found in the  concordances and word-lists generated by the new technology?
Much of the material will no doubt be exploited in all kinds of traditional ways, for the study of meaning, style, and language history.
But the computer also facilitates one particular type of study of texts: the study of their quantifiable features by statistical methods.
The statistical study of style is a discipline which is still in its infancy, but has recently been showing signs of vigorous growth.
It is often called ‘stylometry’.
Stylometric studies may be undertaken for many different reasons.
One may wish to study the statistics of word usage or word order with a view to understanding a text better, to catch nuances of meaning and perhaps to render them into a different language.
Or one may be interested in the history of the development of a language, and study the speech habits of particular authors as an indication of linguistic change.
Or one may hope to use the quantifiable features of a text as an indication of the authorship of a text when this is in question.
Authorship attribution studies of this kind have caught the public eye rather more than other types of stylometric inquiry.
It is sometimes claimed that stylometry enables the scholar to identify the fingerprint of an author, a stylistic criterion, or set of criteria, which can be used to determine with certainty questions of disputed authorship.
Such claims seem exaggerated, on the basis of the evidence presented hitherto; but there is no doubt that stylometry can offer valuable evidence of a novel kind which, taken in conjunction with more traditional evidence, can often throw new light on ancient problems.
The study of hapax legomena formed the basis of one of the earliest sophisticated statistical studies of the Bible, that of K. Grayston and G. Herdan in 1960, who, building on work of P.N. Harrison in the nineteen twenties, argued that the Pastoral Epistles could not be the authentic work of the Apostle Paul.
Probably the best known stylometric work on the New Testament is that of A.Q. Morton.
Morton, in collaboration with others, has argued, from 1966 to the present time, that in the Pauline corpus only Romans, 1 and 2 Corinthians, and Galatians form a homogeneous group which can be attributed to a single hand.
Morton's arguments were based on a study of sentence lengths, on the frequencies of common words, and on other grammatical features such as the proportion of different parts of speech.
My own investigations into the NT, based on the full data provided by the Friberg Analytical NT, came to much less dramatic conclusions than Morton.
I addressed first two ancient problems: the relationship between Luke and Acts (I found that Acts resembled Luke more than it resembled any other book of the NT) and the relationship between the Apocalypse and the Gospel of St John (I found that they differed from each other more than any other two books of the NT).
On the more difficult topic of the Pauline corpus I concluded that the evidence was equally compatible with a widely differing set of authors or a single unusually versatile author, but that it did not support the view that four epistles stood out as uniform by comparison with all others.
The use of stylometry in authorship attribution studies depends on the hypothesis that there are quantifiable features of style which are characteristic of particular authors.
Ideally, a stylometric test of authorship should be a feature which is characteristic of all the known works of a particular author and which is unique to his works.
Features which are to be found in all, and only, the works of a particular author turn out to be frustratingly difficult to come by.
Authorship attribution problems are easier to deal with when they can be cast into the following form: in respect of the measured features, does the doubtful work resemble the work of candidate author A more than it resembles the work of candidate author B?
A classic example of this kind of study was the work of the Harvard statisticians Frederick Mosteller and David Wallace on the Federalist Papers.
These papers were a series of articles published in 1787 and 1788 to persuade the citizens of New York to ratify the U.S. Constitution; most of the papers were known to have been written by Hamilton or by Madison, but the authorship of twelve of them was disputed.
By comparing the rate of occurrence of particular words in the disputed papers with the rates in known Hamilton papers, and rates in known Madison papers, the authors were able to establish beyond reasonable doubt that Madison wrote the disputed papers.
A partly similar problem is presented by the Aristotelian corpus, which contains two ethical treatises of parallel structure, the Nicomachean Ethics and the Eudemian Ethics.
The problem here is not one of authorship attribution, since most scholars regard them both as genuinely Aristotelian; the puzzle is that three books make a double appearance in the manuscript tradition, once as books five, six and seven of the Nicomachean Ethics, once as books four, five and six of the Eudemian Ethics.
In 1978 I published a study (The Aristotelian Ethics) in which I argued, on the basis of twenty-four independent tests based on 60% of the word usage in the text, that the disputed books resembled the Eudemian context rather than the  Nicomachean one.
Similar problems have been studied by similar methods in the very different context of the law-courts.
It is sometimes a question whether a particular confession is the unaided work of an accused or is a fabrication of a police officer.
A.Q. Morton, in his book Literary Detection records how stylometric evidence was admitted in such a case in 1974.
The Californian court trying Patricia Hearst for bank robbery was asked to hear stylometric evidence to help determine whether she composed the propaganda for the Symbionese Liberation Army which was part of the case against her.
The trial judge refused the evidence on the grounds of ‘the relative infancy of this area of scientific endeavour’.
But anyone working in this field will receive from time to time invitations to offer evidence as an expert witness in such cases.
In recent years many authorship attribution problems, in a variety of languages, have been tackled, with varying degrees of success, by stylometric methods.
Thus Thomas Merriam has used these methods to try to show that Sir Thomas More is the genuine work of Shakespeare, and it was partly on stylometric grounds that the poem ‘Shall I die?’ won its place in the Oxford Shakespeare.
Y. Radday has used Hebrew stylometry to address the problem of the unity of Isaiah and to argue against the standard critical theory which sees the book of Genesis as woven from three disparate strands of tradition.
In spite of the proliferation of stylometric authorship studies, it would be wrong to think that attribution is the only goal for statistical studies of style.
One of the most interesting recent works is J.F. Burrows' Computation into Criticism.
This is a study of Jane Austen's novels based on common prepositions, conjunctions, pronouns and the like.
‘It is a truth not generally acknowledged’ Burrows points out ‘that in most discussions of works of English fiction, we proceed as if a third of our material were not really there.’
Burrows shows how Jane Austen makes skilful use of the common words to build up different idiolects for the different persons in the novels to bring out the differences in their characters.
Admiral Croft, for instance, uses ‘we’ and its cases more than ten times as frequently as Lady Catherine de Burgh.
It is by tiny touches such as these — though only a stylometrist would consciously notice them — that Jane Austen builds up the picture of one character's clubbability, and the other's insolence of rank.
Computer-assisted studies of style have often attempted to settle questions of chronology in the work of authors whose works are undated.
Leonard Brandwood has worked for many decades on the chronology of Plato's dialogues.
One of the most recent contributions to this debate has been that of Gerland Ledger, in his book Recounting Plato.
Ledger took the simplest possible feature of language — the occurrence of particular letters of the alphabet — and by subjecting this to a complicated multivariate analysis he came closer than many previous workers to identifying particular authors and even works.
He drew new conclusions about the authenticity of the dubious dialogues, and the chronology of the entire corpus.
In the study of literature, quantitative methods can never achieve the primacy which they enjoy in the experimental sciences and in many branches of social studies.
But scholars who use them with a sober sense of their limitations can add a new dimension to literary studies, and in an age of divided cultures we should surely be ready to welcome a marriage of literacy and numeracy.
However, of the studies I have mentioned it would be difficult to claim that more than a handful fulfilled the criteria I mentioned earlier: both (a) respected as an original scholarly contribution within its own discipline and (b) could clearly not have been done without a computer.
Indeed throughout humanities disciplines, after thirty-odd years of this kind of research, there are embarrassingly few books and articles which can be confidently pointed out as passing both tests.
This has meant that many enthusiasts for computing in the humanities have an uncomfortable sense of crisis, a feeling of promise unfulfilled.
Gone is the glad confident morning in which Ladurie could say ‘L'historien de demain sera programmeur ou il ne sera plus’.
The feeling of disillusion is indeed partly the result of the misplaced optimism and exaggerated claims of some of the pioneers: the belief was sometimes encouraged in the past that feeding data into a computer would automatically solve a scholar's problems.
Rare has been the computer project which did not, in the course of execution, bring to light an initial overestimation of the technical possibilities, and an underestimation of the problems of data preparation.
The proliferation of personal computers in the last decade has often, embarrassingly, gone with an actual diminution in methodological sophistication.
If the fruits of computing in the humanities are less abundant than one might have hoped, there are also disadvantages of the introduction of computers which are becoming all too obvious.
Let me mention six, which I will call: redundancy of people, redundancy of effort, diversion of effort, diversion of funding, distortion of research and transfer of population.
1)
Redundancy of people.
Alexander Cruden spent the best years of his life preparing his concordance to the Bible.
There is no longer any need for dedicated Crudens to make concordances.
Classical scholars among us will all, as students, have stood in awe of our seniors who, given a rare Greek word, could pluck out of their memories the ten contexts in different classical authors in which it was used.
Nowadays any student can discover that information in moments with the aid of IBYCUS.
It is no longer possible to found a reputation for scholarship on being good at spotting allusions.
This problem is the humanistic analogue of the problem of the stoker on the electric train.
It is not a serious one for the discipline, nor even in human terms does it compare with the undesirable side-effects of technological innovations in practical life and scientific research.
2)
Redundancy of effort.
Scholars who are good at knowing where to look for the sources of their subject matter may not be good at knowing where to look for suitable software.
The replacement of mainframe use by desktop computing in recent years means that scholars working alone do not have as much contact with computer-learned people as they used to do.
This means they are less in touch with software developments.
Hence, if they get interested in designing their own software, scholars may spend their time reinventing last year's wheel.
3)
Diversion of effort.
Even if the software designed is genuinely original and useful, there is a danger that scholars become hooked on the production of software.
There is a risk that a scholar whose talents and academic position call for historical and critical work will spend most of his or her working time as a programmer.
4)
Diversion of funding.
Because there is pressure on departments in humanistic subjects to appear up to date and efficient, it is much easier to persuade funding bodies to give money for computers and software than to buy manuscripts, rare books, or second and third copies of frequently used library texts.
After a few years a department may be left with serious gaps in its library and a load of superannuated computing equipment.
5)
Distortion of research.
There is a danger that projects may be undertaken not because they are likely to lead to academically interesting results, but simply because they are susceptible to computerisation.
Scholars feel the need to show they have used the latest technology, even if electronic methods are not the appropriate one.
But in the words of Manfred Thaller it is important for humanists to adapt the software to the scholarship, not the scholarship to the software.
6)
Transfer of populations.
Professor Connor, in a fascinating paper on computers in classical studies, claims that computers came just at the wrong time, at a time when scholar's interests were moving from textual studies to critical theory, women's studies and the like.
He says: ‘Computer technology became available precisely at the wrong moment in the profession's development.
The era of traditional lexical and textual studies had largely passed…
The energy of North American classicists, by and large, has concentrated on interpretive questions and on the writing of essays and monographs of social themes, cultural history and literary criticism…
The questions posed by Feminist, Marxist, Structuralist and Post-Structuralist criticism have not lent themselves to computer-based responses.’
Is this just a malign coincidence?
One may well speculate on possible causes for the phenomenon.
One does not need to draw exaggerated conclusions from the differences between the left and right hand sides of the brain in order to accept that humanists and scientists may be different kinds of people.
Those whose gifts and tastes lie in a certain direction — people who do not have a head for mathematics — may well have been influenced by this in an early career choice.
To their dismay they see quantification invading their own subject: they might as well have become physicists or engineers if English literature, Greek history and even New Testament theology now offer no escape from those wretched numbers.
So now the more abstract, intuitive, and ideological branches of the humanities become more attractive.
The number of practitioners in these areas grows — largely, no doubt, through the genuine fascination of their subjects, but swollen perhaps with an influx of refugees from the relentless advance of technological rigour.
There are still, thank God, a few areas where one can work without either having to use computers or feeling guilty because one does not.
If this is a correct analysis of the situation, the  consequences for our disciplines may be far-reaching in unexpected ways which are only just beginning to be noticed.
I would like to end by asking what lessons can be drawn from the successes and the disappointments of recent decades.
No doubt there are many, but I would like to single out just three.
The first is that it is important never to lose sight of the fact that a computer is an assistant to, not a substitute for, a human researcher.
Neglect of this obvious truth led to computer programs (say, for language analysis) being regarded as failures if they could not record 100% success.
More recently, we have learnt to become grateful for robust programs which will do 95% of the work and leave the human being to make the decisive intervention in the other 5% of cases.
A second lesson is that even when the computerization of a research project fails because of the  intractability of a particular set of data, it can be an intellectually instructive task to try, and fail, to prepare the data in a formalized manner.
And the successful formalization of data can open up questions for research, and types of evidence, which would not have occurred to the research community without the effort to present the data in computer-friendly form.
The third lesson is that it will not be possible for humanists to take full stock of what the computer has to offer to their disciplines until the study of statistics becomes a normal and inescapable part of the training of those who plan an academic career in the humanities.
This needs to be recognized not only at university level (as in France, where now a course of statistics is an essential part of the training of an academic historian) but also in any high school which has an interest in sending on students to do university work in the humanities.
There is still a great lack, in every language, of good textbooks designed to teach statistics outside the context of the natural and social sciences.
Apart from strictly academic consideration, the ordinary citizen of any modern democracy will find an elementary course in statistics of more general educational value than courses in e.g. differential calculus, which are more popular in science-oriented schools.
One of the unquestionable advantages of the growing use of computers by humanists is that it has done something to bridge the divide between two cultures — scientific and humanist — which many writers have lamented as threatening to fragment the community of scholars and the republic of learning.
A by-product of the computer revolution has been an increased understanding by scientists and humanists of each others' methods and preoccupations.
Surprisingly, in the last decades is that literary studies, perhaps especially classical studies, which may seem to be at the other extreme of the academic spectrum from the sciences, have shown themselves more self confident in the use of computers than history has done, even though history is closer to the social sciences which have for long been acclimatized to quantification and computerization.
On the other hand, the flight in some literature departments from traditional methods of study, with their emphasis on the detailed analysis of texts, towards theory-laden forms of criticism with an explicit political agenda, is a development which is not favourable to the search for objectively quantifiable features capable of computerized study.
What will the next decades hold?
Will it be seen, with hindsight, that the introduction of the computer into the humanities disciplines has made no more substantial change in the eventual output of research than was made by the introduction of the electric typewriter?
Or will there be a flowering of research papers, neither pedestrian nor spectacular, containing solid and original results obtained by techniques for which the computer is indispensable?
It is hard to make a confident prediction.
But the testing time has now arrived; because for the first time posts of leadership in humanities departments are being taken up by a generation of scholars who have been familiar with the computer from their earliest schooldays, scholars who are neither frightened by, nor over-respectful of, the new powers which the computer has brought.
References
Brandwood, L., A Word Index to Plato, Manchester, 1976.
Burrows, J.F. Computation into Criticism: A Study of Jane Austen's Novels and an Experiment in Method, Oxford, 1987.
Friberg, B. and T. Analytical New Testament, Grand Rapids, 1981.
Grayston, K. and Herdan G., ‘The Authorship of the Pastorals in the Light of Statistical Linguistics’.
New Testament Studies 6 (1960), 1–15.
Kenny, Anthony, A Stylometric Study of the New Testament, Oxford, 1987.
Kenny, Anthony, The Aristotelian Ethics: A study of the Relationship between the Eudemian and the Nicomachean Ethics of Aristotle Oxford, 1978.
Ledger, Gerard R. Re-Counting Plato: A Computer Analysis of Plato; s Style, Oxford, 1989.
Morton, A.Q. and McLeman J.J., Paul, the Man and the Myth, London, 1966.
Morton, A.Q. Literary Detection, London, 1978.
Mosteller, F. and Wallace, D., Inference and Disputed Authorship: the Federalist, Reading, Mass, 1964.
Radday, Yehuda.
Genesis: An Authorship Study in Computer-assisted Statistical Linguistics, Rome, 1985.
The Humanities Computing Yearbook, 1989–90, ed.
I. Lancashire, Oxford, 1991.