

Acknowledgements
The crucial nature of the questions addressed at this seminar resulted in our receiving the backing of a number of organisations.
The organisers and the participants in the workshop wish to extend their thanks to both the sponsors and the funders for their support.
At its 1992 Conference held in Bologna the International Association for History and Computing accepted a proposal for a workshop to examine issues related to Electronic Information Resources.
The British Library's Research and Development Department offered a substantial grant (RDD/C/160) to make it possible for the seminar to take place.
With the additional assistance of a British Conference Grant from the British Academy the workshop met in London on the 25th and 26th of June 1993.
The venue, the British Academy, had been kindly offered by the Secretary of the Academy, Peter Brown.
Not only did the British Library provide the principal funding for the seminar, but its staff also provided valuable contributions.
We extend thanks to Brian Perry, Director of the Library's Research and Development Department (R&DD) for his encouragement and his ‘after dinner’ thoughts on the 25th of June, and to Stephanie Kenna, also of the British Library's R&DD, for the essential guidance and assistance she gave us throughout the pre- and post-seminar stages.
Sir Keith Thomas PBA, then President-elect of the British Academy, in his opening remarks mused that he was surprised to hear only of plans for printed publication of the contributions.
As a result the publication will be made available in an electronic version from 1 February 1994.
The then President of the British Academy and the current Chairman of the British Library Board, Sir Anthony Kenny FBA, supported, encouraged, and contributed to the seminar.
Dr Manfred Thaller, President of the International Association for History and Computing backed the seminar and arranged for the publication of its proceedings.
1.
Historians, Machine-Readable Information, and the Past's Future
Seamus Ross, Assistant Secretary (Information Technology), The British Academy
1.1.
Background
Awareness among historians of the changing character of contemporary information resources is limited.
Increasingly everyday transactions are recorded and handled electronically.
The increased use of computers has led to a dramatic rise in the production of paper records.
Laser printers and digital printing technology make more flexible output of paper documents possible, and are fostering an industry of ‘on-demand’ and specialised document production and printing.
This paper trail can be attributed to the demands made on electronically stored information by local and national governments, companies, charities, service providers (e.g. medical staff), and individuals.
It reflects their need to satisfy customers, meet regulatory obligations, market services, and make the most advantageous decisions.
In government ‘information is still translated into a paper format in order for a decision to be made and the action arising from the decision to be implemented’(Hammond 1993: 52).
In business paper plays a crucial role in the provision and distribution of strategic management or executive information (Campbell-Kelly 1993; Cashmore 1991).
In the case of operational information, however, this is no longer necessarily nor universally true.
Its capture, storage, and use is already very much electronically-based in many businesses from banking to transportation to retailing, and is becoming more so.
Most transactions involving operational information produce paper as their by-product (e.g. till receipts, credit card vouchers, tickets, prescription forms, invoices, and statements), but paper records represent only a fragment of the material in electronic form.
Electronic information is, furthermore, printed in only a limited number of possible manifestations — frequently the minimal amount of data needed uniquely to record the transaction.
Perhaps, the printed versions in their dispersed form will be of interest and even value to future historians.
The sheer quantity, diversity, and rich quality of the electronic information resources from which these records have been derived would seem to indicate that the preservation of the information in electronic form could provide historians with a better opportunity to understand our period than the paper records alone could ever do.
Just as contemporary workers use operational information as the foundation for paper-based strategic and management reports, historians will want to have the opportunity to analyze the base data for themselves.
They will wish to examine it in ways contemporary users do not need to and to analyze it in the context of other datasets.
Electronic information resources must, therefore, be viewed as cultural products, e-facts, and their preservation seen as crucial to the securing of our cultural heritage.
Discussion of issues surrounding electronic records is not novel; archivists have been concerned with them for at least the last three decades.
Charles Dollar of the National Archives and Records Administration (NARA)(USA) wrote some early studies (e.g. 1971) and these have now been followed by numerous articles in Archivaria ,American Archivist , the publication of special reports, and the summaries of meetings (e.g. Research Issues…1991).
The literature on electronic records has become quite large, but it is very much driven by the discussion of problems and activities in America and Canada.
Throughout most of this period the focus has been on government records and then mainly at a national level, although in rare instances the issues have been addressed at a more local level.
The problems have also been examined from the vantage of the archivist.
Few studies have attempted to study the issues from the point of view of the historian.
A recent issue of the journal History and Computing is one notable exception (1992).
National governments and international organisations were among the first institutions to adopt electronic processing of large datasets (e.g. census data, tax records).
Statutory obligations require they retain their records in accessible form and they have the financial resources necessary to experiment with the possible ways to undertake such preservation.
Here it has been the archivists who have had to confront the difficulties posed by record retention as they attempt to fulfil their statutory duties.
Governments are only one producer and archivists are only one group with a direct interest in the preservation of records.
Other large producers are companies and other interested groups include records managers, historians, and technologists.
1.2.
The Scope of the Problem
The size of the problem from the point of view of governments is widely acknowledged as large and growing.
It is often quoted that 75% of all US government transactions will be electronic by the year 2000.
What the precise implications of this statement are is not quite so clear (e.g. will these be transactions of significance in recording activities, processes, and decisions?; how are transactions defined?).
If the difficulties were restricted to one sector then the implications for future historical study could be placed within a restricted perspective, but this is not the case.
The collection and storage of information is occurring across a spectrum of human activity.
The increasingly significant role that the capture and manipulation of electronic information plays in contemporary day-to-day transactions becomes evident if we consider our last visit to a supermarket.
At the check-out the cashier passes each item being purchased across a scanner which reads barcodes, on the pre-packaged items, which carry encoded information.
Store computer systems use this data to locate pricing information, calculate automatically multi-buy discounts, collect data for sales and stock accounting, perform inventory control and automated ordering, produce item-by-item profit and loss analysis, and identify the items that have attracted greatest customer demand.
Once the barcodes on the items have been scanned and the tabulated amount displayed, the purchaser could present a direct debit card (or a charge card) to make payment.
When the cashier passes the card through a reader, information held in a magnetic strip on its reverse is read.
The cashier then initiates a sequence of automated steps to check a cardholder database to determine whether the card is legitimate (i.e. not stolen) and the amount of the charge is within the cardholders available credit limit.
If these checks raise no anomalies a charge for the cost of the items is allocated to the cardholder's account.
This transaction results in the production of two paper outputs (one of which is in duplicate): a receipt listing the items purchased including the amount charged per item and a voucher signed by the cardholder authorising the account debit or charge (a copy is kept by the store and by the purchaser).
Tens of millions of similar electronically-based transactions take place everyday all producing relatively similar paper outputs and ‘electronic records’.
In the UK alone the 2.5 million customers of Barclays Bank complete some ten million electronic transactions a month[1].
The individual paper by-products may have particularist historical value, but their dispersed character makes it unlikely that their meaning could be reconstructed without the aggregate data from which they were derived.
The electronically captured information could be of remarkable historical value.
This is especially true for research into social history, the history of consumerism, economic history, and ethnic/regional history.
As Morris explains, not only will the transactions themselves provide a remarkable resource, but how they took place (i.e. their context) will have historical significance (1993: 307–308).
Bank and credit card details, airline reservations, patient records, inland revenue data, satellite images, product manuals, and maps, are all examples of the variety of information encoded via a range of input devices from keyboards to scanners on a daily basis.
In the commercial and the public sector, and even on a more private level astonishing amounts of data are being created as word processed documents, some of which are printed out, but others such as email are sent over the wires and viewed only on screen by the recipient or data user.
Currently (1993) there are some twenty million users worldwide of the most commonly used electronic network, Internet, a figure that is growing at a rate of ten percent per month.
Merely imagining the contexts in which we encounter electronic systems offers some ideas about the diversity of electronically stored information.
It is worth bearing in mind just how much data we are talking about.
‘Visa International, the biggest credit-card company handles over 6 billion electronic transactions a year — a figure that is expected to rise to 15 billion by 2000.
To save itself from being swamped in data, Visa dumps its records after only six months’(The Economist , 18.9.93: 120).
Visa's services are marketed by 18,000 banks and its cards are accepted by ten million merchants worldwide.
So far the demographic and market data contained in these records have not been fully extracted before the data are disposed of.
If the commercial benefits to Visa of retaining the data do not outweigh the costs of their retention then there can be little hope that such data will be accessible to historians in the future.
Historians would be likely to ‘mine’ this resource in different ways, examining it with change over time in mind and linking credit transaction data or travel information with say medical records or socio-economic groups.
They will use the information not to understand the individual transactions recorded in the data, but the society(ies) that created it.
The view of our society included in this information category is panoramic.
In the United Kingdom the three large credit reference agencies, Infolink, CCN, and Equifax, hold data on 100 million accounts.
While much of their data relates to credit requests and fraud, an increasing amount of information details customer transactions.
CCN, for instance, has a database of forty-three million consumers in the UK which includes information about an individuals creditworthiness, lifestyle, gender, age, and purchasing habits.
While this information has potential for contemporary target marketing campaigns, it would be immensely valuable to historians studying late twentieth-century Britain.
This would be especially so if bi-annual or annual ‘snapshots’ of this information survived in electronic form.
Historians are not going to have much success convincing these companies to retain this data unless a business case can be established or there are legal requirements that foster its preservation.
Also, as is explained in the Introduction to Part II, in some countries there are statutes which restrict the retention of personal data (Ross 1993: 80).
We must not imagine that such information is confined to large businesses.
The majority of businesses have less than ten employees and an increasing number of them are turning to IT as a low cost way to improve productivity and recordkeeping.
Generally records of small traders have been lost to history and even with the transition to electronic media this is not likely to change.
Charities are also large users of datasets.
Oxfam processes almost all its transactions electronically: orders for gifts from its catalogue, donations to its appeals, and even its bills (such as phone bills).
[2]It also stores vast amounts of data about contributors, and uses this data to target fundraising campaigns.
It is easy to imagine the uses to which historians might put this information: from improving their understanding of the role of charities and giving, to providing material to construct a profile of a twentieth-century charity.
Marketing teams analyze this data to produce profiles of those who give and they use these profiles to better target fundraising drives.
Will the historian of the future be adequately served if only the internal reports profiling donors survive?
Or would their understanding of charities be ‘significantly’ improved if all the data survived?
Discussions of electronic information often focus on large numerical datasets and text files.
The scope of electronic information is broader and expanding.
For instance, architectural historians will want to study plans of contemporary architects.
Many practices use computer aided design/drafting (CAD/CADD) systems, and an increasing number use virtual reality systems to design and ‘test’ building schemes.
The preservation of this sort of information will become increasingly important.
If the information is preserved, it will be in an effort to guarantee its availability in case of legal dispute.
Such preservation will be most likely where particular programs were run to determine the structural stability or the environmental efficiency of a building or the individual or institution commissioning the structure had been asked to accept a particular design after a ‘virtual visit to or walk around of’ the planned building.
Where geographic information systems (GIS) were used to assess proposals and display data in the course of evaluating road or building development plans, not only the data must be preserved, but also the software, any specialised graphics hardware, and even the virtual records that were created during the decision making process.
Since a GIS involves the storage, and manipulation of spatial data as well as the use of a graphical display system (2 or 3 dimensional data display) it is feasible that the impression conveyed by the same data used in conjunction with a different display system might be distorted and might not permit future users to comprehend the meaning that it originally communicated.
It will be crucial to know about the context(s) in which the system had been used.
For example, was the end-user the decision maker or not?
Focusing too heavily on large databases ignores the growing trend toward generating text, keeping spreadsheets (which might be laden with formulae that control calculations and are themselves based on discrete assumptions), and databases on microcomputers.
Increasingly emphasis is being placed on co-operative work and sharing of text files.
The electronic versions of these documents might be crucial if the historians wanted to understand how the documents were developed especially in those environments where cooperative work software or groupware had been used to enhance the performance of those producing documents.
The final records may exist in paper format, while the sources and the evidence for the process of document development are electronic.
Linked and embedded documents pose problems as a change to one document effects a change in another.
Unless a record of these links is kept the context and process of document generation may not be understood.
There is a need for version control.
As document image processing (DIP), involving the use of optical scanners, mass storage devices and networked computers, becomes a more common way to store paper information new problems will arise, because even those strategic documents and the paper products derived from the operational data will exist only in digital form.
Further problems will stem from the growth in visual computing (images, videos (3D) and audio).
The modes of communication are changing and this will have implications for the historical record.
Higg's (1993) points out that the phone and photocopying has changed information distribution and e-mail will change it still further.
Samuel examines the question from the vantage of the corporate and scientific usage of electronic mail (1993).
What status does email have?
There is much to learn from Samuel's discussion of the case of Armstrong v Bush.
The judge ruled that there was information contained in electronic records that printouts did not include, therefore, they were not faithful to the original.
If it will not be possible to substitute hard copy for electronic mail, will this premise eventually be extended to all electronic information/data either by the courts or by legislation.
Rayward describes the value of electronic mail outside government sectors and gives examples of its usefulness to the study of technological advancement and intellectual history (1993: 238–9).
Also electronic mail messages might be the electronic equivalent of the Paston letters that so enliven social history of 15th century England.
Science is being reshaped by electronic communication and storage.
Historians of science who want to understand the development of contemporary theories will need more than the printed journals, they will need access to the electronic resource.
In order for this material to be available for the future good records management practices are essential.
Retention of environmental data, experimental data, and other scientific information will provide valuable benchmarks for future scientists, and useful resources for historians.
What is evident is that the age of electronic records opens numerous possibilities that will enrich the understanding of contemporary culture.
But , on current evidence historians of the future will be left with a large number of disconnected e-facts that will prove difficult to use.
As Swade noted, historians always work with the ‘residue of the past’.
Would a single airline reservation transaction have value to a future historian?
Would it be practical to retain all airline reservations for posterity (Campbell-Kelly 1993: 266)?
Swamping future historians with vast amounts of digital information may impede their research as they attempt to navigate through it.
Zweig explains that faced with the vast quantities of surviving documents in conventional archives few historians can be comprehensive, but with an electronic archive and a toolchest filled with versatile software historians could work with digital information more exhaustively (1993: 256).
A combination of data visualisation tools and neural networks are currently used in military, insurance, and retailing sectors to mine information from data.
These and numerous other software tools and information technology methods will eventually be used by historians.
This assumes most of electronically stored information will survive.
When we met in June 1993 we debated whether we should keep everything.
Perhaps an example would make the issue of the debate clearer.
Digital switching systems allow telecommunications providers to record details of calls to justify charges made for the services provided.
Would there be value to historians if a record consisting of number calling, number called, time, duration, and cost were retained for every call?
Or does this example point up the absurdity of the assumption ‘keep everything’?
One view holds it might be worth preserving only information derived from the telecommunications record, but that it would make little sense to keep it all.
Social historians might want to know that the average phone in the US was used for twenty minutes a day in 1992, whereas the average phone in the UK was used for only four minutes.
The retention of every minor detail might prove impractical, but there will be some historical study that would benefit from all the data being extant.
There will be genealogists who will be fascinated to discover who their ancestors spoke with on the 15th of July 1993 or historical demographers who might wish to use these records to assess the role the telephone played in family communication (Did the phone help to maintain family structure over extended distances?)
1.3.
Retrospective Conversions
Retrospective conversions of printed material into electronic format and the construction of scholarly data resources are not discussed in this volume.
It is true major projects are digitising vast amounts of historical data.
The Archivo General de Indias (González 1992), the Hartlib Papers (Leslie 1990), and the Wittgenstein Nachlaß (Robinson 1993: 85–86) are examples of retrospective conversion projects involving the development of large data resources.
The Archivo project encompasses some 9,000,000 pages.
New data resources are also being produced by the analysis or synthesis of other written or artifact records.
These include Genet's prosopograhy of the members of the University of Paris in the Middle Ages (1988), the British Academy's Lexicon of Greek Personal Names (Matthews, pers comm), the image and text archive of greek pottery being created by the Beazley Archive (Kurtz 1993) and the Corpus of Romanesque Stone Sculpture in Britain and Ireland.
These projects provide fundamental research tools and their creation does raise questions about storage, access, distribution, documentation, and support.
One can only hope that since these are resources being created by scholars who are interested in the dissemination and preservation of scholarship that their broad distribution and usage will enhance the likelihood of their preservation.
Retrospective conversions and new resources offer opportunities for scholars, but they do pose problems for librarians.
Prochaska (1993) and Brindley (1993) both point out that there are real prioritisation issues.
In the humanities there is no decline in interest in traditional printed sources, but there is also a demand for access to new kinds of resources.
Guaranteeing access to additional resources without decreasing the provision of traditional materials is crucial (Brindley 1993: 176).
With eighty percent of the scholarly publications in research libraries printed on acidic paper and major campaigns underway in the US and Europe either to reformat or substitute that material in new formats such as microfilm or digital media it is possible that retrospective conversions will eventually digitize millions of volumes.
But : Digital technologies have significantly greater reliability of reproduction and ease of transmission, and they promise vast enhancement of access.
Still, there are technical obstacles to be overcome before a major shift from film to digital media can occur.
Briefly, there are three principal challenges: improving the quality (resolution) of the scanned electronic product currently available to the user; storing the scanned material in a from that will be accessible hundreds of years from now; and putting into place distribution systems to provide access at any distance (Commission 1993: 14).
The guarantee that the material will be available in even 100 years seems the most difficult of these three challenges.
In some instances the retrospectively converted data will only survive in its new format.
This applies not only to many nineteenth-century books, but also to film.
20th Century-Fox has some 10,000 hours of Fox-Movietone Newsreel film in its library much of which is of interest to twentieth-century historians.
Since this eighteen million metres of film is stored on nitrate media, which is highly flammable and prone to disintegrate under anything but the most optimum storage conditions, 20th Century-Fox has begun digitising it.
The newsreel film, which will be stored on cassettes that can hold up to ninty-six gigabytes of data, will need to be spread over 12,000 cassettes (Fox 1993: 19).
While it is easy to imagine the value of having the information available in digital format, one realises that its preservation is not being undertaken for the value to posterity of the data, but because of its commercial value to 20th Century-Fox.
Retrospective conversions are migrating segments of our heritage from degrading media to a new medium that offers advantages which may prove illusory as the medium or the access mechanisms become obsolete.
1.4.
Resources and Records
The title of the volume attempts on the one hand to avoid the major archival issue of what constitutes a record, and on the other to broaden the issue of debate from electronic records to include digital information resources generally.
It has been argued that there is a chunk of data that must be retained about each record: Where was the document created?
How was it created?
What processes were involved?
What was the purpose of its creation?
Who was it created for?
Who received it?
What effect did its receipt have?
Why has it survived?
(Duchein 1983).
The order of records must reflect the functions and activities at the organisation that created the records.
Often this information does not survive for electronic records.
In some cases they do not conform to the kinds of sources that naturally have such material inherent to their structure.
Would the term information be more comprehensive than records?
What is the difference between information and data?
Does an electronic record consist of the record, the computer system in which it was created, its data structure, a definition of its purpose, associated hardware and software?
Should the term record only be applied to written documents?
What are electronic records?
Morelli examines some of the issues (1993).
There are to his mind terminological ambiguities that cloud the use of the term records in the context of the electronic environment.
Records have a container, content, and purpose.
When the character of the record is electronic it requires a delivery mechanism.
He argues that the ‘computer system itself is an integral part of the record’(Morelli 1993: 88).
Without the system electronically stored data/information can only consist of proto-records.
Whether we adopt Morelli's notion of ‘proto-record’ or not, the definition of a record or ‘proto-record’will vary depending upon the kind of ‘electronic information set’we are dealing with: word processed documents vs multimedia vs relational databases.
The issue is far more complex as Gränström argues.
Unlike paper where records either exist or not, electronically stored information includes ‘potential records’(Gränström 1993: 145).
In Sweden these potential documents ‘which could be made available by the possibilities to combine and search through information using computer techniques were official documents’(ibid.).
In the case of potential or virtual records it is difficult to know, not just where, but even whether the manifestation was created, for what purpose, by whom, and when.
Essentially these records exist only as a set of ‘access and retrieval instructions’ that might have been executed against a given dataset or information-base which was in ‘state’x at time y  .
With the development of end-user computing even the questions of how a manifestation of the data was made and why become different kinds of questions.
In other words the principle of provenance which underlies archival recordkeeping takes on a different form when electronic information is being considered.
The provenance of electronic records in government and business must be related to the culture and organisation that created the information.
As Simpson explains ‘the life cycle of many records will be a patchwork of different units creating, amending, utilising, and determining disposition, possibly without knowledge of what has gone before’(1993: 28).
1.5.
Problems of Context
In understanding documents the context of their creation and use are essential.
Not only is this true for historians (Zweig 1993; Morris 1993; Schürer 1993), but in organisations that have record retention strategies and think information has corporate value this is true for current information (Simpson 1993: 34).
Electronic mail makes the importance of context all the clearer.
If we keep only the email message and lose the medium we give up much valuable information.
A claim supported by the ruling in Armstrong vs Bush.
Who read the document and what was the effect of its having been read?
Historians will have difficulty measuring the significance of particular kinds of electronic communication if the meaning of the information and its context is lost (cf.
Higgs 1993; Zweig 1993; Samuels 1993).
It is essential to assure that information about its meaning, form, format, medium, mode, content, context, usage, and even how it was originally distributed in society is retained.
Software contains significant information crucial to preserve a picture of usage.
Besides the data/information it is necessary to retain the original software; a problem which compounds the complexity of documentation as both information and software must be documented.
The focus seems to be on the preservation of data and information, but as Satarov makes clear in his discussion of INDEM knowledge is also important.
‘Data, like facts, do not speak for themselves: they must be spoken for’(Satarov 1993: 73).
For Satarov there is a need to pass on the ‘knowledge about or based on the data’(ibid.).
There can be little doubt that in this regard he is correct.
1.6.
Information on Sources
Historians give different values to surviving documents and in the future electronic records will be treated in a similar way.
The worth of a set of documents or electronic information resource will depend upon numerous factors from prevailing scholarly attitudes to ease of use.
One of the problems facing historians is gaining information about the sources that exist in electronic form (Lievesley 1993: 208–210).
This is even true for government records as Moissenko explains (1993).
In the Russian case there is a lack of information about what electronic information was created during the Soviet period.
No structure is in place for gathering the data, no co-operation currently exists between government officials, no information exists to assist with access, and there are no statutory bodies or even adequate laws to protect machine-readable records from loss.
For the history of the Soviet people during the last twenty or so years machine-readable data files are crucial and Moissenko argues contemporary historians need to come to grips with them now.
It would be foolhardy to believe that the Soviet case is unique.
All over Eastern Europe similar cases could be found, and a similar state of affairs probably holds sway elsewhere.
The Netherlands, Austria, and the United Kingdom all offer parallels involving the loss of vast amounts of machine-readable data.
Registers of resources should be created.
As electronic records represent a new resource their location and the descriptions of the data they contain cannot easily be made accessible.
So far the great bulk of archives set up to handle this material focus on collecting datasets created by research projects (Doorn 1993; Lievesley 1993; Marker 1993; Nemeth 1993).
When electronic records are deposited in archives there are numerous difficulties associated with their cataloguing and documentation.
For all but the smallest dataset these tasks take on average weeks rather than days (Doorn 1993).
Often the task of documentation fails to preserve remarkably important information, such as contextual information, or details of purpose.
To understand adequately a system the user requirements, functional specifications as well as user documentation must be retained for life of the data.
To this we need to add the details of the behaviour of the organisation that created the resource.
Some research archives such as the ESRC Data Archive have begun to approach the problem by shifting the responsibility for documenting the datasets to the depositor (Lievesley 1993).
While this approach to the creation of documentation is feasible for research data it may not prove as efficient for records created in the electronic office, business, or government department.
1.7.
Selection, Retention, Destruction, and Preservation
The survival of the historical record has always been subject to natural and man-made forces.
Frequently in the past accident and some occasional planning (i.e. legal or administrative reasons) has preserved historical records.
Electronic resources need to benefit from well-planned preservation campaigns, if they are to survive at all.
Contemporary archivists and historians may not be the best prepared to select which records should be stored for posterity.
In passing records of our culture to future historians we may pass an indication of our cultural myopia as certain types of records are selected for preservation at the expense of others.
This is not to suggest that historians should not be involved in the process.
Indeed Higgs (1993) builds a very persuasive case for the need to involve historians in the record creation process from the earliest stage or for historians to involve themselves.
Simpson (1993), Greenstein (1993), Lievesley (1993) and others all stress the same.
In addition to selection criteria used by the NARA (e.g. administrative, legal, and information value as well as long-term research potential) other important criteria, which might be helpful if used when selecting records include: technical issues such as quality of dataset/resource documentation and the uniqueness of the operating system, software, or hardware environment needed to access/use the information.
Preservation is associated with costs.
Among the financial implications are the costs associated with overcoming the problems of hardware and software obsolescence, media degradation, support, and documentation.
The economic costs of maintaining electronic information will not diminish with time.
Electronic records or resources cannot be left to languish unattended, they require continual attention.
Where there is economic advantage in reusing information there will be an easy business case for the preservation of records.
Visa records do not yet appear to have long-term reusable value, whereas 20th Century-Fox's Movietone film has taken on such value.
Preservation of business data is likely to be limited because little of this information has a ‘value-life’ of more than five years.
As Emmerson explained at the Seminar, banks typically only save information where legal or business requirements make the information necessary or useful.
This is not a trend new to electronically-based information, many businesses in the past disposed of paper records after they had served their useful life.
Often it is only when their disposal did not go as planned that their existence or the fact that they once existed comes to the historian's attention.
Current European efforts to preserve electronic information are paltry as most of the existing data archives concentrate on the retention of datasets produced by researchers.
It would (will) take legions of staff to support the growing number of information resources in electronic form.
Whether government departments retain information that only exists in electronic form depends upon whether electronic documents are classed as ‘official documents’.
The Netherlands and Sweden treat electronic information in different ways.
In the former country only when electronic documents are printed are they classed as official documents (Doorn 1993), whereas in the latter country electronic records are considered equivalent to paper records and retained in an accessible form (Gränström 1993).
Record retention will lead to a skewed vision of the contemporary world.
Besides governments, it is likely that only the largest companies will engage in any kind of record retention and archive management.
For most small firms preservation of records beyond periods stipulated by statute will be impossible, but some might argue that this will not really matter because if sufficient quantities of government records survive in electronic form there will be enough data for historians to build a picture of the economic and social history of small business in general.
1.8.
Legal Issues
Legal issues abound.
Walden reviews the statutory regulations that govern recordkeeping within the UK and for some issues in comparison to Europe and the US (1993).
There are three main reasons for retaining records: organisational needs, fulfilment of statutory requirements, and for the provision of evidence in case of dispute.
Electronic records remain a problem as far as evidential law is concerned because there are questions of admissibility[3] and the value or weight the court will give to such records.
Copyright is another problem area.
In the United Kingdom, where the concept of ‘sweat of the brow’ exists it is possible to have copyright in mechanically generated data resources (e.g. phone books and even EPOS data).
By contrast the same is not true in America and many other European Countries.
In the United States, in the case of Feist Publications Inc vs Rural Telephone Service Co the court ruled copyright required novelty and originality (Oakley 1993: 93).
No such novelty or originality existed in the phone book, which was an A-Z list only.
The problem of copyright exerts peculiar anomalies.
Electronic point of sale data, or credit/charge information, or banking data might be protected currently in the UK by copyright, but would probably not be elsewhere in Europe.
Copyright of electronic information influences its conditions of storage, the terms governing its use, its redistribution, and even its preservation.
Adoption of EC Commission proposals would only grant copyright protection to those databases which involved intellectual and creative effort in their compilation.
Mechanical or ‘sweat of the brow’ databases would cease to qualify for copyright protection.
They would be protected by a new right known as the Unfair Extraction Right, but it would only protect the data for ten years.
Problems also exist with the rules of copyright deposit.
In the UK there is no requirement for the legal deposit of electronic materials in the copyright libraries.
Libraries might be ill-equipped to handle the deposit of records in electronic form, but its deposit should be encouraged.
The problems become acute from the vantage of electronic geographic information as was made quite clear to participants in the seminar on The Future of Our Landscape at the Royal Society (London) in October 1992.
The number of information sources that exist only in electronic form continues to multiply.
Without the requirement of legal deposit for electronic information libraries may lose touch with the information heritage.
We have only examined the legal issues from a narrow perspective (Walden 1993; Gränström 1993), they could be the focus of a single seminar alone (see Postscript, 317).
1.9.
A Variety of Issues
The variety of record categories (e.g environmental records, EPOS data, health records) is quite large.
The range of record types (e.g. e-mail, text files, databases, still images, moving images, and audio) expands the dimensions of the problem as they increase the spectrum of material archivists and librarians must preserve.
Documentation is essential for those electronic resources that need to preserve the integral relationships within the discrete data units to provide the information resource.
This difficulty is made worse by the variety of file formats in which the information is stored and encoding strategies which are used to mark-up texts.
In traditional parchment/paper document creation standards of information provision and format arose naturally over time.
Standards of encoding information are crucial and Burnard's discussion of the Text Encoding Initiative (TEI) is an example of a standard (1993; Greenstein 1993; Marker 1993).
Standards are essential in other areas, such as audio, images (e.g. Photo-CD vs GIF vs TIFF), video, and numerical data, but many of these areas are still in flux (see Lesk 1992: 7–9; Robinson 1993).
Another area in need of standardisation is the description standards for archival data exchange, and some of these problems are summarised by Cook (1992).
Currently hypermedia and multimedia standards pose seemingly insurmountable difficulties, and this only intensifies the already complex problems involved in archiving of multimedia information.
Even more seemingly intractable problems will be posed by attempts to store virtual reality.
Here the maintenance of the peripheral devices used to access and present the information will be essential.
In the case of major company resources it is fair to say that the very large companies invest millions in their computer systems and they expect these systems to have a life span of at least a decade.
Companies have also come to appreciate the ‘value’ of information, and are likely to maintain datasets for their ‘value-life’.
Consequently they tend to opt for conservative technologies.
The greatest threat at the moment to the preservation of data resources is lack of a business case and the proliferation of high-powered stand-alone or ‘negatively networked’ computers in organisations.
Storage of data offers opportunities.
In the traditional archive the storage of documents and their use usually took place at one location, but this need not be the pattern of organisation maintained for electronic archives.
The storage could be at a centralised location or decentralised — distributed across a number of sites.
For instance, the users and data might be at different sites, support in their use and access could be provided from a additional site, and the extracted data processed at a further one.
From the point of view of historians this can only mean that training must be broadened and will need to include a heavily element of information technology (Greenstein 1993; Schürer 1993).
Just as, say, medievalists must have a good working knowledge of Latin and be skilled palaeographers future historians will need a range of IT skills.
Not all the information in electronic resources will be correct (Knight 1993: 271).
Contemporary concerns with the dangers posed to the rights of individuals by incorrect information are reflected in the laws and institutions set up on national levels to protect individuals against incorrect data and the implications that might arise from any errors.
Inaccuracies in the data abound and these will need to be considered by future historians.
The distinction between privacy and confidentiality and the implications of both these concepts to electronic records must be considered.
How long should records be kept confidential?
Will we object in 75 years if our ancestor's medical records become publicly accessible?
Should ‘police records’ enter the public domain?
There are also issues of legal sensitivity?
1.10.
Media, Software, and Hardware
Paper, and parchment before it, have a long shelf life although acid paper used in abundance after the 1830s did contain the seeds of its own destruction.
Magnetic and optical media appear to have a shorter life.
In the case of magnetic media the signal degrades, and the access mechanisms become obsolete as new standards of storage and access evolve.
Access to information on paper tapes, punch cards, and magnetic tapes all require the continuing availability of suitable devices and for some media this is already no longer possible.
Improperly stored magnetic media degrades, tapes stretch, signals are destroyed by contact with magnetic fields, and even optical media can be damaged through poor storage or breakdown of the dye-layer on the disk.
Each new kind of medium is likely to have its own built in obsolescence whether deficiencies these are inherent or merely the result of replacement by newer and more efficient technologies.
With software there are two issues to consider: the preservation of software to access data and the preservation of software for its own sake.
Is software, asks Swade, material culture (1993)?
The interest of archivists, historians, and records managers is in its preservation as a means to access the information content of data.
Software, is though, a cultural artifact of the modern world.
How it changes tells historians as much about the technological developments as it does about the interface between human and machine.
Current efforts to empower the end-user are typical of these — the qualities of current software documents how the transition to end-user computing is being made possible.
The growth in end-user computing will transform the way information is created and preserved.
Distribution of the creation and storage of information, whether text, data, or some other form (e.g. images, audio) complicates the process of preservation.
Software poses museological difficulties.
One major issue raised by Swade's paper is the problem of the software ‘archive’.
Huge cost factors would be involved in developing, organising, and running such an archive, and major managerial and curatorial problems would need to be overcome.
A change in copyright law which required the deposit of software in copyright libraries might be helpful here, but it would create a major burden for librarians.
In Swade's view, if software is material culture then museums could offer it a better home because museum curators are equipped to deal with software as object.
Technological developments have been at a fast pace since the 1950s.
Each hardware advance has dramatically improved processing, storage, and display capabilities of equipment, but new systems continue to leave behind earlier ones.
The belief that it would be possible to maintain old computer hardware in operational order for use by future historians is utopian.
The sheer cost of trained staff and the difficulties with obtaining spare parts are two indications of the absurdity of the idea.
Efforts to preserve a Besm6, which was the last indigenous supercomputer made by the Russians by the British Computer Conservation Society, indicate some of the problems.
In Russia the machine's value was that of its precious metal content (Hobby, 1993: 48).
Here is a machine which has historical value for its very design, but in working order it might provide facilities to access data resources and insights into Soviet data processing not otherwise accessible.
One possible way to provide future access will involve the simulation of older generations of hardware on newer equipment using emulation software (Swade 1993).
Solutions to the software, media, and hardware problems need to be found.
When considering the issues we must be mindful of Morelli's question: ‘What is the period of historical relevance?’
If the period of historic relevance is measured in hundreds of years, then even periodic recopying and migration to new generations of hardware will not effectively provide for the preservation of information long enough to meet historians’ long-term needs.
1.11.
Libraries and Archives
Rayward has argued that the creation of electronic resources is blurring the distinction between museums, libraries, and archives (1993).
At first this premise may seem difficult to accept, but as he maps the argument it becomes more and more plausible.
The issue here is information.
Charles Dollar has pointed out that archivists and librarians both preserve and provide access to information (1992).
In the past they each viewed the task and the information differently, but with information technology this is changing.
One of the fundamental differences between the two disciplines is in the area of record classification.
‘Unlike libraries, where knowledge was organised by a subject classification scheme, knowledge (records) in the National Archives is organised by the principle of provenance: the organisation of records reflects the structure of the agencies that created the records’(Dollar 1992).
Besides the conceptual difference there is a practical difference as well.
As how researchers access information evolves it is likely that where the ‘object’ is electronic information the distinctions between the three providers will decrease.
There are shortcomings to this notion.
Archives and museums keep objects that hold information and they store information about the entities (i.e. objects) they hold, but whether they can be said to hold knowledge in the same way libraries do is debatable.
Once again the issue of the relationship between data, records, information, and knowledge arises.
The library of the future will be very different from current examples.
The new Bibliothèque de France might provide just such a model.
When the library opens in 1995/6 there will be 100,000 volumes accessible digitally, but there are also plans to provide access by the year 2000 to a wide range of multimedia resources from sound recordings, to moving pictures, to radio programmes, to television programmes, to still images (Fresko 1993).
The model of the Bibliothèque de France still involves a building at which material is accessible, but the developments of networks mean that in the future this will no longer be necessary.
At the July 1993 conference held by the Library of Congress on Delivering Electronic Information in a Knowledge-Based Democracy , participants conceded that the proliferation of an ‘advanced information infrastructure’ and the growing quantity of ‘dynamic knowledge’would alter the traditional nature of the library.
The Library as a place will be transformed.
The information will have a richer and broader nature, it will be distributed, and the libraries may not ‘own it’, but they will have to support access to it from places other than the ‘library building’, and help to maintain and preserve it.
This is a major issue for an information provider, such as a library, because it draws attention to the obligation to supply support for users of a new and different kind, and to develop ways to reduce the dependence of historians of the future upon the support of more and more highly skilled staff.
A number of papers draw attention to the support questions (Greenstein 1993; Higgs 1993; Zweig 1993).
Archivists and librarians will have to find new ways to provide ‘readers’ with specialised support in the use of access mechanisms (software or hardware) as well as‘meta-data’about the sources.
In the long-run the provision of this support will need to be done electronically and suitable tools need to be developed to allow simultaneous interactive online support of users.
There will be a need for increased access to IT, better and more diverse ‘finding aids’, new kinds of staff, and virtual libraries and archives.
One view expressed by Zweig stresses the loss of independence by historians as their access to information becomes less immediate and their dependence upon archivists and librarians as mediators between the researcher and the source increases (1993).
It is worth remembering that many contemporary historians using conventional paper archives are already very much dependent upon archivists for access to material, and for guidance in identifying and selecting sources.
The implications of mediation between the user and the source is a question that needs to be addressed especially if it increases in the electronic archive.
1.12.
Conclusions and Commencement
A certain narrow-mindedness has pervaded studies of electronic information as the focus has been predominately by national archives on the preservation of records about the national governments themselves.
More attention needs to be focused on other records or information resources that document our culture and on a range of other institutions that produce them.
Especially important are records that will allow us to give life to the many stories history can tell.
I have drawn attention to the immense quantity, the rich variety, the problems posed by its retention, the difficulties with its analysis, and the opportunities that might be offered to future historians by contemporary electronic information resources.
Writing the history of the electronic-information rich countries at the end of the millennium could be done with dynamic resources that would offer comprehensive profiles of political, economic, social, and cultural worlds.
This information might be examined using an array of software tools.
Alternatively, we may have fewer and poorer sources as the vast amounts of electronic information are not accessible because hardware has become obsolete, software and data have not been migrated forward, and the media have degraded.
In the succeeding chapters the authors examine the questions surrounding electronic resources from the vantage of the creators, the technological specialists, the archivists and librarians, and the users — historians.
It is fair to say that during the day and a half of discussions in June 1993 alarm was expressed at the difficulties posed by the future of the historian's resource.
At the same time everyone saw opportunities.
Some themes recur in several papers and this is indicative of their urgency.
It is necessary to give further consideration to the crucial questions about the major differences between paper and electronic storage.
Electronic records may only be proto-records because they require numerous mechanisms to make them accessible.
The immense size of the problem is obvious and the cost of maintaining information for the future is large, but it seems likely that the benefits will outweigh the costs.
Historians should not be tricked into believing that the survival of vast quantities of data will alone provide the fertile soil for the writing of history.
The quality of the data (whether texts, multimedia, databases, or audio), the training of historians, and the tools to investigate the data will each continue to influence the products of research.
What is clear is that a significant cultural artifact has become soft and ephemeral.
The developing of links with record creators will be important if they are to be encouraged to exercise their moral responsibility to preserve these contemporary cultural artifacts, e-facts.
Too great a focus has been placed on text and data.
In recent decades historians have broadened the sources on which they rely as they have widened their views of the past — oral history, archaeology, art, and in the future data and other electronically-stored information will be included.
Current records managers and archivists must not allow the limits of their imagination to constrain the kinds of information they preserve because historians may wish to tap a wide range of possible sources to build up different views of the past.
Just because information has been summarised about a data source may not justify the failure to retain the base data.
Future historians would in all likelihood use the same data to help answer entirely different questions were it accessible in its original form.
Researchers must be mindful of what information meant to its creator and how it was used in the past.
The meaning and value of information contained in records will differ depending upon the user and the context of its use.
Are we about to embark on an information explosion that historians could never handle, or are we about to discover resources to make a historian's vision of our world fuller than the vision historians have of any other period?
New legislation is needed that is mindful of the value of electronic information resources to future researchers.
Historians need to be involved in the development of these regulations because they understand documents, data, and information.
Do we want to pass on our cultural heritage by accident or by design?
Do we want legal and financial concerns to take precedence in the selection of resources for preservation?
Clearly the shortcomings of electronic storage, software, and hardware require active intervention if a rich historical record of the late twentieth century is to survive.
We need to preserve the past for the future in as great a variety as possible.
The broader the array of surviving sources the more likely future historians will be in a position to see the late twentieth century as the rich and diverse world that it really was.
Part I.
The Vantage of the Creators
Introduction
The Flood and the Hunt: Data Creation, Storage and Retrieval in the Electronic Age
It is a commonplace that computers have the potential to revolutionise the way we communicate; perhaps less commonplace, because less obvious, that because our starting-point is our existing methods and practices, we can be very slow to adapt to the computer's potential and find radically new ways of working.
Much of computer-based activity, perhaps the bulk of it, consists of making the powerful new technology behave as closely as possible to the way the world used to be before the computer age.
This is true right down to the detail of how we use electronic mail, for example, in which, until we discover or are shown otherwise, we start from the assumption that this is a cheaper and more rapid method of writing the sort of letters we always wrote, and that those letters should have the same status, and be stored and referred to in the same way, as the written or typed (and duplicated) paper missive.
When we discover that e-mail does not necessarily work like that, we are temporarily at a loss as to what to do about it.
There are massive gains, of course, but also problems.
The concept of publication, a creation of the age of printing, begins to evaporate as printing yields its monopoly in the world of communication; so, even more disturbingly, does a more fundamental concept which long antedates printing, that of the stable document.
The opening session of this workshop demonstrated very clearly that these changes affect the creators, the storers and the pursuers of data in similar ways.
The volume of data being created is continuing to explode at a rate seemingly far in excess of what a diminishing workforce ought to be able to produce.
This reflects a number of trends; our society's lemming-like dash to document, quality-assure and performance-indicate everything that moves, the release from inhibitions (and writing standards) and the productivity gains afforded to document creators by the word-processor, and both the power and ephemerality of electronic mail, which encourage proliferation unencumbered by considerations of cost or accountability.
(It would be difficult to find a sharper contrast than that between the e-mail junkie and the medieval scribe perched over his hand-prepared sheet of parchment.)
The fluidity of much of the resulting ‘data’ is a real problem for archivists, as is the challenge of finding more powerful ways of categorising, labelling and storing it in structured ways.
Of course the computer also offers archivists and historians more powerful tools than were previously available for searching documents and retrieving information.
However, the fluidity of our use of language places limitations on what can be done with these searching tools.
The selection of documents to be saved for posterity is as essential as it has been for the last 30 years; it is not so much that computers cannot store everything (storage capacities are continuing to develop exponentially), as the consideration that to conserve everything makes the past unmanageable and impenetrable.
For all but medieval historians and historians of laundry lists, this has to be accepted as a fact of life.
At this point, however, the similarity of the problems facing data creators and historians breaks down.
Listening to the debate which followed the papers in this session, one was struck by the persistence of an older and more fundamental truth.
Data creation and data capture are at odds with each other; the protagonists and the interpreters of history are still at cross purposes.
The rationale behind data creation is immensely broad, but might perhaps be categorised as as 1) the need for a smooth flow of information to make corporate activity possible, 2) the need for proof of activity or entitlement, 3) the presentation of the ‘public face’ of that activity.
When it comes to data storage, the second and third of these categories predominate, indeed take over from the first (to the extent that it becomes important to the data creators that records which challenge or compromise them are destroyed).
By contrast, historians want to unpick the public face that is presented.
They want to read between the lines, to worm their way into the subtext and assumptions behind the presented image, and to discover the practices that are accidentally or deliberately hidden to view.
This applies to both ‘social science history’ and ‘hermeneutic history’, in Edward Higgs’distinction (if indeed that is a real distinction), inasmuch as‘data’in the sense of meaningful statistical extrapolations depend on ‘sources’of the kind that ‘hermeneutic’historians work with.
It was ever thus.
All that has changed is the nature and volume of data, and the way it is created and accessed.
That historians are increasingly being involved in the process at an earlier stage — being consulted over which documents to destroy and which to preserve, or being involved in discussions with the creators and storers of data as in this workshop — is good news, and helps the two sides to better mutual understanding.
It would be illusory, and dangerous, though, to imagine that there could or should be real partnership.
That is not the function of the historian.
2
The Management of Electronic Information
Helen Simpson, Information Systems Customer Support Manager, British Telecom
Abstract:
Large multinational corporations produce and acquire vast volumes of information in the course of their business.
Information Management strategies are being developed in many large organisations to manage this asset effectively, enabling the organisation to achieve crucial business objectives, in addition to controlling costs.
Electronic systems have long been used for data, but as increasing volumes of information and records are also created and stored electronically issues are raised about the management of this information.
All information is shaped by its creating organisation, so how does the culture, organisation and pace of change within the company affect the management of information, particularly its availability for use by third parties in the longer term?
‘Electronic Document Management is now a practical proposition’(Butler Cox Foundation 1989: 1).
‘Whilst appropriate tools, techniques and procedures are in place to manage traditional paper — based records, there is no equivalent for electronic information generated in office systems.
As a result the corporate memories of organisations are threatened.’
(McDonald 1991).
Large multinational corporations produce and acquire vast volumes of information in the course of their business.
Information Management strategies are being developed in many large organisations to manage this asset effectively, enabling the organisation to achieve crucial business objectives, in addition to controlling costs.
Electronic systems have long been used for data, but as increasing volumes of information and records are also created and stored electronically issues are raised about the management of this information.
All information is shaped by its creating organisation, so how does the culture, organisation and pace of change within the company affect the management of information, particularly its availability for use by third parties in the longer term?
Complex sizeable commercial organisations have certain characteristics and trends in common.
These have implications for their approach to Information Management and more particularly to the management of electronic records.
The sheer size of many large corporations creates an enormous diversity and volume of information.
‘One organisation of 2,000 staff dealt with 45 tonnes of incoming mail last year and 48 tonnes of outgoing, equivalent to generating 25 kilos of paper per person every year.’
(Touche Ross Management Consultants 1992: 12).
The range of information needs is equally varied.
The spectrum of activities make it impossible to understand all the business processes and priorities.
Many large organisations have a distinctive culture, a ‘way we do things round here’ which is informal, but powerful.
This culture may include its own language, ways of communicating, and determines the way in which information is perceived and valued.
The transnational dimension may contribute to the feeling that the corporate culture is a complete world in itself.
The need to meet information needs across national boundaries raises telecommunications and linguistic issues.
The wider ‘family’ may also provide expertise in a new methodology or technology, or examples for a benefit case.
In the last decade many large organisations have highlighted the ability to respond to a changing business environment rapidly as a factor critical to their survival.
The ability to respond constructively to frequent re-organisations and a moving requirements target has become part of life.
One tangible response to the need for improved responsiveness has been a change in approach to how organisational structures facilitate the company's mission.
Traditionally, many organisations were structured as hierarchical pyramids, with several layers of junior and middle management.
One function of these posts was to sift, summarise and present information to the layers of management above them.
These posts are often intensive users of information services of all types —‘knowledge workers’.
They are often the means by which an organisation synthesises externally generated information with its internal information sources.
This model can be represented as in Figure 2.1.
Recent developments in organisational theory, currently in vogue with many large corporations, suggest that hierarchical organisations are slow to respond to change, can limit the optimisation of staff potential and have high fixed costs.
This has led to the current move towards networked organisations.
Networked organisations operate as a fluid inter-relationship of different teams.
An individual many be part of several different teams, and teams are constituted and disbanded as the needs of the business require.
Teams may be outside or cross the conventional boundaries of the business and encompass business partners, suppliers or customers.
Teams and individuals are encouraged to contribute effectively by empowerment.
This pattern could be represented as in Figure 2.2.
This has a dramatic effect on the information management strategy of the organisation.
The lack of continuity in team structure changes conventional patterns of information provision and can create problems in the custodial care of the resulting information early in the life cycle.
The effective management of the information at the time of creation is vital to ensure both that the successive teams do not ‘re-invent the wheel’ and that the corporate memory does not suffer.
In reality, some business processes do not operate best as a networked ‘blob’, so many organisations actually operate to a hybrid model somewhere between Figures 2.1 and 2.2.
This in itself raises issues of how the information infrastructure can best serve two models at once.
This mixed model can be represented as shown in Figure 2.3.
A parallel trend is a concern for quality issues.
Total Quality Management (TQM) and BS5750 initiatives are formal expressions of a desire to make quality a priority which may also be expressed in a more informal way.
Information Management may be sold as a ‘quality issue’.
Due to the scale of operations in large companies it is often possible for staff to specialise in a way that is impractical in smaller companies.
Large corporations often have close relationships with major Information Technology Vendors.
With dedicated account teams and long term relationships there may be a confidential sharing of the vendor's longer term strategy and product development plans, complemented by the client corporation having input to product specification and providing feedback from product testing.
Complementary links will exist with external standards bodies.
This snapshot of the corporate environment makes it possible to analyse the way in which such organisations approach the management of information resources.
Information Management is increasingly emerging as a means to enable organisations to manage their information, of all types and in all media, as a corporate asset.
This strategic approach aims to optimise information and technology as valuable resources to achieve the key business objectives of the corporation.
If the long-term use of information resources by individuals beyond the corporation is included as a benefit at all, it is unlikely to be a major priority for funding and may even be viewed with caution.
The justification will concentrate on business deliverables.
Whilst increasing numbers of large organisations are investing in Information Management strategies, many have not.
‘although the term Information Management is becoming more frequently used it's definition is not widely understood as the effective production, storage, retrieval and dissemination of information in any form.
The majority of organisations do not yet appreciate the value of managing information as a resource, nor the benefits of doing so;’(ibid .:
2).
Given the scale of such organisations, where an IM strategy does exist, the development of strategy and the completion of particular projects may be handled by different parts of the organisation, possibly with a co-ordinating or quality assurance role linking them.
The consequence of ongoing organisational change is to create constant upheaval in the structures which create information and the teams and procedures which underpin record handling in all forms.
The life cycle of many records will be a patchwork of different units creating, amending, utilising and determining disposition, possibly without knowledge of what has gone before.
Records in all media are subject to the disruption or destruction of provenance that can result, but perhaps electronic records are particularly vulnerable as a change in organisation may result in automatic change of office systems which may or may not be compatible in differing ways with the system which created the record.
Sudden changes in funding may also result in the withdrawal of systems, the sudden deletion of information by creation date alone to create space, or uncontrolled over-retention as the result of expenditure on new memory capacity.
It is important to understand something of the recent history of Information Technology Departments.
Where large organisations have historically had separate technology and information specialists the organisational evolution and current relationship between these functions may determine the organisation's ability to have a cohesive view on IM strategy.
Within a decade many technology functions have move from large Electronic Data Processing Departments with substantial directly controlled funding to much smaller Information Systems/Services teams who may be distributed across the business, deriving funding from client business functions.
As the traditional technology powerbase became dispersed, some IT departments began to be more conscious of the ‘I’ in ‘IT’.
For example…
‘Today, information technology must be conceived of broadly to encompass the information that businesses create and use as well as the wide spectrum of increasingly convergent and linked technologies that process the information.’
(Porter and Millar 1985: 149).
Current concerns for many IT functions include the serious consideration being given to outsourcing many of their activities to third party organisations.
Information Services functions (including records managers, librarians, information scientists, archivists and others) have, in many organisations, been re-organised in and out of numerous different departments.
There is no apparent consensus about their optimum position within the organisation.
A significant number of information teams are now elements within the Information Technology Departments, seen as a distinct but valued part.
Alternatively they may be subsumed within the department and treated as a poor relation.
Sometimes the assumption is that information equates to ‘filing’, and ‘filing’ is low skill, low priority and about to be replaced by technology.
This may contribute to the organisation missing major opportunities to manage the records element of the information resource strategically.
In more enlightened organisations interdisciplinary teams have enabled records managers to acquire sufficient knowledge of the technology to make significant contributions to both how particular technologies are used, and to the development of the wider IM strategy.
The relationship of the end-user or client within the business to both technology and information professionals has evolved.
External information provision has seen the advent of end-user searching of external hosts.
There is a greater openness to information sharing.
This trend has been under girded by the pattern of technical evolution.
The central functions required to service the large mainframes are no longer perceived to be necessary on the scale they once were with the development of distributed end-user computing and communications, and a generation of users who are not only ‘literate’ but competent technically.
Client areas may be keen to be at the leading edge in the use emerging technologies, often for all the best reasons, but sometimes to keep up with the latest toy.
From an environment that handled data alone, the electronic systems will now be used not only to handle data, but also information and records based applications.
Clients have considerable expectations about the means of delivery and timeliness of information.
Increasingly a common graphical user interface (GUI) will ease the user's access into the complexities of their electronic world.
Information is increasingly distributed in electronic form either via an office system or, more basically, sent out on a disk.
There has been a breaking down of divisions both between internally and externally generated information and the distinct professional disciplines which act as handmaidens to information.
This evolutionary pattern has affected how the ownership of information and technology is perceived.
Information ownership is often a distinctive trait of the corporate culture and varies from ‘its mine and I'll decide if you use it’ to ‘its the corporation's and we can use it’to even ‘its not my problem, I never need information ‘.
Business users may increasingly interpret custody of information as a localised responsibility especially if charging mechanisms mean they pay directly for assistance from the wider organisation.
Flatter organisations and an emphasis on empowerment may encourage this trend.
This range of factors all create changes which result in the constant need to rethink and repackage IM principles in the light of current business plan priorities and technical developments and to express them in a way which makes cultural sense within the organisation.
The cultural variable of costing and benefit management may require careful handling, although experience suggests that approaches to this are usually cyclic.
The ways in which internal charges are weighted can encourage users to feel that storing everything is cheaper and altogether easier than managing it more actively.
A change in the way the culture assigns worth to information may be required to resolve the benefit equation.
‘One of the main elements of successful information management is to overcome the impression that ‘information is free’.
Encouraging staff to appreciate the cost of information goes some way towards helping them to quantify the value of information.’
(Touche Ross Management Consultants 1992: 43).
Given the enormity of the subject of electronic record keeping, it seems appropriate to use one type of system to exemplify the key issues.
Office Systems (also previously known as office automation or office information systems) provide a good vehicle to explore the major questions.
Office systems provide electronic tools to support a range of administrative functions which most desk-based staff will undertake in addition to their substantive business functions.
Such systems are therefore used across the organisation and often act as a form of corporate cultural glue, enabling and contributing to the issues discussed earlier.
Informal and formal, professional and personal usage is likely to be intermingled.
The functions they provide may include a selection of the following: electronic mail, fax, telex, word processing, records storage and retrieval (both individual and shared), diary scheduling (people and facilities), internal reference information, spreadsheets, access to other applications, and the importing and repackaging of data from other applications.
Organisations are often quite casual in their approach to the information created and stored on office systems and this is explained in part by their origins.
Twelve years ago e-mail facilities were often only available to users competent to cope with the basic and rather cryptic tools inherent in operating systems on mainframes or large minicomputers.
Word processors were bulky specialist machines for dedicated producers of text, and operated independently with little or no ability to communicate.
Records were managed in paper form and word processors were tools to create but not subsequently manage records.
With the advent of increasingly powerful PC's and organisation wide communication networks these originally distinct functions began to be related.
Users skills and expectations rose.
The advantages of beating time-zone differences and ‘telephone tag’, easy sharing of information and the avoidance of re-keying,(to name but a few) ensured the office system remained with us.
The life cycle of a record on an office system can vary greatly depending on the hardware, software, implementation and state of mind of the user.
It may be helpful to quickly sketch a ‘best’ and ‘worst’scenario, acknowledging that the truth may often lie in-between.
At best, our record will be created using a pre-structured format which helps the user to ensure that the information needed to optimise the use of the information through its life cycle is captured as part of the creation process.
For example, the classification given will relate this document to its virtual folder and series, its retention grouping, allow reference to the provisional duration of different phases of the life cycle, who has what access with what rights, authorship, organisational ownership, information architecture key, and potential archival status (to name but a selection!).
These will be prompted and predetermined where possible to minimise the effort involved.
It will be created using the word processing tools, but may include extracts from other documents, other databases, graphics or live links to spreadsheet data.
During a period of active mailing and amendment (with the system providing version control and appropriate access and editing controls) the record will be referenced by authorised users utilising both function based and less structured retrieval tools in the communal electronic record system.
This may be provided within the office system itself, or by automatically dropping into an appropriate application tool.
This stage may represent a blurring of the ‘creation’ and ‘current’phases of a hard copy life cycle.
The record will have a defined relationship to the multiple printouts that will undoubtedly be produced, with the primacy managed for retention purposes.
Ephemeral documents will be deleted in an organised process quite early.
For documents with a more substantial information or evidential content, as the usage rates fall our record may be moved offline, but the retrieval and control information about it will remain accessible online in summary form.
Some records may be transferred to other appropriate media at this phase.
The destruction or disposition of the record will follow the retention decisions made at its creation unless a change of purpose either changes or temporarily suspends its enactment.
The reasons for these decisions are appropriately available and the completion of these stages are all documented.
For potentially archival material long-term plans on the maintenance of supporting documentation, storage media and system documentation will be made and reviewed.
A ‘worst’ scenario could include the creation of our document, in individualistic form by a user who chooses to exercise his/her ability to keep the document ‘personal’, only accessible to those to whom it is directly mailed.
There are no defined relationships to other records as everything is handled at document level.
Retention is at the whim of the user,(often under threat from the system manager) who may delete his copy when the pressure on space forces a clear out.
(It is difficult for him…the incoming mail has become inaccessible until he frees up some of his allocated disk capacity).
Recipients control retention of their copies in a similar way.
Even when the last recipient deletes their access, it is possible that the text remains undeleted, but relatively useless.
(Ask Oliver North about that one).
The lack of any classification and very basic retrieval tools make finding the document rather tricky, but you may find a more organised recipient to re-mail a copy which you could cannibalise to form the basis of your next report on a similar topic.
When the systems manger is stressed by complaints about the slowing access times on the system he may send out warning messages and then complete a tape dump of all documents last accessed before a certain date.
After that your document still exists but it will get much harder and more time consuming to track it down.
Longer-term uses (either within or beyond the corporation) are not taken into account, after all its your document, isn't it?
Current trends suggest that many organisations expect to store increasing percentages of their records in improved versions of such systems.
A 1992 survey by Management Consultants Touche Ross revealed that
‘On average, 37% of all organisations already have some form of office system installed, with another 14% planning installation in two years, of which the majority are larger companies.
Almost 45% of large companies currently have office systems installed.
Although public sector bodies have experienced average take-up of this technology, they have the most ambitious plans for the future.’
(ibid .:
17).
This can be represented as shown in Figure 2.4.
Approaches that were fine for controlling informal e-mail with a status comparable to a telephone chat, are often failing to manage substantial volumes of prime electronic records resources adequately.
The professional input that can make an application ‘best’ rather than‘worst’is complex, time-consuming and therefore expensive.
Only where organisations appreciate the consequences for information resources, which they perceive to be valuable, is that investment likely to be made.
Given the reality of the current difficulties with the use of office systems as tools to manage large volumes of records (a task for which, on the whole, they were not originally designed), how can records managers contribute to improving the situation primarily for the creating organisations, and indirectly also for other potential users of the records?
Involvement needs to begin early in the life cycle of the record, preferably before its creation.
The whole life span of a record, including its creation needs to be planned carefully.
In the case of electronic records this is often only possible before the creation of the record itself.
It is difficult or often impossible to ‘retrofit’ records management controls to a system.
This is why it is crucial to have involvement early in the life cycle of the system.
Increasingly records managers are contributing to the system specification.
There are significant differences between vendors offerings, and future product development plans.
When the product selection has been made some options for managing records may have been made impossible by the operating environment or application software.
Subsequent additions and amendments to software may be technically possible, but inadvisable because of subsequent upgrade issues.
Particularly with Office Systems and Groupware applications the information issues may be at least as complex as the technology.
The lead times for evaluating, selecting, testing, implementing, and ‘QAing’ the information elements of the project can not be completed in small amounts of time around other responsibilities.
It is important to commit appropriately trained and resourced professionals to the task and to structure the project in such a way that information specialists are enabled to contribute as equals with technical colleagues, as an integrated part of the team.
Some issues are likely to be relevant to certain vendors products or certain organisational situations, but experience suggests that certain issues occur across a range of technical and organisational contexts.
These are explored briefly.
Many systems operate at ‘document’ level, rather than allowing folder or series level management of records.
In addition to the extra effort involved in maintenance, this has serious implications for the preservation of the contextual or evidential value of the record.
Early records managers managed the message by managing the medium.
It is important to distinguish clearly between the features of the paper based life cycle which were a product of the limitations of the media and can now be joyfully abandoned, and methods which have an ongoing role when re-interpreted into the electronic life cycle.
It is clear that:
‘…the lack of a record per se is not the source of the problem with decentralised electronic technologies in general or e-mail in particular; rather, it is the lack of shared electronic classification, filing, storage and controlled access and retrieval capabilities.’
(Barry 1990: 7).
As the media and the information that it carries are increasingly separate so records managers need to master managing virtual records, learning to optimise the immense flexibility this can provide, whilst minimising the potential chaos.
‘For example, while concepts relating to the forms, design, and control of paper documents may be used to assist in the design and control of input and display screens in a computer system, management of underlying information may require a radically different approach.
In a paper system there is a one to one relationship between a paper form and the information on it.
In an electronic database, information appears to be a single document, form or table, when in fact it may be spread over different files, databases or in a truly distributed application, over several organisations and dispersed locations…
Therefore, some of the tools developed in paper — based records management need substantial rethinking and extensions if they are to be effective in the context of electronic records management.’
(ibid .:
73).
Many office systems assume that traditional data operations procedures will apply to the control of retention.
This may have been fine for early disposable e-mail, but as office systems of all kinds have developed into tools to create, store, retrieve and delete the only copy of more substantial record types, it is necessary to institute more organised retention methods.
This is often seen as overkill by old guard data processing (DP) staff.
Where products do make provision for active retention management (many do not) it is likely to be at document level alone.
It is vital that ways are found for archivists to have some input into provisional appraisal of records which may not yet have been created.
If the mature wisdom of hindsight takes too long coming, the record will no longer exist to preserve.
In the area of vital records, standard operations backup provisions may be perfectly adequate in the short term for the content of electronic records themselves.
Particular attention may be needed in ensuring a good correlation with related records stored on other media.
Information sharing, access, and security are in many ways the same issue viewed from different angles, with the emphasis varying according to how open an organisation is.
Many systems make background assumptions about enabling or preventing access.
It is necessary to ensure that the access arrangements complement what your strategy is trying to achieve.
Many detailed provisions to protect the integrity of the record lie in this area.
The use of diplomatic form (including structure and layout) can be predefined to help users with the creation process (e.g. telex format) and act as a retrieval key.
This could also contribute to retention validation.
The evidential value of the record lies in part in its context both to other records and within the organisation.
For records with a longer term value this information will need to travel with the record through its life cycle.
Information about the creating system and subsequent storage will need to be added for electronic records to provide a complete record of their provenance.
The Corporate Information Management Strategy may provide a common means of defining information and structuring its inter relations.
It is important that this works well for records to ensure retrieval across media and different sources and is not derived from data management methodologies alone.
It can potentially be used as a tool to ensure the effective utilisation of information resources across information types and organisational peculiarities.
Depending on the literacy of the users and the nature of retrieval requirements it may be necessary to provide consistency for users to retrieve information.
This may be a technical interface which remains the same across several systems, or a common approach in indexing and use of terms.
A major challenge in the management of electronic records is to bridge the technical divides in how the information is stored and ‘re integrate’ information to present the user with the ability to search for information according to their own agenda, not the organisation's technical architecture.
This is particularly an issue with emerging technologies which are not yet able to provide seamless integration into the corporation's wider electronic world.
This is particularly significant in the definition and subsequent management of the same record stored on several different media.
The most obvious example is the relationship between the electronic record(s) and its hard copy counterparts, but other, less conspicuous parallels will exist.
‘One problem arising from the lack of an IM strategy is duplication of information in different forms, which is not cost effective.
Only 4% of organisations never keep paper copies of electronic documents and over 20% always keep copies.’
(Touche Ross Management Consultants 1992: 10).
The control of drafts and different versions of a document can be managed electronically.
Relationships between the same document in different media may require some thought.
The advent of live links raises interesting questions about which is the master copy.
A 1992 survey revealed that 39% of large companies either fear or have already experienced litigation due to lost or missing records.
The issues relating to legal admissibility require careful thought about how a major case could be co-ordinated.
This may be a particular area where the records manager's experience of other media and enabling litigation may be particularly valuable.
Metadata required to enable a credible response to a discovery may well provide some of the information required for other secondary long-term users of the records.
Electronic record keeping presents many new challenges, but these often require a reinterpretation of principles and methods rather than an abandonment of professional expertise gained to date.
Whilst these issues are particularly considered in the context of office systems, opportunities exist to improve the quality of electronic record keeping in the following types of developments: groupware, optical imaging systems, executive information systems (EIS), text retrieval systems, electronic data interchange (EDI), compound document management and work flow management, amongst others.
Let us return to the two quotations with which we began, which may initially appear diametrically opposed.
Perhaps it is possible that both are correct.
It is true that electronic document management is here.
There also seems to be a consensus among professionals from widely diverse backgrounds that the corporate memories of organisations are threatened by a lack of tools, techniques and procedures to enable the effective management of electronic records.
‘It seems that the two sides of the information management equation are set to come together: the technology exists, and people want to use it.
What tends to be missing is a more structured approach to information management, along with more convenient tools to facilitate it’(Thom 1993: 77).
Now is the time to meet this challenge.
What holds organisations back from that commitment?
It may be a lack of understanding of the potential benefits of IM, or a lack of the skills to develop these tools, techniques and procedures, or, more worryingly, a lack of concern for the consequences.
3
Historians, Archivists and Electronic Record Keeping in UK Government
Edward Higgs, Public Record Office
Abstract:
This paper will set the archiving of computer-readable records in the legal and cultural context of record keeping in government.
Archiving in the UK central state is something which happens once records are ‘dead’, i.e. are long outside the business cycle.
But if electronic records are to be captured this needs to be written into systems.
How, or should, one attempt to centralise data collection from a dispersed (chaotic?)
IT infrastructure in institutions rife with ‘short-termism’?
The paper will also cover the distinction between the requirements of social science and ‘hermeneutic’ historians.
The Public Record Office (PRO), in which I work, has traditionally been dedicated to the preservation of the paper and parchment records reflecting the administrative and legal activities of the central government of the UK.
It administers the 1958 and 1967 Public Records Acts which oblige government departments to maintain and select public records for permanent preservation and to transfer them to the PRO when they are 30 years old.
This means that historical and archival selection criteria tend to be applied to records when approximately 25 years after their creation.
There is no Freedom of Information or Right to Know legislation in this country.
The archive has thus always seen itself in terms of historical rather than current social science research.
It has also concentrated on recording policy making rather than dealing with the voluminous series of ‘particular instance papers’(pension records, social benefit files, medical disability records, etc) which form the basis of much quantitative social science research.
The Office is currently considering the establishment of a computer-readable data archive (CRDA), and although it has few answers to the problems facing the world archival community in such matters, I hope that it is in the position to ask some interesting questions.
The present paper has, therefore, two main aims: first, to consider some of the ways in which historians use records and how this might (but probably won't) affect the storage of computer documents by their creators, record managers and archivists; and second, to consider some of the legal, institutional and cultural constraints on electronic archiving in central UK government.
I will start by drawing a distinction between what I will call social science history and hermeneutic history.
The former I conceive as the application of social science methodology to historical research.
In its pure form, models are logically derived and then tested on historical data but I also include here most forms of quantitative research which use such data to test predefined hypotheses.
I will leave to one side any consideration of the philosophical or practical relevance of this model of scientific activity.
Hermeneutic history, on the other hand, seeks to interpret texts created in communicative contexts in order to reveal their internal meanings, their relationships to other texts, and the story in real time which they tell.
Scientists isolate variables to test; in hermeneutics the links between texts are all important.
At one extreme we thus have historical econometrics, and at the other historical biography.
For those cognizant of such matters, this is, of course, the contrast in the positions taken in a famous historical debate between Professor Robert Fogel and Sir Geoffrey Elton (Fogel and Elton 1983).
Whilst the former gathers historical data to test hypotheses about the nature of slavery in the ante-bellum South of the USA, the latter reads texts to reveal the chronology and meaning of the career of Thomas Cromwell in the court of Henry VIII.
Historical research often combines elements of both methodologies but it is useful to bear these idealised approaches in mind when considering the ways in which historians might wish archivists to select and preserve different types of electronic records.
The requirements of the social science historian with regard to the archiving of computer-generated data are the same as those of any social scientist.
Since history is the study of the past, and all data (that which is ‘given’) must be created in time, all databases can be seen as historical artefacts.
Perhaps we tend not to see this year's household survey as an historical datum because it is the human condition to flatter ourselves that our own lives are outside time, and therefore outside history.
If data archives hold their data long enough, however, they will eventually come to be seen as historical sources, and in reality there is no intrinsic difference between the 1991 census and that of 1891.
As long as the social scientist has adequate documentation on the data, the entities recorded, the variables covered and their possible values, he or she can proceed with analysis (Marker, Reinke and Schürer 1987).
Perhaps the social science historian needs more contextual information because the passage of time may make it more difficult to make assumptions about what data signify, but this is surely only a question of degree.
The preservation of such data by the PRO presents technical problems of preservation and storage which will necessitate it intervening much sooner after data is created in departments.
It also presents a challenge with respect to access.
Unlike existing electronic data archives, the PRO will have to consider making data available to those outside the social science community.
How, for example, does one allow access to such material by genealogists, many of whom will be computer illiterate, unfamiliar with research strategies and only interested in tiny potions of vast data sets?
The requirements of the hermeneutic historian are rather different.
She does not principally want to know what the data signify in themselves but how texts reflect processes of communication, and how and why they were used to organise purposeful action, make valid claims, or to express feelings.
She is not interested in data as entities to be modeled but in texts as purposeful acts of communication which carry meaning and which are related to other texts.
He or she will be interested therefore in the products of information technology as a means to communication rather than as a store of data.
Whilst the latter is often collected and structured for the purposes of analysis, the former are created in administrative processes where the concept of secondary use is usually absent.
The tracking of patterns of communication in modern paper-based bureaucratic systems tended until recently to be fairly straight forward since the media of communication and storage were the same.
A letter carried an address and signature, which defined the parties to, and direction of, a communicative act, and the matter under discussion.
Correspondence was placed on registered files which were passed around departments as business was transacted on them.
If a text needed to be amended, or comment made on it, this was done on the file itself, either on the relevant paper within the file or on a note attached to it.
The maintenance of such records in departments was traditionally seen as enabling them to see who did what, when, where and why, and what were the effects of these actions.
Records were needed for audit, disciplinary and precedent purposes, and to enable ministers to take responsibility for the work of their departments before parliament.
In practice, of course, few paper systems delivered all these benefits but they were the rationale behind their maintenance.
Administrative work revolved around files which were the place where people did their work.
transmitted it to others, and, at the same time, stored it.
People found their way round such systems by linking issues to file series based on subject, organisational provenance, or function — the Ministry of X's file series about Y, the registered files of the Ministry of X's Y Branch, or the Ministry of X's files on its discharge of Act of Parliament Y. Since paper records were designed to fit into such registry systems, they carried pointers to their position within them; file covers, file numbers, cross references to other files in the same series or related file series, and so on.
Record offices, which classified their holdings according to the principles of archival provenance, maintained this hierarchical arrangement in the manner in which they stored and described their holdings.
Thus, in the PRO one finds the records transferred from a single institution grouped together under a discrete letter code, and the record series, or ‘classes’, within these clusters described in terms of main institution, branch and paper series.
Departments were expected to ensure that paper series were preserved, that suitable selections were made from them, and that lists itemising the contents of administrative series were produced.
Researchers then use the PRO's guides to the administrative structure of its holdings, original departmental indexes and finding aids, and class lists, to find the information which interests them.
Since papers were filed in an order which had meaning within the administrative process, the files and finding aids give researchers a sense of the context within which communicative activity took place.
The records were their own ‘metadata’.
Unfortunately, departmental finding aids, such as docket books, have often not been preserved in central archives.
On the other hand, many modern historians do not take the trouble to understand this administrative structure, or the diplomatic of the documents they use, which limits their ability to move between record series and across departments.
This is, however, a deficiency in modern academic scholarship rather than in the paper-based systems themselves.
Such traditional systems of documentation began to be undermined in the post-war world by the general spread of the telephone in the office environment and the development of copying.
The former removed business from the written sphere altogether, whilst the effects of the latter were more subtle and insidious.
It now became easier to send multiple copies of papers to colleagues rather than sending a file on the circuit of an institution.
Rather than being the means of communication within business, the file became merely the resting place for papers circulated separately.
As the uniqueness of the written document disappeared, so the status of the file was lowered.
Office workers freed from the veneration of the registry system, increasingly came to keep their own unregistered papers.
Whatever the reason for this change, my colleagues in the PRO working in departments detect a wholesale breakdown in registry systems in the 1970s.
The introduction of automated office systems could carry these processes still further, thus undermining many of the assumptions and procedures which we have conventionally applied to archival work (Gavrel 1990: Hedstrom 1991).
The main issues here, I believe, relate to the dissociation of communicative activity from record storage, the provision of administrative context for record users, and the timing and nature of archival intervention.
In what follows I wish to differentiate between those automated office systems which store unique documents for multiple access, and e-mail, in which multiple copies of documents and messages can be disseminated.
The latter is rather more difficult to deal with in an archival setting.
In e-mail systems it is possible to receive a number of related messages from different people on screen, to then go into a statistical package and do some calculations, and on the basis of this come to a decision which one communicates to a different set of people.
But none of the participants needs to save the particular screen assemblage which was the basis of the final decision.
In order to reconstitute purposeful and meaningful administrative acts we not only need to save the different communicative acts but also to be able to reconstitute their inter-relationships in real time.
In automated office systems there may be a unique record but earlier versions of it do not necessarily have to be stored.
Communicative acts are multiplied endlessly but could become fleeting and ephemeral.
In many ways information in such systems resembles oral tradition more closely than the object-based holdings of archives and libraries.
Will historians be able to reconstruct administrative activity if all that is preserved is the document which initiates that activity and the final formal reply?
If intermediate drafts, the electronic versions of penciled comments, notes, erasures, signatures, and so on, are not preserved, will they be able to make sense of bureaucratic processes?
At the same time, the storage of information on computer systems can be an atomistic rather than an hierarchical phenomenon; horizontal rather than vertical.
Rather than being stored in clusters of papers in files, the layers related intellectually in a hierarchical manner, computer systems often save individual papers on the same level as discrete entities.
In hard-copy systems one finds a paper by applying one's knowledge of how hierarchical systems work.
In a computerised system, however, each document could well be stored at the same level across an entire institution, with access via key word searching on various fields, or full text retrieval.
In these circumstances the traditional hierarchical arrangement of archives could become redundant, with archivists able to say little more than, ‘these are the records of Ministry XYZ in the year 2001’.
The historian of the future might be able to reconstitute all the documents mentioning a particular subject in an automated PRO but the manner in which they inter-relate might take some time to unravel.
If the researcher is interested in discussions of government finance in the Treasury, he or she has a potential problem of informational over-kill.
In this manner, the structures within which organisational communication went on, at present reflected in the ‘metadata’ of file structures, could become much more opaque.
Will this be a problem, or merely a challenge to discover new historiographical methods?
This, of course, might merely reflect the actual flattening effect which information technology appears to have on organisational structures anyway.
As informational flows become less structured, so the water-tight internal divisions of institutions break down into a plethora of working parties and task groups with constantly shifting personnel.
The users of such systems might, of course, create their own directories and computerised registry systems but these will create even more difficulties for archivists and historians if they go undocumented.
Looking further ahead, one wonders to what extent even the notion of the records of a discrete administrative entity will still be meaningful.
In paper based systems information was ‘clumpy’, each department holding its own files, and exchanging comparatively little information with other bodies.
But in the age of ‘information highways’, with government bodies sharing common databanks with other departments and commercial organisations outside Whitehall, to what extent will one be able to identify the records of a government body?
Will such organisations own information or merely rent it?
Such issues have important implications for traditional archives where records are organised in discrete institutional blocks, ‘groups’, or ‘fonds’, according to the principles of administrative provenance.
To these issues must be added the allied questions of selection and appraisal.
Traditionally archives such as the PRO have only selected a small proportion of the records produced in departments for permanent preservation.
Historians usually regard this as a scandal and look forward to the day when computerised storage will allow everything to be kept.
This is understandable but reflects, I believe, ignorance of the sheer amount of paperwork, often of a trivial or duplicated nature, which departments contain.
The Department for National Savings, for example, contains enough papers that if they were piled one on top of the other they would reach the height of nine and a half Mount Everests!
It is unlikely that IT managers will wish to migrate such large amounts of dead material across computer systems.
Nor will future researchers thank us for leaving them a vast amount of dross which makes the location and comprehension of useful material more difficult.
There will be those who argue that everything should be kept from automated office systems and that future document search and storage facilities will overcome any attendant difficulties.
But such systems can only be as good as those who use them in an original research context, and I have a genuine concern that an over-abundance of records will lead to confusion, or to the adoption of narrow and blinkered research strategies.
I am already conscious of the manner in which research students tend to plough the same predictable paths through archival sources.
Will records overkill merely accentuate this?
But if selection is to be made, when should it be done, how and on what principles?
We might ask desk officers to make an initial selection of important records as they come off the live system.
Experience with paper records shows, however, that this would be poorly done.
Should we dump everything down onto a long-term storage medium and select at the end of a longer period of time with the benefit of historical hindsight, say after 25 years?
But how do we ensure that ‘long-term’ computer storage media are usable after the elapse of such a period of time, and how would the PRO or departments cope with accessing and appraising such a vast amount of material to make a selection?
An alternative strategy might be for the archivist to get in early in the life of information systems, probably even at the design stage, to imbed selection criteria of some sort into them.
This might take the form of the archiving of certain types of communication between certain types of people, an extension of the retention and disposal schedules beloved of records managers and archivists (Barry 1993: 119).
Archivists would have to broaden their horizons to undertake studies of the flows of information and authority within organisations, and to determine with the departments themselves which are important.
Some may baulk at such a suggestion but it is only making explicit what archivists do implicitly.
If we shift the emphasis of this process from selecting information on certain subjects, to capturing certain types of communicative activity between certain people performing certain functions, the dangers of historical relativism are diminished, although not removed.
Historians, of course, might feel at this point that they are in some sense losing control over what material is selected for future use (Zweig 1992: 181).
But if selection is to take place, it seems reasonable to attempt to do so on the basis of what an organisation saw as important in its own terms at the time, rather than attempting to apply current research criteria to the process.
Should we flatter ourselves that our descendants will find our concerns of the utmost importance?
Historically records have usually been preserved because organisations thought that they were important for their own work, and departing from this principle might well create more problems than it would solve.
But this always assumes that departments will want to store information about the processes of document creation, as opposed to merely the results of such processes.
It is also conceivable that some constraints might have to be placed upon communications within computer networks.
Automated office systems, which place a premium on the unique record, may have to be given preference over the more ephemeral e-mail.
The accessing of such unique documents and the creation of new versions would be logged for future research purposes.
Those sending e-mail messages might have to fill in communication protocols which not only identified themselves and the recipients of their message, but required the inclusion of indexing terms and a decision on archiving.
To aid this process we might introduce structured directories in which documents could be stored according to a common logic for later retrieval or archiving.
These would be the registry systems of the future.
Such suggestions might be seen, of course, as an attempt to shoehorn information technology into old fashioned structures, and information scientists may well generate more appropriate solutions to the archival challenge.
There is no reason why computer systems should not have these features but only if they are designed into them.
But how far can we expect institutions to fulfil the needs of historians in this area?
We might reasonably expect them to want to store non-ephemeral documents for future administrative purposes and for these to be identified according to predefined index terms.
But will they be prepared to stored multiple versions of an evolving document, and indicate who changed what and when?
Will departments want to introduce hierarchical structures into their records storage solely to give tomorrow's historians their bearings?
Can we persuade departments of the need to ensure that creation of such directories is mapped, and that the terms of reference and composition of working groups which created them are adequately documented?
After all, the whole emphasis of the computer revolution of recent decades as been a movement away from central control and direction to dispersed user sovereignty.
If IBM could not stem the rising tide of networked democracy, will the PRO fare any better?
In this sense, ‘people systems’ are more important than technology.
This brings me to the second part of this paper in which I wish to turn to some of the ‘challenges’ a UK government archivist faces in attempting to implement an archival records management programme in government.
What follows is a personal view, rather than official PRO policy, and is intended to stimulate discussion as to what might be the model legal and cultural infrastructure for such activities.
Many of the issues I want to discuss are equally applicable to paper as to electronic information systems, although the need to write archival concerns into the design and use of the latter at an early stage give them greater urgency.
To those unfamiliar with UK public records legislation, especially those from abroad, the series of questions I have posed above might seem rather odd.
In many countries the stated role of the state archive is to record the history of the state, and to give members of the public access to information necessary for them to undertake their responsibilities, duties and rights as citizens.
It is a formal requirement that government departments should seek to preserve records to fulfil these functions, to be directed in that task by the state archive, and to only destroy material with the agreement of the national archivist.
In some cases the scheduling of records in departments under the guidance of the national archives is mandatory.
Even if this not quite what happens in practice, it is a ideal to which citizens and archivists can appeal (Frost 1991–2).
In the UK, however, we are all subjects of the Crown rather than citizens, or ‘customers’ for government services at best, and records belong to the Crown (in practice government departments) rather than to the public.
As already noted, there is no UK Freedom of Information or Right to Know legislation.
The British State has never had a formal foundation in the aftermath of revolution or war, or as the result of an act of will by an enlightened despot.
Legal rights are therefore not enshrined in a written constitution or roman law but are the results of administrative acts by government bodies or courts of law.
In this tradition the role of the PRO is not to direct on behalf of the public good but to guide and co-ordinate the activities of quasi-sovereign government departments.
The PRO has no direct control over record keeping in government, rather it gives advice on selection for archival purposes.
It does not issue directives on how records should be kept or maintained.
Indeed there is no such overview anywhere within Whitehall.
General guidance in computing in government is the responsibility of the Central Computer and Telecommunications Agency (CCTA) but even they can only advise not command.
The PRO is not, therefore, in a position to issue directives about computer standards, or to advocate the use of particular computer systems, as the Canadians do with IMOSA (National Archives of Canada 1991).
Given that computer procurement is also completely devolved upon departments, and even sub-sections of departments, the issue of standards becomes a difficult one.
We are not in the position of North America where IBM has until recently been a de facto standard and a unifying force.
Some of these difficulties might be solved with genuine open systems architecture but not all.
Nor should it be noted does the 1958 Public Record Act actually lay down what sorts of records should be preserved and why.
It merely indicates that it is the duty of ‘every person responsible for public records…to make arrangements for the selection of those records which ought to be permanently preserved and for their safe-keeping’.
But in what subsists the ‘ought’ of this statement?
From the strictly legal point of view, the PRO is not obliged to consider the interests of historians at all.
The 1958 Act could be interpreted simply in terms of the needs of departments themselves for information, although in practice we do attempt to apply historical selection criteria.
If, however, departments do not feel the need for elaborate electronic archiving systems, structured directories, file use and tracking software, and the rest (however foolish this may be), the PRO might be hard pressed to insist upon them.
Many professional archivists and records managers, who see their role as facilitating the work of the organisations which employ them, would probably say that the PRO had no right to do so.
At the same time the size and nature of the British state is changing in such a way as to reduce the force and scope of the Public Records Acts.
The great wave of privatisations of the 1980s removed a large number of bodies, mainly nationalised industries, from the schedule of public record bodies to which the Act applied.
This programme is now moving on to a consideration of the functions of government departments themselves, with proposals to privatise large swathes of government activities, leaving a small, central policy-making core in Whitehall.
Proposals are being discussed, for example, in which taxes will be collected by private companies, and social security benefits distributed directly via bank accounts.
This is truly what, in another context, has been called the withering away of the state as the state!
As state functions are taken over by private companies, even if made up of ex-civil servants, we will find them dropping out of the public records net.
Ministers of the Crown will no longer be responsible for the internal workings of such organisations, accept in so far as they fulfil their contractual obligations.
This may affect social science data users the most since it might be non-policy areas such as data collection and analysis which are hived off first.
Even within existing state bodies, the workings of legal or administrative imperatives are being complicated by commercial forces, which reflect the revolution in the bureaucratic culture of British institutions over the past decade.
Government has sought to encourage departments to act as if they were commercial organisations, or indeed to become profit-making operations.
We no longer administer legislation, we provide products to customers.
Even within state bodies, the development of internal markets, as in the National Health Service, means that their various sections seek to sell services to each other.
Getting computer professionals to co-operate with record managers, departmental records staff and the PRO on imbedding archival requirements in IT systems becomes even more complex.
Issues of who pays, when and why, are becoming central to computer archiving.
This becomes of crucial importance when the PRO seeks to persuade departments to reformat datasets to make them compatible with its own computer-storage systems.
In addition departments are beginning to see the data they hold as of commercial value, and asking why it should be sent to the PRO for people to view gratis when they can keep it and charge for access.
An additional difficulty for the archivist in UK government is the lack of a sense of history.
This may seem bizarre given the manner in which Whitehall and Britain as a whole appear steeped in tradition.
In the 1950s there were certainly many state bodies which could trace their history and internal structure back over generations but much of this has disappeared into the subsequent maelstrom of administrative change.
The fluidity of internal administrative structures, the increased turnover of staff, and the breakup of centralised registries, have all helped to undermine the role of corporate memory.
The recent shift from bureaucratic to contractual imperatives within government has also weakened the importance of precedent.
Government is becoming increasingly task or product orientated, rather than directed towards carrying out functions over long periods of time in line with established routines.
The nature of time in the British Civil Service as changed, cut up into small blocks covered by forward-looking three year corporate plans, rather than looking backwards to a living corporate tradition.
In this fluid, almost post-modern world, the concept of storing records in perpetuity for long-term administrative or research use is almost completely alien, and no more so than in IT units.
This is also reflected in the low esteem in which records and archival work is held in government departments.
When the present archival system was set up in Whitehall in the 1950s, it was envisaged that the person responsible for records work in departments, the departmental records officer (DRO), would be a person of some standing.
In practice registry work and the storage of ‘dead’ records have been given a low profile and few resources.
DROs are not likely to have very great access to the more confidential and high-status activities where many of the most advanced computer systems are being introduced.
The PRO itself is hardly over-resourced, with only 13 inspection and documentation officers to supervise public records work in all government departments, agencies and the National Health Service.
The PRO's attempts to work through DROs in order to gain control over computer records in government may be doomed to failure because of the latters’ lack of clout and expertise.
This is in contrast to the relatively high esteem in which data and information management is currently held, and if we are optimistic we might hope that by associating work on computer systems with the control of paper, we might raise the DROs’ prestige within government.
It should also be noted that many of these issues have not greatly exercised the academic community in this country.
Historians are used to asking questions about the closure of paper records, and some are actively interested in the preservation of historical datasets.
There has been almost no concern expressed, however, about the implications of automated office systems for future research methodologies.
This, of course, may be the proverbial ‘chicken and the egg situation’, in that historians in this country have not had the opportunity to use such electronic text sources, and have therefore failed to address the issues involved.
One might be rather less charitable and argue that the lack of interest reflects a general lack of computer awareness.
In conclusion, what guiding principles might we draw from the UK experience?
Amongst the most salient are perhaps the following:
the need for a clear statement of responsibilities on the part of government departments for the recording of their activities;
a clear mission and powers for the national archives responsible for maintaining the nation's memory;
the integration of archive, records and information management functions in government; and,
a fully informed and supportive user community for the products of all information technology systems.
Without these elements progress can still be made but the chances of record loss and the expenditure of effort will be that much greater.
4.
Town Halls a Gateway to Democracy?
Public Records in a Changing World
Geoff Hammond, Records Manager, The London Borough of Croydon
Abstract:
Within a constantly changing political, social, legal, and geographical context we are creating more records in a fluid electronic environment.
Local authorities are changing from being direct providers of services to enablers.
Increasingly our role is as a gateway to information.
In this developing environment, can we maintain public access and secure the privacy of the individual?
Can we create an environment where the individual — who is also the all-powerful consumer — and organisations and businesses can talk and trade?
What is our role?
What is all this fluid information for?
And how will we use it to the benefit of the community?
4.1.
Introduction
The creation and keeping of records in a local authority takes place in a very specific political, legal and geographical context — with all the social and cultural connotations that come with records-keeping in the public domain.
Public records are a curious and complex animal — perhaps more so in Britain than in many other countries.
Much of the work that we do is open to public scrutiny through the democratic process — we are accountable to our public through their elected representatives — the Councillors.
And we are, all the time, about the essentially problematic business of supervising the administration of public funds.
We are about the business of delivering public services within the democratic process.
In this, life in our Town, County and District Halls is very similar to the life of central government.
And yet there is a high degree of ambivalence about all of this.
On 19 February 1993, Mark Fisher MP opened the debate in the House of Commons on the second reading of his private member's ‘Right to Know Bill’ commenting thus:
‘Sir Humphrey, permanent secretary to the Rt Hon.
Jim Hacker, said in the very first edition of Yes Minister: ‘Open government is a contradiction in terms.
You can have openness.
Or you can have government.
You can't have both'’.
[1]
Mr Fisher hoped that his bill would seek to prove Sir Humphrey wrong.
This debate about public access is quite wide ranging, covering ideas about a freedom of information act (which is where Mr Fisher's private member's bill is heading) and discussions about a written constitution, the rights of the press and the privacy of the individual.
What we are doing in our Town Halls fits into this broader picture.
We are seeking to ensure the public's legal access to what is going on — and to protect the rights of the individual.
This is not just a theoretical issue.
It has quite difficult practical as well as theoretical implications.
Most service departments hold large quantities of detailed personal information in an electronic form.
In giving the public access to information, we must prioritise the interests of the individual (whether resident, client or customer) against the often broader and more long-term interest of the local historian.
In many ways the life and work of our Town Halls is much more accountable to the public than that of central government.
With a general shift in influence away from the Town Hall and back to Whitehall, however, that may change.
4.2.
Changes in Town Hall life
So what is going on at the moment that is affecting the management and organisation of our electronic records in local government?
There is much legislation on the extension of Compulsory Competitive Tendering (CCT).
So far, this has concentrated on the visible areas of Council service delivery — refuse collection, parks, buses and similar front-line services.
Increasingly the CCT process is being brought to bear on the ‘white collar’ parts of the life of a local authority — areas like finance, architects, legal departments and IT services etc.
So, while in the past we have been placing into the care of the private sector the management of predominantly physical activities, we are now privatising parts of the local authority that are direct holders and processors of information.
Part of the Council's records-base is going through the mill of privatisation.
Where does this leave the status of those records?
What questions does this raise about the ownership, maintenance and accessibility of the records?
In all CCT processes, every time a Council awards a contract for either supplies or services, the key factor is the specification and written agreement between the Council and the supplier — whoever they may be.
The status and importance of those contracts is growing.
Their political profile and financial value is increasing.
The contract is the mechanism by which the Town Hall will monitor the administration of public funds through the privatisation process.
With ‘white-collar’ activities coming up for CCT, the care given to the preservation and legal status of those records will become much more important.
One of the logical developments of the CCT process is the change in the internal structure of the Council.
It is an almost unavoidable aspect of these developments that we experience the emergence of an internal market place, with the growth of business units, with some of the services being contracted outside the authority, and a degree of internal competition and a clearer split between the client and contractor.
This is resulting in flatter organisational structures, and more devolved administrative and budgetary responsibility.
This creates much more diversity and flexibility within the organisation.
Large departments are breaking down into a number of smaller units.
Amidst these changes it is increasingly important to maintain clear channels of communication and have carefully defined ideas about the flow of information.
The records of the local authority, and in particular a whole range of core documents, are becoming ever more important as the agents by which we hold together an increasingly diverse and fragmented organisation and culture.
All these changes are aided and influenced by the work and thinking of the Citizens Charter, concerning the development of service standards.
There are also the ubiquitous activities of the Audit Commission especially their growing requirements to establish performance indicators and publish the statistical results of these exercises.
Of late, we have seen the appearance of league tables for schools — soon we will see their counterpart for other council services — which will then be combined to produce composite indicators.
The public will then, theoretically, be able to assess how their Council is performing.
It will be important to pay close attention to the criteria used and to note how the questions are defined and asked.
It will be interesting to see how these new tables are used.
A major influence on the life of local government is the work of the Local Government Commission.
The Commission has been charged with the task of looking at the implementation of unitary authorities at county and district level.
The Commission has been instructed that it does not have to follow a uniform pattern throughout the country and therefore we may see still greater diversity over time as, in one shire, we see the county council survive and in another the district councils.
This will have a great impact on records-keeping activities in these areas.
Traditionally these are places where county record offices and the work of county archivists have been very important as a base from which to promote records management in local authorities.
The work of the Commission is, however, quite long term and we will not see quick decisions — those caught up in the process may find in time that the cost of conversion to unitary authorities is too prohibitive.
4.3.
The enabling authority
One of the drawbacks of the current changes and reviews in local government is that they mitigate against the development of standard patterns and practices.
When you look at, for example, the organisational structure of the thirty-two London Boroughs, you cannot help but conclude that there are as many structures as there are Town Halls to fit around them.
One of the characteristics of many local authorities in England is a strong sense of federalism.
Often a local authority is a loose federation of (departmental) states rather than one strongly unified body.
Added to this, there is within many authorities, a strong tradition of a culture of secrecy.
The move towards white-collar CCT, business units and devolved management could create considerable confusion.
If an organisation like a local authority does not have clear and effective channels of internal communication, then how will it manage to communicate with the communities it serves when, through the white-collar CCT process, it speaks with many more voices than at present?
And will the public realise who is speaking to them?
Over the past ten years local authorities have moved away from the model which led them to act as direct providers of public services.
Instead we have seen the emergence of the enabling authority , where the Town Hall finds ways of allowing the private and voluntary sectors to undertake the work that, for the last few decades, it has itself accomplished.
Arguments about this change continue to rage, and will probably burn on for the foreseeable future.
What we are seeing is the emergence of much more diverse and fluid organisations operating on much smaller local units.
It is almost as if someone has allowed subsidiarity in through the back door of the Town Hall, but not announced its arrival to anyone.
4.4.
The electronic landscape
It is within the shifting political, social, legal and geographical landscape of Town Hall life that we must seek to understand the questions facing the management of our electronic information resources.
The issues facing the management and organisation of Town Hall life in the 1990s, and beyond, parallel the questions raised by the generation of the Council's information base within a fluid computerised electronic world.
Most of the information that we record, process and transact and most of our reporting finds its creation within the comfortable soft world of a computer.
But for most Town Halls, by far the vast majority of that information is still translated into a paper format in order for a decision to be made and the action arising from that decision to be implemented.
We have yet to learn how to communicate and make decisions in an electronic environment.
This is, of course, a reflection of wider cultural issues within our Town Halls.
Just as the organisational chart of Average Borough Town Hall shows a growing number of units and horizontal spidery lines, so too we have seen an enormous growth in the number of soft and hard systems that have found their way into the Town Hall, with or without consultation or approval from the local IT department.
It therefore comes as no great surprise that these systems are not easily linked up to talk to one another.
All too often the data the systems provide is not only technically inaccurate (because it has not been updated in the last day\week\month) it also no longer answers the questions that are being asked.
The pace of change is such that internal information systems (for example for finance, staff and related issues) are often providing data for an organisation that no longer exists.
The systems were not designed for devolved budgets and the rapid proliferation of small self-supporting business units trading in an internal market place.
Creating a technical structure that allows for a pan-authority electronic mail service is usually as problematic as building the trust and confidence that will allow staff to communicate and make decisions using such a system — let alone remember to log in each day.
4.5.
Communicating with our communities
When we look at the electronic information base of a local authority we need to find ways in which Councillors and Officers can communicate with clarity, confidence and trust at four broad levels:
(1) within the organisation itself.
‘Where there is no vision the people perish’— if there are not clear internal channels of communication, and we are not able to talk coherently among ourselves, then we will not be heard and understood outside the Town Hall;
(2) with the residential community.
Much of this communication is still through the post.
Yet many members of our community come and visit us and walk through any one of numerous doors — for almost all our functions have a public face;
(3) with the business community that operates within our municipal boundaries.
We have a responsibility not only to provide businesses with access to information, but also with structures that enable them to talk to each other and to trade with the residential community; and,
(4) with other agencies that operate either in the public, private or voluntary sectors.
Often we share many common objectives and interests with other agencies operating in our town in seeking to communicate with our communities.
While, at a local level, individual units may be running very efficient operations, the overall picture is still one of general confusion.
There is a need to develop a more comprehensive housestyle for the production and presentation of documentation so that the Council speaks to its community with one clear and coherent voice.
We need standards that succeed in crossing departmental boundaries.
Individual service departments usually have quite a clear understanding of their own legal responsibilities, but are often not too sure how their work relates to other parts of the local authority empire.
What do we do with the records of the ubiquitous interdepartmental working groups — which are often discussing important Council-wide issues?
With all these records being generated in diverse electronic environments, who is responsible for ensuring that they are looked after adequately?
4.6.
Who is responsible?
The organisational changes that come with white-collar CCT and its attendant activities create an environment where it is all the more imperative for the traditionally federal style of Town Hall management to look very carefully at how it manages its electronic records.
Who is responsible for the electronic records?
To whom do they belong?
How will the public be able to gain access to the information that by law they have a right to see?
Privatisation will not inevitably lead to changes in public perception.
When Mr, Mrs or Ms Public write to their Ward Councillor to complain about the service, it is the Council that they will see as being responsible, irrespective of the name of the company or voluntary organisation that holds the current service contract.
In order to guarantee the Council's legal responsibilities; to preserve the public's access to information; and to preserve the clear channels of communication and a smooth exchange of information without which it cannot do its business, local authorities need to give very close attention to who is responsible for the electronic information bases of the departments that are going through the process of white-collar CCT.
When looking at how to develop and improve the management of its electronic information resources, a local authority needs to ensure that the same questions are asked about the electronic environment as are asked about the traditional ubiquitous paper record.
But because the opportunities of the electronic records are far richer than the paper records the electronic environment will require answers to even more questions.
In developing a thorough electronic information resource in the current climate of Town Hall life it is just as important to improve the facilities for a central electronic archive as it is to ensure that the paper records are properly looked after.
4.7.
Channels of communication
It is one of the functions of a Town Hall to ensure a two-way flow of information.
The first, looking inwards, is for the benefit of Councillors and Officers and the second, looking outwards, is for the public — both residential and business.
The records of the Council Committees are historically an important point of access to information for all these groups.
Like many local authorities, the minutes of the meetings of Croydon Council and its sub-committees are accessible to Councillors and Officers in electronic from using a text retrieval system.
Despite a large percentage of the reports (the ‘Part A’ papers) being public documents, this system is not available to the public.
A Committee Minute is, however, usually only part of the answer.
Invariably you need to see the supporting Committee Report.
Though the facilities exist for the storage of reports in digital form for access using text retrieval software, the numerous departments throughout the authority have never succeeded in handing in their committee reports in a satisfactory form.
Policing this activity to ensure submission of the reports is an unenviable and thankless task.
The text retrieval system, like many other systems throughout the authority, suffers from the absence of the political, managerial and cultural will to make the system work at its most cost effective.
4.8.
Action for change
To help carry forward this work, there are several areas that need to be addressed:
(1)
the building of better channels of communication, both internal and external;
(2)
the development of a more trusting environment: too many of the potential developments we have made in using electronic information are undermined by the human factor;
(3)
the creation of a flexible electronic archive to service the increasingly diverse roles of the authority, and ensure there is a clearly signposted point from where public enquiries can be answered;
(4)
the making of our electronic systems more accessible to the elected Members and, where appropriate, to the public; and,
(5)
the clarification of the boundaries of the client/contractor split in the white collar CCT process to ensure that information resources are available for those who need them.
4.9.
The way forward: creating gateways
If Town Halls are to be successful as enabling authorities, then they must make it possible for all their communities (both residential and business, their fellow agencies, and also their own Councillors and Officers) to engage in the free trade of information.
Town Halls need to develop electronic gateways through which all sections of the community can pass, and in which they may stand and talk to their neighbours and do business if they so wish.
The direct provision of information, and the clear signposting of ways into information resources will become one of the dominant roles for the Town Hall in the years ahead.
The local authority needs to hold its own electronic information resources in a much tighter harness if that is to be made possible.
With careful management such change is achievable.
With more efficient use of the existing electronic information resources in our Town Halls this is an approach that could bear much fruit.
The choice of which way to go is ours.
5.
‘Electronic Mail — Information Exchange or Information Loss?’
Jean Samuel, Records Manager, Pfizer Central Research
Abstract:
After defining the domain of electronic mail — business communication networks, bulletin boards/listservers, and voice mail — the paper investigates the issues of use.
It covers the advantages and disadvantages to users, records managers and archivists.
These are issues that include the legal status, audit trails, proof of receipt, version control and provenance.
Using an example of e-mail the paper demonstrates the sorts of data that are lost to history.
It concludes by detailing retention issues and suggesting approaches to the issues made possible by this type of media.
5.1.
Introduction
Dr. Eric Almquist of Decision Research (Lexington USA), gave a paper at the Faxon Institute Annual Conference.
The paper was based on the exploratory findings of some 30 interviews he had conducted with radio astronomers and biologists.
The paper and subsequent report were titled ‘Impact of electronic media on Scientific Research, Communication & Collaboration’.
Here are some quotations from the interviews regarding electronic mail:
‘…pervading daily life…’
‘…the telephone is dead.’
‘It has increased the total amount of communications significantly…’
‘…an accessory to face to face discussion…’
‘…addiction…’
‘All this communication is going to be lost to the history of science forever.
There are no letters anymore.’
I will argue, in this paper that, although valuable information is currently being lost to history, it need not and it should not be lost and the time has come to prevent it.
Communication by electronic means is growing at a rapid rate throughout industry and government organisations.
The growth is in terms of speed, medium, integration, numbers, and area.
It has now reached the point where those professions involved in the management of data must take serious action to prevent information exchange becoming information loss.
Four obvious groups interested in the management of these electronic records are, the legal profession, historians, records managers/archivists, and computing professionals.
The first three groups have some experience of working with each other, aware of the various imperatives in the fight to manage records well.
It is in informed co-operation with the last group, computing professionals, that the key to successful management of electronic records lies.
This paper aims to identify the changes that electronic communication and recording are having on the world of records and will focus on e-mail as a leading example.
With that in mind there will be a brief explanation of the various types of e-mail and their different functions.
The paper will then look at the issues that e-mail raises for the creator/receiver, lawyer, historian and records manager/archivist.
Having identified the scope of the ‘problem’ the paper will cover the wider issues which all organisations or companies will need to face regarding the status and role of e-mail records.
Information will be drawn from a recent legal case in the USA.
The paper will then focus on records in the scientific arena, as an example, to illustrate what impact there could be to historical research because of their electronic nature.
5.2.
Electronic mail
Electronic mail (e-mail) is an electronic communications facility.
It can be sub-divided into three forms: a business communications network, bulletin boards/list servers, and voice mail.
5.2.1.
E-mail as a business communications network
In this context, e-mail is used to send electronic mail messages to colleagues within a company/institution.
It is seen as an effective means of business communication where relevant staff have access to a computer network.
It is of particular value where international companies are working in different time zones and the telephone is not always a convenient method of communication.
Like facsimile messages (fax), it has the advantage of being less expensive than the telephone, as an entire message can be composed before transmission, with only the time of transmission attracting an external cost.
Its strength lies in transmitting factual information where a telephone call may disrupt a meeting or colleague's thought processes or where you cannot see if an office door is open or closed.
It is also a great time-saver when the same message has to be relayed to a group of people, for example, the times of meetings.
In some companies reading e-mail has become the first task of the morning.
When an employee comes into the workplace, s/he no longer opens the paper mail first but will turn on the computer and read what messages have been left overnight or since the last viewing.
This paper will address later the way this sort of e-mail is used and the implications that has for legal and records management departments.
5.2.2.
Bulletin boards and list servers
Individuals with a common interest, for example, engineers, post open messages to other authorised members.
This will be either directly, for list servers, or to a central holding area in the case of bulletin boards.
The aim is to disseminate messages as widely as possible within a given community.
Bulletin boards, for example, can be used to advertise conferences or seminars to people who may be interested because they are in the same profession or may contain data packages, open for use by any user.
A list server is more immediate and is used more like internal e-mail.
Open questions will often be addressed to all subscribers — Has anybody out there come across this problem?
Are there any good books on managing electronic records?
List servers may be used to raise issues for a particular discipline or to air thoughts and to stimulate discussion between a whole range of people who will never meet.
The use of bulletin boards and list servers will be evaluated in the section focusing on the scientific arena.
5.2.3.
Voice records
The third type of e-mail is voice records.
Over the past five years the concept of the ‘ansaphone’ has been extended to incorporate the idea of ‘voice mail’.
Such a system allows you to leave and receive messages on a telephone which is part of a computerised network (and which therefore can digitally store and then replay sounds).
The same message may be sent to many people in the same system.
It has the same timesaving benefit as e-mail in that one message can reach any number of receivers simultaneously.
Received messages can be deleted or stored and even structured to link messages from the same person or about the same subject.
Originally this ‘voice mail’ was aimed at short-term retention and as one of the desk-top conveniences of the electronic office.
Recently, however, companies are demanding, and so suppliers are developing, systems for more effective retention of voice messages to make them truly voice records.
This is not as simple as adding a cassette tape and storing the messages — the sort of companies looking for voice record storage are often in a highly regulated environment.
Here, records on all media may be subject to the scrutiny and approval of a regulatory body and have to be produced and retained as part of a validated system where dates and times, source and receipt, version control and audit of changes must be an integral part of the system and that system approved by the regulatory body.
Without all these controls companies will still be making standard phone calls and writing them up afterwards into signed telephone logs and considering them the best records of a transaction.
A voice message on a cassette with no context to support it cannot be complete as a record.
5.2.4.
E-mail
There is definitely some collaborative work to be done here between the legal and records management people to establish what is required before computing personnel can hope to try and address this for us via their own skills or pressure on manufacturers.
On the subject of telephone links as the supporting network for the dissemination of these messages, records managers must also be aware that we are entering (some are already in) an even more demanding environment where the raw data of a communication, without the man-readable ‘envelope’ will be routinely sent down telephone lines to form the record of an activity.
The relationship, standards and monitoring of these data transfers using commercial telephone companies, internationally, must then be of high interest to the records manager.
This need to manage elements of records through the operations of a whole host of third parties warrants a paper of its own and will not be further discussed here.
It is mentioned to indicate that we stand at the start of communication changes and must prepare to play our part imaginatively in the future and not just retrospectively.
E-mail as a business communication network poses some issues for the records manager, user and lawyer and, because implementing any solutions will lie in their hands, for the computing managers.
An investigation undertaken in the past few years by the United Nations found that ‘…in the majority of reporting agencies, computer staff…have chief responsibility for management policies relating to e-mail communications…
Just three agencies reported that records management staff have a major role in setting policies…
’(Barry 1990).
The report suggested that, due to this trend, the resulting policies reflected technology concerns and not information management ones.
Computing staff are likely soon to be asked to broaden their technology policies to support the life cycle of organisational records.
First let us look at the way e-mail would be used in almost any organisation.
Most systems incorporate some useful facilities:
(1)
all incoming and outgoing messages can be automatically filed (e.g. key words or phrases will be recognised and the messages sent to appropriate electronic directories);
(2)
individual mail messages can be marked for urgent action or later action;
(3)
you can consciously file them into a structure, by subject or source;
(4)
you can move them in and out of word processing (WP) packages so that a message received in e-mail can be incorporated into a document which you wish to send on or work on; and,
(5)
you can edit replies so that the original message can be returned to the sender or to some third person with your reply noted against the original content.
Conversely, the links used to send e-mail into word processing (WP) accounts can also be used to mail WP documents.
Some people tend to overuse it and/or use it for concepts too complicated for this medium i.e. where a proper report or formal memo or even a meeting would be more appropriate than an e-mail message.
A problem for ownership identification can occur when messages are forwarded or circulated to others.
If the original heading (showing source, date etc.) is removed from the message or if the body of the text is edited (both of which are easily done on most systems) then the original temper or content of the message can be radically or subtly altered.
So what may appear as the forwarded message of X is, in fact, the forwarded message of X with additions and changes by Y. Likewise, a message from X may be forwarded, without the original heading, by Y to Z.
To Z the message will appear as Y's own thoughts and message.
This raises some interesting records management and legal issues all of which flow around the key question of the status of an e-mail message as a record.
In terms of use, the status of e-mail messages (excluding mailed WP documents), in most companies, is somewhere higher than a telephone call or a hand-written note but lower than a formal memo or report.
This gives e-mail users another major advantage.
For example, where a meeting that started out being fairly informal raised some key issues and a member of the meeting feels that a record of decisions taken should be made, s/he may feel uncomfortable or that it is inappropriate to write a memo or produce formal minutes of those decisions.
An e-mail message back to the participants, however, can record those decisions ‘in writing’ without making a big issue of it.
This can be extremely useful.
In archival terms these may then constitute some interim ‘minutes’ filling in some gaps to a major series.
But, written in an informal format, not as part of the formal series, the credentials of such mail messages would rely heavily on their context and supporting data.
Where such e-mail messages only occasionally form part of a key record of action (like the pseudo minutes mentioned here), then formal training as part of the records management programme and manual could be the answer — training so that the users recognise such messages and transfer them into a more appropriate work area.
Some companies have an automatic deletion of all e-mail messages more than three months old with the proviso that any important messages can be taken out and incorporated into a formal, structured system (for example, a word processing system or individual accounts) if that message needs to be retained or is a key part of another, more important story.
We will look later at whether this sort of action would be sufficient to answer the question of legal status of the e-mail ‘record’ and how this extraction process would need to be very clearly structured and closely managed to allow such routine deletion.
E-mail is automatically date stamped on receipt so the order of responses or interaction between accounts on the network is indisputable.
Ownership (in terms of the receiving account) of the message can also be established automatically, and this may be important where authentication or time stamping is a crucial issue, as in commercial Electronic Data Interchange (EDI), where orders are placed internationally by subscribing companies via an electronic network.
This raises interesting issues of the status of such messages as records for legal purposes.
A recent, long-running discussion on the records management list server centred on the hypothetical situation of an offer to extend a contract of employment communicated by e-mail.
If the promise was not met, could the employee use the mail message as s/he would a postal mail letter to sue for breach of contract or to support other legal action?
Records Managers fell into two groups; the first suggesting that the e-mail message had no legal status since, there was no signature and, due to the lack of control over people allowing their staff access to personal accounts via their own password, there was no proof that the sender was in fact the account holder from which the message was logged.
Indeed, if the situation was reversed and the message concerned notice of termination, could arrival into the receiver's account prove that the individual for whom it was meant had actually read it?
The argument for the other camp, that the message could have record status ran thus:— lack of control over who uses personal accounts to send or read messages should be governed by strict and well enforced organisational procedures.
Very few institutions have such procedures in place regarding e-mail but that is no excuse.
The standards that are developed to support the authenticity of a piece of information as an ‘official record’ require organizational commitment and formal recognition.
Just as organizations develop a set of rules for documents that are delivered through the postal service they will develop rules for documents delivered electronically’(Truller 1993).
Thus, if an organisation chooses not to recognise its e-mail messages as having formal record status and its procedures accurately reflect this, then they would not be records and vice versa.
This is where the debate stood until a recent court case in the States formally addressed the issue.
The case has direct relevance for the role of records managers and archivists because it focused on the role and responsibility of the U.S. Archivist in implementing organisational policy regarding electronic messages.
The case was known as ‘Armstrong v. Bush’ reference 61 USLW 2427 /1993 WL 4208 (DDC).
Scott Armstrong filed a claim in 1989 against Ronald Reagan et al.,
The Executive Office of the President, claiming that they were ignoring their duty to manage records created by electronic systems.
The core question of the case, as far as this paper is concerned, was, whether the Defendants had failed to comply ‘with the statutory requirements and whether the guidelines are reasonable or sufficiently clear as to provide adequate guidance to personnel employed by the Defendants in their maintenance and preservation of federal records?
The other issue was whether the United States Archivist has fulfilled his statutory duties…
’(Judge Richey's introduction to the Appeal Court decision).
The Defendants argued that the information concerned, on their computerised systems did not constitute records until they were printed out.
The systems in question sent e-mail messages and documents inter- and intra-agency and externally.
They contained transmit and receipt logs indicating to whom messages and documents were sent and indicating date and time.
The Defendants had to prove these communications were not records because if they were deemed records then the records management responsibility clearly stated each agency ‘maintain an active, continuing programme for the economical and efficient management of the records of the agency ‘with ‘safeguards against the removal or loss of records he (the agency head) determines to be necessary and required by the Archivist.’
(44 USC s1032 and s3105).
The Archivist, in turn, would be governed by the 1943 statute that ‘records of historical value involving the public, shall be preserved, particularly where such material reflects the function, policies, procedures, operations or other activities of the department or because of their information value.’
(Congress, USA 1984).
The Defendants put forward a two part argument:
(1)
that none of the material on the agencies’ computer systems (as described above) are records; and,
(2)
any record material on the systems had been saved by staff printing them out into paper copy thus rendering the computer held versions only reference copies in a convenient medium.
The Plaintiffs argued
(1)
that some of the material on the computerised systems in question met the statutory definition of records and thus could not be destroyed without the approval of the Archivist.
The definition in the relevant statute which supports their case uses the words ‘regardless of physical form or characteristic.’;
(2)
that the agencies staff were not necessarily being advised to save all computer records on paper and the process was unmanaged and selective; and,
(3)
that it cannot be assumed that a paper print out of an electronic record from these systems is the same as the electronic version, on the grounds that the paper version does not bring with it the context that gives the information provenance and credibility.
Both parties agreed that not all the information on the systems constituted records.
The Court ruled that, even if the agencies staff were instructed to print out what they deemed to be records, the electronic material are ‘quantitatively different than a copy printed out in paper form and, therefore the Defendants’ record keeping system violates (the statute) because it does not save all the information contained in these electronic records.’
The Court had in mind information regarding when and by whom the information was received, the fact that distribution lists are maintained separately from the messages sent and so would not be printed out, unless specified, on the paper copy.
The Defendants counter argument that this infrastructure information did not constitute a record either was not allowed by the Court.
It maintained that the two elements taken together constituted the record and the material must be retained in a way that includes all the pertinent information.
The Court felt that, ‘Such information can be of tremendous historical value in demonstrating what agency personnel were involved in making a particular policy decision and what officials knew and when they knew it.’
The Court also stressed that, without professional guidance, agency staff were inclined to dispose of records recording their mistakes or not to think of information retention at all.
Thus the need for the Archivist to establish selection standards.
The Court quoted the National Archives Records Management Handbook, ‘…only records officers should determine the record or non-record status of files…
Such authority (given to others) weakens the disposition program by indiscriminate use of the non-record label and can result in the loss of valuable records.’
The records management staff could no longer only involve themselves in the review of paper records they should also safeguard electronic records from inappropriate destruction.
The Court stated that the Defendants should err on the side of preservation for two reasons; the difficulties of deciding what was a record and then whether it should be retained; their duty to preserve records for historical purposes.
The overall message from this case is that internal and external e-mail messages in context constitute records.
The context can be in terms of receipt and transmit logs and distribution lists which will be held separately from the actual message.
They need good records management just like other records and it will take the form of written definitions, guidelines, training and education programmes to ensure that the users — the interface to these record — know their responsibility and how to carry it out.
The professional records management role will need to be a facilitating, not a hands-on one.
Some archivists/records managers have shown concern over the loss of correspondence between historically important people; others that e-mail encourages ‘garbage’ or that its tenor discourages sufficient development of ideas between contributors.
This is currently an unresolved debate.
I suggest that records managers/archivists must first identify how e-mail is used and regarded in their organisation.
The paper will focus now on the use of electronic communication in the scientific community — an academic environment where e-mail could well constitute an important source of scholarly correspondence.
As in nearly every discipline the work of science is carried out in two arena: the laboratory where the scientist is working towards the goals of the institute or company and the academic backdrop where novel work, properly presented, can lead to individual and institution/company credit and personal career development.
The two go hand in hand.
Scientists follow the same route as any other discipline, the novel work is done and interesting results gained.
Past a certain point where ownership of the method or idea is registered and institution/company interests safe-guarded, the ideas are tested in the wider scientific world, using seminars, contacts etc.
Finally the ideas will end up as a useful product and some published papers.
What difference does electronic communication make to this process?
5.3.
The laboratory notebook
At the in house research stage all the information would be entirely within the institution/company concerned.
A key record at this stage is the laboratory notebook.
5.3.1.
Laboratory notebooks in paper book form
Bound, hard cover books, laboratory notebooks have been used for some time and have a strong pedigree.
They are completed by hand by the scientist at or near the time of the work being recorded and form a chronological, step by step log of what was planned, what was done and the results gained.
A book is allocated to an individual or a project leader for team use.
The method of entry is strictly defined and supported by written procedures.
As machines were introduced to record activity, these support data were printed out onto paper, were signed, dated and cross referenced by the scientist and became attached to the book's pages.
The advantages of the bound book form is that they are hughly portable and do not need a power source.
Also, if care is taken in the choice of ink used and the quality of paper they have a proven longevity for archival storage.
Disadvantages include handwriting as an obstruction to understanding, the element of personal ‘ownership’ which discourages the release of the book to some central point at the time of completion and so the sharing of data therein; the primitive linking of support data (anything from staples to sticky tape) with the accompanying disincentive to completeness and tendency to data loss (e.g. through sticky tape perishing); some support data presented even more of a problem, such as photographs and outsize computer print out which led to separate support folders to the actual laboratory notebook.
There are also the difficulties of creating a security copy of bound books and of keeping track of their whereabouts with many discrete users and locations.
5.3.2.
Laboratory notebooks in electronic form
Some obvious disadvantages in comparison to the paper version lie in the technology dependence.
Some hardware can be fairly portable but nothing like as easy as a bound book.
There is also a need for a power supply and if the hardware used is not readily portable it may discourage recording in real time.
As the information recorded has a long-term value it also raises the migration and obsolescence factors.
The advantages are real, however.
The information in the electronic laboratory notebook can be highly integrated including the possibility of other media.
This could mean the inclusion of sound, video, graphics, spreadsheet, databases as well as text.
Thus issue of pasting in photographs etc. would be resolved and so increase the chances of having the complete story.
The information held electronically would be far easier to share.
The sense of personal ownership would be less and the risk of letting go of one's book and reducing one's own access would not exist.
The access of others would not affect the access of the originator at all.
Also, the point of sharing would come much earlier as there is no need to wait for the completion of a ‘book’ before it goes to a central access point.
It is also very easy to build in automatic security copying.
With the right support from application developers and computing staff, the data held in electronic laboratory notebooks would be comprehensive, credible, quality data — created and held on validated computerised systems — with an equally validated migration path and archive system.
Thus with good records management at both the creation and archive ends the scientific population, the legal department, the regulatory authorities AND the historian should have an improved information source.
Long-term, the introduction of electronic laboratory notebooks should be a net gain to history.
Of course scientists confer with their colleagues.
Previously, this would have included chats at coffee breaks, in corridors or writing rough notes for someone's consideration.
This meant that a lot of the interchange would be verbal with no record or a remembered, inaccurate record.
It meant a lot more handwriting — the usual challenge to the reader but with the advantage that different handwriting clearly shows what was original thought and what was later input by colleagues.
In these times of internal e-mail (the business communication level) expectations are higher and the sort of exchange subtly different.
Because it has the element of ‘non-disturbance’ about it, some scientists feel easier about using e-mail instead of waiting for the corridor/coffee break meeting.
It means that the communication might be less opportunistic and more frequent with the targets selective rather than whomever happens to be in the group.
The group will regularly include colleagues on another site, not always possible and definitely less frequent (and more expensive!) in the pre e-mail days.
Another factor is that, because e-mail means keyboards, the ‘rough notes’ take on more structure and formality.
But are the standards of presentation becoming unnecessarily high?,
Will this discourage use rather than encourage it?
Some scientists feel the ease of access brings forward the point of communication, others that the typing element slows it.
Age, and familiarity with technology are factors here.
I am sure the former view will prevail as the technology comfort factor increases.
With the ease of editing, forwarding etc. it is potentially more difficult to identify, retrospectively, what was original thought and what the later additions by colleagues.
Scientists could become exactly the copious ‘self-copiers’ that their records managers are trying to discourage, merely to demonstrate the stage of their input.
My guess is that, if business e-mail is managed and retained, it will provide a fuller account of the development of successful ideas.
But a fuller account for whom?
Whether hand-written or e-mail typed, these records are informal, frequently out of context or mixed in with other subjects and meant purely for internal consumption.
Their retention could therefore be a liability in terms of legal discovery and Regulatory Authority investigations unless some strong discipline were imposed.
This would run counter to the very informal information exchange that gives it meaning in this internal context.
In its present form it would be of some value to the company historian but in public institutions, its status as a record, and therefore its management needs to be decided before we can determine its retention.
I have no doubt it would contain some valuable missing links and insights to the historian.
Prior to the use of electronic communication, at the informal academic exchange stage, scientists would commonly attend conferences, seminars and poster sessions.
Seminar papers would represent work well on its way to formal publication and posters a much earlier stage.
Communication would be limited to the attendees and readers of the published proceedings, posters to attendees only.
In the electronic environment, bulletin boards are paralleling the role of poster sessions and list servers that of seminar papers.
Bulletin boards and list servers can be structured like company e-mail in that subsections can be used to follow up a particular discussion or to follow through all the different accounts and editions so that one person can go back and read the whole story from the beginning to the last entry.
It is very important in this environment that somebody administers each system otherwise it becomes cluttered or unvetted (and for example, objectionable) messages may be sent to others.
If there is no deletion mechanism, a system may be overwhelmed by the volume of messages travelling through it.
On the other hand, although e-mail messages are faceless, they are by no means characterless.
Academic list servers have been known to start useful theoretical discussions over important issues within a field.
Thus, for a profession-orientated list server, messages may act as a record of the change in theory and practice and as such deserve good records management and perhaps long term retention.
At any level, bulletin boards and list servers are making an impact.
In the context of Internet, however, this impact is dramatic.
The Internet, to my mind, represents information exchange at its best.
Because its roots lie in the U.S.A., where the ‘public domain’ character of the information means a liberal approach to information sharing, it has set standards that belated attempts at commercial payback or selfish attitudes of information restriction cannot now erode.
What is the Internet?
‘The Internet is a large ‘network of networks.’
There is no one network known as The Internet; rather, regional nets like SuraNet, PrepNet, NearNet, et al.are all inter-connected…together into one great living thing, communicating at amazing speeds…
All activity takes place in ‘real-time.’
‘(Kehoe 1992).
What sort of information can be found on the Internet?
At the start of this paper I quoted from research done by Dr Almquist.
I would not have learnt of this paper so quickly (and perhaps not at all) had I not had access to the Internet.
No access to the Internet can leave you on the outside looking in, ignorance of the Internet means you do not even know there is a window on this information world.
It is important to understand that no one organisation or sub net ‘owns’ Internet — it has an ethereal existence of its own.
Each subnet has its own ‘domain’ and applies controls within that domain.
This could be a university, a company or an individual.
How does one access Internet?
Individual users from their humble PC can tap into a wealth of information, on an international scale (Figure 5.1).
Databases can be: private exchange type, public domain, or commercial (pay as you use or free, to identify bona fide users as potential customers).
Like international directory enquiries for telephone numbers it desperately needs administering and the US government has recently appointed AT&T to do just that.
(Did you know that Bill Clinton has an Internet e-mail address?)
With over 10 million users on 1.5 million computers connected by 1993, finding your way around does need some formal system and help.
Within information supplier domains you could find the following elements:
a list server as described above — a news group where the managers of the domain have been persuaded (e.g. by a list server member vote) that there is a short-term or ongoing need for a sub group to discuss a special project or specialism.
This section could be highly managed and is a possible focal point for collecting academic interaction on a given subject.
Usually the suggestors get the job of managing the news groups and would only impose their own standards of control.
The domain managers’ role ends at ensuring it is a sufficiently supported initiative to allow it to start.
databases accessed through that domain;
‘working parties’ e.g. setting standards for a common structure to e-mail messages and other documentation used across the Internet;
mail servers, like a list server but messages are received on request only (there is a filter system e.g. by subject or never from x!;
software archives for programs.
The domain is an important part of one's Internet mailing address.
Access to participants is not completely open.
Companies tend to use a ‘firewall’ along their route into Internet so that individuals cannot be looked up in the directory of users — a sort of ex directory.
This would parallel any company policy on not giving out personal telephone numbers but instead routing calls through the switchboard.
Academic bodies tend not to have firewalls.
Why are people so willing to provide all this free information (the only cost is the communication lines)?
Internet users fall into two camps, the information providers and the information gatherers.
The benefit for the gatherers — the parasites of the system- is self evident.
Providers get their intellectual payback in the currency of more information.
A good scientific example of this is the current work on molecular biology and the human genome project (HGMP).
Many groups around the world are working in these areas and the rate of data exchange using ‘traditional’ tapes or CD-ROM meant it could be two months before one group heard of another's ideas.
Fast by the seminar and poster standards, this was unacceptably slow to the scientific community involved.
User groups can now get updated information weekly, or even daily, as they require.
This is because individuals on the Internet took action.
An example among many is the National Centre for Biotechnology and Information (NCBI) at the National Library of Medicine (NLM) makes available its own and other U.S. and European ‘public domain’ molecular biology databases, plus the programs to use them.
It is hoped that other groups using these databases will submit their data in return, but this is not compulsory.
There is peer group pressure, however, to contribute the data.
For example, no reputable journal will publish papers without the reference indicating that the data concerned were accessioned onto one of these databases.
Similarly, the Los Alamos National Laboratory in New Mexico has compiled a database of databases for molecular biology (LiMB).
Another initiative in the pursuit of speedy access to data for the information client is the gopher (as in ‘go for’) system.
Devised by the University of Minnesota, it is a way of short cutting the routes through Internet which can also take advantage of using other host computers which are less busy (e.g. one in a different time zone).
Armed with this vision of the sheer volume and variety of information now available to the scientific community we will compare the pre electronic communication days with now.
At the stage in the growth of a scientific discovery when it is no longer just internal but has not yet become a formal paper, then the scholarly interchange would have depended on whom individuals knew or knew of, the letter writing would be unmanaged and survive by chance (often only one side of the story) and, due to concern that ideas should not be stolen, may only have taken place at a late stage of the discovery process.
The interchange would also rely more on personal meetings whether at seminars or not and some records would not then exist at all.
It is, however, a very personal and supportive communication and, from a historian's point of view could have the advantage of showing more about the people writing the letters than the scientific work under discussion.
But in terms of what records could now be accessed by a historian the quality, volume and completeness have improved drastically with the advent of electronic communication.
The records are not hand-written; they are more focused and less personal due to the wider audience, the variety of sources that can input to a debate is world-wide and some avenues of thought will survive through this to meet success which would have withered in a smaller community; response time is faster and, confident in the formal date stamping and public manner of their input, scientists are more willing to share information earlier.
There is still the factor that the whole process is technology dependent.
The formal publication stage is much the same when it comes to record content — peer and institution review will still be looking for the same things.
What is different in the electronic publishing age is the improvements in terms of access.
Yes, the publications will be technology reliant but for the historian looking at a particular subject, there is also key word searching, text retrieval,(to abstracts as well as full text) and all on a physically smaller source (CD-ROM).
It is also worth noting that my quotation number 5 is to a publication that has not been published except by the Internet and is only available from it.
So, is electronic communication encouraging information loss or information exchange?
Without records management skills and principles it has been and could continue to be the former.
But, given formal records management input, particularly to the list server and news group elements, and the matching input from the technologists to save and migrate those records, then archivists and historians could be entering a golden age.
Part II.
Technology, Standards, and Legal Issues
8.
Collecting Software: Preserving Information in an Object-Centred Culture[1]
Doron Swade, Senior Curator (Computing and Data Processing), Science Museum, London
Abstract:
Computer software is not yet an explicit part of the custodial mandate of the museum establishment and there is a growing alarm at the historical implications of this exclusion.
The nature of software is philosophically problematic.
In practical terms, a programme of acquisition and conservation is technically forbidding as well as resource intensive.
This article attempts to locate software as an artefact in the material culture of museums and explores some of our preconceptions and expectations for a software preservation programme.
It examines some respects in which software is both like and unlike traditional museum objects.
It briefly considers the prospects for extending the operational life of obsolete systems through physical restoration as well as logical simulation.
Museums are part of an object-centred culture.
Their essential justification is the acquisition, preservation and study of physical artefacts.
Physical objects, their meaning, significance and their care, dominate a curator's professional psyche.
One of the first tasks, then, is to locate computer software in the artefactual landscape.
Computer hardware, as a category of object, is seemingly unproblematic.
It is the physical stuff of computer systems and falls painlessly into the custodial universe of conventional object-centred curatorship.
Software, a term in general use by the early 1960s, is usually defined negatively, that is to say, a component of computer systems distinct from hardware.
The Oxford Dictionary of Computing (1986) defines software as ‘a generic term for those components of a computer system that are intangible rather than physical’.
Prentice Hall's Illustrated Dictionary of Computing (Nader 1992) irreversibly severs the material link by noting that ‘software is independent of the carrier used for transport’.
The non-material features of software have ominous implications.
The Science Museum's Corporate Plan for 1992–1997 states that one of its core objectives is to ‘acquire the most significant objects as physical evidence of science worldwide’.
Physical objects are explicitly identified as the evidentiary medium.
We have a prima facie conflict.
If what distinguishes software is something non-physical, and software is in some sense irreducibly abstract, then it falls outside the mandate of material culture and a conscientious museum curator might have qualms about mobilising resources to acquire and preserve it.
The dilemma may seem pedantic.
But there is a real issue: in whose custodial territory does software fall?
Is it the responsibility of the archivist, librarian, or museum curator?
Some software is already bespoke: archivists and librarians have ‘owned’ certain categories of electronic ‘document’— digitised source material, catalogues, indexes, and dictionaries, for example.
But what are the responsibilities of a museum curator?
Unless existing custodial protection can be extended to include software, the first step towards systematic acquisition will have faltered, and a justification for special provision will need to be articulated ab initio in much the same way as film and sound archives emerged as distinct organisational entities outside the object-centred museum establishment.
[2]
The distinction between hardware and software is not absolute.
‘Firmware’(programs held in read-only memory (ROM)) defies categorisation as exclusively one or the other.
The ROM-chip itself clearly belongs to the universe of hardware.
Yet insofar as the chip embodies a symbolic record of a program it is apparently also software.
If forced to answer the question ‘is firmware hardware or software?’, you would be excused for responding with a helpless ‘yes’.
One way of by-passing philosophical misgivings about the materiality of software is to appeal to the broader mandate of science museums to maintain a material record of technological change.
Software represents a substantial human endeavour, and the intellectual, economic and material resources involved in its production and distribution represent a major technological movement.
Its importance is not in dispute.
So perhaps we can bluff it out and collect software by day leaving philosophical disquiet to the troubled night.
In practical curatorial terms the abstraction of software is, in any event, something of a pseudo-problem.
We do not collect prime numbers or polynomials.
We collect instead physical models, mathematical instruments, and the written deliberations of mathematicians.
In much the same way our curatorial concern for software centres on the external physical record of programs and data-coding sheets, punched paper tape, punched cards, flowcharts, manuals, magnetic discs, publicity literature i.e. the distinct physical media of creation, representation, distribution, and storage.
So we could perhaps make a case for offering curatorial protection to artefactual software by regarding it as part of the contextual and functional extension of hardware without which technical history would be incomplete.
But the lump under the carpet is still visible.
Once we grant ourselves the licence to collect the physical artefacts of software, there remain, at least at first sight, respects in which software is both like, and unlike, traditional museum objects.
At the centre of curatorial practice is something called an inventory procedure.
This procedure formally transfers the ‘title’ of the object from the donor/lender/vendor to the Museum.
Each inventoried object is the direct responsibility of a named curator, the collecting officer, who signs a formal declaration of responsibility for each object when it is acquired.
‘I hereby take responsibility for the objects described overleaf’ is the forbidding form.
An object once inventoried is subject to formidable safeguards against disposal and unqualified alteration.
In museum culture the physical integrity of an inventoried object is sacrosanct and the act of inventorying marks its transition into protective custody.
However, objects decay despite our best efforts to conserve them.
Yet, when we acquire a brass telescope, it remains a brass telescope despite inevitable deterioration.
We refer to a rusted telescope as a ‘rusted telescope’ or more impressively, ‘telescope, condition poor’.
The time-scale of its degeneration does not seem to threaten its identity as a telescope, that is to say, its physical deterioration is sufficiently slow to support the illusion of permanence.
That it is a telescope seems not to be at risk.
Ultimately when time reduces our prized telescope to some orphaned lenses adrift in a little heap of metallic oxide we sadly shake our heads over the debris and say ‘this was a telescope’, or, in Pythonesque terms, ‘this is an ex-telescope’.
This acquisition model for conventional physical objects poses two curatorial conundrums for any potential software acquisition programme.
There is the issue of permanence, and the related issue of artefactual identity.
Magnetic media, the most common means of information storage for machine-readable software and data, are notoriously impermanent.
Banks, required to retain computer records for audit purposes, were advised in the US in the early 1980s that no archived magnetic medium over three years old should be regarded as reliable.
Posterity stretches ahead without limit whereas disc and tape manufacturers, when they are prepared to commit at all, are reluctant to do so for more than a few years.
In what sense can a curator responsibly sign the acquisition declaration knowing full well that there is no guarantee that a floppy disc or tape will be readable in a few years even if pampered with executive class conservation treatment — acid-free packing, humidity- and temperature-controlled environment, and low ambient light levels?
While magnetic media are in general demonstrably more robust than worst-case fears indicate, it is only worst-case life-expectancy figures that can responsibly be adopted in the context of systematised software archiving.
[3]
The acknowledged impermanence of the medium leads to the question of artefactual identity.
Is a set of floppy discs for Windows 1.0, say, like the telescope with an identity that transcends its state of repair?
If the information content, represented by the magnetic configuration of the disc coating, is what makes a set of discs Windows, then does ‘Windows 1.0, condition poor’ mean anything?
In more practical terms, does meaningful collection of software imply a functionally intact copy with the promise or potential of running it?
If so then we have at least one clear respect in which artefactual software, acquired in accordance with the cannons of conventional museology, differs from software acquired for archival purposes.
[4]We do not ask ‘functional intactness’ of the telescope.
‘Telescope, broken’ does the job.
We can perhaps draw a useful analogy with pharmaceutical products.
I learn from my medical sciences colleagues that the Science Museum has recently placed some proprietary drugs on inventory.
Panadol, say, is now an inventoried object.
There is valuable cultural information in the physical artefact: tablet form, bubble-pack press-through dispenser, advertising imagery used in the logo and packaging, and information about consumer appeal.
But we can be reasonably sure that the drug company will not guarantee the potency of the sample beyond its sell-by date.
We are clearly acquiring Panadol at least partly as a cultural artefact on the understanding that its chemical infrastructure and therefore its potency is ephemeral.
In museological terms Panadol does not cease to be Panadol when it is no longer chemically potent.
Similarly, the centuries-old ‘poison-tipped arrow’ remains so-called though the likelihood of any residual toxin is remote.
Is the Windows disc like Panadol?
Apart from the facetious difference that the one gives headaches which the other alleviates, there are strong similarities.
‘Potency’ in both cases is not visually meaningful.
Function is not manifest in external form.
Further, the Windows discs are no less a vehicle for contextual and technical messages than the Panadol pack: symbolism and imagery in brand logos and packaging, quality of label print, physical size, soft or hard sectored, whether or not factory write-protected, presence of reinforcing ring and so on.
The discs are informative as generic objects (media) as well as conveying product-specific information about Windows.
However, the richness as a cultural object of a deteriorated Windows 1.0 disc pack is cold comfort to an archivist or historian preoccupied with preserving or regenerating the operational environment of the product.
So we return to the question of functional intactness.
Software we know is ‘brittle’.
It degrades ungracefully.
We are all familiar with the awful consequences of what in information terms may be a trivially small corruption.
One bit wrong and the system crashes.
There are however situations in which the value of magnetically stored information is not bit-critical.
Discs used as storage media for textual data as distinct from programs provide one example.
Parchment deteriorates leaving us with partial or fragmentary records.
A progressively corrupt magnetic record is simply a partial record but a usable record nonetheless.
The residual data is not deprived of meaning or access by partial corruption.
The ‘all or nothing’ fears do not in this case apply and we may be encouraged to re-examine whether there is some give in the apparently uncompromising need for bit-perfect records of program software.
If we look at the effects of corruption on program performance we can identify three broad categories.
Non-critical corruption covers situations in which unused portions of the program are compromised — unused print drivers, irrelevant utilities or subroutines, for example.
If we use a steam engine, say, as an example of a conventional museum object ‘non-critical corruption’ would correspond to the damage to an unused or non-critical part — a nut dropping off, a dented panel.
Damage in this case does not compromise the primary function, that of producing traction.
Critical corruption leading to evident malfunction is a second category — the system hangs, the cursor freezes, the operating system fails to boot, or the program produces obvious gibberish.
In our steam locomotive comparison, the engine loses traction, or makes an expensive noise and stops.
So far the comparison with physical machines works.
The third and most worrying category is critical corruption that produces non-evident errors — a maths program that produces an incorrect numerical result, a data-base manager that cross-labels data records, for example.
Comparison with the stalled steam engine is not obvious.
Perhaps a closer analogy would be with a telescope that misrepresented what we were looking at.
The distant unsighted object is a church steeple.
But observed through our telescope (condition, good) we see the image of a mosque.
It seems reasonable to conclude that if archived program-software is to be run, the need for bit-perfect records is uncompromising.
If the medium of issue is magnetic then the indefinite maintenance of bit-perfect records commits us to an active program of periodic renewal and integrity checking, or a one-off transfer to a more permanent medium.
[5]Engineering instinct favours retaining the medium and format of issue to ensure compatibility with the original hardware.
Transferring software to a more permanent storage medium, optical disc, for example, offers a tempting liberation from the fate of perpetual periodic renewal.
However, the interdependence of hardware and software poses formidable technical difficulties to running programs so transferred.
Machine-independent software is frequently anything but.
Correct operation of applications software relies more often than not on particular revisions of system software, program patches, hardware upgrades, firmware revisions and machine-dependent interfacing to peripherals.
Transferring to an alternative medium requires new data formats yet to be standardised and dependence on a new generation of hardware to read or download stored information.
Interfacing to these devices and executing code so stored is not straightforward.
Transfer to a more permanent medium is not without penalty despite its promise of releasing Sisyphus from his fate in the copying room.
The requirement for functional intactness of software not only entails the maintenance of bit-perfect records but also implies the provision at some time of operational contemporary hardware or a functional equivalent.
Neither the provision of contemporary hardware or a functional equivalent is trivial.
In 1989 the Science Museum, with the British Computer Society, founded the Computer Conservation Society dedicated to the restoration and preservation of historic computers and to the capture of operational know-how of computing machines.
The Society has had signal success in restoring to working order a Ferranti Pegasus , a large vacuum-tube machine dating from 1958 (Figure 8.1), and an Elliott 803 , a discrete component germanium transistor machine dating from 1963.
At best such ventures can extend the operational life of obsolete systems.
The life expectancy of the Pegasus, for example, has been extended by an estimated five to ten years.
But however successful these endeavours, we have to accept the eventual demise of such systems.
The intractable fact of the matter is that in terms of archaeological time scales the operational continuity of contemporary hardware cannot be assured even when suitable specimens are available to begin with.
What meaning, then, does an archive of bit-perfect program software have if the material cannot be run?
One way forward presently being explored by the Computer Conservation Society is to simulate early hardware on present-generation computers using the restored original as a benchmark.
Two simulations are well advanced, one for the Pegasus , the other for a German Enigma cypher machine.
In the case of the Pegasus , console switches, console oscilloscope traces, input/output peripherals (paper tape, teletype-style printers) are visually simulated in facsimile and animated on-screen (compare Figure 8.2 with Figure 8.3).
The operator can write, run and debug programs by ‘driving’ the simulated controls and the simulator responds appropriately even to the extent of execution times.
Since the original storage medium for software is paper tape, surviving data and software libraries can be captured and preserved on modern hardware by interfacing to contemporary optical tape readers and storing the programs in a form that can executed by the simulator.
The museological implications of such simulations are intriguing.
An implicit tenet of museum life is that the original object is the ultimate historical source.
In museum culture the original physical artefact is venerated at the expense of a replica, duplicate, reconstruction, or hologram.
However physical replicas can only incorporate features and characteristics perceived to be significant at the time of replication and part of the justification for preserving original objects in preference to a copy is that the original can be interrogated in an open-ended way in the light of unforeseen enquiry.
[6]If we wished to test a new theory about Napoleon's allergy to snuff, say, it would not make sense to examine look-alikes of Napoleon's clothing.
Prior to the snuff-allergy hypothesis, snuff-content would not be a consideration in the making of a garment replicas.
Only the original artefact with authenticated provenance would suffice for this forensic purpose.
However, logical replication as distinct from physical replication seems to offer more.
Capturing the operational persona of an early machine on a later machine promises possibilities for open-ended analysis of the kind formerly offered only by a working original.
The technique seems to offer a form of logical immortality as computer languages used for the simulations become increasingly machine-independent.
The resource implications of a meaningful software acquisition programme are formidable.
However persuasively we argue to include software in the existing fold of custodial protection, the need for the special provision of resources cannot be evaded.
The maintenance of bit-perfect records requires an open-ended commitment to periodic copying and checking.
This requires staff and equipment.
The transfer of program-software to optical media invokes a raft of technical issues of operational compatibility that would require prohibitively large (for a museum with a conventional mandate) engineering and hardware design investment to solve.
The restoration and maintenance of contemporary hardware on an indefinite basis demands vast financial resources and the opportunity cost is likely to be politically indefensible.
The development of simulations and emulations is technically promising but the necessary skills-levels are high and the financial implications of programming, development, and verification, are substantial.
The progress made in this field at the Science Museum would have been unaffordable without the voluntary efforts and expertise of Computer Conservation Society's members.
In custodial terms, even a successful simulation exercise does no more than transfer the operational persona of an historic early machine to a currently supportable platform (typically a 486-based PC) which will itself be duly subject to generational obsolescence: the potential of the technique lies not in the immortality of current hardware but in the prospect of machine-independent software.
But the Utopia of machine-independence may not ultimately appear on the custodial atlas of the future.
In the meanwhile, simulation buys time and allows us to pass the baton to the next generation which may well have to face similar problems.
Despite the formidable obstacles that face a fully-fledged software preservation programme there is at least one modest but significant programme of software acquisition that is technically achievable and that has affordable resource implications, namely, software for personal computers —‘shrink-wrapped’ consumer software as well as custom-written special applications software.
Acquiring working specimens of significant volume-production personal computer since 1977, and their variant upgrades, is still a realistic acquisition objective.
The technical skills required to recommission, repair and maintain such machines are still available.
The complementary task is to identify and acquire significant examples of contemporary consumer software, from VisiCalc (an early spread sheet package for the Apple II, available in 1979) and Electric Pencil (a word processing package for the Tandy Model I, 1978), through to DOS 6.0 and Windows 3.1 for present day PCs.
The acquisition of these products can be accomplished with existing resources at the Science Museum.
The recommissioning of the hardware, copying onto fresh stock, and documenting the operational quirks of the systems would require additional but affordable technical support.
Once established, the ‘archive’ will be relatively easy to keep updated — this by purchasing off-the-shelf current software products and contemporary hardware.
In the absence of an independently resourced organisation with a specific brief to preserve systems and applications software this programme represents one practical step we can take.
In overall archival terms the venture is no more than a holding operation.
Perhaps the cavalry will still arrive in time.
9.
The Text Encoding Initiative: Towards an Extensible Standard for the Encoding of Texts.
Lou Burnard, Oxford University Computing Services
Abstract:
This paper gives a brief introduction to the scope and development of the Text Encoding Initiative's Recommendations, published in July 1993.
A brief overview of the scheme's modular architecture is given, with particular attention to the proposals for the encoding of bibliographic and descriptive information, of textual records as typed and structured objects and of derived or analytic information extracted from textual records in an integrated and well-defined manner.
9.1.
What is the TEI?
The Text Encoding Initiative (TEI) is a project sponsored and organized by the three leading professional associations in the field of computer-assisted literary and linguistic research: the Association for Computational Linguistics (ACL), the Association for Literary and Linguistic Computing (ALLC) and the Association for Computing and the Humanities (ACH).
It has been funded throughout its four years of activities on both sides of the Atlantic: primarily by the US National Endowment for the Humanities and by the EC as part of its framework for linguistic research and engineering, but also with grants from the Mellon Foundation and from the Canadian Social Sciences and Humanities Research Council.
Most significant however has been the donation of time and expertise by the many members of the wider research community who have served on the TEI's Working Committees and Working Groups.
These have included representatives of many disciplines beyond the comparatively narrow world of literature and linguistics, notably the librarian and cataloguers serving on the TEI Documentation Committee, and the historians serving on the workgroups for historical texts and physical description.
The project will deliver a fully specified set of Guidelines which will enable researchers in any discipline to interchange texts and datasets in machine-readable form, independently of the software or hardware in use, and also independently of the particular application for which such electronic resources are used.
As the name suggests, these Guidelines are not intended to be binding or prescriptive, but rather to provide impartial guidance for the perplexed.
That is not say that those whom it is hoped to benefit are in any sense undemanding or unsophisticated.
On the contrary, the TEI community is a particularly demanding one: the purpose of research is to discover solutions to problems that have not yet been posed, and any scheme designed to support research must therefore place more emphasis on flexibility and extensibility to cope with the unforeseen than on highly optimized solutions to well understood problems.
At the same time, academic researchers are likely to be as impatient as anyone else with solutions that require extensive specialist knowledge of little relevance to the problem in hand.
The TEI scheme must therefore also be simple to grasp in its essentials and of demonstrable benefit in areas that concern researchers: above all, the efficient and loss-free interchange of information across different hardware platforms, amongst different software environments, for the widest possible range of applications.
Many academic research projects take years to complete; a scheme designed to support them must thus be future-proof — and this in a world where technology continues to mutate at an alarming rate.
As stated above, the goal of the TEI encoding scheme is no less than hardware-, software-, and application-independent support for the encoding of all kinds of text in all languages and of all times.
[1]It is becoming increasingly clear that, of these, the stress on application independence is particularly relevant.
The TEI community is, of course, strongly interested in the notion of electronic text as a stage in the production of paper documents.
But it is equally concerned with the usage of electronic text as an end in itself, whether as a research database or a component in non-paper publications.
Like the publishing industry, the academic community is rapidly coming to realise that its stock in hand is not words on the page, but information, independent of its physical realisation.
The TEI scheme is one expression of that important realisation.
This means that the TEI Guidelines have a dual focus: being concerned with both what should be encoded in an electronic text, and how that encoding should be represented for interchange.
The approach taken is a two stage one: firstly, the identification of those distinctions concerning which there is common agreement; secondly the creation of a uniform encoding system within which those distinctions can be expressed for interchange.
Early on in the project, the Standard Generalized Markup Language (SGML; ISO 8879) was chosen as the most appropriate vehicle to represent the textual features identified by the scheme, on the purely pragmatic grounds that no other candidate seemed to meet the requirements discussed above.
If SGML had proved inadequate to the needs of researchers, we would have abandoned it without a qualm; perhaps fortunately, it did not.
On the contrary, it has proved remarkably difficult to find problems for which a solution could not be expressed in SGML.
Viewed as a standard, the TEI scheme attempts to occupy the middle ground.
It offers neither a single all-embracing encoding scheme, solving all problems once for all, nor an unstructured collection of tagsets.
Rather we offer an extensible framework containing a common core of features, a choice of frameworks or bases, and a wide variety of optional additions for specific application areas.
Rather light-heartedly, we refer to this as the Chicago Pizza model (in which the customer chooses a particular base — say deep dish or wholemeal crust — and adds the toppings of his or her choice), by contrast with both the Chinese menu or laissez-faire approach (which allows for any combinations of dishes, even the ridiculous) and the set meal approach, in which you must have the entire menu.
The prime deliverable of the TEI scheme will be a large and integrated collection of SGML tagsets, based on, but not limited by, existing practice in the encoding of textual information for electronic processing.
Its research origins mean that these tagsets must be both comprehensive and extensible.
They will be documented in a substantial reference manual, the Guidelines for Text Encoding for Interchange .
A first draft of this publication appeared in November 1990, and was widely circulated and discussed.
[2]During 1991 and 1992, these initial recommendations were greatly revised and reorganised, largely as a result of work carried out in a number of small specialist work groups, set up following a detailed technical review meeting held in November 1991. [3]A second draft (P2) began publication as a series of electronic fascicles in April 1992, and is due for completion in July 1993, following a further technical review in May, and presentation to the TEI's Advisory Board in June.
In addition to this reference manual — which is a substantial document not intended for casual browsing — the TEI plans to make available a number of smaller introductory tutorials focused on particular application areas.
One such, on terminological systems, has already appeared [4]and several others are in the planning stage.
Probably of equal importance will be the appearance of a large number of demonstration TEI-conformant datasets, for which plans are also in hand.
It is worth noting in this context that the TEI does not at this stage plan to develop any software to support the scheme, independence of software being a prime design goal; subsequent phases of this or successor projects may well however elect to do so.
9.2.
Basic structure of the TEI scheme
As an SGML application, the TEI scheme necessarily requires the existence of some kind of document type definition (dtd).
Indeed, it defines not one but many possible TEI dtds.
A TEI conformant document must be describable by means of a dtd built up from the TEI tagsets according to a set of rules documented within the Guidelines, defining how tagsets may be combined and modified.
Full details of this process are given in chapters ST, MD and CF of the Guidelines: the following informal description simply gives an overview of the process.
A TEI dtd (or, more exactly, a view of the TEI dtd) is composed of the core tagsets, a single base tagset and any number of user selected additional tagsets or ‘toppings’.
At the highest level, all TEI documents conform to a common model.
The basic unit is a text, by which is meant any single document, or any stretch of natural language regarded as a self-contained unit for processing purposes.
The association of such a unit with a header describing it as a bibliographic entity (see further HDR below) is regarded as a single *lt; tei> element.
Here, for example, is a minimal TEI conformant document:
Two variations on this basic structure are defined: a collection of *lt; tei> elements is possible (this we call a corpus); secondly, support is also provided for a variety of composite texts.
The first solution is most appropriate for large disparate collections of independent texts, for example in language corpora, or collections of unrelated papers in an archive.
The second better suits such cases as the complete works of a given author, which might be regarded simultaneously as a single text in its own right and as a series of independent texts.
In the first case, each text is given its own header, with any material common to all texts being factored out to an overall corpus header.
In the second case, a special element *lt; group> is used, which can appear as the body of a <text> and which is composed of a sequence of other, nested, groups or texts.
This mechanism also allows for the encoding of composite works such as anthologies, in which individual texts may have different structural properties (for example, being composed of both dramatic and prose texts).
For the SGML-aware, we present here a somewhat simplified version of the SGML declarations for the highest level elements in the TEI scheme:
9.2.1.
The TEI base tagsets
At the time of writing, eight distinct base tagsets are proposed.
Six of these are intended for documents which are predominantly composed of one type of text; the other two are provided for use with texts which combine these basic tagsets.
It is possible that other base tag sets will be added in the future.
Those currently proposed are as follows:
prose;
verse;
drama;
transcribed speech;
letters or memos;
dictionary entries; and,
terminological entries.
Each TEI base tagset determines the basic structure of all the documents with which it is to be used.
More exactly, it defines the components of <text> elements, combined as described above.
In practice, so far, almost all the TEI bases defined are very similar in their basic structure, though the means exist for them to vary this if necessary.
They do however differ greatly in their components: the kind of subelements likely to appear within the divisions of a dictionary (for example) will be entirely different from those likely to appear within the divisions of a letter or a novel.
To cater for this variety, the constituents of all divisions of a TEI <text> element are not defined explicitly, but in terms of parameter entities.
A parameter entity is an SGML construct which may be thought of in simple terms as like a variable declaration in a programming language: the effect of using them here is that each base tag set can provide its own specific definition for the constituents of texts, which can, moreover be modified by the user.
9.2.2.
Textual divisions
Some forms of text (notably transcriptions of spoken language) are only notionally divisible into multiple levels of structure.
For the majority however, there is a bewildering and highly application or culture-specific variety of high level units into which they may be divided.
Fundamentally however, all objects such as‘chapters’, ‘sections’, ‘entries’, ‘acts’ and ‘scenes’, ‘cantos’etc. seem to behave in the same way: they are incomplete in themselves, and often nest hierarchically.
In the TEI scheme all such objects are regarded as the same kind of element, called here a ‘division’; though a distinction is made between divisions whose hierarchic position is regarded as inseparable from their semantics (these are encoded as <div1>, <div2> etc. down to <div7> elements) and those for which their position in the document tree is regarded as of lesser importance (these are encoded as ‘vanilla’ <div>s).
Numbered and un-numbered division elements may not be mixed in the same <front>, <body>, or <back> element.
A type attribute may be used to distinguish amongst divisions in some respect other than their hierarchic position: the values for this attribute (as for several others in the TEI scheme) are not standardized, precisely because no consensus exists, or is likely to exist, as to a typology.
It should however be possible to specify the range of values legal for a particular application, either in the TEI Header (see section HDR below) or by a user-defined modification.
The lower level components of textual divisions are parameterized by means of another mechanism which characterizes the whole TEI scheme: the use of parameter entities which are specified by the particular base in use.
All divisions are defined in the core dtd with a content model including ‘%component.seq’; the exact value of this parameter entity will however be different in different bases.
In this way it is possible for the divisions of a text using the drama base (for example) to consist of speeches and stage directions, while those of a text using the dictionary base will consist of lexical entries.
In the normal case, the components of all divisions in a particular base are homogeneous — they all use the same value for ‘%component.seq’.
However, as noted above, the scheme also allows for two kinds of heterogeneity.
If the general base is selected, together with two or more other bases, then different divisions of a text may have different constituents, though each division must itself be homogeneous.
A mixed base is also defined, in which components from any selection of bases may be combined promiscuously across division boundaries.
9.3.
The core tagsets
Two core tagsets are available to all TEI documents without formality, unless explicitly disabled.
The first defines a large number of elements which may appear in any kind of document — coinciding more or less with that set of discipline-independent textual features concerning which consensus has been reached.
The second defines the header, providing something analogous to an electronic title page for the electronic text, as further discussed in section 3.2 below.
9.3.1.
Elements available to all bases
The core tagset common to all TEI bases provides means of encoding with a reasonable degree of sophistication the following list of textual features:
Paragraphs;
Passages of verse or drama, distinguishing for example speakers, stage directions, verse lines, stanzaic units, etc;
Segmentation, for example into orthographic sentences;
Lists of various kinds, including glossaries and indexes;
Typographically highlighted phrases, whether unqualified or used to mark linguistic emphasis, foreign words, titles etc.;
Quoted phrases, distinguishing direct speech, quotation, terms and glosses, cited phrases etc;
Names, numbers and measures, dates and times, and similar ‘data-like’ phrases;
Basic editorial changes (e.g. correction of apparent errors; regularization and normalization; additions, deletions and omissions);
Simple links and cross references, providing basic hypertextual features;
Pre-existing or generated annotation and indexing;
Bibliographic citations, adequate for most commonly used bibliographic packages, in either a free or a tightly structured format; and,
Simple or complex referencing systems, not necessarily dependent on the existing SGML structure.
There are few documents which do not exhibit some of these features; and none of these features is particularly restricted to any one kind of document.
In most cases, additional more specialized tagsets are provided for those wishing to encode aspects of these features in more detail (see further section 9.4. below), but the elements defined in this core are believed to be adequate for most applications most of the time.
These features may be categorized or classified in a number of ways.
One classification scheme used within the TEI scheme is based on the fact that many elements share the same set of attributes.
All elements which represent links or associations between one element and another do so using a common set of attributes, and are thus regarded as forming the attribute class pointer.
In the same way, the attribute class name contains all elements used to represent various kinds of name, all of which share the attributes key (used to specify an external database key or other identifier for the object named) and reg (used to specify a regularized form of the name).
The TEI encoding scheme also uses a classification system based upon structural properties of the elements: that is, their position within the SGML document structure.
Elements which can all appear at the same position within a document are regarded as forming a model class: for example, the class phrase includes all elements which can appear within paragraphs but not spanning them, while the class chunk includes all elements which cannot appear within paragraphs (such as paragraphs, for example ).
A class inter is also defined, for elements such as lists which can appear either within or between chunk elements.
Classes may have super- and sub-classes, and properties (notably associated attributes) may be inherited.
For example, reflecting the needs of many TEI users to treat texts both as documents and as input to databases, a sub-class of phrase called ‘data’ is defined to include ‘data-like’features such as names of persons, places or organizations, numbers and dates, abbreviations and measures.
These behave in exactly the same way as phrase elements.
The formal definition of these classes in the SGML syntax used to express the TEI scheme makes it possible for users of the scheme to extend it in a simple and controlled way: new elements may be added into existing classes, and existing elements renamed or undefined, without any need for extensive revision of the TEI document type definitions — though this is perhaps a benefit which only those who have ever tried to modify an existing dtd by hand will truly appreciate.
9.3.2.
The header elements
As noted earlier, the TEI scheme is perhaps unusual in the stress it lays on documentary and descriptive information.
The header allows for the definition of a full AACR2-compatible bibliographic description for the electronic text, covering all of the following:
the electronic document itself;
the sources from which it was derived;
the encoding system which has been applied; and,
its revision history.
The header is one of the few mandatory elements in a TEI document, reflecting the importance attached to this kind of information by the research community.
The TEI header allows for the provision of a very large amount of structured or unstructured information under the above headings, including both traditional bibliographic material which can be translated straight into an equivalent MARC catalogue record, as well as a wealth of descriptive information such as the languages it uses and the situation within which it was produced, expansions or formal definitions for any codebooks used in analysing the text, the setting and identity of participants within it.
This diversity and richness reflects the diversity of uses to which it is envisaged that electronic texts conforming to these Guidelines will be put.
The amount of encoding in a header will depend both on the nature and the intended use of the text.
At one extreme, an encoder may provide only a bibliographic identification of the text.
At the other, encoders wishing to ensure that their texts can be used for the widest range of applications, will want provide a level of detailed documentation approximating to the kind most often supplied in the form of a manual.
Most texts will lie somewhere between these extremes; textual corpora in particular will tend more to the latter extreme.
To give some flavour of the possibilities, we represent below a header for the same text[5] first using the minimal level of required encoding:
We now present the same example header, expanded to include additionally recommended information, adequate to most bibliographic purposes, in particular to allow for the creation of an AACR2-conformant bibliographic record, and also some (mostly imaginary) details about the text itself.
As this example shows, a TEI Header can become very large.
A collection of TEI headers can also be regarded as a distinct document, and an auxiliary dtd is provided to support interchange of headers alone, for example between libraries or archives.
9.4.
The TEI additional tagsets
A number of optional additional tagsets are defined by the current proposals.
These include tagsets for special application areas such as alignment and linkage of text segments to form hypertexts; a wide range of other ‘analytic’ elements and attributes; a tagset for detailed manuscript transcription and another for the recording of an ‘electronic variorum’modelled on the traditional critical apparatus; tagsets for the detailed encoding of names and dates; abstractions such as networks, graphs or trees; mathematical formulae and tables etc.
In addition to these application-specific specialized tagsets, a very general purpose tagset is also proposed for the encoding of entirely abstract interpretations of a text, either in parallel with it or embedded within it.
This is based on the feature structure notation employed in theoretical linguistics, but has applications far beyond linguistic theory.
[6]Using this mechanism, encoders are at liberty to define arbitrarily complex bundles or sets of features identified in a text, according to their own methodological bias.
The syntax defined by the Guidelines not only formalizes the way in which such features are encoded, but also provides for a detailed specification of legal feature value/pair combinations and rules determining, for example, the implication of under-specified or defaulted features.
This is known as a feature system declaration.
A related set of additional elements is also provided for the encoding of degrees of uncertainty or ambiguity in the encoding of a text.
This tagset exhibits in a particularly noticeable form one of the chief strengths of the TEI approach to encoding: it provides the encoder with a well-defined set of tools which can be used to make explicit his or her reading of a text.
No claim to absolute authority is made by any encoder, nor ever should be; the TEI scheme merely allows encoders to ‘come clean’ about what they have perceived in a text, to whatever degree of detail seems appropriate.
A user of the TEI scheme may combine as many or as few additional tagsets as suit his or her needs.
The existence of tagsets for particular application areas in the current draft reflects, to some extent, accidents of history: no claim to systematic or encyclopaedic coverage is implied.
Indeed, it is confidently expected that new tagsets will be added, and their definition will form an important part of the continued work of this and successor projects.
9.5.
Publication of the guidelines
The current TEI development cycle comes to an end in July 1993, following the presentation of a completed draft of P2 to the project's Advisory Board, meeting in Chicago at the end of June.
After implementation of any modifications proposed at that meeting, publication of the completed draft will be undertaken in a variety of forms.
Full details are yet to be worked out, but both electronic and printed publication are anticipated.
[7]At the time of writing, draft texts of the Guidelines and of the TEI dtds are both readily available by anonymous file transfer from a number of Internet sites world wide, and this policy should continue.
A TEI electronic bulletin board is also maintained at the University of Illinois at Chicago: this is used to announce availability of TEI drafts and other publications, and to distribute them in electronic form, as well as providing an open forum for comment and discussion of the TEI recommendations.
To subscribe to this service, send an electronic mail message in the form SUBSCRIBE TEI-L Your Name to the address Listserv uicvm.uic.edu.
10.
Electronic Documentation and the Law
Ian Walden, Tarlo Lyons Research Fellow Centre for Commercial Law Studies, Queen Mary & Westfield College (University of London)
Abstract:
This paper will review the range of statutory regulations which impact the means by which data is stored; the conditions under which such storage must occur and the rules regarding the release of such information.
In particular, it will focus on data protection and intellectual property implications for electronic record-keeping.
10.1.
Introduction
Electronic documentation systems (EDS) are increasingly becoming the mode of information collection, communication and storage for organisations.
Included within this generic term are such systems as document image processing, e-mail, electronic databases (both online and CD based) and electronic data interchange.
This paper considers some of the legal implications arising from the adoption of such technologies:
(1)
what statutory record-keeping requirements exist?;
(2)
what rules regulate the submission of electronic records to court in the event of a dispute?;
(3)
what obligations arise under the Data Protection Act 1984?; and,
(4)
how does copyright protect collections of electronic documents?
Legal requirements need to be considered during the implementation of electronic documentation systems to ensure legal security.
10.2.
Record-keeping
Organisations, whether operating in a paper or electronic environment, need to ensure that adequate record-keeping procedures are established for a range of complementary reasons:
10.2.1.
Organisational needs
Organisations will need to collect information and maintain records for a range of internal management purposes.
Management is increasingly concerned, particularly during economic recession, to be able to monitor the day-to-day performance of the organisation, such as cash-flow, and therefore they desire access to a range of operational figures.
Record-keeping should also be an important aspect of a information security policy against the alteration, loss or destruction of business-critical data.
10.2.2.
To fulfil statutory/quasi-regulatory requirements
These can be sub-divided into two broad categories: record retention requirements, such as the Companies Act 1985, ss.221–222; and requirements to disclose information to various authorities/persons, which imply the need to maintain records, such as the Data Protection Act 1984 (see below).
In addition to the records which are required for statutory purposes, organisations may also be required to retain records for statistical purposes, for national and international governmental bodies.
For example, from 1st January 1993, the legal responsibility to provide Supplementary Declarations (Intrastats) to Customs and Excise authorities will fall on larger traders for all intra-EEC trade.
Finally, organisations may be required to maintain records to satisfy rules imposed by the professional bodies, such as the Chartered Institute of Accountants and the Law Society.
10.2.3.
The evidencing of events in case of dispute
Organisations need to maintain certain records as part of the process of monitoring contract compliance, both by themselves and their trading partners.
In the event of non-compliance, in English law, an action can be brought for breach of contract or tort up to six years after the date of the event, as laid down in the Limitation Act 1980. [1]Documents that originate through the existence of the contract and evidence the events arising under the contract should therefore be maintained for that six year period.
There are no general regulations in the UK governing the type of medium to be used for archival recording, although some regulations stipulate additional conditions for electronic records; and certain authorities have the right to require that their approval is obtained before a company relies exclusively on electronic records, for example:
Companies Act 1985 , section 722(2): Where information is held in a form other than in a ‘bound book’, then ‘adequate precautions shall be taken for guarding against falsification and facilitating its discovery’.
Section 723, ‘Use of computers for company records’, permits recording ‘otherwise than in a legible form, so long as the recording is capable of being reproduced in a legible form’.
The Value Added Tax Act 1983 , Schedule 7, s.38, art.7, expressly authorises the electronic recording of information for VAT purposes, provided that:
i)
the inspector is informed in writing at least 1 month before;
ii)
any conditions laid down by the inspector are followed (Schedule 7, s.3).
Such record-keeping requirements will obviously mean that the electronic records need to be continuously maintained in a readable format for the relevant period from creation.
Organisations therefore need to ensure adequate procedures are in place to enable such maintenance, including the retention of such items as the computer programs, manuals and instructions in use when the record was originally stored.
Apart from the general requirements outlined above, the UK has no clear and comprehensive statutory rules detailing which data and transmission records should be maintained and in what form.
10.3.
Evidential law
A disadvantage of electronic documentation systems (EDS) is their ‘fleeting’ nature.
In the event of a dispute going to court, the parties will be required to submit evidence into court.
Under English evidential law, specific provisions are laid down for the submission and use of computer records in court.
In this respect, it is important to distinguish between two separate issues:
(1)
Can electronic records be submitted into court as evidence: ‘the question of admissibility’?
(2)
What value or weight will the court give to such computer-based evidence?
Obviously, the answer to the first question determines the relevance of the second.
In common law jurisdictions, the main obstacle to the general admissibility of computer records is the ‘hearsay rule’:
‘It derives from the adversarial nature of legal proceedings in the common law tradition whereby a party proves his case by calling witnesses with personal knowledge of events…
A witness without first hand knowledge of the events…cannot be challenged in cross-examination in this way and so such evidence…is generally excluded.’
(Bradgate 1989: 12)
Generally, whether computer records fall under this rule depends on the information contained within the record and the purpose for using such information as evidence.
The main situations under which computer records avoid the hearsay rule are:
(1)
Where the records have been classified as ‘real evidence’; eg. when computers are operating in a simple mechanistic way as automatic recording systems, for example an automated radar tracking system and a breath-testing machine, rather than as complex analytical tools (see The Statute of Liberty [1968]2 All ER 195).
(2)
Where the record is produced simply to show that a statement was made, not whether it is true.
(3)
Where the parties to the dispute agree to allow the use of such evidence.
English law has special provisions to allow for computer records to be admitted, however, it is currently uncertain whether the conditions apply to all computer evidence or only when admitted as hearsay.
The Civil Evidence Act 1968, Section 5 states:
‘In any civil proceedings a statement contained in a document produced by a computer shall…be admissible as evidence of any fact stated therein of which direct oral evidence would be admissible…’
However, only if the following conditions are satisfied:
(1)
That the document containing the statement was produced by the computer during a period over which the computer was used regularly to store or process information for the purposes of any activity regularly carried on over that period, whether for profit or not, by any body, whether corporate or not, or by any individual;
(2)
That over that period there was regularly supplied to the computer in the ordinary course of those activities information of the kind contained in the statement or of the kind from which the information so contained is derived;
(3)
That throughout the material part of that period the computer was operating properly or, if not, that any respect in which it was not operating properly or was out of operation during that part of that period was not such as to affect the production of the document or the accuracy of its contents; and,
(4)
That the information contained in the statement reproduces or is derived from information supplied to the computer in the ordinary course of those activities.
To satisfy the court that these conditions have been met, it is necessary to obtain either oral testimony or, in the vast majority of cases, a signed statement from the person that occupies ‘a responsible position in relation to the management of the activities for the purposes of which the computer was used’(s.5(4)).
Overall, commentators have noted that the draftsman of the Civil Evidence Act 1968 would seem to have confused issues concerning the weight or value that should be given to computer evidence, with that of its admissibility to court.
Such confusion could mean the introduction of over-complex audit practices by companies wishing to ensure that the court will accept the admissibility of their records.
Some commentators have even called for computer evidence to be no longer treated as ‘hearsay’; replacing such controls with:
‘guidelines and codes of practice (perhaps eventually having legally enforceable weight) aimed at ensuring an objectively high standard of computer reliability and security’(Castell 1991: 98).
In the UK, there is a BSI Standard for preparing microfilms that may be used in evidence, No.6498,Guide to the preparation of microfilm and other microforms that may be required as evidence , 1991.
A similar standard is currently being prepared for document image processing systems: draft discussion document (DD206) entitled R ecommendation for the preparation of electronic images (WORM) of documents that may be required as evidence .
In the event of a civil law dispute (as opposed to criminal law cases) actually reaching court, the parties to the dispute are required to give mutual discovery of all documents relevant to the dispute.
This process generally means that the parties agree on the submission of evidence in advance, thereby usually avoiding the need to follow the formalities outlined above.
Indeed, despite the academic controversy that surrounds the problem of interpreting the applicability and nature of section 5 of the Civil Evidence Act, it has given rise to very little directly relevant case law.
In recent years, the provisions of the Civil Evidence Act 1968 have come under increasing criticism for being unnecessary and out-moded for current technology.
The Law Commission is currently considering recommendations to alter the legislation to more accurately reflect the position of computers in modern business.
In computer crime cases, the admissibility of computer evidence is subject to section 69 of the Police and Criminal Evidence Act 1984 and sections 23 and 24 of the Criminal Justice Act 1988.
The Act contains similar requirements to those in civil proceedings.
In a recent House of Lords decision,R v Shephard 16 December 1992, concerning s.69(1) (b) of PACE requiring a statement showing ‘that at all material times the computer was operating properly’, Lord Griffiths stated that the nature of the oral evidence that needed to be provided varied according to the complexity of the computer and its operation.
It was felt that it would rarely be necessary to call a computer expert, since in the majority of cases it would be sufficient to have someone that was familiar with the operation of the computer.
However, the case also stated that the s.69 conditions apply to all computer records, whether real or hearsay:
'…if the prosecution wish to rely on a document produced by a computer they must comply with s.69 in all cases.’
Because of the similarity of wording between s.69 and s.5 of the Civil Evidence Act 1968, it would appear that the courts may hold the same view with regard to civil evidence.
Overall, getting computer records submitted as evidence in court is only the first aspect of the evidential requirements companies need to address when implementing an electronic documentation system; getting the court to accept the contents as adequate proof, or a good record, is an even more important consideration.
Both English civil and criminal evidential statutes contain provisions concerned with how the court should judge the weight, or probative value, of the evidence.
The Civil Evidence Act 1968 states at Section 8 that the court shall make rules regarding the admissibility of hearsay.
These rules have been laid down in RSC Order 38, rules 20–34.
In terms of electronic records, compliance with rule 24 would effectively ensure that the issue of document authenticity had been dealt with pre-trial.
The Act itself simply allows statement to be ‘authenticated in such manner as the court may approve’(s.6(1)).
Where documents are admitted during the discovery process, RSC Order 27 rule 4(1) provides that copies are presumed to be ‘true’ unless authenticity is disputed by the other party.
The Police and Criminal Evidence Act 1984 states that:
‘In estimating the weight, if any, to be attached to a statement regard shall be had to…in particular—
(a)
to the question whether or not the information…was supplied to the relevant computer…contemporaneously with the occurrence or existence of the facts…; and
(b)
to the question whether or not the person concerned with the supply of information…had any incentive to conceal or misrepresent the facts.’
(Schedule 3, at 11).
Section 27 of the Criminal Justice Act 1988, adding to the text in the CEA at s.6(1), states that ‘it is immaterial for the purposes of this subsection how many removes there are between a copy and the original’.
An additional potential issue with regard to the weight (and sometimes even ‘admissibility’) given to computer evidence is the best evidence rule.
This evidential rule states that a party should submit to court the original evidential document rather than a copy.
This could obviously cause problems when information is recorded electronically, since any printout will necessarily be a copy of the original.
The English courts have however held that films, tapes and video recordings are not to be regarded as documents for the purpose of the ‘best evidence’ rule, since ‘they are by their nature reliable’and ‘the old rule is limited and confined to written documents (Kajala v Noble [1982]75 Cr.App.R.149).
This interpretation could be applied to computer records, although some commentators have noted that where a document has undergone processing, as well as recording, it will not apply.
The courts have stated, however, that the best evidence rule is not a requirement where the original is either lost, or ‘its production is physically impossible or extremely inconvenient’(Reed 1989: 652); while generally the judiciary has noted:
‘It is now well-established that any application of the best evidence rule is confined to cases in which it can be shown that the party has the original and could produce it but does not.’
(R v Wayte (1983) 76 Cr App Rep 149)
Therefore, where a document is a copy, the other party could only make significant use of this fact to question the probative value of the evidence if serious questions could be raised regarding the procedures by which the copy was made.
Civil law judicial systems tend to have a much more straightforward evidential system, arising out of the inquisitorial nature of such systems.
The vast majority of such jurisdictions allow all forms of relevant evidence to be admitted, leaving the court with the task of assigning such evidence an appropriate weight, according to the circumstances.
Such differences in approach to computer evidence has been seen by international organisations as a potential obstacle to the adoption of information technology.
In 1985, a UN Commission on International Trade Law (UNCITRAL) report was published, entitled:The legal value of computer records .
It surveyed all Member States, and recorded that:
‘Almost all of the countries that replied to the questionnaire appeared to have legal rules which were at least adequate to permit the use of computer evidence and to permit the court to make the evaluation necessary to determine the proper weight to be given to the data’(A/CN.9/265, p.21).
In 1981 the Council of Europe passed a Recommendation calling for harmonisation of evidential rules with respect to the admissibility of computer records.
The Recommendation calls on Member States to introduce rules for the admissibility of electronic records and limit requirements for the storage of records to a maximum of ten years.
Some countries, such as France and Luxembourg, have revised their evidential rules to remove any unnecessary obstacles to the use of computer records.
10.4.
Data protection
The concept of ‘data protection’ varies between legal systems.
Within West European, the legislation in some countries covers manual records (eg. France and The Netherlands), while others are restricted to computer data.
On the other hand, legal persons, such as companies and trade unions, are protected in the data protection legislation of seven European countries.
While in countries such as the United States and New Zealand, confine data protection concepts to the public sector.
It is also necessary to distinguish data protection from the related, but distinct, areas of ‘privacy’ and data security.
A simple distinction between data protection and ‘privacy’ concerns the use of inaccurate or incomplete information when decision-making, although within the proper scope of data protection, is not necessarily a privacy issue (Lindop Report 1978: para.2.03).
Data security is more closely linked with data protection, being a part of the requirements of adequate data protection, but it also covers issues of computer crime, as well as ensuring that computer systems are protected from physical disasters.
In Europe the primary legal instrument for data protection is the Council of Europe Convention ‘for the Protection of Individuals with regard to Automatic Processing of Personal Data’, 1981.
This Convention attempts to harmonise data protection legislation among the signatory states to remove the need for restrictions on the free flow of data between states on the grounds that personal data is not protected.
The Convention has currently been ratified by 14 European states.
The UK Data Protection Act 1984 arose out of the Council of Europe Convention, and is based upon eight general principles.
These principles are intended to be good practices that data users should comply with in order to protect the data they hold, in both their interests and those of their data subjects.
These principles are fundamental to an understanding of the basis of data protection legislation in Western Europe.
The principles were originally developed in the 1972 Younger Report, which argued that the government should ensure that private sector data users comply with ten principles of good data processing.
Internationally, the data protection principles were adopted as the basis for the 1980 OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data.
The principles contained within the 1984 Act are designed to comply with the principles as stated in the Council of Europe Convention on Data Protection.
Under the UK Act, the Principles are expressed in very general terms and are therefore not directly enforceable through the courts, but only through the actions of the UK Data Protection Registrar (DPR).
Where a data user breaches one of the principles, the Registrar can issue an ‘enforcement notice’, specifying the nature of the breach and outlining the measures that will need to be taken in order to correct the breach.
If the data user fails to comply with the notice, then the Registrar can issue a ‘deregistration notice’.
This notice orders a data user to cease processing personal data immediately.
The Principles are also intended to act as a basis for sectoral codes of practice.
The first principle states:
‘Information to be contained in personal data shall be obtained, and personal data shall be processed, fairly and lawfully’
It would be a breach of the first principle if the data subject, or data provider, were deceived or misled about the purpose for which the data were obtained, held, used or disclosed.
Data protection principle number two specifies that:
‘Personal data shall be held for one or more specified and lawful purposes’.
For example, a contravention of this principle would occur if an organisation were to register the holding of personal data for purposes of personnel management, and use it additionally for marketing purposes.
This principle does not limit the processing of data, it merely requires such activities to be registered, in accordance with the requirements of the Data Protection Act 1984.
The third principle states that:
‘Personal data held for any purpose or purposes shall not be used or disclosed in any manner incompatible with that purpose or purposes’
Disclosure is therefore not restricted as long as the details are made public, within the requirements of the Register.
This principle would be breached if an organisation sold information to a third party after collecting the information purely on the registered basis of an internal purpose of the organisation.
Principle four relates the need to only hold personal data that is ‘adequate, relevant and not excessive in relation to that purpose or those purposes’.
This principle is concerned with the collection of data, often a manual operation carried out separate from the computer process.
It also implies that data users need to continually monitor/audit the data held, to assess changes in relevance etc., a complex and time-consuming process for modern databases, unless accounted for when designing the system.
The fifth principle requires that all personal data ‘shall be accurate and, where necessary, kept up to date’.
If, for example, an organisation purports to keep a list of undischarged bankrupts, but makes no effort to seek information on persons discharging themselves from bankruptcy, it will be contravening this principle.
Opinions that do not claim to be a statement of fact are therefore not covered by this principle.
If information has been obtained from a third party it has to be recorded as such, as well as any challenge to the accuracy of the information by the data subject to which it refers.
If these conditions are met, then the inaccurate data does not breach this principle.
The Registrar has put forward the issues that he may consider when dealing with a potential breach of this principle.
These include, the significance of the inaccuracy; whether reasonable steps where taken by the data user to check the accuracy of information held and what procedures were followed by the data user once the inaccuracy was brought to light (DPR Guideline 4).
Data users will also need to focus on the specific security concerns relating to the identifying data element, ‘which relates the stored data to a certain individual’(Bing 1991: 171).
This component can be viewed as the means of authenticating the data.
The accuracy requirement and degree of protection attached to this element should therefore reflect its unique role.
For example, if identification of an individual is made via an address, then it is critical that this data element is detailed enough to reflect the fact that the location may be divided into separate units
With regard to keeping information ‘up-to-date’, the nature of the information and its purpose, will be relevant to compliance.
For information acting as a historical record, it may not be necessary to carry out periodic examinations to determine if the data require updating; for example, where an evidential/security ‘log’ of all messages sent and received by an electronic messaging system is being maintained.
Principle six states that personal data ‘shall not be kept for longer than is necessary for that purpose or those purposes’.
This principle seems to imply that data should be destroyed when the specified purpose for which they were collected has been achieved.
Such a process will require the same form of periodic review mentioned with regard to Principle 5.
The seventh principle relates to a data subject's right to know ‘at reasonable intervals’ if personal data is held on him and to have access to such data.
The data subject is also given the right to have such data corrected or erased, ‘where appropriate’.
An individual's access rights are central to data protection legislation.
The first necessary stage in the exercise of the ‘right of access’ is to discover the existence of the files, since ‘access’is not a meaningful term unless combined with a knowledge of where the files are.
Two main procedures have been adopted, within European data protection legislation, to achieve the objective of discovering the existence of a file: a public register or a notification procedure.
The former is that contained in the UK Data Protection Act.
The Register is a record of the range of files held by the data user, but does not tell an individual if he/she is included.
Alternatively, a ‘notification’ procedure usually involves the data user informing the subject when a file is created on them (eg. The Netherlands), or upon communication to a third party.
This is very bureaucratic in terms of an organisations administration, but does establish the positive ‘right to be informed’, which is more meaningful than mere ‘access’ rights.
The ‘subject access’ provisions of the Data Protection Act 1984, section 21, came into force on 11 November 1987.
The Home Secretary announced, in early summer 1987, that the maximum fee which data users may charge for subject access is ten pounds, a figure considerably higher than that sought by the Registrar.
Overall, organisations have received many fewer access requests than might have been expected, although this echoes the experience of other European countries.
Public sector data users have generally had considerably more requests than the private sector, which conforms with evidence put forward at the committee stages of the Data Protection Bill that the public sector is viewed as offering the greatest threat to individual liberties.
Statistical and research data [s.33(6)]is exempt from the subject access provisions, but not registration, provided that the data is not disclosed for any other purpose.
The results of such work, when made available, must also be presented in such a form that the data subject can not be identified.
The last principle concerns data security, and requires that ‘appropriate security measures’ are installed by data users against ‘unauthorised access to, or alteration, disclosure or destruction of, personal data and against accidental loss or destruction of personal data’.
Data users are therefore required to maintain the confidentiality of the data, the accuracy of the data, and be able to satisfy the right to access the data and correct it if necessary.
Although a failure to comply with the principle is not directly an offence, it could result in an ‘enforcement notice’ being issued by the Registrar.
The Act quite clearly outlines the range of issues that should be considered when implementing ‘appropriate’ data security.
It states:
‘6.
Regard shall be had —
(a)
to the nature of the personal data and the harm that would result from such access, alteration, disclosure, loss or destruction as are mentioned in this principle; and,
(b)
to the place where the personal data are stored, to security measures programmed into the relevant equipment and to measures taken for ensuring the reliability of staff having access to the data.’
(Schedule 1, Part II)
Data users are therefore expected to consider the sensitivity of the personal data they hold, and implement suitable security procedures.
Such measures involve different aspects: physical security, such as the security of disk storage facilities, from flood as well as unauthorised access; software security, such as maintaining a log of all failed access requests; and, operational security, for example with regard to work data being taken home by employees, and periodic data protection audits of the computer systems.
The Data Protection Registrar has also commented that the mere fact a breach of security has occurred will not be proof that the data user has been negligent, provided the data user has ‘done everything which could reasonably be expected’(DPR Guideline 4).
In July 1990, the European Commission published a draft directive for data protection, as part of the ‘Single Market’ programme.
This proposal will significantly alter UK data protection law.
Since 1990, the proposal has been reviewed by the European Parliament and returned to the Commission for re-drafting.
A new proposal was issued in October 1992, which will now form the basis of a new round of Parliamentary consideration and vigorous lobbying.
In its current form, the Directive, when adopted, will require significant changes in the current UK Data Protection Act 1984 and will have important consequences for the processing of personal data by UK data users, for example:
Article 2 will extend data protection legislation to cover manual filing systems as well as computer data;
Article 7 requires ‘express consent’ before processing can occur;
under the data collection provisions, Article 11, the data user has additional obligations with regard to the information that the data subject must be informed of, such as potential third party recipients of such data;
transborder data flows will not be permitted to countries which do not have an ‘adequate level of protection’.
This could include the US which has no general private sector data protection legislation.
Overall, the Directive is designed to raise the level of protection available to data subjects in the Member States.
The adopted position seems to reflect the current situation in countries such as France and Germany.
The UK government has put forward serious reservations about the scope of the Directive and its potential impact on UK organisations.
10.5.
Copyright
The creation and use of computerised databases has become a key feature of the international information economy.
Such databases are being used in a wide range of contexts; for example, as an international library service (eg. Reuters textline and Butterworth's Lexis); or as a store of specialised technical information within multinational companies, industry groupings or specialised customer/supplier relationships (eg. the CALs initiative of the US Department of Defence).
The accessing of such international databases can be a critical aspect of a company's data communication activities, and indeed its commercial operations.
Under the Copyright Designs and Patents Act 1988, databases can be classified as either ‘literary works’, in the form of a ‘table or compilation’(s.3(1)); or as a ‘cable programme service’(s.7(1)), depending on whether the nature of the service is interactive.
The scope of the copyright protection, however, is not always clear.
Although copyright will exist in the database as a whole, a separate copyright can also exist in the individual components that make up the database; therefore, an infringement could occur in either or both persons rights.
In addition, the copyright monopoly is only given for a certain period of time (eg in the UK, 50 years from the end of the year in which the author dies), but databases are usually continuously updated with new information, therefore does the protection period keep changing?
In the US, the copyright protection of databases is even more uncertain.
A recent US Supreme Court case,Feist Publications Inc v Rural Telephone Service Company , has ruled that a database would only be able to gain copyright protection if it met the copyright standard of ‘originality’ in its mode of functioning; copyright protection would not simply be extended to include items which took effort on the part of the owner to compile.
This differs from the situation in the UK, where the level of originality can be very low, based on the use of some labour and skill [see Ladbroke (Football) Ltd v William Hill (Football) Ltd [1964]1 All E.R.
465, H.L.].
The US case means that the collection of information in many types of US-based databases can not be protected, except through contractual arrangements.
Due to the legal uncertainty created by databases, the European Commission has recently published a proposed directive asserting and harmonising copyright protection of databases.
[2]The proposed Database Directive was adopted in January 1992.
It requires Member States to protect electronic databases under copyright law.
Electronic databases which are copyrightable have the same length of protection that all ‘literary works’ obtain which, in the UK, is the life of the author and 50 years (it should also be noted that there is currently another proposed Directive which will harmonise and extend this period to 70 years).
In addition, the Directive creates a new legal right against the ‘unfair extraction’ of information from databases.
The ‘unfair extraction’ right shall be for a period of 10 years.
This new right is designed to protect databases which are insufficiently ‘original’ to qualify for copyright protection.
The key determination is whether ‘originality’ exists in the selection or arrangement of the database.The current position under UK law would seem to be that a database is protected under copyright law even if it only attains the ‘sweat of the brow’criteria.
Therefore, if the Directive is adopted in its current form it will reduce the available protection for certain electronic databases to the lower 10 years unfair extraction right period.
10.6.
Conclusion
The purpose of this paper has been to review some of the legal issues that organisations need to address when operating in an electronic documentation environment.
To achieve legal security, organisations need to ensure that such requirements are incorporated during the implementation of such systems.
In general, statutory and regulatory record-keeping requirements have been amended to take account of modern technology, However, in specific areas, legal terminology, such as requirements for a ‘written document’, may continue to restrict reliance on electronic systems and create legal uncertainty.
However, where this is not the case, comprehensive record keeping should be an integral aspect of any general information security policy.
Indeed, the use of electronic documentation systems should enhance a organisation's ability to maintain full and accurate records.
Evidential issues are often used, incorrectly, by organisations as an excuse not to adapt to new technologies, or to continue to maintain paper back-up of such documents.
Such continued reliance on paper will represent an increasing opportunity cost to organisations.
As user experience matures, possibly assisted by assurances from judicial decisions, then the desire to maintain a paper back-up can be expected to recede.
Data protection legislation already imposes important obligations upon data users, and such obligations are likely to become more onerous once the EC directive is adopted.
However, the directive is likely to remove any real distinction between personal data held on paper and on electronic systems.
Finally, copyright protection of electronic databases was considered, since legal protection for such databases is of critical importance for the continued growth of this market.
From a UK perspective, however, the concern is that EC proposals will reduce the protection afforded to such collections of information.
12.
Electronic Information Resources and Historians: A Consumer's View
Daniel Greenstein, Lecturer in Modern History and Arts Technological Resource Officer, Glasgow University
Abstract:
For the historian machine-readable information must be readily usable with a minimum of re-editing.
Consequently, process-independent encoding standards are absolutely essential.
So is software that can read and write standard-conformant data.
Do historians have anything to offer the software developers?
Certainly there is little doubt that the technologies which can manage the ambiguous and variously interpreted data of history are likely to be highly transferable ones.
Standardisation needs to take place at another level as well; this time amongst computer-using historians who are willing to attempt collective decisions about what information should be encoded in a machine-readable document and about how encoding decisions should be recorded for posterity and secondary analyses.
Finally, we need to ensure that the creators of machine-readable data upon whom we will increasingly rely adopt standard practices.
Cross-fertilisation with publishers, government printing offices, and the computer industry is essential but likely to be ineffective until the historical profession gets its own house in order and begins to include its computer-literate members fully into the fold.
At present, the acquisition, development, and dissemination of computer methods and the creation of useful machine-readable data are not given anywhere the same credit or standing as traditional forms of research and publication despite the fact that the latter have no greater claim on scholarship and far less a claim on relevance.
As ever, historians have much to offer the modern technical society in which they find themselves and still more to do in order to catch up with it.
12.1.
Introduction
Historians don't so much develop computational solutions as adapt and apply those solutions developed elsewhere.
Consequently, if historical computing is a discipline in its own right, it is a very eclectic one.
The historian compiling a critical edition of Burnett, for example, requires text critical apparatus and thus tends to rely on software tools and computational methods which are on the whole developed by and for literary scholars.
Those very few of us who are interested in linguistic content analysis also borrow shamelessly from our colleagues in literary studies and likewise from the computational linguists.
The vast majority of computer literate historians are far less adventuresome and stick more closely to techniques taken from our cousins in the social and behavioural science.
Most notable amongst them are methods of database management and statistical processing.
But our needs hardly end at this classic trinity of processing aims: document preparation, linguistic content analysis, and categorical analysis.
We rely increasingly for data exploration and delivery upon international networks, online data services, and hypertext and multimedia systems.
Needless to say, none of these systems were developed by historians or with the historical user community particularly in view.
Given our truly catholic approach to computing and our reliance upon methods developed elsewhere, historians may be unique among other computer-using humanities scholars.
Our computational needs are distinctive owing to their extent and not, as some might claim, to the nature of historical data or its analysis.
Why, then, are historians so adamant in developing their computational methods and resources in near complete isolation from other communities with which they have so much in common and from which they have borrowed so extensively?
This ‘splendid isolation’ was explicable twenty or even ten years ago when the field of humanities computing was so wide open and the methods appropriate to historical teaching and research so poorly developed.
The present climate is very different, however.
High-speed computer networks and the relative cost of producing machine-readable information forced isolated communities and computer platforms to talk to one another; representatives from Apple and IBM have even been seen together around the same table.
Yet historians have yet to respond to changing circumstances.
Consequently, it is impossible to address historians’ needs with respect to electronic information resources without being concerned over their relative seclusion and backwardness.
We are never going to take control over the direction of IT developments.
We can, however, have a much greater influence than at present over how such developments impinge upon professional practices.
We can develop a proactive rather than a simply reactive approach to the development of IT resources.
To do this, however, we need to alter fundamentally the ghetto mentality which has emerged amongst the computer-literate members of our profession and seek collaboration with other humanities scholars.
Three areas in which collaborative ventures should be developed as a matter of some urgency are discussed here: the development of interchange standards, training, and institutional support.
12.2.
Interchange Standards
First and foremost we need to recognize that individual historians will have radically different processing requirements for the same body of machine-readable data.
Imagine what might be done with the collected works of Alexander Hamilton when the Founding Fathers project eventually provides them on CD-ROM sometime in the not too distant future.
Linguistic content analysis will help illuminate debates about the extent of Hamilton's nationalism and anti-democratic principals.
Sophisticated database-style analyses will reveal the kinship and patronage networks in which Hamilton was involved.
A text critical edition replete with learned commentary will be developed and become a valuable reference tool.
A multi-media system designed to instruct students in aspects of American history will be built by integrating a selection of Hamilton's works with other literary, visual, and audio materials.
In an ideal world, the one body of machine-readable data would support these four very different processing aims.
The text processing software used for the linguistic content analysis would view the data as running prose interrupted with a heading and some rudimentary bibliographic material wherever a new item in the corpus began.
It might also recognize previously marked passages which dealt with nationalistic themes or with the nature of government.
The database would see the same data differently — as a series of database tables comprising highly structured social profiles of the people with whom Hamilton came into contact and about whom he wrote.
The desktop publishing system used by the text critical scholar would have yet another view of the same data.
The corpus would appear as a series of pages interrupted periodically by a heading and extensive bibliographic information wherever a new item in the corpus began.
The package would also recognize footnotes (perhaps nested to several levels) and support variant readings of those texts which were originally found in heavily edited manuscript form.
Finally, the multimedia system would have an altogether different view, this time of a body of text embedded throughout with handles linking text segments to one another and to a range of externally stored texts, images, and recordings.
Alas, this is not the ideal world.
The works of Hamilton which eventually appear on CD-ROM will be open to basic text retrieval but that will be about all.
Each of the projects outlined above would have to download the data and comprehensively re-edit them before further processing could get underway.
In the case of the database project, it is doubtful that the machine-readable data would be of any use whatsoever.
None of this is meant as a criticism of the Founding Fathers’ project.
On the contrary, the project will produce valuable reference material and should be lauded for its novelty and foresight.
The example is chosen simply to demonstrate how sadly lacking we are in the very facilities which historians require most — mechanisms of exchanging data between different processing environments.
In a forthcoming contribution to Computing for the Humanities , Burnard and Greenstein use a seventeenth-century sasine or record of landholding and transfer, to demonstrate mechanisms which furnish these very facilities.
The problem, we argue, is that hitherto, data processing aims have determined how information is modelled and managed in machine-readable form.
Consequently, data which are collected for one particular analysis or processing application are unsuitable where they are required elsewhere for a different purpose.
By devising and adopting standards for encoding machine-readable information, data exchange can take place independently of processing considerations.
Such a standard they claim, has been developed by the Text Encoding Initiative (TEI) which has provided guidelines and extensive documentation for the use of the markup language known as SGML in the creation of machine-readable text (Burnard 1993).
The standard that they advocate is not a prescriptive one.
It does not tell historians what to encode in a given source and thus impinge upon interpretation.
Rather, it provides a set of very flexible solutions which enable historians to encode whatever they will, but in a manner which can be made generally recognizable.
The importance to historians of encoding standards that will facilitate data exchange cannot be overstated.
Their impact on our data creation practices is likely to be minimal.
Anyone who has attempted to compile machine-readable data either for teaching or research purposes will agree that such an undertaking impinges so greatly on our time and resources as to minimize our interest in providing support for other scholars who might become interested in our data.
It will, I suspect, be rather a long time before historians adopt SGML-authoring software as their own.
At the same time we need to recognize that machine-readable data created elsewhere will increasingly become the stock and trade of our research.
The history of the late-twentieth century, for example, will rely very heavily indeed on machine-readable information.
It is an all too sobering thought that in 1993 we have not developed the mechanisms which will enable us to index, search, and analyze those data according to our very many and particular research interests.
Is it too much to ask that in 2023, the historian confronting a machine-readable run of the Financial Times (FT), 1993–2003 should be able to perform serial calculations with the stock prices quoted therein without having to re-enter or extensively edit the data?
Our interest in standard mechanisms for data encoding and exchange need not be driven, then, by altruistic considerations about the value of secondary analysis.
It should be driven instead by our recognition that we have a vested interest in ensuring that the vast quantities of data that are currently being produced by government, publishing houses, banks and information services can support our many and varied processing aims.
How can we support the development of encoding standards which will facilitate data exchange?
Firstly, by abandoning the notion that only historians can model and manage data in a way which facilitates their particular data processing aims.
Figuratively (literally in an increasing number of cases), machine-readable information is inherently in the public domain.
The electronic collection of Alexander Hamilton's works will hold out as much interest to the linguist, philosopher, and the political scientist as it does to the historian.
In a similar vein, the level of interest in machine-readable copies of the FT or of any number of government records isn't solely restricted to the historical community.
To secure our access to information which will not necessarily be created with the historical community in mind, we must be willing to develop the common ground we share with other scholars and computer-using communities.
Only this will ensure that in future all machine-readable data can be exchanged across disciplinary and professional boundaries.
Of course we cannot expect linguists, text critics or publishers to explain to us how best to interpret, mark and process those classically ambiguous phrases which are now the stock and trade of historical data modelling exercises.
Only an historically trained scholar will be able to identify the likely internal structure of the name ‘Johnny Turner’, of the ambiguous temporal expression ‘a few days before the fair’, or of the relative place name ‘five minutes away from Foorler's house’.
This does not mean that historians are alone faced with the problem of disambiguation.
Anyone who is unconvinced of this is recommended to speak with a linguist about how to mark and interpret the expression ‘wash sinks’.
Historians, too, must determine how to classify their data for analytical purposes.
I for one would not trust a lexicographer or philologist to determine whether the socio-economic profiles of eighteenth-century Scottish immigrants to the American colonies should be represented in social class terms or as indices of their pre- or post-industrial occupations.
Once again, however, the process of coding data whose meaning are uncertain, disputed, or theoretically contingent is not unique to the historical profession.
Here, the non-believer is recommended to discuss with a literary scholar the identification of noun phrases in the works of Charles Dickens.
Finally, we should recognize that ours is not the only computer-using community which has felt constrained by industry standard database packages and markup languages which simply could not cater for our dates, currency measures, and time scales.
The development of object-oriented processing techniques reflects the fact that such constraints were widespread.
Why, then, are some historians so busily proliferating new historically specific ‘data types’ by implementing them in proprietary software while the computer-literate researchers in disciplines are giving generic definition to their eccentric datum so they may be implemented on any platform which permits user-defined objects?
Again, our focus is too narrow.
We need to recognize that at some level, the processes of computer-aided historical research which involve the application of expert or subject-specific knowledge — are explicable in generic terms that can be made accessible to non-specialists.
If the machine-readable data produced by other users are ever to become accessible to historians, and that produced by historians to other users, we need to begin thinking about the research processes in such generic terms.
Having adopted this generalists’ view of data processing and exchange it might be prudent to secure our involvement in the development, testing and adoption of encoding standards.
Only by making our processing needs known can we expect them to be catered for by the computer and data creation industries.
Whatever the ultimate fate of the Text Encoding Initiative, it does seem as good a place to start as any in making our needs felt.
It is not an alien commercial body.
On the contrary, it is a consortium of humanities scholars the likes of which we need anyway to establish more and better contact with.
It has also produced very comprehensive guidelines about to appear in their third edition.
Thus, our participation in a rather advanced project minimizes the danger of our reproducing work already conducted elsewhere.
At present, the Guidelines need to be reviewed by historians.
They need to be read and commented upon by a wide audience.
The Guidelines also require refinement and elaboration through application to real projects, including historical ones.
It is also worth considering whether historians have a role to play in developing SGML-aware software and/or conversion utilities which will enable us to fulfil our favourite processing aims while reading and writing TEI-conformant files.
In sum, it is probably time to overcome the politics of standardization —‘standards are good so long as you adopt mine’.
It is also worth overcoming an approach to computer-aided historical research which assumes that the complexities inherent in disambiguating and interpreting fuzzy information and of representing idiosyncratic data types are at all unique to history.
Overcoming this subject-centred narrowness will enable historians to contribute immeasurably to standards definition and thus to have considerable influence over how large-scale datasets are created in future.
12.3.
Training
More and better training also requires a priority place on this wish list.
There is not much point in delivering information in a manageable and flexible format if the skills required to perform even the most trivial tasks with those data are not widely available.
But who is to provide this training and of what should it consist?
A recent international meeting of computer-literate historians addressed these very questions and resolved to prepare a curriculum of instruction in computational methods and techniques which could be adopted at any number of institutions (Davis,et.al. 1993).
This ‘core’ curriculum will make a useful and important contribution, but its elaboration pointed to trends which are as troublesome as the perception of a currently inadequate training provision.
Firstly, who should deliver the training?
Should it come from humanities computing or computing science departments providing service teaching to the histories, or from the history departments themselves?
Different universities have opted for one or other of these strategies with very interesting ramifications.
Where training is provided by humanities computing or computing science departments, there are pressures to dress it up in formal scientific terms so as to legitimize it in the eyes of the surrounding scientific community.
This is no easy task when we consider that what is required is some competence in a range of applications software and a degree of sensitivity to the data modelling and analytical problems that historical source material and historical research involve.
The alternative strategy — historians providing computer training from within their own departments — is no less problematic.
Firstly, in this constrained economic climate, it is difficult to justify an expert in Italian Renaissance cities teaching students how to use a spreadsheet.
Moreover, historians who are teaching computer methods are under the same pressures as their counterparts in humanities computing departments to justify their activities.
But while the humanities computing people dress up computer training in terms of bits and bytes, historians mystify it by talking about fuzzy data, source criticism, and subjectivity.
The irony of course, is that people involved in what is patently the same activity, are increasingly pulling apart from one another as they pursue their own institutional and professional goals by drawing boundaries around the same very narrow (possibly non-existent) patch.
In the event, historians’ needs and thus the role that the computer should play in the classroom are overlooked.
Indeed, defining the computers’ role in the classroom was a second problematic area encountered by the international meeting.
A large majority agreed a central role for the computer and this underpinned the applications and methods approach ultimately adopted in the core curriculum.
From this group emerged a curriculum made up of various modules, for example about document preparation and text processing, database management and data modelling, and graphical and statistical analysis.
A small rump felt that computers might be employed to deliver information otherwise inaccessible to history students.
In this guise, the computer's role was to elaborate some of the issues which emerged in a course in which historical themes, not computer methods were the central focus.
Thus, a student studying eighteenth-century American history might be advised to query a database of American trade figures or perform basic content analysis on the Federalist Papers.
In this guise, the computer's role was to deliver machine-readable information that would supplement that normally found by students in printed form or received by them in the shape of lectures and seminars.
Undoubtedly these rather different agenda reflected to a certain extent divisions between the needs of teachers operating out of humanities computing and history departments, respectively.
As significantly, however, they reflected very different conceptions of what training historians require.
The strategy which envisages the computer as a sophisticated delivery system for information similar in fundamental respects to the library, is ideal for undergraduates.
Historical content remains the principal staple of their diet.
The applications and methods course, on the other hand, seems more closely tailored to the needs of postgraduate history students.
This already small and still dwindling breed needs to acquire the comprehensive training that such a course provides.
But should we assume the same training is required by the lion's share of our undergraduates who are not destined for academic careers?
Surely, the answer is no.
The bulk of our students require computer training which they can use to advantage at interview or in post with an accounting firm, commercial venture, publishing house, or wherever else they end up.
In crassly materialistic terms, our students are unlikely to benefit from a training that rests on the principal that historical computing is distinctive owing to its scientific properties (as advocated by humanities computing departments), or its especial role in subjective historical analysis (as advocated by history departments).
Hopefully what will emerge from the tensions which are apparent in the community interested in providing historians with computer training is a tripartite approach.
Some training in computer methods and applications should be required for all university students irrespective of their subject, and this should be provided by a service teaching department.
There is no reason that such a department cannot make the training accessible and relevant to liberal arts and science students alike.
At the rather more advanced end, our small community of postgraduate history students requires training in the computer methods and applications that are appropriate to historical research.
The departmental affiliation of such a course doesn't much matter provided that it is taught by people with relevant research experience.
Why not let local circumstances and relative departmental strengths be determinate?
In addition, let us encourage colleagues to integrate machine-readable data and computer exercises into their thematic courses.
Having filled their computer methods requirements outlined above, students will neither shrink from the challenge that such components hold forth nor demand rudimentary applications training from their history teachers.
On the contrary, the use of such materials will consolidate and extend our students’ familiarity with computational techniques while introducing them to a relatively new and important range of historical source material.
Having settled the tripartite and integrated structure of our students’ training needs, we should perhaps consider how best to provide for them.
Training in advanced methods is already reasonably well catered for.
There are any number of examples in Britain, the rest of Europe, and the United States, and the core curriculum developed recently is likely to consolidate and extend existing initiatives.
There are numerous examples too of the more general training that might be offered by humanities computing departments.
The computing component of the thematic history course is more problematic.
At present, the development of such components relies heavily on individual initiatives taken by historians whose own research involves some element of computation.
Their datasets are familiar and close at hand and, in some circumstances, can be tailored for student use perhaps with the aid of self-instructional documentation or workbooks.
The present situation is unacceptable if we consider this strand of computer-aided teaching at all important.
At least, if the resources in question were printed ones we would not consider it at all acceptable for teachers to be compiling course bibliographies solely from their own publications.
University and college libraries enable the enquiring mind to range freely over a vast array of literature.
Isn't it time to consider the creation of a similar library where the data of computer-aided history are available for similarly free-ranging enquiry?
Clearly the Teaching and Learning Training Project (TLTP) projects which are currently underway in the UK point a hopeful direction.
I wonder whether our data archivists might be encouraged to lend support as well by providing basic instructional materials for their most frequently-requested datasets.
Training and support for history teachers is as important as the provision of packaged datasets because it will enable the teachers to develop their own materials.
There are other reasons to support initiatives for using the computer as a means of delivering subject-specific information.
If the present deliberation of the UK's Follett Committee are to be taken at all seriously, the research and teaching library as we now know it will change fundamentally over the next ten years.
An increasing number of journals, monographs, and reference materials will only be available in machine-readable form as high speed computer networks enable hard-pressed university institutions to curtail drastically the escalating cost of their paper library provision.
What delivery systems will be used by university libraries in the twenty-first century is not settled.
It is certain, however, that by developing new ways of integrating machine-readable data into their thematic courses, historians will be contributing to (and thus perhaps influencing) a wider trend which will have a lasting impact on how we conduct teaching and research within our discipline.
In considering our training needs in the present, then, we need to anticipate the future, and here, too, there is every reason to encourage historians to abandon a rather narrow special-interest mentality.
Those few students who require a highly specialized training in computer-aided research skills, can and should be taught by historians though the provision need not be available at every single university.
The rest of our students need a general computer training organized perhaps especially for humanities students.
There are also financial considerations.
By organizing computer training for liberal arts students generally, significant economies of scale can be achieved.
Finally, the use of machine-readable information in the traditional subject-specific theme course needs to be considered but once again, in a general, not in a subject-specific way.
In future an increasing amount of information will only be available to history students and teachers in machine-readable form.
To ensure the best and most appropriate access to that information, historians might be advised once again to pursue collaborative initiatives with scholars in other disciplines who will also rely as heavily upon such materials.
12.4.
Professional support
Finally, it is worth considering how we can achieve some of the aims outlined above.
Firstly, we need to stop paying lip-service to the importance of IT in history and other humanities disciplines and do something to actively take the lead in promoting its development.
Here in the UK there is an impressive array of humanities scholarship which actively employs computers in innovative and fruitful ways.
As is admitted in Information Technology in Humanities Scholarships , a report co-sponsored by the British Library and the British Academy, however, nearly all of that research is conducted on a shoestring (1993: 28).
The intention here is not to turn this essay, or indeed the panel to which it contributes, into yet another moaning session about the state of British higher education.
Rather, it is to indicate that in this particular instance, underfunding is an accurate reflection of the second-class status that computer-assisted humanities research and teaching have in this country.
Why should the government or industry consider funding the activities of scholars who are marginalized by their own professions?
This is neither a bold nor an embittered question.
It is merely one based upon observation.
In the UK anyway, the published products of historical scholarship, monographs and articles in learned journals, are still the principal criteria upon which promotion and professional recognition are meted out within the humanities.
Other contributions which absorb as much in the way of time and intellectual effort — new methodological or computational techniques, datasets which have some general usefulness — are all but discounted.
In the recent research selectivity exercise, major contributions to computer methods and the creation of publicly available datasets which emanated from within history departments across the country, counted for little if anything.
I have as much scepticism as the next person about new techniques for analysing so-called discourse communities which seem to do little else than parse the text of student e-mail with an home-grown SNOBOL program.
I am equally unhappy about those two well tried pieces: ‘What I did with my Database on my Summer Vacation’, and ‘Pedagogy and Paradox: Teaching, Learning, and the Computer’.
At the same time, I am concerned that genuinely valuable contributions be rewarded in the normal way.
When the British Academy gives scholarships for methodological research and applications as well as for historical inquiries of a more familiar kind, when universities begin to make appointments in humanities computing or, dare I say it, even in history and computing, then, it seems to me, we will be in a better position to bemoan our inability to secure more in the way of government and private funding.
At that time we will be able to say that funding agencies are failing to recognize as important something that we ourselves have embraced.
Altering the attitude of a profession,(a culture?) which is so intent on defining a boundary between the pure and the applied, the liberal and the vocational, is the real challenge which faces IT-aware humanities scholars in Britain.
There may, however, be some short-term aims that are worth pursuing; perhaps the British Academy can be enlisted to help.
There are too few conduits between the methodologically sophisticated humanities scholar and potential sources of funding.
These must be developed.
Take two examples.
If they know nothing else, humanists know texts.
The problems they encounter in creating and interpreting them are as sophisticated and as complex as they come.
Accordingly, where humanities scholars have engaged the computer in text creation and analysis, they have done so in a sophisticated way.
Are humanities scholars alone interested in applying computer technologies to texts?
I think not.
The British National Corpus (BNC) represents fruitful cross-fertilization with the publishing industry.
There must be other joint initiatives with publishers as well as with the media and the telecommunications industries which also might prove mutually beneficial.
Similarly, one could argue that humanities scholars understand teaching.
Consequently, they might make a significant contribution to the development of computer-aided instructional materials and delivery systems for machine-readable information which will be found increasingly in the commercial as well as in the academic worlds.
I am certain that with no additional resource, we can do more for our researchers than provide then with yet another data archive where they can deposit the materials which they developed with inadequate support and for which they got too little recognition.
All that is left is a question about how we might open up these and other conduits to more and better funding.
Firstly, form a cartel in the best eighteenth-century mercantile tradition.
This will not be a shipping cartel, but an agreement amongst humanities scholars to develop IT applications generally and by putting an end to territorial squabbling over who owns what encoding, interpretative or processing problems.
Secondly, by bolstering our corporate identity through recognition within our universities and our established professional associations.
We need not be apologetic any longer about our expertise.
It is very much in demand and if the MacFarlane and Follett committees are to believed, that demand is not likely to slacken.
Thirdly, having overcome our sectional tensions and established a corporate identity we can perhaps enlist the support of our universities and our professional associations to establish more and better access to the funding which in relation to the sciences, has historically been denied to us.
Part III.
Archives and LibrariesArchives and Libraries
Introduction
Archives and Libraries
The rise of the acronym is perhaps one of the most important, if regrettable, linguistic phenomena of the modern age and their metamorphoses tell us a considerable amount about expert discourses.
A case in point is the replacement of ADP (Advanced Data Processing) by IT (Information Technology).
This reflects the expansion of the use of computers from mass storage and number crunching to take in all forms of human communication.
With the advent of information networks we are now entering the world of IM (Information Management).
This process affects information professionals, such as librarians and archivists, at a fundamental level.
Not only do new computer tools present them with challenges and opportunities but the very material they store and make available to the public is undergoing radical change as more and more records are produced solely in electronic form.
It is frequently said that like changes have not been seen since the invention of the printed book but a good argument can be made for comparing the IT revolution with the invention of writing itself.
After all, electronic communications has many similarities to the oral technologies which pre-dated the written word.
As many of the papers in this section reveal, the new technologies are providing information specialists with the ability to improve access to collections for academic research, especially via networks, to enhance the quality of texts, and to develop more powerful means of analysis.
It is even suggested that an homogeneous technological base will erase the distinctions between archives, libraries and museums (Rayward 1993).
I have my doubts about the latter vision since archivists, librarians and museum creators deal with materials which are differentiated by the type of activities which created them, rather than simply by the physical form of those artefacts.
Distinct packages of skills and approaches to work, may, however, survive but perhaps in new institutional settings.
It would be useful to further explore some themes raised by the contributions to this section, First, in an age of constrained budgets, how do national institutions afford the infrastructure in order to take advantage of these new opportunities?
Does something else have to be sacrificed, and if so, what?
It should also be noted that most of the institutions represented here are national or leading research bodies.
Will smaller institutions, such as county, provincial, and private archives and libraries be able to participate in the Information Revolution?
Will they become backwaters undertaking work which is qualitatively different from that done in the larger centres?
Secondly, an assumption is being made that the new networks, via which ‘virtual’ libraries and archives will be created, will be perfectly transparent and impediment-free structures created by mutual consent.
One senses here an underlying analogy with the ‘invisible hand’ of perfect competition.
But existing markets are not ‘perfect’ and operate under institutional structures enforceable by law.
How, for example, will the desire for free access to data be squared with the increasing institutional imperatives to make money from information resources?
Can one have perfect knowledge in a world of monopolistic organisations?
Thirdly, will the Brave New World of free text searching be a boon or a catastrophe?
Many scholars expect that in the future everything will be saved from information systems, and access will be provided via networks by some form of query language.
But if researchers home in on the record as the first level of access, ignoring the surrounding administrative context and archival structure which forms part of its meaning, will understanding be fostered or impaired?
It should also be noted that the discussions here revolve around the question of access for a comparatively restricted, specialist research community.
If networks develop which are generally available to the public at large, what impact will this have on information providers?
Will emphasis shift from the provision of services with a high value-added component to the bulk handling of a large number of fairly simple enquiries?
This after all has been the experience of traditional paper archives and libraries in the recent past.
As the following papers emphasize, the electronic record has now to be considered as an informational object to be archived for future research use as well as a working tool.
Perhaps ‘object’ is a misleading term since it is increasingly plain that electronic corruption and software/hardware obsolescence mean that one will not be able to preserve the original electronic text or data.
We will have to maintain material in standard, technology independent forms, as ‘logical’ rather than‘physical’records.
The informational specialists of the future will probably be Platonists rather than Aristotelians!
Surprisingly it appears to be the librarians in this collection, rather than the archivists, who are more concerned with preserving the original tape or disk on which electronic information is deposited.
Perhaps archivists are used to seeing records as merely the physical residues of communicative acts in the past whilst librarians have a greater veneration of the Book as a sacred text.
The archivists are more concerned with the processes of acquiring and documenting data than in preserving computer systems.
On the other hand, it is the librarians who are more conscious of the multiplicity of types of information with which the Information Revolution presents us.
The archivists in this collection are predominantly from institutions which hold electronic data, usually created via official social surveys or opinion polls.
Archives in Europe seem far less concerned with the issues surrounding the preservation of the products of electronic office systems and e-mail which so exercise their counterparts in North America (for a discussion of some of this literature see Cook: 1991–2 and Higgs: 1992; see also discussion in creators section of this volume).
This has important implications for the future role of the archivist.
He or she will be less the guardian of a vault than an information manager ensuring that archival principles are written into automated office systems at the design stage.
One senses here institutions coming to terms with maturing technologies whilst awaiting, with some unease, the next wave of change.
Plainly no branch of the information professions or single institution can provide all the solutions to the challenges presented.
The future, therefore, lies in intellectual co-operation and communication as much as in network infrastructure.
13.
The British Library and the Challenge of Electronic Media: a View from the Perspective of Special Collections
Alice Prochaska, Director of Special Collections, The British Library
Abstract:
The British Library is accustomed to the challenges of preserving information in many media.
But electronic media present difficulties of a new order of magnitude, because of the high costs involved, as well as the volatility of much electronic material, both physically and in its textual content.
The Library is becoming a producer of electronic materials on a large scale in its own right.
In collection building, it is working carefully with publishers to secure the deposit of electronic materials; and it is exploring possible revisions to the law of ‘copyright’ deposit.
In the case of archival acquisitions, the Library aims to acquire material of value to the ‘national heritage’(a definition subject to continual review), in electronic as well as other forms.
13.1.
Introduction
The British Library is devoting a very considerable proportion of its resources during the remaining years of this century to the automated provision of information.
The challenge of the electronic media informs much of its Strategic objectives for the year 2000 (May 1993).
From the enhanced provision of electronic delivery by the Document Supply Centre to a huge programme for retrospectively converting music, map and manuscript catalogues which have been created over a period of more than one hundred and fifty years, to the question of how to extend the provisions of the Copyright Act 1911 to include legal deposit of digital publications, the Library faces a formidable set of tasks arising from the electronic revolution.
Much investment has borne fruit already: in the publication by Chadwyck-Healey of the British Library Catalogue on CD-ROM, in the electronic output (including CD-ROM publications) of the National Bibliographic Service, in the publication by Bowker-Saur of the retroconverted Catalogue of Printed Music , and so on.
In the autumn of 1993, our Online Public Access Catalogue (OPAC) will be mounted on JANET,(and thus eventually on super-JANET) and more specialised catalogues are likely to follow in due course.
Our directorate of Computing and Telecommunications is establishing British Library Network Services in pursuit of the ambitious ultimate aim ‘to be able to supply to the remote user text, sound or visual images from all parts of the collection…
’ The commitment of the Library's Research and Development Department to Information Technology is illustrated both by the present seminar and by the recent publication of its R&D Report no. 6097,Information Technology in Humanities Scholarship , which is also a cooperative venture with the British Academy.
In all of this, the Library recognises that it can only advance in the closest possible cooperation and consultation with those organisations, most notably university departments and university libraries, which have already set in place the academic and technological infrastructure to which we hope to make an increasingly useful contribution.
The British Library is now gradually coming face to face also with another side of the electronic revolution: the question of how to acquire, preserve and make available unpublished research materials which happen to have been produced in electronic form.
The National Sound Archive, which is part of the directorate of Humanities and Social Sciences, is a special case, much involved in most of the issues raised here, but I propose to leave the question of recorded sound to one side for the purposes of this paper, and to concentrate on written and graphic sources for the historian.
My own area of responsibility as Director of Special Collections covers the Manuscript Collections, Map and Music Libraries, the Oriental and India Office Collections, and the Philatelic Collections.
All of these are treasure houses for the historian.
Together they form the main, though not the only area within the Library where the selection and purchase of material tends to be governed by considerations of the ‘National Heritage’.
(And here, I hasten to add, we are of course one among the many collectors of ‘heritage’ material, who also include a large number of university libraries, archives and manuscript collections.)
Large-scale purchases by the British Library as well as others are often supported by grants from the National Heritage Memorial Fund or other bodies, and can involve protracted discussion of their research and monetary value, before they can be acquired.
Similarly, when individuals or organisations offer us extensive personal collections or archives, some thought must be given to their research potential and their contribution to the national heritage profile of the collections, in relation to the Library's capacity to store, preserve, catalogue and make available the material in question.
It is to the connected questions of developing and making available our unpublished collections, and dealing with the problem of what constitutes a published source, that I propose to devote most of this paper.
With the exception of some early state papers in private collections and most notably, the very extensive India Office records, which effectively ceased to be produced in 1948, the Special Collections of the British Library are for the most part private in their origin and unpredictable in the manner and timing of their acquisition.
We are not, therefore, in the position of being able to foresee or systematically to plan for large accessions of unpublished, electronically generated material.
Nor do we face, in the way that government repositories do, the problems of how and whether to cope with vast bodies of statistical material.
We do not have to decide whether to keep in bulk or alternatively to sample, particular instance papers which electronic data handling techniques now make potentially useful to the historian in ways that they simply were not before.
The issues raised by handling census and other survey data as described by several speakers at this seminar are not so central to the management of our collections.
A fundamental question in this area which does concern curators and readers at the British Library involves the nature of the text itself.
Digital technology has introduced (or perhaps I should say that it has reintroduced, for the first time since the establishment and dominance of printing) the unstable, or infinitely variable, text.
I will offer you a few examples of what this means for the British Library, in particular in its role as the provider of sources for historical research.
13.2.
Digitised publication as a replacement for print: the case of the Ordnance Survey
One issue which has yet to be resolved is that of the provision of large-scale Ordnance Survey mapping.
In 1997, the Ordnance Survey will cease to produce its large scale mapping in analogue form, and will move to wholly digitised output.
Having become an Executive Agency of the government, it is under an obligation to cover 100% of its costs through sales of data and services, by 1995.
Most of its cost recovery will be achieved by providing to customers such as surveyors, planners, property developers and the like a vastly enhanced and continuously up-dated digital database.
This huge and exciting advance in the degree of up-to-date detail which Ordnance Survey can provide to its main customers presents difficulties for the copyright libraries.
At present, our Map Library receives new microfiche editions of Ordnance Survey mapping covering the whole of the United Kingdom, under the provisions of the Copyright Act 1911.
We receive it free of charge, but pay a substantial sum for jacketing the fiche.
From 1997 onwards, as matters stand at present, we would have to pay probably well over ten times that cost in order to receive the equivalent digitised output.
We also have to take account of capital investment in the necessary equipment and extra staff, and of the commercial interests of Ordnance Survey, which might well wish to have a say in regulating the way in which the material is used in our reading rooms.
Here is a case where digitisation, which has so much to offer the historian, could actually lead to a deterioration in the nature of the source material available.
At present the historical geographer, the cartographic historian, and the local historian, not to mention solicitors and property lawyers, and a host of others who need large-scale mapping from different dates, use the British Library Map Library as the leading source for the national topographic record.
(The other copyright libraries in the UK provide a similar service, and are similarly affected.)
It is of course essential to our readers' work to be able to compare changing patterns of land use over time, and often to be able to pinpoint a particular feature for various dates during, perhaps, more than a century of the topographic record.
The same will be true of readers in two hundred years' time, and before and after.
How are we to ensure that the comparative information they need over time will be available to them, on the same comprehensive basis as before?
If a reader in the year 2040, let us say, needs to see large-scale mapping of the villages of the Yorkshire Dales covering the period 1995–2040, he/she could find that that record has not been preserved in any library.
Suppose that what is wanted is large-scale mapping of the city of Leeds, it is likely that snapshots of Ordnance Survey mapping will have been preserved far more frequently, for the practical purposes of town planners etc., but they will have been preserved not in libraries but in planning offices, and will they have been preserved after the date when they were needed there?
How would a historian manage to get access, or even begin to find out where they were kept?
Suppose, to take another hypothetical example, that a historian in the year 2100 is undertaking a study of that by then obsolete and quaint custom of preserving public footpaths across private land; and he/she wishes to compare the strength of the ramblers’ lobby during the twenty-first century in the south west and the north east of England.
How can we ensure that the data survives that will make these questions answerable?
There is no easy answer.
Given the good will that exists between the copyright libraries, the Ordnance Survey and other interested parties such as the Public Record Office (whose concern is with the digitised database as a public record) a solution will almost certainly be found.
It may involve compromises, for instance over the frequency with which comprehensive large-scale mapping data is preserved.
We may collectively need to invoke the legal requirements of property law and of such measures as the Environmental Protection Act (1990), in order to persuade government that some residual legal responsibilities adhering to the function of the Ordnance Survey require support.
Almost certainly, workable solutions which take into account the very disparate ways in which people use the original text will call for an interdepartmental committee to sort them out.
This in itself is a measure of the scale of the problem.
The basic difficulty derives from the cost of the technology.
Associated with cost is the gap that has developed between the ways in which essentially the same text is used by different groups of people.
In order to place great benefits of currency and flexibility in the hands of most contemporary users, there is a risk of damaging its usefulness to posterity, and marginalising the unfunded, non-commercial users.
From the point of view of the copyright libraries including ourselves, the case of the Ordnance Survey is also related to the thorny question of maintaining our collections in an era when digital publication is beginning to replace the printed page.
In the British Library we are considering very actively the whole issue of whether, with our fellow copyright libraries, to seek the statutory extension of legal deposit to non-print materials.
Associated with it is the fundamental question of how to define ‘publication’.
And this of course brings us back to the practical and philosophical implications of the unstable text.
13.3.
Digitisation and the enhancement of available sources
If the comprehensive store of published knowledge represented by the copyright libraries appears threatened in the era of digital technology, the opposite is true of unpublished and unique sources.
Digitisation brings with it exciting possibilities for the archival scholar.
The British Library is exploring, for example, a collaborative programme with Yale University whereby we would exchange digitised copies of some of our complementary collections.
One candidate for the pilot scheme is the papers of twentieth-century writers.
The relatively cumbersome medium of microfilm has never presented the sort of opportunities now presented by digitisation, for bringing together collections of complementary material which have been physically separated for decades or even centuries.
Although microfilm will remain for the foreseeable future by far the more acceptable medium of preservation, as a versatile aid to the scholar the computer image is far in advance.
Computers and computer-assisted photography can also provide enhancement of the texts themselves, of course.
Although to my knowledge no digital scanner has yet succeeded in penetrating the ravages of gall, that most seductive and most lethal of enhancing media so widely used by 19th-century archivists, electronic cameras and image processors are working effectively on fire damage, glue and other forms of obliteration.
The directorate of Collections and Preservation has recently collaborated with Professor Kevin Kiernan of the University of Kentucky, to produce digitally enhanced images of the Beowulf manuscript and hence the answers to some mysteries which have long perplexed Beowulf scholars.
Similar techniques have been used on the fire-damaged medieval manuscripts of our Cotton collection.
In both cases, the conservation studio begins by producing a videotape from the electronic camera, which is then digitised and used on an image-processing computer to enhance the image.
Our photographic service offers readers the opportunity to order image-enhanced reproductions.
Our publications now include, in addition to catalogues on CD-ROM, the forthcoming Sources in History: The Medieval Realms , which is designed in part for students and teachers of the schools national curriculum history unit entitled ‘Medieval Realms’.
High-quality images have been scanned from photographic slides of medieval illuminated manuscripts to produce this CD-ROM, which is one of a programme derived from different parts of the library's collections.
Clearly there are all sorts of potential CD-ROM publications based on our manuscript holdings, including full-text reproductions of selected research texts from any and all chronological and geographical areas.
Potentially at least, the technology could also deliver such material across the networks to which the BL is now becoming connected.
At every stage we must beware the many pitfalls.
While the opportunities seem almost infinitely seductive, there are difficulties arising from costs, from relationships between the Library and publishers, where rapid technological changes could outstrip long-term undertakings, and from considerations of preservation.
The cost factor will always loom large in determining priorities.
Electronic advances have to be balanced against the priority of maintaining the necessary level of acquisitions, for instance.
It is likely that most electronic publishing based on the Special Collections of the British Library will be undertaken in partnership with other publishers, despite the fact that the Library is now a successful small-to-medium sized publisher in its own right.
Inevitably therefore, commercial considerations will govern the great bulk of our electronic output.
Any large-scale projects for electronic scanning which might be expected to yield high academic but low commercial benefits, would need to be supported by special grants.
In this area there is surely great scope for the British Library to engage in joint ventures with university departments.
There are good precedents for this sort of cooperation: to take a single example, the collaboration with the Institute of Historical Research and the Public Record Office to produce part of the online Eighteenth Century Short Title Catalogue , using a grant from the US National Endowment for the Humanities.
The Library would be only too pleased to explore further suggestions along similar lines.
13.4.
Preserving the text
From the Special Collections point of view, one of the most speculative questions raised by the electronic media is that of the nature and durability of the archival text itself.
The problem is both physical and intellectual.
The physical survival of computer discs, and of the hardware and software appropriate to reading them, has been tackled by other organisations, including the Public Record Office, and we are grateful to be able to draw on their experience.
Our own collections as yet contain little or no computer-generated material, but without any doubt they will soon do so.
I take the view that a collection of material which is consonant with our acquisitions policy in other respects, should be accepted on the assumption that we must find a way of preserving it and making it available.
Early computer tapes and discs are going to be, to say the least, challenging.
It seems likely that those generated in the 1970s and early 1980s will prove the most difficult to preserve, because of the rapid obsolescence of so much hardware and software during that period.
Many will be the writers who composed on early Amstrads, and how will we ensure that their working discs, successors to the invaluable rough note-books and diaries of writers like W H Auden and Graham Greene or campaigners like Marie Stopes, which we already possess, can be used as readily by future generations of scholars?
Physical degeneration of the material itself is another problem, and here again, it will not necessarily be a straightforward matter to re-copy some of these early discs routinely, in order to ensure their physical preservation.
The intellectual question of preservation is also a challenge.
It is of course a sacred archival principle not to interfere with the original material once it has been incorporated into a collection.
Best practice in the conservation studios dictates that all repairs should be clearly visible.
Among archivists and manuscript curators, it is an essential principle to explain the provenance of the documentary sources and preserve their integrity in such a way that each reader may draw his/her own conclusions with the minimum of interference.
But how, with an electronic document, do you discern its origins, whether it is the work of more than one hand, how much altered from an original first draft, and so on?
From my own work, one particular parallel comes to mind.
I once spent many months as a student using the papers of Francis Place, the radical reformer of early nineteenth-century Westminster.
His collections, put together painstakingly by himself over a long lifetime, and including large quantities of newscuttings, ephemera, overlapping narrative autobiography and correspondence, came to the British Museum (as it then was) through his family.
Being upright Victorians with a strong sense of family pride, they expunged a certain amount of the more salacious material from the record of Francis Place senior.
He had been in his youth a bit of a tearaway; and in middle age he incurred opprobrium for his brave advocacy of contraception.
He also outraged his first family by his second marriage to an actress of dubious reputation.
There are accordingly pages of text in the surviving papers which have been mutilated: sections cut out or pasted over, and some very heavily scored through.
Time and again, it appears that Place himself wrote in afterthoughts or amended text, often many years after first writing it.
It is not difficult to discern where this has happened, and frustrating though it is to have to surmise the contents of the missing portions of text, at least the historian is aware of the partial nature of the evidence.
Compare this archive, one of the richest survivals there is on early nineteenth-century popular politics and culture, with an acquisition now promised to the British Library: the personal archive of Tony Benn MP.
In many ways this huge and fascinating archive will provide for students of late twentieth-century British politics something similar to what Francis Place has left for the historian of the early nineteenth century.
It will be much richer, of course, since Tony Benn has played a more central and significant role in the national politics of his day.
His diaries, written correspondence and collections of political ephemera constitute a treasure trove in themselves.
But where, among the computer discs and tapes, will be the evidence of his own reflections and afterthoughts, or the signs of intervention by other members of the office or household?
We have also to devise a means of editing material which Mr Benn would wish to be withheld for a certain period on grounds of personal sensitivity, in such a way that it can be reinserted in the correct place (if necessary on a disc or tape) when the time comes.
Historians in the future, and contemporary historians even now, will have to face numerous conundrums of this nature.
For the curator of the collections, it poses similar dilemmas.
13.5.
Conclusion
The Special Collections of the British Library thus confront the challenge of the electronic media with particular practical and intellectual difficulties.
The blurred margin between published and unpublished materials affects all of our work.
Our curators will face special challenges explaining and making available computer-generated materials.
They will also find themselves at the centre of the debate over which sources are published and which are not.
And yet at the same time, the electronic revolution offers to the Library's Special Collections and to our readers, great opportunities to use our unique research collections in new and enlightening ways.
14.
Research Library Directions in the 1990s
Lynne Brindley, The Librarian, The British Library of Political and Economic Science at the London School of Economics
Abstract:
This paper will consider the impact of the increasing availability of electronic information on major research libraries.
It will raise issues such as the organisation, integration, and management of research materials in a variety of formats — print, images, multimedia, and digital texts; the increasingly complex task of providing access to such information; and, the requirements for training and support of readers.
The need to rethink our concept of the research library in this wider information environment will be discussed.
Relevant national developments from the HEFC Libraries Review and of the Joint Information Services Committee will be accessed.
14.1.
Introduction
‘Humanists depend to a very considerable extent upon libraries, both in their own and other institutions, and upon a network of museums, galleries, and archives’(Information Technology in Humanities Scholarship 1993: 30).
Indeed, the pattern of research work in the humanities and social sciences is such that perhaps the only generalisation which can be made is that no article or book is written using only the resources of one library, still less the home library of a researcher's institution.
A not atypical list for anyone working in a history-related discipline has been outlined by Dr W Ryan, Librarian of the Warburg Institute.
As an active researcher he uses, as well as his own research library, the libraries of the Institute of Historical Research, School of Slavonic and East European Studies, School of Oriental and African Studies, University of London Library, British Library, Public Record Office, National Maritime Museum library, Society of Antiquaries library, British Library of Political & Economic Science, Bodleian Library, Taylorian Library, John Rylands Library, as well as libraries in Moscow, St Petersburg and Helsinki.
Obversely he comments that, whilst the history department at Birkbeck College in London got a well-deserved five in the latest research assessment exercise — neither the quality of the research not its subject area could possibly be seen as being solely dependent on the strengths of the library of that college.
On the other hand Oxford received a three for Russian despite having one of the best collections of Slavica in Western Europe.
The report cited above also concerns itself with the growth in projects creating electronic resources in the humanities, and the increase in those wanting access to information electronically held.
It mentions as well the growth of new ways of accessing information, from online public access catalogues (OPACs), through CD-ROMs and online searching, to the more recent network navigation tools.
Taken together these observations highlight what is in my view perhaps the major challenge facing research libraries in the 1990s, namely to develop a model whereby consultation of our cultural inheritance manifest in print and manuscript form can be beneficially combined, and handled conjunctly with, access to electronic information in a wide variety of forms.
Getting the balance between these activities right and appropriate to changing academic needs, particularly in an era of severely constrained resources, will require skill, continuing dialogue between the library and its users, and imaginative collaborations.
14.2.
Trends affecting the research library environment in the 1990s
It follows from the introduction that one can conceive of a university scholarly information resource infrastructure of the 1990s comprising several overlapping areas:
(1)
the ‘library’ taken narrowly as the traditional collection of books, journals and a wide range of other media, with its existing set of facilities, operations and services;
(2)
the ‘library of the future’ or the ‘virtual library’, taken broadly as the entire set of information resources and facilities, bringing in other stakeholders, such as computing centres, media centres, film and data archives, museums, electronic information providers, and telecommunication networks;
(3)
the research library network, consisting of the entire set of national and research libraries and other kinds of information resources acting as a cooperative and integrated whole in support of scholarship.
A very major concern for the 1990s is the economics of research libraries, narrowly interpreted.
Indeed it would not be an exaggeration to say that the pressures on the acquisition budget are almost certainly the most pernicious economic problem faced by academic research libraries today.
Rates of inflation in journal prices threaten every other commitment of the acquisitions budget, with estimates from UK university libraries for 1993/4 suggesting price increases in the order of 25 — 40%.
Another major economic concern is with the collections themselves — their storage, preservation and the tools for physical access.
Major research collections are still growing, even though probably at a slower rate than in the 1980s, so they require housing, and the collections of every major research library are in states of disintegration and embrittlement.
Capital provision to support these two areas is scarce and the 1990s are going to test even the most supportive administrations to meet urgent demands for library extensions and preservation/substitution programmes, neither of which has been an established part of institutional budget planning, nor likely to receive much more than sympathy from the funding councils, except perhaps in cases where enormous growth in student numbers have caused such pressures on space.
Libraries are also having to deal with costs incurred in new means of data access — indeed most libraries welcome the opportunities offered by the new media to extend access to information sources.
However, lack of funding which will force more hard choices between competing ‘good causes’ is likely to be a major preoccupation of the 1990s, pitting disciplines against each other, libraries against other information providers, clashes with senior administrators, and so on .
As yet, the costs of electronic information sources represent a relatively minor part of a research library's budget.
However, the funds require to be met from a virtually static budget.
University Statistical Record figures indicate a decline in the financial priority of the library from 1980/81, when the library accounted for some 4% of institutional expenditure, to 1989/90, when the expenditure was 2.89%.
Only to a limited extent are these new electronic services yet replacing specific print products.
Indeed the commercial aspirations of publishers are precisely to ensure additionality, not direct substitution for print in their pricing and packaging structures.
Moving to the ‘library of the future’, many libraries have long given up their comprehensive collecting aspirations: they purchase selectively and rely on access, for example through interlending and document supply, and also on access to bibliographic and other electronic information sources.
Just-in-time management philosophy has moved from manufacturing to the university library sector.
This approach requires very different kinds of investment, unlikely to save institutional funds overall, although it will shift funds towards areas such as networking infrastructure.
Wide access to electronic information resources requires an investment and continuing reinvestment in information technology, particularly in networking bandwidth, gateways to international networks, workstations at the desk and public clusters in the library and elsewhere.
A recent USA study (Hayes:nd) has indicated that on average a university spends 4% of its budget on the library.
A further 6% is spent on computers, telecommunications and other IT hardware and software.
Hayes forecasts that up until the year 2000 the library proportion will remain stable at 4%, but growth will occur in the IT proportion, rising to 11% of the institutional budget.
This implies an increasing shift to access services, based on experience of networked information delivery, with continued severe budget constraint on traditional material acquisition.
Most UK universities publicly aspire to such intensive provision — e.g. 1:1 ratio of workstations:academics, 1:4 for research students, and 1:10 and then 5 for undergraduates — but are finding the level of resources required to achieve and sustain such provision very difficult to engineer in an environment of devolved budgets and uneven ‘soft money’ distribution.
Research libraries and humanities departments should be natural allies in promoting equity of provision across disciplines and user groups.
The development of the ‘virtual library’ is causing some universities to re-examine their organisational structures to bring the range of stakeholders into a closer relationship.
Whilst there is unlikely to be any prescriptive model of such a structure, organisational change will be a feature of the 1990s.
An increasing number of universities are bringing the library and the computing service together, under one operation, or linked through a senior academic administrator post.
Whatever the structure, it will be increasingly important for the institution to take a strategic view of its information requirements and the supporting technological structure, as well as for the library to ensure that it plays a leading role in any new arrangement.
The third strand of the university scholarly information resource infrastructure for the 1990s was identified as the national and international research library network and this will be discussed later in the paper in the context of national initiatives.
14.3.
The importance of electronic access to catalogues and retrospective conversion of catalogue data
The topic of cataloguing and catalogues may not be an exciting one to many of you, yet in the context of this seminar and the concerns of research library provision for the 1990s there is some special pleading to do on their behalf.
In particular I wish to address the development of the OPAC and the associated issue of retrospective conversion of catalogues of ‘important’ collections and networked access to them.
Access to the OPAC is cited by humanities departments as one of the key benefits that campus local area networking can bring, both to identify appropriate material and its location and potentially for document supply (electronic or otherwise).
However, with notable exceptions, the major OPACs of UK universities contain mostly recent material, for monographs and serial titles.
A coherent strategy for retrospective conversion has not been attempted for the UK (although it is known that there are stirrings of such an initiative in a European context, involving the British Library) and lack of institutional funding for such unglamorous activities has ensured at best a piecemeal approach.
If one relates this position to citation studies in the humanities which show that well over 50% of books cited have imprint dates up to 35 years ago and earlier, the increasing tendency to search only the OPAC, and ignore the card catalogue is doing a major disservice to scholars and their potential exploitation of the resources of research libraries.
It is for the humanities to speak up for the value of retrospective conversion, and for some national planning to be undertaken to achieve this, as they, and to some extent the social sciences have most to gain from such an investment.
Where systematic retrospective conversion has been undertaken, resulting in a fully converted catalogue, there are still significant problems of interpretation and retrieval.
Just as manual catalogues are not harmonised retrospectively, so electronic versions are unlikely ever to be.
What is required is the education and assistance of users if their research depends on access to a multiplicity of databases and catalogues, to understand the construction of electronic catalogues, their associated approaches to subject indexing and cataloguing.
I am much indebted to Professor Robin Alston for stimulating these comments, based on his extensive work with individual researchers as they approach the resources of the British Library, Humanities and Social Sciences, both through the converted catalogue and through mediated access to Internet and other database resources (Alston, nd).
Many researchers come to the electronic version of the BL catalogue with experience of its printed version, and so are arguably more fitted by experience to know its idiosyncrasies and quirks.
What asks Alston will happen when remote users have access to these electronic versions only, and to readers who have no understanding of the way in which the catalogue has developed?
Limitations in subject access in online databases are also a serious concern, not by any means eliminated by the increasingly ubiquitous full-text searching capabilities offered by database hosts.
An issue for research libraries over the next few years is to seize the opportunity offered by technology for enrichment of OPACs as electronic access tools.
Possible enrichment includes adding images of contents pages, indexes, thesaural searching and electronic browsing capabilities, the development of intelligent access tools and so on.
Such developments would seem to offer an opportunity for libraries, archives, and museums to work together, drawing on their range of experience to ensure that new designs and software development meets the needs of humanities scholars.
Significant further research and investigation of the possibilities and benefits to be derived from such an approach are recommended.
14.4.
New skills requirements
Alston argues strongly of the need for a new breed of reference librarian, who can understand both the nature of the collections in their care, their description, and the range of access tools.
Such an individual should embody knowledge of cataloguing and indexing rules, at least in the context of a particular research library's collection, its process of catalogue conversion; knowledge of data structures, telecommunications and network gateways; search protocols; and relevant electronic sources.
Librarians are increasingly defining the requirements for a new type of employee within ‘converging’ information services, who possesses the best qualities of current professionals in both organisations.
Stronger subject and technical background and training are important for these new information staff, as well as the ability to work effectively in a more fluid organisational environment.
Assuming that research is likely to depend on access to a multiplicity of sources and databases, users will need such expert advice.
This is an interesting perspective and reinforces the key challenge referred to earlier for the research library, namely the simultaneous access to and consultation of material in all forms to support research, particularly in the humanities, and the need for both ‘ancient and modern’ skills.
Argument for such mediation and user support is not uncontested: indeed much library policy and practical imperatives tend towards the encouragement and support of the ‘self-service virtual library’, where academics at their desks can roam information resources without any intermediary intervention.
Practically the two extremes may be reconciled through more initial effort in training to ensure user understanding of the range of information sources, combined with more indepth support from ‘database or information surfers’(navigators), targeting particular subject groups to optimise their exploitation of this increasingly complex information environment.
14.5.
National developments
I want to turn now from the research library in its university information context to developments at national level relevant to the seminar discussion.
I am involved in two national activities which are moving forward programmes in this field.
The Joint Information Systems Committee (JISC) was set up this year by the higher education funding councils (HEFCs) as a successor body to the Computer Board and the Information Systems Committee (ISC).
It is responsible for the national infrastructure and standards for networking and the encouragement of the effective use of information systems.
Its new brief has a core programme comprising two elements — the provision of a national networking infrastructure for the academic community and specialist information services and datasets.
A major new responsibility is to implement SuperJanet, the national high speed network, which BT has been contracted to supply, starting in March 1993.
There are twelve early connections at 140 megabits per second, with about another forty connections at 10 megabits to be connected late in the year.
The aim is to reach, using for example metropolitan area clusters, as many institutions as possible in a short timescale.
SuperJanet will have the potential for dramatic increase in performance, to carry multimedia applications, such as information services and distance learning material.
It will enable the delivery of new library services to facilitate scholarship, enabling every individual to use distant resources as if available locally.
It is important in this context to ensure the creation of a receptive campus environment, as a multimedia highway which stops at an X.25 gateway on the edge of the campus is not desirable.
The second core programme of the JISC is for the acquisition and provision of dataset services (embracing all categories of bibliographic, text and research data and information) to the higher education community.
Initial mounting of such bibliographic files such as the Institute of Scientific Information citation indices and EMBASE at Bath, and the continuing support with the Economic and Social Research Council (ESRC) for the datacentre at Manchester are manifestations of this programme, which envisages the creation of up to five datacentres.
The policy is to seek a wide range of complementary databases, and it is important for the humanities community to make its requirements known, directly or through the Datasets Inter-Agency Group (DIAG), an advisory body, bringing together dataset providers and funders.
Early in their existence the HEFCs set up a Review of Higher Education Library Provision, under the chairmanship of Sir Brian Follett, Vice-Chancellor of Warwick University.
They were led to do this after major expressions of concern about the difficulties institutions were facing in their libraries in coping with the massive expansion in student numbers.
The brief was early on extended to consider resource sharing across libraries, particularly research resources, which in the context of a new, unified university sector are very unevenly distributed.
The Information Technology sub-group of the Review, which I chair, has the brief to consider the potential contribution of IT, over next three to five years to solving or alleviating any of these problems.
Major areas being considered include the potential for electronic journals (new and possible digitisation of backruns), electronic document delivery, on-demand publishing and tailored textbook provision, navigational tools, training requirements, and the future needs of library systems.
Some key areas of the Review which relate closely to the work of this seminar include:
(1)
possible approaches to selective retrospective conversion of catalogues;
(2)
the idea of an arts and humanities datacentre (as a concept); and,
(3)
discussions with The Consortium of Research Libraries (CURL) on the future handling of their database as a national resource.
The CURL libraries represent major research library collections, around ‘million volumes from 500 years of collecting and maintenance, comprising some 56% of the total stock of pre-1922 universities.
Some 7 million catalogue records of their stock are available, as a potential national database.
Outside the Review the whole area of digitisation of existing research library material is an important one for the 1990s.
The British Library is beginning to consider its digitisation programme, and it is important to ensure a coordinated approach to choice of material to be digitised, including that of the major research libraries.
We can look to the USA where the role played by the National Commission for Preservation and Access has given this type of work a high profile, including recent projects on images.
In my view it is important that there is a partnership between research and national libraries, working with scholars to promote the desirability of a major, coordinated programme of digitisation of existing research library materials, based on user requirements.
The dangers of leaving this solely to the market and enthusiast interests are obvious and potentially serious, to the extent that availability is most likely to guide research effort and energy.
14.6.
Conclusions
The survival of the research library and its use to support future researchers depends on evolving a flexible model, balancing access and holdings, new and traditional skills, and integrating access to electronic, networked and print sources.
Research libraries will face severe economic, space and preservation problems through the 1990s, which are likely to have a particularly adverse effect on the library's ability to acquire traditional research materials.
Campus technology infrastructure developments are required as an essential support to the emerging ‘virtual library’.
Research is required into how the working patterns of scholars are likely to change to cope with the complexity and variety of information sources available to them, and what support they will require in this information seeking process.
Mechanisms for selective retrospective conversion of catalogues and the enrichment of catalogues should be sought.
Cooperation between research libraries, museums and archives should be encouraged to define and develop the design requirements for enriched access tools for humanists.
Research libraries require a new breed of professionals with both subject and technical background to support new access routes to information.
It is important for the humanities community to make its requirements known for the national acquisition and provision of dataset services.
It is recommended that there should be a national strategy for the digitisation of materials in research libraries, closely related to user requirements and priorities.
17.
Increasing the Value of Data
Denise Lievesley, Director, ESRC Data Archive
Abstract:
The benefits of ensuring that data are preserved are well recognised.
However this paper argues that it is essential to support the use of data by providing a flexible and speedy service in disseminating data.
It will also consider the importance of data documentation and will discuss ways in which the value of data may be increased through, for example, integration of datasets and the compilation of teaching packages.
17.1.
Introduction
This paper examines the role of archives of machine-readable data as a resource for research and as a source of teaching material.
Throughout the paper the ESRC Data Archive will be used as an example.
17.2.
The ESRC Data Archive
The ESRC Data Archive has existed for 26 years.
It is based at the University of Essex and is funded mainly by the University, the Economic and Social Research Council (ESRC) and the Higher Education Funding Councils (HEFCs).
Other funding is obtained on a project specific basis.
The Archive comprises a library of computerised data on a wide range of topics mainly within the fields of social and economic information.
Although the data largely result from censuses and surveys, an increasing volume of data arises from administrative processes.
The ESRC Data Archive houses the largest collection of accessible computer-readable data in the social sciences and humanities in the United Kingdom with almost 4000 datasets of interest to researchers from many different disciplines.
The ESRC Data Archive's raison d'etre is to promote the widest use of the data by providing efficient flexible user service, and by sharing information and encouraging the use of the data by means of workshops, bulletins, usergroups, and networks.
The Archive's location in a university which has a reputation for excellence in social research ensures it is perceived to be upholding high standards.
It is also a politically neutral location which is vital with respect to some of the datasets it contains.
The Archive is able to service any user, unlike some data archives which can only provide services to member institutions or which need to impose charges for data.
The ESRC Data Archive is non-profit making and can undertake services on a cost recovery basis.
When the proposed research has received some external funding however administrative fees are imposed.
These differ according to whether the funder is a charity, a non-profit or commercial organisation.
The Archive may, in addition, collect a royalty fee for the data owner.
This is discussed with the data owner at the time they deposit their data and it forms part of the contract between them and the Archive.
17.3.
Data acquisition
17.3.1.
Access arrangements
A critical first step in the process of establishing a data resource for research and teaching is the location and acquisition of relevant data.
The process of acquisition of data is a delicate one in which both depositors’ and users’needs have to be brought into balance.
It is important to establish principles of data ownership.
In many cases the Archive will not own the data but will hold and distribute them on behalf of the owners.
Licences which specify the conditions under which data are provided must be drawn up.
Discussions with data owners need to address the issue of access.
From the user perspective it is important that access to the data should not be unduly restricted, for instance, by the need to apply for permission for each usage of the dataset.
However, many important datasets would not be made available for secondary use without the Archive undertaking to administer some control on behalf of the data owner.
17.3.2.
Data Confidentiality
Since the data depositors’ support and trust must be maintained it is vital that we are sensitive to their concerns.
Frequently they will be anxious about the confidentiality of the data.
In particular they may need to be assured that the data will not enable the identification of individuals to take place.
One of the difficulties facing archives is the need to take account of the diverse needs of different users.
A common strategy to avoid identification involves the removal of variables which provide the location of the individual and to group the geographical variables.
Although this is acceptable to most contemporary researchers — those interested in conducting spatial or multi-level analyses may be prevented from doing so but most researchers accept these limitations as an inevitable consequence of confidentiality requirements — it is not an optimum strategy for historical researchers who often do wish to identify individuals in order to link datasets.
Ideally the information which is suppressed or grouped for contemporary research should be retained for historical research.
In practice this is often not possible because the full dataset is not archived or because the ‘identification variables’ are never included on the machine-readable version of the dataset.
The effect of data protection legislation, especially directives being debated within the European Parliament, will be to exacerbate this problem.
17.3.3.
Cost of data
One of the current concerns is the pressure within many agencies — both official and private — to raise a higher proportion of their income by selling data.
The ‘commodification’ of data may restrict access.
There are particular worries about who, under the ‘user pays principle’, pays for the preservation of data for historical research.
Archives have an important role to ensure that strategic datasets, at the very least, continue to be deposited at low cost.
17.3.4.
Benefits to depositors
In order to maintain the support of data depositors it is essential to consider what benefits accrue to depositors as a result of the dissemination of their data.
Some depositors may believe that they have an obligation to archive and make available their data by virtue of having used public funds to collect it or public time and co-operation in providing it.
Other depositors may be willing to deposit data for reasons of altruism.
The ethos of sharing data can be encouraged:
(1)
by fostering communication between producers and users of data (through user group meetings, newsletters, electronic bulletin boards or networks);
(2)
by recognising that data deposit involves resources particularly in the preparation of documentation and by supporting researchers in seeking funds to cover these costs where necessary; and,
(3)
by urging all secondary analysts to acknowledge data sources — as they would do a publication — in ALL presentations of their research by ensuring that performance indicators incorporate information about the compilation of datasets for secondary analysis.
17.4.
The preservation of data
Archives which contain unique and valuable collections of information must be protected in order to serve future historians.
It is essential that the data are preserved in such a way that they continue to be accessible over time.
The ESRC Data Archive guarantees future access to the material it holds by protecting its information base against technological change.
It achieves this by transforming its collection to a single house standard and by removing the software and hardware dependencies to ensure that the data remain available after the possible demise of the packages and hardware.
The Archive guarantees a secure database by a continuing programme of rigorous checks on the database's physical integrity.
In many cases the Archive holds the only copies of datasets others being unreadable due to changes in hardware or software.
17.5.
Ensuring data are used
17.5.1.
User service
The value of archives is dependent upon the breadth and quality of their holdings but this is not sufficient to ensure that the data are used.
An efficient service to users must be provided which takes account of the diversity of potential users in terms of their backgrounds, their computing and other expertise, and the resources available to them.
Three aspects of this service will be discussed: location of data, dissemination methods, and data documentation.
17.5.2.
Location of Data
An essential part of the research process is to be able to locate the data required as quickly and easily as possible.
The proliferation of data and the increasing need for multiple datasets by researchers mean that online catalogues, navigation systems to link different catalogues and software for catalogue searching are of critical importance.
The ESRC Data Archive has created and maintained a catalogue of its datasets, held on-line.
The Archive provides a central facility for cataloguing and indexing data, using the ‘standard study description’ format to catalogue datasets and to provide a structured format for information retrieval.
Datasets are exhaustively indexed by subject, using a controlled vocabulary or thesaurus.
The catalogues and indexes are available to remote users (both national and international) over the Joint Academic Network (JANET) and Internet systems.
The catalogue and indexes with their associated thesaurus form part of the Bibliographic Information Retrieval On-line (BIRON) system.
The BIRON databases are managed by INGRES software with additional front-end programming to enable users to retrieve information easily without needing to know the internal database language.
Work is continuing on improving the user interface.
At present BIRON consists of descriptive information about studies held, not the datasets themselves, which have to be ordered from the Archive in a separate process.
Searches based on topics
BIRON prompts the user to type in a topic for which data are sought.
This term is matched against a list of several thousand descriptive terms arranged in associated groups.
If an exact match is found, the user is told how many studies have been indexed with the matching term and are offered other associated terms which might assist in focusing the search.
If no exact match is found, lists of similarly spelled words are offered for selection and the process of matching begins there.
A list of titles retrieved may be viewed.
The group of titles may be saved while further selections of topics are made and saved.
A combined search using Boolean operators may then be carried out.
Searches based on bibliographic and methodological data
The BIRON system allows searching on the names of persons or organisations associated with particular studies, titles or part-titles, dates and geographical areas of data collection, particular methodologies of data collection and many other items of information.
Information on spatial units, where relevant, are also included and may be used as search terms.
What information is retrieved from the system?
If the search is successful a list of one or more study titles may be viewed at the end of the search.
Users may then bring up on screen all the information recorded about that study.
The information includes a list of indexing terms showing all the topics covered by the data and a catalogue record giving the Archive number, the title, access conditions, data processing codes, the names of principal investigators, data collectors, sponsors and depositors, an abstract detailing the main purposes of the research, and main variables.
Dates, geographical areas, populations, data collection methodology are also displayed.
Users who are not connected to the Joint Academic Network or Internet systems can seek assistance to identify datasets of interest to them from the user services staff at the Data Archive.
Significant developments are taking place in the use of software, such as WAIS and GOPHER, to enable users to search multiple catalogues of holdings thus avoiding the need for users to have a knowledge of where particular datasets might be held.
This would have particular advantages to researchers wishing to embark on cross-national research, and thus needing to locate data held in different national archives.
17.5.3.
Dissemination methods
As indicated earlier in this paper the range of users of socio-economic data is growing wider in relation to their computing expertise.
A significant number of users are sophisticated computer-oriented quantitative social scientists.
Their demands are shaped by their computing environment.
For these users the desktop machine is becoming the information centre, and they require desktop access to archival material.
This is having an impact on the methods of data delivery.
Data are now disseminated from the ESRC Data Archive in a variety of ways according to the users’ needs.
Network file transfer is increasingly popular but 8mm video tape (‘exabyte’), digital audio tape (‘DAT’) and transfer by portable cartridge tape drives are used, as well as the better known flexible (‘floppy’) diskettes for smaller amounts of data.
The Archive also has a programme of CD-ROM production for well-used collections of data.
Occasionally users still require their data on magnetic tape and so this facility remains available.
New media offer many advantages over the magnetic tapes that they replace.
However in doing so they remove magnetic tapes’ greatest advantage: physical and logical standardisation.
This is more a problem for the Archive than for the end-user.
Typically, end-users will use only one medium — for each there will be a common standard across their individual applications.
The difficulty for the Archive is that there will be many of these ‘common’ standards.
The Archive will be working with other information providers to develop a common approach to data dissemination medium.
In short it should foster acceptance of a single data exchange medium for each broad category of device (e.g. exchangeable disk, cartridge tape).
In the past a social science environment existed in which there were (a) computer users and (b) non-computer users.
Computer users did quantitative analyses and specialised in using the Archive-type of information.
Non-computer users relied on printed reports which were readily available.
The implication for the Archive is that it could make sound assumptions about it's users’ skills.
This is no longer possible; everyone now uses computers.
They no longer mark the specialist.
But an increasing proportion of the users of the Archive are nevertheless very naive and need a large amount of support and assistance from Archive staff in depositing and accessing data.
In summary then the difficulties for the Archive lie in trying to identify users’ needs and in providing them with the appropriate service on a range from an on-line determined access (in which the Archive itself is relatively ‘transparent’) through to a supportive guided approach.
Of course, technological developments are not unambiguous and it is crucial that the Archive does not run too quickly with new developments which subsequently prove to have been misguided in some respects.
Our active participation in the international archiving associations such as International Federation of Data Organisations (IFDO) and Council of European Social Science Data Archives (CESSDA) is crucial in such evaluations.
17.5.4.
Documentation
Documentation of data is essential for secondary analysis but the quality of what is produced by the data generators is very variable.
Data archives have a vital role to play in ensuring that documentation is produced which, as far as possible satisfies user requirements.
This is not an easy task when one considers both the diversity of the user base and of the datasets.
Consultation with secondary analysts and with the data generators is necessary in order to ensure that the documentation supports informed and responsible analysis of the data.
Data archives also have an important role to play in setting standards for the documenting of machine-readable data.
Within the ESRC Data Archive we are attempting to achieve this in two ways:
Firstly by formulating clear objectives for minimum standards, documentation creation procedures and dissemination of documentation within the work of the Archive as a whole.
Secondly by providing well thought out and constructive guidelines for documenting data collections and, if possible, documentation software.
The development of documentation guidelines is a high priority and we are working with some of the major depositors of data to produce them.
Longer term we hope to distribute software to researchers at the beginning of their data collection so that documentation can be produced in standard format and in machine-readable form.
Existing software which incorporates documentation procedures is being evaluated with a view to using it as a basis for this development.
Until recently almost all of the documentation deposited at the Data Archive was in paper form only.
Thus it had to be photocopied — an expensive and time consuming activity — and its quality varied considerably.
Paper documentation also leads to delays in servicing users especially when the data are being delivered over the network or online, but documentation must be photocopied and sent by post.
A major effort is taking place to ensure that documentation is deposited in machine- readable form and, given that generally it is available on word-processed disks, this is proving successful.
17.6.
Promoting the use of data
17.6.1.
Exploiting the potential of data
Although the acquisition and preservation of data are laudable activities they will not result in widespread use of the data unless the archives take an active role in promoting use.
Many datasets are of complex structure with large and sometimes complicated sets of documentation.
It can be very valuable in such situations to provide opportunities for researchers to share their experience of particular data.
17.6.2.
Fostering communication amongst users and depositors
As the holders of information on data as well as on their origins and their users, archives are in a unique position to promote dialogue on analytical methods and research.
The sharing of expertise among users, and between data originators and secondary users is encouraged by archives.
Where participants cross disciplinary boundaries and professional backgrounds, a rich mix of ideas is fostered.
User seminars can be focused upon particular datasets, particular types of data, or particular analytical methods.
The archive may also provide a forum for the discussion of more general issues such as standards for the documentation of data.
Conferences, workshops, electronic and printed newsletters all provide means for the exchange of information.
The ESRC Data Archive holds regular data user seminars to facilitate the sharing of solutions to common problems encountered in analysing particular datasets or to focus on particular research applications.
Formal user groups are organised around the General Household Survey and the Labour Force Survey.
Conferences and workshops discuss research on these data, newsletters are published, and a dialogue maintained between primary and secondary researchers.
A Mailbase Superlist using the JANET mailbase facilities enables the Archive to inform users of popular datasets of new developments, such as the arrival of a new item in a series, and relevant workshops and seminars.
It allows dialogue among researchers and between researchers and the Archive, providing an immediate channel for discussion of substantive and methodological problems.
The ESRC Data Archive Bulletin appears three times each year and is distributed to approximately 5000 readers in the United Kingdom and beyond.
It carries Archive staff news and announcements, news on other data organisations and archives worldwide, one or more feature articles on research carried out using data sources, full details of selected new acquisitions as well as a complete listing with brief details of all new acquisitions, a diary of forthcoming events, and a book review section.
As mentioned above, the Archive also produces regular newsletters on the General Household Surveys and Labour Force Surveys which contain news about the methodology of the surveys, and news about research by primary and secondary investigators.
ESRC Data Archive staff frequently address academic staff and students in social sciences, humanities, statistics and information studies departments, as well as being prominent at national and international conferences and seminars in order to inform potential users of the range and type of data available.
A programme of training courses for the use of the information system has begun and will continue into 1994.
17.6.3.
Specialist data collections
Although most archives encourage the crossing of disciplinary boundaries by researchers it is sometimes useful to set up a specialist unit for a particular group of researchers within the umbrella of a generalist archive.
The appointment of staff with a special knowledge of the research practices and needs of a particular group, e.g. historians or environmentalists, can impact favourably on research in that area.
An active policy of data identification and acquisition in areas should be pursued where there is:
(a)
a considerable collection of under-utilised data, possibly with some characteristics which differentiate them from the main archive collections; and,
(b)
an identifiable producer/usergroup whose expertise can be tapped for greater exploitation of these data.
The characteristics which are common within a specialist collection of data may relate to the subject matter (e.g. political opinion polls), to the time period (e.g. historical social and demographic records), to the geographical coverage (e.g. Northern Irish data), to the administrative units to which they relate (e.g. New Town's studies) or even to the type (e.g. spatially referenced data).
The ESRC Data Archive has two components:
a general archive comprising the core economic and social datasets together with a collection of the data from ESRC-sponsored research
umbrella archives comprising a number of specialist collections of data.
Specialist collections are being established where
(1)
specialist expertise is needed to locate and identify key datasets for inclusion in the Archive, and to assist in the establishment of targeted acquisition campaigns;
(2)
a large number of datasets exist which have not previously been brought together or documented, or the current collection tends to be patchy in coverage;
(3)
particular or special documentation is required in order that the data may be fully exploited;
(4)
personal contacts are needed in order to establish the Archive's credibility with depositors; and,
(5)
particular activities (such as training courses, specialist catalogues etc) are required in order to ensure that possible users of the data can be made aware of their potential.
Specialist data archives differ from the rest of the Archive in a few key ways:
(1)
In almost all cases they are established in conjunction with another department or institution which has especial substantive or analytic expertise to complement the expertise of the Data Archive.
We work with these ‘linked’ organisations on all aspects of archiving but especially with respect to developing an acquisitions policy and promoting the use of the specialist archive;
(2)
In some cases a liaison arrangement is made with an individual who will, in addition to the responsibilities given in (1) above, help with the training of Archive staff so that they have a better appreciation of the needs of particular users of the data.
Such individuals might also help to develop relevant internal Archive procedures, such as documentation, specific to the specialist data; and,
(3)
The Archive will not necessarily acquire all of the identified data files — a specialist archive may comprise a small number of key datasets together with a catalogue of data held elsewhere (meta-data).
The ESRC Data Archive has recognised the special needs of historians by setting up a unit to encourage the deposit of machine-readable data compiled from historical sources and to service the particular data needs of historians.
It has a vital role in promoting the use of machine-readable data particularly amongst historians who have hitherto not used this type of material.
The unit, working within the existing structure of the Archive, consists of a research officer and a data processing assistant and is funded initially for two years.
17.6.4.
Teaching Materials
There is a need for packages or other materials which incorporate data to be developed for use in teaching quantitative analysis in a range of different disciplines.
This need exists at both school and higher education level and is largely unmet.
A few examples of projects which have taken place are described below as an illustration of what can be achieved.
Anglia/ITV have produced a series of teaching manuals together with related data to be analysed using KEY software on the topic Socio-Economic Atlas of Great Britain.
The Archive has collaborated with Anglia TV in the formulation and production of this series.
The Open University is developing two courses which will incorporate socio- economic data held by the Archive; the first on Family and Community Research in the 19th and 20th Centuries and the second on data analysis.
Surrey University researchers worked with the Archive to produce a well documented subset of the General Household Survey.
This teaching package has proved to be a very popular product.
Teaching and lecturing staff rarely have the time needed to identify, obtain, understand and analyse complex datasets such as those held on the Data Archive.
The duplication of work involved if each teacher independently conducts these activities is obviously an inefficient use of resources.
Thus it makes sense to share the teaching resources that have been developed which incorporate relatively straightforward extracts of data formatted for use with specific software and which are documented clearly and comprehensively.
17.7.
Conclusion
Data archiving is essential to conserve very expensive resources.
Unless they are made available for further research, data which have often been collected at great expense and with significant effort may later exist only in a small number of reports which analyse only a fraction of the research potential of the data.
Within a short space of time the data files are likely to be lost or become obsolete as the technology of the host institution changes.
Data archives store, catalogue, index and disseminate the data for further contemporary or historical research.
However this is not sufficient to ensure that the research potential of the data is fully exploited.
Data archives need to be proactive in encouraging the data to be used as well as providing a flexible and supportive service to potential users.
This paper has discussed some of the key ways in which these ‘promotional’ activities might be undertaken.
Part IV.
The Outlook of the User Community
21.
Information in the Business Enterprise
Martin Campbell-Kelly, Senior Lecturer, University of Warwick
Abstract:
In this paper, information in the business enterprise is classified as implicit ,operational , or strategic .
Implicit information is held in the organisational memory (for example, the description of office routine) and is rarely recorded or preserved.
Operational data is used for day-to-day control of the business (for example, for the collection of customer payments) and is usually recorded but rarely preserved.
Strategic information (for example, board papers and minutes, or research reports) is usually recorded and preserved.
The impact of evolving technology on these forms of information is discussed using historical examples.
21.1.
Introduction: a three-layer model
Although the impact of new technology on business records is accelerating, it is not a new phenomenon.
For example, in the 1890s new office machines such as typewriters and calculating machines had a major impact on the day-to-day operations of businesses.
Again, in the 1930s the widespread introduction of punched-card machines had a dramatic impact on commerce, and helped to shape much of the accounting practice of today's businesses.
In recent years there have been three important historical studies of information in business and large-scale organisations.
These are: Joanne Yates's Control Through Communication (1989), Peter Temin's edited volume Inside the Business Enterprise (1990), and Lisa Bud-Frierman's edited volume Information Acumen (forthcoming).
None of these books has included a general model of information in the firm, but most of the enterprises described would fit the simple three-layer model introduced in Figure 21.1.
In this model the top level represents strategic information used to govern the long-term behaviour of the organisation.
Examples of strategic information include board papers, product and marketing plans, financial reports, etc.
Strategic information is very high-grade and of the highest utility to the business historian; it is usually manageable in quantity and readily accessible in hard-copy form.
At the second level,operational information is the raw data which workers of the organisation process to maintain operational control of the enterprise.
The amount of operational data is usually vast — whether it be measured in information-theory terms (e.g. bits per second), or normal business measures (e.g. transactions per day).
Operational information is generally low grade and of low utility for the business historian.
It is rarely retained by the organisation beyond its useful life.
The bottom layer of the model represents the implicit information in the enterprise.
This information is the intangible ‘know-how’ employed by information workers in the prosecution of their tasks.
Implicit information is not generally codified or recorded, and is usually acquired by workers over several years of on-the-job experience.
The economists Nelson and Winter in their Evolutionary Theory of Economic Change (1982) describe this information as ‘organisational routine’, and have likened it to the information codified in computer programs.
To assess the likely impact of electronic media on the business historian, it is first necessary to understand which of the three layers of information are of most interest, which are tangible, and which are currently being preserved.
To illustrate these points I offer two case studies.
First, a nineteenth century example based on the information-intensive insurance industry, and second, a high-tech example from the recent computer industry.
21.2.
Case study 1: industrial assurance
Industrial assurance began in the 1850s as an entrepreneurial response to the need for working-class burial insurance.
Put in its simplest terms, the function of the industrial-assurance enterprise was to use premium income to employ people to produce actuarially determined risk management (Campbell-Kelly 1992).
The quantity of operational information in the industrial assurance enterprise was vast.
For example, the largest British industrial assurer, The Prudential, had some 2.5 million policies in force by the mid-1870s.
At this date, in the first of three information ‘revolutions’, it abandoned traditional hard-bound ledgers in favour of hand-written manila cards.
In the 1920s, a second revolution occurred when hand-written cards were replaced by punched cards.
Finally, a third revolution took place when punched cards were replaced by electronic computer records in the 1960s.
Because of the financial value attached to insurance policies, records were preserved with extreme care and existing records migrated to a new medium only when two criteria were met: first, that the technology was completely proven, and second, that the number of policies in force made the transition practical.
Generally, the operational data (that is the policy records) were destroyed within a few years of their fulfilment.
The knowledge workers in the insurance company were responsible for processing this mass of data to maintain operational control of the business.
By the 1870s, full specialisation of function had been achieved and there were eleven functional clerical departments such as claims, correspondence, finance, lapse, policy registration, and so on.
Clerks obtained their know-how by an apprenticeship system in which school-leavers learned on the job, beginning with low-level tasks and graduating to more complex ones.
At this distance in time we simply do not know in detail how the clerks did their jobs, for there were no manuals of office procedure, and the functional tasks gradually evolved so that they bore little resemblance to their forebears.
The effect of new technology was to automate and de-skill many of the clerical tasks, as more and more routine was taken over by punched card machines and later by computers.
In the case of the latter, the embedded knowledge is well documented in the formal requirements and specifications of computer programs, although I know of no active programs to preserve this information.
The second function of the knowledge workers was to process operational data to produce strategic information; this information then enabled board-level decision-making to achieve long-term strategic control.
For example, one vital task undertaken by the clerks was a quinquennial valuation which ensured that the company's assets matched its actuarial liabilities.
In the 1870s this was a mind-boggling task that took 300 clerks six weeks to complete.
Later, punched-card and computer technology made this an entirely mechanical process.
Regardless of how the data was processed, however, the resulting valuation report was always a short, printed, paper document.
The same printed-report format was generally used for all other strategic information, such as new actuarial tables, investment performance reports, analysis of individual insurance products, annual reports, and so on.
Strategic information is generally modest in volume (only megabits in information-theory terms), of the highest importance to the business historian, and is generally well preserved on paper media in corporate archives.
The impact of electronic media on strategic information is likely to be slight, since the printed paper form will probably persist into the foreseeable future for both cultural and practical reasons.
21.3.
Case study 2: ICL's New Range Planning Organisation
ICL was created in 1968 as a ‘national champion’ computer firm in a government inspired merger which brought together all of the then existing British mainframe computer firms.
As an inducement to the merger, the Government provided a £13.5 million grant for the development of a new computer range to compete with the then best selling System/360 computer family sold by IBM.
Both the ICL New Range and the IBM 360 computer projects have been the subject of historical studies, the former at a strategic level (Campbell-Kelly 1989) and the latter in very much more detail (Pugh 1991).
These two examples illustrate the kinds of records that are produced in large high-technology projects, what records are preserved, and what records are actually of use to the historian.
There have been several comparable studies of high-technology projects in the last two decades (e.g. Hounshell 1988).
The ICL New Range was by far the largest computer R&D project ever under taken in the UK.
Between 1969 and 1973 it was to cost approximately £100 million, and involve a design team of up to a hundred people known as the New Range Planning Organisation (NPRO).
Later the New Range was to consume the largest part of ICL's 30,000-strong workforce in development, manufacture and marketing.
The New Range Planning Organisation was responsible for defining the ‘architecture’ of the New Range, determining the feasibility of development and manufacture, and establishing marketing plans.
Although the reporting process of the NRPO was highly formalised, its day-to-day activity was a chaotic mixture of formal and informal committees and networks, with a core of full-time planners and a larger number of people seconded part-time from within the company, and numerous consultants from outside.
To take one example, at the beginning of the project seven committees were established, each consisting of about six people, to investigate one of a range of competing architectural possibilities.
These committees met frequently, exchanged memoranda, wrote interim reports, made phone calls, met with external consultants, and consulted a wide spectrum of the published literature.
Almost none of this information exchange has survived.
However, because the New Range project is very recent in history, it was possible to interview key participants who were able to elaborate on the minutiae of the planning process.
Clearly, the greatest impact of electronic media on the day-to-day interactions of an organisation such as the NPRO would be the substitution of ephemeral electronic mail for slightly-less-ephemeral paper memoranda, and the use of teleconferencing for some face-to-face meetings.
The scope for the retention of this kind of information exchange is not significantly better or worse today than it was then.
In fact, the most significant change in the last twenty years has been a cultural trend towards less formal minute-taking in meetings.
This highlights the importance of active oral history programs, not merely as a by-product of a celebratory history, but as a routine function of the pro-active corporate records manager.
(In the information technology area, both IBM and AT&T are actively pursuing such programs.)
The output of the two-year's activity of the NRPO was a large number of technical and marketing reports, occupying several shelf-feet.
This constituted the formal, operational information produced by the organisation, and it underpinned the subsequent R&D, manufacturing and marketing activity for the next decade.
These reports included, for example, seven reports evaluating the competing architectures referred to above.
There were also reports defining the necessary electronic, electro-mechanical and software requirements of the New Range; and there were numerous marketing reports analysing the product requirements for ICL's specific market segments.
All these reports were produced in a highly disciplined way and a catalogue was produced.
There was no active retention program by ICL's corporate archives, but ICL's Technical and Social Archive held a complete set of documentation, which has recently been transferred to the National Archive for the History of Computing at Manchester University.
R&D is alive and well in ICL today, but electronic media have had little impact on the operational information produced.
It seems likely that the printed research report will be the dominant genre of the research organisation for the foreseeable future.
Of course, the NRPO reports were simply the tip of an iceberg of documentation generated by the New Range project.
For example, within the manufacturing and software development organisations there was a corresponding, but much larger, networking activity and related documentation.
Virtually all of this information was routinely destroyed as soon as it ceased to be useful (for example, once a set of negotiations had been terminated or a product discontinued).
For the general business historian, this is a loss of little importance.
Yet another mass of documentation was generated for customers and maintenance personnel.
There was a manual for each hardware component and software product, the total running to perhaps a hundred linear feet, not including the numerous multiple editions of manuals.
No attempt is made in ICL (or any other company I know of) to archive product documentation once a product is no longer supported.
Computer users, unfortunately, attach a singular importance to computer manuals and they constitute much of the offerings that arrive at unwilling computer archives, who would soon drown in computer manuals if the flood were not turned back.
(By way of illustration, in the mid-1980s the catalogue of IBM's mainframe-computer manuals occupied about three hundred pages with about fifty titles per page.)
New technology arrived for computer manuals in the late 1970s with the use of microfiche as an alternative to hard copy.
While this has greatly reduced the archival storage problem it has probably made the access problem worse.
At the time of writing, computer documentation is making the transition to CD-ROM which will eliminate the physical space problem, greatly mitigate access, but it is not clear that the archiving of computer manuals will serve any useful purpose, other than to satisfy the cravings of the occasional computer buff with an obsessive interest in the technology.
However, returning to the NRPO documents themselves, they were far too discursive and voluminous to guide board-level strategic decision-making.
Hence, the several dozen reports were reduced to a handful of summary reports for the three principal operational boards (R&D, manufacturing, and marketing) and a slim set of board papers for the main board.
This strategic information was exceptionally rich, and for the business historian seeking to understand the strategic behaviour of the firm, it contained almost all that one needed to know.
Like all board papers and reports, they have been preserved indefinitely in the corporate archives.
Given the cultural resistance of the printed hard-copy format for board papers and reports, there appears to be little likely impact of new technology.
Indeed, the chief difficulty I experienced researching ICL in the early 1980s, was not the emergence of new office technologies, but the fad for making board ‘presentations’ unsupported by written papers.
For these, the board minutes could only record highly abbreviated abstracts.
21.4.
Conclusion
The impact of electronic media on records is one of the big issues facing historians and records-managers in the late twentieth century for two reasons.
First, there has been an explosion in the quantity of information, but no corresponding useful measure of quality.
Secondly, there is uncertainty about what can be kept electronically, and what should be kept.
To make these issues more tractable, it is helpful to consider the historical value of the three types of information produced by businesses, its usefulness the business historian, and the cost of keeping and using it:
(1)
Implicit information is by its nature intangible.
Currently the best way to capture this information is through oral history programs.
The codification of knowledge into computer software, does however, suggest an intriguing new source for the future that is not yet being effectively captured and archived.
(2)
Operational information has undergone several media revolutions in the last century.
In principle, now that operational information is largely electronic, it could be preserved in electronic form in its totality.
However, the volume would be simply huge — to take one example, in the late 1980s the SABRE airline reservation system processed 8,000 transactions per second.
It is not clear that any useful purpose would be served by the indiscriminate archiving of operational information of this kind.
(3)
Strategic information is the rich, top layer of information that is of primary interest to the business historian.
It is in this layer that historians will discover answers to their key questions: how the firm responds to opportunities and threats; how it interacts with government, other firms, its employees, and its customers; how it establishes R&D programs and marketing strategies; and so on.
Thus, the answers to the big questions are generally to be found in the top-level board papers of significant firms with professional records managers, and electronic media presents little threat in the foreseeable future.
Electronic media does however present very great opportunities for the enlightened corporate records manager to provide enhanced storage of strategic information and improved access for researchers.
22.
The 1991 Census Datasets and Social Historians
Virginia Knight, Census Support Officer, Manchester Computing Centre
Abstract:
Manchester University is holding data from the 1991 Census both as microdata and as aggregated tables.
Many of our users are social historians, and we offer support to them at all stages.
The paper will discuss the potential of the 1991 Census for social research, the problems which must be acknowledged and how it may be used in conjunction with data from previous Censuses; it will also describe the setting up of the service and the technical problems in managing such large datasets.
22.1.
The 1991 Census Initiative and the datasets
The ESRC, together with the Information Systems Committee (ISC) of the Universities Funding Council (UFC), have set up a Census Initiative at a cost of some £3.1 million to make information from the 1991 Census available to academics for teaching and research.
The Initiative has purchased data from the Census, set up units to support it, and funded programmes of research, training and development.
As part of the Initiative, datasets derived from the Census are being held at Manchester Computing Centre and the Census Dissemination Unit has been set up initially for the period from 1992 to 1997 to support them; the Census Microdata Unit has also been set up in the Econometrics department of Manchester University.
Manchester Computing Centre, as well as supporting computing at Manchester University and UMIST, acts as a national computing centre for the dissemination and support of a number of large datasets, such as the General Household Survey (GHS) and the Family Expenditure Survey (FES).
It was seen as an appropriate place to hold the 1991 Census datasets since they are large and complex and require specialist support.
Aggregated tables from the Census of Great Britain have been available since the 1966 Sample Census.
The data from the 1966 and 1971 Censuses was not much used, because there was no software to go with it.
However, in response to demand, an initiative was set up to distribute data from the 1981 Census.
The data was distributed from regional computing centres at London, Bath, Edinburgh, Manchester, Aberdeen and Newcastle; this distributed scheme reflects the early developments in networks in the early 1980's.
The data was held at the ESRC Data Archive, which, however, did not provide on-line access, although there was a national support post, whose holder ran courses, produced documentation, supplied data to regional centres and gave advice.
A software package, Small Area Statistics Package (SASPAC), was developed by a team at Durham and Edinburgh Universities to extract and manipulate the data.
The data was held on the Manchester mainframe and Manchester Computing Centre produced a two-volume manual which described the SASPAC package, and explained how to run programs using the data at Manchester.
It was apparent, however, that the support was required at the point of access and supply.
The Computer Board (the predecessor of the ISC-UFC) funded an additional support post at Manchester to help users to access the data online and to develop the online service, for example by implementing specialist software for the Special Workplace and Migration Statistics, and to produce documentation and provide research support.
The support post proved successful enough to establish Manchester as a national datasets centre with considerable expertise in handling and analysing Census data.
For 1991, it was decided to combine the support posts which had existed at the Data Archive and at Manchester in a single unit.
The datasets supplied by the Census Offices under the Initiative comprise the following: the Small Area Statistics and Local Base Statistics (SAS/LBS), which are sets of tables covering all aspects of the Census in Great Britain, the Special Migration and Special Workplace Statistics, which are matrices dealing with migration and journey to work, a postcode/Enumeration District (ED) directory, and digitised boundary data (compiled by the EDLINE consortium) corresponding to various geographical zones for which Census data is being released.
These datasets are supported by the Census Dissemination Unit at Manchester Computing Centre within Manchester University.
There are also samples of microdata from the 1991 Census of Great Britain and Northern Ireland (the Samples of Anonymised Records or SARs); these will be held at Manchester, and are being handled and supported by the Census Microdata Unit.
CHEST (the Combined Higher Education Software Team) has also purchased SASPAC91, the package which accesses and manipulates the Small Area and Local Base Statistics, developed for the 1991 datasets by the London Research Centre.
Most of this paper will be concerned with the Small Area and Local Base Statistics, and to a lesser extent with the Samples of Anonymised Records.
The importance of these datasets for the social historian is probably obvious.
The Census is the only survey of the entire population of the United Kingdom, and contains information about a wide range of topics, including, for example, household amenities, ethnic origin, educational qualifications and socio-economic group, as well as the standard variables such as age, sex and marital status.
This information can be obtained for areas as small as an Enumeration District (a few hundred people).
The postcode to Enumeration District directory and the digitised boundary data enable it to be mapped and compared with data compiled on a different geographical basis, an aid which has not been available for censuses prior to 1981.
All the machine-readable datasets from the earlier Censuses are held at the ESRC Data Archive at the University of Essex.
The Census data from 1981 is also available online at Manchester.
The 1991 Census datasets are or will be accessible at Manchester and a copy will be deposited at the Data Archive.
22.2.
Loading and storing the data
Manchester Computing Centre has a crucial role to play in the care of the datasets.
The task of loading and storing the data has been far from trivial; for one thing, the total raw data in the SAS/LBS for Great Britain at all areal levels comes to some 6–7 gigabytes.
The Census Dissemination Unit has been allocated a certain amount of minidisk space on the Amdahl at Manchester to hold it; this consists of 5 minidisks each with a capacity of 1.2 gigabytes; two contain system files, together with the postcode/Enumeration District directory and some text files with other information, and the rest have been used to store raw data files.
The complete, corrected raw data has been backed up on to cartridges which can be loaded by robot, and any files which are not online can be quickly recalled.
The sheer size of some files caused problems.
The largest single file was the 100% ED level file for Outer London, which was 320 MB.
This was split across three tapes.
Manchester Computing Centre is used to coping with multi-volume files, but a further problem arose when the file was too large to fit onto one of our backup cartridges; as no one had ever needed to copy a file that large on to cartridge before, we had to devise a way of splitting it between two cartridges.
The Census Dissemination Unit also had to carry out quality assurance checks on the data.
The Census Offices are unable to do this themselves since they can only extract six areas at a time!
There were a number of errors in the raw data which meant that the first attempt to convert it into system files failed.
These were of two kinds; a displaced field in the header record at district level in nine counties, and in another ten incorrect grid references which were rejected by SASPAC.
When this happened, the raw data had to be emended.
When the file concerned was an ED (Enumeration District) level file, it was too large for our screen editor to be used, and a FORTRAN program had to be written to change it As a result of these corrections, the data held at Manchester is cleaner than that at OPCS (the Census Office for England and Wales) itself!
(The General Register Office in Edinburgh has been aware of the few errors in the Scottish data).
Further corrections will have to be made to the grid references on the Enumeration District and Output Area level files; some grid references are absent altogether, and others are incorrect, resulting in a number of Enumeration Districts being in the sea!
These corrections will have to be made on the raw data files sent by the Census Offices, since although SASPAC91 can correct individual variables within a system file, it cannot output data in its raw form.
It is not sufficient to amend the system files, since at some point in the future SASPAC91 may become obsolete and these files will become unreadable.
This deficiency in SASPAC91 will not be repeated in the software for the Special Workplace and Migration Statistics.
The software for the 1991 SAS/LBS was set up on the assumption that the Census data was fixed and unchanging; however, this proved to be unrealistic.
In future, it would be better to assume that there will be amendments to the data, both when commissioning software and when planning storage and distribution arrangements.
22.3.
Promoting the data and documentation
Part of the task of the Census Dissemination Unit has been to encourage academics who may not be computer literate to use the data.
The situation has improved immensely since 1966, when the Small Area Statistics were only available on punched cards with no software, but many academics are still wary of computers, particularly those who were trained before they were widely available.
To this end, I have been visiting universities to give half-day courses on the datasets; this includes a general description of the Census Initiative and the datasets, followed by a live demonstration of about fifteen programs illustrating the functionality of SASPAC91.
The handout which accompanies the course and contains the code for these programs has proved to be a valuable supplement to the official SASPAC91 manual, which is not geared to the needs of the typical academic user.
The Census Dissemination Unit is also prepared to answer queries from researchers who are having problems in using SASPAC, either by telephone or e-mail.
So far the volume of queries has not been large, but this probably reflects the fact that many users get help from other users at their institution.
Often the problem can be solved at once; other questions have shown up problems with the data or with the SASPAC91 software.
In the former case these can be referred back to the Census Offices and in the latter to the London Research Centre.
The process of interaction with our users is thus two-way.
An online help system is also being developed, and the Unit currently runs an e-mail list, to which details of latest developments are posted, and maintains a section on the National Information on Software and Services (NISS) bulletin board.
Manchester Computing Centre has reprinted the SASPAC91 manual produced by the London Research Centre, which comes in two volumes, one describing the language and one giving outlines of the tables.
Roz Scott Huxley at Manchester Computing Centre has also written a user note for SASPAC91 on the Manchester mainframe; in addition to explaining commands relating to SASPAC91, it also assumes the user is not familiar with CMS (the Conversational Monitor System running on the mainframe) and the XEDIT editor, and takes them step by step through creating a command file, running it interactively, offline or remotely, and collecting the output (Scott Huxley 1993).
The guides to the Census being produced under the Initiative and elsewhere are also intended to help users, and to supplement the rather bald description of the datasets given in the guides produced by the Census Offices.
22.4.
Modes of access
Most users are accessing the SAS/LBS on the Manchester mainframe.
This requires that they get a Manchester userid, which can be obtained via a Manchester representatives at the computing centre at their institution.
Any problems with the data can be reported to the Census Dissemination Unit and sorted out directly; there is no problem with, for example, supplying a corrected version of a dataset to a remote machine.
There is also a full range of support and a proper user note.
Incidentally, it enables the Census Dissemination Unit to monitor use of the data easily, both by counting the number of users currently registered and by using the project accounting package VMACCOUNT to find out the amount of CPU time which has been used and by whom.
The drawback, from the users’ point of view, is that they have to learn how to use the CMS system.
Even a computer-literate user brought up on a different type of mainframe may find the nested screens in CMS and the XEDIT editor offputting at first.
Moreover, some sites have difficulties in getting full-screen CMS on many of their terminals.
One way of avoiding this problem is to use JTMP (Job Transfer and Manipulation Protocol) to run jobs remotely; once a workable JTMP framework has been set up, there is no need for the user to log on directly to CMS at all.
The CMS system will be replaced by a UNIX one in a year's time; while this system may be easier for some users to learn, others will have to adapt to yet another unfamiliar system.
However, for some time the two systems will run side by side.
A number of sites are holding the SAS/LBS on their own mainframe, and a few are taking all of it.
Because of the size of the data this is a large commitment to space which is only worthwhile where a large number of people want to look at a lot of the data.
Other sites are just taking a subset of the data, perhaps their own county and a few others nearby.
If they are holding it on their mainframe, they have to devise their own implementation of SASPAC91.
SASPAC91 for PC's is being made available under the CHEST deal to academic sites who want to hold it; the cost of a site licence is just under 3,000 for five years, and about 15 sites have taken up this offer.
However, some sites which have received the software under this deal have not requested any data, which suggests that they do not have the resources to run a properly supported 1991 Census data service.
There are certain advantages to the PC version; it is menu-driven and people do not have to learn any computer system to be able to use it.
Instead, they are guided through menus by selecting items.
The drawbacks are that it is significantly slower than the mainframe version, and the user is limited to the amount of data which can be fitted on to a PC (a typical county takes up about 20 MB).
PC SASPAC is therefore not useful to someone who will want to study low-level data for a large number of counties, though it is suitable for someone who is only interested in one or two and not fond of computers!
For this reason it has proved popular with Local Authorities.
The Postcode/Enumeration District directory exists at the moment as a raw data file, with an online guide in another file; eventually there will be some simple software.
The digitised boundary data will be held in various formats, and will be administered by a registration system similar to that for the SAS/LBS.
The Special Migration and Workplace Statistics will have their own associated software.
The anonymised records are also going to be held on the mainframe at Manchester, and the complete dataset or subsets of it will be sent elsewhere.
There is no special software for accessing the Samples of Anonymised Records, but the Census Microdata Unit is offering advice on how to read the data into the standard packages for statistical analysis.
22.5.
Confidentiality and registration
The Census Offices in Great Britain are very concerned that the datasets should not be misused, and the registration procedure has been developed with this in mind.
The conditions of use to which all users are required to agree insist among other things that users should not attempt to identify particular individuals or households, that the data should only be used for academic purposes, and that any publications which make extensive use of the data should acknowledge this.
The Census Dissemination Unit acts as ‘gatekeeper’ by advising on licensing arrangements for the use of the data.
A typical user will contact the Census Dissemination Unit for a registration pack, or collect one from his or her institution.
The registration pack contains a registration form, which asks, among other things, for a keyword description of the user's interests, the length of time for which the data is required and the source of funding.
The user can then be added to the list of people who can access the minidisks on which the data is held.
A similar system will operate for the anonymised records.
The Census Offices require that anyone who is using the data should assent to the Conditions of Use, and to cut down on paperwork, there is a class registration form, to be used when a class is accessing the data.
Similarly, if one person is obtaining data on behalf of another, they should both be registered.
This often happens, for example, when a research assistant pulls information off for a less computer literate academic.
Where sites are distributing the data locally, in whatever form, they must set up their own local registration system.
The Unit has established guidelines for producing local registration forms and conditions of use, which should be based on ours, and approves the proposed forms and conditions before the system is set up.
22.6.
Comparing census data for different years
Many of our users want information about the present situation, or at least the most recent data available, and are not interested in data from previous Censuses.
However, the Census is a diachronic as well as a synchronic dataset.
The historian normally wants a longer perspective and the more time passes, the more likely it is that research will be concerned with change over time rather than with a ‘snapshot’ of 1991.
This will be especially likely when the 2001 Census data becomes available.
There already exists a way of comparing successive Censuses, the Longitudinal Study.
This is a 1% sample of Census data for England and Wales, selecting people who gave one of four days in the year as their birthday on their Census form.
OPCS hold the data on their own mainframe and try to trace individuals from one Census to the next.
The data can be accessed through the Longitudinal Study Support Programme at City University; because it is not supported at Manchester it will not be discussed further in this paper.
Comparing the datasets from different years is not straightforward, for the following reasons.
Firstly, the Census definitions have changed even between recent Censuses, and new or different questions have been asked.
In 1991 there were two new questions which had not been asked in previous Censuses; one on limiting long-term illness and the other on ethnic origin.
The question on birthplace was replaced by one on country of birth.
The wording of other questions, and the coding of variables derived from them, has also altered (Census Offices 1992).
Secondly, the geographical zones for which the Census is produced also change between Censuses.
For 1981 there was an ‘area change file’ which listed the changes since 1971, but this was little used, and it is still uncertain whether a similar file will be produced detailing the changes between the zones of 1981 and those of 1991.
It is possible to find out from the data whether a zone has changed between 1981 and 1991, but not how.
Thirdly, the Small Area Statistics consist of tables which are not exactly comparable between years.
Apart from the addition of variables derived from the two new questions, the other tables do not exactly correspond.
The 1991 Census has also been produced as Local Base Statistics, which were not available in previous years.
These too are a set of tables, but they are more detailed, and are not available for the lowest area level (Enumeration District/Output Area).
Fourthly, an important difference between the 1991 Census and its predecessors was the procedure of ‘imputation’.
This was used to infer values of the Census variables for households which never returned a form.
There were many more such households in the 1991 Census than in previous years; this is generally thought to be because many people evaded the enumerators lest they be made to pay the Community Charge.
The enumerator found out or estimated the number of rooms in the household and the number of residents; the Census Offices then used the ‘hot deck’ method of imputation, finding a similar household in the same area and assigning values for other Census variables to the absent household.
The percentage of imputed households nationally is about 2%, but the problem is much greater in inner-city areas.
The base from which the Census SAS/LBS for 1991 were compiled is thus different from 1981; the SARs will not include imputed households, but will of course be affected by the undercount.
The 1991 Census was the first for which imputation was nationally a major problem, although there was an undercount in 1981 in Northern Ireland.
Ironically, there was no need for imputation in the 1991 Census in Northern Ireland, where the rates system had not been replaced by the Community Charge.
It is potentially worrying that the climate of evasion which produced the 1991 undercount may persist and affect the reliability of future Censuses also.
Finally, anonymised records have not been produced before 1991 in Great Britain.
If the Census Offices are satisfied that the 1991 sample has not been misused, then a similar dataset may be released from the 1981 Census.
This will allow comparisons to be made between the two years using aggregations devised by the researcher, rather than those imposed by the Census Offices.
The variables and definitions for 1981 will differ from those in 1991, as described above; it would obviously be desirable if the 1981 Census sample were organised by the same geographical zones, and the same codings of variables as that for 1991.
The problem of different definitions and codings has already arisen in the need to harmonise the 1991 SARs from Northern Ireland with those from Great Britain (Middleton forthcoming).
Many of our users are using the data in conjunction with that for previous years.
The process is two-way; some users who have used 1981 data want to continue their research with 1991; other users start with 1991 and decide they would like to see how the situation has changed in the last ten years.
At present there is an incompatibility between the 1981 and 1991 system files; the 1991 files are only readable with SASPAC91, while the 1981 ones can only be read with the original version of SASPAC.
The two versions of SASPAC are not compatible, although very similar; various changes have been made in the syntax both of command files themselves and the commands for running them on CMS.
The 1981 system files will be reloaded as SASPAC91 system files; people who have used the 1981 version of SASPAC will still have to learn SASPAC91, but the reverse procedure will no longer be necessary.
22.7.
Problems in analysing the data
Once the user has obtained the SAS/LBS, there are still complications in analysing them.
One difficulty is the amount of imputation as described above; this is particularly a problem for the study of inner city areas and such topics as social deprivation, where the information is likely to be more conjectural.
Nor does the imputation take account of people who did not appear on the Census form returned for their household; the extent of the undercount, and the approximate breakdown of the missing persons by age and sex, can be estimated by comparison with the Registrar-General's population estimates (Census Newsletter , October 1992), but no further adjustment to the statistics has been made.
The SAS and LBS have been subject to a process known as ‘Barnardisation’, in order to blur them and make sure that individuals or households cannot be identified.
The tables for Enumeration Districts and electoral wards (in Scotland, postcode sectors and Output Areas) have had 1 randomly added to and subtracted from any non-zero count, apart from the basic head counts.
The distortion resulting from this can be magnified in various ways; for example, a total or a sub-total within a table will combine the errors in the counts from which it is made up, as will a variable obtained by calculations using affected variables, or by aggregating several Enumeration Districts or wards.
When the numbers concerned are low, the margin of error becomes unacceptable (Cole forthcoming).
A further consideration is that some variables such as occupation, industry and educational qualifications were only coded for 10% of non-imputed households.
Tables involving these variables will only be based on a 10% sample of the population.
Since a typical Enumeration District contains about 200 households, a 10% sample of a single ED is unreliable, and the 10% data should not be used for areas smaller than a few Enumeration Districts (Cole 1993).
The pitfalls in analysing the samples of anonymised records have not yet become fully clear.
However, the Census Microdata Unit staff, including a computer officer and a research associate, will be doing evaluative work on the samples when they are loaded and usable.
It can be seen from this that the data cannot always be taken at face value and that users need to be warned of these problems.
The official reports, produced by the Census Offices, such as the County Monitors and the User Guides, tend to play down the difficulties and in particular have been coy about the extent and the method of imputation.
The half-day course on the Census introduces the problems but does not attempt to discuss them in depth.
The 1991 Census User's Guide , to be produced under the Census Initiative, and the Census User's Handbook will be covering these issues in more detail.
In the meantime, members of the Census Dissemination Unit are writing articles for the national and local newsletters produced by Manchester Computing Centre and giving papers in which the data and its limitations are evaluated.
22.8.
The future
The Census Dissemination Unit and the Census Microdata Unit have been funded in the first instance for five years.
It is possible that this funding may cease after 1997, but as Manchester Computing Centre has been designated a national datasets centre, the expertise to support the data should remain there.
However, in the long term, Manchester cannot be expected to keep all its Census data online indefinitely, and other sites holding the data may dispose of it as the level demand falls off.
A copy of the 1991 Census datasets will be deposited at the ESRC Data Archive (where it will join the data from previous Censuses) when the data is complete and it appears that all corrections have been made.
As was said above, our data is at least as clean as that held at the Census Offices, if not more so.
The SASPAC package may at some point become obsolete, and a more worrying prospect is whether the current 5-year licence under the CHEST deal will be renewed, and in what form.
However, the raw data is readable by other means, for example in SPSS, though this is difficult.
It will not therefore be useless without its associated software.
Assuming there is no change in the relevant law, the data on which the aggregated tables is based will not be available for nearly a hundred years, and we have to keep this timescale in mind when planning for the future.
Even after the release of the original data, our datasets are likely to retain some value, since it would require considerable effort to reproduce them.
The Samples of Anonymised Records will be kept in a form which will allow them to be read into a variety of software packages.
The Postcode/Enumeration District directory and the digitised boundary data will also be readable in their raw form.
The Special Migration and Special Workplace Statistics will have their own software, but this, too, is not essential for the use of these datasets.
The datasets are not likely to become obsolete in the foreseeable future; however, it will be essential to keep with them a record not only of the modifications made to them, such as imputation, sampling and Barnardisation, but also information on how these will affect results derived from the data.
The experience of storing and distributing the 1991 Census datasets has taught some valuable lessons, which will be useful if data from future Censuses is released on a similar basis.
The arrangements for support and distribution have worked well, but with hindsight there are pitfalls to be avoided in future.
In particular, software and support should take account of the possible need to emend the data after it has been released.
24.
Information Technology and the Implications for the Study of History in the Future
Kevin Schürer, Assistant Director, ESRC Data Archive
Abstract:
The focus of this paper is an examination of the extent to which the recent developments in the use of computers in history teaching and research have prepared historians for the study of history in the future.
In assessing this predicament, a number of issues will be addressed.
Principal amongst these are: whether the problems of computing facing historians are different to those in other area of the humanities and the social sciences; the strengths and weaknesses of the standardisation movement; the availability, utilisation, and interpretation of source materials; and the position and needs of data archiving.
The paper will conclude with a plea for the situation to be addressed as a matter of urgency and the case to be presented to appropriate government agencies.
In 1838 having returned from his circumnavigating voyage, Charles Darwin sat down at his desk in his lodgings in Great Marlborough Street, London, to write some brief notes on his views concerning marriage.
This was no general or casual interest, but was directed at no other marriage than his own.
With Shakespearean echoes Darwin headed the paper thus: This is the question — Marry — Not Marry.
Below this he set out his arguments for and against.
When viewing the document it is evident that unlike other aspects of his writing, Darwin's thinking on the subject was not particularly clear.
Not only are some of the positive points he makes in favour of marriage repeated in a negative form in the list of arguments against, but also several times the text is annotated and amended.
In favour of marriage he noted such things as children, companionship and home comforts: ‘picture to yourself a nice soft wife on a sofa with good fire, and books and music perhaps’.
Against marriage Darwin chiefly noted aspects concerned with the lack of one's independence and the financial hardships that a wife and several children might bring: ‘…forced to visit relatives…the expense and anxiety of children…
Loss of time — cannot read in the Evenings…less money for books’.
Having contemplated the pro's and con's Darwin came down on the side in favour of marriage.
Having made his decision he then took fairly swift action and was married to his cousin Emma Wedgwood on 29 January, 1839.
(Darwin Papers, Cambridge University Library, DHR.210.10; Macfarlane 1986: 3–5).
Sometimes it is interesting to play speculative games by projecting historical events and characters forward or backward in time in order to view them in the context of a different time or place.
If a present-day Darwin were to agonize over his matrimonial fate in what ways might it differ from the example above?
Changes in contraceptive practises might lessen his fears over large numbers of children, and the string of TV chat-shows, radio and press interviews, plus pending film royalties following the voyage of the Beadle might well combine to reduce his financial concerns.
But what about the physical document itself, how might this be affected?
Being a forward-thinking ‘man of science’ it seems highly probable that our present-day Darwin would be computer-literate and certainly a user of word processors.
Yet would he use a PC or a Notebook computer for such a document, and if so would he edit it as he re-read the text and his thoughts on the subject became more succinct?
And if he should record the note electronically, would he then bother to save it and retain it until the end of his life?
It seems unlikely.
But even if he did save his electronic jottings, would they be readable by the end of his lifetime, or would magnetic deterioration plus hardware and software obsolescence combine to make the efforts of retaining the file futile?
So much for speculative games.
One hundred and fifty-five years on from Darwin's marital dilemma, the major pre-occupation that gave rise to the organization of the conference to which this paper is submitted, is a marriage of a different sort: the study of history and the use of information technology.
Although the battle has been hard and long, and from time to time skirmishes still occur, computing is now largely accepted as being a necessary instrument in Clio's weaponry, albeit in some cases a weapon of deterrence rather than attack.
Over the last ten years the use of computers in the work of historians has increased dramatically.
(Schürer and Anderson 1992; Igartua 1991).
This development is undoubtedly due to the greater accessibility that personal computers have brought to computing in terms of cost, ease of use and reduced dependence on others.
Although originally adopted mainly by those sympathetic to quantitative approaches to the study of history, with the increase in computer-usage the methodology base has also spread to embrace qualitative analysis and other forms of textual processing (Information Technology in Humanities Scholarship 1993; Teibenbacher 1989; Metz 1987; Hall and Colson 1991).
Those that still persist in the belief that the use of computers in the study of history equates solely to quantification are about as out of date as the dinosaur-like machines available to the computer-using pioneers of the 1970s.
It is true that the acceptance of computing is still greater in research rather than teaching, but it must be said that the latter holds great potential, some of which, hopefully, will be unleashed through the injection of much-needed resources via schemes such as the Computers in Teaching Initiative (CTI) and the Teaching and Learning Training Project (TLTP).
Equally, the current trend of research councils to favour formal training, an important component of which for history will be computing and methodology, should bring with it a fuller measure of recognition of the skills acquired by the computer-using historian (Denley 1990; Kruse 1991).
In keeping with this life-cycle type development, it is also the case that a culture of secondary data use is gradually growing in which there are increasing demands for access to the machine-readable historical data created by others.
For some this demand is generated by the increased desire to undertake comparative research, which in part is further fuelled by greater accessibility to information.
Computer Aided Learning (CAL) also places high demands on access to pre-processed machine-readable source materials.
To facilitate these demands, as well as to preserve the files of historical documents currently being transformed into machine-readable form for the use of future generations, an Historical Data Unit has been established as part of the Data Archive at the University of Essex to collect, store and disseminate machine-readable data of interest to historians (Anderson 1992).
Yet despite these important developments the situation is far from being a bed of roses.
Although the increased awareness, accessibility and use of information technology by historians has and continues to bring many substantive gains, the impact of technology on contemporary society will undoubtedly also bring a host of problems for the future study of history.
Due to the very nature of history with its concern for the reconstruction of the past, these problems lie more so with history than with any other discipline.
Not only was the advent of computing perhaps rather longer and more protracted than in some other disciplines, but invariably it is the case that the very nature of computer application in history is rather different, and it is this difference that lies at the root of the oncoming problem.
Unlike the vast majority of computer-using social scientists, a basic difference lies in the fact that currently the work of the historian focuses around the computerization of documentary sources originally compiled for manual purposes.
This contrasts with non-historians for whom the data they use are generated from the very start of the research process, either via questionnaire or some other survey method, with computer analyses in view.
It is a subtle difference, but one that is fundamental to the problems posed by advances in information technology which the historian of the future will at some point, sooner or later, have to face.
Because of the historian's reliance on the availability of information compiled by previous generations — back-projected third parties, if you like — rather than the self-compilation of information or the use of that compiled contemporaneously by third parties, the availability of ‘documentary’ source materials to the historian's of the future may prove to be a significant problem hindering the study of history.
Awareness of this problem is by no means new (Mallinson 1986; Schürer 1985; Hedstrom 1984).
It is, no doubt, already familiar to all those attending this meeting, yet still little appears to have been done to overcome it.
In brief, developments in technology have resulted in a situation in which increasing amounts of information are stored and communicated electronically.
In the US it has been estimated that by the year 2000, three-quarters of all government transactions will be undertaken electronically (Taking a Byte Out…1991, 2).
It is doubtful if the corridors of Whitehall and the British civil service will quite match this record, but the simple point is that a host of records of potential interest to the historian that would have manifested themselves on paper will no longer do so.
And the same is true of non-governmental records.
Businesses large and small are increasingly using ‘office automation systems’ to communicate internally and externally, to maintain staff records and to monitor financial transactions in the form of sales, costs and budget forecasting.
Nor should we forget our latter-day Darwin: the diarist, novelist or would-be intellectual sitting at home with his word-processor.
Clearly paper will not be abandoned altogether and it is likely that trivial notes of the type cited at the beginning of this article will invariably be recorded as manuscripts.
Yet much, even if publication or long-term preservation is not intended, will be recorded and stored electronically.
To return to Darwin, leaving aside curious insights to his personal life, it is clear that the examination of his manuscript notebooks and papers preserved in the Cambridge University Library alongside the publication for which he is most well-known adds a richer and more penetrating interpretation of the man and his ideas.
But how would these be preserved if he were writing them now, and what would the historian of the future make of the sanitised, word-processed, spell-checked notes that might be left behind with every trace of the intellectual evolution of his argument having been edited and re-edited from the text?
It is clear, therefore, that the archival preservation and maintenance of machine-readable files must take a high priority if the historian of the future is to be presented with the tools to do his job.
The archivist must become the keeper of discs as well as the printed and hand-written word.
It is, after all, a relatively straight-forward logical development of the archivist's role of custodian of records, dealing with what are essentially transactions of one kind or another: at a national level between states, between government departments and between government and subjects of the crown; at a local level between private individuals, within and between local administrative bodies and between such administrative bodies and the individuals they serve.
Traditionally the records of these transactions have tended to be stored on parchment, vellum or paper and the job of the archive has been to preserve these information storage media.
The fact that the storage media are changing rapidly and increasing amounts of transactions are now being stored digitally on magnetic media rather than on a physical ‘page’ does not change the logic of the archival process.
Archivists need to raise their heads above the paper mountains that surround them in order to develop a realistic strategy for the preservation of the new generation of records.
And not a moment must be lost.
The pace of technological change has already rendered obsolete many items that few would argue amongst the worth of preserving.
For some fifteen years now researchers wishing to have special tabulations compiled from the 1961 census of England and Wales have been refused due to‘technical difficulties’, and more recently it has been discovered that the machine-readable ten per cent sample of the 1971 census for Scotland is no longer accessible (Marsh 1980; Schürer 1985).
To think that the Archives are standing idle by, however, would be to give a false impression.
A lead of sorts has been given by the national archives of north America with the Center for Electronic Records of the National Archives of the United States having already archived in access of 10,000 records or wearying size and complexity, and with the Canadians also pursuing an active of storing governmental records in machine-readable form (National Archive 1991; National Historical Publications 1991).
For the UK we are informed that the Public Record Office (PRO) has developed a computer output to laser disc (COLD) system for the storage of electronic records, yet we have no idea of how many or what categories of records are being preserved (Higgs 1992).
The preservation of information captured in machine-readable form undoubtedly presents a host of problems to traditional archives.
First, the magnetic storage formats which are fundamental to computing have been demonstrated to be inherently unstable.
Anyone with a home cassette player can testify to the questionable long-term durability of tape recordings.
The same is known to be true of the reel-to-reel tapes favoured by the older mainframe computers.
The life-times of Digital Audio Tapes (DAT) and Exabyte tapes (similar to home video cassettes) are claimed to be longer, but how will they perform over ten, twenty or fifty years?
Without fairly constant monitoring it will be hard to tell.
Equally, the same story holds true for Optical Laser and Compact Discs (CDs), although hopefully to a lesser degree.
Hi-fi officianardos are already advising people not to be too hasty in relinquishing their black vinyl record collections for fear that the CDs that they are replacing them with will fade in sound quality after ten years of use.
The same fears must apply to non-audio recordings.
A second problem is caused by the ever-changing nature of computing hardware and software.
Machines that were seen to be at the forefront of technology just a few years ago are now readily replaced and cast-off with little or no second-hand value.
It is true that the market-leading software packages have perhaps displayed greater longevity, but whilst the base product apparently remains the same version numbers change rapidly with incompatibilities being introduced with each new version.
Although in the PC world the Macintosh and IBM-backed MS-DOS operating systems have become the two most dominant products how much longer are they likely to remain so?
And what of those that have already fallen by the wayside?
In search of a solution to this seemingly constant change some have advocated an approach which broadly demands the long-term maintenance of the hardware and software environments required to interpret the machine-readable files to be preserved (Swade 1990; Swade 1992).
Although such an approach may appeal to those interested in the operation of machines as historical artifacts and in some cases is vital to ability to ‘read’ those files which technology has already passed-by (a classic example being the maintenance of punch card readers to process the already long-forgotten decks of eighty-column cards currently being rediscovered in the corners of filing cabinets, broom cupboards and the like), a more efficient strategy is to be found in the transfer of machine-readable files to a hardware and software independent format.
Yet despite the soundness of this principal, the practise may prove to be far from straight-forward.
It will require constant monitoring of the technological situation and may in the longer term necessitate several changes of format to keep pace with changes in storage devices and technique.
And, of course, as more complex relational-based software products are developed — multimedia packages, hypermedia and hypertext — the task of rendering the information stored within them truly software independent without significantly reducing their contextual framework becomes more challenging (Zweig 1992).
However, all this said and done, just as historians and archaeologists are today able to reconstruct the meaning of obsolescent forms of recording such as hieroglyphics or the structure and form of ancient buildings through the analysis of postholes, one must not underestimate the ability of future generations to decode the electronic records that are being created by today's society.
But neither should we rely on it.
Lastly, a further problem, and in practical terms possibly the most daunting, is the general lack of technical skills and awareness displayed by the traditional archivist.
Without wishing to be demeaning, computer literacy and competence is not particularly high on the list of archival training.
Gaining the human skills and physical infrastructure necessary to meet the challenge that the large-scale preservation of machine-readable records demands presents enormous resource implications to an already under-resourced archival service (Cox 1987).
Quite rightly strategies to cope with the problems of electronic records are being implemented from the top down, with the various national archives taken a leading role (Higgs 1992).
But if the problems of training staff and investing in the necessary hardware and software systems are proving burdensome to national institutions, they will surely place an intolerable strain on the budgets of local archival services, many of whom are already faced with reduced hours of opening and the like simply in order to survive.
However, it must be remembered that technological innovation has a positive as well as a negative side.
An obvious advantage can be seen in the reduction of the physical storage space required to keep machine-readable records, and in this regard optical scanning may also prove beneficial in alleviating current storage problems (van Horik 1992).
Equally, the fact that in the case of machine-readable files technological advances reduces the need for archival records and their users to be located in physical proximity to one another, the possibility to link and integrate electronic cold-stores removed from the archive per se may result in significant economies of scale (Morris 1992).
Unfortunately the mere preservation of machine-readable documents is not the only problem confronting the archivist.
As archivists are acutely aware, even if electronic records are made ‘readable’, to be of use to the historian of the future the context in which the document was created also needs to be retained (Zweig 1992; Higgs 1992).
This is true of paper records, just as it is so for those stored in any other form.
Indeed, traditional archivists devote much time and resources to providing users with a description of the administrative framework that gave rise to the generation of a particular record or class of records.
The many detailed catalogues and calendars stand as testimony to the archivists’ skill in providing this information.
But the skills required to produce the necessary documentation giving details of the provenance of electronic records, enabling the future researcher to determine how the information was collected and stored, why it was collected and the responsibility for the collection, are markedly different from those displayed in conventional archival practise.
Guidelines for the documentation of machine-readable files have already been suggested for historians currently generating computerised transcripts of historical source materials, and the adaptation of these may prove to be very beneficial.
But the process differs significantly from the source criticism which forms the mark of good historiography (Reinke, Schürer and Marker 1987; van Hall 1989).
In addition to the administrative setting of the record, a detailed account of the technical environment in which the record was created is also required — for example, did encoding of information take place, and if so how was this performed?
Again, this requirement not only stresses the need for the traditional archivist to acquire new skills, but also has important implications in regard to the collection of this information.
Currently, much of the archival work reconstructing the administrative framework of the deposited documents is carried out retrospective to their creation.
Yet in the case of electronic records the details of provenance are all too quickly lost following the process of generation.
The result is that to be effective the documentation of machine-readable files has to be compiled in parallel with their creation rather than in serial.
This simple requirement would demand a radical change in the relationship between the eventual custodians of machine-readable records and the depositors: those generating such records.
Again the resource implications are potentially enormous.
For national institutions such as the PRO with their government departmental record officers a practical solution may be found, but for provincial archives this demand may prove impossible without the introduction of wide-reaching enforceable legislation.
The problem of documentation is further confounded when one considers the nature and range of electronic documents currently being produced.
Detailing the provenance of files such as letters, departmental policy documents and surveys may not prove too problematic, but what of internal electronic mail and memoranda?
How does one ensure that the history of their generation — who received them and when, and how they relate to other items of information — is fully documented (Zweig 1992)?
And what living or organic forms of data such as continuous updated accounts, or records of employment or social benefit are preserved Not only does their documentation create a problem, but the intellectual definition of what constitutes a record is clearly troublesome.
Should one take sample cross-sections of the material on an ad hoc or regular time basis, or should one attempt to capture the ‘record’ in its entirety?
Yet is this taking things just a little bit too far?
To the medieval historian who is invariably left with just the concluding record of administration procedure, this discussion on documenting the provenance of data generation may smack a little of over spoon-feeding the historian of the future.
The interpretation of what remains as historical evidence is after all fundamental to the historians craft, but the deliberate preservation of historical source materials in the absence of context (or worse still the failure to preserve them at all) surely does little justice to the society in which we live.
Irrespective of the immense problems posed by the preservation of electronic record that the traditional archives are now having to face, it would be wrong to claim that many of these problems are entirely new or unchallenged.
In the social sciences and humanities specially designated Data Archives already have a benefit of some twenty-five years of experience in storing and disseminating machine-readable files for secondary use (Anderson 1992).
Indeed, they already store numerous records generated by government agencies and have developed strong links with the producers of machine-readable data.
Moreover, they have developed internationally agreed models of Study Description and are particularly active in the production of guidelines for the documentation of electronically produced data and standards for the hardware and software independent storage and archiving of machine-readable holdings (Dodd 1982; Neilson 1974; Rasmussen 1981).
It is true that the materials they hold do not embrace the full range of information that future historians would wish to have at their disposal, but the expertise and experience that they have established should not be under-estimated.
Given the resource implications facing traditional archives in their attempt to provide archival coverage to the electronically produced records outlined above, there is a real danger in reinventing the wheel, or worse still, several wheels.
The UK faces a particular problem in that the peculiarities of the Public Record Acts empowers the PRO to act only as custodians of the records generated by government.
The ‘ownership’ of records, whether in machine-readable form or not, is still vested in the departments that generated them.
The social science Data Archive has even less muscle, relying entirely on the goodwill of government departments to deposit their material.
Acknowledging the problems of information confidentiality and the need to retain machine-readable records for up to a hundred years before they can be publically released, it still seems feasible for the various interested parties to forge links in order to share expertise and avoid the needless duplication of effort.
Local archives also need the support of larger institutions if they are to provide the necessary technical facilities demanded by the storage of electronic records.
Surely, all of this points to the urgent need to define a common strategy and a corporate working agenda.
Above all, if the machine-readable records of today's society and those of future societies are to be preserved for future use, both resources and direction in terms of policy have to be provided centrally.
PART V.
The Future
25.
Electronic documents and the history of the late 20th century Black holes or warehouses — What do historians really want?
R J Morris, Professor of Economic and Social History, University of Edinburgh
An increasing number of archivists and a few historians are coming to believe that a major change has taken place in the manner in which human society creates the evidence which will be used by the historians who, in the future, come to write about the late twentieth century (Morris,et. al. 1992).
The changes brought about by electronically based information technology are so fundamental that, for the historian at least, they must be equated with the invention and spread of printing or perhaps even the initial development of the written record.
Each of us have different ways of trying to express this change.
Last week (June 1993) I brought home a letter from my daughter.
It had been sent by e-mail, transferred to a 3.5’ floppy disk and as a source of information was useless without specific software and hardware.
The medium was no longer the message.
Access needed a technologically sophisticated method of intervention.
It was no longer enough just to know how to read.
It would be wise to set out the nature of the problem, because it is not one problem but many.
Electronically created data has been and is being produced in a number of different organizational and technological environments.
Each has its own qualities.
The manner in which archivists and historians are dealing and are ready to deal with electronically produced data, and the adequacy of this response and of potential responses varies with the environment and nature of the data concerned.
Since the 1950s there have been three major phases of electronic data creation.
Each has set the archivist and potential historian technical, organizational, and intellectual problems of increasing complexity.
The 1950s initiated the age of the dataset.
They were mounted on large mainframe computers for batch processing using a variety of packages and high level languages of which SPSS and Fortran dominated the social science world from which historians learnt so much.
In the corporate and financial environment, numerically presented financial and survey data was subjected to increasingly sophisticated statistical and econometric analysis.
The available evidence suggests that archivists and historians have been fairly successful in tackling the problems of the dataset and mainframe, although complacency would be wrong (Lievesley 1993).
One example of access to certain parts of the British census of 1971 (Schürer 1993) is a reminder of what can happen in the most favourable environments, whilst the account of the situation of Soviet and Russian data indicates how dependent electronic data is upon organizational structures (Moiseenko 1993).
By the late 70's, the world was full of PC's and word processing.
This expanded dramatically in the 1980s.
These machines were small in capacity, had primitive communicative abilities but slowly increased in power and capability.
All the while, in offices and homes everywhere, piles of floppy disks gathered in drawers, cupboards and archives.
Behind them stood the big brothers of the computing world, the Primes, Vax's and assorted IBM mainframes, more powerful and versatile in their capabilities and developing slow and awkward means of communication and portability with a degree of interactive processing that still left time for coffee and often for a three course meal.
The age of the PC has left us with a messy low grade grumbling crisis.
There are technical problems of some complexity.
There is no doubt that many disks will become degraded and that the variety and rapid rate of technological obsolescence which has affected the hardware and software involved has created major problems of preservation and access.
Much will be lost, but even where disks become unreadable they may well contain information which is ultimately recoverable.
Within the next ten years, a small and elite band of e-palaeographers will emerge who will recover data signal by signal probably using the pattern recognition capabilities of the new neural network procedures (Wilson and Blue 1993).
[1]
Intellectually the problems from this phase are fairly easy to handle.
It is true that we cannot see the crossings out in Tony Benn's diary diskuscript (Prochaska 1993), but the information structures on most disks from the PC era mimic document structures that are familiar from print and paper.
For this reason, the most thorough and reliable long term preservation procedure may well be to print out and preserve in microform (Morelli 1993; Morris 1992).
One of the few things predictable about future input technology is that it will include an optical scan element which will be at its best handling the output from IT related machines.
[2]For documents like this the lessons of TEI provide an increasingly thoughtful guide.
Documents have individual identities and the constant prompting for information on document type, author, origin, version, and information elements within the document are ones that should find a ready response from potential users and archivists of material from this phase.
True the archivist dealing with assorted disks from the 1970s office or even the politician's word processor will leave detailed markup to others, but the demands of producing simple headers seems very relevant here (Burnard 1993).
It is true that when historians come to the basic task of source evaluation and exploitation, they will not be able to examine the alterations and check the handwriting but strategies of text analysis are already available which will allow researchers to evaluate authorship and even make judgements on changing style within an individual author's corpus of text (Holmes 1991).
By the late 80s, the world of networks and powerful applications had arrived.
Databases, spreadsheets, windows, e-mail and many more phrases were added to the language.
The desktop computer grew in power and user friendliness until it was increasingly hard to tell apart from the workstation.
Indeed by the time a modern 486 machine or an SE or LC Macintosh had been linked into a network the distinction was almost meaningless.
The age of the network is by far the most imposing of the problems faced.
Not only are the technical and organizational problems huge but it is not clear that we even have the intellectual concepts needed to talk about the issues we face.
The meaning of simple ideas like document, text and context, of provenance and sequence fall slowly and inelegantly apart (Michelson and Rothenberg 1992; Hedstrom 1991).
Like the ages of flint, bronze and iron these phases were not exclusive.
They overlapped but each new phase rapidly established dominance in the world of information technology.
Thus the large datasets are still there, but those who manage them are more likely to place them on a file server with appropriate network access (Knight 1993).
The PC, the Macintosh and the rest continue to gobble up huge quantities of words and other information.
Many still stand happily alone but they all know that without an ethernet or some other network port or at least a modem, they are very low technology in the world of the network.
Information technology has become a central element in the bitter and disorderly competition of the post modern world.
There is a Darwinian struggle for survival between a wide variety of social, economic and political systems and that struggle is based upon the control of information.
It is not simply enough to amass capital, military might or political support.
Those who go to war with an non-networked PC are likely to be on the wrong side of the line or at least run out of tank spares first.
Spend an afternoon with the gung ho capitalism of the Harvard Business Review and the message is clear.
Those best placed regarding information systems survive.
The capitalism of the Chicago meatyards has been replaced by that of the information system; ‘…organizations have shed more than one million managers and staff professionals since 1979.…technology will enable senior management to monitor and control large organizations more effectively…
’(Applegarth,et  al. 1988).
In the face of all this, what is it that historians really want?
There are two nightmares.
In the first, everything is saved.
Storage is cheap, communication is easy and every scrap of information from the information rich fin de siecle is preserved for eternity in a series of great warehouses in the ethernet.
[3]Ann Arbour, Essex, Leiden and Moscow are on every desk.
For each document, there are at once a single copy and a million copies.
As the system is updated, nothing is lost for storage is so cheap that it is always more cost effective to slap a primitive TEI type header on each block of data and transfer it to the next store rather than sort out what is likely to be needed from what is not.
The world will become a gophers playground and those who navigate it best will command the earth.
[4]Some of the anxiety generated by the issues discussed in this volume arise from the awareness that electronic data has possibilities for future historical analysis which cannot be provided by paper based data.
The potential preservation of usage information as well as the potential for retaining huge quantities of communications and records suggest opportunities which even the historian of the nineteenth century can never have.
In the second nightmare, all is lost.
Technological obsolescence means that information rapidly becomes unreadable.
Punch card readers are already being sort by archivists.
That Superbrain with a quarter megabyte of memory has only a finite life and then any records and software dedicated to its operating system is material for the e-paleographers.
When Irish records were destroyed in 1922, it was because the Four Courts were shelled during the civil war.
In late twentieth century Moscow, the danger appears to be that the collective record of the Russian and other Soviet peoples may simply ebb away electron by electron (Moiseenko 1993).
We shall look back upon the 19th and early 20th century as the golden age of the written word and printed page.
A disaster begun by the telephone will be completed by the electronic network.
So what do historians really want?
The archivists at least need some sort of a provisional answer.
One barrier to giving even a provisional answer is the fact that we, as historians, have very little idea what is actually out there.
One element of weakness in this symposium is the need many of us have to take refuge in generality, anecdote and the familiar.
On this heady diet we move from intoxicating anticipation to deep despair.
We are at our best when discussing census and survey documents.
We have encoded and used versions of them from the past we study.
We learnt about those from our social science colleagues.
Diaries, diplomatic and intelligence records, the use of e-mail in the research process all come within our experience as historians, but after that what do historians really want?
The honest answer is another question; well what is there?
What happens once we get beyond the comforting world of the census and the diplomatic archive?
Let me do what historians should be able to do best: take examples, examine how they work, place them in context, and attempt to generalize.
The first is from Strathclyde Regional Water Authority in the west of Scotland.
A wide range of information technologies are in use producing substantial amounts of data.
They are not at the cutting edge of Harvard Business Review (HBR ) type networking but they are I suspect typical of many organizations.
Active development has and is taking place.
The overall strategy clearly involves creating an integrated network improving the efficiency and effectiveness of the Authority, but the current situation and day to day operations indicate an organization that is not yet fully into phase three.
At Water Department HQ is a GIS (Geographical Information System).
This links via a Wide Area Network to the laboratory which processes water samples.
The data from this is punched into the ORACLE database which physically resides at the lab (a mile or so away), and hence can be pulled down the line to map, say aluminium levels and check water quality against EC regulations.
At some point data entry will be done directly from the analysis process to the database.
Meanwhile at the 12 area offices customer complaints (e.g. poor water, flow, dirty water etc) are received and put into a PC.
These are collected and put into a PC database at HQ where the information can be used with the GIS to map the problem areas.
In another part of the system are meter readers (customers like industries, farms, and some shops pay by volume).
The meter reader punches readings direct into a hand held Husky computer.
This information is transferred directly to water HQ.
Bills can be issued within two days rather than fifteen.
When time is money that can be a considerable saving, or as the planning documents predicted, there was ‘a considerable improvement in cash flow’.
[5]From time to time, a van is sent down to the Regional Chief Executive's Department to get census data (on disk).
The sampling frequency of water depends under regulations upon population density.
Post codes are used to link specific lists of addresses to areas of water supply; i.e. areas which are supplied from one asset (e.g. a storage tank, a reservoir etc).
Digitizing boundaries of these areas was one of first GIS investments made.
Elsewhere in the system is a computer based upon telemetry (e.g. electronic measures of flows, pressures etc) so that a complaint of water failure can be related to the supply network with accuracy, and relevant teams of engineers called out.
This leads on to basic office systems and personnel records.
There are several lessons to be drawn from this.
In theory and in the planning documents this can all be linked into a seamless network.
In actual contemporary practice this is an intermediate technology.
The Wide Area Network moves information around parts of the system.
Elsewhere it is the disk in a van, whilst the paper document and punch operator still has much work to do.
This is characteristic of much current practice.
In theory the age of the seamless network and paperless office has arrived.
In fact hardcopy and the four drawer filing cabinet still play a major role in many ‘information systems’.
Indeed, there are those who consider that hardcopy has a functionality which e-documents can never completely replace and it will always have a place in information systems.
[6]
Secondly data structures are already beginning to come apart from use structures and GIS plays a central role in this.
Thirdly politics plays an important part in this.
The potential privatization of water services in Scotland is a contentious political issue.
The improvement in information technology with the efficiency gains involved may be seen as a strategy which makes the Authority more attractive to a potential profit seeking buyer or as a strategy which enables the Authority to defend its current place in the state sector.
Your choice depends on your politics, nationalist/unionist as well as left/right.
A sub-theme running through many of the papers in this collection concerns the politics of data; data for sale, data for citizenship, data as power, data as a cost…
. Historians need to create their own politics of data; to assemble interests which have a stake in the long term retention of data.
These include very practical interests such as the contaminated land legislation, as well as more general cultural reasons such as the needs of companies to sustain reputations and human societies to explore their own identities and pasts.
For historians the politics of information involve creating and sustaining attitudes and values in society at large which will support and demand the retention of the information sources which will be the basis of writing history in the future.
Initially information preservation will depend upon the compulsions of the organization.
EC and Scottish Office regulations and public access to information requirements will ensure the short run retention of data as will potential legal and contractual disputes.
[7]The survival of documents has always and substantially will still depend upon the needs and compulsions of the document creators, as well as upon the survival of the organizations that guard these documents.
This is nothing new.
What would the historian want from the relatively simple records of the water authority?
There is a very obvious potential here for a history of water supply, of a public service in the age of privatization.
There is a major source for the history of the environment in Scotland which would perhaps be linked with other archives when the ‘green’ generation writes the history of the ‘greenhouse’world.
Beyond that is the history of the impact of IT itself on service delivery.
The second example comes from the growth of EPoSS (Electronic Point of Sale Scanning) in retailing, in other words the increasingly common barcode which is passed over a scanning device linked to the cash register.
The accompanying bleep is now a familiar part of supermarket shopping.
The immediate and obvious impact of all this is an increase in speed and accuracy at the checkout.
In Britain, Tesco, the grocery supermarket chain, estimated a 20% increase in speed enabling a 10% reduction in the number of checkouts (Davison 1993).
The implications for historians of business, of labour relations, of consumer habits as well as of the impact of information technology go far deeper than this.
‘many retailers use EPoSS to replenish a perpetual inventory automatically.
Linked into centralized buying structures, electronic sales and automatic order transmission to suppliers through EDI networks, centralized distribution systems deliver goods into stores without branches being involved in day to day stock control.’
(Davison, ibid.)[8]
It takes little imagination to see that here is a huge resource for the study of consumer and retailing habits, but if we follow the practice suggested recently by some archivists and take the network with its constituent data flows and data tables, software resources and analytical outputs as a whole, as the document itself, this is a major site for examining the cultural effect of these systems.
Each checkout contains a memory and a clock so that data is captured not just on sales but on the work rate of the checkout assistants.
In 1990, it was assumed that each assistant would average twenty items a minute (Cutter and Rowe 1990).
Thus IT and its record entered into the politics of the workplace.
Some analysts claimed that the system intensified work discipline and stress; for supporters, EPoSS enabled management to identify staff who needed to be withdrawn for ‘training’.
In retailing as in many other environments, IT has a double edged influence on power relationships and autonomy in the workplace which will be vital for those who study the perplexing topic of class relationships in the late twentieth century.
Remember that class relationships are part of a wider structure of systems of domination, and that the tension between empowerment and surveillance inherent in information technology is likely to be a major focus for studies of class structure in the 1990s (Morris 1979; Elliot and McCrone 1982).
In the mass retailing of most grocery stores, EPoSS increases centralized surveillance and reduces the autonomy of store management.
At the same time, other managements, notably in clothing and luxury goods, have realized that greater surveillance also means that subordinate management can be allowed greater autonomy.
In ‘craft’ retailing, the store manager and staff can consult central databases of information and back the ‘hunches’which come from face to face contact with customers rather than from postcodes and product flow analysis.
Why not let them?
If they are right profits increase.
If they are wrong central management will know within days (Smith 1988: 143).
The clear lesson here is that the IT archive, whatever form it should take, is not only the carrier of information which the future historian would want to use, but that the very form, structure and dynamic of that archive as it is created and used will make a vital contribution to an understanding of the social relationships, the culture and the power structures of the late twentieth century.
At their most fully developed business information systems provide a formidable challenge to the creativity of archivists and historians alike.
SABRE is an airline reservation system, developed by American Airlines in the 1960s but now generally available.
The sheer quantity of information is mind boggling.
In 1990, its database contained 45 million fares, handled 40 million fare changes per month and created 500,000 passenger name records per day (Hopper 1990).
Do we really want all that lot (Campbell-Kelly 1993: 266)?
Before speculating on the value of such huge quantities of information it is important to see this as part of larger systems of business information and decision taking.
In the 1980s, management came to see its information system as part of a variety of decision support systems.
In the 1960's such applications were dominated by financial control, transaction and personnel records.
By the 1980s, innovation was taking place in the recording and analysis of markets, transportation and logistics (Eom and Lee 1990).
Conference rooms were equipped at great expense and then torn out as views on best practice changed (Lee,et al. 1988).
Two important things happened in such systems.
First, the use structure of information became even more detached from the data storage structure of that information.
In other words, the information as experienced by the user rarely took the forms in which it was held in the memories of the system or network.
Secondly the archive base became much more than the database (or rather its constituent tables).
Between the management conference or decision maker, there was an increasing layer of software intervention.
These were much more complex than the statistical packages and linear modelling applications of the 1960s.
Two sorts of applications deserve attention.
The first are the rule based AI knowledge systems which attempt to encapsulate past experience and collective expertise.
The diplomatic or business historian may gaze at the e-text archive and ask who decided what, but in situations which involve rule based elements in a Decision Support System (DSS), the historian may well ask whose decision?
Those who will seek to understand United States policy on Kuwait, Somalia and Bosnia in the 1990's will want to look not only at the personalities and style of presidents and advisers, but at the content of the knowledge base of their DSS, and above all at those who designed the system and created its rule base.
The second software development is the increasing intervention of visual interfaces between data, complex statistical and rule based manipulations and the decision taker.
GIS is only the most familiar of these (Angehrn and Lüthi 1990).
The old adage garbage in garbage out has been replaced by the new, garbage in pretty picture out, and it is dangerous.
Equally serious for the historian is the knowledge that the decision takers are continually looking at virtual documents which are only infrequently captured for hard or electronic copy.
For example, those who bought an air ticket to come to the seminar which proceeded this collection of articles were probably involved in creating several virtual documents as they discussed travel arrangements with their booking agent.
The full significance of this fleeting moment is only evident when the letter page of HBR reveals a bitter contest over screen design between American Airlines, the developers of the system, and trans Atlantic competitors; ‘a chief legal officer of a major international airline…had just been involved in a long, rancorous struggle to get its non stop flight from Europe to the United States off the 23rd of SABRE's 23 possible screens’(HBR , July-August 1990: 176).
So anyone who wants to study competition, travel and world capitalism in the 1980s for a PhD programme in fifty years time had better take a good look now.
The answer to one question raised in this seminar is very clear.
Software must be part of any archive strategy.
Equal care must be taken with the knowledge systems that accompany such software.
This does not just mean clever things like the expert system in the White House conference room, but simple things such as the British postcodes for the early 1980s and the digitized boundaries of old GIS systems.
Software must be seen as capitalized technique.
[9]It is, as the AI evangelists say, a store of the skills and experience of its human creators and contributors.
As such its nature is essential to an understanding of the decisions and structures of the late twentieth century.
In the nineteenth century account book, the data form and the content, the use structure and the data structure are still closely related but even in these documents, the accounting manuals of the 1830s are important for interpretation.
Software and its related manuals are equally important.
As one participant has suggested, the first intellectual task is to set on one side an archaeology of knowledge which has served us well for at least three centuries.
The world of library, archive and museum, of books, documents and things is breaking under the impact of information technology (Rayward 1993).
Simple notions such as document, sequence and provenance are already gravely compromised.
There is no doubt that future historians will look to a more complex structure of data.
The concepts they use may involve the following, data elements, data tables, databases, knowledge bases, software applications, interfaces…
. Some aspects of all these elements of our information systems will be essential to understanding the present in the future.
The outcome will be influenced by the needs of those who own and manage data.
Already the information manager and the archivist are beginning to merge in a variety of business and government locations.
As has happened in the past the information elements selected for preservation will be influenced by the needs of data owners.
Like all social historians, I find more property deeds than grocery bills in the archives.
Property deeds were needed in court.
The need to preserve experience for knowledge based systems, the need for verification in case of dispute over contract or other liabilities, legal requirements and citizens data access rights will ensure the preservation of wide ranges of data.
Organizations need to be able to attribute credit and responsibility just as historians do.
Bearing this in mind software and expert systems are probably most at risk.
To take a simple example, the Social Science Data Archive at Essex has a fine run of survey data from Family Expenditure Survey and General Household Survey well suited to the needs of those who want to create a time series as a part of current social science analysis.
I doubt that an historian of welfare policy in the 1970s could access data and run it (i.e. view it) just as was done by the policy makers of the 1970s and their advisers, yet we can read the parliamentary reports of the nineteenth century at exactly the same speed and in exactly the same form as the policy makers who used and created them.
In an age when information is power, there are clear motives for archiving.
HBR again, ‘Information systems will maintain the corporate history, experience and expertise that long time employees now hold’(Applegate, et al. 1988: 135).
There is both comfort and danger in this.
As in the past preservation will be influenced not by what historians want but by the needs and fortunes of the social structures which created and guarded such information.
Is this enough?
Thus historians need to create an awareness amongst themselves of the shape and the nature of the information structures which will be used in the future for the history of the present.
If they are to convince archivists and others that preservation should be driven by more than present need, then they need to consider how to answer the question, what are you going to do with it all?
Long ago I went to a seminar of archivists who were confident that particular instance papers could be sampled by a variety of statistical methods and if necessary could be anonymized without damage.
An account of nominal record linkage, I hope, changed several views on this (Morris 1985; Baskerville,et al. 1992).
Concepts like privacy and confidentiality still threaten the long term preservation of nominal records.
If some of the more extreme versions of the current draft EC directives on this subject are allowed to dominate, then the possibility of capturing the individuality of ordinary people and their fortunes in the late 20th century may well be lost.
Historians who write about the late 20th century in the late 21st century will read all the complaints about industrial society destroying individuality and identities.
If concepts of privacy are allowed to operate in an uncritical manner, then such historians will look at the records and say, yes indeed these people had no names, only fragmented unlinkable traces in the disparate records of courts, government and business.
With this in mind historians need to do three things.They must give greater publicity to the electronic methodologies which they are already using.
They must extend knowledge of and training in these techniques amongst themselves.
They must also plunder with much greater vigour the techniques of neighbouring disciplines.
They must also enter the dangerous ground of anticipating the techniques which might be available in the future.
Only then can we give even a sensible guess as to the potential of e-documents and their associate structures and environments.
It is in this context that the strictures on historians for their general lack of curiosity in text analysis needs to be considered.
At the same time, these strictures need to be put with much greater precision.
Historians need to be convinced that prefixes like ‘literary and linguistic’ are an invitation and not a warning to keep out.
Other forms of analysis of communication are likely to spread.
Not just the use of the concept of ‘noise’ but dynamic forms of network analysis tracing power structures and patterns of social and intellectual interaction.
The sheer quantity and complexity of information contained in these archives indicates that a variety of numerical, statistical and symbolic forms of representation will be essential.
With this in mind the use of graphic and visual interfaces will be essential.
Not all historians are numerate; still fewer are mathematicians of any sophistication.
The same applies to their potential audience.
All involved will need these interfaces for the same reasons as the political and business managers whose decisions are being studied.
So far historians have made little use of AI and expert systems.
This is likely to change.
The world of neural networks and pattern recognition is likely to act as a major guide.
The complex world of coding, the multiplicity of codes and of the non-coding structuring of data; the task of searching for patterns in data are all likely to benefit as these techniques become more user friendly and offer shorter learning curves.
Conclusions arising from this symposium must be provisional.
They include practical matters such as the need for historians to be better informed about the nature of information creation in the present, the need for archivists to be involved at the creation stage of ‘documents’ rather than 30 years afterwards.
Preliminary conclusions must include the need for pluralism in retention policy.
Attention must be drawn to the value of Internet as an access medium, the importance of software and knowledge bases as well as data in the traditional 1960s sense.
The training needs of historians need to be continually discussed in the context of information technology.
But above all we need to finish by re-asserting the ‘politics’ of doing history.
Let us finish by reminding ourselves of the traditional skills and purposes of historians.
It is not a matter of finding our way along career paths by seeking something ‘new’.
Our central task is not just to understand the past but also to relate the past to the present.
For this reason each generation requires new stories or the retelling of old stories.
Some aspects of the historian's task will be constant.
We shall create an awareness of change over time, preserve an awareness of context and prepare a critique of the nature of sources.
We shall plunder related sciences, especially the social sciences.
The direction and detail of the manner in which historians of the future tell the story of the present will depend upon the nature of society in the next century.
Historians will seek to understand the late twentieth century in order to relate it to the collective identities and experiences of their own period.
If the current analysis of ‘post modernism’ is any guide to this, that experience will be complex and fragmented (Harvey 1989).
The archives of IT, the e-data, will be one base from which that complexity and fragmentation can be recreated and interrogated.
Even the current fragmentation of historical practice should warn us that historians will want to tell many stories.
IT and e-data will not only be a source of these stories but part of the story itself.
IT has been part of the creation of so called post modernism, through its ability to destroy the meaning of time and space.
The ethernet at once has unity and no unity.
Each document is at the same time unique and existing in a thousand places.
Culture and experience have fragmented.
Citizenship is access to your records in a database.
The family that e-mails together stays together.
Identity and cultural assurance will become access to the e-data of your ancestors or that of the cultures and social formations with which you identify.
Individuals and communities will increasingly carry multiple identities and experience with them.
The literature of the knowledge-based society already talks of organizational structures which are flatter.
They mean that the work team has replaced the hierarchy, that knowledge has replaced capital, that working lives involve tasks not careers.
The class structure is becoming dominated by the knowledge mercenaries who hire themselves out to the feudal barons of top management and state agencies, each of whom is as good as the last battle.
This is an anarchy in which the tension between surveillance and empowerment will make and break individuals and social and economic organizations in rapid succession.
Migrations, mixed marriage, the constant reformation of household and sexual alliances, broken work patterns will mean that class, ethnicity, gender roles and religion which not only divide communities but will also run through individuals.
The meaning of age, gender, religion and skill will shift with bewildering rapidity.
In such a heady and unstable environment, the historian must undertake two major tasks.
Individuals must be able to understand and explore the multiple identities and experiences they, their families and friends will carry with them.
Communities must be taught to celebrate the complexity and diversity, the many stories, which the past has brought into the present.
E-data is a means to this complexity.
If the e-data is allowed to disappear into the black hole of technical obsolescence, then the writing of history will fall into the hands of the ideological warlords of the company handout, the national curriculum and the aggressive myths of ethnic purity and cultural uniformity.
The historian must also struggle to see the future history of the present in terms of the unity of human condition.
If any sense of unity is to be preserved it must be created by the historians who tell the story.
The unity may be in data structures, power structures, or in environmental trends.
Meaningful debate will require well stocked electronic warehouses and historians and archivists able to search and manage their cargo of resources.