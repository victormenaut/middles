

WHITE PAPER pushes for wealth creation The UK sets a strategy for science and technology
The Government's White Paper on Science and Technology is set to have a significant effect on universities, industry and education after its announcement that the funding priorities of the £6 billion R&D budget will be aimed at harnessing Britain's strengths to enhance wealth creation.
However, the White Paper made no reference to any increased funding.
Realising our potential: a Strategy for Science, Engineering and Technology — the first major policy review in the area for over 20 years — was launched by the Minister for Science and Technology, William Waldegrave, at the end of May.
As Waldegrave himself admits, Britain has a strong science base, but it has fallen behind other industrial nations in transferring the results of research to the UK market place.
Often, a technology and its economic benefits have been lost to Japan or the US.
At the press conference, Waldegrave recalled a visit to a UK university, where he had noted a visitors' book full of Japanese names.
The White Paper's aims are therefore to help the business community secure the maximum economic benefit from science and technology, while continuing to support excellence in research.
Industry has welcomed these objectives, along with the measures to increase technology transfer and improve the links between innovators and the market place.
Stuart Nelson, Managing Director of AEA Technology's Industrial Business Group, welcomed the White Paper, and in particular the fact that the Government is prepared to support ‘near market’ research.
‘Many excellent ideas have failed through lack of support at this crucial stage of their development,’ he said.
(Dr Nelson explores the process of innovation further on p11).
The Science and Engineering Research Council (SERC) also responded positively to the policies but was disappointed that opportunities had been missed.
For instance, SERC felt that changes to the Research Council arrangements should have been more widespread.
Tim Buisseret, Researcher at the Programme for Policy Research in Engineering Science and Technology (PREST) at Manchester University, said that his first impression, like that of many people, had been that the ideas were sound, but had ‘not gone far enough’.
One of the criticisms that Buisseret foresaw coming from academics was that ‘the bias was unexpectedly towards changes in the already strong science base rather than addressing priority problems in industry’.
One of the fundamental aims of the White Paper was to bring science and technology firmly into the political arena, and to affirm the importance given to them by the Government; and indeed, science and technology have undoubtedly found a higher placing on the political agenda.
As William Stewart, the Government's Chief Economic Adviser, said, ‘a year ago we had no Minister, we had no Office of Science and Technology, and no White Paper’.
Compared to previous years 1993 has seen a flurry of activity, with a medley of reports and debates on issues such as the economic impacts of hosting international facilities, returns on research and development spending, and various reviews of government expenditure, including the funding of the Government's own research facilities (see related item on Warren Spring, p4).
Strategy ground rules
One of the first areas to feel the effects of the Paper will be the research councils.
Taking effect from September 1994 these are to be remoulded to reflect changing research priorities.
The Science and Engineering Research Council will be converted into an Engineering and Physical Sciences Research Council (EPSRC) and Particle Physics and Astronomy Research Council (PPARC).
The establishment of PPARC as an entity will help to insulate EPSRC's smaller-scale science from major funding fluctuations.
There will also be a new Biotechnology and Biological Sciences Research Council (BBSRC), which will be a modified version of the Agricultural and Food Research Council.
In order to define explicitly the roles of the Research Councils in wealth creation, they will each be given mission statements, as they themselves advised.
A new post of Director General of the Research Councils will be created within the Office of Science and Technology (OST).
The Director, backed by a group of experts, will advise ministers on the performance and resource needs of the science and engineering base.
Essentially this is part of the process of absorbing the Advisory Board for the Research Councils under the auspices of the OST.
There will also be one or two changes in Government responsibilities.
ACOST, the Advisory Council on Science and Technology, is to be given a new name and a newly defined role.
As the Council for Science and Technology (CST), one of its aims will be  to ensure that the Government draws on outside advice when deciding its research spending.
One mechanism for delivering this advice will be the Technology Foresight Programme, which will be conducted jointly by industry and the science and engineering business communities.
The remit of the Technology Foresight steering group includes overseeing the collection of information on scientific opportunities and potential market applications, and with consultation from researchers and industrialists, to produce working documents that will advise CST on the technologies that they judge to be of most importance to the country's economy.
For the Government's part, each year it will publish a ‘Forward Look’ that will give industrialists and researchers a clear and up-to-date statement of the Government's strategy.
The White Paper also restructured post-graduate education and research.
As a result, there will effectively be fewer PhD students.
This is part of the Government's strategy to improve conditions for researchers in academia.
Of those students who do embark on a PhD, more will move on into industry.
To enable these transfers from academia, the PhD will become more relevant to industry, and will include a compulsory pre-PhD MSc year.
Improving innovation performance
Alongside the organisational changes aiming to improve exchanges between Government, the science base, and industry, the industry departments, supported by the Department of Trade and Industry's (DTI's) Innovation Unit, intend to scale up their activities to promote awareness of the importance of innovation among senior managers in the business community.
Schemes such as the Carrier Technology Initiative, LINK and the Teaching Company Schemes will continue to be promoted, and Waldegrave clearly wants to see them grow.
There is good news for smaller companies, with government support for both the network schemes that have been successful in other parts of Europe, and one-stop shops.
There will also be a bias in favour of SMEs when Government research funding is allocated.
DTI speaks out on its responsibilities
As the majority holder of funds for technology transfer, the DTI's announcements relating to the White Paper are where much of the action is.
It is from changes in DTI innovation policy that SMEs will reap their rewards.
Not entirely in line with the Waldegrave's White Paper, the DTI has announced a shift towards exploiting existing technologies more fully at the expense of supporting new technologies.
Although this news may contradict the aims of the Technology Foresight Programme, it is in line with Government promises to support near-market projects.
The DTI's budget for industrial innovation is some £125 million.
This makes up only about 2% of industry's own spending on R&D, and in the past many of the DTI's handouts have landed in the hands of the large firms which account for so much of industry's total.
The DTI has therefore introduced measures to adjust funding allocations in favour of smaller firms.
Small companies need local access to science and technology expertise and services, so the Department will build on local innovation networks and improve companies' access to one-stop shops.
As a direct funding advantage to smaller companies, the SMART and SPUR schemes will continue, and SPUR will be further limited to companies employing less than 250 employees (as opposed to 500).
SMART is an award open to companies of 50 employees or less to help develop innovative technology.
SPUR supports the development of projects and processes under research.
Warren Spring merges with AEA Technology
The President of the Board of Trade, Michael Heseltine, put an end to speculation on the future of Warren Spring with an announcement in June that the government laboratory will merge with AEA Technology.
The move will bring together the world class environmental consultancy and technical services of the two organisations, and will create a National Environmental Technology Centre (NETC) within AEA.
Welcoming the merger, Brian Eyre, Chief Executive of AEA, said of Warren Spring: ‘Their excellent research and technical services in environmental science and technology are fully complementary to our own.
Significantly, there are a few areas of overlap, which will allow very effective dovetailing between our operations.
Moreover, we already collaborate successfully with Warren Spring in a number of key areas.’
Such collaborations include the joint management of SPS, an independent Separations Processes Service which provides industry with up-to-date process engineering information.
Warren Spring's main areas of research and development are industrial processing and environmental pollution monitoring and abatement.
Since its establishment in 1959 it has built up an international reputation as a centre of excellence in its fields.
The environmental capabilities currently at AEA — which already operates the UK's largest environmental consultancy services business — include waste management, integrated pollution control, energy, combustion, and an extensive range of environmental protection services.
The merger will, therefore, Eyre explained, produce a wider portfolio of environmental services for the customers, and provide the potential for developing more business than the teams would have achieved separately.
On-going contracts at Warren Spring are likely to be transferred to the NETC, ensuring that work currently underway will continue, including Government contracts such as those for air pollution monitoring.
A project team consisting of staff from AEA and Warren Spring will examine details of the merger, including the number of Warren Spring scientists who will relocate from Stevenage to AEA's Harwell and Culham sites in Oxfordshire, and the timing of their move to take their places in a fully-integrated operation.
However, it is hoped that relocation will be complete by April 1994.
The announcement followed a review, commissioned by Heseltine from the Cambridge company PA Consultancy, into the future of Warren Spring.
Warren Spring is one of the five DTI laboratories.
The Government is currently reviewing all the laboratories to identify how best to build on the progress made by each of them towards becoming more commercial and cost-conscious organisations.
The DTI had already announced in early April that it was appointing consultants to consider the practicability of privatising AEA.
AEA itself has appointed consultants and merchant bankers to help it develop and refine its work on strategic options as its contributions to the DTI review.
AEA welcomes new Chairman
Sir Anthony Cleaver is the new chairman of AEA.
Sir Anthony, part-time chairman of IBM UK Holdings Ltd, was chief executive of IBM UK from 1986 to 1991.
Sir Anthony became the new chairman on 1 July, succeeding John Maltby, whose term of office ended on 30 June.
Brian Eyre, Chief Executive of AEA, said, ‘I am pleased to welcome Sir Anthony Cleaver as our new chairman from the beginning of July.
He comes to us with a distinguished career in a technologically-orientated industry and I and my board colleagues look forward to working with him in taking AEA forward through an extremely important period in our development.
Sir Anthony said that he was looking forward to working with an organisation which has such a large number of highly skilled people.
‘I firmly believe that business needs to concentrate on adopting and exploiting innovative technology.
AEA has enormous strength in technology transfer and I will be aiming to use my background and experience to help create additional opportunities for what is now such a valuable resource for industry.
EC boost for air safety
European aircraft manufacturers and operators have joined forces in an EC-sponsored programme which is set to revolutionise airframe inspections by 1995.
Companies such as British Aerospace, Lufthansa and SAAB-Scania are working together in a project co-ordinated by AEA Technology.
Major air crashes, such as last September's accident in Amsterdam and the United Airlines DC 10 engine break-up at Sioux City in 1989, have been attributed to structural failure, and have brought under scrutiny the reliability of airframe inspection techniques.
The huge stresses to which airframes and engineering components are subjected, particularly during take-off and landing, cause fatigue which is detectable via flaws such as tiny cracks.
These can lead to catastrophic failures if inspectors do not identify them either visually or by applying non-destructive testing techniques such as X-rays, eddy currents, and ultrasonic inspection.
EC backing for the three year £2.1 million project follows the establishment by the Federal Aviation Administration of a major ageing aircraft research programme in the US.
As the average age of aircraft increases — a particularly large problem in the US — inspection swill become more of a burden on the industry.
If accidents are not enough of an incentive to improve techniques and procedures, the sheer expense of the latest technologies demands a review of inspection development.
The programme (BRITE EURAM) will introduce a complementary capability for Europe, which is needed if Europe is to stay competitive with the US.
The programme aims to increase inspection reliability and to standardise the inspection procedures of all the participating manufacturers.
It is hoped that these objectives can be met through developing airframe inspection techniques as a fully integrated system.
At present processes evolve, for the most part, independently — instruments are first developed, the procedures are written, and only later is the inspector brought into the picture.
The BRITE EURAM project, involving both manufacturers and operators, hopes to ensure that technology fits the purpose and is used correctly, and that new technology is transferred to the field as soon as possible.
In improving inspection techniques, attention will be paid not only to improving non-destructive testing techniques and developing new technologies such as shearography, but also to inspection validation and human errors.
Inspection validation is used to test inspectors, procedures and equipment by way of trials on test specimens with known faults.
These simulations allow weaknesses in an inspection system to be identified and rectified long before it enters practice.
Human factors experts are able to identify how to minimise mistakes by observing the performance of operators in different conditions and circumstances.
Performance falls, for example, if inspectors are pushed too far by the pressure to get aircraft back in the air and earning money.
The programme will draw on the experiences in the nuclear industry where reactor operators have demanded high standards of assurance of the integrity of safety-critical engineering components like pressure vessels.
It is in this area that AEA's integrated inspection techniques were developed.
New assay systems offers high sensitivity
A universal assay technology — called electro-rotation assay (ERA)— is capable of delivering accurate results in a matter of minutes at a fraction of the cost of existing diagnostic systems, its developer, Scientific Generics, claims.
ERA, which could have a significant impact on disease detection and pollution monitoring, was on show at the Royal Society's exhibition, ‘Science into Industry’ in May.
The technology is based on the observation that, when particles are placed in a rotating electric field, they too rotate in a characteristic way.
In ERA, small, polystyrene beads are coated with a specific binding agent which captures the target pollutant, virus or bacteria.
A mixture of the beads and a sample is then subjected to a rotating field generated by a set of our electrodes.
The rotating particles are videoed and each frame processed by computer to determine whether and in what quantities the target is present; and whether it is alive or dead.
The binding agents which coat the 6m beads can be selected for specific chemicals or organisms.
They could be antibodies, or DNA ‘probes’ which bind to stretches of DNA that are unique to the target organisms.
ERA can detect one part in 10 15 , Generics claims.
It is sensitive enough to detect the presence of a single microbe and can discriminate between live and dead organisms in a matter of 10–15 minutes.
ERA thus offers the possibility of real-time quality control not only for industrial pollutants but also for pathogens such as E.coli or parasites like cryptosporidium in drinking water.
Generics has already developed an ERA system for cryptosporidium for Severn Trent Water.
Using current methods these tests can take around eight hours and the diagnosis is not definitive.
The ERA system, however, takes just 30–40 minutes with a definitive diagnosis of the viability and absolute number of crypto parasites present in the sample, Generics says.
Safety culture stressed
The UK Health and Safety Executive (HSE) has highlighted both the need to maintain safety standards in the face of competing pressures and the importance of organisational factors in preventing accidents inside and outside the nuclear industry.
In a report launched in May,Organising for Safety , it was recommended that organisational factors be given the same emphasis as equipment failure and individual human error when considering safety.
The report stressed the need for a safety culture, saying that, the danger exists that an organisation's safety policy, plans and monitoring arrangements which appear, on paper, to be well considered and comprehensive may create an aura of respectability…
The critical point is not so much the adequacy of the safety plans as the perceptions and beliefs that people hold about them’.
The third in a series, the report was written by the Advisory Committee on the Safety of Nuclear Installations (ACSNI) Human Factors Study Group.
It tackled the role of safety management in promoting nuclear safety, but not the extent to which licensees already accept the principles in practice.
Previous reports from the study group dealt with the contribution training can make to good safety performance, and the prediction f human error through the technique of ‘human reliability assessment’.
The background to the report was based on both non+nuclear and nuclear accidents.
Events at Three Mile Island and more recently Chernobyl were considered alongside accidents such as Kings Cross, the Clapham Rail disaster, and the capsize of the Herald of Free Enterprise.
The studies reached a common conclusion that some physical failures or individual errors can be foreseen, and that improved organisation and management should compensate for such errors.
Speaking at the report's launch, Committee Chairman Sir John Cullen said: ‘These are messages which apply not only to the nuclear industry but to industry as a whole.
However, the public rightly expect to have particular assurance about standards of safety in the nuclear industry’.
A second report was also launched which examines many aspects of nuclear safety including computerised protection systems, storage of nuclear waste, and nuclear emergencies.
Thermal imager wins award
A new type of thermal imager camera working in the far infra-red has won this year's Prince of Wales Award for Innovation, which is organised by Business in the Community.
The imager camera has been developed by GEC Marconi and the Defence Research Agency.
Significantly, the camera does not require costly special cooling yet it achieves thermal resolutions comparable with current, cooled images.
The imager was selected from six finalists of the 1991 competition and has gone through a two-year production stage to prove its commercial viability.
Seeing in the dark
As well as making it possible to see in the dark, thermal imagers penetrate mist, smoke and the lighter fogs.
The image presented, whilst it may look like a visible image, it is in reality a complex pattern of temperature differences, and this means that there are a wide range of uses for thermal imaging technologies apart from the obvious military and firefighting applications.
These include healthcare, pollution monitoring, controlling industrial processes, measuring heat loss from buildings, and checking the structural integrity of aircraft and buildings.
The elimination for cooling requirements and the use of integrated circuit fabrication techniques to produce the image's solid state detectors are expected to bring the costs of future models down by a factor of ten or more to under £1000.
At that stage, thermal imaging is expected to become much more common.
One possible consumer application is the installation of thermal imagers cameras in cars to help with night driving.
Declaring war on wear
At a workshop on tribology 87% of attendees admitted now knowing their company's overall costs due to friction and wear.
The workshop was part of the Institution of Mechanical Engineers' (MechE) first year of Tribology Action, a campaign to create awareness and to stimulate action in the area of lubrication and wear.
Although the take up in some areas has been poorer than expected in the programme, which is now into its second year, research into changes in working practices and attitudes has been assessed.
Of workshop attendees questioned, only 46.5% had a procurement policy for tribological materials such as lubricants and 43% had no maintenance/lubrication strategy for plant and products.
However, by the end of the year, a survey of attendees and those who had bought action packs showed that 31% had already experienced lower maintenance costs and 38% had gained improved reliability.
However, only 44% had changed their working practices.
The identified problem areas for companies were lubrication, wear, condition monitoring, bearings, gears and seals.
Of the programme's two main objectives, the Institution believes awareness has increased over the year, but the lack of action taken has, in some ways, been disappointment.
There has been fewer people than expected attending conferences, and fewer take-ups for free advice.
Tribology technologies are ready for application, and can account for substantial savings to industry.
Research has shown that savings of £1.5 billion could be made in the UK alone through a simple review of friction, lubrication and wear, with very little investment.
For many companies this can mean a saving of up to 10% per year on energy, maintenance and labour costs.
Similar campaigns in tribology countries such as China, Germany and Canada have benefited companies with 40-fold annual returns.
Studies in China on once-only investments brought 40-fold to 70-fold returns.
Research at the IMechE has shown that in the UK over 75% of the benefits of tribology are as yet unrealised.
R&D spenders come under scrutiny
Despite the recession the UK spent 6% more on R&D in 1992 than in the previous year.
However, it is the drugs companies that have bolstered the UK's total R&D expenditure and kept it above inflation.
The UK pharmaceutical industry is more immune to economic instabilities, as well as being world leaders in the field.
Four drug companies — ICI (spending more on healthcare than chemicals), Glaxo, SmithKline Beecham and Unilever — occupy the first our places in the UK current spending league.
Three UK companies also feature in the top five international research spenders within the healthcare industry.
After healthcare, the highest spenders in the UK were fuel, chemicals and electrical.
In the fuel industry, in particular, Britain has maintained its position in world class research, with Shell and BP in the top five international fuel companies.
The general international picture is different.
The motor manufacturing giant General Motors leads the field, and the first ten of the top R&D spenders are dominated by motor companies and the electronics industry.
Of the eleven UK companies in the top 200, ICI is highest placed at 47.
The figures have been published in the third UK R&D Scoreboard .
Commissioned by the Innovation Department of the DTI, and Scoreboard was compiled by the Edinburgh consultancy Company Reporting.
It is not the only indicator of the propensity of British industry to invest in R&D.
The latest CBI/NatWest Innovation Trends Survey shows that after two years of decline, expenditure in support of innovation has increased in the last two years.
The most sobering results came on the patent front.
Patent filing data provided by CHI Research shows a continuing decline in the contribution of British companies to the patent register in the US.
Whilst France and Germany's shares have also decreased, contributions from Japan and the ‘newly industrialised countries’, such as Taiwan and South Korea, are growing rapidly.
Japan now files over 40% of non-US patents in the US.
This is no surprise when, even in healthcare, the UK's strongest area, the Japanese now invest over twice the amount the UK allots to R&D.
However, viewed on the international stage the UK's recent increases in R&D expenditure rose 8%, 2% more than the aggregate figure for British companies.
These international companies are already spending more of their profits on R&D.
Their R&D spending reached 4.59% of sales revenue and 94.3% of profits.
UK companies spent only 1.55% and 19.7% respectively.
Some British industry is holding its own in the R&D expenditure tables.
Chemicals and pharmaceuticals, for example, have improved their standing against other companies in the top 200.
UK chemicals R&D spending increased 10% compared to an aggregate 9.3% rise in the top 200, and pharmaceuticals increased 16% compared to an international figure of 14%.
There has been an encouraging 23% increase in general manufacturing R&D expenditure, but yet again, as a percentage of profits, the UK's 11.7% does not compare healthily with the global 95.4%.
Whilst UK investment in R&D as a percentage of profits and sales is well under the international standard, so too is investment in R&D compared to the dividends paid out to share holders.
On average, UK companies spend twice as much on dividends as on R&D.
In contrast, the top 200 international companies spent about twice as much on R&D as on dividends.
ICI, although running at a loss of £384 million, still distributed £393 million to its shareholders, over half its R&D budget.
The Dutch company Philips, which was ranked 17 in the top 200, left their shareholders empty handed.
Giving industry a Competitive Edge
It is particularly timely that the publication of the government's White Paper on Science and Technology should have coincided with the launching of our new magazine,Competitive Edge , since both are concerned with a commitment to the profitable application of technology by industry.
The White Paper (see p2 for detailed news coverage) addresses a number of the structural issues which AEA has identified as barriers to innovation in the UK.
For example, the regionally-based one-stop-shops will open the science and technology base to industry, particularly small and medium sized enterprises (SMEs), which have hitherto found it difficult to access effectively.
As the article by Dr Stuart Nelson in this issue (p11) points out, most SMEs are looking for immediate benefits by improving their products and processes with existing technology at low risk.
The one-stop shops, through their knowledge of the technology base, including demonstration facilities at organisations such as AEA, should help to provide the interface between the technology providers and the SMEs.
Similarly, the co-ordination of the LINK scheme by the Office of Science and Technology (OST) will hopefully focus it more on the needs of SMEs and therefore enhance their participation.
We have always argued that the main motivation for government funding of research should be wealth creation.
This does not mean that the focus should only be on short-term objectives, or that the government should adopt a highly-prescriptive approach in deciding what to fund.
It does mean backing research that is aimed at realising commercial goals.
To adopt this approach successfully it is important that government has a clearly defined framework for investment in research.
This framework must be agreed in partnership with industry, and not just the large companies but also the many thousands of smaller companies that form such an important part of our industrial base.
We believe that harnessing the technology base to the goals of wealth creation within a strategic framework overseen by the Office of Science and Technology is an important principle.
The Forward Look backed up by the Technology Foresight Programme, under the overall direction of the Council for Science and Technology, can provide a firm base on which to built an effective national science and technology policy.
Quite rightly, the government places a strong emphasis on raising awareness of the importance of innovation throughout UK industry.
It is in the drive for more effective dissemination of information that AEA has launched Competitive Edge , which will offer its readers accessible articles on aspects of science and technology relevant to industry.
Everyone agrees that there is much to be done if the UK's innovation performance is to be improved.
AEA has much to contribute, and we look forward to playing a key role.
How we can profit from technology
I was pleased to be invited to contribute a foreword for this new journal on technology transfer, not least because its title —Competitive Edge — so aptly sums up what we all want British industry to achieve.
The ability to compete successfully at home and abroad is crucial to the economic well-being of all of us.
Innovation, which we define as the successful exploitation of new ideas, is a major contributor to competitive success and thus to wealth creation.
A recent joint DTI/CBI survey on best practice in innovation showed that the most successful companies were ones with high levels of innovation.
However, it also showed that only 10% of even our better companies were innovative in all areas of business.
Technology transfer in its broadest sense, which includes information, demonstration and the transfer of knowledge and skills as well as licensing agreements,.
joint ventures etc, is a major contributor to the innovation process.
A journal such as this which seeks to facilitate the spread of technology is therefore very much to be welcomed.
The benefits of technology transfer are widely recognised by government.
The major role in the innovative process — and in identifying the technology which will assist it — rests, of course, with industry.
However, Government also has a role, not least in seeking to maximise the return received from the substantial investment of public money in the science base.
DTI has sought to encourage technology transfer where the market is weakest, particularly in links between industry and academia, the access of small firms to sources of technology, and the spread of new technologies likely to have pervasive applications.
A wide range of measures have been developed to encourage exploitation of technologies, the diffusion of technologies between sectors and the spread of best practice.
These include: awareness in information technology, materials and other widely applicable technologies; support for technology transfer infrastructure, including strengthening industrial units and technology audits in Higher Education Institutes; and programmes which increase the ability of firms to undertake technology transfer, including the Teacher Company Scheme, the Consultancy Initiative and Managing in the '90s.
The recent introduction of pilot one-stop-shops, which are intended to be the main local point of access to and promotion of all DTI services, and the diagnostic and consultancy brokerage services which are being developed to succeed the Consultancy Initiatives, will play a key role in helping firms, in particular smaller firms, to access the technology which they need as part of their business and marketing plans.
The Department's role in technology transfer complements the range of worthwhile activity taking place in industry.
I have been interested to learn more about the valuable work AEA Technology is doing on supplying technology world-wide through technical consultancy, technical products and services and engineering project management.
It gives me great pleasure to wish AEA Technology and their new journal Competitive Edge every success in their efforts to assist the technology transfer process.
Opening the door to innovation
Innovation — the activities necessary to produce profitable new or improved products, processes, services or ways of doing business — is the key to commercial success.
As a result of innovation companies can grow faster, capture market share, be earlier to market with new products, generate more profit for re-investment, and beat the competition.
But how can it all be made to happen?
Shortening product life cycles and rapid product proliferation mean that investment in innovation is critical in global competition.
As a result, innovation now figures high on the agendas of chief executives across the world, and their concerns are backed up by figures which show a strong correlation, for the US at least, between investment in R&D and long term growth.
UK industry, for its part, invests a smaller proportion of output in R&D than its main competitors, and more recent data on R&D as a percentage of sales, as well as data on patent levels, tells a similar story.
Nevertheless, despite concerns about the level of UK research expenditure, the UK science base remains exceptionally strong as a source of innovation.
It still scores highly on the academic community's own measures — the number of publications and their citations — and the British Technology Group has generated more licensing income from UK academic discoveries than all the US institutions earn together from licensing.
The UK technology base exists in a range of organisations — from universities through government industrial and defence laboratories, AEA Technology, the industrial research organisations to technical consultants and industry itself.
Each of these organisations is, to a greater or a lesser extent, involved in transferring technology to an end user.
The UK's relative lack of success in exploiting innovative technology is not, therefore, because the country's engineers and scientists lack good innovative ideas or that its R&D is not of the highest quality.
The weakness lies in an apparent inability to translate sufficient of these achievements — especially from government-funded work in the universities and national laboratories — into successful commercial products and services.
This is a problem the recent Science and Technology White Paper is trying to address.
There are a number of reasons for this.
Key among them has been a failure in the UK to build appropriate or adequate bridges between business, the universities, and other sources of innovation like the national laboratories.
Resource allocations fail to emphasise adequately key elements of the innovation process, and cultural perceptions affect the way in which business and the sources of research see their own and each others' roles and contributions to the innovation partnership.
Such failures of perception reveal themselves most acrimoniously in the issue of intellectual property rights.
Equally important, perhaps, has been the failure to realise adequately that, however intellectually attractive a technology may be, it will never be a commercial success unless a market for it exists or can be created.
Good market intelligence is essential to successful innovation.
‘If you invent a better mouse trap the world will beat a path to your door’.
Perhaps, but not if they do not know about you, if they have no money, or if they all happen to be cat lovers!
A common understanding of innovation
One critical step in overcoming the culture gap is to ensure that all partners share at the outset a common understanding of the innovation process, their respective roles within it, and how it applies to the particular topic being addressed.
Research has suggested that the two linear models of innovation — technology push and market pull — are seriously flawed.
An undue emphasis on the initial technological concept can lead to unrealistic expectations and an early abandonment of the innovation process.
Also, if the market is not defined early the wrong ‘working application’ may be used or indeed it may be found, too late, that there is no commercial application at all.
Similarly, a model in which a market is identified and then the technology sought to fill it also has serious flaws.
Several of these centre upon the fact that innovation takes time while markets change rapidly.
Thus, as the technology is developed the market requirements may change, or the market may disappear altogether.
In reality, the process of innovation is an integrated, interactive, iterative process in which all the various aspects — including R&D, insights into market needs and economic analysis — operate hand in hand.
From the outset, in this model, technological insights interact with perceptions of market needs and cost-benefit analyses to generate and maintain a clear understanding of how the innovation can be profitably introduced.
To ensure success, these interactions then have to be sustained throughout the innovation process as the technology, markets and cost benefits are more and more closely defined and the final decision to introduce is taken.
Indeed, while the process of innovation is itself multidisciplinary, the technological insights at the root of innovation often involve bringing together knowledge from a number of different fields of science.
A new idea emerges at an interdisciplinary interface where there is a fusion of different technologies.
These ‘enabling’ or ‘strategic’technologies are then applied across boundaries.
There is thus a significant need for interdisciplinary centres, such as AEA Technology or MIT, as a fertile source of both new ideas and new applications of existing ideas.
Technology transfer and the SMEs
The ultimate innovation is the production of a brand new product or process which revolutionises an existing market or creates a new one.
However, revolutionary  technological innovation represents only the tip of the iceberg.
It is highly visible, but there is an enormous mass of activity underneath.
A much greater issue for the UK is how to get innovation and existing technology into the small and medium sized enterprises (SMEs) in order to create a growing economic base.
Many SMEs find it difficult to access technology and often do not appreciate how they might benefit from it.
In addition, investment in new ideas represents a high technical and financial risk for these companies which must be minimised.
Often, it is not brand new R&D that is needed at all, but rather the application of existing knowledge to a wide range of problems and processes within manufacturing industry.
This ‘technology transfer’ is a process of innovation, but it involves spreading existing technology from sectors where it is conventionally used throughout industry so as to provide added value and competitive advantage.
The attraction of technology transfer to SMEs is that it often involves incremental improvements to products and processes which offer a rapid return and reduced risk.
Government can play an essential role as a catalyst and in establishing the framework in which successful technology transfer can be achieved.
In fact, the Department of Trade and Industry (DTI) has itself established a number of initiatives to facilitate the take up of technology by SMEs, such as‘LINK’ and ‘Materials Matter’.
However, experience has shown that technology transfer is best achieved within a real business environment.
This means that it is not enough simply to make SMEs aware of sources of technology or of development programmes which are underway.
Successful technology transfer requires that the people who provide the technology, or who act as facilitators of technology transfer, must do so as a business.
Their profitability must depend on successful technology transfer requires that the people who provide the technology, or who act as facilitators of technology transfer, must do so as a business.
Their profitability must depend on successful technology transfer so that they make the effort to market and sell their wares, they follow up all the awareness campaigns, and they actively seek out clients with the aim of transferring technology in a partnership of mutual benefit.
Market intelligence and sales and marketing expertise are, therefore, as crucial to the success of the technology transfer company as they are to innovation in general.
Confident that your market research tells you that peoples' houses are overrun with mice, and having used technology from the nuclear industry to develop a better mouse trap, you still have to tell people about it.
A significant challenge
Given that there are only about 50–100 technology ‘providers’ in the UK, whereas there are some 50,000 or more companies who might benefit, the challenge is enormous.
The technology providers have to assess the existing processes and manufacturing routes currently used and evaluate whether or not the introduction of new technology can add value.
The client must then be persuaded to invest in change.
Such change can represent a high risk, which it is important to minimise through cost-effective trials, demonstrations and evaluations — many of which can be arranged by the technology provider.
This can represent a route to technology evaluation at low risk to the user and there are many examples of useful ventures which have been developed in this way.
Rarely if ever do SMEs go out in search of new technologies.
Thus, it is up to the technology providers to build the bridges to the SMEs.
Different industries require different approaches.
In some industries the process of technology transfer is so poorly established, or the application of a particular technology potentially  so widespread, that the use of conventional mass marketing techniques can be effective.
In other industries, or with other products, much more focused techniques such as seminars or one-to-one presentations may be more appropriate.
Cross-sector consultancy offers a vital route to the transfer of technology between sectors of industry.
A prime example is the application of safety methodology and risk assessment tools developed for the nuclear power industry to the transport and offshore oil and gas industries.
In this example, whilst the market has to some extent been created by the regulators, the safety technology has been transferred mainly through the business activities of the safety consultants.
A well-established and successful mechanism for developing and transferring new technology is through collaborative projects and industrial clubs.
In this a number of companies (which can range from five to 500) contribute funds and technical and market input into a collaborative venture generally based at the laboratories of the technology provider.
The venture can result in a generic research programme for the benefit of members, the dissemination of design data, the promotion of computer codes or the collection and dissemination for world-wide new practice.
The Industrial Research Associations began with such a function in the inter-war period and continue to play a similar role today.
Government laboratories and AEA Technology are also active in this field.
However, it must be remembered that the technology providers see this as a business venture and it is therefore important to establish intellectual property rights at the outset and to produce an adequate return on any investment.
Best practice
As might be expected from the foregoing analysis, an investigation by the DTI Innovation Unit and the Technology Group of the CBI found that innovative ‘Best Practice’ companies go to great lengths to generate and capture ideas, from both internal and external sources, with careful attention to the balance of technology push and market pull and the rigorous implementation of screening processes and on-going reviews.
The study also showed that, while marketing skills are important, technological advantage can still be a key factor for innovative success and its value should not be underestimated.
Thus, the study concluded, on the one hand a number of the best practice companies relied on technology (existing and new) to generate a stream of new ideas, while on the other less successful companies could improve their position significantly by upgrading their technological competence, and by forging links with other companies and academia possibly with the help of government support schemes.
These findings serve to demonstrate the vital importance of technology transfer, and the organisations that encourage it, to the success of UK business.
A model revolution
From the design of vehicle prototypes to the latest anti-cancer drug, mathematical modelling is improving the cost effectiveness of product development throughout industry.
By ensuring that only the most promising designs are subjected to the rigour of costly physical tests, it reduces the time and cost penalties of employing test facilities such as wind tunnels.
By allowing experiments to be performed on a computer which would be impractical or dangerous to perform in the physical world, it also allows the design engineer to probe new areas.
In today's business climate, survival of the fittest means survival of the fastest to predict and respond to the changing market.
The lead time between a product's conception and launch is a measure of a company's responsiveness to changing conditions — if unnecessary delays result from cumbersome development procedures, the products will arrive with an in-built obsolescence.
Design engineers are therefore turning to computational science.
Without computer modelling, the engineer is forced to take short cuts by evaluating fewer design options.
Whether it is the latest family saloon or block buster drug, the more designs that are evaluated, the higher the probability that the optimal solution will be found.
Using computer simulations to complement physical experimentation can reduce the number of design options to be physically tested without compromising the design evaluation.
Computer modelling marries theory and physical experimentation.
From theory, it draws the equations which define the fundamental laws of nature; from experimentation, it receives data, and a means to verify its results.
In return, it provides scientists and engineers with the scope to ask ‘what if?’ questions.
To construct a mathematical model, the designer must translate the essential characteristics of a real world phenomenon, such as air flow around the wings of a fighter aircraft, into a set of mathematical equations.
These are solved to predict how it will behave under a set of prescribed conditions.
The process is difficult, and demands the performance of millions of calculations to reach even the most elementary prediction.
For many applications, mathematical modelling is actually proving preferable to physical experimentation.
Some extraordinary phenomena like nuclear reactions are almost impossible to test physically, and in some cases real time scales prove inconvenient, such as in weather forecasting.
But for industry, the  difficulties and costs of physical experimentation are the two factors which are having a particularly significant impact on product development.
Cutting time and money without short cuts
In no area are these pressures more acute than vehicle design.
As time and cost constraints make extensive physical experimentation increasingly difficult to justify, the transport sector has installed itself at the forefront of the model revolution.
From the latest production line coupé to the deadly stealth bomber, potential prototype designs are being subjected to the rigours of the physical world without a single puff of wind or screech of brakes.
In particular, aerodynamics are a major preoccupation.
Improvements in vehicle drag coefficients lead to greater fuel efficiency and stability at high speeds, and simulation is driving innovation forward.
The stakes are high —at 55mph, for example, a reduction of 10% in aerodynamic drag represents a saving of 5% in overall fuel consumption.
In the aviation industry, where fuel accounts for 16% of the operating costs of an airline, a 1% improvement in fuel efficiency can save $16 million for every 10 wide-bodied aircraft an airline operates over a typical five year cycle.
Squeezing even a modest improvement out of an airframe design is not easy.
It is an iterative process which involves trawling through the design space of possible configurations until the optimal solution is found.
This means testing as many options as possible within a limited time frame.
And because it allows more tests in less time, mathematical modelling has become an essential tool for aircraft designers.
It is the relationship between a vehicle and the medium through which it is travelling that creates drag.
In all cases, the interaction occurs between a solid and fluid (gas or liquid).
At high speeds air offers  as much tangible resistance to automobiles and aircraft as water to marine vehicles such as yachts and submarines.
Almost invariably, mathematical simulations are enlisted to find the safest path of least resistance through the fluid medium, and for this, they employ Computational Fluid Dynamics (CFD).
The ephemeral nature of fluid flow belies the rigid rules which govern its behaviour.
Before a computer can perform a simulation, it requires a blueprint of these physical laws, and that is where CFD comes in.
The subject, whether it is the air flow around the latest high speed train or the distribution of heat in an internal combustion engine, is divided into a grid of computational cells.
For each cell, the equations are solved simultaneously in a series of discrete time steps.
The number of computational cells in the grid depends upon the nature of the flow, the complexity of the physical geometry it is flowing around, the order of accuracy required, and ultimately, the speed of the computer and size of its memory.
Of course, aircraft were flying long before the development of powerful mathematical models.
It was the wind tunnel which gave birth to the characteristic shape of the Boeing 747.
But wind tunnels are notoriously expensive; the highly detailed aircraft models used in the experiments alone can cost as much as  $2.4 million and take 6 to 18 months to construct.
The data they produce is also vulnerable to the test's exclusion of working engines and to interference from the model supports and tunnel walls.
Despite Boeing's considerable investment of $10 million dollars to purchase 10 000 (417 days) worth of wind tunnel time in the 1960s, they still failed to develop the optimal design, as subsequent improvements and refinements to the B747 are testimony.
The high price and time penalties also explain their later enthusiasm for computational science.
By ensuring that only the most promising designs enter the wind tunnel, it has made physical evaluation more cost effective.
Drug design by numbers
The speed and cost effectiveness of computer models has also been recognised by pharmaceutical companies.
Until recently, their quest for the next best seller drug relied wholly on laborious physical trial and error.
The search for anti-cancer drugs, for example, usually involves trials of chemical compounds on human tumour cells.
The chances of finding one with a positive clinical response rate of 20% or more is less than 1 in 40 000.
This means that it takes an average of 12 years and $231 million to develop a new drug.
Numerical drug design could cut this time significantly.
It begins with a detailed knowledge of the target sit on the macro molecule to which the drug — a much smaller molecule — must bind itself.
By this process, it changes the state or development of the macromolecule in a way which will treat the problem.
Without an understanding of the target site, the process would be equivalent to designing a key without any knowledge of the lock.
Once the information is available, numerical modellers can determine the chemical and physical interactions between drug and target.
These calculations form the basis of computational chemistry, a developing technology that draws the laws of quantum and statistical mechanics into computational models.
While still  in its infancy, numerical drug design shows significant potential for boosting experimental research productivity by the early identification of promising candidates for detailed experimental investigation.
A round of golf or an artificial hip
But while mathematical models work hand in hand with research directors in the laboratory, other company executives are appreciating the power of computational science in far more pleasurable surroundings.
In the US, MacGregor Golf has used a finite element model to improve the performance of its T920 titanium metal driver.
Finite element analysis requires the subject — in this case the club head — to be translated into a large number of computational cells or elements.
Each element is mathematically coupled to its neighbours in a way which reflects the physical and mechanical properties of the structure.
The simulation — which was run on a CRAY Y-MP supercomputer — showed the stresses to which the club was subjected on striking a golf ball at 100mph (the speed of a normal swing).
Stress patterns depicted in the simulation helped MacGregor to add stiffness to the club, reducing the stresses and improving its aerodynamic performance.
Previously, MacGregor used experimental methods to analyse the stresses.
However, the quarter of an inch strain gauge could only record stress on one small portion of the club, while simulation can simultaneously display stress levels on over 1,000 grid points.
In much the same way computer simulation has been used in bioengineering in the design of prosthesis such as artificial limbs or hips.
In this case the subject for the finite element analysis is the bone.
The models are developed to simulate the stress train field induced on the living bone by the implant.
Doing impossible experiments
Computer modelling is clearly a versatile tool, adapting to a variety of environments, including some in which physical experimentation is dangerous, impractical, or even impossible.
In the oil industry, for example, modelling is the only practical method for optimising the yield of oil reservoirs.
Like vast spongers soaked in oil, these are formations of porous sandstone and limestone, often extending several miles in length and several hundreds of feet in depth.
The engineer must decide upon the configuration of wells and the extraction method for optimal recovery.
It is a difficult task.
By extracting the oil too quickly, the engineer risks losing touch with the larger deposit and having to drill again.
But with a mathematical model and sufficient computational power at their disposal, a number of scenarios can be tested and such problems avoided.
To predict the flow of fluids through the porous rock, reservoir simulations use CFD.
The reservoir is divided into thousands of two- or three-dimensional square cells of constant porosity and permeability.
Using a CRAY X-MP supercomputer in the late 1980s, BP exploited oil reservoir simulations in its North Sea operation.
These fields display a number of unique features such as wide well spacing, low oil viscosity and economic limitations on the number of injector wells; such complications widen the scope of reservoir modelling considerably.
Another example in which modelling surpasses the limitations of physical testing is the space industry, which relies heavily on electromagnetic modelling of antennas because of the impracticality of making adjustments once equipment is operational.
Using an electromagnetic modelling method called method of moments, AEA Technology discovered that one particular antenna on a satellite was likely to be highly susceptible to interference from other antennas on the same satellite, and its design was adjusted during the initial design phase.
Without mathematical modelling, the problem would not have been discovered until the complete antenna subsystem had been built and tested.
Whatever the driving force behind the use of computer modelling, it is now installed as the third branch of scientific method, along with theory and experimentation.
Although the principles of the techniques are as old as mathematics itself, the ability to solve the complex equations was only available with the advent of electronic computers.
Now, i the modern computer age, it has become a widespread commercial reality.
Counting the costs of accidents
It is hardly surprising, when a cut finger can cost a company £780, that companies are starting to look at accidents from a different perspective.
Whether human, environmental or financial risks are involved, prevention strategies are being shown to be good business.
Insurance companies have hit their hardest times yet.
The business of claiming from a company is becoming wrapped up in more and more paperwork and bureaucracy.
And this is just the tip of the iceberg — the accident iceberg, that is.
This was the clear conclusion of a Health and Safety Executive (HSE) report,The Costs of Accidents at Work .
For a group of five organisations monitored over a period of 13 to 18 weeks the hidden, uninsured costs of accidents at work were identified as 8 to 36 times larger than the insured costs.
These figures represent a considerable increase (at least a doubling) on the estimates on which safety experts have often based their cost analyses.
As might be expected, losses due to accidents took anything from a healthy nibble to a large chunk from the companies' profits.
If the Piper Alpha disaster or one of the horrific plane crashes are coming to mind, think again.
These are large scale losses and very visible, some have been costed individually and have, individually, cost companies huge amounts.
The Piper Alpha explosion itself involved the loss of 167 lives and is estimated to have cost over £2 billion, including £746 million in direct insurance payouts.
But no company is immune.
Costs from more routine accidents build up, and it is these sorts of accidents which have been costed for the HSE study.
The companies chosen for the study were a creamery, an oil platform, an NHS hospital, a construction site, and a transport company.
The types of costs incurred varied from sick pay and repairs to more indirect costs such as loss of goodwill, business interruption and training of replacement staff.
Organisations' insured costs were taken to be the cost of insurance premiums covering the period of study.
Losses which were not under the insurance safety net —such as product loss or damage, investigation costs and loss of corporate image — were also accounted for; these were the uninsured losses.
In the case of the transport company the uninsured costs added up to 8 times the insured costs, but in the case of the creamery the figure was much larger at 36 times the insured costs.
During the study period, none of the participating organisations suffered major or catastrophic losses, nor were there any fatal injuries or prosecutions.
What is more, the participating organisations showed average or better than average health and safety performance in their own industries.
The accident costs which mounted up were for minor, frequent mishaps such as bacterial contamination of equipment used in the creamery which were all identified and contained, and damage caused to vehicles in the transport company from manoeuvring in confined spaces.
The results and methodologies of the HSE Safety Accident Prevention Advisory Unit have now been published and it is clear the study has gone well beyond previous costings.
Twenty years ago, the Confederation of British Industry (CBI), in evidence to the Robens Committee on Health and Safety at work, said: ‘At a company level, if a readily applied simple formula could be devised by which the financial loss caused by accidents and diseases could be measured…it would make a valuable contribution towards reducing industrial accidents and occupational ill health’.
Since then there have been estimates made, but these have mostly concentrated on personal injuries and much of the work depended on the retrospective analysis of data collected for other purposes such as litigation and insurance claims.
However, in this new study the HSE chose a wide definition of the term ‘accident’.
An accident was regarded as any unplanned event that resulted in injury or ill-health of people; damage or loss to property, plant, materials, or the environment; or a loss of a business opportunity.
For a number of months, in each of the five case studies, all accidents meeting the  definition and also involving a loss above an agreed threshold were recorded.
The cost of each accident was then assessed and a judgement made on whether it would have been cost effective to prevent it.
Each accident was assessed also for its potential large-scale consequences, and it was found that 7% of the accidents could potentially have led to major accidents.
In the final analysis one organisation lost as much as 37% of its annualised profits to accidents, another 5% of its running costs.
The costs ran well beyond the realm of damage to assets into damage to reputation, alienation of employees, investigation costs, business interruptions, and replacement of staff.
The business of safety
Safety, which has for a long time been assumed to be at odds with commercial considerations, is now a business interest.
As former Managing Director of BP Basil Butler said, ‘prevention is not only better, but cheaper than cure…
There is no necessary conflict between humanitarian and commercial considerations.
On the contrary, safety at work is good business.’
The HSE report has shown that reducing accidents can be expected to increase profitability.
However, the control of accidents requires a strategy.
A means of costing them is a way of getting to know the enemy, and a first step in the identification of problems and management solutions.
The Managing Editor of the HSE report and HM Inspector of Health and Safety Accident Prevention Advisory Unit, Philip Papard, identifies the underlying causes of accidents — not differentiating between human, financial and environmental accidents — as management failures and makes a clear link between accident prevention and quality management.
His advice to companies would be to try to make the costs of all types of accidents visible as a first step to targeting prevention, and the obvious way would be to put down the costs on to the already established cost centres within a company.
He sees the need for a culture change, and the role of total quality management as ‘setting up a culture within a company so quality becomes part of the environment’.
There have now been several reports that have pointed to the fact that even in industries where there is a commitment to high safety standards, there is evidence that something is going wrong in the implementation and organisation.
The results of the HSE study are not unexpected for companies familiar with risk assessments.
Tony Taig of AEA Technology cited several such examples.
AEA has worked with a profit centre in a large organisation with an annual turnover of £20 million which experienced occasional major incidents and regular small claims for minor accidents and failures.
Neither were fully costed or managed systematically.
A quick, simple costing exercise revealed that the total cost of accidents was in excess of £1 million per year, and that accidents costing more than £2 million were occurring every two or three years.
The total cost of accidents and failures was around 10% of turnover.
In a more general study researching the full costs of accidents at work during 1991/92 some startling figures were revealed.
Taking account of all the knock-on of effects on productivity and business, a cut finger might cost a company £782, and the cost could escalate to £15 306 for a broken arm.
Within AEA itself, an accident costing scheme has shown a failure of a lifting cable on a reach truck to have cost the company £4234, and a crane cable failure (25kg weight) £114 651.
Companies are now starting to develop strategies to combat these losses.
However, it takes time to build skills and networks to identify the real cost of accidents — as BP's experience illustrates.
Richard Read of BP describes their experiences.
‘BP Oil International has operated a total loss control recording system since 1989.
[Between 1989 and 1991]total recorded  monetary losses due to accidents including business interruption, as monitored by the recording system, increased from $8.3 million to $131 million, a reflection not of an increase in the number and size of accidents, but rather of the increasing maturity of the company's accident costing scheme.’
One of the major incentives for managing accidents has come from the insurance industry.
Insurance premiums will eventually be dropped for a low-risk company, but the benefits are even greater, and more rapidly felt, for companies opting for self-insurance.
AS premiums increase and the insurance industries tighten up, many companies are opting for self-insurance and steeper excesses.
The Birse Group notched up a £750 000 saving on insurance premiums through improving their systematic management of health and  safety.
It is clearly in an organisation's interests to be as aware as possible of its weaknesses, both for major accidents, and, since a fair proportion of major accidents are spawned from minor mishaps which themselves cause losses, to invest in accident prevention, and strategies for managing safety.
The commercial incentive to reduce minor incidents therefore goes hand in hand with incentives to reduce major accidents.
There are some infamous examples which illustrate the need for comprehensive strategies.
For fatal accidents such as the Herald of Free Enterprise and the Clapham Rail disasters, the immediate causes were attributed to equipment failures or human errors.
However, in both cases there existed underlying organisational failures.
In the case of the Herald of Free Enterprise capsize, for example, there were management failures both abroad the ship and onshore.
In particular, responsibilities had not been adequately defined, and, although the Ships' Masters had raised concerns, the lack of any appropriate feedback system meant they fell by the wayside.
They are aware in the City…
In the business of safety it is not just accidents that cost — not being able to prove that you have accidents under control can be very expensive.
For instance, suppliers to transport and defence industries are now being faced with strict safety and reliability specifications and do not win contracts if they cannot demonstrate they meet them.
Financiers and investors are also demanding more and more proof of accident management.
Senior Manager of the National Westminster's Technology Unit Duncan Matthews explained that the issues of health and safety and environment are treated no differently to other management issues, and a bank would integrate this into its normal appraisal policy.
‘Risk assessment on behalf of the client may be expected to demonstrate management capabilities and awareness of weakness or exposure.
Over time action plans to remediate would also demonstrate commitment to minimise risk.
In this way a company will find risk assessment a positive tool for demonstrating efficiency and control’.
…especially where the environment is concerned
In particular, an awareness of environmental risks is growing amongst the financial institutions, as well as insurance companies and lawyers.
It may be that, at least in the Western World, we have reached a level of wealth where we can afford to be concerned about the environment.
Many of the risks are not new, nor are the environmental liabilities.
Whatever the reasons, society is more concerned about the environment now than ever before.
Breaches of pollution control are more likely to be picked up; they are more likely to lead to enforcement action, possibly criminal prosecution; more and more remediation is demanded; and they are more likely to have an impact on a company's image — and hence exact a heavier toll on company profits.
It is therefore important for both lenders and businesses to ascertain precisely what the risks are.
Legislation is on the up, including the European Commission's proposal for an eco-audit scheme which seeks to define and regulate environmental audits.
Although in its current form participation in the scheme is voluntary, companies will need to sign up in numbers if they are to meet the mandatory rules which might come into play towards the turn of the century.
Although environmental risks have grown into the business domain in the last decade, they are not being accepted whole-heartedly by the insurance industry.
Cover for these risks is hard to obtain.
For once a company has taken on the risks they are not easy to transfer.
Therefore, more and more, businesses need to be aware of their liabilities, both actual and contingent.
The same is true for lenders.
Some lenders have already had their fingers burnt and have had to write off loans and lose their investments, rather than take on responsibility for contaminated land which was part of their security.
For these reasons, most companies will welcome the eco-audits as part of a coherent management strategy.
Some organisations already implement voluntary eco-audits, and some have been piloting the new British Standard 7750.
This is the world's first standard for environmental management systems, and has strong links with established quality management systems such as BS 5750.
In its final form BS 7750 will also be compatible with the European Community eco-audit developments.
To have any meaningful environmental policy with objectives that allow targets to be set, an organisation needs to know its present position and identify the issues to be addressed.
To do this risk assessment will require a preliminary review — and costing accidents can only help.
Electromagnetic compatibility cleaning up the airwaves
Comply or die.
A drastic measure considering electromagnetic pollution has, as yet, taken only a handful of lives, but compliance with a new European Directive could make or break and electronics company.
Whilst our cities wheeze from the effects of chemical pollutants there is another form of pollution gathering in the atmosphere.
Electromagnetic smog increases with every new car phone and extra metre of electric cabling.
It has been claimed that interference has disrupted train signals, and electronic equipment from Nintendo Gameboys to video cameras has confused aircraft flight management computers.
Although the disasters resulting from interference have, so far, been few and far between, the potential for havoc is growing daily: something must be done.
Most of the time it is just a minor irritant, but interference can have far reaching effects.
The smog has already claimed victims: in the UK, interference caused a computer operated crane to drop its load killing one worker; and in Japan, interference caused robots to run amok resulting in two fatalities.
On a lighter note police radios have picked up a Christmas tree decoration's rendition of Jingle Bells, and their portable radios have operated the police car locking systems.
In the US, streets-full of remotely-controlled garage doors opened and shut at a whim when President Reagan's Airforce One flew overhead.
At a time when we are becoming more and more dependent on electronic devices, it is none too early to regulate interference: fly-by-wire aircraft and computer safety controls are no longer futuristic.
Controlling interference falls into two areas.
First, products must not emit excessive levels of radiation; and second, products must have an adequate level of immunity to interference.
To achieve this a set of regulations has been introduced which is having a profound effect on the electrical and electronics sectors.
The European ElectroMagnetic Compatibility (EMC) Director (89/336/EEC) affects every product which incorporates electrical or electronic components.
This directive is arguably the most complex to emanate from the European legislature.
Its effects are far reaching and require a proactive response from industry — all during a time of change in Europe when the barriers are coming down, the single market is open for trade and it is up to all of us to take advantage of it.
In the past, non-compliant equipment has been turned away at US ports, and in the same way the European market will also be closed to non-compliant goods.
Swift action is needed, not just to gain the advantage, but as a necessity.
The challenge is to understand and implement the complex set of rules which are trying to create the much vaunted ‘level playing field.’
Over the last few years the Department of Trade and Industry (DTI) has been active in consulting with British industry and encouraging it to be competitive in the European market.
More recently, the Department has commissioned an awareness campaign whose major aims are: ensuring everyone who needs to know about EMC does so; ensuring that UK industry is well equipped with information to tackle the directive; setting up mechanisms for British industry to acquire the necessary technical know-how; helping industry to implement EMC measures with minimum cost and disruption; and finally, promoting the positive aspects of competitive advantage in Europe as well as explaining the pitfalls of non-compliance.
Last October the Directive became UK law, although until 1 January 1996 it will not be enforced in all countries.
During this transition period, manufacturers must either conform to EC standards or to those of the importing country.
This means that countries must now accept products complaint with their own standards or the EC standards, but must not specify which until 1996, when the EC standard shall be used throughout.
EMC is not a new thing in the US, although the regulations are not nearly as stringent as the EC directive.
Problems in exporting to the US have provided a taster for the possible repercussions with Europe.
Some major reputable companies are already turning away goods which do not meet the EC standards.
Creating awareness
One of the principal drivers for the DTI campaign is the need to create awareness at a senior management level.
The managing directors of firms affected by the EMC Directive must understand the need to make changes and allocate funds to meet its strictures.
Indeed, senior management commitment is essential so that companies can devise strategies which will make the most of opportunities which the EMC Directive presents.
Once the senior managers within an organisation have developed a strategy for EMC, the next stage is to provide definitive technical information for those whose job it will be to implement EMC measures.
This does not just include design and quality engineers: everyone within the company must be aware to some extent.
Production staff must be conscious that how they build products may affect its EMC performance; service engineers, likewise, must be sure that they do not compromise EMC once products are in the field; the purchasing department must be aware that EMC constraints placed on suppliers may make a component more expensive to buy, but will probably save money when the end product is tested as a whole; and the marketing and sales department must plan to sell the advantage which EMC brings to a product.
The solutions themselves will largely be found, at their easiest and cheapest, in the design stages.
Retrofits have proved more expensive and really only serve for the short term.
An EMC problem discovered as a product goes into production may cost hundreds of thousands of pounds in lost revenue as a result of the delay.
Curing the problem at the design stage may only involve the addition of a component costing a few pence.
Electronics designers will need to follow some rules of thumb, many of which will be familiar, such as mains filtration, adopting a sensible grounding policy, minimising ground impedance, minimising cable lengths, screening cables which are carrying high frequency, large magnitude signals or which contain sensitive signals, and grouping cables to separate the sensitive from the interfering.
Pass or fail
Before it can sell a product the company will need a ‘certificate’— an OK to the CE stamp for approval.
The Directive specifies three ways in which compliance can be demonstrated.
Self certification is the first option.
A manufacturer takes responsibility for his product's compliance and signs a declaration of conformity to say that the equipment meets an appropriate European ‘harmonised’ standard.
There are a range of EuroNorm standards available for this purpose.
These are generally product specific and usually there are separate standards for emissions and immunity.
The European electrotechnical standards committee, CENELEC, is responsible for harmonising existing product standards and developing new ones.
Where no appropriate product standard exists, generic standards are available which relate to the environment where the equipment will be used.
Most of these standards describe limits on the levels of emissions or immunity across the frequency range.
They outline test methods which may be used to ascertain  whether equipment meets those limits.
While there is no legal requirement actually to carry out those test, it is clearly a good way of ensuring that products comply.
With self certification, it is up to the manufacturer to decide the best way of testing.
Setting up a facility in-house may be the best option if many products need to be tested.
This will also provide more flexibility and enhance in-house expertise in EMC.
Third party test facilities are another option for manufacturers which do not want to invest in their own facilities.
Using a NAMAS accredited facility may provide a greater level of confidence in the test results.
The second route to compliance is to produce a technical construction file for the equipment which is then assessed by a DTI-appointed body.
This option is available at any time but will be particularly useful where equipment is too large for transport or for a screened room and where harmonised standards are unavailable or where the nature of the product means that evaluation by an expert would be useful.
The third route affects radio transmitters, which must undergo an EC type examination procedure.
For emissions testing, most standards split the electromagnetic spectrum into two parts.
From 150kHz to 30HMz, interference is assumed to be primarily conducted.
From 30MHz to 1GHz, interference is assumed to be radiated.
Radiated emissions and immunity tests require the most extensive facilities.
Emissions are measured on an open area test site which is free of objects which might reflect emitted signals.
A metal ground plane is used to ensure consistency in ground performance.
The distance between antenna and equipment under test is generally 10m.
Ideally, an open area test site should be situated in a position with low levels of ambient radio activity.
Often, a product is assessed in a screened room and problem frequencies identified before more precise measurements are made on the open area test site.
An anechoically lined chamber must be used for some immunity testing as well as being useful for general emissions and immunity testing.
The chamber is a ‘Faraday Cage’(a metal-walled room with metal ceiling and floor), lined with anechoic material to absorb and deflect emissions.
The technical know-how and facilities are available for the taking, but different levels of awareness and understanding of the new Directive are needed throughout not only the electronics industry, but also any company buying, selling or using electronic equipment.
IATA concerned about air safety
The International Air Transport Association Safety Committee has produced evidence of the serious effects operating electronic equipment in aircraft cabins can have on the flight management computers.
During a six month period ending in February, one unnamed large European had 16 confirmed incidents involving electronic equipment including portable phones, lap-top computers and video recorders.
IATA is now warning against the onboard use of electronic equipment and several major companies are likely to introduce new regulations as a result.
Flight International reported six incidents in which electromagnetic interference from on-board equipment is suspected to have interfered with the flight computers:
Two video cameras were being used in the cabin when failures occurred in the flight-mode annunciator No 2 whilst a McDonnell Douglas MD-87 was taxiing.
Two Nintendo Gameboys were being used when a Douglas DC-10 automatic direction finder suffered bearing discrepancies.
The problem disappeared when they were switched off.
A cellular telephone was in operation on a Boeing 727 when the flight director vertical bar and localiser course deviation indicator moved to the left.
An FM radio digital receiver was being operated in the cabin by a passenger when a McDonnell Douglas MD-80 registered a wrong flight-level capture.
A compact disc player was suspected of interfering with the flight management of a McDonnell Douglas MD-87 which caused an uncommanded change of modes.
Two passengers were operating lap-top computers when a Boeing 747-400 started oscillating left and right during flight.
The oscillation stopped when the machines were switched off.
Biotechnology enters the social arena
Chefs have banned it from their kitchens, farmers see it as a threat to the family farm, and the Swiss have voted on it (amongst many other issues admittedly).
However, biotechnology is growing into one of the largest research fields of the nineties.
Despite the perceived and the real threats, biotechnology research is offering massive opportunities (and profits) to pharmaceuticals, agriculture, and medical science.
The origin of the term biotechnology arose from the gradual emergence of the microbiological industries as a genre offering an alternative to applying conventional chemistry.
The manufacture of chemicals such as lactic acid and citric acid during the first world war were probably the first examples to be labelled with the term ‘biotechnology’.
Throughout the history of the science, predominantly during this century, there have been many interpretations and prophecies.
The Uses of Life explores the complex developments in industry, science and Government that have coloured the present expectations of the technology.
In a similar way that public perceptions and governmental response have affected the rate and direction of nuclear power and information technology, it has affected, and still is affecting that of biotechnology.
The legislators will feel much influence from these quarters, as Europe, the US, and Japan begin to map out their policies on biotechnology.
And  none too early .
Despite the sceptics who were still around in the 1980s, the science now ranks in the top ten technologies (in terms of US patents issued 1992) alongside medical products, electronics and advanced materials.
This book is very much a history of the science but provides a background to the issues present day scientists face as biotechnology enters the commercial and political arenas.
A recent meeting at the Science Museum in London considered both public and industrial resistance to technology.
It is a widespread phenomenon, not restricted to nuclear power or genetic engineering.
Resistance has been shown in various forms in the worlds of information technology and communications (although reluctance to learn to work the video does not necessarily stop people buying one).
In forwarding the development of biotechnology safely and sensitively, it is vital that the scientists take the very real concerns of the public seriously.
The European Federation of Biotechnology has set up a task group on the public perceptions of biotechnology.
There is no shortage of research funds from interested parties in the pharmaceutical, agricultural and medical technology worlds.
Recent research has attempted to show what the European public think about biotechnology, to identify correlations with resistance to other forms of technology, an d to attempt to quantify the support.
Biotechnology in Public reviews the research, principally from a conference organised by the Federation task group, and will be of interest to scientists, industrialists, policy makers and other working in the field.