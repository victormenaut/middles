

The Nature, Sources and Effects of Atmospheric Pollution
Acid Rain
4.1 Dimensions of the Acid Rain Problem
The acidity of precipitation is determined by the concentration of hydrogen ions (H + ) present and it is commonly expressed in terms of pH values, where pH is defined as the negative logarithm of the hydrogen ion concentration.
The pH scale extends from O to 14, with a value of 7 indicating a neutral solution (figure 4.1).
Values less than 7 (distilled water) indicate acid solutions (for example, lemon juice registers pH 2.2, and vinegar pH 2.8); values greater than 7 indicate basic solutions (for example , baking soda registers pH 8.2 and ammonia pH 11.1).
Because the scale is logarithmic, each whole number increment represents a tenfold change in acidity; thus, solutions of pH 6, 5 and 4 contain 1, 10 and 100 microequivalents of acidity (Hsup+;) per litre respectively (abbreviated u eq/l) as illustrated in figure 4.2.
‘Unpolluted’ precipitation is commonly assumed to have a pH value of 5.6 (or 5.65), although in reality the pH of precipitation in    even the most remote areas of the world is typically lower than this, averaging around 5.0.
Consequently, it may be more appropriate to consider ‘unpolluted’ precipitation as having a pH of around 5.0.
This mild acidity is caused by the presence of carbon dioxide in the atmosphere forming carbonic acid, or from chlorine (originating from salt) forming hydrochloric acid.
Natural constituents such as ammonia, soil particles, seaspray, and volcanic emissions of sulphur dioxide and hydrogen sulphide can increase or decrease the pH of precipitation.
It is not unusual for a pH value of between 4 and 5 to occur at certain locations far removed from human interference (Charlson and Rodhe, 1982; Hibbard, 1982).
In tropical rural areas, rainfall may average pH 4.5, due mainly to natural organic acidity, most likely derived from emissions of volatile organic acids from vegetation, such as isoprene and terpene (Bridgman, 1989, 1990).
In contrast, the pH level regularly exceeds 6 in large, dry land areas where there are plenty of basic substances in the material of the soil.
Even given this complicating variability in pH levels, researchers have suggested that human activities have caused a dramatic increase in the acidity of precipitation at local, regional and perhaps even global scales (Likens et al., 1979).
Large quantities of oxides of sulphur and nitrogen are being emitted into the atmosphere by the combustion of fossil fuels and industrial processes, and these gases are being converted into strong acids (sulphuric and nitric) which lead to many areas experiencing precipitation of a very low acidity.
Precipitation with a pH value lower than 5.0 (sometimes 5.6) is termed acid precipitation, or simply acid rain.
The precipitation that fell before the Industrial Revolution and was preserved in glaciers and ice sheets is generally found to have a pH of over S and often as high as 6.
Currently, extensive areas of Europe and eastern North America are experiencing average pH values of less than 4.5 and even less than 4.0 This represents precipitation acidity up to 30–40 times that which would be expected from an unpolluted atmosphere (figure 4.3).
Individual storms may be 500 times more acidic than expected and exceptionally low pH values have been observed during rainstorms as, for example, at Pitlochry in Scotland when on 10 April 1974 a pH value of 2.4 was recorded (Likens et al., 1979).
An even lower pH value of below 2.0 was unofficially recorded during a rainstorm in Wheeling, West Virginia, in 1978.
Clouds enveloping mountain-tops and coastal fogs are usually characterized by pH values much lower than rainfall (Bridgman, 1990).
A winter fog south of Los Angeles in 1984 was claimed to have a pH value of 1.7 (McCormick, 1989).
Although acid precipitation obviously refers to the process of wet deposition of acidic material on the earth's surface, it also includes the process of dry deposition.
In the absence of precipitation, atmospheric pollutants are removed from the atmosphere by gravitational settling and by direct contact with the ground, vegetation and buildings.
Dry deposition rates are measured in terms of velocity of deposition, which is the reciprocal of the resistance of the atmosphere and surface or canopy to the transfer of pollutants.
Typical dry deposition rates of pollutants are 0.1 to 1.0 cm/s for sulphur dioxide, 0.1 cm/s for sulphates, 0.2 to 0.5 cm/s for nitrogen dioxide, and 1.0 cm/s for nitrates.
Dry deposition may make a major contribution to the acidity problem.
For example, dry deposition is the dominant atmospheric input of sulphur to England except in high-rainfall areas.
Over much of central and eastern England, dry deposition exceeds wet deposition by a factor of three (UK Review Group on Acid Rain, 1987).
Both wet and dry   deposition are implicitly included under the term ‘acid rain' and so a better term would be ‘acid deposition’.
Acid deposition is not a new problem: for example, the term ‘acid rain' was used first by Robert Angus Smith in 1872 to describe the polluted air of Manchester, England, which damaged vegetation, bleached the colours of fabrics and corroded metallic surfaces.
However, the current concern for acid rain arose when Odén (1968, 1971) analysed the 1956–66 data collected at 160 sites of the European Air Chemistry Network (figure 4.4).
He showed that precipitation in parts of Europe had become increasingly acidic over this period.
In the late 1950s, the Low Countries (Belgium, Luxembourg, and The Netherlands) were experiencing precipitation with an acidity of pH 4.0 to pH 4.5.
By the mid 1960s, not only was this area experiencing acidities less than pH 4.0 but the area of acid precipitation with values below pH 4.5 had expanded to include most of East and West Germany, northern France, eastern England and southern Scandinavia.
These findings, given international prominence at the United Nations Conference on the  Human Environment in Stockholm in 1972 (Engstrom, 1972), stimulated further national and international research into the problem of acid rain, including the OECD study and the eight-year SNSF project in Norway (OECD, 1977; Overrein et al., 1980).
Following recognition of the existence of widespread acid precipitation in Europe, the probability that a similar problem existed in north-eastern USA and adjacent parts of Canada was highlighted by Likens and his colleagues (Cogbill and Likens, 1974; Likens and Bormann, 1974; Likens et al., 1972).
Their conclusions were based primarily on records of precipitation chemistry taken in north central New Hampshire (the Hubbard Brook Experimental Forest Network) and New York, as well as on some scattered observations from other locations.
From these sources of varying accuracy and length of record, Cogbill and Likens (1974) and Likens et al.(1979) produced isopleths of precipitation acidity for selected periods which revealed increasing acidity between 1955–6 and 1975–6 and an enlargement of the area affected by acid precipitation in eastern North America.
Although isopleths showing the spread of acid precipitation in both Europe and eastern North America have been cited frequently in order to highlight the seriousness of the problem, many researchers have criticized their use.
Kallend et al.(1983) have argued that maps based on the European Air Chemistry Network may be misleading in several respects, such as in failing adequately to show the pattern of time variations over the years or the large degree of uncertainty attaching to individual contours, and assuming a geographical homogeneity which is not borne out by detailed calculations with data taken from adjacent sites.
When Kallend et al.(1983) analysed the 120 sites individually with five or more years of data between 1956 and 1976, only 29 showed a significant trend of increasing annual average rainfall acidity and five showed a decrease.
Critics of the data-base used to define trends in acidity in North America highlight similar problems.
Of the ten common sites available for a comparison of 1955–6 and 1965–6, only four showed worsening acidity, two showed improvement, and the other four remained the same.
Only two common sites were available for a comparison between 1955–6 and 1972–3, with one showing increasing pH, and the other showing a decrease.
If only the common sites were compared, justification for the claim of widespread increasing acidity over time is not supported (Record et al., 1982).
Stensland and Semonin (1982) even claim that severe drought and duststorms affected much of the USA in the 1950s, increasing the calcium and magnesium levels in soils, thereby partly explaining differences in precipitation acidity between the 1950s and 1970s.
The need for accurate and long-term information concerning the composition of atmospheric wet and dry deposition has resulted in the establishment in Europe and North America of sampling  networks aimed at documenting any further changes in the distribution and composition of acid precipitation.
The Canadian Network for Sampling Precipitation (CANSAP) was founded in 1977, while the National Atmospheric Deposition Program (NADP) of the United States commenced a year later (figure 4.5; Likens and Butler, 1981).
Since 1976, the United Nations Economic Commission for Europe has assumed responsibility for the monitoring programme in Western Europe and subsequently extended this to Eastern Europe.
The European network is called EMEP or the Co-operative Programme for Monitoring and Evaluation of Long Range Transmission of Air Pollutants in Europe, and consists of around 90 sites in 23 countries (figure 4.6).
The United Kingdom contributes nine rural monitoring sites to this programme, and in addition has its own Secondary National Acid Deposition Monitoring Network of 50–70 rural sites which can identify regional patterns in precipitation acidity (figure 4.7).
In addition, monitoring networks have been established within urban areas such as in Greater Manchester (Acid Rain Information Centre, 1986).
Very long-term records of lake acidification can provide quite dramatic evidence for the recent accentuation of the acid deposition problem.
Renberg and Hellberg (1982), for example, analysed diatom assemblages in lake sediment cores in south-west Sweden and found that since deglaciation of the area about 12,500 years ago, the pH of the lakes had decreased from about 7 to about 6 due to natural long-term oligotrophication and acidification.
However, during the last few decades a further, more marked, decrease occurred from values around pH 6 to the present-day values of about pH 4.5 (figure 4.8).
Diatom analyses of long-term sediment cores from two lakes in south-west Scotland also revealed marked increases in acidity, especially beginning around 1850 but accelerating during the past few decades (figure 4.9).
The principal precursors of acid precipitation are the anthropogenic and natural emissions of oxides of sulphur and nitrogen which are subsequently catalytically or photochemically oxidized to sulphuric acid (H 2 SO 4 ) and nitric acid (HNO 3 ), respectively, in the atmosphere (figure 4.10).
The acids dissolved in water appear largely in the form of ions (charged atoms).
Anions are negatively charged ions while cations are positively charged.
Precipitation acidity refers to the excess of hydrogen cations (H + ) in moisture, which are normally balanced at least in part by the major anions of sulphate (abbreviated as  or ), nitrate (NO 3 ) and, to a lesser extent, chloride (Cl -).
The total cationic and anionic charges will be equal.
If a correction is made for the amount of ammonium cations () present, and for ions from natural sources such as seawater (e.g. sodium (Na + ) and magnesium (Mg 2+ ), the amounts of sulphate, nitrate and chloride ions can be used as indices of acidity, although strictly it is the concentration of hydrogen ions that is the precise measure of acidity.
Sulphuric acid releases two hydrogen ions per molecule, that is 2H + + , whereas nitric acid releases one, that is H + + .
Some of the acidity present in the atmosphere may be neutralized by ammonia (NH 3 ), derived from natural and biological processes, whereupon ammonium cations () are formed.
However, recently, as emissions of ammonia have increased, it has been shown that deposition of ammonium ions can have an acidifying effect on soils as a result of the oxidation of  to  by bacteria.
Consequently, in areas with high ammonia emissions, acidification of soils and lakes may in part be attributed to deposition of ammonia and ammonium   ions.
Where calcareous soils are widely found, as in north China, airborne particles of calcium carbonate (CaCO 3 ), which produce high Ca 2+ cation concentrations, can neutralize or buffer some of the acidity.
Other buffering cations include magnesium (Mg 2+ ), potassium (K + ) and sodium (Na + ).
The various ions are removed from the atmosphere by the two wet deposition processes of rainout and washout.
Rainout occurs when material is incorporated into cloud water droplets or ice crystals which eventually grow to sufficient size to overcome gravity and fall to the ground.
Washout occurs when material below the cloud is swept out by rain or snow as it falls.
Fossil-fuel combustion (coal and oil) and industrial processes (principally primary metal production) are the largest contributing human-derived sources of sulphur dioxide emissions.
Global anthropogenic sulphur dioxide emission reached 75–100 million tonnes a year in 1980.
Although anthropogenic emissions of sulphur dioxide account for only half of the total global emission, they tend to be very concentrated.
In areas such as Europe and eastern Northern America, they account for 90 per cent of total sulphur dioxide emissions.
Natural sources of sulphur oxides include seaspray containing sulphate from oceans, organic compounds from bacterial decomposition of organic matter, reduction   of sulphate in oxygen-depleted waters and soils, volcanoes, and forest fires (Record et al., 1982).
Global anthropogenic emission of oxides of nitrogen for 1980 were estimated at 75 million tonnes.
Fossil-fuel combustion (coal, oil and natural gas) and transportation (petrol and diesel fuels) contribute most of the anthropogenic oxides of nitrogen emissions.
The principal sources of natural oxides of nitrogen appear to be chemical decomposition of nitrates and lightning.
Estimated ratios of natural to anthropogenic emissions of oxides of nitrogen range from 15:1 to 1:1 because of the variability in estimates of the natural emissions.
The relative importance of the component acids in precipitation varies spatially and temporally, but in general similar values apply in eastern North America and Europe.
For north-eastern USA, measurements suggest that the contributions to total acidity are made up of 62 per cent from sulphuric acid, 32 per cent from nitric acid, and 6 per cent from hydrochloric acid (Record et al., 1982).
For Scandinavia the figures are 70 per cent sulphuric acid and 30 per cent nitric acid (Swedish Ministry of Agriculture, 1982), which    are similar to the values from Scotland of 71 per cent sulphuric acid and 29 per cent nitric acid (Fowler et al., 1982).
However, in some regions, such as the west coast of the United States and in Japan, nitric acid contribution is of greater relative importance.
With emissions of oxides of nitrogen increasing at a faster rate than sulphur dioxide emissions, nitric acid will increase in relative importance in its contribution to acid precipitation (Lewis and Grant, 1980).
Indeed, this has already been observed in Scandinavia and North America: for example, Cogbill and Likens (1974) suggest that the proportion contributed by nitrates has increased from 22 per cent in the mid 1950s to its present value of over 30 per cent .
In central southern England, monitoring since 1954 indicates that whereas sulphate emissions have decreased markedly since the 1970s, nitrate emissions continue to increase (figure 4.11).
Areas that emit most acid precursors are among those which suffer the most acid deposition, but only long-range transport of oxides of sulphur and nitrogen from these sources can explain acid rain experienced at locations remote from significant pollution sources.
Similarly, in areas where local emissions have declined but the acidity of rainfall has increased, this points to long-distance transport of pollutants into the area.
Although a substantial part of the emissions of sulphur dioxide is deposited near their sources, a significant proportion is dispersed further afield.
That proportion of sulphur dioxide not deposited locally will be diffused in the atmosphere and, through oxidation, will be transformed into sulphates.
Conversion rates of sulphur dioxide to sulphate range from 0.1 to 10 per cent per hour, depending upon the presence or absence of other pollutants (reactive hydrocarbons and photochemical oxidants) and atmospheric conditions (relative humidity, temperature, sunlight).
Sulphates are less subject to dry deposition than sulphur dioxide and consequently remain in the atmosphere for days, travelling up to several thousand kilometres in favourable meteorological and synoptic conditions.
Eventually the sulphates are wet deposited by rainout and washout processes.
A similar situation applies to emissions of oxides of nitrogen, although gaseous oxides of nitrogen have a low dry-deposition rate, thereby allowing greater proportions to be converted to nitrate aerosols which are finally removed by rainout and washout.
Since mountainous areas enhance and induce precipitation, it is these locations which cleanse the atmosphere of its long-range transported pollutants and thus experience significant acid precipitation.
Many researchers point to some pollution control technologies, such as the building of tall stacks for power stations and large industry in order to reduce local pollution, as having increased the long-distance transport of pollutants and so having accentuated the acid rain problem (figure 4.12; Patrick et al., 1981).
Thus, for example, the copper-nickel smelter in Sudbury, Ontario, 360 km north-west of Toronto, through which 1.35 million tonnes of sulphur dioxide were emitted in 1973 (or over 1 per cent of the annual world anthropogenic output of sulphur), has a stack over 400 m high (Barnes, 1979).
By the early 1980s its annual emission had been reduced to 630,000 tonnes (equivalent to the total national emissions of sulphur dioxide from countries such as Sweden, Belgium and Denmark) and, in line with Canadian policy, it is expected to decrease to around 300,000 tonnes by 1995.
Although it is clear that mountainous areas with few nearby notable pollution sources are receiving their pollution burden from regions or countries several hundreds or thousands of kilometres distant, it is difficult to determine precisely from which source or sources the pollution originated.
Thus, for example, the existence of the acid rain problem in Scandinavia initially pointed to emission sources in the United Kingdom as the cause.
That the acid deposition problem became accentuated after the 1950s, coincident with the introduction of the ‘tall-stack policy’ for United Kingdom power stations, also apparently indicated where the cause lay.
However, although southern Scandinavia is indeed subject to frequent airstreams from Britain, those airstreams are only mildly polluted compared with the occasional airstreams which originate from central and Eastern Europe (Czechoslovakia, Hungary, Poland, eastern Germany).
These latter occasions give rise to highly acidic precipitation episodes (sometimes producing grey or black snows resulting from the presence of lignite ash) which are the major contributors to the total annual amount of wet deposition of sulphate in Scandinavia (Smith and Hunt, 1978) and which occasionally even lead to black, acidic snowfalls in Britain.
Further, Kallend et al.(1983) reveal that of those European Air Chemistry Network stations showing a significant trend of increasing annual average precipitation acidity, this was primarily due to an increased number of intermittent high-acidity episodes rather than a substantial increase in general precipitation acidity.
Although such evidence may shift the relative contribution each country makes to another's acid rain problem, the international dimension of this atmospheric pollution problem remains the same.
A successful resolution to the problem will require effective action at an international level (refer to chapter 11).
The impact of acidic precipitation on aquatic and terrestrial ecosystems depends not simply on the pH value of the precipitation but also on the capability of lakes and soils to neutralize or buffer the acidic inputs of the precipitation.
The concentration of buffering substances in a solution is expressed in terms of alkalinity, the inverse of acidity.
This ability to tie up the excess hydrogen ions introduced by acid precipitation is largely determined by the composition of the bedrock on which the lakes and soils form, with hard, impervious igneous or metamorphic bedrock giving rise to low calcium and magnesium content producing the most susceptible situation.
This explains why parts of southern Scandinavia and eastern North America have figured prominently in highlighting the damaging effects of acid deposition on ecosystems.
Considering the sensitivity of lakes and soils in other parts of the world and expected rapid industrialization in the coming decades, areas such as Brazil, Nigeria, southern Africa, India, Malaysia and China  are among those likely to be exposed to an increased risk of acid deposition effects in the future (figure 4.13; Swedish Ministry of Agriculture, 1982).
Already, Harte (1983) has reported a pH value of 2.25 in a remote small city in central China, which is one of the most acidic precipitation samples ever recorded.
4.2 Effects on Aquatic Ecosystems
Most concern for the impact of acid rain on aquatic ecosystems is focused on the effects on the fish population.
Dramatic effects of the acidification of water courses on fish populations have been claimed.
For example, in 1900 anglers caught 30,000 kg of salmon in the seven main rivers in southern Norway, but since 1970 no salmon have been taken.
It is generally accepted that increasing lake acidification due to acid deposition has caused fish kills and stock depletion (figure 4.14).
Einbender et al.(1982) report the existence of at least 212 fishless lakes in the Adirondacks, the loss of all fish in 140 Canadian lakes (and the threatened extinction in 48,000 other lakes), damage to 15 per cent of lakes in Minnesota, the threatened extinction of trout in half of West Virginia's trout streams, the near total depletion of buffering capacity in 3000 lakes in the eastern United States, the acidification of more than 10,000 lakes in Sweden to pH 6 or below (with 5000 lakes having pH values below 5), and reduced populations of salmon and trout in major rivers in southern Norway.
It is estimated that half of the 700,000 lakes in the six eastern provinces of Canada (south of 52 °N) have alkalinity (acid neutralizing capacity) values below 50 µ eq/l, that is, they are extremely acid sensitive.
Most scientists consider lakes with alkalinities less than 200 µeq/l to be acid sensitive (Schindler, 1988).
Throughout large areas of geologically sensitive terrain in eastern North America, lakes have lost an    average of over 40 per cent of their original alkalinity.
The most sensitive waters of such areas are now entirely devoid of alkalinity, so that inputs of strong acids cause considerable decreases in pH.
Aysbury et al.(1989) compared data from historic (1929–34) and modern (1975–85) surveys of lake chemistry for 274 lakes in the Adirondack Mountain region of New York.
They found a median change in alkalinity of -50 u eq/l, with 80 per cent of the lakes showing a loss of alkalinity (figure 4.15).
This observed  acidification was greatest in the lakes at high elevation and was of the same magnitude as the current precipitation acidity in the region, that is pH 4.2 (or 63 u eq/l), although mountain-top locations are frequently enveloped in cloud and mist characterized by very low pH values (e.g. pH 3.6 or 250 u eq/l).
Species of fish vary in their tolerance to low pH: among the salmonids, the rainbow trout is the most sensitive, followed by   salmon, with brown and brook trout being the least sensitive (table 4.1).
Adverse effects on fish have been attributed to either sudden short-term or gradual long-term decreases in pH.
Short-term pH changes, following the first heavy autumn rains, or especially in the early spring, when snowmelt releases acidic constituents accumulated during the winter, result in ‘acid shock’ which leads to fish mortality (fish kills).
The Norwegian SNSF project found that 80 per cent of the pollutant content of the winter snowpack is released in the first 30 per cent of the meltwater (Overrein et al., 1980).
Gradual decreases in pH with time lead to prolonged acidity which interferes with fish reproduction and spawning, so that over time there is a decrease in fish population and a shift to a smaller number of more tolerant older and larger fish.
For example, if the pH falls below 4.5, trout cease to produce the enzyme responsible for breaking down the outer coating of their eggs, trapping the larvae and so preventing reproduction.
Consequently, as older trout die they are not replaced and in time the trout population disappears.
An additional problem is that acid deposition reduces calcium concentrations and mobilizes toxic metals, especially aluminium, and this can be an important factor contributing to fish mortality.
The aluminium obstructs the ability of the fish to take in salt through their gills (necessary to maintain their osmotic balance) as it causes the fish to overproduce a sticky mucus which literally clogs   the gills and makes breathing difficult.
However, the toxic effects of aluminium on fish are complex and, if the calcium content of the water is high, its ions are preferentially adsorbed on the gill surfaces where they block the absorption of aluminium ions and so reduce their impact (Mason, 1990).
Acidity and calcium content of a lake are of paramount importance to fish health.
If the concentration difference K=Ca 2 +; -3H +; is positive then, for example, brown trout will thrive; if negative they may not survive (concentrations are expressed in microequivalents per litre).
In a lake with a negative K but in which acidifying inputs are declining, not only will hydrogen ion (Hsup+;) concentrations (and aluminium concentrations) decrease but so will calcium concentrations (Ca 2 +;).
Consequently, the lake may remain with a negative K and be unable to support fish for many years.
If, however, liming of the lake or catchment is undertaken, calcium concentrations will increase and K will become positive such that reintroduced fish have a reasonable chance of long-term survival (Smith, 1991).
It is not simply fish species which have suffered from lake acidification.
Observations of lakes in southern Scandinavia and eastern North America have shown that where the pH level has fallen, a decline in the lower phytoplankton biomass and species variety takes place.
Water transparency may be considered a simple measure of increased acidity since the microscopic single-cell plants (phytoplankton) decline with increasing acidity.
An analysis of high-altitude lakes in New Hampshire in the 1980s revealed that 110 out of 214 were more transparent than they were 30–40 years ago.
Hendrey et al.(1976) have shown that where lake pH values were below 5, about 12 species were observed, falling to as few as three in some very acidified lakes.
In contrast, where pH values were over 6, between 30 and 80 species were present, and the phytoplankton biomass was between three and nine times greater.
Such reduction in lake diversification reduces the variety of food available through the trophic levels.
Particularly noticeable in some Swedish lakes, with pH below 6, is the expansion of the moss Sphagnum, which covers the lake bottom, displacing other plant life and further increasing lake acidity by removing base materials such as calcium from solution and releasing hydrogen (figure 4.16).
Epiphytic and epilithic algae colonies also seem to be expanding in the acidic lakes and watercourses of southern  Scandinavia .
4.3 Effects on Terrestrial Ecosystems
Acid deposition may cause damage to terrestrial ecosystems by increasing soil acidity, decreasing nutrient availability, mobilizing toxic metals, leaching important soil chemicals, and changing species composition and decomposer micro-organisms in soils.
Much concern has been expressed that acid precipitation causes a   reduction in forest productivity.
Vast areas of forests, especially at altitudes above about 600 m, have recently been damaged in Europe with a significant proportion having been severely damaged or even killed.
In West Germany, four of the nation's most important tree species — Norway spruce, white fir, Scotch pine, and beech — showed alarming signs of deterioration beginning in 1983 (Pearce, 1987).
Subsequently, it was shown that many European countries suffer substantial damage to their forests through the effects of air pollution (figure 4.17; table 4.2).
Forests at high elevations and trees more than 60 years old appear to suffer worst, but different tree species are affected in different ways and at different rates.
Deciduous trees appear less affected than conifers, partly because they offer a smaller area of foliage, and partly because they shed their leaves each autumn and hence are not exposed to air pollution for such a long time as conifers.
Nevertheless, symptoms of acid deposition effects on deciduous trees include leaf discoloration, early leaf fall, death of tree-tops (crown dieback) and damage to the bark, as well as lack of rejuvenation.
Damage symptoms on conifers include crown dieback and needles turning yellow (usually on the upper side of branches) and then falling off.
Irregular or claw-like shoots may grow, needles become stunted in growth, cracks appear in the bark, roots become shallower, annual growth rings become narrower, and an excess production of seeds and cones may take place yet the seedlings fail.
In Eastern Europe, forest problems are even more severe than in Central Europe (Tomlinson and Silversides, 1982; UN ECE, 1990).
Extensive areas of forests in Czechoslovakia have been damaged with deaths in widespread areas of the mountainous Krokonose Park on the Polish-Czech border, where precipitation reportedly averages pH 3.8 (over 60 times more acidic than average unpolluted precipitation), and in Krusne Park on the East German-Czech border.
Not surprisingly, a similar situation applies in Poland.
It was reported from Bulgaria in 1989 that the trees in 25,000 hectares out of 35,000 hectares of silver fir forest were severely damaged or dying.
The problems in Eastern Europe are particularly  are particularly severe because of the vast emissions of sulphur dioxide from (largely uncontrolled) industrial plants and the high sulphur content (3.5 to 14 per cent) of brown coal or lignite burned as compared with bituminous coal (1 to 2 per cent ).
A 15-year German study allowed Ulrich et al.(1980) and Ulrich and Pankrath (1983) to offer the first attempt at explaining Waldsterben (forest death).
They showed that acid rain is leaching important plant nutrients such as calcium, magnesium and potassium from the soils, making them unavailable to trees.
In addition, acid rain mobilizes the aluminium in forest soils (from harmless soil compounds such as aluminium silicate) which decreases the ratio of calcium to aluminium in soil solutions to the extent that root growth is impaired.
The fine roots of trees are damaged by the toxicity of aluminium, which, together with nutrient deficiency, causes a stress condition of crown dieback (leaves or needles at the tree-top turn yellow, then brown, and eventually drop off) and the eventual death of the tree.
Seedlings may not survive because of extensive damage to their roots.
However, the Ulrich hypothesis does not explain why trees growing in a wide range of soil types, even calcareous soils, are  apparently suffering acidic deposition effects.
The hypothesis also lacks the expected evidence of a correlation between the intensity of needle chlorosis of, say, spruce and the aluminium content in the soil or in the needles.
Consequently, the consensus opinion is that it is nutrient deficiency which is the key factor in explaining the adverse effects of acidic deposition on a wide range of tree species in a variety of settings (figure 4.18).
Evidence points to magnesium deficiency in soil as a crucial factor in predisposing trees to damage.
For example, the addition of magnesium fertilizers in Norway and Germany's Black Forest has resulted in recovery in young spruce trees that had been suffering chlorosis (Printz, 1987).
However, Schulze (1989) indicates that the type of nutrient deficiency can depend upon the substrate.
For example, old yellow needles on acidic soils show symptoms of magnesium deficiency, young yellow needles on dolomite show symptoms of manganese deficiency, and old yellow needles on limestone show potassium or iron deficiency.
One complicating factor concerned with the nutrient deficiency hypothesis is that the negative effects of nutrient deficiency may be offset initially in established forests by the fertilizing effect of nitrate deposition.
However, this seemingly beneficial aspect of acid deposition may be short-lived because, over the long term, forest productivity may be limited by the lack of nutrients other than nitrogen.
Further, because soil acidification hinders the forest's bacteria and fungi in  recycling nitrogen from decaying plants into the soil, the trees may be robbed of as much nitrogen as provided by acid deposition of nitrates (Gorham, 1982).
Overfertilization by nitrogen may also make trees more palatable to insects.
It seems that there are a variety of reasons why an increasing number of European and North American trees are becoming damaged or killed.
Forest damage similar to that observed in parts of Europe is being observed in North American areas subject to acid rain such as on mountain tops in the southern Appalachians, the Adirondacks, Vermont and New Hampshire where dieback of red spruce is evident (Einbender et al., 1982; Johnson and Siccama, 1983; UN ECE, 1985).
There is wide agreement that the recent forest damage can predominantly be ascribed to atmospheric pollution even if there remains uncertainty concerning which pollutants are to blame at which locations and what the response mechanisms are for triggering damage, as well as over how the contributing natural stress factors should be weighted (Hinrichsen, 1986).
Various researchers point to the potential phytotoxic effect on trees of high concentrations of sulphur dioxide around industrial areas; to the higher acidities (e.g. pH of 3.5) of occult precipitation (mist, fog) from clouds enveloping high-altitude  forests for considerable portions of the year; to the increasing concentrations of photochemical oxidants (PAN, ozone) in Europe; to the increase in oxides of nitrogen and hydrocarbons from motor vehicles; and to synergistic effects between many of these forms of pollution.
Whatever the reason, it is recognized that stressed trees then become vulnerable to further damage through drought, wind damage (through weakened root systems), snow damage (the brittle tops of trees snap beneath the weight of snow), frost, insects (e.g. the bark beetle is attracted by chemicals such as terpenes given off by stressed trees) and needle fungal and virus attack.
Some bird and animal populations living in forests damaged by acid deposition have been shown to have declined recently.
For example, Drent and Woldendorp (1989) studied the great tit and other hole-nesting bird species in nutrient-rich and nutrient-poor soil areas of the Buunderkamp Forest in The Netherlands between 1983 and 1988.
They found a significant difference between the two areas in terms of the percentage of breeding pairs with no eggs or eggshells of inferior quality (e.g. very thin, granular, porous, fragile, without coloured spots).
Poor-quality eggshells indicate insufficient deposition of calcium and this deficiency was shown to have arisen because of increased soil acidification.
Soil acidification impairs the uptake of calcium by trees which results in low calcium levels in the leaves of several tree species (beech, birch, Scots pine, and oak) and subsequently in the content of feeding caterpillars which are an important food source for hole-nesting birds in spring and summer.
From the data on calcium ion status at various trophic levels (soil, tree leaves, caterpillars, eggshells) this study reveals a clear relationship with the effects of acid deposition.
Other studies elsewhere have linked changes in trophic-level populations to acid deposition effects, such as the decline in goldcrests in coniferous forests where trees suffering lost needles support fewer spiders on which these birds depend for food in winter.
In some areas of upland Britain, a decline in otter populations has been linked with the decline in fish populations in acidified water courses (Woodin, 1989).
In southern Sweden, reduced calcium in acidified soils is believed to have caused a decline in terrestrial mollusc numbers and species in the past few decades.
Attention has also focused on acid deposition effects on a wide range of crops.
Record et al.(1982) report on an American study which examined 32 commercial crops grown under controlled environmental conditions and exposed to simulated (sulphuric) acid rain of pH 3.0, 3.5, 4.0 and 5.7.
Preliminary results indicate that some crops suffered extensive foliage damage and yield reduction but others sustained little apparent injury, even under these severe exposure conditions.
A similar variability in response to different rain acidities was reported by Hibbard (1982) when, of 27 crops tested, 5 were inhibited, 6 were stimulated, and 16 experienced no observable effects.
4.4 Effects on Human Health
Heavy-metal cations, such as cadmium, nickel, lead, manganese and mercury, may be bound to colloidal particles in the soil.
Excessive amounts of hydrogen cations introduced into the soil by acid precipitation may exchange for these heavy-metal cations, thereby releasing the metals into the soil and into watercourses (Babich et al., 1980).
The metallic compounds may contaminate edible fish and drinking water supplies, and thereby be passed on to people.
At some locations, concentrations of these elements in drinking water exceed the national drinking water standards, while in some lakes high methyl mercury content in fish could have potentially adverse effects.
In Sweden, it is claimed that nearly 10,000 lakes have such high concentrations of mercury that any fish caught in them should not be eaten.
Pregnant women are particularly warned about such lakes since the foetus is very sensitive to mercury poisoning.
Water supplies become contaminated because increasingly acidic water leaches copper and lead from water supply systems.
In Sweden, drinking water has been found to have levels of copper which may cause diarrhoea in young children, harm to liver and kidneys, and even colour hair green, along with wash-basins, baths and laundry.
It has been estimated the Sweden has 65,000 wells, that is 15–20 per cent of all private wells, which are acidified to the extent that they are eligible to receive government grants to help pay for the cost of restoring water quality.
A study of 4300 shallow wells in Varmland, west central Sweden, revealed that the pH fell from an average of 6.7 in 1960 to just under 6.2 in 1985, that is, the wells had become five times more acidic.
In Norway, studies suggest a possible link between high levels of aluminium in groundwater and senile dementia and Alzheimer's disease, a form of pre-senile dementia (sufferers are under 65 years old).
Cadmium can damage the kidneys, and there has been concern about levels of cadmium in the livers and kidneys of elks and deer in southern Sweden which have grazed on plants that have accumulated cadmium.
Another concern is that acid deposition may accelerate the leaching, mobilization and enhanced biological availability of toxic heavy metals and other toxic chemicals in past, present and future hazardous waste (and other solid waste) landfills.
4.5 Effects on Materials and Visibility
Acid deposition can accelerate corrosion of metals and erosion of stone (figure 4.19).
In the case of monuments and historical buildings, the erosion and damage can destroy carvings and statues and weaken building structures.
Stonework most prone to acid   deposition includes limestone and marble, while sandstone held together by calcareous cement is also susceptible to damage.
The calcium carbonate of the stone dissolves in acid rain and reacts with sulphur dioxide in the air to form gypsum (calcium sulphate).
Crystals of gypsum grow within the pores of the rock where they exert mechanical stress, creating tiny fractures which eventually cause the stonework to crumble.
The gypsum crust is more soluble than the limestone so it is quite rapidly weathered by rainwater.
As the surface crust flakes away, fresh limestone surfaces are exposed to attack.
At St Paul's Cathedral, London, an average of 30 mm of Portland stone has been lost in the past 250 years, with, in one particularly exposed position, 22 mm being lost in just 45 years (figure 4.20; Dudley et al., 1985).
However, because sulphur   compounds are responsible for most of these effects it is very difficult to distinguish between the effects of pollution originating from local as opposed to distant sources (refer to section 2.1).
The transport of sulphates over long distances can lead to reductions in visibility over extensive areas and at locations distant from pollution sources.
Dense summer acid hazes are now experienced over extensive areas of rural southern Britain, being closely related to the advection of polluted continental air masses (Barnes and Lee, 1978; Lee, 1983a, 1983b, 1988).
Similarly, hazes mapped by Hidy et al.(1978) and observed on satellite images by Lyons and Dooley (1978) covered vast areas of the north-eastern USA and were associated with polluted air mass advection from the Midwest.
During winter and early spring, air masses entering polar regions are charged with sulphates which are occasionally sufficient to reduce visibility and cause ‘Arctic haze’(Bridgman, 1990; Stonehouse, 1986).
Sources for the sulphates are the distant industrial regions of Europe, North America and eastern Asia, with most researchers pointing to Europe and eastern USSR/CIS as the major contributors (Barrie, 1986; Shaw, 1982).
Not only does the increased frequency of pollution hazes in rural and wilderness areas cause dissatisfaction and loss of amenity to many, but they also have the potential for affecting global climate (refer to section 2.1).
Ionizing Radiation (Radionuclides)
5.1 Introduction
Radioactive materials are present in the human environment as the result of natural processes and of human technological developments.
These materials produce ‘ionizing radiation’ such as X-rays, gamma-rays, alpha-particles, beta-particles, electrons, protons, neutrons and cosmic rays which ionize the atoms of substances which they penetrate.
Radiation such as ultraviolet and visible light, infrared radiation and radio waves do not produce ionizations.
Ionization is the process by which a fast-moving quantity of energy is transferred, leaving them as electrically charged ions.
Although the physico-chemical changes caused by the ionization of the atoms of living matter occur in a fraction of a second, the processes whereby these changes eventually lead to such changes in living matter as cell death, malignant tumours (cancer), and genetic mutations may take hours, months, or even decades (Coggle, 1983).
With few exceptions, radioactive materials released into the environment become involved in a complex series of physical, chemical, and biological processes.
Some of the processes lead to progressive dilution, others to physical or biological reconcentration, followed by transfer through various or sometimes interdependent pathways to people (ICRP-29, 1979).
Simplified representations of some of these pathways are shown in figures 5.1 and 5.2.
Some compartments in the model could be subdivided: for example, radionuclides are transferred from animals to people by drinking milk or by eating the animal meat, and while the consumption of milk is a fast pathway, that of meat is slower.
The radiation dose received by people is the sum of the external irradiation (from deposited activity and atmospheric activity) and internal irradiation (following inhalation and ingestion of radioactive substances).
The ionizing radiations to which people are   subject are of varying types and, although they interact with living matter in a similar way, different types of radiation differ in their effectiveness in damaging a biological system.
Thus, for example, 10 grays (Gy) of alpha-particles will cause the same damage in living tissue as 200 Gy or 200 sievert (Sv) of X-rays, gamma-rays, or electrons (table 5.1 explains the units employed).
This is   in spite of alpha-particles being less penetrating than gamma-rays.
The considerably higher relative biological effectiveness or quality factor Q of alpha-particles is believed to be due to the fact that the dose is delivered over short tracks in tissue along which ionization is dense.
Figure 5.3 provides  details of the quality factor for some radiations.
To obtain the biologically effective dose for a mixture of radiations, the dose equivalent (units = sievert) is the sum of doses of each type of radiation (in grays) multiplied by the quality factor Q of each radiation type.
Radioactive contamination is removed by the disintegration of an isotope through a series of daughter elements until a stable element is produced.
The radioactive decay, through which a radionuclide loses half its radioactivity, is a fixed length of time for each element, and is referred to as the half-life of a substance.
As a rule of thumb, the radioactivity of an element has usually disappeared   (has reached 0.1 per cent of the original value) after an equivalent period of ten times its half-life (Bach, 1972).
Half-lives of various radionuclides, together with the principal types of radiation they emit, are summarized in table 5.2.
5.2 Effects on People
Evidence for the effect of ionizing radiation on people is derived from several sources.
The survivors of the atomic bomb detonations at Hiroshima and Nagasaki in 1945 provide evidence on the effect of substantially uniform ‘whole-body’ irradiation on people of all ages, both in terms of acute exposure to large radiation doses (table 5.3) and in the long-term effects of lower levels of radiation.
Thus, for example, it has been found that levels of received radiation doses correlate significantly with radiation-induced disorders and diseases such as abnormalities in cell development, chromosomal damage, leukaemia, cancer of the thyroid, lung and breast, impairment of growth and development, and increase in foetal, infant and general mortality.
Recent research suggests that the radiation effects induced in these Japanese people may have been caused by only half as much radiation as has hitherto been believed (Hawkes et al., 1986).
Evidence on the risk for one organ, the thyroid gland, can also be derived from the occurrence of cancers on this gland in populations of two Pacific islands, who were exposed to irradiation and radioiodine concentration in the gland from fallout from a weapon test in 1954.
Occupational exposure to radiation of both uranium miners and others serving the nuclear fuel cycle, and radiotherapy medical staff, provides additional evidence.
Increased lung-cancer fatalities in uranium miners who inhale radioactive gas during the course of their work has been studied in a number of countries.
Finally, patients irradiated in the course of frequently repeated diagnostic examination or during treatment of their diseases by radiotherapy can be studied.
In the former group, patients receiving repeated diagnostic examinations during the treatment for pulmonary tuberculosis have sometimes received substantial radiation exposure to the chest and female patients have been found to develop breast cancer in excess of normal expectation (UNSCEAR, 1977, 1982, 1988).
From these extensive and varied surveys the detrimental effects of ionizing radiations are found to be somatic (manifest in the exposed individuals themselves) and genetic or hereditary (affecting descendants).
The effects may be nonstochastic (the severity of the effect varies with the dose, and for which a threshold of dose may therefore occur below which no detrimental effects are seen) or stochastic (the probability of an effect occurring, rather than its severity, is regarded as a function of dose, without a threshold).
Hereditary effects are regarded as being stochastic.
It has become clear that the most important late somatic effect of low doses of radiation is the occasional induction of malignant diseases, as shown by their increased incidence in exposed populations.
In general, it appears that relatively high cancer induction rates apply to the breast in females and to the thyroid, although the mortality rate from induced thyroid cancers is low — that is , about 3 per cent per 25 years.
The induction rates for lung cancer and for leukaemia are somewhat lower, and those for other organs for which estimates are obtainable appear to be lower still (UNSCEAR, 1977).
Typically there is a long latent period between radiation exposure and detection of a tumour, which is because of the time required for sufficient increase in the size of the tumour to make it detectable, and may also be due in part to a form of induction  period before the initially affected cell or cells start to divide and form a tumour, or before the tumour assumes ‘malignant’ characteristics of growth and spreading.
Mean latent periods for malignant tumour detection after radiation exposure are estimated at about 25 years, although for leukaemia it is between 8 and 14 years.
In addition to cancer inductions, ionizing radiation may have significant effects on pre-natal development and on genetic or hereditary factors.
Irradiation of the embryo or foetus may cause defects of a wide range of severity, including death while still in the uterus (based on animal experiments), impairment of growth, structural changes (or malformations) and functional deficiencies.
Thus, for example, Japanese children who had been exposed in utero as a result of atom bomb detonations at Hiroshima and Nagasaki have shown a possible increased incidence of mental retardation, associated with small head size (microcephaly) and clear evidence of reduction in body size.
It may be concluded that the developing embryos and foetus show a pronounced sensitivity to the induction of malformations by radiation, particularly during the main phase of organogenesis.
Genetic effects of radiation are likely to be predominantly due to damage induced in the DNA molecular structure.
When cells are exposed to ionizing radiation, the chromosomes of the cell nuclei may be damaged by the production of gene mutations (dominant and recessive), involving alterations in the elementary units of heredity which are localized within the chromosomes, or by the induction of chromosome aberrations, consisting of changes in the structure or number of the chromosomes.
When such changes are induced in the germ cells, they may be transmitted to descendants of the irradiated persons.
In 1990, a cluster of increased incidences of leukaemia in young children around Sellafield nuclear power plant in north-west England was explained as being due to reproductive cells in fathers being exposed occupationally to a radiation total dose of over 100 mSv (Gardner et al., 1990).
UNSCEAR (1977) estimates that in a million liveborn children in the first generation of offspring of a population exposed to 10 mGy (1 rad) at low dose rate during the generation, there would be 63 genetic diseases.
The total genetic damage expressed over all generations (or the value in each generation reached after prolonged continuous exposure) is estimated to be 185 per million per 10 mGy (per rad).
However, these estimates do not take full account of the class of mutational events which lead to minor deleterious effects and which, by their large number, might impose a greater total genetic burden on the population than that from a smaller number of relatively more serious conditions.
In order to limit the frequency of stochastic effects to a low level and to prevent the occurrence of non-stochastic effects, the annual dose limit of 50 mSv (5 rem) is recommended by the International  Commission on Radiological Protection (ICRP).
This recommendation provides a safety margin for non-stochastic effects, as the ICRP believes that 0.5 Sv (50 rem) per annum would suffice for all tissues except the eye lens (irradiation leads to lens opacification that would interfere with vision) for which a limit of 0.3 Sv (30 rem) in a year applies (ICRP-26, 1977; Pentreath, 1980).
The estimated risk factors for radiation-induced fatal cancers and serious hereditary damage are given in table 5.4.
The risk factors are mean values although some are clearly age- or sex-dependent: for example, the risk for the development of breast cancer is 5.0 × 10 -3 SV -1 in females and zero for males.
Included in this table are the ‘fractional weighting factors’ which quantify the fractional risk that each tissue contributes to the overall risk and allows one to make an estimate of the risks from partial body radiation.
These values highlight that some tissues are more susceptible to damage than others.
In addition to recommending limits of radiation doses the ICRP urges that all radiation exposures should be kept ‘as low as reasonably achievable’(ALARA), economic and social factors being taken into account; and that no practice involving possible radiation release should be adopted unless its introduction produces a positive net benefit.
5.3 Natural Environmental Sources of Radiation
The major source of radiation to which people are exposed is the natural environmental component (table 5.5).
The contribution by human technological developments is relatively small at present but concern arises because radiation from human sources is ever   increasing and because humanity has the potential, through accidental releases of radioactivity from nuclear reactors and through nuclear explosions, of creating substantial increases in local, regional and global levels of radioactivity.
Radiation exposure to natural sources is a result of both terrestrial and cosmic irradiation and varies spatially because of differences in altitude and radionuclide distribution in the terrestrial environment.
Exposure to natural radiation may be enhanced by human activities such as high-altitude aircraft (and space) flights; construction of buildings using materials of high radium content (pumice stone, granite or light concrete derived from alum shale) producing radon gas; the irradiation resulting from the phosphate industry; from using water from deep wells bored into radon-rich water; from a variety of consumer products containing radionuclides (radioluminescent time-pieces, compasses, smoke detectors, luminous signs, antistatic devices, and colour television sets); or the irradiation due to the release of natural radionuclides from coal-fired power plants.
Such plants produce significant radionuclides (uranium and daughters, thorium and daughters) from coal of various origins and in slag and fly-ash, exposing people to external radiation (from discharged material deposited on the ground) and to internal radiation (from the material inhaled).
The radioactivity comes from radioactive elements in the coal.
As the coal burns, these radioisotopes concentrate because the organic matter in the  coal perishes.
It is even suggested that in the United States the routine exposure of the population to radionuclide release from coal burning in that country could be significant and possibly comparable with the effects from nuclear power (Wilson et al., 1980).
In recent years, concern has been raised over exposure to ionizing radiation from radon gas.
Radon occurs naturally in rocks such as granite, and can move readily through porous rock and soil and can seep into buildings causing dangerous concentrations to accumulate (inhaling radon causes it to be deposited in the lungs so increasing the risks of lung cancer).
Whereas the average radiation exposure per person from all sources in the United Kingdom is around 2.5 mSv per year (3.6 mSv in the USA), enhanced radon exposure in the granite areas of south-west England raises the average level there to 7.8 mSv, with some individuals experiencing 20 mSv in a year (Clarke and Southwood, 1989).
In 1990, an ‘action level’ of 10 mSv in a year (200 Bq/m 3 of radon gas) was adopted as a guide to when remedial measures should be introduced in existing dwellings, and a maximum level of 5 mSv per year (100 Bq/m 3 of radon gas) is incorporated in the design and construction of new dwellings.
The main ways of reducing indoor exposure to radon are to make the floor a better barrier or to divert any seeping radon to the open air.
5.4 Radiological Sources of Radiation
The major sources of ionizing radiation arising from human technological developments include medical uses of radiation, nuclear explosions, and power generation from nuclear fission.
The highest human-derived contribution to global collective dose is caused by the medical uses of radiation, and in particular by diagnostic X-ray procedures.
At present 5 x 10 5 man gray are produced by a small number of the world's countries with developed radiological facilities, while 2 x 10 4 man gray are produced by the majority of the world's population who live without the benefit of frequent radiological examinations (Coggle, 1983).
Diagnostic radiology is growing at a rate of between 5 and 15 per cent per year in many technically developed countries which, together with the rapid growth rate expected in developing countries, will lead to substantial increases in the global dose commitment in the future.
5.5 Radiation from Nuclear Weapons Testing
Atmospheric testing of nuclear weapons produces global contamination, and the most important radioisotopes of fallout are those which give an external gamma-ray dose and those which become  internally deposited in the body, either by direct absorption or via a food chain.
Following a nuclear weapons test the residence time of the radionuclides released depends upon the height to which the radioactive cloud rises (which is related to explosion yield), its latitude of detonation (the height of the tropopause being lower in the polar regions than near the equator), the size of the radioactive aerosols, and whether the radionuclides are in gaseous or particle form.
Large particles will fall out within a few hundred kilometres of the detonation site (the local fallout) while smaller particles and gaseous radionuclides injected into the troposphere may be transported around the earth in the same hemisphere and between hemispheres by, for example, the East African low-level jet stream (Findlater, 1974), to be deposited hundreds or even many thousands of kilometres away (the tropospheric fallout).
Particles and gases which are injected into the stable stratosphere may take months or years (for example, the residence time of strontium-90 is typically of the order of a year) before they become stratospheric fallout.
Ironically, the longer radionuclides remain in the stratosphere, the weaker will be the radioactivity when they return to the earth's surface.
Atmospheric tests by the United States, the Soviet Union, and the United Kingdom ceased with the signing of the Partial Test-Ban Treaty of 1963 although other countries such as France and China continue to undertake such tests.
The total global dose commitment from all nuclear explosions carried out before 1976 ranges from about 1000 u Gy in the gonads to about 2000 u Gy in bone-lining cells (UNSCEAR, 1977).
In the northern temperate zone the values are about 50 per cent higher than these estimates, and in the southern temperate zone about 50 per cent lower.
External exposures contributed by caesium-137 and short-lived gamma-emitting radionuclides account for about 700 u Gy of the global dose commitment for all tissues.
Internal exposures are dominated by contributions from the long-lived radionuclides caesium-137 and strontium-90 (in the skeleton).
Their half-lives of about 30 years will determine the length of time over which the doses will be delivered.
The more short-lived ruthenium-106 and cerium-144 are significant contributors to the exposure of the lung.
The irradiation to which the world population was committed by nuclear tests in the 1950s and early 1960s is already completed for all except caesium-137 (external and internal), hydrogen-3, carbon-14, strontium-90, and plutonium-238.
Caesium-137 makes major contributions to all body tissues which are essentially independent of age, whereas the further substantial contribution from strontium-90 in bone marrow and bone-lining cells is strongly dependent on age.
In comparison, the contributions of the other radionuclides to current annual doses are very small.
Obviously, the irradiation from past atmospheric weapon tests is minute compared with that which would follow a large-scale nuclear war (Ehrlich et al., 1983; Turco et al., 1983; Pittock et al., 1986).
5.6 Radiation from Nuclear Power Stations
5.6.1 Nuclear Power Generation
The generation of power from nuclear fission, generating intense radioactivity, has been developed and used commercially for several decades to meet part of the world demand for electrical energy.
Most of the world's nuclear reactors are termed thermal reactors and are fuelled by natural or enriched uranium.
The pressurized water reactor (PWR) predominates in most national nuclear power industries with some notable exceptions, including the UK (some advanced gas-cooled reactors or AGRs but mostly gas-cooled reactors or GCRs), Canada (the Candu type of reactor which uses heavy water as the coolant), and the USSR (half are of the light-water graphite reactor or LWGR type).
More recently fast-breeder reactors fuelled with a mixture of plutonium and uranium have been developed by some countries.
By the start of 1991, 423 nuclear power reactors were in operation in 25 countries generating more than 17 per cent of the world's electricity (figure 5.4; table 5.6).
An additional 83 reactors are at present under construction.
The International Atomic Energy Agency estimates that total world nuclear electricity generation will increase from just below 1900 TW h at the beginning of 1990 to around 3000 TW h by the year 2000.
Projections for the future are subject to great uncertainty because of the factors involved in energy forecasts and because of uncertainty in the willingness of the public to accept the potential risks associated with increased use of nuclear energy.
Recent reactor accidents have caused a marked slowdown in the installation of nuclear power stations.
Table 5.6 reveals that some countries with small resources in conventional energy sources, including Belgium, Bulgaria, Finland, France, Korea, Sweden, Switzerland and Taiwan, rely heavily on nuclear energy.
The share of nuclear power for electricity generation in these countries exceeded 30 per cent in 1990 but may reach 70–80 per cent by the year 2000 unless, as in Sweden, nuclear energy production loses favour.
Nuclear power production involves a series of steps comprising the processes of mining and milling of uranium, conversion to fuel material (in most cases including enrichment in the isotope uranium-235 from about 0.7 per cent to 2–4 per cent), fabrication of fuel elements, utilization of the fuel in nuclear reactors, storage of the spent fuel, reprocessing of this fuel with a view to recycling, transportation of materials between various installations and disposal of radioactive waste (figure 5.5).
Almost all of the radioactive material associated with the nuclear industry is at present in the reactors and in spent fuel or in well-contained fractions separated from the fuel during the reprocessing operations.
However, at most steps of the operations, releases occur through either the gaseous or     the liquid effluent streams.
Most of the radionuclides released are of local or regional concern, because their half-lives are short compared with the time required for dispersion to greater distances.
However, some radionuclides having longer half-lives or being more rapidly dispersed can become globally distributed.
Although at present nuclear power generation contributes only 0.1  per cent of the total global radiation dose commitment, much public concern has been expressed on the potential local and regional effects of accidental releases of radioactive material.
5.6.2 The Risk of Nuclear Accidents
Although accidents involving the release of radioactive material can occur at any stage of the nuclear fuel cycle, most attention has been focused on reactor accidents.
This is not because reactor accidents are more likely, but because the potential consequences for the general public of some reactor accidents are much greater than from accidents at other stages in the fuel cycle.
Several studies have been made in which the probability of reactor accidents of various degrees of severity were estimated, and in which the environmental impacts of the radiation releases associated with these hypothetical accidents were calculated.
The type of accident considered involves the reactor safety features that control the cooling and running of the reactor.
If they fail, the temperature of the reactor core rises and the fuel melts and may breach the main containment above the reactor.
Such a ‘meltdown’ results in the release of large quantities of radioactive material.
It needs to be stressed that in a thermal reactor the fuel is near to its most active state and there is no way in which the nuclear assembly can go supercritical and explode like a nuclear weapon (Coggle, 1983).
In contrast, the plutonium fuel used in fast-breeder reactors could possibly go supercritical during a core collapse meltdown.
In general, studies have suggested that a large accidental release could cause large numbers of early fatalities and large numbers of latent cancers, but the probability of such a release is low.
However, it is now generally agreed that there is a large range of uncertainty in the numerical results quantifying the risks of an accident, as recent reactor accidents have highlighted.
One example of a hypothetical accident at a nuclear power station is provided by the UK Royal Commission on Environmental Pollution (1976).
It assumes that 10 per cent of the gaseous and volatile fission products are released from a 1000 MW(e) nuclear reactor as a cloud of radioactive material.
The reactor is sited at a semi-urban UK site.
The main health hazard will be from iodine-131 (half-life of 8 days), which will irradiate the thyroid, and caesium-137 (half-life of 30 years), which will cause prolonged contamination of the countryside and buildings.
Since the weather, especially wind direction and wind speed, will markedly affect the behaviour of the radioactive cloud, estimates of the number of people exposed and the hazards can only be expressed in terms of probabilities.
The inhalation of iodine-131 could cause thyroid cancer in people as far away as 24 km, and there is a 20 per cent probability that over a period of 10–20 years between 1000 and 10,000 people could develop thyroid cancer (less than 10 per cent of these would be fatal cancers).
The most probable outcome is 100–150 deaths from thyroid cancer over the same period.
These figures could be ten times higher or lower depending on the circumstances.
The iodine-131 deposited on the ground would decay in a few weeks.
However, the longer-lived caesium-137 would be present in the soil and buildings for many decades.
The radiation levels could necessitate evacuation for weeks, months or longer, even for people up to 50 km from the reactor site.
Similar theoretical studies have been made in other countries such as that by Professor Norman Rasmussen in the United States, who concluded that the worst possible reactor accident would cause 3300 deaths, 45,000 illnesses, and 1500 fatal cancers which would appear decades later (Hawkes et al., 1986).
Serious accidents have taken place in the nuclear fuel industry, most notably with the fire at Windscale (renamed Sellafield in 1981) in north-west England in 1957, the overheating and core and containment damage at the PWR at Three Mile Island, USA, in 1979, and at Chernobyl in the Soviet Union in April 1986.
5.6.3 Windscale/Sellafield Accident, 1957
At Windscale in 1957 a sudden temperature rise and ignition of the uranium fuel cartridges within an air-cooled graphite moderated reactor (a type designed solely for the production of military  plutonium) led to the release of ionizing radiation from the reactor stack.
The most important radionuclide released as far as health consequences to the population were concerned was iodine-131 (a gamma emitter that can cause thyroid cancer).
The radioactive cloud was shown to have drifted over England, Wales and parts of continental Europe (ApSimon et al., 1985; Chamberlain, 1959).
The group of the population which was most at risk was young children drinking locally produced milk.
Over an area of 520 km 2 restrictions were imposed on the distribution of milk which contained more than 3700 Bq/l (0.1 uCi/l) of iodine-131.
The maximum measured individual thyroid dose was 160 mSv to a child in the Windscale area.
The milk restriction was lifted 44 days after the accident when the radioactivity levels fell to an acceptable level (iodine-131 has a half-life of 8.1 days).
Other items of diet, including eggs, vegetables, meat and water, were examined in the most contaminated regions, but it was decided that there was no appreciable health hazard from these foodstuffs and hence that there was no need to implement bans on them.
The collective effective dose equivalent commitment from the release is estimated to have been 1.2 x 10 3 man Sv.
Rather belatedly, Crick and Linsley (1983) estimated that a total of 20 ‘health effects’(cancer deaths plus hereditary effects in the first two generations) would have resulted from the accident.
Thirteen of the health effects would be due to thyroid cancer fatalities, assuming a 5 per cent fatality rate among a total number of expected thyroid cancers of 250.
However, Urquhart (1983) argues that this estimate ignores the contribution of polonium-210 (used by the USA and UK as an irreplaceable initiator for starting the chain reaction at the heart of their first atomic bombs) to the health effects and that the radiation-induced cancer deaths and hereditary effects would have been considerably higher.
In this connection, he claims that an increase in the number of myelomas (tumour of the bone marrow) in the area during the 1970s would not have been expected as a result of iodine-131 contamination, but could have arisen from ingestion of polonium-210, an alpha emitter.
5.6.4 Three Mile Island Accident, 1979
The next known serious nuclear reactor accident occurred at Three Mile Island, Harrisburg, Pennsylvania on 28 March 1979.
One of the two pressurized water reactors initially suffered the loss of normal feedwater supply which led to a turbine trip and later to a reactor trip which subsequently resulted in significant damage to portions of the reactor core.
It is believed that the sequence of events that led to core damage involved equipment malfunctions, design-related problems, and human errors, all of which contributed in varying degrees to the accident (US NUREG-0600, 1979).
The principal radioactive materials released into the environment were xenon-133 (half-life of 5.3 days), xenon-135 (half-life of 9.2 hours), and traces of radioactive iodine, primarily iodine-131.
Some of the radioactive krypton isotopes with half-lives of a few hours may also have been released (US NUREG-0558, 1979).
Radionuclides in particulate form such as strontium-90, uranium isotopes, and plutonium were not detected in the environment and would either have been retained in the fuel or, if released from the fuel, would have remained in the coolant water.
The collective dose to the total population (about 2 million) within an 80 kilometre radius of the Three Mile Island plant has been estimated to be 3300 person rem (for the period from 28 March to 7 April) with an average dose to individuals of 1.5 mrem.
An early estimate of the number of excess health effects (cancer and genetic ill-health) due to the irradiation of this population is approximately two (US NUREG-0558, 1979).
Despite the apparently small health effects, this accident had far-reaching consequences: the credibility of the safety record of the nuclear power industry fell markedly.
People realized that however sophisticated and advanced a technology may be there is no fool-proof technology and no safeguard against human error — and nor, it should be added, against terrorist or military intervention.
NUREG has subsequently calculated that there is a 45 per cent chance of an accident similar to Three Mile Island happening in the next 20 years.
5.6.5 Chernobyl Accident, 1986
If the public's confidence in the safety record of the nuclear power industry faltered following the Three Mile Island accident, the accident at Chernobyl in the Soviet Union caused nuclear reactor safety to be completely re-examined.
After only 4000 reactor years worldwide, Chernobyl approached the worst conceivable nuclear reactor accident, which was previously estimated to be of a probability somewhere near 1 in 10,000 or even I in a million per reactor year.
The Chernobyl accident released 50–100 million curies of radioactivity, according to the official Soviet report, or as much as 287 million curies of radio-isotopes with half-lives greater than one day according to Hohenemser et al.(1986).
This makes it 100 times worse than the Windscale accident and 1000 times worse than the Three Mile Island accident.
The accident at the graphite moderated, light-water-cooled pressure tube reactor at Chernobyl occurred on 26 April 1986 when an explosion produced an uncontrollable fire which lasted several days and led to vast quantities of radionuclides being lifted high into the atmosphere.
The plume of radionuclides was swept across Europe during the following 7–10 days, exposing up to 400 million people in 15 nations to high levels of ionizing radiation (figure 5.6).
In areas   Where heavy rainfall scavenged the radionuclides from the atmosphere, radiation levels peaked up to several hundred times higher than background levels.
The accident resulted in around 30 Soviet citizens dying from radiation sickness (the majority of fatalities — most reactor workers or firemen — had received whole-body doses ranging from 600 rem to 1600 rem).
Another 237 people were treated for radiation sickness, some of whom needed bone-marrow transplants to restore their white blood-cell count.
The number of casualties near the reactor was less than might be expected because the radiation plume was made especially buoyant by the intense fire and so, like the effect of a tall industrial stack, local deposition of radionuclides was reduced whereas the long-range transport of radionuclides was enhanced.
Within a 30 km radius of Chernobyl, 135,000 people were evacuated.
Evacuation was not prompt because the Soviet authorities were initially unwilling to publicize the seriousness of the accident and because few Soviet citizens had access to private transport.
These people now face annual medical inspection for the rest of their lives to assess what the radiation may have bequeathed to them in terms of a legacy of cancer or birth defects.
Of those evacuated, 24,000 are estimated to have received an average of 0.43 Sv (43 rem).
Some of the consequences for this group include over 120 spontaneous leukaemias during the next 50 years and an  increased risk of genetic disorders in future generations (Anspaugh et al., 1988).
Estimates of the number of expected cancers resulting from exposure to the radionuclides vary quite markedly, according to the assumptions made in the model employed (Wilson, 1986; Anspaugh et al., 1988).
Estimates range from several thousands to tens of thousands of thyroid cancers arising from the exposure to iodine-131 and a similar number of cancers elsewhere in the body from caesium-137.
Only a few per cent of the thyroid tumours would be fatal, whereas perhaps half of the cancers from caesium-137 would be fatal.
Hawkes et al.(1986) estimate the likely number of deaths in the Soviet Union at between 5000 and 10,000.
The official Soviet estimate of the projected cancer deaths in western Russia after Chernobyl is put at 4000–5000 due to exposure to iodine-131 and as many as 40,000 from exposure to caesium-137.
Hohenemser and Renn (1988) suggest 28,000 additional cancer deaths worldwide will be the result of Chernobyl contamination, including 10,000–15,000 in the Soviet Union, 1500–3000 in Poland, 4000 in Western Europe and only 10–20 in the United States.
Anspaugh et al.(1988) estimate 17,000 additional fatal cancers in Europe in the next 50 years but highlight that the Chernobyl legacy adds an increment of only 0.01 per cent to European lifetime fatal cancer risk.
Some researchers point out that no radiation dose is so small that the body can perfectly repair all resulting damage to DNA and chromosomes, and they suggest that many of the published estimates of the health effects of Chernobyl radiation are too conservative.
There are many problems involved in calculating the number of cancers and deaths resulting from the Chernobyl accident.
The ground-level deposition of radionuclides is not known accurately and the existence of local hot spots of radiation, where heavy rainfall scavenged the radionuclides from the atmosphere, may not be readily incorporated into some models (although recent improvements have been made as, for example, in ApSimon et al., 1988).
Some hot spots can be very localized: for example, in Konstanz, where it rained on 30 April, ground-level radiation was 15 times greater than nearby Stuttgart, which received no rain.
Rainfall was the dominant factor in determining the areas with the highest depositions of iodine-131 and caesium-137 in the United Kingdom, with Cumbria, south-west Scotland, north-west Wales and Northern Ireland experiencing rain during the passage of the Chernobyl radioactive plume between 2 and 4 May which washed out the radionuclides (Clark and Smith, 1988).
In many places in Europe, fallout from Chernobyl exceeded the peak of the weapons testing fallout in 1963 (figure 5.7; Thomas and Martin, 1986).
Other problems associated with estimating the health effects of the Chernobyl radiation release include the difficulty of taking into account all of the radionuclides involved and all of the protective   measures adopted by governments and individuals to reduce the radiation dose people received.
Protective measures included staying indoors during the passage of the radiation cloud; the removal of the outer leaves and washing of vegetables, and the washing and peeling of fresh fruit; the ban by Western European countries on the importation of fresh food from areas of Eastern Europe within 1000 km of Chernobyl; the restriction in consumption of fresh milk and free-range eggs; the removal of cattle and sheep from open pasture or a ban on their sale (for example, 2.5 million sheep in parts of north Wales were prohibited temporarily from being moved or slaughtered; Kerr et al., 1989); the taking of prophylactic potassium iodide tablets to saturate the thyroid with iodine so as to minimize the amount of radioactive iodine being taken in (this measure was adopted most extensively in Poland); the advice not to drink fresh rainwater; and the warning to the public about possible contamination during outdoor activity.
As with most other air pollution disasters, the Chernobyl accident highlighted many inadequacies in the way in which the authorities reacted to the accident.
The Soviet government failed to provide a warning of the accident to European governments and when information was offered it was incomplete.
Many European governments, such as France and the United Kingdom, failed either to provide adequate answers to the public's urgent questions or to introduce precautionary measures quickly enough.
Radiation monitoring networks were inadequate in many European countries for the purpose of rapidly pinpointing radiation hot spots, though in hindsight a rainfall map of Europe for the period provided a useful approximation.
5.6.6 Post-Chernobyl Attitudes to Nuclear Power and the Risk of Nuclear Accidents
The lesson from Chernobyl is not only that the public requires governments to react more effectively next time, but also that there is a need to ensure that a Chernobyl-type accident does not happen again.
Some would argue that while nuclear power reactors exist, then a repetition is inevitable.
However sophisticated the safety designs incorporated into nuclear power stations, they cannot overcome the human fallibility which can occur.
In both the Three Mile Island and Chernobyl accidents, the operators misread the reactor's condition and shut off the emergency systems at the wrong moment.
Similar human error surfaces in other air pollution disasters such as at Seveso and Bhopal.
What the public and governments have to assess is the need for nuclear energy versus the risk of another Chernobyl-type accident.
To the concern for another serious reactor accident, one has to add the concern for accidents during other stages of the nuclear fuel cycle, the long-term risks associated with the safe disposal of ever-increasing radioactive waste, and the decommissioning of nuclear power plants at the end of their 30-year operating life (about 100 such stations by the end of the century).
Following the Three Mile Island accident and especially after the Chernobyl accident, various responses have been made by national governments and the International Atomic Energy Agency.
The tightening up of safety standards at nuclear power plants inevitably followed the reactor accidents.
In 1989, the World Association of Nuclear Operators was established to encourage exchange of information on operational safety and to initiate technical exchange visits between nuclear engineers to help improve safety standards.
For example, in 1990 the European Community and the Soviet Union were working on a joint nuclear safety programme which may result in the closing, or radical upgrading, of up to 25 Soviet nuclear reactors.
The need for greater international collaboration so as to be able to respond to reactor accidents which have no respect for international borders was urged.
The International Atomic Energy Agency (IAEA), based in Vienna and with 113 member countries, was given a stronger role.
The Agency organized two international agreements, prepared since 1982 but signed in September 1986 as a result of the Chernobyl accident.
The first convention is concerned with the rapid notification of nuclear accidents (even at a military installation).
In 1991, an International Nuclear Event Scale was introduced to help the media and the public understand the safety significance of an incident or accident at a nuclear power plant.
It gives incidents on a scale from one (no hazard to people) to three, and accidents from four to seven (major accident such as Chernobyl with a major release and widespread  health and environmental effects).
The need for neighbouring countries to receive information rapidly concerning reactor accidents is obvious, given the tendency for countries to site their nuclear reactors near their borders.
Consequently, several countries have now set up or increased numbers of radiation monitoring sites near their borders.
In 1989, the United Kingdom established a Radioactive Incident Monitoring Network (Rimnet) to detect overseas nuclear incidents and provide adequate warnings.
The network will eventually consist of 80–90 sites.
The second IAEA convention covers the offering of mutual assistance in the event of an accident.
While such actions are welcome, for some governments and for many people, no amount of safety measures or reassurances regarding the ability to cope with a reactor accident is satisfactory.
Public anxiety following Three Mile Island and Chernobyl was especially evident in the weeks following these accidents as demonstrations became frequent throughout Europe.
Such public pressure, sustained by the media, became translated into political pressure in many countries.
In the USA, in national opinion polls conducted since Chernobyl, there has been a consistent two-to-one margin of opposition over support for new construction of nuclear power stations.
Following a referendum in 1980, Sweden committed itself to phasing out its 12 nuclear power stations by the year 2010 (even through nuclear power provides 45 per cent of the country's electricity).
After Chernobyl, Sweden decided to speed up this decision by planning to close two reactors by 1996.
Austria has decided to dismantle its only nuclear power station, the Philippines has halted construction of its first reactor, and countries such as Australia and Denmark have declined to build any nuclear power plants.
In Italy, a referendum in 1988 produced an 80 per cent majority who voted for closure of its two nuclear reactors and against any further spending on a nuclear power programme.
In Britain, at the end of 1989, all new nuclear power development was halted and no new nuclear power development would be considered before the PWR became operational at Sizewell-B in 1994.
Public pressure against nuclear power was a factor in this decision, but so too was the government's concern at the high costs of nuclear-generated electricity, waste disposal and plant decommissioning which were hindering its attempts to privatize the electricity industry.
Consequently, nuclear power was excluded from the government's privatization plans and instead a state-owned company, Nuclear Electric, was set up to run the existing nuclear power stations (eight Magnox, five AGR, and the PWR under construction at Sizewell) and to continue generating around 20 per cent of electricity in England and Wales (Scotland has a separate company).
Several other countries have either scaled down their original expansion plans for nuclear power, delayed the continuation of their nuclear power expansion or begun to look seriously at the phasing out of existing nuclear power stations.
However, for countries such as France, Japan and Taiwan and a number of developing countries such as the Republic of Korea which lack readily available alternative energy sources, the development of nuclear energy still remains a priority.
In Poland and Czechoslovakia, even though fuel reserves of brown coal (lignite) are large, the government considers the development of nuclear energy as necessary to satisfy the nation's energy needs.
Further, the government and even some environmental organizations consider nuclear power less damaging than the excessive sulphur dioxide pollution emitted by power stations burning low-quality brown coal.
Analogous arguments are sometimes offered with regard to carbon dioxide emissions which contribute to global warming since nuclear power stations, in contrast to coal-fired power stations, do not produce carbon dioxide emissions.
The Soviet Union's commitment to nuclear power appeared strong during the three years following Chernobyl but since 1989 there has been an increasing number of public demonstrations against nuclear power expansion, especially in the Baltic republics, the Ukraine, Byelorussia and Armenia.
As a result, two nuclear reactors in Armenia were closed in 1989 and a number of stations under construction throughout the Soviet Union (and subsequently the Commonwealth of Independent States, CIS) have been halted.
The Ukraine called for the shutting down of all the reactors at the Chernobyl site in 1990 and urged that greater consideration be given to a new energy programme which eliminated nuclear power.
The global situation is that accidents such as Three Mile Island and Chernobyl have initiated a move away from nuclear power but many countries still remain fully committed to its development, albeit with greater concern for safety standards and procedures at nuclear reactors.
Whether this concern can prevent another Chernobyl-type nuclear reactor disaster remains to be seen.
Global Pollutants: Ozone Depletion and Greenhouse Gases
6.1 Compounds Depleting Stratospheric Ozone
The atmospheric ‘ozone layer’ is a layer of relatively high concentration of ozone (O 3 ) located in the stratosphere.
Maximum ozone concentration occurs at a height of 16–18 km in polar latitudes and at about 25 km over the equator.
This ozone layer is important to society and the environment for two reasons.
Firstly, the ozone absorbs solar ultraviolet radiation in the UV-B or 280–320 nm wavelengths, thus warming the stratosphere and producing a steep temperature inversion between 15 and 50 km.
As temperature decreases with height in the troposphere (reaching -40 to -80 °C at the tropopause), the effect of a warm stratosphere is to act as a ceiling or lid to vertical motion in the troposphere — that is, convective processes which produce clouds and precipitation are contained within the troposphere.
Any weakening of the stratospheric inversion would affect convective processes and atmospheric circulation in general, thereby affecting weather and climate.
A reduction of ozone would also cause more ultraviolet and visible radiation to reach the ground, so leading to a warming of the lower atmosphere and the earth's surface.
At the same time, reduced absorption of the ultraviolet radiation in the stratosphere would reduce heating there by perhaps 10 °C.
This situation would in turn tend to promote surface cooling, as less thermal radiation would be emitted from the stratosphere to the ground.
These opposing effects, one of warming and the other of cooling, would be complicated by the changed distribution of ozone in the stratosphere such that the net effect is difficult to predict.
The UK Department of Environment (1979a) suggests the effect of ozone depletion on surface temperature would be barely detectable, while Reck (1976) suggests that the surface temperature would change by between -0.6 °C and + 0.8 °C, the sign depending upon the underlying surface albedo and the presence or absence of clouds  and particulate pollution layers (warming when aerosol hazes are present, cooling when not present).
By 1992 the consensus was that the thinning of the ozone layer has a cooling effect on the surface.
The second reason why the ozone layer is important is because through absorption it controls the amount of ultraviolet (UV-B) or erythemal solar radiation reaching the ground.
UV-B radiation has both beneficial and harmful effects on plants, animals and people.
On the beneficial side, the whole population gains from the action of UV-B radiation in converting steroids in the skin to vitamin D. On the harmful side, it is known to affect the growth, composition, and function — including photosynthesis — of a wide variety of plant species, including many important crops such as soya beans.
Experimental studies with UV-B levels enhanced from zero to 50 per cent above natural levels have shown adverse effects on fishes' eggs, larvae and juveniles, on shrimps, crabs, zooplankton, and other aquatic organisms, as well as on phytoplankton and other aquatic plants which are essential to the aquatic food-web.
In animal studies, UV-B radiation has been shown not only to be carcinogenic, but also to alter the response of the immunological system; this results in impeded recognition of a cancerous tumour as a foreign body.
In people, UV-B radiation is known to cause ageing of the skin, and because of the high correlation between sunshine and skin cancer, it is accepted that UV-B exposure of skin is directly linked to skin cancer (Eaglemann, 1981).
The most serious form of skin cancer is malignant melanoma, which has a 30 per cent fatality rate (Panofsky, 1978).
Malignant melanoma of the skin is caused by cancerous proliferation of melanocytes.
Melanocytes are those cells that produce the brown pigment, melanin, which is responsible for the principal differences in the colours of the human race.
Also, melanin is the pigment that results in protective tanning after exposure to UV radiation (US National Research Council, 1984).
Calculations on the effect of percentage changes in total ozone column on percentage changes in UV-B radiation, using the absorption and scattering properties of ozone and other atmospheric constituents, have indicated an average amplification or magnification factor between 1.5 and 3.0 — depending upon latitude and season.
In other words, a 5 per cent reduction in ozone would result in, say, an increase of between 7.5 and 15 per cent in UV-B radiation received at the ground.
Such an effect has been estimated to cause an increase in the incidence of basal cell and squamous cell (non-melanoma) skin cancer by tens or even hundreds of thousands of cases a year in the United States.
Basal cell and squamous cell carcinomas appear predominantly on exposed areas of the head and neck, while in Caucasians, malignant melanomas frequently appear on the back in both men and women and also on the legs in women.
As skin cancer frequencies increase latitudinally towards the equator, correlating with increasing UV-B exposure, this  projected increased incidence in skin cancer is equivalent in terms of exposure to the population living several degrees of latitude closer to the equator.
Given the significance of the stratospheric ozone layer, concern regarding the effects of pollution of the stratosphere has been expressed in recent years.
Ozone is produced at stratospheric levels by photodissociation of molecular oxygen by radiation wavelengths less than 242 nm and removed by the classical reaction of O + O 3 at levels below the stratosphere.
However, ozone also participates in complex chemical reactions involving trace substances in the stratosphere.
Human activities are increasing the concentration of substances such as oxides of nitrogen, oxides of hydrogen, chlorine and bromine, to the extent that, through catalytic cycles, these substances may be removing ozone at a faster rate than it is being produced.
Stratospheric chemistry is very complex, involving more than 150 chemical reactions between 50 chemical species, so it is by no means certain that all potential threats to the stratospheric ozone layer are yet known.
Possible causes of ozone depletion identified to date include various combustion products emitted from supersonic aircraft; nitrous oxide released from nitrogen-based chemical fertilizers; oxides of nitrogen produced by nuclear weapon testing; and chlorofluorocarbons used in aerosol sprays, refrigeration systems and industrial processes.
6.1.1 Supersonic Aircraft Emissions
Concern for the ozone layer was first voiced when large fleets of the Anglo-French Concorde, the Soviet TU-144, and the American Boeing 2707 supersonic stratospheric-flying aircraft (SSTs) were planned (Sidebottom, 1979).
It was argued that the various combustion products such as soot, hydrocarbons, oxides of nitrogen, sulphate particles, and water vapour introduced directly into the stratosphere would, because of their long residence time (months, years, decades), participate in photochemical reactions leading to a reduction in ozone concentration.
Early emphasis was given to water vapour (Harrison, 1970; Newell, 1970) but soon afterwards oxides of nitrogen became the primary focus (Crutzen, 1972; Johnston, 1971).
It was calculated from relatively simple one dimensional models that 500 Concorde-like SSTs would cause approximately a 3 per cent ozone reduction, whereas a similar number of the planned Boeing SSTs would reduce the ozone by 15 per cent(US National Research Council, 1975a).
This significant difference arose because whereas the Concorde and the TU-144 would cruise at an altitude of 17 km, the Boeing 2707 would cruise at 20 km, and in addition the Concorde and TU-144 would consume only about one-third as much fuel as the larger, faster  Boeing.
Injected at 17 km, a given mass of oxides of nitrogen was estimated to have only about half as great an effect on ozone as when injected at 20 km (figure 6.1).
As it turned out, the Boeing 2707 was cancelled and large fleets of SSTs have not yet materialized.
Estimates of the effect of the few Concordes currently in operation are that they have a negligible impact on the ozone layer.
During the 1970s, knowledge of stratospheric chemistry improved considerably and this led to some radical rethinking of the effect of oxides of nitrogen on the ozone layer, as illustrated by the World Health Organization statements concerning the potential effect of aircraft emissions.
In 1975 it was believed that a large fleet of supersonic aircraft flying at altitudes up to 25 km would have a ‘noticeable effect’ on the ozone layer (World Health Organization, 1976b).
In 1978, this statement was revised to an ‘insignificant   effect’(Panofsky, 1978).
By 1982, stratospheric-flying aircraft, whether supersonic or subsonic, would be expected to have a ‘negative effect’ on ozone — that is, causing a significant depletion of the ozone layer (Hulm, 1982).
An additional finding was that oxides of nitrogen emitted by aircraft flying in the upper troposphere would shift the balance between ozone destruction and ozone production by water vapour radicals, leading to ozone production in that level of the atmosphere.
Derwent (1982) suggests that current upper tropospheric aircraft operations may have already led to such an increase in the total ozone column by up to several per cent.
6.1.2 Oxides of Nitrogen from Agricultural Activities
During the mid 1970s, it was suggested that the increased use of nitrogen-based agricultural fertilizers and/or nitrogen-fixing vegetation might affect the nitrogen cycle and result in an increase in the amounts of oxides of nitrogen released from the surface into the atmosphere.
This would then lead to an increase of oxides of nitrogen in the stratosphere which, through chemical reactions, would deplete ozone.
However, in 1982, the World Health Organization stated that there was little likelihood of a significant change in the ozone layer in this century as a result of changing agricultural practices, but that the topic requires further study because of possible long-term effects (Hulm, 1982).
During the first quarter of the next century, McElroy et al.(1976) suggest that reduction in ozone of the order of 20 per cent might result from current and future use of fertilizers.
In contrast, both Crutzen (1976) and Lie et al.(1976) estimate depletions of only between 1 and 4 per cent for this time-scale.
6.1.3 Oxides of Nitrogen from Nuclear Weapon Tests
Any process which heats the air above approximately 2300 K will produce significant quantities of nitric oxide (NO).
Nuclear explosions produce shock waves which can inject oxides of nitrogen into the stratosphere.
Indeed, the presence of oxides of nitrogen is indicated by the orange colour of the nuclear mushroom cloud at high altitudes, which is a consequence of some of the nitric oxide having been converted to nitrogen dioxide.
The larger the explosion (threshold value of approximately 0.5 million tonnes equivalent of TNT) and the higher the explosion above the ground, the more oxides of nitrogen are injected into the stratosphere.
Some researchers have studied the effects of nuclear weapon testing, which reached a peak in 1961–2, on stratospheric ozone concentration and have arrived at contrasting conclusions.
Goldsmith et al.(1973) calculated that the 1.3–1.7 million tonnes of oxides of nitrogen injected into the stratosphere as a result of 340 million  tonnes of nuclear explosions in 1961 and 1962 were equivalent to the effects of between 600 and 1000 fully operational Concordes, and that there was no significant effect on the stratospheric ozone layer during or after the tests.
On the other hand, Reinsel (1981) studied ozone data from a network of Dobson stations over the period 1958–79 and, in agreement with several other research groups, cautiously suggested a total ozone decrease of between 2.0 and 4.5 per cent due to nuclear testing effects in the early 1960s.
It is possible not only that the increased stratospheric concentration of oxides of nitrogen depleted ozone in the early 1960s, but also that the oxides of nitrogen absorbed incoming radiation by as much as 6–8 per cent in some months (Kondratyev and Nikolsky, 1979).
Claims made in the early 1960s — scornfully dismissed at that time — that nuclear weapon testing was responsible for the anomalous and extreme weather and climate of that period may yet be prove to be valid.
Whether or not the testing of nuclear weapons in the early 1960s caused a significant decrease in stratospheric ozone may remain controversial, but it is generally agreed by researchers that a future large-scale nuclear exchange with detonations totalling 5000–10,000 million tonnes would lead to substantial depletion of stratospheric ozone.
For a 10,000 million tonne nuclear war scenario, it is estimated that the ozone column would be reduced by between 30 and 70 per cent in the northern hemisphere and by up to 40 per cent in the southern hemisphere (Crutzen and Birks, 1982; US National Research Council, 1975b; Turco et al., 1983; Whitten et al., 1975).
Such substantial stratospheric ozone depletion would last for several years and the initial survivors of a nuclear war would have to face a great increase in biologically active UV-B radiation.
The increased UV-B would not only suppress the immune system in people and other mammals, thereby increasing the likelihood of disease, but also increase the incidence of skin cancer, impair vision systems in mammals, and disrupt ocean and terrestrial ecosystems (Ehrlich et al., 1983; Elsom, 1984c, 1985).
6.1.4 Chlorofluorocarbons and Global Ozone Depletion
The increasing domestic and industrial use of a number of stable chlorine-containing compounds has given rise to claims that these compounds may diffuse upwards into the stratosphere where they are dissociated by solar radiation to yield atoms of chlorine which act to destroy ozone through a complex chain of chemical reactions (Crutzen, 1974; Molina and Rowland, 1974).
Of these compounds, special attention has been given to chlorofluorocarbons (CFCs), especially CFCl 3 (CFC 11) used mainly as a propellant in aerosol sprays, and CF 2 Cl 2 ,(CFC 12) used extensively as a cooling agent in refrigerators and air conditioners (figure 6.2).
Another   important CFC is CFC 113 which is used mainly as a solvent in the electronics industry.
CFCs are only very slowly removed from the stratosphere and, with residence times in the range of 65–120 years (Brice et al., 1982), virtually all of the CFCs ever produced (since they were invented around 1930) should still be present in the stratosphere.
Because of the slow diffusion rates into and through the stratosphere, even if all emissions of CFCs into the atmosphere ceased, stratospheric concentration of CFCs would continue to increase for several years.
Other chemicals which pose a threat to stratospheric ozone but whose emissions are smaller include the halon gases (variously containing bromine, chlorine and fluorine) which are used mainly in fire-extinguishers and have an even greater ozone depletion potential (table 6.1); methyl chloroform, which is used as a solvent and cleaning fluid; and carbon tetrachloride, used as a dry-cleaner and to fumigate grain in Eastern Europe (figure 6.3).
Hydrochlorofluorocarbons (HCFCs) and hydrofluorocarbons (HFCs) are groups of chemicals which, because they contain hydrogen atoms, are being substituted for CFCs.
The presence of hydrogen means they are less stable in the atmosphere, leading to a greatly reduced atmospheric residence time.
Most HCFCs have an ozone depletion potential less than one-tenth that of CFC 11 or 12.
Additionally, HFCs do not contain a chlorine atom and so, for example, HFC 134a has an ozone depletion potential of zero (table 6.1).
Estimates of the reduction in global ozone concentration which may be caused by CFCs have varied from 3 to 20 per cent(figure 6.4)(Hulm, 1982; US National Research Council, 1982a, 1984).
Revisions reflect our increased knowledge of stratospheric chemistry such as the recognition that chlorine atoms are removed globally by oxides of nitrogen at a faster rate than was once believed.
How much CFCs have already significantly depleted ozone is difficult to assess.
Natural perturbing effects such as lightning, volcanic eruptions, cosmic rays, solar proton events, global circulation changes, and the variability of solar radiation (ozone increases at times of solar output maxima), all affect stratospheric ozone levels.
The increase in mean ozone concentration with latitude and the existence of seasonal cycles in ozone levels make it difficult to detect any trends in global ozone concentration.
During the 1960s, ozone concentrations generally increased by up to 10 per cent at some monitoring stations (Komhyr et al., 1971), and this was believed to be associated with variation in solar radiation over the 11-year cycle (Keating, 1978).
Analysis of ozone measurements for the 1970s, taken by 36 Dobson spectrophotometers throughout the world, reveal no evidence for either an upward or a downward trend, either on a global basis or on a regional basis (Reinsel, 1981; Reinsel et al., 1981).
However, a World Meteorological Organization analysis of the winter periods between 1969 and 1988 for the zone between 30 and 64 N reveals ozone decreases in the range 3 to 5 per cent, with the larger decreases at high latitudes.
Measurements taken by American satellites have pointed to a reduction of approximately 0.35–1.00 per cent per year through the 1980s which cannot be explained by variations in the solar cycle (Bowman, 1988).
Global ozone depletion by CFCs may be offset in part by natural effects or by the effects of other pollutants.
Thus, for example, a global increase in atmospheric carbon dioxide concentration, through the ‘greenhouse effect’, may be warming the lower atmosphere but cooling the stratosphere, thereby shifting the equilibrium of stratospheric photochemistry in favour of ozone production (Groves and Tuck, 1979).
A doubling of global carbon dioxide concentration is estimated to increase ozone concentration by between 3 and 6 per cent(Maugh, 1984; US National Research Council, 1984).
However, the presence of CFCs, which have strong absorption bands in parts of the infrared spectrum, may induce their own ‘greenhouse effect’ within the stratosphere, thereby offsetting the carbon dioxide effect (Ramanathan, 1975).
Another way in which reductions in stratospheric ozone levels may be partly offset has been suggested in Derwent (1982), who believes that  aircraft operations in the upper troposphere have caused an increase in ozone production.
He predicts that this cancellation effect will last into the 1990s, after which the effect of CFCs will dominate at least in mid latitudes of the northern hemisphere.
In the southern hemisphere, the impact of aircraft operations on the total ozone column amount is anticipated to be much smaller than in the northern hemisphere.
6.1.5 The Antarctic Ozone Hole
In May 1985, measurements at Halley Bay on the Antarctic coast were published revealing that extremely large stratospheric ozone depletions had occurred there each spring since the late 1970s (Farman et al., 1985).
Subsequent measurements using the US Nimbus-7 satellite, aircraft and balloon-borne instruments confirmed ozone losses up to 50 per cent in some (southern) spring months in the lower Antarctic stratosphere.
Whereas Antarctic mean October total ozone levels were between 280 and 330 Dobson units during the 1957–76 period, they dropped to 225–250 by the early 1980s and to less than 200 in some years during the late 1980s (figure 6.5).
Each Dobson unit is a hundredth of a millimetre and refers to the thickness of the stratospheric ozone layer that would result if it were brought to sea level pressure and temperature.
In October 1990, the British Antarctic Survey recorded a daily value as low as 132 Dobson units.
This was unexpected   as previously ozone losses had tended to follow a two-year cycle, with shallower holes evident in even-numbered years.
The two-year cycle was believed to reflect the quasi-biennial oscillation (QBO), a 26-month cycle in which the winds in the lower stratosphere of the tropics alternate between easterlies and westerlies, affecting the size and stability of the polar vortex around Antarctica.
In October 1991, satellite sensors recorded a minimum of 110 Dobson units — the lowest reading in the 13-year record of satellite data.
The reduction in ozone concentration at altitudes between 12 and 25 km (figure 6.6) begins with the first sunlight of the Antarctic spring in late August and is greatest through September and October.
In October 1988, a 25 per cent drop in ozone levels was measured by the total ozone monitoring spectrometer (TOMS) on board the Nimbus-7 satellite, while in October 1987 and 1990 the depletion was about 55 per cent.
Satellite observations reveal the area of major ozone loss to be about the size of the United States.
Immediately outside the ozone-depleted zone is often found a crescent-shaped region of ozone-rich air which accentuates the image of an ozone ‘hole’ as depicted on maps of Antarctic ozone concentration (figure 6.7).
By November/December, the springtime circumpolar atmospheric vortex which has isolated stratospheric Antarctic air from the rest of the southern hemisphere breaks down.
Air from outside the hole, which has not been so depleted of ozone, then enters the region and the mixing of the air masses    dissipates the hole (Rowland, 1988).
At this time, some ozone-depleted air may drift out from the hole, passing over Australia and New Zealand and exposing that continent for a short period to increased ultraviolet radiation penetrating through the thinned ozone layer.
Similarly, drifting of ozone-depleted air to latitudes as low as 40° S may be contributing, in part, to the decreases in ozone levels being measured increasingly in regions outside Antarctica (McElroy and Salawitch, 1989).
The shape and position of the hole varies from year to year.
The southern tip of South America is the only landmass outside Antarctica which sometimes lies beneath the ozone hole.
During October 1987 a 13 per cent stratospheric ozone reduction was measured there.
Although the hole is primarily located over Antarctica and affects few humans directly, the enhanced ultraviolet radiation reaching the surface or penetrating the upper layer of the oceans surrounding the continent may have serious implications for natural ecosystems.
If phytoplankton, the microscopic free-floating algae that form the basis of the marine food chain, are adversely affected, then so will be the krill, the tiny animals that feed on them, and then fish, seabirds, seals and whales.
What appears of particular concern is that the increase in ultraviolet radiation coincides with the time when ocean surface organisms are emerging from the  dark winter period and thus have had no time to adapt to the sun, let alone enhanced levels of damaging UV-B radiation.
The discovery of the Antarctic ozone hole, at a time when global trends in ozone levels were uncertain, came as a complete surprise.
It had not been predicted by any of the models simulating potential pollution effects on the stratosphere.
To explain why the ozone hole exists, research now suggests that the Antarctic region offers an extreme situation in which chlorine molecules can scavenge ozone at a very fast rate (Crutzen and Arnold, 1986).
The key to this appears to be the presence of high-altitude ice clouds, termed polar stratospheric clouds (PSCs), in the lower stratosphere where temperatures dip below -80 °C.
It is suggested that nitrogen compounds in the air become bound up with the cloud ice particles, temporarily removing any free nitrate which would otherwise react and hold the ozone-depleting chlorine as inactive chlorine nitrate.
Researchers believe that the formation of chlorine nitrate in the global stratosphere is the reason why the ozone depletion potential of chlorine is not as great as was originally feared on the worldwide scale.
In contrast, the Antarctic stratosphere with its PSCs provides conditions which prevent the usual reactions between chlorine and nitrogen compounds, so leaving chlorine free to deplete the ozone.
To make matters worse, chemical reactions on the surfaces of the ice cloud particles may speed up the rate of ozone depletion by some forms of chlorine especially chlorine monoxide (figure 6.8; Stolarski, 1988).
If the global pollutant loading of chlorine continues to build up, ozone destruction will proceed even more rapidly, and the Antarctic hole is likely to appear deeper and earlier each year.
The atmospheric circulation and the warmer conditions in the Arctic stratosphere appear less favourable for producing an ozone-hole.
However, during the dark Arctic winter there may be small regions cold enough to produce the all-important PSCs which prime the chlorine into its reactive state.
As winds sweep these chemical pockets into middle latitudes they encounter sunlight and trigger rapid ozone destruction.
In other words, little ozone holes, several hundreds of kilometres across, may be forming in middle latitudes during the winter rather than one large hole, several thousands of kilometres across, forming over the pole in springtime as happens in Antarctica.
Even so, significant reductions in ozone levels have been recorded in the Arctic recently in the spring, especially during March/April (Heath, 1988).
Maximum depletion occurs between 20 and 22 km, slightly higher than the level of strongest depletion in Antarctica, which lies between 16 and 18 km.
Concern exists that once a threshold of a certain concentration of chlorine in the Arctic stratosphere has been passed, a large Arctic ozone hole will become a regular spring feature which will grow and deepen annually to affect the highly populated regions of northern Europe.
6.2 Greenhouse Gases and Global Warming
Gases in the atmosphere, including carbon dioxide, methane, nitrous oxide, chlorofluorocarbons (CFCs), low-level ozone and water vapour, are virtually transparent to incoming short-wave solar radiation but are strong absorbers of outgoing long-wave terrestrial radiation.
This means that some of the energy which would otherwise escape to outer space is trapped within the atmosphere.
The atmosphere is warmed by this process and reradiates long-wave radiation towards the surface, warming the surface.
Because the glass in a greenhouse traps the sun's energy (though it does this mainly by inhibiting convection, thereby stopping warm air rising and escaping), this process has come to be known as the ‘greenhouse effect’.
The natural greenhouse effect is vital to our survival on the earth since, without the extra warmth it produces, the surface temperature of our planet would be well below freezing point.
Our planetary greenhouse effect increases the lower atmospheric temperature by about 32–34 °C, producing a life-sustaining average temperature of around 15 °C.
In comparison, Venus experiences a greenhouse effect of over 500 °C while Mars has a smaller greenhouse effect of 10 °C.
The dominant greenhouse or infrared  absorbing gas for all three planets is carbon dioxide, but whereas Venus and Mars have atmospheric carbon dioxide contents of over 80 per cent, the earth's atmosphere contains only 0.04 per cent .
However, what is currently causing concern is that human activities are increasing rapidly the concentrations of greenhouse gases in the earth's atmosphere.
Most researchers agree that the rise in greenhouse gases will reach the equivalent of doubling the carbon dioxide concentration from its pre-industrial value as early as the 2030s.
This will commit the planet to a warming of 2 to 5 °C once thermal equilibrium is reached (UN IPCC, 1990a; US EPA, 1983a; US National Research Council, 1979, 1982b, 1983b).
This means temperatures will eventually rise by the amount predicted but the thermal inertia of the oceans, which take longer to respond to atmospheric warming, will delay the full rise in temperature for a few decades.
Most researchers expect global temperatures to have experienced about half the predicted equilibrium temperature by the 2030s, that is, 1–2 C warmer than it is now.
This temperature increase may appear to be small, but in reality the earth would then be warmer than it has been for the past 125,000 years (the peak of the last interglacial) and possibly even warmer than it has been for the past two million years (the duration of the Quaternary period of fluctuation glacials and interglacials).
Additionally, global sea level would be expected to rise by 0.2 to 1.6 metres during the twenty-first century because of the thermal expansion of the seawater with increasing temperature as well as from water being added from the melting of mountain glaciers and polar ice sheets.
6.2.1 Carbon Dioxide
Although carbon dioxide (CO 2 ) has been, and continues to be, the principal contributor to the greenhouse effect, methane (CH 4 ), CFCs, nitrous oxide (N 2 O) and low-level ozone (O 3 ) have increased in relative importance in recent decades (table 6.2).
Carbon dioxide levels have been increasing steadily since the pre-industrial nineteenth century as a result of emissions from the burning of carbon-based fossil fuels and changes in land-use practices, especially recent deforestation of the tropical rainforests.
From carbon dioxide preserved in bubbles in ice cores extracted from Antarctica and Greenland, pre-industrial atmospheric carbon dioxide concentrations were found to be about 280 ppm.
The first accurate annual monitoring of carbon dioxide commenced at the Mauna Loa Observatory in Hawaii in 1958 and revealed an atmospheric concentration of 315 ppm.
By 1990 the level had reached 353 ppm, an increase of 38 ppm since 1958 which is equivalent to adding about 80 Gt of carbon to the atmosphere (figure 6.9).
Measurements reveal a pronounced seasonal range in concentration at Mauna Loa of about 6–7 ppm, which reflects an excess of photosynthesis over respiration during the spring and   summer, and the reverse during the autumn and winter.
Variations in sea-surface temperature may be a minor contributor since the dissolved carbon dioxide content of the oceans is dependent on temperature (MacDonald, 1982).
The seasonal cycle reaches 15 ppm at Point Barrow, Alaska, but declines to 1.6 ppm at the South   Pole.
South Pole concentration of carbon dioxide lags behind that of Mauna Loa by a few ppm in any given year, consistent with the interpretation that most of the carbon dioxide is emitted in the northern hemisphere and that there is a time-lag in the mixing of the atmosphere between hemispheres.
Human activities release carbon dioxide to the atmosphere through the burning of wood, coal, oil, natural gas, and other carbon-based materials.
Annual global emission rates of carbon dioxide (expressed in terms of weight of carbon) from fossil-fuel burning were over 5 Gt C per year through the 1980s and approached 6 Gt C per year by 1990, with oil now providing the largest contribution (figure 6.10).
According to UNEP (1989/90) the main contributors to global emissions are the United States (24 per cent), the Soviet Union (19 per cent ), China (9 per cent ), Japan (5 per cent ), the Federal Republic of Germany (4 per cent ) and the United Kingdom (3 per cent ).
Annual increases in carbon dioxide emissions have slowed in recent decades from an average annual rate of 4.5 per cent in the 1960s to less than 2.5 per cent per year since.
This change reflects the effects of the energy crisis in oil and gas in 1973, which prompted greater emphasis on energy conservation, and the effects of economic recession (figure 6.11).
Estimating the future rate of energy growth is of critical importance for predicting future concentration of atmospheric carbon dioxide.
However, this rate is difficult to predict as it will be influenced by rates of population and economic growth, by improvements in energy efficiency, by technological improvements and innovations, and the changing mix of fuels (figure 6.12).
A net release of carbon dioxide to the atmosphere occurs whenever land-use changes result in ecosystems of high carbon    density (e.g. forests) being replaced by those of a lower carbon density (e.g. agricultural or grazing land).
Disturbed soils are an additional major source of atmospheric carbon.
Some changes in land-use can provide a sink for carbon — for example, when farmland is allowed to reforest or when afforestation programmes are pursued.
In addition, those plants for which carbon is a limiting nutrient should experience enhanced growth with increasing carbon dioxide in the atmosphere (this is termed the carbon fertilization or B-effect) which in turn will increase the biosphere carbon sink.
Estimates of carbon releases from the biosphere to the atmosphere in response to changes in land-use have varied enormously in the past but are now generally in the range 0.8 to 2.5 Gt of carbon per year (Bolin, 1986).
In the second half of the nineteenth century, the ‘pioneer’ agricultural expansion across North America, Eastern Europe, Australia, New Zealand and South Africa added substantial amounts of carbon to the atmosphere   through temperate deforestation and field clearances.
Currently, extensive tropical deforestation is responsible for most of the global carbon emissions from land-use changes but the amount is less than that generated from fossil-fuel burning (Revelle, 1982).
As much as 14 per cent of the Amazon basin has been destroyed, especially in Brazil and Colombia (figure 6.13).
In addition, tropical deforestation takes place in other parts of South and Central America, Asia (especially Indonesia, Thailand and Laos) and Africa.
Despite the publicity given to deforestation of the tropics, such land-use changes are expected to continue to contribute less to annual carbon emissions than those from fossil-fuel burning.
Only a portion of all carbon dioxide released into the atmosphere remains there since carbon dioxide is absorbed by the oceans and the biosphere (figure 6.14).
Clark et al.(1982) estimate that about 40 per cent of all carbon dioxide released to the atmosphere remains there to enhance the greenhouse effect.
The upper 70–100 m of the ocean is an effective absorber of carbon dioxide from the atmosphere but the downward diffusion, advection or convection of the carbon-containing waters of the upper mixed layer into the deeper stable layer is a relatively slow process taking 500 years or more (Revelle, 1982).
This slow mixing limits the amount of carbon dioxide the oceans can absorb — at least in the time-scale with which we are concerned.
As the upper oceanic layers become saturated with carbon, the fraction of the atmospheric carbon dioxide which the oceans can absorb may be reduced, thereby exacerbating the carbon dioxide problem.
Depletion of the stratospheric ozone layer, especially over the southern oceans surrounding Antarctica, may enhance the greenhouse effect if more   ultraviolet radiation reaches the surface to kill phytoplankton.
Oceanic plankton currently absorb substantial quantities of carbon dioxide but their destruction would lead to more carbon dioxide remaining in the atmosphere, so worsening the greenhouse effect.
6.2.2 Methane, Nitrous Oxide, Ozone and CFCs
Methane, the hydrocarbon commonly known as marsh or swamp gas, is produced by bacteria which live in an oxygen-free environment such as at the bottom of swamps and rice paddies and in the guts of ruminant or cud-chewing animals such as cattle, sheep and camels where the bacteria help to break down food into a digestible form (enteric fermentation).
Some insects such as termites also produce methane.
Methane is leaked from gas pipes and coal mines as well as from rotting organic matter in waste landfill sites.
In the future, large amounts of methane currently trapped in the frozen tundras of the northern hemisphere may be released as soil temperatures rise in response to global warming (Boyle and Ardill, 1989).
Atmospheric methane concentrations have increased from a level of 0.8 ppm about 150 years ago to around 1.7 by 1990 (Khalil and Rasmussen, 1987; UN IPCC, 1990a) as cattle populations have risen, rice cultivation has expanded, fossil fuels have been extracted in growing amounts and forests burned.
Current annual increases average nearly 1 per cent.
Nitrous oxide, commonly known as laughing gas and once used as a dental anaesthetic, is a natural product of biological processes in soils and water but is also emitted to the atmosphere by fossil-fuel burning, soil disturbance, the application of nitrogen based fertilizers, biomass burning and animal and human wastes.
From a pre-industrial concentration of 288 ppb, levels increased to about 310 ppb by 1990 with a current annual increase of 0.25 per cent.
Low-level ozone, so called to distinguish it from the naturally forming ozone in the stratosphere which accounts for about 90 per cent of the ozone in the atmosphere, is created by the action of sunlight on oxides of nitrogen and reactive hydrocarbons (volatile organic compounds) emitted by vehicles and industry.
In the 1960s increasing ozone concentrations were largely restricted to the major cities of the world, but now widespread areas of Europe and North America are experiencing elevated ozone levels which add to the global greenhouse effect.
Apart from the concern about CFCs destroying ozone in the stratosphere, CFCs pose the problem of being very efficient greenhouse gases.
A molecule of CFC 11 or CFC 12 is 12,000 — 16,000 times more effective as an infrared absorbing gas than carbon dioxide, and CFC molecules remain in the atmosphere for 65–90 years or even longer.
Atmospheric concentrations of CFCs are about one-millionth of carbon dioxide levels but their role in the  greenhouse effect has increased dramatically in recent decades and they may contribute as much as 30 per cent of the greenhouse effect by the year 2015.
Water vapour is a naturally occurring greenhouse gas but the amount of water vapour in the atmosphere is affected by human activities.
For example, combustion adds water vapour to the atmosphere as do some industrial processes (e.g. power station cooling towers).
Atmospheric water vapour is expected to take part in a positive feedback process in that, as the earth warms due to the increase of other greenhouse gases, evaporation will increase over the oceans, adding more water vapour to the atmosphere.
In turn, increased quantities of water vapour will enhance the greenhouse effect, raising the global temperature and increasing evaporation, so adding further water vapour to the atmosphere.
In general, water vapour is ignored in estimates of the human-enhanced greenhouse effect because of the difficulties of measuring global trends in this gas.
6.2.3 Effects on Climate
Grouping carbon dioxide and all the other greenhouse gases together, most computer models predict that an increase in greenhouse gases equivalent to a doubling of carbon dioxide concentration from its pre-industrial level will be reached during the decade of the 2030s (even if CFCs are rapidly phased out as planned).
This increase in greenhouse gases is expected to eventually force global temperatures to rise by 2 to 5 °C, although the full amount of warming will be delayed for a few decades because of oceanic thermal inertia.
About half of the expected temperature rise, say about 1 to 3 °C, may be experienced by the 2030s.
Some models suggest that the earth has already been committed to a warming of at least 1.5 °C by the current increases in greenhouse gases even though global temperatures have actually risen by only 0.5 °C in the past century.
Whether the continued global warming and associated changes in atmospheric circulation will occur steadily is unknown.
It may be that the global warming and atmospheric circulation adjustments will happen in a series of abrupt steps or jumps, the magnitude and timing of which are as yet uncertain.
At high latitudes there are positive feedbacks to warming, such as the melting of the snow and ice which decreases surface reflectivity (albedo), allowing more solar energy to be absorbed at the surface; the thinner sea ice, allowing a greater flow of heat through the ice from the ocean in winter to enhance the warming of the surface; and the increased evaporation, leading to an enhanced water vapour greenhouse effect.
Thus the temperature increase at high latitudes experienced during global warming should be two or three times greater than the warming experienced in the tropics, where enhanced convection, carrying sensible heat away from the ground, acts as a negative feedback.
Various models suggest that lower latitudes will experience a mean winter temperature increase of 1–3 °C, middle latitudes 4–8 °C, and subpolar and polar latitudes 6–10 °C or even greater (UN IPCC, 1990a).
Atmospheric scientists expended considerable effort from the early 1980s in trying to determine whether global warming due to the enhanced greenhouse effect was affecting global temperature already.
The difficulties they faced were that the earth's climate is controlled by many factors and so it is necessary to separate out their relative importance in order to reveal the change in climate resulting from increasing greenhouse gases.
Such an approach is necessary because a warming trend due to greenhouse gas changes may be counteracted initially by a cooling trend caused by other factors.
Major research advances have been made by, for example, Gilliland (1982) who explained 87 per cent of the northern hemisphere 1881-]975 temperature variance (for time-scales greater than three years) by forcing due to the greenhouse effect, volcanoes and the 76-year solar cycle.
This model confirms the importance of the greenhouse gas forcing of the climate, but it suggests that a doubling of greenhouse gas concentration will produce an increase in surface air temperature of 1.6 + 0.3 °C, which places it in the lower range of the generally accepted predictions of temperature increase (Gilliland and Schneider, 1984).
Using an alternative model, Hansen et al.(1981) concluded that radiative forcing by greenhouse gases together with volcanoes accounted for 75 per cent of the variance in the 5-year smoothed global temperature for the past century, while the addition of a hypothesized variation in solar luminosity increased the figure to 90 per cent.
Hansen et al.(1981, 1983) believed that the greenhouse warming should emerge from the noise level of natural climate variability by the end of the century, and expected it to become noticeable during the 1980s.
Indeed, the northern hemisphere   mean temperature decrease which began in the 1940s was arrested and reversed during the mid 1970s (figure 6.15) and the seven warmest years in the past 100 years have occurred since 1980 (1980, 1981, 1983, 1986, 1987, 1988, 1990).
In June 1988, James Hansen became the first prominent atmospheric scientist to tell governments that the greenhouse effect had been confirmed ‘with about 99 per cent confidence’(Hansen et al., 1988).
However, during the 1990s other climatic influences, such as volcanic eruptions and ocean current variations (e.g. El Nino and La Nina in the Pacific Ocean), may partially mask or obscure the sustained global warming trend expected from the human-enhanced greenhouse effect.
To examine the overall northern hemisphere trend this century, Jones et al.(1986) compiled a new record of land and marine temperatures by suppressing variations on time-scales of less than ten years.
They showed there is a clear warming trend of about 0.5 °C from 1900 which they attribute to the greenhouse effect (Jager, 1986).
Similarly, Folland et al.(1984) highlight that northern hemisphere sea-surface temperatures have increased by about 0.8 °C since 1900.
Of more practical importance than estimates of global annual average temperature increase is that of resulting regional and seasonal changes.
Predictions of the regional temperature variations expected to result from a global warming have been compiled by the UN IPCC (199Oa) such as shown in figure 6.16.
Similarly, although a warmer earth implies increased evaporation and precipitation, it is important to know how these two components of the hydrological cycle will balance in particular regions and seasons.
Model predictions of regional changes in temperature and moisture in a future world with the equivalent of 560–600 ppm of atmospheric carbon dioxide have to be treated with much caution.
This explains why attention has been given to producing future warm earth scenarios based on past climatic data.
Wigley et al.(1980) provide such a northern hemisphere scenario by comparing the five warmest years with the five coldest over the period 1925–74.
The average temperature difference between the warm and cold year groups was 1.6 °C for high latitudes and 0.6 °C or the northern hemisphere as a whole — that is, considerably less than the temperature difference predicted for a doubling of atmospheric carbon dioxide content.
In their analysis, maximum warming occurred in winter in high latitudes and in continental interiors.
In contrast, some regions such as Japan and much of India showed negative temperature differences (figure 6.17a).
An examination of the pressure pattern differences between cold and warm years showed that the warm years experienced intensified high-latitude (50–70° N) westerlies, greater cyclonic activity in the Arctic and sub-Arctic regions of the eastern hemisphere, and a westward displacement of the Siberian high during winter.
Precipitation changes showed only a slight (1–2 per cent) overall increase in   warm years compared with cool ones.
Precipitation decreases occurred over much of the United States, most of Europe and Russia and over Japan, while increases in precipitation over India and the Middle East were indicative of a more intense monsoon circulation in warm years (figure 6.17b).
In a similar study, Lough et al.(1983) contrast warm and cold periods (1934–53 and 1901–20 respectively) as scenarios to highlight seasonal differences in Europe that may be experienced in the future.
Their results suggest Europe can expect a greater frequency of blocking anti-cyclones which will lead to warmer summers but cooler winters, a greater frequency of severe winters, drier springs and summers, wetter autumns and winters, and an increased growing season.
Another approach to presenting a warmer earth scenario, based on past climatic data, has been to reconstruct the climate of the mid-Holocene Altithermal or Hypsithermal period of 8000–4500   years ago when temperatures were 1.5–2.5 °C higher than today in middle latitudes.
Butzer (1980) highlights the fact that this was a period when drier conditions prevailed in the wheat- and corn producing belts of North America and the Soviet Union, while moderately increased precipitation occurred in the tropical and subtropical arid zones.
Kellogg and Schware (1981) derived  possible soil moisture patterns for a warmer earth based not only on reconstruction of the Altithermal period, but also on a comparison of recent warm and cold years in the northern hemisphere, and on a climate model experiment.
Their analysis pointed to wetter conditions in subtropical continents but drier conditions in the central United States (figure 6.18).
In contrast to Wigley et al.(1980) and Butzer (1980), Kellogg and Schware (1981) as well as Kellogg (1978) point to wetter conditions in Europe and in the southern Soviet Union.
Such conflicting predictions highlight the limitations of the analogies and models currently employed, and point to the need to relate their predictions to regional trends in precipitation detected already (figure 6.19).
Improved predictions of regional changes in rainfall, snowmelt and evapotranspiration are vital because these parameters determine river flows and any change in river discharge could have serious implications for agriculture (crop yield, irrigation), public water supplies, aquifer recharge, flood protection, hydroelectricity generation, industrial production (e.g. cooling water), navigation, river erosion and sedimentation, water quality and natural ecosystems (Gleick, 1987; Schneider, 1989; Williams, 1989).
One of the principal concerns emerging from scenarios of regional climates which may be experienced due to a carbon-dioxide-induced warmer earth is that of warmer and drier conditions in the Corn Belt of the United States, especially since much of the world food market depends upon the availability of grain from that country.
Bach (1978, 1979) studied data on weather and corn yield in the United States Corn Belt states from 1901 to 1972 and estimated that corn production in those states would change by about 11 per cent for every 1 °C change in average maximum temperature over the growing season, and by about 15 per cent for each 10 per cent change in precipitation.
Bach concluded that warmer and drier weather would decrease corn production in the Corn Belt, and cooler and wetter weather would increase it.
In terms of optimum climatic conditions, the Corn Belt would be forced towards the north on to acid podzol soils which are badly    leached and which would require extensive and expensive soil amendments.
Any reduction in North American grain yields has potentially severe consequences for the world grain market in terms of food shortages, especially for Third World nations with poor purchasing power.
However, Rosenberg (1982) has questioned the validity of estimates of the impact of climatic change on crop yields which assume that crop varieties and production technologies do not change over time.
He quotes the example of hard red winter wheat which has already considerably expanded northward and northwestward since 1920 — being made possible by plant-breeding programmes of more cold-hardy strains and by the introduction of new technological advances and dry land-management practices such as summer fallow and stubble mulching.
In advancing northward from Nebraska to Manitoba during the period, from 1920 to 1980, the hard red winter wheat adapted to a 20 per cent decrease in mean annual precipitation, a 4.2 °C decrease in the mean annual temperature and a 10-day decrease in the length of the frost-free season.
The southern movement of wheat in Texas also involved a significant climatic difference of a 12 per cent decrease in mean annual precipitation, a 1.6 °C higher mean annual temperature and a 10-day increase in the length of the frost-free season.
Not only does Rosenberg (1982) question the implication that a change in US  climate will have an adverse impact on crop yields, but he also questions whether evapotranspiration, which may be a more important parameter than precipitation and temperature, will change adversely.
Rosenberg suggests that evapotranspiration in much of the grain belt may be unchanged or even beneficially reduced, despite changes in other climatic parameters.
6.2.4 Direct Effects on Crop Yields
Whether the changes in climate will adversely affect crop yields in major grain-producing areas of the world may be debated, but there is a growing consensus that increased atmospheric carbon dioxide will directly enhance crop yields.
Controlled growth chamber experiments (as well as greenhouse practices) have established beyond doubt that many plants for which carbon is a limiting nutrient respond to short-term carbon dioxide enrichment with faster growth and greater yields (Rosenberg, 1981).
To what extent this applies to field conditions and to the unmanaged biosphere is as yet unclear.
At full outdoor light intensity, net photosynthesis in many plants increases with carbon dioxide concentration up to at least 900 ppm.
The extent and nature of the effect depends upon plant biochemistry, growth form, age of plant, and status of water and nutrients (phosphorus and nitrogen) in the soil, among other factors.
The two broad categories of plants designated C 3 and C 4 differ in the biochemical pathways through which they fix carbon dioxide in photosynthesis and so differ in the degree to which they benefit from increased carbon dioxide.
(The terms C 3 and C 4 indicate the number of carbon atoms in each molecule of the main type of sugar that each plant manufactures out of carbon dioxide and water as a result of photosynthesis.)
At present atmospheric levels of carbon dioxide, C 4 plants have substantially higher rates of photosynthesis than C 3 plants at all temperatures above approximately 15°C.
As carbon dioxide concentration increases C 3 plants benefit more, until at 1000 ppm some C 3 plants perform almost identically to comparable C 4 species.
C 3 species include temperate species such as wheat, rye, soya bean, rice, barley, legumes, most grasses, and most forest species, while C 4 species are generally those adapted to warm climates and include tropical grasses, corn, sorghum, millet, maize and sugarcane.
On average, for a doubling of carbon dioxide concentration, C 3 plants are expected to increase yields by 10–50 per cent whereas C 4 plants may benefit by 0–10 per cent only.
Yields from C 4 crops may not increase at all if they are subjected to stronger competition from C 3 weeds.
Increased crop damage from insects may also result if the enhanced crop growth is accompanied by less protein in the plant tissues.
With leaves being less nutritious, insects may have to consume greater amounts of leaf.
Carbon dioxide fertilization may also benefit plants by increasing their efficiency in water use.
In some plants, carbon dioxide induces partial closure of the stomata through which carbon dioxide enters the leaf for photosynthesis and through which water vapour simultaneously escapes in transpiration.
This partial closure should still allow the same amount of carbon dioxide to enter, as the carbon dioxide gradient would be greater in a higher carbon dioxide environment, but should reduce the loss of water vapour as the humidity gradient will be little altered.
Laboratory experiments have shown transpiration reduces by between 23 and 46 per cent for a doubling of the carbon dioxide concentration (UN IPCC, 1990b).
The net effect is that some plants should be more resistant to water stress (and more tolerant of atmospheric pollution as partially closed stomata impede entry of potentially harmful air pollutants into leaves).
Sionit et al.(1981) found that carbon dioxide fertilization completely compensated for lack of water when wheat was grown in the laboratory without enough water for maximum development.
Whilst water stress periods depressed production of wheat grown under normal carbon dioxide levels, wheat grown under 1000 ppm of carbon dioxide produced as much as unstressed plants grown at the current atmospheric levels of carbon dioxide, and remained turgid at moisture levels that wilted their unexposed counterparts.
Kimball (1982) reviewed 437 experiments published in this century which dealt with yields of agricultural crops exposed to varying carbon dioxide concentrations, and concluded that a doubling of atmospheric carbon dioxide could increase global productivity by 33 per cent without additional inputs of water or fertilizers.
Other researchers express more cautious optimism as they point out that the carbon dioxide fertilization effect may be offset in some regions by changes in climate.
Revelle and Shapero (1978), for example, point out that if average cloudiness increases, the quantity of incoming solar radiation will be lowered, and the energy available to crop plants for photosynthesis will diminish.
However, whether changes in climate will offset the crop-yield gains attributable to direct enhancement of photosynthesis by higher levels of carbon dioxide will continue to be debated (Waggoner, 1984).
Crop growth will be affected by changes in the length of the growing season, water availability, soil erosion and the frequency of extreme events (floods, droughts, frosts, heatwaves, forest fires) as well as through the loss of fertile agricultural land along coasts due to the rise in sea level associated with global warming.
Global warming may improve the production potential on poleward margins of current agricultural areas and, depending on soil fertility, drainage and topography, might enable areas of new land to be cultivated.
Countries which may benefit include Canada, Norway, Finland, Iceland, Scotland and Japan.
In the tropics and subtropics, regional changes in rainfall patterns and  amounts may have beneficial or adverse effects depending upon the nature of the climate changes.
Since it is difficult to predict how rainfall will alter, it is uncertain which countries will benefit and which will suffer.
However, what is clear is that agriculture in many parts of the developing world is extremely vulnerable to even a slight worsening of conditions (Parry, 1990).
Many developing nations, with limited resources available with which to adjust or adapt to changes in climate, will have to rely on international assistance even more than they do at present.
Countries at particular risk include those in the Sahel region of Africa and South East Asia as well as Brazil, Peru and China.
6.2.5 Effects on Sea Level
Global sea level has risen by 10–20 cm over the last century partly as a response to the 0.5 °C rise in global temperature during that time.
If global temperature rises by 1.5 to 4.5 °C as a result of the greenhouse effect, this could raise global mean sea level by 20 to 165 cm by the year 2100.
Allowing for the various lag effects, this means that by the year 2030 sea level may be 20 to 100 cm higher than at present.
Some extreme-case models suggest that the rise by the year 2100 could be as high as 3.5 metres.
An even greater rise of 5–7 metres is possible if the ice sheet of west Antarctica melted but it would take two or three centuries to do so.
The west Antarctic ice sheet is more vulnerable than the ten-times-larger east Antarctic ice sheet because it rests on the ocean floor rather than on land.
Melting of the Greenland ice sheet would produce a similar rise in world sea level and it may be that its melting would precede that of west Antarctica.
The major cause of the expected sea-level rise will be the thermal expansion of the surface layers of the ocean increasing the water volume.
Water added to the oceans from melting of land-based ice is the other important contributor to the predicted increase in sea level.
Regional variations will occur due to uplift or subsidence of land masses by tectonic and isostatic forces, the latter being the recovery of land that had carried huge weights of ice during the last glacial which ended about 10,000 years ago.
Coastal configuration (e.g. funnel-shaped estuaries), deltaic sedimentation and tidal movement may also play a part in creating local differences in sea-level rises.
With about half of the world's population living in low-lying areas the impact of a significant sea-level rise will be enormous.
A rising sea level will erode shorelines, increase coastal flooding, encourage saltwater intrusion into coastal soils, aquifers and along estuaries, damage coastal marshes and swamps, induce higher storm surges and alter ocean currents (Titus, 1987; Warrick, 1986).
A warmer sea will change the distribution, frequency and intensity of hurricanes (Emanuel, 1987) and alter ocean productivity and the  balance of species.
In the worst-case scenario, coastal resorts, ports and communities face disaster or vast expenditure on coastal protection works.
Extensive areas of prime agricultural land, supporting tens of millions of people, will be lost through inundation and saltwater contamination of the soils.
Natural ecosystems such as coral reefs will be threatened by increased sedimentation due to increased coastal erosion, increased sea-surface temperatures and lowered seawater pH which may lead to destruction of the reefs (Viles, 1989).
Even a modest sea-level rise of 50 cm could have grave implications for low-lying, island and coastal nations.
The low reef islands and atolls of the Caribbean and the Pacific Ocean are particularly vulnerable.
For example, the Maldives in the Indian Ocean consist of 1196 atolls inhabited by 177,000 people, and they would virtually disappear if sea level rose by just 1 metre (Boyle and Ardill, 1989).
The highest land is only 2 metres above sea level and even this would be exposed to storm surges if mean sea level rose by 1 metre.
In the Pacific Ocean the islands of Tokelau, the Marshall Islands, Tuvalu, the Line Islands and Kiribati will face similar problems.
The major tropical and subtropical river deltas of the world including the Mekong delta of Kampuchea and Vietnam, the Ganges-Brahmaputra-Meghna delta of Bangladesh, the Nile delta of Egypt and the Mississippi delta of the USA are also of major concern because of the large populations they support and the importance of such areas for agriculture (figure 6.20).
In addition, subsidence in these deltas, either natural or caused by river damming and groundwater removal, is likely to accentuate greatly the local rise in sea level expected from global warming (Milliman et al., 1989).
A 1 metre rise in sea level would threaten 15 per cent of Egypt's habitable land, requiring 14 per cent of its population to be relocated.
A similar rise in Bangladesh would inundate about 12 per cent of the country and would require 10 million of its people to be moved.
While the rise in sea level will most likely be gradual, the damage resulting from this rise will occur during extreme events such as storm surges caused by tropical cyclones.
The possible devastation is enormous.
For example, in April 1991 a cyclone storm surge killed over 130,000 people in Bangladesh as the sea penetrated far inland over the low-lying coastlands.
Existing coastal defence structures will have to be raised and strengthened to cope with the mean sea-level rise and increased height of storm surges.
In The Netherlands, existing dikes and other protection works will have to be raised and reinforced at a cost of US $3.1–8.8 billion for a 70–200 cm sea-level rise (Barbier, 1989).
In an extreme case of a rise of 5–6 m in sea level, the USA would lose most of its southern coastal lowlands including half of the state of Florida.
For a more realistic scenario of a 1 metre rise, cities and resorts built on sandbanks on the eastern and Gulf coasts including Miami, Galveston and Atlantic City would be vulnerable to serious    flooding during hurricanes.
The Environmental Protection Agency estimates that the cumulative capital cost of protecting developed areas of the United States against the expected sea-level rise by the year 2100 would be $73–111 billion at 1988 prices and that even then an area equivalent to the size of Massachusetts would be lost through flooding (Gribbin, 1990).
Britain is likely to suffer a major loss of land along the east coast especially around the Wash and Thames estuaries and the Norfolk Broads, while coastal protection works such as the ♯500 million River Thames barrier protecting London would have to be raised.
The cost of improving coastal defences in Britain alone for a sea-level rise of 20–165 cm is put at ♯5–10 billion.
Worldwide, the costs of protecting against a substantial sea-level rise could be astronomical.
Whether the world's nations can work together to minimize the adverse consequences of the future rise in global sea level, let alone achieve significant reductions in emissions of the greenhouse gases causing global warming and the associated rising seas, remains to be seen (refer to chapter 12).
Having reviewed the nature, sources and effects of atmospheric pollution in the foregoing chapters, we now move on in part two to examine in detail the various national and international approaches to atmospheric pollution control.
National and International Approaches to Atmospheric Pollution Control
Air Pollution Control Strategies
7.1 Introduction
An air pollution control strategy refers to the master plan adopted by a country, or sometimes a group of countries (e.g. the European Community), to tackle air pollution problems and to ensure that air pollution concentrations are reduced or are maintained below a specific or general level that is deemed acceptable.
Strategies may be short-term or long-term.
They may differ from country to country, as may the level of pollution deemed acceptable, but there are common elements which make it possible to distinguish four types of strategies:(1) air quality management,(2) emission standards,(3) economic, and (4) cost-benefit.
Although all four strategies may be separated so as to discuss their major characteristics, in practice countries commonly adopt a combination of two or more types;(Murley, 1991; UN ECE, 1987).
This is illustrated in later chapters which examine selected national approaches to air pollution control in detail.
7.2 Air Quality Management Strategy
The air quality management strategy, sometimes referred to as the air resource management strategy, involves designating the level of pollution deemed acceptable in terms of a set of ambient (outdoor) air quality standards and then controlling pollutant emissions to ensure that these legal limits are not exceeded (Weber, 1982).
De Nevers et al.(1977) define air quality management as the ‘regulation of the amount, location, and time of pollutant emissions to achieve some clearly defined set of ambient air quality standards or goals’(figure 7.1).
Obviously an approach which involves regulating emissions from millions of stationary and mobile pollution sources which have varying characteristics, locations and use   patterns is not simple and the strategy produces a complex set of control regulations.
Nevertheless, it is an approach employed by many countries: it was first used by the Soviet Union in 1951, but it is the United States with which this strategy is most associated and which has made known its advantages and disadvantages.
The strategy involves several steps which can usefully be discussed within a fivefold typology.
Firstly, ambient air quality standards or goals need to be specified.
These standards may vary from country to country and they may be changed from time to time within a given country.
Standards may be applied nationally, regionally or according to local conditions of land use, topography, meteorology, and so on.
Standards are usually based on detailed scientific investigations of the effects of various levels of individual pollutants (or combinations of pollutants) on public health, animals, vegetation, materials, etc.
The standards also involve economic, social, technical and political considerations.
Given the inclusion of so many considerations, it is not surprising that standards may be amended from time to time: for example , standards may be made more stringent if new scientific research reveals a lower threshold of effect than was previously believed to exist.
Standards may be relaxed if, say, the cost of controlling pollutant emissions in order to achieve the standards is deemed to be economically unacceptable to a nation.
Secondly, to know if the actual pollution concentrations are in compliance with the standards, an air pollution monitoring system  is required.
This requires adequate numbers and distribution of accurate monitoring equipment and specialist staff to maintain and operate the equipment.
It is vital to have accurate knowledge of existing air quality, yet in practice, problems arise.
Thus, for example, although the United States has an impressive air quality monitoring network involving over 9400 stations, some doubts have been raised concerning the effectiveness of this system.
The US General Accounting Office (1979) sampled 243 monitoring stations and revealed that 81 per cent had one or more problems that could adversely affect the accuracy and reliability of air quality data.
Thirdly, in order eventually to control the pollutant emissions in an area so as to achieve the air quality standards, an inventory of current emissions from the various sources is needed.
Ideally, the emission rates of pollutants and plume-rise parameters should be known for each hour of the day and day of the year for all sources of pollution.
In practice, only approximate figures can be obtained, except for the sources which are major individual contributors to the overall emission in an area and therefore subject to more precise measurement.
In the case of the large number of small emission sources, emission inventories frequently have to resort to treating them collectively as area sources.
Fourthly, a mathematical model is needed to be able to calculate air pollution levels from an emission inventory of an area under varying meteorological conditions and to predict air pollution levels arising from proposed changes to emissions in an area.
Such a model is central to the management of air quality in order that alternative regulations of emissions can be examined for compliance with air quality standards.
The accuracy of models varies considerably because of the ability with which various models can handle the emission information and the complexities of physical and chemical relationships and atmospheric dispersion.
The US National Commission on Air Quality (1981) typically found that models may overpredict or underpredict from actual concentrations by a factor of two.
The simple ‘proportional’ or ‘rollback’model assumes linearity in the relationship between emission rates and resulting pollution concentrations such that, for example , a region with sulphur dioxide levels at twice the air quality standard will attain the standard if total sulphur dioxide emissions in the region are halved.
Given that this model has been so widely adopted, de Nevers et al.(1977) outline in detail its many limitations.
Of greater sophistication are the diffusion or dispersion models which can predict pollution concentrations from emission inventories under varying meteorological conditions and take the topography of the area into account.
Such models not only allow various emission control programmes to be examined for existing pollution sources, but also can assess the likely impact of new pollution sources.
More  elaborate models as outlined by Gross (1982) are able to assess the consequences of proposed land-use and transportation policies (figure 7.2).
In this model, modelling land-use and transportation policies represents the first stage of this air quality simulation system (Gross, 1982).
The output from this stage is passed to the emission stage of the model where, based on emission characteristics, the transport and land-use activities are converted to pollution emission rates.
The output from this stage represents the geographical distribution and intensity of emissions over the region.
Based on this information, meteorological and atmospheric dispersion models are employed to predict the spatial distribution   of pollution concentrations over the area.
Such a map of predicted pollution concentrations reveals whether the proposed emission control programme will result in compliance with the air quality standard for a particular pollutant or group of pollutants.
Should it reveal areas which would fail to meet the standard, then alternative sets of options for emission control can be considered.
Fifth and finally, given the need to reduce emissions in an area in order to achieve compliance with air quality standards, a set of emission control options or tactics is needed (table 7.1).
The more emission control options available to a manager, the more flexible the air quality management strategy can be made to suit a given situation.
At one extreme, emission control options may involve comprehensive integral planning of energy supply, transportation, land-use, and industrial development for a region.
At a more specific level, emission control options may include the setting of emission standards, restrictions on the use of certain fuels, the requirement that emission control equipment be fitted, the closing down of factories, and the adoption of tall stacks.
All such options need to be examined for their effectiveness in reducing emissions so as to achieve air quality standards, as well as for their technical and economic feasibility, the speed with which they can be implemented, and their enforceability.
One major criticism of the air quality management strategy relates to its use of air quality standards.
Although such legal standards result in pollution concentrations being reduced to an acceptable level in a ‘polluted’ area, pollution in a ‘clean’area is allowed to increase up to the standard.
This entails degradation of air quality where it is now high.
To overcome this limitation, many countries adopt a special ‘non-degradation’ or ‘no significant deterioration’policy to prevent increased pollutant emissions in clean areas such as wilderness areas, national parks, scenic locations, and so on.
Another criticism of this strategy is that it may result in widely varying control requirements and pollution control expenditures for competing companies in regions of differing air quality.
Stringent control may be placed on an industrial plant in an area of non-compliance while an identical plant in an area of compliance may be subject to little or no control.
However, in order to reduce such anomalies, which could result in unfair economic competition, emission design or performance standards for particular industrial plants or processes are usually applied nationally.
Such action represents the adoption of aspects of a second air pollution control strategy — namely, the emission standards strategy.
It is not unusual to find that countries adopt the best parts of both strategies in order to tackle air pollution problems.
The United States, for example, uses the air quality management strategy for the control of pollutants called ‘criteria pollutants’(for which air quality standards have been set) and employs the emission standards strategy for another list of pollutants termed ‘hazardous pollutants’(e.g. beryllium, mercury, asbestos).
7.3 Emission Standards Strategy
The emission standards strategy specifies the maximum amount or concentration of a pollutant which is allowed to be emitted from a given source.
Emission standards are designated for a large number of pollutants or combinations of pollutants and may be applied to individual or specific groups of emitters.
If emission standards are derived from consideration of air quality standards this makes the emission standards strategy really part of an air quality management strategy.
If emission standards are derived from consideration of the best available control technology and from economic considerations, this strategy is entirely independent of the air quality management strategy.
Frequently, this strategy is referred to as the ‘best practicable means’ or ‘good practice’strategy and is associated with the traditional British approach to air pollution control.
The strategy implies ‘best available and economically feasible control technology’ or ‘best available techniques not entailing excessive costs’(BATNEEC).
Such an approach attempts to reduce  air pollution to the greatest extent possible with the practical methods available, but maintains that the cost of doing so should not be unreasonable.
Emission standards come in a variety of forms, such as the maximum concentration of a substance in a given volume of gaseous effluent or the maximum opacity of a smoke plume.
The former type of emission standard may be qualified further by specifying the type of control devices that sources should install in order to fulfil the emission standards (i.e. equipment or design standards).
In other cases an emission standard may control the nature of the fuel burned: for example, in the case of sulphur dioxide emissions, an emission standard to limit the sulphur dioxide emitted per unit of fuel burned specifies the use of coal with a low sulphur content-say, 1 per cent by weight.
Similar standards may be applied to lead in petrol.
Some flexibility is possible with some emission standards such that, for example , emission standards may be adjusted in relation to stack height, in recognition of the better dispersion achieved by higher stacks.
Since it is more expensive to install pollution control devices in existing installations than in new ones, this strategy often leads in practice to the specification of different requirements for new and existing units.
As technology improves, thereby making it possible to set a lower emission standard, new sources face increasingly more strict controls.
The emission standards strategy is a relatively simple strategy to apply and as such it is a strategy that many countries initially adopt in order to control pollution from large single-emission sources which are deemed to be major polluters.
Some argue that the emission standards strategy is better than the air quality management strategy, since once air quality standards are set, one has to calculate the emission reduction required of sources.
The best that can be done in trying to meet the air quality standards is to persuade each emitter to reduce emissions to meet some emission standard, which will ultimately be based on someone's judgement of what is ‘good practice’.
Thus, they argue, why not short-circuit the development of ambient air quality standards, air monitoring, diffusion modelling, etc. and simply require all emitters to go directly to a ‘good practice’ emission standard?
However, this view highlights the major criticism of the emission standards strategy in that it does not make publicly evident any explicit judgement as to what level of air quality is to be regarded as acceptable, whether for the protection of public health, vegetation, animals, materials, or amenity.
It does not guarantee, however comprehensive it may be, the achievement or maintenance of acceptable air quality (Wilde, 1978).
In practice, however, the decision as to what emission standard is to be adopted inevitably considers not only economic and technological aspects but the effects of air pollution on human health and the environment, as well as the political climate and  public opinion.
Indirectly, therefore, air quality is frequently considered to some extent in this strategy and it is seldom found in its pure form.
7.4 Economic Strategies
Ideally, strategies using an economics-based approach to air pollution control would provide financial incentives for emission sources to pursue the most cost-effective means for reducing pollution (Anderson et al., 1977; Magat, 1982; Rosencranz, 1981).
A variety of economic strategies may be used to control air pollution, although the use of emission charges and emission permits are the most frequently cited (table 7.2).
The emission charges strategy is based on requiring polluters to pay charges related to the amount of pollution they emit, the purpose being to leave polluters the choice to pollute or pay.
This strategy recognizes that different pollution sources have different marginal costs of control, that some can control more cheaply than others, and that there would be net savings to society if some plants reduced emission to a greater and others to a lesser extent.
A properly set uniform emission fee that charged a given sum for each unit (say, 1 tonne) of pollutant emitted, for example, could achieve a collectively established level of air quality at the least total control cost to society (Krier and Ursin, 1977).
Assuming that each source wishes to minimize its total costs under the emission charges system, each would blend control expenditure (to reduce its emissions and thus its emission charge liability) and emission charges (on those units of pollutants emitted after control) in the way it finds cheapest.
Such an economic approach provides incentives   to firms to find new techniques and new products which lead to lower pollutant emissions.
The air quality which results from this strategy depends upon ensuring that the charges are set at precisely the right level for all pollutants.
The emission charge may have to be continually adjusted until the air quality which society deems acceptable has been achieved.
In this sense, this strategy may be viewed as a tactic employed in the air quality management strategy to meet air quality standards.
However, it could be employed in its pure form and perhaps the fees used to compensate victims of pollution and offset damage caused by the pollution.
Thus, for example, several international airports have set up a system of charges related to the level of aircraft noise, with the funds raised being used to finance the insulation of nearby homes.
The emission charges for each pollutant would have to be adjusted to take into account increases in the number of emission sources (if air quality is not to deteriorate) and it would have to be adjusted to reflect increases in general price levels.
Setting the fees for each pollutant would be highly contentious and beset by political wrangling.
This strategy has its limitations in that polluters with strong market power could simply pay the charges and pass them on to consumers.
Also, the equipment to monitor continuously and precisely pollutants at their source is in many instances not available.
Since this strategy requires monitoring emissions from individual sources, it is a strategy which tends to be limited to the larger industrial emission sources.
This is the form in which it has been adopted in Czechoslovakia.
To apply this strategy to numerous small-scale emission sources such as heating in homes and motor vehicles would be more complex.
Even so, Mills and White (1978) have proposed an ‘effluent fee’ for motor vehicle emissions calculated by measuring emission levels at the beginning and end of the year multiplied by the distance driven.
They argue that this effluent fee would give motorists the proper incentive to ensure that exhaust emission control devices were efficiently maintained.
There are many forms of emission charges which may be used to provide economic incentives to polluters either to reduce or cease their pollution, including sales taxes on fuels or fuel ingredients: for example, Norway introduced fuel taxes in 1971 whereby a basic tax is levied on each unit of oil, together with an additional charge which increases for each 0.5 per cent of its sulphur content.
To provide a further economic incentive for emission sources to reduce the pollutant emissions, Norway later introduced rebates to polluters in proportion to the amount of pollution reduction achieved by them (OECD, 1980c).
Penalty fees or fines constitute another form of economic incentive for not violating emission standards and these are widely adopted.
Suzhou, for example, a city in the Jiangsu Province of China, assesses the fine to be paid by a violator as the volume of wastes emitted (in tonnes or cubic metres) multiplied by the amount of pollutants exceeding the national standards (in percentage terms), multiplied by the standard fee for each unit of waste which exceeds the national standard (Chang, 1985).
In Japan, a fee is charged based on emission levels for large sulphur dioxide sources in polluted areas.
The proceeds are used in part for the medical care of patients affected by air pollution (Wetstone and Rozencranz, 1983).
In 1985, France introduced a levy on all 400 combustion plants larger than 50 MW with the proceeds being used for investment subsidies to reduce emissions in the plants concerned or to develop new pollution control technologies.
Other economic incentives adopted by some countries include subsidies or grants for the installation of pollution control equipment, tax remissions for investment in or operation of pollution control equipment, tax remissions for industries which use wastes as their main raw material for production, and import duties and quotas on pollution-producing equipment or materials (table 7.2).
The choice of economic incentive ultimately depends upon the purpose of this strategy, whether it is to reduce specific types of pollution generally or in given geographical areas, to reallocate funds for investment in pollution control or to compensate victims of air pollution.
An alternative economics-based or market-oriented strategy which avoids some of the problems of the emission charges strategy is a system of ‘pollution permits’ or ‘pollution rights’.
The regulating authority determines the total amount of emissions to be allowed in an area and then issues the equivalent number of permits or rights to pollute, which can then be marketed, auctioned or allocated in some way.
Although such a strategy is possible as a distinct strategy, it is more often employed as a tactic for reducing emissions as part of the air quality management strategy.
In the United States, an Emissions Trading Policy has been adopted which represents a move in the direction of a marketable pollution permit system (Brady, 1983).
The Emissions Trading Policy established in 1982 incorporates previous policies known as ‘bubble’, ‘offsets’ and ‘banking’into a single transactional instrument, the ‘emissions reduction credit’.
The bubble policy was the first implicit market-based approach to be used in reducing emissions in the United States.
It allows for firms to place an imaginary bubble over the multiple point sources of its plant and to be given an overall maximum emission limitation for the bubble (figure 7.3).
Within the bubble, firms are allowed to increase emissions at one point source as long as greater or compensatory reductions in emissions are accomplished at another point source within the bubble.
Such a policy allows for intrafirm trades in emission reductions.
The ‘offset’ policy extends the bubble concept so as to allow interfirm trading of emission permits among activities not located in the same plant or not owned by the same firm.
It requires a greater than one-for-one reduction in emissions in order to   achieve a net improvement in ambient air quality.
Firms which reduce emissions by amounts exceeding the reductions required by the authorities under the bubble or offset policies can ‘bank’ emission reduction credits and either use them to expand their own plant later or openly trade them with other companies.
Obviously, constraints have to be imposed on transactions in emission reduction credits in order to ensure that trades do not contribute to a violation of the air quality standards (Brady, 1983).
Given the success of this system to date, it is likely that firms will press for much wider adoption of this strategy for pollution control in the United States (refer to section 8.11).
However, it requires accurate monitoring of emissions and air quality, as well as an effective supervision and enforcement system for this strategy to be successful.
Of course, with this strategy being market-oriented, it is possible that one firm could purchase more pollution permits than it needed, with the intention of keeping out competitors, and the market would cease to function.
Interestingly, Rosencranz (1981) points out that conservation groups could conceivably buy up all available permits to ensure improved air quality.
However, this is unlikely to occur given that the whole concept of the buying and selling of the right to pollute is unacceptable to most environmentalists.
At the largest scale, the ‘bubble’ approach exists in the form of caps or ceilings on national emissions of sulphur dioxide, oxides of nitrogen and carbon dioxide for countries that have signed the relevant protocols put forward by the United Nations.
These protocols specify the percentage reductions of emissions required from the total emitted by the nation during a specified baseline year.
In effect, this permits the country in question to decide how the emissions are to be achieved within their national ‘bubble’.
7.5 Cost-Benefit Strategy
A cost-benefit strategy first attempts to quantify the costs of all of the damage resulting from air pollutants and the costs of all known ways of controlling those pollutants, and then adopts the pollution control option(s) which minimizes the sum of pollution damage and pollution control costs.
Figure 7.4 illustrates this approach in its conceptual form.
If no pollution control expenditures are made, ambient air pollution concentration will be high and pollution damage costs high.
As control expenditure increases, the pollution concentration and associated damage costs will fall.
Expenditure costs rise very steeply as the ambient concentration approaches zero.
The damage cost curve is shown starting at some low value at a small or zero concentration — and increasing rapidly at high concentrations.
The sum of the two is shown to have a minimum value at some intermediate concentration.
This minimum is the optimum; expenditure above or below it is deemed wasteful for society as a whole (de Nevers, 1981).
Such an approach is exceedingly complex, given that one must assign values for all kinds of pollution damage including premature death, varying degrees of illness, property deterioration, reductions in yield and quality of crops, damage to irreplaceable historic monuments, and visibility degradation.
All the effects of pollutants need to be considered for both short- and long-term exposure.
Similarly, all the costs of pollution control options must be considered, including equipment, administration, effects on patterns of development, and unemployment.
At present our knowledge is too inadequate to allow us to quantify all of the many variables involved in cost-benefit analyses of pollution effects and pollution control.
Nevertheless, some progress is being made in assessing the costs and benefits associated with even the most complex pollution problems, such as visibility degradation.
Kneese (1984) describes a methodology being developed to determine how much people are willing to spend to preserve good visibility conditions in national park areas.
In the future, the cost-benefit strategy will gain increased favour with nations attempting to integrate pollution control with other policies such as agriculture, industry, energy, transportation, and land-use management.
7.6 Strategy Adoption
Although some attempts at tackling air pollution problems date back to the last century or even earlier, most countries introduced their strategy for air pollution control in the 1950s, 1960s or 1970s.
Nearly all have adopted either the air quality management strategy or the emission standards strategy, or a combination of both (Martin, 1975; Campbell and Heath, 1977; Murley, 1991; UN ECE, 1987).
At one extreme lies the Soviet Union which has over 100 national air quality standards and few emission standards.
The United States is another strong advocate of the air quality management approach, although it does have a large number of emission standards and has also recently introduced aspects of an economics-based strategy.
In contrast, the United Kingdom has traditionally employed the emission standards strategy, with the concept of ‘best practicable means’ being introduced in the Alkali and Works Regulation Act of 1863.
However, its entry into the European Community, which adopted the air quality management strategy as its common policy in 1980, caused a radical change in the British approach to air pollution control as it did also for Denmark and Ireland.
It is likely that in the future there will be greater international uniformity of strategy adoption as the problem of long-range transport of air pollutants receives increasing attention.
There are many arguments as to which strategy is best (de Nevers et al., 1977; Wall, 1976b) but increasingly the conclusion being  reached is that it is possible to make use of the advantages of two or more strategies.
Persson (1977), for example, argues that for single emission sources and small industrial areas with good atmospheric dispersion, emission standards and requirements on stack heights based on best practicable means will guarantee an air quality better than the air quality standard.
A special non-degradation policy is not needed.
In large urban-industrial areas, the air quality standard should be the determining factor for regulations and systems planning concerning industrial development, energy supply, and transportation.
Furthermore, in times of economic constraints, the incorporation of aspects of an economics-based strategy may be beneficial.
In general, a national strategy evolves and changes over time and ultimately the detailed nature of the strategy adopted by a country is unique to that country.
The following chapters consider the nature of air pollution problems in selected countries or groups of countries, and the strategy and tactics adopted to tackle air pollution problems in order to achieve an acceptable level of pollution.
7.7 Pollution Episodes and Short-Term/Emergency Control Policies
Over a period of days or even hours, pollution concentrations may rise to levels which pose a threat to people's health.
Such an event is termed an air pollution episode.
It may be caused by an accidental or inadvertent release from an industrial plant, and the effects may range from annoyance (e.g. odour) through minor illness or discomfort (nose, eye and breathing irritation, coughing, nausea) to, in some cases, serious illness or even death as happened at Seveso, Bhopal and Chernobyl (table 7.3).
The adverse effects of industrial accidents are usually apparent immediately, but in incidents involving ionizing radiation or toxic chemicals the health effects (e.g. cancers, genetic changes) may not be evident until many years later.
Other pollution episodes, commonly referred to as smogs, occur when atmospheric processes fail to dilute and disperse emissions as happens normally.
At the same time, short-term increases in emissions due to increased space heating demands or increased traffic may also play a part in escalating pollution levels during stagnant atmospheric periods (anticyclonic or high-pressure weather conditions).
Episodes occur when winds are very light or calm and when upward dispersal of pollution is inhibited by a strong thermal inversion which acts as a lid to vertical movement of pollutants.
Extreme values of temperature and the presence of fog can play a role in causing pollution concentrations to reach health-threatening levels (figure 7.5).
The topographical setting of an urban area can increase the frequency and severity of adverse meteorological conditions.
Some smogs can be made worse when   pollution is imported from outside the city boundary from neighbouring urban-industrial areas.
The London and New York smogs of the 1950s and 1960s were some of the worst sulphurous smogs to have occurred whilst, since the 1940s, Los Angeles has provided many examples of the worst photochemical (ozone) smogs.
Today, smogs are a common occurrence in many major cities throughout the world (figure 7.6).
Adverse meteorological conditions should not be considered the fundamental cause of smogs since high concentrations of pollution would not arise unless the general level or baseline of pollution in the urban atmosphere was relatively high anyway: there is a limit to the peak pollution value that the atmosphere can produce.
Smogs become more likely if annual levels of pollution are increasing whereas they become less likely if pollution levels show a downward trend.
This emphasizes the importance of long-term pollution control policy to reduce the frequency and severity of smogs (figure 7.7).
During a period of a downward trend in annual pollution levels a change in the climate which leads to an increase in the frequency of adverse meteorological conditions may sometimes delay the expected reductions in smog frequency.
Since pollution episodes threaten the health of the community there is pressure on central and local government to take action to prevent their occurrence.
Authorities are expected to reduce or eliminate the risk of industrial accidents by requiring industrial plants to adopt specific pollution control equipment and practices.
Commonly, industrial licences are required and safety checks are made before a plant commences operation, and annual inspections are undertaken thereafter.
Factories may be required to be sited at some distance from residential areas in specially designated industrial zones.
Of course, reducing the risk to as low as reasonably achievable (the ALARA principle) does not eliminate completely the possibility of a minor or major accident occurring.
Consequently, the authorities are expected to have devised comprehensive emergency procedures which can be implemented to deal with   an accident, however remote a possibility.
Knowledge of the pollutants involved, their potential effects and the extent of the area affected by the pollution becomes essential, as does having trained personnel and equipment available to deal with evacuation, medical treatment, etc.
Unlike an industrial accident involving a single emission source, the pollutants contributing to an urban smog are emitted from   numerous diverse sources and may be of many types.
Short-term or emergency policies to try to reduce the pollution levels during a smog and to cope with the adverse health effects and economic disruption of a smog are complex.
An operational System should be able to forecast, detect and react to dangerous pollution levels within a short period.
The requirements of an effective urban smog alert or alarm system include the following:
1
The capability of forecasting occasions of adverse meteorological conditions and predicting the associated rise in pollution levels using emission-inventory/meteorology models.
2
The availability of up-to-date pollution information from a network of on-line monitors throughout the urban area (figure 7.8).
3
The designation of smog alert levels of pollution above which the health of sensitive members (e.g. asthmatics) and/or all members of the community are believed to be threatened.
Most countries opt for a three-stage alert scheme, although Japan adopts two levels of smog emergencies (table 7.4).
The smog alert levels may be made more stringent if new research finds that health is affected by lower pollution levels than previously believed.
For example, West Germany reduced its sulphur dioxide alert levels (introduced in 1974) by 25 per cent in 1984 (these levels were adopted by East Germany in 1989 in advance of its political reunification with West Germany).
4
The means of warning the public via the police, radio, television, newspapers and even electric signs along roads or in public places when pollution concentrations reach health alert levels.
Warnings should be accompanied by advice of what actions to take.
A pre-smog public awareness campaign is important to ensure people have been forewarned of what actions to take if a smog alert is issued (figure 7.9).
A programme of public preparedness should also help to minimize anxiety and panic amongst the community.
5
The means of notifying and placing ambulance, police, medical and hospital personnel and facilities on stand-by should they be needed.
Prearranged procedures may be    needed to postpone routine operations at hospitals in case the facilities are needed by smog victims.
6
The availability of emission control options which can be implemented speedily to reduce pollution emissions from traffic, industry and homes.
Deciding which measures to implement needs to be based on detailed knowledge of the emissions which power stations, traffic, homes and industry contribute to each pollutant.
The relative contributions may be expected to vary from one city to another.
The options should range from voluntary to legally enforced actions depending upon the severity of the smog (table 7.5).
Some restrictive measures require very careful consideration so that they do not cause increased health risks to the public.
For example, limitations on domestic heating during winter smogs could lead to the elderly suffering from hypothermia, and prohibitions on the use of private cars could increase exposure to outdoor pollution whilst walking to work or waiting for public transport.
Implementation of all measures requires co-ordination between central and local government departments to ensure a coherent and consistent pattern of response, and this may require the creation of a new organization to take charge overall during smog emergencies.
In theory, one action the authorities could consider adopting during a smog is to modify the adverse meteorological conditions or try to remove the pollutants from the air.
In practice, few attempts have been made to do so.
In June 1988, in Santiago, Chile, aircraft were used to spray the smog with water and detergent to encourage removal of the suspended particulates.
For Los Angeles, Heicklin (1981) has suggested neutralizing the photochemical smog using diethylhydroxylamine.
Obviously, any attempt to introduce additional     chemicals into the atmosphere needs very careful consideration of whether the chemicals themselves pose a threat to people, plants and animals.
Winter smogs are essentially polluted fogs, so there have been a number of occasions when they have been seeded with dry ice or silver iodide at certain airports so as to allow aircraft to land safely.
Similarly, the French employ a system using an array of jet engines alongside the airport runway to evaporate dense fog.
Few authorities consider that short-term emission reduction options can prevent the first stage of a smog occurring.
Eliminating the occurrence of first-stage smog alerts requires long-term reductions in the baseline pollution levels.
Actions taken during a smog alert to reduce emissions aim to prevent pollution levels from rising further during the duration of stagnant atmospheric conditions.
During a five-day smog in the Ruhr area of West Germany in January 1985 the authorities believed they had achieved overall emission reductions of about 30 per cent, including 14 per cent of sulphur dioxide from power stations and 40–50 per cent of oxides of nitrogen from traffic (Lubkert, 1989).
During a four-day smog in West Berlin in February 1987 the authorities claimed that traffic bans and cuts in industrial emissions of up to 60 per cent kept 500 tonnes of carbon monoxide, 40 tonnes of oxides of nitrogen, S tonnes of soot and 0.6 tonnes of sulphur dioxide out of Berlin's air (figure 7.10).
Another two tonnes of soot could have been avoided if diesel engines had not been classed as low-polluting vehicles and been exempt from the traffic ban.
During a smog alert it is essential that the emission control options are promptly applied and that the correct emission sources are targeted.
For example, although industry is often considered as a major contributor to smog, this may not always be the case.
Smogs usually occur when a strong ground- or low-level inversion is present.
If this inversion is only 100–300 metres deep, tall industrial stacks may release their emissions above the inversion, or the emissions may penetrate the inversion because they have great buoyancy due to high temperatures and fast exit velocities.
Consequently, industrial stacks above about 150 metres or even 100   metres may contribute little pollution to smog locally.
A second example of the need to target the correct emission sources relates to traffic.
If sulphur dioxide is giving rise to the principal health threat during a smog, then banning the use of petrol-powered vehicles (which principally emit hydrocarbons, oxides of nitrogen and carbon monoxide as well as contributing to the formation of ozone) will make little or no difference to sulphur dioxide levels compared with banning diesel-engine vehicles which do emit some sulphur dioxide.
Finally, a third example concerns the contribution of imported pollution to an urban smog.
During mid-January 1985 many European cities suffered smogs, but in Brussels, Belgium, on 17–18 January over half the sulphur dioxide content of the air had been imported from outside the city in the form of a smog bank or cloud which originated from Eastern bloc countries and West Germany (figure 7.11).
This meant that actions to reduce emissions of sulphur dioxide within Brussels had only limited success in reducing smog levels and the city simply had to wait until the extensive smog bank drifted away or began to be dispersed.
Similarly, it takes several hours before ozone is formed from photochemical actions involving oxides of nitrogen and volatile organic compounds (hydrocarbons).
In some cases, the majority of these precursor pollutants come from distant urban-industrial areas and consequently local emission reduction measures have only limited success in reducing smog levels.
In conclusion, a smog alert system should be developed and considered very carefully before being introduced, to ensure that   advice to the public is appropriate and does not cause unnecessary anxiety, that any measures proposed to reduce emissions temporarily are directed towards the most relevant sources, and that it does not weaken efforts towards longer-term reductions in air pollution.
Of course, public recognition of the need to introduce a smog alert system may increase awareness that air pollution problems are very serious and that stricter long-term pollution control policies are needed to prevent smogs occurring in the first instance.
7.8 Wartime Pollution: Kuwait's Burning Oil Wells
The deliberate firing of 600 Kuwaiti oil wells by retreating Iraqi forces in 1991 created a major atmospheric pollution problem, and indicated the need to develop a strategy to try to lessen the likelihood of wilful acts of environmental damage being carried out during wartime.
The first wells were set alight in February 1991 and their smoke plumes were clearly visible on satellite images of the region.
By the beginning of March, as many as 640 oil wells were alight and individual smoke plumes had combined to create a regional plume 1000–2000 km in length, extending frequently to the south-east and periodically to the north-east.
It was not until early November 1991 that the last of the oil well fires was extinguished.
Nine months of toxic smoke emissions billowing forth from the burning oil wells created both short-term and long-term severe pollution problems, not only for Kuwait and its neighbours but for distant countries too.
The size of the particles created by the fires and the heights to which they rose were key factors that determined the atmospheric residence times of the pollutants, the extent of spread of the plumes, and hence the degree of impact on weather and climate.
Observations suggest that most of the smoke particles were in the form of  soot (elemental carbon) particles with diameters of around 0.1 um.
They remained in the atmosphere for 5–20 days before settling out or being washed out by precipitation (soot particles attract water and so should readily form droplets).
Variable and strong low-level winds held most of the individual smoke plumes below an altitude of 3–4 km within a few hundred kilometres of their oilfield sources.
Thin veils of smoke, detected by satellite as far away as southeastern Pakistan, 2000 km east of Kuwait, were found to have risen to heights of between 6 and 7 km.
Soot particles absorb sunlight very effectively but allow long-wave thermal radiation emitted by the earth to pass through unaffected.
Blocking sunlight but allowing long-wave radiation to escape reduces daytime heating of the surface but allows the surface to cool at night as usual.
On frequent occasions in 1991, not only did the smoke plumes markedly reduce daytime visibility in Kuwait City but they caused daytime  temperatures to fall by 4–10 °C below normal (Bakan et al., 1991; Browning et al., 1991).
On some occasions, this marked cooling effect extended a distance of several hundred kilometres from the burning oil wells.
In 1991 Bahrain, 450 km to the south of Kuwait City, experienced its coldest May for 35 years, with an average temperature 4 °C below normal.
Temperature reductions of 1–2°C were observed as far away as 2000 km from Kuwait.
The regional smoke plume often stretched 1200 km, covering large portions of Iraq, Saudi Arabia, the Gulf states and Iran (Iran experienced the greatest soot deposits outside Kuwait).
Soot was washed out of the atmosphere, forming black rain or snow, in Pakistan and in the northern Indian state of Jammu and Kashmir, 2600 km away.
Jordan, Turkey, Afghanistan and Ethiopia may have been affected also.
Increased soot concentrations were even recorded in North America, Japan and at the Mauna Loa Observatory in Hawaii, with the soot in each case being traced back to Kuwait.
Within tens of kilometres of the burning oil wells, large droplets of unburned oil fell to the ground, painting the desert black in places.
This ‘black rain' left a sticky oily coating on people, livestock, crops, water supplies and buildings.
Further away, breathing the fine particulates deposited from the smoke plumes caused severe acute and chronic health problems for many people because the fine particles penetrated deep into the lungs, leading to bronchitis and breathing problems, particularly amongst the elderly and young children.
Headaches and nausea were common complaints of people breathing the cocktail of acidic and photochemical pollutants which enveloped Kuwait City frequently.
A threat to health also arose because the fires burned with only 70 to 90 per cent efficiency, which meant that polycyclic aromatic hydrocarbons (PAHs) were created as tiny particles in the smoke; these substances include dozens of known and suspected human carcinogens.
In the longer term, heavy-metal contamination of soils and groundwater  may prove serious.
Contamination of water supplies by oily mists threatens agriculture in the region since it is almost entirely based on irrigation.
In some areas, deposition of sulphur-laden smoke and soot may make the soil too acidic for crops to grow for up to two or three years.
Oil droplets and soot particles falling on to the Gulf waters, already suffering from oil spills, threaten the phytoplankton which supply food for the fish.
During the nine months of burning oil wells, acid deposition posed a significant threat not only to Kuwait and its neighbours but also as far away as 2000–3000 km.
Around four million barrels (660,000 tonnes) of oil were being burned each day in March 1991, nearly three times the pre-war Kuwait output and almost 10 per cent of the world's daily ration.
Kuwait oil contains about 2 to 3 per cent of sulphur and 0.6 per cent of nitrogen (0.2 per cent in the fuel plus 0.4 per cent formed in the hot flames as nitric oxide).
The fires produced 20,000 to 40,000 tonnes of sulphur dioxide as well as many thousands of tonnes of oxides of nitrogen.
Episodes of acid rain (in the range 3.0–3.6 pH) may have caused major environmental damage to regions such as the Himalayan mountains and the Indian state of Jammu and Kashmir (where there were several reports of thick oily or sooty snowfalls at altitudes above 3600 m).
Soviet scientists reported unprecedented levels of acid rain falling in south-east Russia in April 1991 which they attributed to pollution from the burning Kuwaiti oil wells.
Soot-contaminated snow falling in mountainous regions may have darkened snowfields causing the spring snowmelt to be earlier and quicker than usual, leading to river flooding and ecologically damaging acidic meltwater pulses.
There were early fears that the sooty plumes would disrupt the Asian monsoon.
The monsoon winds that sweep north across India during the summer bring vital rains to sustain agriculture.
The monsoon occurs because the air temperature over the Asian land mass heats up faster in summer than the air over the surrounding ocean.
The warm air rises, pulling in cool moist air from the Indian Ocean.
A pall of smoke from the burning oil wells passing over the Indian subcontinent may have cooled the land temperature by 1 °C, reducing the land-ocean temperature gradient and so weakening the monsoon, delaying its onset, reducing its inland penetration and decreasing rainfall amounts (Rao, 1991).
Alternatively, the heating of the smoke plume due to its absorption of solar energy may have been more important than the reduced surface heating such that a net warming of the lower atmosphere produced a slight enhancement of the Asian summer monsoon overall (Browning et al., 1991) or over specific areas such as the Tibetan plateau (Bakan et al., 1991).
Generally, it is agreed that whatever the magnitude of the effects of the soot plumes on the monsoon, it will have been within the range of natural interannual variability and very difficult, if not impossible, to substantiate.
In November 1990, King Hussein of Jordan expressed concern that the burning of Kuwait's oil (an action the Iraqi leader, Saddam Hussein, threatened to take as early as September 1990 if the United States and its allies attempted to oust him) would accelerate global warming significantly.
However, computer simulations using state-of-the-art general atmospheric circulation models undertaken in Britain, Canada, Germany and the United States all suggest the fires will have added only 2–5 per cent to world emissions of carbon dioxide in 1991 and that this increase will not have had a significant influence on world climate change.
In considering the possible effects of soot on global climate, the models suggest that since the soot did not reach the stratosphere (around 14–15 km in this region), where it could have remained for several years, the fires were unlikely to have caused significant global climate effects.
The pollution problems created in 1991 by the burning oil wells in Kuwait indicate the need for the United Nations to make deliberate large-scale devastation of the environment a ‘war crime’ for which victims could subsequently seek compensation and for which the perpetrators could be punished in the future.
In the case of Kuwait, invaded by Iraq in August 1990, this would refer not only to the Iraqis setting alight the oil wells but to the deliberate pumping of six million barrels of oil from refineries into the Persian Gulf waters.
Whether such a United Nations declaration would have deterred Iraq from environmental vandalism is uncertain but, given the enormous scale of the pollution problems which resulted, such a deterrent may offer some hope for wartime situations in the future.
The Kuwait disaster raises the question of how to deal with serious environmental damage during wartime when hostilities hinder or prevent any attempt to tackle environmental disasters.
The creation of a Green Cross/Crescent organization, analogous to the Red Cross/Crescent (or even the United Nations ‘blue helmets’), may offer a solution to this problem.
Such a politically neutral environmental agency might be able to convince both sides in a military conflict that some remedial action is needed urgently, especially if it can be shown that both factions will suffer adverse environmental effects in the short term and/or in the long term.
Pollution Control in the United States of America
8.1 The Early Years: Pollution as a Local Nuisance
Prior to the 1940s, although communities experienced severe air pollution, control measures were largely limited to the passing of local legislation or to private litigation claiming air pollution to be a common-law nuisance.
Concern for air pollution focused exclusively upon visible emissions, especially smoke.
The first municipal legislation prohibiting emission of ‘dense’ smoke was enacted in 1881 by Chicago and Cincinnati (Stern, 1982).
By 1912, of 28 cities in the United States with a population of over 200,000, 23 had programmes for the abatement of ‘dense’, ‘black’ or ‘grey’smoke.
With the increasing growth and concentration of activities that accompanied urbanization, pollution nuisances occurred more frequently and affected larger numbers of people, yet pollution was essentially perceived to be a problem of dense smoke emission to be dealt with at the local level (figure 8.1).
It was not until the California law of 1947 that a state law tackled air pollution other than dense smoke, and not until 1952 that Oregon introduced the first comprehensive state air pollution control legislation.
8.2 California ‘Discovers’ Air Pollution
From 1943 onwards, Los Angeles began to experience increasingly frequent episodes of a brownish, hazy, irritating and altogether mysterious new kind of air pollution that was more persistent than, and quite different from, the instances of smoky fog which had troubled major urban centres from at least the mid 1800s onwards (Krier and Ursin, 1977).
The reduced visibility accompanying these episodes prompted the common use of the term ‘smog’(figure 8.2).
Given that the term was originally coined from the words ‘smoke’ and ‘fog’, it is really a misnomer for photochemical air pollution episodes.
During the Los Angeles smogs many thousands of people experienced eye and nose irritation, and action was demanded to curb these mysterious episodes.
Optimistically, the mayor announced at an August 1943 conference ‘that there would be an entire elimination of smog within four months’(Krier and Ursin, 1977).
A plant   manufacturing synthetic rubber and producing heavy visible emissions was closed down, seemingly as a scapegoat, a quick succession of ordinances aimed at sulphur dioxide and smoke emissions was passed in southern California, and the California Air Pollution Control Act of 1947 was enacted.
The 1947 Act permitted, not required, air pollution control districts to be set up and it introduced a programme to control pollution but at the same time keep industry in business as usual.
However, effective action would be limited while the cause of the smog remained unknown.
The smogs continued undiminished as no control measures were directed towards motor vehicles, whose role in producing smog was believed (as based on the experience of air pollution research undertaken in eastern cities) to be relatively minor compared with industrial and municipal sources of pollution.
The first signs of guilt being attached to the motor vehicle as the cause of the smog occurred during November 1949 when the Universities of California and Washington State football teams played a game at Berkeley, north California.
Many thousands of those attending experienced intense eye irritation, the hallmark of Los Angeles smog, but only at Berkeley.
The weather was similar to that associated with severe smog episodes in Los Angeles, yet no unusual industrial operations had occurred in Berkeley.
The only unusual occurrence was the vast numbers of vehicles converging on the football stadium producing an engine-idling, stopping and starting, traffic jam.
It could only be concluded that the cause of the intense eye irritation was in some way related to vehicle exhaust emissions.
Following this incident, and from research studies, Professor A. J. Haagen-Smit suggested that the Los Angeles smog arose because certain pollutants, principally from motor vehicles, oil refineries and backyard incinerators, were converted to eye- and nose-irritating pollutants by photochemical reactions.
The important precursors were volatile hydrocarbons (volatile organic compounds or VOCs) and oxides of nitrogen which, in the presence of sunlight, produced the irritating oxidant pollutants of ozone and PAN, among others.
These findings were greeted with scepticism by the motor vehicle industry, the oil industry, and the vehicle-loving public.
Nevertheless, by the mid 1950s, independent research confirmed Haagen-Smit's findings and ranked vehicle exhaust emission as the principal contributor to Los Angeles smog, even though the motor vehicle industry issued a series of findings between 1954 and 1959 which added confusion to the debate and caused a delay in the introduction of vehicle control measures.
Following Haagen-Smit's findings in 1950, the Los Angeles Air Pollution Control District was faced with a dilemma in its attempts to introduce an immediate and effective control programme.
It could not require installation of control devices on vehicles sold in the county until a satisfactory device was available — and no such device was available.
This meant that the smog problem could only  be solved in the short term if the Los Angeles County restricted motor vehicle use.
Consideration was given to measures such as encouraging car pools, creating a rapid transit system, prohibiting vehicles at certain times or in certain areas, and imposing a smog tax on vehicles.
Reluctance to adopt any of these measures with any strong conviction highlighted the widespread belief in the Los Angeles community that the problem was a scientific, technological or engineering problem rather than a social, political or legal one.
If solutions were to be found they would be of a technological nature.
Many such ‘solutions’ were put forward during the 1950s.
Suggestions included removing the subsidence temperature inversion which trapped the smog, using ground-based fans, helicopters or thermal means; eliminating the sunlight and thus the smog by producing a gigantic parasol of white smoke laid by aircraft high over the city; removing the smog through tunnels in the mountains around the Los Angeles basin using huge fans; and seeding the air with some sort of agent that would ‘neutralize’ the smog.
Although some of these technological fixes indicate a degree of desperation and hopelessness associated with the smog problem, the magnitude of the problem still existing in the Los Angeles basin in the 1980s prompted Heicklin (1981) to suggest the technological fix of introducing diethylhydroxylamine as a ‘relatively inexpensive and safe’ chemical inhibitor of smog.
However, the idea of adding an extra contaminant to the air in order to reduce smog has little appeal.
With little effective pollution control being undertaken in California, it was not surprising that air pollution worsened during the 1950s.
Beginning in the 1920s, California had experienced a phenomenal growth in population and industry and an associated growth in the numbers of motor vehicles, and this trend continued into the 1950s.
The number of motor vehicles registered in California increased from 3 million in 1945, to 5 million in 1950, and then to 7 million by 1956.
By the late 1950s, air pollution in California had changed from being a Los Angeles problem to one that was experienced by most urban and some rural areas throughout the state.
In 1960, California enacted the Motor Vehicle Pollution Control Act which required all new vehicles to have installed certified pollution control technology; this was the first positive step towards reducing vehicle emissions which caused the smog.
This Act was enacted five years before the federal Act of that name and highlights the tendency for California to lead the way in pollution control legislation, with the federal government following in its footsteps.
The experience of California both in pollution control policies and as a testing or proving ground strongly influenced the direction of federal policies during the 1960s.
The relevance of California's policies became even greater when, in the early 1960s, photochemical pollution began to be experienced in cities such as New York and Philadelphia.
8.3 The Federal Government Becomes Involved
Although the eye-irritating smogs of Los Angeles in the 1940s received nationwide media coverage, it was not until a 1948 six-day smog in Donora, Pennsylvania, causing 6000 cases of illness and 20 deaths, that air pollution succeeded in gaining federal attention (Schrenk et al., 1949).
The Donora episode, together with smogs in London in 1952 (causing 4000 deaths) and New York City in 1953 (resulting in 200 deaths; Greenburg et al., 1962), stimulated plans for a national air pollution conference and the introduction of a few bills in Congress (none of them passed).
The bills had merely called for further federal studies: control was considered a state and local concern.
The Air Pollution Act which was finally enacted in 1955 reflected the same view: federal legislation left regulatory control efforts to the states and local government.
It authorized $5 million annually for five years (later extended to 1964) to support research, provide technical assistance to public and private organizations, and provide for the training of technical personnel.
Although this first national legislation placed the federal role as a passive supporter of state and local government policies, it marked the beginning of federal involvement which was to be transformed into an interventionist and policy-initiating role.
By 1960, recognition that the pollution problem associated with motor vehicles had reached nationwide significance was highlighted by the enactment of the Motor Vehicle Exhaust Study Act (the Schrenk Act).
This 1960 Act required the Surgeon General to undertake studies of the health effects of motor vehicle exhaust emissions.
Air pollution problems gained further national attention in December 1962 with the Second National Conference on Air Pollution.
This meeting coincided with a London smog which caused 700 deaths and so was cited as evidence of the urgency with which air pollution problems should be tackled.
The Clean Air Act of 1963, the namesake for the major federal incursions to follow in 1970, expanded the research and technical assistance programme initiated in 1955, provided limited federal powers to investigate and abate pollution emissions endangering health and welfare, instructed the US Department of Health, Education and Welfare to develop criteria on the effects of air pollution and its control (advisory guidelines to be used or ignored by state and local government), and increased its research and development of devices and fuels to aid in the prevention of industrial and vehicle pollutant discharges.
Federal attention to motor vehicles increased thereafter, with the motor vehicle being repeatedly charged with half of the blame for the nation's air pollution problems.
The Motor Vehicle Pollution Control Act of 1965 brought the federal government into the business of controlling motor vehicle emissions, with national emission standards for new vehicles being  set, starting with the 1968 model year.
No state except California was allowed to adopt new vehicle standards more stringent than the federal ones: California's pollution problems were recognized as exceptional.
The emission standards set for new vehicles gave due consideration to the technological feasibility and economic cost of compliance by the vehicle manufacturers and the standards which emerged were similar to those already adopted by California.
Pressure for stronger federal control of air pollution escalated throughout the 1960s.
Pollution episodes continued unabated with, for example, a four-day episode in New York City in November 1966 causing 80 deaths and resulting in a state of emergency being declared by Governor Rockefeller (figure 8.3).
By 1966 little more than half of the country's urban population enjoyed local air pollution control; of almost 600 counties with a population greater than 50,000, fewer than 90 had control programmes, and most of these programmes were far from adequate (Krier and Ursin, 1977).
As the Clean Air Act had left much to state initiative, congressional dissatisfaction with the rate of progress in some states led to the Air Quality Act of 1967 which at last required states to establish air quality standards consistent with federal criteria, and then to devise implementation plans setting out ways of achieving the air quality standards.
If states failed to set or enforce standards, the Department of Health, Education and Welfare could intervene.
However, although federal government could now intervene, albeit only after pursuing lengthy procedural steps, the Act still left the primary responsibility for air pollution control to the states and local government.
Some states were not slow in following up the 1967 Act.
California, which was exempt from the federal ruling that no state could adopt emission standards for new vehicles more stringent than the federal ones, passed the Pure Air Act of 1968, which   contained explicit emission standards for hydrocarbons, carbon monoxide and oxides of nitrogen for post-1970 new vehicles.
This state Act recognized the flaws of its earlier attempts at photochemical pollution control which had not included an emission standard for oxides of nitrogen.
Only exhaust emission standards for hydrocarbons and carbon monoxide had earlier been set and this ignored the fact that if hydrocarbons and carbon monoxide were controlled, emissions of oxides of nitrogen increased.
Although emissions of carbon monoxide, hydrocarbons and oxides of nitrogen can be reduced to a certain extent by modifications to the engine combustion, meeting the emission standards requires the fitting of catalytic converters.
The simplest type, called an oxidation catalyst, is a canister fitted with a porous ceramic element which is coated with a thin layer of platinum, palladium or other noble metal of the platinum group.
The exhaust gas is mixed with a little air from a small pump at a point just before it enters the canister.
The platinum catalyses the reaction of carbon monoxide and hydrocarbons with air to give carbon dioxide and water vapour.
This type of catalyst does not affect oxides of nitrogen.
To reduce oxides of nitrogen, a three-way catalyst is required (figure  8.4).
Its construction is similar to the oxidation catalyst except that it uses a different mixture of platinum metals, has no air pump, and requires a special mixture strength.
The engine has to be electronically controlled to give a fixed level of oxygen in its exhaust.
This is detected with a special sensor in the exhaust manifold.
With the exhaust gas thus controlled, the catalyst causes a reaction between oxygen and oxides of nitrogen on the one hand and carbon monoxide and hydrocarbons on the other.
Oxides of nitrogen are reduced to harmless nitrogen, and the carbon monoxide and hydrocarbons are oxidized to carbon dioxide and water, as before.
By the end of the decade, despite progress by some states, advancement towards an effective nationwide pollution control programme was slow.
Ralph Nader's Study Group on Air Pollution estimated that the implementation phase would not be completed until well into the 1980s and the Air Quality Act was criticized for providing the Department of Health, Education and Welfare with only very limited powers to press the states into faster action.
With growing public concern over air quality in cities and with prospects for only a slow improvement in the coming decade, the federal government was put under increasing public pressure to introduce a more effective pollution control policy.
Such a policy would be implemented only if the opposition of the very well-organized and wealthy political lobbies of industry could be overcome.
In contrast with earlier times, the environmental movement, peaking in 1969 and 1970, provided the political muscle necessary to counter the lobbying by industrialists.
8.4 The Environmental Movement of the Late 1960s
Prior to the mid 1960s, pollution issues received only limited and sporadic attention from a public whose interest was largely restricted to rather isolated localized pollution issues and the occasional air pollution episode.
However, during the late 1960s a dramatic and unprecedented increase in public concern for the environment occurred.
Public opinion polls revealed the problem of pollution to have risen from ninth of the ten most serious problems facing the nation in 1965, to second in 1970 (Jones, 1973).
Environmental concern sprang suddenly from nowhere to reach almost crisis proportions, not because of a sudden deterioration in the environment but because popular awareness escalated (Bowman, 1975).
Sellers and Jones (1973) point out that, ‘the deterioration of the environment was discovered by the news media sometime in 1969' (figure 1.2).
With the media thriving on doomsday and eco-crisis stories with the recurrent theme of ‘immediate danger, immediate action’(Chase, 1973), public interest, and demands for prompt action, inevitably resulted: the ‘environmental’ or ‘ecology’movement was born.
Whether it was the media, leaders of interest groups, or trigger events (such as the sequence of New York City smogs during the 1960s, publication of numerous books which continued the theme first introduced in Rachel Carson's Silent Spring, or Ralph Nader's environmental reports) which generated the environmental movement, the media sustained public interest for environmental issues (figure 8.5).
Such heightened public concern for the environment in 1969 ensured the emergence of the environment as a political issue.
Environmental pressure groups, aided by the grassroots backing of the general public and the critical monitoring of policy development by the media, were able to exert considerable pressure on government policy.
Their impact on environmental policy development at the height of the environmental movement in 1969–70 was dramatic and resulted in the enactment of the innovative National Environmental Policy Act of 1969 and the sweeping Clean Air Amendments of 1970.
Although the latter legislation is correctly termed the Clean Air Amendments, it is commonly referred to as the Clean Air Act, and this practice is followed here.
Enactment of strong legislation is only part of the process towards achieving an environmental clean-up; it also has to be applied effectively in following years.
The environmental movement was too short-lived to ensure the rigid implementation of the 1970 Clean Air Act and this is one reason why the Clean Air Act was subsequently relaxed and weakened during the 1970s.
By 1974, the ‘national’ problem of pollution was overtaken by the problems of ‘integrity in government’(triggered by the Watergate scandal), ‘energy shortage’, ‘crime’, and ‘taxes and spending’(Sandbach, 1980).
Even by 1972, energy and economic issues displaced environmental issues as more newsworthy, being deemed by the media to be of greater and more immediate public interest (Parlour, 1980).
The media no longer played the role of sustaining public concern for environmental issues.
As Gregory (1972) points out, ‘the media thrive on fashion, and do much to create it.
Fashions are ephemeral; new ones regularly drive out the old.’
Moreover, public demands for federal action were fulfilled with the   enactment of major environmental legislation.
Pollution problems may not have been solved yet but many of the public felt satisfied that they would be, now that legislation existed.
The sudden rise and sharp decline in public concern for pollution seems to follow what happens to most major social problems as exemplified by the ‘issue attention cycle’(Downs, 1972; O'Riordan, 1976) or the ‘worry bead hypothesis’(Kates, 1977).
Nevertheless, although the state of the environment was no longer viewed as the crisis issue that it was in 1969–70, public support for environmental protection remained considerably stronger throughout the 1970s than it was prior to the emergence of the environmental movement (Anthony, 1982).
8.5 Pollution as a Political Issue
The development of federal air pollution policy prior to the 1970s was characterized by incrementalism.
Following some reluctance to become involved in air pollution control, the federal government opted for a research role in 1955 and only slowly did it begin to intervene with the control policies adopted by the states.
At first it left the initiative to state and local government but then it progressively took away more and more of it.
Each step was a careful reaction to circumstances, whether these were pollution episodes or new research findings as to the causes of air pollution problems.
Policy development was incremental in as much as it adopted the smallest, least disruptive step with the least apparent cost (Lundqvist, 1980).
Policy, it was believed, had to be based on sound scientific evidence; otherwise, it would not be ‘fair’ to industry.
All this changed in 1969 and 1970 with the emergence of the environmental movement which made the environment a political or ‘vote-catching’ issue, and which produced massive public pressure for fast, dramatic action, the nature of which is usually restricted to crisis situations (Gunningham, 1974; Jones, 1973).
Environmental policy subsequently underwent dramatic shifts in both stride and direction.
The upsurge in public concern over environmental quality issues during the latter half of 1969 led to the enactment of the National Environmental Policy Act, which was passed with limited debate or opposition despite it being so fundamental and innovative a measure.
The Act attempted to ‘create and maintain conditions under which man and nature can exist in productive.harmony, and fulfil the social, economic and other requirements of present and future generations of Americans’(Goodenough, 1983).
It required all federal agencies to take into account the environmental impact of all their proposed actions by preparing environmental impact statements which had to be approved before the construction of ‘major’ projects.
Although this requirement was directly applicable  only to federal agencies or projects requiring federal licences, in practice it has been extended to all major schemes of industrial expansion and new construction.
Similarly indicative of the public interest with the environment was the passing of the Environmental Education Act of 1970 which provided means and funds for the development of new and improved curricula for an understanding of environmental quality problems.
By 1970, environmental protection was being proclaimed by the press, radio and television as ‘the issue of the day’.
The ‘environment’ had become a major political issue, and with the media constantly monitoring the progress of policy development, this resulted in active competition between elected officials to produce and be credited with strong legislation.
President Nixon led the way with a timely ‘message on environmental quality’ for his February 1970 State of the Union Address.
Nixon summed up public opinion: ‘Air is our most vital resource, and its pollution is our most serious environmental problem.
Existing technology is less advanced…but there is a great deal we can do within the limits of existing technology — and more we can do to spur technological advance.’
The resulting radical pollution control programme outlined by Nixon, calling for a 90 per cent reduction in vehicle emissions by 1980, not only led to him being credited (albeit briefly) as policy initiator of an environmental clean-up but also provided him with the chance to deal a blow to one of his most important opponents in the 1972 elections, Edmund Muskie.
Much of Muskie's political fame was earned from his leadership in environmental affairs, which was now being eroded not only by Nixon's proposals but also by a Ralph Nader report on air pollution which strongly criticized Muskie's role in the past and just stopped short of accusing him of selling out to industrial polluters (Lundqvist, 1980).
Under this considerable pressure and criticism Muskie seemed to be left with no political option but to enter the game of policy escalation.
He modified the Nixon bill (subsequently to become the Clean Air Act of 1970) to require 90 per cent reduction in vehicle emissions below 1970 federal standards for hydrocarbons and carbon monoxide by 1975, with a similar reduction in oxides of nitrogen below 1971 emission levels a year later.
He justified the very strict timetable (suspensions were possible for one year only) on the basis of the need to place the protection of public health above considerations of technological and economic feasibility.
The Nixon administration, now on the defensive, asked for the Senate deadlines to be relaxed, but with strong public and media backing, the policy presented by Muskie was enacted.
Claims that the bill placed unreasonable demands on the motor vehicle industry in seemingly disregarding the costs of compliance, at a time when the industry was already facing intense foreign competition, were dismissed.
As Senator Griffin claimed:
I think one of the problems with this legislation right now is that too many of the discussions with regard to the bill are being made on a political basis.
When everybody is for clean air and against pollution [it is indeed]difficult politically to vote for any amendment that would be characterized by the press as weakening the bill.
8.6 The Clean Air Act of 1970
The Clean Air Act of 1970, swept into enactment by the political strength of the environmental movement, marked the beginning of a new — and the present — era of pollution control policy.
A dramatic expansion of federal intervention took place, and in order to cope with the new legislation, a special pollution control agency, the Environmental Protection Agency (EPA), was established in December 1970.
The major points of the Act included six main features:
1
Uniform national ambient air quality standards (NAAQSs) were to be set by the EPA to protect public health and welfare (table 8.1).
Whereas the 1967 Act allowed states to devise their own standards, taking into account federal advice, the 1970 Act  empowered federal government to set uniform standards to apply to all states.
This overcame the concern that some states might compete for industry by setting lax requirements, so becoming ‘polluter havens’, or that large industry in some states might lobby for the establishment of permissive standards.
Each state was required to submit a state implementation plan (SIP) indicating how its control programme would bring about the attainment of the NAAQSs by 1975.
If a state failed to develop or execute a satisfactory plan, the EPA had the authority to intervene directly.
2
Uniform national standards of performance for new industry were to be established.
Uniformity avoided the unfair economic competition which might have resulted if the setting of standards had been left to the states.
Nevertheless, the Act allowed states to set more stringent standards if they wished to do so.
Federal performance standards were applied to new stationary sources or to existing plants undertaking modifications.
These new source performance standards (NSPSs) have been promulgated and revised regularly by the EPA since 1971.
Before a new stationary source could begin operation, state or federal inspectors were required to certify that the pollution controls would function adequately.
3
National emission standards for hazardous air pollutants were to be established by the EPA and would apply to existing as well as to new industrial plants.
Arsenic, asbestos, benzene, beryllium, mercury, radionuclides, and vinyl chloride have all been designated as hazardous according to section 112 of the Act, although numerous pollutants have been listed as potential; y hazardous and await the setting of standards by the EPA.
The EPA has received considerable criticism concerning the length of time it takes to set such emission standards.
4
Uniform restrictions on emissions from new motor vehicles were to be set.
The stringency of these restrictions reflected public opinion that the motor vehicle was the primary villain in air pollution problems, accounting for between 50 and 60 per cent of total US air pollution emissions.
The emission standards were set without regard to the constraints of technological or economic feasibility that had previously influenced policy-making.
Nevertheless, $89.1 million was authorized for a research programme (1970–5) to develop a low-emission alternative to the current internal combustion engine (Wark and Warner, 1981).
5
Citizens were permitted to, take legal action against any person, including the United States government (for example, the EPA), alleged to be in violation of either emission standards or an order by the administrator.
Environmental pressure groups, such as the Sierra Club, have used this right on several occasions to press the EPA to take action: for example, environmental groups argued through the courts that Congress intended the EPA regularly to  issue air quality standards for pollutants that were judged to be widespread and hazardous to public health.
Litigation between 1974 and 1978 concerning the need for an ambient air quality standard for lead resulted in the EPA being ordered to set an NAAQS for lead (table 8.1).
6
A $30 million research programme was initiated to assess the causes and effects of noise pollution on public health and welfare.
The subsequent research findings presented to Congress resulted in the Noise Control Act of 1972 (later expanded into the Quiet Communities Act of 1978) which required the EPA to set standards of tolerable noise levels for all types of new equipment and machinery.
The initial objective of the noise control programme was to reduce environmental noise everywhere to below 75 dB(A), a level at which there is a risk of hearing damage (Cherfas, 1980).
8.7 Problems Develop with the State Implementation Plans
The 1970 Act required the EPA to prescribe NAAQSs which were not to be exceeded in any region more than one day per year, or during more than a limited period within that day (table 8.1).
Primary or public health standards were aimed at protecting the most susceptible part of the population from adverse effects, with an adequate margin of safety being included, while the more stringent secondary standards were aimed at protecting public welfare, such as damage to vegetation, wildlife, materials, and so on.
In 1971 the EPA promulgated NAAQSs for six ‘criteria’ pollutants, and the strict legislative timetable required states to submit implementation plans which would achieve primary standards for each pollutant by 1975, or, if the deadline was extended as the EPA was authorized to do, by 1977.
Once primary standards were attained, the state was expected to attain the secondary standards ‘within a reasonable time’.
In contrast with the technological fixation which had previously dominated the control strategies of many states, the EPA stressed that the state implementation plan (SIP) should consider incorporating transportation controls to reduce the distance travelled by all vehicles, new and old alike.
Suggestions included the promotion of mass transit systems, parking restrictions, staggered working hours, commuter taxes, and even petrol rationing.
Land-use controls should also be examined.
Economic incentives or disincentives such as pollutant emission charges or taxes should receive more attention than they had received previously.
During emergency periods or times of air pollution episode potential, consideration should be given to temporarily closing polluting plants or requiring fuel switching to be adopted by larger industry.
Having devised the SIP, the state had to present monitoring and modelling data indicating that its control programme would bring about the attainment of the primary NAAQSs.
The models employed to develop the control programme vary from the simple ‘proportional’ or ‘rollback’model which assumes, for example, that a region with sulphur dioxide levels twice the NAAQS will attain the standard if total sulphur dioxide emission in the region is halved, to more complex diffusion or dispersion models.
Dispersion models predict ambient concentrations of pollutants from emission inventories of pollution sources, meteorological conditions, and topographical considerations (refer to section 7.2).
Given the strict timetable for submission of the SIP and attainment of NAAQSs, it was not surprising that some states would face enormous difficulties, not least the state of California.
The seemingly impossible 1975 (or 1977 if extended as allowed under the Act) goal for attainment of the photochemical oxidant standard is highlighted by data for Los Angeles.
In 1970 the Los Angeles County exceeded the California standard for oxidant (0.10 ppm) on 241 days of the year.
Under the 1970 legislation, the country was required by 1975 (or 1977) to exceed the more demanding federal standard (0.08 ppm) on no more than one day per year!
Los Angeles Air Pollution Control District believed that the state programme could, with no interference, achieve marked improvement in the level of photochemical smog by 1980, and that by 1990 the atmosphere would meet the ambient air quality standard (Krier and Ursin, 1977).
Others were less optimistic, pointing out that population and industrial growth would offset the improvements being made.
California's SIP was rejected by the EPA because it did not provide for attainment of the photochemical oxidant standard for Los Angeles.
Following legal suits by citizens' organizations, the EPA reluctantly set out to devise an alternative plan.
It increasingly realized that no technical measures could assure compliance by the extended 1977 deadline, and that since an adequate mass transit system could not be developed in time, only petrol rationing would lead to attainment.
Petrol rationing of over 80 per cent during the smog season from May to October was suggested!
Seemingly valid claims by the Los Angeles County that such a proposal was economically and politically unrealistic contributed to undermining the EPA's demands for maintaining strict attainment deadlines.
Progress by states towards developing an acceptable SIP was further hampered by the Arab oil embargo of 1973.
Several strategies were available to states to bring stationary sources into compliance with air quality standards.
Land-use planning may be used to regulate the number and size of polluting sources within any given area; low-sulphur fuels may be employed; continuous control equipment may be installed (for example, flue gas desulphurization systems); or intermittent control systems may be used, such as switching to low-sulphur fuels during unfavourable  meteorological conditions.
Given these choices, many states opted to control sulphur dioxide emissions by regulating the maximum sulphur content of the fuel allowed to be burned.
However, domestic supplies of low-sulphur oil were inadequate to meet the potential demand and the oil embargo of 1973 dramatically highlighted this problem.
The increasing price of low-sulphur oil and the reluctance to rely upon imported oil in the future brought a growing demand by industry and by some politicians for the adjustment of the 1970 Act requirements in the light of economic and energy realities as well as technological practicability (Lundqvist, 1980).
Subsequently, the Energy Supply and Environmental Co-ordination Act of 1974 allowed short-term suspensions of air pollution standards to June 1975, if ‘clean’ fuels were not available.
8.8 Motor Vehicle Emission Standards: a Change from the Desirable to the Feasible
During and following the enactment of the 1970 Act, the motor vehicle industry launched a vigorous campaign to publicize its claim that the attainment of the vehicle emission standards for carbon monoxide and hydrocarbons by 1975, and for oxides of nitrogen by 1976, was impossible.
Whereas Muskie was convinced that industry could comply with this ‘technology-forcing challenge’(had not industry developed the technology to place a man on the moon following the challenge by President Kennedy?)— and the strong anti-pollution public opinion of 1970 added weight to this claim — counterclaims gained increasing attention during the 1970s.
As public concern for the environmental crisis gave way to the energy crisis (the oil embargo of 1973) and escalating unemployment, claims that the stringent vehicle emission requirements were causing economic hardship in the motor vehicle industry and affecting its competition against foreign producers were given a more sympathetic hearing.
In April 1973, the EPA recognized the pressure for relaxation of the deadlines by granting a one-year extension for compliance with emission standards for carbon monoxide and hydrocarbons.
At the height of the energy crisis, the enactment of the Energy Supply and Environmental Co-ordination Act of June 1974 suspended the emission standards until 1977 and 1978 to allow vehicle manufacturers to devote more time to improving fuel economy.
In March 1975, a brief concern (shown to be unfounded one month later) that the use of oxidizing catalysts on vehicles was encouraging the production of sulphates which could pose a considerable threat to public health led the EPA to suspend the carbon monoxide and hydrocarbon emission standard deadlines for one year.
Economic-practicability and technological-feasibility arguments gained increasing public, media and federal attention as the energy crisis  was followed by the economic recession such that the Clean Air Amendments were passed in August 1977.
These amendments postponed the original 1975–6 carbon monoxide and hydrocarbon vehicle emission standards until 1980–1, and established a relaxed standard for oxides of nitrogen (the revision readjusted the balance between air quality and fuel economy by raising the emission limit from 0.4 to 1.0 grams per mile) to take effect in 1981 or even later (table 8.2).
A standard of 0.2 grams per mile of particulates was
8.9 The Clean Air Amendments of 1977
Increasing pressure for the relaxation of the strict deadlines for motor vehicle emission standards and for the attainment of NAAQSs resulted in the Clean Air Amendments of 1977.
Vehicle emission standards for carbon monoxide and hydrocarbons were suspended until 1980–1 and the original oxides of nitrogen standard became merely a long-term research objective.
With most areas of the country in 1977 not having attained the NAAQS for at least one pollutant, extensions to the attainment deadlines were inevitable.
Non-attainment areas were given until July 1979 to have an approved SIP which provided for attainment of the primary NAAQSs by December 1982.
If, despite the implementation of all reasonably available measures, including the introduction of a motor vehicle inspection and maintenance programme, a state could not attain primary standards for carbon monoxide and photochemical oxidants by 1982, it had to submit a second SIP which provided for attainment by December 1987.
Despite being a non-attainment or ‘dirty’ areas, new sources of pollution could be built providing that first, the proposed plant installed pollution control technology which ensured the ‘lowest achievable emission rate’(LAER); and second, that the proposed emissions were offset by reductions in emissions from existing sources in the area (the ‘emission offset’policy).
Although attention was naturally directed towards improving the ‘dirty’ areas, the Sierra Club successfully pressed through the courts that the EPA should ensure that ‘clean’or attainment areas should not suffer further degradation (Stern, 1977).
Congress incorporated this principle into the 1977 Amendments in the form of the Prevention of Significant Deterioration (PSD) programme.
This assures a level of protection for clean areas beyond that provided by the primary and secondary ambient air quality standards (Ward, 1981).
Not only does the programme require that new sources of pollution must use the ‘best available control technology’(BACT) to minimize their impact on air quality, but it also limits the additional pollution which can be added to the air.
The PSD programme specifies three classes of areas:
Class I
Areas where almost no change from current air quality will be allowed.
‘Pristine’ areas include national parks and national wilderness areas and cover approximately 1 per cent of the federal land area.
Class II
Areas where moderate change or moderate economic growth will be allowed, and where no stringent air quality constraints are desirable.
Class III
Areas where substantial industrial growth will be allowed and where the increase in concentrations of pollutants up to federal  standards will be insignificant.
As all areas were initially classified as class I and II, the EPA left it to the states to redesignate some class II areas as class III, or even to upgrade such areas to class I (Stern, 1977).
Additional air pollution in each of these classes must not exceed specific air quality ‘increments’, thereby assuring a level of air quality which should in all cases be cleaner than that required by the NAAQSs.
For sulphur dioxide and suspended particulates, the allowable increments equal approximately 2–5 per cent, 25 per cent , and 50 per cent , respectively, of the NAAQSs (table 8.3).
In   1990, the EPA was devising fine particulate (PM 10 ) equivalents to replace the TSP increments and was introducing PSD increments for oxides of nitrogen.
Both environmentalists and industrialists agree that the PSD programme is complex.
Industry regards the PSD programme as ‘the most burdensome requirement of the Clean Air Act’(Hart, 1982).
To comply with the provisions of the programme, a company must first spend a year monitoring air quality in the proposed area and employ elaborate computer models to demonstrate that pollution from the proposed plant will stay within the allowable increments.
Even so, environmentalists point out that 75 per cent of all PSD permits were issued in ten months or less.
Protection of air quality in pristine areas is also achieved through the use of visibility standards introduced by the EPA in 1984 for 33 states and one territory (the Virgin Islands) in which national parks and wilderness areas are deemed to need visibility protection as mandated by the 1977 Clean Air Amendments.
States are required to revise SIP requirements as needed to make reasonable progress towards the visibility standards, and to develop long-term (10–15 year) strategies for meeting these goals.
The EPA adopted a phased approach with regard to visibility protection.
Initial action has focused on ‘plume blight’.
Plume blight is defined as an identifiable coherent plume which is objectionable in an area in which visibility is an important value, because of its coloration and intrusive nature.
It is usually caused by the emission of particulates or nitrogen dioxide.
It is controlled by requiring the offending pollution source to employ the ‘best available retrofit technology’(BART).
A much more complex and far more serious cause of long-range visibility impairment is that of ‘regional haze’ caused by sulphate and nitrate particles.
This problem is an integral part of the acid rain problem and will only be solved when emissions of sulphur dioxide and oxides of nitrogen are substantially reduced.
8.10 Pollution Control Policy: Job-Taker or Job-Maker?
One of the criticisms frequently made by industry concerning pollution control policy is that it causes severe economic dislocation and unemployment.
A car sticker slogan sums this up simply: ‘Are you poor, hungry, out of work?
Eat an environmentalist.’
Industrialists argue that the expensive pollution control equipment which companies have to purchase, install and maintain leads to plant closures (Stafford, 1977).
Controls required on new stationary sources delay or even prevent the construction of major industrial facilities, such as power stations and oil refineries, thereby aggravating the energy situation and generally restricting the economy's ability to expand.
In general, whereas previously a site of a new plant could have been selected in three to six months, it may now take two or more years.
By that time it may be too late to get into the market place with the product.
Companies faced with massive investment in new pollution control equipment, if a plant is to be located in a new location, may well find it cheaper and simpler to expand pollution at existing facilities, thereby adding to industrial inertia (Elsom, 1983).
Pollution abatement costs for industry can indeed be immense.
Most industries commit in excess of 5 per cent of their total capital expenditure, with some industries committing far more, such as the paper industry (43 per cent), non-ferrous metals (23 per cent ), iron and steel (14 per cent ) and chemicals (12 per cent )(Starkie, 1976).
In 1985, capital expenditure for air pollution control was estimated to have cost industry $10.5 billion, with operation and maintenance costs at about the same level.
In 1989, industry claimed it was spending $34 billion a year to comply with pollution control legislation.
Such costs compare with many tens of billions of dollars a year saved in wages and productivity which would have been lost due to sickness caused by air pollution and for the annual benefits (reduced medical treatment; less damage to buildings, crops and forests) to the nation as a whole of a less polluted society.
Criticisms of the pollution control regulations frequently imply that the high cost of compliance leads to plant closures and curtailments, which in turn translate into unemployment.
However, in reality the number of jobs lost compared with those gained in the major growth industry of pollution control is small.
Between January 1971 and June 1981, the EPA identified 153 closures or curtailments in firms of 25 or more workers, totalling 32,611 workers having lost their jobs (Kazis and Grossman, 1982).
Most of these reported lay-offs were in four industries: chemical, paper, primary metals, and food processing.
However, environmental regulation was only one of the reasons for the closures, for many firms which closed were the older inefficient or obsolete plants (Harris, 1981).
Moreover, as many as 40 per cent of the lay-offs were rehired by their original companies at other plants.
The numbers involved, not ignoring the personal distress associated with unemployment, seems small compared with other causes of lay-offs.
Thus, for example, the Reagan administration's 1982 budget cuts alone led to one million people becoming unemployed in both private and public sectors.
In contrast, it has been estimated that between 0.5 and 1.1 million people are employed in private and public pollution control (Harris, 1981; Kazis and Grossman, 1982).
8.11 EPA Flexibility: the Emissions Trading Policy
The 1970 Act as enacted offered few compromises in its aim to give the nation a healthy atmosphere by the mid 1970s.
Only consider  able pressure during the energy and economic crises and the subsequent desire for energy self-sufficiency and economic recovery achieved any relaxation in the aim: the primary goal of a clean atmosphere remained largely the same but the stringent deadlines for achieving this were sacrificed.
Once this trend for the relaxation of the 1970 legislation began, the EPA considered that the only way in which it could slow down or stop this erosion, short of a resurgence in the environmental movement, would be to show more flexibility in its administration of the Act, especially towards new industry.
Flexibility would take the form described by McAfee (1982) in that ‘clean air regulations should tell business what to do, not how to do it — what goals to reach, not how to reach them.’
The ‘bubble’ strategy introduced in 1979, and  incorporated into the Emissions Trading Policy of 1982, exemplifies the flexible approach by giving plant managers considerable freedom in finding the cheapest, most efficient way of meeting pollution control standards (Behr, 1979; Liroff, 1980).
Normally, the EPA and state control agencies set limits on emissions from each vent, smokestack, or other source of pollution in a plant.
The bubble strategy considers the plant, or a series of plants, to be enclosed in a bubble and the EPA sets limits on the total discharges of each type of regulated pollutant, leaving it up to the plant manager to decide how to reach the goals (figure 7.3).
Trade-offs between sources of pollution within the bubble must involve the same pollutant such that plants are not allowed to offset, say, reduced sulphur dioxide emission against having to place expensive controls on toxic emissions.
By 1983, the EPA had already either approved or proposed to approve 150 bubbles, with a saving to industry of $500 million.
It is even conceivable that in the future the bubble policy may be applied to an entire city.
One of the successes of the bubble policy to date includes a plant in Providence, Rhode Island, where $2.7 million was saved by replacing expensive low-sulphur oil at its two plants, one with high-sulphur oil and one with natural gas, which also resulted in a net reduction of the sulphur dioxide emission (Smith, 1981).
Initially, the EPA barred the trade-off between smokestack or plant emissions and wind-blown dust, arguing that industrial processes emitted smaller particles that stay in the air longer and lodge more deeply in the lungs.
The agency was forced to reverse its decision under legal pressure from the steel industry, largely because the NAAQSs for suspended particulates did not make a distinction between particulate sizes, even though smaller particles are known to be more harmful.
This ruling allowed Armco (one of the biggest steel producers) to use unconventional ways of controlling its particulate levels at its plant in Middletown, Ohio.
Instead of spending $7 million on back-up equipment to catch the particles that its existing scrubbing plant could not cope with, it spent $1.7 million by periodically spraying its outdoor piles of iron  ore and coal, by paving or periodically wetting down the company roads, by planting trees and grass to trap wind-blown particulates, and arranging shuttle buses so that employees reduced the use of their own vehicles around the site.
The Emissions Trading Policy of 1982 also integrates the Emission Offset Interpretation Ruling of 1976 (amended in 1979 to incorporate banking).
The offset policy recognized that air is a scarce resource and used market forces to accommodate growth without increasing total pollution (US National Clean Air Coalition, 1981).
Sponsors of a proposed new plant were required to offset the pollution which would result from their plant by:(1) reducing emissions from its own plant in the area; or (2) paying another company to reduce its emissions in the area; or (3) purchasing an old plant in the area and simply closing it down; or (4) purchasing emission credits, if available.
An example of an offset agreement is that between General Motors and local oil companies in Oklahoma City, which enabled the car company to build a new $400 million assembly plant there.
The oil companies agreed to reduce their hydrocarbon emissions sufficiently to allow the car plant to be built.
Offsets create an unofficial market in pollution or emission reduction credits.
Many firms using the bubble policy have found that the resulting pollution was less than what the law allows, a circumstance that grants them a credit towards added pollution in the future.
As credits proliferated, the EPA decided to establish a banking or brokerage network through local governments for selling the credits from one firm to another, with the result that Louisville, San Francisco and Seattle initiated banking programmes.
The use of market principles succeeds in facilitating new growth in an area under stringent pollution limits and permits compensation — at whatever the market will pay — for voluntary efforts to reduce pollution.
The Emissions Trading Policy marks a new chapter in flexibility in administrating federal regulations.
Further innovations being considered include the issuing of auctionable pollution permits.
Thus, for example, rather than barring production above current levels of, say, asbestos at each plant throughout the nation, a ceiling on national production could be imposed with the rights to manufacture within the total being auctioned off to the highest bidders.
The resultant high prices would force many out of the market and ensure that manufacturing rights went to those firms to whom they were economically most important.
This version of a marketable discharge licence process employs a national airshed, but it could be applied at a state or local level and for other pollutants in the future.
8.12 Progress towards Improving Air Quality: Visionary Aims versus Reality
The Clean Air Act of 1970 set out to create a healthy atmosphere for the population within less than a decade, but even by the 1990s this has not been achieved and it is now expected that some urban areas will not achieve healthy air until well into the next century.
In 1988, over 120 million people were breathing air which exceeded at least one of the health-based air quality standards, that is, nearly half the population of the country.
The worst problems are ozone (the major constituent of photochemical smog), carbon monoxide and fine particulates.
In 1988, around 112 million people were living in areas exceeding the ozone standard, almost 30 million in areas violating the carbon monoxide standard, and over 25 million in areas exceeding the fine particulate standard (figure 8.6; table 8.4).
Suspended particulate matter includes smoke, dirt, dust, and soot emitted by a wide range of sources including factories, power stations and vehicles.
Wind-blown particles from soils as well as particles created in the atmosphere by the transformation of gases such as sulphur dioxide and volatile organic compounds (VOCs) are also included.
Given that suspended particulates have been the subject of pollution control policies dating back to the nineteenth century, it is not surprising that total emissions and average urban concentrations of particulates have decreased markedly during the past few decades.
In 1987, new air quality standards for particulates were introduced in which the total suspended particulate (TSP) standard (which included particles as large as 100 um) was replaced by the PM 10 standard (particles less than 10 um in diameter) since small particles are considered to pose a greater threat to health than large particles.
However, since there are few sites monitoring PM 10 with long-term records, assessments of trends in particulates usually employ TSP measurements.
These reveal that TSP concentrations have halved since the 1960s. with   the downward trend showing some levelling off during the 1980s (figure 8.7).
Based on 1648 sites, average concentrations of TSP decreased by only 1 per cent from 1982 to 1989, reflecting a similar change in total national emissions of particulates (figure 8.8).
Emission reductions since the early 1970s are attributed to the installation of pollution control equipment, technological improvements, reduced activity in some industries such as iron and steel (figure 8.9) and the use of cleaner fuels in residential areas.
Year-to-year variations may be due in part to variations in precipitation (e.g. the drought of 1988 and associated increase in windblown dust), volcanic activity (e.g. Mount St Helens in 1980) and wildfires (e.g. Yellowstone forest fires in 1988).
Lave and Omenn (1981) claim long-term trends conceal that during the 1970s ‘dirty air got cleaner and the clean air got a little dirtier’, for it was not until the 1977 Amendments that the EPA had the authority to prevent the worsening of air quality in pristine areas.
Furthermore, the deterioration of visibility in some regions associated with the increased occurrence of regional hazes (composed of sulphates and nitrates) reflects the trend of rising emissions from power stations located increasingly outside metropolitan areas.
Based on 400 monitoring sites, sulphur dioxide levels decreased 24 per cent during the 1980–9 period as a consequence of increased installation of flue-gas desulphurization equipment at coal-fired power stations, a reduction in the average sulphur content of fuels, industrial recession, and shutdowns of some large smelters (figure 8.8).
Two-thirds of all national sulphur dioxide emissions are generated by coal-fired power stations with the majority of the emissions produced by a limited number of sites.
For example, 50 individual stations in 15 states account for half of all power station emissions.
Year-to-year variations in sulphur dioxide concentrations occur such as the 1 per cent increase between 1987 and 1988    and the 3 per cent decrease between 1988 and 1989.
Nationally, the urban sulphur dioxide problem has diminished to the point where only a small number of urban sites now exceed the air quality standard.
Remaining sulphur dioxide problem locations are found near large industrial sources such as around non-ferrous   smelters located outside urban areas in the intermontane region of the western United States and in Arizona.
Carbon monoxide, a colourless, odourless and poisonous gas, is produced by incomplete combustion of carbon in fuels.
Average carbon monoxide levels (based on the second-highest eight-hour values recorded at around 280 monitoring sites) decreased 25 per cent during the 1980–9 period (figure 8.8).
This trend reflects a 23 per cent decrease in national carbon monoxide emissions.
The number of exceedances of the eight-hour air quality standard decreased by over 80 per cent during this same period but, even so, 41 areas failed to meet the air quality standard in 1988–9.
Transportation sources accounted for 66 per cent of carbon monoxide emissions in 1989 (table 8.5).
Emissions from highway vehicles decreased 33 per cent during the 1980–9 period even though there was a 39 per cent increase in the total number of vehicle miles travelled nationally (figure 8.10).
Nitrogen dioxide, a reddish-brown and highly reactive gas, is mainly produced in the atmosphere from the oxidation of nitric oxide formed when fuel is burned at high temperatures.
Table 8.5 reveals the two primary sources of nitrogen dioxide are fuel combustion from industrial and power station boilers (56 per cent of total national emissions) and transportation (40 per cent).
Using information from over 100 sites, average national levels of nitrogen dioxides decreased by 5 per cent during the 1980–9 period (figure 8.8).
However, this statistic conceals that levels of nitrogen dioxide have changed little since 1983.
Los Angeles, which reported a mean of 0.061 ppm in 1988 and 0.057 ppm in 1989 (table 8.4), is the only urban area that has recorded violations of the annual air quality standard for nitrogen dioxide during the past ten years.
The average trend for the second-highest maximum one-hour ozone concentration recorded at 400 sites shows that ozone levels   increased by 2 per cent over the 1979–88 period but decreased by 14 per cent over the 1980–9 period (figure 8.8).
Given the enormous and costly pollution control efforts to tackle ozone pollution to date, these variable ten-year trends are very disappointing.
However, ozone trends are strongly influenced by annual variations in meteorological conditions, and the very high 1988   level — 8 per cent higher than 1987 and the highest since 1983 — was in part due to the very hot, dry weather (the hottest summer in almost 60 years in north central states).
In contrast, the 1989 ozone level was 15 per cent less than 1988 as frequent rains fell during the summer in the eastern United States (and the 1990 level was 1 per cent lower than 1989).
The dominating influence of meteorological conditions on the ozone trend is highlighted by trends in emissions of VOCs (involved in the formation of ozone) since these emissions show a decrease of nearly 20 per cent during the 1980s.
This decrease was achieved in part by highway vehicles reducing their emissions by one-third during this decade even though the number of vehicle miles travelled increased by over one-third during the same period.
However, it should be mentioned that estimates of annual emissions of VOCs are based on statewide average monthly temperatures and may not reflect fully the possible impact of the hot weather on evaporative emissions from vehicles in 1988.
Nevertheless, and irrespective of the debate on the degree of influence of annual weather variations on long-term ozone trends, the fact that 101 areas in the country failed to meet the 0.12 ppm ozone standard in 1988 is extremely worrying.
The ozone pollution problem may have had a long association with southern California but it has now become a very serious problem in the Texas Gulf coast, the north-east corridor, and other heavily populated regions (figure 8.11).
Had it not been for the EPA relaxation of the ozone standard from 0.08 to 0.12 ppm in February 1979, following strong pressure from the petroleum industry, many more tens of millions of people would be regarded as living in areas with unacceptable air quality.
One atmospheric pollutant which has shown dramatic improvement is airborne lead.
Since 1978, average airborne lead concentrations have fallen by 93 per cent(figure 8.12).
This situation reflects the spectacular 94 per cent reduction in total national lead emissions during the 1978–89 period.
This has been achieved through   the introduction of unleaded petrol in 1975 for use in cars equipped with catalytic converters (lead ‘poisons’ the catalyst) and which accounted for 89 per cent of all petrol sales in 1989.
In addition, there has been a reduction in the lead content of leaded petrol (reduced from 1.0 gram per gallon to 0.5 g/gal in 1985 and then to 0.1 g/gal in 1986).
The leaded petrol market is shrinking so fast that   some major petroleum companies have discontinued refining leaded petrol.
Total emissions of lead have plummeted from 127,900 tonnes in 1978 to 7200 tonnes in 1989 (figure 8.12; table 8.5).
Transportation sources contributed only 31 per cent of annual emissions in 1989 compared with 88 per cent in 1978.
Although average airborne levels of lead have decreased markedly throughout the country, problems remain around certain industrial sources such as the Herculaneum lead smelter in St Louis, Missouri, and the vehicle battery reprocessing plant in Wallkill, New York.
With so much data being reported for so many pollutants and with the news media and control agencies employing diverse terms to describe air quality, some degree of confusion to the public is inevitable.
To overcome this communication problem, a health-related pollutants standards index (PSI) was devised for nationwide adoption (Crossland, 1978).
The PSI is determined by whichever of the criteria pollutants is highest on that day, and the media employ associated terms for various PSI levels such as good (PSI of 0–50), moderate (50–100), unhealthy (100–200), very unhealthy (200–300), and hazardous (exceeding 300).
By 1981, 102 urban areas, representing nearly 100 million people, were reporting a daily PSI through the media (US National Commission on Air Quality, 1981).
Considerable progress has been made in improving air quality in the United States during the past few decades but greater improvement is still needed (figure 8.13).
The rewards are worthwhile both for the health of the population and for the environment.