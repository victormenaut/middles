

INSIDE SCIENCE
GROUNDWATER
STANDING UP TO EARTHQUAKES
The malleable microbe
A prescription for health research
How to save energy and stay rich
CONTENTS
NEXT WEEK
Breathing dangerously in the city
Clean air acts in the West have reduced some forms of pollution.
But urban air is still killing people
FEATURES
Mars: is it mission impossible?
President Bush wants to see an American flag flying on Mars by 2019.
But no one knows what the biological effects would be on the astronauts undertaking such a trip Christopher Joyce 
COVER: Standing up to earthquakes
An earthquake destroys buildings by shaking them to pieces.
Engineers are finding ways to keep them standing when the ground starts to move.
Christopher Joyce 
The malleable microbe
Just how different is a bacterium from an elephant?
The answer lies in the flexible genes of the lowly microorganism John Postgate 
An unlikely civil servant
Michael Peckham is the first person to take on the task of directing the research effort of the National Health Service Phyllida Brown 
The first steps out of the greenhouse
From fridges to cars, we already have the technology we need to curb the greenhouse effect.
What stops us from using it?
Sue Bowler 
Energy answers for North and South
Planners in Sweden and India think their countries can meet their energy needs, cut emissions of greenhouse gases and get richer by the end of the century Debora MacKenzie 
NEWS
This Week
Judge rules on passive smoking /White House snubs green plan /Science cuts…and debates /First step to a climate treaty /Patent error /Drought hits California's framers /AIDS and apartheid in South Africa /In Brief 
Science
Quantum theorists hit boredom threshold /How much does a smell weigh? /Well-fed baboon babies and reproduction /Ice telescope /Supernova ring sizes up Universe /Tunnel through the Galaxy's fog
Technology
The chip that never forgets /Brewer's yeast makes haemoglobin /Smart arm gets up your nose /Geothermal power research cut back /Atomic memory /Bike Bike wheel with three spokes
VIEWPOINTS
Talking Point
Review
Minamata and the search for justice /Green primer /Gender engenders science /God's aims pseudoscience /PuncturingAnt's tale /Mind dances
Life, the Universe and (almost) everything
Forum
Greatness in the galactic subburbs /Cutting the billion down to size /Per Astra ad ardua /Lindow Moss: excavating history /Thistle Diary /Enigma /Fee
Ariadne
Cleaner cups: is polystyrene better than paper?
Juggling genes: unlike higher organisms, bacteria can exchange DNA
Dominant: why is this tiny predator so successful?
This week's cover is by Darrel Rees
LETTERS
After Chernobyl
Brian Wynne's article gave a good account of the farmer's viewpoint of ‘official’ science and the general problems of scientific communication and credibility raised by the aftermath of Chernobyl (‘After Chernobyl: science made too simple?’, 26 January).
However, both the photograph, caption and the subsequent text leave open the issue of whether Sellafield was, or was not, the cause of the elevated radiocaesium levels measured in Cumbrian sheep after the Chernobyl accident.
While I accept that this is still an open question in the minds of many local farmers, I thought the article should have been more explicit about the scientific consensus on this point.
I have corresponded with a number of individuals and organisations including the National Farmers Union and a local MP.
While I have not pretended that pre-Chernobyl data are extensive, there is compelling evidence about soil and grass contamination in the Cumbrian uplands before and after Chernobyl, based on research and monitoring by the Atomic Energy Authority, the Institute of Terrestrial Ecology, and BNFL.
Levels of caesium in upland grasses increased dramatically after Chernobyl, while the ratio of caesium 137 to 134 fell dramatically to a value characteristic of Chernobyl fallout.
After the event, my group at Sellafield managed to identify a sample of pre-Chernobyl Herdwick mutton from Eskdale (which had been preserved in a freezer).
It had a caesium 137 concentration of only 4 becquerels per kilogram.
S. R. Jones Head of Environmental Studies British Nuclear Fuels plc Risley, Cheshire 
As the author of the article, I would like to correct one or two impressions which your abridgement from the original, much longer, article may have given the reader.
In particular, though I noted that the farmers distinguished between different scientific sources, they credited scientists at the local Institute of Terrestrial Ecology at Merlewood with more openness about the scientific uncertainties.
This is doubly important because it indicates a more positive conclusion about the general challenges of public communication of science — there is something to learn from this example.
Secondly, though it is true that most farmers we interviewed reserved special contempt for the MAFF scientists from Whitehall because they understood little about marginal hill farming, and worse did not appear to think they needed to know, local MAFF officials (not scientists) in the Carlisle Divisional Office were a completely different case.
They were highly regarded by the farmers, both for their local knowledge and their strenuous efforts to have this knowledge understood and used by the MAFF scientists in Whitehall.
Again, the positive message which can be drawn from this part of the whole episode ends up buried by default.
Brian Wynne University of Lancaster Lancaster 
Endangered plants
I applaud Leigh Dayton's report on the growing need for more comprehensive approaches to the loss of biodiversity (‘On the sowing of the species’, 14 January).
But I have often felt that the concern expressed over this issue is actually only addressing those problems associated with animal species.
Plants are more frequently absent from such discussions.
Unfortunately, this article only reinforces my previous observations.
I counted 78 independent references to biodiversity in the article, more than half of these not distinguishing between plants and animals.
This clearly shows that the article is addressing both kingdoms.
Of the remaining references, 35 per cent deal distinctly with examples of fauna, leaving a paltry tenth of the references to indicate that the flora of our planet is under threat from extinction as well.
The emphasis on animals as endangered species may take its roots from the historical preferences of naturalists and scientists.
The list compiled by the World Conservation Monitoring Centre, details 311 species that have perished since 1600.
It is beyond chance that all of these species also happen to be animals; plants, perhaps inhabiting the same regions as these animals, have disappeared without even the briefest of epitaphs.
Allocating a disproportionate amount of resource on well-studied, high profile species is certainly easy, but may well prove to be inefficient when documenting the majority of life inhabiting our planet, and the attempts at dampening the current rates of extinction.
D. S. Hammond University of East Anglia Norwich Norfolk 
Evolutionary message
I would like to point out three errors in the review of my book The Scars of Evolution (Review, 19 January).
The book does not ‘ignore the fossil record altogether’, but devotes a chapter to it.
It does not have the ‘hidden purpose’ of supporting the aquatic theory: that is its declared intention.
And I have certainly never represented myself as the ‘creator of a radical idea’.
The radical idea was proposed twice, independently in Britain and Germany by two highly esteemed professional scientists, Alister Hardy and Max Westenhöfer.
I happen to believe they were right.
Elaine Morgan Mountain Ash Mid Glamorgan Wales 
Cold fusion
I enjoyed Frank Close's review of cold fusion but would like to mention a simple explanation of Fleischmann and Pons's results which does not seem to have been discussed (‘Cold fusion: the discovery that never was’, 19 January).
I was in Brookhaven shortly after the announcement; copies of their first report were being passed around and avidly perused, so I am familiar with the data.
The salient points were as follows.
An outer cylinder of platinum was used as the anode, with a rod of palladium on its axis as the cathode.
After an electrolysis current had been passed for about three months (to charge the palladium with hydrogen), a small excess of heat was observed of the order of one watt or less.
This only occurred while the current was flowing.
Three different palladium rods were tried of various diameters: the thickest rod gave by far the greatest effect.
Presumably, the anode remained the same (no change is mentioned), so with the thickest rod the spacing between anode and cathode would have been the smallest.
The explanation is as follows.
The palladium cathode was charged with dissolved hydrogen.
More ‘nascent’ hydrogen was being liberated by the electrolysis.
A few millimetres away ‘nascent’ oxygen was also being liberated.
Undoubtedly, the bubbling oxygen would have come into contact with palladium.
Both platinum and palladium are excellent catalysts, so the reaction most probably observed was H 2 +O=H 2 O, a well-known liberator of heat.
One would expect this to occur most readily when the anode and cathode spacing was small.
Some emphasis was given to the fact that one night, while the electrolysis was running, the apparatus exploded.
Many of us have seen minor accidental explosions, usually attributed to a short circuit.
In this case the presence of oxygen and hydrogen mixed in the optimum proportions, could well have compounded the effect.
Francis J. M. Farley Le Masage, Le Bar-sur-Loup France
Treating wounds
With reference to Eric Jenkins's letter (19 January), I would hope that there should be no place for the use of hypochlorites in the treatment of wounds.
Much evidence now exists which shows that hypochlorites inhibit collagen synthesis and cause irreversible damage to the micro-circulation.
Furthermore, hypochlorites attack the cell walls of bacteria, releasing endotoxins, which are absorbed.
The toxins are necrotoxic to renal tissue (Schwartzmann reaction), and produce a range of side effects, from mild uraemic toxaemia to acute renal failure.
I would suggest that a 1 per cent solution of povidone iodine might be a safer alternative.
A. Johnson Darlington Health Authority Darlington, Durham
Female circumcision
Your article on female circumcision was indescribably moving and horrifying (‘Female circumcision: fighting a cruel tradition’, 2 February).
Will you please publish the address of organisations fighting this barbaric practice, to where we can send contributions and support?
Alison Bowers Edinburgh Scotland
For more information write to: FORWARD, Africa Centre, King Street, London WC2.
Centrifugal thoughts
I have always believed that the critics of relativity are all crackpots.
But now that John Gribbin has made it known (Letters, 26 January) that the term (and presumably the concept)‘centrifugal force’ is favoured by the relativists, I am beginning to have second thoughts.
T. S. Harriss London
Pencil pushers
Like Roy Herbert in his review of The Pencil: A History , I had grave misgivings about the interests of such a lowly item (Review, 26 January).
When my children attempted to drag me into the Cumberland Pencil Museum in Keswick I started to devise a list of silly museums — corn plasters of the world, Peruvian nose flutes, or some such other daft idea.
But what a revelation it turned out to be.
It was thoroughly well laid out, with fact-sheets for the kids — including free pencils — exhibits dating from the earliest times to the present day and the entrance was through a mock graphite mine.
We gave them full marks for originality.
The Phillips Family Brighton Sussex
Llamas' collective
As llamas are such graceful beasts (in all but manners) I was delighted to read that they may soon widely adorn the British landscape (‘Europe's farmers plough a new furrow’, 8 December 1990).
However, I was alarmed to learn of the cost of these gracious creatures — about £10,000 each.
Like-minded readers may therefore find comfort in a recent article in The Veterinary Record (19 January 1991) which describes ‘successful pregnancy following non-surgical embryo transfer in llamas’.
Such technology, coupled to advances such as ‘embryo sexing’ and ‘embryo splitting’which are proving so useful in cattle breeding could bring livestock prices down and encourage the adoption of these majestic mammals.
We will then, of course, need a suitable collective noun for them, as ‘herd’ reflects neither their nature nor intelligence.
Any advances on a ‘conspiracy’ of llamas?
Leighton-Jones Evesham Worcestershire
Safety management
An uninformed reader of ‘Safety: the perils of self-regulation’ might be forgiven for concluding that the powers that be and their executive arm, the Health and Safety Executive, had been less than wholehearted in promoting and enforcing safety (Talking Point, 12 January).
It would be much fairer if the positive achievements of the last 17 years were also recognised.
Government and the HSE are well aware that the promotion and enforcement of safety demands much more than reliance on individuals and management.
Proper management control of safety is important as is the provision of proper safety systems.
Once those systems are in place, there must be periodic audits to ensure that those operating the systems know what they are doing and are prepared to do it.
Accidents will still happen.
The human factor is a main component in 90 per cent of cases, and in 70 per cent proper management effort might have prevented them.
A good deal of money and effort is therefore going into study of the human factor, which should ultimately lead to further reduction of accidents and risk.
The Hazards Forum was set up in 1989 by the engineering institutions to assemble and exchange ideas on the management of risks and hazards.
It is working closely with HSE, and has been impressed by the progress made since 1974 in moving towards a much more thorough understanding of the real reasons for major breaches of safety, and of how to make them less likely.
Officialdom does care, and is having some success in ensuring that a better informed management cares and will shoulder its responsibilities in ways appropriate to the 21st century.
Frederick Warner, Chairman Hazards Forum London
Star cross
How unfortunate that Adrian Furnham's interesting article on the reasons for the popularity of astrology and graphology should close with the exhortation to ‘…beware…the astronomer’(‘Hooked on horoscopes’, 26 January).
I trust that this is nothing more than a proofreading error.
Nonetheless, it does nothing to help clarify the confusion, which so many people still display, over the difference between the non-science of astrology and the science of astronomy.
Many bookstores, in particular, foster this confusion by displaying books on the two subjects side by side.
As a keen amateur astronomer I take a dim view of being mistaken for a fortune teller!
Jeremy Phillips Loughton Essex
We welcome short letters and reserve the right to edit the longer ones.
Write to: Letters to the Editor,New Scientist , Stamford Street, London SE1 9LS, or fax to 071 261 6464.
Please include your daytime telephone number, and cite the date of articles mentioned.
I'LL BE REMEMBERED BUT YOU LACK CHARISMA.
GREAT MOMENTS IN PENCIL HISTORY 2B OR NOT 2B?
HAMLET HAS DOUBTS
THE DIFFERENCE IS SIMPLE.
I'M AN ASTROLOGER, SHE'S AN ASTRONOMER.
GRIMBLEDON DOWN
Bill Tidy
GOOD GRIEF, 95% OF A CONDOM'S STRENGTH DESTROYED IN UNDER 15 MINUTES…
IF AN OIL BASED LUBRICANT IS USED.
VASELINE, PETROLEUM JELLY ETC!
YES.
IF OIL'S INVOLVED YOU CAN FORGET EFFECTIVE PROTECTION!
DAMMIT, AFTER ALL OUR CONTRACEPTIVE RESEARCH!
GET ME THE WAR OFFICE!
TALKING POINT
A lesson in defence from the Gulf
The Gulf War, like all recent wars, has become a laboratory for military scientists and weapon makers.
As we watch Iraq being systematically destroyed by high-technology weapons — and the country's apparent inability to respond — few can be left in doubt of the overwhelming superiority of the military technology of the industrialised powers.
Recent advances in technology will inevitably change military thinking.
One of the lessons of the conflict for our own policy makers is that, as a result of these advances, a major step forward has occurred in the apparent effectiveness of defensive weapons systems.
Both large and small countries are likely to use the Gulf War to rethink their military postures.
For some, the change may be relatively small.
The US — and to a lesser extent the USSR — will probably wish to maintain their capability to exert military force worldwide, to influence, and if necessary intervene in the affairs of other countries.
With the end of the Cold War, however, major European powers such as Britain and France will hopefully move towards defensive postures (even if the Gulf conflict has emphasised the need to keep some forces for rapid deployment for use in other regions).
And for many smaller powers, the main lesson of the Gulf War may indeed be that, as a result of recent technological advances, the best value for military budgets comes from adopting defensive policies.
Whether offensive or defensive strategies are being reassessed, the Gulf War has brought home the fact that automation has become the cornerstone of virtually all new military systems — so much so that we are moving steadily towards a fully automated battlefield.
Each successful launch of a Tomahawk or Patriot missile strengthens the confidence of both the military commanders and the politicians in such weapons systems.
These new military technologies can be grouped under three main headings.
First, there are technologies using a range of sensors on board satellites and manned aircraft, which allow the precise location and identification of targets and the identification and tracking, in real time, of mobile enemy forces deep within their own territory.
Secondly, there are the sensor and guidance technologies for smart ‘fire-and-forget’ missiles; these are able to detect, identify and attack armoured vehicles, combat aircraft and warships, as well as hardened, fixed targets like command and control centres, in all weathers and battlefield conditions.
Once fired, they do not require further instructions from the launch platform.
Finally, there are the computerised command, control, communications and intelligence systems.
Each of these new technologies has clear offensive uses.
But each also has the capability of being built into defensive weapons systems.
For example, new antitank missiles, particularly when used from helicopters, are making main battle tanks obsolete.
Furthermore, research has shown that antitank helicopters are already very cost-effective; one helicopter should, in battle, be able to destroy 17 times its value in tanks before being shot down.
The implications of all this are that new military technologies are making conventional defence increasingly cost-effective compared with offence (although this argument does not apply to nuclear defence, such as the Strategic Defense Initiative).
In other words, it is becoming cheaper to destroy the opposition's main weapons of invasion and occupation, such as battle tanks, long-range combat aircraft and large warships — than to deploy them oneself.
The result is to increase the attractiveness of non-offensive (or non-provocative) defence strategies, based on the principle that all the activities of the military forces — ranging from their size, structure, weapons and logistics to their training, manoeuvres and war games — can be designed to provide an effective defence, but with virtually no offensive capability.
Schemes for non-offensive defence vary in their details.
But they share a reliance on the use of short-range missiles against attack by, for example, main battle tanks, combat aircraft and warship.
These missiles are precisely the type of weapons that are demonstrating their effectiveness in the Gulf conflict.
They are also cost-effective, as the cost of a missile is roughly proportional to the square of its range.
Critics often claim that non-offensive defence uses only complex and highly centralised technologies.
This is not so.
Weapons and technologies, particularly when deployed by advanced Western nations, would preferably be chosen for specific tasks, and operated well within their design characteristics.
Emphasis would be given to missiles that are simple to operate and cheap to produce in large quantities.
But there would be a judicious mix of antitank missiles, antitank mines, antitank cannon, anti-aircraft missiles and light anti-aircraft guns.
Such a defence strategy — including command, control and communications centres — could be decentralised and would, therefore, not present obvious targets for enemy bombardment.
This is particularly true for a defensive system based on comparatively small, independent units, armed with short-range weapons.
Short-range systems also have the advantage of offering a choice of weapons.
Short-range missiles, for example, can be guided by laser, active or passive radar, infrared, millimetre waves, fibre optics, and so on .
Similarly, mines can be fitted with various types of sensors, whether seismic, magnetic, acoustic or infrared.
Choice increases both the scope for surprise and the effectiveness of the defence.
Democracy in East European countries and the USSR can only succeed if both the Warsaw Pact and NATO restructure those parts of their military forces assigned to Europe to make them non-offensive.
Negotiations between NATO and the Warsaw Pact could, following further agreements to reduce conventional forces, lead to the introduction of non-offensive defence strategies by both sides.
The Gulf War has demonstrated that the negotiation of mutual non-provocative defence strategies (including adequate verification) would be in tune with technological possibilities.
Unfortunately, the war is also likely to ensure that, for the foreseeable future, the major powers will provide part of their military forces with long-range capabilities for rapid deployment.
It may well be that the smaller powers benefit first from the new defensive technologies.
Frank Barnaby 
Frank Barnaby is a former director of the Stockholm International Peace Research Institute
‘The best value for military budgets comes from adopting defensive policies’
COMMENT
Statistics, damned statistics
LAST WEEK'S Parliamentary debate on the funding of science took place on the day the Science and Engineering Research Council announced it was cutting next year's budget by £28 million.
Despite the irony, it was a pretty disappointing affair.
Labour spokesmen made generous use of statistics to castigate the government for refusing to spend more money on science, and for letting Britain slip down the international league table on R&D spending.
Equally predictably, the government ministers used other statistics to argue that, despite budget constraints, British science remains productive and healthy.
Both sides, as so often in such debates, generated more heat than light.
Labour, for example, whose position was voiced by shadow education secretary Jack Straw, was correct to complain that Britain currently invests considerably less in its research base than most of our industrial competitors.
It is wider of the mark, however, when it uses the possible closure of the SERC's Nuclear Structure Facility to suggest that the government's science funding machinery is in disarray.
It should be remembered that the decision several years ago to continue membership of CERN was taken largely at the behest of the scientific community in full knowledge that, if it was combined with a zero-growth science budget, it would mean increasing pressure on the rest of nuclear physics.
Replying for the government, Kenneth Clarke, the education secretary, was correct to state that the current situation of British science, at least when judged in terms of output, is not as grim as some make out.
We may have slipped back in some fields, but in others (such as molecular biology or pharmaceutical research) we remain world leaders.
Where Clarke was wrong was in his broad assessment that British science is being adequately funded.
Such a claim represents that same wishful thinking as his assessment that increasing the science budget by only 6 per cent next year (the official prediction for inflation) will allow spending to remain constant in real terms.
Neither of these points were picked up in the debate.
Parliamentarians scattered OECD statistics around like confetti, but carefully ducked any attempt at serious discussion of what the statistics really mean.
From the government's side, this was partly deliberate.
Clarke virtually admitted that he did not believe that these important issues should be discussed in public when he defended the government's decision not to publish the advice on science funding which it had received from the Advisory Board for the Research Councils.
This is the first time the ABRC's report has not been made public in recent years.
But Labour, too, was disappointing in its reluctance to sketch out what an alternative science policy might look like.
Promising extra money, though welcome in itself, is the easy option.
Given that a prime reason for investing in science is to boost the nation's technological performance, where was the discussion about the implications of the government's plans to privatise the British Technology Group, the descendant of a body set up by Labour itself in the mid-1960s to perform precisely this task?
It would be wrong to conclude from this debate that science is too important to be left to politicians.
That road is both dangerous and unrealistic.
But we do need a public forum in which the issues can be argued out without the sloganising that tends to characterise so many Parliamentary debates on complex issues.
Bringing back the House of Commons Select Committee on Science would be one step in this direction.
Making public the advice of the ABRC (see This Week) would be another.
For, whatever Clarke feels, the debate is also too important for its details to be kept behind locked doors.
Frozen lines
EVERY TIME more than an inch of snow falls on London, the questions are the same.
How come we can send a spacecraft to Venus or write the Bible on a microchip yet remain incapable of operating a sophisticated transportation network when the temperature drops just a few degrees below zero?
Now there's a new twist: how come we can know when a missile is even threatening to fall on Tel Aviv or Riyadh or Dharan, yet remain ignorant of when the next train will leave Waterloo station — or where it is heading?
Philosophers of technology talk of a communications revolution.
Satellites and cellular telephones allow us to converse instantaneously from a London street to the Australian outback.
Yet somehow the message has still to get through to British Rail that the communication of travel information is no longer a luxury, to be fed in titbits to grateful passengers.
Information is now easy to come by; it should be distributed equally liberally, particularly to those who have a right to know.
To be fair, some British Rail employees do their best to inform.
Others merely shrug off questions with ‘your guess is as good as mine’.
And still others remain discretely silent — the last resort of those with power over the microphone button.
Meanwhile, those of us deprived of information struggle on as best we can to make sense of a technological world which — despite Francis Bacon's promise that modern science would make man ‘the master and possessor of nature’— has once again amazed us with its vulnerability.
THIS WEEK
Judge backs case against passive smoking
Ian Anderson, Melbourne
AN AUSTRALIAN court has ruled that passive smoking causes lung cancer, asthma and respiratory problems in children.
This is believed to be the first time that a court of law in any country has declared that scientific evidence establishes cause and effect between exposure to smoke and certain diseases.
The 210-page judgement, by Justice Trevor Morling of the Australian Federal Court, is expected to have ramifications worldwide.
‘The jump has been made between the scientific debate about passive smoking and the law,’ says Peter Cashman, solicitor for the Australian Federation of Consumer Organisations (AFCO).
The case, which lasted 30 months, was sparked by an advertisement that appeared in 14 newspapers in July 1986.
In the advertisement, the Tobacco Institute of Australia claimed that there was ‘little evidence and nothing which proves scientifically that cigarette smoke causes disease in nonsmokers’.
After the AFCO protested to the Trade Practices Commission, the institute revised its advertisement.
According to the federation, the offending wording had not been corrected and so the organisation began legal proceedings in the Federal Court under the Trade Practices Act.
Among witnesses called for the AFCO were Richard Doll, an epidemiologist from the University of Oxford, whose work first established the link between smoking and disease.
Other witnesses came from the schools of public health at Harvard and Yale and from St Bartholomew's Hospital in London.
More than 320 reports were tabled, but the main evidence was based on reports into passive smoking published in 1986 by the US Surgeon General, the National Research Council in the US, the National Health and Medical Research Council in Australia and the Froggatt inquiry into health and smoking in Britain.
Witnesses for the Tobacco Institute questioned the design, methodology and statistical significance of data from some of the epidemiological studies, but Morling said the overall pattern emerging from the studies was clear.
He agreed with Doll that statistical significance was simply a ‘guide to thought’ rather than an absolute standard.
‘The lack of statistical significance calls for the exercise of caution in evaluating the study, but that is not to say that it disqualifies the study from consideration,’ said Morling.
This part of the judge's ruling is important because studies of passive smoking are beset with problems such as establishing a control and of knowing exposure levels.
Morling found there was ‘overwhelming evidence’ that passive smoking triggers attacks of asthma and diseases of the upper and lower respiratory tract in children.
There was also ‘compelling scientific evidence’ that cigarette smoke causes lung cancer in nonsmokers.
There was also ‘more than a little evidence’ that other people's cigarette smoke causes glue ear in children.
Morling said the statement by the Tobacco Institute was likely to mislead smokers and nonsmokers on the effects of smoking.
He granted an injunction preventing the institute from publishing similar advertisements in Australia.
Observers said this week that it was significant that Morling did not rule about the possible effects of passive smoke on the unborn child.
Legal experts said that although medical science claims there is a link, lawyers would challenge the link on a technicality — a fetus cannot read a newspaper advertisement.
‘He stuck with things that were incontrovertible and difficult to challenge on appeal,’ one lawyer said.
Already, a number of lawyers, employers and medical researchers from the US, Europe and Britain have been telephoning Australia to find out more about the judgment and to request copies of the ruling.
According to Cashman of the AFCO, employers will find it difficult to plead that they were unaware that cigarette smoke caused illness, when employees sue them.
The ruling could also prompt the owners of public places such as restaurants, bars, cinemas and sporting venues to think about banning smoking in order to avoid legal action, says the anti-smoking group, Action on Smoking and Health.
‘Black gold’ obscures green vision in US
THE WHITE House's long-awaited national energy strategy spurns energy conservation and instead urges the country to drill for more oil.
Within hours of its announcement, the plan was attacked by environmental groups who say they will bury President George Bush's plan.
Early versions of the plan, drafted by the Department of Energy, discussed efficiency standards for automobiles, taxes to discourage consumption of fossil fuels, and subsidies for investments that would save energy.
The White House later dropped these provisions.
Plans requiring more efficient electric lights and tax benefits for generators of wind, solar, and geothermal energy survived until last week, when the White House finally rejected them.
Joelle Ziemian, a spokeswoman for the DOE, says that Congress will receive a final version within three weeks.
Ziemian says there is more to come; only the portion that requires new laws enacted by Congress has been leaked to the public so far.
A letter from James Watkins, secretary of energy, states that the energy strategy will encourage voluntary efforts to make buildings more energy-efficient and ban wasteful cars.
But most of the plan is devoted to encouraging more energy production.
It will allow oil drilling in the Arctic National Wildlife Refuge — already the focus of a heated debate between environmentalists and developers — and drilling along California's coast.
Companies drilling for oil in these regions will have to pay for any environmental damage they cause, and the DOE can stop any drilling that harms wildlife, according to the plan.
Bush's scheme would brighten prospects for nuclear power by granting companies a single licence to build and operate nuclear plants.
Current laws forbid a company from operating a reactor even after it has been built.
The proposed regime would also allow the DOE to study possible storage sites for nuclear waste without prior permission from state and local authorities.
Michael Fischer of the Sierra Club, an environmental group, labelled the plan ‘an answer to the prayers of the oil, nuclear and auto industries’.
James Wolf of the Alliance to Save Energy, a coalition of conservation groups, called it ‘woefully inadequate’.
The Sierra Club says that a law requiring new cars to use 40 per cent less fuel by the year 2001 would save 2.8 million barrels of oil a day.
This is nearly ten times the projected output of oil wells in the Arctic.
Democrats in Congress have already introduced competing proposals that would reduce American dependence on imported oil by encouraging conservation and alternative fuels.
Congress is likely to incorporate many of these measures into the Bush administration's plan, says Wolf.
Secret report exposes fears for British science
William Bown
CONFIDENTIAL advice to the government last year warned that British science was heading for trouble.
A report from the Advisory Board for the Research Councils argued that next year's budget was ‘substantially less than the sums needed to sustain the health of the UK science base’.
New Scientist has obtained a copy of the unpublished report.
The four-page letter from David Phillips, head of the ABRC, to Kenneth Clarke, the education secretary, reveals a deep rift between the minister and his civil servants.
While Clarke has expressed confidence about the state of British science, Phillips is worried.
‘The proportion of the nation's wealth [GDP]deployed through the science budget will have declined by 15 per cent between 1981 and 1984,’ Phillips wrote.
‘This will make it difficult to sustain scientific excellence.’
Phillips rejects the government's assessment that the 6 per cent increase in the science budget represented level funding in real terms.
He argues that because costs, particularly salaries, are rising by more than 6 per cent, ‘the volume of scientific programmes will have to be reduced’.
In a debate on science in the House of Commons last week, Clarke argued that level funding next year was justified by big rises in the past, ‘especially the big 8 per cent uplift which took place two years ago’.
But Phillips rejects this.
He says the impetus of the 1989/90 funding boost will be ‘more than cancelled out’ by next year's cuts.
Phillips also contradicts Clarke's view that cuts of £28 million forced on the Science and Engineering Research Council are the result of ‘reckless overcommitment’ by the SERC.
He says the ‘offending’ research was ‘started, with your [Clarke's]predecessor's endorsement, during 1990/91’.
Although the SERC last week announced a two-year reprieve for the Nuclear Structure Facility (NSF) at Daresbury which faced closure, grants for new research will be halved, a solution Phillips called ‘extreme’.
This criticism of government policy will carry added weight coming from the ABRC, the government's own advisory body on science funding.
In the previous five years the advice was published, but this year the Department of Education and Science kept it confidential.
Phillips is plainly unhappy with this.
His letters closes with: ‘I should also welcome the opportunity to discuss the publication of the board's advice.’
Phillips also queries the amount of money newly allocated to the research councils to cover overheads of some projects previously paid for by the universities.
The government has allocated £50 million in 1992/93 and £100 million in 1993/94.
Phillips thinks the research councils could lose out on the transfer.
He says ‘more work is needed to refine the amounts corresponding to the change in funding responsibilities’.
Phillips's letter assesses how cuts might affect the research councils.
The Natural Environment Research Council has cancelled plans to build two types of submarine.
The cost of redundancies in the Agricultural and Food Research Council has absorbed half the planned increase in research grants.
The Economic and Social Research Council has cut back work on the 1991 census and its ‘positive health’ centre for health promotion.
The Medical Research Council has also cancelled work for ‘financial rather than scientific reasons’.
One quarter of Phillips's letter is devoted to the SERC's problems.
On top of this year's cuts, Phillips says that the SERC has also decided ‘to consider possible reductions in long-term commitments at its research establishments and through international subscriptions’.
He says the council also believes ‘some further reduction in the proportion of SERC's budget spent on ‘big science’(nuclear physics and astronomy) would be desirable’.
As the government has ruled out cutting the subscription to CERN, the European centre for particle physics, those statements point to a continuing squeeze on nuclear physics.
This week, Mark Richmond, chairman of the SERC, compared the small number of researchers who can use the NSF with the many more who can use other facilities at the same time.
‘The cost per head of the synchrotron radiation source [at Daresbury]is a much more effective use of resource than the NSF,’ he said.
Phillips's letter suggests the ISIS neutron source at the Rutherford Appleton Laboratory is considered more deserving than the joint European source in Grenoble.
‘Particular attention is being given to the possible ending of UK participation at the treaty renewal point in 1992,’ says Phillips.
Phillips believes that Britain's competitors are ‘investing significantly more in research’.
He says the ABRC's major disappointment is that the level of current funding will ‘widen the disparity with research budgets in other countries’.
•Mark Richmond, who sent shock waves through the nuclear physics community by threatening to shut down the NSF, still wants to shift the council's funding from big facilities towards individual grants.
But he says the problems with next year's funding mean that ‘in the short term, we will move in the opposite direction’.
The SERC last week announced cuts of £28 million.
To Richmond's dismay, the biggest cuts are in grants.
The council can fund only half of next year's grants.
The underlying problem is that £130 million of the SERC's budget is tied up in international projects, says Richmond, leaving only £300 million spare.
The astronomy board of the SERC would like to pull out of the Spectrum-X astronomy project with the USSR, says Richmond.
But the project is an intergovernmental agreement and therefore ‘horrendously complex’.
Pulling out may cost more than staying in.
The NSF is guaranteed funding for next year.
In 1992/93, after a review of nuclear physics has been completed, it will require £5 million, open or shut.
This money is not currently in the SERC budget and Richmond hopes it will come either from the ABRC's reserve or as extra science spending from the government.
Otherwise it will rob other facilities — or grants.
MPs condemn government record on funding
IN THE first science debate in the House of Commons for two years, Jack Straw, Labour's shadow education secretary, accused the government of ‘failing properly or sufficiently to fund the science base’.
Kenneth Clarke, the education secretary, disagreed.
‘We have a strong science base in Britain,’ he said.
Opening the debate on a motion condemning the government's handling of science and science education, Straw promised that a Labour government would insulate the science budget from inflationary pressures.
Straw also attacked the government for lacking a science culture.
‘The government's recognition of the importance of science is a token one,’ he said.
He accused Clarke of failing to fund the science base properly, suppressing the advice of the Advisory Board to the Research Councils, neglecting the supply of science teachers and failing to provide a European dimension.
In his reply, Clarke emphasised the success of the government's reforms in education.
‘Any serious long-term policy for science must begin by securing an adequate supply of new, highly qualified, scientists and engineers,’ said Clarke.
By putting science, mathematics and technology in the National Curriculum and by increasing the number of science graduates, the government was securing that supply.
Doug Hoyle, the Labour MP whose constituency is next to the NSF, said: ‘Leaving the present government in charge of scientific matters is like leaving Dracula in charge of a ward full of anaemics.’.
US feels heat on global warming stance
Dan Charles, Washington DC
A RECORD-BREAKING heat wave set the tone for negotiations on global warming in Washington this week.
While the US, as the host to the conference, is making much of its ‘action plan’ to limit emissions of gases that trap heat in the Earth's atmosphere, critics in Congress and from Europe have labelled it an ‘inaction plan’.
The dispute is over the US's reluctance to curb emissions of the most important greenhouse gas, carbon dioxide.
A Dutch delegate described the US plan as an improvement on its earlier position, which was to reject all calls for action until science proved that global warming was a certainty.
But the American plan promotes only steps that have already been taken for other reasons.
The steps include banning chlorofluorocarbons — already limited under the Montreal Protocol to protect the ozone layer — and restricting emissions of air pollutants such as nitrogen oxides and volatile organic compounds, which are limited by the Clean Air Act.
All these gases are potent greenhouse gases.
According to American projections, these actions will compensate for a 15 per cent increase in emissions of carbon.
So even without limiting emissions of carbon dioxide, the American contribution to global warming in the year 2000 will be no greater than in 1987.
At that point, however, the still increasing emissions of carbon dioxide will begin the upward spiral once more.
In contrast to the US, the European Community has agreed to stabilise its emissions of carbon dioxide at 1990 levels by 2000, and to reduce them after that.
The conference, held in Chantilly, Virginia, near Washington DC, marked the start of negotiations to prepare an international treaty on climate change.
The treaty could be signed at a UN conference on the environment and development in Rio de Janeiro, Brazil, in 1992.
A coalition of small island nations, whose existence could be threatened by a rise in sea levels, arrived at Chantilly in force.
Robert Van Lierop of Vanuatu in the West Pacific and chairman of the group, told the delegates that nations like his ‘do not have the luxury of waiting for conclusive proof’ of global warming.
In the past, the US has insisted that the economic costs of reducing emissions of carbon dioxide should be considered in any treaty.
The US wanted to set up a working group to study these costs, but other nations rejected the proposal.
Democrats in Congress used the conference as an occasion to attack the Bush administration's position.
Senator Albert Gore called the US position ‘the Chantilly shell game’— the American equivalent of the three-card trick.
Beneath the rhetoric, he said, there is no substance.
Gore, Tim Wirth of Colorado, and 46 other senators proposed a new law last week that would cut US emissions of carbon dioxide by 20 per cent in 15 years.
Gore and Wirth were backed by a new report from the congressional Office of Technology Assessment.
According to the OTA, US emissions of carbon dioxide could decrease by 35 per cent within 25 years by introducing taxes on energy consumption and tougher standards to promote energy efficiency.
Health hazards that lurk in ‘inert’ gas
SAFETY authorities in Britain and the US are under growing pressure to warn people of the dangers of a common, inert gas used widely in industry.
The breakdown products of the gas, sulphur hexafluoride, have poisoned repair workers in incidents on both sides of the Atlantic.
Yet although the problem is officially recognised, workers may not know the risks, say researchers.
Sulphur hexafluoride itself is harmless except in a confined space, where it displaces oxygen and can asphyxiate people.
The electricity supply industry and others use it to insulate high-voltage circuit breakers and cables.
British Gas uses it to trace leaks.
If sulphur hexafluoride is subjected to electrical sparking in the presence of oxygen, it degrades, releasing toxic breakdown products.
These include sulphur oxyfluorides, sulphur dioxide, and sulphur tetrafluoride, which react with water in the body's mucous membranes to form other acidic compounds that can damage the lungs.
Studies in animals show that the breakdown products of electrically arced sulphur hexafluoride kill cells, and irritate the lungs of mice and rats, making them bleed.
Two incidents in which workers suffered breathing difficulties have now prompted doctors to call for new warnings to workers.
The first incident was originally reported in Britain in 1988 by Keith Pilling, occupational physician for what was then the UK Atomic Energy Authority.
Two repair workers at the JET fusion experiment at Culham in Oxfordshire entered a cylindrical tower that was part of a power supply to the experiment.
Sulphur hexafluoride, which normally filled the tower as an insulator, should have been removed beforehand but they mistakenly began work before the tower had been properly vented.
Both men collapsed, but were revived.
Shortly after, one man developed symptoms that would not be expected from exposure to sulphur hexafluoride: he coughed up bloodstained fluid and doctors diagnosed fluid on the lungs.
Without oxygen therapy, the man might have died, says Pilling.
When researchers analysed the air in the tower, they found a sulphur oxyfluoride, SO2F2.
There was no evidence for sparking, says Pilling, but it cannot be ruled out.
He reported the incident in the Journal of the Society of Occupational Medicine .
‘Then I more or less let the whole thing drop,’ he said.
Last December, however, he read a similar report from two researchers in the British Journal of Industrial Medicine .
Ruth Lilis at the Mount Sinai Medical Center in New York and Allen Kraut at the University of Manitoba investigated the cases of six workers who developed coughs, nosebleeds, tightness in the chest and other symptoms after repairing an underground cable.
Chest X-rays showed temporary damage to the lungs of two men.
Analysis of the site where they worked revealed the presence of sulphur tetrafluoride.
The changes in the men's lungs were ‘consistent with the known effects of exposure to irritant gases’, said the researchers.
Britain's Health and Safety Executive is aware of both incidents but has not issued specific warnings.
‘There is no particular guidance on this problem because it should be well known by the industries that use [the gas],’ said a spokesman last week.
But Pilling said he would be ‘very surprised’ if such knowledge were widespread.
Lilis in New York also said she had been surprised by the findings.
The hazardous potential is clearly stated on the gas's data sheet, but, she says, ‘nobody was aware of the risk, including me’.
The two American researchers say that, despite some of the workers having 20 years' experience in the electricity industry, none of the workers had heard of sulphur hexafluoride, nor did they know that their work could lead to exposure to irritant gases.
‘You can't avoid accidents, but physicians and workers should be aware of the risk,’ she said.
The National Institute of Occupational Safety and Health in the US was not aware of the incidents.
Larry Reed of NIOSH said: ‘We have not done any work in this area.’
In Britain, the HSE says that the responsibilities of workers, employers and suppliers for handling sulphur hexafluoride and its breakdown products are covered in general terms under the Health and Safety at Work Act and the guidelines on the Control of Substances Hazardous to Health.
But, says Pilling, ‘The legal stuff is there, but people need encouragement to read it.’
National Grid, Britain's high-voltage supply company, and London Electricity say they have a code of practice for workers handling the gas.
Phyllida Brown
Legal, decent and patently untruthful
Barry Fox
THE British Patent Office spent last week desperately trying to stop the publication of a series of advertisements encouraging people to patent their inventions.
The full-page adverts contain an extremely embarrassing error.
The Patent Office says it feels ‘terrible’.
It cancelled the advertisement wherever it could but was too late to prevent it from appearing in some national newspapers.
The workload of the British Patent Office is falling as more applications are filed with the European Patent Office in Munich.
To save money it is moving out of London to Newport in South Wales and has been running roadshows to drum up business from industry.
The ill-fated advertisements were to form a preliminary campaign, costing £100,000, as a prelude to a major campaign in the coming financial year.
The Department of Trade and Industry blocked the Patent Office's first draft advertisements.
They showed footballer Paul Gascoigne, who has applied to register his nickname Gazza as a trademark, being grabbed by his genitals.
A caption referred to his ‘other precious assets’ now being legally protected.
The same advert also told how the inventor of FM radio, Edwin Armstrong, had committed suicide.
The DTI objected to both the picture of Gazza and the reference to Armstrong's suicide, saying that they were in bad taste.
This left five days for the Patent Office and its advertising agency, Ayer, to come up with a new version.
They decided to compare the money earned by the inventors of the Polaroid instant camera, the Catseye road stud, and the ring-pull can, with the reward reaped by Armstrong.
‘No FM patent — no financial benefit, £0.00,’ warned the Patent Office.
‘The difference between genius and madness is patently obvious.
Taking out a patent can bring the inventor quite a fortune.
Not taking one out however (as with Edwin Armstrong, the American who discovered FM radio) may bring the inventor nothing but misery.’
Unfortunately for the Patent Office, the advertising agency failed to ensure that it had its facts right — and the Patent Office did not check the agency's new advertisement.
Armstrong spent many years battling over patents (see ‘The battles of Armstrong — radio's forgotten man’,New Scientist , 1 February 1979).
On 1 February 1954 he put on his coat, hat and gloves and walked out of the window of his 13th-floor apartment overlooking New York's East River.
He did not commit suicide because he had no patent and had reaped no rewards.
He had a patent and had earned millions of dollars in royalties.
He killed himself because he was literally sick to death of fighting some of the largest manufacturers of FM radios for the royalties they owed him.
Armstrong was fully aware of the value of patents because he had previously earned huge royalties on earlier inventions which laid the foundations of modern radio technology.
By 1922 he was a millionaire.
Armstrong filed the master patent on FM at the US Patent Office on 24 January 1933.
The patent was granted in December of that year.
In the late 1930s, when he still could not persuade engineers that his FM system would work, he proved they were wrong by spending $2 million of his own money on building and running an FM radio station.
By 1948, 36 manufacturers (including EMI, GEC and Marconi in Britain, and Philips in the Netherlands) were paying him royalties for domestic radio sets.
By 1954 Armstrong had received $4.5 million.
But RCA would offer only a one-off payment of $1 million.
Armstrong sued 21 companies, including RCA and Motorola.
He worked for five years on preparing the case before committing suicide out of bitter frustration.
His widow settled for the $1 million which RCA had offered 14 years earlier.
Spinning satellite
ROSAT, the X-ray satellite that span out of control three weeks ago, was restarted last week.
But if it goes out of control again, there is a one-in-three chance it will happen while beyond the reach of ground control.
For an 8-hour period each day, ROSAT is in the shadow of Earth.
If it spins out of control in the first half of the period, the batteries could run down and put it beyond recovery, says Alan Harris, a ROSAT physicist based in Munich.
British and American scientists say they would have preferred to wait longer before restarting ROSAT.
But astronomers from Germany, the main contributor to the project, were keen to start the second part of the mission as soon as possible.
Dornier, the German company which built ROSAT, is trying to cover how it failed after being hit by high-energy particles from a solar flare.
Software on the craft has already been changed to protect instruments if ROSAT loses control again.
What's wrong with French literature?
FRENCH scientific journals are of such ‘questionable quality’ that few researchers outside France either read them or are willing to contribute to them.
Scientists, publishers, students and government officials meeting last week at a colloquium organised by the research ministry agreed that poor quality produces a downward spiral of falling circulation and a further loss of contributions from respected scientists.
Most of the 1500 French journals are published by small research units and groups of specialists, and have an average print run of 1000.
Only 10 per cent are published by the big research institutes.
The most recent survey showed that of 1460 journals published in France during 1984, only 150 were of an international standard.
Mathematics journals had the highest standing internationally, with nearly a third of contributions coming from abroad.
Those at the meeting agreed that the way to pull French journals up to international standards is to attract articles from foreign contributors and to include foreign specialists on editorial boards.
Synopses in English would widen their readership.
French scientists also need to adopt a higher profile in international circles.
A recent study by INSERM, the national institute of health and medical research, revealed that French specialists are under-represented on the editorial boards of medical reviews in relation to the strength of French biomedical research.
French scientists and engineers are not good at background reading.
Their failure to make the most of the scientific literature seems to begin early in their academic careers.
A survey presented at the meeting showed that half of undergraduate and graduate science students think lecture notes and photocopies are enough to pass their course examinations.
In the six months preceding the survey, 40 per cent of them had spent £18 or less on textbooks.
The problem of poor reading habits is compounded by a lack of science textbooks.
The survey showed that France produces only 6300 new science books a year, compared with more than 9000 in Britain.
University teachers, the survey revealed, have begun to recommend books in English.
One way of tackling the reluctance to read might be the introduction of a hitherto unknown entity in France — the campus bookshop.
There are just five campus bookshops in the whole of France.
The well runs dry for California's farmers
Pratap Chatterjee, Berkeley
FACED with a fifth successive year of drought, California's state authorities cut off water supplies to farmers last week.
The state also plans to cut supplies to urban consumers by half.
Later this month, federal authorities are expected to announce plans to reduce their supplies of water to farmers by three-quarters.
Farmers use more than 80 per cent of the state's water.
By irrigating land that would otherwise be desert, they produce half of the fruit and vegetables sold in the US.
The federal and state government water projects are the two biggest suppliers in California.
But with only 15 per cent of normal rainfall in January, usually one of the wettest months, and the rainy season drawing to a close, they are desperately short of water.
In California, the issue of water supply has pitted farmers against city dwellers and environmentalists, who claim that the farmers waste most of their water on low-value crops and inefficient irrigation systems.
Environmental groups also argue that overconsumption by the farmers has damaged many natural habitats in California.
In most of the state's 900 or so water districts farmers often have priority in the use of water.
They consume 83 per cent of the state's water but contribute only 3 per cent to the state's economy.
One-third of the water irrigates thirsty crops of low value — alfalfa, cotton, rice — and pasture.
One of the reasons why farmers consume so much water is that they pay very little for it — between 1 and 2 per cent of the price paid in cities.
Subsidies encourage them to produce the low-value crops.
Water from northern California is transported to the Central Valley, the main agricultural region.
According to Sami Yassa, of the Natural Resources Defense Council, excessive consumption has led to serious depletion of ground water.
In some regions of the state, this is threatening several species of fish, he says.
Yassa estimates that the loss of salmon and steelhead stocks alone has cost the fishing industry $1 billion over the past 20 years.
Two-thirds of California's water is drawn from the vast delta where the Sacramento and San Joaquin rivers flow into San Francisco Bay.
So much water has already been drawn from the delta that more than 90 per cent of the original wetlands have disappeared, says Yassa.
A previous California resources secretary proposed building bigger dams on California's rivers as a solution to the water crisis.
Now farmers talk about installing new technology such as drip irrigation systems.
Environmental groups want farmers to make more fundamental changes.
David Fullerton, of the Sierra Club, an environmental lobby group, says: ‘They want to use new technology or more water while we want them to talk about the crops they're growing.’
Some southern cities propose building desalination plants to treat water from the Pacific.
This is an expensive option and could increase the cost of water tenfold for city users.
Desalination plants also require a lot of energy, and building new power plants is difficult because of the state's strict laws to control pollution.
Environmentalists claim the highly saline water discharged from desalination plants could harm marine life.
Most of these solutions would take a long time to implement.
Meanwhile farm groups say that the drought will cost tens of thousands of jobs, and economic losses ranging from $580 million to $4 billion.
The state has agreed to pay farmers $250 an acre ($614 a hectare) not to plant their land, but farm groups are demanding $1300 an acre.
California will make its final decision on its water strategy next week, when the governor's drought task force reports its findings.
Dutch plan to take toll on road traffic
For the second time in two years, the Dutch government has drawn up plans to price cars off the roads to ease the Netherlands' growing problems of congestion and air pollution.
The first attempt to drive up the cost of travelling by road brought down the government.
If this latest plan works it will provide a blueprint for transport policy throughout Europe.
The government plant to spend about £6 billion on a scheme to improve public transport and provide new rail and canal links for passengers and freight.
The links will be complemented by urban rail networks to improve connections between national and regional train and bus services.
Transport engineers in the Hague, for example, are developing a new rapid rail system, called Randstad Rail, which uses a sophisticated locomotive to improve speed, safety, comfort and reliability.
Money raised from tolls on roads will help to pay for the scheme.
Despite the government's efforts to reduce the number of cars on the roads, the number of new private car registrations has increased from 99 000 in 1989 to 140 000 last year.
This raises the number of cars in the Netherlands to 5.54 million.
In the next 20 years, the use of cars is estimated to increase by 70 per cent.
Two years ago, the Dutch Prime Minister, Ruud Lubbers, put forward a plan to reduce the number of cars on the road to fewer than 4 million.
This was to be achieved by increasing the cost of buying and running cars by about 50 per cent.
The opposition's rejection of the proposal led to the collapse of the coalition government.
In the new plan to clear the roads for more efficient public transport, the Dutch transport minister, Hanja Maij-Weggen, skirts the issue of increasing the direct costs of owning a car.
Instead, she suggests introducing devices such as tolls for driving in specific, badly congested areas or for driving at particular times, such as rush hours.
Predictably, Dutch road users have not welcomed Maij-Weggen's proposals.
The vehicle owners' pressure group Stichting Weg has branded the plans ‘impractical, unreasonable and inequitable’.
Dutch haulage companies favour the introduction of tolls rather than rush-hour levies.
‘Tolls are a daily expense which may deter motorists more than a rush-hour permit which is a one-off expense,’ says Rob Lenterman, of Nedlloyd, a multinational freight company based in Rotterdam.
Less congested roads, he says, would increase his company's profits.
Peter Spinks, Amsterdam
The cup that cheers environmentalists
POLYSTYRENE cups may note be the environmental villains of popular imagination.
The paper cups preferred by many environmentalists are far worse offenders, according to a Canadian study.
Not only do they consume more natural resources, they also produce more pollution.
Martin Hocking, a chemist at the University of Victoria in British Columbia, compared the environmental credentials of the two kinds of cup (Science , vol 251, p 504).
Hocking calculates that paper cups contain six times as much raw material by weight.
On average, paper cups weigh 10.1 grams; a polystyrene cup weights only 1.5 grams.
‘The paper cup consumes about 12 times as much steam, 36 times as much electricity, and twice as much cooling water.
About 580 times the volume of waste water is produced for the pulp required for the paper cup,’ says Hocking.
The effluent from paper making contains 10 to 100 times the amount of contaminants produced in the manufacture of polystyrene foam.
For each tonne of bleached pulp, 22.7 kilograms of air pollutants are generated, compared with 53 kilograms for each tonne of polystyrene.
But cup for cup the polystyrene generates less air pollution.
According to Hocking, paper cups cannot be recycled because they contain chemical additives.
Contrary to popular belief, the polystyrene ones can.
Nor do paper cups break down quickly if buried.
Even if they did, the breakdown of just 2 per cent of the paper in the cup would give off methane to match the greenhouse warming potential produced by all the pentane gas used to ‘blow’ the foam of a polystyrene cup.
South Africa wakes up to the threat of AIDS
Sue Armstrong, Johannesburg
THE SOUTH African government is to launch an AIDS education campaign in schools in response to alarming new figures about the spread of the disease.
It has also promised to review the ban on television advertising of condoms and the high import duty imposed on them.
There is a serious lack of data on the level of infection in both white and black people, according to Nicky Padayachee, director of community health for the city council in Johannesburg.
But his own estimates, widely regarded as the most reliable of many recent projections, suggest that between 317 000 and 446 000 black South Africans will be HIV-positive by the end of this year.
About 1000 white people are believed to be infected.
The number of HIV-positive people is doubling every 8.5 months.
This level of infection in the total population would be higher than that which exists in Uganda, considered to be the worst-hit country in Africa.
Padayachee has tested people since 1984, for example in clinics for sexually transmitted diseases, antenatal care, family planning and tuberculosis.
He has also studied employees in several large businesses.
Apartheid has played a key role in the AIDS epidemic by promoting migrant labour, which splits families for long periods, encouraging extramarital relationships.
Apartheid has also made the fight against AIDS very difficult, according to Refiloe Serote, director of the Soweto-based Township AIDS Project.
Black people automatically distrust any information that comes from white people, and especially from government sources, she says.
Many are convinced that AIDS is a ‘white disease’ and that the real motive behind the promotion of condoms is to reduce the birth rate of black people so that white people are less heavily outnumbered when universal franchise finally comes to South Africa.
There have been 680 reported cases of AIDS in South Africa, but this is almost certainly an underestimate.
Until 1989, most cases where white gay men, but the majority are now black heterosexual men.
And at least half the cases in the black community may go unreported, warns Padayachee.
The highest incidence and fastest spread of HIV infection are in Kwazulu-Natal, says Padayachee.
‘This is the most troubled part of the country and HIV thrives on social disturbance and poverty.’
He says that violent death is so common that people tend to be fatalistic and unprepared to take precautions.
Moreover, family life and stable partnerships are broken with the destruction of homes, and opportunities for sex education are lost with the disruption of schools.
Evidence suggests that the virus was introduced into this vulnerable environment by long-distance lorry drivers transporting goods from Natal's ports to neighbouring countries, and by refugees from countries to the north which have more widespread epidemics of AIDS.
The first reported case of AIDS was a white gay man in 1981.
Because the Pretoria government has focused on infection among black people, very little is known about the extent of the epidemic in the white population, but the disease appears to have remained largely within the gay community, with a ratio of 15 to 1 men to women infected with HIV, and clusters of infection in urban centres.
By contrast, infection within the black population (which surfaced in 1987 in a migrant mine worker from Central Africa) is a heterosexual epidemic with a ratio of two infected women to three infected men.
The potential for the epidemic to spread is underlined by a survey among black schoolchildren in Cape Town which revealed that 70 per cent of 16-year-olds were sexually active, and 20 per cent had several partners in the past year.
The fragmentation of health services is a major handicap in any concerted effort to combat AIDS.
At present there are 14 autonomous health departments in South Africa.
However, the government has vowed to create a single one.
According to Padayachee, ‘1991 is going to be a crucial year.
Everyone is saying the right things.
This year we'll see whether they turn words into action.’
Dengue fever grips Brazil as officials wrangle over blame
BRAZIL's hospitals are bracing themselves for an epidemic of dengue, an acutely infectious disease caused by a virus.
There are so few hospital beds available that health authorities plan to set up emergency treatment centres in football stadiums.
Dengue is transmitted by the yellow-fever mosquito Aedes aegyptii .
Scientists in Rio de Janeiro blame local authorities for not fighting the mosquito over the past two years.
Local authorities blame the federal government for not giving them enough money to do the job.
Researchers say that the alarming increase in the number of cases in December, the first month of the Brazilian summer, signals worse to come.
Although the most common form of dengue is rarely fatal, a more dangerous form of haemorrhagic dengue looks likely to take hold in Rio de Janeiro and the surrounding area.
Researchers based at the Oswaldo Cruz Foundation say the Rio de Janeiro is the same as that responsible for an outbreak of haemorrhagic dengue in Cuba in 1982.
Some 200 000 people were infected; 10 000 were seriously ill and 156 died.
One hospital in Rio de Janeiro reported 242 cases of dengue (including 21 of the more serious form) in December.
There were only 307 cases in the other 11 months of 1990.
Roberto Morenho, a director of the hospital, said an epidemic of dengue was ‘no longer a risk but a fact’.
The Brazilian Ministry of Health reported last week that 15 of the country's 27 states are affected so far.
The worst hit are Rio, São Paulo and Ceará in the northeast.
The ministry says it will provide special funds to help fight the disease.
In its usual form, dengue lasts about a week.
The victim suffers from fever, headaches and muscular pains.
The symptoms of haemorrhagic dengue are the same to begin with, but on around the third day the victim's nose and gums begin to bleed and blood pressure drops dangerously low, sometimes fatally.
According to the Brazilian health ministry, 7 per cent of cases of haemorrhagic dengue are fatal.
Most people who contract haemorrhagic dengue have previously been infected with the milder form of the disease.
Rio de Janeiro has suffered several outbreaks of dengue in the past four years and is expected to be hit hardest by the new epidemic.
There is no vaccine for dengue.
The only way to control it is to eradicate the carrier, usually by spraying with powerful insecticide.
A. aegyptii lays its eggs in clean, still water such as that found in vases, large leaves, upturned tyres and swimming pools.
Meanwhile, the São Paulo state health authority warned the local population to avoid going to Rio unless absolutely necessary.
Brian Homewood, São Paulo
IN BRIEF 
Salyut shower
THE Soviet space station Salyut-7 made its long awaited return to Earth in the early hours (0400 GMT) of Thursday 7 February.
The 40-tonne space complex showered down over the foothills of the Andes in western Argentina.
In the final minutes of the spacecraft's descent, Soviet ground controllers tried to guide it towards the ocean but lacked the necessary fuel.
Most of the Salyut craft is believed to have burned up while re-entering the atmosphere but the Argentine defence ministry said in a statement that fragments weighing up to 1.5 tonnes reached the Earth.
According to the newspaper La Nacion , Delia Adela Guevara de Palazzo missed becoming the first victim of space debris by 2 metres when a 1.5 kilogram fragment of the space station landed in her back garden.
Sinking ships
TWELVE cargo ships sank last year in mysterious circumstances.
Another 11 were seriously damaged and about 200 sailors drowned.
All the ships were bulk carriers, and many were quite new.
The youngest ship was built only 12 years ago and the oldest was constructed in 1966.
Common factors uncovered so far include a history of carrying cargoes of coal and iron, and damage to the side of the ships' holds.
Lloyd's Register, which carries out structural surveys of ships to enable them to be insured, believes that structural failure was to blame for many of the losses and is mounting an urgent investigation to establish whether this is the case.
Investigators suspect that many of the cargoes of coal had a high sulphur content.
Moisture in the coal would quickly become converted into dilute sulphuric acid, and attack the sides of the hold.
Many of the ships also carried iron ore.
Because of its weight, iron has to be carried in alternate holds to avoid overloading the ship.
This pattern of irregular loading subjects the vessel to quite different stress from that exerted by an evenly distributed load of coal.
Suspect virus
SCIENTISTS believe they have new evidence to strengthen a suspected link between Epstein-Barr virus and Hodgkin's disease, a cancer of the lymph nodes.
Researchers in Birmingham and Aarhus in Denmark have found a protein called LMP, produced by the virus and thought to be involved in the cancer, in cells from about half of a sample of 84 tumours from patients with Hodgkin's disease.
The finding suggests that vaccination against the virus might prevent at least some cases of Hodgkin's disease, which attacks 1500 Britons yearly (The Lancet , vol 337, p 320).
Weighty problem
THE discovery of a simple lead fishing weight sheds doubt on Captain Cook's claim to be the first European to set foot on the shores of Terra Australis .
The weight, apparently made in France or Spain between 1235 and 1400 AD, is the oldest known European artefact to be found in Australia.
The weight was found buried in sediment from Fraser Island, off the coast of Queensland.
Geologist Bill Ward, of Griffith University in Brisbane, and colleagues from CSIRO were collecting samples in order to date the formation of the island's sand ridges.
The weight could have been brought to Australia by Portuguese sailors up to 400 years prior to Captain Cook's landing in Botany Bay in 1788, suggests Ward.
Sorry, no cyclotron
BRITAIN's government has decided not to give £6 million to a plan to build a cyclotron, a machine providing a controversial cancer therapy, at St Thomas's Hospital in London.
The proposed funding had been personally supported by Margaret Thatcher, the former prime minister.
A recent study showed that the neutron therapy delivered by the cyclotron was no more effective than conventional radiotherapy and more dangerous (This Week, 8 December 1990).
Birmingham Six
ALL scientific evidence has been dropped from the case against the Birmingham Six, the Court of Appeal in London was told last week.
The six men were convicted of the Birmingham pub bombs in 1975, partly on the evidence of Home Office forensic scientist Frank Skuse, who told the trial that the men had handled nitroglycerine.
In 1985, a television programme revealed that the solvent Skuse had used in the test for nitroglycerine, 1 per cent caustic soda, would also have produced a positive result if the men had handled nitrocellulose.
Skuse did not keep notes of his tests, and the Home Office retired him days after the TV programme.
At the men's first appeal hearing in 1987, Skuse said he used a 0.1 per cent solution of caustic soda, which would not dissolve nitrocellulose, but which would also reduce the sensitivity of the test for nitroglycerine.
In September 1990 a Home Office review concluded that there was no forensic evidence against the six.
Last week the Director of Public Prosecutions formally told the men's second appeal court that he was not going to rely on the scientific evidence.
The case against the six now depends on confessions.
Scientific evidence shows that some of these have been altered.
French link-up
THE French government has launched a programme to set up a nationwide telecommunications research network, in an effort to catch up with Britain's JANET system and Germany's DFN.
The French network, Renatel, will begin linking up existing research computer systems next year.
Its initial capacity will be 2 megabits per second.
But by the end of the decade Renatel is expected to reach 100 megabits per second — a capacity made possible by France's planned switch to fibre optics throughout its telephone system by the year 2000.
France is planning to develop Renatel in time to play a role within the EC's future multinational link-up of network systems.
Fish's future is a bit of a mouthful
A BULGING mouth is a promising sign for the keepers of this small tropical fish.
Only 30 of the species survive — and the bulge is a sign of pregnancy.
The female keeps her eggs and growing young in her mouth until they are ready for the outside world.
Haplochromis nyereri , named in honour of President Nyerere of Tanzania, was driven to extinction in its native Lake Victoria by the predatory Nile perch.
The future of the species now lies in the hands of Gordon Reid, who runs the aquariums at the Horniman Museum in South London.
Under Reid's care, the three sexually active females have all produced eggs.
Unfortunately, adult fish are prone to eat small fry once they have left the mother's mouth.
To avoid this, the mothers-to-be are moved to a separate tank.
But almost every attempt to move the fish has panicked them, including this one, into swallowing their eggs.
Reid has now persuaded one female to move without swallowing what could be the last of the species.
OBSERVER
David Austin
ALL OUR SNOWPLOUGHS HAVE BEEN BEATEN INTO SWORDS
David Phillips: warned that severe cuts would undermine scientific excellence
Hidden dangers: workers in the tower at JET were overcome by insulating gas
Wrong number: FM radio earned Edwin Armstrong (inset) millions of dollars
How green are my cups?
Bleak house: HIV thrives on the social deprivation and poverty endemic in the South African townships
Mars: is it mission impossible?
President Bush wants to see an American flag flying on the surface of Mars by 2019.
But no one knows what the biological effects would be on the astronauts undertaking such a trip
Christopher Joyce
RAIN or shine, for more than 2.5 billion years living organisms could always depend on one thing: gravity.
People have now changed that by shooting organisms into space.
Weightlessness apparently does no great harm, so President George Bush says that it is now time to send people to Mars.
Biologists, however, are not so sure.
At this week's meeting of the American Association for the Advancement of Science in Washington DC, scientists will report on efforts to prepare for such a mission.
Many argue that the biological effects of lengthy space travel are the biggest imponderable.
The fastest journey to Mars would take eight months each way, according to NASA's calculations.
Dennis Smith, vice-chancellor of the University of California at Irvine and a long-standing adviser to NASA, says the agency is nowhere near ready to send people to Mars.
‘We desperately need to do controlled experiments in space,’ says Smith, but NASA has not bothered.
That view is also shared by Noel Hinners, chief scientist for the company Martin Marietta.
‘We've known for 23 years what the potential problems were but done little to solve them,’ says Hinners.
The two scientists are preparing a study for the National Academy of Sciences to help NASA to organise its research.
NASA's budget for life sciences is less than $138 million a year, out of the agency's total annual spending of $14 billion.
Basic research takes up a tiny percentage of that life sciences budget.
‘But this is not just a plea for money,’ says Smith.
NASA needs a different attitude.
Most astronauts, for example, object to being scientists' guinea pigs.
Experiments in space also need a lot of ingenuity.
Animals are often poor models for humans; bones in rats are quite different from human bones.
Using humans would be unethical if, say, some astronauts did not exercise in space as a comparison and suffered injury as a result.
And biologists at universities are wary of spending years designing an experiment that may be postponed or lost in space.
The effects on astronauts of years in microgravity are the biggest mystery to researchers.
Joe Sharp, director of space research at NASA's Ames Research Laboratory explains: ‘Nobody has the foggiest idea of the effect of even 40 per cent or 20 per cent of gravity for extended periods.’
People will probably go anyway, Sharp adds, ‘but we may kill a few on the way, and we may get some wet noodles back.’
Why should we worry when spacefarers have not suffered any serious harm in so many years of space travel?
There are several answers to this question.
First, only a handful of people have spent more than a few weeks at a time in space.
Secondly, it has been impractical or impossible to gather data beyond vital signs and some blood and tissue samples.
What information has been gathered contains some subtle but ominous signs.
For example, cells in culture go awry aboard spacecraft.
One European experiment aboard a space shuttle showed that mammalian white blood cells divided very slowly.
These are cells that fight disease.
Identical cells were spun in a centrifuge on board to simulate gravity.
The cells divided normally.
This kind of experiment, besides sounding a cautionary note, needs repeating, says Smith.
Soviet and American tests with animals also found a decline in the production of red blood cells that seemed to worsen the longer the animals were in orbit.
The phenomenon is called ‘space anaemia’ and its cause is unknown.
Moreover, the human body is not simply a bag of cells; no one knows how years of microgravity would affect the complex interaction between cells in an organ and between organs themselves.
It is well known that a type of osteoporosis, a demineralisation of the bone, occurs when vertebrates spend time in microgravity.
Why and how it happens is unknown.
The demineralisation is not uniform throughout the skeleton, however.
Studies of people kept in bed for long periods show a redistribution of calcium in the skeleton.
Joan Vernikos, a life scientist at Ames, notes that the effects are not quite like osteoporosis on Earth; microgravity appears to change the levels of calcium in the plasma as well as the way minerals are deposited in the matrix of bone.
So simply making up the calcium that astronauts excrete may not ensure that it is laid down properly in new bone.
In rats that have been sent into space, the breaking strength of long bones decreased by between 30 and 50 per cent, says Richard Grindeland of Ames.
Another recent experiment in which rats with broken bones were sent into orbit showed that they are slower to heal in microgravity than on Earth.
Muscles atrophy in microgravity.
A host of countermeasures have been devised and a few tried, such as exercise bikes and treadmills to which crew members are strapped.
This machinery is bulky and does little to provide the ‘shock’ to bones of earthbound exercise that they apparently require for health.
Moreover, muscles need different types of exercise; exercise that contracts muscles but does not elongate them may not be adequate.
Engineers at NASA are working on several contraptions, such as inclined platforms with a variety of weights and pulleys, that demand a wider range of effort.
It is believed that organisms soon adapt to microgravity and establish a new biological steady state.
But rats sent into space by Wesley Hymer of Pennsylvania State University showed that readapting, whether on Mars or back on Earth, may not be straightforward.
Hymer's rats were unable to produce more than half the normal amount of pituitary growth hormone.
This hormone helps to control growth of muscle, bone and the cells of the immune system.
Hymer speculates that the pituitary cells package hormone molecules differently in weightless conditions than they do under gravity.
‘NASA's approach to this has been an engineer's solution,’ complains Smith.
‘Have them take calcium and run on the treadmill for four hours’, they say.
Our approach is to do basic research first.’
One such engineering solution to weightlessness that NASA is now contemplating is spinning the spacecraft to create at least some gravitational effect on the astronauts.
But a rotating craft exerts Coriolis forces, which work in a different direction from a centripetal force.
The physiological effect of these forces over months is unknown.
Microgravity is not the only threat to humans.
‘Radiation is one of the most pressing questions,’ says Steve Fogelman, deputy director of life sciences at NASA.
With the exception of the astronauts who visited the Moon, space travellers do not go beyond low-Earth orbit, where even the attenuated magnetic field and atmosphere provide some protection from radiation.
Deep space, however, is saturated with cosmic rays.
There is also a significant risk of a solar flare during a mission to Mars, which would increase the flux of protons in deep space to dangerous levels.
Large doses of protons will kill human cells.
Unshielded, an astronaut could be exposed to hundreds of rads during a solar flare.
Cosmic rays include heavy particles.
Those with high energies, such as iron, would penetrate the craft and bore through human cells.
Even if they do not damage people immediately, they could alter DNA and increase the risk of cancer.
One analysis by Stuart Nachtwey of NASA estimates the extra risk of cancer over an 18-month voyage, including 30 days on Mars, is only 1 per cent.
But he notes that three-quarters of the dose would be from heavy ions, whose biological effects are ‘poorly understood’.
No doubt an advertisement in the Los Angeles Times for volunteer space travellers would attract plenty of candidates.
But would any be psychologically or physically fit enough to spend two or three years in a weightless box?
Smith says the message from Soviet data is that one year is the most people can take.
For all his staying power, Yuri Romanenko, who endured 316 days in the Mir space station, spent almost half his waking hours on exercise machinery near the end of his trip.
On other flights, Soviet cosmonauts could work only about four hours a day.
The Soviets ended a mission early because of the crew's cardiovascular problems and, says Smith, a ‘lot of fighting’.
Scientists are beginning to argue that psychological fitness could be the biggest hurdle to a Mars mission.
Criticisms like those levelled during the meeting of the American Association for the Advancement of Science are having some effect.
‘There is a lot of room for improvement,’ acknowledges Fogelman.
The agency has recently outlined some ambitious plans, including a bigger budget for the Ames laboratory to carry out more experiments.
A powerful voice has been the influential panel of experts, led by Norman Augustine of Martin Marietta.
Its report to NASA and the White House on the future of the US space programme put life sciences at the top of the list of experiments for the US space station to be built by the end of the century.
That could reinstate plans to include a centrifuge.
The device is essential to determining the effects of varying degrees of gravity on everything from cells to people.
NASA has also begun to design an environment that can sustain a crew during a Mars mission.
Mike Shafto, one of NASA's human performance engineers at the Ames laboratory, is studying ‘analogue environments’ where people are isolated in a strange environment for long periods — such as the Antarctic, or inside Aquarius, a submersible in the Caribbean Sea near the Virgin Islands.
Shafto is also eliciting what he calls ‘war stories’ from former astronauts.
He will build mock-ups of a capsule for a Mars mission in the hope of identifying those errors ‘designed into’ the automation.
‘We can't afford any Chernobyls in space,’ says Shafto.
At the same time, he must avoid putting too much automation on board and turning the astronauts into bored passengers.
There are also various schemes in progress which aim to safeguard space travellers.
Water is a relatively good shield against cosmic rays.
For solar flares, a system of early warning satellites around the Sun could detect the first sign of trouble.
Sensors on the craft bound for Mars would detect light from a flare and the crew could take shelter in a shielded room before the slower protons arrive.
The report from the National Academy of Sciences will highlight microgravity, radiation and psychological fitness as the three main thrusts of an invigorated biology programme.
Whether it comes about depends as much on politicians as on scientists.
Only half in jest, Sharp and Vernikos at Ames suggest creating a research centre dedicated to space biology in the home district of an important politician.
‘It could be in Indiana,’ says Vernikos.
Indiana is home of Vice-President Dan Quayle, head of Bush's National Space Council.
Human biology could rule out long stays aboard a space station.
Weightlessness is a strain on the body
Life on Mars: NASA may be forced to revise its vision of a space colony, taking into account the frailty of the human body
Pipe dream?
Day trips to the Martian moons are looking increasingly unlikely
SCIENCE
Quantum theorists hit the boredom threshold
William Bown
THE BIGGEST problem facing theoretical particle physicists is boredom, claims an experimenter.
The calculations needed to refine quantum chromodynamics, the theory of the strong nuclear force between quarks, are so tedious that theorists cannot be bothered to do them.
Irwin Gabathuler of the University of Liverpool took theorists to task at a meeting at the Royal Society in London last month.
Accepting that the calculations can take years to complete, he asked: ‘If we can spend 10 years on an experiment, why can't you do these calculations?’
‘It is unfair to ask a graduate student to spend 10 years working out a small correction,’ replied James Stirling, a theorist from the University of Durham.
‘They are extremely time-consuming and tedious calculations.’
The difficulty for physicists is a real one.
Quantum chromodynamics is vital to their understanding of the Universe.
Experiments to test QCD give results with an error of about 10 per cent.
But the theory's current predictions may have the same error.
This could make it impossible to test QCD to better than 10 per cent, bringing the business of science to a halt.
The theorists' problems stem from the many different ways quarks have of getting from A to B. On the way, they can emit or absorb any number of gluons (particles which carry the strong nuclear force), so the possibilities are endless.
The probabilities of each route have to be calculated and summed to give the total probability.
And the more complex the route, the more complex the calculation.
The calculations are then used to predict experimental results.
Theorists carried out the calculations for the routes which involve emitting one or two gluons in the early 1980s.
Now they are trying routes involving three gluons.
According to Gabathuler, theorists in the West have been too busy chasing Nobel prizes.
But in the Soviet Union, physicists have been making steady progress for 10 years in speeding up QCD calculations.
In 1988, Andrei Kataev and two other physicists at the Institute of Nuclear Research in Moscow published the first calculation for routes which involve three gluons (Physics Letters B , vol 212, p 238).
The Soviet figure was 10 per cent higher than the previous calculation — much more than expected — and led to a flurry of checks on the accuracy of similar calculations.
Last year, the Soviets found an error.
Theorists all over the world had been redoing their sums for nothing.
Stirling is not surprised the error took so long to find.
‘Nobody could say it was right or wrong because nobody had any insight into what the answer should be,’ he says.
A new calculation will soon be published in Physical Review Letters by Levan Surguladzo, also of the Institute for Nuclear Research, and Mark Samuel of Oklahoma State University.
The calculation is based on the techniques developed by Kataev in the Soviet Union and implemented on American computers.
If correct — and Samuel says a second Soviet team has independently arrived at the same figure — it lowers the previous estimate by only one per cent.
According to Samuel, the calculation took about three months to complete.
But summing the routes involving four gluons is a long — and boring — problem.
‘It would be monstrous,’ says Samuel.
‘It would take an order of magnitude longer.’
Nevertheless, says Gabathuler, those calculations will become necessary this summer, when the Hera electron-proton collider opens in Germany.
He expects the experimental error to fall to 1 per cent.
Stirling has yet to be convinced even by Samuel's figure.
‘I would like to believe this number,’ says Stirling, ‘but it really needs an independent check.’
But who wants to give up a year or more to check someone else's work?
Summing up the ninety-eight ways from A to B
QUANTUM physicists are trying to calculate the probability that a particle goes from A to B. This probability is the sum of the probabilities of each of the possible routes it could take.
One of these is shown in the accompanying Feynman diagram.
A quark starts at A and later emits a gluon.
It re-absorbs the gluon, emits another, then reaches B.
In the diagram, there are three ‘junctions’ at which the lines describing the motion of the particles split or join.
It is one of the 98 three-junction routes calculated by Surguladzo and Samuel.
Calculating the probability of the quark following this path from A to B involves integration, which is always difficult for computers.
Worse, many of the diagrams have to be grouped with other diagrams to cancel out infinities.
The calculations are aimed at evaluating an expression like Aa+Ba 2 + Ca 3 +…; where a is the probability that a quark emits a gluon, the parameter in quantum chromodynamics which experiments measure.
Aa is the sum of the probabilities of all the routes with one junction, Ba 2 of the routes with two, Ca 3 of the routes with three and so on.
By calculating A, B and C it is possible to estimate the value which experiments should find.
But the estimates could be wrecked by the infinite number of remaining terms.
Their contribution could be large, or small.
The best experimental value for the expression, from the Petra collider in Hamburg, is 0.0527 with an error of 0.005, or 9 per cent.
The approximation, based on one-junction and two-junction routes only in 0.155 (error 0.014, 9 per cent).
Kataev's calculation gave 0.139 (error 0.01, 7 per cent).
Samuel got 0.159 (error 0.015, 9 per cent).
Because the calculations have as big an error as the experiments, there is little point in doing more accurate experiments.
The recent calculations were done only for the collision of an electron and a positron, a collision common in particle accelerators.
For other collisions the sums will have to be done all over again.
How much does a smell weigh?
Simon Hadlington
A TEAM of Japanese scientists appears to have discovered how to weigh smells.
The technique involves the use of a ‘lipid bilayer’, a component of a cell membrane, to absorb an ‘odorous’ substance, then weighing the combination with a delicate balance.
Not only does the technique have potential in the perfume industry, but it may provide clues about how we perceive smells.
Yoshio Okahata and his colleagues, at the Tokyo Institute of Technology, have built an ingenious piece of equipment, based on a quartz crystal microbalance, which can weigh nanogram quantities of lipids.
They were moved to attempt such a weighing feat by the observation that odorous molecules are generally more soluble in organic solvents, such as fats and oils, than in water (Bulletin of the Chemical Society of Japan , vol 63, p 3082).
Scientists believe that molecules of an odorous substance meeting the membranes of specialised olfactory cells somehow trigger electrical signals to the brain, and this is the foundation of smell perception.
The precise molecular mechanisms involved remain largely unknown but they may involve lipid bilayers, which form the backbone of cell membranes and dissolve hydrophobic substances.
Okahata and his colleagues coated their balance with an artificial lipid bilayer, enclosed the balance in a sealed chamber and injected vapours of a variety of odorous substances, including commercial perfumes.
When, for example, they injected into the chamber a saturated vapour of a substance called beta-ionine, the balance registered 760 nanograms.
This was the total amount of beta-ionine that the lipid bilayer absorbed.
The researchers obtained similar results when they repeated their experiment, this time coating the balance with membranes obtained from human olfactory cells.
They also found that when humans judged an odour to be more intense, the microbalance registered a greater weight.
Okahata and his colleagues suggest that the first step in our perception of smells occurs when odorous substances are absorbed by the lipid bilayer of a cell.
At this stage, no special receptor proteins are involved.
They say that the more soluble a substance is in the lipid matrix of olfactory cells, the more intense the smell we perceive.
According to Okahata and his colleagues, the lipid-coated quartz crystal microbalance is both physically stable and reusable.
‘It will provide a new sensor system to determine the intensity of odorants and perfumes,’ they say.
Ice telescope could detect cosmic neutrinos
SCIENTISTS in the US are planning to turn a cubic kilometre of the Antarctic ice sheet into a giant ‘neutrino telescope’.
Neutrinos are elusive particles which interact with matter so rarely that large bodies of material are often needed to detect them.
The neutrino telescope will detect light emitted by high-energy neutrinos as they interact with the ice.
‘It's a fantastic way to do neutrino astronomy, much cheaper than anything anyone ever dreamed of,’ says Francis Halzen of the University of Wisconsin.
‘We are looking for things like active galactic nuclei which put out tremendous energies in neutrinos.’
Halzen's detector will be a collaboration between the University of Wisconsin and the University of California at Irvine and Berkeley.
It will use a similar technique to that currently being used by a Japanese group, which is monitoring solar neutrino reactions with water.
Halzen's proposed detector will observe neutrinos from cosmic sources with energies more than 1000 times greater.
The neutrino telescope will use phototubes to watch for light produced by neutrino reactions.
The Japanese suspend their phototubes in the water.
Halzen plans to put his phototubes into holes melted in the ice sheet, then let them freeze in place.
He says the ice will cool the phototubes, making them more sensitive, and also provide an environment free of natural radioactivity.
Halzen has carried out preliminary tests on the Greenland ice sheet.
The National Science Foundation has provided him with funds to drill 10 test holes near the American station at the South Pole.
One of his goals will be to map the optical quality of the ice so that he can be sure that light from neutrino reactions will indeed reach the detectors.
This will involve measuring how deep below the surface ice bubbles go.
If the Wisconsin group can demonstrate that their scheme works, they could go back every year to make more holes and enlarge the detector.
An array a square kilometre in size should see neutrino sources if there are any, Halzen says.
Well-fed baboon babies become best reproducers
A STUDY of baboons in Kenya shows that the old adage ‘you are what you eat’ is truer than might have been imagined.
Stuart Altman, a primatologist at the University of Chicago, has found that the quality of the nutrition a young baboon receives determines it later reproductive success very closely.
Such success is a measure of Darwinian fitness.
‘By 70 weeks, before the completion of weaning, a female baboon's lifetime fitness is largely established,’ says Altman.
‘Yet, at that age they are still nursed, still sleep in their mother's arms, are still three years away from puberty, and about four years from when they will conceive their first offspring.’
According to Altman, this is ‘a very surprising result’(Proceedings of the National Academy of Sciences , vol 88, p 420).
Altman measured the food intake of a troop of baboons aged between 30 and 70 weeks, in Amboseli National Park on the border between Kenya and Tanzania.
He found that of all the nutrient criteria, the two that were best able to predict reproductive success were protein intake and energy intake.
Altman focused on females in the troop, because it is easier to monitor their reproductive success reliably than that of males.
Reproductive success is measured in terms of the length of the reproductive span, the number of offspring produced, and the survival of those offspring to one year of age.
Altman found that even though all the animals fell below ‘optimum diet’, given the food resources available, those that came closest did best in the end.
This is not a case of individuals that are badly nourished failing to reproduce effectively, he says.
Instead, reproductive success is graded and directly related to the quality of nutrition within a normal range.
‘Do some animals have higher fitness because they eat better, or do they eat better because they are more fit?’ asks Altman.
‘I suspect that both are true,’ he says.
There are, however, no good data to indicate whether reproductively fit mothers produce reproductively fit offspring.
Even if scientists could demonstrate such a correlation, it would still be difficult to be certain whether the effect were the result of social or genetic transmission.
Experiments of the most Draconian nature wold be necessary to untangle these possibilities, says Altman, ‘and you wouldn't be able to do them in a natural setting like Amboseli’.
Roger Lewin, Washington DC
Supernova ‘ring’ helps to size up the Universe
Marcus Chown
ASTRONOMERS now have a far better idea of the size of the Universe, thanks to observations made by the Hubble Space Telescope.
They have used a ring of gas expanding from the remnant of Supernova 1987A to determine the distance to the Large Magellanic Cloud (LMC), a ‘satellite’ galaxy of our own Milky Way.
The distance estimate is important because astronomers use it as the first rung in the distance ladder they extend across the Universe.
Surprisingly, astronomers have only a vague idea of how big the Universe really is because of uncertainties in their ‘cosmological distance scale’.
The new estimate of the distance to the LMC will enable them to measure the size of the Universe to within 10 to 15 per cent.
An international team of astronomers estimated that the distance to Supernova 1987A, and therefore the LMC, is 169 000 light years, a figure which they say is accurate to within 5 per cent.
They obtained it using observations made by the Hubble Space Telescope and the International Ultraviolet Explorer (IUE) satellite.
According to Nino Panagia of the Space Telescope Science Institute in Baltimore: ‘This is an achievement of great importance because the distance to the Large Magellanic Cloud is an essential step to the calibration of the cosmological distance scale.’
The ring of gas in Supernova 1987A was not created in the explosion but ejected from the progenitor star a few thousand years before the explosion.
At that time, the star was passing through a phase during which it was a red supergiant, a class of star which commonly ejects a shell of gas in the form of a ‘stellar wind’.
Later, the star evolved into a blue supergiant, just a few thousand years before the explosion.
This produced a faster stellar wind which compressed the gas.
For some reason, this compression was more efficient along an equatorial belt, making a ring-like structure.
Astronomers observed the gas ring for the first time last August using the Space Telescope's Faint Object Camera.
They were able to see the ring's structure because of the Space Telescope's high resolution — that is, its ability to discern very fine detail.
The ring glows because it was heated to more than 20 000 °C by radiation produced by the supernova blast.
A first step in calculating its distance was a measurement of the ring's ‘angular diameter’, which turned out to be 1.66 arcseconds.
The second step was to determine its actual diameter, in light years.
To do this, the astronomers analysed observations of Supernova 1987A made by IUE over a three-year period.
The images from the Space Telescope had revealed that the gas ring was inclined at an angle of 43 degrees to our line of sight.
It turned out that IUE detected ultraviolet light from the edge of the ring that is tilted towards us 80 days after Supernova 1987A erupted.
On the other hand, light from the edge tilted away from the Earth arrived 420 days after the explosion.
It was a simple matter to determine from this time delay and the ring's inclination that its real diameter was 1.37 light years.
From this and the angular diameter, the astronomers calculated that Supernova 1987A and the LMC are 169 000 light years away.
According to Panagia, it should now be possible to estimate the Hubble constant to within 10 to 15 per cent(see ‘Sizing up the Universe’,New Scientist , 27 March 1986).
The Hubble constant is a measure of how fast the Universe is expanding, and the age and the size of the Universe are related to it.
A tunnel driven through the Galaxy's fog
AN ASTRONOMER in the US has discovered a huge tunnel through our Galaxy.
The tunnel, which is virtually free of neutral, or non-ionised, gas, is at least 1000 light years long and stretches from the Sun to the star Beta Canis Majoris and beyond.
It may allow astronomers to observe distant objects that emit extreme ultraviolet light, a type of radiation which is usually absorbed by intervening neutral gas.
Barry Welsh of the Center for Extreme Ultraviolet Astrophysics in Berkeley, California, discovered the tunnel while extending the work of two other American astronomers: Priscilla Frisch and Donald York of the University of Chicago.
In 1983, they discovered that the Sun is in a void which contains very little neutral gas (Astrophysical Journal (Letters), vol 271, p L59).
The void, dubbed the ‘Local Bubble’, extends about 100 light years in most directions.
But it reaches even farther in the direction of Beta Canis Majoris, about 650 light years away.
Now Welsh has determined that this extension is in fact a huge tunnel that stretches well beyond the star itself.
Welsh used the 1.5-metre telescope at Cerro Tololo Inter-American Observatory in Chile.
He determined the amount of neutral gas lying in front of Beta Canis Majoris and in front of 10 other stars in a similar direction.
All the stars are at different distances from the Sun.
Specifically, Welsh measured the amount of neutral sodium gas in front of each star.
Neutral sodium absorbs orange light — at wavelengths of 5890 and 5896 angstroms — from stars that lie behind it.
Astronomers can estimate how much neutral sodium gas lies in front of the star from the precise amount of light absorbed at these wavelengths.
This, in turn, reveals just how much neutral gas, mostly hydrogen, there is.
In front of most of the stars that were less than 1000 light years away Welsh found no neutral sodium gas.
But he did find sodium absorption in a star 1100 light years from the Sun, and in stars that were more distant than this.
Welsh concludes that the neutral-free gas tunnel to Beta Canis Majoris is at least 1000 light years long, with the Sun located near one end.
He estimates that the tunnel is at least 160 light years across.
Although the tunnel lacks much neutral gas, he says, it may contain some ionised gas.
The tunnel lies in the disc of the Milky Way.
Imagine the Sun at the centre of a giant clock, with 6 o'clock as the direction of the Galactic centre.
The tunnel extends from the Sun in the direction of 10 o'clock.
Welsh speculates that supernovae, or violently exploding stars, may have created the long tunnel.
The blast wave of a supernova is capable of ionising, or even sweeping clear, the neutral gas from a region as large as a few hundred light years across.
However, in order to create the 1000-light-year tunnel, says Welsh, several supernovae would have been needed.
The tunnel is more than a mere curiosity to astronomers.
Regions devoid of neutral gas are vital for astronomers who hope to observe stars at extreme ultraviolet wavelengths.
Until recently, such wavelengths have been ignored because neutral hydrogen absorbs it in the same way fog absorbs visible light.
Extreme ultraviolet radiation arises from very hot objects and has a wavelength between 100 and 912 angstroms, shorter than that of normal ultraviolet radiation (see ‘A new wave in astronomy’,New Scientist , 30 September 1989).
Later this year, NASA will launch the Extreme Ultraviolet Explorer satellite, and Welsh believes that many stars in the tunnel will be detected.
Welsh will be publishing his results in the 1 June issue of The Astrophysical Journal .
Ken Croswell, Boston 
QCD confusion: the Soviet three-gluon calculation (bottom) was wrong.
Now it has been corrected (top)
There are 98 ways that a quark can travel between two points if it interacts three times with gluons.
This is one of them
The reproductive success of a female baboon is fixed when it is still being nursed
Ring of bright gas: by comparing the real diameter of the ring to its apparent diameter, astronomers have calculated its distance from Earth
TECHNOLOGY
Ceramic chip could write off discs
Graeme O'Neill, Melbourne
AN AUSTRALIAN company has launched an erasable computer memory chip that retains data when its power source is switched off.
The chip could revolutionise the design of computers and other electronic devices by doing away with the bulky magnetic disc memories that are currently used to store data permanently.
Current computers rely on a selection of memory devices.
These include chips known as read-only memories or ROMs that store preprogrammed data without power but cannot be erased, and instantly erasable chips that require constant power, known as random-access memories or RAMs.
To store more data and programs when the power is off, most computers use magnetic discs.
The new chip is known as a ferroelectric random-access memory or FRAM.
If it proves as successful as its developer, Ramtron, claims, it could replace all other types of data storage.
Ross Lyndon-James of Ramtron says: ‘All other memory devices have either inflexible static memories, or require information to be shuffled in and out of the chip.’
Data stored in RAM chips while the computer is on must be moved to a disc before the power is switched off.
‘That requires a large software overhead,’ he says.
‘The software must contain instructions to handle all the communication between the chip and the disc.’
Such data management operations take up a lot of the computer's operating time; in a computer based on FRAMs they would be reduced to a minimum.
‘You don't have to switch on the computer, load the program and recall a file from disc — you just switch on and it's there,’ Lyndon-James says.
Programs stored in normal ROMs cannot be changed.
Data in erasable programmable read-only memory (EPROM) chips can be updated many times, but only by removing the chip, erasing the original program with ultraviolet light, and then reprogramming.
Ramtron claims that mass-storage FRAMs could make hard and floppy discs unnecessary in smaller computers, especially laptops, making them more compact.
Batteries will last longer because they will not need to drive the discs or maintain data when the mains current has been switched off.
In current FRAMs, Ramtron have deposited a layer of a ferroelectric ceramic on top of a conventional silicon memory chip known as a static RAM, or SRAM.
Ramtron use a ceramic called PZT, which consists mainly of lead, zircon and titanium oxides, with traces of other elements.
The ceramic's crystals polarise in one of two alternate states when energised by an electric field.
SRAMs store data by circulating a current through one of two circuits of transistors.
A small amount of power is needed — such as from a battery — to keep the current moving.
In a FRAM chip based on SRAM, the current provides an electric field to switch the state of the ceramic.
When the power is switched off, the ceramic above each memory cell is switched to a state representing the digital ‘one’ or ‘zero’of the cell.
The polarity of the crystal is preserved for at least a year and, in theory, as long as 10 years after power to the cell is switched off.
In the more widely used dynamic RAM chips, or DRAMs, the bits of digital code are represented by the presence or absence of static electric charge in a capacitor on the chip's surface.
The charge is let in or out of each capacitor by its own switch but the charge continually drains away so it needs to be constantly topped up from the mains or battery.
But the capacitors on DRAM chips take up a large amount of space; the ceramic of an FRAM chip is capable of storing data in a much smaller space.
Ramtron aims in future to actually replace the capacitor with a small amount of ceramic embedded in the chip.
The ferroelectric effect exploited in the FRAM chip was discovered in the 1920s.
An American electronics engineer, George Rohrer, first conceived the idea of a ferro-electric memory chip in the 1960s, but could not find commercial backing for its development.
However, Ramtron formed a partnership with Rohrer in 1983.
After problems with the original ferro-electric material, Ramtron released a 4096-bit FRAM chip for evaluation by industry engineers late last year.
At the end of last month the company launched a commercial version of the chip.
The first chip is very limited in capacity but Ramtron announced that a prototype 256-kilobit chip is already in limited production at its factory in Colorado Springs in the US, and will be released for evaluation by mid-1991.
A commercial version will follow early in 1992.
Lyndon-James says design is well advanced on a 4-megabit chip — the size of the largest DRAM chip currently on the market — and there are plans for 16-megabit chip.
Yeast brews up artificial blood
A SUBSIDIARY of the brewing company Bass is putting its excess brewer's yeast to good use.
Using genetic engineering, the company Delta Biotechnology has inserted into yeast the human gene for the manufacture of haemoglobin, the protein that gives blood its scarlet colour and transports oxygen to tissues around the body.
The yeast then produces haemoglobin which could be used, in a suitable formulation, to treat people seriously injured in accidents and assist in the treatment of heart disease and tumours.
Haemoglobin alone is toxic so the goal is to bind the protein to a carrier to make a formulation that is safe and effective.
Peter Senior, the technical director of Delta Biotechnology, stresses that haemoglobin can never do the same job as donated blood transfused into patients.
But formulations, which may survive in the body for up to 24 hours, could be invaluable for people whose tissue needs urgent replenishment with oxygen because of serious blood loss.
The company says that the haemoglobin it harvests from the yeast is cheap, safe and identical to natural haemoglobins.
Senior expects Delta's haemoglobin to cost around 50 pence per gram, ten times less than the cost of purifying it from red blood cells, the main existing source.
Moreover, he says, Delta's haemoglobin is guaranteed to be free of the viruses that could contaminate the blood-derived product, and of any residual matter from the blood cells that might trigger allergic reactions in patients.
Delta is negotiating commercial agreements with big health care companies around the world who are seeking to bind haemoglobin into formulations.
One idea is to encapsulate the haemoglobin in spherical structures of fatty material called liposomes, or to cross-link it with chemical agents.
The oxygen-carrying formulations could also be invaluable for the treatment of tumours and cardiovascular disease.
The small size of the formulations compared to red blood cells means they could penetrate solid tumours, making them more susceptible to chemotherapy and radiotherapy, and they could reach inaccessible parts of the heart during surgery.
Harold Gunson, the national director of Britain's Blood Transfusion Service, agreed that a formulation that survives in the body for up to 24 hours would be useful in trauma, angioplasty and stroke treatment.
But, he stressed that donated blood, which lasts for up to 100 days in a patient, can never be replaced by formulations.
Andy Coghlan
Mechanical arm guides the surgeon's hand
Anne Davies
SURGEONS in the US have pioneered the use of a mechanical arm laden with sensors to track the positions of their surgical instruments while they operate on patients' brains.
The sensor arm allows them to watch their manoeuvres live in three dimensions on a video monitor.
James Zinreich and colleagues at Johns Hopkins Medical Institution in Baltimore have used the sensor arm successfully in five operations to remove deep-seated brain tumours, and they are now adapting the system for sinus surgery.
In the US, 38 million people suffer chronic inflammation of the nasal cavity and paranasal sinuses.
Although sinus surgery using a fibre-optic endoscope inserted through the nostril to see inside the sinus has become commonplace, the small size of the endoscope restricts the surgeon's field of view to between 1 and 2 centimetres.
According to Zinreich, the mechanical sensor arm enables the surgeon to see the exact relationship of the sinuses to critical areas such as the eye socket and the brain.
‘Those are the areas that are very often penetrated and therefore serious complications result,’ he said.
The arm, known as a ‘surgical localiser’ has six joints with a sensor on each joint.
The surgeon mounts it on the side of the operating table and fixes to the end a surgical instrument or a biopsy needle for taking samples of tissue.
The arm's sensors are connected to an image-processing computer.
The computer screen displays an image of the patient's skull taken before the operation using computerised tomography, a technique which combines X-ray pictures to obtain a three-dimensional image of the interior of the body.
The position of the surgical instrument in the real skull is determined by the sensors in the mechanical arm.
This position is then superimposed onto the screen image to guide the instrument during the operation.
To align the instrument with the computer image of the skull, specific anatomical reference points on the patient's face, such as the corner of the mouth, are aligned to corresponding points on the three-dimensional image.
‘We choose five superficial structures, easily recognisable and reproducible, on both the subject and the three-dimensional reconstruction,’ Zinreich explained.
Zinreich believes that the sensor significantly improves the precision with which surgeons can excise brain tumours, and enables them to operate through smaller flaps in the skull.
‘After tumour removal, you can actually check the boundaries of the site and see the extent of removal,’ he comments.
Britain abandons power from hot rocks
TECHNICAL problems and high projected costs have persuaded the British government to pull the plug on any immediate plans to tap the heat of the Earth for power generation.
The Department of Energy has announced that support for the UK's current 14-year research and development programme into geothermal energy will be scaled down dramatically, involving the loss of 50 to 60 jobs.
The UK has extensive geothermal resources and its research into this form of renewable energy is the most advanced in the world.
According to an assessment by the department's Energy Technology Support Unit the cost of electricity from this source could be anything between 12 and 60 pence per kilowatt-hour.
This would make it between 4 and 15 times as expensive as power from either coal-fired power stations or the new breed of combined-cycle gas turbine plants.
To date, Britain has spent over £40 million on geothermal R&D.
Over the next three years the government plans to commit just £3.3 million on this field and to redirect British research towards collaboration with other European countries.
An international consortium involving RTZ Consultants and French and German companies is being formed.
For the first two years, the consortium will undertake a feasibility study into the best location for a geothermal demonstration project.
Since 1977 the main focus of British R&D into geothermal energy has been its so-called hot dry rocks project (HDR), based at a disused granite mine in Cornwall.
Drilling and circulation experiments have been carried out there by the Camborne School of Mines.
The aim of this work was to see if it would be feasible to drill 6 kilometres down to reach rock strata containing water at 200 °C, hot enough to raise steam for power generation.
Extracting this heat involves pumping water down an injection borehole drilled from the surface, circulating it through pre-existing fractures, or joints, in the rock, which have been enlarged using high-pressure liquid, and bringing it back to the surface via a second borehole.
The joints behave as the conduits of a subsurface heat exchanger.
To test this technique, engineers first drilled boreholes 2 kilometres deep and 300 metres apart to form an underground reservoir of water heated by the hot dry rocks.
During the tests, however, a large amount of the water from the reservoir was lost and the pressure needed to force the hot water back to the surface was higher than expected.
It proved impossible to get within a factor of 10 of the sustainable production flow of 75 litres per second of hot water.
A second test reservoir needed less pressure but also retrieved less heat.
The temperature of the water at the top of the production borehole fell from 80 to 55 °C between 1985 and 1988.
A ‘short circuit’ route for the water developed which cooled more rapidly than the rest of the reservoir, reducing its effectiveness.
Subsequently a steel pipe was used to block off the short-circuit.
The UK's best geothermal resources are located in southwest and northeast England.
In theory there is sufficient geothermal energy under Cornwall to meet most of the electricity requirements of southern England.
Outside Britain, the main interest in HDR R&D is in the US, Japan, Sweden, France and Germany.
Roger Milne
Atoms pop up to get switched on
Daniel Clery
GERMAN researchers have found a technique to ‘switch on’ small groups of atoms on the surface of a crystal.
If those same atoms can also be switched off, they could form the digital ‘ones’ and ‘zeros’of an atomic-scale data storage device that could store the complete works of Shakespeare on an area less than 1 hundredth of a millimetre across.
The technique was made possible by the use of the scanning tunnelling microscope which allows researchers to look at individual atoms on a surface.
Teams around the world are now using the STM to modify or move atoms.
Scientists at IBM's Almaden research centre in San Jose, California, gave a dramatic example of this last spring when they released a picture of 35 xenon atoms arranged to spell their company's name on the surface of a nickel crystal.
The letters were just 5 millionths of a millimetre high.
Don Eigler, one of the IBM researchers, said last week that he was sceptical about IBM's technique being useful for data storage because of the length of time it takes to move atoms.
‘If you gave me one square inch of surface,’ he said, ‘I could store all the data on all the computer discs produced by IBM last year.
But it would take about ten times the age of the Universe to write it all.’
More recently, researchers at the Hitachi Central Laboratories in Tokyo wrote ‘Peace 91’ in letters less than 1.5 millionths of a millimetre high (Technology, 26 January).
The Hitachi team selectively removed atoms from the surface of a molybdenum disulphide crystal by bringing the fine tip of the STM very close to a particular atom and subjecting it to a pulse of electric field.
This caused the atom to be ejected from the crystal surface.
Both of these techniques involve moving or removing atoms on the surface of the crystal.
But Harald Fuchs of the Polymer Research Laboratory of BASF in Ludwigshafen and Thomas Schimmel from the University of Bayreuth believe that their technique does not alter the structure of the crystal surface.
They found they could switch on particular groups of atoms with a pulse of electricity and they stayed switched on for up to two days.
When imaging a surface, researchers bring the fine tip of the STM down to about 1 millionth of a millimetre away from the sample.
They put a voltage between the tip and the sample so that an electric current will flow.
The current can jump across the gap because the tip is so close.
They then scan the tip across the sample, keeping the current constant by moving the tip up and down very slightly.
These up and down movements as the tip scans over the surface provide topographical information which a computer reassembles into an image of the sample.
The German researchers took a crystal of tungsten diselenide and cleaved it to expose a layer of tungsten atoms which are arranged in a triangular pattern.
As they passed an STM tip over the surface they applied short bursts of alternating voltage, a few milliseconds long and slightly more than 1.5 volts in amplitude, over particular atoms.
When the researchers re-imaged the same area, they found they had created small groups of three atoms, arranged in a triangle, which appeared higher than their neighbours.
The researchers found that these ‘trimmer’ structures remained intact when the area was imaged again two days later.
If stable over a longer period, the presence or absence of trimer structures at certain positions on a surface could act as bits of digital data.
Fuchs believes that the structures could be eraseable, for example by heating.
Besides eraseability, a storage device based on trimers would have other advantages over previous techniques of atomic manipulation.
The IBM team spent 22 hours writing their letters, but the Germans take only milliseconds to create each trimer.
Fuchs believes the time could be brought down to nanoseconds.
The IBM technique required temperatures close to absolute zero.
Free genes for sweet potatoes
ONE of the most important food crops in developing countries, the sweet potato, may soon come in an insect-resistant variety — and at a reasonable cost.
A team of researchers from Britain and Peru are attempting to transfer a gene from cowpeas into sweet potatoes.
For this biotechnology project — one of the minority that may truly benefit poorer countries — one of the collaborators, the Agricultural Genetics Company (AGC) of Cambridge, has agreed to waive its patent rights to the cowpea gene for use in sweet potatoes.
This gesture is applauded by John Whitcombe, associate manager of the Overseas Development Administration's plant sciences programme.
AGC waiving its rights means that anyone will be able to use the sweet potato technology.
The sweet potato's susceptibility to insect damage can slash yields by up to 80 per cent.
With 98 per cent of the world's sweet potato crop grown in developing regions and consumed locally, the key beneficiaries of the modified potatoes will be the farmers and consumers of the developing world.
The growers will benefit from increased yields, lower input costs (in terms of labour, insecticides and equipment) and increased productivity and profitability.
With funds from the ODA, scientists at AGC, the University of Durham, and the International Potato Centre (CIP) in Lima, will attempt to incorporate AGC's insect resistance gene, the cowpea trypsin inhibitor (CpTI) into the sweet potato.
The gene prevents the breakdown of certain proteins in the digestive system of insects so that insect larvae are deprived of essential nutrients and die.
‘The research…will contribute to overcoming the effects of sweet potato weevil which has become an insurmountable problem to control with conventional approaches,’ says the company.
Chemical control of sweet potato pests has had varying degrees of success, with many chemicals being too expensive.
And conventional plant breeding has not been successful in producing high-yielding insect-resistant crops.
Champions of the rights of developing nations welcomed AGC's offer with the caveat that the patent system still favours the rights of richer nations and penalises poor nations.
David Cooper of Genetic Resources Action International, based in Barcelona, pointed out that it was farmers in West Africa who first noted the resistance of some varieties of cowpea to insects and who nurtured superior varieties that carried this characteristic.
‘Decisions about who has access to the gene should not be in private hands in the first place,’ he said.
‘We are worried about patent law restricting access to the gene itself, because the resource existed before.’
Mike Ward and Andy Coghlan
Reinventing the wheel
A BICYCLE wheel with just three spokes will make its professional debut this spring.
The wheel is more aerodynamic than traditional spoked wheels and avoids the problem that dogs many professional cyclists who use solid disc wheels — that of being blown sidewards by crosswinds.
The wheel was designed by a team from the US chemical company Du Pont.
Mark Hopkins, one of the Du Pont team and an amateur bicycle racer, says that the traditional wire-spoked wheel has two drawbacks that drain energy from the rider: the wheel flexes, and its spokes produce wind resistance.
‘A conventional round spoke — and the typical wheel has 24 or 36 — has a drag coefficient of 1.2’, he says.
‘But if you make a spoke of equal thickness in a sleek airfoil configuration, you can reduce the drag coefficient to about 0.06.’
Du Pont says that its aerodynamic efficiency can cut about 10 minutes of biking time in a 180-kilometre run.
The new wheel is made from orientated fibres of carbon, glass and Kevlar, embedded in epoxy resin.
The ceramic that never forgets: an electric field can switch each crystal cell of a PZT ceramic between two stable states
All-seeing arm: tracks operations on a video image
Boring work: heat from the Earth proved expensive
Standing up to earthquakes
An earthquake destroys buildings by shaking them to pieces.
Engineers are finding ways to keep them standing when the ground moves
Christopher Joyce
NATURE doesn't negotiate.
This axiom is worth remembering as we enter the second year of the International Decade for Natural Disaster Reduction, declared by the UN.
Among the non-negotiable events to be expected during the decade are earthquakes.
As an engineer in California observed in the aftermath of the state's 1989 quake: ‘Earthquakes don't kill people.
Buildings do.’
Nobody wants to spend extra money to prepare for something that may never happen and earthquake engineering has long been neglected.
Now that attitude is changing, most obviously on the West Coast, which has more money than other earthquake zones to experiment with new designs and materials.
The value of well-built buildings is not lost on Californians.
The Loma Prieta earthquake of 17 October 1989 in San Francisco reached a magnitude of 7.1 on the Richter scale and killed 62 people; the 1988 quake in Armenia, with a magnitude of 6.7, killed about 25 000.
Earthquake engineers may not have needed to be reminded that building design can make all the difference between life and death, but the Californian quake has increased public interest in earthquake-resistant designs.
These range from the mundane, such as reinforcing masonry walls with steel beams to strengthen them, to the exotic, such as supporting an entire building on rubber so that the structure ‘floats’ in isolation.
To understand how engineers are preparing for earthquakes, one needs to know a little about how buildings behave when shaken.
Place several dishes of jelly on a table and rhythmically shake the edge.
The jellies will shake too, but some more than others.
Each jelly will sway back and forth in a characteristic time, or period, that depends predominantly on its height and consistency.
The reciprocal of the period, measured in seconds, is the pudding's natural frequency of vibration, which is measured in cycles of movement per second, or hertz.
Those jellies whose natural frequency matches the frequency of the shakes will oscillate with the greatest amplitude — the two frequencies are said to be ‘in resonance’.
Buildings behave in much the same way.
A typical building of 10 storeys will sway back and forth in a period of 1 second, giving it a natural frequency of 1 cycle per second or 1 hertz.
As a rule of thumb, every storey makes a difference of one-tenth of a second to a building's period.
So a 20-storey building has a period of about 2 seconds, a natural frequency of 0.5 hertz; a five-storey building has a period of half a second, a natural frequency of 2 hertz.
Buildings above 20 storeys fare relatively well in earthquakes.
The most powerful vibrations from a quake range from 0.5 hertz to about 5 hertz.
As a typical 20-storey building has a frequency of 0.5 hertz, and taller buildings have even lower frequencies, they are likely to be beyond the range of resonance.
In addition, taller buildings flex more than short ones, by virtue of their height and because their frames are built to bend with the wind.
Steel columns and beams, and concrete walls reinforced with steel rods, make the frames flexible — and even ductile, so that they can deform without snapping in the wind.
Flexibility has its costs, however.
Large deflections can help to loosen non-structural cladding and shake up the building's contents like dice in a box.
Buildings of less than 20 storeys are the most susceptible to resonance.
Those that resonate with the strongest vibrations will suffer the most damage.
To understand resonance, think of a child on a swing.
The arc described by the child has its own cycle.
If you apply force just as the swing attains its maximum height and begins to drop, you add energy most efficiently, in synchrony with the swing's cycle.
Too soon, and you dissipate it.
So if the period of a quake's main vibrations are 0.5 seconds, five-storey structures will be particularly vulnerable.
In such cases, the building amplifies the motion it experiences at ground level, and the horizontal accelerations at the upper storeys can be several times those at the base.
Urban buildings set in rows present another threat to themselves and their neighbours.
Take a 15-storey building abutting a three-storey structure.
The periods differ, so the buildings vibrate with different frequencies and slap against each other.
Masonry may crack or crumble on the wall of the taller building where the roof of the shorter one abuts.
This happened in Santa Cruz during the Loma Prieta quake, where many adjoining buildings were of different heights.
How buildings fare in an earthquake can depend on design and materials.
Wooden frames are better able to flex and, in effect, have longer periods than stiffer, brick or stone constructions of the same height.
In San Francisco's Marina district, one design of house suffered badly during Loma Prieta.
These were stylish houses built over an empty space, used as a garage, with few supporting columns.
During the quake, the columns failed to damp the lateral motion of the building, and the top-heavy houses swayed and collapsed.
Ironically, much of the Marina district was built on rubble from the 1906 quake.
Foundations call the tune
Soil, too, has its own periodicity.
The soil between bedrock and the surface behaves like those plates of jelly on the table.
It has a particular stiffness depending on the type of soil — clay, sand, gravel and so on— and how much moisture it carries.
The significance of soil periodicity was driven home in Mexico City after the earthquake on 19 September 1985.
The devastation in the city's Lake Zone, about 400 kilometres from the epicentre, was surprising even for a quake of 8.1 magnitude.
According to calculations by Edmund Booth, an earthquake specialist with Ove Arup and Partners, a large firm of consulting engineers based in London, the soft clay under Mexico City ‘tuned’ the period of the quake's ground motion to something near a pure sine wave with a period of 2 seconds.
Buildings of 20 storeys resonated with these vibrations and, as a result, the horizontal acceleration at the top of these buildings was five times greater than that at ground level.
Ground acceleration, measured as a percentage of the acceleration caused by gravity (980 centimetres per second per second), is of particular concern to earthquake engineers.
This is because the shearing force on a building depends on the building's mass multiplied by its horizontal acceleration during a quake; vertical acceleration is usually much less in quakes, and buildings are built to resist vertical forces anyway.
In Mexico City, the acceleration in bedrock was about 4 per cent of the acceleration due to gravity.
But at the foundations of buildings, the clay amplified it to more than 20 per cent of gravity.
Even buildings whose shorter periodicity put them out of the vulnerable category suffered; the lengthy shaking made them progressively less brittle, in effect lengthening their period until they vibrated in resonance with the quake, at which point many began to collapse.
By the time the earth finally stopped moving, 200 buildings had been destroyed, thousands more were damaged, and as many as 20 000 people were dead, most entombed in collapsed buildings.
Engineers calculate soil periodicity before designing a building, a least in the western US, notes Robert Whitman, an earthquake engineer at the Massachusetts Institute of Technology.
But determining the period of a mass of soil is tricky.
‘If we are in the middle of a broad, flat valley, our mathematical models work well,’ says Whitman.
‘But if we are in a deep, narrow valley, say with width less than four depth, we don't really know the period, due to reflections.’
Besides period, soil is affected by how wet it is.
If you shake a beaker full of dry sand, the particles will settle and the material will become denser.
Shaking saturated soil is another matter.
When a sudden shock wave pressurises water captured in the pores between soil particles, there is no time or place for the water to move: the high pressure pushes the soil particles apart.
During earthquakes the soil becomes a semiliquid soup that no longer supports the weight of the buildings above.
Clay particles are most cohesive than other soils and deter the pressurising of trapped water, while fine sands are the easiest to liquefy.
Buildings may remain structurally intact but tilt like rows of old tombstones as their foundations lose their grip.
This happened in the 7.5 magnitude quake at Niigata, Japan.
Isolating buildings from the ground
Driving piles or draining soils that are likely to liquefy if a hit-or-miss proposition, according to engineers at Ove Arup.
For the Rangoon General Hospital, which will be built on loose sandy soil, Ove Arup proposes to solve the liquefaction problem by laying the foundations deep in the ground, so that the weight of the building will roughly equal that of the soil dug out.
In the event of the sand liquefying below the basement, the soil should still be able to support the building.
Some of the most enthusiastic believers in earthquake engineering can be found in Malaysia.
It was the Malaysia Rubber Producers' Research Association that helped to turn a concept known as ‘base isolation’ into reality at the University of California.
Base isolation is a simple idea.
The building sits on bearings that isolate it from the ground.
During a quake, the bearings intercept, absorb and damp vibrations.
In effect, the building's frequency is lowered or ‘detuned’ below the earthquake's, and the structure moves like a rigid body above the isolators instead of flexing.
The technique was first patented early this century in Britain, but not developed until 1976 when the idea caught the attention of James Kelly, professor of engineering at the University of California, Berkeley.
In 1977, Kelly and the Malaysian rubber industry, with the support of the university's Earthquake Engineering Research Center, began testing rubber base isolators inside a sprawling warehouse in a dingy industrial district north of San Francisco.
‘People thought we were idiots for a long time,’ remembers Kelly.
The warehouse, which belongs to the university, had something no other laboratory had: the country's first large shaking table.
The shaking table is a 50-tonne concrete plate set horizontally in a hole in a concrete floor with several centimetres of play at the edges.
In a cellar-like room below, however, a spider's web of tubing and hydraulic actuator arms is connected to the table.
During an experiment, the air in the cellar is compressed to 3 pounds per square inch (about 21 kilonewtons per square metre) and the table floats.
The hydraulic arms then buffet it to simulate the forces at work during an earthquake.
Kelly mounted experimental structures on bearings on the table to discover the best ratio between the height of a building and the size and elasticity of the bearings.
In 1982, Kelly and Robert Rigney, chief administrative officer of San Bernardino county, persuaded the state to pay for a real building with isolators.
Their goal was to create a building that would remain functional after an earthquake with a magnitude of 8.3, as big as anything that has ever hit California.
They got their way, and the building opened in 1986 as the Foothill Communities Law and Justice Center, about 100 kilometres east of Los Angeles.
The building's 98 rubber bearings, each between 0.5 and 0.75 metres in diameter with steel laminations and weighing 500 kilograms, attracted plenty of attention during the construction project.
Not until 28 February 1990, however, did the structure experience a large quake, when one of 5.5 magnitude struck just 10 kilometres away.
The horizontal ground acceleration measured 26 per cent of gravity in a car park nearby.
But it was only 8 per cent of gravity just above the bearings, and 16 per cent at the roof.
Motion at the rooftops of two nearby buildings was up to three times that at ground level.
One advantage of base-isolated buildings is that they move as a unit.
Ordinary buildings distribute motion upwards from storey to storey, which can cause ‘interstorey drift’ that can knock floors out of line as though the structures were stacks of sliced bread.
Nuclear power stations are especially vulnerable to this kind of drift, which can damage piping.
Kelly has several nuclear stations as customers for his isolators.
California now has about 10 structures with base isolation.
A handful of others, including a 19th-century stone building in Utah whose foundations are fitted with rubber bearings, exist elsewhere in the US.
Kelly notes that builders are constrained by the threat of lawsuits should something happen to a structure built outside normal building codes.
In Japan, however, the concept has caught on (see Box).
Japan has about 40 such buildings of various sizes being built or completed.
The country's six largest construction companies, which together spend between $14 million and $16 million a year on earthquake research, met the extra costs of including base isolation in the structures.
In the US, where the government is the only major backer, research has limped along at about half a million dollars a year.
Japanese engineers are also adding some flourishes, such as steel rods inside the rubber bearings to prevent the isolators deforming too much as they absorb vibrations.
Some buildings have been equipped with oil-filled isolators to damp any vertical movement.
In a recent quake, peak ground acceleration reached 4.4 per cent of gravity at a base-isolated building built by the Ohbayashi Corporation.
The acceleration at the roof measured only 0.71 per cent of gravity; at an adjacent, four-storey conventional building, roof acceleration was 5.9.
Rubber bearings are not the only form of base isolation.
Another approach being tested is the friction pendulum system.
At the building's foundations, each vertical column stands on a semicircular slider that is seated in a concave, spherical surface of chromium steel.
When a quake shifts the column and slider — think of a pestle scraping along the bottom of a mortar — the slider faces ever more frictional resistance as it ‘climbs’ the curvature of the concave surface.
Michael Constantinou of the National Center for Earthquake Engineering Research at the State University of New York (SUNY) at Buffalo champions this approach.
Five buildings of a hospital he is designing will be seated on these bearings.
Adding bearings to a building can increase the cost by up to 10 per cent.
‘That's a bargain when you consider how much protection you get for a building's contents alone,’ says Kelly.
Base isolation, though, has its doubters.
Loring Wylie, chairman of Degenkolb Associates, a firm of consulting engineers based in San Francisco, says the technique requires more testing.
‘We know how a building works on a shake table.
But what if you have one near the San Andreas fault?
In a 1966 quake, the ground displaced 3 to 6 metres.
Somehow a building has to ‘track’ the ground, to follow it.
The shake table always moves back to zero.’
Buildings are connected to pipes and cables in the ground that will break if there is too much movement.
Also, if rubber bearings do not return to their original shape after a quake, the building loses some stability, says Wylie.
California's philosophy underpinning its seismic codes holds that a building should not be damaged in a minor quake, should suffer no more than minor structural damage in a moderate quake and not collapse in a major quake.
While California's engineers do not specify precisely what a minor or moderate quake is, a major quake is one that will accelerate a building at 40 per cent of gravity.
Another quake the size of Loma Prieta would quality.
Memories of Loma Prieta are not the only incentive; the US Geological Survey a 50 to 60 per cent chance of a major quake in California in the next 30 years that will cost $50 billion in property losses alone.
Geologists and engineers have begun preaching the virtues of preparation in the central and eastern US as well, to a somewhat less receptive audience.
The rarity of quakes there has lulled people into a sense of security that many say is false.
While the chances of a major quake are low, the consequences would be great.
‘A small one, even magnitude 5 or 6, would cause a lot of grief,’ warns Ian Buckle, deputy director of SUNY's earthquake centre.
Besides lacking reinforcement, buildings in many central and eastern cities sit above homogeneous basement rock, which conducts vibrations for much greater distances than the ground in California.
The cost of making safe American buildings, bridges and pipelines almost defies imagining.
No one even knows where to expect a quake, as many of the eastern fault lines lie deep and undetected.
For now, engineers at SUNY are trying to make people aware of the risk while perfecting some known techniques for new buildings.
For example, ‘moment resisting frames’— building frames in which columns and beams are rigidly attached and reinforced to carry the building's load — help stiffen buildings.
So do ‘shear walls’ made of reinforced concrete and coupled with the building's frame.
The SUNY team is also working on some novel concepts for existing buildings.
For smaller structures, it has developed something akin to an insect's cocoon: a very fine wire mesh riveted to the walls, floor and ceiling of a building, then sprayed with ferrocement containing metal particles.
The layer reinforces the wall's weak link — the mortar.
Mortar can support vertical, compressional stress but cannot handle lateral forces.
‘It's like a reinforced thin skin,’ Buckle explains.
‘The results on the shaking table are quite astounding.’
Other measures can help stiffen a building but are less than ideal.
Diagonal steel braces can be inserted behind walls, but this is an expensive intrusion.
Shear walls can be added.
But this kind of retrofitting can cost up to half the value of a building, and owners are not likely to make the investment.
Those who ignore the warnings do so at their peril, however.
Earthquake experts like to talk about what lies in store for New York City.
The city's building codes do not incorporate seismic resistance, and more than half of Manhattan's buildings are built of unreinforced masonry.
Many of New York's bridges are made using inadequately reinforced concrete.
The engineers at SUNY are trying to convince the city's department of transportation to take seismic loading into account for its new bridges.
Older bridges are at great risk; they could be made safer by fitting elastic isolators where thermal expansion bearings now sit between girders and columns.
This solution, however, is not suitable for a crossing that consists of one simple span supported at the ends.
In the east, says Buckle, ‘many bridges just sit on the columns like a stack of cards’.
Taller columns will sway more at the top, causing the span to unseat itself.
Then there are the cast-iron pipes that supply New York City with water.
Peter Gergely, professor of structural engineering at Cornell University in Ithaca, New York, says, ‘200 to 300 fires could start, and if the pipes go out you don't have water to fight them.’
As for soil, it varies considerably, ranging from bedrock under much of Manhattan to sandy sediments under eastern suburbs that would amplify the effects of small quakes.
Experts also point to the oil and gas pipelines that feed the eastern cities.
For example, important pipelines pass near the New Madrid fault region in Missouri.
One of them is the Capline System, a 3-metre diameter line that carries about 700 000 barrels a day across the centre of the country.
Soil deformation during a quake could bend pipelines as though they were strands of copper wire.
Ruptures would cut the eastern states off from their supply of oil and contaminate ground water.
Much has been made of the fact that San Francisco emerged relatively unscathed from Loma Prieta.
Engineers are not so confident.
‘They were lucky at Loma Prieta,’ says Gergely.
The quake hit relatively far from the city and it lasted only about 10 seconds; there was no wind to spread fires; and conditions were dry, making landslides less likely.
‘Officials don't like to tell the public,’ says Gergely, ‘but they say they were scared.’
Three huge quakes, greater than magnitude 8, struck the New Madrid fault in Missouri in 1811 and 1812.
A big quake east of the Rocky Mountains — and one greater than magnitude 6 has a 50 to 60 per cent chance of occurring in the next 25 years — could make California's Loma Prieta seem trifling.
Many engineers say such a disaster is imminent.
There is much work to be done in preparation, says Buckle.
New York City and several other eastern jurisdictions are considering the introduction of seismic building codes.
‘These are encouraging signs that the message is being heard in the East,’ he says, ‘but this is a race against time and the odds of winning do not look favourable.’
Structures tuned to the rhythm of a quake
The ultimate in earthquake-proof buildings is going up in Tokyo.
Not satisfied with simply strengthening its new 11-storey office building conventionally, the Kajima company, one of Japan's major construction companies, is adding ‘active controls’.
Justifying their expense requires a genuine earthquake threat and deep pockets.
Japan has both.
Kajima's Kyobashi Seiwa building will look something like a chopstick when it is finished — the structure is 4 metres wide, 13 metres long and 33 metres high.
The brains of active control systems are computers that respond within hundredths of a second to the vibrations of an earthquake.
The muscle of the control system is on the roof.
There, mounted on rollers, are a pair of steel counterweights.
Hydraulic rams move the counterweights across the roof to counteract the back-and-forth swaying of the building.
Similarly, 1 and 4-tonne blocks move to counterbalance the torsional,(twisting) stresses on the structure.
A network of sensors in the basement, sixth floor and roof feed data on the structure's movement, either from wind or stress, to a mainframe computer.
The computer calculates when to roll the counterweights and by how much.
Kajima calls the system an ‘active mass driver’.
Edmund Booth of Ove Arup likens the physics to ‘standing in a railway carriage and flexing your legs to keep your balance’.
Another approach, designed by the earthquake engineering centre at the State University of New York at Buffalo and Takenaka Corporation of Japan, comes even closer to Booth's analogy.
Steel arms, called tendons, horizontally bisect a building's core, stretching like ribs between beams in the walls.
Controlled by computer and battery-powered hydraulics, they push or pull like pistons to damp movement by the walls.
This system was installed in a six-storey experimental building in Tokyo in August 1989 and has passed muster in six small earthquakes since then.
During a quake of magnitude 6.1 on 20 February 1990, ground acceleration was measured at 10 per cent of gravity.
That was halved by the building's bracing.
Fires, falling masonry and ruptured services are among the fatal consequences of not protecting property against earthquakes
Reinforced coatings may strengthen masonry walls (above) say researchers, who test complex structures on a shaking table (right)
San Francisco's earthquake in 1989 highlighted the vulnerability of elevated highways and bridges throughout the country
Since the Niigata quake in 1964, Japan has invested in new building designs
Houses built over garages, which did not have enough columns to damp the lateral motion, crumpled to the ground in San Francisco
Devastation to buildings in Mexico City, after the 1985 quake, could be repeated in Manhattan some time soon, say specialists
The malleable microbe
Just how different is a bacterium from an elephant?
The answer lies in the flexible genes of the lowly microorganism
John Postgate
DURING the 1960s and 70s biologists were apt to remark, assigning the belief to someone else of course, that what is true of the bacterium Escherichia coli is true of the elephant.
The witticism, which no one really believed, mocked researchers at the cutting edge of molecular biology who worked almost exclusively with E. coli but applied their conclusions to all living things.
In truth, elephants and E. coli do have a lot in common, inasmuch as all organisms share fundamental pathways of cellular metabolism — such as protein synthesis or carbohydrate breakdown — as well as the genetic code.
When it comes to genetic flexibility, however, the star performer is quite definitely E. coli .
During the years when the elephant jibe was rife, microbial genetics was at the forefront of molecular biology.
Yet, ironically, it is at the genetic level that the most striking molecular differences are found between E. coli (representing bacteria in general) and elephants (or any higher organisms that have cells with nuclei).
True enough, these two groups use the same basic genetic code.
But they have different ways of retrieving the information coded in their DNA and translating it into synthesis of cell material.
Moreover, unlike higher organisms, bacteria can actually exchange DNA with other bacterial species or genera.
E. coli , for example, has three ways of transferring or receiving genes: transduction, transformation and conjugation (see Box 1).
To a certain extent, transduction and transformation are fortuitous.
Conjugation, however, is part of the everyday biology of E. coli .
It is the bacterial world's equivalent of sex: ‘male’ microbes donate fertility genes to receptive ‘female’microbes.
When the fertility genes (about 30) reside in E. coli 's chromosome, parts of the adjacent chromosomal DNA are often transferred with them during conjugation.
This extra DNA may include genes useful to the recipient.
More often, though, the fertility genes reside on a small, circular piece of genetic material, or ‘plasmid’(see Box 2).
While E. coli carries its main genetic archive on conventional chromosomes, plasmids equip the microbe with small libraries of supplementary information.
Completely separate from any central chromosome, they carry a broad palette of bacterial genes and provide genetic engineers with a useful means of introducing new genes into microbes.
Past research has revealed numerous different E. coli plasmids scattered among different strains of the bacterium.
No strain carries a full set, however, so the chromosomal gene pool of the species as a whole is much larger than its gene pools of plasmid-borne information.
All this may seem a curious way for a species to handle its genes.
But it has a certain logic.
The main habitat of E. coli , the lower intestine of mammals, is very stable.
(E. coli also lives, albeit transiently, in soil and fresh water, which it reaches in faeces.
Here it is usually overwhelmed by indigenous microbes.)
Current thinking is that the chromosome of E. coli meets all the microbe's ordinary domestic needs, which rarely change, and that plasmid-borne information comes into play only when the microbe encounters a stress — a changed nutrient, an antibiotic or competitor.
Only a minority of the population would have the plasmids needed to combat such a stress but at least the species, if not a majority of individuals, would survive.
This implies that E. coli 's chromosomal information ought to be stable.
In fact the opposite is true.
For one thing, when fertility genes abscond with neighbouring DNA during conjugation,E. coli 's chromosomal information fluctuates.
For another, many of the genes carried by plasmids — such as those specifying resistance to the antibiotics kanamycin or penicillin — are flanked by special DNA which enables them to jump from plasmid to chromosome and back, or from one plasmid to another.
Again, these jumping genes, or ‘transposons’, cause chromosomal fluctuations.
Moreover, the chromosomes of some strains of E. coli contain enigmatic lengths of DNA (‘insertion sequences’) which code for no known product but can move about the chromosome during bacterial multiplication, activating or silencing genes.
They are present in plasmids, too.
Finally, the chromosome is not even intrinsically stable — it mutates.
Mistakes are made when DNA is copied during replication, and environmental mutagens such as background radiation also cause errors.
Although E. coli has enzymes which detect and repair such faults, a minority escape repair and produce true mutants.
For the last 20 years researchers have been able to calculate genome sizes and mutation rates.
Elementary arithmetic (see Box 3) shows that the collective daily mutability of the chromosomes of all E. coli in humans is astronomical.
Add in the E. coli living in other mammals and the number becomes even larger.
By far the shakiest part of the calculation is the average mutation rate.
For the E. coli chromosome has both ‘hot spots’, where mutations are more frequent than average, and stable zones where they are less frequent.
Moreover, mutation rates seem to change with the physiological state of the organism.
Even so, the margin for error is so huge that the upshot of the calculation remains valid: among the world's population of E. coli , every possible mutation is occurring an enormous number of times a day.
The net impression is one of tremendous genetic fluidity, brought about by plasmids, transpositions, insertions, transduction, transformation and mutation.
Why, then, are microbiologists still able to identify E. coli from its genetic make-up when they examine faeces or environments polluted with sewage?
The answer lies in the microbe's primary habitat.
The mammalian gut has been stable for many millennia and acts to constrain the flexibility of E. coli 's genome.
Though many genetic fluctuations do occur, most die out because they fail to confer any survival advantages on gut-dwelling microbes.
Different guts will impose marginally different genotypes; no doubt that is why biochemical families of E. coli exist, differing in the fine detail of their enzymes.
More practically, it is probably one reason why the E. coli of distant lands, where different diets provide it with significantly distinct primary habitats, are apt to give the intrepid traveller diarrhoea.
Of course, when a microbiologist takes E. coli out of its usual habitat and grows it in a laboratory, the strain eventually mutates.
But this is a problem which can be lived with because mutations in a test-tube population come in hundreds rather than thousands of billions.
In any case, most laboratory mutations either die out or have no obvious effect, or merely improve the growth and manageability of the strain.
Despite its apparent mutability,E. coli does have the power to protect its genetic integrity.
If alien DNA enters a cell — by transformation or as a bacteriophage, for example—E. coli can recognise and destroy the unwanted DNA using restriction enzymes.
These chop the DNA up into pieces which the cell can degrade and excrete (restriction enzymes have proved immensely useful for manipulating DNA in the laboratory).
Plasmids from other E. coli are not alien, of course.
A close relative of E. coli is salmonella, a bacterium whose primary habitat is the intestines of birds (notoriously chicken and ducks) and which, like E. coli , inhabits soils and fresh water transiently.
The chromosomes of E. coli and salmonella are so similar that there are long stretches of identical DNA in the two organisms.
Salmonella is also a match on genetic flexibility.
Like E. coli it undergoes transduction, transformation and conjugation.
The two organisms readily exchange plasmids, too.
But there is a barrier to the exchange of chromosomal DNA.
If chromosomal DNA from E. coli is introduced into Salmonella typhimurium , for example, the DNA repair system of the salmonella recognises that something is wrong and ‘tidies up’ the alien DNA.
Somehow the salmonella knows it is not E. coli even though it would not require many mutations for it to become E. coli , or the reverse.
Indeed, the global mutation rates of the genomes of both species are such that, given the right selection pressure, their convergence could be rapid.
So what stops this happening?
Evolutionary history provides some clues.
The structure of the ribosomal RNA of bacteria provides a useful molecular marker for assessing phylogenetic similarities (see New Scientist , 21 January 1989).
Using this marker, Howard Ochman and Allan Wilson of the University of California, Berkeley, concluded that the two species diverged about 130 million years ago, ‘intriguingly’(they wrote) about the time mammals originated.
Because the bird and mammalian habitats of the two organisms have barely changed since, salmonella and E. coli have remained distinct, yet recognisably close relatives.
Do similar principles apply to microbes with less constant habitats?
In the last decade microbiologists have realised that, though what is true of E. coli may not always be true of the elephant, it is generally true of other eubacteria.
(Archaebacteria, which inhabit extreme environments such as hot springs, salt pans and oxygen-free decomposing detritus, are biologically distinct, see New Scientist , 11 August 1990.)
In particular, the genetic flexibility shown by E. coli seems to be common.
Transformation was first discovered in pneumococci and largely worked out in the genus Bacillus .
Its discovery in E. coli is relatively recent, and it now seems that most bacteria can be transformed by raw DNA.
Transduction of genetic information by bacteriophages occurs in many groups of bacteria.
And other mechanisms have come to light, such as cell fusion among bacilli and an enigmatic gene transfer agent in some photosynthetic bacteria.
Perhaps most significant, microbiologists now realise that plasmids, once thought to be exceptional, are so common as to be almost the rule among bacteria.
Many are ‘cryptic’, meaning that their discoverers have no idea what their DNA codes for.
Rhizobia, the nitrogen-fixing symbionts of leguminous plants, usually have a few, some of them huge (up to 30 per cent of the size of the chromosome).
In R. leguminosarum , which colonises peas, genes both for recognising the right species of host plant and for fixing nitrogen reside on plasmids.
But not all plasmids are good news for plants.
The plant pathogen Agrobacterium , for example, carries a plasmid which makes plant cells cancerous.
Some microbiologists believe that Agrobacterium 's chromosome is identical to that of the friendly Rhizobium , and that the two microbes differ solely in their plasmid complement.
Azotobacter chroococcum , a free-living nitrogen fixer, usually has between five and seven plasmids (none of which carries the fixation genes).
The staphylococcus family boasts a wide repertoire of plasmids, too.
And most soil and water bacteria carry at least one plasmid.
One common group of soil bacteria, the pseudomonads, carry plasmids enabling them to metabolise exotic chemicals such as toluene, camphor or oil hydrocarbons, as well as plasmids coding for drug resistance.
Some pseudomonads carry fertility genes which enable them to mate among themselves and pass plasmids to unrelated, or distantly related, bacteria.
Armed with transposons and other genes, these ‘promiscuous’ plasmids are not sensed as alien by other bacteria.
Quite the opposite: they are willingly accepted by a diverse range of microbes, from Proteus and Azotobacter to E. coli .
And any of these recipients can usually pass the plasmids on again.
In fact, many of the plasmids used in research on E. coli were first found in pseudomonads or salmonella; it can be difficult to tell in which species a plasmid first originated.
How important are these gene transfer processes in the natural environment?
Transduction seems to be too drastic and to involve fragments of DNA too small to be of serious ecological importance.
But transformation may well be common in environments with a high microbial turnover, such as decomposing organic matter.
Examples of plasmid transfer in the external world are well established; for instance, it is the mechanism by which drug resistance spreads in hospitals or intensive farming units.
It is likely, too, that the chromosomes of all eubacteria are as mutable as that of E. coli .
Although some yield mutants less readily in the laboratory than does E. coli , this is because they are better equipped to repair mutations.
So, in principle, the calculation in Box 3 applies to all eubacteria.
A slow-growing soil bacterium, good at DNA repair, might require weeks, even a couple of months, for its global gene pool to undergo as many mutations as afflict the world's E. coli ; but even so, the potential of eubacteria for rapid mutation is phenomenal.
The late Bob Hedges, of the Royal Postgraduate Medical School at Hammersmith, a pioneer of plasmid research, suggested almost 20 years ago that bacterial evolution has not been linear, as in higher organisms, but rather a patchwork, with organisms drawing from a communal gene pool.
Advances in molecular genetics have reinforced that view.
Restriction enzymes and DNA repair systems confer a certain degree of integrity on bacterial genomes, but these systems mutate too.
The speed at which bacteria can mutate and their readiness to pass around packages of genetic information means that bacteria are poised to react to selection pressure with rapid and substantial genetic changes.
It is safe to conclude that at any stage of this planet's history the world of bacteria has been overwhelmingly conditioned by the state of the biosphere.
Anxiety over global environmental change has perhaps made us more aware of the converse idea: that the activities of microbes largely determine the state of the biosphere, and in particular the evolution of the Earth's atmosphere.
But both propositions are equally true.
Just as laboratory bacteria are artefacts of the culture media, so the bacterial world can be viewed as an artefact of the rest of the biosphere.
Yet in the face of environmental stress, a malleable genome has given the microbe the edge on the elephant.
Finally, reflect, if you will, on how drastically humanity has changed the biosphere during its brief strut on the terrestrial stage; one wonders how much of today's microbial world we have ourselves created.
1: How Escherichia coli pass their genes around
TRANSDUCTION: Some bacterial viruses (bacteriophages), after they have infected an E. coli cell, combine with some of their host's DNA and make it part of themselves.
As the virus DNA multiplies, the piece of host DNA multiplies along with it.
In due course the host dies and releases into the environment virus carrying fragments of its DNA (see diagram below).
When the hybrid virus attacks and enters a new cell, it carries in that DNA.
Most of the cells attacked will succumb to the infection, but a few will be resistant, and these will integrate the alien DNA into their own genomes.
If the new DNA includes DNA sequences which the resistant host can use, the host will do so, and its genotype will thus have been changed.
The pieces of DNA transduced by bacteriophages may carry one or two whole genes, but they are often smaller fragments which may nevertheless modify genes already present in the recipient.
Transformation:E. coli at a certain stage of their growth cycle can, after treatment with chemicals such as calcium chloride or rubidium chloride, take up raw DNA (for example, DNA purified in the laboratory) and, if it contains appropriate sequences, will incorporate them into their own genomes.
Transformation can involve pieces of DNA comprising dozens of genes, such as purified plasmids (see Box 2).
Conjugation:E. coli only conjugate when one of the cells possesses fertility genes and the other does not.
The two organisms come together, a tube grows between them, pulling them close, and the fertile strain donates fertility genes to the recipient, which then becomes fertile itself.
Conjugation may involve the transfer of anything from about 30 to over 100 genes.
2: Plasmids
THESE ARE mini-chromosomes found in bacteria.
They are distinct from the cell's main store of DNA, the bacterial chromosome, yet still multiply during cell growth.
In E. coli plasmids range from about 3 to 20 per cent of the size of the chromosome.
Most often there is one copy of a given plasmid per chromosome, but with the small ones there may be several copies.
Sometimes two or more kinds of plasmid are present in a given strain of E. coli , though plasmids seem to belong to groups, not all of which are compatible with each other.
Plasmids that do not carry fertility genes are unable to move from one bacterium to another, but they can be mobilised by plasmids which do have them.
Fertility genes enable a plasmid's genetic information to be transferred from a donor to a recipient strain.
Plasmid genes can determine a wide variety of bacterial properties, including resistance to antibacterial substances, ability to make an antibiotic or toxin, or to metabolise the sugar sucrose.
As an example, a plasmid known as R100 (left) comprises 90 000 base pairs of DNA (compared with the 4 million base pairs of the chromosome) and has genes which make its host cell fertile and resistant to five antibacterial substances.
3: Global mutations
THE BACTERIUM E. coli (above) grows at a phenomenal rate.
An average human might discharge 200 grams of faeces a day.
The E. coli count of human faeces is surprisingly constant at about 2 10 cells per gram.
Thus one human produces about 2 x 10 10 E. coli per day, almost all freshly grown since the last defecation.
There are now about 5 x 10 9 humans.
So the global growth rate of our intestinal E. coli is about 10 19 cells per day.
The E. coli genome comprises about 4 x 10 6 base pairs of DNA.
Genes differ in length, averaging about 10 3 base pairs.
So E. coli has about 4 x 10 3 genes.
Spontaneous nonlethal mutations (to drug resistance, to a new nutritional requirement, etc) occur in the genomes of multiplying E. coli at frequencies in the range of 1 per 10 4 to 1 per 10 9 new progeny.
For the sake of argument, say 1 in 10 7 .
Each one signifies an altered gene.
Therefore more than 10 12 E. coli genes mutate daily inside humanity.
Which means that every E. coli gene of the E. coli genome traversing the human intestine mutates at least 2.5 x 10 8 times daily.
John Postgate FRS is emeritus professor of microbiology at the University of Sussex.
Transduction: a bacteriophage attacks E. coli,proliferates inside and carries off microbial genes.
A subsequent attack on a resistant strain results in gene transfer 
Mean machine: a soil-dwelling pseudomonad.
Plasmids, transposons and whip-like flagella equip it for gene transfer
An unlikely civil servant
Michael Peckham is the first person to take on the task of directing the research effort of the National Health Service.
Next month he will be telling ministers what his plans are
Phyllida Brown
THERE ARE not many senior medical researchers in Britain who would list the political upheavals of Paris in 1968 among their formative experiences.
There are fewer who read Czech poetry, or have their paintings exhibited regularly in London and Edinburgh.
But Michael Peckham does all three — which makes him seem, on the face of it, an unlikely character to move to Whitehall as the government's new Director of Research and Development at the Department of Health.
He may be an unlikely civil servant, but his qualities should stand him in good stead for the job of producing a national strategy for the NHS's science.
A cancer researcher by profession, he has the appropriate academic and medical credentials, the respect of his colleagues, and a strong commitment to the health service.
He also shows a healthy disrespect for the over-specialised, and is as much at home with the nuts and bolts of medical practice as he is with basic research in the laboratory.
Curiously, his other life as a painter could be part of the explanation: ‘If you're an artist you have to take the holistic view,’ he suggests.
‘I think it's prevented me from retreating into ultraspecialist activities.’
A striking feature of Peckham's approach is this willingness to cross cultures and boundaries, perhaps due in part to his early career in France.
‘If you penetrate one language and culture, it gives you the confidence to try others,’ he says.
In the late sixties, he was studying cell biology at the Gustave-Roussy Institute in Paris, under Maurice Tubiana, who retired last year.
The events in the city at the time ‘had a profound impact’, he says.
As the first director of the NHS's research and development, Peckham is having to set his own goals.
The post was established last year on the recommendation of the House of Lords Select Committee on Science and Technology.
Although he has had his feet under the desk only since January, he has been preparing for the job since last summer by meeting researchers and officials, visiting institutes abroad and drawing up an initial draft of his plans.
How effective can he be in drawing Britain's fragmented medical and health science into a national framework?
Can he raise the profile of good research and weed out the poorest work?
John Galloway at the Cancer Research Campaign feels that there are only two relevant questions: has he got any money, and does he have any teeth?
As far as money goes, Peckham has the existing yearly budget for research in the NHS plus new money to pay for a support staff of more than 50 people in the research programme at the Department of Health.
The existing money includes £25 million from the department, about £19 million from the NHS Management Executive, and a much smaller amount through the Medical Research Council.
Most of this is tied up in grants, salaries and existing programmes, some of them five years long.
In addition, there are separate existing funds, worth more than £77 million a year, to support the clinical workload that research generates in hospitals.
Will all this be enough?
It sounds very small in relation to the costs of war, but so do most budgets.
With his plans close to being finalised, Peckham is diplomatic.
Money is ‘a challenge’, he says.
And while he accepts that a lack of funds could be a serious obstacle to change, he does not want to be ‘mesmerised by difficulties’.
He feels that many people, both inside and outside the health service, are not aware of the sort of research funds that are already present in the NHS, and he would like to see these funds made more explicit.
And he wants to see a careful scrutiny to make sure that the existing money is used to greatest effect.
‘But it is quite clear that in order to generate the development side of the research, new funds would need to be made available,’ he says.
In the early spring, Peckham will meet the health minister, William Waldegrave, to present him with a set of firm plans developed over the past months.
These plans will be costed, giving him the best possible chance to make an effective case.
As for teeth, he certainly has them, in the shape of a seat on the NHS's Management Executive.
Initially, the post had been envisaged at a lower level, but the Lords select committee pushed for this power.
Before he came to the department last month, Peckham was director of the British Postgraduate Medical Federation in London, which coordinates numerous research institutes.
He was an oncologist at the University of London and at the Institute of Cancer Research, based at the Royal Marsden Hospital in Surrey.
Colleagues think he is ideal for the new job, because of the ease with which he communicates with different people and his breadth of vision.
His academic interests focus on cancers of young adults, particularly lymphomas and testicular tumours.
The result of his planning so far is what he calls ‘the R&D Initiative for the NHS’— something that he recognises must be visible and simple so that he can present it ‘succinctly’ to boardrooms and to nonspecialist health professionals, such as managers.
The plans cover both medical research proper, and what is known as health service research.
This second field concerns, for example, such questions as why there are regional differences in health care, the costs and benefits of particular treatments, and ways to measure more nebulous concepts like quality of life.
He wants to develop a network to devolve research in key disciplines, such as epidemiology, health economics, statistics and medical sociology, into the regions.
The network would be national, seeking to assist collaboration between the regions, but would also bring academics to work closely alongside the clinicians ‘at the coalface’— where, he believes, many of the best ideas originate.
‘I want to avoid creating ivory towers that might not benefit the coalface,’ he says.
One aim of the network would be to hasten the entry of new research into clinical practice — a process that most researchers have long felt to be much too slow.
‘Novel therapies stay novel for too long,’ says Peckham.
Although he is realistic about the need for focused research in specific areas, he is also keen to avoid fragmentation.
‘I'd like some sort of broad template so that we can develop the initiative nationally.’
Another of his plans is to have ‘task forces’ to coordinate research into the major killer diseases, so that teams do not overlap each other and duplicate work.
There is already such planning in some areas — for example, by the UK Coordinating Committee for Cancer Research.
This body involves the Medical Research Council, the major cancer charities and the Department of Health.
Another plan is to have a small set of advisory groups that would act as ‘think tanks’ in certain areas.
These groups, which would be multidisciplinary, would aim to set priorities for research.
Bad research will not, one suspects, get an easy ride from him.
‘Britain can only support so much research,’ he says.
Although keen to attract industry into funding health science, he is profoundly opposed to companies setting up ‘slush funds’ that would cover the costs of a particular project.
Given such easy money, he says, researchers do not have to give so much thought to their grant proposal, as they know that it will not face such a rigorous peer review.
‘If you are in a very competitive situation it may cause you to think very carefully about what you are doing,’ he says.
‘You may refine your hypothesis.
I think the availability of soft money short-circuits that process.’
There is nothing more dispiriting, he says, than seeing a young doctor who has spent years on a project and has no thesis to show at the end of it because the project was ill-considered and ill-supervised.
Peckham is critical of past failures in clinical trials of new treatments.
‘I think the whole business by which novel therapies have been tested leaves a lot to be desired,’ he says.
‘We have not been as good as we could have been in producing statistically valid trials.’
Nor has there been sufficiently critical evaluation of programmes to screen for some diseases, he believes.
The obvious example is in breast cancer: researchers agree that there is no evidence of any benefit in screening women under the age of 50 for breast tumours, yet private health firms offer the service widely.
He is anxious that all demands for screening from pressure groups — of doctors or patients — should be weighed carefully.
For example, he says, there have been strong demands from some groups to screen all pregnant women for toxoplasmosis, an infectious disease that can damage the nervous system.
‘Yet there is no valid scientific case for doing so,’ he says.
Ineffective screening is not just a waste of money.
For Peckham, like many researchers, there is the additional concern that it may bring people more anxiety than help.
He wants continued support for all the existing research units that evaluate screening programmes, and for careful assessments of the long-term outcomes of screening on the population.
‘With the increasing emphasis on health promotion and screening we need to be more critical and to regard intervention with the same rigour that we use to examine a new drug in a clinical trial,’ he says.
His critical approach to ill-conceived treatment and ill-considered research may be one side of a coin.
The other side is an unusually fresh approach to new ideas, complementary therapies, and the kind of questions that put most of the medical establishment into a cold sweat.
For example, he believes doctors should not underestimate the ‘very potent’ effect of mind on body.
And, perhaps because his time in France immersed him in French culture, which takes the social sciences more seriously than Britain's, he has plenty of respect for medical sociology.
He would like to see, for example, more acknowledgement that hospitals themselves have a strong impact on people, and he thinks some study should be devoted to this.
On complementary medicine, he is open-minded — provided, like every other therapy, it is properly evaluated.
‘The question should not seek to separate alternative therapies from orthodox ones,’ he says.
‘It should ask, say, ‘What's the best way of relieving low back pain?’’
He is referring to a study published last year in the British Medical Journal in which the complementary therapy of chiropractic was compared with conventional physiotherapy in the treatment of low back pain.
Chiropractic performed best.
The study was one of the first randomised trials to measure the effectiveness of a non-orthodox therapy.
Peckham's interest may have been more than academic.
At a meeting last summer, he mentioned that he was suffering from a sudden bout of back pain and had just visited an acupuncturist.
His back was cured within a couple of weeks — not, it turns out, as a result of the acupuncture but after a bit of dancing while on holiday.
If we are to evaluate complementary medicine properly, he says, there are several separate questions that must be asked.
First, and obviously, does it do anything for the sufferer's disease?
Secondly, and quite separately, does it improve the patient's quality of life?
What is the cost?
Some of these areas may become blurred, he feels: for example, one feature of complementary medicine that patients may prefer is simply the longer consultations that its practitioners tend to give.
‘Orthodox medicine needs that quality of dealing with people,’ he suggests.
Peckham graduated from Cambridge in 1960 and went to University College Hospital in London, where he worked with Gwen Hilton.
He applied for an MRC scholarship in clinical research at the Gustave-Roussy after seeing the application papers in a friend's office.
‘The details of the scholarship were sitting on his desk,’ he says.
When he left Paris, it was to go to the Royal Marsden.
Sir David Smithers, with whom he was to work, sent him a return air ticket.
‘I incorporated the envelope into a collage which I've still got,’ he says.
He is struck by what he calls ‘the immensely accidental quality’ of events in his life.
Originally, he thought he would become a poet and it was ‘by accident’ that he became a doctor.
‘Paris was an accident too.
Or at least, they look like accidents, although they were probably not.’
He is married to Catherine Peckham, an epidemiologist at the Institute of Child Health in London, who is currently involved in a major coordinated study of children with HIV around Europe.
One of the Peckhams' sons is a poet, another is an environmental consultant, and the third is a doctor.
His profound affection for France has persisted.
In 1984, he was offered the directorship of the Marie Curie Institute in Paris —‘the greatest temptation of my life’.
For personal reasons, he turned it down, but he is an enthusiastic European, and is the president of the Federation of European Cancer Societies, and on the councils of other international research bodies.
He is also the editor-in-chief of the European Journal of Cancer , launched last year.
His European contacts have given him ideas — such as the Dutch government's research projects to look into the specific health needs of the country's future.
He is also convinced of the need to ‘sell’ Britain's medical tradition abroad.
‘Our standards here are very high indeed,’ he says.
Like others, he recognises too that the NHS is unique in the West as a relatively uniform, national, infrastructure that is ideal for clinical trials and other research.
But he wants doctors to learn more rigorously how to assess the effects of their treatment, borrowing, for example, a scheme from Harvard in which qualified doctors receive full training in research methods and statistics so that they can evaluate their work.
What does he do for fun?
Paint and draw, ‘all the time’.
He draws people on buses and planes, and drew members of the Senate when he went to Washington; he paints whenever he can.
His images are striking human forms (though the painting visible behind him is not one of his own).
His most recent London exhibition was at the Christopher Hull Gallery off Belgrave Square, but he has exhibited since then in Edinburgh at Richard Demarco's gallery.
He does not believe there is any overt connection between his paintings and his medicine.
But the act of confronting a piece of white paper — bringing an idea into being — is similar for artist and scientist.
He reads William Carlos Williams (also a doctor), and the Czech poets, particularly Miroslav Holub.
Between now and the early spring he will continue to consult people and build up his plans for the crucial meeting with ministers.
If they decide to put some financial weight behind his thinking, Britain's research in medicine and health can expect a gigantic shot in the arm.
‘We need to evaluate screening with the same rigour we use to examine a new drug’
‘We have not been as good as we could be in doing statistically valid trials’
INSIDE SCIENCE Number 42
WATER FROM THE GROUND
There is more water in the ground than in all the world's rivers and lakes combined.
To use this valuable resource to good effect, we need to understand its place in the water cycle
Michael Price
WATER is one of the most commonplace compounds on Earth.
Yet, thousands of people die every day because they do not have enough, or because their supply is contaminated.
How can this happen on a planet that has an estimated 1400 million cubic kilometres of water?
That is 250 000 million litres for every man, woman and child (1 cubic kilometre is 10 12 litres).
The problems arise not from a shortage of water, but from its unequal distribution.
So, where is all this water?
By far the largest proportion is in the oceans, which hold roughly 1370 million cubic kilometres of salt water.
The largest store of freshwater is in glaciers and icecaps — particularly in Antarctica — which holds about 30 million cubic kilometres of water as ice.
Rivers, lakes, soils and the atmosphere, the obvious sources of freshwater, contain about 200 000 cubic kilometres of water — less than one-fiftieth of 1 per cent of the world's total water supply.
There is another important store of water, much of it fresh, that is easy to overlook.
The rocks of the upper part of the Earth's crust contain many holes.
Some are caverns, but most of them are tiny pores — such as the spaces between grains of sand in a sandstone, or networks of equally fine cracks.
Most pores are filled with water.
After the oceans, porous rocks contain the Earth's largest store of water; one calculation puts the total at more than 50 million cubic kilometres, of which at least 4 million cubic kilometres is freshwater.
Some rocks are more porous than others.
More important, the pores in some rocks are either large or join up so that water can flow through them easily.
Such rocks are said to be permeable; sandstones and gravels are good examples of permeable rocks.
In other rocks, water can hardly flow at all: clay has very small pores, whereas pumice is full of good-sized holes but they rarely link up.
These and similar rocks are impermeable.
Layers of rock that are porous and permeable enough to store water and let it flow through them easily are called aquifers.
In temperate countries such as Britain, the absence of water in the landscape is often a good clue to the presence of one of these great underground stores.
Where there are rocks of low permeability at the surface, such as clays or granites, or altered rocks such as slates, only a little rain soaks into the ground.
Most of the rainwater flows straight to streams or rivers.
These impermeable rocks form landscapes with rivers that rise quickly, even flooding their banks after heavy rain, but diminish or even dry up after a spell of dry weather.
In areas of permeable rocks, most rain soaks into the ground.
It reaches rivers only after passing slowly through an aquifer.
Permeable areas usually have only a few streams, but they flow with little variation throughout the year and rarely flood.
Far below the Earth's surface, the rocks are so compressed that the pores are closed.
Water cannot easily flow in these impermeable rocks.
They effectively mark the bottom of the storage space for water in the Earth's crust.
One of the difficulties in estimating how much water is stored in the ground is knowing how deep this ‘floor’ is.
Geologists have found water deeper than 10 kilometres, but the greatest depth at which water actively circulates is usually about a kilometre.
Above this floor, rain soaks down to recharge an aquifer.
The aquifer fills up with water until water reaches the surface of the land in one or more places.
This generally happens where the ground is lowest, usually in river valleys.
There, water flows from the aquifer as springs or seepages.
The aquifer therefore becomes saturated to a level where the water flowing out balances the recharge that comes in from rainwater.
The place where the water leaves the ground may be a long way from where recharge occurs.
An aquifer may therefore carry water from a humid area to a dry one, even a desert.
In this case the water flowing from the ground forms an oasis.
At Kufra in the Libyan Desert, for example, water emerges from an aquifer called the Nubian Sandstone to form small lakes.
The presence of such oases made human travel and nomadic existence possible in arid lands.
The top of the saturated rock is called the water table, and the water that saturated the rocks beneath the water table is groundwater.
The part of the aquifer that lies above the water table is termed the unsaturated zone.
The water table is not horizontal.
It slopes towards places where water leaves the aquifer — the river valleys — and away from where water is coming in — the hill sides and high ground.
It is thus higher beneath hills than beneath valleys, following the landscape in a subdued way.
In most countries, groundwater flows from aquifers throughout the year but rainfall replenishes them for only part of the time.
So the water table is not at a constant level, but rises and falls during the year as the amount of water stored in the aquifer increases and decreases.
Down hills and up dales
Artesian wells
AN AQUIFER often lies on top of a layer of less permeable rock.
This stops or slow the downward flow of groundwater, and helps to separate the water in this aquifer from flow in other aquifers that may be underneath it.
And unlike water in rivers or streams, groundwater is under hydrostatic pressure that is greater than atmospheric pressure.
Thus it can flow upwards, as well as downwards or sideways, just like water in the plumbing system of a house.
This is especially important when an aquifer dips beneath another layer that is much less permeable.
As usual, rainwater fills up the aquifer and water flows out from it where it meets the surface.
Below the impermeable layer, water is trapped in the aquifer, which is said to be confined.
The water presses on the less-permeable confining layers above and below it.
If you drill a borehole into a confined aquifer, water will rise up the borehole until the column of water is enough to balance the pressure in the aquifer.
Most major aquifers have a confined portion, where the aquifer is covered by impermeable rock layers, and an unconfined portion, where the aquifer is covered simply by soil, or river or glacial deposits.
Rainwater enters the aquifer through the unconfined part.
If we drilled many boreholes into the aquifer and found the level of water in all of them, we could imagine a surface made by joining all the individual levels.
In the unconfined part of the aquifer this surface would be the water table.
It separates the saturated part of the aquifer from the unsaturated zone above it.
But in the confined part of the aquifer, the spaces between mineral grains, every pore in fact, is filled with water: there is no unsaturated zone.
Here, the surface is an imaginary one called the pressure surface, or potentiometric surface.
It passes through the confining layer somewhere above the aquifer.
If the unconfined part of the aquifer is beneath high ground, and the confined part beneath low ground, then the potentiometric surface may be above ground level.
If we drill a borehole into the aquifer the groundwater will be under sufficient pressure to overflow from the borehole.
Such a borehole is called an artesian well.
But the borehole must go deep enough to reach the aquifer, even if this means drilling far below the potentiometric surface.
Otherwise it will yield little or no water from the impermeable confining layer.
In many dry or semi-arid parts of the world, the availability of water from artesian wells makes agriculture possible.
In Australia, many ranches use artesian wells, which can be relied upon to provide water unattended to stock in remote areas.
At the beginning of this century people in North and South Dakota even used artesian wells to power machinery — until they realised the dangers of taking too much water and depleting the aquifers.
When groundwater flows naturally or is pumped from an unconfined aquifer, it drains from the pores.
When water flows out from an artesian well in a confined aquifer, none of the pore space drains completely.
The water that flows out comes from elastic storage.
Groundwater in a confined aquifer is compressed elastically, rather like air in a tyre.
If you open the valve on the tyre, air flows out until the pressure inside the tyre equals that outside.
Then the flow ceases although the tyre still contains air.
Although water is far less compressible than air, the aquifer holds a lot of water compressed in this way because it is much larger than a tyre.
Also, the aquifer itself is slightly elastic; the pressure of the groundwater it contains forces apart the mineral grains and increases the volume of the pores slightly.
Even if no more water were to flow into the aquifer, water would flow out, like the air from the tyre, until the pressure was no longer enough to lift the water above ground level.
At that point we could still take water from the aquifer, but we would have to pump it out in the same way as we would from an unconfined aquifer.
If we pumped long enough we could eventually bring the potentiometric surface below the top of the aquifer.
At this stage, part of the aquifer would no longer be saturated.
But before we reached that point, we might have other problems.
When the air escapes from a tyre the tyre goes flat.
Although it still contains air, it can no longer support the weight of the car.
In the same way, the pressure of the groundwater in a confined aquifer helps to support the overlying rock layers.
When this pressure drops, more of the weight is transferred to the rock of the aquifer itself.
If the rock is weak, or contains weak layers, it may be compressed and the ground above could subside.
As a result, in parts of California and Mexico City the ground has subsided by several metres causing spectacular and costly damage to buildings.
Long term residence
Natural storehouses
EXCEPT where rocks are heavily cracked or fissured, it usually takes many years for water to find its way down through the soil and the unsaturated zone to the saturated zone of an aquifer.
Once there, it can take tens or thousands of years for that water to emerge from a spring or a well.
Australian hydrogeologists have used radioactive isotopes as natural tracers to determine how long water resides in the Great Artesian Basin of Australia.
They can now say that some of it has been in the ground for more than a million years.
The time that water spends in aquifers is an indication of their importance as natural stores of water.
The water cycle is often depicted as a simple system of circulation that takes water from the sea as vapour, deposits it on land as rain or snow and returns it quickly to the sea by way of rivers.
Here, the ocean is the only large store of water and the only place where a water molecule stays for any length of time.
The rest of the cycle is portrayed as a continuous path with no storage.
In fact, the cycle often transfers water between two stores, the ocean store (the primary and largest one) and the secondary store (the icecaps, glaciers and groundwater).
The residence time in, for example, an aquifer can be millions of years, and it may be even longer in an icecap at high latitudes.
Virtually no water infiltrates deserts such as the Sahara at present, but there are some large aquifers beneath them.
Some of the water in these aquifers comes from recharge in wetter areas nearby, but much of it is ‘fossil water’, rain that fell in the last Ice Age.
In Libya, a huge pipeline is under construction to carry water from desert wells to coastal towns and farms.
Schemes such as these are the equivalent of ‘mining water’, that is to say using a resource that is not replenished.
Some of the water in very deep aquifers may not have entered them as rain.
It may have been trapped in the pores of rocks that were formed beneath the sea; it is saline, and not good for drinking.
Another theory is that some is juvenile water, released from molten rock deep within the Earth's crust, often during volcanic eruptions.
In fact, much so-called juvenile water is probably either rainwater that has circulated to great depths, or water that was trapped in rocks that have been carried down at subduction zones (see Inside Science, No. 6, 23 February 1988).
If we include all the deep groundwater that we can rarely use, as well as the water trapped in pores in sediments such as clays, the total volume of groundwater is probably more than 50 million cubic kilometres.
Of this, 4 million cubic kilometres is a reasonable estimate for the freshwater we could extract.
It excludes water that will not drain from small pore spaces, saline water, and water in deep confined aquifers.
Safe to drink
How much treatment?
WHATEVER its origin, groundwater spends a long time in an aquifer.
It has time to dissolve minute quantities of minerals which can give it definite characteristics such as hardness or taste.
Because of this, many people find groundwater more pleasant to drink than water from rivers and lakes.
It is often bottled and sold as mineral water.
Another attraction of groundwater is its safety.
The long residence time generally means that any harmful bacteria that enter an aquifer do not live long enough to pose a threat to anyone using groundwater.
So groundwater rarely needs treatment.
This is particularly useful for small communities and in developing countries, where special water treatment is not available or prohibitively expensive.
In countries such as Bangladesh, for instance, there have been major campaigns to give each village its own borehole and hand pump.
The consequent supply of water free from pollution has greatly reduced the incidence of waterborne diseases such as diarrhoea and cholera, which formerly claimed many lives.
The long residence time even means that short-lived radioactive substances, such as those released by the Chernobyl accident, will be largely harmless by the time they emerge from an aquifer.
The long, slow filtration process does not remove anything dissolved in the water.
But this time lag, combined with dilution by water already in the aquifer, means that if a pollutant enters the ground, it may not affect water supplies for many years.
And this may be the greatest threat to our reserves of groundwater.
Groundwater may become contaminated without our knowledge.
By the time we realise the danger, large amounts of pollutants may already be in an aquifer.
One pollutant that has raised concern is nitrate (see Inside Science number 37).
The nitrate problem has taught us that to protect our drinking water, we must monitor the unsaturated zone.
This gives us an early warning of the pollutants that may arrive in our groundwater in the future.
We must also study the processes by which pollutants decay and change.
Researchers are now looking at what happens to pesticides and industrial organic solvents, which will probably be the next threat to groundwater quality.
Aquifers in action: wet towels and dry summers
IF YOU hold a dishcloth, a towel or a capillary tube with its lower end just touching a container of water it will draw up water until an equilibrium is reached.
At this point, the forces of surface tension between the water and the fabric, or between the water and the tube, balance the weight of water.
Water will not drain from the fabric or the tube when you lift it clear of the surface.
If you could measure the pressure of the water in the capillary tube or in the pore spaces of the cloth, you would find that it was below atmospheric pressure.
The water was sucked into the pores.
If you now add water to the top of your suspended capillary tube or towel, water will drain from the bottom until equilibrium is regained.
Water in the pores of rock and soil in the unsaturated zone of the ground is held in place by the same forces of surface tension.
This water is also at pressures less than atmospheric.
Left to itself, water in the unsaturated zone would reach an equilibrium in the same way as the water in the capillary tube or the towel.
But over a period of time, two things disturb this equilibrium: evaporation and infiltration.
Evaporation, directly from the ground, or from the leaves of plants, which draw water from the ground, will take water from the top of the unsaturated zone.
This will increase the suction effect slightly and can make water move upwards, much as a lamp wick draws up oil to replace that being burned.
More important, in most areas, is infiltration.
Rain entering the soil disturbs the equilibrium in the same way as water added to the top of the capillary tube or towel; water drains through the unsaturated zone to the water table.
In regions with marked wet and dry seasons, or where rainfall is evenly distributed, but the winters are cool and there is a distinct growing season, the behaviour of the unsaturated zone becomes very important.
In the dry or growing season, plants take water from the unsaturated zone.
This dries out downwards, just as a wet towel dries out if it is placed with its top in a current of warm air.
Hydrologists measure the drying out of the soil in terms of the amount of rainfall that would be needed to make water just begin to move downwards again.
This amount is called the soil moisture deficit.
This deficit must be satisfied before any water can infiltrate downwards through the soil to replenish the aquifer.
Recharge normally happens in the winter or rainy season.
During summer, the soil moisture deficit usually prevents rainfall from reaching the aquifer, and the water table falls as water flows from the aquifer and is not replaced.
Sometimes the rainfall in winter is barely enough to cancel out the soil moisture deficit.
Then water levels in the aquifer do not recover, but may continue to fall as summer approaches.
The storage in aquifers is usually much greater than that in surface reservoirs, so for one year this is not too serious.
But a spell of hot dry summers with high soil moisture deficits, and of winters with low rainfall, would be serious for most aquifers.
Some of the world's most important aquifers are currently affected by this problem.
Much of South-east England depends on the Chalk aquifer for water.
Here, low winter rainfall and dry summers for two successive years have caused record low water levels in wells.
The Ogallala aquifer, which supplies many farms in the US Midwest, is suffering the same problem, as also are aquifers in Spain, Greece and elsewhere in Europe.
It is too early to blame climatic changes for these effects.
Much of the problem, in South-east England for example, stems from increased demand for water in a densely populated and relatively dry area.
As water is pumped from the Chalk the water table is lowered, in some cases falling below valleys so that streams dry up.
There is still some water in storage but it cannot be pumped out without this effect on the environment.
In the end, people have a stark choice: use less water or accept environmental deterioration — reduced flow of streams, or costly reservoirs flooding valuable land.
Water supplies: groundwater on tap
GROUNDWATER is useful in many ways.
It sustains the flow of rivers, from which we take water for drinking and many industrial uses.
And by sinking a well or borehole into an aquifer, groundwater can be pumped to the surface.
The borehole can often be sited precisely where the water is needed — in a village or factory compound, for example— so avoiding the need for expensive water mains.
Rather than operate a borehole pump whenever water is needed, the usual arrangement is to pump water from the borehole at regular intervals to a tank or distribution reservoir on high ground or on top of a water tower.
From here the water can flow by gravity to wherever it is needed.
A distribution reservoir holds enough water to supply the area it serves for about a day.
Pumping water from an aquifer lowers the water table.
This may reduce or stop the natural ‘overflow’ from the aquifer, causing springs or small streams to dry up.
In Libya, pumping from wells around the Kufra Oasis has reduced the natural outflow.
In South-east England, some streams on chalk flow less than they used to do; in part this is the result of abstraction.
People must decide which they value most — a cheap supply of water, or preserving the countryside exactly as it was.
But some changes are blamed unfairly on abstraction.
In the 1850s, when there was still little pumping from the Chalk aquifer, water levels were nearly as low as they are now; low rainfall was the cause then, and it is part of the cause of Europe's recent droughts.
Although pumping out water can bring problems if aquifers do not have a chance to recharge, groundwater is such an enormous resource that it would be unthinkable for us to stop using it.
But we should also be aware of the cost of alternatives.
We simply could not build, nor afford to build enough reservoirs to replace the aquifers we depend on today.
Reservoirs provide a relatively small supply of water and they are expensive to build, but relatively easy to understand.
Aquifers hold an enormous amount of water, and we do not have to raise the money to build them.
In contrast, we do have to invest time and money in understanding groundwater and its place in the water cycle.
And in the light of increasing concern about the possibility of global warming, the more we understand about our water, the better.
We may have to learn to live with a greater demand for water and less recharge to aquifers.
FURTHER READING
For a more complete but still simple introduction, see Introducing Groundwater , by Michael Price (George Allen and Unwin, 1985, £13.95).
A more general treatment of the hydrological cycle is provided in the third edition of Principles of Hydrology , by R. C. Ward and M. C. Robinson (McGraw-Hill, 1990, £15.95).
Groundwater Hydrology , by D. K. Todd (second edition Wiley, 1980, ppb £21.50), offers a more advanced treatment.
Overhead and underfoot: water soaks into the ground to collect within an aquifer, where the rock is permeable.
When the pores are full, the rock is saturated
Michael Price is hydrogeologist at the British Geological Survey, Wallingford, Oxfordshire.
All diagrams are by Peter Gardiner
Drill here: when an aquifer is confined by impermeable rock, only a borehole that hits the aquifer below the potentiometric surface will yield water.
And water will flow without pumping — in an artesian well — if the ground is also below this imaginary surface (far right)
Where in the world is the water?
After the sea, groundwater is the biggest store
The next Inside Science will be published on 9 March and will look at biological pest-control.
The first steps out of the greenhouse
From fridges to cars, we already have the technology we need to curb the greenhouse effect.
What stops us taking the first step and using it?
Sue Bowler
LIGHT BULBS are a symbol of all that stands between us and the greenhouse effect.
If everyone in the developed world made energy efficiency a priority in all parts of their lives, the carbon dioxide we add to the atmosphere could drop quickly enough to make a real difference to global warming.
There is no need to wait for technologists to develop new, efficient equipment — you can already buy devices that will save energy and money.
But no one seems particularly interested.
The question is not what can we do about the greenhouse effect, but why don't we do anything?
Over the past few months, governments have been considering action to cut back on emissions of greenhouse gases.
They have taken as read the scientific conclusions of the Intergovernmental Panel on Climate Change that global warming is a real threat.
But plans, such as Australia's aim of 20 per cent cuts in carbon dioxide emissions by the year 2005, mean that everyone should make that 20 per cent cut in the emissions they generate, directly and indirectly.
Nations have to do this at the same time as they keep up economic growth, or accept a drop in standards of living.
Developing countries have a harder problem; they need a huge increase in prosperity, which could produce a corresponding growth in greenhouse gas emissions.
Industry is the main way countries bring in the money they need for food, education and health.
If efficiency forms a part of development, then a country will be able to spend less on fuel and power, as well as releasing fewer greenhouse gases.
Rich countries can afford to spend money on the environment, but few have made it a priority in the past.
Poor countries need wealth, without necessarily following the path taken by the industrial world.
Energy efficiency has a role to play in both cases, but we cannot depend on technology alone.
Society and culture have an equally important place.
In order to make the changes that climatologists say we need, scientists and technologists will need to talk and listen to economists, social and political scientists.
One effort to foster communication between such groups of specialists took place over a week in Berlin at the end of last year.
The occasion was a Dahlem workshop organised by the Free University of Berlin.
Its goal was to identify what stands in the way of effective action on the build-up of carbon dioxide in the atmosphere.
The workshop was convened by Graeme Pearman, an atmospheric chemist at the Commonwealth Scientific and Industrial Research Organisation's Division of Atmospheric Research at Mordialloc, in Victoria, who summed up the issues.
‘Scientists and engineers are devising ways of living that drop carbon dioxide emissions, but do we want to live that way?’
The workshop identified key areas where lack of knowledge and research are hindering effective and speedy action to combat global warming.
Principally, they concern the spread of energy efficiency in both industrialised and developing countries.
Scientific and technological advances will help the world deal with greenhouse gases, but not for a hundred years or so.
Simply using energy and fuel efficiently could stop us releasing much of the carbon dioxide we currently emit.
We do not need to wait for new efficient technology: as Stewart Boyle described in ‘More work for less energy’(New Scientist , 5 August 1989), the technology exists to take the first step towards the targets we have set to save energy and fossil fuel, and to cut back on carbon dioxide emissions.
But little of the latest technology makes its way into ordinary homes and businesses.
One reason is that people are not the ‘cost-rational economic consumers’ beloved of energy planners.
When choosing say, a new fridge, do we worry about the power it uses compared with our existing model, or do we go for the one with the clever arrangement of shelves that gives plenty of room for the mineral water?
Even if people do worry about energy use, how can they find out how a particular model performs?
Some manufacturers provide figures on the energy that new appliances use, but there is usually nothing to compare them with.
It is impossible to work out what your existing matching consumes from a quarterly electricity bill.
Clearly labelled appliances, frequent, itemised bills, and ‘smart’ meters that keep track of where the power goes might give the concerned consumer a chance.
In business, the problem is magnified.
One of the biggest obstacles to an efficient industrial society is the instinctive feeling that equates emitting less carbon dioxide with privation — using less energy, producing less and losing profits.
This is not necessarily so: the US used roughly the same amount of energy each year from 1973 to 1985, while its gross domestic product grew by more than a third.
In the accompanying article, ‘How to save energy and stay rich’, Debora MacKenzie describes plans for Sweden and the Indian state of Karnataka that increase economic growth while diminishing carbon dioxide output, or increasing it only slightly.
So what will make us move from plans to action?
Amory Lovins, from the Rocky Mountain Institute in Snowbird, Colorado, put his faith in the simple argument that efficiency saves money.
‘Electricity is the costliest form of energy, so it is the most lucrative kind to save’, he said.
His institute claims the record for growing bananas at the highest altitude using passive solar heating alone, among other more widely applicable examples of energy efficiency.
Lovins reckons that by improving the efficiency of lighting, especially commercial lighting, the US could save a quarter of the electricity it uses.
According to Lovins, ‘the equipment more than pays for itself by costing less to maintain’.
Lee Schipper, an energy demand analyst from the Lawrence Laboratory in California, agreed.
He is resigned to public indifference to the benefits of efficiency, as well as to the effects of greenhouse gases.
To achieve changes, he suggests selling it not as an end in itself, but as a way to make money.
‘If you approach energy conservation as an energy problem, you will fail.
Accelerating efficiency comes from pushing profitability.’
Schipper favours a back-door approach, appealing to those who use energy and benefit directly from using less, rather than those who produce it.
‘Go to housing or cars, not energy, for efficiency.’
Something as simple as setting building standards that improve insulation could cut heating bills, but Lovins suggested that more far-reaching effects could come from thinking about the energy used in the whole process of building.
If you start from scratch and design processes that do not need high temperatures, you can use solar energy to the full.
‘You can build a house from limestone in three ways: using blocks of stone, cement or chickens,’ says Lovins.
Cutting stone needs power tools and cement needs heat; chickens manage to make eggshells — a strong, light material — without either.
‘If we were as smart as chickens, we could utilise ambient temperature technology.’
Environmental know the value of efficiency in itself, and of its role in diminishing carbon dioxide emissions.
But to most people, the greenhouse effect is at best remote — certainly nothing that we can alter.
Igor Bashmekov, an economist from Moscow's Institute for Energy Research, put it most succinctly: ‘Citizens do not care about carbon.’
Cunning plans for cars
Take the example of highly efficient light bulbs: they cost several times as much as equivalent incandescent bulbs, but can last 13 times as long, so the household budget benefits quickly.
But if you don't have the figures, don't see the bulbs in your local shop, and can't fit your best lampshade onto them even when you have managed to find them, you are unlikely to choose them and save energy.
Lovins suggested that policymakers should be more devious, and make ‘good equipment easy to use, and bad equipment hard’.
He cited a scheme in Ohio that boosted the sales of high efficiency light bulbs, with consequent savings of electricity and money.
Shops took incandescent bulbs off the shelves and kept them under the counter; the only light bulbs in sight were the high-efficiency versions.
If you wanted to buy an ordinary bulb, you had to ask for it specially.
Other stratagems he suggested were offering prizes and lots of publicity to firms who produced the most economical designs, and bonuses for retailers who told efficient appliances.
It may yet be possible to convince manufacturers, governments and the public that energy saving is worthwhile for its own sake.
But to achieve this, researchers need to know far more about how people live.
They have to address questions such as why people buy an inefficient fridge, when more efficient models are available, and how consumers balance a greater initial cost with savings made over the life of an appliance.
Without information on what motivates consumers now, they cannot make reliable models of how things might change — for example, if labels did stress efficiency and savings.
The few useful studies that exist usually include factors that might be peculiar to one country, or society.
Bashmekov emphasised how much these factors affected the action that even concerned people could take.
Most Moscow flats do not have individual metres, for example.
‘I can't do what I want in my room alone.
My block of flats has 14 floors, each with eight flats.
We have to work together.’
Transport is another area in which the discussion ranged from immediate action to changes only possible in the distant future.
Cars, trucks, planes and boats currently contribute more than a quarter of the annual greenhouse gases worldwide, a percentage that is likely to increase.
Eventually, we will have to look for different fuels, according to Larry Jenney, of the US Transportation Research Board, in Washington DC, but fuel efficiency is an essential first step.
Vehicles that can provide 100 kilometres of driving from 6 litres of fuel are on the market, as well as in manufacturers' laboratories, but few people buy a car solely because of its fuel economy.
Advertising speaks, in the main, of accelerations that are laughable on crowded city streets, top speeds that are illegal and handling that is beneficial only to rally drivers.
It is difficult to imagine a world in which the status of your car — and yourself, by implication — centres around miles per gallon, but making economy desirable and fashionable seems to be the way to change the habits of private car owners.
But cutting greenhouse emissions from transport will take more than just improving the efficiency of cars and trucks.
It will mean far fewer cars, and far less driving all round.
In the West, a car is more than a way of travelling; it represents freedom and flexibility and is a potent status symbol.
Public transport, in contrast, is something that most people use only if they have to, with notable exceptions in the cases of some European cities.
Schipper pointed out that researchers are again handicapped by not having enough data.
‘The literature on transport and behavior is not good enough.
We can't measure transport use in cities.’
Changes in these fields raise interesting problems.
Jenney wondered whether it is better to try and reduce congestion, or to let it increase, if we want fewer people to drive?
Speeding up urban traffic could save 10 per cent of fuel, and so cut back on carbon dioxide.
But there is a snag: more roads lead to more cars, in general.
And advances in technology have made congestion no longer a total waste of time.
If you have a radio, a cassette player, a car phone or fax, then those stationary hours in the car are not totally wasted.
One way to cut down on cars in cities might be to be to make this form of travel much less convenient, by cutting the number of parking places available in cities, for the public and companies alike — hardly popular measures.
Alternatively, and in the long term, plan building in cities to cut down on travel overall.
Transport is an acute problem for the people of Eastern Europe, affecting food supplies and industry directly.
‘The USSR has only a small number of private cars,’ says Bashmekov.
‘But there are not enough light trucks.
We need a better truck system, and better fuel economy for them.’
The urgent need for transport as industry develops can produce some strange solutions.
Converted tractors are increasingly popular as buses in China and India, according to Lovins, who said they use ‘75 per cent more fuel per mile than a fully loaded 4-tonne truck’.
But similar improvisation in Taiwan is producing efficient transport from motorcycles and scooters.
The pressure on establishing industry in Eastern Europe and the developing world makes efficient technology a priority.
But simply supplying the latest efficient machines, or sending experts to a country for a few months are not long-term answers, and do not address the most intractable barriers to action.
Ogunlade Davidson, of the University of Sierra Leone's Research and Development Services Bureau, in Freetown, emphasises that national culture, and the organisation of different societies matter.
‘Access to information is a problem in developing countries which do not have good infrastructure,’ he says.
The problem is finding out who to talk to, and what sort of information is available.’
One way to provide this information is through a network of ‘energy efficiency centres’, to provide information, act as contact points for outside aid, and perhaps demonstrate innovative technologies.
Energy efficiency centres already exist in Czechoslovakia and Poland.
Bill Chandler, of the Batelle Memorial Institute's Pacific Northwest Laboratory in Washington DC, has been instrumental in this initiative.
The centres aim to bring the best energy-saving technology into everyday use, by equipping planners, engineers and consumers with the information they need to choose the technology that will suit them best.
The practical bias is deliberate.
‘We can't choose priorities for developing countries’, says Chandler.
‘It's better to organise resources rather than sending experts.’
Plugging the gaps
The Czech centre opened in September last year, and the Polish centre is organising its offices and recruiting staff in Warsaw, Krakow and Katowice.
Much of the funding so far has come from the US, through the Environmental Protection Agency, the Agency for International Development, and the US World Wide Fund for Nature's conservation fund.
Dennis Tirpak, of the EPA, spelt out his organisation's involvement: ‘We see our role as having provided seed money, but we hope that the centres would be able to pull on substantial sums from foundations and industries.’
Chandler stressed that the centres will not depend on US staff, ideas or money in the long run.
‘We want to make the centres independent, so that they will search for clients.
We are just the first ones.’
Each centre aims to build up in a few years the basis for energy conservation throughout industry, transport and construction — an ambitious target, which Chandler feels is achievable.
‘I'm so impressed with the speed with which the Czech centre has moved that I'm confident about the future,’ he said.
‘The staff are beginning to take over, which is what we hoped would happen.’
According to Chandler, delays in saving energy arise mainly from a lack of information and lack of resources.
He sees the efficiency centres plugging those gaps: they will act as information banks and, more importantly, set standards and policy for buildings, electrical appliances, labelling, testing, training and education.
This framework is the necessary first step in changing the way countries in Eastern Europe use energy.
If a government wants to impose standards for the efficiency of new buildings, then it must also ensure that the materials are available, and that people are trained to use them.
The centres will be able to look at all aspects of energy use in a society, and perhaps help convince people with very different interests.
‘If Czechoslovakia could persuade its utilities to buy efficient light bulbs,’ Chandler says, ‘then it would be worth building a factory to manufacture them.’
One task for the centres will be to seek out and assess projects suitable for joint ventures with companies or investors elsewhere.
Factories need insulation, he says, but no one manufactures a suitable product locally.
‘At present, these materials have to be imported.
We would love to see companies producing insulation, perhaps with money invested by other countries.
We have to create an industry to support energy conservation.’
The role of each centre will depend on local needs — that is one of their strengths.
Priority areas for the Polish programme include training the people who control small boilers, the mainstay of local industries, setting up ‘car-care centres’ to tune cars and save fuel, and working out a system of efficiency labelling.
At each centre, staff will coordinate and select the best methods.
Much of their work will consist of assessing how much impact the measures are having.
Requiring temperature controls in factories would mean that both thermostats and insulation would have to be imported into Poland, until factories could be established there.
One of the first projects for the Krakow centre will be the modification of an existing hospital to make it an energy efficient building.
Such ‘retrofitting’ will be an important part of the changes needed to save energy fast.
‘Hospitals are not high priority places in themselves, but it is a good way of training and demonstrating what can be done,’ says Chandler.
‘It's a very visible place.’
One advantage of setting up these centres is that they may help a two-way flow of ideas for efficient technology, possibly serving as a model for changes to Western society.
Ideas and techniques from developing countries could have special value in the energy-conscious future.
But most important of all, such centres could help spread the message that the western way of life need not be a model for all development.
Countries can become prosperous without repeating the mistakes that the industrial world has made.
As Bashmekov concluded, ‘perhaps Americans could say that their way of life is not the best in the world, rather than the opposite’.
Lack of communication underlies the very different problems facing developed and developing countries alike.
To achieve international agreement, different countries will have to understand each other 's societies.
Different groups in society see the environment and the way we live in it in diverse ways.
Nature is a precarious system, likely to accelerate to catastrophe if tampered with, is one extreme view.
A stable, or resilient world, at another extreme, takes little notice of our presence.
It is not surprise that ‘deep green’ environmental groups, closer to the first view, find it difficult to communicate their concerns to businesses and governments who feel more at home with the second.
Although many of the gaps in understanding lie in nebulous areas such as society and economics, the responsibility for plugging them falls on professionals in a range of fields.
Engineers need to know how economists think, and vice versa.
Scientists will have to make their priorities clear, in terms that their audience — whether professionals in another discipline or the general public — can appreciate.
And if governments wish to achieve their targets for cutting carbon dioxide emissions, they will have to address the priorities of industry and electorates.
As Lovins said: ‘You have to talk to people where they're at, not where you're at.’
CLIMATE CHANGE
QUIT SMOKING!
Energy answers for North and South
Planners in Sweden and India think their countries can meet their energy needs, cut emissions of greenhouse gases — and get richer — by the end of the century
Debora MacKenzie
ON THE FACE of it, Sweden wants to do the impossible.
The country produces half its electricity from 12 nuclear plants, the highest proportion of nuclear power per inhabitant in the world.
But last month, the Swedish government renewed a promise to close down the nuclear plants within 20 years (This Week, 26 January).
It has also promised not to build any new hydroelectric dams, the source of the rest of the country's electricity.
And to top it all, it has pledged to maintain high employment and an annual economic growth rate of 1.9 per cent.
In conventional planning, this would mean that Sweden should build new power plants that burn coal or oil, to replace the nuclear power.
But Sweden has also promised that in 20 years its emissions of carbon dioxide (CO 2 ), the main greenhouse gas, will have dropped to 1986 levels.
This means burning less, not more fossil fuel.
Can the Swedes do it all?
No, say conservative politicians and industrialists, who are campaigning to save the nuclear plants.
Yes, say Thomas Johansson and colleagues in the Department of Environmental and Energy Systems Studies at Lund University.
Johansson arrives at this positive conclusion by using a new kind of energy analysis, which compares the costs and benefits, both economic and environmental, of the ways a society can meet its energy needs.
And the new method of analysis shows that developing countries such as India, as well as rich countries like Sweden, can meet their people's needs for energy without breaking the bank or destroying the environment.
Cost, and the fair comparison of costs, is critical.
No energy planner will ever convince a government to change its sources of energy or invest in energy efficiency without a cost estimate, and preferably one that quantifies the cost of protecting the environment in a way that electorates can understand.
Energy planners have also developed ways of including the energy efficiency of each fuel source in cost-benefit models.
Johansson has analysed Sweden's future energy needs using a model of this type.
He looked at the sectors of the power industry that the closure of the nuclear plants will hit hardest: electricity, its combined production with useful heat (cogeneration), and district heating.
These account for 63 per cent of the primary energy used in Sweden.
But it is not industrialised countries such as Sweden that matter most.
The world's climate depends on whether China and India can develop without producing tonnes of CO 2 , as the industrialised countries did while they were developing.
The biggest conflicts looming over negotiations for a climate treaty, which begins this month, involve the money that rich countries must give to poor ones to enable them to develop with reduced CO 2 emissions.
Amulya Reddy and his colleagues at the Indian Institute of Science in Bangalore say India can improve its standard of living, generate less CO2, and spend less on energy.
Reddy calculates that, rather than costing more, environmentally benign energy will cost far less then plans that continue with the energy policies of the past.
Both Johansson and Reddy reached their conclusions by using a simple, yet innovative approach.
The ordinary householder might think it normal to estimate needs, list the various ways of filling them, then adopt the cheapest options.
But the energy business does not work that way.
According to Johansson, government planners typically assess how energy demand has grown alongside economic growth.
Then, for an economy forecast to grow a certain amount, they predict that a corresponding amount of extra energy will be needed.
They then build the plants required to generate the energy.
It is as if you discover that you spent £1000 last month, and therefore assume you will have to spend £1000, or a bit more, next month, without ever wondering if some of your expenditure was wasted.
The alternative, says Johansson, is to plan your power supply in terms of the services you need, considering the possibility of making existing energy go further, as well as simply generating more power.
The point is to provide ‘hot showers, cold herring’ and whatever else electricity users want, rather than power for its own sake.
This is the way the first power companies worked.
In 19th century New York, Thomas Edison's electric company sold not electricity, but its end product — illumination.
The crafty Edison also held the patent on the light bulb.
His profits climbed as the efficiency of the bulbs increased, and he could sell more lighting as a service, without having to pay to generate more power.
It may not be possible now to reorganise the power industry to sell services, rather than energy, but it is at least possible to plan in terms of the uses to which the energy must be put.
The planner can then show how to meet those needs, either by generating more power, or by using existing supplies more efficiently, for the least cost.
Least cost might mean less money required to install equipment or buy fuel.
On the other hand, it might mean less CO 2 .
That is the analysis Johansson has done for Sweden, and Reddy has done for the state of Karnataka, India.
The results show that Sweden can spend less than it now expects on district heating and electricity and still cut CO 2 emissions by 35 per cent by the year 2010 (see Figure).
Reddy estimates that Karnataka could meet its energy needs better than it could under current plans, while avoiding the doubling of CO 2 by the year 2000 that those plans would bring.
At the same time, the state could save the $3000 it would have cost to produce each tonne of CO 2 .
In Sweden, if there are no increases in energy efficiency, the electricity demand in 2010 will be for 194 terawatt-hours (TWh) of energy — half as much again as demand today.
(Each TWh is 1000 million kilowatt-hours, kWh.)
But with electricity prices expected to increase 50 per cent by 2010, the extra cost of power should, in itself, lead consumers to increase their own energy efficiency.
Johansson calculates that efficiency will increase enough in this way to allow the demand for electricity to be met by only 140 TWh.
The government can, however, promote a more extensive adoption of efficient technology than would be achieved by relying on such market forces.
Methods could include imposing efficiency standards, allowing tax concessions for efficient appliances and replacing electric space heating, common now in Sweden with its abundant electricity, by more efficient heat pumps and fuel.
If such measures lead to replacement of all today's inefficient equipment as it wears out by the most efficient equipment now on the market, Sweden will need only 111 TWh of electricity in 2010, rather than 140 TWh.
If consumers adopt energy-efficient equipment currently in the research and design stage as well, Sweden can meet its electricity needs with 96 TWh by 2010 — a drop in power requirements of a third.
This, says Johansson, is the benefit of calculating energy requirements in terms of need, rather than supply.
Increased efficiency can be taken into account.
The environmental impact of such increases in efficiency, however, depends on how you generate the power.
Johansson's team calculated the effect of different energy mixes, used at different levels of efficiency.
One energy mix simply chose the cheapest options for supplying energy — using entirely coal, oil and natural gas.
Another chose energy sources that minimised the production of CO2.
The solution that produced least CO2 used no coal, some oil and gas, but substantial biomass, a renewable fuel that does not increase the net amount of CO 2 in the atmosphere.
Vatenfall, the Swedish energy board, estimates that Sweden already has 50 TWh per year of unused biomass power, chiefly the branches, bark, sawdust and other residues from the timber industry.
To this Johansson's team added another 40 TWh per year that Sweden could produce from energy plantations — fast-growing trees planted solely as fuel.
They calculated the energy, and costs, that would result if the wood were used to fuel highly efficient cogenerating power stations, where both the electric power generated by steam, and the waste heat, are used.
They rounded off the environmental scenario by including the 3 TWh of wind power Vatenfall thinks could be installed in Sweden by 2010.
The team then calculated the costs, both in money and CO 2 , of each energy mix at three different levels of efficiency: efficiency improved only by market forces, adoption of all commercially available efficient technology, and using efficient technology now in the planning stage.
Not surprisingly, the more energy efficiency was included, the cheaper energy became.
More surprisingly, there was little difference in price between scenarios which optimised costs, and those that optimised CO 2 .
All the options came out filling Sweden's energy needs at a cost of between $5.5 and $7 billion per year, or from 2 to 2.9 cents per kWh.
Electricity in Sweden now costs 2.8 cents per kWh.
The real surprises were in CO 2 production under each scenario.
If the basic source of power was fossil fuel, no amount of efficiency was enough to reduce CO 2 emissions to the 1986 level of 11 million tonnes per year — equivalent to 3 million tonnes of carbon — as Sweden has promised.
Even with maximum efficiency, this plan, although otherwise the cheapest, produced 7.6 million tonnes of carbon per year (see Figure).
But even by relying on biomass, Sweden will not meet its targets unless its overall energy needs are reduced by efficient technology.
Maximum use of biomass, plus market-driven energy efficiency, still produced 5 million tonnes of carbon per year.
Using commercially available, efficient technology in the plan using maximum biomass, carbon emissions drop to 2.7 million tonnes.
The target can be exceeded, with carbon emissions falling to 1.9 million tonnes, if the most efficient technology that researchers can imagine is adopted.
Johansson's team tested a third energy mix that did not achieve the target.
This was a halfway house in which Sweden used no coal, and only its existing biomass, without the energy plantations.
The rest came from natural gas.
Gas is promoted in some countries, including Britain, as a way to reduce emissions from fossil fuels, because it generates less CO2 per unit of energy produced.
In Johansson's model, this option produced lower emissions, but they were not low enough.
It produced 3.8 million tonnes of carbon per year, even with the highest levels of energy efficiency.
The difference between the cost of energy under these low-CO 2 scenarios and the costs of the other plans gives the cost of achieving Sweden's CO 2 target.
The plan that produces the least CO 2 costs less than simply letting market forces prevail, because its high level of energy efficiency saves enough money to pay for the investment needed to switch to biomass production.
Johansson estimates that under such a plan Sweden would gain $40 for every tonne of carbon it does not produce that it would have produced under market forces alone.
But this would not be the cheapest option.
Carbon emissions under the cheapest plan are less than under market forces, thanks to the efficient methods it uses.
But they are far too high to meet Sweden's target.
The difference in cost between the cheapest scenario and the plan that produces least CO 2 is $102 for each tonne of carbon not released.
This may be taken, says Johansson, as the cost to society of adopting an energy policy which minimises CO 2 , versus one that minimises costs.
The Swedish government plans to introduce a tax on carbon emissions in 1991, at a rate of $150 per tonne of carbon.
This would generate the extra revenue needed to pay the higher cost of replacing nuclear energy with biomass, compared to replacing it with fossil fuels.
But it is one thing to say that the rich, orderly Swedes can put their energy house in order with a minimum of CO 2 .
It is quite another to assert that huge and poor India can do the same.
Yet India has a stronger case for putting its house in order, according to Reddy.
The Third World faces an energy crisis even without the problems posed by global warming.
In 1987, the state of Karnataka produced a ‘long range plane for power projects 1987–2000’, known as the LRPPP.
It predicted future power needs in the classic manner, by assuming they would continue to grow at 9 per cent per year.
This predicted an increase in demand from 2500 megawatts in 1986, to 10 000 megawatts in 1999.
Producing this would require the investment of $17 billion, at $3.3 billion per year, a sum Reddy describes as ‘astronomical’.
The plan depends on the building of a 1000-megawatt station fired by fossil fuel, and quadrupling the size of the planned Kaiga nuclear power plant.
Despite this, the LRPPP predicted continuing power shortages in Karnataka.
Reddy disagrees with the assumption that such expensive energy supply is necessary.
But it is an assumption that is made throughout the developing world, where current energy plans would cost $100 billion per year.
These expansive plans will never be realised; only $4 billion is available from the World Bank for developing electric power, and $16 billion from other aid and loan sources.
So there must be another approach.
Reddy says that traditional plans also lack a ‘development focus’, and take no account of how the benefits of energy are distributed in society.
In developing countries, energy can be used wastefully by whoever has the money to use it, while the poorer and less powerful go without.
This creates situations such as that in Karnataka, says Reddy, where one aluminium plant uses 20 per cent of the present power supply, in a world with a glut of aluminium, and a state where not all houses have electricity.
Reddy's alternative analysis of Karnataka's energy needs includes this political focus on meeting development needs.
‘We arrived at the energy required in the year 2000 by estimating the growth rates for each category of consumer,’ says Reddy.
The constants in the model were growth rates in electricity use by commercial premises, industry that uses little electricity, and irrigation, which now takes 20 per cent of Karnataka's electricity.
They speeded up the spread of electricity until all homes were connected.
And they shut the aluminium smelter.
‘The aluminium plant was a bad plan,’ says Reddy.
‘It is cheaper to address the needs of the people.’
Reddy employed the same technique as Johansson to arrive at the least expensive energy — first estimating needs, then filling them through sources of power and energy efficiency, starting with the cheapest method, and when that was used as fully as possible, moving to the next cheapest.
Unlike Sweden, however, Karnataka's low-cost solution also produced least CO 2 .
This is because Karnataka does not have to replace nuclear plants with alternative power sources, where the least environmentally damaging happens to be most expensive.
It aims to expand the energy supply, and it has a broader choice of energy sources.
Small-scale hydroelectric power, which has mostly already been exploited in Sweden, and biomass are both cheaper and quicker to implement than fossil fuels in India.
Karnataka also has a range of options for cutting energy use, for example by switching cement production, which now consumes 20 per cent of the state's electricity, to a less energy-intensive technology.
The combination of development focus and end use gave the model its name — Defendus.
‘It is the only thing that will defend us in the present crisis,’ says Reddy.
Defendus came up with a projected energy requirement in 2000 of about twice the current consumption, some 40 per cent of the projection made by the LRPPP.
Reddy says that about 40 per cent of that reduction came from improving efficiency.
The rest came from increasing the electricity supply only until it met specific development needs.
Largely because of the ‘development focus’, the plan would cost a third as much as the LRPPP.
Such savings may allow energy planners like Reddy to muster the political clout it takes to close aluminium smelters.
‘From an energy point of view, it is very expensive to keep poor people poor,’ says Reddy.
‘It is far cheaper to address the needs of poor people than to ignore them, since poor people consume far less’— less, that is, than those who now use the lion's share of energy.
The cash crisis facing developing countries means that lending organisations, such as the World Bank, are listening to Reddy.
‘They say, we know you can't continue in the old way,’ he says, but until now there has been no alternative.
‘Least cost planning is just starting,’ according to Reddy.
‘This is the first analysis for a developing country.
Before, we knew you should save energy, but all we could do was wave our hands.
Now we have a proposal with numbers on the table for the first time.’
The most compelling numbers are for CO 2 emissions.
The LRPPP, with its heavy reliance on massive coal-fired power plants, would produce an extra 830 000 tonnes of CO 2 each year, more than doubling current emissions by 2000.
Defendus would increase emissions by a mere 11 per cent.
The costs of the two plans are very different: Reddy calculates Karnataka would gain $3000 for each tonne of carbon it does not generate, that it would have under the LRPPP.
The question begged by all these glowing predictions is whether they will ever be fulfilled.
The cash crisis in the Third World will help concentrate the minds of lenders.
Third World industry could also become an ally in promoting efficient technology.
At present, by cooperating uncritically with energy suppliers, ‘industry is kept inefficient’, says Reddy, and its energy costs are high.
It could profit from increased efficiency, as long as the technology is available.
Reddy's shopping list starts with ‘compact fluorescent light bulbs, efficient motors, better pump sets, and solar water heaters’.
All are on the market today.
Meanwhile, the political difficulties in the way of Reddy's plan are only too evident.
Karnataka's aluminium smelter may make little sense to an energy planner, but it makes a lot of sense to the politically powerful elite that reap the profits, small though they may be, from the production of aluminium.
The growth of the middle class in industrialising countries such as India could play an important role here.
They have to pay for plans such as the LRPPP, and if it chiefly benefits big energy users, pressure to switch to more people-oriented plans such as Reddy's could mount.
‘Developing countries are afraid funds for combating climate change will come out of funds that would have gone for development aid,’ says Reddy.
‘This is fallacious.
We can advance development with the bonus of decreasing our impact on climate if we have the proper energy strategy.
People miss the point in thinking extra investment will be required (in the Third World) to combat climate change.’
The question now is whether energy planning will continue to be dominated by big oil, big plants, and big bills for the taxpayer and the planet.
GET IN SHAPE!
CLIMATE CHANGE
Costing the Earth?
If Sweden chooses energy policies that put the environment first, it can reach its target of cutting carbon dioxide emissions to 1986 levels by the year 2010
‘It is far cheaper to address the needs of poor people than to ignore them’
REVIEW
Minamata and the search for justice
Paradise in the Sea of Sorrow: Our Minamata Disease by Michiko Ishimure, translated from Japanese by Livia Monnet Yamaguchi Publishing House, pp 365, 5000 yen/$35 
Michael Cross
A LONG time ago and a long way away, a factory dumped sludge containing organic mercury into the sea.
The mercury found its way into fish.
Some people who ate the fish died in nasty ways.
Others gave birth to children with almost unimaginably awful deformities.
The town where this happened, Minamata, became the name of a disease and a worldwide symbol of industrial pollution.
If this explanation is simplistic, it is because there is little point in saying more about Minamata now that the classic written account of the tragedy is available in English.
Paradise in the Sea of Sorrow appeared in Japanese in 1969, the first part of a trilogy.
It spans the first 15 years of the disease, from its appearance in the early 1950s to the Japanese government's official statement that mercury pollution was to blame.
Don't expect a systematic or even chronological history.
Paradise is the work of a passionate activist, written at the height of the struggle to win recognition of the disaster from the Japanese establishment and even people in Minamata itself.
The key word in the title is ‘our’— these are the participants' voices.
And as so often happens when ‘victims’ have the chance to tell their own story, the lasting impression is of human dignity.
These are people who were too proud to register for compensation, who were shy about meeting delegations from Tokyo and who were diffident about making a political issue over ‘our little pollution incident’.
They faced the prospect of an agonising premature death with anger and humour, stoicism and frustration, wisdom and stupidity.
In short, as human beings.
Mercifully, Ishimure leaves to our imagination most of the physical horrors.
The little details she slips in become almost unbearable: a teenage boy who weighs no more than a wooden Buddha; a fisherwoman raging at the routine indignities of being in a hospital bed.
The book's format, mixing victims' accounts with extracts from clinical and government reports and the author's own powerful descriptions of dignity and suffering, matches the subject ideally.
The result, rendered into beautiful and economical English, lives up to the publisher's claim of ‘a new literary form’.
Even the abrupt and anticlimatic conclusion tells a story.
For, almost incredibly, the story is still not over.
Nearly 35 years after the first lawsuit, arguments over responsibility and compensation for Minamata are still ploughing through the Japanese courts.
The government is resisting calls for a ‘quick settlement’ because of the principle involved.
The official responsible for explaining this stance hanged himself the week before I read this book.
I do not know how easy it will be to find Paradise in the Sea of Sorrow outside Japan.
Perhaps a canny distributor will see possibilities for three trendy niche markets, Japanism, environmentalism and feminism.
But if you get a copy, save it for when you can read it at one sitting.
Then go for a walk and let the night air dry your tears.
The colour of the future
Green or Bust by James Wilkinson,BBC Books, pp 202, £4.99 
Chris Baker
IN THE past year ‘green’ books have been flooding onto the market so I had to ask myself: ‘What more can Green or Bust offer?’
A glance at the awful title suggested the answer ‘nothing’ but I soon found that this book is not like all the others.
It does not aim to tell readers what they can do to help the environmental cause, nor is it written in an alarmist way.
Instead the book details the major green issues that have been in the headlines, such as the greenhouse effect, acid rain and the hole in the ozone layer, offering a history of each and listing the consequences and possible remedies.
All are dealt with in a clear and readable manner.
The science is handled well and often the explanations begin with the basics, such as the structure of oxygen when discussing what ozone actually is.
But the book tells us nothing new.
It offers little discussion of some topics and is not controversial (contrary to the claim on the back of the book).
So I can recommend this book only to someone who has little knowledge of green issues and the science involved.
The book simply summarises information that most people know.
Gender engenders science
The Science of Woman: Gynaecology and Gender 1800–1929 by Ornella Moscucci,Cambridge, pp 278, £35 
Ludmilla Jordanova
IN THE 19th century, gynaecologist Robert Barnes declared: ‘Any disease occurring in a woman will almost certainly involve some modifications in the work of her sexual system’.
In her lively and original book,The Science of Woman , Ornella Moscucci explores the heated medical and scientific debates about the nature of ‘woman’ as they took shape in the new medical speciality of gynaecology.
Once new fields are delineated they come to be seen as natural, their boundaries appear to derive from logic, and a world in which they had no place becomes unimaginable.
A detailed study of the gestation and birth of a new specialism reveals a different story.
Gynaecology enjoyed an intricate and fraught relationship with obstetrics, which had its origins in the 18th century.
Both gynaecology and obstetrics challenged traditional lines of demarcation between medicine and surgery.
It was extremely difficult to draw such lines with clarity in practice, yet, at another level, the distinction was an important one given the existence of two powerful corporations (the Royal College of Surgeons and the Royal College of Physicians) and of their vested interests.
Furthermore, the context in which gynaecology and obstetrics developed during the 19th century was one of intense conflict among different kinds of medical practitioners, of tremendous change in the organisation of the medical profession, and of social and economic insecurity for many of its members.
The Science of Woman shows the difficult and painful steps that groups of practitioners took to shape a new field according to their concerns and interests.
Moscucci further argues that this historical situation is fully comprehensible only if we take into account the 19th-century debates about femininity.
It was concern about prostitution, venereal disease, maternal mortality and the control of female sexuality that gave gynaecology its distinctive qualities.
She shows how heated controversies about the use of the speculum, which was seem by some medical men as an instrument of rape, about operations that ‘unsexed’ and ‘castrated’women, and about the relationship between male doctors and their female patients can be explained more effectively by viewing gynaecology as a ‘science of woman’.
She also points out how deceptive that phrase is, with its implication that all women are somehow the same.
Medical practitioners were only too well aware of the social differences among their patients, and hence were as much preoccupied with class as they were with gender.
She suggests that there was a significant kinship between medicine and anthropology.
The latter arose out of the interest during the Enlightenment in a ‘science of man’, which would explain, scientifically, all aspects of human nature.
By extension, for many 19th-century commentators, there was an urgent need for a science of woman that would account for the nature of femininity.
Of course, the assumption behind this is precisely that enunciated by Barnes, that woman are dominated by their reproductive organs.
According to Moscucci, this claim commanded the general assent, although practitioners interpreted it in vastly different ways.
The book is particularly good at showing how diverse medical opinion was.
There were fundamental disagreements — both medical and moral — about basic procedures and operations, about the institutional settings that would best serve gynaecology and about the kinds of practitioners who should be involved.
Moscucci claims that women were far from being passive in relation to their doctors.
By avoiding simplistic assumptions about the united male medical profession manipulating supine female patients, she makes a major contribution to our understanding of a complex part of medical history.
Although she always insists, quite correctly, on the importance of social and economic aspects of medical practice, Moscucci also reveals the complexity of medical thinking, and the capacity some practitioners had for developing critical perspectives on their own procedures.
In this way she indicates the intellectual vitality and the social significance of medicine in the period.
Historians of science and medicine, like sociologists, have become especially attentive recently to the practice within hospitals, laboratories, educational institutions and consulting rooms.
They wish to develop alternatives to the traditional approaches that have given privileged positions to discoveries, innovative theories, to the world of ideas.
The value of Moscucci's approach is that she has provided a detailed picture of the development of gynaecological practice.
She has also drawn a wonderfully rich picture of the belief systems and scientific world views held by a distinctive, yet hitherto little-known group of medical men.
God's aims
On Purpose by Charles Birch,New South Wales University Press, pp 195, A$14.95.
John Habgood
‘FALLACIES of the modern worldview have to do with the conception of the world as substance or machinery, mistaking abstractions for reality, confusing origins and truth, failing to attribute feeling to things that feel, recognising ethics as exclusively anthropocentric, thinking a posteriori, objectifying facts as separated from values, reducing the complex to the simple and dividing knowledge into distinct disciplines that produce experts who are often wrong.’
In their place, Charles Birch would like a philosophy and religion ‘that makes room for purpose as an effective causal agent in the universe’.
This would entail a view of nature as organic and ecological, rather than mechanistic; an interpretation of lower forms of organisation in terms of higher ones, as well as vice versa; an acknowledgment of sentience much further down the organisational ladder than is at present commonly imagined; a biocentric ethic; and a holistic approach to knowledge.
Strong stuff.
It amounts to a call for a revolution in ways of thinking to break out of the constraints that a falsely mechanistic interpretation of science has imposed on ourselves and our world.
As a good biologist, Birch begins with life, with what we directly know of feeling, living, purposing and choosing.
His post-Cartesian starting point is: ‘I feel, therefore I am.’
On Purpose is a kaleidoscope of a book.
Birch is an ebullient polymath, a former professor of biology at the University of Sydney, no mean theologian, a philosopher and devotee of A. N. Whitehead, a distinguished member of the World Council of Churches, a raconteur, an activist in a multitude of good causes, and one of last year's recipients of the Templeton Prize, the most prestigious and valuable prize in the world for progress in religion.
Characteristically he has given it all away.
He writes as he lives: with passion, elegance and humour.
Although my brief summary of his main thesis may arouse fears that this is just another antiscientific diatribe, and though some of the ideas in it, notably his panpsychism, are hard to swallow, and though parts are maddeningly imprecise, persistence pays dividends.
Furthermore, he manages to articulate the feeling that something has gone seriously wrong with the so-called rational scientific world, as well as with traditional theology.
One of this aims is to develop an understanding of both in which they are seen as essential to one another.
God for him is the ground both of order and of novelty.
He is the persuasive presence rather than the all-powerful ruler.
He is the enabler who lures the network of events and relationships, which constitute the Universe, to fulfil its potentialities.
He is the holder and preserver of value in an everchanging process.
He is, in Whitehead's words, ‘the fellow-sufferer who understands’.
I am not myself much enamoured by Process Theology, which is the technical name for what Birch is expounding here.
It has won greater favour in the US than in Britain where it is generally regarded as conceding too much to a particular, and somewhat incoherent, philosophical system.
Birch fails to convince me that a more traditional theology could not do the job.
It is important, therefore, to see that the kind of revolution for which he calls does not entail the acceptance of his whole philosophy.
Many thinkers besides Whitehead have seen that events and processes may be more fundamental expressions of the ‘thingness’ of things than concepts like substance and matter.
Many have pointed out the absurdity of concluding that the reality is impersonal and purposeless on the basis of a methodology which deliberately and systematically excludes such notions.
Many have begun to question rigid distinctions between fact and value, and the revolution toward a greater appreciation of the intrinsic value of all life is well under way.
On Purpose is a useful stimulus.
A book which begins with Whitehead's introduction of Bertrand Russell at a Harvard lecture —‘Bertie says I am muddle-headed.
But I think he is simple-minded’— cannot fail to be full of good things.
But Birch is a modest man, despite his immodest ideas, and would not see his book as any more than a starter.
Public understanding of technology
Levitating Trains and Kamikaze Genes: Technological Literacy for the 1990s by Richard P. Brennan,John Wiley and Sons, pp 262, £12.95 
RECENTLY I have been dragged, most unwillingly, into a ludicrous debate about whether hi-fi systems sound better if the room, records and electronics have been treated with coloured inks to convert the adverse effects of the gravitational field of the Earth into beneficial effects.
I regret that this is not a joke.
Some people do seriously believe that their records sound better if the labels are smeared with ‘neutralising cream’ and stored with the sleeve notes pointing in a particular direction to align the printing inks.
Needless to say no one has yet proved by blind testing that any of the effects claimed are reproducible.
There is no incentive to run tests and analyse the results.
Without statistics to prove the theories daft, the opportunity remains to rely on the powers of suggestion to perpetuate the myth.
I was thus thrilled to read, in the first chapters of Richard Brennan's book, an argued attack on the cult beliefs, unproven theories, ignorant superstitions and mumbo-jumbo pseudoscience that so many people now prefer to the hard work involved in becoming technologically literate.
Unfortunately, this work is made unnecessarily hard because most technical textbooks are unfriendly, obscure and aimed at specialist readers.
Condensed encyclopaedias of the ‘how things work’ variety inevitably cover so much ground that there is not enough room to cover any one subject in relaxed and readable style.
Brennan has adopted a completely different approach.
In nine chapters he takes a score of technical topics always in the news and explains them clearly and readably.
The author makes it plain from the outset that he has no interest in trying to convince bar-stool pseudoscientists that they are wrong in believing that early cave dwellers battled marauding dinosaurs and that astrology tells all.
He merely notes that the bad news for technological illiterates is that the world is not as they think it is, and if they are interested in the truth his book will fill some gaps.
Those who want equal time for creation science in biology classes might as well make similar demands for Flat Earth theory in astronomy classes.
They have picked up the wrong book and are probably in the wrong bookstore.
Although this is an American book, the same beliefs persist in Britain.
A local newspaper in Sussex recently published a reader's letter complaining that Darwin's evolution theory is still taught in schools and colleges instead of creation, despite ‘scientific evidence limiting the age of the Earth and Universe to a few thousand years’.
First Brennan looks at space, the Universe, big bang theory and Einstein's theory of relativity.
Those who dream of a holiday on the Moon will be disappointed by the calculations of the ratios for rocket weight to transported load.
There is a good chapter on biotechnology and genetic engineering, with a simple explanation of gene splicing.
Another excellent chapter deals with the greenhouse effect and ozone depletion, which Brennan admits is a personal hobbyhorse.
He does a similarly good job on energy, with some sobering figures on inefficiency and the build-up of toxic waste.
The chapters on high technology medicine, superconductivity and super weapons (including Star Wars) all tell a lot, in easy language.
As Bob Hope said of the Stealth bomber, ‘If it is supposed to be invisible to radar, why don't we just not build it and tell the Russians it's there?’
There are a few areas where descriptions are skimped (for instance of Doppler effect) and I found a few small errors.
The abbreviation for computer display is surely wrong; VGA stands for video, not virtual, graphics array.
The references to ‘kilo’ and ‘mega’in the context of computer technology are misleading simplifications; kilo is 2 to the power of 10, not 1000, and mega is 2 to the power of 20, not a million.
Doubtless others with specialist knowledge will spot similar mistakes.
But all in all this is an excellent book, highly recommended for anyone interested in escaping the black hole of technological illiteracy, which in Brennan's own words, sucks in all the pseudo-sciences.
The ant's tale
Applied Myrmecology: A World Perspective edited by R. K. Vander Meer, K. Jaffe and A. Cedano,Westview, pp 741, £55 
Stephen Buchmann
ANTS run the world.
They are found on every continent with the exception of Antarctica, Iceland, Greenland and a few remote Polynesian islands.
Whether you stand in a pristine lowland rainforest, beside a road or in an urban park, any casual glance towards the earth will quickly pick out ants, from a few to many individuals — and often from several species.
Belonging to the hymenopteran family Formicidae, they are extremely diverse with more than 8800 known species placed in 292 genera by modern systematists.
Although individually insignificant, ants make up for this in sheer numbers and total biomass; they are among the dominant extant organisms on Earth.
Ants are the major predators, scavengers, herbivores, granivores, seed dispersers, soil turners and channellers of energy and nutrients (especially carbon, nitrogen and phosphorus) in most temperate and tropical ecosystems.
Their diversity and the richness of species is most apparent in the lowland neotropical rainforests found in Amazonia, and in Australia which have more than 2000 living species.
Only termites, leafcutter ants (tribe Attini) and some social bees (the stingless melipones) even come close to ants in their large-scale efforts to alter their environments and cycle materials through food webs.
A single hectare of Amazon rainforest soil can contain more than 8 million ants, and between 10 and 15 per cent of the entire animal biomass consists of ants.
(Animal biomass includes birds, mammals and reptiles.)
Their ecological dominance and biological success for the past 80 to 100 million years may stem from the fact that they were the first group of predatory social insects that both foraged and lived in the realm of rotting vegetation on the ground.
This was a somewhat underexploited arthropodan niche that ants have claimed as their domain.
Unfortunately, this weighty tome does not go nearly far enough into this fascinating world of the interrelationships that ants have with the plants and other animals in their day-to-day business of running the world: Rather, we have a specialised symposium that concentrates on the largely negative aspects of viewing some of the world's most fascinating species only as anthropogenic pests.
The editors proudly proclaim their effort as the birth of applied myrmecology, a subject that deals with the voluminous literature on pestiferous ants and attempts to bring together authorities on problem species in a modern forum.
Ants become a problem species when they somehow interfere in the human pursuit of wealth, happiness and growing food.
The book is an outgrowth of a 1988 colloquium on ‘Pest Ants’ held in Caracas, Venezuela.
Only three years earlier, a similar symposium resulted in a 1986 volume on the same subject by one of the authors.
Both volumes are largely devoted to the two most economically-damaging ant groups; the so-called fire ants and leafcutting ants.
The fire ants (largely the genus Solenopsis ) are noted for their painful stinging and alkaloidal venoms, while the leafcutting group of the attine genera are severe agricultural pests in the neotropics.
Unfortunately, we hear little about the role and inestimable value of leafcutters in channelling nutrients through tropical forests, as they convert leaves into rich humus.
I was disappointed in the quality of many of the photographs and graphs.
There are, however, some real gems in a small fraction of the 67 chapters.
For example, we are treated to the results of a 10-year study of the foraging activity of a laboratory colony of Atta , information on the biology of the immense carpenter ant genus Camponotus and unique insights into the alkaloidal pharmacopoeia of fire ants.
Other exemplary chapters include an examination of the underlying physical and chemical factors controlling leaf selection (herbivory) by the attine genus Acromyrmex and a novel — perhaps unique — investigation into the foraging patterns of the Argentine ant (Iridomyrmex humilis ) as a self-organising system.
Here, the researcher explores how the foraging patterns develop in a similar way to the patterns that can be produced on a computer by following simple, repetitive rules.
Two other extremely good chapters include one on the granivorous genus Pogonomyrmex and the exciting area of cooperative colony foundation in Atta leafcutting ants.
While I do not recommend Applied Myrmecology: A World Perspective to the average armchair naturalist, it is an indispensable reference for those working with fire ants and leafcutting ants, especially in applied research.
It will also find its way onto the shelves of most major university libraries.
Mind dances
Correspondences: The Work and Imagination of Mathematician S. Ramanujan A Baratha Natyam dance opera by the Shobana Jeyasingh Dance Company, Festival Hall, London, 20 and 21 February
William Bown
AN UNUSUAL performance by traditional Hindu dancers and an opera singer sets out to illuminate the life of Srinivasa Ramanujan, the Indian mathematical genius, to the accompaniment of string and percussion.
It works.
Ramanujan came to Cambridge in 1914 at the age of 37.
He was from Tamil Nadu, a region of India that has, historically, prized mathematics.
Self-taught, he worked as a shipping clerk in Madras until the mathematician George Hardy saw his work and invited him to England.
The maths he produced in the six short years before his death in 1920 is treasured for its originality.
In places, it defies understanding to this day.
The show begins with a soft rich voice reading from the letters of Ramanujan.
The man is dressed in the elegant black and white of Cambridge before the First World War.
‘If nought is divided by nought is the answer infinity?’ he asks.
Suddenly we are shot into the puzzling unknown realm of infinity.
The dancing combines the traditional and abstract.
This may be no coincidence: it is based on Bharatha Natyam, the dance of Ramanujan's mathematically aware Tamil Nadu.
The dancers are dressed in orange and blue robes.
They go barefoot and with each precisely placed step the bells round their ankles ring.
Occasionally, character steps out of the angular interlocking patterns to hold a bowl or pour from a jug.
The impression is, however, mostly of four self-contained thoughts whirling round each other, sometimes interacting, sometimes separate.
Watching it takes you into the mind of Ramanujan as surely as a long look into his eyes.
(I saw the performance on video.
Oddly, it is impossible to tell from the dance movements whether you are rewinding or forwarding.)
Ramanujan invoked the Hindu goddess Namagiri as his inspiration and the sacred touch is echoed by the dancing, itself originally religious.
To unlock this requires as much knowledge of the Vedas, the ancient holy books of the Hindus, as The Satanic Verses does of the Koran.
The whole thing is more dance than maths and more Ramanujan than either.
It closes with a reading of a series of formulae from one of his papers but even here, where the ‘mathematicalness’ of the performance is forced on you, the dance is more absorbing.
What ran through my mind was: ‘What was going on inside his?’
Most mathematicians understand what is going on inside the heads of most of their colleagues.
No one understood what was going on inside Ramanujan's.
But watching this show is probably as close as you or I will get.
I enjoyed it.
Michael Cross is a science writer based in Japan.
Eugene Smith's photographs showed the pain of Minamata disease
Chris Baker is studying science at Maidstone Grammar School, Kent.
Ludmilla Jordanova is a historian of science at the University of Essex.
John Habgood is Archbishop of York.
He began his academic career as a physiologist and pharmacologist.
Stephen Buchmann is an adjunct professor of entomology at the University of Arizona, Tucson.
His research focuses on the social insects, especially bees as pollinators.
The universal dance of mathematics set to music
LIFE, THE UNIVERSE & (ALMOST) EVERYTHING! 120
PLANT PROTECTION!
Plants — the principal source of medicinal drugs for all the world's civilisations & cultures…
I thought it was for purely recreational use
ONLY WITH THE ADVANCES IN SYNTHETIC CHEMICALS HAVE ‘DEVELOPED’ NATIONS BROKEN THEIR DEPENDENCE ON PLANT MATERIALS
Fools…
In Western medicine, plant chemicals are used in 3 main ways…
1.
DIRECTLY INCORPORATED INTO MEDICINES & MEDICATIONS
2.
AS TOOLS TO UNDERSTAND PHYSIOLOGICAL & PHARMACEUTICAL MECHANISMS — EG IN DRUG TESTING & DEVELOPMENT
3.
CHEMICAL COMPOUNDS USED AS BLUEPRINTS FOR THE MANUFACTURE OF NEW/SYNTHETIC CHEMICALS & DRUGS
80% of the world's population relies entirely on local medicines.
Get moving!
Find some dock leaves!
AROUND 7000 PLANT-DERIVED MEDICINAL COMPOUNDS USED IN WESTERN MEDICINE COME FROM ONLY A SMALL NUMBER OF PLANT SPECIES…
120 OR SO PLANT-BASED DRUGS PRESCRIBED FOR WORLD-WIDE USE COME FROM JUST 95 SPECIES — MANY KNOWN FOR CENTURIES
POISONOUS HENBANE — SOURCE OF ALKALOIDS HYOSCYAMINE & HYOSCINE ARE ACTIVE, ALONG WITH DEADLY NIGHTSHADE'S ATROPINE, AGAINST STOMACH & DUODENAL ULCERS; IN OPHTHALMOLOGY, TRAVEL SICKNESS PILL, & OLD DETECTIVE NOVELS
Ancient Greek women used it to dilate their pupils…hence, ‘belladonna’…
I get that with catmint…
WHITE WILLOW BARK — USED SINCE ANCIENT TIMES TO TREAT PAINS & FEVERS.
IN 1827, ITS ACTIVE INGREDIENT, SALICIN, WAS ISOLATED; & WITH MEADOWSWEET'S SIMILAR SALICYLIC ACID PROVIDED THE CHEMICAL BLUEPRINT FOR ASPIRIN
Nature usually provides the appropriate remedy…
MANY DRUGS STILL CAN'T BY SYNTHESISED — LIKE FOXGLOVE'S DIGITALIS — EFFECTIVE ON THE CARDIOVASCULAR SYSTEM:
…& MORPHINE…
GREATEST NATURAL PAINKILLER — & ONLY ONE OF 25 ALKALOIDS PRESENT IN RAW OPIUM — INCLUDING CODEINE
SOME SPECIES OF CLIMBING YAM CONTAINING HIGH LEVELS OF DIOSGENIN (NOW MOSTLY SYNTHESISED) TURNED OUT TO BE THE BASIS OF THE ORAL CONTRACEPTIVE INDUSTRY
Failing that, I throw 'em!
…& COTTONSEED OIL, CONTAINING GOSSYPOL, COULD BE A SOURCE OF ORAL CONTRACEPTION — FOR MEN…
Yeah…if there was a pill to make them take it…
OVER 100 ALKALOIDS HAVE BEEN ISOLATED IN THE ROSY PERIWINKLE, INCLUDING VINBLASTINE & VINCRISTINE — SUBSTANCES WHICH HAVE REVOLUTIONISED THE TREATMENT OF HODGKIN'S DISEASE & ACUTE & LYMPHATIC LEUKAEMIA
5000 flowering plants' potential tested — out of 250 000!
QUININE — THE CLASSIC MALARIA REMEDY — 60% OF PRODUCTION USED IN MEDICATION — 40% IN FOOD & DRINKS — THE LARGEST PART IN TONIC MIXERS…
Never drink the water…
IN 1981, A PREVIOUSLY UNDETECTED ‘SUGAR MIMIC’ ALKALOID, CASTASPERMINE, WAS IDENTIFIED IN THE FRUIT OF THE AUSTRALIAN MORETON BAY CHESTNUT…
Only 19% of plants estimated to exist in Amazonia have been scrutinised…
…
So no rainforests & a huge new disease crop!
THE HIV VIRUS IS COVERED WITH PROTEINS TO WHICH SUGAR CHAINS ARE ATTACHED.
CASTASPERMINE SEEMS TO HELP ALTER THEM, LEAVING THE VIRUS NON-INFECTIVE
Next Week: THINGS THAT GO JUMP IN THE NIGHT!
FORUM
What's so special about the centre?
Ken Croswell says it is time for a little post-Copernican thinking
EVER since Copernicus removed the Earth from the centre of the Solar System, scientists — and astronomers in particular— have been rendering our place in the Universe less and less special.
In the 1910s, for example, Harlow Shapley discovered that, just as the Earth is not at the centre of the Solar System, the Sun is not at the centre of the Galaxy.
Rather, the Sun is offset from the Galaxy's centre by thousands of light years.
And a few years later, when Edwin Hubble found that nearly all other galaxies are flying away from our own, he correctly reasoned that this did not mean we were at the centre of the Universe.
Hubble argues instead that the Universe is expanding, and every galaxy is flying away from every other.
All of this is fine with me.
I never saw what was so great about being at the centre of the Solar System, or the centre of the Galaxy, or even the centre of the Universe.
It's probably no more pleasant than living at the centre of some large, dirty, crime-ridden city.
But Copernican thinking has gone too far.
Certain astronomers now incorrectly assume that everything about our circumstance must be unspecial, insignificant, and just plain mediocre.
Yet the more we look at our surroundings, the more we find that they are in many ways far above the average.
Take the Sun, the star from which we ultimately derive nearly all our energy.
Open most astronomy textbooks and you will find a statement somewhere that calls the Sun an average star.
Science popularisers often make the same claim, dismissing the Sun as just a run-of-the-mill star.
Statements like these are dead wrong.
Confront such Copernican thinking by taking an actual census of the Galaxy's stars, and a startling thing appears: nearly all stars are less massive and intrinsically fainter than the Sun.
In fact, astronomers who conduct such surveys estimate that 95 per cent of stars are less luminous than the Sun.
In other words, the supposedly modest Sun is in the top 5 per cent of all stars in the Galaxy.
Nothing very average or run-of-the-mill or mediocre about that, now is there?
The most common type of star in the Galaxy turns out to the humble red dwarf.
Red dwarfs are faint, cool and small, and they account for 70 per cent of the Galaxy's stars.
Because they are so faint, not a single one is visible to the naked eye.
Another 15 per cent of stars are orange dwarfs.
Orange dwarfs are brighter than red dwarfs, but they are still fainter and less massive than the Sun.
And yet another 10 per cent of stars are white dwarfs, dim stars the size of Earth and all invisible to the naked eye.
So the Sun is hardly average.
And neither is our Galaxy, the Milky Way.
Again, contrary to popular (and Copernican) belief, the Milky Way is much bigger and brighter than most other galaxies in the Universe.
Most galaxies are pathetic objects called dwarf galaxies — so dim that astronomers can't even seem them unless the galaxies are close to us.
Of the two dozen nearest galaxies, only one — the giant spiral Andromeda — is bigger than the Milky Way.
So neither our star nor our Galaxy is an average representative of its class.
What about our planet?
The truth is, we don't know.
The only planets we can see are the ones that orbit the Sun — far too small a sample to draw any conclusions from.
Despite this lack of data, science popularisers delight in calling Earth an insignificant ‘speck of dust’, and science-fiction writers love to populate the Universe with races far more advanced than ours.
But hold on.
If our Sun is above average, and our Galaxy is above average, why can't our planet be above average, too?
Maybe the reason scientists have yet to receive signals from extraterrestrial intelligence is because there isn't any extraterrestrial intelligence sending signals.
We may live on one of the most advanced planets in the Galaxy; other planets, if they exist, may either be completely lifeless or populated with creatures too stupid to send any radio signals at all.
Again, we really don't know.
But Copernican thinking has already led astray many astronomers who should have known better, and we'd best be careful when they use the same logic to try to convince us that we and our planet are nothing special.
And so what is we aren't at the centre of the Galaxy?
Life is much better in the suburbs, anyway.
The Ppound in your pocket
Ian Stewart has finally sorted out his millions from his billions
TELEVISION news irritates me.
In fact, there's no end to the things that irritate me about television news — the inane scripts, for one.
‘The trade balance is one billion pounds in the red this month, but this will not leave the Chancellor much room for manoeuvre on interest rates.’
‘Secondary picketing has been outlawed, but this will not please the unions.’
The use of ‘but’ where common sense demands ‘and’is symptomatic of a general malaise: the scriptwriters don't think about the meaning of what they've written, they just want it to sound pretty.
Watch the next News at Ten with your now-sensitised ears: you'll be amazed.
(Especially since you can't watch with your ears.
Just testing.)
And that's not what I want to talk about here.
(Testing again.)
What really annoys me is what happens whenever the magic words ‘million’ or ‘billion’are uttered.
You can watch the newsreader's lips getting into gear, like Fatima Whitbread psyching herself up for the big throw.
The Capital Letters hurl themselves like javelins across your living room and impale themselves in your ears with an audible splat.
(Testing again.)
My first example would sound like this: ‘The trade balance is One BILL ION Pounds in the red…’— pronounced ‘bill -yun’ with a strange rising cadence that reminds me of nothing more than Leslie Crowther: ‘A bill -yun pounds — come on down !!!’
Typographical effects are inadequate.
The sing-song voice, the perverse placing of stress — you've heard them, you know what I'm referring to.
The same goes for millions: just replace ‘B’ by ‘M’.
Never mind that the former exceeds the latter by a factor of a thousand.
(Like it or not, the American billion has won.
Accept it.)
They're all just big number, right?
They don't mean anything, do they?
‘Zillion’ would do equally well; the actual amount doesn't matter.
All they're saying is ‘a HUGE, absolutely HUMONGOUS quantity of MONEY!!!’.
Or are they?
Let's take the moderately humongous million, for starters.
Typical context: ‘The health secretary has announced that next year the National Health Service will receive an additional FIFTY MILL ION Pounds…’
Friends: if you're Her Majesty's Government, £50 million is peanuts.
I admit it sounds like a lot; but that's because everybody thinks of what they could do with it if it ended up in their bank account.
However, there are more than 50 million people in this country, and HMG has to look after them all.
(Well, quite a few of them, anyway.)
What HMG has spent, per person, is one measly pound.
We desperately need to get these ‘big’ figures into perspective.
How?
It is my modest belief that I have the answer.
It is the concept of the Ppound.
One Ppound (Personal pound, symbolised by P£) is the sum of money which, when divided equally among the adult inhabitants of Britain, will put one real pound into every pocket.
With roughly 40 million adults, this means that P£1 is worth £40 million.
There is a derivative, the Ppenny (Pp), which is worth £400 000.
European Monetary Union no doubt will demand the invention of the Pecu, the Personal European Currency Unit.
(Coincidentally,pécule is French for ‘savings’.)
You can work out the pecuniary value of one Pecu as an exercise.
I draw the line at Ppesetas and Ppfennigs.
Especially Ppfennigs.
My contention is that a national government should treat a sum of one Ppound in the same manner that an individual treats an ordinary pound.
After all, spread around, that's what it would buy.
From this Pperspective, our hypothetical generous increase in NHS funding amounts to roughly one Ppint of beer.
It really does bring everything down to earth.
The fabulous MILL ION becomes a paltry 2.5 Ppence.
The equally fabulous (equal because equally incomprehensible)BILL ION becomes one thousand times as big, as it should be: P£25.
It's not a sum you'd throw away; but you'd be happy if it bought a new handbag, a train fare to London or a tankful of petrol.
Take a look at our nation's economy from this new viewpoint.
I'm using 1985 figures because my encyclopedia's a bit long in the tooth.
Britain's gross national product weighs in at around £250 billion; that is, P£6250.
For a typical family of two adults, this is equivalent to an annual income of P£12 000.
Truly we are a nation of shopkeepers, and the tiny corner shop at that.
The figure does tend to confirm my contention that the general order of magnitude is about right.
For comparison, the GNP of the United States is P£12 000.
Britain's National Debt is P£4000, much less than a mortgage.
The Public Sector Borrowing Requirement, to eliminate which we put two million people out of work, drained our resources at a rate of P£300 a year: the equivalent of an electricity bill.
HMG's total budget is P£4,000 per year, so maybe you should multiply all my figures by about three to see it from a minister's perspective: the poor dears are trying to run the country on poverty-line wages.
Our gold and dollar reserves are around P£400: a rather ordinary current account balance at NatWest.
Our record trade deficit is around P£40 per month, or about P£480 per year: a heating bill.
I'm forced to conclude that the government may be right; maybe it doesn't matter that much.
Except that the world's financial markets aren't thinking in Ppounds, and they get very nervous.
Imagine it as an ever-increasing overdraft.
What would your bank manager think?
The Science and Engineering Research Council, currently a complete shambles because of underfunding, could be bailed out, thus Saving British Science, for precisely P£1: milk for two days.
A mere P£25 per year — half a subscription to New Scientist — would create the best health service in the world and pay the nurses a fair wage.
Motorways are an unexpected bargain at around 25Pp per mile, while the Channel Tunnel weighs in at around P£200, a cheap hi-fi.
Mind you, all we'll get is P£200 worth of traffic congestion, because HMG won't spend P£35 (one donkey jacket) on a high-speed rail link.
The much-publicised government bungle over state pensions will cost the taxpayer P£150; not an amazing amount, but you'd be pretty annoyed if you lost that much in the laundry.
Especially since the ‘efficiency savings’ from 10 years of government cuts are much the same size.
In one spectacular piece of incompetence, HMG has thrown away everything you have suffered for a decade.
But you see, it was never really worth the saving in the first place.
Would you let the lawn grow knee-deep, ignore the leaky roof, send the kids to school without shoes, and sack the dustman, to save £15 a year?
No?
Nonsense: you did.
Per Astra ad ardua
Barry Fox tries to rescue high-definition TV from the Sky/BSB merger
NEXT Thursday (21 February), an Ariane rocket lifts off from the European Space Centre in French Guiana to put the second Astra satellite into orbit at 19° East.
For two years now, the first Astra has been broadcasting 16 channels of TV programmes, including Rupert Murdoch's Sky Channels, right across Europe.
Any project involving rockets is always risky, but the technology is maturing all the time.
If this week's launch is successful, it will double Astra's capacity to 32 channels.
Astra has been so successful that its owner, Société Européenne des Satellites of Luxembourg, has already placed firm orders for third and fourth crafts to be launched in early 1993 and 1994.
The four satellites will let Astra broadcast 48 channels from the same point in space, for reception by a single dish aerial, and with backup in space if any transmitters go wrong.
Astra has thus created a de facto standard for domestic satellite reception.
BSB's Marcopolo satellites at 31° West are out of the frame.
There never has been room in the consumer electronic marketplace for two directly competing, but incompatible, systems.
Astra is VHS to Marcopolo's Beta.
The tragedy is that most of the transmissions from Astra are in the old-fashioned PAL TV system, with analogue sound.
PAL cannot be upgraded to become a high-definition TV (HDTV) system, with twice today's number of picture lines.
PAL can be used to broadcast wide-screen pictures (for display in cinema-screen shape on a new kind of set) only if viewers with existing receivers are prepared to tolerate a picture like a letterbox.
They will see thick black borders at the top and bottom of the screen, and almost certainly rebel.
Transmissions from Marcopolo are in the new MAC system, which gives clearer pictures and digital sound.
Most importantly, MAC was engineered from the start to provide an elegant upgrade path to HDTV and wide-screen TV.
With MAC, viewers with conventionally shaped screens do not have to suffer letterbox images for the benefit of those with new wide-screen sets.
Although full-blown HDTV will for many years be expensive overkill for the home, wide-screen TV is available now.
The first domestic wide-screen TVs, from Thomson of France, were scheduled to go on sale in France on 12 February.
They receive MAC transmissions from France's TDF satellites.
The effect of wide-screen display is mesmerising.
Anyone who sees a wide-screen TV and can afford one (initially around £3000) will not rest until they own one.
On the face of things, the collapse of BSB (transparently dressed up as a ‘merger’ with Sky to form BSkyB) might mean the end of the Marcopolo satellites for domestic broadcasting, the end of MAC in Britain and a dead end to the elegant upgrade path to wide-screen TV.
It thus makes Britain a technological backwater.
The manufacturers of BSB's MAC receivers, the traders that tried to sell them, and the public who were encouraged to buy them, have lost confidence in the whole business of MAC, Marcopolo and 31° West.
Many dealers have lost a lot of money on stocking BSB systems which they cannot now sell.
They have not been paid for aerial installation work which BSB had promised to subsidise.
Over 100 000 receiver systems, worth nearly £400, are gathering dust in stockrooms.
The high-street chain Comet had a tighter contract than most small dealers and has sued BSB for £10 million; the manufacturer Philips has sued for over £50 million.
Another company, Tatung has sued, too.
Others have writs at the ready.
More than 100 000 people bought BSB receivers.
All they now have is a limited reassurance from the Independent Television Commission that broadcasts will continue for two years.
Without support from the trade, and without interest from the public, no broadcasters in their right minds will now invest in providing a new broad-appeal service in MAC from Marcopolo.
Worthy talk of ethnic minority, ‘green’ or educational broadcasting looks likely to remain just talk.
The only real hope for MAC is for Rupert Murdoch and the new BSkyB service to give (or be forced by the ITC to give) a long-term commitment on making all their PAL transmissions from Astra simultaneously available in MAC from Marcopolo.
The BBC, ITV and Channel 4 could make selected programmes available for ‘simulcasting’, too.
This is what happens in France on the TDF satellites.
This would cost the broadcasters nothings.
The Marcopolo satellites are already in place.
BSB's original backers, who applied for and won the contract to broadcast from 31° West, signed to use MAC.
These companies could earn revenue from continuing the business data services which BSB previously offered.
The MAC system was designed to carry encrypted data and teleconference links either instead of normal programmes, or piggyback on top of them.
The best chance of making this plan work comes from a new idea now being secretly floated in the electronics industry.
The TV rental companies will try to bypass public and trade distrust of MAC and Marcopolo by offering viewers a rental package for the simulcast service, with the bonus of improved picture and sound quality.
As an environmental incentive the aerials can be small because Marcopolo's signals are more powerful than Astra's, especially in Scotland where large, ugly dishes are needed for Astra.
The rental company would install the aerial and receiver, with the customer never needing to know that the signal was in MAC instead of PAL and coming from 31° West instead of 19° East.
This plan will work only if Rupert Murdoch and BSkyB act fast to compensate the retail trade for their losses and obsolete stock.
Otherwise traders will dump the stock on the market at giveaway prices, thereby discouraging the public from renting.
The simulcast and rental scenario would give Britain a second chance on MAC, wide-screen and HDTV technology.
Astra has already promised to provide four full-blown HDTV channels from its fourth satellite.
No decision has been taken whether to use MAC or ‘stretched’ PAL.
The long-term guarantee of a full and desirable simulcast service from Marcopolo in MAC would give the British electronics industry both the incentive to continue developing MAC technology and the live transmissions on which development depends.
The rise and fall of the British bog
Brian Moss witnesses history in the unmaking
I HAVE loved visiting peat bogs ever since, as a sixth-former, I first experienced the wonderful combination of springy surface and spongy sphagnum, glistening red sundews and white bog cotton that they offer.
There is a strong sense of history, too.
Great lumps of ice, sometimes kilometres across, broke off and were buried in sand and gravel as the glaciers retreated ten thousand or more years ago.
Once melted they left basins to fill with lake water.
Then there was the steady encroachment of reed and sedge, alder and willow, from the edges, slowly laying down peat and squeezing the lake smaller and smaller towards the centre of the basin.
And finally as the peat of these mineral-loving fen plants, fed by percolating ground water, built up to the water table, there was a switch to acid bog vegetation fed by mineral-poor rain.
As rainwater came to dominate ground water in the wetting of the peat surface, the acid-loving sphagnum began to build up its mounds and domes.
Most vigorous at the centre, the sphagnum growth eventually created a surface like an upturned saucer — a raised bog or moss.
Written in the layers of peat, in preserved pollen and seeds, is the region's environmental history and, with it, the role that we humans had in it.
Compared with the extensive tracts of a century or so ago, there are few raised bogs left in England — a handful of sites in Shropshire and Cumbria, almost nothing in Lancashire, Cheshire and Staffordshire.
Recently I stood on the dereliction that is Lindow Moss in Cheshire.
Its history was revealed in the sections cut by the machines hacking out the sphagnum peat to be packed in bags and sold because people are conditioned to prefer heathers in gardens to those in their natural habitat.
It was a sad sight: bare, brown and bleak.
And when, only months from now, all the sphagnum peat is gone, they will start to mill the underlying fen peat to mix with droppings from battery chickens on which to grow mushrooms.
Ten millennia of history will have been sacrificed to a crop of little taste and to gardeners who must keep up with the Joneses.
At Lindow, however, it is even worse than that.
In 1983, a human head appeared preserved in the peat and in 1984 a body, near intact from the waist up.
Legs, perhaps of the same body have turned up and last year yet another body, or rather the 70 or so fragments of what had been a body, had it not been mangled by machinery, were recovered.
If the very existence of a raised bog elicits a sense of history and continuity, the bog bodies give personality to it.
One body, at least, was a sacrificial victim, probably a highborn man thrice ritually killed, with an axe, by garrotting, and by having his throat cut, perhaps to appease terrible gods in a dark phase of Celtic history.
Go and see it in the British Museum; it is a far more moving subject than any jewelled bauble or elegant vase.
He is one of us.
Lindow Moss almost certainly has more to offer, yet within years it will be only a hole in the ground — the same hole that the ice left.
We will have destroyed some of our roots, burnt the family photographs for want of a tiny sum to buy the site compulsorily, block the drains which are drying it out, excavate it with a greater delicacy than the present JCBs can offer and restore it as a regenerating wetland of immense cultural significance.
Shortly before I visited Lindow Moss, Britain had just wasted £100 million in futile repairs to a submarine which is now to be scrapped.
We will annually spend an unnecessary £10 million to £20 million on a misdirected reorganisation of the Nature Conservancy Council, which was ruthlessly promoted against all informed advice, largely to appease Scottish landowners already hell-bent on destroying other sorts of peatlands.
And at the same time a paltry £5 million a year is available for all archaeological survey, conservation and management.
All sorts of reasons are given for the lack of enthusiasm in schools for physical science and technology, but never that which might be the fundamental one.
Perhaps young people perceive that our current values of the pursuit of wealth and power are hollow and react accordingly.
THISTLE DIARY
They can't say they weren't warned
Comment from Westminster by Tam Dalyell
‘I TOLD you so’ is the least edifying of attitudes between friends, but I did harangue the House of Commons for 55 minutes on 19 December on the ecological consequences of what was then known as the ‘military option’.
Specifically, I did predict that oil would be pumped into the waters of the Gulf.
If I regurgitate this, it is simply to put down a vote of censure from this column on ministers, particularly Douglas Hogg of the Foreign Office, who did nothing to prepare for the eventuality of oil slicks.
At the end of January, transport secretary Malcolm Rifkind met his European colleagues to discuss the standing task force of national experts on oil pollution.
So I asked him if the task force would be going to the Gulf; how many people were involved; and what experience they had.
Patrick McLoughlin, the junior minister, replied: ‘Member states expressed a willingness to help.’
So far so good.
‘However, I am also aware that considerable counter pollution resources already exist in the Gulf area as a result of existing contingency plans to deal with spillages of oil from tanker casualties and other sources.’
So the conclusions is: ‘It is important therefore not to mobilise European resources until it is clear that they would be needed and not simply hinder the operation.’
So what exactly is the task force?
‘[It]has no specific number of personnel.
It is drawn from member states in response to specific requests to the EC for assistance and its composition will vary from incident to incident depending on the type of help and advice being sought.’
At the time of writing this column, we have been told that there are in the Gulf both the largest and the second largest oil slicks in the history of the world.
In response to Saudi requests, Britain has sent precisely two experts.
McLoughlin says that one is from the Department of Transport's Marine Pollution Control Unit, the other from Warren Spring Laboratory.
He adds, and I am sure he is right, that they both have considerable expertise in dealing with oil spills, including in the Gulf.
Too little, too late, is not the half of it.
MONTY Finniston FRS was one of the most remarkable Scots of his generation.
The fourth child of a Glasgow Gorbals haberdasher he rose by sheer hard work to be chief metallurgist of the Atomic Energy Authority and chairman of British Steel.
Ministers with whom he dealt have varying views of this combative technocrat.
Roy Mason, once minister of power, is an unqualified admirer.
Tony Benn, a qualified admirer.
Yet his drive and energy for constructive solutions are undisputed.
My abiding memory is of an acrimonious meeting of British Steel where he was much name-called for closures.
‘Well,’ he said disarmingly, ‘I may be the devil, but I don't drink and I've been married to the same woman for 40 years, so I can't be that bad.’
Whether as Gorbals kid street fighter, trustee of New Scientist , Fellow of the Royal Society or captain of industry, I really did like the man.
WITH all the talk about the Gulf, it has passed almost unnoticed that the European Community has agreed a new regulation on substances that deplete the stratospheric ozone layer.
Ministers have committed themselves to cut CFC production by 85 per cent by the middle of 1995, and phase it out by the middle of 1997.
Sensibly, exemptions may be permitted for essential uses, such as medical aerosols and some fire extinguishers.
As David Trippier from the Department of the Environment said, this is two-and-a-half years ahead of the schedule laid down by the Montreal protocol — and the government would have liked it even faster.
I suppose all this would have been welcome in happier times.
I cannot help feeling that any good done might have been cancelled out by oil fires in the Gulf.
Ken Croswell is an astronomer and science writer based in Cambridge, Massachusetts.
THE GOVERNMENT WILL SPEND 10 MILLION POUNDS ON SCIENCE…
OR, IN REAL TERMS, A BAR OF CHOCOLATE.
Ian Stewart is a professor of mathematics at the University of Warwick, and the author of Does God Play Dice?and Game, Set, & Math .
Brian Moss teaches and researches in the Department of Environment and Evolutionary Biology at the University of Liverpool.
ENIGMA No 602
Damsel in her prime
by Susan Denham and Philip Lloyd Lewis
As usual, letters have replaced digits in this addition sum, with different letters consistently being used for different digits: SUSAN DENHAM ––––––
DAMSEL
We can also tell you that the damsel is in her prime, that is, SUE is a prime.
Please send in your SUM.
A £10 book token will be awarded to the sender of the first correct solution opened on Thursday 7 March.
Please send entries to Enigma 602, New Scientist, King's Reach Tower, Stamford Street, London SE1 9LS.
The Editor's decision is final.
The winner of Enigma 597, ‘Break-uprithmetic’, was J. Hayes of Reading, Berkshire.
Answer to Enigma 597
Break-uprithmetic
Enigma 595c
Sharp-eyed readers will have noticed that Santa's address, the answer to Enigma 595c, failed to appear on 2 February with the rest of the answers to the Christmas Enigmas.
He lives at 93 750 000 000 Christmas Street.
FEEDBACK
THE wonders of market forces never cease to amaze.
The Meteorological Office is selling its forecasting ability through its new Weather Initiative, backed up by a suitable array of managerial titles including director of commercial services.
Antennae keenly tuned by the hopes of lucrative business, the new director, Bernard Herdan, was quoted in the Independent on Sunday as looking forward to even greater wonders: ‘The day is not too far distant when we will be able to tell the ordinary public whether it is going to rain in their street within the next hour.’
Nice to know it's not too far distant.
On 15 June 1955 (yes, that's 35 years ago) the Daily Express ran a story about the Met Office's new telephone service.
Just dial HOL 3434, and you are put through to ‘a very special weather expert watching a radar screen’.
Said Dr J. S. Farquharson of the Weather Division: ‘For close forecasts the early results have been excellent…
We hope to be able to tell within minutes what time the rain will start and cease over a wide area around London.
And in what streets the rain will fall in showery weather.’
There is a different though.
A phone call in 1955 would have set you back less than one new penny.
A Weather Initiative forecast — very comprehensive, of course— comes a little more expensive: try £100.
ANYONE can make a mistake, so it would be unfair, wouldn't it, to poke fun at someone who wrote Advisory Committee on Pollution of the Sea (instead of Protection of the Sea).
Even if, in this instance, the mistake appears on the committee's headed notepaper.
And just in case no one notices the error, the letterheading has been corrected by hand.
EUROPEAN physicists have dubbed their latest megabucks enterprise the Alsace-Lorraine experiment.
They hope to detect incoming neutrinos from the Sun with vats of gallium underground.
The neutrinos, they hope, will turn some of the gallium atoms into germanium.
As a very old colleague points out: ‘It's better than going to war over it.’
(But worryingly, the properties of germanium were predicted by Mendeleev in 1871 — the year France lost control of the two border provinces.)
JUST as European science budgets are recovering from the cost of the LEP (Large Electron-Positron) particle collider at CERN, the European Laboratory for Particle Physics in Geneva, governments are now being asked to cough up £500 million for a Large Hadron Collider there.
One of the key arguments put forward by CERN is that the LHC can be built in the same 27-kilometre tunnel as the LEP thus saving massively on the cost.
Financiers should beware.
The LEP tunnel has room for not just two colliders but three.
Unkind souls are suggesting that once the money for the LHC has been sewn up it can only be a matter of time before CERN gets out the begging bowl for the VEC — Very Expensive Collider.
MEANWHILE, the bureaucrats in Brussels have decided to fight the dreaded Not In My Back Yard syndrome in acronym-to-acronym combat.
Nowadays when faced with impossible demands from an agitated NIMBY, they reply soothingly, ‘NIMTOO’.
Apparently it stands for ‘Not In My Term Of Office’.
MORE on the greening of commerce.
This from an American supplier of products for molecular biologists: ‘Paper is creatively recycled by citizens of the future in our day-care centre.’
The company might perhaps be saying that, as happens throughout Britain, nursery children are drawing on waste computer paper.
THIS magazine has so far avoided the temptation to stick compact discs, screwdrivers, knitting patterns or any other free gift on its front cover.
(Free accelerator with this week's issue — take an electron and hit hard with rolled-up magazine.)
One publication that must be wishing it too had resisted the lure is World , the furry animal/environment magazine now in the BBC's stable.
The latest issue has a free guidebook to Moscow (noted wildlife habitat) on the cover.
Next month, the islands of the Gulf?
CONGRATULATIONS to the Department of Health and the Ministry of Agriculture, Fisheries and Food, for proving the law that if one department is capable of one stupid press release, two between them can raise the stupidity by an order of magnitude.
Last week a press release issued jointly by the two departments warned pregnant women of the dangers of Chlamydia psittaci , an organism that infects sheep and can cause pregnant ewes to abort.
Expectant mothers who come into close contact with ewes face a slight risk of infection to themselves and their unborn child.
To reach the sensible advice, however, you have to pass through the headline on the press release: ‘Advice to pregnant women during lambing periods’.
COMPETITION time.
Some people are not sure about Donald Quicke's suggestion (Forum, 26 January) of sponsoring the name of a new species.
Derek Johns of Sheffield writes in to say that a company that has a rare new species named after it could find its share price sensitive to any environmental news.
Johns does, however, have his own suggestions for suitable, or indeed unsuitable, sponsorship.
Examples are Impatiens thatcherii, Nigella lawsonii, Aloe 'allo …and so on.
The Feedback tenner will go to whoever suggests the best sponsored name.
Suggestions should include some idea of the creature involved.
ARIADNE
TWO researchers at the Department of Earth Sciences at the University of Cambridge were impressed, as I was, by the experiments at Yale which showed that the smell of chocolate helped subjects to remember words more efficiently than those who were not exposed to the smell.
An additional experiment demonstrated that the smell of mothballs had the same effect.
The astonishing work was mentioned here in the 15 December issue.
I now have the results of a small experiment carried out in Cambridge.
Researchers there say that they achieved similarly fascinating results, though handicapped first by a small budget and also by several death threats from potential subjects.
A control group were asked to name the first vegetable they thought of and another group were asked to recite ‘fifteen times’ fifteen times and then to name a vegetable.
All the subjects were ignorant of the earlier work at Yale.
The control group's vegetables were random enough.
They included cabbage, potato, onion, carrot and aubergine but interestingly, as the researchers point out, not a single pulse.
The second group, however, when asked to name a vegetable after the recitation, all answered ‘carrot’.
The results purport to show, significantly, that the repetition of ‘fifteen times’ leads to the suggestion of the vegetable, carrot.
They also hint that perhaps there is some hidden carrot symbol hidden deep in the human psyche.
The researchers think that a nationwide survey could reveal much more, perhaps a number that triggers the response, ‘chocolate’.
I have two criticisms of all this.
It is obscure on what ‘fifteen times’ 15 times means.
The second objection is more serious.
The researchers seem to have been completely unaware that the carrot is not a vegetable.
The European Community has decreed that it is a fruit.
That is enough to invalidate the Cambridge work.
FOR £19.95, The Reader's Digest Association will sell you a book called How to Write and Speak Better .
I hope it sells a lot, for the book offers well-organised help in handling the native language, a skill that seems to be held in contempt by some teachers and academics.
The offer of the book is, as usual, accompanied by such blandishments as ‘Can YOU claim a Mystery Gift?
Look for the enclosed green Wallet now!’ and four free opportunities to win prizes, which must irritate those who can see that if everybody gets four opportunities the odds remain the same.
I should think it might put some customers off.
The book gives advice on punctuation, so it is a surprise to find the opening paragraph of the persuasive announcement of the publication in need of some.
The following extract is exactly as written: ‘When I tell you that something you use every day could be put to better use — and that using it better could help you to win greater influence, more respect and higher status, promotion and a larger income…it could widen your circle of friends and possibly even secure you fame and power — you might well be curious to know what that ‘something’ is…
It is, in fact, the English language; and knowing how to use it…’
And so on.
All the same, Reader's Digest is a successful publishing house and it seems to me that the company would not be producing the book if it were not convinced that there is a big market for such works.
A pleasing thought, that, innit?
Noter mean?
NEVER before have I seen a more pressing need for a statement from Dame Barbara Cartland.
A small paragraph from a newspaper was overlooked by me until now because it had been made even smaller in photocopying.
It reported that the process of falling in love was being studied at the University of Leicester in an attempt to explain how men and women select their partners.
The leader of the research, it was said, is Dr Julian Boon, grandson of the founder of the publishers of romantic novels, Mills & Boon.
Immediate reactions to this news brought about utter mental confusion, rapid breathing and a tendency to hop about the place, with a risk of injury, diagnosed as a case of galloping boggle.
For once, here is a project that I do not want to know more about.
THE small advertisements in American magazines are as riveting as those in English ones.
You can obtain the Anarchist Cookbook from an address in New York and a Resource Guide for the Responsible Non-Monogamist from California.
Those with a blank space on a wall to fill might be interested in the Penis Poster (23 by 35 inches), which depicts 12 animal penises (man to whale).
This is called a scientific novelty.
Not surprisingly, a box number has to be written to for a course in witchcraft, promising tuition by the World's Foremost Witches.
‘Proof Jesus Fictional’ could arrive from another PO box.
But would you be writing from the right address?
‘Go Home!’ is the advice from Wildwood, NJ.
‘Relocation astrology shows where you really belong!’
IN the Tel-Aviv University news magazine, Dr Alexander Kohn has an article on what he calls ‘unethical research’.
In it he mentions several examples of sharp practice in the laboratory, one or two of which are new to me.
For example, the author of a paper may, in citing other researchers in the same field, intentionally omit a competitor's name.
Another and widely spread practice is for the head of a laboratory to add his name automatically to any paper published from it, though he may have no contribution at all to the work.
Readers of William Cooper's novel The Struggles of Albert Woods will remember the frantic indignation this caused.
There are, the article reveals, scientists who pad a reference list of their publications by publishing almost the same material in several journals, altering the title a bit, dropping a paragraph or adding one, or doing the same with a table or graph.
Sometimes abstracts of contributions to conferences are sent in before the results claimed have been achieved.
What to do if the results haven't materialised by the time the conference takes place presents another ethical problem.
ALBERT THE EXPERIMENTAL RAT.
BY DAVID AUSTIN…
THE HISTORY OF THE NEXT FEW YEARS.
SCIENTISTS DEVELOP A MISSILE DRIVEN BY A SIXTH GENERATION COMPUTER.
IT EVALUATES TACTICS.
AND STRATEGY.
AND POLITICS.
IT TAKES UP GARDENING.
YOU'RE SUPPOSED TO BE ATTACKING THE ENEMY.
UTTERLY POINTLESS.