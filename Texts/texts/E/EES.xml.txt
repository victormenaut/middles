

Chapter One
Introduction
This thesis describes work undertaken by the author over a period of four years in the Department of Computing at Nottingham Trent University.
The work has been funded by the European Commission under the ESPRIT initiative.
The area of research is the automatic recognition of handwriting and printed text by computer.
The development of reliable text recognition systems serves two important functions.
Firstly, it allows a more user-friendly means of communicating with computers.
For example, people who are unfamiliar with keyboards could choose instead to interact with the computer using their normal handwriting.
Secondly, existing paper documents could be scanned into a computer and then converted to electronic form to allow further processing.
For example, a library could convert its paper resources into electronic form, for reasons of space-saving, safe-keeping or filing.
1.1 Text Recognition
The visual ambiguity of handwriting is such that a number of possible interpretations may be made for any written word.
Indeed, this is true of any text, but particularly handwritten text since the segmentation between the individual characters is often indistinct.
For example, do the following words say’ clock’or’dock’,’close’or’dose’?
When seen individually, these words may be hard to disambiguate.
However, when seen within a meaningful context, the correct interpretation seems almost obvious.
Indeed, it may seem almost so obvious that the alternative interpretation is not consciously perceived at all.
Effective text recognition, whether by human or computer, relies upon the successful disambiguation of confusions such as the above.
Unfortunately, the English alphabet contains many similar-looking characters, for example:’ n’and’h’,’o’and’a’,’c’and’e’,’a’and’d’.
Upper case letters are also ambiguous, e.g.’ U’and’V’.
Furthermore, letters can also be confused with digits, e.g.’ O’and’0’,’I’and’1’,’Z’and’2’,’S’and’5’, etc.
The problem is magnified when handwriting is cursive, since it is difficult to tell where one character finishes and another starts.
Consider the word’ minimum’, written cursively:
Is it really clear where one letter finishes and another begins?
Printed text is generally easier to recognise than cursive handwriting, since the characters are (nearly always) physically separated from each other.
However, once printed text has been photocopied a few times, or faxed, or degraded in some other way, the characters can become similarly indistinct.
For the purposes of the current project,’ text’is categorised according to how it is produced (either by machine or by human hand) and how it is recognised (e.g. dynamically or statically).
1.1.1 Methods of Text Production
Text can be produced either by machines or people.
When typewriters and computers produce text, it is in the printed form.
Often the text is in a variety of fonts (e.g. Arial, Courier, Helvetica, Modern, Roman, etc.) and font sizes.
Handwritten text can also be printed (i.e. with each character written separately), but normally it is cursive.
Both types of handwritten text are more difficult to process than machine-printed text due to their greater variability.
Figure 1.1 shows the three types of text, listed left-to-right in order of their recognition complexity.
Since cursive script is the most difficult type to recognise, it represents the major focus of the work described in this thesis.
1.1.2 Methods of Text Recognition
The first stage of computerised text recognition is the input of data.
Since the computer has no’ eyes’with which to read, another form of input device must be used.
There are two methods by which text may be input to a computer for the purpose of recognition.
These are referred to as’ static’and’dynamic’systems.
1.1.2.1 Static Systems
With static systems, recognition is performed some time after the handwriting has been produced.
In other words, the text already exists on paper.
Using this method, the input device is an optical scanner, which captures the image as a pixel representation.
To a certain extent, this process may be seen as part of the more established technology known as OCR (Optical Character Recognition), but OCR has traditionally been associated exclusively with machine printed text rather than handwriting.
1.1.2.2 Dynamic Systems
Dynamic systems attempt to recognise handwriting in real-time, i.e., as the user is writing.
The input device is either a digitising tablet or a device known as’ electronic paper’.
The latter is of particular significance, since it allows two way communication: input and output through an LCD screen, using a special pen.
Using this method the handwriting is represented as a sequence of 2-dimensional co-ordinates.
In practice, this means the recognition process lags behind the production of the writing, usually by one or two characters, but keeps up with the speed of the writer [Tappert, 1989].
Dynamic systems have the facility for collecting further information such as the speed or direction of writing, number of strokes used, and the order in which they were written.
This information can be used to improve the accuracy of the recognition process, since there is more information with which to identify characters and to separate overlapping points.
However, dynamic systems are susceptible to’ noise’in the data, which may take the form of spurious points created during the writing process.
Likewise, the scanning procedure involved in static systems can introduce additional noise to the data.
For both approaches, the accuracy of a particular system will be ultimately limited by the resolution of the hardware.
In some ways, dynamic recognition is more restrictive since both the writer and a suitable input device must be present at the same time, and the technique is evidently only applicable to handwritten text.
1.2 Motivation for Text Recognition
The development of reliable text recognition systems serves two important functions.
Firstly, it allows a more user-friendly means of communicating with computers.
People who are unfamiliar with keyboards could choose instead to interact with the computer using their normal handwriting.
Secondly, existing paper documents could be scanned into a computer and then converted to electronic form to allow further processing.
1.2.1 User-Friendly Interaction
Despite rapid advances in computer technology in the last three decades, there have been few changes in the methods by which communication with computers is achieved.
The QWERTY keyboard was, and still is, the principal input device.
However, to use the keyboard efficiently requires extensive learning and much practice.
Its layout reflects the mechanical limitations of early typewriters and as such hardly constitutes an intuitive ordering that would facilitate rapid learning.
Non-typists therefore experience considerable difficulty in finding the desired keys, and errors are commonplace.
Attempts have been made to change the layout of the keyboard, but since millions of typists throughout the world have learnt using the QWERTY arrangement, resistance to change is considerable.
The development of the mouse constitutes a valuable extension to the keyboard, but this is a supplement, not a replacement.
For many computers and computer-controlled machines, the keyboard remains the principal method of input.
However, the proliferation of computer technology continues, and applications involving novice users are becoming increasingly prevalent.
The need for an alternative input device therefore persists.
1.2.1.1 Speech or Handwriting: Practicalities
There are two methods of communication that are natural to human beings: speech and writing.
The automatic recognition of human speech has long been the subject of science-fiction fantasy and (more recently) the subject of extensive scientific investigation.
Speech is the most rapid form of human communication — faster than both handwriting and the output from a trained typist.
From the human perspective, it is highly convenient since it is almost universal in its use and requires no special training on the part of the user.
The automatic recognition of human speech is, however, an immensely difficult problem, and the actual progress so far achieved falls far short of original research expectations [Wheddon, 1990].
The other natural and widely used mode of human communication is that of handwriting.
This medium has existed in nearly all societies for centuries, in a variety of forms.
Most computer users are capable of reasonable handwriting, and can usually write quicker than they can type.
Shorthand adds a further dimension of speed, since trained writers of shorthand can transcribe speech faster than keyboard entry [Leedham, 1989].
However, handwriting recognition has attracted far less research investment than speech, possibly due to the less’ glamorous’image it possesses.
Nevertheless, pen-based systems offer a number of distinct advantages over speech or keyboard based systems.
This is because the pen as an input device can be used to facilitate many other sorts of interaction besides the input of freehand text.
For example, it can also function as a pointing device.
In this role, the pen can be used to select items, pull down menus, move objects around the screen; in effect, to handle any task that hitherto required the use of a mouse.
Furthermore, the pen is more compact and requires no mouse mat.
Secondly, pen-based interfaces allow the creation of sketches and drawings.
Such drawings, along with any handwritten input, can be subsequently edited or annotated, again using the pen (and a set of freehand gesture-based editing symbols).
In short, the pen-based interface is highly versatile, and offers many further capabilities besides that of handwritten input.
Evidently, the choice between the use of speech, handwriting or keyboard for a particular application will vary according to a number of factors:
Ability of User
: where the users of a system are likely to be casual or perhaps untrained in keyboard skills (a situation that is becoming increasingly more commonplace), the use of speech or handwriting may be more appropriate than typed input;
Noisy Environments
: speech recognition is made difficult if interference is created by noisy machinery or extraneous conversations;
Quiet Environments
: in a lecture theatre or library speech input would be unsuitable, and typed input could create a similar distraction;
Security
: where confidentiality is required, speech input could be overheard and therefore unsuitable;
Social Constraints
: In some environments (e.g. doctors' note-taking on hospital wards) speech or typed input would be deemed inappropriate for social reasons, whereas handwritten notes are already an established procedure;
Verification
: systems developed for legal or commercial applications could include automatic signature verification as a useful part of their functionality;
Data Storage
: with text (as opposed to speech) there is always a verbatim record (hard copy) of the dialogue or interaction.
Multi-Media Communication
: in some environments it is necessary to listen and make notes — speech input would therefore not be suitable.
For a long time the hardware available for handwriting recognition was such that the input would take place on a graphics tablet, while the visual feedback appeared on a monitor screen.
This division of attention inevitably caused a major distraction to users.
However, recent hardware developments have created a device known as’ electronic paper’, in which input and output takes place through a combined unit.
It uses an LCD screen and a special pen such that marks appear directly below the tip of the pen whenever contact is made with the surface.
This has led to the emergence of a number of commercial pen-based systems, such as PenWindows, Paragraph and PenPoint.
Although these systems are based on printed rather than cursive handwriting, they nevertheless contrast sharply with the current state of commercial speech systems.
Currently available speech recognition systems typically impose a number of constraints on the user, such as a limited vocabulary (possibly only a few hundred words), speaker dependence (new users require individual training) and the use of disconnected speech (rather than the more natural continuous speech).
1.2.1.2 Speech or Handwriting: Technicalities
A major problem associated with both speech and text recognition is variability of the input.
This is particularly true of speech, whereby the pitch, volume and tempo of an utterance can vary according to meaning.
For instance, the same sentence can be expressed as a statement or a question, simply by varying its pitch (e.g.’It's OK ’ versus’It's OK ?’).
Similarly, by altering the volume, a speaker can express anger (with a loud voice) or secrecy (with a whisper).
By altering the tempo of an utterance, the speaker can express excitement (with rapid speech) or deliberation (slow speech).
All these variations combine to the extent that even words spoken by the same person are never identical [Vassiere, 1985].
Together, they represent a considerable problem for speech recognition systems: a given word may be uttered in a number of ways such that it never exactly matches the examples with which the system was trained.
Handwriting is also variable, along the dimensions of size, slope and’ connectiveness’.
However, written language tends to be more structured than speech, since it can use punctuation (e.g. question marks, exclamation marks, etc.) to indicate meaning within individual sentences and physical layout (e.g. headings, etc.) to identify the various components of a discourse.
There are certain operational difficulties associated with speech recognition.
For example, different instances of sounds that human listeners perceive as the same may have very different waveforms.
Similarly, there are certain words between which human listeners only hear one difference (e.g.’cap ’ and’cab ’), yet there may be many differences between their waveforms.
Another major problem for speech systems is interference from background noise, since it is necessary to remove sounds from the input data that are not part of the speech signal.
In a noisy environment this presents a considerable problem.
Moreover, with a variety of acoustic transducers in use there is yet no agreement on performance characteristics: different microphones can produce different acoustic signals, and these need to be standardised.
A particular problem for speech recognition is the reliable identification of word boundaries.
Speech sounds are produced as a continuous sound signal rather than discrete units, and knowledge of the language is required to determine where one word ends and another begins.
This problem may be illustrated by listening to a foreign language (about which one has no knowledge) and trying to determine the location of the word boundaries.
This is an extremely difficult task, since in normally articulated speech there are seldom pauses between the individual words.
Moreover, when words are spoken in continuous speech they often sound different from when spoken in isolation.
This is known as co-articulation .
For example, some words may be concatenated, such that certain sounds are omitted, e.g.’go away ’ may be pronounced as’go way ’.
Similarly, adjacent sounds may be modified to sound more like each other, e.g.’gone back ’ may be pronounced as’gom back ’.
These problems add a further complication to the way in which speech recognition systems are designed, since training with isolated words may be inadequate for the recognition of connected speech.
With handwriting, there are similar problems concerning the identification of letter boundaries, as illustrated in the first example (’ cl’and’d’can be easily confused).
Furthermore, there may be problems regarding context dependent effects, whereby letters are written differently according to their surrounding context.
However, the detection of word boundaries is relatively simple since they are usually indicated by physical spacing on the page.
Consequently, the problem of reliable segmentation is more prevalent within speech than within handwriting.
Furthermore, the number of different sounds (phonemes) used in English speech is greater than the number of letters used in handwriting, adding further complexity to the recognition of speech.
1.2.2 Document Processing
Although the use of electronic media in the business world is increasing, there remains a vast amount of communication that takes place on paper.
Evidently, the arrival of the’ paperless office’is still some years distant.
In addition, there are numerous textual resources around the world that are only available in their original (paper) form.
This is particularly true of libraries and archives in which valuable information is stored in a manner that takes up vast amounts of space, is prone to decay, and may not be easily accessible.
Consequently, there is a need for a means by which text can be translated from a paper form into an electronic form.
When a document is in electronic form it can be subjected to a variety of processing options: stylistic analysis, statistical analysis, copying, editing, forwarding, filing, and so on.
Until recently, the only means by which this could be achieved was by manual entry, involving many hours of repetitious labour.
However, the development of the optical scanner has provided the hardware necessary for converting a paper document into an electronic form, albeit as a pixel representation.
What is further required is the conversion of the pixel representation to a textual one, which is where optical character recognition (OCR) algorithms take over.
A number of commercial OCR systems are currently available, such as ReadWrite and TextPert.
Some systems are able to identify characters along with their size, location and other layout information, showing robust performance on high quality machine-printed documents.
However, once documents have become degraded (i.e. faxed, or photocopied) the performance rapidly deteriorates.
There is yet no commercially available OCR system that can cope with handwritten (or even hand-printed) text.
1.3 The Need for Higher Level Knowledge
There is much evidence to suggest that reading is more than just the recognition of individual characters.
Studies as early as 1885 [Cattell, 1885]showed that characters are more easily recognised when they form part of a word than when they do not.
This is known as the word superiority effect .
Further work by Reicher [1969]extended this to show that familiar words are perceived as units rather than strings of letters.
Studies of eye movement during the reading process provide further evidence of the role of higher level knowledge.
Javal [1879]showed that the eyes do not scan smoothly across the lines of print, but instead make a series of discrete fixations with rapid movements (known as saccades ) in between.
Analysis of these eye fixations during reading provides insight into the visual information being processed.
For example, Just and Carpenter [1987]showed that typically only 68% of the words may be fixated during normal reading, suggesting that higher level knowledge must contribute to the processing of remaining 32%.
Furthermore, they showed that over 80% of the content words were fixated, compared to only 40% of the function words.
For this distinction to take place higher level knowledge must be affecting the reading process.
Eye movement studies have also been used to demonstrate the role of syntactic knowledge in the reading process.
Carpenter and Just [1983]showed that syntactically ambiguous words take longer to process than syntactically unambiguous words, indicating that the reader is trying to determine the syntactic role of the ambiguous word while fixating it.
Similarly, eye movement studies have been used to demonstrate the role of semantic knowledge in the reading process.
Pairs of sentences that have a coherent semantic relation have been shown to be more rapidly processed than pairs in which the relation is less distinct [Just & carpenter , 1978].
Evidently, the most successful text recognition system to date is that of the human information processing system.
Although it takes many years for a child to master the process of reading, once acquired, the skills are comprehensive and flexible enough to cope with a diversity of written material in a variety of fonts and formats (including previously unknown ones such as unfamiliar handwriting).
The principal strengths of the human information processing system lie in its ability to make selective use of available visual cues and to utilise an understanding of the text to compensate for any degradation or ambiguity within the visual stimulus.
Word images occur within a meaningful context, and human readers are able to exploit the syntactic and semantic constraints of the textual material [Rayner, 1983].
Analogously, computerised text recognition needs to use higher level knowledge to achieve comparable levels of performance.
For both printed and handwritten input, the stimulus alone is insufficient to unambiguously identify the text.
An ideal OCR system would be 100% reliable and always output the correct character (and no others).
However, in practice there is always some degree of error, which increases if the text is printed at an angle, or characters overlap, or an unknown font is used, and so on.
Furthermore, in most fonts there will be confusions between letters (e.g.’ e’and’c’) and ambiguities regarding the correct segmentation (’d’can look like’cl’).
Handwriting recognition is subject to all the above problems plus further ones due to its increased variability.
Reliable segmentation is particularly difficult, as demonstrated by the’ minimum’example above.
Most character recognisers handle this ambiguity by producing a list of characters for each ambiguous letter position, to represent the plausible matches between the data and a database of stored character templates.
When placed within the context of a whole word, the combination of these candidate letters forms a’ lattice’, from which the correct word must be extracted.
To reduce this ambiguity, it is necessary to eliminate the incorrect letter candidates in each position, or at least rank them according to some measure of their plausibility.
This can only be achieved by using higher level information.
1.4 System Overview
The work described in this thesis was developed in the context of an integrated handwriting recognition system that uses a number of sources of higher level knowledge.
These knowledge sources and the processes associated with them are shown in Figure 1.2.
It should be noted however, that the serial layout is for illustrative purposes only: the syntactic and semantic analysers can and have been run effectively in parallel.
In addition, the semantic analyser has since been successfully applied to other text recognition applications (notably OCR systems).
1.4.1 Character Recognition
The first stage of the current system involves the process of character recognition.
For dynamic input (e.g. using electronic paper or similar) the movements of the pen tip are captured a series of x-y co-ordinates.
There are a number of algorithms by which characters may be extracted from this data, including spatial analysis methods, whereby strokes are coded by a numbering system on a grid, and topological feature methods, whereby attempts are made to identify the constituent shapes within letters.
The present project uses a vector chain method known as Freeman Encoding, which codes letters into sequences of strokes that conform to a number (usually 8) of preset geometric directions.
This is then further reduced to a combination of the five most significant vectors.
A training database is created by encoding samples of handwriting using this technique, and at run-time the input is similarly encoded and then compared with the characters stored in the database.
The stored characters that most closely match the input are identified as the most likely interpretation of that input.
For static recognition (e.g. using an optical scanner) the process is slightly different.
The x-y co-ordinates of the input can still be calculated, but in this case, there is no information concerning the order in which they were created.
Instead, a pre-processing stage is required whereby a probable sequence is determined [Wright, 1989].
However, this process is not completely reliable, and alternative methods are being investigated [Tappert et al, 1990].
1.4.2 Lexical Analysis
The principle underlying the use of the lexical analyser is that input to a text recognition system will normally consist of English words rather than arbitrary strings of characters.
Therefore, letter strings that form words are considered to be more plausible than letter strings that do not form words.
This may not always be the case, but when it does apply it constitutes a very tight linguistic constraint.
For example, consider a sequence of four letters taken from a 26-letter alphabet.
There are 26 4 possible permutations of these letters, i.e., 456,976 different combinations.
However, the number of four letter words taken from a lexicon of approximately 14,000 English words is 1,323.
This represents about 0.3% of the possible letter combinations.
The character recogniser produces strings of candidate letters for each word in the input.
It is likely that many of these letter strings will not form English words.
Therefore, if these non-word strings are eliminated from consideration, the output will consist of a much smaller list of genuine English words, rather then a large list of candidate letter strings.
However, word-lookup methods such as the above are not the only method of lexical analysis.
Other systems have been developed that employ’ sub-word’knowledge, i.e. information concerning strings of letters that are allowable in English, but do not necessarily form whole English words.
Examples of this include n-gram techniques [Higgins & whitrow , 1987], transitional probabilities, Markov models, etc.
However, n-gram techniques have been shown to be less effective for the present project than word-lookup methods, as they fail to exploit letter-level constraints to the same extent [Wells, 1989].
1.4.3 Syntax Analysis
The principle underlying the syntax analyser is that input to a text recognition system will normally consist of grammatical English phrases and sentences rather than arbitrary sequences of words.
Therefore, word sequences that are grammatically acceptable are considered to be more plausible than word sequences that are grammatically unacceptable.
The output from the lexical analyser is a series of candidate words for each position in the input.
Syntactic knowledge may be used to identify those word combinations that are grammatically acceptable.
There are two schools of thought concerning the application of syntactic knowledge.
The first approach is advocated mainly by theoretical linguists, and is based on the notion that there is a universal grammar underpinning human linguistic competence [Chomsky, 1957].
This approach argues that the best way in which to use syntactic knowledge is to formalise the grammar and encode it as a set of rules to which input must conform if it is to be considered acceptable.
The second approach is based on the notion that there is no such thing as a grammatical/ungrammatical distinction, but instead a gradient of probability whereby certain combinations are more grammatically acceptable than others [Sampson, 1987].
This approach is broadly statistical in nature, as it involves corpus analysis to determine the empirical likelihood of various syntactic combinations.
Although it has been criticised as a’ descriptive’technique using’weak’methods, this approach has proved more suitable for the present project, due to its greater robustness, coverage and computational efficiency [Keenan, 1992].
Furthermore, this position is supported by the findings of other researchers [e.g. Atwell et al, 1993].
1.4.4 Semantic Analysis
The principle underlying the semantic analyser is that input to a text recognition system will normally consist of meaningful English phrases and sentences rather than arbitrary sequences of words.
Therefore, word sequences that are semantically acceptable are considered to be more plausible than word sequences that are not semantically acceptable.
The output from the lexical analyser is a series of candidate words for each position in the input, and semantic knowledge may be used to identify those word combinations that are meaningful.
The application of semantic knowledge to the problem of text recognition is the subject of this thesis.
1.4.5 Other Levels
There are further levels of knowledge that have yet to be incorporated into the present project.
One such level is that of discourse .
Knowledge of discourse aids the selection of coherent sentence progressions from incoherent ones, and involves aspects such as argument formation, dialogue continuity, story grammars, etc.
Another knowledge level of is that of pragmatics , or’ knowledge of the world’.
Pragmatic or contextual knowledge can be seen as a type of meta-knowledge , having effects throughout the other levels.
Winograd [1983]demonstrates the importance of contextual knowledge using the following example:’She dropped the plate on the table and broke it.’
Pragmatic knowledge of the context of this sort of sentence would normally indicate that it was the plate that broke, and not the table.
Human readers usually have little difficulty with most types of ambiguity, since they can effortlessly apply a variety of contextual information.
Computers do not have the knowledge and experience of the average human reader, so for them to cope with the ambiguities shown above they need to have access to repositories of the different sorts of knowledge.
1.4.6 Other Applications
Evidently, higher level knowledge is required for effective text recognition.
Indeed there are a number of other computational implementations that have required the use of linguistic information, and the extent to which they have proved successful may identify useful avenues of research for the present project.
1.4.6.1 Speech Recognition
Considerable research effort has been invested over many years into both speech recognition and speech understanding, but very few robust implementations have been developed.
Some of the more successful systems were products of the North American DARPA research program into speech understanding, which ran from 1971 to 1976.
All the systems so produced were designed to handle the ambiguity in the signal and processing by using a number of diverse, co-operating knowledge sources (e.g. acoustic-phonetics, vocabulary, grammar, semantics, discourse, etc.).
However, the systems differed in the types of knowledge they used, the interactions of the knowledge, the representation of the search space and the control of the search.
Possibly the most well known is HEARSAY II [Lesser et al, 1977].
Also significant are HWIM [Wolf, 1980], the SRI System [Walker, 1980]and HARPY [Lowerre, 1980].
1.4.6.2 Machine Translation
Work in machine translation (MT) began in the 1950's with very high hopes but a somewhat naive attitude concerning the difficulties involved.
Many researchers considered MT to be an extension of the code breaking techniques developed during World War 2, whereby foreign languages were little more than a complex coding of words and translation required merely the use of a bi-lingual dictionary.
Consequently, early systems were unsuccessful, and the realisation that effective MT would not be possible without fundamental work on text understanding led to a cutback in funding.
The more recently developed systems have addressed the need for higher level knowledge, by making greater use of linguistic information, particularly semantic (e.g. SYSTRAN [Toma, 1977]and EUROTRA [Raw et al, 1988]).
Other contemporary systems make use of statistical information extracted from parallel corpora (such as English and French versions of parliamentary proceedings).
Initial work at IBM [Brown et al, 1989]suggests that such an approach may be highly effective.
1.4.6.3 Information Retrieval
Since there is so much information available in natural language form — such as books, journals and reports — another application to receive attention is automatic information retrieval.
Even though much of this information may already be in electronic form, retrieving the correct document from a database is not simple.
Information retrieval (IR ) systems attempt to allow the user to input a query and extract the relevant text from a corpus of documents.
Difficulties arise due to the polysemous nature of English words and that documents may have been filed under keywords that are different to those used in the query.
Progress in this area has led to developments in the field of knowledge representation, since the appropriate documents can only be retrieved if the content of the text can be matched with the content of the query [Hirschman & sager , 1982].
1.4.6.4 Human-Computer Interfaces
Natural language is the most convenient mode for communication with interactive systems, particularly for users who are not computer literate.
Some progress has been made in this area, since the input to such systems is typically simpler than the text to be processed in MT or IR systems, and the interactive nature of the application allows the system to resolve certain ambiguities by asking the user to rephrase the question.
The technology has advanced to the point where systems are being used for real (albeit simple) applications rather than demonstrations [Grishman, 1986].
Other human-computer interface applications include text-to-speech systems such as reading aids for the blind and partially sighted [Pugh, 1992].
1.5 Sources of Information
For human readers, the knowledge sources required for the recognition of text (or indeed language in any form) include those gained from experience and those which are inherent.
Either way, the acquisition of this knowledge is essential, and in the case of computers this represents a considerable problem.
Indeed, much natural language research has been addressed to precisely this problem.
Some earlier researchers resorted to laborious hand-crafting of knowledge sources, which, for a substantial vocabulary, can prove an insurmountable task.
Others have attempted to extract information from pre-compiled sources such as machine-readable dictionaries and thesauri.
In many ways, the issue of knowledge acquisition has been one that has separated the practicable representations of natural language semantics from those that can only remain as theories in textbooks.
Semantic information can be obtained in two ways:
(a)
In a pre-compiled form, from a machine-readable dictionary (MRD ).
The definitions contain encyclopaedic information, syntactic information and semantic information in the form of sense relations that describe the relationship of any one word to a number of others.
MRDs are the most accessible source of semantic information.
(b)
From large bodies of naturally occurring text (corpora ) which can be processed to derive further information on word patterns.
Such resources must be compiled systematically, i.e. the corpus should be large enough to cover the requisite variety of language structures, and to be representative of the type of text to be recognised.
As the need for lexical resources has grown, greater numbers of machine-readable dictionaries have become available, and progress has been made regarding the issues of standardisation and format.
However, the fact that a dictionary is in machine-readable form does not necessarily mean that the required information is instantly available — pre-processing may be necessary to organise the information.
Indeed, such progress has not been common to all publishers — some still produce little more than typesetting tapes with cryptic codes labelling the various components of each entry, and little or no accompanying software or documentation for their search or extraction.
By contrast, others have invested heavily in the natural language market, and the design of their dictionaries reflects those needs.
Longman's Dictionary of Contemporary English (LDOCE ) in particular has been designed with computational applications in mind, and to this end it has formed the resource for many projects, e.g. machine-readable databases, syntactic parsing, semantic analysis [Boguraev Briscoe, 1989].
Some dictionaries have used contemporary labelling standards such as SGML (Standard Generalised Markup Language) to facilitate the logical organisation and efficient extraction of required information.
The Oxford Advanced Learner's Dictionary (or OALD ) uses such markup, and for this reason has become a major resource for the current project.
Details of some widely available MRDs are shown in Table 1.1.
1.6 Summary
The development of reliable text recognition systems serves two important functions.
Firstly, it allows a more user-friendly means of communicating with computers.
For example, people who are unfamiliar with keyboards could choose instead to interact with the computer using their normal handwriting.
Secondly, existing paper documents could be scanned into a computer and then converted to electronic form to allow further processing.
For example, a library could convert its paper resources into electronic form, for reasons of space-saving, safe-keeping or filing.
The output from a character recogniser requires further processing to reduce the ambiguity and hence increase the accuracy of recognition.
Three levels of knowledge have been investigated and incorporated into the current system: lexical, syntactic and semantic.
Lexical analysis eliminates non-English words, syntax analysis ranks competing word sequences according to the grammatical plausibility, and semantic analysis ranks competing word sequences according to the plausibility of their combined meaning.
Text recognition systems developed to date have been mainly concerned with the pattern recognition level, and the use of higher level information is often restricted to some form of lexical analysis.
Comparison between published systems is difficult, due to the lack of standard measures of assessment or’ benchmarks’.
There are many parameters associated with the performance of a system, and their relevance will vary according to the use of the system: some systems may be designed for single users with a particular style of writing; others may attempt to be more generalised and therefore need training.
Furthermore, earlier systems were constrained by hardware restrictions such as a lack of available memory, which would then restrict the size of the vocabulary.
Unless systems are tested in a comparable way, the measures of performance have no relative meaning.
As with speech, the most successful recognition system is the human information processing system.
It takes many years for a human child to master the process of reading, even though they already possess established linguistic and cognitive subsystems.
However, once mastered, these skills are comprehensive and flexible enough to cope with a diversity of written material, in a variety of fonts and formats (including previously unknown ones such as unfamiliar handwriting).
Some handwriting may be difficult to read due to a lack of visual clarity, but most readers can perform some sort of recognition resulting in a meaningful interpretation.
This skill relies on the human ability to consider information and constraints from a variety of knowledge sources to arrive at a’ solution’to the’problem’of making a plausible interpretation of some arbitrary handwriting marks on a page.
This solution represents the best compromise between information from each of the knowledge sources.
As with many aspects of human performance, it is perhaps inappropriate to talk of’ rights’and’wrongs’as in the right choice (i.e. correct) and the wrong choice (i.e. an error).
A reader of any given text can never be 100% sure of the writer's original intentions; they can only select the most likely interpretation of the marks on the paper, based on their outward appearance and the various sources of linguistic and general knowledge.
Evidently, text recognition is a substantially different problem to text understanding .
Whilst full understanding of a text is an immensely difficult and contentious problem (there are few agreed definitions of the word’ understanding’) the present project aims’only’to recognise text.
Understanding implies recognition, but the converse is not necessarily true.
Both objectives require the application of higher level knowledge to identify inconsistencies at individual levels, so it is possible that techniques developed for recognition applications could contribute to the development of text understanding systems.
However, the difference, in practice, is that whereas human understanding involves the identification of the most consistent of a number of interpretations, computer recognition usually involves the identification (and hence elimination) of the least consistent ones.
The emphasis in human understanding can be seen as top-down , or hypothesis-driven , whereas computer recognition is usually bottom-up or data-driven .
This thesis is concerned with the application of semantic knowledge to the problem of text recognition.
Semantics, in its strict linguistic sense, is concerned with the meaning of words , and not with non-linguistic facts about the world.
However, it has become clear that to implement such knowledge in language processing systems it is necessary to use semantics in its broader sense, i.e. using encyclopaedic knowledge and general knowledge.
Consequently, the semantic information used by the present project is acquired either from (a) machine-readable dictionaries, which contain encyclopaedic knowledge as well as the purely semantic sense definitions (and should strictly be referred to as pragmatic in nature) or (b) from text corpora (which are syntagmatic rather than semantic in nature).
However, the label’ semantic’will continue to be used, although it is intended that the reader should not restrict its denotation to the strict linguistic sense.
The use of semantic knowledge, its theory, application and relevance to text recognition forms the basis of this thesis.
The next section reviews published natural language applications that have in some way addressed the problem of semantic knowledge representation and processing.
Chapter Two
Literature Review
2.1 Introduction
This section reviews published research into natural language applications that involve some element of semantic processing; with particular emphasis on the applicability to text recognition systems.
Much of this research is purely theoretical, in that it explores computational implementations of linguistic theories that are independent of any specific application area.
However, techniques that are developed for theoretical purposes or for other applications may have relevance to the present project, and this is indicated where appropriate.
2.1.1 Computational Linguistics
Computational Linguistics could be defined as the study of computer systems for understanding, generating and processing natural language [Grishman, 1986].
The motivation for research in this field may be applied (e.g. the development of human-computer interfaces, machine translation systems, information retrieval, etc.) or theoretical (e.g. the investigation of grammars proposed by theoretical linguists).
Language understanding is generally regarded as being of greater importance than generation, since understanding requires the recognition of many paraphrases for the same command or information, whereas generation may be satisfied with the production of just one.
Recognition is a prerequisite to understanding, since what has not been recognised can hardly be understood.
However, both can involve similar stages of processing in applying orthographic, lexical, syntactic and semantic constraints to the perceived input.
The difference is that with understanding, the semantic processing must eventually involve translation of the natural language input into an internal language with a semantics that is based on the knowledge representation structure of the system in question.
For example, if natural language is to be used to control a robot, then the English commands must ultimately be translated into the same knowledge representation formalism used by the robot to describe its own state, its goals, and the state of the environment around it.
Once translated, this command can then be acted upon, and the goals and states of the robot and its environment can be updated accordingly.
Recognition stops short of this stage, requiring’ only’the use of semantic information to eliminate inconsistencies in the input data, and (hopefully) arrive at a unique interpretation.
It is not necessarily essential to design knowledge representation formalisms into which the input text must be translated and subsequently acted upon.
However, such formalisms may be used as a knowledge base against which input can be checked for semantic consistency (e.g. Grosz, 1986).
2.1.2 Linguistic Semantics
The progression from theoretical representation to physical implementation is common to research in both natural language syntax and semantics.
For example, linguists produce theories of syntax, which specify what a human (or machine-based) parser has to compute.
Psycholinguists then produce theories of human parsing, and computational linguists produce theories of automated parsing.
Similarly, semantic theories derived from a number of disciplines (e.g., linguistics, philosophy, formal logic) have formulated theories of meaning, on which psycholinguists can base theories of how those meanings are computed in humans.
Computational semantics can extend these semantic theories to determine how those meanings could be computed by machine.
2.1.2.1 The Meaning of Meaning
The ability to understand is not the same as being able to explicate the concept of meaning.
Indeed, it is claimed that there are as many as sixteen senses of the word’meaning ’[Ogden and Richards, 1923].
Although psycholinguists need not be concerned with all sixteen (and perhaps computational linguists less still), there remain some distinctions of which both disciplines should be aware.
Of greatest importance is Frege's distinction between reference and sense .
The reference (or denotation ) of an expression is the thing that it stands for; e.g. the reference of the predicate’is blue ’ is the set of things that are blue.
The sense of an expression is more closely connected with its actual meaning, and is roughly equivalent to the content of the expression.
The sense of an expression determines which things it can denote.
One must also distinguish between the way meaning is applied to words and to sentences.
When applied to words in the form of a question, e.g.,’What does ‘spider’ mean?’the required answer concerns spiders in general, not any specific spider.
It therefore concerns the sense of the word’spider ’.
However, questions about the meaning of sentences, such as’Who did he mean by the woman he saw last night?’ can usually only be answered with reference to facts about the world, i.e., the specific denotation.
What is in doubt, with this second question, is not the semantic information that it conveys (the sense ), but the objects to which it refers (the reference ).
A complete semantic theory, whether human or computationally oriented, must specify for each expression what semantic information that expression conveys, which in turn determines what that expression can refer to.
It should therefore be able to compute all possible senses of an expression, so that the application of specific facts and world knowledge can then be used to determine the referents.
The process of understanding needs to be based on a complete semantic theory and representation; recognition does not.
To illustrate, consider the robot example given earlier.
For the robot to understand an English command, it must compute the semantics of the input text, i.e. determine the senses of the expression and refer these senses to objects in the environment, before the intended action can be taken.
Recognition, however, requires no such computation.
Recognition can be facilitated by the use of semantic knowledge to eliminate semantic inconsistencies within the input.
(Of course, this process could be guided by an algorithm that we can label as’ semantic’, but that hardly constitutes a semantic theory in any true sense.)
2.1.2.2 The Extent of Linguistic Semantics
Given that the origins of semantics are diffuse (logic, philosophy, linguistics, etc.) it may be also observed that the boundaries of the subject are equally amorphous.
It has been said that the word’ semantics’refers to the analysis of the meaning of single sentences.
By contrast, the analysis of the meaning of collections of sentences is referred to as discourse processing [Grishman, 1986].
However, upon closer analysis this distinction proves somewhat superficial.
A clearer distinction is that between semantics and pragmatics .
Morris [1938]proposed that semantics was the theory of the relation between signs and objects (i.e. words and their referents), and pragmatics that of signs and their users.
So for example, the assignment of the referent of a personal pronoun such as ’I ’ or’you ’depends on who is speaking (i.e. the user), and as such remains within the domain of pragmatics.
Consequently, most semantic theories have focused their attention on single sentences, rather than larger units (such as paragraphs, etc.).
These larger units of text are more than just sets of sentences, and convey more meaning than the sum of the individual sentences.
Perhaps a better definition of discourse processing is to state that it attempts to describe the’ extra meanings’that come about due to the combination of individual sentences within a larger passage of text.
For example, consider the following fragment of discourse:
Harriet was hungry.
She walked over to the fridge.
People can understand the relationship between these two sentences, and hence the coherence of this discourse, through the activation of relevant knowledge sources and elaborative inferences.
In this case, knowledge of human plans to relieve hunger and the location of typical food stores create the required coherence (i.e. the’ extra meaning’) between the two sentences.
These knowledge sources are essential to the comprehension of all but the simplest discourse, and hence present a considerable acquisition problem for computers.
The identification of the extra meanings mentioned above can only take place once the initial sentences have been in themselves understood.
This is a creative, active process, and one that relies upon the translation of input sentences into an internal knowledge representation.
This point is discussed further in Chapter Six.
Some researchers have attempted to model typical human knowledge sources in the form of’scripts ’ that could be activated, like human knowledge sources, when deemed relevant.
However, there is no reliable algorithm for the identification of relevant scripts, or when the new scripts should be activated, or how detailed they should be (or indeed how they could be efficiently acquired).
2.1.2.3 Subdivisions within Theoretical Semantics
Semantic theory may be further subdivided into the fields of lexical semantics and structural semantics.
Lexical semantics
refers to the meaning of individual words.
In computational theories of semantics, these meanings may be stored as the sense definitions of a machine-readable dictionary, or by some other representation.
In psychological theories of semantics, the meanings may be stored in one of a number of ways that are described in further detail below.
Structural semantics
refers to the way in which lexical meanings combine to produce complex semantic expressions.
It has its origins in formal logic, and owes much to the writings of Aristotle and Frege.
Montague [1970]argued that translation from natural language into a logical notation (such as predicate calculus) provides the basis of a semantic theory for that language, and that a precise method of translation could be determined and executed mechanically.
Tarski [1931]proposed the notion of semantic truth for a formalised language, arguing that the purpose of structural semantics is to show how sentences come to have the truth values they do, given the meanings of the individual words and the way the syntax combines them.
The majority of work on structural semantics has remained philosophical or at best highly theoretical, and has inspired few computational implementations.
2.2 Semantics and Psycholinguistic Theory
2.2.1 The Meaning of Words
Psycholinguists are concerned with two particular questions about word meaning:
(i)
How is knowledge about word meaning stored in the mind?
(ii)
How is it accessed during the process of understanding?
The first question concerns the issue of representation; the second that of processing.
Considering the first issue, most psycholinguists support the existence of a mental lexicon that contains knowledge about words.
The entries in the lexicon do not actually contain semantic information, but have instead pointers to locations in a separate store known as semantic memory in which the meanings are held.
The meanings of words can also be represented as textual definitions within an ordinary dictionary, so a third question may therefore be:
(iii)
How might semantic memory be related to dictionaries?
This is an important question since (a) dictionaries are attempts by people to represent externally what they know about language; and (b) they are an existing source of information that has evolved over hundreds of years.
Dictionaries define one word in terms of others, and this characteristic may be shared to some extent by semantic memory.
However, dictionaries and semantic memories have different purposes, and this fact is reflected in the way in which words are interconnected in each.
For example, dictionaries can also be used to define unknown words, relying on the assumption that the majority of words in the definition will already be known by the user.
In so doing, they assume that the users of dictionaries are linguistically knowledgeable.
Semantic memory cannot work in this way, since in itself it is partly responsible for performing this function.
2.2.2 Theories of Natural Language Semantics
Psycholinguists and AI researchers have both contributed toward theories of word meaning, and in recent decades a number of distinct theories have evolved.
Psychological theories of language semantics are especially relevant, for two reasons:(a) the most successful language processing system to date is that of the human information processing system; and (b) such theories attempt to produce information-processing models of cognitive processes that should, in principle, be computationally implementable.
In so doing, they should provide insight into the way these processes work.
The theories discussed below may have evolved from different assumptions, but it is still difficult to discriminate completely between them.
The representational aspects of the five theories differ widely, but all suffer from the same problems of knowledge acquisition and inefficiency when implementations are attempted.
2.2.2.1 Feature Theories
Chomsky [1965]argued that word meanings can be accurately described by sets of bivalent features, which he called semantic markers, e.g. male, animate, human, etc.
Where hierarchical relations exist they are represented by redundancy rules, e.g. if a word meaning has the feature HUMAN, then it also has the feature ANIMATE.
In this way, semantic markers decompose the meanings of words into more primitive elements.
Katz and Fodor [1963]proposed that there was a universal set of markers that could represent the meaning of words from every possible language.
Attempts to perform automated semantic analysis by means of the selectional restrictions provided by semantic features have almost universally exposed the theory as being crude and inefficient.
At the simplest level, semantic features may help identify correct word senses in sentences such as:John hit the post with a ball 
(where both’ hit’and’ball’have two possible senses and feature lists), since only one combination of features is possible.
However, given a sentence such as:James hit John at the ball 
it can be seen that the feature list for the’ dance’sense of’ball’needs to be modified to include the act of’hitting’; and with it, everything else that can occur at such a function.
This list would thus grow to considerable length.
Furthermore, it can be seen that feature lists would be duplicated for words such as’ dance’,’ball’,’party’, etc., which is grossly inefficient, and would result in very complex feature lists for each word in the lexicon.
2.2.2.2 Semantic Networks
Semantic networks became established in psycholinguistics through the work of Collins and Quillian [1969].
In a semantic network, concepts, which refer to word meanings, are represented by nodes.
The nodes are joined by a variety of links that represent the different relations between concepts such as set membership, set inclusion, part-whole, property attribution, etc.
The meaning of a word is determined by its place in the network as a whole, the most important characteristic being the hierarchical organisation of the set inclusion links (usually known as’ISA ’ links).
These hierarchies are most easily demonstrated by concrete nouns (e.g. collie ISA dog ISA animal, etc.)
It can be shown that semantic network representations are formally equivalent to semantic feature representations, in as much as any information that can be represented in one can be represented in the other.
However, semantic theories must consider not only how knowledge is represented but also how it is used, so these two approaches are treated separately.
Semantic networks inspired a number of natural language understanding programs during the early 1970's, with notable contributions from Schank [1972], Rumelhart [1972]and Anderson [1973].
The limitations of this representation soon became apparent; in particular its inability to deal with quantifiers.
Hendrix [1979]made some progress regarding this problem, introducing a technique known as partitioning to deal with quantifiers.
However, other difficulties remained: sentences that could not readily be decomposed into’ SUBJECT and PREDICATE’form also proved troublesome.
The basic problem with semantic networks, as indeed with many other techniques, is that they are a language in which the meaning of sentences can be expressed — a language in need of its own semantics.
However, some progress has been made concerning the knowledge acquisition problem: Amsler [1982], Calzolari [1984], Chodorow [1985]and Alshawi [1988]have all demonstrated the use of a machine-readable dictionary in the automatic construction of semantic relations and networks.
2.2.2.3 Case Frames
An alternative approach is to define words according to the sentence contexts in which they occur.
For example, Fillmore [1968]proposed the notion of a case grammar, in which each sentence was analysed into the cases attached to the verb:
Agent
: animate being initiating action;
Instrument
: inanimate entity involved in the action;
Recipient
: animate being affected by the action;
Object
: inanimate entity affected by the action;
Locative
: location or direction of the action.
The assignment of these categories need not necessarily follow the grammatical assignments (e.g., the object case need not be the syntactic object).
Using this technique, the lexicon defines each verb according to the cases it can take.
The main difference between this method and that of semantic features is the level of detail they specify.
For a verb such as’collide ’, all that is specified by the case restrictions is that the object case can be any inanimate entity.
Thus these case roles could be filled with nonsensical objects such as’sincerity ’ or’steam ’; i.e., one could say’steam collided with sincerity ’.
The case roles have been modified since Fillmore's original definition to include restrictions such as these, but the restriction lists grow to an interminable length and the technique is still unable to recognise that two expressions (e.g.’the woman ’ and’she ’) may refer to the same individual.
2.2.2.4 Prototypes
The theory of prototypes owes much of its origins to Wittgenstein's ideas about word meaning [1953], and its development to the work of Rosch [1975].
Wittgenstein argued that most words could not be defined in terms of lists of necessary and sufficient conditions for membership of the set for which they stood (his most noted example is that of the word’game ’).
Furthermore, network theories and feature theories fail to explain two important facts:
(a)
the correct classification of an object may be in doubt when its features are not;
(b)
some examples of a concept are more typical than others, and are more easily brought to mind.
Prototype theory sees entries in the mental dictionary as being centred on a representation of the prototypical member of the class to which the word belongs; e.g. a robin may be seen as the prototypical bird.
A prototype is located in a multi-dimensional space with dimensions corresponding to the characteristics on which examples of the concept can vary.
Boundary spaces can be drawn around the prototype that demonstrate the extent of the definition.
To give meaning in prototype theory is to determine how far something can differ from the prototype and still be a member of the class.
Minsky's frame system [1975]is a computational implementation of prototype theory.
Each concept is represented by a frame, which contains slots that may be filled differently for separate instances of the same concept.
These slots may have default values that represent the characteristics of the prototype.
The boundaries of the concept are implicit in the frame structure and in the constraints on the values of the slot fillers.
As with the scripts mentioned above, attempts to represent semantic knowledge in frames have suffered from the problems of relevance and detail, along with the usual acquisition problem.
2.2.2.5 Semantic Primitives
Some researchers have attempted to capture the core meaning of words by decomposing them into a small set of’ building blocks’known as semantic primitives [Wilks 1973].
Schank [1972]drew up a list of 12-15 primitive actions that he claims underlie the meaning of all active verbs, of which a number are listed below:
ATRANS
: transfer of possession
MTRANS
: transfer of mental information
PTRANS
: physical transfer from one location to another
MBUILD
: build memory structures
ATTEND
: sensory input
Schank argued that every verb in the lexicon could be expressed by a combination of these primitives, e.g.’give ’ can be ATRANS or in the case of’giving advice ’it can be decomposed to MTRANS .
So instead of specifying case frames (or indeed semantic features) for each individual word, Schank only needs to provide case frames for the primitives.
Moreover, verbs that involve the same primitive automatically have the same case frame, eliminating the duplication of effort seen with the other approaches.
However, this reduction of different verbs to the same primitive does have its disadvantages, as information may be lost (as in the case of’joke ’,’say ’ and’preach ’all being reduced to MTRANS ).
Furthermore, systems based on semantic primitives also suffer from the usual acquisition problem.
Schank used the notion of semantic primitives as the basis for a number of semantic analysis programs, which took as their input natural language text and gave a semantic representation composed of primitives as their output.
Typically, the program would look at the words from left to right, and test whether each word in the sentence was a likely candidate for the case slots of the main verb.
If later words in the sentence suggested a different categorisation of the main verb, then re-interpretation of the sentence was possible with re-allocation of the case roles.
In many ways, this type of semantic’ parsing’had all the abilities of a syntactic parser, plus the ability to allocate case roles according to semantic information extracted from the lexicon (e.g., an object such as’book ’cannot fill the Agent case role).
The semantic analyser would then link each of the primitives into a structure known as a Conceptual Dependency network, which represented the causal relations between actions and states.
2.2.2.6 Meaning Postulates
A meaning postulate is a formula expressing some aspect of the sense of a predicate [Hurford, 1983], using a predicate-calculus-like notation that permits any number of arguments.
Bar-Hillel [1967]argues for the superiority of meaning postulates over semantic markers due to their ability to represent arguments of lexical items, which are essential for expressing the relation between the meanings of words like’buy ’ and’sell ’:for any x, y, z (x sells y to z if and only if z buys y from x)
Strictly speaking, theories involving meaning postulates have their origins in formal semantics rather than psycholinguistics.
Although meaning postulates have been used by some psychologists to represent semantic relations, the theory has inspired little empirical research or computational implementation.
This is mainly due their inability to adequately represent certain phenomena; for example:
— temporal relations, i.e. the time at which a predicate applies.
This requires the development of a more elaborate logical framework;
— gradable predicates such as’tall ’,short ’,’large ’,’small ’, etc.
These words do not have an absolute meaning, since it varies according to context.
Meaning postulates are designed to account for truths that hold in all contexts, and are therefore less able to adequately represent such phenomena.
2.3 Semantic Processing
2.3.1 Requirements of the Present Project
The needs of the present project could be defined as’ to use the semantic constraints inherent in natural language to reduce the ambiguity in the output from a text recognition system’.
It can be argued that the successful development of such techniques requires an adherence to semantic theory from both the computational and linguistic perspective, to provide a sound theoretical framework.
However, during the present project the limitations of the established semantic theories have become apparent.
Indeed, the techniques that have proved to be of greatest use are empirical or almost’ trial and error’in their approach.
To use the linguist's terminology, they would be referred to as weak methods , to reflect their lack of theoretical rigour.
However, there is another important reason why the established semantic theories are less relevant: they pursue the goal of understanding rather than recognition .
Understanding requires the determination of the meaning of a sentence.
This usually implies translation into a formal language with a simpler semantics, and, using Tarski's definition, the determination of its truth conditions.
Recognition, however, does not necessarily require such processing (although under certain circumstances it may be desirable).
Instead, it may be sufficient merely to apply semantic constraints to assess the plausibility of a particular combination of words in the input.
Criteria such as’theoretical integrity ’ and’psychological plausibility ’dictate that any adopted techniques should be based on established semantic theory.
However, it may nevertheless be possible to’ simulate’the processes described by semantic theory using techniques based solely on empirical results.
This would be described, using the linguist's terminology again, as using’ weak’methods to achieve’strong’results.
After all, it could be argued that the purpose of a semantic analyser used within a text recognition system is not to demonstrate the plausibility of a particular semantic theory, but to simulate the output of a human reader.
Put another way, the process of human semantic processing as described by psycholinguistic theories may be simulated by programs that bear little resemblance to any established theory of linguistic semantics.
Published literature on the role of semantic processing within computerised text recognition is sparse.
The majority of research has tended to focus on the pattern recognition level, with the higher level processes being progressively of lesser interest.
However, a number of researchers have investigated semantic processing applied to related NLP problems, and some of the techniques and resources used have shown direct relevance to text recognition.
In particular, dictionary definitions and co-occurrence statistics have been identified as valuable sources of semantic information.
The acquisition and use of such information is discussed in the following sections.
2.3.2 The Acquisition of Semantic Information
2.3.2.1 From Machine-Readable Dictionaries
Many researchers are currently engaged in the processing of dictionaries in order to extract semantic information and reconstruct it in an alternative (more accessible) form; the objective often being the creation of lexicons for large-scale natural language processing.
This trend towards the construction of lexicons using machine-readable dictionaries (MRDs) follows the realisation by many that a major restriction on the functionality of many NLP systems is the small size of their lexicons [Alshawi 1988].
Zernik [1989]has identified a number of shortcomings with existing lexicons in terms of’ lexical gaps’; such as :
Single words
— where entries are missing from the lexicon;
Compound words
— e.g.,’respective ’ cannot be processed as’respect ’+’ive ’;
Word senses
— which vary according to topic or context;
Collocations
— e.g.’strong ’ and’powerful ’may be similar, but we cannot talk of’a strong car ’or’powerful tea ’;
Idioms and phrases
— of which the meaning is not a simple product of the constituents;
Metaphors
— of which the interpretation is not literal;
Others
including prepositions, noun group compounds, individual constraints, synonyms, etc.
Alshawi's [1988]analysis of the sense definitions in LDOCE attempts to provide sufficient semantic information to enable an NLP system to cope with unknown words.
The process starts with a hand-coded classification of the core vocabulary, and then the propagation of these structures throughout the dictionary so that all sense definitions are included.
In the case of processing nouns, this process involves the location of the semantic head (superordinate term), and the exploitation of other information present (e.g. modifiers and predications).
The structures so produced have some properties of a linguistic analysis of the definitions and some properties of a semantic definition of word sense concepts, and they take the basic syntactic form of nested feature lists.
Although the performance of the system seems respectable (correct semantic head located for 77% of the definitions, additional information recovered for 61% of definitions, of which 88% was correct), there are still problems associated with the processing of idioms, phrasal verbs, circular definitions and cross references.
Vossen, Meijs and den Broeder [1988]have carried out similar studies of the meaning and structure in dictionary definitions, based on Dik's [1987]’ stepwise lexical decomposition’, which reduces the meaning of lexical items to a restricted set of basic lexical items.
This procedure involves firstly the application of a grammar code to all the words in the core vocabulary (of the LDOCE) and their inflections.
These codes are then inserted in each of the sense definitions to create a corpus of coded definitions.
This is followed by the development of a syntactic typology for the meaning descriptions, and the subsequent creation of parser-grammars for each part of speech.
These grammars can then be applied to the coded corpus, with the intention of identifying the premodifiers, kernel and postmodifiers of each definition.
Finally, the development of a semantic typology allows the identification of horizontal and vertical links between words, the tracing of hyponyms and hypernyms of given words, and the identification of the properties of premodifiers and postmodifiers.
Further investigation of the semantic typology has identified four distinct structural patterns within the meaning definitions of nouns:
(1) Links
— in which the syntactic kernel is semantically a hypernym of the entry word, with pre- and post-modifiers expressing restrictions on the extension of the hypernym;
(2) Synonyms
— all semantic information is expressed by just one word (no need for modifiers or other restrictions);
(3) Linkers
— in which the kernel is somewhat meaningless and most of the semantic weight is carried by another part of the meaning description;
(4) Shunters
— in which the interpretation is’ shunted’from a nominal structure to a non-nominal structure (e.g. a verb phrase).
Further work on MRDs includes Guo's [1989]attempts to build a machine tractable dictionary (MTD) from the LDOCE, based on the fact that a set of 1,200 words (known as the Key Defining Vocabulary or KDV) is found to define the 2,219 words of the core vocabulary of the LDOCE.
It was proposed that the entire LDOCE vocabulary could be defined by the KDV by a series of four’ defining cycles’that progressively add more of the core vocabulary to the KDV, until after three cycles all the core vocabulary is accounted for and the fourth cycle defines the remaining 27,758 headwords.
The knowledge structures used to represent the dictionary entries are known as integrated semantic units, or ISUs.
These structures can be regarded as the semantic primitives of the MTD, incorporating linguistic knowledge with general world knowledge in the representation of each word sense.
The set of primitives that best suits a particular MRD can be found empirically, and an average of three basic senses of 1,200 KDV words requires the hand-crafting of 3,600 ISUs.
An initial attempt at hand-crafting a small set has been successful, and alternative approaches to pure hand-crafting are currently being investigated.
Although the extraction of semantic information from dictionary definitions may be considered an objective in itself, many researchers pursue a specific application, or at least have a range of applications in mind.
For Jensen and Binot [1988], that application was to create a knowledge source for syntactic disambiguation.
In particular, they addressed the issue of the attachment of prepositional phrases and relative clauses, but also considered anaphoric reference and the interpretation of dangling modifiers.
It was their intention that the natural language of the dictionary definitions should be used as the knowledge representation language, eliminating the need for hand-coding.
For example, given the sentences:
I ate a fish with a fork
I ate a fish with bones
it can be seen that the prepositional phrase can have two sources of attachment.
The objective, in each case, is to determine that source of attachment.
In the first example, the system compares the link between’ate ’ and’fork ’with that of’fish ’and’fork ’.
It finds that according to the dictionary definitions there is a link between’ate ’ and’fork ’in that they both have the phrase’taking up ’in common.
The absence of any link between’fish ’ and’fork ’confirms the choice that the prepositional phrase’with a fork ’should be attached to’ate ’.
In the second example, the system compares the connection between’ate ’ and’bones ’with that of’fish ’and’bones ’, finding a connection in the latter case through the word’vertebrate ’, and the prepositional attachment is chosen accordingly.
A similar technique is used to resolve anaphoric references and similar syntactic ambiguities.
Chodorow, Byrd and Heidorn [1985]have investigated the use of MRDs in the construction of semantic hierarchies, based on the assumption that all noun definitions have’ genus’and’differentiae’terms.
The genus extraction is performed by a limited form of parsing, usually involving the identification of the head of the defining phrase.
From this data, two types of hierarchy can be created, the processes involved being known as’sprouting ’ and’filtering ’.
The sprouting process involves the creation of a semantic tree from a specified root, organising the results of the head-finding into a’ hyponym index’.
For example,’vehicle ’ may have as an entry:vehicle: ambulance…bicycle…car…tanker…
Although all the words have at least one sense bearing the property for which the root was originally selected, it must be noted that to locate all words bearing a particular semantic feature must involve the careful selection of several roots (e.g. to find nouns with a [+female]feature, sprouts should begin from’ female’,’woman’,’girl’and possibly’wife’).
The filtering process also produces lists of words bearing a certain feature, but only those words of which all the senses have the feature.
It is created using a’ hypernym index’, in which each word is listed with its hypernyms.
The data so produced may be of use to parsing systems, whenever it becomes necessary to know whether a noun must have a certain feature, not merely that it may have it.
Byrd et al[1987]have extended this work, complementing the above two processes with other lexicographic tools and methodologies, such as Head Finding (a method for automatically discovering hypernyms of words); Matrix Building (for clustering synonyms into senses and analysing senses in bilingual dictionaries); TUPLES (a system for finding frequent words and phrases) and others.
Markowitz [1986]investigated the creation of large lexicons for NLP using semantically significant patterns in Webster's Seventh Collegiate Dictionary (W7).
She saw the problem as that of making the information implicit in a dictionary explicit, by finding taxonomies, set membership, recognising human nouns, etc.
For example, noun definitions that begin with the word’Any ’ usually indicate a taxonomic relationship between the noun being defined and the word following the word’Any ’.
Similarly, definitions beginning with’A member of …’ indicate a member-set relation; usually human.
Generic agents were signalled by the sequence’One that …’ and human nouns were often identified by the suffices’-er ’,’-ant ’, etc.
2.3.2.2 From Co-occurrence Statistics
A technique for describing text types based on statistical data has been investigated by Huizhong [1986].
His use of frequency of occurrence and distribution data to identify scientific and technical terms suggests that it is possible to characterise text types according to the collocational behaviour of these terms.
Three criteria for text characterisation have been identified: firstly, that of subject matter; secondly, that of genre (for whom the texts are written); and thirdly, that of topic-type (information flow and concept structure).
Preliminary results have shown good differentiation between science texts and general texts; although to what level of detail this can be applied (i.e., identification of topic?) remains unspecified.
Plate [1989]has investigated the use of co-occurrence statistics obtained from LDOCE.
He restricted his investigation to the 2,200 or so words in the LDOCE core vocabulary, and took the sense definitions as the textual units over which to collect co-occurrence data.
The data obtained forms the triangle of a 2,200 by 2,200 matrix (requiring 4.7Mb disk space), and as such required further processing to be rendered comprehensible (in this case, the application of programs called PATHFINDER and BROWSE).
Tests comparing concept relatedness based on co-occurrence data with that of human judgement showed a strong correlation.
Co-occurrence data has been used by the present project; the data having been collected from a number of corpora rather than LDOCE.
Such free-text sources were identified as being more representative of the type of text that the system would eventually have to recognise.
An alternative representational structure was chosen to reduce the need for extended processing and memory overheads.
This information has been shown to make a significant contribution to the recognition process, and is described in detail in Chapter Four.
Fraenkel [1980]has investigated the semi-automatic construction of’ semantic concordances’, in which homographs are distributed into disjoint classes with one semantic value per class.
The technique involves the partitioning of words in the text into classes based on certain similarities, and then further partitioning based on small word contexts.
An editor can then check one representative word from each small class as to its correct meaning in context, and this meaning can then be assigned to all other words in this class.
Other researchers [McKinnon, 1975]have applied the statistical techniques of cluster analysis to samples of language that conform to regular patterns within a specific subject domain (known as’sublanguages ’).
Sager [1981]has identified a number of sublanguages that are sufficiently regular for this technique, and provides a methodology for the reliable identification of their constituent semantic classes.
These semantic classes can then be used to define a sublanguage semantic grammar , which constitutes a further level of semantic constraint on the text.
She does, however, point out that these techniques are only suitable for text that displays the specialised sublanguage characteristics.
Smadja [1989]used co-occurrence data as an aid to language generation.
He observed that there are certain classes of English word combinations that neither syntax nor semantics can justify; for example, although the words’strong ’ and’powerful ’may have a similar meaning, people prefer saying’drink strong tea ’to’powerful tea ’and similarly prefer’drive a powerful car ’to’strong car ’.
Smadja argues that knowledge of relations such as these is necessary to both understanding and generation, and he outlines an approach for automatically acquiring such restrictions from a corpus, and then using it to augment an existing lexicon.
He also makes the distinction between lexical and conceptual collocations; the latter being word pairs that co-occur simply because they are associated to the same context or topic (e.g.,’bomb ’ and’soldier ’,’trouble ’and’problem ’, etc.).
Although Smadja suggests no specific use for conceptual co-occurrences in his lexicon-building research, it is shown in Chapter Four that both types are of value to the current project.
Choueka [1988]has designed an algorithm for locating collocational expressions in corpora that does not include any morphological or syntactic component and does not require any dictionary lookup.
His algorithm was applied to the New York Times News Wire Service and produced various lists of collocational expressions from length two to six.
Lancashire [1987]also gives details of such an algorithm, and this has been adapted to meet the needs of the present project (see Chapter Four).
2.3.3 The Use of Semantic Information
Semantic information may be used for a variety of linguistic purposes.
A good example is that of word sense disambiguation, since it is relevant to many NLP applications.
Word sense disambiguation may be seen as a knowledge-intensive problem.
Jacobs [1989]has identified some of the knowledge sources contributing to sense discrimination as:
Morphology
— senses cannot always be derived by affix-stripping, e.g.’conductivity ’ derives from’conduct ’, but corresponds only to the electrical sense (one would not talk of the’conductivity ’of an orchestra);
Word frequency
— some word senses only occur in specific expressions;
Topic
— word senses vary according to subject area or domain;
Word senses from a dictionary
— generally, dictionaries have more senses than necessary for broad applications but too few for specific applications;
Collocations
— some word senses appear only when used in particular idioms or collocations; Intersections — e.g. if’conduct ’ and’violin ’appear together, this would suggest a different sense of’conduct ’than if it appeared with’wire ’;
Semantic preferences
— often more structured than collocations and intersections, e.g.’the sun rises ’;
Syntactic information
— e.g. the word’conduct ’ in’John's conduct at his violin lesson ’invokes the sense of behaviour due to its status as a noun, despite other connections between’conduct ’and’violin ’.
Lesk [1988]described dictionary-based techniques for determining the senses of words used in text and choosing the most appropriate one according to the sentential context.
The correct sense is chosen by accessing the word's sense definitions within the O.E.D., then counting the words that each sense definition has in common with the definitions of the other words in the immediate context.
The sense definition with the highest commonality is the one chosen.
This technique has been applied extensively by the present project to the problem of discriminating between different candidate words in a single sentential position (rather than different senses of the same word).
It has been shown to make a significant contribution to text recognition, and the results are described in Chapter Three.
Black [1988]has described a number of methods for discriminating English word senses, again by examining the sentential context.
The first method, after Debili [1982], uses a listing of word pairs observed to have entered into certain syntactic relations in a previously analysed corpus (e.g., subject/main verb or noun/adjectival modifier, etc.).
Word pairs on this list are given a’ validity score’of 1, and those not on it are given 0.
When the program finds a word with multiple senses, it produces a list of all the synonyms of each of the senses of that word, which are then known as’ word families’.
The program then chooses a neighbouring word (the sense of which is unambiguous) and determines the syntactic relation between this word and the word in question.
The maximum is now calculated of the products of the validity scores of the word families and the unambiguous word, and the word family yielding this maximum determines the sense chosen.
This technique may be seen as an extension of the basic co-occurrence methods, as it includes the concept of matching using a group of semantic relatives (rather than just the word itself) and also matches according to specific syntactic relations.
The second method, after Gross [1985], uses a lexicon-grammar, in the form of a two-dimensional matrix.
Columns of this matrix are labelled with possible syntactic properties of each entry word and semantic properties, e.g.,’concrete ’,’animate ’, etc.
Candidate word senses are determined by inspecting the context of a given word and eliminating those words whose conditions of usage (according to the matrix) are not met.
This technique may be seen as an extension of the established corpus-based parsing techniques to include a limited coverage of semantic features.
Kelly and Stone [1975]restricted their sense determination research to that of a KWIC concordance of 1,815 types taken from a 510,976-token corpus.
Their objective was to use the concordance and dictionary definitions to produce an ordered set of disambiguation rules that would determine the correct sense of a word by testing for both part of speech and membership of sixteen specific semantic categories.
The results of each test would then either assign a specific sense to the word or activate other rules within the set.
Sinclair [1970]has used the statistics of occurrence of various word senses to guide the determination process, using the 7.3 million-word Birmingham database of English as his corpus.
Amsler and Walker [1986]have used the subject categories (referred to as domain codes) present in the LDOCE as a source of data for sense disambiguation.
Each word within a paragraph is assigned all its possible subject categories, and the category most frequently represented over the whole paragraph is deemed to be the subject area of the text.
The senses of each word within the paragraph carrying this domain code are then selected as being correct, in favour of alternative senses not bearing this code.
This technique is significant for two reasons — not only does it aid the process of word sense disambiguation, it also provides a method whereby the topic of the text can be identified.
These codes, along with a further set derived using an original corpus-based method, are investigated in Chapter Six.
Slator [1989]also used the domain codes within the LDOCE for the same purpose, but only after having restructured the hierarchy of the coding system.
He suggests that this restructuring gives a better intuitive ordering of the important concepts in each text, and enables a knowledge-based and context dependent strategy for making word sense selections.
Dahlgren [1986]approaches the problem of sense discrimination using an ordered set of categorial rules that are applied in sequence to the text to be discriminated.
The first type of rule uses data on concordances, referred to as’ frequent collocates’.
The second type of rule is based on syntax, and checks for dependency relations such as(in the case of a noun) the presence of an associated definite article, personal pronoun, etc.
The third sort of rule uses’ common-sense knowledge’as defined by a number of psycholinguistic studies, and represented by a’tangled hierarchy’of ontological predicates.
These rules test for similarity between the word in focus and its neighbours, the highest similarity indicating the correct sense.
Black [1988]compared experimentally three techniques for sense discrimination: a domain general method and two domain specific methods.
Five experimental words were chosen; four of which had four senses and the remaining word three.
About 2,000 concordance lines were obtained for each test word, taken from a 22 million-token corpus.
Each experimental method consisted of a set of 81’ contextual categories’; such that the context of a word was represented by the pattern of presences or absences of each of the 81 categories within each concordance line.
The domain general method (henceforth known as’ DG’) was based on Amsler and Walker's subject category approach, using the domain codes in the LDOCE.
Each of the 500 most frequently appearing words in the 2,000 concordance lines was analysed with respect to their definitions, to produce a profile of the concordances from the point of view of the domain codes in LDOCE.
The requisite 81 categories were then derived from this profile.
The first domain-specific method (’ DS1’) was based on the frequencies of different lexical items in a training corpus of 1,500+ concordances.
Two classes of category were identified: the first consisted of the 41 types occurring most frequently in a window of + or -2 word positions, to capture those words in close grammatical construction with the node.
The second class consisted of the 40 most frequent words excluding function words and extending over the entire concordance line, to represent collocates of the node.
The second domain-specific method (DS2) resembles the first in that 20 of its 81 categories were one or two-word sequences occurring most frequently in a window of + or -1 and 2 word positions, respectively.
The remaining 61 were derived from the concordances of 100 randomly chosen types occurring in the corpus.
In the experiment it was found that all three methods produced results that were better than random selection — but the domain specific methods were far superior to the domain-general method (DG was 27% better than chance, the DS methods both roughly twice as good as chance).
It was suggested that the DG method failed to represent the thematic organisation of the concordances analysed, as the categories that might have been chosen on an’ intuitive’basis do not seem to attract any of the 500 most frequently appearing words of a test item.
Concerning the usefulness of the three techniques, it may be noted that all suffer from the necessity for hand-labelling of concordances, which remains a task of considerable size.
Whilst the automation of this process remains only a distant possibility, it must be conceded that none of these techniques is directly usable as described.
Other researchers, concerned with word sense disambiguation for the purposes of thesaurus creation, have also used the overlap technique.
For example, Byrd [1989]solves the problem of matching word senses in different thesauri by computing a set of’ sense property vectors’for each entry from the two thesauri, and then computing an’optimal’mapping of the two sets.
This’ optimal’mapping is based on maximising the overlap between the sense property vectors (which in this case take the form of synonym lists).
Byrd has also investigated the use of the overlap process to map between dictionaries, matching word sense definitions.
He describes his results as showing’ limited’success; with fewer than 50% of the mappings being correct.
Two types of failure were observed; with incorrect mappings being proposed in some cases, and correct mappings being missed in others.
Byrd concludes that lists of undifferentiated definition words are not selective enough for adequate mappings, but the overall plan shows promise.
He suggests that assignment of specific sense properties in the definitions and then matching on a property-by-property basis may offer the greater degree of constraint necessary.
The identification of the’ theme’or subject area of a passage of text can provide valuable information concerning the likely semantic content of that passage.
In applications where the domain is restricted or known in advance, semantic knowledge structures can be used to constrain the range of words that are statistically likely to occur within that subject area.
Grosz [1986]has demonstrated the use of semantic networks as a knowledge source to aid knowledge-based discourse processing.
Alternatively, the work of Critz [1982]has demonstrated the possibility of automatic theme recognition used frame-based representations.
His system determines the thematic continuity of English texts by first parsing to find the head noun, which is chosen initially to represent the theme, and declared as the’ theme noun’.
Then a list of frames indexed by that theme noun is searched to associate it with any frames currently active (i.e., those indexed by previous theme nouns).
If no direct association is found, interpretive rules are applied to attempt an indirect association, through one of the frames normally associated with the object but not yet associated with the text.
The work of Walker and Amsler [1986]on word sense discrimination (described above) involves the use of subject codes in LDOCE to identify the topic of a piece of text.
A similar process of topic identification can contribute to the present project, by reinforcing the choice of words whose senses contain subject codes that have been identified as being representative of the overall text.
For static recognition, this could be achieved pre-processing the whole text to determine the topic.
For dynamic recognition, processing may proceed from left to right through the text with subject codes of new words being compared to a’ running profile’of subject codes taken from previous words.
These techniques are discussed in greater detail in Chapter Six.
2.4 Current Resources
2.4.1 Machine-Readable Dictionaries
It was mentioned earlier that MRDs were designed for human rather than computational use.
In many cases, they are provided as little more than typesetting tapes, in need of considerable’ cleaning up’or normalisation.
This involves the removal of typesetting codes, removal of errors, and the identification and labelling of all the information such that it can be easily accessed without detailed knowledge of the formatting conventions [Byrd et al, 1987].
Consequently, it is possible to identify types of information that are needed by language processing systems but are either absent from or wrongly presented within the dictionary.
Krovetz [1987]has identified four such types:
Sense frequency information
— which can give preference to one sense when other factors are equal; and can be biased according to sublanguage/domain;
Collocation information
— the words that a word sense co-occurs with, along with an indication of frequency (N.B. an exception to this is the COBUILD dictionary, which provides many such examples);
Proper nouns
— a greater coverage of these is needed;
Semantic class
— the grouping of words according to similar semantic behaviour offers potential for predicting semantic roles based on their classification.
Braden-Harder and Zadrozny [1989]have identified what they refer to as a’ wish-list’for MRD organisation; in terms of enhancements to existing information and additional information:
Short hierarchical definitions
— i.e. collections of short sentences, listing the more important facts first (it is easier to parse short sentences, and controlling the’ spread of activation’of background knowledge is easier);
Cross reference
— which requires the disambiguation of word senses in the definitions to control the spread of activation caused by cross-referencing;
Synonyms and Antonyms
— which should be easily accessible from a given entry;
Example sentences
— which are a good source for typical subjects and objects; compressed versions are difficult to parse and therefore best avoided;
Information from bilingual dictionaries
— which may enhance the lexicon.
For , typical prepositions may be included in verb definitions.
Preference/frequency
— ordering the word senses according to frequency is desirable, and the inclusion of co-occurrence information shows promise;
Combining multiple dictionaries
— to solve the problem of mapping sense information from one MRD to another.
It must be noted that the use of MRDs is not a panacea to the problem of lexicon construction.
They were constructed for human interpretation and they are thus designed to provide verbal explanations and translations of words, rather than the morphological, syntactic and semantic data required by NLP lexicons.
McNaught [1988]has commented:
’…publisher's MRDs have the wrong form, and the wrong content, and while some ad hoc programming may achieve rapid partial results…further study or exploitation of existing MRDs will lead to diminishing returns.’
Jacobs [1989]adds the deficiencies of circularity of definitions and obsolescence to the above; and argues that MRDs are only a piece of the solution to the lexical acquisition problem.
Furthermore, he observes that most NLP systems ignore a major source of knowledge, i.e. that of the input itself.
Instead of maintaining the general domain of a text topic and preserving partial results to form hypotheses about new words and meanings, most natural language programs preserve little or nothing from one sentence to the next.
Amsler [1989]has drawn attention to the inadequacies of MRDs in the context of developing lexical knowledge bases for NLP.
He describes a comparison between the word forms found in a sample of text taken from the New York Times Newswire Service and those listed as entries in the Merriam-Webster Seventh Collegiate Dictionary, which showed that 64% of the words in the text were not in the dictionary.
Of these omissions, one quarter were inflected forms, one quarter were proper nouns, one sixth were hyphenated forms, one twelfth were misspellings and one quarter were not yet resolved, but were likely to be new words occurring since the dictionary was published.
Amsler also draws attention to the assumptions made by early lexical knowledge bases, in particular the notion that a word is a contiguous sequence of alphabetic characters.
Amsler shows that this notion ignores important classes of words such as open nominal compounds, phrasal verbs and idioms.
He states that NLP systems’…lacked a complete lexicon of the language they were attempting to manipulate intelligently and had no rules for understanding how to recognise these lexical concepts when they appeared in text.’
Furthermore, he argues that proper-nouns have a grammatical structure, and that the compositional rules for such compounds will have to be written into NLP systems.
By contrast, Sampson [1989]investigated a derivative of the OALD by running it over some 50,000 words from the LOB Corpus, and found the coverage to be surprisingly high: of the 45,622 tokens he believed should be handled by the dictionary, 43,490 were found as they stood.
When minor morphological changes were made (e.g. hyphen removal, etc.) there remained only 1,477 word tokens not found in the dictionary: that is, 3.24% of the target domain.
He identified the largest category of omitted types to be (predictably) proper names, but noted also a bias against technical vocabulary and derived forms with negative meanings.
He concludes that’…a modest standard dictionary…is remarkably successful at covering the vocabulary of ordinary printed documents ’.
Ahlswede and Evens [1988]have investigated techniques for extracting information from MRDs using the concept of a defining formula.
Defining formulas are certain words or phrases that are frequently used in definitions, such as’the quality or state of being ’ or’of or relating to ’, and these have been used to identify a variety of lexical-semantic relations in dictionary definitions.
In particular they demonstrate how definitions can be used to generate word-relation-word triples that are then used as data to build the lexicon.
2.4.2 Text Corpora
A corpus is a body of text or speech that provides a representative sample of a language.
Text corpora provide empirical data concerning language usage, and as such may be used in the design and testing of NLP systems.
However, they are of limited use in their raw state — they must be statistically analysed to provide meaningful information, such as word frequency data or collocations.
Yet for many years the idea of using probabilistic information within an NLP system was viewed with some disdain by the linguistic community.
Many linguists felt that since corpora were finite and degenerate they were unable to deal with many of the phenomena present in language, and could offer no insight to the’ real’question of how people process language.
Stylistic analysis was one of the few tasks for which such statistical information was deemed appropriate [Ellegard, 1962].
Furthermore, the use of text corpora was hampered by severe practical difficulties: computers were rare and expensive, and the only method of acquisition was manual input.
Consequently, text corpora were limited both in size and availability.
However, in recent years the processing power and storage capacity of computers have increased dramatically, and many more textual resources are now available in electronic form.
These developments have fostered a proliferation of corpus compilation projects, with some of the more recent ones having target sizes set at 100 million words, e.g.:
The Lancaster-Oslo-Bergen Corpus [Johansson, 1980]— 1 million words;
The COBUILD Corpus [Sinclair, 1987]— 20 million words;
The Longman/Lancaster Corpus [Crowdy, 1992]— 30 million words;
The TEI Corpus [Walker, 1989]— 100 million words;
The British National Corpus [Leech, 1993]— 100 million words.
Evidently, corpora have increased in size as resources have expanded and techniques become more refined.
This is a reflection of the fact that there is a huge imbalance in the frequency of words in the English language, and large corpora are needed to provide adequate coverage of low frequency words.
However, size is not the only important factor: for a corpus to be truly representative of a language it must also be’ balanced’.
There are a number of variables against which a language can vary, e.g. time-span, geographical origin, gender of author, discourse type, subject area, etc., and the sources from which the corpus is compiled must be selected carefully to maintain a representative balance.
2.5 Semantics and Text Recognition Systems
The most successful text recognition system to date is that of the human information processing system.
This is largely because human readers use an understanding of the text that can guide the reading process.
Word images occur within a meaningful context, and human readers are able to exploit the syntactic and semantic constraints of the textual material [Rayner, 1983].
Indeed, it is argued that the conspicuous gap between the reading performance of people and that of algorithms may reflect the fact that few text recognition systems utilise the many knowledge sources or recognition strategy of the human reader [Hull, 1987].
Handwriting is inherently more ambiguous than printed text, and consequently the role that higher level knowledge plays in its recognition is particularly significant.
It would be reasonable to assume, therefore, that particular attention would be given to the incorporation of such knowledge in the development of handwriting recognition systems.
However, almost exclusively, early systems have made little or no attempt to use any such knowledge beyond that of the word level (e.g. Earnest [1962], Eden [1964], Sayre [1973], Tappert [1984]).
Indeed, a recent and very comprehensive survey of handwriting recognition techniques and systems considered its significance to merit just one sentence:’Higher level linguistic rules such as syntax and semantics can also increase the recognition rate ’[Tappert et al, 1990].
Even if the argument concerning whether semantic knowledge should be incorporated is upheld, there still remains the question of how it should be incorporated.
Ultimately, the issue of integration within a complete system architecture will have to be addressed.
There are those who argue that psychologically plausible NLP systems cannot be constructed by conjoining various knowledge-specific modules in series or hierarchically; they must instead be massively parallel and strongly interactive.
Some systems have addressed this problem using a blackboard approach (e.g. HEARSAY,[Erman, 1975]) in which a neutral working area is set up for components to store the results of their analysis.
This approach has experienced some degree of success [Erman, 1980].
The work of Waltz and Pollack [1985]shows a different commitment to parallelism, involving the co-operation of many knowledge sources, such as word use, word order, phrase structure and’ real-world’knowledge.
Their model offers insights into a variety of linguistic phenomena, such as:
Ambiguity
— the system can compute multiple readings, and shows increased processing load with ambiguous language;
Single interpretation
— the system can consider only one interpretation of an ambiguous sentence at a time, but can easily’ flip’between interpretations (as in visual disambiguation of the Necker Cube);
Comprehension errors
—’ garden path sentences’have more natural and complete explanations as side-effects of strongly interactive processes;
Non-grammatical text
— humans are able to interpret non-grammatical language, relaxing constraints as necessary to handle ill-formed input.
The structure of their model is that of a network displaying the characteristics of both spreading activation and lateral inhibition .
The network can be seen as divided into four layers: the first shows the syntactic parse tree for the sentence; the second, the actual input words; the third, a cluster of meanings for individual words (with mutually inhibitory links); and the fourth, a contextual interpretation of the input (with activatory and inhibitory links between lexical categories and meanings).
The model can be used to represent various components of human comprehension, including:
Semantic priming
— to bias the competition in the network and influence the stable states;
Autonomy and integration
— to represent competing alternative parses and account for garden path sentences with priming;
Errors in comprehension
— effects that depend on the arrival time of words can be modelled, allowing’ snapshots’to be taken during the processing of a sentence that induces a’cognitive double-take’;
Case frames
— selectional restrictions of various words can be modelled by the type of links attached to their representative nodes.
Dyer [1989]has suggested ways in which connectionist and symbolic systems may be combined, demonstrating desirable characteristics from each (i.e., variable bindings, logical rules, hierarchies and inheritance from symbolic systems, and reconstructive memory, graceful degradation, category formation, etc. from connectionist systems).
Using a technique known as symbol recirculation, Dyer shows that distributed symbol representations can be learned through recurrent networks that will generate expectations concerning words that will occur next in tasks such as script-learning and comprehension.
Using this approach, words gain their meanings through how they are used in context (by iteration through a training set), such that each word in the lexicon implicitly represents all the language-use experiences in which it has so far been involved.
Although much further research is required, evidently connectionist models can provide a framework for modelling comprehension phenomena that cannot be tackled using ordinary serial or symbolic models.
Hull [1987]has investigated a computational theory of reading and made progress towards defining an algorithmic realisation of this theory.
His theory is based on psychological studies of the reading process; acknowledging the extent to which the integration of understanding and recognition is responsible for the fluent reading capabilities displayed by people.
Inspired by the human model, the theory states that there are two steps of visual processing, which are influenced by higher-level cognitive processes.
A gross visual description of a word is used as the starting point, to which a small number of feature tests are applied to discriminate between the possible interpretations.
Further selective analysis is performed through the application of language characteristics such as syntax and semantics.
This technique has demonstrated recognition rates of 96% when applied to images of 12,600 words.
Ramsay [1987]has identified a number of linguistic levels as being necessary components within an NLP system architecture, and these are shown in Table 2.1.
He argues further that any system that assumes a simple uni-directional flow of information (either bottom-up for generation or top-down for comprehension) will be ineffective.
This is because at any level there are competing interpretations of the data that cannot be disambiguated, for which the appropriate information is available at some other level.
Ramsay identifies two solutions to this problem.
The first is to use the blackboard approach described above [Erman, 1975], but this is criticised as having serious implementational problems relating to the format of entries on the blackboard and the control over the resources that should be available to each component.
The second solution is to try to carry ambiguities around in the form of constraints [Sussman & steele , 1980].
Using this approach it is not necessary to resolve alternative interpretations immediately — they may instead be maintained until their combination with other constraints produces in a single interpretation.
2.6 Summary
Semantics is the study of meaning, and as such derives much of it theoretical inspiration from disciplines such as philosophy, psychology and linguistics.
A semantic theory attempts to formulate ways in which meanings can be represented and processed, either in the abstract, or by people, or by machines.
Theoretical semantics operates on two levels:(a) lexical semantics, which is concerned with the meaning of individual words; and (b) structural semantics, which deals with complex expressions produced by the combination of words within sentences.
In principle, linguistic semantics is concerned with the analysis of single sentences; the analysis of collections of sentences being generally referred to as discourse processing.
It does not concern itself with encyclopaedic or’ world’knowledge — that is deemed the province of pragmatics.
Psychological theories of semantics have been shown to be important, since the human information processing system is the best example of a language processing system, and that such theories attempt to produce cognitive models that should be computationally implementable.
A number of semantic theories have been discussed; these are shown to differ widely in terms of their representational aspects.
However, all suffer from a variety of implementational difficulties, the most common of which being the problems of knowledge acquisition and inefficiency.
A number of sources of semantic information are identified, the most notable of which being machine-readable dictionaries and text corpora.
Several techniques for the extraction of such information are discussed and evaluated.
A variety of natural language applications that make use of such semantic information are described, and their relevance to the present project is indicated where appropriate.
None of these applications is specifically concerned with text recognition.
Moreover, in many cases the objective is language understanding rather than language recognition .
The issue of lexical acquisition has been discussed, and a number of suggested improvements to the design of MRDs has been identified.
The ways in which semantic information may be used within a text recognition system have been described, and a number of possible system architectures discussed.
These have included bottom-up and top-down systems, blackboard systems, constraint-based approaches and connectionist models.
The present project uses a highly eclectic definition of the word semantics.
All sources of information, if they are related to meaning, are deemed worthy of investigation and possible subsequent inclusion in the design of a’ semantic knowledge base’.
These include discourse-based phenomena, encyclopaedic knowledge and syntagmatic information such as collocations.
Chapter Three
Dictionary Definitions
3.1 Introduction
Natural language semantics can be defined as the study of the meaning of utterances, and to this end a number of semantic theories have been proposed.
These theories attempt to define the method by which meanings are computed (by people or machines).
Traditionally, semantic theories have been more concerned with language understanding rather than recognition ; and have in most cases attempted a full exposition of language in all its semantic complexity.
This contrasts sharply with the needs of the present project.
Recognition (not understanding) is the objective; and when applied to a limited domain, only a subset of the language may be necessary.
To what then, should the present project turn, for its semantic theories, principles, and data sources?
The established semantic theories are severely limited in terms of their computational applicability.
Although these theories may work for artificial domains that are both small and concrete, extending them for large, real world vocabularies is difficult.
Firstly, there is the problem of acquisition.
The hand-crafting of semantic information for a large vocabulary would be a complex and time-consuming job.
Secondly, while some theories may work for concrete subjects, they may not be as applicable to abstract concepts, such as’ justice’,’insurance’or’business’.
Thirdly, such theories can easily become unwieldy and inefficient when applied to larger domains [Bookman, 1987].
As an alternative, it is suggested that simpler techniques involving the processing of machine-readable dictionaries and data within the text itself are more practical, and offer better prospects for a successful implementation.
Several applications have successfully used MRDs as their source of semantic information, e.g. taxonomy creation [Chodorow, Byrd & Heidorn, 1985]and knowledge-based parsing [Jensen & binot , 1988].
Lesk [1988]used dictionary definitions to disambiguate polysemous words within a passage of text.
Where a word in a sentence has more than one meaning, dictionary definitions can be used to select the appropriate sense.
For example, consider the sentence:I swim across the river to the bank 
This sentence (like most others) is composed of the two types of words: function words and content words.
Function words are those which give structure to a sentence, such as articles, pronouns, prepositions, etc.
In this example, the function words are’I ’,’across ’,’the ’, and’to ’.
The content words are the remainder, i.e.’swim ’,’river ’, and’bank ’.
It is this latter set of words that is of semantic interest, as it is they that’ seem to contain more meaning’[Bolinger & sears , 1981]
However, the meaning that these words contain is not always clear and unambiguous.
For example, the word’bank ’, in isolation, can refer to a financial institution or to the sides of a river (among other things).
The actual interpretation chosen by people is guided by context — in this case most would choose the second meaning.
This is because their world knowledge tells them that the banks found near rivers are not usually of the financial type (although it must be stressed that this interpretation is not wrong , it is just less likely ).
By contrast, a computer has no such knowledge.
It has no general knowledge source to aid the process of disambiguation.
However, there is much information contained in dictionary definitions, and this can be used as a crude replacement for some aspects of human knowledge.
Lesk's method is to compare the various sense definitions of a word with the definitions of other words in the immediate sentential context.
The degree of commonality or’ overlap’between definitions is measured, and the word sense showing the highest overlap is selected as the appropriate sense for that context.
Consider the above example.
The word’bank ’ would have a number of sense definitions, including a financial one and a geographical one.
The geographical one might contain words like’land ’,’river ’,’side ’,’water ’, etc. and the financial one words like’establishment ’,’keep ’,’money ’,’safe ’, etc.
When these are compared with the word’river ’ and its definition (which might contain words like’stream ’,’water ’,’flow ’, etc.), we can immediately see a greater overlap with the geographical definition of’bank ’.
A similar result is obtained if the senses of’bank ’ are compared with the word’swim ’.
In this way, word senses can be disambiguated simply by comparing them with the definitions of neighbouring words.
Experimentation with this technique has yielded accuracies of 50-70% on short samples of text [Lesk, 1987].
It should be noted, however, that the disambiguation required by the present project is not between multiple word senses , but instead between multiple interpretations of the input.
This is best illustrated by considering the flow of information through the various stages of recognition, as shown in Figure 3.1.
The first stage of recognition is pattern recognition, which outputs a list of candidate letters for each letter position.
Post-processing then begins with the lexical analyser, which differentiates the acceptable character strings (i.e. English words) from all the other permutations identified by the recogniser.
The syntactic analyser identifies the most syntactically acceptable word strings, whilst the semantic analyser identifies the most semantically plausible.
For example, consider the following typical input to the semantic analyser:
superbly restored victorian terraced house…
superbly restored victorian terraced rouse…
superbly restored victorian terraced rouge…
The object is to determine which of these phrases is the most semantically acceptable.
So in word positions where there are a number of alternative candidates (e.g. the last position) the semantic analyser must identify one as being the most plausible (e.g.’house ’,’rouse ’ or’rouge ’).
Lesk's technique has been adapted to meet these requirements: the definitions of these three candidates are compared with those of the rest of the sentence to find the one with the highest overlap.
The commonality between two definitions can be measured by counting the number of words they have in common.
There are two ways in which this can be measured:
(i) Strong overlap:
This occurs when word1 is found in the definition of word2 or vice-versa (or both), e.g.’ house’is found in the definition of’terraced’;
(ii) Weak overlap:
This occurs when a third word is common to both definitions, e.g.’ brick’is found in both’terraced’and’house’.
In the example above, if the other candidate words (’ rouse’and’rouge’) showed negligible overlap, then’house’would be selected as the semantically most plausible word in that position.
Most human observers of these three sentences would also confirm this as the most likely choice.
Understandably, the behaviour of various words and their overlaps will vary according to the dictionary used — the current project has used the Collins English Dictionary (CED), the Oxford Advanced Learner's Dictionary of Common English (OALD), and Longman's Dictionary of Contemporary English (LDOCE).
Definitional overlap uses dictionary definitions as a source of semantic knowledge, and follows a sequential comparison algorithm to select one from a number of alternative word candidates as being the most’ semantically plausible’within that sentential context.
It is perhaps misleading to state that definitional overlap selects the’ correct’word in any particular case, because ultimately the correct word is a product of the writer's original intentions, and is therefore subjective.
Concepts such as’ semantically correct’and’semantically incorrect’remain somewhat contentious, and in practice are inessential to the text recognition problem: the measure of success is not found in adherence to some formal semantic proof but simply the ability to choose the same word as a human observer would.
This, as will be seen, can be achieved through methods that have very little to do with the linguistic notion of semantics but much to do with the empirical processing of text-based knowledge sources.
3.2 Definitional Overlap
3.2.1 Data Structures
A dictionary, in either machine-readable or paper form, constitutes a very large textual resource.
If this resource is to be stored and processed efficiently, some thought needs to be given to the manner in which it is represented.
Clearly, to store it as a character-based file involves a considerable storage requirement and much complex processing to relate words in the dictionary to words in the data.
It is desirable therefore to devise an indexing system , by which words can be related to some other data structure that is more easily processed and stored.
One such form is the short integer .
By representing words in the definitions as integers, storage requirements decrease dramatically and efficient sorting routines become easily applicable.
This process, by which words are replaced by integers, is known as indexing .
Furthermore, it is necessary to relate inflected forms to root forms.
For example, the words’ made’,’making’and’makes’are all inflections of the root form’make’, so there seems little point in assigning them separate indices when their origins (and hence much of their semantic content) are shared.
This process, by which inflected forms are related to their root forms, is known as lemmatisation .
All the dictionaries used during the present project have been processed in the above manner.
To eliminate the possibility of incompatible representations, a’ standard’list of words and their indices was produced.
This list was designed to serve as an ultimate reference in the indexing process, and was compiled using the following algorithm:
1.
A definitive list of words was derived from a number of machine-readable dictionaries.
This list comprised as many root forms as possible plus their inflections.
2.
Each root form in this list is given a unique index, starting from’ 1’.
3.
All inflections inherit the index of their root form.
For example, the words’ made’,’making’and’makes’are all assigned the same index as the word’make’.
This was achieved using the information in Text710 version of the OALD [Mitton, 1986]and a degree of manual post-editing and checking [Keenan, 1992].
4.
All function words in this list are assigned an index of zero, to indicate their lack of semantic content and hence non-participation in the overlap process.
This indexed list is referred to as the lexicon .
It can be used to replace the words in the dictionaries by the appropriate indices.
Evidently, when data is processed by the current system, candidate words must also be replaced by their indices to unify the representations.
An example from this lexicon is as follows:
This lexicon should not be seen a static repository of data.
As new dictionaries become available, such valuable data should be exploited, and for this reason the lexicon has been continually updated as the project has progressed.
Consequently, some dictionaries have been indexed with lexicons of differing sizes.
For example, in early investigations, the largest list available consisted of some 5,240 root forms plus their inflections.
Later investigations had the benefit of a 18,800 lexicon, which provided a much greater coverage of the English language.
The effects of such differences are discussed later in this chapter.
Dictionary entries may thus be represented by an index corresponding to each headword, followed by a list of indices corresponding to each of the definition words.
These lists are sorted in numerical order and delimited by square brackets.
For example, the entries for’abandon ’,’abase ’ and’abash ’in the CED may be represented as follows:
This structure is represented at run time using an array of structures and an array of short integers, which is stored as a binary file.
3.2.2 The Overlap Algorithm
The definitional overlap technique is currently implemented as a C program running under UNIX.
The input to the program is the output from the lexical analyser, which consists of a number of candidate words for each word position in the text.
Each candidate has a score associated with it, which is initially set by the pattern recogniser and then updated by each of the analysers (this score is not shown in the example below for reasons of clarity).
Consider the sentence’this is a new savings account which you can open with one pound ’ written as input to a handwriting recogniser.
A typical output from the lexical analyser, showing the alternative candidates in each column, is as follows:
The definitional overlap technique compares the definitions of the content words and ascribes a score to each, proportional to the number of words in common with the definitions of its neighbours.
Once a complete sentence has been processed, the semantic scores are normalised according to an appropriate scale (see Chapter 5).
At this point, any function words are assigned a score equal to the maximum overlap score multiplied by a constant (they were not overlapped by the program and therefore have a score of zero so far).
Clearly, there are a number of choices to be made regarding this algorithm.
For example, what should the value of this function word constant be?
(At present, it is set at 0.66, which reflects the relatively high frequency of function words in normal text.)
What constitutes a neighbouring word: one that is adjacent, or any word in the same sentence?
Issues such as these are investigated and discussed in detail in later sections.
The definitional overlap technique has been successfully applied to the problem of sense disambiguation [Lesk, 1987].
Text recognition is, however, an entirely different problem.
To test whether definitional overlap could contribute to such an application, it is necessary to show that where genuine semantic relationships are present between word pairs, the technique is sensitive to them.
One way to achieve this is to identify semantically related pairs of words and compare the performance of the technique using these pairs to that of unrelated pairs.
A positive result would provide evidence of the technique's ability to identify genuine semantic relationships between words, independent of any particular application area.
3.2.3 Semantic Priming
Theories of human word recognition allow for both bottom-up and top-down influences on processing.
Bottom-up influences include those of the input stimulus and its environment, and top-down influences include expectations and hypotheses that come from higher-level cognitive functions.
It has been demonstrated that there are various types of context that can influence the speed and ease with which words are identified (and hence recognised), including lexical, syntactic and semantic contexts.
Regarding the latter context, it has been shown that when a word is preceded or accompanied by a’ semantically’related word, recognition of that word is facilitated with respect to unrelated controls.
For example, Meyer and Schvanevelt [1971]presented subjects with pairs of letter strings, to which they would answer’ yes’if both letter strings were words, otherwise responding’no’.
They found that recognition of semantically related words (such as’ knife’and’fork’, or’doctor’and’nurse’) was faster than that of unrelated words.
This evidence suggests that the semantic relation between two words can affect recognition performance.
This phenomenon precipitates an interesting question: is the effect repeatable by a computer?
In other words, can semantically related word pairs be differentiated from semantically unrelated word pairs by a computer?
If this proves to be the case, then evidently the process is sensitive to semantic relationships between words; specifically those that people are sensitive to.
Therefore, it may be adapted to identify semantically related words in text recognition data.
3.2.4 The Semantic Priming Effect
Objective
: To determine whether definitional overlap can distinguish between semantically related and unrelated word pairs.
Method
: Forty semantically related word pairs were selected, drawn from Postman and Keppel [1970], with a control for each pair [Evett & humphreys , 1981].
These pairs were chosen to be balanced for frequency, imageability, etc., in the same way as if selected for human subjects.
Example related pairs included:
The definitional overlap program was run on both sets of word pairs.
In each case, the program output constituted a set of scores, which corresponded to the number of strong overlaps and weak overlaps for each pair.
Definitions were taken from the machine-readable version of Collins English Dictionary.
Results
: The semantically related pairs were given higher scores than the control pairs in 34 out of 40 cases.
The breakdown of scores between the two sets is as in Table 3.1.
This data can be subjected to the student's t-test for statistical significance (see Appendix A), giving the value: z = 4.24.
This z-score can then be checked against statistical tables to determine the level of significance:(z [df 40]= 2.021, p < 0.05);(z [df 40]= 2.704, p < 0.01).
This shows a significant difference between related and unrelated pairs.
The result provides evidence that definitional overlap can identify semantically related word pairs.
3.3 The Choice of Dictionary
There are a number of commercially available dictionaries that can be obtained in machine-readable form.
The CED is one of them; others are shown in the table in Chapter One.
All these dictionaries are similar, inasmuch as they can all be used to create a list of words and definitions.
However, there the similarity ends.
MRDs are generally compiled by human lexicographers (with the aid of some computer-based tools), and hence are exposed to the subjective design guidelines and style of the particular publisher.
Although there is much current research effort directed toward standardising the design of dictionaries, there still remains a large degree of variability in the format and content of each.
Often this variation is due to differences in purpose or target readership.
For example, the OALD is a learner's dictionary, and it employs a style that seems almost informal or colloquial when compared to the comprehensive, encyclopaedic style of the OED.
Evidently, different dictionaries can provide quite different definitions for the same word.
Consider the following definitions taken from four dictionaries for the noun’ deposit’(omitting details such as grammar, phonology, etc.):
(1) The Shorter Oxford English Dictionary (SOED):
Deposit sb .
1624 [-L. depositum , subst. use of neut. of pa. pple. of deponere ; see DEPONE, DEPOSE.]1 .
Something laid up in a place, or committed to the charge of a person, for safe keeping.
Also fig .
1660. b .
spec .
A sum of money deposited in a bank 1753. c .
something committed to another person's charge as a pledge 1737. 2 .
The state of being deposited; in phr. on, upon, in d .
1624. 3 .
Something deposited, laid or thrown down;esp .
matter precipitated from a fluid medium, or collected in one place by a natural process.
In Mining an accumulation of ore,esp .
of a somewhat casual character, as in pockets.
1781. 4 .
The act of depositing; cf. prec. senses, and DEPOSIT v .
1773. 5 .
A depository, a depot.
(Chiefly U.S.)1719.
(2) The Collins English Dictionary (CED):
deposit …–n 6. a. an instance of entrusting money or valuables to a bank or similar institution.
b .
the money or valuables so entrusted.
7 .
money given in part payment or as security, as when goods are bought on hire-purchase.
See also down payment .
8 .
a consideration, esp.
money given temporarily as security against loss of or damage to something borrowed or hired.
9 .
an accumulation of sediments, mineral ores, coal, etc. 10 .
any deposited material, such as a sediment or a precipitate that has settled out of solution.
11 .
a coating produced on a surface, esp.
a layer of metal formed by electrolysis. 12 .
a depository or storehouse.
13 .
on deposit .
payable as the first instalment, as when buying on hire-purchase.
[C17: from Medieval Latin depositare , from Latin depositus put down]
(3) The Oxford Advanced Learner's Dictionary of Current English (OALD):
deposit n [C]1 money that is deposited(2,3):The shopkeeper promised to keep the goods for me if I left/paid/made a –.
money on –, money deposited in this way.
–account , money deposited in a bank, not to be withdrawn without notice, on which interest is payable.
current account at current(3).–safe , safe in the strongroom of a bank, rented for the custody of valuables. 2 layer of matter deposited(4):A thick–of mud covered the fields after the floods went down .
3 layer of solid matter left behind (often buried in the earth) after having been naturally accumulated:Valuable new –s of tin have been found in Bolivia .
(4) Longman's Dictionary of Contemporary English (LDOCE):
deposit n. 1 something deposited:There are rich deposits of gold in those hills.
| There's some deposit at the bottom of this bottle of wine 2 usu. sing.
a part payment of money, which is made so that the seller will not sell the goods to anyone else:You must pay a deposit to the hotel if you want them to keep a room free for you compare earnest(1) 3 an act or action of depositing:The rate of the river's deposit of mud is about one inch a year deposit account n. a bank account which earns interest and usu. from which money can be taken out only if advance notice is given.
compare savings account, current account 
It can be seen that the SOED contains historical information, is formal in its style and content, and tends to use short punctuated phrases to provide coverage of each sense definition.
By contrast, the OALD is more straightforward, uses whole sentences wherever possible, and makes extensive use of example sentences.
The CED is somewhere in between these two styles.
LDOCE is of particular interest, since it is claimed that its entries are defined using a controlled vocabulary of around 2,000 words, and that the entries have a simple and regular syntax [Boguraev & briscoe , 1989].
The effect that this has on LDOCE's efficacy for text recognition is discussed in later sections.
Lesk [1988]has used the OED as a source of information for his work on automatic sense disambiguation.
He compared the OED with the OALD, CED and W7 and concluded that the OED was the most suitable due to its sheer size (the greater the number of headwords, the more chance that a particular word will be included) and because’a quick count of several dictionaries indicated that the OED surpassed all others in the number of useful content words in its definitions and quotations ’.
It is not clear, however, what criteria were used in the measure of’ usefulness’, or just how’quick’the count was.
It may be true that some dictionaries are more suitable for language processing than others, but such distinctions should not be based on size alone.
The OED may contain a greater overall quantity of’ useful’information than other dictionaries, but just how concentrated is this information?
A brief glance at their relative sizes shows that the OED is several times larger than any of the other dictionaries, resulting in a considerable computational overhead in terms of memory requirements and search times.
Furthermore, comparing the definitions shown above suggests that the concentration of useful information may be proportionately less in the SOED than in other dictionaries.
In the long term, a dictionary needs to be above a certain size to provide sufficient coverage over a wide range of domains.
However, it may transpire that the most important factors are information concentration rather than sheer size, and the degree to which a dictionary is up-to-date and covers contemporary material (this after all is the type of material a text recognition system would typically have to handle).
To resolve this issue, a version of the OALD was obtained and indexed in the same way as the CED.
By running the definitional overlap program on some sample text using definitions extracted from both dictionaries, an objective comparison could be made.
3.3.1 The OALD
Objective
: To determine the effectiveness of the definitional overlap technique using definitions extracted from the OALD.
Method
: Due to constraints on the availability of data pads it was not always possible to obtain test data from the original source.
As large samples of data were required, further programs were written to facilitate this process, by simulating recognised output for any given input sentence.
This confusion simulator program gives an output similar to the real pattern recogniser, working on the same database of characters, probabilities and dependencies [Keenan, 1990].
The OALD was indexed using the same technique as that applied to the CED, and used as the source of definitions by the overlap program.
A typical business letter of some 153 words was selected from a corpus of business documents.
This was put through the confusion simulator and then used as input to the overlap program.
It was then necessary to determine the’ neighbourhood size’around each word, i.e. the limit beyond which words are deemed to be too far apart to be considered as neighbours.
This quantity is also referred to as the window size , since semantic relations are said to take place within a limited window of so many words in the text.
It was known from collocation studies (see Chapter 4) that the information from co-occurrence relations is optimised at a distance of four words, so the window size was provisionally set at this distance.
Results
: The breakdown of scores is shown in Table 3.2:
Discussion
: Firstly, can be seen that the process of definitional overlap selects the correct word in 70% of cases.
At first sight, this may appear to be a poor performance but it is in fact over twice as frequent as a random selection (which for this data would be 32.7% of cases).
Percentage measures of performance should therefore be compared to that expected from random selection.
Secondly, this performance measure is somewhat crude; it measures whether the correct word was chosen or not, irrespective of the score or margin by which it succeeded over the other candidates.
Such quantities cannot be ignored if measures of the program's performance are to be valid.
One method of quantitative assessment would be to use a comparison between the score given to the correct word and the score given to the highest other content word in the same sentence position.
These scores may then be recorded as’ correct-score:other-score’pairs, and subjected to the relevant statistical test for significant mean difference.
The methodology used in the treatment of results of subsequent investigations therefore includes the following procedure:
(1)
Iterate through the sentence positions in the output;
(2)
If the correct word is a function word, proceed to next sentence position;
(3)
If the correct word is a content word, add its score to the’ correct-word scorelist’.
Find the highest score of all the other content words in that position, and add that score to the’ highest-other-word scorelist’.
(4)
When all candidate content words have been assigned a score, use the Student's t-test (see Appendix A) to determine whether the difference between the means of the correct-word scores and the highest-other-word scores is statistically significant.
When this test is applied, the following result is obtained: z = 2.15.
This z-score can then be checked against statistical tables to determine the level of significance:(z [df 19]= 2.093, p < 0.05), and (z [df 19]= 2.861, p < 0.01).
This shows a significant difference (to 95%) between correct-word scores and highest-other-word scores.
The student's t-test is a recognised test for statistical significance of the difference between two means and allows for small sample sizes.
Use of the OALD therefore surpasses the 95% confidence limit.
Conclusions
: This investigation has demonstrated two important findings concerning the definitional overlap technique:
(1)
An appropriate performance metric is required and has since been designed and applied to the original results, to reveal more objective patterns in the data;
(2)
The OALD makes a contribution significant to the 95% confidence level in reducing the ambiguity in the output of a handwriting recognition system.
3.3.2 The CED
Objective
: To determine the effectiveness of the definitional overlap technique using definitions extracted from the Collins English Dictionary.
Method
: The CED was coded using the 5,240-word lexicon.
The document used above was presented as input to the semantic analyser, and the CED used in place of the OALD as the source of definitions.
Results
: The breakdown of scores is shown in Table 3.3.
The z-score can then be checked against statistical tables to determine the level of significance:(z [df 20]= 2.086, p < 0.05), and (z [df 20]= 2.845, p < 0.01), i.e. this difference is not significant.
Discussion
: There are two effects to be explained.
The first is that the CED worked with the semantic pairs, in the sense that it gave a statistically significant result, but it does not in this investigation.
There are two reasons for this:(a) the correct words do not have a proven semantic relationship (unlike the semantic pairs), and (b) the control words are not artificially selected but instead are random orthographic derivations of the correct words and therefore not necessarily unrelated.
The second effect is the inferior performance of the CED when compared to the OALD.
Possible reasons for this difference are revealed through closer examination of the definitions within these two dictionaries.
In both dictionaries, the purpose of the definition is to provide a precise, sense-based statement of the meaning of each word, preferably in terms of their hyponyms, synonyms, etc.
However, the OALD is more encyclopaedic in its exposition, and more liberal in its use of examples.
Consider the word’ payment’:
In the CED:
payment n. 1.
the act of paying.
2. a sum of money paid.
3. something given in return; punishment or reward.
In the OALD:
payment n. 1.
paying or being paid:demand prompt–; a cheque in–for services rendered .
2. sum of money (to be) paid:$50 cash down and ten monthly –s of $5 .
3. reward; punishment.
The OALD evidently provides more encyclopaedic knowledge in the form of examples, and provides simpler, more concrete definitions, using everyday language.
The design of the definitions within the OALD is therefore more likely to reflect the patterns of word usage found in everyday text than those in the CED.
The use of examples in each definition encodes information concerning the actual usage of words in running text, which is more representative of the type of text that the semantic analyser will usually be required to handle.
It thus appears that the OALD definitions contain more information of direct use to a text recognition system, and the results of this investigation reflect this difference.
However, the OALD differs in another important way: the indexing.
In the discussion of data structures earlier in the chapter, it was stated that a lexicon, derived from a number of machine-readable dictionaries, was used as the ultimate reference in the indexing process.
However, this lexicon has been updated as the project has progressed, and for this reason the OALD has been indexed using a different list.
The first list comprised some 15,350 words (inflections of some 5,240 root forms), and this was used to index the CED.
When the OALD became available, a more comprehensive lexicon had been compiled, comprising some 56,940 types (from 18,800 root forms), and this was used to index the OALD.
So the two dictionaries differed on another dimension.
To what extent did this difference in indexing affect the performance of each dictionary?
To answer this question, a further investigation was carried out.
3.3.3 The Re-indexed CED
Objective
: To assess the use in semantic analysis of definitions extracted from the CED and re-indexed using the 18,800-lexicon, to reduce the ambiguity of output from a text recognition system.
Method
: The CED was re-indexed using a lexicon of some 18,800 lemmas.
The data used above was presented as input to the semantic analyser, with the newly indexed CED definitions as the source of definitions.
Results
: The breakdown of scores is shown in Table 3.4.
This z-score can then be checked against statistical tables to determine the level of significance:(z [df 19]= 2.093, p < 0.05),(z [df 19]= 2.861, p < 0.01), i.e. this difference is not significant.
Discussion
: In both investigations involving definitions taken from the CED the results were well short of the 95% confidence level.
However, the z-score for the newly indexed definitions (using the longer lexicon) is almost double that of the earlier version.
This suggests quite strongly that the indexing process is important.
There are two factors associated with this difference:
(1) Lexical Coverage:
Consider the sizes of the two lexicons used in the indexing process:
Lexicon1 = 15,350 words, from 5,240 roots
Lexicon2 = 56,940 words, from 18,800 roots
When a dictionary is indexed, any word that has no entry in the lexicon is ignored (and subsequently discarded).
The second lexicon, being much larger, will obviously cover a much larger subset of English, and hence cover many more of the contents of a definition.
The larger the lexicon used in the indexing process, the more information in the definitions is retained, and the greater the contribution of these definitions to the recognition process.
The process of indexing should therefore employ as large a lexicon as possible.
(2) Grain-Size
: The first list relates 15,350 types to 5,240 roots.
This is a ratio of 2.93:1.
The second list relates 56,940 types to 18,800 roots.
This is a ratio of 3.03:1.
In this example, the ratios are comparable.
However, it is possible to produce lexicons of comparable size that relate words to a much smaller number of roots.
Depending on the purpose for which it is intended, the lexicon can be compiled in many ways.
For example, syntactic applications may require some discrimination between different grammatical categories, e.g. between the verb forms such as ’ work’,’works’and’working’and the noun forms such as ’worker’or’workers’.
This would suggest the assignment of one index to derivatives of the verb form and another to those of the noun form.
However, the rules by which such indices are assigned to words are not totally reliable.
This is because they are based on the spelling of each word, which is inconsistent due to the non-deterministic morphology of the English language.
For example, in assigning an index to a word like’making ’ or’baking ’, it may be possible to work backwards to arrive at the root (’make ’or’bake ’) by removing the’ing ’ending and adding the letter’e ’.
But what of words like’king ’ or’fling ’?
The same rules for finding the root are no longer relevant.
Similarly, what is the root of’leaves ’?
Is it’leaf ’,’leave ’ or both?
There are many other such examples throughout English, and all are reflections of morphological inconsistency.
The compilation of a lexicon therefore requires considerable manual intervention to ensure that indices have been assigned in a reliable manner that fits the application.
The lexicon used by the present project reflects both the needs of the various analysers and the subjectivity of manual intervention to resolve the morphological inconsistencies.
For example, consider the words’pay ’ and’payment ’.
Neither is an inflection of the other, so strictly speaking their differing linguistic origin should dictate separate indices.
However, by manual intervention they may be deemed sufficiently semantically related to justify a common index.
Now consider the sentence fragment:…payment on your account…
In the OALD, the word’account ’ occurs in the definition of’pay ’, but not in the definition of’payment ’.
So if’pay ’ and’payment ’are indexed as having separate roots, the above fragment would show no overlap.
However, if’payment ’ were assigned the same index as’pay ’, then a strong overlap would result.
The need to represent such relationships may be accommodated using a smaller list of roots (and hence a coarser grain-size ), and more meaningful overlaps may be the result.
It is suggested that this issue of root-assignment (or lemmatisation) and grain-size form the basis of further investigation.
One final point concerns the calculation of the z-score.
Although both the OALD and the re-indexed CED produce 70% correct choices, the z-score for the OALD is 2.15 whereas for the CED it is 0.98.
This is due to the number of incorrect choices, which being higher in the case of the CED lowers the mean difference and hence the z-score (see Appendix A).
Moreover, to be able to perform a reliable statistical analysis the sample size must be considerably larger.
The original text may have consisted of 153 words, but many of these are function words and a further number produces no alternative candidates when put through the simulator.
Only positions of’ semantic interest’are subjected to statistical analysis, i.e. those positions where the correct word is a content word accompanied by a number of alternatives.
This sample text yielded 30 or 40 such positions, henceforth referred to as data points.
With the standard deviations being relatively high (in the order of 8.0 to 12.0), clearly a much larger data sample is needed (to provide at least 100 data points, i.e. 25-30 sentences).
3.4 Definitional Overlap and Domains
It is expected that a fully functional text recognition system will be required to work with a range of material, taken from a range of domains (e.g. banking, insurance, estate agents, medical, technical,etc.).
Although some degree of’ tailoring’of individual systems may be possible (or even desirable in some cases), the basic techniques on which the system relies must work consistently across a range of domains.
So far, all the investigations of definitional overlap using the MRDs have been based on one sample of data, which was selected at random from a corpus of business letters.
It may be typical of many business letters, but it is still only one sample of data, and as such represents only one domain — in this case that of banking .
The significant results achieved using this text may have been specific to this domain and hence not necessarily repeatable in other domains (i.e. the OALD may provide unusually good coverage of commercial or financial terminology).
It is necessary, therefore, to assess the reliability of the process across a range of domains, which requires the selection of a number of domains and the acquisition of appropriate data samples.
The overlap program can then be run on these samples to establish the relationship between performance and domain.
3.4.1 Domain Specificity
Objective
: To investigate the extent to which the definitional overlap technique contributes to the text recognition process across a range of domains, using the OALD coded with the larger lexicon.
Method
: Three domains were chosen for investigation:Banking, Estate Agents and Music .
This choice reflected both the potential application of the eventual system (i.e., the Banking and Estate Agents ) and ease of availability (the Music documents could be easily collected from a research bulletin board).
Test data was gathered by random selection from each domain until the target of approximately 17 sentences per domain had been met.
These texts were then processed by the confusion simulator program, and the output used as input to the definitional overlap program.
Results
: Before considering these results, it must be appreciated that the t-test is very sensitive to changes in sample size (see Appendix A).
For this reason, these results show a much larger z-score for a relatively small percentage correct.
Note also that the test is also sensitive to the standard deviation — a wide’ spread’of scores (as demonstrated by the results for music) will produce a lower z-score than a domain with the same percentage correct but a narrow spread of scores (e.g. estate agents ).
(a)Banking : The breakdown of scores is shown in Table 3.5.
This z-score can then be compared to the value of z required for 95% and 99% significance level (obtained from statistical tables):(z [df 90]= 1.99, p < 0.05),(z [df 90]= 2.63, p < 0.01).
7.13 > 2.63 therefore reject null hypothesis at 99% significance level; i.e. it can be said with 99% confidence that the technique selects the correct word in favour of alternative candidates within the domain of banking .
(b)Estate Agents : The breakdown of scores is shown in Table 3.6.
This z-score can then be compared to the value of z required for 95% and 99% significance level (obtained from statistical tables):(z [df 133]= 1.98, p < 0.05),(z [df 133]= 2.61, p < 0.01).
3.69 > 2.61 therefore reject null hypothesis at 99% significance level; i.e. it can be said with 99% confidence that the technique selects the correct word from alternative candidates within the domain of estate agents .
(c)Music : The breakdown of scores is shown in Table 3.7.
This z-score can then be compared to the value of z required for 90%, 95% and 99% significance level (obtained from statistical tables):(z [df 87]= 1.66, p < 0.1),(z [df 87]= 1.99, p < 0.05),(z [df 87]= 2.63, p < 0.01).
1.44 < 1.66 therefore accept null hypothesis; i.e. no evidence to suggest that the technique makes a significant contribution to the recognition process within the domain of music .
Discussion
: It is necessary to appreciate a number of factors concerning the t-test.
Firstly, the use of a statistical test is only as a guideline to highlight trends in the data — not to define immutable standards of performance.
Moreover, the t-test is quite strict, as are the levels of significance selected for this investigation (and others).
Furthermore, the t-test takes into account background variation, unlike the percentage measures — which partially explains the less than total symmetry between the two measures of performance.
(Other explanations concern the variation in the’ strength of the competition’, as indicated by the’%correct expected from random’figure.)
Regarding the results themselves, it can be seen that the performance of definitional overlap across a range of domains is consistently positive but highly variable.
It ranges from the highly significant (well above the 99% level in the case of banking ) to the statistically insignificant (in the case of music ).
These results are based on considerably expanded sample sizes so may be accepted as being more representative of the expected performance within each domain.
Possible reasons for this spread of results include issues related to both the origin of the dictionary and the origin of the test data:
(i) The origin of the dictionary:
This explanation is concerned with the’ accessibility’of the domain, i.e. the degree to which its constituent terminology is commonly understood.
The type of terminology used in a business letter taken from the domain of banking is relatively well used in everyday life and consequently well understood.
Most people to some extent have to manage and understand their own financial affairs and its concomitant terminology, and media coverage of financial affairs encourages this basic knowledge of terminology.
So although lexicographers may purposely employ experts to contribute to the compilation of definitions in more esoteric domains, it is nevertheless the case that more everyday words will have more widely understood patterns of usage that are reflected in both the definitions they possess and the manner in which they are used in a typical business letter.
In other words, the more’ everyday’a domain is, the more’everyday’its constituent words will be, and the more they fit into stereotypical patterns of usage that are quoted as examples in the dictionary definitions.
More abstract or esoteric domains such as music are reliant upon expert knowledge to provide precise definitions and typical examples of usage, and hence are more subjectively compiled.
The words themselves, being less’ everyday’, are less likely to have widely accepted definitions or stereotypical patterns of usage that may then be quoted as examples in their dictionary definition.
In this sense, the dictionary and its definitions are less suited to the recognition of domains that are more abstract or esoteric.
It is possible therefore that specialist dictionaries may be necessary to maintain performance in these domains.
(ii) The origin of the test data:
Another explanation for this pattern of results is based on the purpose of the test documents.
The banking document and estate agent's document were both designed to be readable by lay people, and hence used language structures that could be immediately understood.
This was achieved firstly by avoiding unusual (e.g. low-frequency) words, and secondly by adhering to commonly used contexts or patterns of usage (e.g.’…saving for a rainy day…’ and’…free to open, easy to run…’).
The music document, on the other hand, was not restricted by such limitations.
It was designed to be a communication between experts, discussing technical issues within their own subject field and hence was not intended for’ public consumption’.
The patterns of usage therefore reflected the writers' own individual styles, and their choice of words similarly reflects their own subjective knowledge.
In this way, the dictionary, being a general-purpose source of knowledge designed for use by non-experts, is being used to contribute to the recognition of text that is both highly specific and intended for a specific audience only.
It is possible that this disparity of purpose contributes to the discrepancy in performance indicated above.
The two explanations above show a high degree of commonality in their reasoning.
This is because there is a high degree of commonality in the processes being described: usage of a word contributes to the way in which it is defined in a dictionary, and a dictionary definition contributes to the way in which a word is actually used.
Conclusion
: There is a great degree of variability in the performance of the overlap technique across different domains.
This is due to factors relating to both the design of the dictionary from which the definitions are taken, and the purpose of the text being recognised.
It is suggested that the use of general dictionaries may be insufficient to recognise text taken from more esoteric domains or documents that are intended for a specific audience.
Text such as this may require the acquisition of specialist dictionaries to maintain the high performance shown in other domains.
3.5 Definitions and Semantic Networks
3.5.1 Introduction
Most human observers would describe the words’mortgage ’ and’money ’as being semantically related.
However, to find such a connection in a dictionary it may be necessary to go beyond the’ first level’of definition.
Consider the definition of’mortgage ’ in the OALD:
mortgage –(to)(for), give sb a claim on (property) as a security for payment of a debt or loan:–a house (to sb for $40000); land that may be –d. n act of mortgaging; agreement about this:raise a–(on one's house) from a bank.
I can buy the house only if a–of $40000 is obtainable.
We must pay off the–this year …
The word’money ’ is not present, although the use of the currency sign does suggest a monetary value (the investigation of the semantic role of such special characters will form part of future studies).
However, the definitions of’payment ’,’debt ’ and’loan ’all contain the word’money ’, so it can still be reached if the search goes far enough.
The’ semantic tree’thus created has already been exploited by researchers to provide information relevant to other natural language systems [Jensen and Binot, 1988].
It may thus transpire that definitional overlap could operate more effectively by considering both the definition of a word and the expansion of that definition.
However, the average length of a definition (after reduction, indexing and sorting) is 21.81 words in the OALD and 14.55 in the CED.
So to expand a typical definition from the OALD to just the second level would involve the processing of (21.81)2 words, i.e. approximately 475.
This produces a considerable combinatorial explosion, making processing beyond the first level somewhat impractical.
What is required therefore is a method of compressing these definitions so that this combinatorial explosion is reduced.
One reason for the length of the definitions is that English is a highly polysemous language, and within a typical entry there will be separate definitions for every sense of the word.
However, when language is used within a specific domain, it is often the case that only a subset of those senses is appropriate.
For example, in financial documents, any references to the word’bank ’ are unlikely to involve the sense related to rivers and canals.
It follows therefore that if the domain is known, it may be advantageous to eliminate the alternative word senses from a definition so that processing focuses only on those that are relevant to the domain.
This reduces the combinatorial explosion and decreases the potential for spurious overlaps through the co-incidence of alternative word senses.
For example, if definitions within a domain can be halved, i.e. reduced to an average of 11 words in length, the processing at the second level involves some 11 2 words, i.e. 121 or 25% of the previous total.
3.5.2 The Filtering Method
This compression of definitions is achieved by a process involving the’filtering ’ of definitions through a filter set chosen to represent the core vocabulary of a domain (in this case, that of banking ).
Consequently, the overlaps can be seen as being’ pre-processed’into the definitions of each word.
This process consists of the following steps:
(1)
Create the wordlist for which filtered definitions are required.
This may be done using a frequency distribution from a domain corpus.
(2)
Derive the filter set.
This may be based on the core vocabulary, or a subset thereof.
The core vocabulary is usually derived by taking a frequency distribution of a domain corpus, comparing this with a frequency distribution from a general corpus, and sorting according to distinctiveness.
In the example below, the filter set consisted of the first 30% of the core vocabulary.
(3)
Iterate through the wordlist, repeating the following steps for each word:
(i)
overlap the word with each member of the filter set;
(ii)
when a strong overlap occurs, place the index responsible in a’ strong overlap’list;
(iii)
when a weak overlap occurs, place the index responsible in a’ weak overlap’list;
(iv)
when the word has been overlapped with all members of the filter set, take those weak overlap indices with a frequency greater than a certain threshold (this was set to 1 for the example below) and append them to the strong overlap list.
(4)
The list so obtained forms the new definition for that word, containing all strong overlaps (with the filter set), and the more frequent weak overlaps.
In this way, words in a definition that represent senses inappropriate to a domain will be unlikely to overlap with the filter set, and so be excluded from the new definition.
The following example shows the effect of filtering the definition of the word’charge ’(taken from the CED), using the domain of banking :
Example words removed from the old definition:
Military sense:
battle, command, control, horse
Legal sense:
accusative, assault, commit, evidence, fault, injunction, judge, legal
Electrical sense:
electricity, electron, explosive, formic, negative, phenomena, solution
Example words remaining:
account, debit, demand, finance, liable, price, service, set
The original definition contained 129 words, related not only to the financial sense (i.e. charge as in’require payment ’) but also to the’electrical charge ’ sense and the’military charge ’sense, etc.
When this definition is filtered through the banking domain filter set it is reduced to 84 words in length.
Evidently, the definition has been reduced to 65% of its original length.
The words relating to the non-financial senses have been largely eliminated, without losing words relevant to this domain.
Sense-based definitions of words could contribute to the subsequent overlap process by providing more concise, pertinent definitions and reducing the chances of spurious overlaps due to inappropriate but co-incident word senses.
3.5.3 Discussion
Although initial studies using this technique have shown promise, further tests are required to determine the optimum settings for the various parameters.
It may transpire that tightening (raising) the weak overlap threshold produces more concise definitions in some domains, whereas in others valuable information in the original definitions starts to be lost.
Moreover, the choice of filter set is not at all fixed (e.g. what proportion of the core vocabulary is an adequate representation of the domain for these purposes?)
There are other ways in which definitions may be filtered.
Another possibility, for example, uses the same concept of a filter set, but instead of taking the definition as a whole, takes the separate senses and progressively eliminates those that show the least overlap with the filter set.
However, in its simplest form, this technique results in words having one single sense definition remaining.
This is inappropriate, since in most domains, more than one sense of a word is relevant.
To conclude, the filtering process provides an automatic way of eliminating irrelevant material to produce sense-based definitions.
This can help provide more domain-specific definitions, and reduce the combinatorial explosion produced by expanding definitions beyond the first level.
Practical uses of this technique, and the interrelation with other knowledge sources within the semantic analyser are discussed below.
Whether they are filtered or not, expanding dictionary definitions may provide a way of accessing semantic relations that are not accessible at the first level.
To resolve this issue, a further investigation was carried out.
3.5.4 Definition Expansion
Objective
: To assess the extent to which the expansion of dictionary definitions provides information of use in semantic analysis.
Introduction
: It has been suggested that dictionary definitions may be’ expanded’to create of’semantic trees’, and that these trees can be traversed and searched to provide information for NLP systems [Chodorow, Byrd & Heidorn, 1985].
Evidently, the information so obtained may provide a further constraint of use in semantic analysis.
However, is necessary to firstly define what is meant by’ useful’information, and then to measure the quantity of useful information obtained from this process.
’ Useful’semantic information is that part of a word's definition (or its expansion) that is semantically related to the sentential context of that word in typical usage.
Useful semantic information therefore facilitates the incidence of meaningful strong overlaps in normal text.
Since documents usually have an overall structure, objective and topic, they can be said to represent a particular domain (e.g. banking ,insurance,etc.).
Therefore, in measuring the amount of useful information contained in a definition, we cannot simply measure the total number of words.
Instead, we need to measure the number of words that have senses related to the same domain, since these are more likely to co-occur in typical usage.
This can be accomplished by collecting a corpus of texts for a chosen domain and producing a frequency distribution for the words therein.
The definitions and expansions of various words can then be compared with this list to check for membership.
The greater the degree of common membership, the greater the coverage of that definition or expansion, and hence the greater the amount of useful information.
This amount may be expressed as a ratio of the amount of useful information compared to the amount of redundant information.
For example, if we take the domain of banking and consider the definition of the word’ mortgage’as derived from the CED:
mortgage n. 1.
a conditional conveyance of property, esp.
real property, as security for the repayment of a loan.
2. the deed effecting such a transaction.
3. the loan itself –vb (tr.) 4. to convey (property) by mortgage.
5.
Informal.
to pledge.
mortgage rate n. the level of interest charged by building societies and banks on house-purchase loans.
We find that a proportion of these definition words will be relevant to the domain of banking (as defined by membership of the domain wordlist), whilst the remainder will not (or have negligible relevance).
In the definition above, if the words’charged ’,’property ’,’interest ’,’repayment ’ and’purchase ’were all members of the domain wordlist, we could say that expansion of this word provides a total of 25 content words, of which 5 are directly relevant and hence useful.
Expressed as a ratio, the coverage of this definition is , i.e. 20%.
A similar procedure may be applied to the expansion of each of the definition words, to determine the coverage of the 2nd level expansion, and the relative contributions of the two levels can then be compared.
The use of frequency distributions facilitates the investigation of a second issue: the relationship between relative frequency (distinctiveness) and coverage.
In other words, does the expansion of highly distinctive words result in a greater proportion of useful information?
The objective of this investigation can now be re-stated as attempting to determine the following quantities:
(i)
the coverage given by expansion to the 1st level (definition);
(ii)
the coverage given by definition expansion to the second level;
(iii)
the relationship between relative frequency and coverage.
Furthermore, the investigation of these three issues allows a fourth quantity to be determined: the relationship between syntactic category and coverage.
In other words, do nouns have more useful definitions than verbs, adjectives or adverbs?
Method
: A number of documents related to the domain of banking were collected, and a frequency distribution produced.
Every tenth word from this list was selected for investigation.
This process entailed two stages:
(i)
Expansion of the word to its 1st level (definition), and subsequent calculation of the coverage;
(ii)
Expansion of the definition to the next (second) level, and subsequent calculation of the coverage.
Coverage for each level was calculated as above.
Spearman's Rank-Order Correlation Coefficient was calculated to determine the correlation between relative frequency and coverage.
Results
: Table 3.8 shows the results for the first ten words selected from the relative frequency distribution list.
’ Rank’refers to their position on that list.
’ 1st Level’refers to the coverage given by expansion to the first level, expressed as a percentage (as described above).
’ 2nd Level’refers to the coverage given by expansion to the second level, also expressed as a percentage.
Calculation of Rank-Order Correlation Coefficient:
Correlation needed for 95% confidence level = 0.3809, i.e. this correlation is insignificant.
The above table displays the words in rank order.
These words can be re-grouped according to the syntactic categories of noun, verb, adjective and adverb.
The average coverage of the first-level definitions in each group may then be calculated.
The relationship between syntactic category and coverage may then be expressed by Table 3.9.
Discussion
: It can be seen from the first of the above tables that the mean coverage given by the second level (8%) is just over half that of the first level (14%).
From this we can infer that the proportion of useful information obtained by expansion to the second level is less than that available at the first level.
At present, the text recognition system employs only first level information.
To combine second level information with first level information would result in an overall decrease in the ratio of useful information to redundant information.
In practical terms, this would imply an overall increase in the number of spurious overlaps and a proportional decrease in the number of meaningful overlaps.
Suggested explanations for this pattern of results focus predominantly on the highly polysemous nature of the English language.
This is reflected in a multiplicity of senses for any given word in the dictionary.
Whereas at the first level there may be a number of words representative of the typical sentential context (say 10%), beyond this point each of the definition words is in itself polysemous, involving further unrelated sense definitions.
The percentage of related words at this level is therefore 10% of 10%, i.e. 1%.
Hence the expansion of dictionary definitions descends into progressive generality, displaying a weaker and weaker semantic relationship with the original word.
It may be possible to reduce or even eliminate this effect by’ filtering’the definitions in the manner described earlier, but the pattern is extremely persistent (almost every word investigated exhibited a descent into generality) and it is unlikely that the filtering process could substantially alter this trend.
This issue is suggested as an area for further research.
It can also be seen that there is a negligible relationship between relative frequency and coverage.
There is thus no evidence to suggest that definition expansion may provide useful information when applied selectively to highly distinctive words.
There is a strong relationship between syntactic category and coverage.
On average, nouns are two and-a half times more useful than adverbs (coverage is 20% and 8% respectively).
Verbs and adjectives are in between these two extremes (13% and 10% respectively).
Possible explanations for this pattern include the fact that nouns generally refer to concrete objects that can be easily described and related to other objects.
There are more of them, so they can refer to more specific concepts within a definition.
Verbs, on the other hand, and especially adverbs, are more likely to be abstract concepts that are harder to define and more unpredictable in their use.
There are less of them, so they have to be more general and’ domain-free’in their use.
In this respect, a noun that is strongly related to a particular domain will have much useful information in its definition (e.g.’ mortgage’,’account’and’payment’are all strongly related to finance).
A verb or adverb, however, is unlikely to be as strongly constrained to one domain and will therefore be more general in its use (e.g.’ withdraw’,’open’and’save’all have a variety of meanings besides the financial sense).
However, the descent into generality applies to all words, and is not confined to any one syntactic category.
It should be noted that this investigation was based on definitions derived from the CED.
Other studies have shown the Oxford Advanced Learner's Dictionary to be more suited to the needs of a text recognition system.
However, there is no evidence at this stage to suggest that the expansion of OALD definitions would descend into generality at a significantly different rate.
Conclusions
: At present the semantic analyser uses only first level information.
The inclusion of second level information would not increase the proportion of useful information to redundant information.
There is no evidence to suggest that expansion to the third level would reverse this process of increasing generality.
Filtering the definitions may go some way towards reducing this effect, and is suggested as an area for further research.
There is no evidence at this stage to suggest that the expansion of definitions derived from a different dictionary would descend into generality at a significantly different rate.
The correlation between relative frequency and coverage is negligible, therefore there is no evidence to suggest that descent into generality can be avoided through the selective expansion of distinctive words.
The strong relationship between syntactic category and coverage exists only at the first level of information.
Descent into generality cannot be avoided by using syntactic information.
3.6 The Overlap Algorithm
3.6.1 Introduction
There are a number of variables associated with the overlap process.
It is necessary to identify the effect of these variables on the performance of the semantic analyser, and to determine the values by which optimum performance is obtained.
Variable 1 — The complexity of the algorithm:
There are two ways in which the overlap program can iterate through the word positions in the input data:
(1)
The simple approach
1.
Identify the sentence position (at the start this is word position 1, candidate 1).
This word is known as the’ active word’.
2.
Scan forward, within the window size (4 positions), overlapping the active word with each of the candidates in each position.
3.
Calculate the score for each overlap and attach this score to the active word and the candidate word in each case.
4.
Move on to the next candidate, make this the active word, and repeat steps 1-3 for all candidates in the data.
(2)
The complex approach
This algorithm assumes that for each sentence position only one of the candidates is correct (this condition generally holds, except in cases where the correct word is missing from the list of candidates).
1.
Identify the sentence position (at the start this is word position 1, candidate 1).
This word is known as the’ active word’.
2.
Scan forward and backward within the window size (4 positions), overlapping the active word with each of the candidates in each position.
3.
For each word position, calculate the best overlap between each of the candidates and the active word, and attach this score to the active word (only).
4.
Move on to the next candidate, make this the active word, and repeat steps 1-3 for all candidates in the data.
The second algorithm is necessarily more complex than the first.
This is because it assumes that only one of the candidates in each position is correct, so the assignment of scores to words has to be delayed until the maximum for any given position is known.
For this reason, the algorithm needs to scan backward as well as forward, and therefore takes twice as long as the simple algorithm.
The question is, does this added complexity and the assumption on which it is based add anything to the overall performance?
Variable 2 — Window Size:
It is known from studies of collocations that the information derived from co-occurrence information is optimised at a distance of four words.
However, when other parameters are varied it may transpire that the optimum window size varies as well.
For this reason, in each of the trials discussed below the window size is varied from 1 to 10 words.
Variable 3 — Strong Overlap versus Weak Overlap:
Throughout the earlier investigations it became apparent that the contribution of weak overlap was considerably smaller than that of strong overlap.
Furthermore, the computational overhead associated with weak overlap is much greater.
Considering these factors, is its inclusion in the overlap algorithm justified?
Would strong overlap on its own suffice?
Variable 4 — Definition Length Compensation:
The larger the definition, the greater the chance of a successful overlap occurring by chance.
This factor can be compensated for, by dividing the semantic score between two words by the joint length of their definitions.
This should reduce the biasing effect of large definitions, but to what extent does it improve performance?
3.6.2 Investigations with the Overlap Algorithm
Objective
: To determine the optimum settings for a range of parameters associated with the overlap algorithm.
Method
: It was decided to investigate each parameter in succession, i.e. to investigate one, find the optimum, set this as the default, and then turn to the next parameter.
The exception to this is the window size, which was varied from 1 to 10 words in each trial, for the reasons outlined above.
The test data was as before (i.e. documents taken from the domains of banking ,estate agents and music , after having been processed by the confusion simulator).
Results and Discussion
: Figure 3.2 shows the performance of each algorithm, measured as a ratio between correct and incorrect choices (it is insufficient to consider solely correct choices).
The complex algorithm is consistently superior, particularly when the window size is greater than 6 words.
Evidently, there can only be one correct word in a given sentence position, and this result may reflect the effective exploitation of this constraint.
However, the window size is currently set at four words, and at this distance the difference between the two algorithms is very slight.
(It is unlikely that the window size will be changed in the foreseeable future since it currently represents the distance at which collocational information is optimised [Lancashire, 1987]).
It may thus transpire that a marginal improvement is insufficient to justify the increase in complexity associated with this algorithm.
To resolve this, the execution time required by each algorithm to process all three documents was measured, and the result is shown in Figure 3.3.
Not surprisingly, execution time increases with larger window sizes in a fairly linear fashion, due to the greater number of word positions to consider.
What is more important, the complex algorithm is shown to take roughly twice as long as the simple algorithm across all window sizes.
This is understandable, since it must make twice as many comparisons as the simple one.
It would appear that the slightly superior performance of the complex algorithm does not justify the increase in execution time.
The simple algorithm has therefore been be adopted as the default in subsequent trials.
(N.B. — these timings are based on a prototype LISP implementation running under PROLOG and are therefore unrepresentative of the current semantic analyser, which has been coded in C and optimised for efficiency.)
Another aspect under scrutiny was the effect of domain on performance, which is shown in Figure 3.4.
Evidently, once the window size exceeds two or three word positions in any domain, the performance tends to stabilise.
When the window size is four words, the result for banking is 78% correct,estate agents is 58% and music is 50%.
The average for all three domains is 61% correct.
The percentage incorrect is 39%, which gives the ratio correct:incorrect of approximately 1.5:1.0.
This can be cross-referenced with Figure 3.2.
Another important parameter in the overlap process is definition length.
To illustrate, let us consider the case of a word overlapping with two other candidates, the first of which has a long definition and the second has a short definition.
All other things being equal, there will be a higher probability of a random overlap with the first candidate than with the second.
As a result, some words will provide more overlaps than others purely as a result of the length of their definitions.
This is particularly significant in the case of weak overlaps.
However, it is possible to compensate for definition length by dividing the score assigned to any overlapping pair of words by the joint length of their definitions.
Figure 3.5 clearly shows the improvement in performance obtained when scores are calculated in this manner.
In future trials, therefore, definition length compensation is a default parameter setting.
One further important aspect of the overlap process is the role of weak overlap in the allocation of scores.
Would strong overlap on its own be sufficient?
Figure 3.6 shows the comparison between two sets of scores; one based on strong and weak overlap and the other based on strong overlap alone.
With window sizes greater than four words, weak overlap has a beneficial effect, albeit minimal.
Conversely, for window sizes of less than four words, strong overlap alone is the most effective.
However, the peak on the left hand side of the graph is deceptive: with small window sizes and no weak overlap, the vast majority of positions remain unaffected; i.e. they remain as ties.
To operate the program in this manner would therefore be pointless, since the vast majority of results would remain undecided.
At a window size of four words the performance is identical, suggesting that the information obtained from weak overlap is redundant.
3.7 Another Choice of Dictionary
3.7.1 Introduction
As with all research, investigations are performed within the context of available time, resources and knowledge.
For this reason, it is not always possible to carry out the preferred size or type of investigation until certain data or resources become available.
For example, all the earlier investigations in this chapter have used either the CED or OALD.
The LDOCE, widely used and recommended by many other language researchers, was simply not available.
When it did become available, it was possible to repeat some of the earlier investigations with the CED and OALD using the larger sample size that was recommended, and to include the LDOCE as a further choice of dictionary.
The subsequent acquisition of Longman's English Language Corpus (LELC) provided two further benefits: there was no longer a shortage of suitable test data available in machine readable form, and the raw corpus could be analysed to produce collocation dictionaries (see Chapter 4).
3.7.2 Investigations with LDOCE
Objective
: To repeat the investigations of the OALD and CED with much increased sample size, and to compare their performance with that of the LDOCE.
Method
: Fifteen documents were extracted from the Longman Corpus (LELC) and retained as test data,not to be used in any subsequent lexical processing (e.g. dictionary creation, etc.).
These test documents covered a wide range of domains (see Chapter 4 for a fuller discussion of the structure of LELC).
Each of these test documents was at least 500 words in length, which compares favourably with the average of 200 words in previous investigations.
Furthermore, where previously 3 domains had been investigated, there was now 15.
These test documents were passed through the confusion simulator to produce alternative candidates as their output.
This was then used as input to the overlap program that was run separately for each of the 15 documents and for each of the 3 dictionaries.
Results
: The performance of each of the dictionaries across each of the domains is shown by Table 3.10:
Discussion
: Evidently, the LDOCE outperforms the CED and the OALD.
The main reason for this must surely be related to the manner in which LDOCE definitions are constructed.
It is claimed that the entries within LDOCE are defined using a controlled vocabulary of about 2000 words, and that the entries have a simple and regular syntax [Boguraev & briscoe , 1989].
This has the effect of reducing the entropy of the definitions, by cutting down on the randomness with which their constituent words are chosen.
In so doing, the chance of strong (or weak) overlaps are increased, since the probability of two semantically related words being defined using common terms is now proportionately increased.
(Consider the nonsensical case where the core vocabulary is only half a dozen words — strong overlaps between words would be almost inevitable!).
This reduction in the’ noise’within definitions means that where semantic relations are present, the overlap technique is more likely to detect them.
Performance across domains, is however, highly variable, with no obvious pattern emerging.
The CED is the most consistent, with 12 of the 15 scores being in the 60-70% range, and a standard deviation of 4.57.
The LDOCE shows more variability, with 9 scores in the 70-80% range, and one particularly low score (57.9%, for engineering) which gives it a slightly higher standard deviation of 5.41.
The OALD shows the most variability, with 5 scores in the 70-80% range and 3 in the 50-60% range.
Consequently, this has the highest standard deviation; in this case 5.86.
As mentioned previously, the Longman Corpus consists of superfields that are in turn subdivided into subfields.
For example, the superfield Applied Science contains the subfields Computing, Energy and Engineering (amongst others), The superfield Commerce contains the subfields Business, Employment and Finance , and so on .
Given that the eventual needs of a working system may be biased towards the domain of Commerce , the results for the Business, Employment and Finance documents take on a particular relevance.
The LDOCE scores consistently in the 70-80% range for these documents, whilst the CED and OALD are both consistently in the 60-70% range.
This result provides further justification for the recommendation of the LDOCE as the most suitable of the machine-readable dictionaries.
On the whole, these results are more reliable than those of previous investigations due to the vastly increased sample size.
Whereas before the test data consisted of 3 domains, with a test document of 200 words each, there are now 15 domains, each document being at least 500 words in length.
This means that the figure for the average performance (at the bottom of the table) is based on some 7,500 words of text.
3.8 Summary
Dictionary definitions constitute a valuable source of semantic knowledge, and the definitional overlap technique has been shown to be a suitable method for applying such knowledge.
The results of the semantic priming investigation provide independent evidence of its ability to identify semantically related words.
The technique has been adapted to suit the format of text recognition data, and has been shown to be effective as a means of identifying correct words from alternative candidates.
Dictionaries differ widely in their style and content, and this has been shown to affect recognition performance.
LDOCE is slightly superior to the CED and OALD in this respect, most probably due to its use of a core vocabulary.
The way in which a dictionary is indexed is also important.
A larger lexicon provides a greater coverage of English and therefore can be used to provide indexed definitions that are more representative of their verbal originals.
However, such a lexicon may not always represent semantic relationships as effectively; the ratio of foot forms to inflections (i.e. the’ grain-size’) is also important.
Further research of this issue is suggested.
The efficacy of definitional overlap varies greatly across domains, such that specialist dictionaries may be required for more esoteric or specialist domains.
A technique for compiling such dictionaries is described, and the assessment of this technique together with the nature of domains in general are also suggested as areas for further research.
The expansion of dictionary definitions does not appear to provide any further useful information than that which is available at the first level.
Instead, the information descends into generality in a manner that cannot be avoided by recourse to either word-frequency or syntactic information.
It is suggested that semantic analysis using machine-readable dictionaries is restricted to their definitions and not their expansions.
Several aspects of the overlap algorithm have been investigated, and their optimal values (where appropriate) have been identified.
These include:
the use of a window size of four words;
the use of the simple rather than complex algorithm;
compensation for the length of definitions;
the use of strong overlap only (rather than strong and weak).
The importance of a reliable and valid method by which results may be analysed has also become evident.
A number of assumptions are related to any method of analysis, and these must be appropriate to the data to retain any validity.
For example, if the process selects the correct word one in four times this does not necessarily imply 25% accuracy — it depends on the number of candidates from which each choice is made.
The importance of adequate sample sizes has also been made evident.
Chapter Four
Collocations
4.1 Introduction
There are certain classes of English word combinations that cannot be explained using existing syntactic or semantic theories.
For example, consider the use of’strong ’ and’powerful ’in the following phrases:
to drive a powerful car
to drink strong tea
Both fulfil the same syntactic role (as an adjectival pre-modifier), and both make a similar semantic modification to the subject.
However, to interchange them (’powerful tea ’ &’strong car ’) would undoubtedly be judged anomalous by most English speakers.
These predisposed combinations are called co-occurrence relations or collocations , and account for a large proportion of English word combinations [Smadja, 1989].
Similarly, the notion of collocation may be explained with reference to the Oxford Advanced Learners Dictionaries of Current English (OALD):
collocate :–(with),(of words) combine in a way characteristic of language:'Weak' –s with ‘tea’ but ‘feeble’does not .
collocation : coming together; collocation of words:‘Strong tea’ and ‘heavy drinker’are English collocations; So are ‘by accident’and ‘so as to’.
These collocations could be regarded as extensions to the base meaning of a particular word; for example, in the case of’weak ’, we could regard the base meaning as’lack of physical strength ’ and then acknowledge modifications to this base meaning when used in the context of describing solutions.
Indeed, this separation of meaning is reflected by the definition of’weak ’ in the OALD, with a distinct sense reserved for its use when pertaining to that of solutions.
This is perhaps less surprising when one considers that these definitions are derived by examining and grouping the actual collocations found for any particular word, and then working backwards to a definition from the separate contextual groupings [Mackin, 1978].
Collocations represent a further linguistic constraint upon text, and as such may be exploited by the semantic analyser.
However, unlike dictionary definitions, there is no convenient repository from which to extract them.
Evidently, it is necessary to compile some sort of’ collocation dictionary’by automatic means.
In the case of the human language processing system, collocations are learnt or compiled by experience, using feedback from language use, performance mistakes, etc.
However, if collocations like’weak tea ’ and’powerful car ’are so numerous as to evade any method of acquisition other than years of learning, how then should a machine-readable collocation dictionary be compiled?
What type of collocations should be included?
One metric by which collocations may be measured and grouped is to rate them on a scale of probability.
At the bottom would be little-used expressions and combinations, and at the other end would be uniform, predictable or fixed combinations such as clich s, sayings, metaphors, etc.
A threshold could then be determined beyond which certain combinations could be deemed too infrequent to be worthy of inclusion within the dictionary.
Another metric by which collocations can be classified is according to the behaviour of the constituent words within the immediate context or concordance .
Some collocations such as’mortgage-property ’ or’insurance-client ’come about because both words are associated with the same context or subject domain.
These may be referred to as paradigmatic or conceptual collocates [Smadja, 1989], and are characterised by an equal distribution of one term about the other within a given context (e.g.’mortgage ’ &’property ’can occur anywhere within the same sentence in relation to each other).
Other collocates exist as lexical phenomena, and are referred to as syntagmatic or lexical collocates .
In such cases, the order of the terms is important.
For example, in business letters the words’hesitate ’ and’contact ’may form a collocation, as in the phrase’please do not hesitate to contact me ’.
However, because this is a stock phrase, it usually occurs as a fixed pattern.
There is no longer an equal distribution of one term about the other:’hesitate ’ always precedes’contact ’by two words.
4.1.1 Collocation Dictionaries
Previous methods of collocation dictionary compilation have included:
the use of other dictionaries;
using one's own’ competence’;
corpus analysis.
The first method is uncreative and derivative, and the second is notoriously unreliable [Mackin, 1978].
The third, however, is both reliable and objective; and furthermore has benefited greatly from the advancement of computer technology and a proliferation of textual resources in electronic form.
Increased processing power and memory capacity have greatly reduced the manual’ word-crunching’that was previously involved in corpus analysis, rendering this a viable technique for the compilation of collocation dictionaries.
The precise format of a collocation dictionary depends largely upon the application.
A’ glossary’of sayings, proverbs, clich s, etc. for human use may require no more than a simple alphabetical listing of all recorded combinations; but for text recognition, the design of the dictionary and its subsequent use must be considered in conjunction, to reflect the run-time processing needs.
4.1.2 Collocation Analysis
There are many ways in which language can be analysed to produce collocational information.
Phillips [1985]identifies three broad groupings:
(i) Classical Approaches:
statistical approaches; viewing texts as random samples from a population described by a theoretical stochastic distribution;
(ii) Multi-Dimensional Scaling:
conceiving the set of collocations of a word as its co-ordinates in multi-dimensional space;
(iii) Cluster Analysis:
conceiving of each word as being characterised by its set of collocations, allowing subsequent fusion of individual data points.
Each has their relative advantages and disadvantages.
For the purposes of the current project, the two major criteria were computational feasibility and rapid prototype development.
The classical approach, as implemented by Berry-Rogghe [1970]proved the most suitable.
This method is based upon an algorithm to determine the likelihood that two adjacent words are truly a collocation, rather than an accidental association.
The process starts from a list of words for which collocation information is required.
Very often this involves no more than manual selection, or simple extraction of the higher frequency words from a distribution taken on the corpus.
The lower frequency words exhibit statistically unstable behaviour, because the sample of contexts in which they participate is too small to be an adequate representation.
These lemmas are then analysed in turn, regarding each one as the’ node’of a set of linguistic collocations.
So in the case of’money ’, the corpus would be searched for all appearances of that word, and then the immediate contexts of each occurrence (the concordances ) would be collected and truncated to extend no more than four words either side of the lemma.
This four-word span or window size has been derived empirically and used effectively by other researchers (e.g. Sinclair et al[1970]).
This collection of concordances now forms a subset of the corpus and can be treated to a separate frequency analysis, to discover the collocates of the original node (in this case the word’money ’).
A simple statistical procedure known as the z-score assigns a score to the strength of association between the lemma and each of its collocates.
This distance relation score measures the degree of attraction between a lemma and its collocates, by comparing the frequency of a collocate in the concordances with the frequency that would be expected were all words distributed in the text at random .
(For example, the frequency of the collocate’pay ’ in the context of’money ’would be compared with its frequency outside of that context: this may return a high positive value, indicating a high degree of association, or a low or even negative value indicating non-association or even repulsion.)
Lancashire [1987]has suggested the value of 1.49 as a threshold for association, although he provides no formal justification for this.
A typical set of collocates for the word’mortgage ’ could be:
Certain words, such as’lend ’,’property ’ and’advance ’are highly associated with the node (’mortgage ’); whilst’advice ’and’term ’show a weaker association.
This information, given suitable formatting, can form the basis of a collocation dictionary and hence be used by the semantic analyser.
4.1.2.1 The Collocation Program
The AWK programming language has been used to implement a variation of the collocation technique described above.
The program uses an algorithm based on that of Berry-Rogghe [1970], which is described in Appendix B. However, instead of producing a concordance represented as a list of z-scores, it reformats the collocates into a list headed by the lemma being analysed.
The degree of repetition of any one collocate in the list corresponds to the degree of association between the lemma and that collocate.
For example, the above concordance of the word’mortgage ’ could be represented as: mortgage [lend lend lend own own own property property property advance advance advance authority authority pay pay money money increase increase insurance advice term]
with the degree of repetition of each collocate representing the z-score divided by a constant (in this case 3.0) and rounded to the nearest integer.
The value of this constant has been chosen so that the lists of collocations so produced are comparable in size with the definitions taken from machine-readable dictionaries.
The format is also comparable, i.e. a headword followed by a list of related words in parentheses.
The output of the program may be modified by adjusting a number of parameters:
(1) Window Size:
the extent of the concordance around the node;
(2) Z-score Threshold:
the minimum z-score for inclusion in the collocate list;
(3) Replication Constant:
the constant by which z-scores are divided before inclusion in the list of collocates;
(4) Distribution Threshold:
the level below which certain collocates are deemed to be of too low frequency to be statistically stable.
To date, the collocation program has been tested on a variety of corpora, including the 1 million-word LOB corpus, and a 5 million-word subset of Longman's English Language Corpus.
In so doing, the program extracts and organises information concerning likely sequences of words, based on analysis of genuine text (rather than subjective judgement, or ad hoc collection techniques).
This information can then be used by the semantic analyser to discriminate between alternative sequences of candidate words, by comparing their collocational information.
Since the format of the collocation dictionary is compatible with that of the MRDs used earlier, it can be substituted directly into the overlap program described in Chapter 3.
Initially, the program was run on a corpus of financial documents.
It was appreciated at an early stage that the efficacy of collocational information could vary greatly according to the domain from which it was taken.
For example, collocations extracted from the domain of Banking may be of little use when processing a medical report, since words like’charge ’ behave differently when used to describe a type of payment rather than a type of nurse.
This financial corpus was relatively small, totalling 5,113 running words in its unedited state.
After reduction and lemmatisation it comprised 2,344 lemmata.
A frequency analysis of this corpus was produced, showing a distribution of some 651 types amongst these 2,344 lemmata.
The first 156 items on a frequency ranked list were selected as input to the concordance program.
Beyond this threshold the absolute frequency of each lemma was 2 or under; at which point the sample size becomes too small and the statistical behaviour unstable.
The output from the program was a domain-based collocation dictionary consisting of 156 entries, with headwords listed in alphabetical order and collocation information in the form of wordlists appended to each headword.
Its efficacy could then be assessed by substituting it for the dictionary definitions in the overlap program.
4.2 The Pilot Study
Method:
The collocation program was run on a corpus of financial text in the manner described above.
A financial document that had NOT been used in the compilation of the corpus was selected as test data (this was the same document as used in the earlier investigations of Chapter Three).
Simulated recogniser output was produced for this text, and the overlap program run on this data using the collocation dictionary as its source of information.
Details of this algorithm are described fully in Chapter Three, so only a brief outline is given here:
(1)
Iterate through the sentence positions and their candidate words;
(2)
Compare the collocation list of each word with that of its neighbours (up to four sentence positions away);
(3)
Record the number of’ strong’and’weak’overlaps associated with each word — worth 50 points and 1 point respectively (NB — strictly speaking, it is inappropriate to talk of’strong overlaps’or’weak overlaps’in this context, but the terminology is used to indicate the consistency of the algorithm);
(4)
Total up each word score and output finished sentence with scores normalised over the scale of 0-100 for each word.
Results:
In many word positions the overlap process was biased since the correct word appeared in the collocation dictionary whilst the alternative candidates did not.
Only the fair comparisons should be submitted to statistical analysis, and these fall into two categories:
(i)
Cases where both the correct word and one other candidate had an entry as headwords in the collocation dictionary; or
(ii)
Cases where none of the words in a position appeared as headwords in the dictionary, but some nevertheless received a score due to their inclusion in the collocation list of some other word.
The results were analysed in percentage terms and using the Student's t-test, as shown in Table 4.1.
The t-score can be checked against statistical tables to determine the level of significance: i.e. it can be said with 99% confidence that the technique selects the correct word in favour of other candidate words.
Discussion:
Evidently, collocational information can significantly improve the recognition process.
However, this investigation used collocations extracted from a domain-specific corpus, and test data taken from the same domain.
Therefore, it is more accurate to state (so far) that collocation information’extracted from and used within a specific domain ’ can significantly improve the recognition process.
It remains to be seen whether this process can be extended to other domains.
In theory, a collocation dictionary created from a large and general corpus should be sufficiently comprehensive to aid the recognition of text taken from almost any domain.
To test this hypothesis, a further investigation was carried out.
4.3 The LOB Corpus
Method:
The collocation program was run on the LOB corpus.
A collocation dictionary of 7,130 entries was produced (after tidying and removal of empty entries), and this was used as the source of semantic knowledge in the overlap program.
Test data was as in the previous investigation.
Results:
An important difference with this investigation is the coverage of the collocation dictionary.
In the pilot study, the collocational dictionary was very small, and therefore covered the just correct words (being from the same domain) and a few alternative candidates.
However, the collocation dictionary extracted from the LOB consists of some 7,130 entries, and therefore covers a much larger proportion of the candidates in the data.
So in every sentence position, many more alternative candidates are now able to compete with the’ correct’word.
In effect, the correct word now has more’ competition’.
The results were analysed in percentage terms and using the Student's t-test, as shown in Table 4.2.
The t-score can be checked against statistical tables to determine the level of significance:(z [df 18]= 2.101, p < 0.05), and (z [df 18]= 2.878, p < 0.01).
2.84 > 2.101 therefore: reject null hypothesis at 95% significance level; i.e. it can be said with 95% confidence that the technique selects the correct word in favour of other candidate words.
Discussion:
The LOB collocation dictionary makes a contribution to the recognition process that is significant to the 95% confidence level.
We can conclude therefore that both domain-specific collocations and a general collocation dictionary extracted from the LOB corpus make a significant contribution to the recognition process.
The domain-specific dictionary appears to perform slightly better than the general dictionary.
This difference in performance may be explained by factors related to the dictionary compilation process.
The domain-specific dictionary has been compiled from documents associated purely with Banking .
The senses of polysemous words (such as’charge ’ or’rate ’) are therefore most likely to have been used in the sense related to financial affairs.
The collocations so formed are therefore representative of such financial material and hence will be more specific than those from the general dictionary.
For example, consider the definition of the word’access ’ taken from the general collocation dictionary: access [house house give give right terrace terrace terrace access access access pupil pupil point west various usual trade route route road require provide mean market important given gave freedom free found experience ensure educate direct deny committee case cabinet cabinet]
As can be seen, it refers most strongly to the physical sense of’access ’, with words like’give ’,’right ’,’terrace ’,’trade ’,’route ’,’road ’ &’freedom ’, etc.
This is a reflection of the composition of the corpus from which it was taken.
This entry may now be compared with the entry in the domain-specific dictionary: access[ addition build combine confident exception exception facility future good knowledge manager month mortgage notice society sum time want world give high instant instant interest money]
It can clearly be seen that this entry indicates most strongly the financial sense of’access ’(e.g.’mortgage ’,’sum ’,’high ’,’instant ’,’interest ’,’money ’, etc.).
Hence, when used to disambiguate a document taken from the domain of Banking , the second entry is more appropriate, because it is more representative of the likely word senses and collocations found in such a text.
This result implies that domain-specific collocations may be superior to general collocations in analysing documents from the same domain.
However, it is not necessarily the case that these characteristics will be exhibited within other domains.
Indeed, there is much evidence to suggest that many collocations found in natural text are domain-independent , and that only the analysis of a sufficiently large and general corpus will provide coverage of such structures.
This issue is considered in further detail in the following sections.
It is possible that a domain such as Estate Agents will be adequately covered by the LOB corpus, due to its nature as a concrete, well-understood domain that is concerned with’ everyday’words and concepts such as houses, towns, rooms, etc.
In such a case, domain-specific dictionaries would be unnecessary, since the LOB provides adequate coverage of such words.
To test this hypothesis, three different domains were selected and investigated.
4.4 Domain Dictionaries
Introduction:
The above investigations have demonstrated the significant contribution of collocation dictionaries to the semantic analyser.
However, these results were based on sample documents taken from a single domain.
In the pilot study, a domain-specific dictionary was tested with a document from the same domain.
In the second investigation, a general collocation dictionary was tested using the same document.
Although the pilot study demonstrates that a domain-specific dictionary can make a significant contribution to the recognition of a same-domain document, it is not necessarily the case that this effect will be repeated in other domains.
Moreover, it is desirable to quantify the degree of reciprocity between domains — i.e., the extent to which collocations from domain X contribute to the recognition of text from domain Y, and vice-versa.
For example, domains that are closely related may have a large number of collocations in common , such that the recognition of one could be facilitated by a dictionary taken from the other.
Conversely, it may transpire that apparently similar domains make radically different use of those constituent words and hence demonstrate extremely different collocational patterns.
Indeed, it is likely that cross-domain recognition is constrained by the coverage of a particular dictionary.
For example, it is unlikely that a Banking dictionary would have sufficient coverage to aid the recognition of medical texts, regardless of how suitable the collocations were.
Evidently, the general collocation dictionary derived from the LOB corpus can make a significant contribution to the recognition of domain-specific documents.
However, it has only been tested on one domain.
Therefore it is desirable to apply this dictionary to a range of domain-specific documents and to compare its performance with that of the appropriate domain-specific dictionary.
Method:
Three domains were chosen for investigation:Banking, Estate Agents and Music .
This choice reflected both the potential application of the eventual system and ease of availability.
Corpora were built up in each case to over 10,000 words, then domain-specific collocation dictionaries compiled using the method described earlier.
The size of each domain-specific dictionary is shown in Table 4.3 Test documents were selected for each of the domains, approximating to 17 sentences in each (the same documents as used in the early investigations in Chapter Three).
Each of these documents was processed by the confusion program to produce simulated recognition output.
The performance of each dictionary was tested using each of the three documents, and measured in terms of percentage correct and z-score (the t-score could now be replaced by the z-score since sample sizes were sufficiently large).
Results:
This investigation involved four dictionaries and three documents, and hence gave a total of twelve combinations, as shown in Table 4.4.
The scores in each column show a triple score representing the percentage correct/tied/incorrect, with the z-scores underneath.
By way of comparison, the significance levels for a comparable sample size are as follows:(z [df 120]= 1.980, p < 0.05),(z [df 120]= 2.617, p < 0.01).
Discussion:
The general dictionary proved significant to the 99% confidence level across all three domains, which justifies the effort required to produce such a comprehensive general collocation dictionary.
However, the spread of results was quite wide (z = 2.69 to z = 7.18).
Indeed, the range of results may reflect the proportions of text-types represented in the LOB corpus.
The high result for the Estate Agents' text may be a reflection the commonality between Estate Agents' literature and other genres contained in the LOB Corpus.
For example, words that are high frequency in such text (e.g.’buy ’,’house ’,’room ’,’door ’, etc.) are concrete, everyday terms that are also high frequency words within other genres such as fiction, hobbies, DIY, etc.
They may therefore be found within many other text types contained with the LOB, and used in a manner that tends to be consistent across each domain.
This commonality is demonstrated by examination of entries taken from each dictionary.
Consider the entry in the Estate Agents dictionary for the word’buy ’: buy [ability active add advantage afford aim alike auction average borrow breaker cash certain charter consult consultant counsel couple distribute fair feel find found general go grow guide happy leasehold majority marry necessary opportunity package permit post potential present probable prohibit raise reach reason reduce result safety see specialise splendid step stop telephone tenth thatch waive whole wont young arrange cent ideal likely lot new seek take want week finance part people purchase sell house right home will first time]
As would be expected, the majority of senses of the constituent words are related to property and its purchase (e.g.’leasehold ’,’house ’,’home ’,’first ’,’time ’, etc.).
Such references to property purchase are also highly evident in the entry in the general dictionary (e.g.’house ’,’money ’,’build ’, etc.): buy[ house house money money build afford afford afford small save save vote vote people make able store store sell rent part need cheap cheap car book want ton ton stamp society run provide price paper large income improve home farm expense cost buy bin bin white told start rich public process politic pack operate library instance firm encourage conservative colour activity wise win tool style site secret sand risk remember purchase proportion property proper potato market luxury likely invest hotel holding heavy finish feed favour export equip enter enlarge dress distribute distinct department dear client client clean charge champagne champagne cent cement cement business box bird big appeal aerial advantage accuse]
Although the entry in the general dictionary understandably includes a variety of collocations that are representative of other domains (e.g.’vote ’,’politic ’,’conservative ’,’society ’, and’public ’, all of which suggest an origin in parliamentary proceedings), there are still a large number of words common to both entries.
These include’advantage ’,’afford ’,’cent ’,’distribute ’,’home ’,’house ’,’likely ’,’part ’,’people ’,’purchase ’,’sell ’, and’want ’.
It is this high degree of commonality, of which the entry for’buy ’ is typical, that enables the general dictionary to provide a highly relevant source of information for recognising text from the estate agent's domain.
The performance of the domain-specific dictionaries varies greatly across domains (the Banking dictionary varies from z = -0.99 to z = 6.46).
Each domain-dictionary achieved its highest z-score when used in the recognition of text taken from the same domain.
This is to be expected, since the purpose of a domain-specific dictionary is to capture precisely those collocations that are specific to and therefore representative of that domain, in preference to any others with which the individual words may otherwise be associated.
Another objective of this investigation was to determine the extent to which collocations taken from one domain could aid the recognition of text taken from another.
It was suggested that cross-domain recognition may be constrained by a lack of mutual dictionary coverage and the specificity of the collocations they represent.
However, this was not exclusively the case.
The Banking dictionary made a significant contribution (to 95% level) to the recognition of Estate Agents' text.
This suggests that many of the language structures used in the Estate Agents' corpus are also present in the Banking corpus.
Moreover, this result may simply be a reflection of the generality and’ everydayness’of Estate Agents' text: just as the LOB gave good coverage of this domain, the Banking corpus also contains language structures that are representative of estate agent's text.
However, this cross-domain recognition does not appear to be mutual.
Although the Estate Agents' dictionary contributes to the recognition of the Banking text, it is nowhere near significant (z=0.88).
This implies that the Estate Agents' dictionary does not cover a large enough subset of the language used in the Banking text to provide significantly representative collocations.
The Music dictionary contributes only to the recognition of Music text.
For both Estate Agents and Banking it makes a negative contribution (-1.09 and -0.35 respectively).
This reflects the specificity of musical language and terminology, and its consequent inability to adequately represent the language structures found within other domains.
What is more important, this result underlines the need for accurate & reliable domain identification.
Evidently, this particular specific dictionary is only of use in recognising text from its own domain.
Were it to be used on any other domain, it is likely that a negative (and therefore potentially damaging) effect would result.
From this set of results it is not clear whether separate domain dictionaries are a necessity.
Although the specific dictionary out-performs the general dictionary for Banking text, for the Estate Agents' text the general dictionary out-performs the domain dictionary.
For Music text the performances are comparable.
However, it may transpire that the result for the Estate Agents' text is somewhat unrepresentative.
The general dictionary works well in this case because the’ concreteness’and generality of Estate Agents' literature is well represented in the LOB corpus.
The majority of domains, however, do not share these characteristics and may not be so well represented within the LOB corpus.
For such domains separate dictionaries may remain a necessity.
To resolve this issue, a further investigation was set up, involving the processing of a much larger corpus and the testing of a greater number of domains.
4.5 The Longman Corpus
Introduction:
The Longman English Language Corpus (LELC) is a collection of texts divided into mainly 40,000-word chunks, taken from over 2,000 sources (books, magazines, journals, leaflets, advertising material, etc.).
The corpus can be subdivided in many ways; one of which is by subject area.
The main subject areas are referred to as superfields , of which there are ten.
These superfields are in turn subdivided into smaller subject areas, which are referred to as subdomains .
The number of subdomains within each superfield is variable, as shown in Table 4.5.
The LELC is available on a custom basis, and the present project has acquired some 13 million words of text from this corpus.
Unfortunately, these 13 million words are not evenly distributed across the 10 superfields.
Fiction is heavily over-represented whilst many other domains are heavily under-represented.
For this reason, the largest’ balanced’corpus that can be derived from these 13 million words consists of 5 million words, and represents approximately 500,000 words from each of the 10 superfields.
However, the problem does not end there, since within each superfield the subdomains are not evenly represented.
For example, the superfield of Applied Science should adequately represent all the subfields listed above, but within the text so far obtained this superfield contains 6 computing texts, 3 engineering texts, 1 energy text, 1 transport text, and none on technology or communications.
Due to this distorted coverage, it has been necessary to choose test data with some care, ensuring that they are adequately covered by the corpus.
If this were not the case, then the collocation dictionaries would not adequately represent the test data.
Method:
Fifteen subdomains were chosen for investigation, such that five of the ten LELC superfields were represented by texts from each of three constituent subdomains.
The criterion for selection was mainly that of sufficient coverage in the corpus, as outlined above.
A general collocation dictionary of some 12,475 entries was produced from the 5 million-word general corpus, and separate domain dictionaries created for each of the five superfields.
The size of each dictionary is shown in Table 4.6.
This table also shows the fifteen subdomains selected for investigation, and the superfields to which they belong.
Test documents were selected for each of the fifteen domains, approximating 500 words in each.
No part of these test documents had been used in the creation of any collocation dictionary.
Each of the documents was processed by the confusion program to produce simulated recognition output.
For each test document, the overlap program was run once using the general collocation dictionary, and once using the appropriate domain-specific dictionary.
Results:
The breakdown of scores in terms of percentage correct is as shown in Table 4.7.
Discussion:
The average performances of the general and the domain-specific dictionaries are extremely close (they differ by only two per cent).
This is somewhat surprising, since it would be reasonable to assume that domain-specific dictionaries would contain the most appropriate collocations for domain-specific documents.
However, for 8 of the 15 documents, the general dictionary is more effective (by as much as 11.9% in one case).
Explanations for this inevitably concern (a) the content of the textual material used as data, and (b) the content of the collocation dictionaries.
Evidently, any given document will consist of a variety of language structures, some of which will be general (i.e. not exclusively associated with any particular domain) and some domain-specific (i.e. with restrictions on word senses, etc.).
This ratio of’ general’to’specific’material will vary between documents and domains, such that a high proportion of’general’material may render the use of a domain-specific collocation dictionary less appropriate.
Moreover, the specific dictionaries were derived from smaller corpora than the GCD and therefore contained fewer entries: 5,545 (on average) compared to 12,475 in the GCD.
Furthermore, although the domain-specific corpora were all the same length, due to variations in the type:token ratio the resultant dictionaries varied greatly in size (from 3,960 entries to 7,748 entries).
Indeed, this variation in size very closely matches their performance: those larger than average tend to do better than the GCD, and those smaller tend to do worse.
This variation in performance is further reflected by the higher standard deviation of the specific dictionaries.
Although the domain-specific dictionary outperforms the general dictionary on average, there are good reasons why the general dictionary would still be preferred in the majority of situations.
The first is a practical issue: domain dictionaries are only of use if the domain has been accurately identified in the first place.
It is not necessarily the case that this will have happened, nor can it be assumed that the document in question belongs to a given domain at all(it may be some sort of hybrid, or simply too ambiguous to fit neatly in one domain).
Another reason is that of coverage.
A domain-specific dictionary may provide good performance within its own particular domain, but outside this its performance is brittle and inflexible.
As we have seen from the above results many common language structures are domain-independent, and to provide comprehensive coverage of these a collocation dictionary must be based on as varied a corpus as possible.
Additionally, good coverage is required to process all the alternative candidates produced by a recognition system.
A further reason is that of consistency.
The specific dictionaries have a superior average, but their performance is inconsistent.
The specific dictionaries show a standard deviation of 9.95%, whereas for the general dictionaries this figure is only 5.95%.
An analysis of the ranges confirms this: for the general, scores vary from 64.1 to 84.7; for the specific, they vary from 61.5 to 94.4.
The ranges are therefore 20.6% for the general and 32.9% for the specific.
Considering this, the domain specific dictionaries can be said to be less reliable, and based on assumptions about the accurate identification of the domain that may not always be applicable.
For this reason, the general collocation dictionary is suggested as being the more appropriate.
4.6 Summary
There are certain classes of English word combinations that cannot be explained using existing syntactic or semantic theories.
These predisposed patterns are known as co-occurrences or collocations , and account for many English word combinations.
Collocations (and the concordances from which they are derived) have been successfully used for a variety of linguistic purposes.
For example, they represent a valuable resource to the lexicographer in the dictionary building process, as they provide empirical information concerning word usage.
Strictly speaking, collocations represent syntagmatic and paradigmatic knowledge rather than semantic , but it is argued that they represent the implicit application of syntactic, semantic and pragmatic knowledge [Sharman, 1990], and for reasons of simplicity have been referred to as a source of semantic information.
Collocations are similar to dictionary definitions inasmuch as they can be expressed as a headword followed by a list of semantically related words.
However, unlike definitions, there is no convenient repository from which they can be instantly extracted.
Instead, they must be compiled, as a product of corpus analysis.
To this end, a number of algorithms were investigated, and one selected to be coded as an Awk program.
Initially, this’ collocation building program’was applied to a small corpus of financial documents.
The small collocation dictionary so produced seemed highly plausible, so a previously unseen document from the domain of banking was passed through the confusion program to produce suitable test data.
The result was that the’collocational overlap ’ technique made a significant contribution toward the identification of the correct words from the alternative candidates.
The next step was to’ scale up’this pilot study.
The collocation-building program was run on the 1 million-word LOB corpus, to produce a’ general collocation dictionary’(or’GCD ’) of some 7,130 entries.
This was tested using the same unseen banking document, and again gave a significant result, which suggested that collocation information compiled from a general corpus could be effective within a specific domain.
However, to fully test this hypothesis, it was necessary to investigate more than one domain.
To this end, three domains were selected, and test data for these domains gathered.
As part of this study, it was additionally possible to investigate (a) a variety of domain-specific dictionaries and compare their performance with the general dictionary, and (b) the extent to which collocations compiled from one domain could contribute to the recognition of text from another.
However, the results proved inconclusive.
The domain-specific dictionaries made a significant contribution to each respective domain-specific text, but so did the general dictionary.
The need for domain-specific dictionaries had therefore not been clearly proven.
Cross-domain collocations did not appear to be effective, as was expected, which underlined the importance of accurate domain identification when using domain-specific dictionaries.
The acquisition of the Longman Corpus enabled investigations to proceed in a more rigorous fashion using much increased test data sample sizes.
A balanced corpus of 5 million words was extracted from this to produce another general collocation dictionary; this time of 12,475 entries.
Five superfields of the Longman Corpus were selected for investigation, and 15 documents, representing three subdomains from each of the five superfields were extracted.
These documents were all at least 500 words in length and had NOT been used in the compilation of any dictionary.
Additionally, it was possible to create 5 domain-specific dictionaries for the domains under investigation, each based on at least 500,000 words of domain-specific text.
The performance of these specific dictionaries was then compared with that of the new GCD.
The results were extremely close.
Although for some domains the specific dictionary far outperformed the general, the overall margin, on average, was insufficient to justify their continued use.
In fact, for many domains the general dictionary outperformed the specific.
This suggests that there are many language structures that are not exclusively attached to any one domain, and that the only way to provide a collocation dictionary that is sufficiently flexible and comprehensive is to process as large and varied a corpus as possible.
The inadequacy of the domain-specific dictionaries on these occasions reflects an attempt to constrain the highly unpredictable phenomenon of language by using too narrow a framework.
Furthermore, there were other reasons why a general dictionary was to be preferred over the domain-specific dictionaries.
Firstly, specific dictionaries can only be relied upon if the domain of the test data has been accurately identified in the first place.
It is not necessarily the case that this will have happened, nor can it be assumed that the data in question fits neatly within a specific domain at all.
Secondly, the requirement of coverage suggests that many specific dictionaries will be inadequate due to their small size and inflexibility when used on text that strays from domain-based patterns.
Additionally, good coverage is required to process all the alternative candidates produced by a recognition system.
Thirdly, specific dictionaries are less reliable due to their inconsistency.
Across the 15 domains they show a standard deviation of 9.95%, as compared to 5.95% for the general dictionary.
In sum, the domain specific dictionaries are less reliable, lack sufficient coverage and are based on optimistic assumptions about domain identification.
For this reason, the general collocation dictionary is suggested as the more appropriate.
Evidently, there are a number of limitations to the collocation analysis technique.
Firstly, it is based on lemmatised (root) forms rather than inflections.
However, it is clear that some collocations only exist in particular inflected forms [Schuetze, forthcoming].
Consequently, it is intended to acquire inflected versions of the above collocation dictionaries and compare these with their lemmatised equivalents (using the same text recognition data).
Secondly, the current technique makes no use of function words.
However, these are an essential part of a number of important linguistic phenomena such as phrasal verbs [Sinclair, 1987].
It is intended therefore to incorporate such information into future acquisition methods, and compare the results with their’ content-word only’predecessors.
Thirdly, no use is made of word order information.
However, linear precedence has been shown to be a significant factor affecting the manner in which words associate with each other[Church & hanks , 1989].
Indeed, this is particularly relevant to a run-time recognition application, since data can only be meaningfully processed in one direction anyway (i.e. left-to-right).
Consequently, the next phase of collocation acquisition will be to create a set of uni-directional collocations and compare them with their bi-directional equivalent.
Finally, the current technique makes no use of distance information.
Clearly, there are some collocations that are independent of distance, but there are others whose behaviour is highly distance dependent [Jones & sinclair , 1974].
It is appropriate that future system development should exploit this constraint.
The acquisition of collocational information is still somewhat problematic.
Whereas definitional information can be obtained from LDOCE for some 55,000 headwords, the acquisition of a similar number of collocational entries requires the processing of an immense corpus.
The 5 million-word subset of the Longman Corpus can only reasonably provide a collocation dictionary of some 12,331 entries, allowing for reduction, repetitions and the low frequency of occurrence of some words.
To provide anything like the coverage given by the LDOCE, a corpus of much greater than 5 million words is necessary [Jelinek, 1985].
It is suspected that the issue of lexical acquisition will form the basis of further studies.
Chapter Five
System Integration
5.1 Introduction
The most successful text recognition system to date is that of the human information processing system.
Its principal strengths lie in the ability to (a) make selective use of available visual cues (for fluent readers much of the visual stimulus remains unattended [Just & Carpenter, 1987]) and (b) utilise an understanding of the text that can guide the reading process and compensate for any degradation or ambiguity within the visual stimulus.
This is possible because word images occur within a meaningful context, and we are able to exploit the syntactic and semantic constraints of the textual material [Rayner, 1983].
Analogously, computerised handwriting recognition can be enhanced by using such higher level knowledge: for both printed and handwritten input, the stimulus alone is not enough to unambiguously identify the text.
This is not to say that adequate recognition cannot be achieved without understanding, but rather an appreciation of the processes involved in understanding may facilitate the design of more efficient recognition algorithms and systems.
The conspicuous gap between the reading performance of people and that of algorithms may reflect the fact that few text recognition systems utilise the many knowledge sources or recognition strategy of the human reader [Hull, 1987].
Evidently, if the design of text recognition systems is to be at all inspired, it may as well be by the best natural example available.
Ramsay [1987]argues that two types of knowledge are used in the process of understanding:linguistic knowledge (i.e. knowledge about language itself), and world knowledge (i.e. knowledge about the world in general).
To some extent, this represents the current state of NLP system development: there are several examples of programs that demonstrate the effective application of some aspect of linguistic knowledge, but very few practical theories regarding the use of world knowledge.
This dichotomy becomes further evident upon closer analysis of the components of language processing systems.
5.1.1 The Components
To design a natural language processing (NLP) system, whether for text recognition, machine translation or some other application, it is necessary firstly to identify the requisite components, and then to specify how they must interact.
Table 5.1 shows the levels of knowledge required by a typical system (in this case for understanding and interpreting English [Ramsay, 1987]).
These levels need not necessarily correspond to autonomous modules within any computational implementation, but rather organised according to whatever configuration is most appropriate for the particular application.
Indeed, the issue of autonomy remains highly contentious [Cairns, 1984].
Let us consider each level of knowledge individually.
The lower three levels (lexical analysis, morphological processing and syntactic analysis) have been investigated during the present project, and are described in detail elsewhere, e.g. Wells [1992]and Keenan [1992].
Similarly, the use of semantic knowledge is described in other sections of this thesis.
Together, these levels represent the linguistic knowledge sources defined in the earlier dichotomy.
Clearly, the work on these lower levels is far from complete, but the contrast between these and the higher levels (in which no investigations have yet been made) is conspicuous.
Some would argue that above semantics lies a level concerned with the use of language in its social context.
This level, referred to as pragmatics , takes into account the purpose of language in achieving pragmatic ends, such as persuading or requesting information [Greene, 1986].
However, for reasons of clarity and simplicity, this level will be regarded as being implicit within discourse rules and (to a lesser extent) world knowledge.
No attempt has so far been made to incorporate the two higher levels (discourse rules and world knowledge) within the present project.
This reflects both the purpose of the current system (i.e. recognition rather than understanding), and the current state of NLP system development — i.e., the lack of practical theories regarding the use of discourse and world knowledge.
For these reasons, it is appropriate to address these areas specifically in this section.
5.1.1.1 Discourse Rules
The meaning of a connected set of sentences is greater than the sum of their individual meanings.
Consider the following discourse:
Harriet was hungry.
She walked over to the fridge.
Human readers have little difficulty in following the focus of this text, due to their ability to make elaborative inferences and recognise the thematic links that bind the sentences together into a cohesive whole.
The presence of these links is often signalled by explicit cues within the text.
For example, when a human reader sees a word such as ’ hence’,’therefore’or’thus’, they interpret it as a signal that the next sentence will express some consequence of what has just been said [Brooks & warren , 1970].
Similarly, when they see words like’ however’or’but’, they interpret it as a signal that the next sentence will express something opposing what has just been said.
These words, and other connectives like them, constitute lexical cues within text that help the human reader to maintain the coherence of a discourse (although inferences can still be made without such cues).
Additionally, human readers are able to make a multitude of inferences about the sentences within a discourse.
For example, backward inferences enable a reader to refer some new information in a sentence to something implied by an earlier sentence, and forward inferences allow the reader to embellish the representation of the currently read text, and create expectations about what is to follow [Clark, 1975].
This draws attention to another aspect of human discourse processing: the ability to construct the referential representation of a text or sentence.
At the sentence level, this can involve tasks such as pronominal reference, which exploits linguistic information such as gender, number and case.
At the level of connected sentences, it involves the identification of focused elements (i.e. those which are more thematically central to a text)[Chafe, 1972].
This enables the reader to constrain their inferences and expectations to those that are the most relevant.
At the level of a complete text, the main theme may be extracted by constructing high-level generalisations or abstractions [Kintsch & van Dijk, 1983].
Discourse rules constrain the structure of a particular text genre (descriptive, narrative, exposition, etc.) and the purpose of that text (informative, entertaining, persuasive, aesthetic, etc.)
[Brewer, 1980].
They enable the reader to activate the relevant template for the text and create expectations of how the text should progress.
Apart from perhaps the use of story grammars [Mandler & johnson , 1977]and the concept of macrostructure formation [Kintsch & van Dijk, 1983], very little progress has been made towards the implementation of discourse rules in NLP systems.
5.1.1.2 World Knowledge
Much of the information that makes a text coherent is not included in the text at all but resides in the world knowledge shared by the author and most of the readers.
The importance of such knowledge becomes apparent when attempts are made to produce computer programs that can understand text [Schank & abelson , 1977].
This knowledge has many facets: knowledge of people (their needs, wants, attitudes, values, plans, etc.), knowledge of physical laws, knowledge of cultural and social laws, etc.
The’ Harriet was hungry’example may be only nine words long, but it nevertheless demonstrates many aspects of knowledge that need to be made explicit to a computer before it could be said to understand.
For example:
Human needs
: the computer has no implicit notion of Harriet's status as a sentient being, nor her concomitant need for sustenance;
Physical laws
: the computer has no knowledge of the biology or chemistry involved in the digestive process, and hence no conception of the way in which the intake of food satiates hunger;
Cultural laws
: the computer has no knowledge of the cultural norms that would identify the fridge as being a likely repository for food.
There is also knowledge of specific content domains, such as the events and objects involved in attending a lecture or visiting a restaurant.
In the latter case, the knowledge may include likely events such as sitting at a table, reading the menu, ordering, waiting, being served, eating, paying the bill and so on.
Attempts have been made to represent such knowledge using a structure known as a script [Schank & abelson , 1977].
Scripts can be thought of as slot-and-filler structures, in which the slots have default values so that events can be inferred even when they are not mentioned explicitly in a text.
They aid understanding by imposing an organisation on the information in the text, and providing any extra information required to maintain its coherence.
The acquisition, representation and use of world knowledge are all highly problematic issues, involving many (yet) unanswered questions.
For example, how can such a vast quantity of knowledge be acquired?
Which pieces of knowledge are relevant to a particular system?
How should knowledge be represented, and within what framework should the inferences be made?
How should expectations be passed between levels, and how strongly should they influence other processes?
Until these questions are answered, no computer can be said to understand language.
Consequently, recognition systems may not show the speed, adaptability and flexibility of the human system until they do.
5.1.2 Interaction between Components
The interaction between the various components involved in recognition (or understanding) is one of the major problems facing the NLP system designer.
Steps towards its solution often begin with considerations of the I/O of the individual modules.
In the present project, the semantic analyser has been developed to take input in the form of word candidates, and output those words with their associated scores.
Consequently, the semantic analyser can be applied to any system within which word candidates are produced: handwriting, OCR, possibly even speech systems (see Appendices D & e ).
The same applies to the syntax analyser, so these two modules can run independently, in parallel if necessary, producing their own sets of results.
The lexical analyser (which includes morphological processing) has been designed to accept input in the form of a character lattice (see Appendix C), so this can work with any recogniser that produces output in this format.
Consider the use of these modules in the design of an integrated OCR system.
Given a TIFF file as the starting point, the data flow and processes could be organised as in Figure 5.1.
The’ voter’constitutes a module in which results are combined and a unique solution identified.
This design has actually been implemented using a network of transputers [Sherkat et al, 1993].
The architecture is such that processing begins in each module as soon as data becomes available, i.e. partial results flow along pipelines between the modules so that all may work simultaneously whenever possible.
It is arguable that the design of the system should be dictated by the optimal information flow, in which case the simplest approach would be to use these sources of knowledge in a serial fashion, with a uni-directional flow of information.
In the case of text recognition, this would mean a bottom-up flow of data and results, working from the character recognition stage to the application of world knowledge.
However, this approach has severe limitations, since at any level there are alternative interpretations (i.e. ambiguity) within the data for which the appropriate information is instantly available at some other level.
There are two main alternatives to this design: the blackboard approach and the constraint satisfaction approach .
In the former, the’ blackboard’refers to a neutral working area in which partial results may be stored and hypotheses offered to higher-level components.
The HEARSAY project [Erman et al, 1980]used this architecture and has proved to be highly influential (albeit less than completely successful) as an example of collaboration between different levels of processing.
The system can understand spoken utterances by simultaneously analysing them at different levels (including syntax and semantics), and then combining the results.
However, such an approach incurs serious implementational difficulties concerning (a) the format of entries on the blackboard,(b) its coherence as results are added and deleted, and (c) control over the resources available to each module.
The latter approach attempts to carry these ambiguities around in the form of constraints.
In so doing, multiple interpretations are held simultaneously, and resolved when further data restricts those alternatives to just one unique solution [Stefik, 1981].
Neither approach seems entirely satisfactory, although the latter may have the advantage of facilitating a simple architecture, with a uni-directional flow of information from the bottom upwards.
The problem is how to represent the knowledge at each level as multiple constraints that may be mutually conflicting.
The OCR system illustrated above incorporates aspects of both approaches, since the information flow is uni-directional, but includes an element of parallelism and a neutral area (the voter) in which results are collected and resolved.
An important point concerning all modules apart from the character recogniser is that they are effectively acting as filters in this context, i.e. they cannot contribute further interpretations of the data.
The character recogniser is the only module that actually suggests possible interpretations of the input — the other modules merely work on these suggestions, eliminating various possibilities or modifying their plausibility each time.
In a genuine top-down system, hypotheses would be made concerning the expected input, and part of this process would be the contribution of possible interpretations from the higher levels.
This issue constitutes an important aspect of the design of integrated systems, and one that is discussed more fully in Chapter Seven.
Even if the issue of architecture is resolved satisfactorily, or determined by some other overriding consideration, there still remains another problem: the combination of results.
How should the importance of each knowledge level (relative to the others) be determined?
One possible solution would be to represent the relative influences of each analyser as a set of numerical weightings.
For example, lexical knowledge may be twice as’ important’as syntactic knowledge, which may in turn be twice as’important’as semantic knowledge (evidently, these magnitudes will vary according to the specific application and particular data).
In this case, the relative weighting of lexical:syntax:semantics could be 4:2:1.
These weightings may then be adjusted relative to the pattern recogniser.
Indeed, these weightings could possibly be adjusted’ on the fly’, according to the degree of confidence associated with each analyser.
However, the assignment of confidence ratings to the output of each analyser remains a highly contentious issue, and one that is discussed at greater length in Chapter Seven.
An empirical investigation could provide some answers to this question, by testing a variety of permutations, running the system using a given input, and measuring the overall recognition rate for each permutation.
Evidently, there are many aspects to the question of integration.
A comprehensive study (although highly desirable) would involve much time and effort, and is outside the considerations of this thesis.
However, the importance of the semantic analyser relative to the other modules constitutes a narrower issue and one that can be investigated in a reasonable period of time.
One of the simpler aspects of this is the relative importance of syntax versus semantics.
To investigate this, both modules were integrated in the same program and relative weightings were adjusted using a variety of permutations.