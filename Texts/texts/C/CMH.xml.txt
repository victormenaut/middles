

Introduction
In The Hitch-hiker's Guide to the Galaxy the author, Douglas Adams, describes a planet inhabited by a race of hyperintelligent pan-dimensional beings who build a hyperintelligent computer to help them come up with the answer to Life, the Universe and Everything.
After many millions of years of deliberation this machine finally delivers its verdict: the answer is forty-two.
Naturally the heirs to the computer-builders are somewhat disappointed about this.
After all, one expects better value for money for one's computer time.
In its defence the computer points out that the problem lies not in its answer but in the question it was asked in the first place.
In order to understand the answer they first have to understand what the question really means, and that, they are told, will require an even more powerful computer!
All of which is a roundabout way of saying that what matters in this world is asking the right questions, rather than knowing the right answers.
It is in this spirit that this book is intended to add to your education in psychobiology.
It is a book that focuses on the questions that psychobiologists ask, rather than on the answers that they give to them.
It is concerned with telling you as much about what we don't yet know as about what we do know.
Psychobiology, the study of the biological bases of behaviour, is a broad area covering everything from the evolution of mating systems in the toad to the functions of subregions of the human cerebral cortex.
In a book of this length it isn't possible to cover everything, so let me declare my hand right away: what interests me most is the brain.
Specifically, what fascinates me is how the operations of the billions of nerve cells that cohabit inside our skulls to form our brains are capable of giving rise to our actions and our conscious experience.
This problem, so easy to state, is undoubtedly the greatest scientific challenge of our time, which probably explains why some of our most able scientists are turning their attention to it.
Psychobiology is particularly exciting at the moment because advances in instrumentation are bringing us new ways of studying the brain while areas like artificial intelligence are providing new ways of thinking about how it might work as the ‘organ of behaviour’.
One of the frustrations of learning about psychology and psychobiology is that the practitioners frequently behave in what appear to outsiders as totally illogical ways.
This usually takes the form of obsessively pursuing the minutiae of experimental phenomena and theories that leave a subsequent generation cold.
Someone reading the latest work on the control of food intake, for example, might be puzzled at the amount of effort that went into understanding the effects of hypothalamic lesions on eating and at the theories that were erected around those experiments.
To appreciate why certain questions are currently preoccupying psychobiologists, why they favour the particular answers that are in vogue, and why they seem to have neglected, until recently, many of the issues that non-psychologists would consider central to understanding the biological bases of behaviour, a short lesson on the history of psychobiology is relevant.
The history of psychobiology
Contemporary psychobiology is a blend of two traditions of research and theory.
One starts from the question of mind and behaviour and asks: ‘How is it possible for a physical system, the brain, to produce this?’
In other words, the focus is on psychological issues.
The other starts from the brain and asks: ‘What does this organ do?’
In other words, the focus is on neurological issues.
For many years these traditions operated independently of each other but recently they have converged to give the field a much more integrated look.
The psychological tradition
Psychobiology literally means the ‘biology of mind’.
As such its history can be caricatured as having had three stages.
In the first the academic community denied the possibility of a biological basis of mind at all, in the second it denied the existence of mind and in the third, into which we have now entered, we are coming to accept both the existence and physical basis of the mind.
These changing attitudes in part reflect changes in attitudes in the world at large but they also stem from the evolution of our concepts of ‘mind’ and its possible physical bases.
For example, the increasing secularization of the western world means that there is less pressure to accept dogma, based on prescientific thinking, about the status of the mind.
The development of electronic devices like digital computers that can have mind-like properties means that a physical basis of mind is much less improbable to us.
All scientists work through analogies with other systems that they already understand.
The problem for would-be physiological psychologists is that until relatively recently there have been no other natural phenomena or man-made devices that we understand better than human behaviour that  could act as a model or analogy.
Indeed, for most of human history we have tended to do the reverse, to use the analogy of the human mind to explain what happens in the physical world, an approach known as ‘animism’.
By the seventeenth century, however, engineers had become sufficiently skilled at making complex mechanical devices, powered by clockwork or water, to be able to make toys that moved in a fairly convincing approximation to the way that people and animals move.
Some were constructed so that they would only move when a passing person activated a treadle of some sort so that the toy would suddenly spring into ‘life’.
The French mathematician Descartes remarked on the similarity between this sort of toy and the behaviour of animals, arguing that the latter were nothing more than automatons whose reactions were constantly triggered by events in the environment (Jeannerod 1985).
Not knowing about electricity, but knowing about hydraulic systems, Descartes suggested that the control of these reactions in animals was mediated by movements of fluids, initiated in the sensory nerves by stimuli, being carried to the ventricular system of the brain where contact was made with the motor nerves.
The movement of fluid in the motor nerve then caused the actual movement.
Since there is a large number of nerves the number of possible combinations and sequences of stimuli is enormous so it is possible for a system like that, in principle, to produce a very large number of possible reactions.
Descartes was reluctant to extend this model to human behaviour.
In part this reflected religious scruples since mechanical explanations for our behaviour were incompatible with religious teaching.
It also reflected philosophical scruples since it wasn't obvious how a mechanical device could be conscious and, as Descartes is famous for pointing out, the only thing of which we can be certain is that we are conscious.
Although Descartes was wrong to use a hydraulic analogy, his idea that animals come pre-equipped with a range of motor responses to sensory input, reflexes, proved correct.
In the nineteenth and early twentieth centuries reflexes were studied with enthusiasm.
This work demonstrated two things.
The first is that reflexes are largely mediated by the lower parts of the brain, and the second is that they are less rigid than one might think.
Both points can be illustrated by studies on frogs.
If the brain is removed from a frog but its spinal cord left intact it will still display reflexes including defensive reactions.
One of these involves using the hind leg to scratch an area of skin to which an irritant has been applied.
Furthermore, if the animal is prevented from using one leg, it will scratch with the other.
Thus a sophisticated reflex is mediated by the spinal cord and the ‘reflex’ appears to be goal-directed.
If sophisticated reflexes can be mediated by the spinal cord, what is the purpose of the brain?
William James (1950) argued that it was there to elaborate on reflexes, a position largely adopted by the behaviourist school of the early to middle twentieth century.
A Cartesian reflex model holds that behaviour will be constant in a constant environment or, if not constant, it  will not change in a predictable way.
Thorndike (1913) and Pavlov (1927) showed that that is not so.
The behaviour of man and other animals does change lawfully, under the impact of reinforcement contingencies.
It therefore seems reasonable to suppose that the function of the brain is to mediate these more flexible links between stimuli and responses.
Stimulus-response (S-R) psychology has largely gone out of fashion now, yet in its day it carried the field, and even now its influence is still felt.
For example, when describing experiments psychologists still refer to the subjects' ‘response’ to the ‘stimulus’materials.
Neurobiologists too work within this framework.
For example, workers like Kandel and his colleagues (Hawkins and Kandel 1984), who are interested in the cellular bases of memory, are actively studying ‘associative’ learning tasks like classical conditioning in the belief that this represents a fundamental mammalian learning process.
The attraction of S-R psychology, in its most radical form, lies in two things.
The first is its simplicity; it reduces the whole of psychology to the study of learning.
The second is that it unites psychology and biology since, in the early formulations, ‘responses’ were contractions of muscle groups, stimuli were physical events occurring at sensory receptors, and ‘learning’was a real event occurring in the brain.
This then set the agenda for how we study the brain; we treat it as a large reflex arc and trace the circuit from the stimulus ‘analysers’ to the motor system.
This attitude is especially apparent in Pavlov's writings.
Indeed, Pavlov believed that the study of classical conditioning was the only way to study the functions of the cerebral cortex.
S-R psychology didn't have things all its own way.
Even in its heyday it was challenged by the Gestalt school (Koffka 1935), mainly working in Germany, who pointed out that there were real perceptual phenomena that were quite incompatible with the more radical versions of S-R theory.
What the psychologists showed was that stimulus elements in groups had properties not present in the individual elements.
Moreover, those properties are often distortions of the true appearance of objects.
That is to say, lines which are physically straight will appear bent, lines that are the same length will appear different, exposure to one stimulus will alter the appearance of one presented subsequently, and lines and edges that are not physically present will be seen by the subject.
The consistency of these distortions and the immediacy with which they occur convinced the Gestalt psychologists that they were dealing with a fundamental property of the brain rather than something that we have learned.
Using an analogy fashionable at the time the Gestalt psychologists maintained that these distortions were due to interactions between ‘electric’ fields induced in the visual cortex by the stimuli.
As was the case with Descartes's hydraulic model, the electric field model has not been substantiated by more recent studies of the brain.
Nevertheless, the insight that the neural representations of stimulus elements interact with each other in the brain to distort our perception of the world  has been validated again and again by sensory physiologists.
Clearly the brain is more than a passive relay from stimulus to response.
Gestalt psychology is really only a minor nuisance to S-R psychology.
The real problems came from studies by people like Tolman (1932), Crespi (1942), and Lashley (1963), who showed that behavioural change can come about too rapidly for incremental learning to explain it and that what animals learn is not a set of reflexes but the location of desirable objects and events.
During the 1930s the most commonly used piece of laboratory apparatus was the maze, often modelled on the one at Hampton Court.
Rats were trained to negotiate these mazes to obtain food.
It was assumed that the food reinforcement in some way strengthened the response tendencies that led to the food, at the expense of other response tendencies.
Tolman showed that rats would learn to negotiate mazes without experiencing reinforcement at the goal-box, although the introduction of reinforcement was necessary to persuade them to display their knowledge.
Learning of this sort was termed ‘latent-learning’.
Crespi showed some surprising effects when you alter the amount of reinforcement a rat is given to run down an alley way.
If you increase the amount of reward the rats run faster than rats that have always received the large reward and if you decrease it the reverse happens; the rats run more slowly than those that have always had the small reward.
Furthermore, the change in speed occurs within a couple of trials of changing the magnitude of reward, which is too fast for conventional learning mechanisms.
Lashley, for his part, showed that surgical damage to parts of the brains of rats could alter the types of movements they would make to negotiate a maze without disrupting their ability to reach the goal.
In some of his experiments the rats were rolling along to get to the food in the goal-box.
Experiments like these provide a fairly conclusive demonstration that, even in simple learning tasks, animals are learning about the nature and location of biologically important events like food.
Their behaviour is a very flexible result of the interaction between that knowledge and their needs, such as their need for food.
They are not forming stimulus-response associations.
During his long and distinguished career Lashley (1950, 1951), in fact, carried out a fairly effective hatchet job on S-R psychology.
In addition to demonstrating the ease with which rats substitute one movement for another in reaching their goal he also demonstrated that cutting the nerve fibres that join the visual to the motor cortex does not interfere with associative learning, as Pavlov had predicted it would.
He presented a detailed theoretical analysis of skilled movement, showing that the sequencing of actions, the serial order of behaviour, could not be due to feedback stimuli from one movement triggering the next, as S-R theory argued, but had to be due to central programming of the sequence.
Unfortunately, Lashley's achievements were largely negative.
Along with people like Tolman and Crespi he showed the inadequacy of the S-R model  without really replacing it with anything of use to the psychobiologist.
As a consequence, psychobiologists turned away from these sorts of issues and concentrated on supposedly simpler problems like emotion and motivation.
Even while the behaviourist debate had been raging, people had been studying emotion.
Indeed, as William James implied in 1884, emotion is a serious challenge to reflex models of behaviour because it implies experience without action.
James's solution to this problem was simplicity itself.
He redefined emotions as sets of bodily reactions which could, in turn, effectively act as stimuli to control further behaviour.
In other words, emotions are the product of behaviour.
Even if it isn't true, James's position was an excellent stimulus for research as it forced people who would adopt an alternative position to consider very carefully how else emotions might come about.
Cannon and his co-workers launched a fierce attack on James.
More accurately, they launched an attack on the idea that activity in the autonomic nervous system was a sufficient condition for emotional experience.
The attack was based on a number of strands of evidence, including the fact that activation of the sympathetic branch of the autonomic nervous system was too diffuse to underpin the range of subtle emotional experiences of which we are capable, and the observation that severing the spinal cord in dogs does not prevent them displaying facial signs of emotion when provoked in an appropriate way.
As an alternative, Cannon proposed a centralist theory in which emotion was seen as the result of activating specific mechanisms in the central nervous system.
However, his evidence didn't permit particularly precise localization within the brain.
It wasn't until Papez (1939) published his theory of emotion that the field really moved again.
Papez set the agenda for how emotion would be studied for half a century.
This is remarkable, given how little evidence he really had for his theory and how much of the evidence he had was wrong!
The basis of his argument is that emotional experience and emotional behaviour involve separate, although interlinked, parts of the brain.
As Cannon and his colleagues had already shown, animals lacking cerebral cortex could still display emotional behaviour.
Indeed, their emotional behaviour was often exaggerated.
Papez therefore located the emotional behaviour mechanisms in the brainstem, especially in the mammillary bodies which, he believed, received sensory information via subcortical relay routes, and cognitive information from the cerebral cortex via the hippocampus and its subcortical projection system, the fornix.
Emotional experience, on the other hand, he thought was the result of activating specific areas of the cerebral cortex in much the same way as activating other areas produced visual or auditory experience.
On the basis of some remarkably weak clinical evidence he decided that the site of emotional experience was the cingulate gyrus, which lies on the medial surface of the hemispheres.
All of the areas that Papez implicated in emotion are part of an anatomically identified system known  as the limbic system.
Many people, therefore, encoded his conclusion to be that the limbic system was the seat of emotion.
As we shall see in a later chapter, the issues raised by Papez have yet to be resolved.
There are still those who believe that the limbic system is the source of emotional experience, while others have implicated parts of the circuit in spatial ability (McNaughton and Morris 1987) or memory (Olton 1983).
The major developments in psychobiology to take place in the 1940s and 1950s concerned motivation.
Motivation is inextricably linked with most S-R theories, explaining both why learning takes place and why animals engage in behaviour.
Thorn&e explained learning in terms of the action of reinforcement.
Positive reinforcement was seen as being due to the presentation of ‘satisfiers’ but that is, of course, completely circular without being able to predict in advance what will satisfy an animal.
This problem was largely solved by explaining reinforcement in terms of drive reduction, since drives could be readily manipulated by depriving animals of food or water and then using these items as reinforcement.
‘Drive’ was also called upon to explain variations in behaviour that could not be explained in terms of learning; but what is drive?
Following on from Cannon (1947) most psychologists assumed that drive was simply bodily discomfort brought about by the deprivation state and that drive reduction was due to eliminating this discomfort.
Again, this is brilliantly simple and probably wrong.
It is wrong because you can eliminate all of the possible sensory mechanisms that might detect the tissue disturbance produced by a particular form of deprivation and motivation still persists.
For example, cutting the nerve supply to the genitals does not interfere with sexual motivation in the short term; cutting the nerves to the stomach does not interfere with hunger.
This led Morgan (1943) to argue that drive must be due to activating a central nervous system mechanism that represents the drive state, much in the same way as Papez was claiming that activation of the cingulate cortex represented emotion.
Morgan did not know where to locate the source of this ‘central motive state’ in the nervous system.
It was Stellar (1954) who put the finger on the hypothalamus as the seat of motivation, basing his arguments on a number of lesion studies that had shown quite specific disturbances of motivation after lesions in this region.
It is a testimony to Stellar's insight that, even now, no discussion of the neural bases of motivation is complete without a discussion of the hypothalamus.
The 1950s were a watershed for mainstream psychology.
They were the era when psychologists rediscovered mind, or, to be more accurate, mental processes like attention and memory.
This was hardly accidental.
In part it came about as a reaction to the inordinate complexity of S-R theory but, I believe, it had much more to do with S-R theory's failure to cope with real psychological problems like the performance of radar operators, and with the availability of machines, computers, with mind-like properties that made it  respectable to think in mentalistic terms again.
The rediscovery of mind had relatively little impact on the psychobiology of the time.
If one looks back at the text-books and review papers written about psychobiology during t, his period one finds that they were largely preoccupied with topics like motivation and emotion.
Some workers, like Hernandez-Peon (Hernandez-Peon, Scherrer, and Velasco 1956), began to explore the neural bases of attention, but their approach was tightly linked to the topic of motivation and emotion through the prevailing arousal theory that was gripping people at the time.
It is true that a number of books were written that attempted to draw parallels between brains and computers but these largely served to remind us how different they really are.
In fact it was largely people working outside the framework of experimental psychology who kept alive research into the physical bases of mental processes.
It is to these people, who worked in the neurological tradition, we turn next.
The neurological tradition
Although our commonsense view of the world now tells us that the brain is the seat of mind and action, the idea is remarkably new.
Even up to the end of the eighteenth century it was widely held that mind resided in the spaces within the brain, the ventricles, rather than in the neural tissue itself.
To be fair to the scientists of the time the brain, as it appears when freshly removed from the skull, is hardly an appealing or inspiring sight!
It looks like a sagging, browny-grey jelly that fairly rapidly decays into an unpleasant mush.
Only when it is exposed to chemicals that harden the tissue and inhibit decay, a method known as fixation, is the brain easily studied and only then will it reveal the complexities of its organization.
By the end of the eighteenth century a number of facts about the brain had emerged.
The brain contained two sorts of tissue, grey and white matter.
The outer mantle, the cerebral cortex, was folded into a pattern of sulci and gyri that was remarkably consistent from brain to brain and, within the cerebral cortex, some parts were anatomically distinct from others.
This last point raised the interesting possibility that not only is the brain the seat of the mind but also, different parts of the brain might be involved in different aspects of mind.
Nevertheless, none of the data really substantiated the imaginative tour de force of Gall and Spurzheim (in Kolb and Whishaw 1985) who, at the turn of the nineteenth century, argued that the human cerebral cortex is subdivided into functional areas, each responsible for a different mental faculty.
Although the details of their theory are now seen to be clearly wrong, their insight, that different parts of the brain are responsible for different processes, has stood the test of time and research and informs much of our current understanding of the brain.
The century and a half of research that followed was largely fixated on testing out this hypothesis of ‘localization of function’.
In truth, Gall and Spurzheim's theory of localization was ahead of its time because there were no suitable methods available for testing it, although test it people did.
For a start there were no agreed criteria for subdividing the cerebral cortex into regions, and criteria were not destined to be developed for about another century.
Furthermore, there were no psychologists around to devise tests of mental faculties.
Advocates of localization were in the same position as the explorers of the sixteenth century; they knew there was something out there but they didn't know where it was or what it might look like.
Neurologists in the nineteenth century studied the effects of surgical removal of, or accidental damage to, the cerebral cortex in man and other animals without knowing the boundaries of the functional units they were proposing or their likely functions.
Under the circumstances it is astounding that people managed to reach agreement so quickly.
While some of the clearest evidence for localization of function came from neurological studies on humans with damaged or disturbed brains, some of the best evidence against it came from experimental studies on animals.
Broca (1961, in Kolb and Whishaw 1985) had reported that a series of patients with severe disturbances of speech had all suffered damage to the inferior part of the third frontal convolutions of their left hemispheres and Jackson (in Kolb and Whishaw 1985) had described cases of ‘petit mal ’ epilepsy which involved only a limited number of ‘faculties’.
On the other hand, Flourens (in Phillips, Zeki, and Barlow 1984) removed various parts of the forebrains of birds and found that the degree of disturbance of behaviour was very much more a function of how much brain was removed than which part was eliminated.
Even in 1881 Goltz (in Phillips et al .
1984) was claiming that motor function invariably recovered from damage to the cerebral cortex.
It was not until the 1880s that there were consistent experimental findings to support localization.
By then Fritz and Hitzig (in Phillips et al .
1984) had demonstrated the existence of ‘electrically excitable motor cortex’ in dogs and monkeys and Ferrier (in Phillips et al .
1984) had shown that a sufficiently extensive ablation of the appropriate areas of the cortex would produce complete and abiding paralysis in experimental animals.
By the time of Fritz and Hitzig's work it had been known for nearly a century that electrical stimulation of nervous tissue would produce movements.
The importance of Fritz and Hitzig's work lay in their demonstration that with brief, low-intensity electrical stimulation it was only possible to evoke movement from a restricted area of the cerebral cortex, hence the name ‘electrically excitable motor cortex’.
Ferrier showed that if the whole of this region was removed, then paralysis ensued, whereas sparing any of this region allowed the recovery of motor function.
In a highly dramatic demonstration during a meeting on localization function held in London in 1881 Ferrier demonstrated that the recovery of movement described by Goltz in his studies was due to sparing part of this motor cortex.
By the turn of the century it was accepted that certain parts of the brain were specialized for either sensation or movement but, as Brodmann (in Kolb and Whishaw 1985) demonstrated, there are large areas of the human cerebral cortex that are neither obviously motor nor sensory.
Moreover, these ‘association’ regions consisted of a number of distinctive regions that could be identified on the basis of their internal cellular organization.
By this time, clinical neurologists had begun to catalogue the effects of damage to association cortex, describing deficits in speech comprehension, object recognition, and spatial orientation, depending on which part of the brain turned out to be damaged.
Experimental studies on animals tended to lag behind the neurologists' at this time and for many years to come.
In fact, it could be argued that the only significant finding on association cortex, made using non-human animals up to 1950, was the memory impairment in monkeys lacking frontal cortex described by Jacobson (in Gross and Weiskrantz 1964).
This is not totally surprising.
Using animals in experiments requires us to make inferences about psychological processes from behaviour.
This is virtually impossible from the day-to-day behaviour of animals in the laboratory, and the theoretical preoccupations of most psychologists distracted most of them from thinking about devising the sorts of formal test procedures we now use.
Those, like Lashley, who were developing specialized test procedures for use with animals were either using them to explore general psychological capacities like ‘intelligence’ or were testing predictions from learning theories.
It is only in the last thirty-five years that animals had been allowed to have ‘mental’ processes other than learning and so it is only in that time that psychologists have developed the behavioural tests to investigate them.
Studies like those described by Dean (1982), in which he attempted to distinguish between deficits in visual memory and visual object categorization following damage to part of the visual association cortex in rhesus monkeys, would have been unthinkable forty years ago.
Neurologists expressed interest in the psychological consequences of brain damage long before psychologists got involved in this field.
Indeed, clinical neurology predates the emergence of psychology as an independent discipline.
They looked at psychological processes simply because they were the only way of monitoring what the brain might be doing.
The deepening understanding of how brain cells work, that developed during the late nineteenth and early twentieth centuries, changed the situation dramatically.
One significant development was the emergence of the neuron theory that held that the brain was composed of discrete cellular elements, the neurons, that were physically separated from each other by gaps called synapses.
This opened the issue of how nerve cells might communicate with each other and eventually led to our now-sophisticated understanding of neurotransmitters.
A second crucial development was the discovery that nerve cells are  spontaneously electrically active.
Although electrical excitability in the nervous system had been demonstrated before the beginning of the nineteenth century, it was not until the late nineteenth century that it was shown that the brain was spontaneously electrically active Jeannerod 1985), and it was only in the 1930s, after the invention of the valve amplifier, that it was possible to make meaningful records of this activity.
Even then, these records were either the sum of the activity of many millions of nerve cells, as registered in the electroencephalogram (EEG), or the action potentials of single peripheral nerves.
Recording the electrical activity of single brain cells in mammals only became a viable proposition in the 1950s.
Many of the other facts of life that we now take for granted in psychobiology, like the identity of neurotransmitter substances, the actual structure of the synapse, and the availability of reliable methods for tracing connections in the central nervous system have a similar short history.
Seen from this point of view, what is remarkable about neurobiology is how much we know about the brain, not, as you might think, how little.
The lessons of history?
People have been studying the behavioural functions of the brain for something less than two centuries and, for much of that time, their efforts have been hampered by a combination of inevitable ignorance and adherence to implausible psychological theories.
The fact that any progress has been made is remarkable.
What progress we have enjoyed has come about either as a result of skilled, painstaking experimental work or inspired guesses about how the brain works.
Into the first category we can place the work of Ferrier and Fritz and Hitzig, who showed that previous ambiguous results in studies of experimental brain damage in animals were due to inadequately sized and placed lesions.
Into the second category we can place Papez's theory of emotion.
A number of historical accidents have shaped contemporary psychobiology.
Psychology's obsession with behaviourism owes far more to the fashion for logical positivism emerging from physicists in the 1930s than to any understanding of the needs of psychology.
According to the logical positivists, the only permissible scientific concepts are those that can be tied directly to measurement.
Anything that cannot be measured does not exist.
Since the only things that psychologists can measure are stimuli and responses, these are the only things they can include in their theories.
Undoubtedly behaviourism diverted people from thinking about fundamental issues, like the design of a brain that is capable of generating actions rather than reactions.
It also gave us the belief, probably misguided, that there are two fundamental learning processes: classical and instrumental conditioning.
The widespread belief that the limbic system is the substrate of emotion is also the result of an accident, in that Papez's theory of emotion was not  justified by the evidence available at the time and there are other, equally acceptable, interpretations of the effects of limbic system damage.
Historical accident is inevitable in science.
It doesn't mean that the science is poor, but it does mean that we need to be sceptical of the assumptions that we receive.
Often the most exciting work has come about precisely because those assumptions have been rejected.
Considerable progress has been made but a number of fundamental issues are still with us.
For example, we are still concerned about localization of function and questions about the neural bases of consciousness are again being asked.
Our measure of progress is that the way that we ask these questions has changed.
No-one seriously questions that different parts of the cerebral cortex are specialized to carry out specific functions.
They do question the likely degree of functional independence of these areas, given the amount of ‘vertical integration’ of cortical areas being demonstrated in recent anatomical studies.
They also question the relationship between the processes occurring in discrete areas of the brain and the psychological processes that are the end-product of neural activity in the brain as a whole.
Consciousness has, once again, been put on the agenda but the concept of ‘consciousness’ has been altered to fit in with ideas coming from fields like machine intelligence.
It is no longer something mystical but is seen by some as an internal commentary on our own behaviour (Gazzaniga 1985).
The general issues
Many of the issues in psychobiology are specific to the topics concerned.
Problems encountered in the study of perceptual mechanisms, for example, are distinct from those affecting the study of motivation.
Nevertheless, there is a thread of general issues that runs through this book.
They are:(1) the relationship between psychology and biology and the possibility of dispensing with psychology altogether once physiology has been developed sufficiently;(2) the value of studies on non-human species;(3) the degree of functional specialization in the sub-areas of the brain and the ways of analysing and describing those functions;(4) the way we are responding to the challenges of cognitive psychology; and (5) the importance of being able to explain what happens in the real world, rather than just the laboratory.
The relationship between psychology and biology
Psychologists often divide up their subject into ‘hard’ and ‘soft’areas.
Hard topics are supposed to have a large number of facts and a small amount of theory while soft topics have a large amount of theory and few facts.
To many psychologists psychobiology is the epitome of a hard area, a reputation based on the belief that the psychology is underwritten by a solid body of biological fact.
In reality biological knowledge is often no more secure than  psychological knowledge but psychologists often feel a sense of inferiority in the face of biological data.
This is exacerbated by the fact that there are some extremists who would argue that psychology is nothing but a crude way of approaching brain function that has been superseded by advances in physiological technique.
On the other hand, there are others who take the opposing position, that biological evidence is irrelevant to understanding psychological processes.
Neither view is tenable.
A complete description of the workings of the brain must include a statement of its behavioural functions, and this is impossible without a proper idea of what those behavioural functions are.
On the other hand, while it is true that it is rarely useful to explain behaviour in terms of neural events, our description of psychological processes must at least be compatible with what is known of their neural substrate.
Let me illustrate this point by introducing some recent work on the topic of visual perception.
There is a long and distinguished history of studying the visual system using ‘psychophysical’ procedures in which variations in subjective visual experience with variations in visual input are studied.
Perhaps the greatest achievement of this approach was the prediction by Young and by Helmholtz (in Mollon 1982), in the nineteenth century, of the existence of three classes of colour-sensitive receptor in the retina, each sensitive to a different wavelength of light, a prediction only confirmed by direct measurement in the last thirty years (Mollon 1982).
You might think that Young and Helmholtz were wasting their time using psychophysical procedures when improvements in physiological techniques would allow direct measurements of this sort.
To the extent that psychophysical measurements are indirect this appears to be a valid criticism but it does, in fact, ignore an important point about the psychophysical approach.
The point is that the psychophysical approach forces you to develop a model of how the system, in this instance the mechanisms of colour discrimination, might work and it is this model that is used to guide the direct physiological measurements.
In many ways the achievement of Young and Helmholtz was that they showed, in principle, that colour vision would be possible with only three receptors, providing those receptors were most sensitive to the right colours.
The model told people what to look for both when doing psychophysical studies and when carrying out the direct measurements of receptor function all those years later.
The impact of psychological theory on physiological research is apparent in studies of the visual system using the single cell recoding technique.
Individual brain cells are electrically active, producing the characteristic impulses known as ‘action potentials’ which can be detected by fine probes placed near to the nerve cells or their axons.
Cells in sensory pathways usually increase or decrease the rate at which they produce action potentials as a function of the type of stimuli presented.
Many physiologists believe that an exhaustive study of the types of stimuli that increase or decrease the  discharge rates of different types of sensory system cell will provide us with a description of how that system works.
There are just two problems with this programme.
The first is a simple logical point that without some model of how the system might work, there is no way of deciding which of the millions of possible stimuli available to use in testing a particular celt.
The second is a historical observation, that during the thirty years or so since the first observations of single cell response properties in the visual systems of mammals, our understanding of how the visual system works has been driven as much by theoretical developments in the psychology of perception as the other way round.
For much of the 1950s physiologists probed the visual cortex using the single cell recording technique.
At the time it was widely held that the early parts of the visual system acted like a closed-circuit television, with the eye acting as the camera and the visual cortex acting as the TV monitor.
Activity in the visual cortex would, therefore, be a point-by-point replica of what happened at the retina, with cells at all points registering the intensity of the image at that location.
A pessimistic interpretation of the findings based on this approach was that the visual cortex had nothing to do with vision, since few, if any, cells in that part of the brain showed any interest in spots of light they were presenting them with.
In 1959 Hubel and Wiesel demonstrated the reason for their failure.
Celts in the visual cortex responded very reliably to visual stimuli, providing they were bars or edges and providing they were properly oriented.
By solving a physiological conundrum Hubel and Wiesel immediately created a psychological puzzle: how is visual perception possible in a system that only relays information about the location and orientation of edges?
The psychological theory of ‘feature detection’ filled the gap, providing a context for interpreting these challenging findings on the visual cortex.
According to this model we recognize objects by comparing a list of basic elements of the visual image, like lines, edges, and angles, with a list, stored in memory, of the attributes associated with a particular object.
Thus the upper-case letter ‘A’ can be identified by the presence of two tilted uprights, an acute angle at the top, a horizontal bar, and two ‘T’junctions between the uprights and the bar.
One of the significant properties of this sort of model is that ‘features’ are very much an all-or-none attribute.
Either they are there or they are not.
This sort of theoretical framework encouraged people to study the responses of cells in the visual cortex by looking for ‘trigger-features’, using single stimulus elements and classifying their responses into ‘present’ or ‘absent’.
Feature analysis is an attractively simple way of explaining visual perception but, it turns out, ‘features’ are much more complex than we first thought.
In fact a ‘feature’ is an interpretation of part of the retinal image rather than an entity with its own physical reality.
Recognition of the problems inherent in identifying features led to new ways of thinking about vision and to new ways of analysing the ways celts in the visual cortex  respond to visual events.
One such development was ‘spatial frequency analysis’, which is a complex mathematical way of representing visual images without reference to features.
This approach led to a new series of studies in which visual celts were studied quantitatively using ‘grating’ stimuli instead of bars and edges.
A grating is a set of tight and dark stripes, usually of equal width.
Gratings can differ both in terms of how many stripes there are to the centimetre and the differences between the brightness of the light and dark parts.
Studies using these stimuli demonstrated further properties of visual system celts that had not been previously identified (Enroth-Cugell and Robson 1966).
In short, how we think about the way in which psychological processes, like visual perception, are carried out determines how we study the visual parts of the brain.
Analysis of psychological processes is not a second rate substitute for studying the brain directly but a necessary adjunct to guiding our explorations of that organ.
It is possible for outstanding physiologists to describe properties of the brain without reference to psychological theories, of course, but it is surprising how far the acceptance of physiological results depends on the availability of a psychological context in which to place them.
Animal models of man
Psychobiology is based largely on studies of behavioural and physiological processes in non-human animals.
Most people study psychology because they want to understand human behaviour and question the relevance of work on animals to their overall goal.
The time when scruples about extrapolating from studies on animals to humans would have seemed absurd, because it was widely held that basic behavioural processes were common to all species and that the complexity of behaviour was simply a function of the capacity of the organism to learn, is long since past.
The resurgence of interest in cognitive processes makes it impossible to hold such a simple view any longer and nowadays most psychologists recognize that there are psychological processes, like verbal communication and the symbolic representation of future events, that can only reliably be demonstrated in other people.
Since we believe that all variations in behavioural capacity reflect underlying variations in the nervous system, we have to accept that the human brain is, in some way, different from that of other species.
Psychological processes unique to humans are a serious problem for committed psychobiologists.
We can respond to this in two ways, either by abandoning studies on non-human animals and thus losing a major source of evidence, or by finding ways of making legitimate cross-species extrapolations.
The problems of making extrapolations arise at both the behavioural and the neurological level.
We are reasonably certain there are psychological processes unique to our own species.
There is a grey area of uncertainty about others.
This is  especially true in applied areas, such as the study of psychopathology, where we are uncertain whether animals experience the same problems as humans and, if they do, we are uncertain about identifying which disturbances in animals correspond to particular disturbances in humans.
For example, we don't know whether non-human species get depressed and even if we were sure that they did there would still be a lot of doubt about which behaviours signified depression in an animal.
The usual solution to this problem is to develop animal models of the human problem, in which animals are constrained to exhibit the sorts of disturbances that a particular scientist believes to be the salient feature of the human disorder.
For example, one scientist might think that the most crucial feature of depression is a failure to escape from noxious stimuli, in which case the ‘learned helplessness’ model would seem the most appropriate.
Another might think that it is a failure to respond to rewards, in which drug treatments that make animals insensitive to reinforcement would be the best model.
Needless to say, there is a continuous and lively debate about whose model is the best.
How can we justify studies on other species in terms of their relevance to man?
Our response to this problem is determined very much by our view of the evolutionary relationships between man and other animals.
A widely held view is that animals can be ranked on a scale, a ‘phylogenetic’ scale, based on their degree of similarity to humans.
This leads us to study animals that are most like humans and to restrict our studies to these animals, like the chimpanzee.
Here we run into three problems.
The first is a moral one, that if chimpanzees are really like man they may have the same capacity to understand what is happening to them and imagine their futures, which would make laboratory studies, especially physiological interventions, unacceptable.
The second is an evolutionary one, that the biggest difference between man and other apes lies in the development of the human brain.
The gulf between the human and the chimpanzee brain may be smaller than that between the human and the rat brain, but it is still large.
The third is a logistical one.
Chimpanzees are such an unsuccessful species that they are considerably outnumbered by neuroscientists.
Research would be impossible if we restricted ourselves to this species.
Not only is it impractical, and possibly unethical, to restrict psychobiological studies to work on humans and great apes but it would also mean throwing out most of the work done to date, since most of that has involved the use of non-primates like cats, hamsters, and especially rats.
Clearly we have to find some way of using these data without assuming that rats, for example, are just very small people.
The degree to which this is possible depends very much on how the brain has evolved.
The most pessimistic viewpoint is that the brains of different groups of mammals have very little in common because they evolved separately from a common ancestral form in which the brain was relatively formless, lacking the distinctive  sub-divisions that can be identified in the brains of modern mammals.
If this is the case we can do very little unless we are dealing with large blocks of brain that correspond to units in the primitive ancestral brain.
For example, since all mammals have a cerebral cortex we must assume that the ancestral form also had one.
This means that we can legitimately ask questions about the functions of cerebral cortex and just as easily study hamsters as humans to get our answers.
What we cannot do is ask questions about sub-areas of the cerebral cortex, for example the striate cortex of primates, and expect to get the same answers from studies on different groups.
The most optimistic view is that evolution has involved a gradual accretion of new structures, added on to primitive ones common to all mammals.
This means that there will be brain systems present in both rats and humans that can be just as easily studied in the former as in the latter.
In either event it is no longer safe to assume that non-human animals are just small versions of us, so direct extrapolations are going to be a risky business.
Knowledge of how the brain evolved is likely to reduce that risk, but it can never eliminate it completely.
Extrapolations are, therefore, best seen as working hypotheses requiring confirmation by studies made on people.
Indeed, this is precisely how the more insightful psychobiologists, such as Weiskrantz (1968) and his collaborators, carry on.
The point about studies on non-human animals is not that they replace studies on humans but that they provide us with pointers to what we should study in people and how we should study it.
The modular brain
It is now generally accepted that there is a division of labour within the brain, with different parts of the brain carrying out different functions.
As you will discover in the remaining chapters, problems remain about specifying the actual functions of these modules, their degree of functional independence, and the consequences of functional independence for the integration of behaviour and consciousness.
Take what we know about the neural mechanisms of language, for example.
Over a century ago Broca demonstrated that patients with damage involving part of the prefrontal cortex of the left hemisphere had severe speech deficits, although they could understand what was said to them.
Damage to the corresponding part of the right hemisphere had no effect on speech.
The immediate temptation was to identify this region of the left hemisphere, now known as Broca's area, as a speech centre, with the implication that this is all that is needed to generate speech.
This view is, of course, too simplistic.
Many other parts of the brain are important for normal speech.
Furthermore, although it is smaller, there is an area that corresponds to Broca's area in the right hemisphere that doesn't seem to have anything to do with speech but which seems to have many of the same  connections.
This suggests that there is nothing inherent in Broca's area by itself that makes it crucial for speech.
Rather, its importance results from where it lies in the circuits of the left hemisphere and the way that it processes the inputs it receives.
To understand why damage to Broca's area impairs speech we need to know both where it gets its input from and what it does to that input.
What it does to that input may not be susceptible to description in psychological terms.
Although the lesions that produce the speech loss are localized it would be a mistake to assume that their effects are similarly restricted.
Broca's area is part of a larger brain circuit and has connections with many other brain areas.
It is, therefore, possible that some or all of the effects of damaging this area are due to disruptive effects on these other regions.
Since the effects of the damage are largely restricted to language, the disruption cannot affect the whole brain, so in that sense we are justified in thinking of the brain as consisting of functionally independent modules.
Nevertheless, the disruptive effects of damage may spread to other structures intimately linked to Broca's area.
As a consequence, the functional modules of the brain may well be bigger than the individual areas of the cortex or subcortical nuclei that are the conventional units of analysis.
Since speech takes place in the wider context of non-verbal behaviour, we also need to understand the relationship between the speech mechanisms and the other mechanisms in the brain.
For example, we need to know how the speech system gets information from the systems that programme other movements so that we are capable of commenting on our own actions.
While driving my car I am able to say to my passengers that I am about to turn right before I have actually done so.
This requires integration between the systems that are programming my driving behaviour and those programming my speech.
One of the challenges of psychobiology is to explain how this integration is achieved (Gazzaniga 1985).
The cognitive challenge
Psychobiologists have reacted to the rediscovery of cognitive processes in two ways.
One has been to include cognitive processes in the list of functions that they are trying to relate to particular brain regions or systems.
The other, much more recent development, has been to pay serious attention to modelling how cognitive functions might be carried out by the brain.
The first point can be illustrated by looking at the history of thought about the hippocampus.
Few scientists paid any attention to this part of the limbic system until Papez published his theory of emotion in the late 1930s, whereupon considerable effort was put into identifying its role in emotion.
When the results of lesion studies proved incompatible with this simple model scientists in the 1960s shifted to response modulation models (McCleary 1966).
The 1970s saw a major shift in focus and suddenly people  were describing the hippocampus as a ‘cognitive map’(O'Keefe and Nadel 1978) or as the repository of ‘working memory’(Olton 1983).
What most people wisely avoided was specifying how a ‘working memory’ could be constructed, or how ‘temporal context’could be encoded in the nervous system.
Recent developments in modelling cognitive processes on computers have changed the situation quite dramatically and there is now a realistic chance of being able to produce models of the way cognitive processes are carried out that are sufficiently precise and detailed for them to be tested properly against what happens in the brain (McNaughton and Morris 1987).
However, these developments are in the early stages so there is very little concrete to report at the moment.
Laboratory models of the real world
Psychologists, like all other scientists, cope with the complexity of the real world by making models of it and working with those instead.
We call these models theories.
A theory is essentially a model of the world that draws an analogy between the process we are trying to understand and one we already know about.
For example, we often talk about ‘stress’, which is a term that has a very precise meaning to an engineer.
In psychology we use the analogy of mechanical stress, and its consequences, to describe and explain what happens to people when they are subjected to excessive psychological demands.
Theories are not optional extras in science.
They serve two useful functions.
The first is to provide simple descriptions of complex data so that we can understand what is going on in our experiments.
The second is to lay down the ground-rules for generalizing from one set of conditions to another, for example, from the laboratory to the real world.
Psychologists have a fairly negative attitude to theories, having been badly scarred by the experience of the grand theories of behaviour published in the 1930s and 1940s.
They argue that we do not know enough to develop meaningful theories of psychological processes and relegate theories to the level of intellectual devices for stimulating experimentation.
Many of the theories that are published are not really theories of psychological processes at all, but are theories of experimental phenomena.
For example, in 1940 Hetherington and Ranson observed that rats with lesions involving the ventromedial hypothalamus ate excessively and became obese.
In 1954 Stellar incorporated this finding in his ‘two-centre’ theory of motivation.
Since then there have been a number of theories published on the experimental phenomenon of hypothalamic obesity (Powley 1977).
While these theories address the problem of hunger, they do so only indirectly.
The other aspect of theories is that they enable us to make generalizations.
By itself an experiment allows us to draw conclusions about the particular conditions in that experiment, and nothing else.
A theory tells us that a set of experimental conditions is a particular embodiment of a  general process.
For example, there are theories of memory that tell us that the recall of recently presented items reflects the operation of short-term storage.
We can therefore generalize from one experiment in which short-term storage is believed to operate to another in which we also believe it to be present.
Generalization from one set of experimental conditions is daunting enough, but the real aim of a theory is to allow us to generalize to what happens in the real world.
For example, we want to be able to take knowledge gained from studying the free recall of nonsense syllables and make predictions about our ability to remember things like telephone numbers, or the names of people we have just been introduced to at parties.
Many of us escape from that problem by redefining our experimental paradigms and phenomena as things worthy of study in their own right, so we have a psychological literature that abounds in studies of ‘classical conditioning’, the ‘serial position effect’, the ‘lateral hypothalamic syndrome’, and ‘risky-shift’.
The problem of generalization is a serious one in most of psychology but is particularly poignant in psychobiology because of the damaging consequences of getting it wrong.
Realistic generalization is only possible, however, when there are good theories of the phenomena that occur in the real world.
Fortunately more and more psychobiology is being done in this way.
Overview
For much of its history psychobiology has been a subject ahead of its time, often asking questions for which it had no meaningful answers.
Part of our reaction to that situation has been negative, especially the behaviourist interlude that sought to define out of existence many of the issues that confront us, but for the most part the approach has been a steady accumulation of experimental data in anticipation of the day when meaningful theories could be developed.
This work, it is true, was largely conducted in the belief that simply gathering information alone would eventually lead to an understanding of the brain, but the fact remains that we now have an impressive body of evidence against which to evaluate the new theories that are emerging.
Furthermore, while many of our questions about the relationship between mind and brain had to be held in abeyance, it was possible to make steady progress in answering certain fundamental questions about the organization of the brain.
For example, most scientists are now convinced that the cerebral cortex is parcellated into specialized sub-areas rather than working as a fully integrated system.
By virtue of being interdisciplinary, psychobiology has some special problems not encountered in mainstream psychology.
One of these is its uneasy relationship with biology.
Psychologists are often made to feel the poor relations of biologists, because the latter have more facts at their  disposal, and to put a lower value on psychological than on biological data and theories.
In fact, psychology and biology have to be equal partners in our analysis of the brain.
A second problem concerns the use of animals in psychobiological experiments.
Apart from the ethical concerns some people feel on this matter there is the pressing issue of the degree to which it is possible to extrapolate from one species to another, especially from non-human species to ourselves.
There can be no definite answer to this problem at the moment.
Instead, all extrapolations should be, and indeed usually are, considered to be working hypotheses that guide the way we study our own brains.
Much of the work that appears in the literature is largely descriptive in nature.
There is a dearth of good theories in psychobiology.
This is not surprising, given the difficulties encountered in developing them, but it does limit the ease with which we can generalize from one experiment to another or from experiments to the real world.
For the sake of form, experiments are presented as if they are serious tests of hypotheses or theories, but this reflects the conventions of scientific journals rather than the realities of science.
In fact, most people publish in the hope that their findings will continue to be relevant long after the particular theory to which they have attached them has been finally laid to rest.
Fortunately, the situation is beginning to change as allied disciplines like machine intelligence provide us with models that will allow us to construct theories that are both realistic and have heuristic power.
Animal subjects in psychobiology
There is a convention among psychologists and neuroscientists that leads us to refer to ‘the’ brain, no matter whether we are talking about hamsters or humans.
In other words, we act as if the brain of one species is very much like that of another.
This is a convenient fiction that allows us to carry out research on non-human species, but it is by no means clear that it is true.
The degree of similarity between the brain of one species and that of another should not be taken for granted.
It is an empirical matter that requires careful investigation.
As things stand at present it seems more appropriate to talk about ‘brains’ than about ‘the brain'.
In this chapter I want, first of all, to outline some of the reasons for believing that different types of animal have different types of brain, and second, to discuss ways of getting round some of the difficulties created when we want to make extrapolations between species.
It is certainly necessary to be able to make cross-species extrapolations, for otherwise we should be deprived of our main source of data.
There are some purists who would argue that this is no bad thing, and that those interested in the human brain and human behaviour should stick to studying humans.
They point to the alarming consequences of ova-enthusiastic extrapolation across species.
For example, it is widely held that Egas Moniz was stimulated into developing the now largely abandoned technique of controlling psychological disorders by the use of psychosurgery after hearing about the ‘beneficial’ side-effects of frontal lobe removal in chimpanzees.
This, however, is probably unfair on both Moniz and the man whose work he was supposed to be responding to, Jacobsen (Valenstein 1980).
It is unfair to Moniz because he probably would have developed the technique of prefrontal leucotomy anyway and it is unfair on Jacobsen because the substance of Jacobsen's work was that frontal lobe removal produced a severe cognitive deficit which no-one would wish on another person.
Whatever the rights and wrongs of this debate, the fact remains that we do not live in a perfect world and, like it or not, we have to rely on a large amount of data derived from animals.
Evolution and the brain
One of the reasons for believing that cross-species extrapolation is possible at all is that all living animals have evolved from common ancestors that existed at some time in the distant past.
They might, therefore, reasonably be expected to share characteristics that were also possessed by those common ancestors, even if they have unique characteristics as well.
This is especially likely since evolution is considered a very conservative process, ‘advanced’ species building on features that were present in their more ‘primitive’ancestors.
Since all mammals evolved from a common ancestral stock we would expect their brains and bodies to have large numbers of common features which could be the basis of a reasonable level of extrapolation (Sarnat and Netsky 1981).
The real issues concern the number of common features that are likely to be shared by different species and the impact of features unique to a species on the functioning of structures or systems that they share in common with others.
Much depends upon how evolution took place.
On reading many accounts of evolution one might be forgiven for thinking that its purpose was to produce us, the species Homo sapiens sapiens , in a rerun of the biblical account of creation with the agency of God replaced by the blind force of natural selection.
Evolution is presented as a progression from simple to complex forms with us at the pinnacle of it all as the most complex.
Other animals living today are, in some sense, failed evolutionary experiments that approximate to us but do not reach the right level.
This belief is echoed by most texts on physiological psychology that show pictures of rat, cat, monkey, and human brains with the implication that rats are simple versions of cats, and cats simple versions of monkeys, and so on.
There are two reasons for believing that this isn't true.
One is that the history of evolution shows it to be a process of radiation rather than of progression.
The second is that comparisons between the brains of living mammals show that many of them contain specializations that are not present in our own brains.
Making comparisons between brains is a very risky business because there are confounding variables to confuse the issue.
One of the most obvious is the size of the animal possessing the brain.
All things being equal, small animals need less brain than larger ones and large brains might be different from small ones simply by virtue of their size (Russell 1978).
For example, the folding of the neocortex of large mammals is largely a response to the need to cram more cortex into a small space in the cranium.
Folding does not, by itself, indicate neural complexity.
Were one to create a mouse the size of a moose, that mouse would probably have a folded neocortex if its cortex had increased in proportion to the increase in its body size.
Increasing the size of the brain is likely to have other effects.
For one thing, increasing the number of cells increases the amount of space that will be taken up by axons and their terminals, and this will alter the appearance of the tissue, so that regions of the brain will not always look the same in different species.
For another, with  a large brain the boundaries between areas will take up a smaller proportion of those areas than in small brains.
As a consequence, the boundaries will be more distinct and it may be possible to identify more sub-areas.
What this means is that if we want to make realistic comparisons between species we have to make allowance for differences in the sizes of the animals concerned.
Fortunately there are some very straightforward ways of making such allowances.
They rely on the fact that brain weight will increase in proportion to body area, which is, in turn, related to body weight.
It is therefore possible to calculate how much brain a mammal should have for its size and, by taking the ratio between the actual and the expected brain size, obtain a measure of how much ‘extra’ brain a species possesses.
If you do this sort of calculation and compare mice and monkeys you find that monkeys have much more brain than would a mouse of equivalent size (Russell 1978).
This indicates that there is something that distinguishes monkey from mouse brains other than the relative sizes of the two species.
Humans belong to the group of animals known as mammals, which are characterized by the presence of hair, mammary glands and sweat glands, tooth specialization, and the regulation of a constant body temperature.
Mammal-like animals have been around for some 200 million years but didn't become particularly numerous until about 70 million years ago, around the time the dinosaurs became extinct.
It is generally agreed that the primitive mammals existing at that time were shrew-like creatures that lived by eating insects, and that the various groups of mammals that we see today, such as cats, rats, monkeys, whales, and horses, all evolved from this unpromising ancestor.
We also know that the specialization took place very early in the evolution of mammals and that the ancestors of the main groups that are alive today all appeared around the same time, give or take a few million years.
Since then those lines have been undergoing separate evolution, giving rise to yet further, more recent branches.
For example, the primates, to which we belong, date back to around that time but our own branch of the primate line,Homo sapiens , dates back to less than a million years ago.
Historically, evolution has been a process of radiation, with a considerable amount of time for lines to have diverged having elapsed since the first mammals appeared (Figure 2.1).
The net result is that there is no more reason to believe that our brains are like a cat's than there is to believe that they are like a rat's.
Nevertheless, since evolution is a conservative process our brains are likely to contain some components in common with other mammals in so far as they contain features that were characteristic of the common ancestor.
All we have to do is to find out what the brains of our common ancestors looked like to find out how similar we might expect the brains of mammals to be.
There is just one problem: brains are part of the soft tissue of the body and so do not survive fossilization.
Consequently, no-one has ever seen the brain of one of those early mammals.
Can we reconstruct one on the basis of what we currently  know?
Palaeontologists have applied two approaches.
One, which could be seen as a little circular, is to look at the brains of mammals alive today and determine what they have in common.
These common features then become the features that they share by virtue of having a common ancestor.
Of course, it is more complex than that because allowance has to be made for convergent evolution, which is the acquisition of common characteristics by virtue of having been subjected to the same selection pressures, rather than because of sharing a common ancestor.
Nevertheless, if you could identify common features it would be a good starting point for reconstructing the brain of the common ancestor.
The other strategy has been to look for a species that is alive today that has physical characteristics similar to those of our common ancestor, and has the same lifestyle, and to use its brain as a model for the primitive brain.
Neither procedure is satisfactory, but the combination might produce something of interest.
Figure 2.1
A simplified phylogenetic tree showing the relationships between the major mammalian groups
Making comparisons between species involves identifying what are known as homologous structures.
Homology is identified on the basis of a number of criteria, including the form of the tissue, its embryonic origins, its connections with other parts of the brain or body, and, to a lesser extent, its function.
It is assumed that homologous structures are there as a consequence of evolution from a common ancestor, so homologous structures common to all mammals would be presumed to have been present in our common ancestor.
It is important to note that homologous structures don't have to look the same, and that structures that look the same don't have to be homologous.
For example, a seal's flipper is homologous with the human arm, even though they look  very different, while the wings of bats and birds are not homologous, even though they look similar.
Similarity of appearance and function that is not based on homology is known as analogy.
In the case of hard tissue, like teeth and bones, the identification of homology is relatively straightforward because it is possible to compare present forms with the common ancestor and this can be used as an additional criterion for homology.
In the case of soft tissue, like the brain, we are on less certain ground.
Despite the problems we are inclined to accept some homologies in the nervous system, although there is disagreement about how far one should take it.
Most people are happy with the idea of homologies at low levels in the nervous system.
Few would dispute the fact that the brainstem of the rat, below the level of the thalamus, is largely homologous with that of primates.
Structures with similar appearances, in comparable locations, that differentiate from the same embryonic tissue at the same stage in development, can be identified relatively easily and, in general, their connections tend to be similar.
This isn't particularly surprising because the brainstem structure was probably inherited by the mammals from their common ancestor in the reptile stock.
These homologies would provide considerable scope for direct extrapolation providing the functions of these brainstem regions are not modified by forebrain structures that differ from group to group.
Neuroscientists may amuse themselves by thinking up objections to homologies between brainstem structures, but this isn't pursued that seriously.
The real conflicts arise when people get on to the cerebral cortex.
Again, most of us are happy to accept that the cortices of all mammals are, as units, homologous with each other.
The problems arise when people seek to draw homologies between specific cortical areas.
Anybody who has ever looked at sections of a rat's brain and then at sections of a monkey's brain will tell you that they look completely different.
For one thing, in the monkey brain it is possible to identify quite sharp boundaries between areas that can be distinguished in terms of the shapes, sizes, numbers, and laminar distributions of their cells.
In the rat brain there are regional differences in the structure of the cortex, but the areas that can be identified tend to blur into each other.
For another thing, areas that one would expect to be homologous on the basis of their connections appear totally different in other ways.
For example, the area of the primate cortex that receives the direct visual projection from the thalamus is distinguished by a dense band of white matter in layer four.
This stripe, which gives rise to the alternative name for this area, the striate cortex, is visible to the naked eye in sections of the human brain.
It is completely absent in the rat.
Indeed, it is extremely difficult to distinguish between the primary visual cortex and surrounding areas in this species.
Nevertheless, some optimists continue to refer to the primary visual area in the rat as ‘striate’ cortex.
Other procedures reveal other differences between these areas.
If the visual cortex of a monkey is exposed to a stain that highlights the enzyme cytochrome  oxidase it is possible to identify densely labelled blobs, which indicate high concentrations of the enzyme, in the upper layers of the visual cortex (Livingstone and Hubel 1984).
Exposing rat cortex to the same stain produces uniform, moderately dense, labelling in the primary visual area.
In many ways it looks like one big cytochrome oxidase blob!
The extra detail visible in the monkey's brain might just be a consequence of it being bigger than the rat's, but a comparison between the somatosensory cortices of the two animals shows that this isn't so, for this time it is in the rat that the extra detail is visible.
If the cytochrome oxidase stain is used on the rat somatosensory cortex, the cortex contains a large number of patches shaped like barrels (Land and Simons 1985).
No such barrels are present in the monkey.
We therefore have to face the fact that there are structural differences between rat and monkey cortex that cannot be simply attributed to differences in size.
None of this actually precludes the visual or somatosensory areas in the two species being homologous, but it does mean that we need evidence other than appearance to support the case.
Embryology isn't a good guide because it isn't sufficiently precise.
We can tell that the primary visual areas in rats and monkeys come from the same primordial tissue in the embryonic nervous system, but this isn't surprising since all of the cortex arises from this same region.
Consequently, all that embryology tells us is that both areas are part of the neocortex.
The connections of cortical areas may give some clues about homology.
It is encouraging that the primary visual areas of both rats and monkeys receive input from the lateral geniculate nucleus of the thalamus but we should be aware of some circularity here since one of the criteria used for saying that the lateral geniculate nuclei in the two groups are homologous is that they both project to primary visual cortex!
Certainly the lateral geniculate nucleus in rats looks nothing like the lateral geniculate nucleus in monkeys.
On the output side the similarity is less clear-cut.
In the monkey the primary visual cortex projects into a band of specialized cortical areas surrounding it (Maunsell and Newsome 1987).
There are many such areas, each containing a full or partial representation of the visual field, some of which receive a direct input from the primary area and some of which rely on indirect projections for their inputs.
Higher-order visual areas are also present in the rat, but it is unclear whether their organization is the same as in the monkey (Espinoza and Thomas 1983).
In the rat the primary visual cortex has a sizable projection to the pontine nuclei in the brainstem (Legg and Glickstein 1984), while this projection is extremely sparse in the monkey (Glickstein, May, and Mercier 1985).
In monkeys the lateral geniculate nucleus projects almost exclusively to the primary visual cortex, with only a few axons straying into the surrounding secondary cortical visual areas.
In the cat, in contrast, the lateral geniculate nucleus projects to two distinct, but adjacent, cortical zones (areas 17 and 18), each containing a representation of the entire visual field.
Clearly we shouldn't expect too many functional similarities between the visual cortices of rats, cats, and monkeys.
Once we move beyond the primary sensory receiving areas the situation gets even more tenuous.
One widely used criterion for dividing up the cerebral cortex is the organization of inputs from the thalamus.
For example, Leonard (1969) used this approach to identify the ‘homologue’ of primate prefrontal cortex in rats.
In monkeys the dorsomedial nucleus of the thalamus projects exclusively to the prefrontal catex so it seems reasonable that the dorsomedial nucleus projection zone would correspond to prefrontal cortex in rats.
Accordingly, Leonard investigated the connections of the dorsomedial thalamus in rats and showed that it projects to two cortical areas, one lying on the medial edge of the anterior cortex and the other on the lateral edge, in the rhinal fissure.
The problem with using thalamic nuclei as a guide to cortical organization is that primates have a thalamic nucleus, with extensive cortical projections, for which there is no clearly identifiable homologue in other species.
That nucleus is the pulvinar, which projects throughout occipital, parietal, temporal and frontal cortex.
We can be fairly certain that the pulvinar is not homologous with anything in the rat, or indeed in any other group, because its embryonic origins are so distinct (Rakic 1974).
This means that the cortical areas to which the pulvinar projects are not strictly comparable with anything seen in non-primate species.
Comparative analysis suggests that there has been a considerable amount of divergence of evolution in the cortices of different mammalian lines.
This level of divergence is compatible with the results of attempts to identify a living mammal that is comparable to the mammalian common ancestor.
The favoured candidate at the moment is the humble hedgehog, a nocturnal insect-eater which possesses a remarkably undifferentiated cortex (Diamond and Hall 1969).
In this species there is no clear distinction between primary visual cortex and the secondary visual areas that are present in most other species, ranging from rats to monkeys, and there is almost total overlap between the motor cortex, defined in terms of the area of cortex from which movements can be evoked by electrical stimulation, and the somatosensory cortex, defined in terms of the area from which electrical activity can be evoked by stimulation of the body surface (Kaas 1982).
We are left with the rather worrying conclusion that cerebral cortex, as a whole, is homologous in all mammalian species but we should be very, very cautious about accepting homologies between specific cortical areas.
The implications of differences in cortical organization go beyond our understanding of the cerebral cortex.
They are also likely to lead to differences in the way brainstem structures work, since the cerebral cortex of mammals is one of the major sources of input to these areas.
This point is of more than academic interest since there are, in fact, quite significant differences in the functional organization of apparently homologous structures in different species.
For example, the superior colliculus, a visual structure lying on the dorsal surface of the midbrain, has subtly different properties in different species (Goldberg and Robinson 1978).
Electrical stimulation deep within the  colliculus produces eye movements, even when very low-level currents are used, but the characteristics of the movements vary between species.
In monkeys, stimulation produces rapid movements, called saccades, of an amplitude and direction predictable from the site of the stimulation in the colliculus alone.
The effects of stimulation are ‘all-or-none’: either a saccade is elicited or it isn't, and its size and direction are not related to the intensity or frequency of stimulation in the colliculus.
In cats, superior colliculus stimulation also produces saccades, but the effect is not all-or-none.
Instead, the amplitude of the saccades increases with the intensity of the stimulation, up to a plateau that is characteristic of a particular stimulation site.
McHaffie and Stein (1982) have also reported eye movements after colliculus stimulation in rats, although it is not clear that they are true saccades.
In some ways the control is similar to that observed in the cat, in that eye-movement amplitude varied with stimulus intensity.
In other ways the results were curious.
First, the largest excursions of the eyes were only around 10 degrees.
Second, the stimulation thresholds were much higher than in cats and monkeys.
Finally, stimulation tended to cause the eyes to protrude from their sockets before moving.
It may turn out that the differences are an artefact of the methods used in different laboratories, but it is difficult to see what the relevant differences may be.
Identifying homologies at the level of brain structures is fraught with difficulties.
Fortunately, there are some issues that do not depend on such a level of specificity.
One concerns the cellular mechanisms underlying processes like learning and memory, since we might reasonably expect that the basic properties of nerve cells are the same in all species.
This is probably acceptable so long as we restrict ourselves to a single group, like mammals, but there is some dissent when people seek to extrapolate mechanisms from non-vertebrate species, like molluscs, to the mammalian brain (e.g. Hawkins and Kandel 1984).
It would certainly be naive just to assume that nerve cells are the same in molluscs as in people, but fortunately it is possible to rely on more than assumption.
The advantage of this sort of work is that the descriptions of cellular mechanisms are very detailed and this provides a good basis for determining whether the same cellular mechanisms are available in mammals.
For example, plasticity seems to depend on identifiable ion channels and catalytic molecules that activate or inhibit these channels.
It is possible to use advanced biochemical techniques to look for comparable ion channels and catalytic units in the mammalian brain, without directly studying the role of cells in learning.
The problem with this approach is that it doesn't allow for the possibility that there are additional cellular mechanisms of plasticity in mammals, that have evolved to underpin our greater learning abilities.
Extrapolation rules
Do these problems with identifying homologies rule out extrapolations between species?
The optimistic consensus is that they don't but that one has to be  very careful about making them, and the level of detail one can achieve may not be as great as one may like.
There are a number of reasons for this optimism.
The first is that the amount of evolutionary divergence in the organization of primate brains has not been anywhere near as great as the divergence between primates and other groups.
Providing one sticks to primates there is much scope for successful extrapolation.
The second is that homology probably doesn't matter that much.
What matters from the functional point of view is analogy: carrying out comparable functions.
Brains may carry out tasks in the same way even if the tissues involved are not strictly homologous.
One might expect this because convergent evolution would lead to similar mechanisms to carry out similar functions.
Finally, we can probably take advantage of variations in brain organization to understand the relationships between structure and function.
The differences between the brains of different species are not accidental but reflect differences in the abilities and lifestyles of the species concerned.
We might then be able to work back from lifestyle and abilities to predict what the brain of a particular species should be like (Legg 1983).
Comparative anatomists tell us that there are remarkably few differences between our brains and those of monkeys and apes, other than that ours are bigger.
Since we are quite different in our behavioural capacities, notably in our possessing language, this may reflect a lack of subtlety on the part of the anatomists but it is probably reasonable to assume that the input and output ends of the system are pretty much the same in monkeys as in people.
For example, it wouldn't stretch the truth too much to assume that the organization of the visual system is the same in rhesus monkeys as it is in humans.
As we shall see in a subsequent chapter there is good evidence for this optimism.
Although no-one has explored the human visual cortex in the fine detail that the anatomists and electrophysiologists have studied the visual cortex of monkeys, there are a number of indirect sources of evidence to support this position.
First of all, brain damage in people has been found to cause inability to perceive particular attributes of visual scenes without causing total blindness (Maunsell and Newsome 1987), and the single cell recording data on monkeys show that those attributes are processed by distinct parts of the cortex.
For example, area V4 in monkeys is specialized for processing colour information but doesn't encode other attributes like motion or position.
We would therefore expect to find patients who had lost colour vision without losing other attributes of vision, like motion detection.
Such patients have been reported (Heywood and Cowey 1985).
Second, the independence of processing of attributes like colour and motion suggests that they should be found to operate independently in psychophysical studies, and this is what happens (Nakayama and Silverman 1986).
Finally, studies of the metabolic activity of the human brain have shown that different areas of the occipital cortex become active when different attributes of stimuli are highlighted in visual displays (Raichle 1983).
Although visual system physiologists treat the monkey as the model system of their choice for studying the visual cortex, a surprising amount of useful data has been gathered using cats as subjects, despite the known anatomical differences between cats and monkeys.
The obvious reason for this Is convergent evolution: the visual systems of cats and monkeys are doing similar jobs and use similar mechanisms to do so.
It is important to remember, for example, that our main concepts of the functional organization of primary visual cortex were developed in studies on cats rather than monkeys.
These include the existence of orientation selectivity as the main receptive-field characteristic, columnar organization, ocular dominance, and the distinction between simple and complex cell characteristics.
Nevertheless, it is dangerous to get too carried away with the similarities since they can blind even the best researchers to new observations.
A case in point are the receptive-field properties observed in the cytochrome oxidase blobs that are present in the upper layers of primary visual cortex of monkeys but not of cats (Livingstone and Hubel 1984).
These blob areas are curious in that the cells have only poor orientation selectivity but are tuned to the wavelength of light instead.
Hubel and his colleagues had studied the visual cortex of the rhesus monkey for many years without observing these cells and it was only when the cytochrome oxidase blobs had been demonstrated consistently and they started to look for receptive-field properties within them that they obtained these surprising results.
Since their work on cats had conditioned them to expect all parts of the visual cortex to contain orientation selective cells this oversight is, perhaps, understandable.
Differences in neural organization must reflect differences in function.
We could, in principle, come to understand how the brain works by correlating neural differences with differences in the behaviour of species.
If, for example, we knew what animals with neocortices could do that species lacking them couldn't do we would know something about the cortex without having to ablate it in a single animal.
If we know what animals with cytochrome oxidase blobs in their visual cortices can see that those lacking them cannot we can learn something about the functions of the blob system.
Whitfield (1979) has used this correlational approach to analyse the functions of the auditory cortex.
Most vertebrates, including those without forebrain auditory connections, with functioning auditory systems can detect sounds, discriminate between sound frequencies, and localize sounds by means of orienting movements, indicating that these abilities do not depend on neocortex.
What these animals cannot do is localize sounds independently of their position relative to the animal's body.
Interestingly, this is also true of mammals from which the auditory cortex has been removed.
When applied to the newly discovered cytochrome oxidase blobs in the visual cortex of primates the correlational approach is quite revealing.
The single cell recording studies carried out on rhesus monkeys show a very good correlation between the blobs and wavelength tuning, suggesting a role in  colour vision.
However, there is one monkey, the owl monkey of South America, that has little useful colour vision, being a nocturnal animal, but which still possesses these blobs.
This suggests that the blobs are involved in something much more fundamental, like the detection of very coarse stimulus features, rather than colour analysis as such.
This comparative approach can also be used to analyse brainstem areas, the functions of which have been modified by the evolution of neocortical areas.
The approach involves identifying variations in the functioning of target areas and relating those variations to known differences in cortical function.
Once allowance has been made for variations in cortical function it should then be possible to identify the function of the subcortical area.
An excellent example is the work that has been done on the hippocampus.
The most obvious consequence of removing the hippocampus in the rat is that the animals have an immense difficulty in refraining from responses that lead to punishment, a deficit in passive avoidance apparently due to an inability to inhibit inappropriate responses.
In humans, in contrast, surgical removal of the hippocampus has been found to produce a profound and abiding loss of memory.
How can the findings be reconciled?
One approach was to argue that the human memory loss was secondary to the failure of a more fundamental process, like the ability to inhibit inappropriate responses.
The other approach has been to argue that rats have difficulty with passive avoidance because they cannot remember recent events.
Although people with hippocampal lesions have appalling memories they are, nevertheless, capable of learning.
In the early 1970s this led Weiskrantz and Warrington (see Parkin 1987) to argue that the human deficit was not a failure of memory as such but a deficit in retrieval brought about by undue interference from incorrect items at the time of recall.
In other words, the fundamental deficit was the same in animals and people: a failure of response inhibition.
This proved quite a successful theory for a number of years, in that it stimulated a lot of research and produced a corpus of compatible experimental results.
There were, ultimately, some findings that were distinctly incompatible with the theory and it was abandoned by its originators.
More recently, people have reversed the logic applied by Weiskrantz and Warrington and have suggested that the deficits in rats are really due to a memory loss (Kesner and DiMattia 1987; Olton 1983).
In other words, rats with hippocampal lesions have difficulty in refraining from punished responses because they cannot remember what happened to them the last time that they did whatever it was that led to the punishment.
Rats are normally very good at remembering where they have received reinforcement but lose this ability after hippocampal damage.
For example, Olton (1983) tested rats on a ‘radial arm maze’, which is an elevated star-shaped maze with a number of arms radiating from a central choice point.
At the beginning of each trial food is placed at the end of each arm and the rat left to move from arm to arm, in whatever sequence it chooses, in order to retrieve the food.
Normal rats learn  rapidly to avoid arms from which they have taken food on that trial.
Rats with hippocampal lesions re-enter arms quite regularly.
This isn't a deficit in response inhibition as such because the animals never develop a history of being reinforced in one arm of the maze in preference to any other.
It is therefore simplest to interpret these results in terms of an inability to remember that food has just been obtained from a particular arm.
Other data, discussed in chapter 7, support this point.
We should be cautious about putting too much weight on the analogy between the memory losses observed in people and the deficit in ‘working memory’ observed in rats, simply because the humans concerned have a deficit in verbal memory, and the rats don't have language.
It would be interesting to know whether humans with hippocampal damage are impaired on the equivalent of a radial arm maze.
This may sound frivolous, but it emphasizes the point that it is often far from clear what constitutes equivalent behavioural tasks when making comparisons between species, and this uncertainty creates problems for the comparative approach to brain function.
Conclusions
Using animals as a substitute for humans is a far from straightforward business.
There are difficulties in identifying areas common to the brains of people and other animals and, even when that can be done, it is unclear how far one can rely on the areas working in exactly the same way.
Nevertheless, there are certain things that can be done.
One is to study our closest relatives, the monkeys and great apes, a second is to study the way that particular functions, like the visual analysis of form, are done, irrespective of the species involved; and the third is to take advantage of differences between species and correlate structural with behavioural differences as means of analysing the functions of brain systems.
Whichever strategy is adopted there are going to be problems and it is important to be tuned into them when trying to interpret research in psychobiology.
Methods in psychobiology
A science is only as good as its methods.
Physiological psychology presents a range of major methodological challenges, and how well we meet these challenges affects the ease with which we can interpret the experiments that we carry out.
Some of these challenges are entirely technical but others are conceptual, stemming from the fact that most methods only make sense if one makes certain assumptions about the brain which may, or may not, be valid.
There is nothing alarming or unusual about methodological problems; all sciences have them and it is quite common for the questions that people ask to run ahead of the techniques available for answering them.
It is also quite common for theoretical predictions to go untested for want of appropriate experimental methods.
The important point is to recognize that these methodological difficulties exist and allow awareness of them to influence our interpretation of experimental results.
Psychobiologists want to explain behaviour in terms of physiological events occurring in the brain and the body.
To achieve this they need to be able first to specify the functions of individual components of the nervous system and related mechanisms in the body that have behavioural significance, such as the digestive system, and second to explain how these components, working together in an integrated system, give rise to human behaviour and human consciousness.
Much of their effort is devoted to determining the functions of anatomically distinguishable parts of the brain.
In pursuit of this goal they have developed a number of specialized techniques.
The most commonly used is the lesion method, in which part of the brain is injured or entirely removed and the resultant changes in behaviour studied.
Others probe the functions of different areas by applying electrical stimulation and looking at the behaviours, if any, that are elicited by it.
Finally, advances in instrumentation have encouraged the study of neural correlates of behaviour, either by monitoring the electrical activity of cells or, very recently, by analysing the metabolic activity of different brain areas.
In the past decade there has been a resurgence of interest in the role of non-neural systems in the control of behaviour, especially in the control of eating and drinking.
Techniques analogous to those used in the study of the brain have been applied to these systems and it can  be quite instructive to compare the way in which people have approached these non-neural systems with these methods with the way they go about studying the central nervous system.
Methodological problems come in two varieties, technical and conceptual.
Technical limitations are those that affect our ability to do what we set out to do.
Conceptual factors are those which limit our ability to draw conclusions from experiments, even if they are technically perfect.
Technical problems usually revolve around the degree to which we can isolate or manipulate a single target system in a consistent and reliable way (Bures, Buresova, and Huston 1976).
For example, in practice it is extremely difficult to make a brain lesion that entirely removes one part of the brain while leaving the rest intact; they are either too large or too small.
Often lesions damage the connections of other systems that happen to pass through the lesion site.
Unless carefully monitored, stimulation of the brain leaves us with the problem of not knowing the extent of the area that has been affected by the stimulation.
Recording studies are not entirely exempt from these problems.
Measurement of the metabolic correlates of psychological processes can only be done with a limited degree of spatial resolution at the moment (Raichle 1983).
The problem with other recording studies, notably single unit recording work, is often too much selectivity with a resultant bias towards a particular subset of cells in a particular region of the brain.
Different groups, using slightly different recording techniques, can end up with quite different descriptions of the response properties of the cells in the same region because they are, in fact, recording from different cell populations (O'Keefe and Conway 1978; Olds, Disterhoft, Segal, Kornblith, and Hirsh 1972; Stryker and Sherk 1975).
as if technical problems were not enough, there are also a number of conceptual problems surrounding the application of these methods.
The basic problem is that most methods make implicit assumptions about the way the brain is organized.
Clearly, bits of the brain do not exist in isolation.
The functions of any single region of the brain are going to be expressed through that region's connections with other parts of the nervous system and the effects of our experimental procedures are going to depend on the nature of those interconnections.
Understanding the functions of one part of the brain is going to depend on understanding how these others work.
This is coming dangerously close to saying that we cannot find out how the brain works until we know how the brain works!
We escape from this vicious circle by using the ‘boot-strapping’ procedure of making simplifying assumptions about the functional organization of the brain and interpreting our experiments in the light of them.
As long as our experiments produce results that make sense we continue with these assumptions.
When our experiments stop making sense we should go back to these assumptions to see whether they need revision.
To understand what is going on in physiological psychology one needs to know what these assumptions are.
Some of our assumptions are about how the brain is organized but others concern behaviour itself.
Psychobiology is characterized by our attempts to study the brain by correlating neural factors with behaviour, and there is genuine disagreement amongst researchers about how to study the behaviour itself.
For example, some focus on ‘natural’ units of behaviour, while others use inferential behavioural measures to tap underlying cognitive, sensory and motor processes.
With the inferential approach it is clearly necessary to have a good understanding of the psychological processes involved in performing the test procedures we use and good theories of those processes themselves.
Good psychobiology, using the inferential approach, requires good psychology.
Using ‘natural’ units of behaviour is not without its problems as it begs the question as to what constitutes ‘natural’as well as sidestepping the issue of why particular parts of the nervous system are important for particular behaviours.
Since behavioural methods are central to psychobiology they are tackled first in this chapter.
We then turn to the main techniques for studying the physiological side of the equation, lesions, stimulation, and recording, looking at the sorts of problems that affect our interpretation of experiments.
Relevant features of the main methods are outlined where necessary.
For more detail the reader is referred elsewhere (Bures et al .
1976; Carlson 1986).
Behavioural testing
In principle it should be possible to describe the functional organization of the brain simply by studying its anatomical organization and the functions of its individual components, the neurons.
In practice that is a recipe for disaster since, without knowing what the brain and its components do, we will never know whether we have a good description of how they work.
Since the brain is the organ of behaviour we must approach its functions through behaviour.
There are two issues we must consider here.
The first is whether we should study behaviour as behaviour, in the way that ethologists do, or study behaviour as a reflection of underlying psychological processes, as cognitive psychologists tend to do.
The second concerns the nature of the ‘functions’ we ascribe to neural regions as a result of our studies.
Should we identify brain functions with components of psychological processes revealed in psychological studies or do we need a new set of concepts for describing brain function?
Neuroscience has two traditions, the psychological and the ethological.
According to the psychological tradition, behavioural tests are tools to give us access to psychological processes (Dean 1982).
According to the ethologists, behaviour is interesting in its own right (Ewert 1980).
Those working in the psychological tradition devise tasks or tests that are believed to place varying levels of demand on the cognitive, sensory, or motor capacities we believe our subjects to possess.
Performance on these tasks is then correlated with our physiological manipulations and, on the basis of these results, we ascribe  functions to areas of the brain.
For example, if we have three tests that are apparently identical except for the increasing ‘memory’ load they entail, and the electrical activity of part of the brain increases systematically across the three, we would be inclined lo think that memory in some way involved that part of the brain.
Ethologists, in contrast, work on ‘natural’ units of behaviour, such as prey catching or mating, correlating these with their physiological manipulations.
For example, a region of the brain that showed increased electrical activity during sexual behaviour, but not during other motivational processes or comparable movements, would have a sexual function ascribed.
It would seem that the choice of approach is irrelevant because the ethologists are, eventually, going to have to start explaining why particular parts of the brain are involved in behaviours like sex or hunting and that will involve ascribing psychological subprocesses to them.
In other words, the results of the two approaches should converge.
However, that is only true if the psychological processes that operate during natural behaviour sequences are also accessible to our arbitrarily selected behavioural tasks (Rozin 1976).
They may not be.
To take a fairly dramatic example, honey-bees are capable of extremely sophisticated mathematical computations that enable them to navigate from their hives to sources of nectar, using the sun's position as a guide and compensating for its movement during the course of the day.
It is unlikely that even the most devious psychologist could devise a behavioural test, other than navigation, that would harness that computational ability.
This point relates back to the issue of cross-species extrapolation raised in the preceding chapter.
Those of us who use inferential methods to study underlying psychological processes are inclined to argue that these processes are available to all of the animals we study.
For example, it is argued that both rats and humans have memory mechanisms and that you can, therefore, study memory in either species.
In practice the situation is complicated by the fact that our ability to study psychological processes in a particular species is highly dependent on the behavioural methods used.
With some methods a particular species will appear totally devoid of a particular capacity yet, with others, a startling level of ability can be revealed.
Let us take vision as an example.
Rhesus monkeys readily discriminate between objects and scenes on the basis of their visual appearance.
In contrast, the laboratory rat appears totally blind to the casual observer, using touch and feel, instead of sight, to negotiate its way around the environment.
In formal testing situations, in which the animals are rewarded for approaching one visual stimulus in preference to a second, rats will regularly fail to learn to make the discrimination.
Nevertheless, rats can be trained to carry out visual discrimination tasks and will use visual cues to guide their natural behaviour.
The problem is to devise tests that will demonstrate this.
As Cowey (1968) has pointed out, rats readily learn visual discrimination tasks when the contiguity between the stimulus, the response, and the reinforcement is high.
Ideally, the animal should make its response by manipulating the visual  stimulus itself and should receive its reward from very close by.
Even small reductions in contiguity, such as getting the animal to make its response on a lever next to the stimulus or putting the reinforcement dispenser in a different location, will result in a staggering loss of performance.
Under these circumstances, there are grounds to doubt the value of comparing the visual capacities, and underlying visual mechanisms, of the Mt with those in rhesus monkeys and other primates, such as ourselves.
One of our goals is to describe the ‘functions’ of particular parts of the brain, but what is a ‘function’?
Is a function a psychological process or is it a neural process?
There is a general consensus that psychological processes are a function of the whole brain, not of its constituent parts.
It therefore makes no sense to say that ‘memory’ or ‘perception’is the function of any part of the brain.
One solution, widely endorsed, is to break down psychological processes into putative substages and assign these to different parts of the brain (Dean 1982; Luria 1973).
However, these substages are still described in psychological terms.
For example, in his review of the functions of the inferotemporal cortex in rhesus monkeys, Dean concludes that this area may be involved in ‘stimulus categorization’, rather than visual perception.
Given our lack of knowledge of both how circuitry in the brain actually works and how visual perception may be mediated at the neural level, this state of affairs is scarcely surprising.
Terms like ‘stimulus categorization’ have to be used, because we don't have any other language in which to describe what parts of the brain do, but a term like this should be taken as a shorthand for ‘neural processes that could mediate stimulus categorization, whatever they may be’, rather than a conclusive statement about what an area does.
Given our present level of ignorance there is a good case for muddling along as best we can until the theory catches up with the data.
Indeed, our studies may actually stimulate psychological theories.
There is just one problem, which is that the way in which we conceptualize the processes we are studying influences the way we study the brain.
An inappropriate model of visual perception will lead us to ask the wrong questions about the parts of the brain we believe to be involved in it.
This point is well illustrated by the ‘decision tree’ approach described by Dean (1982).
He rightly argues that the best way to find out what part of the brain does is to start out with very general questions about the sorts of thing it might do and then work through to more specific questions.
For example, start off by asking whether lesions produce sensory or non-sensory impairments before finding out whether the non-sensory deficits involve ‘memory’ or not.
Figure 3.1 shows a decision tree for applying this sort of approach to analysing the functions of the monkey inferotemporal cortex.
At each point the decisions are couched in terms of psychological processes or subprocesses.
Were perception conceived of differently, it is quite likely that the decision tree would have had a different structure and led to different conclusions.
Fortunately, there is a check in the system.
If you are thinking about the brain properly you should get sensible answers to your  questions.
If, for example, you draw a distinction between two processes and your experiments keep on producing equivocal results, it is probably time to start thinking about the validity of the distinction.
Figure 3.1 An example of a decision tree to be used to design behavioural experiments
Note the progression through the tree from very general distinctions, in the upper left panel, to very specific distinctions, in the lower right.
Neural processes
The lesion method
Our goal is to make statements about the way different parts of the brain contribute to behaviour.
It would seem that the simplest way of approaching this question is to remove part of the brain and assess the changes in behaviour that are produced.
As might be expected, there are a number of problems with this approach.
‘Removing’ part of the brain is easier said than done.
Brains are not designed like electronic circuits, with easily identified separate components that can be taken out independently.
In fact, one never really removes a component cleanly in that sort of way; one has to damage the brain in order to eliminate a component.
Consequently, we are always dealing with  the effects of brain injury, and those effects probably extend beyond the mere loss of a single component.
For example, scar tissue may form or there may be a disturbance to the blood supply, both of which can affect the working of adjacent regions.
The term ‘lesion’, meaning ‘injury’ or ‘damage’, reflects these problems.
There are, therefore, technical problems about the lesion method.
In addition there are conceptual worries.
Even if the brain were designed so that components could be easily removed, there is the issue of what we can conclude about the functions of its components from knowing the effects of removing one of them.
What are we entitled to conclude about the functions of an area we have removed from knowledge of how behaviour changes when it is eliminated?
Technical problems
To the naked eye the brain has a disturbingly homogeneous appearance.
A small number of distinctive areas may be identified, but the functional sub-units that are the stuff of modem physiological psychology are nowhere to be seen.
You cannot see prefrontal cortex, primary visual cortex, parietal association cortex, the ventromedial nucleus of the hypothalamus or most of the other structures that we are inclined to talk about.
These can only be discriminated in thin sections viewed under the microscope after they have been exposed to appropriate dyes.
Even then, adjacent areas often merge into each other so that the boundaries are blurred.
Deep within the brain, in areas like the thalamus and hypothalamus, these problems are exacerbated by the presence of long fibre bundles that pass through interesting regions, carrying the connections of completely unrelated regions.
As a consequence, attempts to remove one component often compromise the connections of others.
What are physiological psychologists to do in the face of these difficulties?
They have three weapons in their armoury.
The first is knowledge of the remarkable consistency in the organization of the brain, at least within individual species, the second, the availability of histological techniques which enable them to specify where a lesion has been made, after the event, even if it isn't completely possible before, and third, access to new chemical techniques for selectively destroying some components of the brain while leaving others intact (Kohler, Schwartz, and Fuxe 1979).
Much of our work concerns the mantle of the forebrain, the neocortex, which is now recognized by anatomists to be divided into a number of subfields or ‘areas’.
Within any species the topographical distribution of areas is constant (Zilles and Wree 1985).
For example, you know that the visual cortex will lie towards the back of the brain, the motor cortex towards the front.
Moreover, in animals with convoluted cortices, like cats, dogs, monkeys, and man, areas usually have a fixed relationship to the pattern of sulci and gyri on the cortical surface.
For example, in the rhesus monkey, the primary visual cortex (area 17) always lies just behind the lunate and preoccipital sulci.
This guidance is, of course, not available in smooth-brained animals like rats and mice, and in these it is extremely difficult to make accurate cortical removals.
To reach structures deep within the brain it is not possible to use external landmarks, like folds in the cortex, directly.
Nevertheless, it is possible to take advantage of the consistency of the three dimensional organization of the brain.
Within any strain of a species, deep structures tend to be in a fixed position relative to certain landmarks on the skull, such as the joints between the bones that form its upper surface (Pellegrino, Pellegrino, and Cushman 1979).
This enables us to use a stereotaxic instrument to guide probes like lesion-making electrodes into the brain at known distances in front of, to one side of, and below one of these landmarks and guarantee to get it into a selected subcortical area.
In most physiological psychology we take it for granted that lesions will centre on the structure selected by the experimenter.
Most of the disagreement centres around how completely a structure has been eliminated, and what other structures have been damaged as well.
This hinges on the quality of the histological information provided by the experimenter.
Typically, at the end of an experiment the animals are killed, their brains removed and thin sections cut through the lesion sites.
Exposure of these sections to dyes that are selectively taken up by either the bodies of neurons or by their axons enables us to see the structure of the lesion site.
Often this is supplemented by other information.
For example, the neocortex is connected to the thalamus in such a way that destruction of part of the cortex causes cells in a corresponding part of the thalamus to die, a process known as ‘retrograde degeneration’.
You can monitor the amount of cortical damage by looking at the extent of retrograde degeneration in the thalamus.
All of these procedures are routine with animals, but are obviously the exception rather than the rule in studies of human neuropsychology, when all concerned fervently hope that the subjects will survive.
To some extent we are able to get round the problems of localizing lesion sites by using modem imaging techniques like NMR (nuclear magnetic resonance) and CAT (computerized axial tomography) scanning (Carlson 1986).
Nevertheless, as long as we continue to define the functional subcomponents of the brain in terms of features seen in stained microscope sections, these scanning techniques are of restricted utility.
As a consequence there are, at present, serious obstacles to analysing directly the functional organization of the human brain.
Some of the problems of unintended damage to other systems could be eliminated if we could selectively destroy only certain parts of nerve cells, like their axons or their cell bodies.
Recently the neurochemists have come up with a set of toxins which appear to be fairly effective in destroying only cell bodies, leaving axons intact (Kohler et al .
1979; Schwartz and Coyle 1977).
These are not without their problems, however.
Owing to the fact that some cells are more sensitive to these toxins than others it is possible for injections of the toxin in one site to result in lesions in a distant area to which the toxin has  diffused, or been carried in the blood supply of the brain.
This is known as the ‘remote lesion effect’.
Many areas of the brain are insensitive to the lesion making effects of the toxins currently available, so this approach has to be used quite selectively.
A final problem is that these substances may be taken up into the blood supply as it passes through the brain and carried to other parts of the body where they may have toxic effects that confound their effects on the nervous system.
Conceptual problems
Despite these technical limitations, physiological psychologists take the lesion method very much for granted.
If one were to suggest to an electronics engineer that he might want to study how a circuit works by pulling bits out and observing changes in function his reaction would probably be one of horror!
His argument would be that most electronic circuits are organized interactively, by which we mean that the proper operation of one component depends on the normal operation of all of the others.
Pull one part out and all of the others cease to function properly.
As a consequence, the changes produced will tell you nothing about the normal functions of that part.
Gregory (1961) illustrates this nicely.
Imagine, he says, a radio set that works perfectly until you pull out a single component but then proceeds to emit a high-pitched whine.
We would be unlikely to conclude that the function of that component was to inhibit the emission of high-pitched whines.
Knowing how radio sets are typically organized we would be much more likely to conclude that the whole circuit was working incorrectly.
Interactively organized circuits are highly resistant to analysis by the elimination of single components so we can only really use the lesion method if we can convince ourselves that the brain is organized differently.
Gregory was unable to do so and counselled us to abandon the lesion method altogether.
Others, like Weiskrantz (1968) and Dean (1980; 1982), are much more sanguine and argue the brain is generally not interactive so that knowledge of what is lost after a lesion can be used to frame conclusions about the normal function of a region.
Part of the case is anatomical in that some parts of the brain are scarcely connected to others.
We must be careful with this argument because it is sometimes based on anatomical studies that have used insensitive methods or only a partial consideration of the data.
In fact the brain has considerable anatomical scope for being interactive.
The stronger argument is a functional one, that brains do not respond to damage like typical interactive systems.
Interactively constructed devices are extremely sensitive to damage to their components.
This is not surprising since damage to one part of the circuit affects how the rest of it behaves.
In contrast, it is remarkable how little change in function there usually is when the brain is damaged.
The brain is highly resistant to damage.
Engineers know that the way to make something that resists damage is to give it a modular structure.
A modular  structure is one in which groups of components are functionally isolated from each other so that, if one group is damaged, the rest continue to function normally.
In fact, there are two ways of achieving this.
One is to design functionally independent modules that all carry out different functions, the other is to have a number of identical modules, all capable of the same functions.
I shall call the former a ‘complementary’ system, the latter a ‘redundant’system.
Complex electronic devices tend to involve both types of modular organization.
For example, the central processor unit of a digital computer is usually a separate module from the memory and so they have complementary functions.
Within the memory itself there will be a number of chips, each capable of working in the absence of the others, thus employing redundant modularity.
Neither Weiskrantz nor Dean argues that the whole brain is organized in modules, but they do maintain that there is sufficient evidence for modularity in enough systems for it to form a useful working hypothesis.
Resistance to damage can be explained in terms of modular organization but is scarcely conclusive evidence for h.
The apparent persistence of behavioural function after brain damage may equally reflect the insensitivity of our methods of assessing function.
To continue with Gregory's example, his radio might emit a whine so high-pitched that it is undetectable to the human ear and can only be picked up by special test equipment.
What makes the modularity argument more compelling is the fact that brain lesions usually do produce effects, but those effects are highly specific to the area that has been damaged.
Damage to a particular part of the brain affects one function while leaving all others intact.
Specificity means that not only is a function lost or impaired after damage to one part of the brain but that it survives damage to other parts of the brain that, in turn, produces other effects.
This logic is embodied in the double dissociation paradigm (Teuber 1955), which is the most widely used experimental design in physiological psychology.
The approach is summarized in Table 3.1.
Two types of lesion are studied, both groups being given the same two behavioural tests.
A double dissociation is said to occur when Lesion 1 produces a deficit on Task A, but not Task B, while Lesion 2 gives a deficit on Task B but not Task A. A good example of this approach is the work of Schneider (1967), who studied visually guided behaviour in hamsters.
He found that destruction of the visual cortex (Lesion 1) produced impairments in visual discrimination learning (Task A) but not in visual orientation (Task B).
Damage to a visual area in the brainstem, the superior colliculus, had the reverse effect.
There was an impairment on visual orientation (Task B) but not visual discrimination learning (Task A).
There is, therefore, a double dissociation between the effects of visual cortex and superior colliculus lesions, suggesting that they are functionally independent.
Double dissociations don't only favour modular organization, they suggest a particular type: complementary systems.
Redundant systems won't provide such clear-cut results because all of the modules are capable of doing the same  job.
Such a system will be highly resistant to the effects of damage to individual modules and will only begin to show marked alterations in function when a large proportion of the modules has been damaged.
Moreover, the effects of damage will be much more a function of how many modules have been damaged than of which modules have been eliminated.
Table 3.1
The double dissociation paradigm
Wood (1978, 1980) raises the interesting question of whether double dissociations are really a good sign of what I am calling complementary modular organization.
Most physiological psychologists expect the brain to be made up of complementary modules and that lesions will produce double dissociations.
Their experiments are designed accordingly and tasks selected and modified until the dissociations emerge.
If the brain is genuinely composed of complementary modules this is a perfectly legitimate procedure since one is selecting the behavioural task that most closely reflects the functions of the system under study.
If the brain isn't organized in this way then this approach is artificially biased towards finding double dissociations.
In fact, the approach of modifying tasks until dissociations emerge is logically equivalent to what statisticians term ‘optional stopping’.
Optional stopping is the dubious procedure of adding subjects to the sample in one's experiment until a significant difference appears and then terminating the experiment.
What Wood points out is that, if the system were not made of complementary modules but was redundantly organized with function distributed across all components, there are some conditions under which double dissociations would still arise.
This conclusion is not based on studying real brains but on modelling a simple brain with known redundant organization.
Consider a brain designed as in Figure 3.2.
In this brain there are eight ‘neurons’ sending information into the brain and eight sending output, presumably to the muscles.
Each of the input neurons connects to every output neuron.
Output neurons can be activated in one of two ways, either by the ‘output pattern generator’ line, or by the input cells.
The input cells are activated only by sensory input.
The system is designed so that it will ‘learn’ to generate output patterns in response to input patterns.
It can do this because the capacity of an input neuron to activate any of the output neurons increases every time the ‘output pattern generator’ activates the output neuron simultaneously with external activation of the input neuron.
After a number of such pairings a particular pattern of input will come to evoke a selected output pattern, without the ‘output pattern generator’ being activated.
Associative learning will have taken place.
This system is, by  definition, interactive since every input neuron can affect every output neuron.
Using a mathematical model of this system, Wood has explored the consequences of ‘lesions’: removal of one or more neurons.
For a wide range of input and output patterns this system responds to removal of neurons in a surprisingly robust fashion, showing little degradation of function until a large number of neurons have been removed.
What is significant is that with some input patterns the picture is different.
Under the conditions illustrated in Figure 3.3 this system gives a double dissociation.
Figure 3.2
A simple associative network
There are three types of neurons, labelled X, Y, and Z. The X-neurons and Y-neurons are input cells, the Z-neurons, output cells.
Each Y-neuron contacts only one output cell and is capable of activating that cell.
Each X-neuron contacts all of the output cells.
The activity of a Z-neuron is therefore, the result of summing the effects of all of the X-neurons on that cell.
The strength of the synaptic contact between an X-neuron and a Z-neuron is increased every time activity in the X-neuron coincides with activity in the Z-neuron induced by input via its Y-neuron.
Removing input neuron 1 produces difficulties with remembering output pattern A but not B. Removing input neuron 2 produces the reverse effect.
Thus, by selecting the right ‘task’, we have constrained a system we know, because we designed it that way, to be organized redundantly to give a double dissociation.
Under these circumstances the process we know to be distributed across all of the input/output cell connections appears to be localized in one neuron.
Even with a simple system like this we cannot draw direct conclusions about the functions of a component from knowing what is lost when we destroy it.
Figure 3.3
Training an associative network
Figure 3.3 (continued)
Simulation of the effects of training a simple, eight-neuron, associative network with four combinations of input patterns and then removing one of the X-neurons.
Columns F1 to F4 represent the four training patterns on the X-input cells and rows G1 to G4 represent four ‘forcing’ patterns on the Y-inputs.
Columns Z1 to Z8 show the resulting strengths of synaptic contacts between each X-input neuron and each Z-output neuron.
Rows Go1 to Go4 illustrate the magnitudes of the outputs produced at each of the output neurons (Z1 to Z8) by applying the four training patterns, F1 to F4.
Go1 shows what is produced by applying F1, Go2 shows what is produced by applying F2, and so on.
The right-hand side of the figure shows what happens if input neuron X1 is removed after training, represented by zeros in the X1 row, and the four training patterns applied again.
The modified patterns are represented by F'1 to F'4.
The resulting output patterns are shown in the bottom right-hand block.
Again, row Go1 is associated with input pattern F'1, and so on.
The output for training patterns F'1 to F'3 is little affected by removal of X1, but the response to F'4 is greatly distorted.
Removing neuron X2 has a similar effect on response to training pattern F'3.
Those of us who want to go on using the lesion method shouldn't be too despondent about Wood's results because the conditions under which a system like this will give double dissociations are likely to be very rare in nature.
Close inspection of Figure 3.3, which is based on Wood's simulation, show's that two conditions have to be satisfied for this sort of result to emerge.
The first is that the inputs in the two tasks are restricted to two different neurons.
The second is that these neurons must not be involved in any other task.
If they are, then the system simply won't learn the two tasks in the first place but will consistently generate inappropriate responses.
If one task uses only one input neuron and the other uses the same neuron as part of a pattern of input, then the system responds to activity in that neuron as if it is a part of the larger pattern.
This capacity to generate the same response to part of a pattern as to the whole of it is one of the merits of network systems (see chapter 7).
In fact, the level of independence of inputs required for this sort of system to produce double dissociations is only likely to be achieved if it is composed of complementary modules.
Many studies stop at demonstrating double dissociations.
This is unfortunate because there is a more stringent test of complementary modular organization; combining lesions to two modules should have effects that are the sum of those of the individual lesions.
When lesions are studied in combination it is surprising how often this second test is failed.
Returning to the visual system, it is known that simultaneous destruction of both visual cortex and superior colliculus produces complete insensitivity to brief light flashes in monkeys whereas destruction of either structure in isolation has no effect (Mohler and Wurtz 1977).
This is compatible with the other version of modular organization, redundancy, but it is equally compatible with the system being  interactive, if we assume that removal of any one component has effects that are not detected by the behavioural task.
This illustrates another problem, that the only effective difference between an interactive and a redundant modular system is the degree of disruption produced by damage to a single component.
As yet we have no agreed criteria for how much disruption a single lesion needs to cause for the system to be deemed interactive, although common sense suggests that an interactive system would be much more vulnerable to disruption than the brain appears to be.
A further problem with the lesion method is how to move from the symptoms produced by the damage to a description of the normal functions of the brain area concerned.
It isn't safe to assume that the normal functions of a system are equivalent to what is lost after a lesion because the effects of a lesion may be masked by positive symptoms.
A positive symptom is something that happens in the brain-damaged animal that doesn't happen in the normal.
A good example of a positive symptom is the spasticity that occurs after some forms of damage to the motor system.
In spasticity, all of the muscles controlling a limb tend to be contracted at the same time, so that the limb locks into position.
It is clearly circular, and not very helpful, to argue that the normal function of the structures concerned is to suppress spasticity.
Disconnection
Before leaving lesions in the central nervous system there is one variant that needs mention: disconnection.
Instead of trying to eliminate neural centres the aim is to sever the pathways that connect them, leaving the centres themselves intact (Geschwind 1965).
The aim here is to find out how much a system can do without the benefit of other forms of input.
By concentrating on what survives after a lesion, rather than on what is lost, this approach seems more readily interpretable.
The best example of the application of this approach is the work on the effects of cutting the corpus callosum in humans (Gazzaniga 1985).
Some thirty years ago an operation was introduced to control the spread of epilepsy from an affected cerebral hemisphere to the other, unaffected one.
This operation involved cutting the main fibre tract that joins the two halves of the brain, the corpus callosum.
This operation has little effect on day-to-day behaviour, although it is not completely without consequences.
It has attracted the interest of physiological psychologists because it enables them to study the operations of one cerebral hemisphere, isolated from input from the other.
Conclusions about the functions of one hemisphere that had previously been based on observations of the effects of injury could be confirmed by studies that showed that the self-same functions were retained by that hemisphere after section of the callosum, but were not available to the other.
For example, many studies have shown that language is vulnerable to left, but not right, hemisphere damage in most people, suggesting that the left hemisphere has a special role  in language.
In split-brain patients it has been shown that the left hemisphere retains language function, the right does not.
Not only is the left hemisphere normally necessary for language, it is also sufficient.
That adds significantly to our knowledge of the role of the brain in language, and also confirms our faith in the conclusions that we can draw from lesions.
Analogous studies outside the central nervous system (CNS)
During the 1930s and 1940s physiologists were deeply concerned with the role of feedback from the digestive system in the control of hunger.
Some authors argued that such feedback was the primary signal for hunger while others argued that the signal came from elsewhere.
The issue seemed to be settled when a number of studies were carried out that showed that animals and humans still regulated their food intake even after the stomach had been removed, an experimental procedure analogous to ablation of part of the CNS.
More recently, people have stated to reinvestigate the role of the stomach in hunger and have found that, when the stomach is intact, feedback from it plays a vital part in controlling food intake (McHugh and Moran 1985; Rolls and Rolls 1982; Smith and Gibbs 1979).
In this instance, when dealing with something as simple as the digestive system, the lesion method gave a false picture of the role of a particular component.
This is because the method poses what may be a false question, ‘is this structure necessary for the process I am studying?’, when what we really want to know is ‘what part does this structure play in the process I am studying?’
Stimulation
Many psychologists are very unhappy about lesions.
They are much more at home with stimulation.
Stimulation involves feeding a signal into some part of a circuit and measuring its consequences at some other point.
Since you are not altering the brain in any other way, the way the brain is organized would not seem to affect the interpretation of stimulation studies.
Stimulation will work as well for interactive as for modular systems.
Nevertheless, in reality there are difficulties with this method that limit its use.
One of these is technical, the other conceptual.
The technical problem is that of ensuring that the stimulation is of a duration and intensity to be physiologically realistic.
Ten minutes of stimulation at 100 volts in a system that operates on a millisecond timescale and deals in ion currents that generate a few thousandths of a volt is scarcely going to simulate what happens normally in the brain.
The conceptual problem is that of interpreting the effects of localized stimulation in a system that probably deals with patterns of input.
Stimulation can either be electrical or chemical.
Electrical stimulation is delivered via electrodes, often similar to those used in lesion-making studies.
Chemical stimulation is delivered via fine tubes, cannulae, lowered into the  intended stimulation site.
Usually stimulation devices are positioned using a stereotaxic device.
The main technical problems concern delivering stimulation at an intensity that mirrors the level of activity that occurs spontaneously in the brain, and determining which structures have been affected by it.
Whether one uses electrical or chemical stimulation there is inevitably a gradient of intensity of stimulation as one moves away from the source.
That means that in order to stimulate over a large area the intensity of stimulation at the input site is likely to be many orders of magnitudes in excess of normal levels of electrical or chemical activity.
Not only are there likely to be unphysiological levels of stimulation, but it is frequently unclear, from published reports, how extensive the effective stimulation zone actually was.
Something can be learned from relatively crude techniques in which there is a large stimulation zone and high stimulation levels in the centre.
They suggest the sorts of function an area might be involved in, and some degree of localization is possible by varying stimulation sites and finding the one that produces the largest response for the lowest level of stimulation.
Nevertheless, by itself the traditional stimulation approach, using large sources of stimulation, is imprecise.
Recognizing these problems, many people engaged in electrical stimulation work have moved to microstimulation.
This involves stimulating through microelectrodes of the son used in single unit recording studies (see below), with very low currents at a level calculated to affect a small population of adjacent cells or fibres.
With this technique it is possible to restrict activation to a few cells and, by simultaneously monitoring single unit activity through a nearby recording electrode, establish the extent of spread of effective stimulation.
What is remarkable is that, in some systems, such as the deep layers of the superior colliculus and the motor cortex, such stimulation still elicits reactions such as eye movements or muscle twitches (Donoghue and Wise 1982; Schiller and Stryker 1972).
With these procedures it is possible to measure the extent of the effective stimulation zone, rather than assuming it.
Even if one has a stimulus of physiologically realistic intensity, that doesn't necessarily mean that one can readily interpret the effects of stimulation, because there are still two problems.
The first is that the brain is organized with a considerable amount of parallel wiring, so that information about the same event may be encoded in the activity of a number of cells, not necessarily adjacent to each other.
The second is that the electrical activity of the brain is temporally patterned: when a cell is active or inactive is as important as how much it is active.
Unless one knows in advance how information is represented in part of a brain one can neither stimulate the right combination of elements, nor produce the correct temporal pattern of activity.
Stimulation, therefore, only provides readily interpretable data when one either knows a lot about the brain, or the brain is organized in a simple way.
Stimulation provides readily interpretable data under two conditions.
One is if local elements are arranged in parallel with independent access to the output mechanisms.
The other is if a system is organized so that the amount of activity  in its neurons is more important than their spatial or temporal pattern.
The first condition is satisfied in parts of the motor system, in which cells in the brain project directly on to motor neurons in the spinal cord so that a small group of cells in the CNS have a ‘direct line’ to a small group of muscles, and perhaps in visuomotor centres in the brainstem (Donoghue and Wise 1982; Schiller and Stryker 1972).
In both of these, microstimulation leads to discrete, repeatable movements.
The second is probably satisfied in the ascending catecholaminergic pathways running through the hypothalamus (Stricker and Zigmond 1976; Stricker 1983).
They are not obviously satisfied elsewhere and it is significant that these are the main areas that have proved susceptible to analysis by stimulation.
Analogous studies outside the CNS
Stimulation has been the method of choice for most researchers studying the peripheral mechanisms of eating and drinking.
By producing precisely controlled changes in the concentrations of nutrients or salts in the blood it has been possible to explore the role of factors like blood glucose level or cellular dehydration in eating and drinking (LeMagnen 1985; Rolls and Rolls 1982).
Within the last decade, analogous methods have been used to study the role of the gastro-intestinal tract in satiation.
By loading specific parts of the system with food or water it has been possible to investigate, for example, the role of the stomach in the termination of eating and drinking.
What emerges from this work is the complexity of the interactions between components of the system.
For example, loading the stomach with food or water directly has some satiating effect, but not as great as when the substance is also allowed to pass into the intestine or has previously passed through the mouth.
These findings raise another problem with stimulation methods, which is that the effects of stimulating at individual points in a system are only readily interpretable when the effects of activating separate parts of a system are additive.
That is to say, the effects of activating two components together have to be the sum of the effects of activating either in isolation.
If this doesn't occur with a relatively simple system like the peripheral mechanisms controlling appetite, there must be some doubt about how often this condition is satisfied in the central nervous system.
Recording
If part of the brain is especially important for a particular process, there should be some change in the activity of that area when that process is evoked.
An area involved in memory should be more active during tasks that have a high memory component than during those that place few demands on memory.
For many years ‘activity’ in the brain has been considered synonymous with electrical activity.
That the brain is electrically active was established a century  ago.
Some fifty years ago the engineers came up with devices that could record the electrical activity of living brains and living nerve cells.
Physiologists capitalized on this instrumentation to demonstrate that the electrical activity of the brain and its nerve cells was a function of what the individual was doing, or not doing, or how sensory receptors were stimulated.
All along they were also aware that brains were composed of living cells that were active in non-electrical ways as well, but until recently the techniques for looking at this other activity were not available.
The situation has changed and it is now possible to determine, for example, how much oxygen is being used by a particular area of the brain while a subject does a task like reading or memorizing a word list (Raichle 1983).
A wide range of electrical recording techniques are currently available, although not all are suitable for studying behavioural processes.
The most important distinction is between those that register the activity of large numbers of cells and those that record action potentials from individual units.
The simplest of the first type to record, but most difficult to interpret, is the electroencephalogram (EEG), in which the activity of the brain is monitored continuously through the scalp.
This is used widely to determine the functional state of the brain, but is difficult to relate to the operations of different areas.
More useful in this respect are ‘event-related potentials’, which are extracted from the EEG by signal-averaging techniques (Donchin 1984).
They rely on the fact that although the EEG is very noisy there are, embedded within it, consistent changes in voltage that are time-locked to specific events like the presentation of stimuli or the performance of movements.
By averaging the EEG across a large number of these events the noise cancels out and the signal emerges.
Considerable localization is possible.
Moreover, because the potentials have complex waveforms quite subtle analyses of the impact of experimental variables are possible.
For example, it is possible to compare the effects of two experimental conditions on early or late components of the response, the former often being considered to represent ‘sensory’ events, the latter ‘cognitive’.
Event-related potentials require many trials for the averaging procedure to work.
They are, therefore, insensitive to trial-by-trial variations in performance or experimental conditions.
They are also insensitive to the possibility that an area is, in some way, functionally heterogeneous.
Recording from single units provides this additional information.
Obviously it is rarely done in humans, but is routinely carried out in non-human subjects.
Early work focused on single cell correlates of sensory stimulation in anaesthetized animals, but advances over the last twenty years have enabled us to monitor the single-cell correlates of behavioural activity in alert, freely moving animals (O'Keefe and Nadel 1978; Olds et al .
1972).
In some respects, action potentials are the tip of the neuronal iceberg.
There is a lot of cellular activity, that may be relevant to behaviour, that does not get translated into action potentials.
In anaesthetized, immobilized animals, this  may be studied using intracellular recording techniques, in which very fine microelectrodes are passed into the cell, but these don't work well in freely moving animals.
The more promising approach is to study the metabolic activity of the brain during behaviour.
Two approaches have been developed.
One, now widely used in animals, involves taking a ‘snapshot’ of the metabolic activity at a particular stage in an experiment (Sokoloff, Reivich, Kennedy, DesRosiers, Patlak, Pettigrew, Sakurada, and Shinohara 1977).
The other allows for continuous monitoring (Raichle 1983).
The snapshot technique involves injecting the animal with a radioactively labelled version of glucose, 2-deoxy-glucose (2DG), that ‘sticks’ in cells.
The 2DG gets locked into cells in an amount proportional to that cell's metabolic activity (Sokoloff et al .
1977).
If the animal is killed and its brain cut into thin microscope sections, the concentration of glucose in different parts may be determined by measuring the concentration of radiation.
The need to kill the animal restricts the value of this method.
Potentially more useful are the non-invasive monitoring techniques based on Positron Emission Tomography (PET) scanning (Raichle 1983).
In this technique metabolically significant compounds, like glucose or oxygen, are given a very low-activity, short-lasting, radioactive label which decays in a highly characteristic way, giving off sub-atomic particles called positrons.
By surrounding the subject's head with a detector array, connected to suitable computer hardware, it is possible to build up images of the brain, indicating the areas of different levels of radioactivity, and hence of metabolic activity.
As yet this technique has limited spatial and temporal resolution but it does offer a means of looking at human brain activity in a fairly safe way.
Apart from its limited spatial resolution the main disadvantage of this technique is that, for it to be safe, it has to be done with isotopes with extraordinarily short half-lives and that means that they have to be prepared on site.
As a consequence, only laboratories with immediate access to particle accelerators can carry out this sort of work.
In some ways recording techniques offer the most ‘direct’ access to the brain and promise to provide objective indices of what part of the brain does.
Nevertheless, there are still problems.
Many of these are technical.
For example, people who wish to record the activity of single cells in the brains of freely moving animals argue about whether to use very fine electrodes that can record from even the smallest cells but give very unstable recordings, or to use larger electrodes that bias the sample to larger cells but give more stable recordings (O'Keefe and Conway 1978; Olds et al .
1972).
Since it may take many hours to establish what influences the activity of a single cell, this is a far from trivial issue.
As many parts of the brain contain cells of varying size, which may not all be involved in the same functions, biases of this sort may determine which functions are most strongly associated with unit activity.
With studies of metabolic activity there are still problems with calculating how much of the radioactivity in any part of the brain should be attributed to the utilization of metabolic fuels by the areas concerned, rather than simple uptake.
The difficulties with recording are not only technical, they are also conceptual.
They relate to the issue of how information is represented in the nervous system.
The basic question is this: is information represented in the brain at the level of individual cells or at the level of groups of cells?
An analogy with what happens in digital computers illustrates this point.
Computers are effectively large arrays of switches, which can be either ‘on’ or ‘off.
The commands being executed are represented as a pattern of ‘on’ and ‘off’switches in the central processor unit, so that a switch that is in the off state is carrying as much information as one that is on.
In simple computers there are typically eight such switches, still allowing a large number of possible patterns.
For example, having the first four on and the last four off may lead the computer to add together two numbers while the reverse may lead it to subtract them.
It is clear that, in this example, any single switch in the Central Processor Unit (CPU) will be involved in all instructions.
Nevertheless, in some instructions it will be ‘on’ but in others it will be ‘off’.
Finding the switch in the ‘on’ state would be good evidence that it was involved in an instruction.
Finding it ‘off’ would be equivocal.
Only by knowing how the processor was constructed could you say whether that state signified anything.
If representations depend on single units then changes in single cell electrical activity or overall metabolic activity can be fairly easily related to the function of the systems concerned (Barlow 1985).
Any set of conditions that leads to an increase in the activity of an area does so because it is tapping its functions.
Conditions that do not do so are irrelevant to its functions.
If a process is distributed across a number of elements of a system then the situation is much more complex.
Activity in any single unit can farm part of many representations and the inactivity of a cell can be just as important for representing something as its activity.
The same argument applies in principle to studies of metabolic activity.
In a system that encodes information in terms of patterns of activity information processing could be going on without a net increase in metabolism.
In practice this is unlikely, however, because changing the state of a cell is likely to involve energy consumption irrespective of whether the change involves an increase or decrease in activity.
When recording single cell activity there is an additional question: how do you know that the changes detected by an experimenter are detectable by other parts of the brain?
In other words, how do you distinguish between those changes in neural activity that reflect the coding of information in the brain and those that are merely signs of activity with no functional significance?
For most of us, some evidence of increased or decreased neural activity after the onset of some experimental condition seems sufficient evidence that something is being encoded, but as Burns and Lennie have pointed out (Burns 1968; Lennie 1981), the brain has no knowledge of these outside events.
Its job is to decide whether some change in activity in a cell reflects the presence of such an event or is simply a spontaneous change unrelated to function.
Both suggest that the brain does this by monitoring the statistical properties of  activity.
Information is then encoded as deviations from those statistical properties.
In other words, it is not enough that a cell should be more or less active; the change must be sufficiently unusual statistically for it to function as a code.
A further problem comes in deciding how to interpret differences in the amount of activity elicited by a set of conditions.
Does the brain use all of this information, or does it work on a trigger basis so that, once a level of activity has been exceeded, the actual level is irrelevant?
For example, if a cell doubles its discharge rate under one set of conditions but quadruples under another, does that difference affect how subsequent levels in the circuit respond so that one group of cells is excited by the first set of conditions but a second group is activated by the second set?
These issues are far from resolved and affect our interpretation of some potentially significant findings.
Analogous studies outside the CNS
Numerous attempts have been made to correlate activities like eating and drinking with spontaneously occurring variations in the state of the body, like blood glucose level or salt concentration.
In general these studies have tended to reinforce the conclusions based on stimulation studies and have yielded few surprises that affect the validity of the method.
Overview
Apart from the fairly depressing conclusion that it is very difficult to study neural mechanisms of behaviour, what else is there to learn from this summary of methodology?
There are two main points that the reader needs to bear in mind when reading the rest of this book, or any other on physiological psychology.
The first is that, although all methods have their shortcomings, they are not always the same.
Consequently, although experiments conducted using a single methodology are open to many interpretations, studies of the same process using a variety of methods should be less equivocal.
The second is that the experiments we do are only as good as the psychological theories that underlie them.
If we don't understand the psychological processes that form the bases of performance on the tasks that we use in our behavioural studies our chances of understanding how the brain controls behaviour are limited.
A good example of convergence comes from studies of hemispheric asymmetry in man.
It has been known for many years that damage to the left hemisphere is more likely to affect language ability than damage to the right, suggesting that the left hemisphere is very important for language.
Disconnection studies in split-brain patients supported this conclusion, showing that the left hemisphere is sufficient for language.
PET scan studies of metabolic activity during various forms of cognitive performance have further reinforced this point, showing that not only is the left hemisphere more active  than the right during linguistic function but also that the sub-areas implicated in language by the lesion studies are more active than surrounding areas.
Not only does this convergence substantiate our theories of specific psychological processes, it also enhances our faith in our methods, suggesting that the working assumptions we have made to justify their use may, in fact, be valid.
Nevertheless, convergence isn't always forthcoming.
For example, single unit recording studies suggest that the hippocampus is involved in classical conditioning (Thompson 1983) but lesion studies show that classical conditioning proceeds quite happily without a hippocampus.
Having shown that a part of the brain is involved in a process like language, for example, we are then faced with the question of specifying the nature of that involvement, which depends on having a serviceable psychological model of that process.
Presumably the area is not responsible for the whole of language, only a component of linguistic performance, but what do these components look like?
Psychological studies could provide some answers, but it is not obvious that the subprocesses of language, identified by the psychologist, will map on to individual brain areas, any more than the computational processes of a digital computer map on to individual parts of its hardware.
Even if we can show that this mapping occurs, it is not a complete answer, since we then need a theory of how operations conducted at the neural level translate into behavioural subprocesses.
Methodological problems in psychobiology are not going to go away.
Their existence should not be taken to mean that studying the neural bases of behaviour is beyond us, but they should be recognized when designing and interpreting experiments.
Since all of our methods rely on making working hypotheses about the functional organization of the brain, these assumptions should be kept in mind all the time, and, if necessary, modified in the light of experimental evidence.
Perception
Perception is the basis of all action.
Without perceptual systems to provide us with knowledge of the outside world we are impotent.
The job of perceptual systems is to provide us with representations of what is happening in the outside world, representations based on information gathered from receptors based in different parts of the body that are tuned to specific classes of physical events.
This process of building up internal representations turns out to be considerably more complicated than was initially thought.
Our eyes are not cameras, our ears are not tape-recorders, faithfully but passively recording what is in the outside world.
The physical events that activate our sense organs are already imperfect versions of the properties of objects that we wish to know about.
For example, the perceived colours of objects are only loosely related to the wavelengths of light that they reflect into our eyes.
Two objects may reflect the same wavelengths into our eyes yet be seen as having different colours.
The same object may reflect different wavelengths at different times yet be seen as having the same colour.
This is the stuff of colour constancy.
Our sense organs are not passive relays, but encode the constantly fluctuating patterns of stimulation impinging on the receptors into fluctuating patterns of neural activity.
These patterns are related to the patterns of input but already selective recoding has begun, since some aspects of the input will activate the nervous system more than others.
For example, edges tend to activate the visual system more than areas of uniform brightness.
Changes of pressure across the fingertips produce much more intense input from touch receptors than steady pressure.
You can verify that for yourself by running your fingertips ova a slightly rough surface and then stopping.
The roughness that you feel while moving the fingers suddenly disappears.
The job of perceptual systems is to take these fluctuating patterns of activity occurring at the receptors and interpret them in terms of what is going on in the outside world.
Many of the issues in the contemporary study of perceptual systems have come about because of our improved understanding of how the systems might work, based largely on people's attempts to build models of perceptual processes.
One of the most exciting developments in recent years has been the degree of convergence between models of visual function built up by computer  scientists trying to figure out how to build machines that can see and physiologists trying to work out what different parts of the visual system do.
Other issues have derived directly from the study of perceptual systems.
Two of the most challenging have been the demonstration of extensive parallel pathways conveying information about the same sensory modality into the brain and the demonstration that sensory systems contain multiple representations, or maps, of the outside world.
Some of the most interesting advances in perception have been made in the study of vision.
The plan of this chapter is to review the main issues in the study of vision and then to consider how far what we have learned about the visual system can act as a model for understanding how other perceptual mechanisms work.
The visual system
The traditional view of the visual system is that it involves a cascade of processing stages running from the retina to the ‘association’ cortex.
At each stage in the cascade information about the retinal location of the input is lost and is replaced with information about the properties of the stimulus object until, at the highest levels in the system, the identity of the object or objects is encoded.
The system is supposed to be organized hierarchically, so that the properties of the information processing units, presumably single cells, at one stage are due to the convergence of inputs from a number of units at the previous level.
Many anatomists identified three such stages of visual analysis in the cortex.
In the first, primary visual cortex, V1 or area 17, as it is also known, there was considered to be a fairly faithful map of the retina with relatively little recoding of information having taken place between the eye and this part of the brain.
In the second stage, usually identified with areas 18 and 19 (also known as prestriate cortex), information about where things are on the retina is lost and replaced with information about where they are in relation to each other.
For example, Kolb and Wishaw (1985) claim that areas 18 and 19 contain cells that are maximally excited by the presence of corners.
In the third stage, identified with areas 20 and 21 in the temporal lobe by Kolb and Wishaw (1985) and with the parietal cortex by Luria (1973), information about the retinal location of stimulus elements is largely lost and replaced with some form of representation of the object giving rise to the pattern of retinal stimulation.
This model, based initially on the fairly crude neuroanatomical data available a century ago, has stood the test of time remarkably well but data obtained over the last two decades have prompted modifications in the details.
One of the most interesting findings to come out of recent research is that the visual system consists of a set of circuits arranged in parallel, rather than an hierarchically organized cascade.
Parallel organization pervades the visual system at all levels.
In most species there are at least three distinct classes of  ganglion cells in the retina that conduct information through to the visual cortex in physiologically distinguishable streams.
The optic tract projects to a number of anatomically distinct regions in the brainstem, each of which makes a unique contribution to visual function.
Finally, at the level of the cortex, information is relayed in parallel streams to a set of cortical areas, each of which is devoted to analysing a particular attribute of the visual field, such as colour or stereoscopic depth.
These developments have greatly enhanced our understanding of how the visual system functions.
On the other hand, studies of the higher stages in the visual system, at the level at which information about the attributes of objects, such as colour or depth, is integrated into representations of objects, have been largely disappointing until recently.
In part this has been due to a failure to look at the right parts of the higher order visual cortex, using the right stimuli.
In part it has also been due to a lack of suitable models of how the combination of attributes could take place.
Without such models it has been difficult to determine what the properties of these cortical areas should be.
Inevitably, much of the stimulus for our changing views of the visual system has come from empirical studies but, over the past decade especially, attitudes have also been changed by the development of detailed theoretical models of how visual information processing might take place.
This computational approach has proved exceptionally useful in illuminating many of the findings emerging from the experimental literature.
The computational approach
There are two strategies for finding out how the visual system works.
The first is to take visual systems apart, identify their components and characterize the way these components work.
The second is to look at what visual systems do, build something that will do the same job, and then determine whether it works in the same way as a real visual system.
The former strategy is the traditional experimental approach, the latter is the computational approach.
In reality it is difficult to divorce them completely.
Even the most hardened empiricists start off with some idea of how the system might work.
They have to, in order to be able to decide what constitutes a sensible experiment.
Computational modellers, for their part, usually have some idea of how the visual system is put together and let that guide their model building.
Nevertheless, there are features peculiar to the computational approach that have important implications for physiological psychologists and these are the topic of this section.
Historically the computational approach has been most actively pursued by engineers eager to build machines that can do the same things as humans.
They concentrate, in the first instance, on how things could, in principle, be done rather than focusing on whether humans do things in that particular way.
Their approach is to go back to first principles, looking at the constraints that link  objects and events in the real world to what happens in the retinal image and then looking for rules that can be used unambiguously to recode retinal events in terms of the outside world (Poggio, Torre, and Koch 1985; Ullman 1986).
Take, for example, the problem of an edge.
To most of us the location of boundaries is self-evident but, surprisingly, boundaries of an object are far from obvious in the raw visual image which consists of a continuously varying distribution of light intensities across the retina.
The location of edges has to be reconstructed by the visual system.
Is there an invariant property that links edges in objects to the retinal image?
Marr (1982) and others have argued that there is, and that it is the fact that edges are associated points in the image where the intensity is changing most rapidly.
To locate edges in images all you need is some means of calculating the rate of change of intensity of the image and locating the points where that rate is maximal.
This analysis corresponds to what Marr terms a ‘computational theory’ of edge detection, since it specifies the goal of the computation and the strategy by which it will be carried out.
Needless to say, there are many possible ways of turning this rule into practice.
The selection of the most appropriate way is the subject of the next stage, the stage of representation and algorithm, in which a set of decision criteria for identifying maximum rate of change is established.
The mathematical principles of determining where the rate of change is maximal are, for mathematicians, very straightforward.
They involve the process of double differentiation.
If you knew the equation relating light intensity to position in an intensity profile it would be a relatively simple matter to apply double differentiation and extract the points of maximum change.
Unfortunately, your visual system cannot do this.
It can only work on local regions of the image and, moreover, in the real world the distribution of light intensity is two-dimensional rather than one-dimensional.
As a consequence the mathematics become more complex.
Nevertheless, there is a computation that can be carried out on local areas of the image that will do the same job.
When this computation is carried out on local areas of the image, the product of the computation goes from positive to negative at the sites of maximal change.
These transitions from positive to negative, known as ‘zero-crossings’, provide an indication of the location of edges.
From the point of view of an engineer wanting to make a machine that can ‘see’, these two stages are adequate.
From the point of view of the physiologist or psychologist another stage is necessary, that of turning an algorithm into the hardware of the brain.
It is unlikely that any component of the nervous system carries out exactly the same computation as the zero-crossing detector described in the previous paragraph but it is possible to make a simpler device that works very much like a zero-crossing detector.
This device is a spatial filter with two antagonistic inputs, an excitatory input from a narrow central region and an inhibitory input from a broader region that covers the centre but extends beyond its boundaries.
The filtering occurs because the effectiveness of a stimulus varies with its location, being normally distributed  around the centre of the receptive field of the device.
Thus point stimuli located at the centre of the receptive field produce a large output while stimuli falling away from the centre produce a smaller output.
The effect of the inhibitory zone is subtracted from the output from the excitatory region.
Such a device is known as a Difference of Gaussians or DOG filter (Figure 4.1).
The reason for this amount of detail is that DOG filters are not just a figment of the imagination, they are actually embodied in a class of retinal ganglion cells, the X-cells.
Figure 4.1
An idealized Difference of Gaussians (DOG) function
This results from subtracting the effects of a broad, inhibitory surround from a narrow, excitatory receptive field centre.
A DOG filter is not an edge detector.
It needs another stage to interpret its output and locate the zero-crossing it may have encoded.
The problem is that, because it is simply a filter, the output of a DOG will vary with both the location and the contrast of the edge that is present.
Since it gives only a single output there will be no distinction between the zero in its output due to an edge being properly located in its receptive field and a zero due to the absence of an edge altogether.
One solution to this problem is to sweep the image  backwards and forwards across the retina so that edges modulate the output of the DOG filter.
A subsequent stage could then monitor the change in output over time and locate edges at the points at which the cell's output changes from below to above background as the edge sweeps over the receptive field.
In practice this wouldn't work because the detectors would need impossibly precise information about the speed of eye movements and the phase lag between the central movement commands and the actual movement of the image.
Marr and his colleagues have come up with a simpler explanation which relies on the fact that there are two classes of ganglion cells, on-centre and off-centre.
Both will fire more when an edge is placed slightly off-centre than when it is symmetrically placed, an on-centre cell when there is more light than dark edge in the field, and the off-centre cell when the conditions are reversed.
If you take an on-centre and an off-centre cell with slightly overlapping receptive field and add their outputs this value will be greatest when an edge is positioned between the two.
Thus unambiguous edge detection and localization can be achieved by feeding the outputs of two such cells into a subsequent level of the system in which cells will only respond when both of the input cells are active.
This, Marr and his colleagues argue, is precisely what happens in the visual cortex (Figure 4.2).
I have spelled out the computational approach to edge detection for two reasons.
The first is to show the level of attention to detail required to make  the approach work.
It is not enough to say that the visual system must contain edge detectors, one also has to say how edge information could be reconstructed from the retinal image and how far the units in the CNS that appear to respond to edges do, in fact, operate according to these computational principles.
For example, it is necessary to show mathematically that a DOG filter will act as a zero-crossing detector and that there are cells in the visual system that operate as DOG filters.
The computational approach is, therefore, much more rigorous intellectually than that to which many psychologists have been accustomed.
The second reason for spelling out the approach is to highlight the limitations in current attempts to relate computational models to the actual operations of the visual system.
Figure 4.2
An edge detector circuit
Circuitry that will convert the output from DOG filters into an unambiguous signal about the position of an edge.
‘A’ represents an ‘on-centre’cell that is maximally excited by the onset of stimulation in its centre and ‘B’represents an ‘off-centre’cell that responds maximally to the offset of stimulation in its centre.
Both cells will be most active when an edge is just off-centre of each, with the bright area over the ‘on-centre’ field and the dark area over the ‘off-centre’field.
The presence of an edge, signalled by simultaneous activity in the two cells, is detected by the AND-gate.
At present computational models of the visual system are running ahead of our neurophysiological understanding.
Marr (1982) describes many processes that have yet to be mapped on to the nervous system.
Nevertheless, even if the level of precision demanded by the computational approach has not yet been realized in most instances, the approach does have other benefits.
The main one has been to make us think more closely about how the visual system might be organized.
One of the major points to emerge from computational theory is that the best way of achieving a representation of the environment is via a modular system that represents different properties of the environment in different parts of the system.
Mathematically this is relatively straightforward.
It means little more than that you can compute the direction and velocity of movement of an object independently of its colour or its depth.
In terms of how the visual system is organized it raises the exciting possibility that different properties of objects are computed by anatomically distinct circuits.
As we shall see, this prediction is being borne out by current work on the visual cortex.
The parallel visual system
Parallel pathways exist at a number of levels in the visual system.
At the gross anatomical level the optic tract can be seen to terminate in a number of discrete relays in the brainstem.
Of these the largest are the dorsal lateral geniculate body, in the thalamus, and the superior colliculus, in the midbrain, but there are a number of other sites which also receive direct retinal input.
These include the pretectum, the ventral lateral geniculate body, the suprachiasmatic nucleus of the hypothalamus, and the three terminal nuclei of the accessory optic system in the midbrain.
Initially it was believed that only one of these, the dorsal lateral geniculate nucleus, was involved in visual perception and that the rest were involved in visual reflexes like pupillary control (the pretectum), orientation movements (the superior colliculus), and stabilization of the visual field when the head moves (the accessory optic system).
Since the late 1960s, however, a number of authorities have argued that some of these other pathways are also involved in visual perception.
The issue is far from resolved.
One of the most provocative findings in recent years has been the degree of parallel organization hidden within individual anatomical regions.
Enroth-Cugell and Robson (1966) started the ball rolling by showing that, in cats, there are two functional classes of ganglion (relay) cells in the retina.
One class, the X-cells, responds in a sustained way to effective stimuli but only when an edge is located in the correct part of the receptive field.
The other class, the Y-cells, responds transiently to steady stimuli but will respond to an edge no matter where it is located in the receptive field.
Since 1966 our knowledge of ganglion cell classes has increased in leaps and bounds (Stone 1983).
A new class, the W-cells, has been added, descriptions of the properties of the three classes have been extended, and it has been shown that this segregation into three classes extends to subsequent stages in the visual system, including the visual cortex.
People like Sherman (1985) now argue that there are three separate streams or channels of information passing up from the retina and through the visual cortex.
The great challenge is not so much describing the physiological and anatomical properties of the cells in these streams but understanding their functional significance.
Independently of the discovery of X-, Y-, and W-cells came the demonstration of multiple representations of the visual field in the secondary visual cortex.
This is most clearly seen in studies of the primate visual cortex (Maunsell and Newsome 1987).
Visual field representations are mapped by sampling successive locations in the visual cortex with a microelectrode and, at each location, determining where you have to place a stimulus in the visual field to activate single cells at that location.
You find that cells in adjacent parts of the visual cortex are activated by stimulation in adjacent parts of the visual field.
The cortex is said to be retinotopically mapped.
It has been known since the turn of the century that the primary visual cortex must be retinotopically mapped.
What came as a surprise was that the secondary visual cortex contained not one further map but a number of them, at least five.
In some of them the retinotopic organization is quite loose but in others it is very precise.
Why are there so many visual areas?
What is their relationship to each other?
What is their relationship to the visual cortex?
The subcortical visual system and visual perception
A distinction is often drawn between the relatively direct pathway from the retina to the visual cortex, by way of the dorsal lateral geniculate body, and the projection to other sites in the brainstem.
These other sites are often referred to as the subcortical visual system.
The question is whether this system contributes to visual perception in the normal brain.
The strongest evidence for such a contribution comes from studying the effects of removal of the primary visual cortex.
In most mammals, apart from the cat, the dorsal lateral geniculate nucleus projects exclusively to area 17.
Ablation of area 17 leads to almost total loss of cells in the dorsal lateral geniculate body, a process known as retrograde degeneration.
Nevertheless, mammals that have had the whole of area 17 removed surgically can still engage in non-reflex visually guided behaviour.
They wilt readily learn to distinguish light from dark and there is some evidence that they can also distinguish patterns and colours, although it must be said that this capacity is extremely limited compared with the normal capacities of the animals concerned (Pasik and Pasik 1982).
Some of the best evidence for sophisticated residual pattern vision after visual cortex ablation comes from work on tree shrews which can relearn some pattern discriminations after total removal of primary visual cortex (Kiltackey, Snyder, and Diamond 1971).
However, the patterns used in these studies were extremely coarse and could probably be discriminated by developing tricks like scanning the eyes across the stimuli and judging how much the brightness changes during the course of the scan.
It is significant that when the stimuli are modified to make that sort of strategy unworkable, for example by surrounding pairs of upright and inverted triangles with broad rings, tree shrews lacking visual cortex can no longer do the discriminations, whereas normal animals can.
I have described these studies in detail because they are considered by many to be the best evidence for sophisticated residual pattern vision after visual cortex ablation.
In nearly all of the other work residual pattern and colour vision was found to be extremely poor and only demonstrable after extensive training with special testing procedures.
When it comes to humans the issue is more clouded.
For one thing it is rare for a human to suffer total loss of the visual cortex without extensive damage to the rest of the brain that confounds interpretation of the results.
Most of our knowledge is based on studying regions of blindness in the visual field (scotomas) produced by partial damage to the visual cortex.
This means that attempts to relate studies on animals to those on humans are not really comparing like with like.
Furthermore, with subtotal damage you have no real way of knowing in advance exactly how large the scotoma should be.
A major problem is that the results are so inconsistent.
There is a smalt number of well reported cases of residual visual function within scotomas in people.
For example, a number of groups have described patients who can point to the location of briefly flashed stimuli, even though they deny seeing them.
Others have failed to replicate these findings (Barbur, Ruddock, and Waterfield 1980; Blythe, Bromley, Kennard, and Ruddock 1986; Campion, Latto, and Smith 1983; Weiskrantz 1980).
Even within the positive results the details are often inconsistent.
Most groups say that their patients are not conscious of the stimuli but some have reported that their patients are conscious of them (Barbur et al.1980; Zihl 1980; Zihl and VonCramon 1985).
The debate over what has become known as ‘blindsight’ has become very heated in recent years and it is difficult to see which side is correct.
Two points do seem in order.
The first  is that, if it exists, blindsight, like residual vision in other mammals, is again rudimentary.
The second is that it is unclear whether the findings on animals really predict what has been observed in humans.
One difference between the animal and human studies has already been mentioned, the fact that, in humans, residual vision is studied within scotomas.
Some work has been done on monkeys with restricted visual cortex ablations (Mohter and Wurtz 1977; Weiskrantz and Cowey 1970) and, in these, it has been shown that the ability to detect light flashes and make saccadic eye movements to fixate stimuli within the blind field is retained.
No-one has studied pattern discrimination or stimulus localization by reaching movements within the scotomas of monkeys.
A second difference is that animals are tested using instrumental learning procedures to shape their choice behaviour.
These have never been used in studies with humans.
Finally, the most convincing evidence for residual function in animals comes from studies using very coarse stimuli that are presented for a long time.
In contrast, residual vision in humans is usually tested with relatively fine stimuli presented for a hundred or so milliseconds.
What can we conclude from all this?
Clearly, information from the subcortical visual system can, under some circumstances, be used for more than the control of visual reflexes.
It can also influence learned and voluntary reactions to visual stimuli when the visual cortex is absent.
However, it is a big jump from saying what a system can do in the absence of the visual cortex to describing its functions in the normal, intact brain.
We need additional, converging, evidence to show that the subcortical visual system contributes to normal vision.
The most convincing evidence would be that capacities that are retained after ablation of the visual cortex are lost after damage to subcortical systems.
There is some evidence for this, but once again the situation is not clear-cut.
There is good evidence that visual reflexes retained after visual cortex ablation are lost after subcortical lesions.
Destruction of the pretectum abolishes the pupillary light reflex, leaving the pupil permanently dilated.
Ablation of the superior colliculus interferes with reflex orienting movements to stimuli in the peripheral visual field (Goldberg and Robinson 1978; Sprague, Berlucchi, and Rizzolatti 1973).
The problems arise with visual discrimination performance.
Deficits in visual function following subcortical visual system lesions have been reported many times but it is difficult to interpret them.
Part of the problem is that they are often on tasks that are also severely impaired after destruction of primary visual cortex.
Furthermore, the results are often inconsistent, especially when comparisons are made between different species.
For example, superior colliculus lesions are reported to affect many forms of visual discrimination performance in tree shrews (Casagrande and Diamond 1974).
In cats comparable lesions have some effect, but only when the animals are being trained on discriminations that are novel and with which they have had no preoperative experience, or if the lesions extend into the pretectum  (Sprague et al .
1973).
In rats and monkeys superior colliculus lesions have no detectable effect on most forms of visual discrimination performance (Goldberg and Robinson 1978; Sprague et al .
1973).
Brightness discrimination performance usually survives destruction of primary visual cortex (Bauer and Cooper 1964).
It is selectively impaired by lesions involving subcortical visual relays but detailed analysis of the deficit shows that it is probably due to a mild impairment in detecting contrast similar to that obtained with small cortical lesions (Legg 1988; Legg and Cowey 1977; Legg and Turkish 1983).
Furthermore, a similar effect can be produced by applying a substance to the eyes that artificially dilates the pupils (Legg 1988).
It is unlikely that these deficits are due to loss of a neural mechanism dedicated to brightness discrimination.
There is also evidence that different mechanisms are involved in residual brightness discrimination.
For example, lesions involving the pretectum can produce a deficit in normal brightness discrimination (Legg 1988) but other work suggests that it is the area rostral to the pretectum that is critical for residual brightness discrimination (Cooper, Battistella, and Rath 1981).
In all, the search for visual relays working in parallel with the primary cortical visual mechanisms has been disappointing, even though there is good evidence that these areas must be capable of operating in the absence of the visual cortex.
This suggests that the visual mechanisms involved in discrimination performance change after visual cortex ablation.
Table 4.1
Some of the main distinguishing characteristics of X- Y-, and W-type ganglion cells in the cat retina
The X-, Y-, W-cell debate
The idea of three separate channels running from the retina to the visual cortex via the dorsal lateral geniculate body is now well established.
The criteria for distinguishing between the three are laid out in Table 4.1.
Most of the criteria are electrophysiological but X-, Y-, and W-cells in the retina may also be distinguished on morphological grounds.
Most of the electrophysiological properties are only apparent when appropriate testing procedures are used.
Conventional procedures for studying receptive fields involve presenting small spots of light or dark, or light/dark edges and moving them around until you find the point in the visual field where they affect the response of the cell.
In more sophisticated tests two stimuli may be used, one probing the centre of the receptive field while the other explores the surrounding region to detect the presence of antagonistic effects from the surrounding region.
This allows you to say where the receptive field lies and a little bit about its spatial organization but, unless you are very lucky, it would not allow classification into X-, Y-, and W-categories with any confidence.
Most of the classificatory procedures involve studying response to periodic stimuli.
Visual stimuli can vary periodically in space, time, or both.
With periodic stimuli you can begin to ask questions about the optimal frequencies for generating responses and whether the response is linear.
Temporal periodicity is fairly self-evident It is the number of times part of the display cycles from light to dark to light again in each second.
Spatial frequency is less intuitively obvious but is related to the number of times the display cycles from light to dark to light again as you go a fixed distance in a single direction across the display.
For technical reasons spatial frequency is expressed in cycles per degree rather than cycles per centimetre.
For most experimental purposes spatial variations occur in only one dimension so that the stimuli appear as light and dark stripes.
Such patterns are known as gratings.
Although it is not strictly necessary, most work is done with stimuli that vary sinusoidally in space or time.
The luminance profile of a typical grating pattern is shown in Figure 4.3.
Linearity is a complex concept that relates to the degree to which the output of a system mirrors its input.
A system is linear if its output is an undistorted representation of its input.
This may be assessed by moving a grating over the receptive field of the cell and measuring the output of the cell as a function of the position of the grating relative to the centre of the receptive field.
A linear cell will give a response profile like that of the middle curve in Figure 4.3.
A non-linear cell will give a response like that of the upper curve in Figure 4.3.
Note that cells with linear response properties usually have a ‘null position’.
That is to say, there is a way of positioning the grating relative to the receptive field centre that results in no response.
This means that absence of output from linear cells is ambiguous.
It could mean that no stimulus is present or it could mean that it is in the wrong position.
On the other hand, the presence of a response is highly informative since it not only indicates the presence of a stimulus but also gives  some information about the location of the stimulus relative to the centre of the receptive field.
To complicate matters some cells behave linearly under some conditions and non-linearly under others.
Figure 4.3
Idealized representation of the responses of X- and Y-cells to periodic stimuli
‘A’ indicates the experimental condition, with a grating (cross-hatched) placed across the receptive field (concentric circles).
Usually, the grating is moved slowly across the field.
‘B’ indicates variations in the input and output over time.
The lower curve shows the light intensity at the centre of the receptive field as a sinusoidal grating drifts across it.
The middle curve shows the corresponding output of an idealized X-cell.
Note the modulation of activity at the same frequency as the input intensity and the fact that the mean activity of the cell does not change.
The upper curve shows the corresponding output of a Y-cell.
Note the modulation at twice the input frequency and the elevation of mean activity level.
We can now begin to make some sense of the differences between the three classes of ganglion cells.
The first point to emerge is that W-cells are not a homogeneous class.
Some are linear, others non-linear.
Some have clear centre-surround organization, the others do not.
The only things that unite this class are the slowly conducting axons, the large receptive field sizes, and their poor sensitivity and resolution.
X-cells are fairly straightforward.
Conduction velocity is average, spatial summation within the receptive field is linear (they have a null position usually), the receptive field has a centre-surround organization, and receptive fields are small.
These cells respond quite vigorously when medium and high spatial frequency stimuli are used, but are less responsive when low spatial frequency stimuli are employed.
Good spatial resolution means that they tend to respond to high spatial frequencies.
Their temporal resolution, which is fair, refers to the highest temporal frequency with which the cell can follow reversals of stimulus intensity within the receptive  field.
Y-cells are possibly the most complicated of the lot.
Their axons conduct very rapidly.
However, as Lennie (1980) points out, that does not mean that these cells respond more rapidly to visual stimuli than the other classes, since the latency of response to visual input is determined largely by events in the retina At low spatial frequencies they are linear but as the spatial frequency is increased non-linear properties emerge.
This is believed to be due to the complex organization of their centre-surround mechanisms.
Receptive fields are of medium size, i.e. larger than those of X-cells, smaller than those of W-cells.
They respond well to low spatial frequencies.
They will respond to high spatial frequencies but, as mentioned above, the response is non-linear.
Their temporal resolution is better than that of X-cells.
Under the conditions that usually obtain in receptive field plotting studies one further distinction has been reported.
X-cells respond to standing contrast with a sustained response that persists as long as the stimulus is present, while Y-cells give a transient response that fades during the course of the stimulus presentation.
This is not a good criterion, however, for three reasons.
The first is that it says nothing about W-cells.
The second, and more important, is that it depends on the test conditions.
It is possible to make X-cells behave transiently and Y-cells to behave in a sustained manner if the stimulus intensity is altered.
It is therefore unlikely that, under normal conditions, X- or Y-cells behave in a reliably sustained or transient manner.
The third is that, in the real world, the eyes are moving all the time so that even cells that would behave transiently in electrophysiological studies in which the eyes are routinely immobilized, would generate a continuous output as the stimulus sweeps backwards and forwards across the receptive field.
The distinctions between the three cell types are quite subtle and may only be obvious after quite extensive testing.
What then is their functional significance?
The first point to note is that no-one has a convincing explanation for the existence of W-cells.
Most of the debate centres on the X-/Y-cell distinction.
Given that the number of W-cells in the retina probably exceeds the number of Y-cells, this is unsatisfactory.
For many years the favoured explanation for the X-/Y-cell distinction was that Y-cells are involved in the analysis of stimulus change, especially movement, while X-cells are involved in spatial vision and the localization of features (Lennie 1980).
This argument is based on the fact that X-cells have better spatial resolution than Y-cells while the latter have better temporal resolution, plus the fact that Y-cells tend to respond only transiently to sustained contrast.
In fact, this hypothesis makes much of what are fairly small differences in the functions of these cell types and, more importantly, ignores the fact that under most normal conditions visual perception operates well within the limits of resolution of the visual system.
We can still identify objects when they are blurred, which removes the higher spatial frequencies, and most of the time moving objects are moving neither very fast nor very slowly.
A hypothesis that stresses how these cell types work at their limits is unsatisfactory (Sherman 1985).
Lennie (1980) has come up with a more challenging suggestion.
He argues that Y-cells are so rare in the retina they cannot possibly contribute to visual perception because their resolution, by themselves, is inadequate to convey any useful information.
Instead, he argues, they provide a tonic input to the cortical units that are processing input relayed by X-cells, modulating the activity of these units to improve the efficiency of visual processing.
Sherman (1985) objects to the idea that Y-cells are irrelevant to spatial vision on the grounds that spatial vision persists after removal of the X-cell system.
The evidence for this comes from work on cats, a species in which the X-cells terminate almost entirely in area 17 of the cortex while the Y-cells project additionally to adjacent areas of the visual cortex.
In cats, ablation of area 17 alone does not abolish form vision but additional removal of adjacent parts of the visual cortex does.
On the basis of these studies Sherman argues that it is the Y-cell system that is responsible for form vision while the X-cell system is responsible for visual functions that require very high acuity, like hyperacuity tasks and stereoscopic fusion.
While the evidence he cites shows that Y-cells may be capable of sustaining some form of pattern vision it does not show that they are responsible for it in the normal brain.
For a start, the visual tests used in these experiments are simple in the extreme and place few demands on the discriminative capacity of the system.
Second, it has yet to be shown that the reverse experiment, removing the immediately adjacent areas of the cortex while leaving area 17 intact, has the devastating effects on visual perception that would be expected.
A simpler interpretation is that the experimenters have rediscovered what Lashley (1950) showed many years ago, that partial removal of the cortical area to which the dorsal lateral geniculate body projects, has remarkably little effect on simple form discrimination tasks and that it is only when the entire cortical projection zone is removed that severe deficits, detectable in the simple behavioural paradigms we use, emerge.
Undoubtedly the existence of three parallel visual channels running from the retina to the visual cortex is one of the most important findings in visual physiology in the past two decades.
Any workable theory of visual perception must incorporate this discovery.
At present, however, we lack an entirely satisfactory theory to explain the existence of the three mechanisms.
Lennie's (1980) proposal makes the most sense of the relative number of the cell types in the retina but is difficult to reconcile with the lesion data.
Since these behavioural data are so crucial to interpreting the physiological findings they will repay careful scrutiny.
Multiple visual areas
One of the major discoveries of the last two decades has been the mosaic of visual field representations that bounds the primary visual cortex in all of the mammals that have been studied so far.
When these representations are studied in detail it turns out that they differ from each other in a number of important  ways (Maunsell and Newsome 1987).
The amount of cortex given over to the central as opposed to the peripheral visual field can differ, as can the size of receptive fields of cells within the area.
Receptive field properties also differ from area to area.
For example, V4 in rhesus monkeys contains a very high density of colour coding cells that are unresponsive to white stimuli.
Such cells are rare in the other visual areas.
Area MT is rich in movement-sensitive cells and V2 contains cells tuned to ‘retinal disparity’, the cue that is the basis of depth perception.
One of the puzzling features of these areas is that the receptive field properties of their cells appear to repeat properties already present in primary visual cortex.
For example, Vl has colour-coding cells that do not respond to white light.
It also has movement-sensitive cells and cells tuned to particular retinal disparities.
Why are these properties repeated in secondary visual cortex?
To answer this question we need to know what is and is not being done in the primary visual cortex and here ow understanding has been greatly advanced by the computational approach to vision.
One of the paradoxical achievements of the computational approach is that it makes even very elementary processes seem complex while, at the same time, actually simplifying problems that were once thought complex.
A good example of a simple problem made complex is movement discrimination.
To understand the problem we need to look at a moving object from the point of view of a single cell in the visual cortex.
What does that cell see?
It sees a tiny patch of the visual field measuring a few degrees of visual angle across.
The object that it is looking at will be many times larger than a single receptive field.
Under these conditions the ‘aperture effect’ operates, which is that no matter what the true direction of movement of the object through the visual field what you see through the aperture is movement at right angles to the orientation of the  edge that is passing across the aperture.
The problems that this will create can be seen from Figure 4.4, which shows a diamond moving across two receptive fields, one located above the other.
As the leading edge of the diamond crosses the upper cell, that cell will ‘see’ movement up and to the right.
The lower cell will see movement down and to the right.
When the trailing edge crosses the reverse will happen.
The upper cell will see movement down and to the right and the lower cell will see movement up and to the right.
Both cells will see movement to the right but the direction seen will vary by 90 degrees.
The brain's problem is to convert this ambiguous information about the movement of edges through the visual field into an unambiguous representation of the movement of an object, in this case a diamond moving from left to right.
Figure 4.4
The aperture effect
A diamond moving from left to right, but viewed through one of the two small apertures, will appear to move either to the lower right or the upper left.
Movshon and his colleagues (in Maunsell and Newsome 1987) have shown that this translation from the movement of edges to the movement of objects probably takes place in area MT.
The experimental demonstration is quite ingenious.
If you present someone with a set of stripes on a TV screen and make them move at right angles to their long axis, that is the direction in which the person will see them move.
If you add a second set of stripes, at right angles to the first but also moving at right angles to their axis, the person will not see two sets of stripes but a set of chevrons moving at 45 degrees to the true directions of movement.
This is because the aperture problem has been overcome at some level in their visual system.
Movshon took stimuli like these to test the responses of movement-sensitive cells in Vl and MT of rhesus monkeys.
Initially he tested preferred direction of movement for each cell using a simple stimulus and then tested again using the double grating patterns.
Cells in both Vl and MT responded to the double grating stimuli but with one important difference.
In Vl the cells would only respond to the double grating if one of the two moved in the preferred direction of the cell as tested with simple stimuli.
They did not respond if the perceived direction of movement of the chevron was in the preferred direction.
In contrast, cells in MT only responded when the perceived direction of movement of the chevron matched the preferred direction assessed with simple stimuli.
The cells did not respond if one of the gratings moved in the preferred direction.
In a more complex version of the same test the stimuli were three gratings moving in different directions.
Under these conditions the human observer sees a chevron, made up of two of the gratings, moving against a background made up of the third.
Which two gratings are combined into the chevron depends on a number of factors.
What is important is that cells in area MT of monkey always responded according to the illusory direction of movement of the chevron perceived by a human observer rather than to the physical direction of movement of the component gratings.
Cells in Vl, in contrast, always responded according to the direction of movement of the component gratings.
These findings explain why movement is represented twice in the visual cortex.
In the first representation it is movement of features across the retina that is represented.
In the second it is the movement of objects.
How the computation of object motion from image motion is carried out is currently the subject of intense research (Hildreth and Koch 1987).
It is well established that objects are perceived to have the same colour despite quite extensive variations in the colour of the light with which they are illuminated and hence the wavelength of the light they reflect back to the retina.
Colour constancy, as this is known, proved a problem in visual perception for many years.
It was usually solved by arguing that our visual systems use our knowledge of the identity of objects we are looking at to dictate the colours we attribute to them, independently of the wavelength information available at the retina.
This theory makes the unlikely prediction that you will not experience colour constancy with objects you cannot identify.
Nevertheless, we do not have difficulties judging the colours of unfamiliar objects.
Pieces of abstract art do not suddenly change colour because we have moved them from artificial light to daylight.
The theory also has difficulty in coping with colour perception when a particular object may come in a number of colours.
For example, apples can be red, green, yellow, or brown, depending on their type.
Even oranges can be green!
It makes much more sense to assume that our colour experience is linked to the properties of the retinal image rather than to our knowledge of objects.
How is this to be achieved?
Land has come up with a credible answer in his Retinex theory of colour perception (Land 1974).
His starting point is the fact that we have three classes of receptors in our eyes, each tuned to respond to a different wavelength of light, one to ‘red’, one to ‘green’, and one to ‘blue’.
This has, of course, been recognized for over a century.
Where Land's theory differs from previous models of colour vision is that, instead of relating colour directly to wavelength, he argues that colour experience is related to the relative outputs of these three classes of receptor in different parts of the visual field.
If the wavelength composition of the light falling on a scene changes then the wavelengths reflected by, say, a red area will change accordingly but so will the wavelengths reflected by an adjacent green area.
As a consequence, although the amount of, say, blue light reflected by the red area may have gone up, so will the amount of blue light reflected by the green area As a consequence, the ratio of blue light in the two areas will remain the same, as will the ratio of green light in the two areas and the ratio of red light.
If the system is set up to compare the amounts of each wavelength coming from different parts of the scene the problem of colour constancy is solved immediately.
Recent work by Zeki (1983a, b) shows that Land's theory is of more than theoretical interest because cells in area V4 have properties compatible with it whereas cells in Vl do not.
There are many wavelength selective cells in area Vl.
These cells are tuned to a particular wavelength and will only respond when that wavelength alone is present.
If their receptive fields are illuminated with spectrally pure light, say red, these cells respond vigorously, even though the stimulus may not look particularly red to a human observer.
If the stimulus conditions start with a spectrally pure red light shining on to a red area the cell will fire even though the area looks like a washed-out red to a human observer.
If the stimulation conditions are altered so that a red coloured area is presented, illuminated by red, green, and blue light, that area will look strongly red to the observer but the cell won't fire!
In contrast, cells in V4 respond relatively sluggishly when spectrally pure colours are used to illuminate scenes, especially when the scene looks washed-out to a human observer.
These cells respond most vigorously when a combination of wavelengths is used for the illumination, and the area of the stimulus is seen as strongly coloured by human observers.
These properties of V4 cells only appear, however, as long as a large part of the scene is illuminated and the scene contains more than one coloured area, which Zeki achieves using a multicoloured stimulus array he calls a Mondrian.
If the area visible to the cell is reduced to its classically defined receptive field the cell is less likely to respond to coloured objects.
Interestingly, the loss of responsiveness in the cells is correlated with a change in appearance to human observers looking at the stimuli through a similarly sized aperture.
The work of Land and Zeki is important for two reasons.
The first is that Land's theory of colour perception makes sense of there being two cortical stages in colour vision.
The second is that it is only when people think about what the visual system does in the real world that they begin to study it appropriately.
Zeki's results would never have been obtained if he had continued to study V4 cells using spectrally pure light sources that are the tradition in visual system physiology, rather than coloured objects which are the normal source of our colour perception.
Theoretical analysis of the function of the visual system helped the experimental analysis.
This work has, in turn, had an impact on behavioural research.
Two recent reports have described colour specific impairments following prestriate damage in monkeys (Wild, Butler, Carden, and Kulikowski 1985) and humans (Heywood and Cowey 1985).
Zeki's data show that colour is represented in V4 while only wavelength information is prominent in Vl but, while these data go some way towards answering some important questions about colour vision, they don't solve all the problems.
The main outstanding issue is that we don't know how colour-coding receptive fields are constructed out of wavelength-coding receptive fields at other stages in the system.
Are the colour-coding receptive fields created by the internal circuitry of V4 or is the relevant processing carried out in Vl?
Analysis of this problem is greatly facilitated by the fact that we can identify anatomically the subregions of the visual cortex most likely to be involved in colour vision.
V4 receives input from V2 which in turn receives input from Vl.
It is now known that information flow from Vl to V4 is segregated in an anatomically discrete channel so that V4 receives input from identifiable subfields of V2 which receive their input from similarly identifiable zones in V1.
What identifies these  zones is that they contain a high concentration of the enzyme, cytochrome oxidase (CO).
Brain tissue can be stained so that CO is visible, areas with a high CO concentration having a dense brown colour.
In Vl, CO-rich regions appear as regularly spaced blobs in the upper layers of the cerebral cortex while, in V2, they appear as stripes.
The neural pathways involved in converting wavelength to colour information are restricted to these regions so, in order to understand colour coding, we need to restrict our attention to them (Maunsell and Newsome 1987).
The nub of the issue is that colour analysis of the sort proposed in the Retinex theory requires analysis of the wavelength output of different parts of the visual field.
Put into operational terms this means that, at some point, there are likely to be cells that respond in one way to one wavelength at one point in the receptive field and differently to the same wavelength in a different part of the field.
For example, we would expect to find cells that give an excitatory response to long wavelength light shone on one part of the retina and an inhibitory response to long wavelength light shone on to an adjacent part.
Such cells have never been described in Vl (Livingstone and Hubel 1984).
However, cells with a more complex form of spatially inhibitory organization have been identified.
These are the double opponent cells of Vl (Livingstone and Hubel 1984; Maunsell and Newsome 1987).
These cells give a large excitatory response to the onset of a small spot of one wavelength shone on to their receptive field centres and a much smaller response to a much larger spot.
This indicates the presence of an inhibitory surround.
When a complementary wavelength is used the response to the small spot only occurs at the offset of the stimulus.
Again, there is no response to a large spot.
These cells will convey information about wavelength boundaries that could be used to compute true colour.
The colour-coding cells in V4 probably depend on input from these cells.
However, as Livingstone and Hubel (1984) point out, they cannot be computing colour according to the algorithm derived by Land.
The significant point to emerge from Zeki's work is that a perceptual phenomenon once believed to be the result of high level cognitive processing now turns out to have a single cell correlate at an early stage in the visual pathways.
Other constancies may turn out to have similarly straightforward explanations.
Indeed, perceptual constancies are only a puzzle if it is assumed that the visual system acts like a camera.
Once it is recognized that the job of the visual system is to recode the retinal image in terms of a representation of the outside world, and that much of this encoding can be done by comparing the properties of the retinal image in different parts of the visual field using fairly straightforward rules, constancies present no problem.
However, it is important for us to remember that we still don't really understand how this recoding is done by the brain.
Visual association cortex and the recognition of objects
We have seen that the visual areas of the prestriate cortex recode image properties into properties of the object that gave rise to the image.
At no point in this system is the object itself represented.
According to traditional views this takes place in the next der of visual areas.
Immediately we have a problem: what form would the  representation of an object take?
The simplest, in principle, would be that individual cells in these higher order areas would respond whenever a particular object was presented.
They would do so because they receive convergent input from cells at the lower levels of the system that represent the defining properties of that object.
The model is referred to as the ‘grandmother cell’ theory because it predicts that we should have populations of cells in our brains that fire only when we see our grandmothers.
Barlow (1985) has argued very strongly for just such a model.
The model is simple to test.
All we need to do is to push microelectrodes into the regions to which the prestriate cortex projects and look for the cells that only respond to particular objects.
There are two main areas that receive input from the prestriate cortex.
One is the inferior temporal cortex and the other the inferior parietal lobe.
In general the results of single cell recording studies on these areas have been disappointing.
What is usually found is cells with massive receptive fields with very non-specific receptive field properties.
Virtually anything waved around in the receptive fields of these cells will evoke a response under the right conditions.
In other words, these areas are visual but the degree of stimulus specificity demanded by the theory is generally absent.
There are some notable exceptions that have attracted attention.
Gross and his colleagues have reported that cells in the inferior temporal gyrus sometimes had very specific stimulus requirements before they would respond (Gross 1973).
For example, one cell would only respond when a hand was moved in front of the animal's eyes.
However, these cells were embedded in a much larger population of cells that were non-specific in their requirements.
More impressive are the recent reports from Perrett and Rolls (Perrett, Rolls, and Caan 1982; Perrett, Smith, Potter, Mistlin, Head, Milner, and Jeeves 1985) that cells in the superior temporal sulcus, which forms the upper margin of the inferior temporal lobe, are often only responsive to faces or parts of faces.
Even here there is debate about whether the activity of any single cell could be used to identify a particular face.
Usually the cells fire strongly to one face and less strongly to others.
However, in nearly all instances there is some response to any face.
Furthermore, the response is dependent on the orientation of the face to the animal.
A cell that will respond to somebody in full face will fail to respond to the same person in profile.
It is fair to say that cells in this area are encoding the properties of faces but they cannot be encoding their identities.
In some ways Perrett's findings can be seen as an extension of the results on prestriate cortex in that higher order properties of objects are being encoded, using information from representations of lower order properties.
Nevertheless, the identity of objects is still missing.
This raises the question of whether it is necessary to represent objects at the single cell level.
One good reason for not doing so is that ‘grandmother cell’ models have real difficulty in explaining how our visual systems cope with novel input.
Clearly we would have to have a separate system for representing the presence of objects and their features that was not dependent on knowing their identity.
This duplication would be wasteful.
An alternative to the grandmother cell theory is network theory.
Network theory has already been mentioned once in this book when we discussed methods.
Network theory holds that information can be represented in a distributed form, in terms of the pattern of activity in a large number of elements.
Associations can be formed by allowing two networks to intersect and interact so that patterns of activity in one network can evoke predictable patterns in the other.
Network theory is proving very powerful in modelling a range of cognitive processes.
It is too early to say whether this is the way the brain works but, as Rolls (1987) points out, it would make a lot of sense of a lot of uninterpretable data if it was; it also makes sense on logical grounds, since it gets round the problems associated with grandmother cells, like the problem of perceiving novel objects and the complexities of arranging the massive degree of convergence of input required for grandmother cells to work.
Of course, the question of how the identity of objects is coded is separate from the issue of whether object recognition is localized to these high levels of the visual system at all .
The possibility that this sort of information is represented across an array of cells, rather than at the level of the single unit does, however, mean that single cell recording alone will not give us a useful answer.
What about lesion evidence?
Since 1954 it has been known that focal lesions of the inferotemporal cortex produce a severe and abiding impairment in visual discrimination performance in monkeys (see Dean 1982).
Indeed, this was shown before it was demonstrated that the inferotemporal cortex was anatomically part of the visual system.
Initially it was thought that there was an impairment in memory for visual stimuli but work carried out in the 1970s largely dispelled this notion.
Instead, the factor that most influences the outcome of inferotemporal cortex lesions is not memory load but whether or not the animals are required to categorize the stimuli before making their choices (Dean 1982).
Given that object recognition is a categorical process, in that one doesn't recognize each individual chair one sees but identifies it as belonging to the same category as other chairs one has seen, this suggests that the inferotemporal cortex has a major role to play in object recognition.
According to Cowey (1982) and Humphreys and Riddoch (1987) humans with damage to the temporal visual cortex, or the prestriate areas that feed into it, also suffer major impairments in object recognition, known as ‘agnosias’.
In this instance there has been a remarkable convergence of evidence, from a variety of sources, on  the conclusion that temporal visual areas are involved in object recognition.
As Rolls (1987) implies, the real question now concerns the identity of the coding processes going on there.
What of the parietal lobe areas identified by Luria (1973) as being so important for object recognition?
Considerable progress has been made in analysing the functions of the visual areas in the parietal lobe, but little of the evidence favours a role in object recognition.
Ungerleider and Mishkin (1982) argue, from behavioural evidence, that the parietal areas are involved in spatial vision, an argument that is supported by clinical evidence in humans.
After parietal lobe lesions monkeys have difficulty with a landmark test in which they have to remember the location of a food reward relative to prominent stimulus on the test board and humans have difficulty recognizing objects when they are presented in unusual orientations (Warrington 1982).
Other sensory systems
We do the same types of things with our other sensory systems as we do with vision.
For example, we identify the sources of stimuli on the basis of patterns of stimulation on the receptors.
We might, therefore, expect them to work in the same sort of way as the visual system.
Anatomically, the sensory systems with large cortical representations, the auditory and somatosensory systems, are very similar to the visual system.
There are relatively direct, ‘lemniscal’ pathways from the receptors to the cortex and other, less direct, routes that involve largely subcortical relays.
At the cortical level there are multiple representations of the receptor system.
For example, even the hedgehog has two somatosensory representations in its cortex, while monkeys have up to seven (Merzenich and Kaas 1980).
Nevertheless, we shouldn't attach too much significance to the similarities because we either don't have the relevant evidence or the details are different.
Clinical and some behavioural evidence from animals (Kolb and Whishaw 1985; Whitfield 1979) suggests that there must be auditory and somatosensory areas akin to the temporal lobe visual areas of the primate.
On a priori grounds we would expect humans to have an auditory area corresponding to temporal visual cortex since speech requires the categorical perception of sounds.
However, such areas have not been well characterized in physiological and anatomical studies.
Most studies on these systems have concentrated on the topographically organized representations that probably correspond to striate and prestriate cortex in monkeys.
Turning to differences in detail, in the primate visual system all visual submodalities are represented in the primary visual cortex.
It is the second tier of visual cortex that specializes.
In contrast, the data we have on the somatosensory cortex indicate that all of the areas specialize in some way and that there is no generalist area.
It may turn out that the somatosensory and visual systems are not organized in exactly the same ways.
This wouldn't be surprising in some ways, since it is not clear that the visual system is organized in the same way in all species.
For example, in primates the projection from LGd is almost exclusively to area V1 while in cats it projects to peristriate cortex as well .
In rodents it is difficult to identify an area that corresponds to temporal visual cortex, there is debate about the existence of a parietal visual area, and the prestriate cortex has yet to be shown to be subdivided on the basis of submodality as in primates.
If the same system can vary so much between species there is no good reason why different systems should be the same within a species.
As things stand, the visual system is proving a good model to guide investigation of other sensory systems but we need to be cautious about interpreting data and to avoid forcing these other systems into a straitjacket defined by the visual system.
When it comes to other sensory systems, much of the problem is that we haven't the sort of clear ideas about how they might work that we have about the visual system.
In recent years there have been some significant advances in studying the auditory system, especially in the analysis of computational maps that recode intensity and phase differences in the two ears into the representation of auditory space (Knudsen, DuLac, and Esterly 1987), but this work has focused on subcortical mechanisms.
We know that the auditory cortex is involved in more complex processes like the categorical perception of sound, but at this point our lack of good theoretical models becomes apparent.
A similar state of affairs affects our understanding of the somatosensory system.
At this stage what would prove helpful is computational models of hearing and touch.
Overview
Although we cannot yet say conclusively how objects are recognized visually we are beginning to converge on an answer.
The insights gained from computational models of visual perceptions are facilitating the interpretation of findings of physiological studies of the visual cortex and a picture is emerging of a modular system in which different attributes of the image are processed in different cortical areas before the information is passed on to higher order systems.
Our understanding of hearing and touch tends to lag behind our understanding of vision.
In these systems we have good descriptions of their anatomy and of the physiological properties of the major types of nerve cells but, at the cortical level, we don't have good explanations of what the systems do or the way they do it, except in the broadest terms.
The main outstanding issue in perception, including visual perception, concerns the processes that combine information about the attributes of a particular scene into a representation of the object that gave rise to the image, sound or pattern of touch.
In the case of vision we can pinpoint the brain regions likely to be involved, but it remains to be seen how they actually work.
At present the most promising approaches are coming from network models, but they are in their very early stages and the day of a convincing object recognition network is still some way off.