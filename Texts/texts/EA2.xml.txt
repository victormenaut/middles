

Editorials
Cardiotocography during labour
An unsatisfactory technique but nothing better yet
Maternal blood flow to the placenta may be severely diminished by uterine contractions, so labour has the potential sometimes to damage the fetus.
In the last century some asphyxiated babies were noted to have abnormally fast — or slow — heart rates, and auscultation evolved as a component of intrapartum care.
The human ear is insensitive to subtle changes in rate, so electronic methods of recording were developed.
These generate paper traces that show additional features not obvious on auscultation, including the degree of variation of the heart rate and the shape of accelerations and decelerations of the heart rate.
But more information is not necessarily more instructive, or we should judge textbooks by their size.
The effectiveness of medical interventions is best evaluated by a systematic analysis of randomised controlled trials.
The overviews of intrapartum monitoring in the Oxford database of perinatal trials (now metamorphosed into the Cochrane Centre pregnancy and childbirth module) have been discussed widely.
Briefly, they showed that the use of continuous electronic fetal heart rate monitoring caused higher rates of medical intervention including caesarean section during the first stage of labour and instrumental vaginal delivery during the second.
When compared with intermittent auscultation alone continuous electronic fetal heart rate monitoring doubled rates of caesarean section for ‘fetal distress’ even when it was complemented by selective fetal blood sampling and pH estimation; the rate was quadrupled when monitoring was used alone.
Further evidence of excessive intervention after cardiotocography came from a recent trial in Plymouth of analysis during labour of the waveform of the fetal electrocardiogram.
In this 1200 women were randomly assigned to either cardiotocography alone or cardiotocography plus waveform analysis; the operative delivery rate for ‘fetal distress’ was double in the group assigned to cardiotocography alone.
Increased rates of operative delivery are not necessarily bad if genuinely compromised babies are rescued from death and damage.
Various outcomes have been examined in randomised controlled trials of intrapartum fetal monitoring; the only complication to show a lower (and largely consistent) decrease in the monitored groups was neonatal convulsion, and this protective effect seemed to be confined to prolonged, induced, or augmented labours.
Neonatal convulsion is an uncommon problem but one that is potentially serious.
Many parents would presumably place high priority on avoiding this risk because of its implications, even though the Dublin trial (which by its size and quality dominates overviews of trials of intrapartum monitoring) showed the subsequent prevalence of cerebral palsy among survivors to be no lower in the monitored group.
Continuous cardiotocography is more restricting than intermittent auscultation, although the method of fetal monitoring is less important to women than the support they receive from staff or companions.
The cardiotocography machine must certainly not be used as a poor substitute for midwifery care, nor should a shortage of midwives be used as an argument for a policy of continuous cardiotocography.
A compromise in current favour is the admission cardiotocography test followed by intermittent auscultation and cardiotocography.
This approach has appeal, but its effectiveness has not yet been rigorously assessed.
With these disadvantages established for intrapartum cardiotocography (not to mention local trauma and infection from scalp electrodes), should the technique be abandoned altogether?
The answer is complicated by medicolegal implications.
Cardiotocography records are carefully scrutinised and sometimes pivotal in expensive legal actions.
The trace provides a permanent record, which may be used, for example, to show that the poor condition of a baby at birth was neither predictable nor preventable.
Arguably, however, the midwife's record of a normal fetal heart rate should be just as acceptable as evidence.
Where, then, do we stand?
The available evidence does not support routine continuous fetal heart rate monitoring during all labours.
In a normal labour intermittent auscultation with a Pinard stethoscope could not be regarded as an inadequate or negligent form of assessment.
Some obstetricians argue that the choice of method should be left to the woman.
If she opts for cardiotocography it must be done properly and not haphazardly.
Any unusual features should be noted and the response to them should be made by an appropriately qualified and experienced person.
All members of the obstetric and midwifery staff should speak the same language and use the same method of classification — something that can be achieved only by regular meetings on cardiotocography.
The classification of decelerations as early, late, and variable, as used in the Dublin trial, seems best; the terms ‘type I’ and ‘II dips’are archaic and should have been abandoned long ago.
Staff should be made aware that some of the most sinister fetal heart rate decelerations (late) are often shallow and difficult to detect; some of the most obvious with greatest amplitude (variable) may be of little or no pathological importance.
In practice, the adherence to a clearly defined policy for fetal heart rate monitoring may be more important than the actual method itself.
If only because cardiotocography has been shown to reduce the rate of neonatal convulsions after complicated labours its use should not be abandoned.
Maternity units should have clear, explicit guidelines on the grounds for recommending cardiotocography so that the clinical and legal responsibility does not lie with a junior obstetrician or midwife.
Suitable grounds might include prolonged, augmented, or induced labours; multiple pregnancies; thick meconium staining of amniotic fluid; and a growth retarded or preterm fetus.
Audible heart rate decelerations also indicate a need for electronic monitoring.
Improvements in the results of monitoring will come not only by better identification and interpretation of abnormalities of the fetal heart rate but also through better responses by attendants to unusual patterns.
Simple measures (stopping the oxytocin infusion, altering the mother's posture, restoring blood pressure to normal) often suffice; techniques such as amnioinfusion deserve further investigation as alternatives to operative delivery.
Recording the fetal heart rate has become a fixed feature of intrapartum care; we need to reconsider and remember that it is an extremely limited method for assessing the health of the fetus.
We do not expect doctors to run an intensive care unit simply by measuring the pulse rates of their patients.
The status of intrapartum fetal heart rate recording (whether assessed by ear or by machine) is similar to that of the neonatal test frequently used to assess its effectiveness — the much maligned but universally used Apgar score.
Both are clearly unsatisfactory, but a practical and popular alternative has not yet emerged for either.
The search for better techniques of fetal assessment during labour continues.
For ‘high risk’ fetuses, waveform analysis of the electrocardiogram shows promise; oximetry still has some way to go.
Mothering skills of women with mental illness
Not enough known about the postpartum period
The traditional role of mother and baby units in psychiatry has been to treat acute postpartum mental illness without separating the mother from the infant.
Increasingly, however, such units are being asked to assess the quality of mothering by women with chronic disorders, particularly schizophrenia but also poor control of impulses.
Many such referrals are initiated by social services and the legal departments of social services, possibly as a result of growing pressure on these agencies to protect children in accordance with their statutory responsibility and powers under the Children Act.
As a result, psychiatrists take on the crucial rule of assessor and expert witness in child care cases in which the mother has a mental or behavioural disorder.
How should specialist services respond to this challenge?
What is known about the capacity of mentally ill mothers to care for their babies?
Maternal mental illness may adversely affect a child both directly — causing neglect, physical harm, and psychological upset — and indirectly — through associated features of the illness such as marital disharmony and repeated admissions to hospital.
In a series of women who had killed their children 84% had mental and behavioural disorders, but diagnoses varied, with 16% being psychotic and personality disorder being the commonest diagnosis.
Acute schizophrenia can severely impair mothering skills when maternal distress leads to distraction and neglect.
Close supervision of patients is needed when they have delusions or hallucinations concerning the baby.
Impairment of warmth and rigid or anxious mothering styles also occur during the acute illness, and improvement may be delayed for up to one year, depending on the premorbid social adjustment.
In women with chronic schizophrenia blunted or incongruous emotions, poor motivation, disturbed behaviour, and lack of responsiveness to the child's cues may all affect mother-infant interaction.
Maternal responsiveness is thought to be important in cognitive development and for secure mother-infant attachment: thus infants of mothers with chronic schizophrenia are at risk of disrupted bonding and its sequelae of impaired social behaviour and problem solving.
Personality disorder is the parental diagnosis most associated with emotional and behavioural disorders in children, particularly when they have been exposed to hostile behaviour.
In addition, lack of warmth and chaotic behaviour may result in child neglect, and impulsiveness and reduced tolerance to stress can be expected to increase the risk of physical abuse.
Abuse and neglect are also commonly seen in children of mothers with learning disabilities and are particularly common if the fathers also suffer from learning disabilities or if the children themselves are of normal IQ.
In severe depression apathy can lead to neglect, irritability to physical harm, and depressive delusions to infanticide.
Cognitive impairment in children may follow milder postpartum depression and may be detectable four years after the resolution of maternal symptoms.
Behavioural problems at 3–4 years of age have also been associated with maternal depression, both concurrent and postnatal.
In the longer term the offspring of depressed mothers are more likely to suffer from childhood depression.
Despite these findings key questions remain concerning the prediction of successful mothering after postpartum psychiatric disorder, and there is little evidence on how maternal  competence can best be assessed.
One descriptive account of assessment recommends full psychiatric evaluation, comprehensive observations of the mothers with their infants, and interviews with relatives to estimate future support.
Although this commonsense approach is probably used in most mother and baby units, including those that now admit mothers with chronic illness for preliminary assessment and subsequent training in mothering skills, the practice is open to criticism.
To be clinically useful, observations must be simple enough to be made by multiple carers on a psychiatric ward; excessive simplicity, however, may undermine their value.
Also in doubt is their predictive validity, as no study has determined mother-infant interaction after discharge from a mother and baby unit or identified which observations during the admission are most crucial to prognosis.
A potential problem now exists in deciding who should pay for such detailed assessments: mother and baby units must attract referrals from outside their district to remain financially viable, and their high ratio of staff to patients is likely to prove expensive.
District health authorities will in future be asked to purchase this form of care, and social services will be required to pay directly for the advice they need.
If they do not the risks are considerable as decisions may be made without the benefit of comprehensive observations and specialist experience.
Some children may be removed prematurely from their families while others may be entrusted to incapable mothers whose difficulties remain undetected and untreated.
Newborn screening for Duchenne muscular dystrophy
Acceptable to parents
Bradley and colleagues describe a programme for screening newborn infants for Duchenne muscular dystrophy on p 000.
Symptoms of this severe X linked inherited disorder usually occur before the age of 6, and the patient is bedridden by 12 and often dead by 20.
Newborn screening for genetic and other disorders is usually confined to diseases for which there is treatment — either a change of diet (as in phenylketonuria and galactosaemia) or replacement therapy (as in hypothyroidism)— or in which there is a chance of affecting morbidity or mortality early in life, as occurs in the haemoglobinopathies (particularly sickle cell disease).
Screening for disorders in which the clinical course cannot be altered is generally avoided: a programme of newborn screening for disorders like Duchenne muscular dystrophy could therefore create an ethical nightmare.
Bradley and colleagues are well aware of these problems as they seek to justify newborn screening for this untreatable genetic disorder.
The authors argue that newborn screening for Duchenne muscular dystrophy decreases emotional distress because it avoids delays in diagnosing the disease, and without newborn screening two or more boys could be born with Duchenne muscular dystrophy before parents realised that they were at risk of having an affected child.
Accordingly, if an infant with Duchenne muscular dystrophy is identified early the family has the option of prenatal diagnosis for future pregnancies.
On the other hand, the authors recognise that neonatal diagnosis could interfere with normal social and emotional growth and the child's interaction with his family.
Data are presented from a programme throughout Wales, which began in July 1990 with several aims.
These included giving families reproductive choice in future pregnancies, enabling them to plan for the future with a disabled child, avoiding the experience of a delayed diagnosis, and identifying a presymptomatic cohort who might benefit from future treatments.
Parents were given the option of having their male infants screened for Duchenne muscular dystrophy in addition to the routine tests for phenylketonuria and congenital hypothyroidism.
Informed consent was sought from parents, who were given an information sheet.
Blood was obtained by heel prick.
Out of 16 cases that tested positive for creatine kinase, nine were confirmed, giving an incidence of 1:3082 births.
Seven infants had a transient increase in creatine kinase activity; the authors say that no evidence exists that transient neonatal rises in creatine kinase activity are associated with the development of muscular dystrophy later in life.
The refusal rate has varied between 5% and 6%.
Considering the possibility of emotional trauma resulting from such a programme is crucial.
Interestingly, only one of the nine families with an affected child had an above average level of emotional trauma.
The parents in the other families indicated a preference for knowing about their son's condition early in life.
Undoubtedly, the low incidence of emotional trauma was due in part of the careful protocol, which acknowledged that families risked finding out that their apparently healthy baby had a lethal, untreatable disorder.
Accordingly, the programme was designed so that it could be modified in the light of experience, and families could choose to discontinue investigation at every stage from the initial screening test to the confirmed diagnosis.
Readers should pay particular attention to the careful, extensive community outreach organisation, which undoubtedly led to the minimal emotional impact of the programme.
The Clintons go to Washington: a healthy move?
Americans expect some reforms
Like the overnight success of a movie star, health care reform in the United States has leapt from oblivion to notoriety.
In the autumn of 1991 the Bush administration showed no interest in health care reform.
Then Harris Wofford upset a popular Republican senator in Pennsylvania, a victory largely due to Wofford's campaign slogan, ‘Universal access to health care.’
This victory ignited a political fire, which in turn fuelled voter concerns and inflamed the key participants into action.
Bill Clinton's election was, of course, primarily the result of a troubled American economy.
From January 1992 to the voting hour on 3 November, however, health care was consistently the voters' second greatest concern.
Now the new president has appointed his wife, Hillary Rodham Clinton, to lead a task force on health care reform.
What should she do?
The Clintons must address the driving forces behind the upward spiralling costs of health care if America is to truly reform its health care system.
Access to health care insurance is driven by the cost of coverage, which in turn is driven by the cost of care.
These driving forces are directly related to the supply driven nature of health care.
They include the high intensity services delivered to insured patients — especially, but not exclusively, patients aged over 65.
For example, with scant evidence that either length or quality of life is enhanced by coronary artery bypass surgery, American surgeons are four times more likely than Canadian surgeons to perform this procedure on patients aged over 75.
Although the popularity of living wills and ‘right to die’ initiatives is growing, these will do little to address the overuse of procedures and services that fuels the United States health care system.
Much tighter controls are needed over the use of technologies and procedures if health care costs are to be controlled.
These controls could be put in place through legislation or indirectly through the expansion of tightly managed care plans that in turn monitor use of technology.
If legislative controls are used developing sophisticated clinical practice guidelines for only those procedures with high cost and high variability, and requiring those guidelines to apply to all providers and payers, would be a healthy start.
A global budget, or even a more flexible expenditure target, could begin to slow total system costs.
The managed competition approach — creating health in-surance purchasing corporations which help small businesses and individuals choose between competing health plans based on cost and quality — is an untested theory.
If it works appropriate use of technologies and procedures will lead to success for any health plan.
If it doesn't low income workers will be stuck in low option plans, high income workers will opt out to fee for service schemes, and ‘quality’ health care will be as elusive as it is now.
A related driving force is the ratio of specialist to general physicians, which the United States arguably has backwards — two specialists for every generalist.
Specialists are far more likely to recommend or provide technologically intense services.
The fresh supply of specialists pouring out each year can be altered only by restructuring American medical education, but the intensity of services provided by existing specialists could be altered by any of the strategies described — practice guidelines, global budgets, or managed competition.
Not only is the total cost of the health care system overwhelming (14% of gross domestic product, $800bn a year), this cost is unevenly distributed.
It favours the employed with good incomes on the one hand and the Medicare and Medicaid programmes on the other.
Risk selection by large insurers and cost shifting by federal and state agencies leave lower and middle income families paying a larger share of the bill.
They also leave them more vulnerable to losing their insurance.
Taxing all health in-surance benefits provided by employers over a basic insurance package would reduce some inequities at the high end while producing revenues to help small employers and self employed workers to buy insurance.
Requiring all insurers to provide a single premium rate within a community and to guarantee to issue and renew policies to any member of that community (eliminating the practice of ‘red lining,’ refusing to cover people with certain illnesses or risk factors) would reduce many inequities as well as overall costs.
So too would establishing the same charges for both public and private payers.
These solutions could be put in place with or without the implementation of a managed competition system.
What is most likely to happen?
The relation between the economy and health care reform is vital.
The vulnerable middle class — worried about losing jobs and health insurance and not being able to afford health care — was the force behind the Clintons' election.
Unless the needs of this group are addressed it will be the force behind their defeat in 1996.
To win again in 1996 the Clintons must address the country's economic woes first.
If Americans believe that progress is underway in creating jobs and reducing the deficit they will be satisfied with incremental health care reform.
Consistent with campaign promises, insurers and pharma-ceutical companies are certain targets, but providers will not be let off scot free.
The most likely incremental reforms include insurance premium caps and price controls on hospitals, doctors and pharmaceuticals.
Beyond price controls, employer sponsored health care benefits over a basic insurance package are likely to be taxed.
These revenues will probably be used to increase middle class access to health care by providing tax credits to small businesses in a managed competition ‘play or pay’ system.
A less expensive, but less politically viable solution would be to expand children's access first, through a new programme or by expanding Medicaid.
Given the leading role that Ms Rodham Clinton and Donna Shalala, the Health and Human Services secretary, appear to be taking in developing solutions, and their explicit interest in children, we should watch for this ‘wild card.’
PAPERS
Experience with screening newborns for Duchenne muscular dystrophy in Wales
Abstract
Objectives —
To assess the acceptability of screening newborn boys for Duchenne muscular dystrophy.
Design —
Screening is offered on the basis of informed consent in response to an information sheet entitled ‘A new test for baby boys — Do you want it?’
The programme includes a prospective long term evaluation of family responses to early diagnosis and a comparison of their experiences and perceptions with those families who have undergone the later traditional clinical diagnosis.
Setting —
All maternity units throughout Wales.
Samples obtained through screening programme for phenylketonuria and congenital hypothyroidism.
Subjects —
Those families whose son had a positive screening test.
Main outcome measures —
Creatine kinase activity.
Venous blood test to confirm positive result.
Molecular genetic mutation analysis.
Muscle biopsy and dystrophin analysis.
Qualitative measure of satisfaction among affected families.
Results —
34 219 Boys have been screened and nine affected families have been identified.
Eight families were very positive about the programme.
Three chose not to complete the diagnostic process.
Conclusion —
The programme should continue to permit a full evaluation of the issues involved and should serve as a model for other initiatives within the community for genetic disease.
Introduction
Screening of newborns for Duchenne muscular dystrophy by measurement of blood spot creatine kinase activity has been technically possible since a suitable assay was devised in the mid 1970s.
Since then it has been offered on a limited basis in Germany and as a routine service in Lyons, France, and Antwerp, Belgium.
There have also been pilot programmes in Iowa, USA, New Zealand, and Edinburgh.
More recently, after the advances in the molecular genetics of Duchenne muscular dystrophy, programmes have started in Manitoba, Canada, Pennsylvania, USA, and now in Wales.
Before the advent of molecular genetics, screening of newborns in the United Kingdom was generally regarded as ethically unsound because no treatment was available; it offered little advantage to the family other than the possibility of terminating all subsequent male fetuses, more than half of whom would be normal.
Apart from a brief pilot study in Edinburgh it was not undertaken.
With the isolation of the gene responsible for Duchenne muscular dystrophy it became possible to track the defective gene in most pedigrees and to offer increasingly accurate prenatal diagnosis either by deletion analysis in many cases or by using linked markers.
Consequently, although the condition was still untreatable the neonatal identification could facilitate reproductive choice in further pregnancies in the immediate and the extended family.
This led to a reassessment of the ethical issues surrounding newborn screening.
The common clinical experience of diagnosing two (or occasionally more) affected boys simultaneously in a single family has to be weighed against the availability of prenatal diagnosis in future pregnancies once the first case has been identified.
In addition, diagnostic delay causes considerable distress to families and may result in subsequent bitterness if the child's symptoms are not taken seriously by family or health professionals or if a second affected boy is born.
Despite such considerations, there has been concern that a neonatal diagnosis would interfere with the normal social and emotional growth and interaction of child and family.
Gardner-Medwin proposed screening 18 month old boys who are not walking as an approach to the problem of delayed diagnosis.
This should detect half of affected boys.
Such a project was tried in Wales and though it did identify cases, logistical difficulties in community health care made the rate of detection unacceptably low.
After this an attitude survey found that most new mothers were in favour of screening newborn boys.
This group stated their (hypothetical) preference for the early diagnosis of serious disorders such as Duchenne muscular dystrophy even when no treatment was available.
This background led to the present study.
The six programmes currently in place worldwide vary in their structure as they function within the constraints of their particular national health care systems.
Here we present data from the programme throughout Wales, which started on 1 July 1990.
The aims were to give families reproductive choice in future pregnancies; to enable them to plan for the future with a child with a disability; to avoid the experience of a prolonged diagnosis; and to identify a presymptomatic cohort who may benefit from future treatments.
The principle and practice of the programme were assessed to determine whether screening was helpful from the perspectives of both families and health professionals.
Screening for Duchenne muscular dystrophy was introduced as a parental option for baby boys in addition to the routine tests for phenylketonuria and congenital hypothyroidism.
The decision to offer the test on this optional basis is unique within the context of neonatal screening.
This led us to monitor the implementation of the programme closely and to modify follow up protocols in the light of our experience.
This programme is also unique in including both prospective developmental and social evaluations, which will be described in detail elsewhere.
Methods
Informed consent is sought from parents on the basis of an information sheet entitled ‘A new test for baby  boys — Do you want it?’ printed on blue paper.
Midwifery staff are asked to give this to the mother sufficiently ahead of her discharge from hospital so that she can ask midwifery or paediatric staff any questions which might occur to her or her partner.
When the community midwife calls at home on day 6 or 7 to take the heel prick blood sample for the usual screening tests she asks whether or not they want their son tested for Duchenne muscular dystrophy and marks the screening card accordingly.
Our protocol (available on request from the authors) for the follow up of a positive test result was governed by three main considerations.
Firstly, we are offering a family the diagnosis of a lethal untreatable disorder in an apparently healthy baby; secondly, the protocol was designed so that we could modify practice in the light of experience; and, thirdly, families should have choice at every stage from the initial screening test to a confirmed diagnosis.
Clearly this is different from the normal practice in routine neonatal screening.
To help with consistency of approach paediatricians in each of the nine health districts in Wales were asked to choose one or more of their number to be nominated for this project.
After a positive test result the family practitioner and health visitor are contacted when the baby is about six weeks old to develop a social profile of the family and decide how best to approach them to offer a venous blood test to confirm or refute the screening test.
Before the family are contacted an appointment is arranged with the nominated paediatrician so that they can see the family within a day of their being approached by the primary health care team.
At this meeting the paediatrician ensures that the family are aware of the screening programme and the options that are open to them.
If they agree a venous blood sample is taken and another appointment arranged within 24 hours to give the result so that the period of uncertainty and anxiety is minimised.
After the result of the venous creatine kinase test has been given and explained families are invited to take part in the social evaluation of the screening programme.
Should the screening test result be confirmed the family are offered genetic counselling and the opportunity to further the diagnostic process, initially by molecular genetic mutation analysis and subsequently by muscle biopsy and dystrophin analysis.
Creatine kinase activity is measured semiquantitatively, fluorimetrically by a semiautomated version of that previously described.
Though screening programmes for Duchenne muscular dystrophy have detected cases of Becker muscular dystrophy, whether all or any cases will be detected in this programme is not known.
Until the results of long term follow up are available the ability of our creatine kinase assay to detect Becker cases remains uncertain.
Results and discussion
The overall figures for the first two years are given in table I. There have been a total of 16 positive test results, nine of which were confirmed on a subsequent venous sample, giving an incidence of 1:3802 male births.
The creatine kinase increase in all nine infants has been shown to be due to the muscle isoenzyme.
There have been seven infants with a transient elevation in creatine kinase activity.
This is a rate of 1:4888 (0.02%), which we consider acceptable and is in line with the best of current programmes, where rates of up to 0.2% have been observed.
uptake
Since the programme started there has been a steady refusal rate of 5–6% (fig 1).
One problem which proved far greater than anticipated was where no option was marked on the screening card.
This was in excess of 10% for the first few months and was over 20% in some parts of Wales.
As testing was done on the basis of parental choice these infants would not be screened.
Parents may have opted for the test, however, and, hearing nothing more, might assume that the result was normal.
If they were subsequently found to have an affected son, the apparent failure of the programme would bring it into unjustified disrepute.
We were anxious therefore to reduce to a minimum the number of samples where parental choice was unclear so from November 1990 we sent lists of infants untested to the local health district at the end of each month so that parental choice could be checked while the original sample was still usable.
This approach led to a steady decline in the number with no stated option (fig 1) and produced a steady stream of information about infants on the lists: requests to test, occasional refusals, and corrections of erroneous sex assignment on the sample.
The response to these lists, however, has been also quite variable (fig 2), ranging from some districts supplying information on over 80% of the infants listed to one with an 8% response.
Nevertheless, this  approach has reduced the percentage of samples with no parental choice recorded to 3%.
transient cases
Though our screening method is semiquantitative, the seven unconfirmed cases did have creatine kinase values lower than those which were subsequently confirmed as increased on a later venous sample.
Interestingly, five of these infants had some unusual perinatal features.
One had been given intramuscular ampicillin for a septic cord; one had had a traumatic caesarean birth; one had had a difficult birth with vacuum extraction and he also had a ventricular septal defect; one had a heart murmur; and one had been premature and in a special care unit with hypotonia and jaundice.
There is no evidence from any of the screening programmes to suggest that a transient elevation of creatine kinase activity in a neonate may lead to developing a muscular dystrophy in later life.
All seven families with a transient result were interviewed to gain their perspective of the programme and ensure they had no lingering doubts about their sons' health.
Generally anxiety was minimal with two exceptions.
One experienced considerable anxiety waiting for the result of the venous creatine kinase test because there was a family history of Duchenne muscular dystrophy on the maternal father's side.
The other family were made aware of the need for the venous sample some 24 hours before it was taken and were then not given the normal result for a further 24 hours.
Nevertheless, all the families were positive about the programme and would have another infant tested.
False positive results with a persistently elevated creatine kinase activity have been reported for the dominantly inherited but benign blood anomaly in which the creatine kinase BB isoenzyme is elevated in erythrocytes and thrombocytes.
They might also occur in infants with congenital hypothyroidism and in a few other muscle disorders; for example, the Manitoba group reported a false positive in an infant with a congenital myopathy as part of the Walker-Warburg syndrome.
It is too early to know of any false negatives but these would probably arise only by laboratory or administrative error.
positive cases
Gene studies — All nine families in which the babies had persistent creatine kinase elevation chose to have DNA analysis.
Five of the nine infants have been shown to be deleted from part of the Duchenne muscular dystrophy gene (table II).
Our laboratory had been routinely screening with probes cf56a, cf56b, and 9–7, which cover about 93% of deletions in our population.
We have recently altered this practice and now use a multiplex polymerase chain reaction that detects 98% of mutations, and we use Southern blotting analysis for confirmation.
This reduces reporting time to a few days.
Muscle biopsies — Though all nine families opted for DNA analysis only five then chose to follow up with muscle biopsy and dystrophin analysis.
This confirmed Duchenne muscular dystrophy in each case (table II).
In family 5 the mother's brother had previously been confirmed by muscle biopsy and so a biopsy on the infant was thought unnecessary.
In spite of increasing knowledge about the gene, at the present time the only way to distinguish between Duchenne muscular dystrophy and the more benign allelic Becker muscular dystrophy is by muscle biopsy and dystrophin analysis.
Though the incidence of Becker is only one tenth that of Duchenne muscular dystrophy, the three families who refused a biopsy did so on the grounds that while they accepted their son had a muscular dystrophy they preferred to live in the hope that it might be Becker and so chose not to complete the diagnostic process.
Familial cases — In two cases the screening programme identified two affected boys in a sibship before the elder boy had been diagnosed (table II).
When family 1 was detected the elder brother was under orthopaedic management for his abnormal gait but Duchenne muscular dystrophy had not been suspected.
Seven mothers seem to be carriers based on their serum creatine kinase values.
The mother in family 5 had a brother with Duchenne muscular dystrophy.
Genetic counselling was sought by all nine families, but it is still too soon to comment on their reproductive choices.
Prenatal diagnosis, however, has been performed on the fetus of the aunt of one boy, and genetic counselling has been requested by several other members of the extended families.
Experience of the families — It would be neither feasible nor desirable to intrude upon these families at frequent intervals to monitor their response to the emergent diagnosis of Duchenne muscular dystrophy.
We have therefore sought only one in-depth interview with each family so far, about six months after the positive screening test; any other direct contact with the families has been limited and only when sought by them.
To monitor their responses we have kept in close touch with each family's health visitor.
The health visitors have maintained frequent contact, and we have guided their observations to record specific aspects of the evolving mother and baby relationship.
This information has proved invaluable and could not have been gathered in any other way.
Emotional trauma — The detailed results of the social evaluation will be reported elsewhere, but so far only one of the nine families seems to have experienced an above average level of emotional trauma.
The others have been very positive about the programme, stating that they preferred to know about their sons' condition from an early stage.
Apparently this family was not given the information sheet about the programme, and they feel that, had they seen it, they would have refused the test.
The disclosure about their son's positive test result was made at 2 to 3 weeks.
Their health visitor noted a difference in mother's handling of the baby and visitors were no longer encouraged to hold the boy.
This lasted for some weeks but at no time was the baby at risk either physically or psychologically nor was the mother and baby bond threatened.
There were additional factors with this family which compounded their inevitable distress, not least the subsequent diagnosis of Duchenne muscular dystrophy in an elder son, in whom signs of clinical abnormality had been recognised before the birth of his younger brother.
This family was the second detected and our practice has since been modified to make a disclosure no sooner than 6 weeks.
Interestingly, this family lives in the area that is our poorest responder to the monthly listings of infants where parental choice was not stated on the screening sample (fig 2, area 27).
The refusal rate in this area is now the highest in Wales and increased dramatically after the disclosure (fig 3).
Probably, in a close-knit community the negative response of this family has influenced the uptake of the test.
Our concern is not the actual refusal rate but to ensure that families have choice on the basis of informed consent.
As part of the study of the emotional response of families to the early diagnosis of Duchenne muscular dystrophy we are also interviewing those families whose boys have undergone the later traditional diagnosis.
This will allow us to compare the experiences of both groups.
Implications for community education — From the outset, we have devoted a lot of effort to providing a continuing educational programme for health professionals.
The importance of this cannot be overemphasised.
The response from midwives, health visitors, and paediatricians to the education programme has generally been positive and has helped in the practical implementation of the test.
It has also enabled the health professionals to give us feedback on the screening from their perspective.
Conclusion
When screening began we agreed to discontinue it at the end of the first year if there were a number of traumatised families.
This has not been the case; in fact most families have been pleased to know the diagnosis so early.
This permits future informed reproductive planning, avoids diagnostic complications and delays, and allows planning of care for the affected boy.
We are confident that the programme should continue to permit a full evaluation of the issues raised by screening newborn males for Duchenne muscular dystrophy and hope to extend its duration so that the perspectives of the families may be assessed as the boys develop symptoms and signs of muscular weakness.
We also believe that this project will serve as a model for the evaluation of other community genetic interventions, with its emphasis on offering optional tests based on informed consent and the education of primary health care teams.
Explaining socioeconomic differences in sickness absence: the Whitehall II study
Abstract
Objective —
To describe and explain the socioeconomic gradient in sickness absence.
Design —
Analysis of questionnaire and sickness absence data collected from the first phase of the Whitehall II study.
Grade of employment was used as a measure of socioeconomic status.
Setting —
20 civil service departments in London.
Subjects —
6900 male and 3414 female civil servants aged 35–55 years.
Main outcome measures —
Rates of short spells (>-7 days) and long spells (>7 days) of sickness absence.
Results —
A strong inverse relation between grade of employment and sickness absence was evident.
Men in the lowest grade had rates of short and long spells of absence 6.1 (95% confidence interval 5.3 to 6.9) and 6.1 (4.8 to 7.9) times higher than those in the highest grade.
For women the corresponding rate ratios were 3.0 (2.3 to 3.9) and 4.2 (2.5 to 6.8) respectively.
Several risk factors were identified, including health related behaviours (smoking and frequent alcohol consumption), work characteristics (low levels of control, variety and use of skills, work pace, and support at work), low levels of job satisfaction, and adverse social circumstances outside work (financial difficulties and negative support).
These risk factors accounted for about one third of the grade differences in sickness absence.
Conclusion —
Large grade differences in sickness absence parallel socioeconomic differences in morbidity and mortality found in other studies.
Identified risk factors accounted for a small proportion of the grade differences in sickness absence.
More accurate measurement of the risk factors may explain some of the remaining differences in sickness absence but other factors, as yet unrecognised, are likely to be important.
Introduction
An inverse relation between socioeconomic status and mortality has been documented in many studies, no matter how socioeconomic status has been measured.
The challenge is to explain this relation.
The Whitehall study of British civil servants, which started in 1967, showed a steep inverse association between social class (assessed by grade of employment) and mortality from a wide range of diseases.
After 10 years of follow up those in the highest grades of employment had about one third the mortality of those in the lowest grades.
This difference in mortality was only partly explained by differences in age, smoking, systolic blood pressure, height, and plasma cholesterol and blood glucose concentrations.
The limited data available suggest there are also substantial socioeconomic differences in morbidity, but these differences remain largely unexplained.
The Whitehall II study examined a new cohort of 10314 civil servants between 1985 and 1988 with the aim of explaining socioeconomic gradients in morbidity and mortality.
In addition to the established risk factors included in the first Whitehall study, the second study has documented differences by grade of employment in work characteristics, social support, and health related behaviours.
This paper seeks to describe and explain observed grade differences in sickness absence.
Rates of sickness absence are used as a measure of morbidity.
Like most measures of morbidity sickness absence is influenced by social and psychological factors as well as illness.
Sickness absence is important as a measure of ill health; as a measure of use of health services; as a cause of lost productivity; and as an indicator of an employee's ability to cope with and maintain normal roles at work.
The costs of sickness absence to the government and to industry are substantial.
In the United Kingdom more than 370 million work days are lost each year owing to certified incapacity.
A survey by the Confederation of British Industry in 1986 estimated that absence from work cost British industry at least £5 billion.
Trends in sickness absence since the 1950s suggest that these costs are likely to increase.
Subjects and methods
study population
All non-industrial civil servants aged 35–55 working in the London offices of 20 departments were invited to participate in the study.
The overall response rate was 73% (74% for men and 71% for women).
The true response rate is likely to be higher, however, because around 4% of the civil servants on the lists provided by the civil service had moved before the study and were therefore not eligible for inclusion.
The response rate varied by grade of employment, being 81% in the three higher grade categories (defined below) and 68% in the three lower categories.
In total, 10314 civil servants participated, of whom 67% (6900) were men and 33% (3414) were women.
Most participants (94%) gave consent for follow up based on their sickness absence records.
A small proportion of records (5%) could not be identified.
Sickness absence records of 9072 participants (88% of the total sample) were examined over a mean period of 20 (range 0.3–39.6) months.
Information on grade of employment was obtained by asking participants to give their civil service grade at the time of the baseline survey.
Changes in grade during the follow up period were not analysed.
On the basis of salary, the civil service identifies 12 non-industrial grades which, in order of decreasing salary, consist of seven ‘unified grades,’ senior executive officers, higher executive officers, executive officers, clerical officers, and clerical assistants and office support staff.
Unified grade is used by the civil service to refer to the combination of administrative grades (previously known as permanent secretary, deputy secretary, under secretary, assistant secretary, senior principal, and principal) and professional or technical staff with equivalent salaries.
Similarly, the remaining professional or technical staff are combined with administrative grades on the basis of salary.
To obtain significant numbers we combined unified grades 1–6 into one category and clerical officers, clerical assistants, and office support staff into another category, thus producing six grade categories (table I).
There was a steep increment in salaries between grade categories — from an annual salary in 1987 of £18020-£62100 in category 1 to £3060-£6790 in category 6.
However, most of the civil servants in the top category were at the lower end of the pay scale, with 82% of men and 83% of women in category 1 earning between £18020 and £27065.
Differences in other socioeconomic indicators (education, housing tenure, car ownership, and father's occupation) by grade of employment have been described.
baseline survey
Between November 1985 and March 1988 participants completed questionnaires and attended a screening examination.
The questionnaire included personal details (age, sex, current grade of employment, ethnic group, marital status, years of full time education, highest level of education, partner's and father's occupation, housing tenure, and car ownership); health (self rated health over the past 12 months, presence of longstanding illness, and presence of recurring health problems based on questions used in the general household survey, and presence of psychiatric symptoms based on the 30 item general health questionnaire); health related behaviours (current smoking habits, usual frequency of alcohol consumption in past 12 months, and amount of alcohol consumed in the past seven days); psychosocial work characteristics (assessed with a 67 item questionnaire based on the occupational strain model of job demands and decision latitude which included questions on control, variety and use of skills, and work pace as proposed by Karasek, support at work, and job satisfaction); social circumstances outside work (number of dependent children, social contact with relatives and friends, and personal difficulties such as financial problems); and types of social support (assessed by 15 self report questions on up to four nominated close friends or relatives).
Three types of social support were confirmed by principal components analysis: confiding or emotional support, practical support, and negative aspects of support.
Further details on the work and social support have been reported.
sickness absence records
Computerised sickness absence records to the end of March 1988 were obtained annually from the civil service pay centres.
These records included the first and last dates of all absences and the reason for absence.
For absences of seven calendar days or less (short spells), civil servants were able to complete their own certificate explaining their absence.
For absences of more than seven calendar days (long spells), a medical certificate was required.
Sickness absence records were checked for inconsistencies.
Overlapping, consecutive, or duplicate spells of sickness absence were merged after taking account of weekends and public holidays.
This affected less than 1% of all spells of sickness absence.
statistical analysis
Risk factors for sickness absence (or the size of their effect), may differ for short and long spells.
Short spells (seven days or less) and long spells (more than seven days) of sickness absence were analysed separately.
For each employee, the number of spells of sickness absence of each type was computed and the follow up period was measured in person years.
Rates of sickness absence are expressed per 100 person years.
Age adjusted rates were calculated by direct standardisation with the total sample as the standard.
The number of spells of sickness absence is a form of count data and therefore Poisson regression models were fitted to the data.
We assumed that for each participant the occurrence of short spells followed a Poisson distribution.
The regression model for the i t h individual was log (y i ) =log (T 2 i) + α+; β 1 x i 1 +…=+β p x i p +Ε i , where y i is the observed number of short spells of absence T i is the number of years of follow up, x i 1 …x i p are the explanatory factors of interest, and Ε i is a Poisson error term.
Adjusted rate ratios and their 95% confidence intervals were calculated for men and women separately by this method.
The Poisson model implies that the variance, in the rates of sickness absence between individuals is equal to the expected rate of sickness absence.
If rates of sickness absence vary between individuals after taking account of the explanatory factors this may lead to extra variation (overdispersion) relative to that predicted from the Poisson model.
Considerable excess residual variation was found in the rate of sickness absence for short spells.
A scale parameter was therefore estimated by dividing this residual variation (deviance) by the degrees of freedom and was used to adjust the width of the confidence intervals.
This had no effect on the rate ratio estimates but the width of the confidence intervals was increased by about 50%.
For long spells of absence, the Poisson model was used but no overdispersion was detected.
Participants with incomplete data were excluded from the analyses including the missing variables.
Several questions on health and social supports were added to a later version of the screening questionnaire.
Consequently, the number of subjects varied when different explanatory variables were used.
Comparison of age adjusted and fully adjusted rate ratios, however, were based only on subjects with no missing data.
The overall trend in sickness absence rates across the grades was assessed by fitting a linear term for employment grade.
The regression models were fitted by using the statistical package GLIM.
The error term was set as Poisson and the logarithm of person years at risk was declared as an offset.
All other analyses were performed with the statistical package SAS.
Results
Table II shows the total number of short and long spells and the overall rates of sickness absence for men and women.
On average, men and women had 1.3 and 2.1 absences per person per year respectively for short  spells and 0.1 and 0.3 absences per person per year respectively for long spells.
Figures 1 and 2 show a strong inverse relation between grade of employment and rates of short and long spells of sickness absence adjusted for age.
Except for women in the highest grade, there was a stepwise increase in rates of both short and long spells.
For men, those in the lowest grade had rates of short and long spells of sickness absence 6.1 (95% confidence interval 5.3 to 6.9) and 6.1 (4.8 to 7.9) times higher than those in the highest grade, respectively.
For women, the corresponding rate ratios were 3.0 (2.3 to 3.9) and 4.2 (2.5 to 6.8) respectively.
The grade differences in sickness absence were present in all age groups.
Table III shows the relation between other risk factors and sickness absence.
Men and women who reported average or worse health in the past 12 months had 60% higher rates of short spells of absence and more than twice the rates of long spells compared with those who reported good health.
Differences were also observed for other self reported measures of health, including the presence of recurring health problems, longstanding illness, and psychiatric symptoms.
Participants who smoked also had higher rates of short and long spells of sickness absence compared with non-smokers.
The relation between alcohol consumption and sickness absence will be reported elsewhere.
For men, there seemed to be a U shaped relation between alcohol consumption and the rate of short spells of sickness absence, whereas for long spells only those who were frequent drinkers (more than once daily) had higher rates.
For women, there was no clear relation between alcohol consumption and sickness absence.
There was also a relation between ethnic group and sickness absence with higher rates of both short and long spells of absence in Asian men and women.
Psychosocial factors at work and outside work also predicted rates of sickness absence.
Men and women who rated their jobs as low for control, variety and use of skills, pace, support at work, or satisfaction had higher rates of short and long spells of sickness absence compared with those who rated their jobs high for these characteristics.
This was particularly striking for control and variety and use of skills.
For example, men who reported low variety and use of skills had 72% and 82% higher rates of short and long spells of absence respectively, compared with those who reported high variety and use of skills.
Similarly, participants who reported negative support from their closest friend or relative or financial difficulties had higher rates of sickness absence.
Men who reported that talking with their closest friend or relative made things worse had 29% and 40% higher rates of short and long spells of absence, respectively, compared with those who reported positive support from this person.
Men and women who reported great financial difficulties had about 40% higher rates of both short and long spells of absence compared with those who reported few financial difficulties.
Table IV shows the rates of sickness absence by grade after adjusting for the health related behaviours (smoking habits and frequency of alcohol consumption), ethnic group, work characteristics, and social circumstances outside work which predicted rates of sickness absence.
The effect of the adjustment is greater in the lower than the higher grades, reflecting the fact that the mediating factors show progressive differences across the employment grades.
When fitting a linear term for employment grade, the increasing trends in short spells of sickness absence across the grades were reduced by 27% for men and 40% for women from those seen before adjustment.
Grade differences in long absences were reduced by 11% for men and by 36% for women after adjustment.
Even after adjustment the differences between those at the top and those at the bottom remained substantial: among men, those in grade 6 had 3.7 and 6.4 times  higher rates of sickness absence than those in grade 1 for short and long spells respectively.
Among women, the corresponding ratios were 2.1 and 2.6 respectively.
Adjustment for work characteristics and social circumstances outside work alone reduced the trends in short absences across the grades by 26% for men and 35% for women.
The trends in long absences across the grades, however, were reduced by only 5% for men and 15% for women.
Other risk factors examined were the frequency of social contact, father's social class, age joined civil service, number of children, attendance at religious services, and physical activity.
None of these was a confounder or mediator of the relation between grade and sickness absence.
Discussion
Our findings are consistent with reports of higher rates of sickness absence among less skilled non-manual or manual employees.
Despite the consistency of the socioeconomic differences in sickness absence no studies have examined possible explanations for them.
We found striking gradients in both short and long spells of sickness absence, with higher rates among employees with lower status.
Moreover, the gradients were observed even among managerial and executive staff (grade categories 1–4).
We have used sickness absence as a measure of morbidity.
Several findings support this interpretation.
Similar grade differences in both short and long spells of sickness absence were observed.
This suggests that the grade differences cannot be attributed to employees in the lower grades being more likely to take the odd day off or to those in the higher grades staying at work despite minor illness.
Perceived health status was a strong predictor of rates of both short, and to a greater extent, long spells of sickness absence.
Earlier studies have reported that perceived health status predicts subsequent mortality.
As further follow up data accumulate we will be able to examine whether sickness absence predicts serious morbidity and mortality in the Whitehall II study.
Psychological and social factors predicted rates of short, and to a lesser extent, long spells of absence.
Most measures of morbidity which assess functional impairment, of which sickness absence is one, are influenced by factors other than health.
The decision to take time off work or to return to work is complicated and will vary between individuals and at different times.
explanations for differences
Substantial differences in sickness absence by grade remained even after adjustment for a wide range of risk factors.
This is consistent with several earlier studies which have attempted to explain socioeconomic differences in morbidity and mortality by differences among subjects in known or suspected risk factors.
In the first Whitehall study risk factors such as age, smoking, systolic blood pressure, height, and plasma cholesterol and blood glucose concentration accounted for about a fifth of the twofold to threefold grade differences in mortality from coronary heart disease.
Similarly in the British regional heart study age, smoking, systolic blood pressure, and serum cholesterol concentration accounted for less than a sixth of the 1.5 fold social class differences in coronary heart disease morbidity and mortality.
In the Alameda country study age, sex, race, socioeconomic factors (income, education, employment), health status, health related behaviours (smoking, alcohol consumption, physical activity, body mass index, and sleep patterns), access to medical care, social support, and psychological factors accounted for less than a sixth of the 1.7-fold difference in mortality from all causes between poor and more affluent areas.
These studies suggest that, even when recognised risk factors exist, as for coronary heart disease, and differences in subjects' risk factors are taken into account, the relation between socioeconomic status and health cannot be fully explained.
In the Whitehall II study, differences between subjects in health, health related behaviours, psychosocial aspects of the work environment, and social circumstances outside work accounted for about a third of the threefold to sixfold differences in sickness absence by grade.
This still leaves a large proportion of the variation unexplained.
Several explanations for the persistent grade differences in sickness absence need to be considered.
Firstly, the grade differences in sickness absence may have been spurious.
Anecdotal evidence suggests that managers and professional employees are more likely to be absent from work without record than employees with lower status.
There was general agreement among senior civil servants that the recording of sickness absence may be incomplete among a small proportion of participants in grade category 1, but this was unlikely to occur in other categories.
Although it was not possible to evaluate the completeness of the sickness absence records directly, the records were used for pay purposes and were therefore likely to be complete for most participants.
Incomplete recording may partly account for the low rates of sickness absence among those in the highest grades, but it is unlikely to explain the large differences between other grades.
Secondly, several explanatory variables were difficult to measure and were only measured at one point in time.
The work characteristics and social support were more likely to have been misclassified than some of the other explanatory variables.
This would result in underestimation of their effects and incomplete adjustment of the grade differences in sickness absence.
The risk factors therefore probably accounted for a greater proportion of the grade differences in sickness absence than was observed in the multivariate analysis.
Thirdly, factors which were not measured might be important.
Risk factors specific to the individual such as work ethic or commitment to the organisation need to be considered.
Alternatively, risk factors at the group level, such as attitudes about acceptable levels of sickness absence, may be important.
Several papers have discussed the concept of absence cultures.
Absence cultures refer to shared attitudes about the level of sickness absence which is tolerated by groups within organisations.
Over 40 years ago, Hill and Trist suggested that on joining an organisation employees observe the formal and informal responses of the organisation towards different levels of sickness absence and adopt levels of sickness absence that reflect these observations.
While these factors may influence rates of short spells of sickness absence, they are less likely to  influence rates of long spells.
It is more likely that other factors, as yet unrecognised, are important.
application outside civil service
To what extent can these findings from the British civil service be generalised to other occupational groups?
The study population consisted of a representative sample of non-industrial civil servants in a diverse range of departments in London.
At the time of the study, about 75000 civil servants from a total of about 700000 were based in central London.
The organisational culture of the civil service may differ from organisational cultures in the private sector but is likely to be similar to other office based white collar organisations in the public sector.
The civil service has a generous sickness absence policy, and civil servants receive full pay for sickness absence of up to six months in any 12 months.
Employees with prolonged or repeated sickness absence are referred to the occupational health service.
The civil service tends to offer good job security.
In this study, the mean length of service was 19 years for men and 15 years for women.
However, since 1985 several participating departments have undergone large organisational changes which are likely to affect job security.
Factors which are more common in large urban areas, such as complex commuter travel, have been shown to predict rates of sickness absence.
We did not assess travel to work in detail.
However, participants' living circumstances — for example, housing tenure and car ownership — were similar to those in comparable socioeconomic groups in national surveys (Department of Employment, personal communication 1987).
Despite these considerations, overall rates of sickness absence in the civil service are similar to, or lower than, those observed in other British organisations.
The civil service had several advantages for a study of this type.
It was possible to examine sickness absence within an organisation with a single sickness absence policy.
Turnover was low so there was minimal loss to follow up.
Successful collaboration during the first Whitehall study made it easier to agree on potentially difficult issues such as obtaining sickness absence records from the pay centres.
Compared with studies based on the registrar general's classification of socioeconomic status, the occupations within grade categories in the civil service were relatively homogeneous.
However, between grade categories there were striking differentials in income and other measures of socioeconomic status.
Finally, it is possible to compare the findings for men and women with similar occupational status.
There were interesting differences in the sociodemographic background of men and women in the same grades — men in the higher grades were more likely to be married and have dependent children than women, whereas the opposite pattern was observed in the lower grades.
Explanations for sex differences in sickness absence are being analysed and will be reported elsewhere.
The implications of these findings need to be considered.
Sickness absence is a big problem both in terms of lost productivity and cost and in terms of employees' wellbeing.
Some of the risk factors identified, such as the work characteristics, could be modified to reduce sickness absence.
However, substantial grade differences in sickness absence persist after taking account of a wide range of risk factors.
These large differences in sickness absence rates among employment grades categories parallel differences seen in morbidity and mortality in other studies.
None of these studies have been able to ‘explain’ fully the observed gradients.
One possible explanation for this difficulty is that research rarely considers the broad range of causal factors that is probably necessary.
A dominant tradition in public health research has been the risk factor approach, emphasising such phenomena as cigarette smoking, obesity, and blood pressure.
Another tradition, focusing on psychosocial factors, typically de-emphasises or neglects the role of these established risk factors.
We are now collecting both types of data, which will allow us to examine their joint effects on sickness absence and mortality.
However, it is also likely that other factors, as yet unrecognised, are important.
Socioeconomic differences in sickness absence warrant further investigation.
Vitamin A supplementation in infectious diseases: a meta-analysis
Abstract
Objective —
To study the effect of vitamin A supplementation on morbidity and mortality from infectious disease.
Design —
A meta-analysis aimed at identifying and combining mortality and morbidity data from all randomised controlled trials of vitamin A.
Results —
Of 20 controlled trials identified, 12 trials were randomised trials and provided ‘intention to treat’ data: six community trials in developing countries, three in children admitted to hospital with measles, and three in very low birth weight infants.
Combined results for community studies suggest a reduction of 30% (95% confidence interval 21% to 38%; two tailed p<0.0000001) in all cause mortality.
Analysis of cause specific mortality showed a reduction in deaths from diarrhoeal disease (in community studies) by 39% (24% to 50%; two tailed p<0.00001); from respiratory disease (in measles studies) by 70% (15% to 90%; two tailed p=0.02); and from other causes of death (in community studies) by 34% (15% to 48%; two tailed p=0.001).
Reductions in morbidity were consistent with the findings for mortality, but fewer data were available.
Conclusions —
Adequate supply of vitamin A, either through supplementation or adequate diet, has a major role in preventing morbidity and mortality in children in developing countries.
In developed countries vitamin A may also have a role in those with life threatening infections such as measles and those who may have a relative deficiency, such as premature infants.
Introduction
In 1928 Green and Mellanby noted that though vitamin A was then known as the growth promoting vitamin, evidence from animal studies showed it was also an anti-infective vitamin.
Four years later, Ellison reported a controlled trial of 600 English children hospitalised with measles which showed that cod liver oil reduced mortality by 58%.
But the role of vitamin A in preventing xerophthalmic blindness, combined with the discovery of antibiotics, overshadowed its possible anti-infective role.
In 1986 Sommer et al reported a seminal randomised trial showing a 34% reduction in the all cause mortality of preschool Indonesian children without florid signs of deficiency as a result of two 200000 IU doses of vitamin A given six months apart.
Several months later Barclay et al reported a 50% reduction in mortality in children hospitalised with measles, although this result was not statistically significant.
A recent large randomised study in India found no such effect but rather an apparent slight excess of deaths in the supplemented group.
In the subsequent exchange of letters it was asserted that there is no evidence that high doses of vitamin A reduce the rate of infection and, as a mechanism was lacking, the effect on mortality was equally suspect.
Given the magnitude of the apparent benefit and the simplicity and inexpensiveness of the intervention, clarification of the role of vitamin A is of considerable importance.
We therefore performed a meta-analysis of the available randomised trials of vitamin A supplementation, looking at total mortality, cause specific mortality, and morbidity from infectious diseases.
Methods
identification of trials
We aimed to examine and combine all randomised controlled trials of vitamin A supplementation for the prevention of death or morbidity from infectious disease, in particular respiratory and gastrointestinal disease.
Two methods were used to locate primary research data.
Medline was searched independently by one of the authors, a research assistant, and a librarian for the years 1969 to 1992, using combinations of the following key words: vitamin A, respiratory disease, diarrhoea, random allocation, and clinical trial.
In addition, the references of the available primary studies, review articles, and editorials were checked to identify references not found in the Medline searches.
A report was dropped from further analysis if the study did not include concurrent controls or contained no original data or if the report did not address mortality, respiratory disease, or diarrhoea.
Several trials that looked at cancer prevention were not considered within the scope of this analysis.
On the basis of these initial broad criteria, clearly irrelevant articles were discarded after consideration by a single reviewer.
The methods sections of the potentially relevant reports were then extracted with reference to results and identifying material (authors, title, journal, institution, and country of origin) removed.
These methods sections were assessed independently by two reviewers with respect to randomisation (individual, cluster, or none), use of a placebo, loss to follow up, measuring outcome blind to treatment assigned.
Papers that did not include a control group, were not randomised, did not allow calculation of ‘intention to treat’ results, or did not collect information on mortality or the incidence of respiratory or gastrointestinal infection were excluded.
statistical methods
We extracted from each trial data for an analysis by intention to treat — that is, an analysis that retains all individuals within the group to which they were randomised, regardless of compliance.
Regrouping of data was necessary for one trial: table III of that paper was used to calculate the deaths in each group irrespective of the dose of vitamin A actually received.
The Mantel-Haenszel estimate of the overall odds ratio and its variance were calculated from a set of studies by the Robins-Breslow-Greenland method.
To test whether the variation between studies was explicable by chance, the Breslow and Day approximation  for Zelen's homogeneity statistic was used.
All calculations were done with StatExact.
One concern with any meta-analysis is that statistically non-significant studies are less likely to be submitted for publication and hence that the results of combining published studies is biased towards a positive effect.
To examine the potential for such publication bias, we calculated Rosenthal's ‘file drawer’ N, which is an estimate of the number of similar sized, but unpublished, studies with no effect (that is, an odds ratio of 1) needed to render the meta-analysis result no longer statistically significant.
Results
Table 1 shows the 20 controlled trials identified and gives details of the population and methods of these trials.
Most were community based trials of children in developing countries, with children who had overt vitamin A deficiency excluded.
The included studies fell into three distinct groups: six community based trials, three trials of children hospitalised with measles, and three trials in very low birthweight infants.
Eight of the 20 controlled studies were excluded: four were not randomised, three did not provide intention to treat data, and one did not provide sufficient outcome data.
Ellison's 1932 British study of measles showed a significant mortality reduction, but the patients were allocated by the ward to which they had been admitted.
The other non-randomised studies were three community studies.
Of particular interest was the Indonesian community study that used fortified monosodium glutamate, rather than capsule supplementation, in five programme villages, with five other villages used as controls.
This showed a reduction in mortality in the programme villages: the odds ratio for death was 0.69 (95% confidence interval 0.57 to 0.84; p=0.004).
The largest excluded study was conducted in Sudan but used an allocation method whereby the families given vitamin A and placebo were recruited by two separate researchers.
Thus, differential abilities to recruit may cause selection biases; indeed the baseline results in this study showed highly significant differences in important confounders — for example, household cleanliness was worse in the vitamin A group (p<0.0001), indicating a failure of randomisation by this procedure.
A recent randomised trial from Ghana was described in a letter but details on loss to follow up, on use of an intention to treat analysis, and on several end points, including mortality, were missing.
Similarly, a recent study in Bangladesh excluded an unknown number of poor compliers and did not describe mortality.
A randomised trial of children in South Australia who were prone to recurrent respiratory infections showed a reduction in respiratory events for those taking vitamin A, but not a reduction in total days of illness.
A subsequent trial in children previously infected with respiratory syncytial virus showed no reduction in either incidence or days of symptoms.
However, in both trials almost a third of randomised individuals were excluded from the analysis either because of loss to follow up or insufficient compliance with medication, and an analysis by intention to treat could not be reconstructed from the data presented nor from data still with the principal author (personal communication).
Inclusion of any of these studies would have made little difference to our conclusions about mortality reduction.
All cause mortality
Table II shows the death rates for trials that reported at least one death.
One community study looked at the incidence of respiratory and diarrhoeal disease but reported no mortality.
The overall mortality reduction in the five included community studies (fig 1) was highly significant, both statistically (two tailed p<0.0000001) and clinically: a common odds ratio of 0.70 (0.62 to 0.79; confidence interval not adjusted for cluster design of community studies) indicating a risk reduction of 30% — that is, a reduction of about a third in mortality from all causes.
The test for homogeneity in the five community studies did not suggest any significant differences among them (p=0.16).
The three studies we included of children hospitalised with measles were of the highest quality — placebo controlled, individually randomised, and with complete follow up— and also showed a somewhat greater mortality benefit than the community studies: an odds ratio of 0.34 (0.15 to 0.77)— that is , a risk reduction of 66%.
This may in part be explained by the lower compliance in the community studies: using intention to treat analysis results in a diluting of effect when compliance is low.
Sommer and Zeger have estimated that if full compliance had been achieved in the Aceh study the reduction in mortality in months 4 to 12 would have been 72% rather than the 41% given by the intention to treat analysis.
This estimate is close to that of the combined measles studies.
Two community or measles studies that we included  had more than a single death and were not individually statistically significant: those by Barclay et al and Vijayaraghavan et al .
The point estimate for the study by Barclay et al is close to the combined estimate; that by Vijayaraghavan showed least effect but also had the lowest mortality in the control group, and its confidence intervals include the overall community study reduction of 30%.
Figure 2 shows the excluded studies in which some deaths occurred.
The overall result for the three community studies is almost identical to that of the included studies (odds ratio 0.74; 0.64 to 0.86) but showed significant heterogeneity (p<0.0001).
We cannot know whether this is due to difference in the real effect or differences in degree of bias.
One remaining concern is publication bias — that publication has favoured those trials with positive results.
We therefore calculated the ‘fail safe N,’ the number of negative papers that would be required to overturn this result, based on the five included community studies with any deaths.
This was 53, which would mean that 53/ (53+5) =91% of all trials of the effect of vitamin A on childhood mortality would need to be unpublished.
This seems unlikely; hence the overall evidence for a reduction in mortality is strong.
cause specific mortality
Of the included studies, three of the five community trials plus two of the three measles trials reported cause specific mortality.
The third measles trial reported that of the 12 deaths (10 in controls, two in those given vitamin A), 10 were due to respiratory disease.
We used the conservative assumption that both children given vitamin A and hence eight of the 10 controls died from respiratory disease.
Figure 3 shows the combined data on cause specific mortality.
The community trials combined show a 39% reduction in diarrhoeal deaths (24% to 50%; two tailed p<0.00001), a 55% reduction in measles deaths (13% to 77%; two tailed p=0.017), and a slight increase in respiratory deaths, though this was not significantly different from an odds ratio of 1.
Most interestingly, the community trials showed a 34% reduction in other causes of death (15% to 49%; two tailed p=0.0001), excluding diarrhoea, respiratory causes, and measles.
The three measles studies combined show a 70% reduction in deaths from respiratory causes (15% to 90%; two tailed p=0.02) and a  reduction in deaths from diarrhoea consistent with the community trials but with very wide confidence intervals.
effects on morbidity
Morbidity has been less extensively studied.
Only six of the studies reported results for respiratory or diarrhoeal morbidity: three of the community trials, two of the measles trials, and one of the trials in very low birthweight infants.
Data on morbidity were reported in a variety of ways, and the results in table III are a mixture of prevalence, incidence, and episode duration (measles studies).
Hence overall summary measures have not been presented.
However, the results show either a decrease in morbidity or no change, suggesting effects at least in some circumstances.
In particular, morbidity for those hospitalised with measles is clearly reduced.
The trial by Shenai et al showed a reduction in lower respiratory tract infections from 55% in the control group to 21% in the vitamin A group (p<0.029).
Two of the three studies of very low birthweight babies also showed significant reductions in the incidence of bronchopulmonary dysplasia and duration of ventilator requirements.These were both double blinded trials with the control group receiving placebo injections; hence reductions in ventilator requirements cannot be explained by the investigator's knowledge of treatment.
Discussion
These results show clear evidence that vitamin A reduces all cause mortality in children in developing countries by around one third.
A similar but apparently stronger effect is seen in children hospitalised with measles, with a reduction of 66%, although this was not significantly different from the 30% seen in developing country community settings.
Of note is that the study by Vijayaraghavan et al is consistent with this overall effect: the confidence interval of that study included this 30% reduction, and the test of homogeneity did not suggest a difference between studies.
The sum of evidence is now too strong to justify further placebo controlled trials in communities with vitamin A deficiency.
Rather, optimal methods of delivery should be explored — for example, dietary alteration, supplementation through other products such as monosodium glutamate, or supplementation before peak incidence periods or in high risk groups such as refugee camps.
Daulaire calculated that one death was prevented for every 55 children given supplements.
The other communities studied all had lower death rates, and hence this ‘number needed to treat’ would be higher.
Patel has recently calculated that vitamin A supplementation is among the most cost effective interventions in developing countries, with a cost of around $95 per life saved, compared with $230 for oral rehydration and $850 for measles vaccination.
Although capsule distribution could be linked to other programmes such as immunisation, it must be given two to three times a year to achieve continuous coverage, and the risk of toxicity also needs consideration.
Without other support programmes to improve dietary and general health and hygienic conditions, capsule distribution has had poorer results than anticipated in controlling xerophthalmia.
Reductions in mortality similar to that found by our meta-analysis have also been seen in the controlled (non-randomised) study by Muhilal et al , achieved by fortifying monosodium glutamate supplied to villages; this maintains retinol levels better than infrequent high doses.
Such food fortification programmes would seem to be the most practical method in countries where no major improvement in dietary intake is anticipated.
Clearly, in the long term, improving dietary habits and food supply is the most desirable approach.
The other major target group is those hospitalised with infectious illnesses.
The results of this meta-analysis confirm the 1987 World Health Organisation recommendation to give vitamin A to children with measles in countries where vitamin A deficiency is a recognised problem — a statement released when only the results of Barclay et al 's study, which were not statistically significant, and of that by Ellison, which was non-randomised, were available, but the sum of current evidence is now too strong to justify further trials in hospitalised measles patients in developing countries.
Furthermore, since Hussey et al showed that hospitalisation was decreased by an average of 4.7 days, supplementing children hospitalised with measles is likely to be not only effective but also cost saving.
Whether this is true in developed countries is yet to be seen: although Ellison's 1932 study in the United Kingdom is consistent with the results in figure 1, it preceded immunisation and antibiotics and hence is not comparable in 1992.
An incompletely resolved puzzle is the mechanism by which vitamin A reduces mortality.
The two major causes of death in children hospitalised with measles and in the community settings were respiratory and diarrhoeal disease.
The overall mortality from diarrhoea was significantly reduced, whereas a reduction in deaths from respiratory disease was seen only in the measles studies.
This reduction in deaths from pneumonia was also noted in Ellison's 1932 study.
Perhaps the most interesting finding, however, was that other causes of death were reduced by about 34%.
This has two possible explanations.
Firstly, the other causes of death may have included a high proportion of respiratory or diarrhoeal deaths that had been misclassified — for example, the cause of death in the trial by West et al was ascertained from a verbal necropsy report.
Secondly, vitamin A may indeed affect other causes of death: the causes reported most frequently were malnutrition, other infectious illness, and convulsions.
Although this result is not significant, it is  within the confidence limits for the mortality from the combined measles studies (see fig 1).
All reported morbidity results showed some reduction in the groups given vitamin A supplements (see fig 3), though the only significant results were the duration of illness in the measles trials and the incidence of respiratory illness in the Thailand trial.
Coutsoudis et al also found fewer and less severe episodes of illness in a six month post discharge follow up of the supplemented group who had had measles.
The Ghanaian trial reported no differences in incidence of 19 of 21 symptoms but found a 38% reduction in hospital admissions for the supplemented group.
The two Indian community studies found no significant effect on incidence or the duration and severity of diarrhoea and pneumonia.
The available trial results are consistent with a shift in the spectrum of illness to being less severe, of shorter duration, and with a greater proportion of subclinical disease.
It would have been useful if the various studies had reported all morbidity results (whether ‘statistically significant’ or not) and in a compatible format.
What applications might these results have in developed countries where vitamin A is plentiful?
It would seem that there are few general applications, but several specific conditions may be helped by higher doses of vitamin A. Firstly, though, it should be remembered that retinol is toxic in large doses, and the public should not be given the impression that it is a safe supplement for children who may have recurrent respiratory or diarrhoeal illness.
The only evidence to date regarding respiratory illness is that of Pinnock et al (see table I), which was equivocal.
In the case of very low birthweight infants, two trials showed reductions in incidence of bronchopulmonary dysplasia and respiratory infection and of duration of ventilation and stay in intensive care.
Other groups with possible relative deficiency would be those with malabsorption and acute or chronic infection.
Hence, in developed countries research should focus on high risk groups, such as Australian aboriginal children (who have considerable respiratory tract illness) and other disadvantaged groups, those with severe infectious illnesses such as measles, and those with chronic illnesses such as cystic fibrosis and AIDs that are prone to infections.
Finally, we note that all of the reviewed studies used retinol at doses near the recommended daily intake as the source of vitamin A. Therefore the results should not be confused with the many cancer prevention trials that are investigating the effects of high dose —β carotene.
Clinical symptoms associated with seroconversion for HIV-1 among misusers of intravenous drugs: comparison with homosexual seroconverters and infected and non-infected intravenous drug misusers
Abstract
Objective —
To study the clinical symptoms associated with seroconversion for HIV-1 among misusers of intravenous drugs.
Design —
Case-control study in cohorts of drug misusers and homosexual men.
Setting —
Outpatient clinic, Municipal Health Service, Amsterdam.
Subjects —
Misusers of intravenous drugs from our prospective cohort who seroconverted for HIV.
Controls were drug users positive for HIV, drug users negative for HIV, and homosexual men who had seroconverted.
Results —
Five out of 18 (28%) drug users were admitted to hospital with bacterial pneumonia in the four to six months between their last visit at which they were HIV negative and their first visit when they were HIV positive.
For comparison none of the 27 homosexual men who seroconverted for HIV, three out of 177 (2%) drug users negative for HIV, and 10 out of 112 (9%) drug users positive for HIV reported bacterial pneumonia.
One out of the 18 drug users who seroconverted suffered from oesophageal candidiasis at the time of seroconversion.
Other clinical symptoms did not differ between drug users who seroconverted and those who remained negative for HIV, probably due to the high background morbidity among the drug users.
Conclusions —
Seroconversion to HIV-1 among intravenous drug misusers is associated with bacterial pneumonia.
Those drug users with previously negative test results for HIV who are admitted to hospital for bacterial pneumonia should be tested to detect primary infection with HIV-1.
Introduction
Primary infection with HIV-1 may cause an acute viral syndrome with symptoms varying from mild fever, malaise, headaches, and maculopapular rash to more severe symptoms like aseptic meningoencephalitis and even oesophageal candidiasis.
In general symptoms are mild and admission to hospital is necessary in only a few cases.
Most reports describe primary HIV-1 infection among groups other than misusers of intravenous drugs.
This group differs from other groups at risk of infection with HIV, like homosexual men and men with haemophilia, as their lifestyle often causes a high background incidence of disease and disease may be camouflaged by the use of illegal drugs.
The incidence of bacterial infections such as endocarditis and pneumonia is higher regardless of HIV infection than in other groups at risk.
In addition, immunological disturbances due to frequent injecting among drug users negative for HIV have been reported.
We therefore performed a study to see whether the clinical manifestations associated with seroconversion for HIV-1 among intravenous drug misusers differed from those described in other groups at risk.
Subjects and methods
Participants of the follow up study among drug users in Amsterdam have been recruited since December 1985.
All participants were interviewed by using a standard questionnaire and were asked to return for follow up examination every four months.
From April 1989 all participants, positive and negative for HIV, underwent physical examination and, in addition to serological tests, immunological tests were performed routinely at each visit.
As the purpose of the present analysis was to study the clinical and immunological manifestations accompanying seroconversion for HIV-1 antibody among intravenous drug users, we selected only drug users who seroconverted in the period April 1989 to December 1991.
Subjects were eligible when the period between the last test result negative for HIV and the first test result positive for HIV-1 was no longer than six months.
As controls we selected drug users negative and positive for HIV from our cohort and randomly selected one visit of each drug user.
Subjects with follow up visits in the period April 1989 to December 1991 were eligible.
The period between the selected control visit and the previous visit had to be less than six months.
Visits from drug users who were positive for HIV and had AIDS were not eligible.
To compare seroconversion for HIV between drug users and homosexual men an additional control group of seroconverters for HIV-1 was selected from our cohort of homosexual men in Amsterdam, which has been extensively described.
This cohort was started in 1984.
The organisation of the cohort of homosexual men was the same as for the cohort of drug users, and all serological and immunological tests were performed in the same laboratories.
Participants returned for follow up every three to six months.
From this cohort we selected homosexual men who seroconverted in the same study period as the drug users who seroconverted.
Immunological tests were not performed on homosexual men negative for HIV after September 1987.
Therefore immunological data for the homosexual men were not available at the visit preceding seroconversion.
HIV antibodies were detected with enzyme linked immunosorbent assay (ELISA), confirmation was performed with competitive ELISA or western blotting, or both.
Lymphocyte immunophenotyping was performed with a fluorescent activated cell sorter scanner (Becton and Dickinson); whole blood lymphocyte culture is described in detail elsewhere.
Statistical analysis was performed with χ&sup2; and Student's t tests.
When expected cell frequencies were less than 5 Fisher's exact test was used.
In case of non-normal distribution the non-parametric Mann-Whitney U test was performed.
A p value of & <-10.05 was chosen as significant.
Logistic regression models were used to determine which variables significantly and independently could predict bacterial pneumonia and to adjust for potential confounders.
A significance level  of p<-10.05 according to the likelihood ratio statistics was chosen for entry in the model.
(All analyses were performed with the SPSS/PC+ statistical package.)
Results
In the study period 18 intravenous drug misusers who met all criteria seroconverted for HIV-1.
The mean time since their last visit when they were negative for HIV was 129 days.
The visits (referred to as index visits) of 177 drug users negative for HIV-1 and 112 drug users positive for HIV-1 were randomly selected from the cohort to serve as controls.
In the cohort of homosexual men 27 seroconverted in the study period; the mean time since their last visit when they were negative for HIV was 165 days.
Table I shows the general characteristics of the drug users who seroconverted, the drug users positive for HIV, the drug users negative for HIV, and the homosexual men who seroconverted.
Table II compares the clinical symptoms of the 18 drug users who seroconverted, the 177 control drug users who were negative for HIV, and the 27 homosexual men who seroconverted.
Minor symptoms accompanying seroconversion, such as fever, tiredness, pain accompanying swallowing, headaches, and nightsweats, were not reported significantly more often by the drug users who seroconverted compared to controls for HIV.
Skin abscesses due to injecting, however, were reported significantly more often by drug users who seroconverted compared to controls negative for HIV.
Pneumonia was the clinical symptom most strongly associated with seroconversion among drug users.
As described above five out of 18 (28%) injecting drug users who seroconverted reported pneumonia and all were admitted to hospital (table III).
In contrast none of the 27 homosexual men who seroconverted (p=0.007 compared to drug users who seroconverted) and only three of the 177 (2%) of the drug users negative for HIV (p=0.0002 compared to drug users who seroconverted) reported pneumonia in the preceding period.
None of the three drug users negative for HIV with pneumonia was admitted to hospital.
We considered subjects positive for HIV as controls in the comparison of pneumonia because the incidence of pneumonia among drug users positive for HIV was shown to be significantly higher compared to drug users negative for HIV.
Ten out of 112 (9%) controls positive for HIV developed pneumonia of whom three were admitted to hospital (p=0.038 compared to drug users who seroconverted).
To study whether demographic, social, drug related, or immunological factors influenced the development of bacterial pneumonia among drug users, the characteristics of drug users who seroconverted but did or did not develop bacterial pneumonia were compared.
The immunological data of the index visit (first HIV positive visit of the seroconverters), the visit preceding the index visit (four to six months before), and the visit four to six months after the index visit were compared.
No variable was univariately related to a history of bacterial pneumonia among the drug users who seroconverted (data not shown).
The results of the multivariate logistic regression analysis in the total group of intravenous drug misusers (seroconverters and HIV negative and HIV positive subjects) are shown in table IV.
The unadjusted odds ratio of drug users who seroconverted to acquire bacterial pneumonia was 3.92 compared with drug users positive for HIV.
The odds ratio adjusted for sex, age, steady housing, season in which the participant suffered from pneumonia, years since first injection, frequency of injecting, and drug mainly injected (heroin, cocaine, or a mixture of both) was 4.89.
The odds ratio for drug users negative for HIV to acquire bacterial pneumonia compared with those positive for HIV was 0.18 unadjusted and 0.13 adjusted.
Discussion
Our prospective study has shown that serious clinical symptoms associated with seroconversion for HIV among intravenous drug misusers occur often.
Of the 18 drug users who seroconverted, five were admitted to hospital for bacterial pneumonia and one woman was  treated as an outpatient for oesophageal candidiasis.
In total six out of 18 (33%) of the drug users who seroconverted in our cohort had serious manifestations associated with seroconversion.
For comparison, of the 27 homosexual men who seroconverted none had suffered from bacterial pneumonia, nor had any participant been treated for complications of his HIV infection.
Moreover, from an additional 90 homosexual men who seroconverted between the start of the homosexual cohort in October 1984 and the start of the present study in April 1989, none suffered from pneumonia in the six months before seroconversion.
To our knowledge this is the first prospective study that has shown that bacterial pneumonia is associated with HIV seroconversion among intravenous drug misusers.
Self selection among the drug users who seroconverted may have biased our results.
If the subjects with pneumonia were more likely to return for follow up visits than those without this could have resulted in an overestimation of the incidence of bacterial pneumonia among the drug users who seroconverted.
Among the drug users who acted as controls and remained negative for HIV this selection bias would, however, also exist, and therefore the odds ratio for drug users who seroconverted to acquire bacterial pneumonia is likely to be unaffected.
Owing to small numbers, however, the power to detect potential confounding in the relation between pneumonia and seroconversion is limited.
The immunological suppression associated with seroconversion is probably an important factor responsible for the increased susceptibility to bacterial pneumonia among drug users who seroconvert.
Whether those with pneumonia were more immunocompromised than similar subjects without pneumonia is unknown.
We could not detect a significant immunological difference between drug users who seroconverted with bacterial pneumonia and those without, but owing to the long sampling interval the lowest CD4 counts were not necessarily measured.
There may be several reasons why seroconversion seems to be associated with bacterial pneumonia only among intravenous drug users and not among homosexual men.
Historically drug users are known to suffer more often from bacterial infections such as pneumonia, and this observation agrees with our results and those from a previous study of our cohort.
Several factors such as tobacco smoking, heroin smoking, cannabis use, asthma, bronchitis, heavy alcohol use, malnourishment, and bad social conditions resulting in poor housing and crowding are known to predispose to bacterial pneumonia.
Many of these predisposing factors are observed more often among drug users than among homosexual men.
In a previous study we have shown that asymptomatic drug users positive for HIV have a nearly fourfold increased risk of acquiring bacterial pneumonia compared with drug users negative for HIV.
This is in contrast to homosexual men positive for HIV who, in general, suffer from bacterial pneumonia more commonly only in the advanced stages of HIV disease.
Altogether we found a pattern that indicated a moderately increased incidence of pneumonia among drug users negative for HIV caused by predisposing (and possibly immunological) factors.
The strongly increased incidence among drug users who seroconverted is probably due to the temporary immunological depression associated with seroconversion.
The risk of bacterial pneumonia among symptomatic drug users positive for HIV is significantly increased compared to those negative for HIV.
Skin abscesses due to injecting were reported more often by drug users who seroconverted than by controls but the selection of seroconverters who had probably used infected injecting equipment may have biased this result.
We also compared minor clinical symptoms such as tiredness, fever, nightsweats, pain on swallowing, and headaches between drug users who seroconverted and controls.
None of these clinical symptoms, which have been recognised as symptoms of primary HIV infection, differed between drug users who seroconverted and those who were negative for HIV.
Among drug users many of these symptoms may be caused by the drugs used, withdrawal of drugs, or, alternatively, symptoms may be suppressed by the use of drugs.
We conclude that among intravenous drug misusers seroconversion for HIV is significantly associated with bacterial pneumonia.
When a drug user previously negative for HIV is admitted to hospital for bacterial pneumonia serological tests should be performed to detect possible seroconversion for HIV.
Alcohol and cor pulmonale in chronic bronchitis and emphysema
The development of peripheral oedema is a major determinant of a poor prognosis in patients with chronic bronchitis and emphysema and, when unexplained by other medical conditions, is called cor pulmonale.
Chan et al found that raised alcohol consumption was associated with hypercapnic respiratory failure in bronchitis patients.
As oedematous patients with bronchitis tend to be hypercapnic, we have studied the relation between alcohol consumption and peripheral oedema in patients with bronchitis.
Patients, methods, and results
We studied 73 patients (39 women and 34 men with a mean age of 68 (SEM 1) years) with severe chronic bronchitis and emphysema (forced expiratory volume in one second <1.5 l).
None of the patients had liver, renal, or ischaemic heart disease; systemic hypertension; or hypoproteinaemia.
All had been clinically stable for over three weeks.
Lifetime histories of alcohol consumption and smoking were taken from each patient with a reproducible structured interview technique that used key life events (for example, job changes, marriage, childbirth) to help patients remember.
The presence or absence of dependent oedema was noted and records reviewed for previous documentation of oedema.
We measured the patients' arterial blood gas tensions, forced expiratory volume, and forced vital capacity.
Within the year before the interview 27 patients had had pulmonary arterial pressures measured directly and 23 had had total lung capacity (helium dilution) and gas transfer measured.
Unpaired Student's t tests and χ&sup2; tests were used to compare data between groups.
Logistic regression analysis with forward stepwise entry of variables was used to assess the relative contributions of different factors to the development of oedema.
For this analysis, average alcohol consumption was graded into four bands: 0, 1–10, 11–30, and >30 units/week.
Odds ratios were also calculated for the different factors.
The oedematous patients had similar lung function, smoking histories, and plasma protein concentrations to the patients without oedema.
The oedematous patients tended, however, to be more hypoxaemic, with higher arterial P co 2 , and to have higher pulmonary arterial pressure, higher alcohol consumption, and higher body mass index, than those without oedema (table).
The only variables significantly related to the development of peripheral oedema on regression analysis with high alcohol consumption (odds ratio 4.1 (95% confidence interval 1.6 to 7.1), p=0.002), being female (10.2 (1.7 to 59.1), p=0.01), and high body mass index (1.12 (1.00 to 1.25), p=0.04).
There was no significant correlation between arterial P co 2 and alcohol consumption (r <0.14).
Comment
This study shows that alcohol consumption is an important independent predictor of peripheral oedema in patients with chronic bronchitis and emphysema.
How alcohol predisposes to peripheral oedema is not clear, but it is not through hypoproteinaemia.
The patients with oedema tended to have higher pulmonary arterial pressures than those without (p=0.12), and alcohol may possibly intensify the effect of hypoxaemia on the pulmonary circulation to increase arterial pressure.
Alcohol may also contribute to nocturnal hypoxaemia by causing narrowing of the upper airways.
Another possible explanation is that alcohol impairs renal sodium excretion; the resulting sodium retention could contribute to the development of peripheral oedema.
Obesity can increase nocturnal hypoxaemia, and we found a positive relation between body mass index and the presence of oedema.
We also found that peripheral oedema was more common among our female patients, but we did not identify the cause of this effect.
Unlike Chan et al , we found no significant relation between alcohol consumption and arterial P co 2 .
But the alcohol consumption of Chan et al 's hypercapnic group averaged 124 units/week, which is much higher than that of our patients, whose consumption was similar to that of the population in Edinburgh studied by Chick et al .
GENERAL PRACTICE
Fundholding in Northern region: the first year
Abstract
Objective —
To describe the experiences of 10 fundholding practices in the Northern region during 1991–2 and to elicit subjective assessments of the impact of their change in status on practice management and patient care.
Design —
Semistructured interviews were conducted with clinicians and practice managers; other staff in the practices were asked to fill in questionnaires.
Questions were asked about the preparatory year, the impact of fundholding on clinical practice and practice management, perceptions of the costs and benefits of fundholding, and views about the future of the scheme.
Setting —
10 of the 28 first wave fundholding practices in the Northern region, March-July 1992.
Results —
Two interviews were conducted in nine practices and one interview in the tenth practice.
Replies to the questionnaire were received from 35 general practitioners (73%) and 89 (58%) non-medical staff.
Practices sought independence in applying for fundholding status and found the preparatory year challenging and time consuming.
General practitioners thought that the greatest change had occurred in relationships with consultants and the least change in relationships with patients.
Most respondents thought that fundholding had changed the way they worked.
The perceived benefits of fundholding were mentioned more often than the perceived costs.
Conclusions —
The results offer some encouragement to the proponents of fundholding, but more longitudinal studies are needed to evaluate the misgivings of critics of the scheme.
Fundholders are uncertain about their ability to make savings year after year, particularly in an increasingly cost contained environment.
Introduction
The introduction of fundholding in April 1991 was accompanied by a good deal of speculation about its impact on general practice.
Some of this speculation was contained within general commentaries on the NHS reforms as a whole, and other commentators focused their attention on fundholding alone.
Opinion was divided on the potential costs and benefits of the scheme.
Those in favour of fundholding saw it as an opportunity for general practitioners to have a direct impact on access to and quality of secondary care, which they could now purchase on behalf of their patients.
According to this view, significant benefits would accrue to both fundholders and patients.
Fundholding practitioners would be less constrained in their clinical decision making and patients could anticipate more choice and improvements in services.
Opponents of fundholding expressed a range of concerns about the introduction of market principles into health care.
Some thought that fundholding general practitioners had been given a perverse incentive not to spend money on their patients or even to be selective in the types of patient they enrolled on their list.
Behaviour motivated by such economic criteria, it was argued, could only harm the doctor-patient relationship.
It was feared that a scheme which allowed some practices and patients to be ‘winners’ would necessarily result in other practices and patients being ‘losers.’
Later, these fears surfaced in a more public debate about the emergence of a two tier service.
The debate has been conducted on the basis of very little evidence.
The government implemented its reforms without commissioning any evaluative or pilot studies and the experience of similar developments in the American health care system provided only limited comparisons.
To date only two research studies, based on fundholding practices in the south of England, have published any results.
The study reported here aimed to describe the experiences of 10 fundholding practices in the Northern region during the first year of the scheme (1991–2).
We were particularly interested in the practices' subjective assessments of the impact of this change on practice management and patient care.
Methods
A sample of nine practices was randomly selected from a list of fundholders stratified by the nine family health services authority areas in the Northern region.
In one case two practices had joined the scheme as joint budget holders, making a total of 10 in the sample.
data collection
Three forms of data collection were used.
They were piloted in another fundholding practice.
Firstly, two semistructured interviews were held at each practice with the lead clinician involved in fundholding and the practice manager between February and July 1992.
Each audiotaped session lasted between one to one and a half hours.
The first interview asked about becoming a fundholder; budget and contractual arrangements for 1991–2; impact on clinical practice; and impact on practice organisation and management.
The second interview asked about the situation at year end 1991–2; the budget for 1992–3; contracts for 1992–3; management and organisational changes; patient services; information handling; and fundholding in the next three to four years.
Secondly, questionnaires were developed for self completion by other clinicians and non-medical staff in the practice.
These aimed to elicit individual views about fundholding and details of the impact that it had had on their roles.
Clinicians were asked about perceived costs and benefits of fundholding; involvement in fundholding; the impact of fundholding on work and on relationships with consultants and others; use of information technology; and feedback from patients.
Non-medical staff were asked about their involvement in the decision to apply for fundholding  status; perceived costs and benefits; and the impact of fundholding on work, use of computers, and patients.
The questionnaires were designed so that responses would be given in one of three ways: a yes-no answer; a comment; or a choice from either three or five possible responses.
Thirdly, a schedule was distributed to practice managers, asking for information about the practice.
This information was used to construct a profile of each practice.
data analysis
The audiotaped interviews were transcribed with a word processing package.
Alongside this a freeform database enabled the creation of a set of topic headings under which summarised answers from each transcript could be listed.
Within each topic heading the listed answers were then further coded to determine groupings of like answers.
This computer assisted process of summarising and coding made a large amount of semi-structured material amenable to qualitative analysis.
Information from the structured questionnaire was entered on to computer spreadsheets.
Simple statistical analysis of the spreadsheets resulted in totals and percentages of responses to each question for each practice.
Comments and responses to the practice profile questionnaire were analysed with a more structured database program.
Results
In each of the 10 practices selected for the study the practice manager or lead fundholding clinician, or both, agreed to be interviewed.
In nine practices two interviews were conducted: the first, in March or April 1992, concentrated on a review of progress to date; the second, in June or July 1992, concentrated on plans for the coming year.
In the same nine practices general practitioners and non-medical staff agreed to complete questionnaires.
Thirty five replies (73% of those distributed) were received from general practitioners and 89 (58% of those distributed) from non-medical staff.
preparation for fundholding
The first question in each of the opening interviews was why the practice had decided to apply for fundholding status.
The most common reply was that the initiative offered an opportunity to be independent.
This was expressed in terms of freedom to do certain things (refer to hospitals of their choice; spend money on new staff and equipment) and freedom from externally determined policies and procedures (for example, contracts set by the district health authority).
For some, the scheme matched their self perception as being ‘progressive’ and ‘innovatory’practices.
There was a high level of involvement in the decision to apply for fundholding status (table I).
But involvement of general practitioners (other than the lead clinician) in other aspects of fundholding (such as setting and monitoring contracts) was at a lower level.
There are indications, however, that their level of involvement was greater in year 2 (1992–3) than year 1 (1991–2).
The day to day business of preparing for fundholding had been the task of the practice or fund manager(s) together with the lead clinician.
Five of the nine practices had been collecting the types of data required by the regional health authority for several years, and all of the practices had computer systems in operation before the preparation period.
Even so, most of the practices said that the data collection and processes of budget and contract setting were extremely demanding.
This was attributed mainly to the need to liaise with a large number of external agencies and bodies either by telephone or in face to face meetings.
impact of fundholding
Tables II and III show general practitioners' and non-medical staff's assessment of the impact of fundholding on their relationships and roles.
Table IV shows how the practices ended the first year in financial terms, and Table V shows how general practitioners and non-medical staff assessed the costs and benefits of fundholding in their practices.
General practitioners were asked to assess the extent to which they thought relationships with a number of other persons and organisations had changed as a result of acquiring fundholding status.
The greatest amount of change was thought to have occurred in relationships with consultants: 13 (37%) said that this had changed ‘a great deal’ or ‘quite a lot .’
In the interviews six practices cited changes for the better, which included overcoming difficulties in getting terminations; improving discharge summaries; discussions about a protocol for infertility referrals, and, in a different practice, for back referrals.
Apart from these particular initiatives there was a general enthusiasm for the way in which fundholding had opened up communication with hospital colleagues.
One doctor summed up the change: ‘We've actually seen them for once; they've been out here visiting just to make sure that we are aware of their potential services.’
The least amount of change was thought to have occurred in relationships with patients: just one doctor thought that this had changed ‘quite a lot.’
In the interviews most of the lead clinicians said that day to day relationships with patients had changed very little.
Some patients had voiced initial apprehension about their continuing to receive medication, but most were thought to be neutral about fundholding.
When asked how they thought patients had responded to the practice becoming a fundholder, 25 (71%) general practitioners replied ‘neutrally.’
Practices, however, had not been proactive in advertising their change in  status to patients: only one of them had a notice to the effect in the surgery.
Within the practices, fundholding had an appreciable impact on management and organisation.
Five of the practices had formed a fundholding team consisting of the practice manager, lead clinician, and one or two secretarial staff.
Only one practice had employed a person (part time) with the title ‘financial advisor,’ and this had caused friction with the existing practice manager.
Three practices had employed an extra member of staff either during the first year or at the beginning of the second to work specifically on fundholding duties or to release the practice manager to do this.
Most respondents thought that fundholding had in some respect changed the way they worked (table III).
General practitioners were asked to distinguish between changes from a medical and administrative point of view.
More of them thought that changes had occurred in administration, with more paperwork and more meetings being the most frequently mentioned aspects.
Almost half (49%) felt that fundholding had affected their job from a medical point of view.
The most frequently mentioned aspect of change was that the consciousness of the cost of treatment now had a bearing on decision making.
‘It's not stopped me referring patients,’ said one doctor, ‘but I do think about where I send them.’
More of the non-medical staff said that fundholding had changed their job, and when asked to elaborate said that they had been involved in a lot of extra work.
One example of this was the frequent necessity to enter data on transactions twice because of the lack of integration of fundholding and clinical software systems.
It is not surprising, therefore, that in the interviews several practice managers reported increased tension and stress among staff as a result of the changes.
Table IV shows that seven of the 10 practices made overall savings in 1991–2 and that three practices overspent their budgets.
Most of the practices making savings had done so on their hospital budgets, and this was attributed to a combination of setting favourable contracts, making fewer referrals, and doing more minor surgery in house.
At the time of the second interviews (June or July 1992) it was not clear whether the regional health authority would treat all of the savings as ‘genuine.’
Only two of the 10 practices were overspent on their hospital budgets, and these two practices also had an overall overspend.
They both accounted for this outcome in terms of the rigidity of the block contracts they had been recommended to make by the regional health authority.
The third practice had overspent because it had taken over a smaller practice after the preparatory work on budget setting had been completed, and it had not been able to make an accurate estimate of prescribing costs for the 1300 patients involved.
To summarise the impact of fundholding, general practitioners and non-medical staff were asked an open ended question about the costs and benefits of the scheme in their practice.
The results are listed in Table V in descending order of frequency with which the items were mentioned.
Both general practitioners and non-medical staff mentioned benefits more often than costs.
The main benefit for general practitioners was improved practice management and computer systems (10 mentions); for non-medical staff it was swifter referrals (11 mentions).
All respondents saw increased time and effort in administration, or simply ‘extra work,’ as the main cost of fundholding.
Discussion
In this study attitudinal data were collected by means of interviews and questionnaires from 10 first wave fundholding practices in the Northern Regional Health Authority.
We found that doctors and non-medical staff thought that fundholding had altered their roles and relationships with each other and with agencies outside of the practices.
Seven of the 10 practices had made an overall saving on their first year's budget, and everyone felt that the benefits of being in the scheme outweighed the costs.
These findings offer some encouragement to the proponents of the fundholding initiative.
If fundholders were to be a lever for change in the reorganised National Health Service there is evidence from this study that they are achieving this objective.
Each of the practices reported some improvement in the provision of hospital care or practice services that they felt could be attributed to the acquisition of fundholding status.
All of the practices were looking forward to the second year in which the ‘steady state’ of year 1 would give way to even greater freedom and opportunity.
Unfortunately these findings cannot support or reject the reservations held by critics of the scheme.
The main misgivings relate to the perverse incentives fundholding was thought to create: not to refer patients; to shift cases from elective to emergency care; to select or deselect patients according to their cost liability.
Such matters were occasionally raised in the interviews and always received vigorous denials that such considerations would ever influence decision making.
Answers to these issues, however, require more longitudinal research designs and more rigorous forms of data collection than were possible in this  study.
There is clearly a need for more evaluative studies in this area.
Although all of the practices were looking forward to continued participation in the scheme, there were some concerns about the future.
A variety of difficulties were foreseen about the ability of fundholders to continue making savings.
Some realized that their first year savings were intrinsically ‘once only’: ‘We can't go generic every year,’ said one lead clinician.
One doctor raised the issue of random variations in service needs addressed in a paper by Crump et al .
As he put it: ‘You could blow your budget by 15% without consciously doing anything.’
For all these reasons the future was thought to be uncertain.
Some practices were therefore thinking again about employing additional professionals out of budget surpluses.
Such arrangements, it was argued, required a more reliable source of income than they could currently envisage, and a clearer indication from the region (or the Department of Health) concerning policy about the retention and spending of savings.
A related concern stemmed from predictions about the direction of health care funding.
Given the demands of the United Kingdom's aging population and the tendency of health care costs to rise, it seemed certain to some doctors that governments' policies in this area would contain an appreciable element of cost containment.
A primary health care sector with half or more of the general practitioners as fully fledged budget managers would certainly facilitate the implementation of such a strategy, as has been pointed out.
It would then be a classic example of double talk if an initiative that was sold to general practitioners as a vehicle for enhancing freedom turned out to be a means of constraining it.
EDUCATION & DEBATE
‘Not clinically indicated’: patients' interests or resource allocation?
The decision that a particular intervention is not clinically indicated may conceal two quite different ethical assumptions.
The first assumption is that the intervention is not of overall benefit to this patient.
The second is that limited resources should not be used for this patient.
These issues are discussed with reference to cardiac surgery in elderly patients with reference to the main theories of allocation: QALYs, needs theories, the sanctity of life theory, the lottery theory, and market forces.
A report from the Royal College of Physicians recommended that elderly patients should have better access to cardiological services, including coronary angioplasty and cardiac surgery.
It also noted, however, that the facilities presently available are inadequate to provide for the patients of all ages who could benefit from these procedures.
It is clear that some elderly patients can benefit from surgery that may previously have been denied on the grounds of age alone.
Patients in their 80s now constitute 3.5% of those undergoing coronary artery bypass grafting at the Mayo Clinic; 79% of 115 survivors were free of angina after a mean period of 29 months.
The risks of surgery are undoubtedly higher in patients of this age — in two series the hospital mortality was around 10% and the incidence of postoperative stroke was 4%.
An increased use of angioplasty may limit the number of elderly patients with coronary disease who need bypass grafting for refactory angina, but there is no effective alternative to surgery for the large number of patients in their 70s and 80s with symptomatic aortic stenosis.
When stenosis is the cause of severe symptoms, the prognosis without valve replacement is worse than that of many cancers, with a three year mortality around 75% Balloon valvoplasty offers only short term palliation.
In published series of patients aged 80 years and over undergoing aortic valve replacement the operative mortality was around 15% (compared with around 4% for patients under 70).
By three to five years after surgery the actuarial survival rate exceeded that of a control population.This enhanced survival may reflect the fact that patients referred for surgery were highly selected, and an important limitation of these data is that the selection criteria were not explicit.
Two different points to consider
For many elderly patients it will be decided that cardiac surgery is not clinically indicated.
But the use of the phrase ‘not clinically indicated’ often conceals and confounds two quite different points.
The first point is that the operation is not of overall benefit to the patient — for example, the risk of death during the operation outweighs the likely benefit of the operation for the patient.
The second is that it is not the right allocation of available resources to use them for this patient.
Both these statements have important ethical dimensions, and they require separate analysis.
In general, where the patient's welfare is the crucial issue the patient should judge.
This is a central value in a liberal society.
The role of the doctor is to give as accurate and helpful information as is possible to enable the patient to come to a decision.
Even though a patient aged 80 years will usually face a higher operative mortality and a shorter expectation of life than a patient aged 40 years, it does not follow that surgery is not worthwhile for the 80 year old.
Paradoxically, the older person might be willing to take a higher risk of death during surgery than a younger person.
Indeed surgery, offering, in simplified terms, either a quick death or good health, may be particularly attractive to the old.
Some people aged 80 would not consider major surgery worthwhile because their life is near to close, but others would judge quite differently.
If our interest is what is of benefit to the individual patient we must take these differences into account, and in general that means allowing the patient to decide.
Carrying out surgery on an old person is the wrong allocation of resources
Old people might be denied surgery on the grounds that the available resources are better spent on younger people.
But what ethical principles should guide the allocation of resources?
We shall briefly discuss some of the main theories of allocation that have been suggested and examine their implications for elderly people with aortic stenosis.
Qalys
Quality adjusted life years (QALYs) form the core of the most thoroughly articulated theory of allocation in health care.
On this theory one year of healthy life is taken to be worth 1.
A year of unhealthy life is considered to be worth less than 1: the value is lower if the quality of life of the unhealthy person is worse.
A beneficial health care activity is one that generates a positive number of QALYs, and an efficient health care activity is one for which the cost per QALY is low.
QALYs are, in effect, units of welfare.
A given budget will buy the maximum amount of QALYs if resources are spent on those aspects of care with the least cost per QALY.
In this way welfare is maximised.
The application of QALY theory would result in old  people having less chance of cardiac surgery than younger people for two main reasons: firstly, the operation is likely to have a higher mortality and morbidity for older people; secondly, older people have in general a shorter life expectancy and therefore fewer life years to gain from the operation.
The main criticisms of QALYs focus on two central points: firstly, that QALYs are unfair because they do not take into account who gains the QALYs; secondly, that ‘welfare’ is not the only value to be put into the equation.
Critics have argued that QALY theory is attractive as long as we are considering one person who is weighing up the likely outcomes with different treatments.
It is eminently reasonable for an individual to choose the treatment that is likely to generate the most QALYs.
But applying QALYs to the allocation of resources is quite different because it involves a choice between the welfares of different people.
Thus, for example, those with mental handicap would obtain fewer resources because one year of mentally handicapped life would rate as lower in quality than one year of normal life.
Similarly, people with physical handicap affecting the quality of life would have less claim on resources.
This would result in those already unfortunate in having one handicap then being less entitled to further medical care.
The fact that old people would lose out with QALY theory has been given as a further example of injustice.
The reverse has, however, also been argued: that QALY theory does not discriminate enough against old people.
This argument, which has been called ‘the fair innings argument,’ was summarised by Lockwood: ‘To treat the older person, letting the younger person die, would thus be inherently inequitable in terms of life lived: the younger person would get no more years than the relatively few he has already had, whereas the older person, who has already had more than the younger person, will get several years more.’
It follows from this that younger people will normally take preference over older people in the allocation of resources that would postpone death.
Is this fair?
Do we not all face the same loss, namely death?
And do we not all have the same right to be saved?
This raises the fundamental issue of how, ultimately, the value of life is to be determined.
Harris has argued that the value of life can only sensibly be taken to be that value that those alive place on their lives.
To avoid some of these difficulties with QALYs, Nord has recently suggested the ‘saved young life equivalent’(SAVE).
Instead of requiring a figure to be placed on quality of life, the SAVE approach requires a direct comparison to be made between the value of a given intervention (for example, valve replacement in an 80 year old with severe aortic stenosis) and saving the life of a young person, restoring him or her to full health.
Any particular evaluator is free, therefore, to rate a treatment for an 80 year old as equally valuable as the equivalent treatment in a 20 year old, or as less valuable, or as more valuable.
The SAVE approach gives a way of obtaining the views of a population sample.
However, it does not help individual evaluators in deciding on what basis to make their evaluations.
Who will be allocated to cardiac surgery and the chance of a longer life — and who will decide?
needs theories
Needs theories arise from the second major criticism of QALY theory: that ‘welfare’ is not the only value.
A distinction is drawn between needs and benefits.
In allocating resources, needs should be met before benefits.
This theory requires an account to be given of this central distinction.
There may be clear cut cases, but there is likely to be a large area where it is unclear whether something is a need or a benefit.
Furthermore, the theory is unlikely to be comprehensive.
If the total resources are insufficient to meet all needs then how does one choose the needs that should take precedence?
And if there are sufficient resources to meet all needs how does one choose which benefits should be funded?
It is unclear whether aortic valve replacement, for example, should be classified as a need or a benefit.
If replacement is a need it will in general be such for old and young alike, and similarly if it is classified as a benefit.
Thus needs theories are likely to share resources equally between young and old.
If aortic valve replacement were classified as a need and not simply a benefit, then needs theories, if implemented, might lead to an increased total budget for such surgery to meet the needs of old people with aortic stenosis.
‘sanctity of life’ theory
This theory states that, above all, life must be saved.
It is a type of needs theory in which the most important need is to save life.
It suffers therefore from the same problems as all needs theories.
A further difficulty is that it would apparently require us to put enormous resources towards trivial increases in length of life.
Indeed, the whole idea of ‘saving life’ is rejected by some philosophers and replaced by that of ‘postponing death.’
It does not seem, in general, valuable to postpone death by a few hours at enormous cost.
In the context of valve replacement in old people, the sanctity of life theory would give high priority to a person of any age who is in imminent danger of dying.
In other situations it gives no guidance.
the lottery theory
The lottery theory is a reaction to the perceived injustices of QALY theory.
Where QALY theory concentrates on welfare and ignores justice, the lottery theory concentrates on justice and ignores welfare.
It originates from considering, for example, the situation where an older and a younger person would benefit equally from treatment but resources allow only one to be treated.
Rejecting the QALY calculation, Harris suggests that since there is no rational way of choosing between the two people the way closest to being just is to choose randomly.
Hence the lottery.
Thus, an 80 year old and a 40 year old should have an equal chance of being offered cardiac surgery in equivalent circumstances.
Many people believe that a lottery is no more just than any other way of choosing — perhaps less so than many other ways.
But even if it is just, it does not seem to be comprehensive.
It may give a method for allocating resources in a situation like that described, but it is quite unclear how it could be used to allocate resources in general.
market forces
Market forces are part of a vision of a health care system in which individuals buy the care they can afford.
With this view, if the 80 year old or the 40 year old can pay for the surgery then it is offered.
Otherwise it is not available.
This way of allocating resources faces the problems of justice in the same way as the QALY view, but immensely increased.
In effect the distribution and quality of health care will be entirely on the basis of personal wealth.
Conclusion
Doctors may feel that in many cases surgery is not clinically indicated.
The word ‘clinically’ here can be dangerously misleading, for the decision being made has a major ethical component.
It is as important to clarify the ethical judgments being made as it is to understand the basis for technical decisions.
Public health medicine and purchasing health care
Public health medicine is a goal driven medical specialty.
Although the tasks of public health doctors are always changing, their goal remains firm: improving the health of the population.
As the route to this goal is neither obvious nor agreed and differs between places and eras continuous debate is essential and occasional heresy welcome.
Thus Whitty and Jones argued recently that public health medicine embraces the purchasing role at its peril.
They achieved their main aim — debate — but I now examine whether their analysis was correct.
Health care and health
Underpinning Whitty and Jones's argument was the belief that health care only minimally influences health.
This owes much to McKeown and Lowe who suggested that socioeconomic and environmental circumstances are the prime determinants of longevity.
If the goals of public health medicine are health promotion and the prevention of ill health, warn Whitty and Jones, then the specialty is in danger of veering off course in pursuit of the purchasing function, which ‘is likely to be ineffective in improving the population's health and may even be in direct conflict with this role.’
Whitty and Jones's analysis stands or falls on the meaning of health, whether health services have an impact on health, and on the central functions of public health medicine.
I examine these variables below.
meaning of ‘health’
The meaning of the term health is not self evident.
Health is usually described as either the absence of disease or infirmity, the efficient functioning of human beings, or a state of wellbeing.
The World Health Organisation's definition that health is a state of complete physical, mental, and social wellbeing and not merely the absence of disease or infirmity redresses the medically biased emphasis on disease or infirmity.
But disease and infirmity cannot be ignored.
impact of health services
The NHS, through its preventive, curative, and rehabilitative services, reduces premature death and disability and improves people's ability to function.
Health services enhance wellbeing by providing care, reducing distress and pain, and issuing a diagnosis or prognosis which may provide reassurance or comfort.
The benefits of health services are not well quantified and may never be quantified, particularly in the face of scientific, technological, and organisational change.
Absence of evidence is not synonymous with evidence of ineffectiveness.
Compared with social, economic, lifestyle, and environmental factors health services have a limited impact on life expectancy and health status.
Nevertheless, the findings of McKeown and Lowe should not be interpreted as denying the role of health services in improving health.
Instead, they should be taken as emphasising that the determinants of health — particularly as they affect life expectancy and the incidence of disease — are mainly environmental.
McKeown and Lowe arranged the main influences responsible for falling mortality in descending order of importance as a rising standard of living, improved hygiene, and specific preventive and therapeutic measures.
McKeown's analysis of life expectancy and (mainly infectious) diseases in the eighteenth, nineteenth, and early twentieth centuries requires updating.
Szreter's counterargument — that the fall in mortality in the nineteenth century was deeply influenced by ‘human agency in the form of politically negotiated expansion  of preventive health provisions and services, rather than the impersonal invisible hand of rising living standards’— may be even more pertinent now.
central functions of public health medicine
Public health medicine takes responsibility on behalf of the medical profession for many of the medical functions that cannot be discharged at the bedside or in the consulting room.
These include policy development; analysing the causes, control, and prevention of disease; promoting the health of communities; and developing frameworks for effective and efficient health services.
Public health doctors are not, however, the sole custodians of these responsibilities; arguably, the prime custodians are politicians.
Other health care professionals and scientists working in the health sector also have an important role.
But it is the specific duty of public health doctors to ensure that the entire medical profession and those whom medical practitioners work with and seek or need to influence (including health authorities) are working towards improving the public health.
Discharging these responsibilities requires public health doctors to work with both purchasers and providers.
Cochrane's criticism of the impact of health care has had a lasting influence and is often used to undermine health services.
He emphasised the ineffectiveness of many health care activities and the importance of evaluation to determine which of them benefited health.
As the first president of the faculty of community medicine (now public health medicine) his view on the role of the specialty remains pertinent — with ‘specialists equally skilled in administration and epidemiology and with a broad view of social medicine and its research’ we should have the chance of making the NHS more effective and efficient.
Public health doctors cannot shrug off their recent or distant legacy of a central role in health care.
Compared with lifestyle and environment health services have only limited impact on life expectancy and health
achieving public health goals
In this light the interest of public health doctors in the purchasing function is clear.
It gives them a new opportunity to refocus priorities for improving the public health.
When we are spending about 6% of our gross national product on health services we have a right to expect improvements in the public health from them, both directly and through the influence of health service staff on health policy.
If improvements do not occur health care policies need changing.
Public health doctors are more likely to influence public health if they collaborate with the million or so people employed in the health service.
They should ally themselves with health service workers (including managers) and inspire them to help analyse the determinants of good and bad health, assess health needs, promote health, and evaluate services, thereby achieving public health goals, albeit indirectly.
When a million health care workers are arguing that social, economic, and lifestyle factors underpin health and that health care plays a lesser, though important part their arguments may influence politicians and the public.
Then we may see a reversal of the policies whereby the deepest roots of public health misery are often ignored.
purchasing function as lever for change
Whitty and Jones's gloomy predictions about the future of public health medicine in an era of purchasing are premature.
Much more debate is required.
Potentially, the purchasing function provides a powerful lever for change.
If applied with wisdom and science the purchasing of health care could speed the evolution of those effective and equitable services that improve health.
Purchasing will also clarify the necessity of and priorities for health services research.
Two dangers of the purchaser-provider split are that collaboration among health care professionals may be undermined and the provider aspects of the public health role may become removed from the purchasing role.
Doctors trained in epidemiology, statistics, and the social and management sciences are needed to help manage the implementation of purchasing policy.
For public health doctors to withdraw from or refuse to give support to the task at this critical moment would greatly endanger the specialty, as people without the necessary combination of medical and public health skills would attempt the job.
The challenge is to influence purchasing policy to achieve better public health.
It is in the nature of a goal oriented specialty to take on new tasks, which has often been necessary in the past.
The NHS reforms and The Health of the Nation provide an unprecedented opportunity to shift health services into an explicit health improvement mode, and public health medicine must guide the change.
Whitty and Jones fear that traditional public health tasks may be ignored in this process.
Adaptation to a changing environment may be necessary as before to achieve traditional goals.
Toxic shock syndrome after elective minor dermatological surgery
Consider toxic shock syndrome when severe diarrhoea and shock develop hours after even minor surgery
The toxic shock syndrome has been commonly associated with menstruation since the definition of the condition in 1980 in the United States.
Almost half of the reported cases seem to be associated with menstruation, but other associations include surgical incisions, burns, nasal packing, abscesses, a postinfluenza syndrome, and parturition.
There is only one reported case associated with minor dermatological surgery and that was from the United States in 1987.
We report a fatal case of toxic shock syndrome in a 14 year old girl associated with minor day case dermatological surgery.
Case report
A 14 year old previously healthy white girl was admitted to the children's ward with an 18 hour history of severe watery diarrhoea, lower abdominal pain, myalgia, and headache.
She had become very ill rapidly.
Twenty four hours before admission she had had a transitional mole removed from her back under local anaesthesia in the outpatient department.
The elective removal had been advised as the mole had been bleeding.
There was no sign of infection in the wound.
The diagnosis was complicated as four days earlier a 14 year old boy from the same school had collapsed and died after a short illness of severe diarrhoea, which proved to be due to a fatal Salmonella enteritidis infection.
On admission she was extremely ill with a high fever, 39.2°C, peripheral cyanosis, and an unrecordable blood pressure.
At that stage the abdomen was soft.
Resuscitation was started with crystalloid fluid, but it proved extremely difficult to raise the blood pressure to normal.
She remained feverish over the next 14 hours with cold peripheries and an arterial blood pressure of 90/50 mm Hg.
By 14 hours after admission the platelet count had fallen from 209 to 134×10/l, the blood urea concentration had risen to 16.2 mmol/l, and the creatinine concentration from 392 to 418 µmol/l.
There was an increase in prothrombin time to 19 seconds and a measurable level of fibrin-fibrinogen degradation products (4 mg/l).
The patient was moved to the intensive care unit.
A rash then became visible over the whole of the patient's body and extending on to her limbs; this was an erythema and not a purpura.
She became progressively more acidotic in spite of continued resuscitation with high volumes of intravenous fluid.
Twenty four hours after admission toxic shock syndrome was finally diagnosed.
She did not regularly use tampons and was not menstruating at the time of admission.
Detailed examination established that there was no forgotten tampon.
In the intensive care unit the patient was managed with central venous pressure monitoring and direct arterial pressure monitoring.
In view of her continuously rising urea and creatinine concentrations and persistent oliguria, plans were made for haemoperfusion.
She then suffered a cardiac arrest 32 hours after admission.
Resuscitation restored the circulation, but the patient required artificial ventilation and inotropic support thereafter.
Peritoneal dialysis was started.
The diagnosis seemed confirmed three days later when desquamation of the rash became evident.
Biochemical investigations continued to show characteristic hypocalcaemia of 2.18 mmol/l.
Her clinical course was very difficult with general deterioration after the cardiac arrest.
She became jaundiced and four days after admission developed adult respiratory distress syndrome with poor pulmonary compliance and a high fractional inspired oxygen requirement.
The wound from the dermatological surgery was debrided on the fourth day.
Even though the wound appeared to be completely dry, debridement revealed some deep necrotic tissue from which Staphylococcus aureus was grown (later confirmed as toxic shock syndrome toxin type I producing organism).
Initial antibiotic treatment was with chloramphenicol and flucloxacillin, but subsequent sensitivity tests showed that the organism was relatively insensitive to flucloxacillin, so clindamycin was added.
On day 17 the sudden onset of acute right heart failure, a right atrial gallop, and right heart strain on the electrocardiogram indicated a pulmonary embolus.
Anticoagulant therapy was started with heparin.
By day 18 her condition was further deteriorating.
She had well established adult respiratory distress syndrome, requiring artificial ventilation with 90% fractional inspired oxygen.
She required continuous inotropic support and was now developing interstitial emphysema.
We attempted to transfer the patient for extracorporeal membrane oxygenation, but she did not survive the journey.
Comment
This patient fulfilled the criteria for toxic shock syndrome.
She had multiple organ system failure (respiratory failure, renal failure, and jaundice), severe diarrhoea, persisting shock, disseminated intravascular coagulation, and a typical rash followed by desquamation.
She also had typical hypocalcaemia.
The expected organism was grown from the original site of minor surgery on her back and was confirmed as a type of S aureus producing toxic shock syndrome toxin I. Isolation of such a toxin is diagnostically helpful, but there is substantial evidence that other staphylococcal enterotoxins contribute to a high mortality.
Despite the large numbers of cases reported in the American literature, half of them associated with menstruation, the original description of Todd et al in 1978 was of seven patients all aged 8–17 years and none associated with menstruation.
There are about 20–40 confirmed cases of toxic shock syndrome a year in the United Kingdom, not all of them associated with menstruation, and with three or four fatal cases.
Although attention has been focused on the risks of tampon use, in the United Kingdom there appears to be an equally important incidence of toxic shock syndrome associated with surgical procedures.
The risks of this condition after elective minor surgery under local anaesthesia have probably not been appreciated.
Although a rare complication of minor surgery, the toxic shock syndrome may be diagnosed by the classical presentation of severe diarrhoea, rapid progression to a shock-like state, and development  of a fine macular rash within hours of the surgical procedure.
Prompt antibiotic treatment, aggressive resuscitation, and debridement of the surgical wound, no matter how clean it may appear on the surface, is essential for successful management.
The absence of accurate figures points to a need for this condition to be notifiable, to allow its epidemiology to be defined.
LETTERS
Counselling in general practice
Editor ,— Mike Pringle and John Laverty's editorial on counselling in general practice fails to distinguish between counselling skills and a counselling service, does not mention patient satisfaction, and omits any reference to the strategy discussed in The Health of the Nation .
An effective general practitioner must have counselling skills to elicit important diagnostic information and manage consultations appropriately.
This, however, is quite distinct from a counselling service, which would seek to offer protected time in which the counsellor and client could focus on a psycho-social problem.
Time is the key point here, and though it would not exclude interested general practitioners from offering such a service, the present structure of British general practice (with comprehensive medical responsibilities for about 1900 people) militates against this.
One of the principal findings of an evaluative study of the introduction of a practice counsellor into two general practices in Cambridge was that professional and patient satisfaction with the new service was high.
The service offered short term focused counselling and had established referral protocols.
General practitioners were not just avoiding ‘heartsink’ patients, as the editorial suggests, but recognised the mutual benefit of bringing new skills and knowledge into the practice and extending the range of options within their primary care team.
Patients who used the service seemed satisfied, and in a self administered questionnaire 72 of the 85 who responded (87% response rate) reported finding the service helpful or very helpful.
This professional experience partly explains the growth of counselling services in primary care that Sibbald et al 's survey shows.
Interestingly, hospital maintenance organisations in the United States and general practice fundholders in Britain have also developed counselling services despite apparent costs.
The Health of the Nation identifies mental health as a key area, and a recent publication by the NHS Management Executive indicates that family health services authorities should, in their corporate contracts, work to secure a comprehensive range of local mental health services, monitor prescribing of psychotropic drugs, and help general practices to gain a good knowledge of mental health services.
It also mentions encouraging practices to offer services aimed at the primary prevention of mental illness (for example, counselling services, self help groups, and stress and anxiety management groups).
Though I acknowledge the need for caution in the development of counselling services and for more scientific evidence for the effectiveness of different types of counselling, this should not conflict with taking the first steps.
Editor ,— There are problems associated with employing counsellors in primary health care teams.
The editorial and paper addressing the apparently haphazard appointment of counsellors highlight the need for regulation.Much confusion surrounds the training, accreditation, and supervision of counsellors.
This is compounded in the editorial, which refers to both the Rugby conference of the British Association for Counselling and the British Psychological Society's recognition of ‘chartered psychotherapists.’
Counsellors are indeed accredited by the British Association for Counselling, but such accreditation has nothing to do with the Rugby conference.
The Rugby psychotherapy conference, now called the United Kingdom Standing Conference for Psychotherapy, represents most organisations of practitioners of psychotherapy in Britain.
The essential aims of the standing conference are to protect the public by promoting appropriate standards for training, research, education, and the practice of psychotherapy and to disseminate information.
The term ‘chartered psychotherapists’ is incorrect.
The British Psychological Society advises: ‘If a general practice wishes to appoint a counselling psychologist, who has specific competence in the psychology of counselling we recommend that only chartered psychologists with appropriate experience of counselling be appointed.’
Editor ,— We find the inaccuracies and lack of logic in Mike Pringle and John Laverty's editorial on employing counsellors in general practice disturbing.
The authors argue that ‘attempts to evaluate effectiveness have shown little or no benefit,’ although among the references they quote are three studies that did not entail counselling in the accepted sense of the term at all, two of which produced distinctly positive results.
Most of the remaining papers dealt with therapy for single problems (smoking and obesity), the results of which can hardly be generalised.
The plea for rigour in selecting counsellors accords oddly with the conclusion that no benefit has been shown from the services that they provide.
If no benefit accrues what is the evidence for the value of training?
Even more dubious is the assertion that ‘they should concentrate on non-directive counselling, being careful to avoid dictating solutions.’
What is the evidence for this?
The two studies quoted giving positive results together with another positive major study not quoted were all of cognitive-behavioural therapy, in which a collaborative directive approach is encouraged.
It is difficult to find scientific evidence of the value of using the Rogerian client oriented counselling method, which is widely advocated, and on this we share the authors' doubts.
On the one hand, cognitive-behavioural interventions with a high turnover and short contact have yielded promising results in the few studies in which they have been attempted.
Individual psychological treatments entailing long sessions with any kind of therapist seem of limited value in general practice and make little economic sense.
A flexible approach, however, using short interviews and introducing self help by groups or through use of publications and other media may be of more value.
We suggest that this sort of service led by fully trained clinical psychologists who are able to evaluate and adapt their approach to the particular needs of general practice should be explored further.
Editor ,— Mike Pringle and John Laverty recommend that general practitioners should employ only accredited counsellors or ‘chartered psychotherapists recognised by the British Psychological Society.’
The British Psychological Society does not recognise psychotherapists as such; the correct terminology is ‘chartered psychologists.’
In general practice the type of psychologist most used is the chartered clinical psychologist, many of whom have had specific training in psychotherapy.
Chartered clinical psychologists are not allowed to use that title until they have completed a degree in psychology plus a further three years of postgraduate training and supervised practice.
A team or teams of qualified clinical psychologists can be found in every district health authority, so there is no excuse for general practitioners using unqualified staff to treat patients who have a psychological problem or condition.
Editor ,— Renewed interest in the activities and professional training of counsellors in general practice is welcome.
In the 1990s there will be a concerted move by professional bodies such as the British Association for Counselling and the British Psychological Society to regulate counselling and ensure that those who practise are both competent and qualified to do so.
Contrary to some popular wisdom, bad counselling can be damaging to patients.
The question of who counsels patients in medical settings needs to be addressed.
There is some difference between being a counsellor and having counselling skills.
Almost all health care professionals with primary training in medicine, nursing, physiotherapy, or other allied professions counsel people during their work: if the broadest definition of the term is used an episode of counselling occurs in every medical consultation.
Health care professionals constantly give patients information, clarify treatment options, and help people to adjust to new, and sometimes unwelcome, circumstances.
Specialist counsellors, on the other hand, have usually had advanced training in counselling, psychotherapy, or family therapy, and some may be professionally trained in other disciplines such as medicine, clinical psychology, social work, or nursing.
Although specialist training is not a requirement to practise as a counsellor, a professional may occasionally refer a patient to a specialist counsellor in the same way that a doctor may refer medical problems to a specialist colleague.
Clinical and counselling psychologists do not deal only with mental illness.
Our training (at MSc level) emphasises the need for counselling skills for people with many complex medical problems, including HIV infection, management of diabetes, infertility, problems after disasters, pain control after surgery, and neurological problems.
In addition, psychologists conduct collaborative research with their colleagues in general practice, provide health education to patients, and conduct psychosocial assessments and may also provide a consultation and liaison service.
Though there may currently be an undersupply of qualified and accredited counsellors, this should not deter doctors from referring patients to them when resources permit.
Editor ,— The editorial and paper on counsellors and counselling in primary care are misleading.
Mike Pringle and John Laverty's editorial seems to muddle Balint's work in developing the psychotherapeutic skills of general practitioners to aid their work in consultation and the role of a counsellor or psychotherapist working alongside the general practitioner as part of the primary care team.
Pringle and Laverty also state that counsellors who work in primary care should concentrate on non-directive counselling.
That is only one form of counselling or psychotherapeutic approach.
Many others are now being used effectively — for example, brief therapy models, behavioural change techniques, and gestalt — as the counsellors match the patients' needs to therapy and not vice versa .
Pringle and Laverty are apparently unaware of the many family health services authorities that have set up approval procedures for employing counsellors in primary care.
The imminent publication of guidelines on employing counsellors in primary care prepared by a working party of the British Association for Counselling will greatly help those family health services authorities and general practitioners who have yet to develop such guidelines or do not understand how to assess the qualifications and competence of counsellors they wish to employ and work with.
Bonnie Sibbald and colleagues conclude that, because general practitioners did not know the qualifications of the counsellors they employed, those counsellors were probably unqualified.
As most general practitioners would be hard put to describe accurately the qualifications of the nurses they employ, this, I believe, is an unwarranted conclusion.
I hope that Sibbald and colleagues will go back to the practices in their survey to examine this important issue and to explain the uneven distribution of counsellors working in general practice.
Research by the Counselling in Primary Care Trust has shown that in an admittedly small randomised sample of counsellors working in primary care 24 of 26 had had three or more years of training.
Nearly all were undertaking their own personal therapy or had done so in the past, and all were appropriately supervised.
Monitoring treatment with aminoglycoside antibiotics
Editor ,— Guidelines for monitoring aminoglyco-side antibiotics are essential, but those provided by J K Aronson and D J M Reynolds contain many misleading statements.
Kanamycin is included in the guidelines, but this has long been superseded by other aminoglycosides.
The article fails to emphasise the two most important tenets of safe prescribing of aminoglycosides — namely, that they should be used only if clinically justified and that they should be stopped as soon as the patient's condition permits.
We are concerned that those who prescribe aminoglycosides will be distracted by the minutiae of calculations concerning half lives and body loads from the real problem of deciding whether the agents are indicated at all.
Infections requiring prolonged courses (more than seven days, or in our opinion five days, not one to two weeks as the authors suggest) are uncommon, are unlikely to be encountered by most clinicians, and require expert supervision.
Gram negative endocarditis is exceptionally rare and certainly does not include that due to Streptococcus faecalis , which is a Gram positive organism correctly called Enterococcus faecalis .
The case histories quoted highlight the common errors in prescribing aminoglycosides.
Firstly, there is ignorance of their synergistic role in the treatment of viridans streptococcal and enterococcal endocarditis.
A 120 mg loading dose of gentamicin followed by 80 mg eight hourly is excessive for such infections.
National recommendations are clear: 60–80 mg of gentamicin twice a day, and peak concentrations should be between 3 and 5 µg/ml, not 9 µg/ml as suggested in the first case history.
Secondly, adjustment of the aminoglycoside regimen during treatment often leads to the course being prolonged unnecessarily and diverts attention from the selection of suitable alternatives (cases 2 and 3).
Finally, it is unrealistic to recommend (desirable though it might be in theory) repeated estimates of auditory and vestibular function and, likewise, to believe that busy house officers will take samples for measurement of peak concentrations precisely 15 minutes after the end of an infusion and one hour after intramuscular administration as the authors advise.
We, and others, recommend that a sample should be obtained after one hour for both routes and are pleased to receive one at all, for it immediately involves the medical microbiologist in the care of the patient, and close cooperation is important.
Editor ,— J K Aronson and M Hardman do not do justice to the range of antimiocrobial drugs for which monitoring of serum concentrations is necessary, and Aronson and D J M Reynolds include several statements that we find unacceptable in their review of monitoring of aminoglycoside antibiotics.
Serum estimations are of proved value for all aminoglycosides and for the closely related agent streptomycin.
Serum concentrations should also be assayed in all patients receiving vancomycin, flucytosine, or cycloserine; neonates and perhaps those under 4 years old receiving chloramphenicol; those with severe sepsis receiving teicoplanin; and patients receiving prophylactic itraconazole.
With all these agents serum concentrations have been putatively related to toxicity or clinical efficacy or there are inconsistencies between the dosage and serum concentrations.
In selected patients assays of penicillin, co-trimoxazole, flucloxacillin, ciprofloxacin, metronidazole, and rifampicin may also be of clinical value.
Serum aminoglycoside concentrations should be measured one hour after the dose, not 15 minutes after as stated.
One hour after the dose the phase of rapid equilibration between the blood and tissues is generally complete and the elimination phase becomes dominant, and thus the observed concentration more accurately reflects the concentration in tissue, where the infection is most likely to be.
The serum concentration is falling rapidly at 15 minutes, and small changes in the timing of the sample will greatly affect the observed result, which could result in day to day inconsistencies and unwarranted changes in dosage.
We suspect that the high concentration in the first clinical vignette was due to sampling too early.
There is little evidence, however, to correlate serum concentrations after the dose with toxicity, and indeed experimental nephrotoxicity caused by gentamicin is more severe when the total daily dose is divided than when it is given by a single bolus, when concentrations after the dose are higher.
Two other inaccuracies are perpetuated.
One is that a loading dose is required in patients with apparently normal renal function, and the other is that the ‘standard’ dose is 80 mg intravenously eight hourly.
We disagree strongly with any calculation of dosage being based on plasma half life derived from the difference between only two observations, especially if the first of them is taken early in the distribution and not in the elimination phase.
A half life calculated in this way would be falsely short.
It is also important to realise that alternative and less toxic antimicrobials were probably available for the patient who had sepsis due to Escherichia coli and renal impairment.
Finally, aminoglycoside concentrations are  measured in mg/l, not mg/l or mg/ml as given in Reynolds and Aronson's article on making the most of measurements of plasma drug concentrations.
Editor ,— We believe that some of the guidelines on the use of gentamicin in J K Aronson and D J M Reynolds's article are potentially misleading.
The use of a ‘standard’ maintenance dose will inevitably cause either subtherapeutic or toxic concentrations in an appreciable proportion of patients.
The dose should be calculated on an individual basis, with age, body weight, and renal function being used as a guide.
In particular, the frequency of the dose should be reduced in patients with renal impairment, including elderly people.
An audit of prescribing of gentamicin in Leicester showed that this approach considerably increased the number of gentamicin concentration that were within the therapeutic range when measured after one to two days' treatment.
Editor ,— J K Aronson and D J M Reynolds's article on monitoring treatment with aminoglycoside antibiotics does not adequately address optimal dosing with these agents or the timing of assays, nor does it specify which patients need monitoring.
Serum aminoglycoside concentrations should be assayed in all patients with renal impairment, patients receiving other nephrotoxic agents, elderly patients, and patients who have recently received aminoglycosides, since these groups are at increased risk of toxic side effects.
The indications thus include most patients treated with these drugs except those receiving only one to three doses — predominantly those in whom aminoglycosides are prescribed for antimicrobial chemoprophylaxis.
Aminoglycosides are often given in insufficiently high doses.
In Nottingham we encourage the use of a nomogram that takes account of age, sex, renal function, and body weight, factors that influence the body's handling of these agents.
An alternative policy advocates a single large daily dose of aminoglycosides, such as 240 mg instead of 80 mg eight hourly, since the antibacterial activity is related to the dose and these agents exert a relatively long antibiotic effect, at least in vitro.
Such regimens reduce the cost of administration and may be less toxic.
Larger doses of gentamicin given less often result in higher peak concentrations of 10–15 mg/l and low trough concentrations.
A trough concentration of 1.5 mg/l in a patient receiving 300 mg daily does not, however, have the same significance in terms of predicting toxicity as a trough of 1.5 mg/l in a patient receiving 100 mg eight hourly.
With the single daily dose it may be more appropriate to assay the concentration eight or 12 hours after administration of the dose or to accept that a lower trough (predose) concentration may be more appropriate.
Blood samples for assay of the aminoglycoside concentration should be taken at around the third or fourth dose, when the steady state has been reached.
In critically ill patients or those with changing renal function, assays may be necessary earlier than this so that the dosage can be adjusted to avoid tissue accumulation.
Subsequently, concentrations should be measured every 48–72 hours, depending on the clinical context, previous results, and likely duration of treatment.
ment with aminoglycoside antibiotics
Editor ,— We are concerned that streptomycin is not mentioned in the article on monitoring treatment with aminoglycoside antibiotics.
Although the use of streptomycin to treat tuberculosis has declined in Britain, the drug remains an essential component of treatment for some patients.
These patients are generally those infected with drug resistant Mycobacterium tuberculosis or patients with severe disease that has been partially treated (due to poor compliance or intermittent treatment overseas) in whom the results of tests of bacterial susceptibility are not yet available.
Drug resistant M tuberculosis is now a major problem throughout the world, and in response to this the production of streptomycin has been increased.
We believe that good clinical practice should include monitoring the concentrations of streptomycin during treatment, both to avoid potentially toxic concentrations and to ensure adequate dosage.
Unfortunately, the assay kit for this drug previously produced by Abbott Laboratories has recently been withdrawn.
We are currently seeking alternative rapid methods, but the lack of a suitable rapid assay must not be taken as an indication that therapeutic monitoring is unnecessary.
Authors' reply ,— W R Gransden and Susannah Eykyn say that we failed to emphasise the need for safe prescribing of aminoglycosides, and they mention two truisms applicable to all drugs.
In the articles in the ABC series we have addressed the ways in which drug treatment should be monitored after it has been started.
It was beyond our brief to discuss whether or not treatment is required and for how long.
Gransden and Eykyn chide us for mentioning kanamycin, which has been superseded in Britain by newer drugs.
It would have been parochial of us not to have mentioned it as it is still used in many other countries.
Alasdair P McGowan and David S Reeves discuss measurement of serum concentrations of other anti-infective drugs in special circumstances.
We agree that there is a case for this for some of the drugs they mention, particularly to avoid toxicity in renal failure or, in the case of itraconazole, to ensure that adequate dosages are used.
We apologise for having used old fashioned terminology in misnaming Enterococcus faecalis as Streptococcus faecalis , but the apparent redesignation of the organism as Gram negative was not intentional and arose because of the eradication of one too many ‘organisms’ from the text, which should read: ‘Important indications for this category include infections due to Gram negative organisms; organisms, such as Enterococcus faecalis , causing infective endocarditis; osteomyelitis; and infections of vascular grafts.’
We are surprised that Gransden and Eykyn consider our back of an envelope calculations to be minutiae in aminoglycoside treatment.
Such simple calculations are essential in using aminoglycosides safely and effectively.
We agree with MacGowan and Reeves that the half life may be inaccurate if it is calculated from only two values; but perhaps that is why they have never seen a patient with normal renal function in whom the half life is as long as 7.6 hours, since in clinical practice two measurements are all one has to go on.
In these circumstances one makes the best of limited information.
We are disturbed that audiometry is regarded as unrealistic.
The number of occasions when prolonged treatment with aminoglycosides is required is small, but in these cases the risks of ototoxicity are high and the costs of failing to monitor for this disabling condition can be considerable.
We agree with the recommendation that blood samples should be taken one hour after an intravenous bolus dose.
Our comments about sampling 15 minutes after the end of a dose applied to intravenous infusions.
Nowhere did we make any fixed dosage recommendations, and we certainly did not write that ‘standard’ doses should be used.
We described what we believe usually happens.
Thus we stated that the initial maintenance dose is ‘usually…about 80 mg three times a day’; this is so, and there is leeway for altering dosages in patients of low body weight.
Loading doses are used, and, because of the large variability in half life, even when renal function is thought to be normal we think that that is sensible.
Although there is now some interest in the use of large single daily doses of aminoglycosides, it is far too soon to recommend such regimens with confidence.
The review quoted by H Humphreys and D Greenwood makes clear that this type of regimen has mostly been studied in infections that generally respond well to treatment and not in patients with complicated illnesses (in particular, renal disease) and that its use is currently limited.
Some of the correspondents have misunderstood the point of the case histories in the articles in this series: far from being examples of perfect treatment, they are examples of problems that commonly arise.
For example, we clearly stated in the text that serum concentrations should be measured 24 hours after the start of treatment; the case to which MacGowan and Reeves refer was one in which the clinician measured the serum concentration after 48 hours.
We also made it clear that both dose and frequency should be adjusted as dictated by serum concentrations.
Finally, we emphasise that close cooperation between microbiologists and clinicians is important.
Electroacupuncture in fibromyalgia
Editor ,— Several important issues arise from Christophe Deluze and colleagues' paper on electroacupuncture in fibromyalgia.
The use of electroacupuncture rather than ordinary acupuncture in this study is questionable.
To treat musculoskeletal pain, manual manipulation of the needles is all that many experienced physicians consider to be necessary.
Indeed, excessive stimulation of the needles may exacerbate the symptoms.
This and the unpleasantness of needle insertion (suggesting poor needling technique) led to an unusually high drop out rate in both the group given electroacupuncture (17%) and the controls (15%) in the study.
The authors also seem uncertain about the rationale for selecting acupuncture points; they say that they used ‘four common acupuncture points,’ but it is unclear exactly which points these were.
The authors claim that ‘traditional acupuncture points’ were used, and the hand point does seem to correspond to the Chinese acupuncture point Hegu (LI4).
The leg point, however, corresponds only approximately to the Chinese acupuncture point Zusanli (ST36), the location given being inaccurate.
This point should be located by using a system of proportional measurement, which allows for differences in the size of patients.
The correct distance below the inferior border of the patella is not 5 cm but 3 cun , which corresponds to the width of the patient's four fingers.
Needle insertion into the tender points was avoided in this study.
In treating myofascial pain, however, the best results are generally considered to be obtained by inserting the needles into the points of maximum tenderness, though electrical stimulation is usually unnecessary.
Lastly, the points used in the control group cannot be regarded as suitable: electrical stimulation of needles only 2 cm from the real points is unlikely to be without some effect.
The true efficacy of acupuncture may have been underestimated, and further studies of higher quality are needed.
Authors' reply ,— Peter J Lewis believes that we used non-optimal techniques in our study.
Most of the objective data on acupuncture analgesia have been obtained with electroacupuncture rather than dry needling as described in the reference that Lewis cites; therefore we prefer electroacupuncture.
We chose acupuncture points that had been used in most experiments with acupuncture analgesia, as well as in the two previous clinical studies, both of which were of electroacupuncture.
The points used most often correspond to the Hegu (L14) and Zusanli (St36) points.
As most readers of the BMJ are not acupuncturists we found it more accurate to give the location in centimetres rather than cuns .
In fact, the location of the traditional Chinese points is given in a variable manner in different acupuncture books.
Fibromyalgia is not identical with the specific myofascial pain syndrome as defined by the International Association for the Study of Pain.
Patients with fibromyalgia often complain of exacerbation of their symptoms when the tender points are stimulated by pressure, electric current, needling, or vibration.
Insertion of needles into tender points therefore seems inappropriate for the treatment of fibromyalgia.
Another African disaster
Editor ,— I visited Mogadishu in Somalia before the United States and UN peacekeeping forces were deployed.
Anarchy prevailed.
Most men sported an absurd amount of lethal weaponry, which they were fully prepared to use.
Four wheel drive pick ups, crudely converted to gunships with heavy machine guns welded on, were everywhere, guarding the warlords or parked ready for hire.
All outsiders required protection, so business was good for ‘the technicals,’ so named because the UN could not employ gangsters and paid them instead as technical assistants.
There were many food kitchens and supplementary feeding centres run by Unicef and the Red Cross; several were based on projects already started by local women.
What they had achieved with a few weeks of supplementary feeding was remarkable.
Previously there had been fighting when the food was served because of the fear that there would not be enough to go around; now there were orderly queues.
In camps where previously 10–20 children had died each day death was uncommon and occurred mainly among new arrivals.
In one feeding centre the room for the dead had become a Koranic school for young children.
A few hospitals remained open throughout the fighting.
Digfar was run almost entirely by local Somali staff — 25 doctors and nearly 100 nurses — who had not been paid for over two years.
At the height of the troubles the hospital was seeing 100 cases of major trauma a day.
Three non-government organisations also stayed throughout: Médecins Sans Frontières in Medina, the International Medical Corps (based in the United States) in Digfar, and SOS Children, keeping open its obstetric unit.
How they managed to cope is a mystery.
The people are the only resource left.
The political, economic, and social difficulties are catastrophic.
With the UN peacekeeping forces now being deployed Somalia will be fed.
And then what — UN trusteeship and massive international aid?
The worry is that as international interest wanes and other disasters compete for attention Somalia will be allowed to slip back into starvation and anarchy.
There are no short cuts or easy answers to this crisis or any of the others that curse Africa.
Editor ,— John Wright and Helen Ford describe the anguish and despair affecting many sub-Saharan African countries and made so much worse recently by drought.
It is extraordinary, therefore, that these desperately poor countries still owe money to (and still struggle to repay) the rich financial institutions in the North at the expense of feeding or delivering health care to their own people.
Debt relief was barely discussed at the meeting of the World Bank and International Monetary Fund last September.
Endless rescheduling by creditors has caused these very poor countries,’ debts actually to increase.
Over the past 10 years the debt of sub-Saharan countries has tripled to $180bn and these countries now hand over one third of their foreign exchange earnings in debt payments.
Every Zambian citizen now owes his country's creditors $1000 — three times what he earns in one year.
Mozambique's external debt is $4.7bn, Ethiopia's $3.0bn, and Somalia's $2.1bn.
With 40 million people starving and infant mortality rising at an alarming rate, doctors must start pressurising governments and banks to stop this spiral of deprivation and environmental degradation in which the world's poorest countries are trapped.
MEDACT (Medical Action For Global Security, 601 Holloway Road, London N19 4DJ), is making this important human rights issue central to its educational programme and campaign this year.
As Wright and Ford say, the overwhelming responsibility of governments is to feed and care for their own people.
Unless they are allowed to do this, efforts by the medical profession to supply vaccines, drugs, or aid are futile.
HIV transmission, travel, and Thailand
Editor ,— Ahilya Noone and colleagues warn about the danger of HIV infection to travellers in Thailand.
I have just visited Thailand and met venereologists there.
Since 1984 there have been constant campaigns alerting the public in Thailand to the dangers of HIV, AIDS, and sexually transmitted diseases.
Though there is still a disproportionate preponderance of sexually transmitted diseases in prostitutes, labourers, and agricultural workers, since 1986 the incidence of all venereal diseases has decreased.
Use of condoms has been enthusiastically advocated locally among the Thai people.
Surveillance is centred on communicable disease control based on national returns from an effective network of 96 sexually transmitted diseases clinics  in 73 provinces.
For instance, at the central sexually transmitted diseases centre at Bangrak Hospital, Bangkok, there were separate clinics for female and male sex workers, with adequate laboratory facilities for all the diagnostic modalities for HIV and sexually transmitted diseases.
The number of people attending sexually transmitted diseases clinics fell from 1103813 in 1987 to 877146 in 1991, and the number with venereal diseases (defined as syphilis, gonorrhoea, chancroid, and lymphogranuloma venereum) fell from 410406 to 182024.
The table shows the incidence of various diseases in 1987 and 1991.
Genital ulcer disease is an important factor in the spread of HIV.
In Thailand in 1991 the falling incidence of chancroid approached that of herpes genitalis for the first time, reaching 0.22/1000 compared with an incidence of herpes genitalis of 0.18/1000.
In December 1991 the prevalence of HIV infection in different groups remained high: intravenous drug users 34%, brothel sex workers (local users) 22%, higher class (tourist) sex workers 5.5%, male patients of sexually transmitted diseases clinics 5.6%, pregnant women 0.67%, blood donors 0.8%.
Thailand is, as Fred Lenihan points out, making an enormous effort to combat these infections.
Recommendations have been made (Chavalit Mangkalaviraj, personal communication) that information given to travellers and epidemiological data should be exchanged internationally and that study tours for health staff in interested countries should be set up.
Editor ,— Ahilya Noone and colleagues point out the need for travellers to receive appropriate information on safer sexual behaviour.
A four week campaign, jointly organised by Greater Glasgow and Argyll and Clyde Health Boards, was run in the international departure lounge of Glasgow airport in July and August last year.
During peak periods of departures of chartered aircraft to Mediterranean destinations a small team of sessional workers was in the lounge.
Departing travellers were encouraged to complete a prize crossword on beermats, whose clues related to transmission of HIV and safer sex.
On completion the crosswords were placed in a ‘treasure chest,’ part of a large and colourful photomontage prominently displayed in the lounge.
A prize draw was held regularly, and winners were awarded specially designed T shirts.
Participants were also offered free condoms and information about HIV and about health while abroad.
During the four weeks an estimated 66000 people passed through the lounge when the team was present.
About 10500 crosswords were completed, almost all correctly.
About 30% of respondents were under 21, 36% were 21–30, and 34% over 30; 52% were female.
Around 2500 T shirts were given out, and nearly 6000 people took condoms and information leaflets.
The campaign provoked great interest and virtually no adverse comment.
In a separate survey most respondents correctly understood the aim of the campaign and thought that it was appropriate in such a venue.
The positive reaction to the campaign suggests that airport departure lounges are ideal locations for conveying information about safer sex.
There is a captive audience, most of whom can understand the relevance of the message, and many are prepared to participate as they await their flight.
National standard for entry into general practice
Editor ,— The working party set up to make recommendations on a national standard of entry into general practice seems to have been unaware of European Community directive 86/457/EEC.
This is mostly concerned with current standards of training in general practice in the health services of member states.
Article 11, however, states that suitable proposals should be brought forward by 1997 for appropriate training for every general medical practitioner in order to satisfy the specific requirements of general medical practice.
Informal contact with officials in Brussels through diplomatic sources has indicated that this is intended to apply to all practitioners, in public or private practice, in any form of primary health care.
In other words, vocational training in general practice will become the minimum standard for practice of any form of medicine other than specialist practice, or training for such.
Therefore proposals to fail people in general practice training will, if the directive is implemented, be tantamount to striking people off the medical register, and the idea of a subprincipal grade may actually be illegal after 1997.
I am also concerned about the working party's proposals for other reasons.
Firstly, it seems unreasonable, against nature, and misogynistic to suggest to a young woman that she will have to wait until her late 20s to complete her minimum training and then be able to start a family.
Secondly, I find it impossible to accept that general practice is a specialty.
Something that is general by definition cannot be a specialty.
The alternative term of family practitioner is not acceptable either.
I practise as a ship's doctor.
I am certainly a general practitioner, but I am not a family practitioner.
One problem with the Royal College of General Practitioners is that it seems to equate general practice exclusively with being an NHS general practitioner.
There are other forms of general practice.
History indicates that general practice is not so demanding that unqualified people cannot pass themselves off as principals for many years and get away with it.
I wonder how many years unqualified people could pass themselves off as consultant thoracic surgeons, for example, without detection.
Pretending that general practice is something that it is not runs the risk of people remarking on the emperor's new suit of clothes.
I would therefore ask for a sense of proportion over the future of general practice, along with some breadth of vision and tolerance for alternative types of general practice.
I do not want to have to found a Royal College of Ships' Surgeons to carry on practising my brand of general practice.
Risk stratification for open heart surgery
Editor ,— If operative mortality is to be of any use as an indicator of quality in cardiac surgery a system of risk stratification must be in place.
The purpose of our paper was simply to show that the Parsonnet system can be applied easily and effectively to cardiac surgical practice in Britain.
That we found a lower mortality than expected is not relevant to the paper's message.
I wish to reassure Andrew Allan and Andrew T Forsyth that our trial was indeed prospective.
David J Spiegelhalter suggests that the Parsonnet system overstates risk.
Whether or not that is the case, the system remains an excellent method of stratifying patients into well defined risk groups.
Kenneth M Taylor and Peter K H Walton mention other methods of risk stratification such as PANECAN (the pan-European cardiovascular network) and bayesian analysis.
For such methods to gain widespread acceptability they must be comparable to the Parsonnet system in ease of implementation.
I look forward to the day when a statistically perfect model is available for assessing the quality of cardiac surgical care.
In the meantime, however, widespread implementation of the Parsonnet system would represent a great improvement in the audit of cardiac surgery over crude mortality figures.
Laser treatment of port wine stains
Editor ,— M J C Van Gemert and colleagues echo the opinion of most of those who use laser treatment for port wine stain malformations that pulsed tunable dye lasers with a wavelength of 577 or 585 nm are the best choice for treating children.
Their suggestion that pulsed dye laser treatment should be offered as first line treatment for port wine stain malformations whenever possible, however, is controversial with respect to adult patients and ignores the complex nature of these lesions and their treatment with lasers.
Although many port wine stains in adults can be effectively treated with a pulsed dye laser, this may not be the most effective treatment and a considerable proportion of lesions do not respond.
Port wine stains are heterogeneous, and responses to different lasers vary considerably.
My experience suggests that many lesions that are macular and blanch on direct pressure respond more favourably to pulsed dye lasers than to alternatives such as copper vapour lasers, although in many cases responses are similar.
Lesions that do not blanch on pressure (which are often dark in  colour and raised) tend to respond poorly to pulsed dye lasers: better and more rapid responses can often be achieved with alternative lasers such as the frequency doubled neodymium yttrium aluminium garnet laser in conjunction with an automated delivery system.
Deeper cavernous areas or hypertrophic tissue may require more non-specifically destructive treatment such as neodymium yttrium aluminium garnet or carbon dioxide lasers to improve their appearance.
None of these alternative treatments rule out subsequent treatment of residual port wine stain with a pulsed dye laser, and using several different lasers in combination often gives the best results.
Mathematical modelling can determine theoretically ideal laser variables for treating port wine stains.
This work has made a great contribution to our understanding of the subject, although the structural diversity of port wine stains confounds the best attempts at mathematical modelling and there is no substitute for good clinical assessments and comparisons of different laser systems.
Further research needs to be done before Van Gemert and colleagues can be confident in suggesting using pulsed dye lasers as first line treatment for all port wine stains.
Cardiovascular disease in developing countries
Editor ,— Unfortunately, the emerging epidemic of cardiovascular disease in developing countries cannot be explained by selective quotation or wishful thinking.
Uffe Ranskar is sceptical about evidence from death certificates.
Of course, there is a particular problem with death certificate data in developing countries, but there is plenty of other evidence for the epidemic.
Ravnskov attributes the epidemic to the effects of westernisation.
Dietary change is a central component of this process, and there is a mass of evidence to support the role of a diet high in saturated fat in atherogenesis.
The epidemic is preventable and is being prevented in many industrialised countries.
Unfortunately, it is not yet being prevented in eastern European or developing countries.
Quality of life measures
Editor ,— D J Spiegelhalter and colleagues discuss measuring the benefits of health care in terms of quality of life and describe using quality adjusted life years (QALYs) in resource allocation.
This measure may be of use to health care planners, but, even if it is eventually validated and accepted as an equitable way of allocating resources, it will be of little help to general practitioners, whose decisions are mostly concerned with managing self limiting minor illnesses.
I suggest that general practitioners' decisions are generally what Spiegelhalter and colleagues call stage 1 decisions — that is, unaided intuitive judgments.
I strongly support Tony Delamothe's view that evidence that treatments are actually efficacious is needed before we start rationing care.
General practitioners might be helped most, and would consequently help their patients most, by research defining the best way to manage these common, unexciting illnesses.
Such research would be helped by the development of valid, reliable instruments to measure quality of life in common minor illnesses.
Most of these minor illnesses last only a few days or weeks: what are needed, therefore, are the measures quality of life days or quality of life weeks.
Such instruments would have several advantages over QALYs.
Firstly, they would be more reliable and valid.
They could be constructed by random surveys of a general practitioner's practice population (culturally valid), most of whom would have had personal experience of common conditions.
The responses could be analysed for differences in opinion depending on the time since the respondent had suffered from the illness or between those who had ever and those who had never suffered from the illness in question.
Secondly, patients would not be asked to include death in the range of responses.
Thirdly, the instruments would be used primarily to compare different management choices for one illness and not to make rationing choices between different illnesses.
The results of research with such instruments would enable general practitioners to provide scientific evidence to managers on why they may be using more expensive management options for a particular illness when the managers exert pressure on them to use the cheapest.
Moreover, by choosing an option that has been shown to be the best in terms of quality of life days gained general practitioners would be more certain that they were acting with beneficence; at present, using simply their ‘unaided intuitive judgment’ to make such decisions, they can only hope that this is the case.
Rationing
Editor ,— Tony Delamothe writes that there was much common ground between two recent conferences on rationing, one sponsored by the Royal College of Physicians and the other by the Radical Statistics Health Group, Public Health Alliance, Critical Public Health, Socialist Health Association, National Union of Public Employees, National and Local Government Officers' Association, and Confederation of Health Service Employees.
His view was not shared by another reviewer.
The ‘great and the good in serried ranks’ at the Royal College of Physicians heard clinicians from different specialties confront managers with the problems of rationing at the clinical interface.
This specialty focus could be perceived as ‘plea bargaining’ and was taken as such by at least two chairpeople of health authorities.
Many of the other presentations reduced rationing to an academic exercise which ignored the current impact of decreased resources on the provision of health care.
Focusing on effectiveness and outcomes as a basis for decisions on rationing is a desirable aim that cannot be achieved at present when so much of health care is unevaluated.
Health authorities have already begun to restrict services, most commonly removal of tattoos and cosmetic surgery.
These interventions are surgically effective, and their removal from the list of services has more to do with social judgments.
Discussions of the Oregon experiment were repeated, although this debate has already been well rehearsed.
The Oregon experiment was introduced to ensure a basic level of health care for poor and elderly people.
In contrast, in the NHS we now face reduced access to care in a system that once prided itself on its aim of universal accessibility.
Rudolf Klein, in a remarkable about turn, concluded at the meeting at the Royal College of Physicians that, since purchasers cannot make their decisions on rationing explicit, clinicians should make the decisions but make explicit whom they will not treat.
Are the political constraints that prevent purchasers from making their decisions on rationing explicit any different from those facing clinicians?
And are clinicians any more immune from social judgments?
Rationing should not be the exclusive domain of managers and professionals.
Our conference, which Delamothe said was attended by ‘political activists and pensioners,’ was intended to open up the debate to the public and to inform people using the service.
The proceedings of the meeting are to be published by Critical Public Health.
If pensioners, with potentially the most to lose in the rationing process, do not participate in discussions about rationing who should?
By holding an expensive meeting on a weekday the Royal College of Physicians denied a voice to the very groups that most need to participate in the debate.
Could this have been the intention?
Working with adult survivors of child sexual abuse
Editor ,— I was struck by the subtitle of Penelope Campling's editorial on working with adult survivors of child sexual abuse: ‘Much can be learnt from what goes wrong.’
I would like to draw attention specifically to the trauma of vaginal and rectal examinations for adult survivors.
I am the daughter of a doctor and was sexually abused throughout my childhood.
I have been through a long period of psychotherapy dealing with this abuse and continue to find internal examinations traumatic.
In September 1989 I was taken into hospital with the classic symptoms of acute appendicitis.
An understanding female house officer took a history and examined me.
Then I saw the senior registrar, who had already been briefed by the house officer about my history of abuse.
I also explained to him myself that I had been abused by my father, who was a doctor, and that I did not wish to be examined by a man.
An argument of 45 minutes ensued, during which I became distraught.
In his professional opinion this was overreaction on my part, and he was extremely inconvenienced by my insistence not to be examined internally by him.
Eventually my husband and the house officer managed (with great determination and much shouting) to persuade him that if it was absolutely necessary to examine me internally the house officer would do so.
She did, and my appendix was  removed the next morning.
After I had recovered from the anaesthetic the house officer came to tell me that while I had been anaesthetised the senior registrar had in fact examined me internally.
I was furious at this.
I am fortunate to have had enough support to recover from the hell of being abused.
But even being able to articulate my needs was not sufficient to protect me from further abuse.
How much worse it must be for those who haven't been as fortunate as I in recovering from such trauma.
I write because I wish to bring this to the attention of the medical profession.
I have seen many doctors who, sadly, lack sensitivity in dealing with this issue.
Since internal examinations are common it is important that doctors respect the needs of those who may require particularly sensitive handling.
Maybe much could be learnt from my experience, which did go badly wrong.
Editor ,— I was heartened by the sensitivity of Penelope Campling's editorial on working with adult survivors of child sexual abuse.
Even though child abuse — mainly the sexual variety — has been a more open subject in the past few years, I have not seen many articles of this nature in professional magazines.
Though not wishing to detract from Campling's writing, I wish to point out that many of the reactions, feelings, and transferences that she mentions also apply to those who were not sexually abused but ‘only’ emotionally abused.
The sexual act does violate the child, but the real damage is done to the psyche, wherein lies the trauma — the memory, the damage to self esteem.
Similar damage is done by repeated emotional abuse, and I believe that the true damage done by this is only beginning to be seen.
Repeated put downs, rejections, and criticism can produce the same feelings of confusion, fear of all authority figures, guilt, low self esteem, seeking for love, and other reactions listed by Campling.
Doctors who diagnose child sexual abuse from their observations of a patient may be wrong.
Sexual abuse may not be the abuse that took place.
Emotional abuse may be the underlying trauma (and this often comes with a fear of being physically harmed).
As an adult survivor of both types of abuse and a professional dealing with others coming to terms with their abuse, I know.
I am delighted to see people like Campling dealing with this subject with such caring.
Bronchospasm during disulfiram-ethanol test reaction
Editor ,— We would like to add some comments to the case report of E Zapata and A Orwin dealing with bronchoconstriction and hypertension experienced by a 45 year old man taking alcohol and disulfiram.
We investigated the role of acetaldehyde in causing respiratory distress in subjects with impaired hepatic acetaldehyde dehydrogenase activity or receiving treatment with disulfiram by giving alcohol to guinea pigs.
Intravenous administration of acetaldehyde elicited a prompt rise in blood pressure and increase in bronchial resistance associated with augmentation of blood histamine concentration.
Antihistaminic agents significantly antagonise acetaldehyde actions.
Furthermore, we established that captopril, a well known angiotensin converting enzyme inhibitor, strikingly potentiates the effect of acetaldehyde.
Thus, since the administration of NK2 tachykinin receptor antagonist reduced the response of the airways to acetaldehyde in animals pretreated with the angiotensin converting enzyme inhibitor, a contribution of neuropeptides to acetaldehyde activity was considered a critical point.
Unexpectedly, the infusion of very low doses of acetaldehyde induced hyperresponsiveness of bronchial smooth muscle to substance P. Acetaldehyde is clearly able to interfere with complex neural and humoral mechanisms regulating the bronchomotor and cardiovascular functions.
These findings in guinea pigs may have implications for human respiratory pathology.
Robuschi et al showed that aerosol challenge of asthmatic patients with acetaldehyde caused a consistent reduction in forced expiratory volume in one second.
Thus it is reasonable to presume that asthmatic patients, particularly those treated with angiotensin converting enzyme inhibitors, may face exacerbation of broncho-obstructive symptoms on taking alcohol and its conversion to acetaldehyde.
Furthermore, in patients with a genetic disposition or pathological alterations characterised by a reduced aldehyde dehydrogenase activity the repeated alcohol intake, through the mechanism here described, may result in bronchial hyperresponsiveness and haemodynamic alterations.
More information will put into correct perspective, in conjunction with the known pathogenetic factors (allergic, inflammatory, infectious), the contribution of alcohol intake to the multifactorial pathogenesis of bronchial asthma.
Monitoring cyclosporin treatment
Editor ,— D J M Reynolds and J K Aronson provide a clear approach to monitoring cyclosporin treatment after organ transplantation.
Although they acknowledge the use of this potent immunomodulator in other diseases, however, they fail to emphasise the differences in monitoring cyclosporin treatment in autoimmune diseases.
As a second line agent cyclosporin is a relatively new drug in rheumatoid arthritis.
Generally, lower doses are used than in organ transplantation.
Most rheumatologists agree that monitoring the treatment in rheumatoid disease does not routinely require measurement of blood cyclosporin concentrations: patients' blood pressure, renal biochemical variables, creatinine clearance, and full blood count are monitored.
Patients with rheumatoid disease are particularly susceptible to nephrotoxicity.
A rise in creatinine (<30% above the baseline value) is acceptable and, indeed, expected.
Cyclosporin concentrations are checked only in those patients who are receiving additional treatments that may have an appreciable interaction with the immunosuppressant.
Cyclosporin is becoming increasingly important in the management of a wide variety of diseases; monitoring of the drug will vary according to the underlying disorder.
Treating hypertension in elderly patients
Editor ,— I agree with Kevin O'Malley and Eoin O'Brien that ‘a cautious approach to implementing the results of recent landmark studies may be wise.’
The authors declare that the consistency across a range of interventional studies in the past decade is impressive.
On the contrary, the consistency is virtually non-existent, except that all interventions seemed to do some good, but not the same good, and all but one had no significant effect on overall mortality.
An Australian trial of treatment of mild hypertension in elderly people showed no reduction in fatal strokes but a 75% reduction in ischaemic cardiac deaths.
In the double blind component of the study by the European working party on high blood pressure in the elderly there was an apparently impressive but barely significant reduction in cardiac deaths (-47%, p=0.048).
The authors reported ‘a non-significant decrease in cerebro-vascular death’— that is, no significant reduction in cerebrovascular death.
In contrast, Coope and Warrender reported a 50% reduction in total strokes, but the ‘incidence of myocardial infarction was unaffected by treatment.’
At the end of the five year systolic hypertension in the elderly trial there was a reduction in total stroke incidence of 6 per 1000 patient years of treatment.
There was no significant reduction in deaths from myocardial infarction.
In the Swedish trial in old patients with hypertension the reduction in overall mortality was largely due to a reduction in mortality from stroke.
There was no effect on fatal or non-fatal myocardial infarction.
The recently published Medical Research Council trial showed a reduction in strokes but no significant reduction in coronary events.
If treatment was applied to the generality of elderly people with hypertension these conflicting trial results suggest four possible outcomes.
Firstly, cardiac death would be reduced but not death due to stroke.
Secondly, death due to stroke would be reduced but not cardiac death.
Thirdly, both cardiac death and death due to stroke would be reduced, but the near universal absence of a fall in reported overall mortality makes this unlikely.
Fourthly, neither death due to stroke nor cardiac death would be reduced.
On this basis I suggest that there remains a great deal of doubt that mild to moderate hypertension in elderly people should be treated.
In elderly people in industrialised societies the prevalence of arterial hypertension approaches or  even exceeds 50%.
Britain has the highest proportion of population over 75 of any country in Europe.
The World Health Organisation-International Society of Hypertension guidelines for initiating treatment in hypertension include a six month period of monthly reviews.
Is it seriously suggested that the entire elderly population should be regularly screened, that a clear majority should be followed up monthly for six months, and that half should be treated for life on the basis of even the most optimistic interpretation of the trial data?
I submit that there are logistical, financial, and indeed ethical questions to be addressed first.
Contracting arrangements
Editor ,— B Olsburgh raises the question of rational distribution of health care resources in relation to coronary artery bypass grafting.
My initial reaction on hearing that his health authority will not purchase further cardiac surgery during this financial year was to share his concern and frustration.
As a surgeon in the regional cardiac unit serving his health authority, however, I recently analysed figures showing the rates of coronary artery bypass grafting (per million population) for all the districts that the unit serves in the Northern region.
Obviously many factors play a part, but there is over a twofold difference (179 v 364 per million) between the worst and best served districts.
With our current facilities we can do a set number of coronary artery bypass graft operations in any period, and thus if we operate on more patients from one district by definition we operate on fewer patients from another.
The rate of coronary artery bypass grafting for 1991–2 in Olsburgh's district was almost exactly the target figure of 300 per million, and I assume that his health authority has based its purchasing decisions for this year on last year's achievement.
I presume that more patients from his district were near the top of our waiting list; in addition to, a third of the patients having such bypass operations from his district this year had unstable angina and were operated on urgently.
Thus we have already achieved the target figure for his district for this financial year and are behind our targets for other health districts.
If these operations are to be distributed evenly across our region, for the next few months we should concentrate on patients from other health districts (assuming equal clinical priority).
Thus as I look at the figures I see the question in a broader context.
This is no comfort for a patient who has to wait a few months (in the past, waiting times in some centres were over two years) but is perhaps a more rational means of distributing limited health care resources than the random waiting lists used in the past, when patients were given fairly arbitrary degrees of priority.
As a clinician, however, I believe that the needs of individual patients remain paramount, and if any patient is thought to need urgent surgery it will be performed without question.
Olsburgh's district has confirmed its agreement with this, and in any case the current system allows for this with mandatory payment for emergency extracontractual referrals.
Facilities for coronary artery bypass grafting in the Northern region are expanding, both in the current unit and with the opening of a second centre later this year.
Condyloma acuminatum and squamous papilloma of the palate
Editor ,— Zaid P Shehab and Ahmes L Patior describe the papillary lesion of the hard palate in the photograph on the Minerva page as a condyloma acuminatum and imply that it arose as a result of orogenital intercourse.
The lesion is clearly a squamous papilloma of the palate.
This has a histological appearance identical with that of condyloma acuminatum (a genital lesion), even to the extent of presenting ideas of koilocytosis among the squamous cells, indicating infection with human papillomavirus.
The essential difference is that the palatal lesion, like most squamous papillomas of the upper air and food passages, is not associated with malignant change of adjacent stratified squamous epithelium.
Condyloma acuminatum may be so associated.
For this reason we believe that the papillomas at the two sites should continue to be given different names.
The female genital condylomas are sexually transmitted, but evidence for this with the palatal lesions is only anecdotal, as in this case.
Health promotion in general practice
Editor ,— I have great sympathy for Duncan M Williams.
I hope that he can accept comment from a colleague who through professional conscience has already lost many thousands of pounds since the imposition of the new general practice contract.
I agree that the new health promotion package is far from ideal.
It has limited aspirations and a humiliating level of reimbursement.
It does, however, remove much of the quackery of the 1990 contract: three year health checks and ‘Noddy’ health promotion clinics.
It is no wonder that the General Medical Council is reluctant to deal with the issue of treatments that are not scientifically validated.
For the past three years every general practitioner in Britain has been forced to waste professional time and scarce resources on unproved and possibly harmful procedures.
The government is having difficulty managing the NHS, and I suspect that it no longer wants the task.
If this is the case I hope that it has the courage and honesty to admit it.
I believe that only frank and open discussion can reduce the agony and frustration of those like Williams and me who are at the grass roots of the service
Community care
Editor ,— After the full implementation of the community care reforms local authorities will be under increased pressure to use all sources of information about their clients' and population's needs for community care services.
As David Browning points out, practitioners from health and social services will need to coordinate their assessment activities.
We recently undertook a survey that gives a snapshot of community assessment of elderly people in district health authorities in England and Wales.
Altogether 124 of 185 authorities completed our postal questionnaire during November and early December last year.
We found a high level of assessment activity by community nursing staff using structured questionnaires.
The information obtained was being used not only for patient care but also for management purposes such as audit, planning services, resource management, and monitoring contracts.
Information was being shared widely with social workers and social services agencies.
Thirteen districts used well established instruments, 33 used locally developed instruments, and 66 used a combination.
The Barthel index and the Clifton assessment procedure for the elderly were the most widely used established instruments, in 41 and 22 districts respectively.
Only eight districts had a computerised information system that incorporated information from assessments.
Given the importance of the uses to which information from assessments is put, greater standardisation of methods of assessment is needed to ensure that the validity and reliability of information obtained are acceptable.
Considerable investment may be also required to develop information technology systems.
Unfortunately a request to send a similar questionnaire to local authority social services departments was declined by the service evaluation, research, and information committee of the Association of Directors of Social Services.
Editor ,— I practise in ‘the large sparsely populated area of rich farming country’ in the east of Gwent that Roger Robinson mentions in his article on community care.
In 1985 the Welsh Office produced a booklet,A Good Old Age , which described a community health care team for elderly people based on a population of about 10000.
This envisaged a coordinator to integrate the various official and voluntary agencies for elderly people.
In east Gwent a project was designed around this philosophy.
The Bridges Community Centre was opened in 1987 in what had been a derelict factory canteen on a large industrial site in Monmouth.
Joint funding by the Welsh Office and Gwent Health Authority provides a day hospital on two days a week and a day centre, run by volunteers, on three days a week; the day centre has about 90 clients a week.
From the start the project incorporated a screening nurse, who has competent administrative help.
Like my colleagues in Newport, I have been dismayed by the checks for people over 75: it is not just the collection of medical facts but the appreciation of the social and personal difficulties of elderly people.
If we screen intelligently how do we satisfy their uncovered needs?
The answer must be to have a community based resource that unites medical and social health professionals.
The project awaits community care funding with some trepidation.
Its five year funding has finished, and the family health services authority and health authority are making encouraging supporting moves.
I fear, however, that the project will become lost in the confusion and enormity of the changes in community care funding.
The family health services authority and health authority met with social services at the end of January to explain community care funding, but will this have come in time to save this local initiative?
Clearly, the community care fund will require the active participation of both social service departments and the family health services authority and health authority in these community ventures.
Consultant episodes
Editor ,— Allyson M Pollock and Azeem Majeed's arguments in response to our editorial on consultant episodes could perhaps be summarised as ‘the internal market is intrinsically flawed and we should not do anything that would delay its demise.’
That is a valid view, but we believe that the internal market is likely to exist for the foreseeable future and that we should try to make it work as well as possible.
When we wrote the editorial it was true that most, but not all, of the evidence of opportunism had been reported from the United States.
This was inevitable as the American market based system has been in place longer than the British, but there is increasing evidence of it in Britain as well.
Pollock and Majeed argue that this behaviour is somehow different in the two countries as the motive is profit in the United States but survival in the internal market here.
This argument is untenable as some of the most dramatic changes in behaviour by American hospitals occurred at the time that the prospective payment system was introduced, when only 10% of hospitals were private, for profit institutions, and there is considerable evidence that, especially for rural hospitals in the United States, the threat has indeed been to survival.
A somewhat more complex issue that has not received adequate attention in Britain is the meaning of the word ‘profit’ in the context of trusts, and some of the ideas being discussed resemble the strategies that have caused the Internal Revenue Service to question the tax exempt status of some American not for profit hospitals.
The thesis underlying our editorial is that if purchasing authorities are to base their activities on meeting epidemiologically defined need they should use epidemiological principles.
This means that they should use data that are based on people and defined events.
We cannot see how the continued use of inappropriate information can enhance equity of health care.
Junior doctors' hours
Editor ,— Ellis Field suggests that increasing the number of consultants to help reduce junior doctors' hours would alter the nature of senior posts for the worse and advocates a substantial increase in junior staff.
While a corollary of the new deal may be that some consultants will have to take a more direct role in the acute management of patients, this in no way diminishes the attraction of a senior post.
The thought of spending another five years in training to achieve that goal does.
Field describes the increase in the number of junior doctors as temporary but fails to offer any mechanism to phase out these posts in the absence of appreciable expansion in the number of consultants.
Had we been as free from the fetters of manpower planning as Field when we negotiated the new deal the problem could have been solved overnight.
An increase in the number of seniors is only one option proposed by the signatories to the new deal, who included consultant representatives.
Changes in the working patterns of juniors are necessary.
In addition, consultant firms are no longer appropriate in many settings and a team based approach will often result in better care for patients and improved experience for juniors, who will be sufficiently awake to benefit from the ‘apprenticeship method of transmitting skills and knowledge’ that Field advocates.
Where a rota system remains the most appropriate form of out of hours cover a consultant may have to be directly on call to an experienced senior house officer.
None of these could be regarded as solutions in themselves, but taken together, and in combination with other measures outlined in the new deal, they can make a substantial improvement.
The Department of Health must accept that without a substantial increase in the consultant body and rationalisation of acute services the new deal will undoubtedly fail.
If it does the alternative of a statutory limit on hours of work similar to that proposed by the European Commission would destroy any semblance of manpower control just as surely as Field's proposals.
Editor ,— I have no solution to the problem of junior doctors' hours, but Jeremy Wight's allegation that the Junior Doctors Committee was duped by ‘the hierarchy of the BMA’ requires comment.
As long ago as the ‘work to contract,’ juniors were negotiating and acting independently, although they needed the sanction of the BMA council to make their action legal, as did the consultants.
They submitted their evidence to the Short committee separately despite the protests of the other craft committees and council.
At last year's annual representative meeting we were informed by the chairman of the Junior Doctors Committee that he was engaged in secret talks with the chief medical officer on the future of training and specialist recognition, without the knowledge of the seniors.
He told the representative body that the talks were too confidential for us to be let into the secret.
He was followed in the same debate by a junior who informed the representative body that even if we did not vote for the juniors' motion a solution would be imposed by the Department of Health in conjunction with the juniors whether we liked it or not.
I was present at and involved in all these events.
The image of the juniors as passive dupes of the BMA machine is a myth.
We have been asked over the years to vote for a freeze on medical school places when juniors were unemployed, for more manpower when they were overworked, and for more consultants when they were having problems with their careers.
Every time the juniors have been supported.
The equation may be insoluble.
It may be because the juniors act independently with equal quantities of youthful enthusiasm and historical ignorance that they get themselves into situations that the dinosaurs have seen before.
It is usual for frustrated youth to protest that ‘it's not fair’ and to look around for bogymen to blame.
Looking for them in BMA House only emphasises the juniors' ignorance of the enormous amount of work put in on their behalf by all sections of the profession.