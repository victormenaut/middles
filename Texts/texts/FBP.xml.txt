

Europe's research after Maastricht
Whenever the European enterprise takes another step forward, its leaders say there will be no change.
The European Communities' research policy is just one illustration that such a doctrine does not make sense.
THE time has come round in the European Commission's calendar (see page 5) when another set of committee meetings will set out to define yet another ‘framework’ programme, the commission's name for the programmes of largely applied research which it plans and administers on behalf of the European Communities (EC).
There will, no doubt, be the usual trimming of the sums requested (about $2 billion over four years), but there is unlikely to be root and branch objection of the kind raised by Mrs Margaret Thatcher's British government in its first few years.
The truth is that Europe has grown comfortable with the now-familiar collection of projects in fields such as information technology (ESPRIT), broad-band communications (RACE) and semiconductor technology (JESSI).
The question is whether Europe should be so content.
The question is all the more pointed now that Europe is within an ace of having its Maastricht Treaty; the British government now seems confident of winning assent from the British Parliament (although something could still go wrong) while Denmark will probably vote YES in its second referendum on the issue on 15 May.
When all the paperwork has been done, the European Communities will be renamed the European Union.
Otherwise, and immediately, little will change.
The most hard-fought provisions of the treaty, those legislating for monetary union by either 1997 or 1999, are fast becoming dead letters as European economies waywardly diverge from the strict requirements on inflation rates and government budget deficits defined by the treaty.
Yet Maastricht is more than a symbolic milestone.
It will also be a turning point in the way that Europe regards itself.
Ambitions
Although Maastricht is mainly about monetary union (with something about the powers of the commission and the European Parliament) and the more distant prospect of joint working on foreign policy and security, it will also  be taken as (and perhaps mistaken for) a sign of Europe's ambitions for itself.
There is a danger of internal contradiction.
The preamble of the Treaty of Rome emphasizes two goals: an end to intra-European conflict and prosperity not confined to industrial populations (whence the Common Agricultural Policy).
In retrospect, that is a conservative agenda.
But the rhetoric of the past quarter of a century has been about prosperity founded on technology as advanced as that found anywhere, and about economic power to match that of the United States and Japan.
More recently, with the end of the Cold War, Europe has also nursed the ambition to recruit as members countries in Northern and Central Europe.
It is not surprising that there should seem intolerable tension between the goals of the 1950s and those now in the front of people's minds.
Contradictions
Policy on research illustrates that difficulty and is, indeed, central to it.
The commission's goal is to encourage industrial organizations, mostly companies, to collaborate on advanced technology.
It is a standard condition of its schemes that partners in its projects should come from at least two member states (but the more, the merrier).
The commission expects to pay no more than half the costs of agreed projects.
One obvious snag is that the insistence on international collaboration, whatever cultural value it may have, is inconsistent with the Single Market (ostensibly a reality since the beginning of this year).
Why should the commission believe that advanced mousetrap manufacture, say, should be represented in at least two member states when one manufacturer might supply the whole of Europe with the products and thus could be better placed to export as well?
The difficulty, for the commission, is that if it abandoned the present formula, it would face exactly the same problems as national governments, seeking to foster advanced technology but fearful of the temptation of ‘backing winners’.
Luckily, there are other courses the commission could follow.
There is much the public sector can do to encourage advanced technology.
To ensure that there is a sufficient supply of skilled people is the first of them, which requires that attention should be paid to advanced education.
(That is why this journal has consistently argued for a redirection of commission funds towards basic research, for the sake of the skill it can in principle provide.)
The public sector can also legitimately support advanced technology by providing standards and testing laboratories.
(The commission's difficulties in running the Euratom laboratories may explain its lack of enthusiasm for the infrastructure of industrial innovation.)
Above all, the public sector can hope to cultivate a climate in which innovation prospers, intellectually and financially.
The commission would say that these are precisely the goals of its present research programmes; that may be so, but the reality is different.
Too often, companies locked into collaborations with rivals regard commission projects as means of making sure that they, not their rivals, are ahead.
Not all the individual projects are well managed, while there is too little support, beneath the commissions' ambitious umbrella of applied research, for the multiplication of modest innovations on which the growth of Europe's prosperity is likely most immediately to depend.
In an ideal world, the commission (like national governments) would behave more like Japan's Ministry of International Trade and Industry in relation to industrial innovation.
Sadly, the commission has not yet the skill and the reputation to pull off such successes.
Moreover, it is unlikely to be in such a position until it has come to grips with the broader contradictions underlying its current operation — its devotion to the Common Agricultural Policy on the one hand and its ambition to recruit the states of Central Europe as members on the other.
There is a prior question: can a European Union committed to maintaining an agricultural policy whose size is historically specified also realistically hope to compete will all comers in advanced technology?
Nobody doubts that, in principle, Europe is large and sophisticated enough to be a powerhouse of new technology, but the achievement of that goal can only be impeded by the cost of supporting agriculture (which consumes resources and engages people in traditional, not novel, tasks).
Europe's laudable ambition also to become a gleaming citadel of environmental protection (for which the European Parliament was calling the other day) may have the same effect.
One of the European enterprise's long-standing problems is that matching its ambitions with reality.
Agriculture, as it happens, has become one of the chief reasons why progress towards a wider Europe will be complicated.
As things are, the states of Central Europe are best placed to export farm produce (and steel) to the West.
Although the EC has signed association agreements with these countries, it is prevented by the political need to protect its own industries from allowing imports of these commodities on the scale that would be possible.
The result is that Central Europe mist make do with financial subsidies instead, as if it were an undeveloped region of the world.
That is demeaning, uneconomic and a recipe for disaffection.
The obvious danger is that if they are unable to earn a living for themselves, that states of Central Europe will slump back into their earlier condition.
Is that what the brave new European Union wishes?
If not, the ratification of Maastricht should not be taken as a signal that the Europe of the past few decades has been assured of perpetuity, but rather that a redefinition of its ambitions is overdue.
One uncomfortable consequence of the seemly recognition of the needs of the rest of Europe is that there is no way of bringing Central Europe into the fold except by letting its members earn their keep, implying a repartition of the industrial division of labour now in place.
That will mean change not only in the East, but also the West of Europe.
The current problems of Germany are an object lesson.
The constant repetition by European politicians in the past few months that Maastricht will not necessarily mean change may have been tactically expedient, but squaring the circle in which the EC has snared itself requires, on the contrary, upheaval, not just in research policy.
OPINION
Sob tale of the rich
The decline of a London insurance market is a warning and a threatened cost to everybody.
LLOYD'S of London is a unique means by which people anywhere may buy insurance against risks of all kinds, but which has now fallen foul of risks its members should themselves have anticipated.
Lloyd's has hitherto relied on the promises of rich men and women to meet exceptional insurance losses from their personal resources; in recompense for this personal risk, they were often handsomely rewarded.
But in the past few years, the insurance market has been losing money, and it has turned out that the resources of even the richest people are less than infinite.
Lloyd's lays many of its troubles at US doors.
During the past decade, the zealous attempt to rid public buildings in the United States of the flimsiest traces of asbestos, and the personal damage claims that have simultaneously multiplied, have been financed to a substantial degree by the people at Lloyd's.
So too have the damages awarded by US courts against manufacturers found to have polluted the environment to a degree that has recently been decreed unacceptable.
The implicit complaint is that many British insurers would not have been saddled with these claims if US law and regulation had not been  retrospectively changed.
There may be something in the view that many environmental regulations in the United States are over-stringent, but an insurance market that boasts of insuring all risks, and which is paid for doing to, cannot logically complain when unforeseen risks are realized.
Maybe it has been charging too little for the uncertainties involved, but that is another matter.
The real difficulty now is that risk insurance will be harder to buy, and will certainly be more expensive.
And that will put a brake on industrial well-being.
For companies need protection from potential catastrophe if they are to remain productive, even in business.
Implausible though it may seem, insurance companies also provide a social service.
That is why it is important that the recent troubles at Lloyd's have more to do with its constitution and management than with the riskiness of the world in which the organization operates.
For more than a decade, Lloyd's has been riven by scandals of various kinds; professionals employed to assess the riskiness of insured risks and apportion them among the rich risk-takers have been discovered to have salted funds away on their own account.
Others have specialized in insuring only the more hair-raising elements of their colleagues' risks, which is a recipe for wealth when times are good and bankruptcy at other times.
Yet in 1982, Lloyd's won exemption from the Financial Services Act by which the British government sought to protect ordinary people from financial charlatans, accepting instead that Lloyd's should regulate itself (which it has failed to do).
The proposed remedy is that the liability of those carrying the risk in the insurance market should not be unlimited and that capital should be recruited from corporations as well.
What that means, sadly for us all, is more expensive insurance.
NEWS
International gamma-ray project favoured for ESA launch in 2001
London .
A new gamma-ray observatory satellite to be launched in 2001 seems likely to become the first joint space mission between European, Russian and US space agencies.
The satellite would cost an estimated £400 million (US$600 million) and would be between ten and 50 times more powerful than the two gamma-ray observatories now in orbit.
As currently planned — and providing formal approval is obtained from each of the three space agencies — the project, known as INTEGRAL (for International Gamma-Ray Astrophysics Laboratory) would be run by the European Space Agency (ESA) as the second ‘medium-sized’ project in its science programme, Horizon 2000.
One of the two main instruments, a germanium spectrometer, would be provided by the US National Aeronautics and Space Administration (NASA), which has also been asked to make available data-receiving facilities at two ground stations.
The satellite would be launched by a Russian Proton rocket from the Baikonur Cosmodrome in Kazakhstan, in return for which scientists from the Russian Space Research Institute (IKI) would have access to the data.
INTEGRAL was given top ranking over three rival projects last week at a meeting in Paris of ESA's space science advisory committee.
The choice followed a two-day meeting at the headquarters of the United Nations Educational and Scientific Organization (UNESCO), in which details of the four projects were presented to 250 space scientists.
In addition to the germanium spectrometer, INTEGRAL will carry a caesium iodide imager and two smaller instruments, an optical transient camera and an X-ray monitor.
The overall goal is to improve both the angular and spectral resolution of gamma-ray observations.
These will be made primarily in the plane of the galaxy, although observations will also be made of gamma-ray sources from outside the galaxy.
INTEGRAL will follow up observations from the Russian GRANAT mission (which uses the French gamma-ray telescope SIGMA) launched in 1989 and now coming to the end of its mission, and the US Compton Gamma Ray Observatory, which was launched by the space shuttle in April 1991.
The advisory committee's recommendation now goes to the ESA's science programme committee, representing the scientific communities of its 13 member states, which meets in early June.
The committee must also take into account the project's international and financial implications.
INTEGRAL appears to score highly in all three areas.
The international cooperation in seen as a way to reduce costs.
The US contribution of $70 million is likely to come from a NASA programme to support international collaboration, and the overall cost to the European agency is estimated to be within the budget of £260 million.
ESA's medium-sized missions are intended to complement the four larger missions that form the cornerstones of its scientific programme: the SOHO solar observatory, the XMM observatory for X-ray astronomy, a far-infrared space telescope and the cometary probe Rosetta.
Thirty proposals were submitted for the 2001 launch, of which four were selected for further study.
Last week's meeting in Paris presented the results of a two-year study of the competing projects.
INTEGRAL's main competition in the astronomy field is a project called PRISMA, an all-European missing to study the internal structures of stars by looking at seismic oscillations on stellar surfaces.
The two other finalists are MARSNET, a scheme to set up a network of seismological and meteorological stations on the surface of MARS, and STEP, a set of instruments designed to provide accurate tests of the principle of equivalence between inertial and gravitational mass.
Although all four projects received high scores for their scientific content, PRISMA appears to have suffered from the high costs of its proposed European launch, and MARSNET from uncertainty over US plans for the study of the planet, while STEP was seen by some participants as requiring further technical refinements.
Each will still be eligible for submission for the next ‘medium-sized’ mission to be launched in 2003.
British space officials do not expect any major difficulties in finding the money for INTEGRAL, because ESA science programmes are paid out of the subscriptions of member states which are calculated several years in advance.
There is a similar feeling in Bonn that ESA's science programmes provide good value for money, in contrast to high-prestige projects such as the planned spaceplane Hermes and participation in the US space station.
The collaboration also coincides with the desire of Germany's new science and technology minister, Matthias Wissmann, to make use of ‘existing investments’ for research.
David Dickson 
Study proves Iraq used nerve gas
Washington .
An analysis of soil samples from a Kurdish village has proved for the first time that in 1988 Iraqi armed forces used nerve gas as well as mustard gas in air raids against civilians.
Physicians for Human Rights, based in Boston, and the New York group Human Rights Watch brought the samples back last summer and arranged for their analysis at the British chemical weapons laboratory at Porton Down, Wiltshire.
The group says that the survival of evidence over a period of years defies expectations and improves the prospects for enforcement of the recently signed international Chemical Weapons Convention (see Nature 361 , 105; 1993).
‘This is the first time scientists have been able to prove the use of chemical weapons, and specifically nerve gas, years after the event’, says Eric Stover, director of Physicians for Human Rights.
The study found a small number of parts per billion of isopropyl methylphosphonic acid (iPMPA) in soil from bomb craters in the village of Birjinni in northern Iraq near the Turkish border.
Alastair Hay of the University of Leeds, who chairs the group's chemical weapons group, says that iPMPA is a ‘unique fingerprint’ of the nerve gas GB, often known as Sarin.
The air attack occurred in August 1988 during a campaign against the Kurdish separatist movement.
Hay says that Sarin would normally degrade in an open environment in a matter of hours.
There are no published studies on the survivability of its degradation products, although some classified work may have been carried out.
The human-rights group hopes that the findings will discourage the proliferation of chemical weapons because of fear that their use will be discovered.
‘The message for those involved in chemical warfare is that they might be detected, even years afterwards’, says Hay.
However, as many as two dozen countries are still thought to be developing chemical weapons, and the treaty will not come into force until 1995 at the earliest.
Colin Macilwain 
Wolfson Foundation's policy on animals angers other British medical charities
London .
British medical charities are concerned that a refusal by one foundation to provide grants for experiments involving animals may stimulate the anti-vivisectionist movement and increase the difficulties facing medical researchers who consider such experiments to be essential.
The criticism follows complaints to the Physiological Society by some of its members about guidelines issued to potential grant applicants by the Wolfson Foundation, one of Britain's leading sponsors of university research.
These state that the trustees of the foundation ‘do not normally make grants for…research involving animals’.
There is particular concern about the impact of this restriction on applicants for the foundation's ‘intercollated’ awards, which enable medical and dental students to carry out a research project leading to a BSc.
Barbara Rashbass, director of the foundation, says that charitable bodies are entitled to decide how to allocate their funds and that unsuccessful applicants are free to apply for support from other medical research charities, none of which has similar restrictions.
Last week, the Physiological Society decided that it would not pursue the matter at the present time, and would accept the foundation's argument.
The Wolfson Foundation spends about £17 million a year on medical research (compared with a total of about £400 million spent by the other charities).
The guidelines are widely believed to reflect the personal antipathy towards animal experiments of the foundation's chairman, Lord Wolfson.
The foundation does support research using animal tissue.
And it has also provided financial backing for research into alternatives to the use of animals.
Thus it has supported research in the department of human morphology at the University of Nottingham into the possible use of the fluorescein leakage test as an alternative to the Draize rabbit eye irritancy test to measure the potentially harmful effects of cosmetics.
Although there is little criticism of the foundation's decision to support such research, other medical charities argue that its explicit exclusion of research on animals may encourage other smaller research-funding bodies to take a similar stand, a move that could make life increasingly difficult for medical researchers.
The larger charities — which have recently formed a group known as Research for Health Charities to respond to criticism of the use of animals in experiments — also feel that Wolfson's stand undermines their attempts to establish a united front.
‘I an sorry that a well-respected body such as the Wolfson Foundation should have taken this step’ says Major General Leslie Busk, director-general of the British Heart Foundation and chairman of the Association of Medical Research Charities.
The association of Medical Research Charities.
The association, all of whose members are required to sign a statement pledging their support for the use of animals in research, is planning to raise the issue with the Medical Research Council.
Wolfson is not a member of the association and, with its current policy, would not be eligible for membership.
Other charity administrators say that Wolfson's policy could help to legitimize the campaigns of anti-vivisectionists and could be seen as implicit criticism of the efforts of laboratory researchers.
‘If you happen to be using animals, this is like a kick in the teeth’, says one.
‘If all the charities followed the Wolfson's example, medical research should come to a standstill.’
Such arguments are dismissed as overreaction by those campaigning for a significant reduction in the use of animals in research.
‘I would prefer to see the medical charities emphasizing the large amount of research they sponsor into alternatives to the use of animals, rather than making a big fuss out of this’, says Michael Balls, chairman of the trustees of the Fund for the Replacement of Animals in Medical Experiments, and until recently professor of medical cell biology at the University of Nottingham.
Further support comes from Sir David Phillips, chairman of the Advisory Board for the Research Councils and a trustee of the foundation.
‘Wolfson's policy could be seen as an inducement to people to devise experiments which do not involve the use of animals, and almost everyone would agree that that is a good idea’, he says.
David Dickson 
Glowing report may not avert closure of Dutch primate centre
Munich .
Europe's leading primate centre, which pioneered bone-marrow transplantation in the 1960s and which now supplies animals for AIDS research, faces a financial crisis that could lead to closure.
Although an independent report on its activities has recommended continued support for the centre at Rijswijk in the Netherlands, many fear that the Dutch government will adhere to an austerity programme that provides no money to operate the centre.
Cuts last year in the Dutch health budget were passed on to the national applied research organization (TNO), whose own grant is being halved by 1994.
In response, TNO told the primate centre and its associated radiobiology and immunology research institute ITRI to make good a deficit of around DF13.7 million (US$2 million).
The prospect that the primate centre could be closed raised such a storm of protest that the government agreed to delay a decision until after the completion of a report on the centre's performance by the Royal Netherlands Academy of Arts and Sciences.
The international investigating committee, chaired by Jon van Rood, professor of immunohaematology in Leiden, has strongly recommended that the centre and its research facilities should be preserved despite the cost.
It judged the centre, which carries out work on AIDS, infectious disease and transplant research, to be a world-class laboratory and says that it offers unique facilities to researchers in Europe such as a rhesus colony for which the microbiological status is known and typed for major histocompatibility complex, essential for transplantation studies and infectious disease research.
It also has the only breeding colony in Europe for chimpanzees.
An estimated 5,500 primates were used in Europe last year, according to the report, mostly by the pharmaceutical industry.
The primate centre received requests for 300 macaques and 14 chimpanzees, many of which could not be supplied because of a policy that it should be primarily a supplier for TNO institutes.
The report proposes a new structure for the centre, which it says has been underused as a TNO institute.
It recommends that it should be taken over by the academy because of its focus on basic research and that it should at the same time establish links with a university.
It also recommends that the European Communities (EC) adopt the centre as a ‘Grande Installation’ under their Human Capital and Mobility scheme, coordinating its activities with those of the other two European primate breeding and research units, in Göttingen, Germany and Strasbourg, France.
The EC has shown interest in the idea, which could provide the centre with DF11.8 million per year.
Transferring the centre to the academy, however, will not solve the fundamental problem of tightening government budgets.
The academy has already said that it will consider the proposition only if the government provides continuing support to operate and maintain the facility, which the report estimates would require DF17 million (US$4 million) a year.
If that is done, according to the report, the centre could attract at least DF110 million a year from national and EC grants, industrial contracts and the sale of animals.
But the government is unlikely to want to foot the bill.
TNO vice-president Arthur Rörsch says that the government is pleased that the centre is valued so highly but that, ‘against a background of budget cuts, with health budgets particularly under fire, it is not really easy to be optimistic’.
Minister of education and science Jo Ritzen has promised an answer this month.
If the government decides against underwriting the venture, then the centre must decide the fate of hundreds of primates.
Some of the chimpanzees could be transferred to zoos, but the chimpanzees infected with AIDS must be cared for in special facilities until their deaths.
Alison Abbott 
These primates face uncertain futures.
Japan adds supercomputers in one-time boost to budget
Tokyo .
Research institutes of the Ministry of International Trade and Industry (MITI) and the Science and Technology Agency (STA) will receive supercomputers in a supplementary budget designed to help pull Japan out of recession (see Nature 362 , 381; 1993).
Many Japanese national universities will also benefit by getting much-needed Local Area Networks (LANs) to link on-campus computers.
But an attempt by the science-related ministries and agencies and the ruling Liberal Democratic Party (LDP) to create a new budget category for these items and to crack the rigid ceiling on outlays for science has been defeated by the Ministry of Finance.
The supplementary budget, details of which are expected to be approved by the cabinet in the next few weeks, will include ¥25–30 billion (US$225–$270 million) for 11 supercomputers and mini-computers to be shared among six science-related ministries and agencies (see table).
This is three or four times the number normally purchased each year by the government.
MITI will receive three, including one for its new National Institute of Advanced Interdisciplinary Research in Tsukuba which will be the centre for a large-scale government-industry project on nanotechnology.
STA also gets three and the Ministry of Education, Science and Culture gets two for national universities or university-related institutes.
The computer for the National Cancer Center will form the central database for an online national network of imaging data on cancer patients.
The supplementary budget is also said to include several billion yen to set up the network, part of the next ten-year cancer research programme to begin next year (see Nature 361 , 672; 1993).
The Ministry of Agriculture, Forestry and Fisheries is expected to use its supercomputer for the rice genome project at the National Institute of Agrobiological Resources in Tsukuba, but the ministry has yet to reveal its plans.
And the Ministry of Posts and Telecommunications will get a supercomputer for its single institute.
Supercomputers are not the only scientific elements in the supplementary budget.
About ¥8 billion will be set aside for on-campus LANs for 20–30 national universities, according to a government official involved in the negotiations with the Ministry of Finance.
A few tens of billions of yen will also be spent on new buildings and facilities for the science-related ministries and agencies.
But the science-related ministries and agencies failed to classify such items as ‘new social infrastructure’ outside the present budgetary system.
Despite the backing of the LDP, in particular Hiroshi Mitsuzuka, chairman of the ruling party's policy research council, the Ministry of Finance argued successfully that such changes require rewriting Japan's finance laws.
The LDP and science-related ministries and agencies are now trying to ensure that next year's budget includes provision for items such as a high-capacity national backbone network for linking computers in government institutes and universities.
The ruling party is considering introducing a bill to allow national construction bonds to be used.
Alternatively, it might use funds from the sale of government-held shares of Nippon Telegraph and Telephone, Japan Tobacco and JR railways as well as earnings from a proposed system of betting on football matches.
Although government officials are confident of somehow finding a way next year to pay for the national backbone network, they are sceptical of the LDP's ability to break through the ceiling on overall spending on science.
The party's most likely recourse is a face-saving compromise with the Ministry of Finance that does not alter the status quo.
David Swinbanks 
Research ministers approve Framework
Munich .
The council of research ministers of the European Communities (EC) last week approved the structure and the proposed ECU13.1 billion (US$15.8 billion) budget for the EC's fourth framework programme, which will fund research from 1994 to 1998.
The positive response raises hope that the final text of the programme could be approved by the commission, council and parliament by the end of the year (see Nature 362 , 778; 1993).
EC research commissioner Antonio Ruberti also announced preliminary plans to improve programme management by simplifying and decentralizing procedures.
Details are not yet available, but one proposal is to establish regular deadlines for grant applications and to require for the first selection round a short summary of project proposals rather than the full 20-page application, as is now the case.
A.A. 
Stolen report alleging discrimination and favouritism in NIH office raises furore
Washington .
An independent report supporting allegations that senior managers in the procurement office at the US National Institutes of Health (NIH) have systematically discriminated against African-Americans and have operated a ‘social network’ in which sexual favours are exchanged for career advancement has become the focus of a campaign by two civil rights organizations to improve conditions for blacks at NIH.
Their anger has been fuelled by the theft of the report from the consultant's office on the morning after it was submitted to NIH last autumn — including the codes intended to provide anonymity for witnesses who supplied the consultant with evidence of misconduct.
The report was prepared by Benjamin Alexander, a former senior US government official and a former university president who was trained as a chemist.
Alexander was hired to investigate the validity of complaints of discrimination and favouritism dating from 1987 against the office of acquisition management within the NIH director's office.
In the course of the five-month study, Alexander interviewed several dozen witnesses who, under the promise of anonymity, described an ‘ole boy network’ of preferential hiring and promotion, a pattern of unfair performance ratings and the use of inappropriate educational requirements for available jobs.
Those conclusions were reached on the basis of answers from a control group of employees in the office that matched the administrative levels of those accused of acting improperly and the racial composition of those making the allegations.
Alexander found that the control group shared the views of the complainants and disagreed with those supporting the managers in 18 out of 20 areas.
On 19 September, the day after Alexander submitted the report to Diane Armstrong, director of NIH's Office of Equal Opportunity, thieves broke into the offices of Alexander's company, Drew Dawn & Associates of Silver Spring, Maryland, and stole the report along with the codes.
The local police found no incriminating evidence at the scene and they believe that the theft, which did not involve office equipment and other items of value, was done by professionals.
Last week, the Montgomery County (Maryland) chapter of the National Association for the Advancement of Colored People (NAACP), tired of waiting for the police to solve the case and for NIH to comment on the report, held a press conference to express its outrage at the events of the past seven months and to demand that the lives of those affected ‘be made whole’.
The group, along with the NIH chapter of Blacks in Government (BIG), wants the Federal Bureau of  Investigation to help solve the break-in and wants NIH to act promptly to resolve the complaints of discrimination and to punish those responsible.
The civil-rights groups are particularly upset with NIH's response to the news that the confidentiality of those who provided information to Alexander had been breached.
Within 24 hours, NIH security police were given the list of names and told to call each person to tell them of the theft.
NIH officials say only that the witnesses were notified ‘in the most expeditious way possible’; members of BIG insist that a telephone tree was created in which one person was told to call several others.
In any case, the names of those who had provided confidential information were soon common knowledge within the relevant offices at NIH.
NIH says that criticism of its response to both the report and the theft is unfair.
Armstrong says that the report has convinced NIH officials that ‘there are some serious problems in the office of acquisitions management’ and that, as a result, NIH has hired another consultant ‘to advise us if there is information that we can act upon’.
The Alexander report was revised four times before it was acceptable, according to Armstrong, who said that the final version ‘does not contain any sworn statements or evidence that could provide the basis for specific actions against anyone’.
In the interim, she adds, NIH has scheduled training on sexual discrimination and cultural awareness for the entire division.
As for the breakin, Armstrong says that the report ‘was not our property’ at the time it was stolen, so it is not clear what role NIH should play in an investigation.
The civil rights group say that NIH is not doing enough.
‘We are not happy with what is going on’, says Vincent Thomas, president of the BIG chapter at NIH.
‘We've said all along that the key is managerial accountability, and until NIH is willing to punish people for their  behaviour , nothing is going to change.’
Jeffrey Mervis 
Russia gets first batch of emergency grants
Moscow .
With 100 $500 grants already on their way to scientists working in Russia, the International Science Foundation (ISF) has taken the first step towards meeting its goal of helping to preserve research in the former Soviet Union.
The foundation, created last autumn by US financier George Soros, expects to make a second set of emergency grants to as many as 20,000 scientists after the 31 May deadline for applications.
ISF officials are selecting researchers on the basis of the quality and quantity of their published output (see Nature 362 , 95; 1993), although only a portion of the 35,000 who are eligible are applying for money to keep their laboratories going.
Scientists may also be reimbursed for the cost of attending international conferences.
Segments of the Russian press have criticized the ISF and point to the modest size of ISF grants as evidence that the foundation is trying to ‘buy’ Russian scientific brainpower at a fraction of its real value.
Others are grateful for the additional money but believe that the ISF should work more closely with government agencies.
Aurab Yakobashvili, deputy minister of science, says that the lack of such coordination makes it more difficult for the government to plan for the future of Russian science.
Still others have raised questions about the criteria adopted by the ISF for judging applicants.
Alex Goldfarb, executive director of the ISF and formerly a biochemist at the Engelhardt Institute in Moscow, sees the gap between the number of applicants and the number of eligible scientists as a sign that many of his colleagues have left the country for greener pastures in the West or have taken up another occupation.
The imminent loss of perhaps a third of the cream of Russian science, he says, demonstrates the importance of taking rapid action, by whatever means available, to help those still willing and able to do science.
In addition to the emergency grants to individuals and small teams, the foundation has also given $500,000 to the Komarov Botanical Institute in St Petersburg for the preservation of one of the world's largest herbaria, Some $80,000 was awarded to a popular science magazine Chemistry and Life and $4,000 was given to a special high school in Moscow that maintains high standards for teaching physics and mathematics.
Soros has promised to spend a total of $100 million on strengthening science in the former Soviet Union.
Boris Saltykov, the minister of science, believes that Soros's commitment has spurred Western governments into doing more than they would have done otherwise.
‘I can't prove it’, he says, ‘but I believe that the activities of George Soros helped to push the Clinton administration into taking effective steps to assist Russian science’.
Vladimir Pokrovsky 
Clinton's plan for ‘green car’ faces bumpy research road
Washington .
The Clinton administration is likely to have considerable difficulty in carrying out a proposed joint ‘clean car’ research and development programme with the major US car manufacturers.
Diminishing environment returns from refinements to petrol-driven cars and uncertainty over which alternative power sources to pursue will complicate the plan, according to motor industry experts who last week testified before the clean air and nuclear regulation subcommittee of the Senate Environment and Public Works Committee.
In addition, the industry's preference for an inexpensive way to meet tougher environmental regulations leads to different research goals from those of the administration.
There is also concern that too much collaboration between US car makers could blunt their ability to compete in global markets.
‘If the companies all draw from the same research and technology base, we may inhibit technical and product innovation, just when it is most needed’, Albert Sobey, an industry consultant from Michigan, told the subcommittee.
In February, US President Bill Clinton announced his interest in working with the automotive industry on ‘clean car’ technology.
Since then, senior representatives of General Motors, Ford and Chrysler have met several times administration officials led by the president's science adviser, John Gibbons, and Deputy Secretary of Commerce-designate John Rollwagen.
But whereas the administration likes to talk about a greener and more internationally competitive car industry, Detroit wants to spend any available support on overcoming bread-and-butter engineering obstacles in complying with environmental legislation.
Laws already passed in California and under consideration in other states would require car makers to reduce each year the hydrocarbon, carbon monoxide and nitrogen oxide emissions of their fleets.
The manufacturers would like to channel any new government money through their existing collaborative research clearing house, the US Council for Automotive Research (USCAR), which coordinates joint research in ten areas, ranging from battery technology to passenger safety.
But only one, the Advanced Battery Consortium, has so far attracted significant federal funding, with half of its four-year, $260-million programme being financed by the Department of Energy.
Detroit research managers see the other USCAR consortia as obvious avenues for additional federal support.
Joseph Colucci, and executive director of research at General Motors, says that a consortium founded last summer in low-emissions technology offers the best match between the administration's goals, the capabilities of the federal laboratories and the industry's priorities.
‘It would be very supportive to have the federal laboratories working on this issue’, he says.
The low-emission consortium deals with technology that will cut petrol engine emissions to meet the tightening regulations.
Existing catalytic converters can produce extremely low hydrocarbon and carbon monoxide outputs only in optimal conditions.
The challenge is to develop converters that will do this at once from a cold start and for the life of the vehicle.
Federal support could help Detroit to develop cleaner emission-control systems.
By focusing on this and other problems that USCAR believes are well-suited to collaborative research, the administration would avoid disrupting current plans by Detroit.
But it would also miss the chance to coordinate efforts in the most strategically significant areas — such as engine technology — which the ‘Big Three’ have purposely excluding from USCAR.
Gibbons believes that the Department of Energy weapons laboratories can play a significant role in strengthening the US car industry.
But in some areas of apparent synergy, such as materials technology, the motor industry's requirements for cheap mass production differ from the military's emphasis on small-volume, high-quality manufacturing in which cost is secondary.
In the meantime, Detroit hopes to get some help in tackling the research problems it faces in dealing with environmental regulation and is counting on better relations with Washington.
‘In the past, the industry and the government have had an adversarial relationship in this area’, says Colucci.
‘That should change, but it is only what the Japanese have been successfully doing for years.’
Colin Macilwain 
Research councils want more control of European projects
Strasbourg .
A ‘club of national research councils, researchers and institutes’ to serve European research interests being neglected by the European Communities (EC) has been recommended by participants at a meeting organized last month by France's basic research agency, the CNRS (Centre National de la Recherche Scientifique).
They would also like to have an independent body to evaluate European science and special one-year grants to help reintegrate post-doctoral students returning to their country of origin after working elsewhere in Europe.
The recommendations, which reflect concern that the EC is accruing too much power over research, are being presented to France's new research minister, François Fillon.
Fillon has pledged to support efforts, particularly those by the CNRS, to increase the control of individual countries over international research collaborations.
Widely known for his anti-Maastricht politics, Fillon says that he wants to avoid ‘getting bogged down at the EC level…whose cumbersome procedures put a damper on the research community’.
The proposals are also being presented to national research councils, which have recently discovered that a growing proportion of Europe's research budgets come from the EC's research commission.
Meeting for the first time in Bonn in January (see Nature 361 , 576; 1993), the councils are trying to make EC-supported international research programmes more efficient and have scheduled a meeting in London in October to discuss the issue.
CNRS was host for the meeting partly because of its desire to take a leading role in Europe's changing research landscape.
The agency is trying to decentralize French research by moving half hit laboratories to the regions (see Nature 356 , 373; 1992), and taking a bigger role in international research politics would help to fill the vacuum in Paris.
The CNRS has been less successful in setting up an international company called Euroresearch.
Under discussion for two years, the company is supposed to allow France, Germany and Spain to administer EC grants to their research institutes and postdoctoral students.
One problem it would address is the proportion of grants — as much as 10 per cent— that are lost to currency conversions because French research institutes cannot hold foreign currencies.
But the plan has foundered in the EC's demand for a general liability clause, a feature that has proved difficult to sort out on an international level.
Alison Abbott 
International facilities said to boost national economy
London .
A report published by the UK Office of Science and Technology (OST) challenges the traditional view of the Treasury that hosting an international scientific facility has little long-term impact on the economy of the host nation.
The report, prepared by the consulting group Segal Quince Wicksteed of Cambridge, says that international facilities can boost the long-term economic fortunes of companies that supply them with equipment and services, act as a magnet for other high-technology companies and generally raise the public profile of science.
Although it is difficult to calculate the extent to which the funds represent a net gain to the economy, according to the report, the income and value added tax paid to the government by foreign scientists and other employees represents a clear economic gain.
The Treasury had accepted the idea that international facilities are a cost-effective way to support large-scale research, but is has been reluctant to provide financial incentives to attract such facilities to Britain, following the example of France and Switzerland, because of doubts that they contribute to the country's economic wealth.
This conclusion is challenged by an analysis of spending on equipment and salaries at four international scientific centres: the Joint European Torus (JET) at Culham, England; the European Laboratory for Particle Physics (CERN) at Geneva, Switzerland; and the European Synchrotron Radiation Facility (ESRF) and the Institute Max von Laue-Paul Langevin (ILL), both in Grenoble, France.
Overall, the analysis found that between 40 and 70 per cent of the money to operate the facilities was spent in the host nation.
While admitting that some of this funding could have displaced other funding sources — for example, technicians in short supply would likely have found alternative work — the authors of the report, Robin Brighton and Virginia Aschas, say that they also found clear evidence of economic gain.
For example, in many cases local companies providing scientific equipment and other services were able to benefit from reduced delivery costs, personal contacts and a more detailed knowledge of a facility's needs.
Those factors give them a competitive advantage in bidding on contracts.
For scientists, the main economic advantage of having a facility in their home country is the reduced cost of travel and accommodation.
Brighton says that it is impossible to quantify the likely gains from an international facility, because a number of important factors (such as displacement costs) depend heavily on prevailing economic conditions.
He says, however, that the study ‘produced enough evidence to suggest that the potential economic benefits [from an international facility]should be studied very carefully’.
OST officials hope that demonstrating these likely gains will make it easier to obtain Treasury backing for any future bids to locate scientific facilities in Britain.
At the top of the list is a European Neutron Source, a successor to the Nuclear Structure Facility at Daresbury that was closed down last month, which many physicists would like to seen built at the Science and Engineering Research Council's Rutherford Appleton Laboratory.
David Dickson 
Swiss look for ways to reap benefits of next Framework
Basel .
Swiss researchers, who lost access to millions of dollars from the European Communities (EC) when voters decided in December to delay the process of joining the EC, are waiting for the reaction to their government's attempt to participate at some level in the EC's fourth Framework Programme, which begins next year.
The referendum blocked Switzerland's entrance into the European Economic Area (EEA), the first step towards full EC membership.
Switzerland already participates in some 200 individual EC projects, paying researchers the costs normally paid to full members, which average SFr30 million (US$20 million) a year.
The change in status would have allowed Switzerland to participate fully in broader Framework programmes, although it would not yet have been eligible for direct financial support.
Swiss researchers would also have been allowed to join programme committees and to serve as project leaders on individual projects.
The vote deprived Max Hess, an immunopathology professor at the University of Berne, of a promised position on the Biomed programme advisory commission.
A third of the 140 projects in this programme have Swiss participation, he says.
‘The worst thing is that we get no information about what is going on’, he says.
Fred Paccaud, a professor of social medicine from the University of Lausanne, held a special exemption to serve as a project leader in the MR4 programme, a predecessor of the Biomed programme, but his proposal to continue working on Biomed was rejected in January.
‘I was told I am no longer entitled to apply’, he says.
Tim Guldimann, head of economics and foreign policy in the Swiss science policy coordinating group, says that all sides suffer from the exclusion of Swiss researchers.
With three per cent of its gross national product devoted to research and development, the country ranks with Japan as the heaviest investor of science in the world.
The Swiss government was prepared to pay SFr477 million to participate in the fourth Framework programme and had encouraged researchers and industry to submit proposals.
These proposals are now being used as a negotiating tool to demonstrate Switzerland's strength in particular areas.
The EC Council of Ministers needs to establish a general mechanism for negotiating with Switzerland now that it is the only country in Europe that has neither EC nor EEA status.
Last week it agreed in principle that Switzerland could take part in projects where its participation would strengthen the research effort.
Oliver Klaffke 
Germany revamps system to fund medical research
Munich .
German research minister Matthias Wissmann last week announced a major restructuring of medical research funding that for the first time establishes independent external refereeing for projects in universities and their associated clinics.
Wissmann hopes that his plan, called ‘Health Research 2000’, will eliminate inefficiencies within German biological research (see Nature 357 , 182; 1992) and strengthen the weak links between basic medical and clinical research.
Some DM2.15 billion (US$1.36 billion) has been set aside by the research ministry, the BMFT, to support the first three years of the plan.
Around DM1.4 billion will be used to fund medical research in the large national research centres, such as the German Cancer Research Centre in Heidelberg, and DM750 million will be spent on individual projects in other research institutes or universities.
The BMFT hopes that new methods for allocating money will encourage a restructuring of university research which, it believes, is in urgent need of reform.
Although university researchers can apply for BMFT project grants, universities receive their basic research money from their local Länder governments.
This money is not usually distributed on a truly competitive basis, a process that is widely criticized.
Clinical research is carried out mostly at university clinics, which receive 80 per cent of their cash from medical insurance companies and the rest from the Länder .
Although this 20 per cent is intended for teaching and research, much of it is used to make up deficits in general health-care budgets.
Inefficiency in the clinics and the universities is compounded by the fact that research groups tend to be small and isolated, shunning collaborative links with other small groups, either locally or nationally.
Wissmann wants to change this.
The BMFT is allocating DM250 million over the next eight years to support six to eight interdisciplinary research centres.
These centres will be composed of several research teams, usually local, that focus on defined collaborative projects.
The projects undertaken by the centres will be assessed by a formal refereeing systems of international standards.
Most of the cost will at first be picked up at the federal level, but the BMFT will gradually reduce its share of financial support over the eight years.
The clinics will have to make up the difference with funds from the Länder , thereby ensuring that the money is used for its intended purpose.
Remaining funds will be allocated to individual projects, a third to public health research and two-thirds to clinical research.
Joint applications will be shown preference, as well as projects that ‘strengthen cooperation between basic and applied [clinical]research’.
Alison Abbott 
Whaling meeting expected to leave issue unresolved
Tokyo .
Despite years of debate and scientific analysis, next week's meeting of the International Whaling Commission (IWC) is unlikely to resolve the fate of commercial whaling.
Anti-whaling forces at the annual meeting in Kyoto do not have sufficient support for a French proposal to create a whale sanctuary in the southern ocean which would prevent the Japanese from resuming commercial whaling.
Neither does Japan have the votes to achieve a resumption in commercial whaling, despite evidence from the IWC's scientific committee that there are sufficient numbers of minke whales.
In 1982, the IWC introduced a moratorium on commercial whaling on the basis of incomplete data on the size of the population and whether it was sufficient to sustain commercial activity without risk to the species.
By 1991, the IWC scientific committee had developed a revised management procedure (RMP) to provide adequate safeguards and had estimated the population of Antarctic minke whale at 760,000, well above the number needed to resume commercial whaling (see Nature 357 , 532; 1992).
But the IWC has yet to decide whether to continue or end the moratorium, preferring to ask more questions of the scientific committee regarding the management procedure (see Nature 358 , 99; 1992).
‘My guess is the commissioners will be desperate for a reason to postpone a decision’, says one scientist participating in the closed scientific committee meetings that have preceded next week's plenary session.
‘If they voted to lift the moratorium, they'd get lynched at home.’
A proposal by France to create a whale sanctuary in the southern ocean below 40° S may provide grounds for delay.
The Japanese are confident they can muster the necessary 25 per cent minority to block the French proposal if a vote is called.
Instead, those against whaling a likely to push for further discussion of the proposal.
The Japanese are in no hurry to push for resumption of commercial whaling.
‘Even if the IWC permits us to resume, commercial whaling [in the Antarctic]has died’, says Japan's IWC representative, Fukuzo Nagasaki.
‘It would take two or three years to develop a concrete plan.’
Japan sees its fight to preserve commercial whaling as a test case for its exploitation of other commercially more important marine resources, for example the bluefin tuna in the Atlantic.
A defeat in a situation where sustainable management of the resource seems possible could threaten other resources where there is greater cause for concern about depletion.
David Swinbanks 
Montagnier to assess French AIDS effort
Paris .
Luc Montagnier of the Pasteur Institute in Paris has been appointed to lead a panel reviewing France's effort against AIDS.
The breadth of Montagnier's assignment — to examine social and public health issues as well as current research — has prompted speculation that the government may appoint him to a new position of national AIDS coordinator.
‘The government considers that more needs to be done’, says Montagnier, whose preliminary report is due in August.
Although he says that he intends to keep an open mind, Montagnier has criticized the government in the past for not supporting more research outside the mainstream and for not paying more attention to Africa.
In January, he and Federico Mayor, director general of UNESCO, created the world Foundation for Research and Prevention of AIDS (see Nature 361 , 102; 1993)D.B. 
CORRESPONDENCE
Problems in Ireland…
SIR — In your leading article ‘What Brussels should do for research’(see Nature 362 , 93; 1993) you emphasized the need to increase the quality, quantity and participation in science research in countries such as Ireland.
Your article is indeed timely as it is likely that financial support by the Irish government for research will be significantly reduced from its already low level.
I believe that scientists would warmly welcome the development of new strategies that would address many of the problems that are in danger of causing major damage to the scientific base in Ireland.
This had begun to develop significantly in the past few years in terms of quality.
In addition, a number of new universities and tertiary institutes were established.
However, if these and the well-established universities and institutes are to continue and improve, what is required is an increase in support, not a major reduction.
Ireland has a well developed educational system that supplies good candidates for science and technology courses.
This is in no small measure due to the out-of-school science activities of organizations such as the Royal Dublin Society and the Aer Lingus Young Scientists Exhibition and many dedicated teachers.
Hence, Irish Young Scientists have done remarkably well at international science competitions.
At tertiary level, there is a variety of courses of a very high standard.
However, the support for research, particularly basic research, is severely limited.
This has implications for advanced training of students in techniques relevant to modern industrial processes, it severely limits the ability of lecturing staff to be fully up to date with new technologies and it limits the ability to provide ‘state-of-the-art’ technology and training at tertiary level for both indigenous and multinational companies.
The argument is often made that what is required is applied research to deliver products and processes directly to industry.
Undoubtedly, this approach is absolutely necessary but because technology advances and changes so rapidly, failure to keep up will give a very limited life to any industry.
There are many examples where what was ‘academic’ a few years ago is now very much applied.
What is required is a balance where basic and applied research co-exist and catalyse further developments.
It is also important to remember that where jobs are indeed a problem, as in Ireland, postgraduate research positions should be clearly seen as making a major contribution to employment — where else would employment cost so little and the employee undergo intensive training?
It is very good value for money.
What Brussels should do is to provide direct support for research students and equipment in the less advantaged areas provided they interact with major European collaborators.
In this way all involved would benefit.
Otherwise, there is a real danger of a one-way traffic whereby aspiring scientists will move to the well-developed centres and then will be unable to return to improve their countries of origin, due to the lack of positions and of research support.
If this happens, surely the whole concept of a truly equal and united Europe will be merely hearsay.
Richard O'Kennedy School of Biological Sciences,.
…and Spain
SIR — In addition to the problems encountered by Spanish researchers in finding a post in Spain after they have obtained a doctorate in another country (see Nature 306 , 502; 1992 %and;361 , 578; 1993), there is also the slowness of the procedure for obtaining recognition in their title.
Although this document is an essential prerequisite for the competition for posts in universities and in the CSIC (Spanish Council for Scientific Research), it can be obtained only after a bureaucratic process that takes at least one year and sometimes more than two.
During this period, researchers who have obtained their degrees abroad cannot be candidates for these posts and are therefore at an enormous disadvantage in relation to homegrown researchers.
Having applied in June 1991 for recognition of my doctorate, which I obtained in Paris, I presented my candidacy for a CSIC post as a palaeobotanist.
I was accepted as a candidate — indeed, the only candidate — for this post, but in March 1992 the jury refused to let me take the examination owing to the absence of the document recognizing my title.
Despite the decision of the president of the CSIC to readmit my candidacy, the jury, in June 1992, refused to examine me once again.
In July I was readmitted a second time, having finally obtained recognition of my French doctorate after more than a year.
Despite this, the new president of the CSIC, José Mato, decided in November to exclude me from the examination and to cancel the competition.
AS I was the only candidate, the post has been lost.
The opposition displayed by certain Spanish scientists towards the return to Spain of young researchers trained abroad often takes advantage — with the complicity of the administration — of protectionist bureaucratic mechanisms dating from the Franco period.
The time wasted in awaiting recognition of their title puts a substantial block on these young researchers at the most critical point in their career.
Maria Fernanda Sánchez Goñi University of Cambridge .
…and Italy
SIR — It has already been reported that, contrary to official policy, the scientific merits of candidates for academic promotion in Italy are not given primary consideration by the members of the judging commission, so it often happens that a loser has a curriculum vitae (c.v.) clearly superior to that of a winner.
Unfortunately, it may also be superior not only to those of the winners but also to those of the commission.
A recent experience of mine is a case in point.
I participated in a national competition called to fill eight positions of associate professor in gastroenterology.
A seven-member commission had the job of ranking the candidates on the basis of their scientific qualifications and on their performance at an oral examination consisting of a discussion of their scientific work and an academic lesson.
Before the examination, one of the commissioners told me that I would not be one of the winners because the positions had already been assigned to others who were, for the most part, assistants to the various commission members.
As forewarned, I did not win, despite a good oral presentation and the best c.v.
of all the candidates.
One of the most disturbing aspects of this experience, which I think merits emphasis, is that my c.v., rated on the basis of the number of publications (Medline Data Base), on the number of citations in the literature (Science Citation Index ), and the impact factor, was not only better than that of any of the winners but also of those of the commission members as well.
By bringing still more evidence to light, I hope I have made some contribution towards reforming an outmoded and unfair system, one that permits arrogant and unscrupulous commissioners to do as they please.
Lucio Gullo .
Progress with inventories
SIR — When regretting the lack of an inventory of life (see Gaston and Mound Nature 361 , 579; 1993), we should be careful to distinguish the complete list of organisms, that is including many as yet undiscovered species, from the smaller set of just those that have already been described.
That we have no inventory of the former is inevitable (we have neither seen them all, nor estimated with certainly how many millions there are), whereas the more surprising omission is that there is no master inventory even of those species that have been dealt with by taxonomists.
Gaston and Mound offer sensible advice: concentrate our forces on selected groups.
What they omit to mention is that substantial progress is now being made in doing exactly this, the production of master inventories for selected groups.
Be it of bacteria, protists, insects, molluscs, fish, fungi, plants or plant fossils, a series of global synonymized taxon checklists are making progress.
These are databases carefully prepared by teams of specialists to be available soon on-line or distributed on disc.
Producing master catalogues requires specialist software (for example, ALICE, ‘Linnaeus’, pcTROPICOS) and cooperative management to obtain a very wide range of taxonomic expertise.
New levels of international organization have been needed in projects such as the ILDIS Leguminosae database to turn regional monographs into workable worldwide classifications and to select a preferred reference system where taxonomists debate alternative taxonomies.
The first public taxonomic databases have experimented with the handling of alternative taxonomies (as in the US Nature Conservancy's taxonomic inventory with local variants), with attaching biological data in such a way that it can be refreshed for subsequent taxonomic changes (for example, ILDIS/Chapman and Hall Leguminosae phytochemical database), and with the use of images (such as The Plant Fossil Record ).
The adoption of standards by TDWG networking and the first steps towards ‘taxonomically intelligent’ integrity checking are enabling a second generation to appear.
Gaston and Mound may have to wait before the 750,000 insects can be handled, but IOPI is already building on the expertise of ILDIS, TROPICOS and the Australian Plant Census to create a system for the world's 250,000 vascular plants.
ETI is similarly working on protists, birds and molluscs.
If insects are being named at the rate of about 7,250 species a year and synonymized at about 1,450 a year, then these rates are within our capacity for entry into master inventories.
Resources are, however, a major limiting factor.
It remains something of a surprise that at a time when conserving species diversity is valued so highly, so little priority is given to listing the basics of what species there are to be conserved or lost.
But substantial progress is being made and the most important resources of all, expertise and know-how,are now becoming available.
Do not despair!
Frank A. Bisby Biodiversity & Bioinformatics Research Group, University of Southampton..
SIR — Gaston and Mound consider that ‘for the foreseeable future, description of all the world's species will remain impossible’.
They are right.
It is impossible to describe all the species that currently inhabit the planet given, for example, that species evolve and that there would be no obvious way of knowing when the table was complete.
The more practical challenge that faces the taxonomic community is to describe and classify all the species that are known to inhabit the world.
This task is not impossible, just very difficult.
The productivity and size of the work force involved and the rate at which undescribed species become known are clearly important factors.
The Instituto Nacional de Biodiversidad (INBio) in Costa Rica has shown that, together, parataxonomists and taxonomists can make astonishing progress towards an inventory of species.
INBio provides a model that other tropical countries are already starting to follow.
Information technology lies at the heart of almost all current projects and international initiatives such as the International Organisation for Plant Information (IOPI) are converging on common approaches.
The taxonomic literature and major catalogues of species are clear targets for conversion to machine readable form and technologies for automating this process are increasingly being used.
Gaston and Mound are right that in taxonomy, as in science in general, the drive comes from individuals.
But the community comprising these individuals is responding to the biodiversity crisis by setting priorities and defining focused research programmes.
In the major taxonomic institutions, the emphasis is firmly on collaboration and concerted action, as initiatives such as the recently formed European Museums Network emphasize.
New ways of working will take taxonomy into the fast lane, providing information on species, the components of biodiversity.
This is essential if the scientific quality of studies of biodiversity at the ecosystem and genetic level is not to be compromised.
Stephen Blackmore Natural History Museum .
SIR — I believe that if a current inventory of known insect and other species was available, descriptive work might be more efficient than the current 20 per cent wastage rate (due to synonymy).
But we are faced with a cultural problem where descriptive work goes unrewarded, and systematics and taxonomy are often treated as one.
With encouragement, a competent amateur can diagnose species and varieties without resort to parsimony analysis.
Also, the extinction of a species is not the end of the story: insect taxonomy simply transfers to archaeoentomology with attendant information loss.
Systematists need not only to communicate with nature conservationists but also to appraise them.
Otherwise, Nero will play at the computer while forests burn, and sentiment rather than science will decide the issue.
E. A. Jarzembowski Brighton Borough Council .
COMMENTARY
A revived opportunity for fetal research
Diana W. Bianchi, Merton Bernfield and David G. Nathan
One of President Bill Clinton's early decisions was to lift the ban on federally supported research on human fetal tissue.
Under what circumstances can research on this material be justified?
DURING the four decades that have followed the US Supreme Court's landmark Roe vs Wade decision, the debate over abortion has intensified to the point of ferocity, culminating recently in the murder of an obstetrician.
Much of the effort of the anti-abortion movement has been directed against clinicians and against researchers working on fetal tissue.
Thirty years ago, a national commission chaired by Kenneth Ryan produced guidelines for research on living human fetuses.
The Ryan commission assumed that the tissues of dead fetuses would be treated precisely like those of any dead individual, under the terms of the Anatomical Gift Act.
Indeed, dead fetal tissue did not become a subject of contention until 1988 when the first results of treatment of people with Parkinson's disease using fetal adrenal glands were announced.
In the minds of ‘right-to-life’ adherents, the reports conjured up the image of a massive abortion market fuelled by the need for fetal parts.
The Bush administration responded by prohibiting the use of dead fetal tissues for research of any kind in institutions supported by federal funds.
Now that the Clinton administration has swiftly and decisively reversed the ban, the time is right to discuss the scientifically and ethically appropriate uses of this material.
We believe that the use of discarded fetal tissue for research and/or therapy should be to increase knowledge of human development and/or improvement of the human condition.
Acknowledgement of the unique and non-trivial nature of the material should be mandatory.
The review panels now assembling to evaluate research proposals must ensure that human material is essential, and animal or substitute models should be used for preference when possible.
Although pre-implantation conceptuses and zygotes fertilized ex utero have contributed to our understanding of phenomena occurring during early human embryogenesis, aborted fetal tissues are more available and so provide new opportunities for clinical research, which if encouraging will stimulate more basic research in human development.
What are these opportunities?
Human development
Relatively little is known about the cellular events dictating differentiation in humans.
The use of fetal tissue can provide insights, for example the characterization last year of two distinct subsets of pluripotent stem cells from human fetal bone marrow.
Individual fetal cells with specific immunophenotypes were found to differentiate either into haematopoietic precursors or into the stromal cells capable of supporting them.
The work, if confirmed, could eventually lead to clinical application in the treatment of aplastic anaemia and malignancy.
Other research should help to show why the fetus is particularly susceptible to human teratogens.
Detailed study of placentation, fetal organs and physiology can now be more easily related to human development.
Of particular interest is the occurrence of confined chorionic mosaicism, a condition in which the fetal karyotype is discordant with the placenta, which has been described in 1 per cent of cases of chorionic villus sampling performed for prenatal cytogenetic diagnosis.
Fetal trisomy with subsequent selective chromosome loss has already been demonstrated to have significant effects on fetal well-being, and may be the initiating step in the development of disorders associated with uniparental disomy.
Material obtained from terminated fetuses will allow cytogenetic and molecular characterization of the relationships between the fetus and its extraembryonic tissues, contributing to an understanding of events preceding various clinically recognized conditions.
Human therapy
Although the transplantation of fetal neuronal tissue into the basal ganglia of patients suffering from Parkinson's disease had received media attention, the potential for fetal cells to act as a vector for gene-transfer therapy will probably have a more significant long-term impact.
Retrovirus-mediated gene transfer into transplantable cells has been proposed for several conditions, but treatment of immunodeficiency caused by adenosine deaminase (ADA) deficiency appears imminent.
Transfer of the human ADA gene into haematopoietic stem-cell precursors has been achieved in murine models, and long-term expression of the transplanted human ADA gene in rhesus monkeys has now been demonstrated.
Fetal cells are inherently preferable to their adult equivalents for transplantation because they are immunologically immature and retain the ability to proliferate.
Fetal material is transplantable either as an organ or as a cellular suspension.
Cellular transplantation can be performed either pre- or postnatally, the techniques are straightforward, cell scan be manipulated in culture, and there is the possibility of extended storage by cryopreservation.
In France, haematopoietic stem cells derived from human fetal livers (7–12 weeks' gestation) have been transplanted into four unrelated second- and third-trimester fetuses with inherited immunodeficiencies or haemoglobinopathies.
In three of the four cases, engraftment was successful, resulting in amelioration of symptoms.
Human fetal tissue has already been used to treat disorders ranging from inborn errors of metabolism to neurodegenerative diseases for all age groups.
In France, fetal liver transplantation has been performed for the past 16 years, with encouraging, if preliminary, results.
There are improvements in patients with advanced Parkinson's disease who received human fetal ventral mesencephalic implants.
Experiments can now be designed systematically to study factors related to the clinical success of these transplants.
The results are likely to have application in a range of degenerative disorders..
NEWS AND VIEWS
The dark side of molecular biology
The general enthusiasm for the fortieth anniversary of the structure of DNA should not blind us to a few blemishes on the otherwise fair face of molecular biology.
THE season of celebration of the fortieth anniversary of Watson and Crick's account of the structure of DNA, now past its peak, has been conducted, with great restraint.
Nostalgia by the great, good and elderly has been delicately muted and mostly amusing.
Equally, triumphalism of the dotty kind has not been in evidence at all; nobody, for example , has let slip the belief that the present knowledge of genetics is so certain and detailed that the future of the human race can now be predicted.
Nor has anybody suggested that its survival can be assured by pumping into interstellar space countless replicas of the human genome under the slogan ‘Peace through panspermia’ or something like it.
All that is eminently creditable.
Indeed, the research community's self-restraint may have been carried too far.
It has been mostly left to outsiders to remark that knowledge of the structure of DNA has made possible and has enforced a change of thinking about the nature of living things.
The process has been demystified and brought within the gambit of the familiar laboratory discourse.
Perhaps the most telling lesson of the past forty years has been the recognition that very different forms of life are built around essentially similar molecular mechanisms.
All species are discovered to have more in common with each other than their differences would suggest.
Humble Drosophila is a better genetic model of human beings than people ever had before.
The potential benefits are also huge.
A proper understanding of the history of the surface of the Earth is not the least of them.
But it is also probably the case that, in molecular biology, the gap between basic research and its application is smaller than in any other field of technology, and may even have shrunk to zero.
That is why the imaginative imitation of life processes will bring great benefits to our successor generations, who will no doubt give ‘biotechnology’ a name that will reflect more accurately the grandeur of its promise.
Two other features of this anniversary have not been sufficiently remarked upon.
One is the speed with which the revolution in biology has been effected.
The past forty years have been technically foreshortened by a succession of powerful techniques for unravelling the behaviour of seemingly complicated macromolecules.
You seek a way of making DNA that is complementary to a molecule of messenger RNA?
Then find the natural enzyme that allows some cells to do just that.
(The name ‘reverse transcriptase’ then coins itself.)
In retrospect, the techniques that have given molecular biology its pace have hung on the principle that life, which has evolved the manipulative techniques by which it is itself sustained, must also embody the techniques for which laboratories cry out.
It has also been remarkable, in the past few weeks, that people who had not been born in 1953 have been conspicuous at the anniversary celebrations.
That is not so much a sign that molecular biology is a young person's game, but rather a proof of how great a magnet for young people's enthusiasm the structure of DNA has proved to be.
On this occasion, at least, the schools and even the general newspapers have not failed the research enterprise.
Even so, it is a pleasing feature of the recent celebrations that the young and mostly confident practitioners of molecular biology have been able to rub shoulders with its inventors — not just with Watson and Crick, but with their mentors, people like Perutz, for example .
All too often, the antecedents of revolution are separated by more than a human lifespan from their fruition.
Younger people have a natural tendency to believe that the science they practise has been extant for the whole of time.
It is both salutary and stimulating to show that it has roots in people's minds.
So what can be amiss with a torrent of intellectual change as imaginative and potentially as beneficent as that represented by the present condition of molecular biology?
On the face of things, nothing.
Are not the journals full of new discoveries, some them important?
It is not now plain that molecular biology stands in relation to all the life sciences as atomic theory does to the physical sciences?
(The old argument, fashionable in the 1960s, whether molecular biology is a separate discipline ir, instead, an ingredient of every other has long since been settled in favour of pervasiveness.)
Yet there are blemishes, some of them important, that deserve wider attention.
First, there is a somewhat technical point.
For all its success, molecular biology is still preoccupied with enumeration — the enumeration of the molecular components of cells and of the distinctive organisms to which they belong.
The practitioners are still largely in the state of new recruits to any government's army, who will be required at an early stage to learn by rote the names of the standard machine-gun issue.
This is the spirit in which people are now collecting the details of new genes, and of new nucleotide sequences to go with them; new proteins and their amino-acid sequences; and novel membrane protein molecules, channels or receptors as they may be.
None of this implies that molecular biology is simply a higher form of botanizing as practised a century ago; on the contrary, people's descriptions of their experimental methods are replete with numerical information about such things as the concentrations of reagents.
With some exceptions (the assembly of tubulin into microtubules is one), molecular biology does not collect quantitative information about the rates of processes happening in cells that physical chemists would consider essential to understanding.
Nor is it much concerned with exceptions to the rule that, to every effect, there is one principal cause.
That, given that only forty years have passed, may be forgivable.
It is less easily defensible that the practitioners appear to think less deeply about the meaning of the present abundance of data than is the case in many other fields of science.
To be sure, coffee-breaks in molecular biology laboratories are as marked by speculation as in any other field, but the published literature gives the impression that its authors are more concerned with the correctness of their observations than with their significance.
Especially because of all the evidence that different species, and different processes within a single organism, use variants of similar molecular mechanisms to carry out specified tasks, those with the good fortune to have the time to think about the data accumulating in the literature would probably reap a rich harvest of understanding.
The explanation of the unreflective state of molecular biology is easily accounted for by its third contemporary characteristic: competitiveness.
There can never have been a field of research in which the likelihood that people would make similar discoveries almost simultaneously has been as great.
The anxiety to publish quickly, if unreflectively, is reinforced by the reward system in research, which links the award of research grants and promotion to people's publications records.
This is an old diagnosis of other ills, of course, but it adds both tension and anxiety to a field of science that would be different, and perhaps even more fun, otherwise.
But of course, if Watson's The Double Helix is to be taken seriously, competitiveness has been the norm since 1953.
John Maddox 
NEWS AND VIEWS
FULLERENES
Collapse and growth Robert F. Curl
THE hardest part of the buckminsterfullerene story to accept has always been the formation of this highly symmetric molecule in condensing carbon vapour.
If extremely directed efforts by conventional organic chemistry cannot yet make this molecule, its spontaneous formation out of chaos stretches credulity.
von Helden, Gotts and Bowers, on page 60 of this issue, and Hunter et al .,
in the Journal of Physical Chemistry , report the first experiments which demonstrate that more open and disorganized carbon clusters collapse into a fullerene.
Bower's team at the University of California, Santa Barbara, proposes such collapse as the means by which buckminsterfullerene is synthesized.
The problem is not just that buckminsterfullerene forms, but that it does so with such efficiency.
A carbon plasma is created in an electric arc struck between two graphitic electrodes; up to a fifth of the carbon vaporized ends up as buckminsterfullerene.
Scientists have responded to the challenge of this result: the mechanism considered by von Helden, Gotts and Bowers is the fifth to be put forward.
The first was proposed by Smalley, who believes that the spherical molecule is produced by the growth of open sheets of pentagons and hexagons.
The minimum energy path for such growth incorporates at each step the maximum number of pentagons consistent with a rule that each pair of pentagons should be separated by a pair of hexagons.
This prescription he calls the ‘pentagon road’; it leads inevitably to buckminsterfullerene.
Alternative
Heath has proposed an alternative ‘fullerene-road’ scheme in which fullerenes are formed in the size range of 30–40 carbon atoms and grow by the addition of small carbon radicals.
The growing fullerenes eventually become buckminsterfullerene which, being the most stable structure in its size region, should be relatively inert to further radical attack.
In the other two mechanisms, buckminsterfullerene is formed by a combination of specific precursor carbon clusters.
Goeres and Sedlmayr propose the sixfold combination of napthalenic C 10 units; Wakabayashi and Achiba propose a mechanism that starts with the combination of napthalenic C 10 unit with a ring C 18 .
The trouble with these latter two mechanisms is that napthalenic C 10 , comprising a pair of benzenic rings sharing a common edge, is much higher in energy than monocyclic C 10 and should be present only in extremely low concentration.
Generally, any mechanism that relies on reactions between specific pairs of medium-sized clusters will be flawed, because their specific presence in the plasma will be too small to account for the high yield of the experiments.
The observed carbon-cluster distributions exhibit no extremely special medium-sized clusters.
The fullerene-road mechanism seems to have a similar failing; there must still be sufficient small radicals around when the smaller fullerenes have developed for growth to continue.
As the formation of these smaller fullerenes seems to require extensive annealing of more disorganized clusters, they may form at a relatively late stage of clustering.
One may doubt whether the small carbon radicals are still present.
High buckminsterfullerene yields have been obtained in a heated pulsed laser vaporization where there is no continuous source of new small radicals as there might be from the arc.
Moreover, buckminsterfullerene can be made with metal ions incorporated in its interior; the internal cavities of small fullerenes are too small to hold metal ions.
In the fullerene-road mechanism, the metal ion would have to be taken up on an open cluster early on during growth, and the fullerene shell would then have to grow around the metal.
The pentagon-road scheme does not have this problem.
The highly reactive pentagon-road fragments, if they form at all, would form at an early stage of clustering when many small radical species remain.
The reliance on highly reactive and as yet unobserved species in the pentagon road is certainly not a fatal objection: the bulk chemistry of combustion is carried by highly reactive free-radical species never present in high concentration.
More of a problem is the fact that the mechanism does not spell out a route to producing C 70 fullerene.
But under the appropriate conditions, C 70 can be produced in high yield.
There is no evidence ruling out either the pentagon-road or fullerene-road schemes, but the field is at a state where new suggestions for the fullerene formation mechanism are welcome.
The Californian team submits in this issue that high yields of C 60 and C 70 can be explained in terms of the collapse of larger clusters of carbon atoms with various structures.
Such a collapse had been hinted at in molecular dynamics computer simulations.
The new experimental results in this issue and by Hunter et al .
now confirm that this amazing thing does actually happen, at least for cluster ions near .
Bowers's group has previously shown that distinct isomers — chains, single rings, double rings, shells and so on— of carbon cluster ions can be separated in a variant of gas-phase chromatography.
That group and Hunter et al .
now adapt this method to observe the fate of mass-selected carbon clusters accelerated into a drift tube filled with inert gas.
They find that the less stable isomers of the larger clusters, for example, heated by collisions with the inert gas, collapse and rearrange themselves into more stable fullerene shells.
Annealing
Bowers's team imagine that near a carbon arc, the less stable isomers grow to become larger than  or  and then collapse to the stable structures as heat from the arc anneals them.
Not only have similar events now been seen with smaller isomers, but the mechanism relies on species known to be abundant in the carbon plasma.
Also, it is well established that efficient synthesis of buckminsterfullerene requires high temperatures, which can be understood in terms of the annealing mechanism.
For the scheme to work, it is necessary that the trigger point or detonation temperature for collapse of the non-fullerene cluster depends rather sensitively upon cluster size.
Obviously, if collapse to a fullerene occurs before the cluster has accumulated 60 atoms, buckminsterfullerene cannot be produced.
If the clusters grow too big before they collapse, they will be unable to shake off enough smaller carbon fragments to get back down to 60 atoms.
One can speak loosely of a ‘non-fullerene cluster’, but there is likely to be a variety of such clusters and the collapse size must depend on the chemical bonding in the cluster.
A corollary of the mechanism is that either not too many kinds of clusters must be involved or the collapse size has to be fairly independent of the cluster structure.
The viability of the new mechanism can be examined in several ways.
The dependence of the collapse of ions of  various structures into fullerenes upon size and ion energy can be investigated.
From such studies, the detonation temperature as a function of cluster size might be estimated.
Laser vaporization in a heated tube provides a means for synthesizing fullerenes by carbon condensation under controlled conditions.
The composition of the material produced under a variety of conditions may plausibly be compared with simulations based on the data obtained from the drift-tube collapse.
As a simple example of a prediction that can be tested using the laser vaporization tube set-up, the collapsing model predicts (as does the pentagon road) that the ratio of the C 70 yield to the C 60 yield will be high when the tube temperature is low, and vice versa .
Time and further work will tell.
Robert F. Curl is in the Department of Chemistry, Rice University ,.
SIGNAL TRANSDUCTION
How receptors turn Ras on Frank McCormick
THROUGH a remarkable convergence of biological and technical insights, a collection of papers (four in this issue of Nature , three in Cell , one in Science ) describe events by which signals are transmitted from a receptor tyrosine kinase to Ras through specific protein-protein interactions.
All eight papers describe the role of Grb2 (Sem-5 in Caenorhabditis elegans , drk in Drosophila ) in recruiting a Ras activator to the receptor to form a stable complex.
The Ras activator is the mammalian homologue of Drosophila Son of sevenless, referred to as mSos1, which functions as guanine-nucleotide-releasing protein in converting inactive RasGDP to the active, GTB-bound form by nucleotide exchange.
Recruitment of Sos1 facilitates Ras activation and subsequent signal transmission down the Ras-dependent kinase cascade (see figure).
Also, the sites of interaction between Grb2 and Sos1 have been mapped, and the biological consequences of Ras activation by Sos and Grb2 documented.
These papers, the fruits of extensive collaboration among most of the groups involved, provide a comprehensive description of the first steps of this critical pathway along which messages pass from the cell surface to the nucleus.
The flow of information from tyrosine kinases to Ras proteins had been anticipated by analysis of signalling in mammalian cells and genetic analysis of signalling pathways in worms and flies.
These studies defined a pathway that is at the heart of growth control in higher eukaryotic organisms, one which is so highly conserved that its components are functionally interchangeable between mammals, flies and worms.
Within any one cell of an organism, it is likely that components of the pathway are used to detect multiple signals, Ras acting as a kind of turnstile through which the signals must pass.
From cell-surface to Ras and beyond.
Grb2 binds to activated receptors by the SH2 domain, and to Sos by the SH3 domain.
Receptor-associated Sos provokes GDP-GTP exchange on Ras, triggering a cascade of serine-threonine kinases that send trophic signals to the nucleus.
PLC, phospholipase C.
One puzzling aspect of this notion was that a variety of tyrosine kinases (receptors and non-receptors) and possibly other activators, such as certain G-protein-coupled receptors, could all funnel signals into the same, highly conserved recipient, Ras.
This would seem to run counter to the idea that receptor activation initiates a cascade of diverging biochemical pathways that conspire to produce an effect (cell division, in this case); and it seems to require that multiple kinases phosphorylate a small number of substrates, which, in turn, feed signals into a smaller number of Ras activators.
Accordingly, each step in the signalling funnel would have to be relatively permissive in signal reception yet specific in transmission.
Two events helped lead to resolution of this puzzle.
First, there was the identification of the ‘Src homology’ domains SH2 and SH3 and the recognition (see refs 10 and 11 for reviews) that these regions are binding sites for specific tyrosine phosphoproteins (for SH2 regions) and specific proline-rich motifs (SH3 regions).
SH2 and SH3 regions have been identified in a large number of proteins involved in signal transduction.
Their profound significance lies in their restricted permissivity, meaning that an SH domain within a single enzyme can be ‘plugged in’ to a number of different upstream activators.
The second event was the identification of ‘adaptor proteins’, of which Grb2 is now the best known (others include Shc, Crk and Nck).
These adaptors were proposed to connect other signalling proteins together through SH2 and SH3.
Given the semi-permissive nature of interactions between SH2 and tyrosine phosphoproteins, and (possibly) between SH3 and its targets, this presented a daunting number of possible protein-protein combinations.
The new papers cut through these complexities and provide a remarkably clear and simple picture of SH2- and SH3-mediated interactions (see figure).
In this scheme receptor activation results in autophosphorylation, creating a binding site for Grb2, as described previously.
The new insight is that Grb2 associates with Sos1, and thus recruits it to the activated receptor in the plasma membrane where Ras activation is presumed to take place.
Grb2 binding of Sos1 in vitro facilitated the rapid mapping of the sites of interaction: the SH3 domains were clear candidates for interaction on the Grb2 protein because mutation in these domains had been detected in genetic screens in worms, and indeed both SH3 domains of Grb2 are required for high-affinity binding.
Attention was drawn to the proline-rich regions of Sos1 as potential SH3 binding sites by the mapping of other SH3 binding sites to proline-rich  domains.
These regions are indeed the targets of Grb2-SH3 interactions, although exactly which sequences in Sos1 bind to which SH3 domain is not yet completely clear.
These data raise a number of specific questions, and some more general issues.
Specifically, how is Ras activated by Grb2-Sos binding?
This interaction is not dependent on receptor activation; it clearly occurs in vitro using recombinant proteins, and Grb2-Sos complexes can be detected in non-activated cells.
Binding of Grb2-Sos to activated receptors seems to increase the affinity of Grb2-Sos interaction, but guanine nucleotide exchange of Sos1 itself looks to be indifferent to the binding state of its chaperone adaptor protein.
This leaves open the possibility that the act of recruiting Sos to the plasma membrane triggers Ras activation by generating an increased local Sos concentration.
In support of this, over-expression of Drosophila Sos1 in mammalian cells causes transformation.
So it seems that Ras can be activated by merely increasing the total cellular concentration of activator, without requiring receptor-dependent modification.
The general idea that receptor activation recruits enzymes to their membrane substrates (Sos1 to Ras in this case) is not a new one: the appearance of lipid-metabolizing enzymes (PI3' kinase and phospholipase C for example) in receptor complexes adjacent to their substrates may be a related phenomenon.
More generally, it is likely that Grb2-Sos can be ‘plugged in’ to many other receptor systems, as already discussed.
Indeed, Egan et al .
go beyond the association between the epidermal growth factor (EGF) receptor, Grb2 and Sos, and show that, in Src-transformed cells, the Shc adaptor protein replaces the EGF receptor as the phosphotyrosine target and presumably achieves the same end point (activation of Ras).
Identification of the precise mechanism by which Shc connects to Src, and its generalization to other non-receptor tyrosine kinases, cannot be far away.
At first sight, a number of proteins (PI3', phospholipase C, p120-GAP, for example) implicated in transducing signals from receptors to Ras are left out of the picture.
But this does not mean they are not involved.
It is conceivable, for example, that the accumulation of RasGTP following Grb2-mediated activation may be compromised by high levels of GAP activity, so that effective signal transmission occurs only when GAP is inhibited.
It is also possible that Grb2 or Sos proteins are modified by other signals emitted from activated receptors to compromise or increase their ability to couple.
We can now test these possibilities.
Another protein that appears to be left out of this picture is RasGRF, a mammalian Ras activator with a more restricted tissue distribution than the Sos proteins.
This protein does not contain the proline-rich sequences that couple Sos to Grb2 and hence to receptor tyrosine kinases.
Presumably, RasGRF is recruited to the membrane by a different mechanism in response to incoming signals, the nature of which has yet to be determined.
A corollary is the prediction that cells expressing only the Sos variety of Ras activator may be able to respond only to signals transmitted through tyrosine kinases.
One thing is certain.
If research continues at this pace, stoked as it is by a network of collaborations, these issues should be resolved in months rather than years.
Frank McCormick is at Onyx Pharmaceuticals ,.
COSMOLOGY
Dark matter strikes again Bernard Carr
DARK matter has been invoked in many astronomical contexts.
There is good evidence that it is associated with individual galaxies as these seem to have dark haloes which extend well beyond the visible stars.
There are also good reasons for associating it with rich clusters (groups of several hundred galaxies) as these seem to have a collective halo, possibly formed from the material in the original haloes of the constituent galaxies.
If one believes the prediction of the inflationary cosmology, that the total density of the Universe must be very close to the critical value above which it recollapses, then there must also be unclustered dark matter as even the dark matter in galaxies and clusters has too small a density to account for this.
The evidence for unclustered dark matter is less equivocal but there are some indications from recent studies of large-scale cosmic structure.
On page 51 of this issue, Ponman and Bertram report a discovery which suggests that the presence of dark matter in yet another context — small groups of galaxies — so that the dark-matter problem rears its head again.
Ponman and Bertram's work concerns the compact group of four galaxies called HCG62.
The first step in the argument is their finding that HCG62 is enveloped in a large cloud of hot gas which extends well beyond the galaxies themselves.
The evidence for the gas comes from observations by the Rosat satellite, which is able to detect its X-ray emission.
The evidence for the dark matter comes from the temperature of the gas, which is found to be about 10 7 K. This is because the temperature is determined by the strength of the gravitational field, and the visible material alone (the combination of the gas and the galaxies) does not provide enough gravity to explain such a high value.
It has been known for many years that rich clusters contain a lot of gas and dark matter, but Ponman and Bertram's result shows that these features apply more generally.
There is a precedent for this discovery in a similar result reported in February by Mulchaey et al .
for the compact group HG92.
These authors also found evidence for hot gas and dark matter extending beyond the visible galaxies, and their work was itself the subject of a News and Views article.
However, there are important differences between the two cases.
The first concerns the metallicity of the gas, the abundance of elements heavier than lithium.
In both cases the metallicity is low compared with the solar value; this indicates that the gas is mainly primordial, with a small admixture of material processed by stars and then ejected from the galaxies.
However, whereas it is only 6 per cent of the solar value for HG92 (the smallest value ever found for the intragalactic medium in a group or cluster), it is 15 per cent for HCG62.
The second difference concerns the ratio of the baryon mass (that is, the mass in the form of stars and hot gas) to the dark mass (which may be in the form of exotic particles like neutrinos or axions).
This is about 4 per cent for HG92, which is comparable with the ratio of the average baryonic density, derived through models of cosmological nucleosynthesis, to the critical density.
If one believes that the total density of  the Universe must be very close to the critical value, this implies that the baryon fraction in compact groups is representative of the mean cosmological value.
However, the baryon fraction is at least 13 per cent for HCG62, which is significantly larger than the global value, so the situation is not as simple as it first appeared.
Indeed, Ponman and Bertram question Mulchaey et al .
's result on the grounds that the baryon fraction in HG92 may be bigger if one considers a larger region around the group.
There are therefore two problems with HCG62: there are not enough baryons to explain all the dark matter but there are too many to be consistent with the simplest picture of cosmological nucleosynthesis.
In fact, the second problem already applies for rich clusters, because these tend to have baryon fractions in the range 20–30 per cent, which is even larger.
This puzzle was highlighted by Frenk at the UK National Astronomy Meeting, in Leicester last month in a paper discussing what he termed the ‘baryon catastrophe’ for the Coma cluster.
This is one of the closest rich clusters and the first one for which a dark-matter problem was identified (see figure).
In this case, Rosat observations suggest that the ratio of baryon mass to total mass within 3 megaparsecs is about 25 per cent, which is five times as large as the cosmological ratio in the standard model.
Baryon catastrophe?
The central region of the Coma cluster of galaxies, which is 170 megaparsecs away and around 3 megaparsecs in radius.
Gas between the galaxies contains too few baryons to account for all the apparent mass but too many to be consistent with the simplest Big Bang models.
(Distances assume Hubble's constant .
There are various ways out of this dilemma.
Perhaps the cosmological density is well below the critical value (despite the anticipation of theorists) or perhaps models of cosmological nucleosynthesis must be revised (as some people have already asserted).
More palatable to cosmologists would be the possibility that the gas in groups and clusters has become more concentrated than the dark matter.
The difficulty with this is that it is not obvious how the extra baryon concentration would come about — cooling would be unimportant in the outer parts of clusters, so the gas would not naturally sink inwards as in a so-called cooling flow.
Indeed many astrophysical processes, such as the spreading out of gas by galactic winds and supernovae, would decrease the local baryon fraction.
Another possibility is that the gas density may have been overestimated for some reason.
This would be reasonable if the gas were highly clumped as the X-ray intensity per unit mass would then increase.
In this context, it may be relevant that the degree of baryon catastrophe seems to increase with metallicity (indicating that cooling may be an important factor).
However, for Coma at least, one must reconcile this with the fact that the X-ray emission seems to be rather smooth.
Recent Rosat observations do show evidence for substructure on some scales — and this is important because it indicates that larger-scale cosmological structures built up hierarchically — but not the smaller-scale clumping required to remove the baryon catastrophe.
There are several other interesting aspects of Ponman and Bertram's paper.
One important point is that they have been able to determine the temperature profile of the gas and this shows that, within the central 75 kiloparsecs, the temperature decreases as one goes towards the centre of the cluster.
This suggest that the gas is undergoing a cooling flow, as is expected anyway as the gas density inferred from the X-ray luminosity requires that the cooling time be less than the age of the Universe within that region.
There is good evidence for cooling flows in rich clusters and around individual galaxies but this is the first example of their occurring in compact groups.
Another interesting point is that measurements of the velocity dispersion of the galaxies in HCG62 suggest that they have drifted inwards by a factor of about three compared with the dark matter.
This could be explained as resulting from the dynamical drag of the surrounding gas provided that the galaxies start off with individual dark haloes.
(The dynamical friction time varies inversely with the galaxy mass and must be less than the cosmological time.)
Thus HCG62 provides evidence for dark mass, not only in the group as a whole, but also in its individual members.
Another interesting consequence of the drag effect is that, within a further billion years, all the galaxies should have merged to form a single central elliptical galaxy surrounded by an extensive gaseous and dark halo.
Ponman and Bertram suggest that there might be many such ‘fossil’ groups in the Universe and, if so, that these might be detectable by Rosat.
Indeed candidates for such sources may already exist among the cooling-flow galaxies found by the Einstein satellite.
So Ponman and Bertram's paper impinges on a number of important issues: the prevalence of the dark matter, the existence of cooling flows, and the evolution of galaxies and clusters.
But perhaps its prime message is the crucial role that Rosat is likely to play in our understanding of these issues.
Bernard Carr is in the Astronomy Unit, Queen Mary and Westfield College ,.
SOLAR SYSTEM
Planet X: a myth exposed Gerald D. Quinlan
THERE is no reason for the planetary system to end at Pluto.
Astronomers have long debated the possibility of a more-distant tenth planet, usually referred to as Planet-X.
But the scant dynamical evidence requiring the existence of this planet is largely refuted in a new analysis by Myles Standish of the Jet Propulsion Laboratory.
The history of the hypothetical tenth planet begins with the discovery of Neptune.
By the late 1830s the existence of this unseen planet had become undeniable because of discrepancies approaching one arcminute between the observed and computed orbits of Uranus, attributable to gravitational perturbations.
The discovery of Neptune in 1846 within one degree of its predicted position was without question the greatest triumph of celestial mechanics at the time.
The discovery reduced the discrepancies in Uranus's orbit to the level of arcseconds, comparable with the noise in the observations, but that did not stop some astronomers from predicting a planet beyond Neptune, of a few Earth masses.
A long series of searches beginning with Lowell in 1905 culminated with Tombaugh's discovery of Pluto in 1930.
Pluto appeared tiny right from the start; it is now known to have a mass only 0.2 per cent that of Earth, far too small to have caused the irregularities in the orbits of Uranus and Neptune attributed to Planet X. A dedicated group of astronomers have maintained that a tenth planet is required.
Some have proposed other reasons for Planet X, blaming it for causing periodic comet showers and mass extinctions on Earth, or for stealing Pluto from Neptune and reversing the motion of Neptune's satellite Triton.
Observational surveys by Tombaugh, Kowal and the Infrared Astronomical Satellite (IRAS) would almost certainly have revealed another Pluto or larger planet, but their failure to do so has not stopped many predictions from being made, including two more in just the past year.
Standish shows in his new work that the dynamical evidence for Planet X is readily explained by uncertainties in planetary ephemerides.
The construction of an ephemeris involved processing and evaluating the wide variety of data on planetary positions, choosing a dynamical model for the Solar System, and varying the adjustable parameters to give computed orbits that best fit the observations.
Planet X's supporters point to three ephemeris difficulties suggesting a tenth planet: the Uranus observations cannot all be made to fit, and the residuals show systematic trends; a few known sightings of Neptune before its identity was established are difficult or impossible to fit; and predictions of Neptune's orbit have tended to be inaccurate.
Standish examines the first two difficulties; the third is not surprising given Neptune's long orbital period, 165 years (so that not even one orbit has been completed since it was discovered).
The Uranus difficulties result mainly from the use of an incorrect mass for Neptune.
The masses of the jovian planets are now known accurately from the Voyager missions, but in the past they were not, and much of the speculation on Planet X has been based on residuals from the Jet Propulsion Laboratory's ephemeris DE200 and it predecessors, which used a Neptune mass in error by 0.5 per cent.
Standish has constructed a new ephemeris with the correct mass and finds that this reduces the root-mean-square Uranus residuals by about 20 per cent and, more importantly, removes the systematic trends.
There remains a strong positive bias in the right-ascension residuals from 1895–1905, but Standish has traced the majority of these to a single catalogue of the US Naval Office 9-inch transit telescope (data from two other sources do not show this bias); an observational error such as an incorrect equinox correction is a plausible explanation.
Regarding the prediscovery sightings of Neptune, Lalande's in 1795 is said too differ from ephemeris predictions by seven or more arcseconds, but Standish shows by extrapolating two ephemerides backwards that the predictions are uncertain by at least several arcseconds; combined with the scatter in Lalande's observations this makes the discrepancy rather dubious.
The discrepancy for Galileo's observation in 1613 is larger if true, but Galileo's notes are ambiguous, and, moreover, Standish and Nobili have found another Neptune observation by Galileo, this one agreeing with ephemeris predictions (personal communication).
Accurate planetary observations being collected today will in time lead to more stringent tests for a tenth planet.
These observations are valuable irrespective of Planet X, because planetary ephemerides are needed for other purposes such as spacecraft navigation and pulsar data reduction, and are instrumental in precise tests of theories of gravitation.
But for now the search for Planet X is futile.
The search for smaller objects in the  outer Solar System has only just started, however.
Dynamical limits allow a few tenths of an Earth mass in a belt of planetesimals beyond Neptune.
This belt, named after Kuiper who suggested it would be left over from the formation of the planets, has been much-discussed as a possible source of short-period comets.
In 1987 David Jewitt instituted a systematic search of the outer Solar System for faint, slow-moving objects.
The effort was rewarded with the discovery last August of the faint smudge (7,000 times fainter than Pluto) which he and Jane Luu tagged 1992QB 1 (they later named it Smiley, after le Carré's spy).
Four months' observations of QB 1 reveal an orbit that would be expected for a Kuiper-belt object, although there is still considerable uncertainty in the eccentricity.
QB 1 's estimated diameter of 200 km puts it at one tenth the size of Pluto: much larger, and the whole distinction between planets and minor planets would be thrown into doubt.
There is now news of a second possible Kuiper-belt object, 1993FW, also found by Jewitt and Luu.
The discovery of even a few of these objects could tell us much about the early history of the Solar System, perhaps more than we would learn from the elusive (or illusory) Planet X. Gerald D. Quinlan is at the Lick Observatory, University of California ,.
RÉSUMÉ
Twist or bulge?
THE recovery of no fewer than eleven kinds of filamentous microfossil from 3,465-million-year-old rocks in Western Australia (J. W. Schopf Science 260 , 640–646; 1993), shows that life was not only in existence a few hundred million years after the planet was formed, but was thriving: rarely does one find single fossils of this antiquity, let alone entire communities.
The fossils represent bacteria and presumably photosynthetic (and therefore oxygen-producing) cyanobacteria little different from those a billion years younger.
But, as Schopf asks, if such diverse communities were present at this early date, why did their members remain essentially unchanged for so long?
Waste disposal
THE growing end of a neuron receives the necessary materials by axonal transport from the cell body.
The metabolic products of the activity at the growth cone come back down the axon by retrograde transport and home in on lysosomes in the cell body, or in some cases are processed by proteases in the cytosol.
P. J. Hollenbeck now illuminates the mechanisms by which the unwanted proteins are despatched from the growth cone (J. Cell Biol .
121 , 305–315; 1993).
The vehicles that carry them back, he finds, are membrane-encased vacuoles, which take up fluorescent dextran, introduced into the cell, and convey it to the lysosomes in the cell body.
If the neuron is allowed to take up similarly labelled materials from the outside world by endocytosis at the growth cone, all the fluorescence again appears in the same kind of vacuole, travelling down the axon.
This is a finely tuned mechanism for responding to the imperatives of the law of conservation of mass.
Slit decision
THE number of games that can be played to illustrate wave-particle duality is proliferating.
Besides diffracting light through material slits (Young's slits), one can diffract atoms through gratings composed of immaterial light waves, atoms through slits composed of other atoms and, now, light off a pair of atoms suspended in a vacuum in a magnetic field (U. Eichmann et al.
Phys.
Rev. Lett. 70 , 2359–2362; 1993).
The alert might recall that various groups have made unusual, tenuous ‘crystals’ out of small numbers of ions suspended in a magnetic field; the pair of mercury ions in the new experiment constitutes the simplest such crystal.
So the diffraction of laser photons becomes a novel form of Bragg reflection, which effect showed originally that X-rays are electromagnetic waves and that atoms in solids line up in ordered arrays.
Such is progress.
SCHISTOSOMIASIS
Proselytizing with immunity Edward J. Pearce
THE reality of schistosomiasis is that at least as many people are infected today as ten years ago — this despite the availability of the wonder drug praziquantel.
It was against this gloomy backdrop, and in the knowledge that disease resistance, disease severity and drug efficacy are contingent upon the immune response to the parasites concerned, that immunologists got together earlier this year to assess the state of play in understanding and tackling the disease.
The outlook, however, is not altogether depressing.
The meeting provided a sparkling overview of the very real advances in getting to grips with the immunoregulatory and immunopathological mechanisms involved, and in vaccine development.
Even the sceptics were left with no doubt that the production of an effective vaccine against schistosomiasis is not only necessary, but possible.
There is, it is clear, more than one way to make a vaccine against schistosomiasis.
Recombinant schistosome glutathione-S-transferase, which confers only a partial resistance to infection, induces a potent anti-fecundity immune response that dramatically reduces the number and viability of eggs produced by female parasites, and so reduces the risk of transmission and the pathology associated with infection (A. Capron, Pasteur-Lille).
The anti-pathology effects of this vaccine are most striking in valves and patas monkeys, both of which were essentially protected from disease when unvaccinated controls developed severe lesions.
There was, too, an intriguing report of unprecedented levels of immunity induced in rodents, and partial protection in baboons, by a recombinant surface-antigen (rIrV5), which share homology with myosin II (M. Strand, Johns Hopkins).
The plan is to take these vaccines into phase I human trials in the near future.
There is also progress to report in identifying immune effector mechanisms, an area in which research on schistosomiasis has been central.
Amongst those working with human populations in endemic areas there is accord that the intensity of reinfection following treatment, as assessed by faecal egg counts, is negatively correlated with levels of immunoglobulin E (A. Dessein, INSERM; D. Dunne, University of Cambridge), lending support to the view that IgE protects against schistosomes.
(Incidentally, the time-honoured use of faecal/urine egg counts to assess infection intensities may be on the way out; many groups are now planning to use circulating adult antigen to provide specific information on worm burdens.)
On the subject or responses mediated by the Th2 subset of CD4+ cells, the story of the eosinophil grows more intriguing with news that these cells are a major source of interleukin-5 and possess IgA receptors (M. Capron, Pasteur-Lille).
Another approach to vaccine development is the elicitation of Th1-like responses, as illustrated by the report that immunity in mice immunized with attenuated parasites is entirely dependent on the Th1 cytokine, interferon-γ (ref. 2; A. Wilson, University of York).
The demonstration that endothelial cells can be activated by IFN-γ to kill schistosomes (S. James, NIAID) raises the prospect that these cells, which like the pulmonary capillaries (a dangerous bottleneck in schistosome migration), have a part in the IFN-γ-mediated effect.
The take-home message here is that both Th1- and Th2-stimulating vaccines have the potential to be protective.
During the egg-laying stage of infection, Th responses in mice are highly Th2-like, suggesting the pathology, which is Th dependent, is mediated by Th2 cytokines.
This concept was discussed in light of observations that IL-2 (D. Boros, Wayne State; A. Sher, NIAID), the α form of tumour necrosis factor (ref. 3; Boros; J. McKerrow, UCSF) and IL-4 (Boros; Sher) are all involved in granuloma formation; further, in mice, high TNF-α levels seem to be predictive of severe disease (D. Colley, Centers for Disease Control).
Studies of the antigens involved in anti-egg Th responses show egg proteases to be particularly immunogenic (B. Doughty, Texas A&M).
The involvement of IL-10 in the inhibition of macrophage anti-schistosome effector functions (James), and the Th1 down-regulation which accompanies egg-associated Th2 responses, came in for some debate.
This is a critical area — experimentally, the suppression of Th1 and CD8+ cell activity makes mice with schistosomiasis more susceptible to experimental vaccinia infection (in these mice, virus persists specifically in granuloma fibroblasts; Sher).
These data bear on the correlation between hepatitis and schistosomiasis, and (possibly) on how the human immunodeficiency virus is dealt with by helminth-infected individuals.
The extent to which Th1 responses are down-regulated in infected patients, who clearly are mounting antiparasite Th2-like responses, remains unclear, although some studies have indicated less of a suppression than that seen in infected mice (Dunne; C. King, Case Western Reserve).
The belief that pathology in schistosomiasis is wholly egg-centred requires reassessment in light of the report of ‘capillarization’ of hepatic sinusoids in infected mice before the onset of egg production (J.-A.
Grimaud, Pasteur-Lyon).
Other news on pathogenesis is that fibroblast stimulating factor 1, a cytokine first described in schistosomiasis, stimulates deposition of extracellular matrix (D. Wyler, Tufts).
The relationship between a failure to down-modulate  anti-schistosome immune responses during infection, and the likelihood of developing severe life-threatening disease, was the subject of much comment, as was the importance of exposure in utero to regulatory idiotypes and/or antigen to prevent severe disease following infection (Colley).
Schistosomes use host molecules for their own devices.
In this context, the discovery that Schistosome mansoni uses TNF-α as a stimulus for egg production was given added significance by the observation that the site where these parasites achieve sexual maturity, the portal venous system, is particularly rich in this cytokine (McKerrow).
On this subject of direct parasite-hose interactions, egg-and larval-stage surface glycoproteins have been shown to carry lacto- N fucopentaose III; this contains the Lewis-X trisaccharide, a ligand for P-selectins (D. Harn, Harvard), raising the possibility of direct communication between schistosomes and host cells through adhesion molecule interactions.
An exciting implication, by analogy with certain tumour cells that express Lewis-X and metastasize to the lungs and liver (the two major systemic organs through which schistosomes migrate), is that this sugar provides a means for the parasite to find its way around the host.
Lacto- N -fucopentaose also activates B cells from schistosome-infected mice (in an antigen receptor-independent manner), to make IL-10, a cytokine implicated in the development of the Th2-like response evident in infected animals (Harn).
So, experimentally, progress is tangible.
But if there was a sense of excitement at the meeting about these new results, there was also a sense of urgency.
In some countries things are getting worse rather than better — for instance, the current epidemic of schistosomiasis in Senegal is characterised by the emergence of unexpectedly severe disease and the alarming failure of praziquantel.
The hope is that unusually fruitful conferences such as this one, in which a course in the basics of immunology preceded seminars on the latest developments, and in which the participants in most part came from areas badly affected by the disease, will show us the way forward.
Edward J. Pearce is in the Department of Microbiology Immunology and Parasitology, New York State College of Veterinary Medicine, Cornell University 
STRUCTURAL BIOLOGY
Max in a complex affair Paul Freemont
ON page 38 of this issue, Max meets DNA courtesy of Stephen Burley and colleagues.
Max is a transcription factor, a protein that binds to specific DNA target sites and controls the polymerization of RNA from adjacent DNA templates.
There are surprises to be found in the striking images of the complex between Max and its DNA target sequence, and in those surprises are hints about the interactions of Max with several other proteins, which are directly involved in the control of cell growth.
In all living cells, proteins have to recognize and bind to specific DNA sequences, recognition generally resulting from the interactions of particular amino acid side-chains with the nucleotide bases of specific DNA sequences.
Quite a number of protein-DNA complexes have now been determined to atomic resolution by X-ray crystallography, which has led to the characterization of a variety of discrete protein modules which mediate these interactions.
What Burley's group has now done is to add a new protein structural motif to this list by solving the cocrystal structure of the basic/helix-loop-helix/leucine zipper (b/HLH/Z) domain of Max bound to DNA at 2.9 A resolution.
The helix-loop-helix motif (HLH) is found in a variety of dimeric eukaryotic transcription factors and, as the name indicates, it was first characterized on the basis of two amphipathic α-helices connected by a loop.
Amino-terminal to the HLH region, a further helical sequence of basic residues is found and, in some members a well-conserved leucine zipper is present after the HLH motif.
Each of these regions has been the subject of extensive mutagenesis and biochemical experiments, from which it seems that the basic regions mediate DNA binding, and the HLH and zipper region promote dimerization (see refs 4 and 5).
Representation of the crystal structures of the complexes between DNA and Max (b/HLH/Z motif) and GCN4 (bZIP motif) showing the similarity in DNA interaction for both types of domain.
Top, the Max-DNA structure of Burley and colleagues.
The α-helices are drawn as ribbons (red and yellow) and the DNA duplex is shown in blue.
Below, the GCN4 bZIP-DNA structure.
The α-helices (yellow) are drawn as smooth continuous curves with the DNA coloured red.
The structure of the Max homodimer-DNA complex shows that the b/HLH/Z motif is made up of two lengthy α-helices separated by a loop, and not several helices as the motif's name would suggest (see figure).
The amino-terminal helix (25 residues) includes residues for the basic region and the first helix of the HLH, whereas the carboxy-terminal helix (43 residues) comprises the second helix of the HLH motif and also the leucine zipper motif.
The two helices are in contact with each other near the loop region (10 residues), specific contact residues being conserved throughout the HLH family.
Most unusual, however, is the way in which the Max homodimer (and by inference other b/HLH/Z proteins) recognizes its DNA target sequence.
The two basic amino-terminal α-helices sit in the major groove so that they are perpendicular to one another on opposite sides of the DNA duplex; they resemble nothing so much as a pair of short chopsticks.
These helices extend from a novel four-helix bundle structure which is stabilized by conserved hydrophobic residues that form a hydrophobic core.
Four specific DNA base contacts are made by residues from the basic helix, as are a large number of phosphate contacts spanning the entire recognition sequence.
These interactions make sense of the high degree of sequence conservation observed within the basic helix for different b/HLH/Z proteins, as all the conserved residues make either base or phosphate contacts.
Most intriguing, though perhaps not surprising, is the similarity between the Max-DNA structure and that of the yeast transcriptional activator GCN4 bZIP-DNA complex (ref. 6, see figure).
The DNA-binding domain of GCN4 also has a basic helical region and a leucine zipper (bZIP) but without the helix-loop-helix that characterizes the HLH family.
Like Max, GCN4 also binds to its DNA target sequence as a dimer, with the two basic helices acting like a pair of chopsticks gripping opposite sides of the major groove of the DNA duplex.
Curiously, the leucine zipper structures for both proteins are almost identical, and one could view Max as a bZIP protein but with an extra pair of ‘prongs’.
However, the Max basic helices are shorter than those for GCN4 by some two turns of an α-helix, and the angle between the two chopsticks interacting with DNA within each complex is slightly different (about 14° down the DNA helix axis in Max and 18° in GCN4).
The different interhelical angles observed between GCN4 and Max might reflect the need for both proteins to recognize different DNA target sequences.
But they probably result from the nature of the dimerization interfaces, which are very different for each protein.
When compared to the bZIP family, the HLH family has evolved a much more substantial interface for dimerization, as illustrated in the Max structure, where dimer contacts are made between four α-helices as opposed to two for bZIP proteins.
This distinction suggests that dimer stability (and to some extent inflexibility) are key features of the b/HLH/Z family, which is consistent with the fact that in vivo they can form reasonably stable homo-and heterodimers not bound to DNA.
In contrast, the bZIP domain seems to be very flexible; the basic helix is unfolded in the absence of DNA, resulting in a potentially more versatile DNA-recognition element which could adopt different conformations on different DNA sequences.
There's more, however, to the new Max cocrystal structure than the revelation of a novel structural domain for specific DNA interaction, for it also provides insights into the mechanisms of differentiation and malignant transformation.
In vivo , Max is the specific partner for the c-Myc oncoprotein (see ref. 8).
Furthermore, heterodimerization of Max with c-Myc mediates DNA binding by c-Myc, which is essential for both the normal and the oncogenic activity of c-Myc.
Max has been found tightly associated with a new partner Mad/Mxil, which points to Max being at the centre of a network of factors whose relative concentrations dictate preferential heterodimerization and thereby control transcriptional activity.
So the Max homodimer-DNA structure provides a first glimpse of the potential molecular interactions between Max, c-Myc and Mad/Mxil, and as such provides pointers as to the combined role of these proteins in controlling cellular growth.
What next?
Before too long we will probably be presented with the structures of other HLH-DNA complexes including the upstream regulatory factor, a Myc-Max heterodimer and perhaps other Max heterodimer complexes.
Armed with these and other detailed structural pictures, we may at last be able to get a handle on some of the many and complex eukaryotic transcriptional control mechanisms at the molecular level.
That is a prospect to savour.
Paul Freemont is in the Protein Structure Laboratory, Imperial Cancer Research Fund 
ASTRONOMY
Shocking behaviour of young stars in Orion John Dyson
STAR-FORMING regions in dense molecular clouds offer rich hunting grounds for astronomical observers and theorists interested in exotic dynamical phenomena.
Generally, such phenomena are interpreted in terms of the interaction between steady winds from young stars with material in the surrounding cloud.
Now, on page 54 of this issue, Allen and Burton present spectacular infrared photographs of active star-forming regions in the Orion molecular cloud which reveal the cloud's deep interior.
The intriguing structures they find may be evidence that sudden explosive events play the dominant role in at least some dynamical phenomena, and that molecular clouds therefore have a rightful place in the current perception of the interstellar medium as one subjected to assaults of extreme violence.
Discovered some 40 years ago, Herbig-Haro (H-H) objects are deceptively nondescript patches of emission seen in projection against dark cloud backgrounds.
Their spectra show that the radiating gas is excited by hydrodynamical shock fronts and that shock velocities of up to a few hundred kilometres a second are involved.
Corroborative evidence of this comes sometimes from the spectral widths of the lines, and sometimes from observation of movement of the patches, revealed in sequences of photographs.
The objects come in various shapes and usually, though not always, seem to be associated with nearby star.
As diverse evidence also indicates that the stars found in star-forming regions lose mass either from their surfaces or from surrounding accretion disks, a causal connection between stellar mass loss and the H-H objects has become a widespread article of faith.
In principle, the generation of any supersonic disturbance of a high enough Mach number (velocity divided by the local speed of sound) in a molecular cloud will produce H-H-like spectra, provided that the shocked gas can radiate sufficiently well.
It is therefore not surprising that many variants of the interaction between stellar winds and molecular clouds have been explored to account for the differences from one H-H object to another.
It was realized very early on that interactions involving isotropic stellar winds had serious inherent problems involving energy.
Were the kinetic energy of the wind to be converted by the shock into radiant energy, the localized H-H objects could expect to take up only a small fraction of the wind's energy, in which case the radiated power far exceeds that available.
Ingenious mechanisms were invoked to collimate the winds and so focus their energy.
A few years ago, it was found that nature had done this already: young stellar objects have  jets associated with them.
Jet-cloud interactions can generate localized H-H objects in a variety of ways, ranging from the production of internal shocks in the jet to radiation behind the shockwave systems at the front of the jet as it bores its way though the surrounding cloud.
Considerable attention is currently being given to the production of H-H objects by jets that precess or in which the jet velocity varies.
Curiously, how stars produce jets is an open question.
One of the earliest attempts to explain H-H objects was in terms of the motion of fast-moving ‘bullets’ of material through the molecular cloud.
The radiating gas — the H-H object — originates in shocked cloud gas behind the bow shock on the leading face of this cosmic missile.
This model fell out of favour for numerous reasons.
For example, the origin of the missile was far from clear; and as detailed structures of many H-H objects were elucidated, the jet interactions described above seemed a much better proposition.
Allen and Burton now argue that, at least as far as their observational data are concerned, the projectile model best fits the facts.
They note that the molecular hydrogen emission reveals long tail-like structures which project back to a more or less common origin in the vicinity of well-known stellar residents of the Orion molecular cloud (see cover photo).
The ends of the tails furthest away from the ‘origin’ are not visible in molecular hydrogen but are bright in the light of forbidden transitions from singly ionized iron.
This latter emission is a well-known indicator of shock activity for shock velocities of up to several hundred kilometres a second.
Allen and Burton also point out that there are families of H-H objects in the Orion cloud that are bright in the forbidden lines of singly ionized oxygen and that all such knots have corresponding iron emission features.
The oxygen lines, studied in detail have spectral widths up to 400 km s -1 , which must indicate actual gas velocities of this magnitude.
Allen and Burton propose the following model.
A few thousand years ago, some violent explosive phenomenon occurred which hurled numerous bullets (‘shrapnel’) into the molecular cloud with velocities probably in excess of 400 km s -1 .
The bullets swept up cloud material through the bow shocks on their leading faces, producing the iron and oxygen emission behind the shocks.
The velocity widths of the lines will range roughly up to the bullets' speed.
Molecular hydrogen is rather fragile (in interstellar terms), and is destroyed by shock waves with velocities greater than about 50 km s -1 and cannot exist behind the leading shock.
However, bow shocks become wrapped around blunt obstacles and become much weaker the further away they are from the obstacle head.
Molecular hydrogen could survive and be excited by passage through these extremely oblique shocks.
The origin of the bullets is obscure and a real understanding of their energetics is needed to pin it down.
A rough estimate by the authors suggests that some scaled-up version of so-called FU Orionis events might be plausible.
These are quite well-known occurrences in which stars of a certain class liberate a considerable amount of energy through an astrophysical trauma involving instabilities of the accretion disk.
Alternatively, Allen and Burton note that a supernova would have enough kinetic energy to power the bullets, although there is certainly no other direct evidence for one having occurred in this region.
On the positive side for such a picture, observations of (astrophysically) recent supernovae such as Cassiopaeia A or even the Crab show that supernova explosions are extremely fragmented.
These various possibilities are extremely exciting, but a great deal of follow-up observation and modelling needs to be done.
It is crucial to develop realistic shock models to see whether plausible parameters can be found which give the correct behaviour of the observed emission.
It may perhaps be premature to jettison continuous wind interactions completely.
Although Allen and Burton, probably correctly, discount the possibility of a collection of individual jets each powering a molecular-hydrogen ‘finger’, excitation by a precessing jet may be plausible, though this depends on whether continuous emission can be maintained as the jet precesses.
Recent work on sporadic jet outbursts by a precessing jet shows considerable promise for explaining families of H-H objects.
Finally, there are puzzling aspects of the oxygen emission, notably that it is particularly strong at essentially zero velocity.
This is very hard to fit into a projectile model.
Whatever the resolution of these problems, Allen and Burton's observations show that many surprises are in store as modern observational techniques enable us to probe into hitherto unexplored regions of interstellar space.
John Dyson is in the Department of Astronomy, University of Manchester .
DAEDALUS
Monopoly money
EVEN the most advanced modern economies cycle regularly between boom and slump.
This reminds Daedalus of an ecology with relatively few interacting species.
The populations of predators and prey both rise and fall violently.
By contrast, a complex ecology with many different species shows a deep stability.
Individual populations may still fluctuate, but the overall balance of the community remains steady.
What lack of diversity destabilizes an economy?
Daedalus reckons it is the single currency.
The EC's goal of a single currency is thus exactly wrong, as you might expect.
Instead, it should achieve financial unification by making all its currencies legal tender in every member state.
The experiment would be highly instructive.
The present national currencies are mutually well insulated.
Their rates of exchange are quite artificial, being set by government interest-rate policies, and are easily exploited by parasitic speculators.
But once pounds and deutschmarks and francs and lira were all competing against each other in every high street, their exchange rates would always reflect what each currency could actually buy.
Even the biggest speculators could not distort them, or make much profit by doing so.
Ecological theory holds that no two species can occupy exactly the same niche.
So Daedalus expects that the various currencies would diversify into different ‘economic niches’.
In doing so, they would highlight the various roles that money plays in society: a medium of exchange or of speculation, a store of value, and so on.
One likely niche corresponds to the scavenger species in an ecosystem.
This currency would dominate declining industries and regions (the British pound seems a natural candidate).
By depreciating against the other currencies as needed, the scavenger currency would insulate them from this local depression.
A society with n independent currencies should be  times as stable as one with a single currency.
Even better, governments could no longer cheat their citizens by inflation.
At present, any government can simply print currency and spend it, enriching itself while impoverishing its citizens.
But in a multicurrency society, the citizens could hedge away from such trickery.
A currency that started inflating strongly would rapidly be abandoned for all purposes, except that of paying tax to the government printing it.
David Jones 
SCIENTIFIC CORRESPONDENCE
Amorphous polymer granules
SIR — Poly- R -3 hydroxybutyrate (PHB), a hydrophobic storage polymer found in bacteria, is used commercially as a biodegradable thermoplastic.
It is stored in vivo in granules whose structure has long been debated.
Isolated PHB is highly crystalline, and it was originally believed that the native granules were also crystalline.
It is now clear, however, that the polymer is stored in cells as a mobile elastomer.
Many hypotheses have been advanced to explain the metastable amorphous state of the polymer in vivo , most recently that it is simply the result of the slow nucleation kinetics that are operative for small particles.
We now provide experimental support for that explanation by preparing artificial granules which effectively contain only PHB and are virtually  indistinguishable from native storage granules.
FIG. 1 Wide-angle X-ray diffraction patterns for a , whole A. eutrophus cells;b , crystalline PHB powder;c , artificial amorphous PHB granules;d , artificial granules dried overnight at 125 °C.
The essence of the in vivo kinetic model is that polymer that is newly biosynthesized in an amorphous mobile form within the granule will only crystallize as a result of homogeneous (spontaneous) nucleation.
In the absence of heterogeneous nucleation or granule coalescence, and ensemble of 0.25-µm granules should have a crystallization half-life of more than 1,000 yr at 30 °C, whereas bulk samples of the polymer quenched to 30 °C from the melt typically crystallize within a few minutes.
It should therefore be possible to reconstitute stable amorphous PHB granules in vitro .
Accordingly, a solution of PHB (99.9% pure) in CHCl 3 (5% wt/v) was sonicated at 20 kHz with 20 vol of an aqueous solution of cetyl trimethylammonium bromide (CTAB, 5 mM).
The resulting emulsion was dialysed exhaustively over 48 h against additional 5 mM CTAB to remove the organic solvent.
Later light scattering measurements confirm that the remaining PHB is particulate, with median diameter 0.3–0.4 µM.
Nycodenz gradients show that the PHB particles have density 1.18 g cm -3 , the same as native granules and pure amorphous PHB; crystalline PHB powder has a density of 1.241 g cm -3 (ref. 8).
The surfactant-coated, amorphous PHB granules were collected by centrifugation for further studies.
The wide angle X-ray diffraction patterns obtained from pastes of either artificial granules of PHB-rich whole cells of Alcaligenes eutrophus show only an amorphous halo, whereas crystalline PHB powder gives a series of sharp peaks (Fig. 1).
In addition, the NMR spectra of the reconstituted granules show the same signals and temperature dependence as the polymer contained in whole cells (Fig. 2), implying essentially identical mobility properties.
No signals are detectable from CTAB associated with the artificial granules.
Isolated PHB powder, which is about 70% crystalline, has no C-NMR spectrum at these temperatures.
Artificial amorphous PHB granules prepared as described are stable in aqueous suspension at 30 °C for at least 6 months.
We have thus established that PHB mobility in vivo can be explained solely on the basis of the intrinsic material properties of the polymer.
Figure 1 d demonstrates that by a simple drying treatment the artificial amorphous granules can be caused to coalesce and returned to their original highly crystalline state.
Presumably, the removal of surface water destabilizes the surfactant-polymer interactions that keep the granules physically separate.
The findings we report here have interesting implication for the preparation of polymer coatings.
Daniel M. Horowitz Joachim Clauss Brian K. Hunter Jeremy K. M. Sanders Department of Chemistry, University of Cambridge 
Scientific Correspondence
Scientific Correspondence is a relatively informal section of Nature in which matters of general scientific interest, not necessarily those arising from papers appearing in Nature , are published.
Because there is space to print only a small proportion of the letters received, priority is usually given according to general interest and topicality, to contributions of fewer than 500 words, and to contributions using simple language.
FIG. 2 Natural abundance 100-MHz  spectra at three temperatures of artificial PHB granules (AG) and whole cells of A. eutrophus (C).
Artificial granules were collected by centrifugation (33,000 g ), resuspended in  and examined in a NMR tube containing a  -benzene capillary.
Spring blooms and stratification
SIR — I would like to comment on the paper by Townsend et al , who reported that spring phytoplankton blooms occurred in the absence of vertical stratification of the water masses.
Balsfjorden, near Tromsø (69° 20' N), is a 60 x 3-km-wide fjord with a bottom depth of 210 m in which the spring bloom takes place in homogeneous water columns.
In these areas, freshwater run-off from melt water starts to increase early in May and peaks in June.
Hence no stratification of the water masses resulting from lowered salinities can be observed.
The spring bloom starts during mid March and culminates at the end of April, following the yearly spring increase in incident radiation, in the absence of water column stratification throughout.
Similar results have also been obtained from Skjomen, a fjord close to the polar circle, and for Ullsfjorden, north of Tromsø.
During 1984–90, cruises to the Barents Sea have shown that the spring blooms may progress in unstratified water masses (H. C. E. & A. Evensen, manuscript in preparation).
Statistical tests clearly show that the biomass increase was correlated to the yearly spring increase in radiation, and not to physical properties of the water, such as temperature, salinity or density.
The figure shows a typical bloom from open coastal areas approximately 60 km west of Tromsø.
The sampling at the station was performed approximately 2 weeks before the culmination of the spring bloom.
During model runs simulating the onset and the progress of the spring bloom the ‘critical depth’ was calculated to be about 55 m.
The vertical distribution of phytoplankton biomass decreased steadily down to about 100 m.
The water masses were nearly isopycnal from the surface to 90 m.
Further down, both temperature and salinity increase, causing a smooth increase in density.
It is widely accepted that for a bloom to develop the critical depth must be shallower than the mixed depth.
In other words, the null hypothesis is that phytoplankton cannot stratify in the absence of a density gradient (if waters are not shallow).
The large bulk of the data that have led to this conclusion originates from areas south of the polar circle.
Looking at the spring bloom maximum at several locations along the coast of Norway and at Spitzbergen, from Korsfjord (outside Bergen) to Kongsfjorden (Spitzbergen), shows that there is a delay in the onset of the spring bloom north of the polar circle.
This lag is due to the time lag in the seasonal increase in total radiation, as also pointed out in reports of spring blooms in areas from Lofoten and southwards.
However, north of the polar circle no significant delays are observed.
The most surprising observation is that the spring bloom occurred almost at the same time in the Tromsø area and at Spitzbergen, approximately 10° further north.
In my opinion, the explanation for this is that until the vernal equinox (21 March) the length of the day increases from north to south in the Northern Hemisphere.
After the equinox, day length increases faster further north, that is, the period between 24-hour darkness and midnight Sun is shorter at high latitudes north of the polar circle.
This means that day length increases faster at high latitudes, and that primary production is enhanced compared to areas further south.
Hans Chr.
Ellertsen Norwegian College of Fishery Science, University of Tromso ,
Cerebellar flocculus hypothesis
SIR — Lisberger and Sejnowski, and Judge in News and Views, discussed what is known about the cerebellar control of the vestibulo-ocular reflex (VOR).
The ‘flocculus hypothesis’ is that a small area of the cerebellum, the flocculus, adaptively controls VOR by regulating signal flow from labyrinths to the flocculus to the relay cells of VOR, and has been challenged on several grounds.
But I believe the hypothesis can still be defended in the light of recent knowledge.
The traditionally defined monkey flocculus includes the genuine flocculus and the ventral paraflocculus.
Earlier data contradicting the flocculus hypothesis were obtained in the ventral paraflocculus.
But, when recorded only from those sites where local electrical stimulation evoked lateral eye movement, suggesting their close connection to the horizontal VOR, the behaviour of Purkinje cells is consistent with the flocculus hypothesis.
The characteristic synaptic plasticity in the cerebellum, long-term depression (LTD), an important basis of the flocculus hypothesis, has now been well documented.
One reason why Lisberger and Sejnowski assume another synaptic plasticity in the brain stem pathway is that the LTD alone does not explain the two-way adaptation, increase and decrease of the VOR gain.
However, this argument overlooks the fact that two subsets of input to flocculus Purkinje cells arise from bilateral labyrinths, which modulate out-of-phase to each other and which undergo LTD separately.
Whether VOR gain increases or decreases will depend on which subset or input undergoes LTD.
Brain stem synaptic plasticity has also been suggested based on measurements of eye movements and related neuronal responses during VOR before and after adaptation in monkeys and goldfish.
However, the evidence is rather indirect.
Further, the fact that lesion of the cerebellum x  including the flocculus in rabbits, cats and monkeys, or subdural application in rabbits and  monkeys of haemoglobin (which blocks LTD), abolishes the entire VOR adaptation, raises the question of why brain stem synaptic plasticity does not show up in the absence of cerebellar plasticity.
Finally, Purkinje cell responses inconsistent with the flocculus hypothesis in monkeys and goldfish have been observed under visual cancellation of the VOR.
Such responses would be free of eye-velocity signals, but at the same time would be contaminated by visual signals or higher command signals acting to cancel the VOR.
The signal contents of these responses need careful reconsideration before they can be related to VOR adaptation.
Masao Ito Frontier Research Programme, RIKEN, Wako ,
LISBERGER AND SEJNOWSKI REPLY —
We are surprised that Ito is so strongly opposed to a model that accounts for his data and ours, and that allows the cerebellar cortex of the flocculus and/or the ventral paraflocculus to be one of two sites of learning in the VOR.
We also find that Ito's response to our Letter contains some omissions and errors that undermine his position.
(1) Ito claims that Watanabe and Miles et al .
recorded from different Purkinje cells in different parts of the cerebellum.
He fails to mention that the two studies recorded in common from at least two folia in the ventral paraflocculus and he fails to cite the fact the individual Purkinje cells in these folia show the same effects reported by both groups.
Therefore neither group's data can be discounted and any model of the VOR must account for both sets of results.
Our model does, and the ‘flocculus hypothesis’ does not.
(2) Ito does not mention evidence that the earliest expression of learning in the VOR evoked by an electrical stimulus in the labyrinth occurs only 5 ms after the stimulus, too soon to allow time for transmission through the cerebellum.
He does not cite the existence of a group of cells in the brain stem that express learning 12 ms after the onset of head motion, before most Purkinje cells in the cerebellum show any response.
(3) Ito incorrectly explains why our model requires sites of learning in the vestibular inputs to both the cerebellum and the brain stem.
Our model requires two sites of learning so that constant head velocity can generate constant eye velocity.
If learning is localized in just one site in the cerebellar or the brain stem pathway, then a constant head velocity generates an eye velocity that ramps inexorably toward positive or negative infinity.
This behaviour is a consequence of the structure of the model and of the fact that the model processes inputs and generates outputs that vary as a function of time.
The need for two sites of learning is unrelated to the exact mechanism of learning and, in fact, the form of long-term depression supported by Ito could be a mechanism of learning in our model.
(4) In defending the ‘flocculus hypothesis’, Ito fails to cite a study done in rabbits showing that motor learning in the VOR can be acquired without vestibular stimulation.
This study contradicts the ‘flocculus hypotheses’, which requires activation of vestibular inputs during motor learning.
Our model, although it requires more than one site of learning, accounts for data from multiple laboratories and reconciles observations that had been thought previously to be contradictory.
Even for a relatively simple behaviour like the VOR, a biological system must face many constraints that require a complex neural network with the possibility of multiple sites and mechanisms of learning.
The challenge is to elucidate the operation of the whole system, not merely to determine the function of an individual structure or an isolated synapse.
S. G. Lisberger Department of Physiology, University of California.
 T. J. Sejnowski Department of Biology, Salk Institute for Biological Sciences, University of California 
Palaeoclimate sensitivity
SIR — Those studying global warming generally define climate sensitivity as the response of the globally and annually averaged temperature to a doubling of CO 2 .
Such a quantity is, in theory, well defined, but to assume that major climate regimes in the past arose from such simple forcing is unjustified.
The use past climate to infer sensitivity to a doubling of CO 2 , as was done recently by Hoffert and Covey, is equally unjustified.
In their Fig. 2, both the warmer and colder climates considered may have had tropical temperatures approximately the same as the current climate.
But the differences in temperature between the tropics and high latitudes were very different.
This, in turn, implies that these past climates were associated with very different north-south dynamic heat fluxes in the atmosphere-ocean system.
Such heat fluxes are important in at least two major respects: first, without them, the annually averaged pole-to-Equator temperature difference for the present climate would be about twice as great as it is.
Second, given that the Earth's major greenhouse gas, water vapour, varies greatly with latitude and altitude, it is impossible to calculate that net greenhouse effect without knowing where heat is deposited by the dynamic heat transports.
In addition, the atmospheric contribution to this heat transport is at least as great as that from ocean currents, and is dominated by fluxes in winter.
Dynamic fluxes depend on motions which in turn depend on spatial variations in heating.
Thus, orbital variations are believed to have caused the cyclic glaciations of the past 700,000 yr (the Milankovitch hypothesis).
These variations involve very small changes in globally and annually averaged insolation, but give rise to major changes in the spatial distribution of heating.
Atmospheric fluxes are likely to depend on the intensity of the Hadley circulation in the tropics.
This intensity, in turn, depends strongly on the displacement of the zonally averaged summer surface temperature maximum from the Equator, and on the meridional sharpness of the zonally averaged temperature maximum.
The former is extremely sensitive to orbital parameters, whereas the latter depends greatly on the nature of monsoons.
Monsoons clearly depend on the distribution of land and sea in the tropics; this was certainly very different during the mid-Cretaceous period considered by Hoffert and Covey.
Incidentally, neither of these two factors depends significantly on gross radiative  parameters like CO 2 concentration or solar constant — at least within the range of variation considered in ref. 1.
Hoffert and Covey's conclusion, that the existence of widely varying climatic states excludes the possibility of low sensitivity to doubling CO 2 , is only plausible for a one-dimensional Earth.
For the real Earth, it ignores factors crucial to climate not included in the somewhat simplistic climate sensitivity appropriate to CO 2 increases.
In particular, if an altered distribution of heating produces a large change in dynamic heat flux, then major changes in global climate may occur, even if the sensitivity to changing CO 2 is extremely small.
Indeed, if it should turn out that the tropics are thermostatically stabilized, then changes in forcing which are associated with little change in dynamic heat flux are likely to cause very little global change.
The fact that the tropics appear to have remained at approximately the present temperature despite major changed in the Equator-to-pole temperature difference (and the associated heat flux out of the tropics) strongly suggests the presence of some stabilizing mechanism for the tropics.
Richard S. Lindzen Massachusetts Institute of Technology 
HOFFERT AND COVEY REPLY — Lindzen challenges our derivation of global climate sensitivity by suggesting that large climate changes could have resulted from changes in poleward heat flow and/or the seasonal and latitudinal distribution of sunlight, that is, by moving heat from one location to another.
Such factors could certainly be the pacemaker of climate change, as in Milankovitch forcing.
We believe, however, that significant globally averaged temperature change would ensue only if additional feedbacks from slowly responding elements of the climate system altered the global mean radiative forcing.
Palaeoclimate reconstructions indicate that the meridional temperature gradients decrease, and poleward heat flow increases, as global mean temperature increases.
This does not mean that changes in poleward heat flow cause significant warming, however,
The global energy balance must be satisfied.
Near-linearity of longwave flux versus surface temperature makes it unlikely that mere redistribution of surface temperature can affect global mean temperature much by changing planetary cooling.
On the solar absorption side, the tropical cirrus shielding cited by Lindzen is a negative feedback on temperature, whereas high-latitude ice albedo-temperature feedback is positive.
Because an increase in poleward heat flow cools the tropics while heating the poles, such heat flow could by itself lower planetary albedo.
But how much would the Earth warm?
Fast albedo feedbacks are too weak to change global temperature significantly without global mean radiative forcing.
That is why the ‘old’ Milankovitch theories fail (see below).
Melting all the present-day sea ice can only increase global radiative forcing by .
The much smaller sensitivities favoured by Lindzen could account only for minuscule fraction of Cretaceous and Jurassic warmth.
Regarding the Hadley cell and monsoon conjectures, current research on Milankovitch forcing indicates a major role of feedbacks on the fast-response atmospheric variables (wind, temperature, moisture, snow and sea ice, clouds) from slowly-responding variables (ice volume, ocean currents, CO 2 ).
Orbital forcing driving an atmosphere model alone cannot simulate the initiation of a glacial cycle, but can do so if the atmosphere is coupled to slow glacial dynamics (ref. 12 and M.E. Schlesinger, personal communication).
Another serious problem with purely atmospheric dynamical models is that the hemispheres were out of phase with respect to the orbital precession cycle believed to have triggered the transition from the Last Glacial Maximum (LGM) deep freeze 11 kyr before present to Holocene warmth.
There is evidence that insolation changes somehow trigger changes in the ocean circulation, biological productivity and alkalinity carbon pumps regulating atmosphere/ocean partitioning of CO 2 , a greenhouse gas.
Coupling with slow ice sheet and ocean feedbacks can account for orbitally-paced planetwide changes of temperature and CO 2 recorded in ice cores in ways that purely atmospheric models cannot.
We used reconstructions of ice sheets and compositions in ice cores to derive the albedo and greenhouse forcing of the LGM at zero solar mean forcing.
With forcing and response known, whether CO 2 changes lead or lag climate changes is irrelevant for deriving the climate sensitivity of the LGM.
The most convincing test of any theory is comparison with observations.
Our objective is to improve models for the prediction of climate change by tests against palaeoclimate data.
We reported early findings that the ratio of global mean temperature change to global mean radiative forcing is very similar for two very different past climates.
Our analysis has been expanded to meridional response patterns and additional palaeoclimates.
Such validations are necessary regardless of one's disposition towards the current generation of global climate models.
We hope Lindzen supports his ideas with quantitative models whose assumptions can be compared with observations.
At this point we find his conjectures implausible when compared with the view that global mean temperature is determined by global mean radiative forcing, including (but not limited to) greenhouse forcing from CO 2 changes.
Martin I. Hoffert Earth Systems Group, Department of Applied Science, New York University,Curt Covey Lawrence Livermore National Laboratory .
BOOK REVIEWS
In the name of genetics Dorothy Nelkin 
Exploding the Gene Myth.
By Ruth Hubbard and Elijah Wald.
Beacon: 1993.
Pp. 206 $24 .
Biology as Ideology: The Doctrine of DNA.
By Richard C. Lewontin HarperPerennial: 1993.
Pp. 128. $10 (pbk).
THE Human Genome Project has encouraged, with research funds, studies of the social implications of a rapidly developing science that promises to reveal the essence of life and to predict our future fate.
Much of this work, by sociologists, ethicists and historians, has been critical, focusing on several related themes.
Science, and in particular genetics, is shaped by social forces and ideological agendas: the history of eugenics is the usual example.
Advances in genetics have troublesome ethical implications, bearing on questions of privacy, justice and equity: the key words are genetic discrimination.
And the implications of genetics must be seen in a political context, for the use of genetic information is tied to prevailing political and economic ideologies and the interests of social institutions.
These two books also explore these themes, but two of the authors, Hubbard and Lewontin, are neither philosophers nor social scientists but biologists — critics of their own colleagues.
How does this influence their perspective?
Hubbard and Wald's agenda is evident in the subtitle of their book: ‘How genetic information is produced and manipulated by scientists, physicians, employers, insurance companies, educators, and law enforcers.’
Writing in accessible terms, they offer a broad overview of the science of genetics and its social implications.
Their intention is to convince readers that genetics will have a powerful influence on their lives and that scientists cannot be trusted to protect them: ‘We cannot just sit by as passive  worshippers or victims’.
They describe their book as ‘a basic survival handbook’.
Their task is to explode ‘gene myths’— that genes are an all powerful basis of health and disease, that biotechnology is the ‘wave of the future’, that science is immune to political and social pressure, that organisms can be explained in terms of inherited tendencies.
Their point is that these myths developed in the political context of inequality in the distribution of power and resources, in the economic context of capitalism with its market pressures, and, above all, in the social context of a science that is controlled by commercial connections.
Scientists, they argue, have interests in defining normality and deviance, health and disease, in terms of genetics rather than social or environmental conditions.
The authors dwell on the conflicts of interest arising from the commercialization of genetics and biotechnology.
Scientists, economically linked to their enterprise and preoccupied with patents, cannot be trusted to protect the public interest even when they seem to try.
The authors suspect the programme funded by the National Institutes of Health on the social and ethical implications of genetics of ‘putting the Human Genome Project in a position to supervise what questions are asked’.
So they conclude with a social policy agenda.
Citizens cannot leave critical decisions in the hands of experts.
They must be vigilant, informed and directly involved in decisions about the development and application of science.
Lewontin's book, a collection of his essays on the ideological basis of contemporary biology, is more about political philosophy than social policy.
He too explodes the myths encouraged by scientific ways of thinking: that it is possible to understand the world in reductionist terms, that there are clear distinctions between cause and effect, that the world is made up of bits and pieces that can be isolated and studied in isolation.
These beliefs, he argues, are just as ideological as the holistic world view of a more mystical, premodern age.
Lewontin is highly critical of ‘biological ideologues’ who perpetuate the belief that science offers simple explanations for social problems.
Rather, he says that ‘things are complicated, uncertain, and messy, that no simple rule or force will explain the past and predict the future of human existence’.
There is, in fact, only a limited relationship between scientific understanding and the social practices that draw support from science.
As history shows, science has ideological and social purposes; it can lend legitimacy to social policies, provide legitimation for social inequities and justify power relationships on the basis of natural categories.
Lewontin describes how the genetic world-view — deterministic and reductionist — encourages the view that human life is what it has to be.
To suggest that everything we are — our health, our wealth, the very structure of society — is encoded in our DNA is simply to justify the status quo.
Lewontin counters such thinking with an interactionist perspective.
Denying the distinctions between nature and nurture and biology and ideology, he calls for ‘an entirely new level of causation’ based on the complex interactions between the biology of living organisms and their environment.
And science too must be viewed as an interaction between scientists and society.
Just as science affects society, so scientific priorities and ideas are shaped by social and economic forces.
For scientists are social beings, their ideas moulded partly by social experience.
That Hubbard and Lewontin are biologists by profession shows up in several ways.
They are especially skilled at popularizing the technical aspects of genetics.
But, beyond general references to market forces and social ideologies, they are not inclined to analyse the structural features of a society that lends credence to gene myths.
While proposing solutions through public participation, they feel no need to define appropriate constituencies or organizational strategies.
Nor do they deal with the real dilemmas of participation that have been the focus of considerable research.
And they can gloss over the social forces that contribute to the appeal of reductionist and deterministic ideas.
Indeed, my research on images of the gene in popular culture is revealing trends that make me wonder about the results of public participation.
Would it encourage decisions that these authors (and I) would really welcome?
Finally, these books are less constrained and more cynical in their criticism of science than those written by nonscientists.
Is it because Hubbard and Lewontin are insiders in the scientific enterprise, protected by their credentials and free to debunk their colleagues?
Or are they reflecting growing disaffection among scientists with recent changes in their profession?
Science, and especially genetics, has moved from academe to the commercial world, tied to private corporations, wedded to profits.
As its image as an independent search for truth has changed, scientists have had mixed reactions.
Some respond by trying to capture media attention: thus the gene becomes a ‘master molecule’ and the genome the ‘Book of Man’.
Others, disillusioned with their colleagues, react with cynicism and dismay.
The perspectives of these two books is clearly influenced by the broader tensions in the scientific community over the changing character of science.
Dorothy Nelkin is in the Department of Sociology, New York University .
Against megabuck science Arnold Pacey 
Invention: The Care and Feeding of Ideas.
By Norbert Wiener.
MIT Press: 1993.
Pp. 159. $19.95 £17.95 .
MANY of us were brought up on stories of great inventors and individualistic engineers, and have since had to reeducate ourselves to understand the development of science and technology as a social process, or even to believe that inventions are ‘socially constructed’.
Of course, there are social processes that need to be appreciated, but the suggestion that everything should be described in these terms seems extreme.
It is refreshing, therefore, to see the publication of this book, written nearly 40 years ago, in which an inventor of real achievement acknowledges ‘an interplay of economic and social forces’ and the importance of the ‘social environment’, but also asserts and describes the role of individual insight.
Norbert Wiener's basic view is that the ideas of individuals can trigger the larger processes that lead to innovation by their influence on the ‘intellectual climate’ in which science, the arts and technology all develop.
Sometimes, new ideas involve such radically novel perspectives that the lack of key individuals ‘in the chain of thought leading up to them…could easily have delayed them for…a generation’.
At other times, though, ideas are less dependent on the right person coming along because they are implied by the current state of knowledge or technique.
Such ideas tend to emerge ‘nearly at the same time in many fields of work, among many individuals, in many countries’.
Awareness of the role of key individuals leads Wiener to make a series of points about how research is organized.
Most notably, he is critical of ‘megabuck science’, meaning the large-scale project carried out by a big laboratory or industrial corporation.
Although he concedes that this kind of research is sometimes necessary, he deplores the tendency to regard it as the norm.
Scientists working in a team, each looking at a separate facet of a problem, may well throw light on details, but they are no more likely to make fundamental discoveries than monkeys with typewriters.
Stemming from his view of the role of the individual, Wiener has some telling comments to make about patents and about the nonsense of leaving scientific research at the mercy of markets and the profit motive.
Such policies usually mean that long-term potential will be neglected, and will result ‘in the inability of the scientist to furnish the long-time and deep-lying developments on which the community as a whole…ultimately depends’.
Where the patent system is concerned, Wiener notices how it is manipulated by industrial corporations to the disadvantage of individual inventors, and one of his main aims in the book is to provoke thought about a fairer approach to patents and about comparable rewards for scientists whose discoveries cannot be patented.
Much of this has a bearing on today's concerns, and one must constantly remind oneself that the book was written in 1954 by an author who is best remembered for his writings on cybernetics.
His arguments do not seem dated, although the ideas about scientific revolutions put forward by Thomas Kuhn could have helped him along in one or two places.
The book is published now for the first time as a result of scholarly study by Steve Joshua Heims of the cybernetics group of the late 1940s.
An introduction by Heims outlines Wiener's own experience as inventor of noise filters, an analogue computer and an electrical network concept.
It is interesting to seek now this experience led Wiener to ignore the conventional boundaries between ‘science’ and ‘technology’, especially when he explains how mathematics is an ‘organ of invention and discovery’.
The implications of this perspective are that euclidian geometry, newtonian mechanics and the probabilistic theories of Gibbs and Planck are crucial points in the history of invention because of the way they changed the intellectual climate.
This short book might have been more accessible to the lay reader and perhaps generally clearer had it included some extra detail or more specific examples.
But Wiener's comments on the corruption he observed in megabuck science are so scathing that he could have been open to a libel suit had he been too specific.
Instead, he quotes Benjamin Thompson, Count Rumford, active around 1800, as ‘the prototype of the scientific adventurers who have beset the present age of great projects.
Where the carcass is, you will find the flies buzzing.’
Perhaps Wiener left this book unpublished because he felt it would start too many battles.
Whatever the reason, its appearance now is both useful and welcome — especially for students of science, technology and society.
Arnold Pacey is at 
PRESENCE of the past — this chameleon appears in Prehistoric Animals in the Modern World by M. Ferrari and F. Pratesi (Prion, £18.95).
With over 120 dramatic colour photographs, the book unearths creatures that survive today largely unchanged from their ancestors of millions of years ago.
Facts and fables P. M Rattansi 
Isaac Newton: Adventurer in Thought.
By A. Rupert Hall.
Blackwell: 1992.
Pp. 468. £19.95, $29.95 .
‘FORTUNATE Newton, happy childhood of science…
Nature to him was an open book whose letters he could read without effort’, wrote Einstein in the foreword to a 1931 edition of the Opticks (1704).
Newton's account of his discovery of the true nature of light and colour has, however, as Professor Hall comments in new biography, ‘misled posterity down to recent times’.
The crucial experiment on which he placed so much weight was ‘largely fictitious, newly invented for its present purpose’.
Hall guides us through the far more tortuous path that led Newton to his discoveries and explains why not all his many critics can be dismissed simply as benighted traditionalists.
The story of the falling apple, which Newton himself loved to tell in his old age, would place his discovery of the law of universal gravitation in the anni mirabili of the plague years of 1665–66 ‘when I was in the prime of my age for invention’.
That, too, is unlikely.
Newton seems at that time to have accepted a variant of the ethereal vortices that Descartes had set rotating around the Sun to carry the Earth and the planets in orbital motion.
Only after he had set to work, at Halley's urging, on the tract that eventually became the first book of the Principia did Newton abandon vortices and envisage attractive — and repulsive — forces in central bodies.
It was Newton himself, again, who, during his priority dispute with Leibniz, encouraged the notion that the conclusions presented in a classical geometrical garb in the Principia were originally derived by him through his fluxional calculus — one more ‘fable’.
It is now clear that ‘the published state of the Principia …is exactly that in which it was written.’
These radical revisions of the conventional account by no means diminish our awe at what Newton achieved in mechanics, mathematics and optics.
Hall skilfully weaves the historical research of the ‘Newton industry’ over the past three decades into the biography.
Only occasionally does the account seem like a patchwork of notes left over from his editorship of the last three volumes of Newton's Correspondence .
Sir David Brewster's victorian biography, first published in 1855, remained the standard account for a long time.
It was not wholly superseded until the publication of Richard Westfall's Never at Rest in 1980 (a condensed, updated version of this biography is soon to be published by Cambridge University Press as The Life of Isaac Newton ).
Hall's substantial biography is about half the length of Westfall's and takes account of much of the more recent historical research.
Roubilliac's bust of Newton.
(Courtesy of Trinity College, Cambridge.)
Brewster was provoked by French suggestions that Newton's surprisingly voluminous theological studies indicated the extent to which his nervous breakdown of 1692 had left him mentally crippled.
Brewster demonstrated that Newton had been a ‘searcher in the scriptures’ throughout his life.
He rejoiced in this confirmation of the essential harmony of science and religion.
Brewster was troubled, nevertheless, by another side of Newton: an alchemical passion that had left behind a hoard of manuscripts, consisting largely of transcripts from alchemical works.
The Sotheby sale in 1936 made the contents of Newton's Portsmouth papers much better known and gave rise to Lord Keynes's famous description of Newton as the ‘last of the magicians’.
Those papers, too, have been extensively quarried by a number of historians in recent years.
What significance does Hall attach to them?
While conceding that ‘thirty years of study and the writing of one million words must have left their mark on Newton's mind’, Hall is inclined, on the whole, to reaffirm his verdict, published in a joint paper in 1958, that Newton attempted to retrieve useful chemical facts from the enigmatic and mysterious terms employed by alchemists to conceal them.
It would be quite rash, therefore, to conclude that Newton in any way accepted the truth or validity of the larger claims advanced by the alchemists.
Hall criticizes the flimsy evidence on which, for example, Newton has been depicted as a member of an active alchemical coterie at Cambridge, supposedly including Barrow, the Lucasian professor whom he was to succeed, and Henry More, the Platonist philosopher.
With scrupulous fairness Hall also reproduces in an appendix a report by Karin Figala of Munich, who provided him with a summary of her own research on Newton's alchemical studies.
But Figala's conclusions have devastating implications for any portrait of Newton which, like that provided by Hall, is still largely cast in the conventional mould, even if at great pains to depict Newton ‘warts and all’.
Figala asserts that Newton was no exception to the seventeenth-century tendency to ‘search for a combination of the exact sciences with magical thought’.
She believes that in Newton's alchemy we can catch a glimpse of ‘at least a small part of his attempt to reconcile magic and science’.
Acceptance of such a reading of Newton's work would move the alchemical (and prophetical) studies of Newton from the periphery towards the centre of his lifelong endeavours.
Many of the documents whose recovery we owe to Hall's industry (notably the ‘Chemical’ notebook and the tract known as De gravitatione ), and much other evidence that finds a place in his own narrative (the Cambridge Platonist attitude to the mechanical philosophy, for example), would then form part of a different story.
Westfall's biography attempted to integrate the theology and the alchemy with what we now see as the genuinely ‘scientific’ activities of Newton.
Here they still appear as regrettable or puzzling distractions.
With its many virtues, this biography os of a piece with Hall's two other recent books — the revised and enlarged edition of The Scientific Revolution (Longman, 1983) and the biography Henry More (Blackwell, 1990).
It is tempting to see these works as belonging to a formidable and sustained effort to return the history of science to where it was before it was invaded by what Hall may see as sociologizing and ‘hermeticizing’ interlopers.
All these books have strengths that justify their place on the library shelf and on every reading list on the history of seventeenth-century science.
But whether it is worthwhile — or even possible — to return to the older project, to which Hall himself made so many pioneering contributions, is a different matter.
P. M. Rattansi is in the Department of History and Philosophy of Science, University College London and .
Born free Roger F. Searle 
The Immunology of Human Pregnancy.
By Henry N. Claman.
Humana: 1993.
Pp. 232. $59.50 .
THE birth of the topic of reproductive immunology is widely acknowledged to stem from a seminal paper by the late Sir Peter Medawar in 1953, in which he recognized and addressed the immunological problems presented by the evolution of viviparity in vertebrates.
Important advances in cellular immunology and molecular biology, especially in the past 10 years, have largely clarified the immunological paradox of pregnancy, but not yet definitively explained how the ‘foreign’ fetus survives to term without provoking maternal immune rejection.
The intellectual challenges posed by the maternal-fetal immunological relationship are now particularly relevant: issues such as HIV infection in pregnancy, and the development of immune approaches to birth control and fertility regulation, attract much attention.
Recently, this rapidly moving and multifaceted subject has been the focus of several comprehensive state-of-the-art texts such as Immunology of Pregnancy edited by G. Chaouat (CRC, 1993) and The Immune System in Disease edited by G. M. Stirrat and J. R. Scott (Baillière Tindall, 1992).
It is against this background that Henry Claman's review has to be judged.
Claman sets out to explore in depth fundamental questions, but readers expecting such will be left unfulfilled and let down.
The book has been written very much with the busy clinician in mind and conveys only the most superficial impression, an approach that is less than satisfactory for the serious researcher or specialist.
It is therefore curious that although the author's original impetus for writing was his involvement in the controversial treatment of recurrent spontaneous miscarriage by immunotherapy, he has largely ignored the topical clinical arena.
This is a considerable shortcoming.
Disappointingly for a book published in 1993, there are few references more recent than 1991 (and very few 1991 references at that).
On the credit side, the text is highly readable and will easily guide the non-specialist clinician through the complexities of fundamental immunology and provide a useful introduction to the immunology of human pregnancy.
The format of the book follows predictable well-tried themes, covering immunological aspects of the placenta, maternal immunocompetence during pregnancy and maternal responses to fetal antigens.
The style is relaxed and informative.
Topics of current intense interest such as the expression and regulation of HLA-G class I molecules on trophoblast cell populations at the maternal-fetal interface are mentioned, but inevitably provide a picture from around early 1991.
The implications of HLA-G expression for the outcome of pregnancy are given scant attention and ‘remain to be explored’, which today is simply not true.
Similarly, statements such as‘the study of cytokines in the placenta is just starting’ are nonsensical in light of the rapid advances highlighting the crucial regulatory role that growth factors play in reproductive processes.
The chapters on immune disorders of pregnancy and recurrent pregnancy loss, however, are especially informative and Claman is here to be congratulated for summarizing  succinctly a large body of data in a balanced and objective way.
The attraction of the book is that it gives clinicians an easily readable albeit superficial overview that will serve as a useful introduction: and even specialists in the field are likely to find useful information in one or more of the chapters.
Roger F. Searle is in the Department of Immunology, The Medical School, University of Newcastle upon Tyne ,.
Molecular receptors in the round Leonard F. Lindoy 
Macrocyclic Chemistry: Aspects of Organic and Inorganic Supramolecular Chemistry.
By B. Dietrich, P. Viout and J.-M.
Lehn VCH: 1993.
Pp. 384.
DM148, £61, $99..
MACROCYCLIC ligands — cyclic organic molecules with a central cavity — are used by nature to bind metal ions in such important molecules as haem, chlorophyll and vitamin b 12 .
Several natural antibiotics are also macrocyclic and work by metal binding.
There are good reasons for nature to choose macrocyclic species to bind metal ions — foremost among these is that the binding is usually considerably stronger than for comparable noncyclic ligands.
In addition to natural rings, a lot of synthetic macrocyclic molecules have been investigated over the past 30 years.
Many can act as receptors for small molecules as well as different ion types (including anions).
Ionic and molecular recognition is a frequent characteristic of such host-guest interactions, aided by a cavity in the host with fixed or semi-fixed binding sites.
Host-guest assemblies of this type fall in the realm of supramolecular chemistry — the chemistry of synthetic receptors and large molecular assemblies that often mimic nature's chemistry.
Supramolecular chemistry is now a large and expanding field that is serving to underwrite an emerging new age in chemistry: the age of molecular devices.
In 1987, the Nobel prize for chemistry was awarded to Charles Pedersen Donald Cram and Jean-Marie Lehn for their pivotal contributions to macrocyclic and supramolecular chemistry.
This book is based on a series of lectures on macrocyclic chemistry delivered by Professor Lehn at the Collège de France some years ago.
It is divided into two sections: ‘Macrocycle synthesis’ and ‘Macrocyclic Complexes — Cryptates’.
The first section contains a valuable discussion of synthetic approaches and general strategies for obtaining macrocyclic ligands.
In keeping with the French group's formidable record in synthetic macrocyclic chemistry, the content of this section is wide ranging covering both practical and theoretical aspects.
The remaining section focuses on the complexes of both natural and synthetic macrocyclic molecules and presents a panoramic view.
Included in and extended discussion of the chemistry of macrobicyclic ligands (the cryptands) and their complexes (the cryptates).
Overall, the book provides an effective way for researchers and graduate students to gain a foundation in the fascinating concepts of the field.
It supplies the background for appreciating the many exciting advances currently unfolding.
Leonard F. Lindoy is in the Department of Chemistry and Biochemistry, James Cook University ,.