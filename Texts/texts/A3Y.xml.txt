

After the deluge, life goes on for trout
By MICHAEL SMITH
AFTER such a long hot summer, with rivers running dry, it's difficult to imagine what a good flood looks like.
But — very soon — sluggish trickles over riverbed pebbles will be transformed by heavy rain in a matter of hours into dangerous raging torrents.
What happens to a river's aquatic life when the deluge strikes?
The answer, it seems, is that next to nothing is known about it.
Paul Giller and Helena Twomey, of the Department of Zoology at University College, Cork, have been awarded a grant by the British Ecological Society to try to find out what damage this water-based equivalent of a hurricane can do, and how quickly river life returns to normal.
Some of their early findings are very surprising.
What they have done is to look in detail at the impact of two severe floods which hit the little Araglin river in Co Cork in August 1986 and how it has recovered since.
The Araglin — a stony, soft-water, trout river — was chosen because of a welter of data on its fish and invertebrate populations collected well before the flooding.
What they found was that the river bottom invertebrates which spend their larval stages among the gravel and pebbles on the riverbed plummeted in numbers immediately after the inundation.
They dropped from between 4,000 and 9,000 per square metre of riverbed — a natural variation according to time of year — to about 500 per square metre.
Two years later their numbers were still no higher than 2,000 per square metre.
The species richness of this invertebrate community also fell by half — 69 species were recorded prior to flooding — and had not recovered after two years.
Among the groups of invertebrates worst affected were those that prey on others, the grazers on plants such as algae, and the so-called detritus collectors.
But the big surprise was what happened to the Araglin's brown trout population.
They were hardly affected by the deluge, even though their diet consists very largely of the very riverbed invertebrates which were so badly affected.
Only the few three-year-old trout in the river and the salmon disappeared temporarily — but were back again within three months.
Giller and Twomey suggest that the fish coped perfectly well with the floods because the Araglin river anyway has an over-abundance of invertebrate food.
So a catastrophic drop to 5 per cent of its pre-deluge invertebrate quantities still provided them with three square meals a day.
Gut analysis did not suggest, either, that the trout changed diet after the flood depleted the aquatic invertebrates, to consume more terrestrial flies dropping on to the river's surface.
But what is not clear is why such an excess of water-living invertebrates — if this is the explanation — does not support a larger population of fish (and water birds such as Dippers and Grey Wagtails) in the first place.
With so few studies of the impact of severe floods on river life, knowing how typical the response of the Araglin is could be anybody's guess.
Giller and Twomey reason that their river seemed more severely affected than the very few others ever studied.
Perhaps, they argue, it is because it suffered two sudden floods in quick succession at a time of year — summer — when the invertebrate populations are least adapted to cope.
Whatever the answer, do not fall into the trap of imagining that the impact of a flood in one of this year's drought-ridden rivers will pass as quickly as the time it takes for the river to ‘look’ normal again.
If your home got hit by a tornado it wouldn't recover quickly either.
A feast fit for a prawn: Revolutionary food capsules may lead to bigger and better harvests of shellfish.
Ted Nield reports
By TED NIELD
PRAWNS are not small fry.
According to the Shellfish Association of Great Britain, they are our most popular seafood.
Four out of every five restaurants serve them, and our import bill reached £100m in 1987.
Britons now chomp their way through more than 27,000 tonnes a year — about half a kilo for every man, woman and child.
Dr Eric Edwards of the Shellfish Association says: ‘Customer reaction is that they are stylish and upmarket.’
Demand in Europe and North America continues to rise by 4 per cent a year -despite the fact that natural sources are already overfished.
For this reason, prawn farming thrives in South and Central America, Taiwan, China, Japan, Indonesia, the Philippines and Thailand, where tropical species grow very quickly, giving two or three crops per year.
From a modest 1,000 tonnes a year in the early 1970s, the world's prawn farms are expected to produce 440,000 tonnes by 1990.
But while rising demand, high prices and fast growth provide the stimulus, the prawn's life-cycle presents many problems for the would-be farmer.
A team of scientists from the distinctly untropical environment of the University College of North Wales in Bangor has solved some of these problems by designing food fit for a prawn to eat.
But this tale of commercial enterprise by academics has, ironically, rebounded on them.
Peneaeus monodon, the Asian prawn, spends much of its life in coastal waters, migrating at maturity to deeper water for spawning.
Here the female may produce 800,000 eggs which hatch within 36 hours into larvae.
The larvae undergo a complex cycle of 12 stages in almost as many days, making their way back inshore as they reach their adult form.
Traditionally, prawn farmers have trapped these young adults (or ‘post-larvae’ as they are called) and put them into ponds.
But with this method, supplies are unreliable.
Ideally, farmers would like to grow the larvae from eggs, and so a new industry — prawn hatcheries — has grown up to meet the demand.
There are more than 1,500 hatcheries in Taiwan alone.
Hatchery owners then face the problem of getting supplies of the right feedstuffs for their metamorphosing crop.
During the larval cycle, prawns change their diet dramatically, from plant plankton to animal plankton and finally to the creatures which live in the sediment.
In the past, hatcheries have had to rely on live feed, which is as unreliable and expensive to harvest from nature as it is to grow artificially.
Clearly, some form of artificial feed was needed.
However, throwing organic matter into water feeds the bacteria as well as the prawns, which are quite fussy about who shares their environment.
So the problem was to create a feed which came in particles of the right size (five millionths of a metre across), could be freeze-dried, would not foul the water, dissolve or feed the bugs.
Until it was developed, prawn hatcheries would remain at the mercy of factors beyond their control.
The North Wales scientists came up with the solution.
A team led by Dr David Jones, of Bangor's renowned School of Ocean Sciences, designed a revolutionary new feed, using a process called microencapsulation which puts a thin, digestible membrane around each tiny food particle.
‘It's exactly the same, essentially, as making nylon,’ he says.
‘To do that, you take a lot of identical molecules called amines and join them together to form a chain.
To make nylon thread, you dissolve the amines in a liquid.
Then you take an acid chloride, and dissolve it in another liquid which won't mix with the first one.
‘When you put the two together, the cross-linking reaction occurs, and all the amine molecules at the interface of the two liquids join together in a polymer.
By drawing the polymer off, new cross-links are formed and you go on drawing the thread until all the constituents are used up.
‘Now if, instead of having a single interface between the two liquids, you emulsify the two (like oil and vinegar in French dressing) the reaction occurs over the surface of each suspended droplet, creating a capsule.’
Originally, the team mixed prawn nutrients with amines, thinking that they would be needed to form the capsule.
But it soon became obvious that the proteins of the nutrient contained the correct chemical groups, and that the food molecules themselves were polymerizing and creating the capsule membrane.
‘Haemoglobin, chicken egg and many other natural proteins are rich in the important chemical groups, and these cross-link without any problem at all.
We have now been able to remove all non-nutrients from the mixture — which is lucky because amines are actually very toxic.’
After their initial breakthrough, the team still had much to do before the process became commercially viable.
The first membranes they made were far too thin and would not survive transfer into air, let alone freeze-drying.
‘Once we had solved the thickness problem, it became a commercial possibility,’ says Dr Jones, ‘and Mars — the people who make the chocolate bars — became interested.
Eventually we got together to devise the patents and scale up the process.’
A subsidiary company, Frippak Feeds, was set up in Batley, West Yorkshire to manufacture the feed — the only one which can be used exclusively, without the need for supplements.
Developing the diet has had great scientific spin-offs.
‘It's amazing that a multi-million pound industry was built on an animal nobody knew anything about,’ Dr Jones says.
‘We have 50 years of data on the chicken or the pig.
We know exactly how to change their feed day by day to produce optimum growth.
None of this had been done for the prawn.
‘Through refining our feed, by leaving things out and seeing what happens, we have been able to define the dietary requirements of the prawn throughout its development.
We've learnt about its physiology and how it changes to keep pace with the larva's changing eating habits.
‘We know how fast food moves through its gut, how much energy it takes to make one post-larval prawn from the day it hatches, and so on.
We've learnt an enormous amount because the microcapsule is such an ideal tool for simplifying and controlling food intake.’
Dr Jones and his team are now extending their work to other species, and to the post-larval stage, which leads to the ultimate product — the prawn on the end of your fork.
And expertise gained has enabled the university to launch a new MSc course in shellfish biology, fisheries and culture — due to take its first students in the coming academic year.
‘The commercial aspects are very good, of course— though I still find the teaching and research spin-offs the most exciting part of it all,’ says Dr Jones.
But, he admits, one would not have happened without the other.
It is tempting to see this tale as a classic example of how academic work can stimulate opportunity, create wealth, and enrich both research and teaching.
But Dr Jones's involvement with industry has led him into difficulties — which lend a critical edge to the parable.
‘Frippak support our work generously,’ he says.
‘But they are understandably reluctant to pay for research students, because they will probably go off and join the competition.
But I had to get research students from somewhere, so I asked the research councils to fund them, in the usual way, by submitting research proposals.
‘That didn't work either.
The project's got alpha ratings, but the council saw the industrial involvement and said ‘Go to industry — we're short of money’.
‘It's a real Catch 22 situation.
Once you get working with industry, there's a feeling that industry must pay for everything.
But I don't see why they should be asked to shoot themselves in the foot by paying to train a competitor's workforce, and neither do they!’
The end result is that Dr Jones takes on overseas researchers.
‘That's fine as far as I am concerned’ he says.
‘They don't cost British industry or taxpayers a thing, but of course they take all their expertise abroad.
‘Whatever the Government's idea is, I don't think that's it, somehow.’
Seeking an ethical watchdog for the genetic scientists
By TOM WILKIE
IN A RARE display of consensus last week, industry, consumers' organisations and the environmental movement called for a national committee to examine the social and ethical implications of biotechnology and genetic engineering of plants and animals, Tom Wilkie writes.
The proposal came at a meeting organised by the National Consumer's Council, a representative body established by statute in 1975.
Ann Foster, the council's food policy adviser, said: ‘There should be the equivalent of a Warnock Committee to look at aspects of genetic manipulation other than simply matters of safety.’
The Committee of Inquiry into Human Fertilisation and Embryology was set up in July 1982 under the chairmanship of Dame Mary Warnock to investigate in vitro fertilisation and human genetic research.
Ms Foster put forward the idea of a similar investigation for plant and animal biotechnology because she was troubled by the unnecessary secrecy and seeming insensitivity of government departments to issues other than the narrow technical one of safety.
She stressed that such an investigation would not hinder progress or hold up commerical companies, but would further public acceptance of the technology and so foster developments.
The suggestion was supported both by companies involved in biotechnology and by environmental groups.
The NCC meeting provided an unprecedented forum for all sides — representatives of consumer interests, environmental groups such as the Green Alliance, and commercial firms such as ICI — to come together to discuss the implications of the new biotechnology.
They exhibited a surprising degree of consensus and common interest throughout the meeting.
The only persistent jarring note came from civil servants responsible for framing safety regulations and administering the various safety committees.
as if they were stuck in the 1950s, the civil servants could see no way in which consumer, environmentalist or the broader public interest could be represented.
In stark contrast, industry seems almost eager to put all its cards on the table.
Keith Pike, a manager of ICI Seeds, told the meeting that ‘industry likes to know where it stands.
We are investing millions in these technologies and we want to know what are the rules of the game before we take the risk of bringing a product to the marketplace.’
Mr Pike welcomed the idea of a general Warnock-style committee on the ethical aspects of the technology.
But he opposed any suggestion that representatives of the environmental movement might sit on the technical committees set up to decide on the safety of any release of genetically manipulated organisms into the environment.
ICI is in the vanguard of genetic engineering.
Its scientists have snipped through the genes of tomato plants to produce a version that will ripen and develop full flavour without going soft and spoiling in transit between the grower and supermarket shelves.
If its research is successful, it should mean the end of mass-produced tomatoes that are tasteless because they are picked before they are fully ripe to prevent wastage during handling.
If the product is approved, genetically engineered tomatoes could be on the supermarket shelves by 1993.
Slow progress in the war on diabetes: Nicholas Russell says investigative research may beat flashy technology
By NICHOLAS RUSSELL
ONE IN 50 Britons suffers from sugar diabetes and, despite 100 years of scientific study, we still do not know how it is caused.
It is unlikely that further knowledge will improve treatment, but it may allow prevention of the disease, at least in a proportion of cases.
Normal individuals maintain the concentration of blood sugar at a constant level by their own insulin, which causes cells in muscle, liver and fatty tissue — target cells — to take up excess circulating sugar and store it.
The diabetic is unable to regulate sugar in this way.
In theory there are two distinct versions of the disease.
Either no insulin is produced (Type 1 diabetes) or the muscle, liver and fat cells fail to respond to insulin (Type 2).
In practice the distinction is not so clear-cut.
When insulin was isolated at the Toronto Medical School in 1922, a far-sighted decision by an executive of the Eli Lilly Company ensured the dominance of his firm in commercial production.
With astonishing speed, in the era before regulation of new drugs, Lilly developed systems for extracting pork and beef insulin from slaughterhouse offal.
Effective therapy has therefore been available for more than 60 years.
There are defects, the most obvious being the inconvenience of daily injections, the relatively crude control they provide (sometimes resulting in coma) and long-term damage to organs, including eyes and kidneys.
Some of these are unavoidable, others are said to result from the use of animal insulin, which differs slightly in structure from human insulin.
In the 1970s, genetic engineering made it possible to clone human insulin genes in micro-organisms which secreted insulin with the same structure as natural human material.
Two systems for engineering a ‘human’ insulin were developed and most patients have been switched to these engineered products.
But doubts surround the long-term benefits.
There is alarm at the subtle changes in reaction to the new insulin, which seems to give less warning of the danger of erratic sugar control.
The new techniques were used to ‘improve’ existing therapy, while fundamental research into the causes of diabetes has only recently begun to yield important results.
Three papers published recently in Science move us a little closer to understanding the basis of the disease, which turns out to be highly complex.
Most diabetes is the Type 2 form, in which the patient still generates insulin, but the body's target cells fail to respond to it.
Detailed biochemical work has shown that target cells carry an insulin receptor molecule on their surfaces, which reacts with insulin molecules in the surrounding body fluids.
A series of complex activation steps then occurs which changes the state of the target cells.
One consequence is the appearance of sugar-transporting molecules which bring sugar in from the surroundings.
Diabetics may develop errors at any stage in this process, making their cells resistant to insulin stimulation.
Therapeutically, this resistance can often be overcome by insulin injections.
The increase in concentration of insulin seems to force a response from the slow or faulty system.
Two of the papers in Science came from Japan and reported on individuals suffering from Type A syndrome, a rare variant of Type 2 diabetes.
In both cases it seems that the problem lay in the transmission of the activation signal across the target cell membrane, once insulin has bound to the surface receptor.
The activation molecule is an enzyme called ‘tyrosine kinase’ and in both individuals this molecule was not functioning properly because they had inherited a faulty kinase gene.
But the faults in the genes and the resultant proteins were different in each case: one producing an enzyme molecule with large structural changes, the other having a single amino acid change at a critical part of the molecule.
The third paper, from the US, reports work with rats given diabetes experimentally with the drug streptozotocin.
Analysis of cells from these rats showed that drug action had interfered with the expression of genes for some of the proteins designed to move sugar into muscle and fat cells, exactly the target tissues which must be stimulated by insulin if they are to take up excess sugar.
This provides circumstantial evidence that some cases of human Type 2 diabetes result from a similar failure.
Working out the whole story will be a long, difficult process of unravelling the pathways of insulin action in cells.
By contrast, genetically-engineered ‘human’ insulin is flashy stuff, a rapid application of advanced molecular genetics to modify a therapeutic product.
But this modification may be more of a public relations coup than a genuine therapeutic improvement.
In the long run the technological hare may be outrun by the investigative tortoise.
Parallel lines to the future: Britain is among the front runners as tomorrow's supercomputers take shape, says Greg Wilson
By GREG WILSON
FROM desk-top micros to the largest mainframes, conventional computers are built around a single processor and a single store of memory.
This architecture was adopted because it was simple to build, fitted the theoretical preconceptions of early computer scientists, and seemed to offer the greatest reliability.
At a seminar hosted by the Edinburgh Concurrent Supercomputer Project last month, researchers and vendors could finally say with confidence that supercomputing in the 1990s and beyond will be done by parallel computers.
There are fundamental limits to how fast a conventional computer can go.
Data cannot be moved from the processor to the memory and back faster than a certain speed, which limits the performance of the computer as a whole.
Another limitation is the extent to which components can be shrunk before quantum effects decrease their reliability.
Faced with these problems, computer scientists began in the mid 1970s to experiment with using many processors in a single machine to work in parallel on a single problem.
The advantage of this approach is that it can be scaled up to build computers of any size and speed.
There are practical difficulties with parallel computers.
For one thing, builders had to solve the problems of making processors communicate with one another quickly and reliably.
More important was the software — how should a parallel computer be programmed?
Would it be possible to re-use old programs written for conventional machines, or would users have to start from scratch and throw away their existing investment in software?
While some manufacturers advocated the latter, a more popular approach has been to build computers out of conventional processors so that many current programs and, more importantly, programming concepts, can be recycled.
There are now manufacturers world-wide pursuing variations on this approach, building everything from high-powered graphics work stations for computer-aided design and advertising, like soon-to-merge American firms Ardent and Stellar, through to Teradata, another American company which has sold several hundred of its parallel database machines, to Britain's Meiko Scientific, manufacturer of the Computing Surface.
Meiko was set up in 1985 by members of the group which developed the transputer chip for Inmos.
The transputer contains a processor, memory, and communications links on a single chip, which makes it an ideal building block for multi-processor computers.
More importantly, the transputer allows modular supercomputers to be built.
When users want to improve the performance of their machine, they do not have to throw it away and buy a newer model.
Instead, they can buy more processors and add them to the existing machine.
The success of this approach can be judged by Meiko's growth.
From six members, the company has grown in less than four years to a staff of more than 90, and a base of more than 200 installed systems.
Only 27 per cent of these are in academia; the rest have gone to research (20 per cent), the military (23 per cent ), the financial sector (11 per cent ), and direct industrial use (19 per cent ).
Still owned by its employees, Meiko has broken into the American and Japanese supercomputer markets, and now has offices across the US and in Europe.
The largest of its Computing Surface machines is at the University of Edinburgh.
Faced with the loss of their previous high-performance computer in 1987, the university got backing from the Science and Engineering Research Council and the Department of Trade and Industry to purchase a large Computing Surface to serve both as a research vehicle and a flagship site for Meiko.
The Edinburgh Concurrent Supercomputer Project (ECSP), under Professor David Wallace, now has the largest parallel computer in Europe, with 400 processors which can perform 400 million arithmetical calculations a second.
The example of the installation at Edinburgh has led to several million dollars' worth of business which, according to Meiko, would have been much harder to obtain without a such a proof of cost-effectiveness.
What this power has meant to the scientific and industrial users involved in the ECSP was made apparent during the project's second annual seminar last month.
Applications ranging from molecular sequencing (an important tool in bio-engineering) to modelling oil reservoirs, image processing and high-energy physics were described.
Modern science is critically dependent on high-performance computing; studies of the world's changing climate, structural engineering, and medical imaging simply could not have progressed to their present state without access to the sort of computing power that can only be provided by parallel machines.
Much funding for the ECSP has been raised from industry.
Major industrial partners, like Shell, Hewlett-Packard, and Meiko itself have contributed both money and equipment to help the project to the point where it can offer a reliable multi-user service to people from all over the UK.
In exchange, ECSP has developed a pool of software and made these available to its sponsors.
The keynote speaker at the seminar, Dr Paul Messina of the California Institute of Technology, emphasised that such developments are an international phenomenon.
His group has been involved in parallel computing since the early 1980s; software produced at CalTech has become a de facto standard for the various ‘hypercube’ machines manufactured in the United States.
He pointed out that every major supercomputer manufacturer, including such giants in the conventional supercomputer market as Cray, were now building multi-processor machines in order to stay competitive.
Parallel computing has matured so much that users and manufacturers are beginning to discuss standardisation.
Until now, each parallel computer has been so different that users had to re-write applications every time they changed machines.
Now that CalTech's operating environment is being marketed commercially by Parasoft under the name Express, and with Meiko's introduction of a machine-independent programming system called CS Tools, a single, uniform environment is becoming increasingly likely.
The emergence of such standards, and the ability to re-use programs written in conventional languages, should bring down the last barriers to wide acceptance of parallel computing.