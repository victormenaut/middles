

Background to the NFER Project
The origin of the project
This report is concerned with an NFER project set up to study the feasibility of devising tests in mathematics at several levels of difficulty, for lower attaining pupils aged 14 to 16 years.
Such a series of tests is generally referred to as "graded" or "graduated" tests.
The project was one of three commissioned by the Department of Education and Science (DES) in their post-Cockcroft Report programme of research development.
The Welsh Office Education Department (WOED) provided funds to include two local education authorities (LEAs) in Wales in the project and to carry out some testing in Welsh.
One of the other two projects in the DES programme was concerned with an operating system of graduated tests.
It was known as SSCC from the first letters of its four partners: the School Mathematics Project (SMP), Suffolk LEA, Chelsea (now King's) College, London University, and the Cambridge, Oxford and Southern Schools Examination Council (COSSEC).
The schools in the project were located in a number of LEAs including Suffolk, SMP provided the tests, COSSEC the accreditation, and King's College evaluated the project.
The third project in the programme, known as LAMP (Lower Attainers Mathematics Project) was based at the West Sussex Institute of Higher Education, with the aim of developing a curriculum in accordance with the principles of the Cockcroft Report.
The project team worked with teacher-researchers from six LEAs: Dorset, Hampshire, Isle of Wight, Surrey, West Sussex and East Sussex.
These projects arose from the recommendations in the report of the Cockcroft Committee on the teaching of mathematics published at the beginning of 1982.
The Committee recommended (para 243) that: Mathematics teaching at all levels should include opportunities for:
exposition by the teacher;
discussion between teacher and pupils and between pupils themselves;
appropriate practical work;
consolidation and practice of fundamental skills and routines;
problem solving, including the application of mathematics to everyday situations;
investigational work.
The Committee also made a number of recommendations specifically related to the curriculum and assessment of lower attaining pupils.
For example, their report stated that syllabuses for pupils of lower attainment had been constructed from those designed for pupils in the top quarter of attainment by deleting a few topics and reducing the depth of treatment of others.
By contrast, Cockcroft stated (para 450) that the development of the mathematics curriculum for lower attaining pupils should be "from the bottom upwards" by considering the range of work which is appropriate for lower-attaining pupils and extending this range as the level of attainment of pupils increases.
The Cockcroft Committee's recommendation relating to graduated tests is stated in para 553 of the report: a study should be commissioned to consider whether it is possible to devise a means of providing evidence of achievement in mathematics for lower-attaining pupils in ways which will support, and not conflict with the provision of suitable mathematics courses in schools.
The Committee's views on the issues involved in obtaining evidence of the achievement in mathematics of lower-attaining pupils are contained in paras 537–556 of their report.
These paragraphs are reproduced in Appendix I (p.248) to this document, together with some other relevant paragraphs.
The main points from these extracts are summarized below.
It was pointed out that candidates obtaining low marks in public examinations were demonstrating more of what they did not know than what they did know.
Most current schemes of assessment for lower-attainers existing at the time the report was written, concentrated on computational skills.
They therefore encouraged correspondingly narrow-based teaching which had attracted adverse comments by Her Majesty's Inspectorate (HMI) in the National Secondary Survey.
In paragraph 521 the Committee gave "two fundamental principles of examining" .
Examinations should enable candidates to demonstrate  what they can do, and should not undermine the confidence of those who attempt them.
They should provide suitable targets and reflect suitable curricula.
The Committee thought that these principles might be satisfied by using a series of tests from about the age of 14.
These could be criterion-referenced at each level* and have a high pass mark.
It had been reported to the Committee by schools operating such a series of tests that the motivation of their pupils was increased because tests at different levels provide both an incentive and evidence of progress over several terms.
The content of the tests, it was stated, should reflect that of the "Foundation List" of topics given in para 458 of the Cockcroft Report.
The Cockcroft foundation list is reproduced in Appendix II to this report.
Cockcroft stated that the list should "constitute by far the greater part of the syllabus of those pupils for whom CSE is not intended, that is, those pupils in about the lowest 40 per cent of the range of attainment in mathematics" .
The brief of the NFER project was to investigate the feasibility of devising graduated tests according to the principles listed above.
In addition, a study of the feasibility of devising criterion-referenced tests rather than norm-referenced ones was to be undertaken: that is , tests which give a description of the achievement of pupils who take them rather than placing the pupils in a relative order of achievement.
The NFER was also asked to review current assessments of lower attainers in order to update the information on schemes recorded in the Cockcroft Report (paras 541–544).
The aims of the project
The aims of the project were:
to draw up a framework for graduated tests which would include a specification of their content, context, and alternative schemes for defining levels of performance;
to develop and pilot-test items and consider the nature of the tests into which they might be assembled;
to consider how the administration of the tests might be organized in schools;
to evaluate the evidence obtained of the feasibility of producing and using graduated tests.
The development of the project
Assessment framework and test development
The project team used the assessment framework developed by the Assessment of Performance Unit (APU)(see Appendix III) and the results obtained by the Unit in their mathematics surveys as the starting point for test development in this project.
Working with LEAs and schools
The work of developing tests and achievement criteria was carried out in association with schools in 19 LEAs.
(i) The liaison groups
The project began in September 1983 by recruiting liaison groups of teachers in six LEAs in England and two in Wales.
Altogether teachers from 36 schools participated in these groups; a member of the project team was assigned to each group and meetings were initially held once or twice a term.
Overall, the schools represented a range of characteristics relating to location (rural/urban); size; age-range; proportion of ethnic minority pupils.
The schools in the liaison groups were asked to:
provide facilities for the research team to trial test items with their lower attaining mathematics classes;
carry out some test trials themselves;
comment on the tests and test items and discuss issues relating to their development and administration in the schools;
discuss more general issues concerning the idea of graduated tests.
(ii) Second-phase schools
In September 1984 a further 33 schools joined the project, three from each of 11 LEAs in England.
These schools were asked to provide facilities for trying out amended versions of tests first used in liaison group schools.
Communication with these schools was largely by post, and sometimes by telephone.
In six of the LEAs introductory meetings were held with representatives from all three schools present.
The development of work with schools
From September 1984 until April 1986 schools associated with the project were sent test assignments once or twice a term.
Meetings with liaison groups began to falter early in 1985 when the teachers' dispute began to affect school activities other than actual classroom teaching.
In fact only two liaison groups in England met regularly from the beginning of 1985; from September 1985 meetings were arranged twice a term for these two groups.
However, all but three of the 36 liaison group schools collaborated with the research team throughout by continuing to trial packages of materials sent to them and to provide facilities for the research team to visit the schools.
Similarly, 29 of the 33 second-phase schools continued their collaboration with the project.
The main effect of the situation in the schools was that the development work on practical, oral and investigative assessments was slowed and the collection of views from teachers on issues relating to graduated tests was reduced.
However, some development was possible in each test mode and views on issues were obtained from a number of liaison groups and second-phase schools.
The pupils and the assessments
In the course of the project, assessment materials were developed in a variety of modes.
These included written, practical, oral, mental tests and a few microcomputer administered tasks.
There was also a limited inquiry into the assessment of investigative work.
The tests developed by the project team were not related specifically to the curriculum operated by the schools associated with the project but they were based almost exclusively on the topics in the Cockcroft foundation list.
Completed test scripts were marked at the NFER and computerised data printouts of results for individual pupils fed back to the schools.
More general results were disseminated to the project schools in a series of newsletters.
Most of the pupils participating were in the fourth year, but some third years were also included in the last round of testing.
The purpose of the testing was to obtain empirical data on the test items in respect of their difficulty and their discrimination, and to consider their placing in possible graduated test levels.
A reference test was developed in order to control for any difference in the samples of pupils who took the test materials.
The reference test (known as GT4) consisted of items used in the Assessment of Performance Unit's mathematics surveys (with the permission of HMSO and the APU); the parameters of each  item were therefore accurately known for national samples representative of the whole range of attainment.
Questions of feasibility
Graded assessment, referred to as "Graduated tests" in the Cockcroft Report, has a number of features associated with it, some or all of which may be found in existing schemes.
These are: 1 detailed specification of assessment tasks; 2 careful specification of learning objectives, derived from the subject matter and teaching strategies which involve choices about the subject matter to be taught and assessed; 3 a sequence of tasks generally representing some kind of progression.
This may be linked to:(a) structures of the subject being taught;(b) progression in the number of components of the subject pupils may be expected to have learned in the course of their instruction;(c) the degree of difficulty of typical test items;(d) sequences which correspond to the teaching approaches of the designers of a graded assessment scheme; 4 all of the items in a test are at a similar or the same level of difficulty; 5 tests are intended to be taken only when pupils are ready for them; 6 performance on tests is described in terms of "pass' and" fail", rather than a mark or grade.
The term "mastery" is often used to describe proficiency in a graded assessment scheme; 7 graded tests are generally criterion-referenced.
There are two overriding questions relating to the feasibility of such tests, both leading to a host of technical and organizational questions, only some of which could be adequately tackled in this project.
The two main issues are:(i) Can tests be devised to provide evidence of the attainment of lower attainers?
The evidence would need to be of positive performance; that is, pupils taking the tests should have a high chance of success.
It would also be necessary for the competency attained to be describable; that is, the tests would be criterion-, not norm-referenced.
(ii) Can the tests support a curriculum with the features recommended by the Cockcroft Committee?
Such a curriculum would include practical, oral, mental, and problem solving mathematical activities in context, and cover the foundation list of topics.
Underlying these questions are contextual issues of relevance and motivation.
Will it increase the motivation of a student to undertake a series of increasingly difficult tests, each taken when the student has a high chance of success?
Will a curriculum with all the features recommended by Cockcroft have greater relevance than that currently experienced by many pupils?
These underlying questions can only be fully studied in a situation where the curriculum is of the type recommended by Cockcroft, and a graduated assessment scheme is in operation.
Where such a situation does prevail it will not be easy to evaluate the effects of the graduated tests from those of the greater relevance of the new curriculum and the enthusiasm engendered by teachers with a belief in the efficacy of the change.
The NFER project team concentrated its investigations on the technical issues of devising evidence of positive achievement at different levels, and the organizational problems perceived by teachers working with the project team in implementing a graduated test scheme.
The findings, summarized in the final chapter of this report (Chapter 8, pp.234–8), are based on data of various kinds obtained from teachers and pupils in the schools associated with the project.
Issues involved in graduated tests were analysed and discussed with liaison group teachers.
These issues related to mathematics in context, criterion-referenced tests, levels of difficulty in a graduated test scheme, and the resources likely to be needed to implement a scheme.
Views on some of these questions were also obtained from teachers in second-phase schools by questionnaire.
Pupils were asked for their views on the tests and their approach to taking practical tests was observed in some instances by members of the project team.
The pupils tested were identified in relation to the schools' rating of their achievement.
Their level of attainment, however, was classified in accordance with their score in the project's own reference test.
The data from the results were used to examine issues related to defining levels in a graduated test scheme.
These included the effects on the pass rate of different pass marks, and of different aggregation procedures.
Also studied were the consistency of performance of individuals within and between topics, between mode of assessment and over a period of time.
Evaluating the evidence: an outline of the project report
Chapter 2 is a brief account of developments in assessment with particular reference to mathematics.
Chapter 3 provides details of the sample of lower attaining pupils used in the programme of testing.
The results of the reference test are also given in this chapter.
Chapter 4 begins with a brief description of the modes of assessment used and contains the main body of the results of testing.
The results cover most, but not all of the topics in the Cockcroft foundation list.
The data in this chapter are drawn upon in subsequent chapters where issues related to graduated tests are discussed.
Chapter 5 is concerned with issues related to criterion-referenced tests.
First, criterion- and norm-referencing are contrasted.
Then examples of criteria in several topics are given, based on the results of the testing.
The problems of deciding whether a pupil has mastered a concept or skill are discussed in relation to an experiment in giving pupils the same tests in different modes but similar in content and difficulty on two separate occasions.
Chapter 6 considers the question of how tasks can be assigned to levels within and between topics.
Following this, the "coherence" of a level is analysed in terms of the consistency of pupil performance within and between topics.
The effect on the clarity of criteria descriptions of aggregating scores is also discussed.
Chapter 7 contains an evaluation of the test modes used by the project team.
Data from a questionnaire sent to teachers in the project's associated schools are then used in a discussion of practical issues relating to the adequacy of resources of time and staffing needed to develop and operate a graduated test scheme.
There is also a consideration of the cost of running a scheme.
Chapter 8 provides a summary of the findings from the previous chapters and draws conclusions.
Developments in Assessment
Summary
Pupils may be assessed by a variety of different procedures during their school careers.
Every question and answer interaction in the classroom, or observation by the teacher of a pupil's work constitutes an assessment.
In more strictly controlled conditions, tests or examinations may be given, set by the teacher, the school or by an external agency such as an examination board.
Recent trends, especially following the announcement of the introduction of the GCSE, are leading to changes in emphasis on a number of features of assessment: for example, what is being assessed; how assessments are carried out; how achievement is recorded; the balance between school-based and externally set assessments; and the different purposes of assessment are also under review.
This chapter briefly reviews these trends, both generally and specifically in mathematics, with particular reference to low attainers, in order to set the NFER project in the context of current developments.
Contemporary trends in assessment
The range of learning outcomes assessed
Assessment in particular subject areas, including mathematics, has traditionally been concerned mainly with knowledge and techniques.
There is now considerable interest in the assessment of processes, problem solving strategies, and creativity in mathematics.
Other  learning outcomes not previously acknowledged, explicitly include personal and social characteristics such as initiative, self-confidence and the ability to work with others.
Alternative modes of assessment
There is an increasing awareness that there are limitations on what can be assessed by formal written tests or examinations.
The assessment of learning outcomes related to problem solving and personal qualities requires mathematics to be undertaken in context.
The Cockcroft Report pointed out that traditionally mathematics in school has rarely been about anything.
Mathematics which challenges pupils, and set in real or realistic situations is more likely to involve practical, oral and mental activities than writing.
Social and practical situations are interactive and pupils' competence in them may be quite different from their abilities with pen and paper alone.
Attempts are therefore being made to assess activities and processes in more natural settings, either real or simulated.
Teachers of mathematics who have experience in assessing extended mathematical investigations generally judge work through direct observation structured by a framework of categories and criteria.
This method of judgement is relevant whenever tasks and their solutions have not been completely specified in advance.
Such tasks may have been stated so that they are open to interpretation and worded so that they are accessible to most pupils, rather than aimed at a notional difficulty level.
Such tasks are referred to in the GCSE coursework handbook as "neutral tasks" .
The purposes of assessment
Assessment provides information to facilitate decision making.
There is a variety of decisions which might be made and different people who might make them.
For instance, a teacher might require information for: evaluating his or her teaching, monitoring the progress of individuals in the class, communicating to pupils and parents about achievements and progress, diagnosing pupils' difficulties; a school might require information for: placing pupils in sets, deciding what examinations to enter pupils for,— screening pupils for remedial or enrichment activities; an external body might require information for:— selecting pupils for different types of school,— monitoring the progress of the school,— grading individuals in examinations,— selecting pupils for further education, training or employment.
Assessment may also be used to motivate pupils by providing a target to aim for and a reward for passing in the form of accreditation.
Indeed, as stated earlier, pupil motivation was one of the reasons why the Cockcroft Committee recommended a study of graduated assessment.
Different assessment purposes require different kinds of assessment instrument.
For several of the purposes listed above a comparison between pupils is required.
For example, suppose pupils are to be placed in sets according to their attainment.
For this purpose a test may be used to discriminate those pupils who have attained a higher level from those at a lower level.
The test would need to be constructed so that a range of marks is produced, for a narrow band of scores would not discriminate sufficiently to facilitate the allocation of pupils.
Some assessments are much more directly related to action for improving a teacher's teaching and assisting pupils in their learning rather than for classifying them by their level of attainment.
A test for diagnosing pupils' difficulties would fall into this category.
It would focus on specific knowledge or skills; the test questions would need to be carefully designed to reveal any lack of understanding which may exist.
Assessment which is used for providing feedback to those most centrally concerned with a pupil's learning, the pupil, the pupil's teacher and parents, is often known as "formative" in contrast to "summative" which is mainly for external purposes.
In summary, the purposes of assessment might be viewed under three headings:(a) Information for the benefit of pupils, parents, teachers or employers.
(b) Development of learning and teaching by providing information about progress or diagnosis of learning difficulties; also, on a larger scale, for appraisal of the course of instruction.
(c) Accreditation, i.e. proficiency certification often in some way a summary"or measure of achievement on the course of instruction and often used for selection purposes, or to acknowledge success.
The teacher's role in assessment
In the above paragraphs a distinction was made between assessment for teaching and learning purposes and assessment for purposes external to the classroom.
Public examinations relate to the latter purposes; they have mostly been set and marked by bodies such as the Examining Boards (now grouped for GCSE examining).
A number of public examinations, particularly those administered by the CSE boards, allowed both setting and marking to be carried out by a school or group of schools, with a moderation procedure built into the scheme.
Many schools taking advantage of this approach, Mode 3, were able to use coursework, including projects as part of their examination requirements.
This feature of examinations is to be expanded with the introduction of GCSE, albeit voluntarily in mathematics until 1991.
The increasing use of diagnostic information and the requirements of public examinations should bring to the fore teachers' role in assessment in comparison with their somewhat peripheral involvement hitherto.
Reporting achievement
The different purposes of assessment require different kinds of assessment; the changing content and methods of assessment also require different methods of reporting achievement.
Traditional examinations rely on a single mark or a grade to indicate the level of performance relative to other pupils, although it is an indication which says nothing about the nature of the achievement.
Indeed, where success is possible with a pass mark of 50 per cent or less and there is a choice of questions on the examination papers, two pupils may pass who have little in common beyond having attained the same grade.
A range of options for recording achievement may be adopted whenever there is no purpose to be served by deriving a single total score from the items on the test, or where it is not necessary to rank the examinees in order of merit.
(i) Criterion- and norm-referencing
When selection is the main focus of assessment a test that provides information on where a pupil stands relative to the group taking the test is most likely to be used.
When feedback to those centrally concerned with a pupil's education is required then information on what the pupil knows and can do would be more valuable.
The measure of relative performance is known as "norm-referencing" and  that of absolute performance is referred to as "criterion-referencing" .
The trend towards criterion-referencing signals an increasing interest in assessment for learning in contrast with assessment for selection.
(ii) Self-referenced assessment
One solution to the difficulty of what the Cockcroft Report refers to as "recording positive achievement" which attempts to avoid laying too much stress on what pupils have not achieved is to record the growth in attainment over a period of time.
This procedure, is sometimes referred to as ipsative assessment.
It takes as its baseline the previous attainment of the pupil and indicates what progress has been subsequently made.
(iii) Profiles and records of achievement
The recording of a wider range of qualities than the attainment of particular knowledge and techniques is known as a profile or record of achievement.
As many aspects of pupils' achievement as possible are represented in the profile, in contrast with a test for which a single grade or mark may be awarded.
The profile may contain test scores or examination results; it may also contain statements about personal qualities and attitudes.
Pupils may be asked to review and record their experience and feelings.
A test or examination score may be obtained by aggregating sub-scores or marks from different papers; for a profile little or no aggregation of sub-scores takes place.
The stress laid by profiles on a comprehensive statement of achievement, much of it in prose, is presented as an argument for their value in acknowledging success and encouraging pupils in their development in particular subjects as well as in their personal growth.
The document of recorded achievement is intended, inter alia, to assist with selection and placement in further education or employment.
Assessing mathematics
The assessment of low attainers: schemes in operation in 1983–4 and 1985
A review of current practice in the assessment of low attainers in mathematics was carried out in 1983–4 as an adjunct to the main project in its first year.
Supplementary information was also collected in September 1985.
These surveys constituted an update of a developing situation previously summarized in paragraphs 537 et seq.
of the Cockcroft Report.
They were confined to assessment schemes  specifically designed for lower attainers or specially adapted for them for example, restricted grade CSE examinations were included.
In the initial review about 70 per cent of the LEAs responded with information, as did nearly all GCE and CSE Boards, the City and Guilds and the RSA.
Details of 53 schemes were collected, 51 already in use at the time of the review in 1983–4 and two of which were in their final stage of development and have since come into operation.
There were also several other schemes for which further information was not subsequently obtained.
Twenty-one of the 53 schemes led to CSE examinations, six of these having been devised initially by LEAs.
Four other schemes used assessment procedures developed by examining bodies but which did not lead to CSE.
The remaining schemes were developed either on an LEA-wide basis or by schools working individually or in groups of six to eight.
Some kind of graduated assessment or testing was a feature of 14 of the schemes.
The number of levels ranged from two to five, but only one scheme had a level designed for the lowest ten per cent of attainers.
Nineteen of the schemes were concerned with arithmetic only.
However, 11 different modes of assessment were noted, including mental, practical, calculator, project and investigative work.
They were used in a variety of combinations in the different schemes.
Organizers of schemes interviewed during the initial review indicated their reluctance to modify their schemes substantially in view of the time and expense required.
Implementing the Cockcroft recommendations, however desirable that might be, was seen as difficult.
For example, few schemes incorporated practical work using equipment.
Many teachers mentioned the difficulties of doing practical work in classrooms without storage space, in rooms scattered over a large campus, in departments without money to buy equipment.
The time taken to make equipment and give it out was frequently cited as a difficulty: "I'd need a lab technician" .
But underlying all of these reasons was the uncertainty of being able to cope: "What about discipline?" .
There were several teachers who maintained that their classes no longer needed equipment: "they are not remedial, you know" .
However, from the supplementary information collected, by questionnaire, from LEAs a year alter the initial review, there appeared to be some increase in the use of the reported incidence of practical work, and about two-thirds of the completed questionnaires indicated that calculator skills were tested.
Thus, despite the difficulties, some  modification appears to have taken place consistent with the recommendations of the Cockcroft Report.
Modes of assessment noted in some of the schemes for lower attainers which were appropriate to a Cockcroft type curriculum, had a wide variety of interpretation.
These included "practical" , "mental" , "oral" and "coursework" .
It may be that the introduction of the GCSE will clarify the meanings of the words, but some difference in interpretation may remain.
(i) "Practical" 
For some, practical work meant "in context" or "of practical use in life" .
That is, a question involving calculation was deemed to be practical only if it involved a real-life situation.
Sometimes these situations were constructed to act as a vehicle for testing abilities in the use of standard algorithms.
Linked to this, the urge to produce suitably motivating real-life contexts could lead to stereotyped and unreal settings.
For instance, adding 1.630 Kg and 250 g might be seen as sterile so the question becomes, "A recipe says add 250 g of sugar to 1.630 Kg of flour — what is the total weight?" 
It was not common to require pupils to do the actual weighing.
Most "practical mathematics" schemes consisted of social arithmetic, e.g. wage-slips, VAT, gas and electricity bills, areas of floors and walls, reading timetables, adapting recipes.
Some schemes also included topics thought useful in industry, such as basic geometry and circle calculations.
The use of an appropriate context was said by respondents to be very important in motivating pupils.
 "Practical" could also mean hands-on experience of mathematical equipment and manipulative materials scales, pinboards, three-dimensional shapes and so on.
Some schemes made provision for such practical work in the classroom.
A few schemes assessed or intended to assess, practical work.
In one scheme, "practical work" meant the use of surveying instruments, in another it referred to a narrow range of conventional geometrical constructions using compasses and set squares.
Some schemes involved the use of a wide range of equipment.
Those schemes which included "practical test" among their modes of assessment used the term to mean either a task using manipulative materials or one-to-one interactive interviews of the Assessment of Performance Unit type.
The latter are not exclusively practical in the sense of always utilizing apparatus, but instead might include tasks such as reading tabular data or looking for number patterns.
However, the handling of materials features prominently in APU practical tests — blocks, tiles, money, scales.
The APU team distinguish four meanings of "practical" :(a) "Practical" in the sense of "empirical" is concerned with knowledge gained through observation and experimentation.
These activities are prominent in subjects such as science and home economics; they can provide mathematical data and mathematical patterns.
(b) "Practical" in the sense of "construction" ; using materials and apparatus to make things which require the use of mathematical skills such as measuring and knowledge of spatial relationships.
(c) "Practical" in the sense of concrete modelling of mathematical concepts and relationships.
This includes activities, well known in primary schools, in which apparatus (e.g. number rods) is used to represent and manipulate mathematical ideas, relationships and operations.
(d) There are practical situations as well as practical activities: situations such as those in everyday life which may involve mathematical skills, although not necessarily using concrete apparatus.
These activities relate to planning, making decisions about and carrying out events at work or for leisure: for example in shopping, planning a journey or organizing a party.
Five of the schemes included practicals of the APU type, which like APU, vary in the amount of handling of equipment involved.
One other scheme allowed for one-to-one tests but left the decision about including them to individual teachers.
The phrase "interactive testing" was also used in these schemes to describe APU type practical testing.
Most examination papers contain practical problems such as fitting furniture into a room making use of scale drawings, or questions on practical experiments, e.g. "Draw a graph from the table which shows the distance travelled by a clockwork car for different numbers of turns of the key" .
One school incorporated the use of a real room and full size furniture; another scheme provided opportunities for the candidate to perform actual experiments.
(ii) "Coursework" , "project work"  "problems and investigations" 
More confusion arose over the terms "project work" , "coursework" , "investigations' and" problem solving"and they were often used indiscriminately.
The GCSE National Criteria state explicitly of coursework that such an assessment must be complementary to, and not a duplicate of, written examination papers.
Course work may take a variety of forms including practical and investigational work; tasks should be appropriate to candidates' individual levels of ability.
Course work must be moderated in accordance with the General Criteria.
In the review of practice current in 1983–4, "coursework" in particular had a wide variety of meanings.
Sometimes it referred to a "project" , i.e. a sustained piece of work on a topic often chosen by the pupil and done with a minimum of supervision.
Popular examples include buying and running a motorbike, redecorating and/or furnishing a room, planning a holiday or an event like a party.
A project was almost invariably done by one pupil alone.
This contrasts with projects done in junior or earlier secondary classes where it is more usual for a group of pupils or even a whole class to participate, often sharing the work between them, pooling the results and producing a joint account.
By the fourth year of secondary school this approach has been largely abandoned in the schemes reported.
Even for low attainers projects are done individually if they are to be assessed.
Regular tests, usually written tests, which are given at the end of a section of the syllabus, were also termed coursework".
The sections could take from two weeks to half a term to complete, so the actual number of tests varied from scheme to scheme.
Samples of classwork (exercise books, folders, files) were also described as "coursework" .
Some schemes incorporated all these variations.
Sometimes it was not possible to know precisely which kind were actually included in a particular scheme.
The term "investigation" was also used in a variety of ways, for example, to describe a survey such as  "An investigation into traffic flow on the High Street" or "Investigate people's attitudes to TV" .
The investigation in this sense is a project or a survey.
It could also mean a study of an open-ended nature usually with a mathematical theme (e.g. pattern spotting using numbers and/or shapes) or shorter pieces of work prompted by the "what happens if…?" type of question.
Where a scheme included "problem solving" more often than not this meant "worked problems" .
That is, the term used to describe solving a longer, more difficult, traditional examination-type question involving several stages and often of great linguistic complexity.
 "Problem solving" was also referred to as a more global idea, or a process which prompts active thought rather than a question designed to initiate rote-learning techniques.
The distinction between this type of problem solving and investigations is blurred.
They may both require planning strategies, conjecture, specialization and generalization; and producing, recording, recognizing and explaining patterns.
Some schemes did not distinguish between the two terms, others preferred "investigations" , for the open-ended problems, and "problem" for a situation which results in a predetermined answer.
(iii) "Oral" and "mental arithmetic" tests
 "Oral" was often used by the writers of a scheme interchangeably with "mental" to refer to a test which required pupils to work out answers mentally although it was normally the teacher who performed the oral task of reading out the questions; the pupil's task being aural and written.
But "oral" can also be used of tests given in the form of a one-to-one interview, or where answers are tape-recorded.
One-to-one tests may include topics other than simple arithmetic, so in this sense "oral" is equivalent to the APU type of "practical" test.
One reason for the confusing variety of interpretation found in the review was that different aspects of tests were being stressed.
In the traditional mental test, for example, questions are given orally and are supposed to be carried out mentally by the pupils.
Furthermore, the answers are written down, so the test could equally well be called a written test.
In the APU "practical" the questions are given orally, pupils usually carry out the task practically and then give their answers orally.
This analysis suggests classifying test modes in terms of their question, task and response modes.
For example, applying this analysis to mental and different kinds of practical tests produces the following scheme shown in Table 2.1.
Other dimensions could be added to this scheme.
For example, the APU practical is an interactive test whereas the others in the list are not.
Assessment frameworks: recent developments
An assessment framework is a structure to which any test item can be related.
It is a formulation of the objectives of an assessment scheme, classifying what is to be assessed and possibly how the assessment is to be carried out — or at least the constraints within which choices of  assessment method is permitted.
The classification may be an organizational one, arranging detailed criteria into lists, or it may represent some kind of conceptual hierarchy of increasingly higher order objectives.
Assessment schemes vary in the extent to which they rely on a structured syllabus and defined test modes.
The picture is further complicated when judgement is involved using a set of criteria or rating scales.
Teachers may form a consortium in order to administer and validate the scheme and moderation procedures of some kind will be required to ensure reliability.
A variety of assessment schemes is currently being developed following the publication of the Cockcroft Report.
The schemes are an expression of similar beliefs about school mathematics and its assessment as embodied in the Report, but represent diverse interpretations of the Report's recommendations.
The assessment structures of three of the schemes are described below.
They are presented in order to illustrate some similarities and differences in the way the Cockcroft recommendations are being interpreted in relation to assessment and are not intended as a review of all the initiatives currently being undertaken.
All three of the schemes are concerned with mathematical processes.
Content is referred to explicitly in only one of the three and is assessed "incidentally" in another.
Graded Assessment in Mathematics (GAIM)
This scheme is one of five parallel graded assessment schemes being developed by the Inner London Education Authority (ILEA), and the University of London School Examinations Board (ULSEB) in cooperation with other bodies.
GAIM aims to produce a complete mathematics assessment scheme appropriate to all first to fifth year students in secondary schools.
The scheme will provide a detailed continuous record of what each student has achieved in mathematics.
There are 15 levels in the scheme.
The later levels (8–15) have been designed to correspond to the seven GCSE grade levels.
Agreement to award a GCSE grade on the basis of achievement in the GAIM scheme is being sought.
The GAIM assessment has two essential components: topic criteria and open-ended coursework activities in two forms — practical problems and investigations (see Figure 2.1).
The Topic Criteria provide a comprehensive list of mathematical skills and processes.
There are five headings for the Topic Criteria in levels 1–8: Logic, Number, Space, Measurement, Statistics.
In the later levels there is also a sixth topic, "Algebra and Functions" .
Criteria statements are associated with each topic heading.
For example, a criterion under the heading "Number" is listed as "can estimate without counting about how many are in a group of between 10 and 30 people or objects" .
The open-ended activities relate especially to the process aspects of mathematics.
They provide a way for Topic Criteria and more general processes to be assessed in context.
The preferred method of assessment is "incidental" assessment which takes place when a teacher, while in discussion with a student or observing the student's work, notes that the student has, in fact, demonstrated competence in a particular criterion.
 "Incidental" assessment is contrasted with "Purposive" assessment in which a specially arranged procedure, such as a test, is designed to assess pupils' achievement of specified criteria.
The Oxford Certificate of Educational Achievement (OCEA)
OCEA is a scheme covering the assessment of different subjects within the school curriculum.
Schools become accredited "OCEA schools" .
There are three facets to assessment, the P-component, a personal record compiled by the student in consultation with a teacher, the G-component which is a detailed statement of what the student has  achieved as defined by explicit criteria, and the E-component recording all external qualifications such as GCSE or graded music examinations.
The framework constituting the criteria for the G-component in mathematics is based on a view of "problem solving" .
This involves the application of strategies, skills and reasoning, as described by Figure 2.2.
Nine "processes" have been identified, each associated with a number of criteria.
Cluster headings are used to create a link between the criteria and each process.
For example, the process "representing" is linked to criteria through four cluster headings (Table 2.2).
OCEA literature states that students need to experience a wide range of mathematical activity in order to achieve the criteria, including practical work, quick response to questions, short problems, project work and investigations.
Students' achievements can be communicated in various ways, in written or diagrammatic form, orally, during interaction with the teacher or by teacher observation.
SMP graduated assessment and SSCC study
The SMP assessment scheme was developed alongside the SMP G materials for use with fourth and fifth year students in the lowest 40 per cent of the attainment range.
The SSCC study, sponsored by the DES, was based on the scheme.
The study was intended to evaluate the extent to which the graduated tests are compatible with the Cockcroft recommendations and how effective they are in generating curriculum change without the use of the SMP G materials.
The SMP graduated assessment has three stages, each of which contains assessments in written, mental, practical and oral modes.
A stage certificate is awarded when all aspects of a stage have been passed.
A final certificate, awarded at the end of the fifth year, indicates the highest stage reached in each aspect of the assessment, together with a maximum of four titles of topics on which sustained work has been undertaken.
An assessment framework was developed by the SSCC study team for trial as a preliminary stage of the SMP scheme intended for the very lowest attaining third and fourth year students.
It is illustrated in the documentation as a two-dimensional structure consisting of processes and content.
The processes are described as: problem solving skills; practical skills; language skills; and technical skills.
Each heading has a number of skills listed under it.
For example, problem solving skills (PS) are given as: PS.1 compare, order, match; PS.2 recognize equivalent forms; PS.3 estimate; PS.4 find by trial and error; making a list; using a pattern; PS.5 organize by sorting, grouping; and PS.6 plan, devise rules, check and test.
The content categories are given as number, money, time, length, weight, data interpretation, and space.
Example tasks are given for each skill in relation to a particular content area.
Thus, PS.5 (organize by sorting; grouping) has these example tasks in the content area "number" : "group according to simple criteria: odd/even; >10/<10; belonging to a sequence" .
An accompanying Student Record Book offered specific examples of these tasks in student language.
It was intended to encourage students to make self-assessments and to keep a record of their own progress.
The framework was to be used by the teacher as a checklist of each student's progress.
It was intended that assessment was made through observation, practical work and discussion during normal lessons.
Assessing investigative work
Investigative work is expected to be an important feature of courses developed for the GCSE examination.
It was not possible for the NFER project team to study in any detail the role of investigative work in a graduated test scheme, partly because few schools associated with the project had gone very far towards introducing it into their curricula.
However, because of the importance of investigative work, samples of marking schemes used by teachers experienced in its assessment were collected and analysed.
They indicate that marking schemes may be designed in different ways:(a) written before pupil responses have been seen; or derived after a sample of answers has been seen, and constructed so as to reflect the strength and weaknesses of candidates' performance (often combined with the first point).
(b) question specific; or, consist of generalized categories reflecting either a view of mathematical features, e.g. "skills" , "knowledge" , "systematic working" , "can make conjectures" ; or, may refer to phases of work undertaken by pupils, e.g. "method" , "results" , "conclusion" , not all to mathematical processes or content.
Where a marking procedure which is not specific to particular test items is employed, grading methods used are that:(i) Marks may be awarded by teachers exercising professional judgement about the quality of a piece of work based upon the  experience of the teacher regarding the standard of work that has been achieved.
A grading may be arrived at by allocation of each grade to a range of marks.
Alternatively there may be no marks as such but simply a set of grades to which the quality of pupils' work may be assigned directly.
(ii) The professional judgement of the assessors may be focused to some degree by the employment of criteria.
The criteria may offer guidelines as to standards appropriate to grade levels and may in addition at any grade level be split into separate features of the mathematical investigation, leading to a set of marks or grades which then have to be aggregated in some way.
For example, in one CSE mode three which has been operating for about ten years, the course objectives are divided into content and process.
While written tests are primarily concerned with the content objectives, mathematical investigations submitted as coursework test the process objectives.
There are five categories of process objective: formulating the problem; carrying out the task-use of mathematical strategies; carrying out the task-level of mathematical argument; evaluation and interpretation of results; report as a communication; personal contribution.
For each objective the scores available range from 0 to 3; the general description of the criteria for each score is given.
For example, those given for "Report as communication" are as follows: Score Criteria 3 A logically structured report with suitable selection of what to present.
Full explanations of the problem, its development and conclusions.
Well-written and appropriately illustrated with examples, tables and diagrams.
2 A coherent and generally well-expressed report, but lacking some of the qualities of 3 in terms of structure and presentation.
1 Lacking in coherence and organization.
Parts are too brief or repetitive and verbose.
Some difficulty in understanding writing or diagrams.
Poor selection or important omissions.
0 An untidy collection of results, badly organized, little or no explanation.
No agreement commonly exists about grading procedures.
Two examples are given below.
In the first, a grading scheme designed for a mode 3 CSE, it was decided to rely on professional judgement to grade the standard attained in each of the five aspects, rather than employ  detailed criteria like those given above for "communication" .
This was done to provide sufficient scope for the scheme to assess a wide variety of projects, ranging from exploratory and practical to textbook work, encouraged in part by the large element of self study or group work which was a prominent feature of the course.
Feature Marks available or number of levels Knowledge decided by experience of Understanding requisite standards Communication Inventiveness Technique and skills An alternative way of providing foci for assessors is to split the work undertaken by pupils and its write-up into phases.
An example of this approach is illustrated below.
Feature Marks available or number of levels Aim (e.g.) judged by experience or Method by reference to specific skills Results Conclusions
Discussion
The developments briefly described in this chapter demonstrate that there are a number of ways in which an assessment scheme consistent with the Cockcroft Committee's recommendations could be implemented.
There are schemes which assess processes only, those which assess processes and content separately and those which assess content "incidentally" while pupils are undertaking holistic tasks.
There are variations in the way mental, oral and practical work are incorporated into a scheme.
Only some of this range of procedures could be used in this feasibility study with a brief centred on the Cockcroft foundation list.
In deciding what procedures to use the project team had to take into account the fact that few of the schools associated with the study were likely to have moved towards a Cockcroft curriculum.
Some types of assessment, particularly practical and oral, would, therefore, be an introduction to this type of work for the schools.
There was also a need to cover as many of the topics in the foundation list as possible and assess them appropriately.
These considerations led to the decisions about the modes of assessment used in the project which are described in Chapter 4 (pp.41–102) together with the results of the testing.
The Sample of Schools and Pupils
Summary
A continuous programme of testing took place between September 1985 and April 1986.
The aims of the programme were to obtain data and evidence for the technical and practical aspects of feasibility.
In this chapter some features of the schools associated with the project are first described.
There follows a discussion of the identification of the target sample, variously described in the Cockcroft Report as "the lowest 40 per cent of attainers" and "those for whom CSE is not intended" .
The development of a reference test is discussed.
This test was taken by nearly all pupils participating in the programme of testing in order to facilitate the comparison between different groups of pupils in the target sample.
There is an account of the results of this test including sex differences in performance.
The LEAs and schools associated with the project
Teachers' liaison groups in six English LEAs and two in Wales assisted with the development of the assessment materials.
There were 36 liaison group schools altogether.
Two LEAs provided six schools each and there were four schools from each of the remaining LEAs.
A further 33 schools from 11 LEAs in England were associated with the project in a less direct way.
These were known as second phase schools.
A list of the 19 LEAs and 69 schools involved is given in Appendix IV.
Of the 19 LEAs involved, 12 were metropolitan boroughs and 7 were counties.
All associated schools were asked to provide various details about their school population and staffing in their  mathematics departments.
Nearly all the schools responded to the request (34 out of the 36 liaison group schools and 30 out of the 33 second phase schools), but some of these did not provide all the information asked for.
A frequency count of some of the details given is recorded in Table 3.1 to illustrate the range of schools participating.
The selection of LEAs in England was made by the project team and the selection of schools by their own authorities.
The two LEAs in Wales were invited after advice from the Welsh Office, as it was intended to include tests in Welsh in the schools.
Two of the eight project schools in Wales were English medium, two were designated Welsh medium schools in mainly anglicized areas, and four served bilingual catchments and provided varying degrees of Welsh medium provision.
Some school populations were almost exclusively in one of the five categories and others were divided between categories 2 or 3 as follows:
The sample of lower attaining pupils tested
Schools associated with the programme received a package of test materials once or twice a term.
In order to obtain data on the difficulty of the questions at different levels it was necessary for the sample of pupils taking them to be drawn from the complete range of target pupils in the schools.
The table above shows that schools associated with the project covered a wide range in terms of characteristics such as location (inner city, suburban, rural) and proportions of ethnic minority pupils.
It was certain that some would have more and some less of the "lowest 40 per cent" nationally, however this target group was defined.
There were two separate issues for the research:(i) which pupils would be included in the testing; and,(ii) if a wide range of tests was to be developed, each of them would be taken by different and fairly small samples from among the target pupils, how could the comparability of the samples be checked?
It was decided to include mainly the fourth year pupils that each school defined as being below average in attainment in mathematics, that is, the lowest 50 per cent of pupils rounded up to a complete set.
(In fact, sets right at the top of this range were only rarely included in  the testing programme.)
Fourth year pupils were the principal subjects of the study as they would be members of their school for a full school year and were likely to have a lower absentee rate than the fifth year.
The schools supplied the team with lists of pupils in each of their below average mathematics sets and gave the order of the sets from those with the lowest attainers to those with the highest attainers.
The problem of comparing different samples drawn from the lists was dealt with by developing a reference test which was administered to all the pupils participating in the research.
The reference test (GT4)
Permission was obtained from the APU and HMSO to make up a test of APU items for the purpose of comparing different samples participating in the project.
The test characteristics of these items for a national representative sample were well-known, including the likely success rates for the target sample.
This meant that it was possible to make up a test focused at the middle of the target attainment range with a minimum of trialling.
The APU collection of questions was searched for items which conformed individually and collectively to the following criteria:— nearly all the items would have success rates between 60 per cent and 95 per cent nationally;— all the items would have low omission rates among the bottom 20 per cent band of attainers nationally; that is, they would not deter this group of pupils, who would be willing to "have a go" at them;— overall, the items should cover a reasonable variety of topics, including basic ideas such as fractions, decimals, graphs; and,— as far as possible the items selected would have a high incidence of a distinctive error or errors which would give them some diagnostic value.
Three selections of items named GT1, GT2 and GT3 were made and trialled.
A final selection was made of 29 items which was subsequently used as the reference test and labelled GT4.
The test yielded a mean score of just over 50 per cent for all pupils participating.
A total of 367 pupils in Wales took the GT4 test (two schools did not return tests) approximately one-third using the Welsh language versions.
The mean score for pupils in Wales was significantly higher statistically (57 per cent) than those in England.
This might be because  two schools were below average size for a secondary comprehensive; the instruction to offer tests to pupils in the lowest sets rounded up to a complete set could have resulted in more higher ability pupils being included than was the case in larger schools with more sets.
Since the results of the GT4 test give an overview of performance among the pupils included in the project surveys, a selection of the results is described in the next section.
The results of the reference test
Mean scores and sex differences
The main topic areas represented in the reference test were number and measures, but there were also one or two questions on each of several other areas including geometrical concepts, graphs and number patterns.
The results obtained for each question in test GT4 were compared with the results from the same question when used in APU surveys.
Direct comparison of the results showed that the performance of the pupils in the project's sample was extremely close to that of the lowest 40 per cent of attainers in the APU surveys.
The sample can thus, on this basis, be regarded as representative of the desired target population the lowest 40 per cent of attainers, those for whom the GCE or CSE system was not originally intended.
The overall mean for the 1984–5 sample was 54 per cent and for the 1985–6 sample it was 53 per cent.
The results from the sample of 2,745 fourth year pupils tested in 1984–5 were analysed for sex differences.
There were 1,540 boys and 1,205 girls in the sample.
The boys' mean score was 57 per cent and the girls' 51 per cent.
The boys obtained significantly higher success rates than the girls for 17 out of the 29 items and the girls were more successful in four.
Eight items showed no statistically significant sex differences.
* The pattern of differences in relation to topics was very similar to that obtained in APU surveys: boys scored up to 17 percentage points more than girls on measures and scale reading items; the largest difference, 19 points, was for an item in which angles were to be compared.
Girls' highest mean success rates relative to boys' was on a pure computation item (112 ÷ 7; girls six percentage points higher).
The number of boys and girls within two score bands was studied.
This analysis was carried out for mixed schools only (there were four single-sex schools in the sample, three boys' and one girls').
The following results were obtained for 2,515 pupils in mixed schools: There were more boys in this sample (55 per cent) than girls (45 per cent ); this imbalance was not present amongst low scoring pupils, half of the low scorers being boys and half girls.
The greater proportion of boys in the sample reflected more high scoring boys than girls being in the lower mathematics sets in the schools working with the project.
This picture was looked at in more detail and the two lowest mathematics sets in each school were focused upon.
In these schools' sets the 0–17 score band contain 1.1 boys to each girl, but the higher score band (18–29) was made up of 2.4 boys to each girl.
* The reason for the imbalance may be partly attributed to the policy of allocating pupils to mathematics sets.
Decisions on allocation do not usually depend solely upon the pupil's attainment in school but often take account of such factors as pupil motivation and behaviour.
Certainly the sample of lower attainers used contained more high scoring boys, as measured by the GT4 reference test, than girls.
Attainment band differences
The results for each question were analysed in three attainment bands, each band containing one-third of the lower attaining pupils who took the test during their fourth year in secondary schools (1984–5 or 1985–6).
The attainment bands were formed on the basis of pupils' score on the GT4 test, not from the particular mathematics sets in which they do  mathematics in their schools.
Thus, some pupils in bottom mathematics sets in their schools are in the middle or even top third attainment bands in the results given here.
The results for all the other NFER tests trialled during this project are referred to the attainment bands formed from pupils' scores on the GT4 test.
Throughout this report the three attainment bands are referred to as a bottom (B), middle (M) and top(T).
The mean GT4 test scores rose sharply from 30 per cent produced by the bottom third of pupils to 77 per cent obtained by the top third.
The middle third yielded a mean score of 54 per cent and this was also the overall mean score.
Results for individual items: a curriculum snapshot
An inspection of the results for individual questions shows that although fewer pupils in the bottom third obtained the correct answer in each case than did other pupils, a substantial proportion did so in many cases.
Five questions produced success rates of 50 per cent or more for the bottom band pupils, and for seven questions between 40 per cent and 50 per cent obtained the correct answer.
Overall, 14 of the 29 questions were answered correctly by 30 per cent or more of the lowest band.
Nearly all the items answered correctly by 40 per cent or more of the bottom third pupils require straightforward counting or calculation.
Four examples are given below.
A few of the questions yielded low or fairly low success rates even for the top third of pupils.
These will be discussed first because, for these questions, conceptual difficulty is very likely to be the problem for all the "lowest 40 per cent" (and for pupils at higher levels of attainment).
The questions below, which asked pupils to compare fractions and decimals less than one respectively, had very low success rates.
Comparing numbers is an efficient way of diagnosing basic understanding of these concepts, if the numbers to be compared are carefully selected, so that the correct answer is unlikely to be obtained by pupils who have misconceptions.
When comparing fractions some pupils may compare numerators only or select as the largest fraction the one which has the largest numerator and largest denominator.
Thus the fractions selected for comparison must require the pupils to consider both numerator and denominator and the largest fraction must not be one which has the largest numerator and denominator.
Questions set on these lines have good diagnostic potential.
Lower attainers are very likely to ignore decimal points when comparing decimal numbers.
If the numbers to be compared are all written to the same number of decimal places, those pupils ignoring the decimal points will get questions "right" .
Another misconception made most frequently by pupils in the middle or just below the middle of the complete attainment spectrum, is to regard the "longest" decimal (or the larger of two equally "long" ones) as the smallest, that is 0.089 in the item given above.
Thus, if in a set of decimals to be compared, the longest one really is the smallest, it will attract such pupils when they are asked to pick out the smallest as well as those who select it for the correct reason.
The proportions of pupils in the project sample of lower attainers making these two errors were as follows: These results indicate that the understanding of fractions and decimals is a considerable conceptual problem for all lower attaining pupils — and for many of average or above average attainment.
Decimals are essential to the understanding of measurement in the metric system.
Equally important a factor in measurement is reading  a scale.
The difficulty many pupils experience with the scale illustrated below is that each division represents 0.2 of a centimetre and not 0.1.
For the top third of the project sample their problem may be less of a conceptual one, but there seems to be an automatic assumption that a single division on a scale represents a single unit of some kind.
The idea that a single division can represent more than one unit is conceptually difficult even for some pupils in the top third.
Further questions which yielded a success rate of less than 50 per cent for the middle third of the pupils are now considered.
The proportion of the bottom two-thirds of pupils who get the following item correct is considerably reduced compared with the number who correctly answer "How many halves are there in 2½?" (see p.33) Reading a scale with divisions which do not represent one unit again proves difficult as the next item illustrates: The next question which fewer than 50 per cent of the middle third could answer was the last in this group of three items.
A direct reading from graphical or tabular data is usually carried out successfully by a high proportion of low attainers.
Any other task related to information in the table usually reduces success rate considerably.
In Example 12 the temperature change over a time interval was to be worked out.
A Cockcroft type curriculum places a good deal more emphasis on "pattern spotting" than is currently the case.
Lower attainers' inexperience with this kind of activity probably contributed to the following results.
Results in relation to the curriculum in the schools
The GT4 test was taken by almost every pupil who participated in the project trials.
The difficulty of the test was pitched at the middle of the "lowest 40 per cent" range.
The items were selected so as to give a quick overview of performance in relation to a range of topics including number concepts, measures, spatial concepts, algebra, graphs and number patterns.
The fact that 14 of the 29 questions were answered correctly by 30 per cent or more of the lowest band suggests that there is a range of questions within the conceptual grasp of all or practically all the lowest band of attainers.
Some may have given the wrong answer for reasons other than conceptual difficulty: making a slip, an arithmetic error, misreading or not being able to read the question, not trying and so on.
All but one or two questions were concerned with basic mathematical concepts and skills, so that differences in curriculum might be thought not to have contributed greatly to failure.
The curriculum exposure"of the items was studied by asking teachers of one large group of pupils (N 1,845) who took the GT4 test in 1985 to rate their pupils' experience with items of the kind in the test.
Separate ratings were made for each item.
Five points on a scale of experience were described as follows: 1 no experience; 2 occasional experience but some time ago; 3 occasional experience including last year; 4 fairly frequent experience but not necessarily this term; 5 frequent experience, including this term.
The question which teachers rated their pupils as having had most experience with was 112 -: 7; 71 per cent of pupils in the sample were said to have had "frequent experience, including this term" of this kind of item.
The next most highly rated questions, in terms of pupils' experience, were on computation or number concepts.
Interestingly, the comparisons of fractions and decimals which were found most demanding by all the pupils, were rated low in terms of experience.
This could indicate that it is computation rather than the understanding of concepts which is stressed by teachers at this stage.
By contrast 85 per cent of the pupils were said by their teachers to have no experience of Example 14, although it produced one of the highest success rates among the lowest third.
This is a skill which many pupils bring to the classroom but may have little opportunity to show.
Discussion
The location and other characteristics of the schools associated with the project suggested that the sample of low attainers drawn from them would be representative of the lowest 40 per cent of attainers nationally.
That this was so was demonstrated by using a reference test of APU items which had parameters established in national surveys.
Nearly all fourth year pupils in mathematics sets in the lower half of each school's attainment range took the reference test.
The results showed areas where boys' and girls' performance differed.
Some pupils, more boys than girls, placed in the lowest mathematics sets in their schools, were among the higher scorers on the reference test.
The pupils taking the reference test were divided into three equally sized bands according to their scores.
Some questions in the tests were answered correctly by a substantial proportion of pupils in the lowest third.
This result suggests that these questions should be within the grasp of most of these pupils.
Some of those who answer such questions incorrectly may do so because of slips or lack of recent practice in the topics concerned.
There was evidence that pupils had more experience with skills than with work which might probe their understanding.
4 Test Development and Results
Summary
The chapter begins with a brief description of the modes of assessment used in the testing programme.
The content of the tests was taken from the range of topics in the Cockcroft foundation list.
Items covering a range of expected difficulty levels were included in most of the tests in order to check on their actual difficulty.
The information was also required to ascertain what assessment materials could be included at different levels of a graduated test scheme so that pupils taking each level could achieve a high degree of success.
This latter purpose was also served by assigning each pupil participating in the assessments to one of three attainment levels on the basis of their performance on the GT4 reference test.
The data reveal that most of the questions and tasks successfully undertaken by pupils in the lowest third of the target pupils involve basic processes of counting and calculation carried out on paper, mentally, and with a calculator; direct readings from tables of data; recognition and drawing of familiar shapes; and some visualization tasks.
These skills can be applied to a fairly wide range of tasks which can be tackled by the lowest achievers.
For example, counting is involved in various tasks related to money (bills, change), some calendar questions; finding areas and perimeters especially those with whole number lengths.
Computation involving halving and doubling can be carried out mentally or in writing and the range of numbers involved extended by the use of a calculator.
The results also showed that the understanding of fractions, decimals, measurement scales and units, and the meaning of numbers in context is still difficult for the top third of the target attainment range.
Despite their lack of understanding of these ideas, many pupils could perform the skill based on them reasonably well.
The results were related to the topics in List 1 of the GCSE National Criteria for mathematics.
They suggested that some of these topics are too difficult for pupils in the range studied in this project.
In relation to sex differences the results showed boys ahead in anything to do with measurement, except money.
Girls were equal to, or in advance of boys in topics concerned with money and on calendar questions.
The development of tests for the feasibility project
Test development and administration
Test development was undertaken in conjunction with teachers in the liaison groups.
The test items were written by the researchers and trialled both by them and by teachers in liaison schools.
The teachers' views were sought on the relevance and importance of each section of the Cockcroft foundation list.
Views were also sought on the appropriateness of the wording and language level of the test questions and the contexts in which the mathematics was set.
In most instances the contexts were thought to be appropriate to pupils in schools in different locations.
One example of an inappropriate context was in rural areas of Wales where some teachers considered that timetables were too unfamiliar to the pupils to provide a fair test of their skills and understanding of the mathematics involved.
For schools in Wales, second drafts of pilot tests were normally translated into Welsh as also were any tests used in Wales in the wider scale trials.
Some of these tests included local features such as advertisements in Welsh.
The translations were undertaken by mathematics education specialists in Wales: an adviser, an advisory teacher and a retired teacher.
Although only a small number of pupils in Wales completed the tests in Welsh, no problems were found specifically relating to the Welsh language in translated tests.
There is no evidence to suggest that the results and findings recorded in this and other chapters of this report do not apply equally to tests taken in the Welsh language.
Similarly the findings of the report apply equally to pupils in England and Wales.
Guidance on likely task difficulties was sought from the research evidence of projects such as Concepts in Secondary Mathematics and Science (CSMS) 1 and APU.
The APU has data on difficulties in written tests within live 20 per cent attainment bands.
The two lowest bands in this range fitted the description of the target group for this project.
Teachers and their pupils commented on the initial drafts of the tests which were then revised in preparation for wider scale trials in schools which were to become associated with the project in its second phase from September 1984.
Views on the tests were also sought from the teachers and their pupils in these second phase schools when the tests were administered.
Some liaison schools, in both England and Wales, also participated in these wider scale trials.
Teacher and pupil views are considered in detail in Chapter 7 (pp.l 88–253).
The scripts were marked at the NFER and the results fed back to the schools in the form of a computer printout.
In addition to marking right or wrong a number of the more common errors were recorded.
One computer printout format included incidences of errors and diagnostic information as well as right or wrong codings, but it proved time-consuming for teachers to use and expensive to produce and post, so generally only the simpler right or wrong data were sent to the schools.
Assessment framework and assessment modes
It is evident from the development work described in an earlier chapter that there are a number of ways in which a framework for assessment that accords with the Cockcroft Committee's recommendations might be conceptualized.
In this project the APU framework with its dimensions of content, context and learning outcomes was used as an overall guide to the development of tests, the content being drawn from the suggestions in the Cockcroft foundation list.
A dimension added to the APU framework for the purpose of this project was that of "Level" ; one of the principal aims of the research being to determine features of tasks which are associated with levels of difficulty.
The Cockcroft foundation list is prefaced by the following statement: 458 Throughout their mathematics course, pupils should — read, write and talk about mathematics in a wide variety of ways; carry out calculations in a variety of modes — mentally, on paper and with a calculator;— associate calculation with measurement in appropriate units and become familiar with the relative size of these units.
At all stages, pupils should be encouraged to discuss and justify the methods which they use.
The aim of the test development was to reflect as many of these activities as possible in the assessment modes.
There are several possible ways of administering tests which might be derived from the Cockcroft Committee's recommendations.
As well as developing modes of assessment which covered a range of activities it was intended to begin the development with more formal group tests and later work towards more informal individual assessments, particularly in liaison group schools.
This plan was affected by the teachers' dispute which took place during the main development period from 1984 to 1986.
In the circumstances many teachers involved with the project had difficulties in organizing practical and oral tests, particularly on an individual basis, and so the number of pupils taking these assessments was much reduced from what had originally been envisaged.
Consequently, while many pupils in the samples in 1984–6 completed written and mental tests, fewer did practical and oral tests.
The assessment modes used were as follows.
(i) Written tests
A series of tests was developed on the following topics: number, money, measurement, tabular data and spatial concepts.
These tests consisted of a set of short response items.
There were also tests with a contextual theme such as "Healthy eating" , "A day out" , "Designing a bed-sitting room" , "Shopping and" Map reading".
Short response items made up the initial sections of these tests.
The last section included a more open-ended problem or question which had more than one possible answer.
Teachers were asked not to impose a time limit when the tests were taken and to help pupils with reading difficulties.
(ii) Calculator written tests
The topics included money (bills, change), measurement, conversion and number patterns.
Some items tested interpretation of the calculator display, and performance in handling sequences of two operations.
Again, no time limit was prescribed and pupils could be helped with reading difficulties.
(iii) Mental tests (aural)
Three mental tests were developed, each consisting of 20 questions.
The topics included change and bills, calendar, time  intervals and conversion from 12 to 24 hour clocks (and vice versa); percentages (10 per cent ); estimation of length and temperature; simple ratio and proportion.
Teachers were asked to read each question to pupils twice.
Groups of questions on similar tasks were prefaced by a statement of their nature (e.g. "The next two questions are about change from 50p" ).
The pupils were provided with an answer sheet, and in some administrations the numbers mentioned in the question were printed on the sheet so that the effect of the memory load could be estimated by comparing the results with and without numbers given.
The same questions as those in the mental test were also given to some pupils as a standard written test so that performance on it could be compared with the two mental test versions.
(iv) Oral
As yet, there is little consensus on what mathematics can appropriately be assessed orally.
It is clear, however, that oral interaction can be a means whereby teachers may probe pupils' understanding of various aspects of mathematics.
In this project the team studied pupils' methods of computation or explanations of number sequence rules.
(v) Practical "circus" 
A practical circus consists of a series of tasks each placed at a different position in the room.
Pupils move from one station to the next either at a specified time or when they are ready.
Tasks for 18 stations were devised covering some geometrical concepts (symmetry, constructing three-dimensional shapes); measuring (length, capacity, mass); money (change); reading and interpreting a map; reading tabular data.
A box of apparatus for the circus was provided for all but the most recent trials.
Each of the items, specially written for the project or adapted from APU practical tasks were written on to separate sheets and put in position along with pupil answer sheets and the apparatus for the task.
The circus was initially administered by a member of the project team.
After experience of observing and administering a practical circus in their own schools, teachers in one liaison group travelled to schools in second-phase LEAs* and instructed other teachers in the administration of the test.
Most recently, second phase schools were sent question and pupil answer  sheets as before but asked to provide their own apparatus from a detailed list supplied along with administration instructions by NFER.
(vi) Microcomputer assisted assessment
Informal trials were undertaken of interactive testing using the microcomputer.
It is possible to test pupils with options to vary the pace of testing and to use moving VDU pages, colour displays and auditory stimuli.
Branching routines can be written into the software so that the computer can "talk back" to pupils according to their responses.
It can be programmed to provide easier or harder material during the course of an assessment according to an analysis of the pupils' performance.
The computer can record input and present analyses to a high level of detail, and can also dictate to an extent the order in which mathematical tasks are performed, e.g. estimation before computation.
Finally, in trials both within and outside the graduated testing initiative, the microcomputer has been found to have a markedly beneficial effect upon motivation and performance.
(vii) investigations
Limited investigative mathematics with pupils was undertaken by a number of liaison group teachers assisted by advisory teachers.
A diary based development and reporting approach was used in a few cases where close contact with the project team was feasible.
Marking sessions and discussions on the marking of investigations took place with the teachers as part of the project's inquiry into procedures for assessing investigative mathematical work.
The Cockcroft Foundation List and project test results
The Cockcroft foundation list was used to suggest the content of the tests in this project and the results of testing recorded here are organized in relation to the list.
Some sections of the list were covered widely both in relation to topic and modes of assessment and others less so.
One section of the list,"Statistical Ideas', was not included in the test development although one of the microcomputer tasks was built around pupils' ideas on probability.
The topics in the foundation list have been divided into sections and the relevant extract from the list given at the beginning of each section.
For most of the sections there is a summary table indicating success rates for particular tasks in the three attainment bands bottom, middle and top thirds of the sample pupils.
Pupils were assigned to the bands on the basis of their scores on the GT4 reference test.
The results are from written tests unless otherwise stated.
In the summary table the tasks are briefly described in words as far as possible.
As criterion referencing is concerned with descriptions of performance this exercise may give an indication about how some criteria might be expressed.
The notion of criterion-referencing is discussed in more detail later in this report.
In the summary table the success rates are given to the nearest five per cent.
The success rate for a question is the percentage of those pupils taking the test containing the question who obtain a correct answer.
Where a range of success rates is given, this indicates that there were several assessment items for a particular skill or task and that performance covered a fairly wide band — an immediate warning that a criterion statement based on such tasks will not tell us much about pupils' achievements.
The results in the three attainment bands are based on small samples usually from 50 to 100 pupils in each instance differences of up to ten per cent can usually be discounted as a consequence.
Results for some of the practical circus items are included as appropriate.
The practical circus was used mainly to introduce the idea of practical testing to teachers.
Although at least one test session was conducted in all liaison group schools and most second phase schools, the actual use of the results for statistical purposes was very limited.
Similarly, interviews with pupils are included but were not carried out on a systematic basis and the results have not been correlated with pupils' GT4 scores.
In addition to a summary table some specific examples are given of questions and the results they produced.
In some instances the incidence of particular errors (error rates) are given in addition to a success rate.
The results for specific questions are those actually obtained and have not been given to the nearest five per cent.
Comments on particular difficulties or errors are also provided.
Number
Count, order, read and write positive and negative whole numbers and use them in context; e.g. what is the rise in temperature from -3°C to 10°C?
Understand place value in numbers of up to four digits.
The task of ordering whole numbers produced one of the highest success rates attained during trialling of assessments by the project, and provides an example of a tack where virtually all pupils can succeed.
However, any task which required the pupil to break down a number into its place value components — hundreds, tens etc. — was more demanding.
The higher success rates were for providing column headings in a table, around 85 per cent for supplying "thousands' and" hundreds "given" tens "and" units".
Dismantling a number was more demanding: The main difficulty factor here appears to be the need to see a number as made up of place value components rather than viewing numbers as a complete entity.
The reading and writing of whole numbers was looked at by asking pupils to write in numerals a number presented in words.
The difficulty factors noted covered both the general size of the number concerned and, more importantly, the need to put in spacing zeros to establish the place value of a digit.
Success rates were 95 per cent for 109, but 74 per cent for 1,035.
Large numbers presented great problems to many pupils.
 "One million" was translated successfully by 35 per cent of pupils with 10,000 being given by 44 per cent as their answer.
Tasks requiring understanding of negative numbers produced success rates around 30 per cent both for representation on a number line and for the question below: The presence of the diagrams allows the pupil to count to find the answer or to check an answer reached by computation.
Even so, the  obvious incorrect answer 15 was given by more pupils than the correct answer.
The context, temperature, is one of the most obvious and frequently used for negative numbers.
Understand and use the decimal system in practical situations and problems.
A variety of tasks looking at decimal place value was used, including for comparison purposes, both those in context where decimals represent measures and, those not in context.
As the summary table above indicates, virtually all of the basic tasks concerning decimal number concepts proved to be extremely difficult.
An example of the success rates obtained for ordering decimals not in context has already been given on p.50 for an item used in the reference test GT4.
Placing such tasks in context does not increase success rates but does alter the pattern of incorrect responses.
Two of the questions used are illustrated in Example 18.
The "largest-is-smallest" error is less common in questions set in a measurement context.
The number of pupils giving such answers is higher when the whole number part of the measurement is the same, and highest when all the numbers are of the form 0.
xx.
Success rates are no higher when the test item is placed in context, the chief effect of a context being to increase the rate for the "decimal point ignored" error.
Success rates for identifying the place value of a column or digit were low.
Three sample questions are shown below.
The general picture obtained for decimal place value is not a very happy one.
The only tasks where high success rates were obtained were those where the pupil can effectively treat decimals as whole numbers.
In the discussion surrounding the foundation list the Cockcroft Committee stressed "We believe too that, as a necessary accompaniment to the list we have given, it is important to have the feeling for number which permits sensible estimation and approximation…" .
On the evidence presented here it appears that the majority of low attaining pupils do not have sufficient working understanding of decimal place value in context to develop such a feeling for number.
The section on measurement (p.67) where the difficulties with decimal place value are very apparent, takes up this point further.
Pupils should appreciate the implications of movement of figures relative to the decimal point as a result of multiplication or division by a power of 10.
Several questions related directly to multiplication or division by a power often.
As the table shows, only those involving multiplication of a whole number by ten yielded a reasonable level of performance among the lowest third of the pupils.
All the sample had some difficulty with decimal numbers.
Success rates were generally low for questions which asked for knowledge of the decimal equivalent of common fractions: The answer 1.2 is suggestive of the view that a fraction a/b is equivalent to the decimal a.b.
Success rates were similar for writing 0.5 as a fraction.
The task of converting less familiar fractions to decimals with the aid of a calculator was also given to pupils.
The difficulty factor here is knowing that the operation to perform using the calculator is division.
Only amongst the top band did a substantial number of pupils perform the incorrect version 8 ÷ 3 rather than 3 ÷ 8.
These data suggest that the conversion of fractions to decimals, either by knowledge or use of a calculator, was unfamiliar to many pupils.
The conversion of fractions to decimals is a requirement for effective use of the calculator, and a further calculator question about the wages received for a 37½ hour week produced evidence that a number of pupils worked out the wages for a 37 hour week using the calculator and then made, or tried to make, a pencil and paper calculation for the remaining half hour's money.
Money
Recognise coins and notes and know that 100p = £1.
Handle money with confidence.
Carry out simple transactions, performing necessary calculations either mentally or on paper.
Add and subtract small sums of money without a calculator.
Multiply or divide a sum of money by a single digit without a calculator.
Questions on money were given in a variety of different modes: written, mental, practical and oral.
In the practical circus test items, pupils were asked to check change using actual coins.
A one-to-one practical item was also trialled.
Items on change, and those involving a number of articles costing £x.99 yielded some of the highest success rates obtained in the tests used in the project.
Although working out change for two items involves two operations (adding the prices of the two items and then subtracting or counting on to the sum presented (50p, £1 etc.)) the success rates were similar to those involving one item only.
Results for the same items were compared in written and mental test formats.
In one version of the mental test the prices of items bought were printed on the answer sheet so that they did not have to be memorized: in an alternative version the prices were not printed so that it was necessary both to memorize the numbers and operate on them.
No marked differences in performance were found unless the numbers were quite complex.
Some classes were asked to describe their methods for calculating answers to questions.
In each class at least four methods were used by different pupils to work out the answer to this item.
A few classes were tested by the research team.
Some pupils did not appear to understand method two but recognized its value when the calculation was to be carried out mentally.
In calculating the number of 17p stamps which could be bought for El, a few pupils used knowledge gained from playing darts: treble 17 was known to be 51 and 51p doubled was 102, making one 17p too many.
For the same calculation other pupils rounded 17p to 20p, a strategy which works here but would not work for £1.50.
Percentages
It is clear that very many pupils find great difficulties with the concept of percentage.
We recommend that teaching of percentage should be based on the idea that 1 percent means "1p in every pound" or "one in every hundred" , and not on the use of particular formulae.
Calculate a percentage of a sum of money.
Increase or decrease a sum of money by a given percentage.
Appreciate the use made of percentages in everyday life.
The use of percentages should be linked to activities of the kind listed under "money" .
The examples used should be realistic and relevant to the pupils concerned Emphasis should also be given to the way in which percentages are used both for comparative purposes in many everyday situations and also as a numerical measure based on a 100-point scale of reference.
A relatively small number of questions on percentages was used in the tests developed by the project.
One using the "pence in the pound" format was used in the GT4 reference test.
The "pence in the pound" format recommended above by Cockcroft is known, from the APU data, to be easier than the equivalent format using percentages.
Nevertheless, only the top third of pupils showed a high success rate for this task and the incorrect responses 10p and 40p were common in the middle and bottom attainment bands.
Given the difficulty level of this task, the topic of percentages was not chosen for more detailed investigation, but two further tasks are illustrated in Example 28.
These were part of a mental/aural test designed for the upper end of the target attainment range and success rates for the lowest band are thus not available.
The common error made in such tasks — for x per cent more add x — is apparent in all three tasks.
Use of calculator
Use a calculator efficiently to add, subtract, multiply and divide, and to convert a fraction to a decimal.
Appreciate the need for careful ordering of operations when using a calculator.
Be able to select from the calculator display the number of figures which is appropriate to the context of the calculation.
Substitute numbers in a simple formula expressed in words and evaluate the answer; e.g., gross wage wage per hour x number of hours worked; total cost number of units used x cost per unit + standing charge.
The summary table in this case simply indicates the considerable range of difficulty covered by most types of task.
The difficulty is that of translating the verbal problem or formula into the correct operations in the correct sequence.
A few examples illustrate the range of difficulty.
Adding
The above examples indicate that if the amounts to be added are expressed in different units (£ or p in the second and third questions above) then the question becomes much harder for the bottom and middle third pupils and even for those in the top third in the last example.
Multiplying
One question on a calculator test was about patterns formed by multiplying numbers together.
When multiplying was directly indicated success rates were high; over half the lower third band pupils obtained the correct answers: When the multiplication operation had to be inferred, success rates depended on the context.
In both the following examples relating to money transactions, the display also had to be interpreted because the result of the multiplication in each case was a complete tenth of a pound.
Selecting numbers from the display
There are several questions which may require pupils to select appropriate numbers from the calculator display after an operation has been carried out.
Two examples are given overleaf   Each of these can be calculated by several methods, one of which is by division.
Thus the number of 17p stamps which could be bought for a pound could be obtained by converting the pound to 100p and dividing by 17.
The calculator display would show a five followed by a decimal point and further numerals which would need to be ignored.
A similar situation obtained in the second example which unsurprisingly proved much more difficult because it is unfamiliar and the numbers are more complex.
The use of the skill of selecting appropriate figures from a calculator display in these examples depends on the method used.
The achievement of such a skill by a pupil in response to this question could only be recorded if the  method was known.
In fact the incidence of giving an answer such as 5.882 etc. in the stamps question was low; probably because many pupils in the sample did not use a calculator to do the question.
Substitute numbers in a simple formula expressed in words
One of the examples trialled proved to be easy.
The success rate appeared to depend mainly on the pupils' understanding of the figures involved.
A version of the formula given as an example in the Cockcroft foundation list (see below) was exceedingly difficult even for top third pupils.
One of the main problems for pupils in the above example is understanding the idea of a "mathematical" cost: a fraction of a penny.
Another calculation involving a similar relationship produced higher but still poor success rates.
This result suggests that the structure itself is difficult for many pupils.
These results show that calculations with a calculator produce high success rates when the context is familiar, the numbers involved can be easily interpreted, and the structure of the calculation is simple.
These factors interact making it difficult to predict the likely level of difficulty of a specific problem for pupils.
For example, it would appear that substituting in a simple formula expressed in words is too difficult for lower attainers, although superficially it seems to require little more than following simple instructions.
Several of the calculations which produced low success rates could probably be undertaken with a good deal more success in the actual context concerned.
Thus one liaison group teacher commented that when lower attaining pupils are abroad on school journeys they quickly pick up a feeling for the conversion rate of the local currency.
To do so, however, the conversion rate had to be given to the pupils in a more convenient form than the number of foreign money units to the pound and pupils did not necessarily learn to apply the arithmetic algorithm to conversions.
This suggests that a high degree of practice of a specific task in the situation where it is needed can lead to success, but whether such skills so acquired in one situation are transferable to others is a contentious point.
Time
Be able to tell the time and understand times expressed in terms of 12- and 24-hour clocks.
Be able to calculate the interval between two given times, and the finishing time given the starting time and duration.
Use a bus or train timetable.
Solve simple problems involving time, distance and speed.
Emphasis should be placed on mental calculation where appropriate and on the use of both digital and traditional "clock" display.
A number of questions involving 12–24 hour clock conversion, calculations of time intervals and calendar dates (without the presence of a calendar) were included in parallel mental and written test versions together with questions on other topics.
The written test versions tended to produce the higher success rates because the questions were simply phrased and did not involve memorizing information as did the mental tests.
The written test success rates are given in the table below.
In another written test on tabular data one set of questions was about a calendar which was provided.
There was also a "thematic" test relating to a day out journey in which a bus timetable was used.
24 hour clock time Conversions
These questions yielded high success rates in all three attainment bands; that is, if 7.45 is accepted as an answer to "What time is 19.45 on the 12 hour clock?" .
In their answers to that question a high proportion of pupils in all three bands left off the "pm" after the 12 hour clock time.
It would superficially seem reasonable to accept 7.45, but it should be noted that more of the highest third of pupils placed "pm" after the clock time than did pupils in other bands.
In other measurements, too, it has been noted that the importance of units was more likely to be appreciated among higher attaining pupils.
Time intervals
The success rate for calculating time intervals depended on whether there were figures other than zero in the units column of the minutes.
Non-zero figures made the question harder and it was also harder if the interval spanned more than one hour.
Some records were made by both the research team and by teachers of the methods used by pupils to obtain answers to the first two questions above.
Nearly all the pupils interviewed used counting on or a mixture of subtraction and counting on methods.
In the case of the 9.43–10.12 interval, some counted on to ten or subtracted 43 from 60 and then added 12.
Others adjusted the 9.43 to 9.40 or 9.45 and then, after adding 12, readjusted, sometimes incorrectly.
The following time interval questions related to bus timetables (see Example 41) given in a thematic test on a day out trip.
The 9.43–10.12 question appeared in this test context also, and fewer pupils were successful than when the question was asked directly, probably because the information required had first to be selected by the pupils from all the figures given in the timetables.
Nearly all pupils tackled the question and a variety of answers was given.
The context of a bus timetable does increase the number of possible ways the question might be done; on one occasion when a member of the research team gave the test informally to a class, one pupil said he had found his answer (several hours) by adding up the intervals between all of the stops.
Calendar dates
About three-quarters of pupils in the lowest third knew how many days there are in certain months, although a number of pupils when interviewed said they had not been taught by their parents or at school any way of remembering the number of days in the months.
These questions appeared in pairs of parallel tests, both of which were taken by some pupils with an interval of about two months between testings.
One teacher said that he had taught his class a rhyme for remembering the days between the two testings.
Questions about finding the date a given number of days after another date were asked in written tests, with and without a calendar, and in a mental test.
The effect of these factors can be seen from the results below.
In the test question with a calendar provided, the number of days in July was not given; the success rates would probably have been higher had this information been given.
The lower third band of pupils are particularly affected by the absence of a calendar, but the memory requirements affect the top third of pupils more than the others.
Another question in which memory requirements affected the lowest third pupils particularly is now illustrated.
One finding of interest provided by the test with a calendar was that a high proportion of pupils in all attainment bands knew that dates in the following year fell a day later than the previous one.
The calendar given was 1985, the year in which the test was taken.
The questions on clock time showed that a high proportion of pupils understand time expressed in terms of 12 and 24 hour clocks.
Success in computation relating to time intervals is highly dependent on a number of factors and there fore is variable.
These factors include: whether the interval is within a particular hour or goes across hours; and whether or not the minutes are multiples often.
Timetables are difficult for many pupils because the arrangement of the information has to be understood and the relevant figures extracted before calculations on time intervals, etc. can be made.
Questions on the calendar were generally well done if a calendar was provided.
Finding dates and days of the week both within and across months was harder without a calendar.
There was some evidence that a substantial number of pupils do not know a mnemonic for remembering the number of days in a month.
Measurement
Measure length, Weight and capacity Using appropriate metric Units.
Understand the relationship between millimetres, centimetres, metres, kilometres; grams, kilograms, tonnes; millilitres, centilitres, litres; know that I litre is equivalent to 1000 cubic centimetres.
Have a "feel" for the Size of these Units in relation to common objects within the pupils' experience.
Use the following imperial Units: inch, foot, yard, mile; ounce, pound; pint, gallon; and know their approximate metric equivalents, i.e. that 3 feet is about equal to 1 metre, 5 miles is about 8 kilometres, 2 lb is about 1 kilogram, 1 gallon is about 4½ litres.
Understand and use simple rates; e.g. £ per hour, miles per gallon.
Read meters and dials of various types.
Emphasis should be given to practical activities involving measurement and estimation.
Opportunity should be taken to relate teaching to measurement of the kind encountered in other curricular areas such as science, geography, home economics, craft and sport.
The picture of performance on the measurement tasks suggested by Cockcroft is a complex one.
One factor leading to this complexity is the co-existence of two systems of measurement — the metric and Imperial systems.
For many pupils their experience in school will have been mainly or solely with metric units and their out of school experience mainly with Imperial units.
Some pupils expressed strong negative views on having to deal with the two systems, and several tasks were tried out in two versions, one with metric units and one with Imperial units, the version with Imperial units often proving to be easier.
Examples are given below.
A second complexity factor  concerns the sensitivity of success rates to the sizes of the numbers and the relationships between them, the measure and units in use and the measuring instrument chosen or provided.
The summary table above makes use, in several places, of a more finely detailed task classification than elsewhere in this chapter but the range of success rates is still very wide.
The implications of this for criterion-referencing and graduated tests are discussed in the next chapter.
The central underlying difficulty factor is the degree to which decimals have to be dealt with in a particular situation, from "not involved" (i.e. whole numbers) through "can be avoided" (nearly all questions on money) to "cannot be avoided" .
Measurement skills were tested in written, mental and practical tasks.
The practical tests included: measuring a door to select a replacement; measuring a table to select an appropriately sized tablecloth; measuring photographs to select appropriate frames.
One of the problems of the door task illustrates the metric-imperial issue: most schools had only metric measures available but doors are sold commercially in Imperial units.
Success rates for tasks where the pupil has to make a measurement, or work from a given measurement, were found to depend upon two specific difficulty factors whether the number concerned is an awkward decimal and whether or not the measuring instrument in use has to be applied repeatedly.
Taking length as an example, the effect of these factors can be seen from Table 4.7.
Pupils in all bands had more difficulty when decimals were involved and when the 30cm ruler had to be used twice to take a measurement.
tines longer than 30cm will not fit conveniently on conventional sized paper (A4) and it may well be that because of this many pupils had relatively little experience of measuring or drawing such lines.
Questions on the relationship between metric measures again produced a wide range of success rates.
As might be expected any questions involving decimals were rather demanding, but it is noticeable that where the answer included a half success rates were higher than for 0.1 or  and other decimals.
The familiar fraction "½" "is a  much more common answer for lower attainers than 0.5, and in the section on number it was evident that many pupils find it difficult to go from the fraction to the decimal even in this the most familiar case.
When the answer required is a whole number the general pattern of difficulty was found to cover a sharp difference between cases where the answer is I, 000 (a litre, a kilometre, a kilogram or a tonne) and cases where the answer is 10 or 100 (1 cm or 1m).
This may be due to a lesser degree of familiarity with the larger units which are 1,000 times as large as the smaller units with which the pupils are likely to have more practical experience.
It is also the case that the two commonly used relationships with ratios often and 100 concern millimetres, centimetres and metres and are perhaps the units which most pupils use most frequently.
Sample questions are shown in Examples 45 and 46.
Of all the questions used, only those with answers often, 100 or a half — looking at the relationships between millimetres, centimetres and metres — produced high success rates.
Most low attainers lack knowledge of the relationship between metric units and they also have problems with decimals.
These two factors together mean that the fundamentals of the metric system present difficulties to them.
This is likely to make it difficult for them to check the reasonableness of their answers and to question their estimation of measurements.
A further factor here is pupils' general knowledge of the value of a measure when applied to "everyday" objects.
Two separate tests which  looked at such general knowledge were constructed.
The same question was usually set in two versions, one multiple choice and one where the pupil had to supply an answer.
For many of the questions there was little difference between the knowledge displayed by top band pupils and that displayed by bottom band pupils.
The analyses of the multiple choice versions suggest strongly that many pupils were guessing and it is only the non-multiple choice versions which are reported here.
Some items were also tried out with Imperial units and these were often much easier than the same question with metric units.
Sample questions are shown in Example 47.
Success rates, as can be seen from the summary table, were generally low for all questions on metric units except some of those on length and temperature.
It may well be the case that both lack of familiarity with this type of question and lack of practical experience in measurement of everyday objects are factors leading to low success rates.
The sort of task illustrated above is closely related to estimation tasks.
Success rates for estimating the current temperature of the room were similar to those shown above.
Estimation questions bring into focus the problems of decimals, the range of answers regarded as correct, the role of practice in effective estimation and the pupils' perceptions of what constitutes an acceptable answer in mathematics.
These problems are further discussed in the succeeding chapters in sections on mastery, criteria and feasibility.
Success rates were also low for knowledge of the common Imperial-metric approximate relationships that the Cockcroft Committee suggested pupils should be familiar with.
The highest success rate obtained for items on the relationship between metric and Imperial units was for Example 48.
Although the majority of pupils were unable to give approximate equivalences between metric and Imperial units of length, most of the middle and top band of pupils were able to place the metric and Imperial units in order of size.
In summary, the performance of pupils in the sample on measurement questions was affected by three principal factors:(i) difficulties with decimals;(ii) lack of knowledge of the relationships between units of the same measure;(iii) lack of general knowledge about the value of a measure for everyday objects.
If the aspirations of the Cockcroft Committee for low attainers are to be met, then these three aspects of difficulty will need to be dealt with in future teaching schemes.
Certainly the emphasis on mathematics in everyday contexts and the more central role given to practical mathematical tasks, such as measuring, in many of the newer schemes and syllabi should be helpful in all three cases.
Graphs and pictorial representations
Organise Systematically the collection and tabulation of simple data.
Read and interpret simple graphs and charts and extract specific information from them; construct them in simple cases.
Extract information presented in tabular form; e.g. cost of making a telephone call.
Pupils should be introduced to a wide variety of forms of graphical representation and use should be made of published information.
Pupils should be encouraged to discuss critically information presented in diagrammatic form, especially in advertising.
The drawing of graphs should wherever possible be based on information collected as a result of practical activities,
Be able to interpret a simple flow-chart.
Work with flow-charts should include discussion aimed at developing the logic used in mathematical arguments, such as "if,…then…" .
The reading of some charts and graphs and also of information presented in tabular form was the only topic covered in the testing from this section.
(It was not possible to cover the complete range of topics in the Cockcroft foundation list during the project.)
Some questions related to the interpretation of the information.
Most of the data came from published materials, advertisements in newspapers and tariffs of various kinds such as the cost of a telephone call.
Tables and graphs vary considerably in three respects: the form of presentation; the density of information provided; and the nature of the information.
Each of these has its effect on the difficulty of reading and interpreting the data, but not always in a highly predictable way.
The highest success rates obtained (90–100 per cent) were for answering simple questions about a calendar: on which days of the week were given dates and how many days there were in a given month.
The results for some other questions on calendars have already been considered in the section on time.
Lists and two way tables
One of the simplest tables provided is illustrated below (Example 49).
It consists of two lists of numbers with clearly labelled rows and columns.
There is no extraneous information to interfere with the layout.
Pupils were asked, in separate questions, when it is sunniest and when it is hottest.
The success rates were similar for both questions; 45–50 per cent of the lower third band, 70–75 per cent of the middle third, and 80–85 per cent of the top third pupils giving both relevant months in each case.
Nearly 20 per cent of the bottom third and about 10 per cent of the top chose July on each occasion, possibly because it is both one of the two hottest and one of the two sunniest months.
The question "which month has the least sunshine" has a unique answer and higher success rates were recorded: 77 per cent in the bottom, 80 per cent in the middle and 94 per cent in the top bands respectively.
Thus, a trace of ambiguity in the data can lower success rate.
The next item (Example SO) is an example of a single list of charges, but the table is more crowded than the first one illustrated and there is  a good deal more information to absorb.
There is a potentially confusing distinction between "charges per hour" and "charges per session" — most of the sessions being an hour long.
Looking up charges for a cricket net or a tennis court per hour yielded high success rates (around 80 per cent for each of the bottom two bands and around 90 per cent for the top band).
The following question (ExampleS]) produced much lower success rates.
While there is some simple arithmetic to carry out in this question, it is much more likely that the drop in success rate is due to problems with interpretation and the distribution of the error response, £2.60, confirms this.
One of the most crowded tables was an extract from a guide to hotels, motels, etc. in England.
The context was one unlikely to be familiar to the pupils.
Despite these unfavourable factors a high proportion could read direct information from the table, but any interpretation required by a question or any processing of the required data reduced success rates.
Charts: mileage, postal and telephone charges
Although the basic structure of all mileage charts is the same the presentation varies and there can be more than one set of information (e.g. distances in miles and kilometres).
The chart (Example 53) was considered one of the simplest in presentation.
The Birmingham-London distance proved by far the easiest question, It is only necessary to deal with the outer row and column of the table.
The error response in the Gloucester-Holyhead question is of interest because it is the top band pupils who are most likely to treat the obliquely placed town names as arrows pointing in the directions of the required figures.
Fewer do this in response to the next question, presumably because it is not so obvious an effect unless the required figures are being directly pointed at.
The last question illustrates the difficulty of wording a question about real distance which does not get confused with distance on the tables, but this is more likely to happen in a pencil and paper test than in a practical situation.
No doubt chart-reading skills would also improve in an actual situation, but there do seem to be some serious difficulties in understanding how the numbers are related in the table.
The telephone charge table, illustrated in Example 54, produced high success rates for direct questions.
Examples of these are "what rate is a phone call at 10 am on a Tuesday" (62 per cent, 78 per cent , 82 per cent correct within each band respectively) or "what is the most expensive time of the day for telephone calls?" (63 per cent , 69 per cent , 85 per cent ).
Costing the price of a letter
Several questions were asked about letter post charges.
There was a general decline in performance as the weight of letter increased!
This may be due to greater familiarity with the cost of lower priced letters or to the greater difficulty of keeping in mind which band of prices the weight related to in the body of the table.
There was a practical circus item which asked pupils to weigh given letters and then cost them using the postal charges table (see Table 4.8).
One of the letters in the practical was the same weight as one in the written test.
Some pupils observed doing the practical task attempted to treat the charge as if it was a continuous scale, but there was no firm evidence of this happening in the written test version.
In general, the success pupils had with tables of data depended on the complexity and clutter in the table.
More particularly difficulty is related to questions which are not direct readings, but have to be interpreted (however little) or data which need to be processed (no matter in how minor a way)(Table 4.9).
Spatial Concepts
Recognise and name Simple plane figures; understand and use terms such as side, diagonal, perimeter, area, angle.
Understand and use terms relating to the circle; centre, radius, diameter, circumference, chord.
Appreciate the Properties of parallelism and perpendicularity; be able to measure angles in degrees.
Draw a simple plane figure to given specifications.
All pupils should learn to use drawing instruments such as ruler, compasses, set square, Pro tractor.
For some pupils this may require considerable practice.
Plane and solid shapes should be available which pupils can handle and measure, and use for building.
Recognise and name common solid shapes; cube, rectangular block, sphere, cylinder, cone, pyramid.
Attention should be paid to the representation of three-dimensional figures in two dimensions, Pupils should be able to recognise the two-dimensional drawing corresponding to a given solid, and be able to recognise and build solids from such drawings.
Use should be made of such things as plans of models, dress patterns, scale drawings, photographs, maps.
Find the perimeter and area of a rectangle.
Find the volume of a rectangular solid.
Recognizing and drawing simple plane figures
In one item pupils were asked to draw a square, rectangle, parallelogram, circle, kite and rhombus.
In another, a square, parallelogram, trapezium, kite, rectangle and rhombus were illustrated and pupils asked to provide a name.
The success rates for each type of task are given in Table 4.10.
The results show that, generally it is easier for most pupils to draw a named shape than provide a name for a shape.
The exceptions were for the square where performance in the two tasks was about the same and for the rhombus where "diamond" was accepted as a response to the naming task.
These items only permit one example of the shape to be given by the pupil or presented in the test.
They do not explore the range of shapes pupils will accept or regard as examples of a shape.
In another item pupils were asked to select rectangles from a set of six quadrilaterals.
The quadrilaterals included four rectangles (one of which was a square), differing in shape and orientation, a parallelogram and a trapezium.
Only ten per cent of the sample overall picked all the rectangles and rejected the non-rectangles further 26 per cent picked the rectangles and parallelogram but rejected the square as a rectangle.
A practical test item asked pupils to make several shapes from four right angled isosceles triangles.
The results are shown in Table 4.11.
The results of items on pupils' knowledge of shapes suggest that pupils judge shape on general appearance rather than on the geometrical properties which define them.
The terms "diagonal" , "side" and "perimeter" of a rectangle were known by a high proportion of pupils, "area" by fewer.
Pupils' knowledge was studied by asking them to mark appropriately a diagram of a rectangle and its diagonal drawn in a dotted outline.
A similar item revealed less knowledge of terms relating to a circle than to a rectangle (Table 4.12).
"Radius' was more often confused with other circle-related terms than were the other words.
About 20 per cent indicated the circumference, and seven per cent the diameter, when asked to give the radius.
 "Parallel" was rather better known than "perpendicular" or "at right angles to" (see Example SS).
Making the given line diagonal to the page had an effect on the success rate of the bottom band only.
However, when asked to pick out parallel lines from parallel and non-parallel sets varying in several factors, the success rate was much lower; about a quarter of all pupils taking the test rejected lines of different lengths as parallel.
Asked to draw lines perpendicular to or at right angles to a given line, fewer succeeded than for parallel lines.
Nearly half the bottom band drew a parallel line, not a perpendicular one, and as many drew a diagonal line when asked for one at right angles.
Drawing a plane figure with instruments
Accurate drawing was required for some constructions and teachers were asked to supply rulers, protractors and compasses.
The success rate for an acceptably accurate construction is given in Table 4.13.
The constructions rarely suggested that instruments had been used appropriately.
Compass arcs were only rarely seen, for example, and it appeared that many of the drawings had been obtained by trial and error.
Angles: understanding and measurement
The understanding of the concept of "angle" was tested by items asking pupils to compare angles drawn on a grid or on plain paper.
The drawings had contrasting features, including some irrelevant to the size of an angle, such as the length of containing line and the size of the arc indicating the angle.
About 80 per cent of the top band pupils were not distracted by the irrelevant features of the drawings but nearly half of the bottom band and a third of the middle band of pupils were.
It is evident from these results that if an irrelevant feature of an angle, such as the length of the containing lines and the actual size are correlated, then a large proportion of lower attainers could make a "correct" comparison for the wrong reasons.
Measuring techniques can be learned without fully understanding the underlying concepts.
Despite the fairly low success rates on questions probing the concepts, those questions asking pupils to use a protractor to measure given angles or draw an angle to a specified size yielded quite high success rates for both acute and obtuse angles.
The highest was for measuring an angle of 30-, 72 per cent of the lowest band, 83 per cent of the middle and 90 per cent of the top band of pupils giving the correct answer.
Just over 10 per cent of the bottom two bands gave an answer which indicated they had used the wrong scale.
Measuring an obtuse angle of 150- was successfully undertaken by 55 per cent, 74 per cent and 75 per cent respectively of the pupils in the three bands.
This time nearly a quarter of the bottom band and 10 per cent of the top two-thirds of pupils used the wrong scale.
There was a lower success rate when the angle to be measured was not on a labelled part of the scale (Table 4.14).
Success rates among the bottom band were reduced to 30–40 per cent and in the top band they ranged from 65–80 per cent.
There were three practical circus tasks which involved 3-D shapes: making a label for a tin can; making a cover for a cuboid; and classifying various shapes.
In response to the first task, 72 per cent of the pupils obtained an acceptable height measurement for the label but 44 per cent only cut out the label with acceptable accuracy.
The task of making a cover for a cuboid was achieved with acceptable accuracy by 77 per cent of the pupils, the lowest attainers being more successful than the top band.
Several versions of the classifying 3-D shapes task were given on various occasions but there are insufficient results to give any meaningful statistics.
Pupils were asked to group shapes and say in what ways they were the same as or different from shapes in the other groups.
Pupils used no more than two attributes and these mostly related to names of 2-D shapes (e.g. "They're both triangles" said of a pyramid and a cone).
Identification of 3-D shapes from their 2-D representation was the subject of two groups of items in written tests.
One had pictures of a cube, sphere, cylinder, cuboid, pyramid and cone and asked pupils to provide the names for them.
Over half the bottom third and more than three-quarters of the top third pupils provided appropriate names for the cube, cylinder, pyramid and cone.
The mathematical names for the sphere and cuboid were not well known: a high proportion instead provided the name of the face of the cuboid, or gave "circle" for"sphere (Table 4.15).
The cube was described by nearly a third of the pupils in the sample as a "square" .
Success rates were higher when names of shapes were provided and had to be matched with pictures of real objects in the forms of a cylinder, prism, cuboid and some pyramids.
The list of names from which the choice for each object was to be made included four relevant ones (e.g. "rectangular box" for cuboid).
For three of the shapes the appropriate name was provided by around three-quarters of the bottom third pupils and by nearly all the top third.
For the prism (represented by a Toblerone chocolate bar) success rate was 37 percent, 58 per cent and 75 percent respectively for the three attainment bands.
Another task was to identify the nets of a cube, pyramid, cylinder, cuboid, cone and tetrahedron.
A list of names was not provided from which to select the appropriate ones.
The pyramid was most easily identified, and the cuboid the most difficult.
There was an even greater tendency to give names of plane shapes in this task than when names were to be provided for pictures of 3-D mathematical shapes.
Some examples are given in Table 4.16.
Visualizing 2-D representations of 3-D shapes
A question asking pupils to interpret a 2-D depiction of a 3-D situation was included in the GT4 reference test and reported earlier (p.40).
Over half the pupils in the bottom third gave the correct answer to that question and a fair proportion did so on two visualization questions in other tests.
In the first instance the pupils were provided with the diagrams illustrated in Example 56 and asked to draw, on isometric dotted paper, two more arrangements.
About a third of the lower band pupils did so, 40 per cent of the middle band and 67 per cent of the top band.
 "The diagram overleaf represents a building" , pupils were told, and they were asked to draw on dotted paper (square matrix) what the building looks like from A and from B, having stated how many cubes were in the diagram.
There is ambiguity in the question asked but a high proportion interpreted it literally.
Less than five per cent gave ten cubes, which is a possible interpretation of the diagram.
Perimeter and area of a rectangle
There were a number of questions in written tests to assess pupils' performance in finding the area or the perimeter of rectangular shapes.
One station in the practical circus included a task requiring pupils to draw a rectangle of a specified area.
It was already known from the APU results that the use of terms such as "area" and "perimeter" , especially the former, made questions harder than when explanatory phrases were substituted.
In different questions either the mathematical terms were used or phrases such as "How many squares could you fit into the shape?" and "How far is it all the way round the edge of the shape?" 
Other variations in the questions were that the shapes were presented with or without square grids and with all four or just two lengths labelled in the diagram.
The highest success rate was obtained for the following question: The diagram had a square grid and the question was phrased "How many squares…?" ; the word "area" was not used.
Without a grid covering the shape, success rates for area or for counting squares fell to between 35–65 per cent for the bottom band, and there was generally some reduction for the remaining pupils.
The question below produced the lowest success rate for the bottom third.
 "How far round the edge…?" questions also produced higher success rates than those with the word "perimeter" in the wording.
Many pupils found the area instead of the perimeter when the word perimeter was used — about 30 per cent of the pupils — in response to one question.
Volume and capacity
Several questions in the written tests related to a 2-D drawing of a cuboid 4cm x 3cm x 2cm.
There were several variations on the presentation including the wording ( "How many small cubes…?" or "What is the volume of the box?" ) but the success rates were very similar, 10–20 per cent for the bottom, 3550 per cent for the middle and 60–80 per cent for the top band of pupils.
The presentation seemed to affect the error patterns more than the success rates (Table 4.17): there were several ways in which the dimensions of the box were used to obtain an answer.
Map reading
Appreciate the concept of scale in geometrical drawings and maps, and the use of coordinates to locate areas (as on a street map) and points (as on an ordnance survey map).
There is likely to be considerable overlap between this topic and work on graphical and pictorial representation, especially in respect of the interpretation and construction of simple plans and elevations.
Tasks based on road, town, London tube and contour maps proved to have a wide range of success rates.
Tasks which placed greater demands on interpretation and judgement resulted in lower success rates.
Another map showed contour lines, the positions of a few prominent objects and the heights of one or two locations.
Nearly all of the pupils found it easy to locate an object and read its height (Example 64).
Interpreting gradients across contour lines proved more difficult still (Example 66) though performance was fairly consistent between the three tasks.
Ratio and proportion
Understand the use of ratio as applied to such things as mixtures, e.g. 2 parts sand to 1 part cement; and recipes, e.g. work out the quantities required for 6 people from a recipe which serves 4.
This work provides opportunity to discuss ideas such as "best buy" .
Applications to craft work and model making provide overlap with work on scale drawing.
Understand informally simple ideas of direct and inverse variation.
For example, as speed increases, distance travelled in a given time increases; as speed increases, time to travel a given distance decreases.
For some pupils it will not be appropriate to attempt detailed calculation.
Ratio and proportion are difficult concepts and therefore a limited number of questions was used, but it is not suggested that these represent the only possible ones for lower attainers.
The questions used appeared in a mental test, in a "thematic" test ( "Design a bed-sitting room" ) and other written tests.
No summary table is given here because there were so few questions.
Increasing or decreasing amounts in a given ratio
Given small numbers and simple ratios for which doubling and halving methods can be used some fairly high success rates were obtained as is suggested by the CSMS3 project results.
The results for one set of three questions on a recipe was in the GT4 reference test and has already been discussed in the previous chapter.
The following items were in a mental test (Examples 67,68,69).
The error of 5 lbs made in the recipe question is an example of using an additive instead of a multiplicative relation.
It has been noted as a common error in response to ratio and proportion questions, but its incidence is much influenced by the numbers in the questions and particularly by context (Bell, APU).
The only evidence of it in these three items is in the recipe question.
A few pupils in some classes were interviewed about the methods they used to answer the questions and most used doubling and halving as expected.
The additive method for the recipe question was also noted.
There were also some unique methods (for this small sample).
In response to the pills question, one pupil found that 50 pills weighed 1 g and then added lots of 50 pills until 250 was reached.
A more sophisticated use of doubling and halving was used by another pupil to find how much 3 lbs of pears would cost: the pupil doubled 2 lbs costing 30p to obtain 4 lbs costing 60p and then said that 45 was halfway between 30 and 60.
The "thematic" test, "Design a bed-sitting room" , involved tasks on scale (Examples 70,71).
Pupils were provided with a rectangular grid on which was drawn a rectangle 13 x 16 cm representing a room with a door and window marked on it.
Pupils' difficulty with the scale (4 cm representing 1 m) was tested by some initial questions asking for the length and width of the room.
The low success rate for finding the width is due to the measure not being a whole number of metres.
A substantial proportion of pupils in all three bands were able to get the whole number of metres correct.
Below the "room" was a scale (4 cm to! m) and a drawing of a rectangle representing a bed 2 m by 1 m.
Space was available on the grid, below the room ", to cut out the bed, and other pieces of" furniture".
The "furniture" was then to be arranged into a design for the room and drawn on to the grid.
The success rates for correctly representing the size of the pieces of furniture in the design were related to whether or not a whole number of squares represented a length (Table 4.18).
Schools in the sample were asked to provide pupils with scissors for this test, but it was clear that not all did so for in some classes no one used scissors.
In other classes some did and some did not; perhaps some rejected their use.
In all about one-third of the pupils who did the test actually used scissors.
An interesting feature of the results was that the pupils who did not use scissors had higher scores on the non-practical part of the test, but represented fewer pieces of "furniture" on the diagram of the room than those who used scissors.
Also the room designs of pupils not using scissors generally had more unacceptable features.
The design task set in this test has some non-mathematical features and it was used in an exercise with some groups of teachers to obtain their views on what should be marked.
There are three main features of the task:(a) interpreting the scale of the diagram;(b) appreciation of the positioning of furniture in relation to doors, windows etc; and (c) general design considerations.
There was unanimity among the teachers about awarding marks for (a) and a very large measure of agreement that some aspects of (b) should also count towards the assessment.
Few thought that there should be marks for the design, some on the grounds that this was not a mathematical feature and others because an objective mark could not be given.
The results of the items discussed in this section confirm that a high proportion of pupils can deal with ratio and proportion questions when doubling and halving methods can be used.
Sex differences in performance
Sex differences in the results of the GT4 tests were reported in chapter 3 (pp.31–2).
They showed that, in the sample of pupils participating in the testing, boys had a higher mean score than girls on the test.
They also indicated that some topics favoured boys and others favoured girls.
The test items developed for this project added further details to the picture.
Items in three contexts in particular led to the girls consistently obtaining equal or higher success rates than the boys in the sample.
The questions were about (i) calendar dates (some with and some without the presence of a calendar),(ii) the cost of goods of various kinds: shopping for food and other articles; renting or buying television sets; reading a table of car ferry prices and so on, and (iii) multiplying or adding numbers in calculator tests.
Examples of a calendar item which appeared in three different tests, and the results obtained are given in Example 72.
It should be remembered that it is the consistency of the results over a number of items and not isolated examples which suggest that the context favours girls.
This item was in a written test and also in a mental test.
In the written test 47 per cent of the boys and 57 per cent of the girls obtained the correct answer, while in the mental test the corresponding figures were 34 per cent and 43 per cent respectively.
In another test there was an item involving the same date and time interval but a calendar was provided.
This time both sexes had a success rate of 79 per cent.
In the calculator tests there were questions on money, measures and some number pattern items involving addition and multiplication.
The pattern of sex differences was that girls generally scored better than the boys on the money and number pattern items and boys on the measures.
One of the number items is illustrated in Example 73 with the success rates for boys and girls.
When asked to find similar patterns for other numbers, only seven per cent of boys did so compared with 28 per cent of the girls.
The boys very consistently had higher success rates in items involving anything to do with measures: estimating, measuring or calculating lengths, time intervals etc.
For example, boys obtained success rates from 10–20 percentage points higher than girls in reading a chart of distances between towns.
Boys' knowledge of measurement units was much higher than that of the girls: their success rates were between 20 and 30 percentage points higher on items probing this knowledge.
There were two tests of number concepts which looked at pupils' understanding of whole number and decimal place value, the number line, and some aspects of fractions.
These tests resulted in a much superior performance by the boys relative to the girls.
The concepts involved are more vital to measurement than to those areas where girls do relatively well.
It is possible that boys' greater experience with measurement in other curricular subjects, such as technical drawing, as well as in mathematics, improves not only their ability to measure but also their understanding of the underlying number concepts.
The geometry tests included questions on drawing, visualization knowledge of names of shapes and parts of shapes, and area and volume items.
On most of these the boys did better than the girls, especially on drawing and visualization items.
For example, asked to draw accurately a triangle with all the sides 6 cm long, 52 per cent of the boys were judged to have done so compared with 30 per cent of the girls.
Girls generally did as well or better than boys on questions concerning knowledge of the names of parts of rectangular shapes (cuboids, rectangles) but a good deal worse on similar questions relating to 2-D and 3-D shapes with circular sections.
There were three tests on map reading.
The boys' success rates were higher than those of the girls in 80 out of the 81 questions.
Although sums of money are represented as decimal numbers, they can easily be thought of as pounds and pence, or pence alone: the idea of decimals can therefore be avoided.
The questions on calendar dates  require pupils to count, or at most calculate, using whole numbers.
Thus neither context in which girls do well relative to boys is likely to add to their measurement skills.
The picture presented by the results of the GT4 and project tests is of boys and girls underachieving in different ways.
More boys than girls are in the lowest mathematics sets although quite a number obtained fairly high test scores on the GT4 test.
Girls' underachievement relates to specific areas of mathematics, especially those involving measurements and the concepts which underlie them; this kind of underachievement is almost certainly of long standing.
If graduated tests are to have a large measurement component then it seems that, as things stand at present, a significant number of girls will be disadvantaged.
Project results and GCSE List 1
The level of pupils' performance in most tasks rose fairly sharply from the bottom to the top third of attainment in the sample.
If 80 per cent of pupils were to take GCSE then most of the top third of lower attaining pupils in succeeding cohorts would be among them.
How are they likely to fare in the GCSE?
This issue is now discussed in relation to the topics given in List 1 of the National criteria for mathematics.
In particular, some topics are picked out which are very likely to be difficult for the top third of the "lowest 40 per cent " .
It should be remembered, however, that the results of this project were obtained from 4th year pupils, not the 5th year, and that the schools involved were not operating a GCSE curriculum.
The summary which follows is a guide to the probable areas of difficulty, based on the current levels of performance of pupils a year or more younger than those who will be taking GCSE.
list 1 topic Guide to performance of top third Vulgar and decimal fractions 48 per cent compared fractions correctly and around ten per cent compared decimals correctly (pp.50,51); 33 per cent wrote 0.4 as a fraction (p.52); 50 per cent correctly wrote "½" as a decimal (p.53); 35 per cent correctly divided 56 by 10 (p.52).
Percentage of a sum of 69 per cent gave the amount money saved when there is a reduction of 10p in the pound (p.56).
Increasing sums of money by ten per cent was given correctly by 48 per cent, and five per cent by 22 per cent (p.56).
Scales, including map scales Questions concerned with the sizes of objects in a diagram of a room with scales 4 cm 1 metre were correctly done by From 33 per cent to 71 per cent depending on the measurements involved (p.94).
Measures: reading a scale 53 per cent correctly read a scale when a division did not represent a single unit.
29 per cent could read a value between graduations. 50–65 per cent could measure in cm lengths longer than the ruler in use (p.69).
Common measure of rate 57 per cent worked out, using a calculator, how many gallons of petrol will be used by a car travelling 280 miles at 30 mpg (p.61).
Personal and household 18 per cent worked out, using a finance: wages and salaries calculator, weekly pay for 39½ hour week at £2.65 per hour.
List 1 topic Guide to performance of top third Use of tables and charts 60 per cent could use a hotel guide (p.75) and 41 per cent answered correctly the hardest question about a distance chart (p.77).
Time: intervals 47 per cent worked out length of time for a train journey (p.64).
Simple change of units: foreign currency 42 per cent worked out, with a calculator, the number of Swiss francs for £250 (p.59)
Substitution of numbers for 52 per cent could do this for an words in formulae easy standing charge and number of units' formula and 17 per cent for an electricity bill (p.61).
Vocabulary of triangles 50 per cent could name an isosceles triangle and 33 per cent could draw one on a Quadrilaterals squared grid (p.79).
Over 90 per cent know square "," rectangle "" kite".
45 per cent know "parallelogram" and seven per cent"rhombus' (p.80).
Use of drawing instruments 50 per cent drew an acceptably accurate construction of an equilateral triangle (p.82).
Elementary ideas of ratio 59 per cent worked out 12/18 = 8/x?
These results illustrate that the understanding of fractions, decimals, measuring scales and units, and the meaning of numbers in context, are the areas of work which will be most difficult for the top third of low attaining pupils.
It is unlikely that many of the top band of lower attainers will get a high score on questions concerned with these topics.
If performance is to be improved then discussion and examples relating to these concepts rather than practice in routine skills is probably required.
The difficulty features of tasks, identified from the results of the testing, can provide foci for teaching — a theme which will be taken up in Chapter 6 (pp. 153–87).
Discussion
This chapter has provided information on the difficulty levels of tasks from a whole range of topics in the Cockcroft foundation list.
In addition, there is a wealth of detail relating to those features of tasks which are associated with the steep rise in their difficulty in one or more of the attainment bands within the target sample.
Thus, using decimal numbers causes a steep drop in success rates in all three attainment bands relative to questions with whole numbers only.
By contrast, answering some questions about intervals between dates is much more difficult for the bottom third when no calendar is provided than when it is; the top third are about as successful whether or not a calendar is provided.
Not all difficulty factors have been identified and there can be factors unique to a task which influence performance.
In some tasks several interacting factors may be influencing performance; in these circumstances success rates become more difficult to predict.
This has implications for the description of performance, and will be taken up in more detail in the next chapter of the report.
In measuring tasks the difficulty factors have been found to relate to a fragile understanding of basic concepts such as fractions and decimals, and number scales.
Other difficult concepts include area and angle.
Lack of understanding of decimal place value, combined with limited knowledge of the sizes of measuring units in both Imperial and metric systems contribute to pupils' problems in reading and using measuring instruments with a scale.
Many pupils are bemused by the dual system of measures.
Although nearly all have used the metric system throughout their school careers they use Imperial measures in everyday situations.
Results from the APU have demonstrated that decimal place value is a problem for pupils who are average or above average attainers.
That it is a very severe one among the lower attainers was confirmed in this project.
A large number of lower attainers treat decimals as whole numbers, a strategy that"works' when they are written to the same number of decimal places.
The proportion of pupils who treated decimals as whole numbers appeared to be greater when the numbers were in context than when pure numbers were used.
The interpretation of 2-D drawings of 3-D objects and drawing and measuring angles of stated sizes were carried out quite well.
It seems likely that practical work in subjects such as CDT has helped to develop these skills for some pupils.
Although skills relating to angles were carried out quite well this does not mean that the idea of angle was well understood.
In general, pupils' interpretation of spatial concepts such as area, angle and shape are highly influenced by context and presentation; they often provide cues to pupils about the way to interpret a task, and can be misleading.
If these cues correlate with the correct procedure (e.g. finding the area of a rectangle drawn on a grid of squares), then a correct answer may result, but the problems of understanding are hidden.
Discussion with pupils on their methods of carrying out a number of the test questions showed that most of those interviewed had developed mental methods for the simplest kind of questions and could, on occasions, make good use of the knowledge they possessed.
For example, some dart-playing pupils found how many stamps they could buy for £1 by using the fact that treble 17 is $1.
This question also revealed several examples of pupils rounding the number 17 to 20 to obtain an approximate answer, although generally the process of rounding is not appreciated by pupils.
Checking or correcting is another procedure not usually engaged in unless directed to, but discussion of methods often produced a correction to the original wrong answer.
Other anecdotal evidence from pupils suggests that whether and how they use relevant knowledge might be dependent on their familiarity with the operations involved; for example, several instances were observed where pupils did not use their knowledge of available lengths to estimate an unknown length.
They are unused to the idea of estimating measurements.
Pupils' general knowledge about sizes of units and the relationships among them which could assist them to get a "feel" for numbers and help them decide whether measurements were reasonable, was lacking.
Spontaneous examples of positive performance such as using specific knowledge, rounding, correcting and so on are generally recorded during oral interaction with pupils and are not revealed by written tests which ask for specific answers to specific questions.
It was found, however, that a number of pupils, mostly those towards the top attaining end of those tested, could write good descriptions of their methods and explanations of number patterns.
Lower attainers are known to have difficulties with reading and may be deterred by printed tests.
Some of the pupils have problems with recognizing and pronouncing words.
In relation to shape, for example, there is a lot of vocabulary to learn, some of the words being long and unfamiliar looking to those pupils who have problems with reading.
Some of the difficulty with words is probably due to pupils' problems with the concepts involved and to questions about them being out of context.
However, given simply worded questions on computation, and time to complete their working, most of the lowest attaining pupils had no more difficulty with written than with orally  presented questions.
Oral presentation places an additional load on the memory in mental tasks which was found to be only partly ameliorated by supplying the numbers or other information required to answer the questions.
These results have a number of possible implications for a graduated test scheme which will be taken up in more detail in later chapters.
In particular the issues relate to:
(i) Skills and concepts
Pupils perform better on questions relating to skills than on those testing the understanding of concepts.
This implies that questions relating to skills and understanding of the same concept might be placed at different levels of a scheme.
(ii) The performance of boys and girls
Many of the questions are of different difficulty for boys and girls.
Questions relating to counting and calculating with whole numbers are generally tackled by girls as well as or better than by boys.
Those relating to measurement are usually done much better by boys.
How well each gender group fares in a scheme will therefore depend on the balance of questions at each level.
(iii) Oral and written test modes
Simply written test items do not necessarily disadvantage the lowest attaining pupils, while some orally administered questions do.
Thus, although these pupils generally have difficulty with reading, this does not mean that written tests should be ruled out.
Criterion-referenced Assessment and Graduated Tests
Summary
The chapter begins by contrasting norm- and Criterion-referenced assessment, the former placing candidates relative to other candidates and the latter placing them relative to a description of performance — a criterion statement.
The derivation of criteria and issues related to describing performance are outlined.
Examples of criteria which might be used in a graduated test scheme are then given and are discussed in relation to test items.
The validity of the test items as exemplifications of the criteria is considered and also the implications of the test results for the teaching and learning situation.
In the final section some problems concerning decisions about passing and failing candidates taking criterion-referenced tests are discussed.
Norm-referenced and criterion-referenced assessment
Norm-referenced assessment
A "norm-referenced" test is concerned with placing individuals in rank order of attainment.
One person's score can then be compared with those of the other individuals (the norm-group).
An underlying assumption of this form of testing is that performance mirrors amount  of "ability" or "attainment" and that this quantity is normally distributed within the population, like height.
Such testing is only indirectly related to providing a measure of particular competencies or curricular objectives and not at all to providing a description of them.
The marking procedure is one main reason for this.
Pupils gaining the same overall mark may have done so by obtaining a different pattern of marks for individual questions.
Thus the meaning of a mark is unclear, particularly when items in the test cover a range of topics, and a choice of questions is allowed.
Another reason for norm-referenced tests not providing a description of what a pupil knows and can do is the lack of clear specification of learning objectives, at least as far as the formulation of pupil performance on the test is concerned.
The test items themselves may indeed have been specified in terms of their subject matter but a norm-referenced test result is given as a measure of performance rather than a description of what examinees know.
Norm-referenced tests or examinations are particularly suitable for allocating pupils to particular classes or to assign grade levels, when there is not usually a need to describe knowledge and skills with any precision.
The test items need to discriminate between individuals.
Items which discriminate best are ones which are passed by more of the higher scorers than the lower scorers; the best discriminators of all are generally those items which are passed by about $0 per cent of the candidates overall.
The Cockcroft Committee pointed out that, for pupils with very low scores in GCE and CSE examinations, much could be said about what they don't know, but little about what they do know.
Should such a test be used in the course of teaching, low scores would not enable the teacher to decide what tasks can reasonably be given to the pupils who obtain them nor to decide what experiences may be suitable to forward their development.
A norm-referenced test can provide little information to assist in the teaching and learning of low attainers.
Criterion-referenced assessment
The recent interest in"criterion-referencing is linked with the wish to provide more information about what pupils have learned than traditional examinations do.
This information may be used by teachers to plan their lessons or to diagnose the difficulties of individual pupils.
Some information on pupils' specific knowledge and skills is also likely to be of interest to parents and employers.
Thus, criterion-referenced assessment may have both formative and summative functions.
The term criterion-referencing is a contentious one.
There are many references on the topic in the educational measurement literature, and some 57 different definitions have been identified.
The term has even been used to refer to any form of assessment that is not norm-referenced.
In criterion-referenced assessment particular attention is paid to the setting of intended learning outcomes or teaching objectives.
Knowledge and skills to be acquired are specified in criterion statements for example, "can add whole numbers" .
If sufficiently precise, as in "can add whole numbers summing to between 100 and 1,000" the criterion statement may specify the domain of test items from which a representative sample can be selected to assess pupils' performance in relation to the criterion.
In certain instances a pass/fail cut-off score is specified which is often referred to as a criterion.
Thus the term "criterion" can refer either to a description of the knowledge and skills possessed by the learner or the score which has to be reached in order to qualify for the description.
A high cut-off score is necessary since achieving it is taken to indicate mastery of a particular domain, or readiness to proceed to the next level of learning.
But quite how high the criterion score needs to be in order to indicate mastery of a domain or readiness for further learning is not easily established- A very high cut-off score may fail pupils who have learned the material but have made slips.
The further the cut-off point is reduced below 100 per cent, however, the more difficult it becomes to specify what has been learned.
For a summary of the terminology used above in relation to a specific example, see Table 5.1.
Criteria: some issues related to describing performance
Deriving criteria
Criteria for criterion-referenced assessment may be derived on the basis of an analysis of the subject matter in terms of the skills, concepts and strategies required to meet a criterion.
 "Subject matter" may be viewed broadly in terms of both content and process.
The analysis needs to be related to what is known about children's learning and the curriculum experienced by the pupils.
Assessment tasks are Constructed using the criterion descriptions as a guide, and pupils' mastery of the domain of a criterion judged from their performance on the tasks.
A set of criteria might be given as a list, but it is more likely that at least some of the relationships between individual criteria will be indicated.
These relationships may be fairly loose groupings into categories or a more complete hierarchical structure.
Given that the Cockcroft foundation list can be turned into operational criteria, it would be an example of a partially ordered structure in terms of topic area.
Several examples of assessment frameworks or structures used in a number of research and development projects were given earlier.
For example, the APU's assessment framework consists of the following related dimensions: content; learning outcome; context; mode of assessment.
In OCEA's model "problem solving" is subdivided into nine processes; "cluster headings" are finer distinctions and these are subdivided into criteria.
Hierarchical structures can be produced by subdividing into a variety of different elements.
Examples are given in Figure 5.2.
Clearly there are a number of levels of analysis and it is a matter of choice and convenience how fine grained the subdivisions become.
The advantages and disadvantages of very finely grained criteria are discussed in the next section.
There are also different ways in which the subdivisions can be made and each analysis will result in a different set of criteria, although there may well be considerable overlap between some of the sets.
One kind of analysis relevant to some topics is to describe the procedures which feature in representative tasks and the relevant concepts associated with them.
For other topics, such as map reading, a classification of typical tasks is a better starting point for analysis since their subject matter is defined by practical rather than mathematical needs.
Where generalization is concerned processes common to generalizing in a variety of contexts are identified rather than a list of topics.
At each level of analysis a set of tasks could be devised of different orders of complexity.
Thus "measurement" could relate to length, area, volume or other kinds of measure.
The measurement of length might relate to a straight line drawn on a piece of paper, to the height of a tree or to the distance of the moon from the Earth.
Tasks might be defined in terms of the processes they involve: calculating, ordering, representing and sorting etc.
Their dependency on the understanding of particular concepts could be indicated.
How is complexity to be defined?
Some tasks may be logically more complex than others in the sense that they subsume simpler tasks.
Thus in order to construct a triangle of stated dimensions, it is first necessary to be able to draw a line of a particular length.
By contrast, the relative complexity of tasks involving the same mathematics set in different contexts or tasks involving a number of different mathematical ideas is not so easily established.
Validating test items
Criterion statements may be derived from the subject matter as a prelude to writing test items.
However, where, as in the case of arithmetic, subject matter is defined by characteristic tasks which may be used to assess its attainment, criteria may be written subsequently to test items, as a description of typical items.
Alternatively a mixture of these methods may be used, possible alternative stages of test creation are represented in diagrams below (see Figure 5.4).
Validation procedures are indicated on these diagrams, since the validity of the items used to test attainment of a criterion is an  important issue.
Criterion statements which are ambiguously worded or imply broad domains may suggest a range of test items of varying difficulties.
In such cases an agreement to limit the range or to reword the criteria may be essential.
Alternatively, modified test items or different test procedures may be necessary.
Describing performance
Criterion descriptions may be regarded as setting curriculum objectives, as describing the performance of those attaining the objectives, and as specifications for test items or assessment situations.
The issues discussed in this section relate to how reliably and validly these functions can be fulfilled.
A criterion description should enable interested groups (pupils, parents, teachers, employers) to know what skills are encompassed by the particular criterion.
There are problems about the differing needs of the various interested groups: at what level of detail should criteria be written for different audiences?
There are two factors particularly which may make the aim of clarity of description difficult to achieve: the breadth of a criterion and ambiguity in its description.
Criteria may range in breadth from very narrow ( "can add two single-digit numbers when no regrouping is required" ) to very broad ( "can calculate using any of the four rules of number" ); the narrower a criterion statement the less equivocal its reference and so the more precisely assessment tasks based on it will be specified.
If the domain of a criterion statement is broad the representativeness of assessment tasks is more difficult to arrange and criterion performance may thereby be ambivalent.
But how precisely can test items be specified by a criterion statement anyway?
It is not difficult to show that even an apparently high level of detail in a criterion does not specify the test items unequivocally.
The criterion "can add two two-digit whole numbers without regrouping" will illustrate this point.
The following items are exemplifications of the criterion (Example 74).
(The items might be set on a particular occasion as a test of the attainment of the criterion or they might arise in the context of more holistic tasks on separate occasions.)
Some of these ways of interpreting the criterion may be harder than others.
Indeed, the APU results have shown that for a national sample of pupils (c) is harder than (a), which is harder than (b).
The more fine-grained the criteria descriptions, the more criteria there will be.
Proliferation of criteria brings problems relating to recording and reporting.
Teachers have to note and keep records on individual pupils' state of knowledge with respect to all the criteria in a scheme.
This can be difficult to organize and may not be of great use to the teacher either in planning lessons or in providing feedback for diagnostic purposes.
Parents and employers are likely to want a fairly general level of information without too much detail.
Schemes being piloted or in operation have used a varying number of criteria, some mainly content, some mainly process.
It is not yet clear how many criteria teachers and others can be expected to manage, or find useful, but the limited amount of research and observation which has been conducted in this area suggests that teachers do not always work at a high level of detailed information.
There are occasions when teachers would want detailed feedback for diagnostic purposes or to evaluate their teaching, but an assessment scheme which is tightly based on a large number of criteria would compel teachers to work to them even when it might not be appropriate to do so.
It may be that those aspects of a curriculum, such as skills which are readily expressible in behavioural terms could usefully be laid down in some detail while other aspects, such as problem solving would be more loosely specified or, perhaps, not specified at all.
However, even in the case of concepts and skills there is a danger that a curriculum based entirely on large numbers of criteria would become too rigid.
A detailed set of criteria may be seen to imply a specific sequencing of curriculum objectives where there are several pathways to understanding.
This is likely to apply particularly to concepts, which can be understood on a number of levels, while skills are generally easier to specify as they are procedures carried out in practice and are therefore more "visible" , whereas pupils' understanding of concepts and strategies can only be inferred from behaviour.
Examples of criteria
The following sections contain examples of criteria of the kind that could be used in a criterion-referenced assessment scheme.
The criteria illustrated were derived from an analysis of several topics selected from the Cockcroft foundation list.
Fairly detailed criterion statements are considered in order to relate them to the test questions used, which themselves are derived from the foundation list topics.
As described in Chapter 4 (pp. 41–102), trials of appropriate test items were carried out in order to ascertain how difficult pupils found the items.
Illustrative items were included in that chapter, together with a description of the modes of assessment into which suites of related items are incorporated.
Foundation list topics for which criteria appear in this chapter were selected from four of the five content categories and one of the process categories of the Graduated Test Project assessment framework given in Chapter 2 (pp. 9–26).
The  selection of topics included examples where performance is well known (Number, Measures, Geometry) and those for which the details of performance are less well documented (Data interpretation and Problem solving strategies).
It was thought that many of the problems of criterion-referencing would be highlighted by the particular topics selected.
Algebra was not tested in this project as it is not explicitly included in the foundation list, and so is the only content category of the framework missing from the topics considered in this chapter listed below.
(i) Content categories Number criteria for understanding place value, whole numbers and decimals.
Measures criteria for a theory applying generally to measures, exemplified by detailed criteria for length.
Geometry- broad criteria are discussed and ways of constructing more detailed criteria are considered.
Data interpretation- criteria are proposed for tabular data.
(ii) Problem solving strategies Problem solving and investigative mathematics — criteria for generalizing.
In each of the examples given in the following sections there is (i) an analysis of the main procedural and conceptual aspects of the category,(ii) detailed sets of criteria for selected subtopics, and (iii) examples of test items based on the criterion statements.
Success rates are provided for the whole sample taking the test items, not for the three attainment bands defined in Chapter 3 (pp. 27–40).
There are two reasons for this:(i) the analyses for the technical feasibility of graduated test levels in Chapter 6 (pp. 153–87) are based on these success rates;(ii) for economy of presentation as difficulty factors can easily be identified using these figures.
Some examples of criteria
Number
One way of illustrating the main procedural and conceptual aspects of number is shown in Figure 5.5.
This is followed by a listing of more detailed criteria relating to one aspect: place value.
As an example one section of the diagram above, that of number concepts, has been selected.
There are several reasons for this selection.
First, number concepts are fundamental to success in work with number and thus merit consideration.
Secondly, the APU data suggest that this may well be a neglected area in the later years of secondary schooling both from an assessment viewpoint and in curriculum terms.
With regard to low attainers in particular, the teachers' comments on the familiarity of pupils with the items in the GT4 reference test (see Chapter 3, pp.27–40) lend support to this view.
The process used to derive criteria commenced with a general view of the learning objectives to be focused upon.
Three related areas are seen as central: understanding of place value; ability to order numbers; and ability to represent numbers.
A further division may be made between whole numbers and decimal numbers.
From this starting point a series of criteria for whole numbers and a parallel set of criteria for decimal numbers have been developed.
Criteria for number concepts: positive whole numbers
The pupil can:
1a identify the place value of a column or a digit in it for values of units, tens, hundreds and thousands; 1b represent in numerals a whole number given in words; 1c represent in words a whole number given in numerals; 1d order whole numbers; 1e provide a whole number which is between two given numbers in size; 1f represent a given whole number on a number line or read one from a number line or scale; 1g demonstrate understanding of relationships of the form 13 x 8 (10 x 8) + (3 x 8).
At this stage, there is little reference to the likely difficulty of the criterion or to factors which are associated with variation in difficulty.
The criteria refer to learning objectives for this topic and a separation is made where the task requirements differ.
The criteria above appear to imply fairly well-defined tasks, but their attainment can be checked in different ways; for example by direct questions or by implication during work on a task which involves the criterion as a subtask.
The following examples of test items are based on the above criteria.
A comparison of the success rates of related items in written tests provides a guide to difficulty factors.
Criterion 1a
The pupil can identify the place value of a column or a digit in it for values of units, tens, hundreds and thousands (Examples 75,76 and 77).
As can be seen from the sample tasks above, identifying the place value of a column was generally rather easier than giving the place value of a particular digit.
Placing the mathematics in a context can influence its difficulty, sometimes making it easier, sometimes more difficult.
The provision of a context in this case (ballot papers) reduced the success rates.
Was this because of the wording of the items or the unfamiliarity of the task?
Such questions would need to be studied and evaluated at a local level.
Criterion 1b
The pupil can represent a whole number given in words or in numerals (Example 78).
Two difficulty factors are apparent in the pattern of results, the number of digits involved and the need to insert spacing zeros in the answer.
For "one million" per cent of pupils gave the answer 10,000 and a further 13 per cent gave 100,000.
Criterion 1d
The pupil can order whole numbers (Example 79).
A further four per cent of pupils ordered the numbers correctly but put the smallest first.
Criterion 1e
The pupil can provide a whole number which is between two given numbers in size (Example 80).
As for 1d, this provides an example of a task most pupils successfully completed.
The most popular correct answer was 22, given by 73 per cent of pupils.
Criterion 1f
The pupil can represent a given whole number on a number line or read one from a number line or scale (Example 81).
A further 14 per cent of pupils gave answers suggesting that they thought that the left hand end of the line represented either 0 or I. Whether this is due to a conceptual difficulty or a casual approach was not established.
Criterion 1g
The pupil can understand relationships of the form 13 x 8 (10 x 8) + (3 x 8)(Example 82).
Questions looking at this sort of relationship, explicitly mentioned in the foundation list, produced low success rates.
The place value identification of criterion 1a is required in addition to the distributive law and this combination proved very demanding for low attainers.
Criteria for number concepts: positive decimal numbers
2a The pupil can identify the place value of a column or a digit in it for values of tenths, hundredths and thousandths; 2b the pupil can represent in numerals a decimal number given in words; 2c the pupil can represent in words a decimal number given in numerals; 2d the pupil can order decimal numbers; 2e the pupil can provide a decimal number which is between two given numbers in size; 2f the pupil can represent a given decimal number on a number line or read one from a number line or scale; 2g the pupil can understand relationships of the form 1.3 x 8 (I x 8) + (0.3 x 8); 2h the pupil can represent a fraction in tenths or hundredths as a decimal; 2i the pupil can represent a decimal with not more than two decimal places as a fraction.
Criterion 2a
The pupil can identify the place value of a column or a digit in it for values of tenths, hundredths and thousandths (Examples 84,85 and 86).
In contrast to the similar criterion 1a for whole numbers, there was no difference in success rates between identifying the place value of a column and that of a digit.
Tasks representative of criterion 2a would be expected to be more demanding than those for 1a and this was indeed the case.
The difference in success rates was marked, no success rate for decimals exceeding 15 per cent, and those for whole numbers ranging from 50 per cent to over 80 per cent .
Criterion 2d
The pupil can order decimal numbers (see Example 87).
The sample tasks all include a measurement context in which decimals have to be ordered.
The success rates are uniform, all around five per cent.
The only apparent difference concerns the rates for the "largest is smallest" error.
The difference in success rates between criterion Id, with whole numbers, and criterion 2d, with decimals, is extremely marked, being over 80 per cent.
The validity of such questions as tests of the attainment of the criterion is an important point.
In each of the above questions the pupil must order decimals correctly to obtain the right answer; the decimals are of differing lengths and care has been taken to ensure that both of the two common errors in decimals produce incorrect orderings.
Had the numbers been written to the same number of decimal places all those ignoring decimal Points would have got the correct answers, producing artificially high success rates.
However, the use of decimals written to varying numbers of places is not strictly valid from a measurement point of view, so there is a conflict between two types of validity.
Criterion 2e
The pupil can provide a decimal number which is between two given numbers in size (Example 88).
The validity of questions on this criterion again needs to be considered.
The wording of the question is not the factor leading to difficulty the same wording used with whole numbers for criterion 1 e produced a success rate of over 80 per cent.
Care is, however, needed to ensure that decimals have to be dealt with: a version of the same task with 4 m 10 cm and 4 m 20 cm (success rate 68 per cent) is unlikely to test pupils' understanding of decimals since many will deal just with the centimetres a whole number task.
The first two questions require an answer with two decimal places although there is only one in the question.
The third question requires only one decimal place to be given and even this could be avoided by giving the answer 832.
This answer, given by 41 per cent of the pupils was marked correct.
It is difficult to exclude vulgar fractions as correct  answers to such questions by rewording them, and it may be worth considering modifying the criterion to specify "between two decimal numbers in size" .
Criterion 2f
The pupil can represent a given decimal number on a number line or read one from a number line or scale (Example 89).
The success rates for two tasks differing only in the lengths of the rod shown is again marked.
While the second question produces a success rate of less than 20 per cent, typical for most questions shown so far, the first question does not.
Again, validity needs to be considered, and it seems likely that the length of 2.8 cm can be obtained by using a whole number strategy which avoids the use of decimal concepts.
Answers of the form 2.3a where a = 1, 2…9 were accepted as correct.
A high proportion of pupils gave an answer which showed that they could represent the number to one decimal place: 31 per cent gave the answer 2.3, seven per cent gave 2.4 and 22 per cent gave 2.3.
It appears that a reworded criterion: "can represent a given decimal number on a number line to one decimal place" would have yielded a success rate of 75 per cent, about the same as that for the first item.
However, once again, the validity of this item as a "true" measure of the criterion must be in doubt.
Criteria 2h and 2i
The pupil can represent a fraction in tenths or hundredths as a decimal.
The pupil can represent a decimal with not more than two decimal places as a fraction (Example 90).
Again, success rates for a basic task concerning the meaning of decimals, in this case the place value of a digit in a particular column to the right of the decimal point, were low.
Indeed, even for the most familiar fraction, "J" , only 26 per cent of pupils were able to express this as 0.5 while 31 per cent of pupils were able to write 0.5 as ½ (or ).
Many of the incorrect responses given suggest totally algorithmic methods of conversion with no thought given to meaning.
Responses of the type 0.4 = ¼ were given by around 13 per cent of pupils to most of the questions where a decimal was presented, but 2.3 was given as ⅔ by 45 per cent of pupils.
Around 10 per cent of pupils gave responses of the form 0.4 = .
When given a fraction, responses such as  = 8.1 were given by around 25 per cent of pupils for each question.
Measures
A way of illustrating the main procedural and conceptual aspects of measures was illustrated earlier in this chapter (pi 07).
More detailed general criteria for measures are listed below.
More specific criteria for length are then given.
(a) General criteria for measure
1 Basics of measurement.
The pupil understands the following: 1a measurement is repeated covering by a unit; 1b different sized units will give different numerical values whet, the same measurement is made; 1 c for the same measurement a larger unit will give a smaller numerical value than will a smaller unit; 1d the transitivity of the relationships "larger than" and "smaller than" .
2 Units of measurement
The pupil can: 2a associate appropriate units with the quantity to be measured; 2b relate units for the same measure to each other; 2c relate a given unit to an everyday object with a sensible numerical value.
3 Estimation
3a The pupil can make an estimate of a measurement to an appropriate degree of accuracy: i) in a given measure; ii) after selecting an appropriate measure.
4 Use of measuring instruments
The pupil can: 4a choose an appropriate instrument; 4b make a measurement within the usual range of the instrument to an appropriate level of accuracy; 4c make a measurement outside the usual range of the instrument to an appropriate level of accuracy.
5 Comparing/Ordering
The pupil can: 5a compare or order objects when physically juxtaposed; 5b compare or order objects or quantities when measurements are given; 5c compare or order objects when measurements must be made.
6 Computation of a measure
The pupil can: 6a compute to find a measure e.g. area or volume or capacity).
7 Computation with a measure
The pupil can: 7a compute using a measure, no equivalence of units required; 7b compute using a measure, equivalence of units required.
(b) Criteria for length
The general criteria for measures, outlined above, can be applied to a particular measure, that of length.
Most of the general criteria given may be easily translated to criteria for length with the exception of 6a and 6b.
Criterion 4c, for example, is exemplified by measurement tasks in which the length to be measured exceeds that of the longest ruler, rule or tape available to pupils.
As for the detailed number criteria there is little reference at this stage to likely success rates or to the difficulty factors which are associated with variation in success rates.
However, in some instances an order of difficulty is suggested, for example, because one criterion includes an additional step in the working as compared with another.
Thus 5b refers to ordering or comparing with given measurements, while in 5c measurements have to be made first.
It is likely, therefore, that there will be lower success rates for 5c than for 5b, but no such assumption is made automatically; empirical confirmation is required.
Similarly there is no assumption that repeated use of a ruler in 4c is more demanding than single use of it in 4b, but this and similar instances need to be examined empirically to see if the task differences concerned are also difficulty factors.
The general criteria above suggest a range of assessment tasks in both practical and written test modes.
In deriving more detailed criteria capable of being used, for example, to generate the required assessments, it is necessary to make use of what is known from previous research about pupils' learning and attainment in the topic concerned.
In particular, if a graduated test scheme with levels of difficulty is envisaged, known difficulty factors need to be taken into account and the effect or otherwise of putative difficulty factors investigated.Following such a process the following more detailed set of criteria could be derived.
For each criterion sample tasks are illustrated together with success rates for the lowest 40 per cent of attainers; difficulty factors and common errors are also pointed out.
Sample criteria for length
Success rates are given for tasks taken as representative of the criterion concerned.
Criterion 1b
The pupil understands that different units of length will produce different numerical answers for the same length.
Example 91 uses non-standard units to avoid a relationship between different measures being known or guessed.
The success rate for the item shown is typical of those obtained and the most popular response: No, No, Yes; given by 39 per cent of pupils suggests a view that 30 is greater than 11 so Robert is taller than Dean.
A further eight per cent of pupils also appeared to make an assumption about the relationships between cubes and hand spans and concluded that Robert was not taller than Dean.
Criterion 1 c
The pupil understands that a larger unit of measurement will produce a smaller numerical answer than a smaller unit of measurement applied to the same length (Examples 92,93).
In both tasks the most common error, given by around 20 per cent of pupils, suggests that a larger unit gives a larger numerical answer than a smaller unit.
These are tasks which pupils could investigate practically; the second task at least seems likely to yield a higher success rate in a practical mode.
Criterion 1 d
The pupil understands the transitive properties of "longer than" and "shorter than" (Example 94).
According to some researchers (e.g. Bryant and Trabasso, 1971) children understand simple transitive relations by the age of four.
Questions of the type illustrated above are applications of the simple transitive relations "longer than" and "shorter than" and they make relatively high linguistic and logical demands.
The relatively straightforward use of the transitive property in the first question produced a much higher success rate than did the second series of questions.
Both of the common incorrect answers to the "Yes/No/Can't tell" items involve making assumptions about the relationships involved which are unwarranted.
This sort of response was also noted in the sample task for criterion 1a.
Making a diagram to represent the given relationships could be helpful in understanding questions of this type.
A member of the research team interviewed a number of pupils in two classes who had tackled the questions.
None of these pupils had used a diagram to assist them to obtain their answers, although a few diagrams were found among the descriptions of methods sent in by teachers in one liaison group.
Units of measurement: length
Criterion 2a
The pupil can associate appropriate units with the quantity to be measured.
The word "appropriate" in this criterion statement is one of the vague words frequently utilized in criterion statements.
In the questions below the term used is"suitable units'.
In what sense are units "appropriate" or "suitable" ?
The ground rules might be"avoid nasty numbers' i.e. very large numbers or decimal numbers.
However, a different but related interpretation could be: what units are normally associated with measurements of a particular size?
In the building trade millimetres are often used; this avoids the use of decimals but not large numbers.
A teacher in a liaison group pointed out that Imperial units are often more usable than metric ones which are either too small or too large for many common measures.
The objects given should cover the range of units of length from millimetres to kilometres.
It is often necessary to confine answers to either metric (Example 95) or Imperial (Example 96) units as appropriate.
Earlier trials of questions similar to those below suggested that Imperial units would be frequently given if not specifically excluded in the rubric.
For the tasks illustrated the pupil has been directed to either Imperial or metric units for the set of questions concerned.
The higher success rates obtained for Imperial units suggest that out of school experience may well be an important factor in making the associations required.
An example of this was given by a liaison group teacher who said that he always asks his classes for their height and weight: he had yet to come across one pupil who used the metric system.
Criterion 2b
The pupil can relate units of length to each other.
There are three possibilities: relationships between metric units; relationships between Imperial units; Metric Imperial relationships.
For criteria about knowledge of the relationships between metric units the difficulty factors concern the nature of the required answer — whether it is a whole number, a half or a decimal such as -0 or 00.
The highest success rates were obtained from questions dealing with the smaller metric units, those with answers of 10 or 100.
The units concerned, centimetres and millimetres, are those with which the greatest level of practical experience might be expected.
The data obtained from these questions and similar ones on other measures suggest that relationships of the form 1x = 1000y are less well known than 1x 100y or 1x = 10y.
Both greater practical experience of the smaller units and greater facility with smaller, rather than larger, numbers are possible explanations.
As might be expected, relationships involving decimals are much more demanding unless the answer is 0.5.
In this case the familiar fraction was given by nearly 90 per cent of pupils who gave a correct answer.
For relationships between Imperial units (Example 98) success rates were low, 30 per cent or less, rather lower than for metric units.
This is in contrast with questions on criterion 2a: association of appropriate units with a length to be measured.
Concepts concerning relationships between units are probably acquired in school, and most pupils appear not to deal with Imperial units to any great extent in the mathematics classroom.
Knowledge of approximate Metric-Imperial relationships (Example 99) which is included in the Cockcroft foundation list, was also scanty.
Criterion 2c
The pupil can relate a given unit to an everyday object with a sensible numerical value.
There are two basic possibilities for questions, multiple choice or supply type.
The multiple choice format can, in this case, be thought  of as demanding a similar sort of thinking to that required when checking an answer obtained by measurement or calculation to see if it is sensible.
 "Sensible" is another of the vague words favoured by writers of criteria.
What does it mean in this instance?
It has something to do with the extent of a pupil's knowledge; for example, anyone giving 50 feet as the height of a door cannot know very much about the size of a foot.
Alternatively a pupil might know that the height of most adults is between five and six feet but fail to apply this knowledge to estimates of other lengths or heights.
In one class a research team member asked for estimates of his height (5′ 7½″); the estimates ranged from 4′ 3″ to 6′ 5″.
Pupils giving estimates at the end of this range might well be considered sensible to give the height of a door as 5 feet and 10 feet respectively.
In general for specific cases it seems better to replace the word "sensible" with stated limits (Example 100).
Success with such questions requires not only knowledge of the measures concerned but also general knowledge.
The tasks shown above presented in multiple choice format were also tried out in supply format, with success rates of 67 per cent, 24 per cent and $3 per cent respectively.
The more difficult questions, such as the height of a door in metres, tended to be answered in a broadly similar way across the attainment range of pupils tested in the project.
Success rates were not markedly different for higher attainers and lower attainers.
This lack of discrimination suggests that the general knowledge required is not highly related to attainment in mathematics.
Use of measuring instruments
Criteria 4b and 4c
The pupil can make a measurement within the usual range (4b) or outside the usual range (4c) of the instrument to an appropriate level of accuracy.
The tasks below require the pupil to measure lines with a 30cm ruler.
Some lines are over 30cm long.
Tasks where the ruler has to be used twice (Example 102) are, as might be expected, more demanding than those where a single use of the ruler will suffice (Example 101).
The second difficulty factor concerns the nature of the answer — whether it is a whole number, involves a half or is a more awkward decimal.
The accuracy required is important in determining difficulty, the crucial factor being whether or not the acceptable range allows a whole number answer, one with a half or contains only more awkward decimals.
Comparing/Ordering
Criterion 5b The pupil can compare or order objects when the lengths are given.
The task difficulty will depend on whether or not knowledge of equivalence of units is required (Example 103).
The example task is one where both knowledge of the relationship between millimetres and centimetres and that between centimetres and metres is required.
A wide range of responses to this question was noted, and most of these involved combinations of two errors, i.e. not knowing the number of centimetres in a metre, and holding the view that "any number of millimetres is smaller than any number of centimetres" .
A criterion involving only millimetres and centimetres would be a little easier to achieve: 18 per cent obtained the correct order with just the one metre misplaced.
Geometry
Spatial and measurement concepts and skills
 "Geometry" itself does not encompass such a clearly unitary idea as "number" or"measures' and it is consequently more difficult to represent in a linked set of the main procedural aspects.
Some possible criteria for this area of work are listed below: The pupil can: I name, draw or sort 2-D shapes; 2 name, draw or sort 2-D representations of 3-D shapes; 3 visualize 3-D shapes represented in 2-D; 4 construct accurately 2-D shapes with a ruler, protractor and compasses; 5 find the area and perimeter of a rectangular shape (whole units only); 6 find the volume of a 3-D cuboid given a 2-D representation; 7 compare and measure angles.
Trial tests for geometry comprised typical items selected to test spatial concepts as referred to in the Cockcroft foundation list.
Detailed criteria are discussed under more general headings.
1.
The pupil can name, draw or sort 2-D shapes.
This criterion concerns pupils' knowledge of the properties of 2-D shapes and their knowledge of the names of the shapes.
Knowledge of the properties of shapes means that any exemplars can be recognized as such and distinguished from non-exemplars However, some pupils recognize some exemplars of a particular shape as such and not others.
For example, in the question below (Example 104), over one-third of the sample did not accept the obtuse angled triangle as a triangle.
Thus, to get a picture of pupils' understanding of what a triangle is, it is not sufficient to ask them to name one exemplar (especially if it is a  special case such as an equilateral triangle), nor to draw one exemplar, for they are likely to draw a special case.
Neither is it sufficient to obtain a correct definition (e.g."a triangle has three sides') as this does not guarantee that the pupils will not reject an obtuse angled triangle as a triangle.
It may be that at a low level of a graduated test scheme teachers may wish to acknowledge a pupil's recognition of only an equilateral or isosceles triangle as a triangle.
However, writing a criterion such as "can recognize an equilateral and isosceles triangle as a triangle" may encourage exposure of "standard" triangles only and thus limit pupils' chances of understanding why the obtuse-angled figure is a triangle.
Another issue associated with this criterion relates to the learning of names for shapes.
Some shape names are complex and, if pupils have a limited appreciation of the properties of a shape, it is not likely that their memory for its name will be enduring.
Thus, asking for a shape, given its name, is not a valid way of testing for knowledge of a shape; it is probably more important to build up pupils' understanding of similarities and differences between shapes rather than simply teaching their names.
There is a steep gradient of difficulty from knowledge of familiar shapes such as "square" to the more complex ones such as  "parallelogram" or "rhombus" .
2.
Can name, draw or sort 2-D representations of 3-D shapes Similar considerations apply to this criterion, and more detailed ones derived from it, as were applied above to 2-D shapes.
One additional feature of the responses to tasks exemplifying this criterion is that many pupils refer to 3-D shapes by the names of their faces, i.e. "a cube" may be referred to as "a square" , or a pyramid as a "triangle" .
This does not necessarily mean that pupils cannot distinguish the 2-D from the 3-D shape — they may be using a more familiar name for dual reference — but it probably does mean that they have not formalized for themselves the relationships among the various features of cubes and pyramids.
Some work is indicated, and concomitant criteria produced relating to classifying similarities and differences within and between 2-D and 3-D shapes.
3.
Can visualize 3-D shapes represented in 2-D.
Sample tasks include asking what 3-D shapes would be made if given 2-D nets were folded up and asking for an interpretation of a 2-D diagram of a model made from cubes.
In respect of the former there is a confusion between visualizing a shape and knowing its name.
For example, 41 per cent gave the correct name to the net of a cube but a further 40 per cent described it as a "square" .
Similarly only 13  per cent gave the correct name to the net of a cuboid while 70 per cent described it as a "rectangle" or "oblong" .
Not surprisingly, less familiar shapes are named incorrectly more frequently than more familiar ones.
This might be overcome by using a matching task of net to drawing of shape and rewording the criterion to"can match net with drawing or actual object.
4.
Can construct accurately 2-D shapes with a ruler, protractor and compass The test results, described in Chapter 4 (pp. 41–102), suggest that rectangular shapes can be drawn with good accuracy but they were not drawn with the use of geometrical instruments.
This is an inference from the lack of construction lines on pupils' test booklets, not from observation.
It is not clear whether some schools did not have the instruments available as requested or whether pupils decided not to use them.
In the light of this evidence it would be necessary to observe what pupils do in order to decide whether or not they have achieved this criterion.
5 Can find the area and perimeter of a rectangular shape (whole units only) The test results for items exemplifying this criterion showed that the presentation of the figures influenced what the pupils did.
They were more likely to find an area if a grid of squares was provided, and more likely to find a perimeter if there was no grid but distances round the edges of the figure were labelled; the use of the terms "perimeter" and "area" , especially the latter, reduced success.
Given these details the criterion as worded would seem to foster an over optimistic view of what many lower attainers understand when they "find the area and perimeter of a rectangular shape" .
In finding an area many pupils are probably doing no more than counting squares without appreciating the space-covering aspects of area.
This suggests that, in order to develop pupils' grasp of the concepts, the criterion needs to be broken down into several more basic criteria relating to space-covering aspects and terminology.
Some of these might be: recognizes that a number of squares can be rearranged and form several different shapes; knows that, in order to cover a given rectangle with different sizes of squares, the smaller squares will produce the larger numbers; knows that to find an area is to find the number of unit shapes which will cover a shape.
6.
Can find the volume of a 3-D cuboid given a 2-D representation Most of the success rates for items exemplifying this criterion were around 40 per cent.
A typical item is illustrated in Example 105.
The results indicate that pupils need more practical experience but the result of an APU practical test item suggests that lower attainers would currently do no better in a practical than in a written test situation.
In the APU test pupils were presented with a perspex box and a wooden cube and asked to estimate how many cubes would fill the box.
The actual number of cubes was 60, and 67 per cent(of a full attainment range sample) gave an estimate of between 36 and 90, which was considered acceptable.
For checking their estimate further cubes were provided but if pupils attempted to fill the box, they were prompted that not all the cubes were needed.
A total of 71 per cent of the full attainment range sample obtained the correct answer.
Although it is not possible to translate this directly into a success rate for the pupils in this project, an estimate of 40 per cent would be reasonable — about the same as that obtained for the written test items.
7.
Can compare and measure angles The results in Chapter 4 (pp 41–102) indicated that "compare" and "measure" have distinctive difficulty factors.
Breaking this criterion into separate ones: "can compare angles' and" can measure angles' has obvious dangers since the ideas of comparison and measurement are not separable.
Many of the lowest attainers have not distinguished the concepts of angle and space or length and so need tasks and concomitant criteria which attempt to clarify the distinction.
One set of tasks might, for example, compare types of movement of everyday objects; drawers, keys, doors, pages of a book etc.
This could be a context in which vocabulary could be learned, and the criteria for performance might read: can give examples of rotation, and translation in everyday situations can state appropriate measures for rotation and translation.
It is not supposed, however, that such ideas would inevitably bring understanding to all those pupils who currently lack it; there is always a potential for confusing concepts whatever situations are devised to clarify them.
For instance, a liaison group teacher reported an interesting case of lack of conservation of door width.
In order to make more real the diagram provided with the "design a bed sitting room" test, the teacher discussed the width of a door of the classroom in relation to the overall measurements.
One pupil was convinced that an open door was wider than a closed door.
It seemed that the pupil was focusing on the widening gap as the door was being opened, rather than on the width of the door itself.
Measuring angles incorporates the difficulty of reading a scale into the problem of understanding what an angle is.
Not surprisingly, therefore, angles are more easily measured successfully when the angle corresponds to a marked and labelled point of the protractor scale, and the criteria for success would need to take these difficulty factors into account.
Reading tabular data
Since little is known specifically about the difficulty factors of reading tabular data it was decided to study these in a range of tables.
Criterion statements were then developed from the results of the testing.
The tables were taken from newspapers, holiday brochures and advertisements.
The results of the testing showed that whilst most pupils can read information contained in a graph, chart or table, success rates are considerably reduced when pupils have to process the information.
From the test items used in this project, the approximate difficulties of tasks involving tabular data have been graded.
The grading is only approximate because of the variation in success rates resulting from such contextual factors as the arrangement of data in particular tables.
The following task demands were found to affect the success rate of a sample of low attaining pupils.
single or multiple entry table;— search requirements;(How easy is it to find the information in the table?
This depends partly on how "cluttered" the table is and also on pupils' familiarity with the subject of the data.)— interpretative requirements;(Any differences between the questions asked about the table and the labelling of the table's headings appear to reduce success rates.
For example, in a table showing the highest temperature in each month in a country, a question phrased "When is it hottest?" has to be interpreted as "Which month(s) has the highest temperature?" )— computational requirements;(Any requirement to process data spread from a table, no matter how easy, increases difficulty (e.g. doubling, subtraction)).
Table 5.2 represents the characteristics and demands of tabular data tasks of increasing complexity.
Although these task demands can be identified, they may interact or compound the difficulty in particular tables.
Moreover, there are common types of table which are quite difficult to read because of special features.
One example is the distance chart found in road atlases.
Because the distance between each pair of towns is generally entered only once, the table is a stepped triangle, not rectangular, and this produces difficulties in identifying the correct column and row for a reading.
Criteria for tabular data
The criteria listed below do not correspond to distinct levels of difficulty, although they represent increasing levels of complexity from 1 to 4.
There is considerable overlap in success rates, attributable to the demands of particular tables.
1.
Can read single entry tables with low task demands.
2.
Can read single or double entry tables with some task demand.
3.
Can read single or double entry tables and can make a simple comparison with the data obtained.
4.
Can read multiple entry tables and carry out basic comparisons or computations with the data obtained.
These are composite statements, each taking into account the four factors identified as relating to difficulty.
They can only operate as a rough guide to classifying tables of data, and it is not possible to say how they might relate to the levels in a particular graduated test scheme.
Further experimentation might lead to the identification of other difficulty factors which could be incorporated into criterion statements.
The difficulty factors can operate as foci for teaching: pupils' search strategies, their competence in interpretation and in calculation may separately or collectively need attention in order to improve performance.
Investigative mathematics
Generalization
The term "generalization" appears in lists of criteria in several assessment schemes for investigative mathematics which specify processes only, not content.
In such schemes generalization is regarded as a process applicable to different areas of content.
The Draft Grade Criteria for GCSE include, at all grade levels, criteria relating to generalization applicable to problem solving or investigational tasks set by teachers, examination board or formulated by pupils themselves.
Criteria relating to generalization at grade A are given as: pupils will be able to make and test conjectures and formulate general rules, making use of symbols where appropriate, and, pupils will be able to appreciate the difference between testing a generalisation by looking at special cases and proving it deductively.
Generalization is a term sometimes taken to include any anticipatory statement made by pupils to explain the results which they obtain from exploring a mathematical situation such as in the example given  below.
The formulation of inductive generalizations (using algebraic notation), or the application of rules to a new context, are more specific meanings attached to the term.
The exploration by pupils of number or geometrical patterns is sometimes used by teachers to develop and test pupils' ability to recognize and to use generalizations (Example 106).
Pupils may be asked directly to describe a rule for predicting how many matches would be needed for any number of boxes, or they may instead be asked how many would be needed for, say, 500 boxes and then asked about their methods as referred to below.
As a preliminary analysis, features of generalization particularly relevant to the exploration of number patterns, but applicable elsewhere, were identified as follows.
1.
Making generalizations
deriving number patterns
explaining number patterns
stating properties of numbers or patterns
invariant features under different transformations
2.
Using generalizations
using a property to predict behaviour
proof by appeal to a generalization
3.
Awareness of generalized statements
understands concepts of general or recurrent features which are established by rules and properties
recognizes that a generalization defines a domain
can adapt a generalization to a new domain.
4.
Affective and social
degree of conviction
willingness to make use of generalization
recognition that making a generalization is different from stating a rule by arbitrary choice, or through teacher authority.
In one test, different responses were noted of pupils who were asked to explain their methods of working out their answers to a question about a set of related patterns of tiles.
The task"work out how many shaded squares will go round a line of 50 unshaded squares', implicitly suggested the use of a generalization strategy for its completion.
The following samples of pupils' work, written to explain their answers to the task, indicate the generalization strategies they used to find out how many squares were needed.
Some (e.g. pupil 3) attempted to obtain the answer to the question from the pattern of numbers relating to unshaded squares but most pupils saw the relationship between shaded and unshaded in each diagram which enabled them to calculate the required number directly.
Several attempted to draw SO squares and count the unshaded squares.
This process enabled some to see a simpler method (e.g. pupil 1).
The response given by pupil 2 is couched in terms indicating that the method is general for any number of squares.
The pupil seems to be giving instructions for others to follow rather than saying what was done in this specific instance.
The language used seems significant: "Whatever number of unshaded squares you have to go round, limes the number by two and add six." 
 "There is always…" (our italics).
To back up the explanation there is a diagram too.
This could be regarded as a superior explanation to the others.
How could the others be differentiated?
The following criteria are adapted from Bell, Shiu and Horton.
The pupil can: 1 detect a generalization and express it informally; 2 describe a generalization, using particular examples; 3 describe a generalization in general terms; 4 state a generalization using algebra. 1–4 represent increasingly higher levels of response and may be too general for a graduated test scheme.
What is needed is criteria relating to each of the aspects of generalization listed earlier in this section for each of the levels of a scheme.
During the period of the project, because of the teachers' action, it was not possible systematically to collect data relating to generalization.
Whether it is possible (or appropriate) to work out a comprehensive set of criteria for processes such as generalization is an issue which will not be settled until there is a good deal more experience with process criteria in the classroom.
Mastery
The origins of mastery learning
Attaining a criterion means that the pupil has, in some sense, mastered the learning involved.
Modern ideas of mastery learning arose from Carroll's model of school learning.
Carroll listed five elements in learning: aptitude; ability to understand instruction; perseverance; opportunity given to learn; and quality of instruction.
By "aptitude" Carroll meant"the amount of time needed to learn a task under optimal instructional conditions' and not its more usual connotation of how much a student is capable of learning.
He postulated that given the right conditions and sufficient length of time, all (or nearly all — Carroll left out  the lowest live per cent) pupils can achieve mastery.
While this is an optimistic view the necessary conditions of teaching and length of time are not often available.
The idea of mastery learning involves a clear statement of objectives which pupils can reach in small learning steps — a legacy from the programmed instruction movement.
In the rather broader context of a graduated test scheme, evidence of mastery would be needed so that(i) a pupil's achievements could be informatively reported (e.g. for certification), and (ii) to indicate readiness for further learning to be undertaken at the next level of a scheme.
These are two rather different purposes, the first relating more to a summative statement which could be used to inform parents or employers of the pupils' knowledge and skills.
The second is more concerned with formative needs, that is to inform pupils and their teachers of what might be needed for the pupils to progress further in their learning.
The close relationship between criterion-referenced assessment and the curriculum has been referred to in the previous section.
The examples given suggest a flexible scheme whereby criterion statements might be amended in the light of pupils' difficulties and the identified difficulty factors might provide foci for teaching.
Assessing the achievement of a criterion
There are a number of ways of assessing specific criteria.
One would be to set a test with items exemplifying the criterion.
Another would be to note the skills and knowledge used when pupils were engaged in problem solving or investigative work.
When tests were used, mastery of the criterion would be achieved by setting a criterion score.
But how is the boundary between mastery and non-mastery to be established?
Boundary setting is used frequently in a variety of situations: the age at which "primary" becomes "secondary" education; the minimum height for entry into the police force; the marks to be obtained for a particular grade level in an examination and so on.
In some instances, the boundary depends on the context: young people gain the rights of adults at different ages; at 16 if they wish to smoke; at 18 if they wish to vote.
It is possible that the boundary between mastery and non-mastery might be set differently for different educational purposes, but it is much more likely that different purposes will prescribe whether a boundary needs to be set at all.
Thus if a test is given for diagnosing a difficulty it seems less relevant to decide on a boundary score than to obtain details of the nature of the difficulty.
By contrast, if a decision is to be made about whether a pupil is to pass or fail an examination then a boundary would have to be set.
Boundary setting is obviously a  matter partly of judgement, in relation to a purpose, and partly an arbitrary procedure.
Various procedures for standard setting have been described in the literature.
A recent review by Berk (1986) identified 38 methods, most of them developed in the USA during the 1970s.
The methods make different assumptions about mastery.
For example, some assume that mastery is an all-or-nothing state and set the standard at 100 per cent although there is some flexibility depending on the nature of the errors made, if these are few.
Alternatively, other methods assume some variability about a boundary between mastery and non-mastery.
The methods range from those based solely on the judgement of one or more individuals of the difficulty of test items without the guidance of performance data, to those based mainly on statistical analysis of empirical data from test results.Judgement, however, is an element in all the methods described.
The purely judgemental methods have highly prescribed procedures.
A seminal one due to Angoff(1971)1- requires curriculum experts and representatives of interested groups (teachers, employers, etc.) to consider the individual test items and estimate the probability that a borderline candidate would answer them correctly.
Each judge's estimates for all the items are combined and then the combined estimates aggregated to provide the passing score on the test.
Whether such complex formal procedures could be incorporated into a graduated test scheme seems doubtful; the few existing schemes in Britain make use of less formal teacher judgement, within guidelines from the central development team, or laid down criterion scores in tests.
Ways of assessing specific criteria apart from setting tests have been described in current development projects.
In both the OCEA and GAIM projects, for example, teachers note pupils' use of knowledge and skills in the course of problem solving.
The OCEA project distinguishes between "satisfying" a criterion and "achieving" a criterion.
A criterion is satisfied when a skill involved is used satisfactorily in accordance with a teacher's judgement.
In order to achieve a criterion rather stronger evidence is required of the satisfactory use of the skill on several occasions and in different contexts.
Thus, there are two "levels" of mastery.
The judgement of achievement can be made by the pupil's teacher.
The mastery criterion for the Graded Assessment in Mathematics (GAIM) project is that the teacher feels the criterion has been achieved on the basis of evidence, which has to be recorded.
So far, there has been no review of the way in which the British procedures operate and what the consequences are should a pupil be wrongly failed at a particular level which employers may come to  accept as appropriate for some jobs.
Currently, however, graduated assessment is being used more for formative purposes internal to a school where wrong decisions do not have such serious consequences.
Consistency of achievement
It has already been noted that a criterion statement such as "can add two whole numbers summing to between 100 and 1,000" does not refer to a precise domain.
To make the domain precise would necessitate an impractical and unacceptable narrowness in an already narrowly stated objective.
Impractical, because if all areas of the curriculum were treated at this level of detail, hundreds or even thousands of criteria would be produced; educationally unacceptable because of the curriculum fragmentation which would result since it would be difficult to link up such large numbers of criteria.
Furthermore, should such precision in stating a criterion enable test constructors to produce, unambiguously, a set of test items which exemplify it, there may be alternative ways of tackling the items.
Different methods can influence the difficulty of test items and so the precision in the criterion statement may still prove illusory.
To these uncertainties has now been added the problems of deciding what performance constitutes mastery.
Other uncertainties remain to be discussed: how should the influence of context on difficulty, for example, be taken account of?
An even more pervasive problem is that of consistency of performance across time.
Consistency is important for summative purposes, since a statement of what a pupil can do is misleading if true only on a single occasion and in a single context*.
Perhaps it would be more accurate to qualify or amend a criterion statement to "has done…or" can do after practice".
Used for formative purposes, an assessment based on weak evidence could be misleading to the pupil.
Furthermore a student who had a fragile grasp of particular concepts and skills would have a reduced chance of getting much further in a graduated test scheme.
This might not matter so much in relation to a skill such as "estimating" as for a concept such as  "area" or "place value" .
Skills require practice while concepts need to be represented mentally and linked with other concepts.
Some skills or knowledge may be incorporated into higher order or more holistic tasks in later levels of a graduated test scheme.
In such cases, pupils would automatically gain additional experience and practice with more basic skills and knowledge, but, as has already been noted, not all basic tasks can be directly related to higher order ones.
The team studied one aspect of consistency — consistency across time by constructing parallel tests* to be given to the same group of pupils on different occasions.
little research has been carried out on consistency of performance on similar test material, especially of mathematics in context.
One recent finding with YTS trainees, undertaking tasks with which they were familiar in the work place "" , was that consistency can be low on some tests.
Two kinds of work-based tasks were used in the research, micrometer and invoicing exercises.
The micrometer trainees generally performed fairly consistently on the different assessment exercises which they completed but invoicing trainees did not.
The researchers suggest that the micrometer trainees' performance was more consistent because most had "mastered the skills involved to the point where they are secure" , while the invoicing trainees were often still in the process of mastering procedures.
For the NFER project there were three pairs of parallel tests, two pairs were 25-item mental tests, one with and one without the numbers in the questions printed on the pupils' answer sheets.
The third pair of parallel tests consisted of two written tests.
The first 25 of the 30 questions on each of these were equivalently worded written versions of the orally presented mental test questions.
The interval of time between the test occasions varied.
The first set in each pair were sent out to the participating schools in October and November 1985, and the second parallel set in February 1986.
A first question to be asked is whether the difficulty of the questions was the same for the parallel tests within the limits of precision for tests of this length and with the size of the sample of pupils.
About two-thirds of the success rates for each question were less than ten percentage points different.
Some of the differences over ten per cent were almost certainly due to real differences in the questions.
For example one pair of questions asked pupils to estimate the room temperature in degrees Fahrenheit in one test and Celsius in the other.
In both mental and written parallel tests more pupils were able to give the temperature in Celsius than in Fahrenheit.
The overall correlations between the scores on the two tests were 0.80 for the two pairs of parallel mental tests (N = 51 and N = 114) and 0.66 (N = 104) for the written tests.
The correlations for subtests of groups of  items on a particular topic were mostly between 0.6 and 0.8, although two were 0.4 and 0.5 respectively.
Suppose a judgement about reaching criterion performance had to be made on the basis of the results of the tests used in the consistency experiment described above, what would be the decision for individual pupils?
Tables 5.3,5.4 and 5.5 show the pass and failure rates for each pair of parallel tests and for two arbitrarily chosen pass marks, 50 per cent and 70 per cent.
The figures for test occasion one is presented vertically and for test occasion two horizontally in the tables.
Marginal totals are also given in each case.
The correlations between the pass and failure rates on the two occasions is given by 0 coefficients.
The 0 coefficients vary between 0.44 and 0.67 for the 50 per cent pass mark and between 0.44 and 0.74 for the 70 per cent pass mark.
These coefficients are sensitive both to the difficulty of a test as well as its reliability (consistency) and so it is not easy to assess the latter.
However, at the level of decision-making for individuals, it is clear that there is a fair amount of inconsistency.
Over the three pairs of tests (N = 269), 46 pupils would have passed on one of the two occasions had the pass mark been 50 per cent and 54 had it been 70 per cent.
This compares with 163 and 60, respectively, passing on both occasions.
The tests used covered a number of topics and were not specifically related to the curriculum.
A teacher in one liaison group carried out a similar experiment with one of his classes with tests on a single topic.
The pupils were given two parallel tests on some length criteria at an interval of two months.
The tests were of a type which could be used for diagnostic purposes.
The rank correlation between the two sets of scores was 0.5 (N = 22), and a few pupils had markedly different scores on the two occasions.
The scripts were inspected and discussed by the liaison group.
It was considered that consistency of performance depended on a number of pupil factors: their motivation and mood, and their perception of the purpose of the test.
On the evidence of these test results was it possible to say whether or not pupils could estimate and measure length?
It was agreed this was not easy to do and it illustrated that deciding whether pupils had attained a long list of criteria would be a very considerable task.
Discussion
In this chapter two different approaches to assessment have been contrasted.
Norm-referencing emphasizes relative differences between pupils while criterion-referencing stresses absolute descriptions of what pupils have achieved.
The criterion-referencing of assessments is not an essential feature of graded tests, but it is an approach which has been strongly recommended because it is intended to make clearer the targets which pupils need to aim for and to provide information on achievements for parents, teachers and employers.
The main problems of criterion-referencing relate to the accuracy with which descriptions of performance can be made, and the possible effects on curriculum.
It has been suggested that there will always be some ambiguity about the skills and knowledge referred to in a criterion statement, no matter how high the level of detail in the statement.
There are also important areas of knowledge and skill which cannot be described with any precision either because of their nature (e.g. "understanding" ) or because not enough is yet known about the factors causing difficulty to pupils.
Process aspects of mathematics may prove to be particularly difficult to express in criterion statements, but little work has yet been undertaken in this area.
Examples of assessment tasks accompanying the criterion statements may help to pinpoint more precisely the domain referred to in a criterion statement.
They may, however, do no more than narrow or broaden the criterion's reference, depending on the range of examples provided.
Criteria are likely to be derived from an analysis of subject matter in relation to children's learning.
As there is no unique way of analysing subject matter, each teacher or group of teachers may derive a set of criteria which differs in structure or in detail from those derived by others.
Once derived, criteria may be interpreted differently by different teachers, unless a group have worked together on producing the criteria or have been trained to use them.
Employers' and parents interpretations may diverge even more widely, particularly as the level of detail these groups are likely to be able or willing to handle will be broader than that used by teachers.
Another area of ambiguity relates to decisions about mastery.
Decisions about the attainment of criteria may be made either by formal testing, or in less formal conditions during an interview or when observing pupils undertaking classroom tasks, or a combination of these.
Evidence from the results of parallel tests with questions differing only in the numbers used showed that test results for individuals can be variable and so produce misclassifications of mastery.
Informal assessment is costly in terms of staff resources and no evidence is yet available on the consistency of decisions made using informal assessment.
Such assessments have to contend with other factors which could potentially produce misclassifications, such as inconsistency of teacher judgement.
However, whether focused tests or informal assessment is used, judgement must always be a factor in deciding on mastery at a particular point in time.
Differences in results over time may be due to errors which must be attached to judgements of mastery, whether test results or teachers' judgements or both are used in making the judgement.
They may also be due to real differences over time.
Some pupils who failed on the first occasion might subsequently succeed because of increasing insight into a concept or because a skill has been practised.
The performance of a skill is dependent on practising it and so a pupil's success will be crucially dependent on how recently it has been practised.
Thus the meaning of a criterion statement worded "The pupil can…" must be related to the time at which the skill or knowledge was demonstrated and the reliability of the judgement used.
The problems of criterion referencing are considerable in relation to the benefits claimed: a structuring of the learning environment for pupils; a focusing on objectives for teachers; providing more information about what pupils know and can do.
If the introduction of criterion-referenced assessments is intended to assist the process of curriculum change the balance of advantage must lie with that approach.
There must be doubts, however, whether summative  assessment, involving criterion-referencing across a large part of a curriculum, will ultimately be beneficial.
It may prove to be restrictive unless used very flexibly.
It may not even prove to be possible unless the criteria are very broad, and therefore ambiguous, because of the problems of recording mastery of a long list of criteria.
However, formative uses of criterion-referencing, given for diagnostic purposes for example, and at the convenience of the teacher rather than of the scheme, may be very valuable in a classroom.
As currently conceived graduated tests serve both formative and summative functions.
Finally, the relationship with curriculum and children's learning needs to be considered.
The model of learning which underlies much of the literature on criterion-referencing is that of the learning hierarchy: the small step by small step approach from no proficiency to perfect performance.
Few current models of learning are in accord with this simple picture.
Some models involving branching hierarchies and several different types of learning have been described by Robert Gagne.
The types of learning involved range from simple associations and learning names to the understanding of concepts and principles based on several concepts.
However, learning hierarchies for only very simple tasks have been analysed.
The problem, even for these simple situations, is to validate the proposed hierarchies in the sense of confirming (or not) that lower steps are in fact prerequisites for higher ones and in what sense they are prerequisites.
Organic metaphors of growth and development are more prominent in present models; these are more likely to involve a description of progressive differentiation in the complexity of concepts and the formation of links or integration between concepts.
It will be necessary to see how far it is possible to go along with a strict criterion-referenced system or what kind of compromises may be worked out if such a system has advantages of motivating pupils and aiding changes in curriculum.