

Drop the big one
IT IS becoming increasingly difficult to take seriously claims that the civil side of nuclear power has nothing to do with nuclear weapons.
The United states has done a great deal to blur the barrier between the bomb and the watt.
Britain has been an accomplice in this.
It has sent plutonium across the Atlantic, and it has shut its eyes to the consequences.
So far, no one has explained satisfactorily why the US needed plutonium from Britain if it was not to make weapons.
Nuclear power does not have to mean nuclear weapons; but who will believe this when these two countries appear to have blurred the distinction?
Where did all that plutonium go?
For the stated uses, the US needed grams.
Britain sent kilograms, if not tonnes.
There are few peaceful uses for plutonium.
You can put it into fast-breeder reactors — the US has few of these — or you can convert it into isotopes for medical uses.
The extent of the trade far exceeds the US's needs for either use.
No one can point to a particular warhead that the US has made from British plutonium.
A British atom does not wave a union jack as its explodes in a mixture of fissile atoms.
However, any suspicions as to the fate of British plutonium in the US will inevitably undermine international attempts to prevent the proliferation of nuclear weapons.
For some time the weapons states have maintained that the peaceful trade in peaceful atoms has been for purely peaceful purposes.
In any case, they argue that if a country really wants to build its own bomb it will do so.
It won't need anything as big and expensive as a nuclear power station to provide the fissile material.
Such an argument misses the point that nuclear reactors bring with them a great deal of technical information, and experience.
It is true that a country does not need a nuclear power programme to be able to build a nuclear weapon.
But if it has that civil capability it is all too easy to turn it to less than peaceful purposes.
And a country with a nuclear power programme can have nuclear weapons for a modest extra cost.
Such arguments may seem trivial when bodies as thoughtful as the Church of England have taken it upon themselves to ponder the issue of the use of nuclear weapons; but the bomb has not gone off and when it has receded into the background we will be left with the demand for electricity.
One day that demand will begin to grow.
Even if it does not we will need to replace ageing power stations.
The atom is there and when short-term oil surpluses vanish there will be pressure to build more nuclear power stations.
How do we do that without spreading too widely the ability to make bombs?
Probably by encouraging the world's wealthy nations to build nuclear power stations so that other countries need not follow suit.
Then the others will be free to burn the oil and coal that might otherwise be consumed in the nuclear nations.
There is a pressing case for the nuclear nations to deny fissile materials to would be customers.
It matters not that those customers are today seen as perfectly reasonable countries without weapons intentions who should not be denied the benefits of modern technology.
Unfortunately, today's reasonable government may well be ousted by crazed fanatics — 50 years ago in Germany, for example.
This area of business is so important that the nuclear nations should throw overboard all thought of evenhandedness.
They should not try to be fair to other countries.
It isn't fairness that the world needs.
While the US and USSR continue to play their game of trying to reduce the numbers of nuclear weapons, the battle must continue to prevent the spread of nuclear weapons to seemingly peaceful nations.
That means forgetting about the quick bucks to be made from selling nuclear technology.
With so much talk of reducing the number of bombs, the weapons implications of nuclear power seem to have been forgotten.
The trade in plutonium between Britain and the US has shown that two countries that proclaim their peaceful intentions as loudly as any are not beyond suspicion and that it is difficult to believe assurances that nuclear power stations have no weapons connection.
Four-letter computers
THERE hasn't been what you would call a flood of letters following our move to computerised printing.
We've had several comments on the new design.
This had little to do with the computer but it did spur us into action.
We also heard from someone who suffered with us in the hands of Southwark Offset, which tried to modernise us in the 1960s (Letters, p 476).
We've had a few technical difficulties with the computer, or rather our printer has had trouble with it.
When we wrote of the new system at the time of its introduction, we did not mention the technical problems.
For example, there are two computers in the system; five keyboards feed into one and four into another.
Each computer is connected to a phototypsetting machine.
The two computers should be able to talk to each other so that all of the keyboards can make the typesetting equipment jump into action.
Unfortunately, the people who make the system decided to redesign the bit that connects the two computers — they've been redesigning it for months.
To begin with, that meant moving discs from one machine to another.
Now there is a magnetic tape system acting as a go between — a slow and unsatisfactory interim measure.
There have also been software problems.
More than once, for example, the machine has been rendered catatonic.
The keyboards could put copy into the computers but because of the software problem the computers refuse to release it.
They just would not communicate with the outside world for several days at a time.
We survived those problems without too much delay.
With luck we'll survive any other difficulties that the computers decides to throw our way.
If nothing else it does give us a new insight into other people's technical problems.
Cheating charge rocks malaria research
AN EMINENT scientist stands accused of stealing his former PhD student's ideas and publishing them under his own name.
Nine other scientists in Britain and the United States have told New Scientist that they back the pupil, Ian Clark, against his former teacher, Anthony Allison.
The uproar began with the Christmas issue of the premier medical magazine,The Lancet .
It carried a paper by Allison and his assistant Elsie Eugui from the Institute of Biological sciences, Syntex Research in Palo Alto, California, entitled A radical interpretation of immunity to malaria parasites .
The paper could prove enormously important to the future of the 200 million malaria sufferers around the world.
But all the evidence suggests that it has one staring error — the researchers' names.
Last Saturday's issue of The Lancet brought an angry reply from Clark (who did his PhD under Allison in 1977) and W.
R. Cowden and N. H. Hunt of the John Curtin school of Medical Research in Canberra, and G. A. Butcher of the University of Newcastle Medical School in New South Wales.
They claim that the ideas in Allison's article were theirs — and that Allison knew it.
The hypothesis, which both Clark and Allison claim to have thought up, is this.
They believe that the parasites which spread malaria by infecting red blood cells, may be killed by the release of highly reactive substances called ‘free-oxygen radicals’ from specialised cells produced by the body's immune system.
The idea, if it proves correct, explains many seemingly-inconsistent research findings in malaria.
It will explain:
* How malarial parasites that infect red blood cells can be killed.
* How new anti-malarial drugs might work — new ones are badly needed.
*Why some people with red-cell mutations such as thalassaemia and sickle cell anaemia, inherit an immunity to malaria.
*Why certain complications of malaria, develop.
Red-cell damage, for example, could be a side-effect of the release of the free-oxygen radicals, which could damage normal cells as well as infected ones.
Allison's paper says: ‘We now wish to add the hypothesis that these effector cells kill asexual forms of malaria parasites in erythrocytes by means of oxygen-derived radicals, particularly the superoxide anion (O2)’.
Allison claims to have started the idea in 1978 when he worked at the Clinical Research Centre in Harrow.
His experiments showed that a substance called polyamine oxidase could kill parasites infecting red cells.
But, says his co-worker Dr David Morgan, if Allison thought that this was due to the production of free-oxygen radicals, ‘He certainly did not say so at the time.’
What Clark did was to imagine what substances are responsible for killing parasites in these systems — and to carry out the experiments that prove his idea.
Clark says that the idea had already gelled in his mind as far back as September 1981.
He enthused about it to colleagues in Australia and later he took it Europe and the US.
He bounced ideas off colleagues everywhere he went, and they were greeted with enthusiasm.
His last talk in the US was in Allison's laboratory in California.
Back home, Clark found, to his delight, that by injecting malaria-infected mice with alloxan, a drug known to produce free oxygen radicals, he could reduce the level of parasitaemia dramatically.
Unable to hold his excitement, Clark telephoned Professor Frank Cox of King's College London and told him of the results.
This was in Spring 1982.
Clark then wrote up the experiment, which appeared to confirm his initial brainwave, and submitted it to Nature .
The paper was returned because it did not suit the magazines's readership, but was accepted by Infection and Immunity .
It was published in January 1983.
Meanwhile in August 1982, Clark visited Allison and gave him a copy of his paper for Infection and Immunity .
One month later Allison wrote to Clark asking for more details about Clark's work — this time on t-butyl hydroperoxide, another chemical known to produce free oxygen radicals.
The substance had never been given to animals before.
The point of this experiment was to strengthen the alloxan data.
Allison said: ‘I should like to try that on endothelial cells.
We shall not use it on parasites, since that would be in direct conflict with your own programme’.
Clark gave Allison the information.
But alarm bells rang when Allison wrote back in December.
He said: ‘Thank you for the information.
I had already performed a literature search on t-butyl OOH, and we have done some experiments with it on our parasite systems…some of our experiments on the parasites worked well and will appear in The Lancet .’
Later that month the paper appeared.
Allison later told Mark: ‘My first reaction when you told me about your work was to withdraw, and let you have all the credit, but that would have been Quixotic.’
Malaria specialists have rallied round Clark.
Dr Geoffrey Pasvol of the John Radcliffe Infirmary in Oxford said: ‘There was so much there [in Allison's paper]that Ian had said to me in the summer.’
Professor Frank Cox said: ‘The paper is full of internal discrepancies’.
Other specialists agreed.
They had no doubt that the hypothesis stated in Allison's paper really belongs to Mark.
But Allison has not been silent.
He claims that, while the alloxan experiments may have been conducted by Clark, the ideas and conclusions are his.
In a reply to Clark's letter, he says ‘The allegations of Dr Clark compel me to place on record facts that I should have preferred to remain private.
During his work for a PhD in my laboratory, Dr Clark revealed a strong attachment to ideas, irrespective of their origin.’
He mentions two examples of work that he implies Clark tried to claim as his own.
One of these papers in fact turns out to list the authors as Clark, Cox and Allison ; The other referred to research carried out in 1944.
Clark's work on malaria has been backed by the World Health Organisation.
He said this week: ‘If we don't get grants for this work we're on the dole.
The main reasons for writing to the Lancet are frustration that we have been discredited, that our grants depend on this work and also that all the evidence is in writing.’
THIS WEEK
Sizewell safety remains shrouded in mystery
SIX WEEKS after the Sizewell inquiry opened, key safety issues surrounding the plan for a pressurised-water reactor (PWR) remain unresolved.
And the technical reliability of a crucial design feature of the plant — its steam-generators — remain a matter of conjecture.
Last week the inquiry's inspector, Sir Frank Layfield, criticised the Nuclear Installations Inspectorate, the government's watchdog that will license the station for delays in providing a full assessment of the Central Electricity Generating Board's pre-construction safety report.
The NII has still not reported on crucial areas such as the dangers of external hazards like fires, earthquakes and aircraft crashes, protection systems that should stop a small accident becoming a larger one, and the vexed issue of steam-generator tubes.
It is becoming increasingly clear that the inquiry will never have before it any detailed assessment of the safety of the actual design which the CEGB will use to build the Sizewell reactor.
Most of the NII's work so far has been on an earlier ‘reference design’, which has now been  superseded by a ‘contract’ design.
Friends of the Earth has already demanded that the inquiry be adjourned until the NII has ended its negotiations with the CEGB and is ready to issue a safety licence for the reactor.
Last week Sir Frank agreed that the objectors were entitled to complain about the delay in producing the full NII safety assessment.
The evidence so far has ‘left, uncertain or obscure considerable areas of importance,’ he said.
And he pointed out that the NII had given assurances to both the House of Commons select committee on energy and to successive energy ministers that there would be no unresolved safety issues by the time the public inquiry got under way.
Sir Frank will rule on the call for an  adjournment next month, by which time the NII promises that it should have a new batch of reports out.
The NII, which blames staff shortages and hold-ups at the CEGB for the delays, is due to make a full statement next week.
If the PWRs have an Achilles heel it is their steam-generator tubes.
The generator is where hot, pressurised and radioactive water from the reactor transfers its heat, via tiny tubes, to a second system of uncontaminated water.
Corrosion of these tubes has occurred regularly inside American PWRs, leading to an average loss of generating capacity of 3 per cent.
The inquiry heard that a tube rupture at the Ginna PWR in the US last year ended with radioactivity being discharged into the atmosphere.
The US Nuclear Regulatory Commission will soon publish what is expected to be a definitive assessment of generator problems.
The CEGB, meanwhile, wants to use a generator that is untried anywhere in the world, to date.
The more famous accident at Three Mile Island also looms large at the inquiry.
The board's John Harrison listed 17 design changes between the Sizewell PWR and the TMI reactor, whose accident in 1979 proved such a traumatic shock to the nuclear industry.
‘It is highly unlikely that such an accident and sequence of events could occur (at Sizewell),’ he said.
The safety argument continued this week with Framatone, the French nuclear giant, giving evidence to the inquiry on the reliability of its pressure vessels — the heart of the PWR.
The company has been promised the contract if Sizewell gets the go-ahead.
Angry astronomers gag their critics
BRITAIN'S science establishment is trying to suppress reports by a university group which has been assessing the cost-effectiveness of some of the country's most expensive science projects.
The first report, by John Irvin and Ben Martin of Sussex University's science Policy Research Unit, is published this week.
It is a swingeing attack on the ‘scientific productivity’ of Britain's most expensive optical telescope, the Isaac Newton, and its operator, the Royal Greenwich Observatory.
But, the report took three years to publish because of what the unit calls ‘political discussions’.
A similar paper on radio-  telescopes is now bogged down in legal proceedings, initiated by members of the Advisory Board for the  Research Councils.
A proposal by the unit to investigate engineering research has recently been turned down by the Social Science Research Council.
Members of the unit blame lobbying by outsiders for the decision.
The report on the Isaac Newton telescope, published today, investigates how it was used between 1969 and 1978, when it was taken from its Herstmonceux home in Sussex to the Canary Islands.
It finds that the telescope helped to produce an average of only seven research papers each year, compared with around 40 papers from similar American telescopes.
Each paper cost £67 000 — seven times greater than US equivalents.
Nor were the British papers especially good.
Only two were cited more than 15 times a year — one tenth the rate of citing of the US papers.
Britain's cloudy skies are only partly to blame for this poor performance.
The report notes drily: ‘It is not standard practice to erect large optical telescopes on a site adjacent to both misty marshes (those of Pevensey) and bright city lights (those of Eastbourne).’
But, even allowing for this handicap, Irvin and Martin found that university astronomers were not happy with the ‘inflexible’ administration of the telescope by the Royal Greenwich Observatory.
They suggest that the new observatory at La Palma in the Canary Islands, where the telescope has been taken, should not be run by the RGO.
Paul Murdin, who is in charge of the Isaac Newton telescope at its new site at La Palma, believes it is unfair for today's operators of the telescope to be blamed for past errors.
When the telescope was first proposed in 1946, astronomers did not consider siting them abroad, he said.
And he said that, since 1965 when the science Research Council took over the telescope, the observatory has been collaborating more closely with university researchers.
Irvin and Martin's assessment techniques have been in great demand abroad, even as the British science establishment closes ranks against it.
They have just finished a report for the Norwegian government on its atomic research centre.
They have also been commissioned to analyse the workings of the European Coal and steel Programme.
Their methods, assessing the value to science of published papers, have met great hostility from several of Britain's leading astronomers, who believe they should be judged by conventional ‘peer group’ methods.
This week the Science Policy Research Unit, stung by the attacks, withdrew a press release announcing the report and titled Britain's biggest telescope — a failure and replaced it with one titled Britain's biggest telescope — how well has it performed ?
Censorship hits Turin lead survey
David Price, Brussels
SCIENTISTS have accused the European Commission of allowing the lead industry to censor an important report on the effect of lead in petrol.
The report, which reviews an 18-month experiment, in Turin, Italy, was altered by a steering committee which included representatives of petrol companies and makers of lead additives in petrol.
The experiment found that some 25 per cent of lead in the bodies of Turin residents came from petrol.
But the committee removed key sentences which suggested that flaws in the experiment might have resulted in this being a serious underestimate.
The experiment involved thousands of people in Turin and cost £½ million.
Its basic findings were first revealed in New Scientist (4 November 1982, p 281).
The experiment, known as the Isotropic Lead Experiment, involved substituting a special isotope of lead mined at Broken Hill in New south Wales, Australia for the normal additive in all petrol sold in the area.
A mass screening of residents then assessed what proportion of the lead in their bodies was of this isotope.
The published report reveals that the ‘vehicular fraction’ of lead was 24 per cent.
But it omits a crucial sentence that appeared in the scientists draft report which read: ‘These figures appear to be minimum values because a) the decrease in the isotopic ratio in blood probably had not reached equilibrium in 1979 and b) they reflect solely the contribution of the petrols affected by the lead isotopic ratio change,’
In other words, 18 months was not long enough for the ‘marked’ lead to work its way through the local food chain.
Lead from petrol bought outside the Turin area would not have been picked up in the study.
In addition petrol-lead in food brought into the region would not be spotted.
The detail of the report reveals that the proportion of the Broken Hill lead in blood was still rising after the 18 month period when the experiment ended.
The steering committee which made the late changes to the report consisted of representatives from the European Commission, the lead additives companies Associated octel, Ethyl and SIAC, the International Lead and Zinc Research organisation, Agip Petrol and the United States' Environmental Protection Agency.
Meanwhile a similar experiment in Belgium suggests that the contribution of petrol exhaust to body-lead may be between 50 and 60 per cent.
The work is by Professor E. Picciotto and Dr D. Petit from the Brussels Free University and has not yet been published.
The study is tentative, because it is based on a handful of subjects.
But it is interesting because, in Belgium petrol has been dosed with Australian lead for more than 10 years.
This has given time for the ‘marked’ isotope to work its way through the environment much more thoroughly than in Turin.
Civil servants who won't belt up
ONE IN 10 civil  servants at the Department of Transport are breaking their own law by not wearing seat belts.
A spot check by New Scientist of 140 vehicles entering the underground car park beneath the Westminster headquarters of the department on a freezing St Valentine's Day morning revealed 18 beltless persons.
The wearing rate of the men (mostly) from the ministry was 87 per cent, virtually the same as the wearing rate for all London drivers, which is 86 per cent according to the Automobile Association.
Several civil servants obviously understood their legal obligations.
One beltless man, in an official car, signalled to turn right into the car park, saw our photographer, abruptly changed his mind and drove off at high speed down the street.
He returned a few minutes later — this time wearing his belt.
Another man, again in an official car, was a bit late spotting the photographer and, as he turned into the car park, made a halfhearted attempt to pull on his seat belt.
Our count of the beltless excludes a man from the agriculture ministry who had an over-the-shoulder belt slung under his arms and a severely disabled man, who we charitably presumed to have one of the few exemptions.
New scientist asked a DoT spokesman if civil servants had special exemptions from the seat belt law.
‘No’, he said.
Pile 'em high
THE BRITISH government has announced that it has set up a small stockpile of strategically important materials for use by industry.
In a short statement in the House of Commons on Monday, John MacGregor, a junior industry minister, said that the government has made some purchases and is planning others.
MacGregor said nothing about how much the stockpile is costing and said other details are secret.
The announcement of the stockpile was predicted in last week's New Scientist (10 February, p 357).
The metals the government is hoarding probably include  manganese , chromium and cobalt, which are important in steel production and the defence industries.
Civil servants may have spent no more than a few million pounds on the metals, although they may authorise more hefty purchases later.
The idea of the stockpile is to safeguard industry against a shortfall in supply, possibly caused by political upheaval in countries such as South Africa where the metals are mined.
Alert over data thieves
NEW NETWORKS of computers, which swap information about individuals, may become fertile ground for information thieves, says the latest annual report from France's data protection commission.
To combat this new breed of criminal, all computerised data on individuals should be transmitted in code, says the commission.
But there is one problem: French law forbids the coding of such data.
The commission, which was set up in 1978, says the threat to the confidentiality of the masses of information on individuals now held on computers is a major cause for concern and ‘the problems of network security should be  addressed urgently’.
The commission goes on: ‘Until such security has been achieved, the commission will have severe reservations about the setting up of high-capacity name-linked data stores.’
Scots enterprise
CANNY Scottish graduates may do well out of a scheme being launched on Saturday.
The Scottish Enterprise Foundation, a consortium of companies and public bodies formed last November to encourage new  business in Scotland, is inviting students with bright ideas to submit them to a panel of entrepreneurs and experts.
The foundation will tour Scotland's universities, starting with St Andrews this week, to select the 40 best ideas.
The winners will receive an 18-week business training course — which the foundation says is worth £5000 — at Stirling University in the autumn.
Computer aid in trouble
A SCHEME to computerise the Third World has hit trouble because France, which provides a third of the funds, want to go it alone.
The Inter-governmental Bureau of Informatics was set up by the United Nations.
It has 35 members — including France, Italy and Spain from Europe — and a budget of £5 million.
But a plan to bring ‘political awareness’ into data processing — which will be developed at the next major IBI conference in Havana next year — is creating controversy.
Now France says it wants to concentrate its effort to help less developed countries to make the best use of computer technology through its World Centre for Informatics, in Paris.
Shooting holes in the Northern Lights
AN INTERNATIONAL team of scientists is using rockets to shoot holes in the aurora borealis in an attempt to ‘turn off’ the arctic Northern Lights.
The plan is the work of a joint Canadian-US team under the direction of the National Research Council of Canada.
The council's Brian Whalen wants to find out what produces the fantastic display which dances across the arctic and subarctic skies.
‘Project waterhole’ was begun four years ago.
It attempts to modify the electric currents in the ionosphere during a display of the lights and to observe the results.
The first ‘waterhole’ rocket was launched in March 1980 from the Churchill testing grounds in Northern Manitoba.
A Black Brant rocket from Bristol Aerospace of Winnipeg carried aloft a package of explosives prepared by the Los Alamos National Laboratory in New Mexico.
The 100 kg of nitrogen-methane and ammonium nitrate was detonated at an altitude of 300 km, just poleward of an auroral arc.
The explosion created water vapour.
The water molecules were spread widely so as to react with the plasma effectively reducing the plasma density through a process beginning with ion-exchanges.
A following rocket then passed through the 50 km hole in the aurora and observed energetic electrons as they reached the denser atmosphere at the edges of the ‘hole’.
The researchers noted charges of ½ kilovolt — reduced in intensity from a normal auroral charge by several orders of magnitude.
The brightness of the aurora was halved.
Both phenomena lasted for about two minutes before the auroral arc recovered.
Waterhole two, in January 1982, was to have been a repeat performance.
But the rocket malfunctioned.
Waterhole three took place on 6 February.
Scientists on the ground had final control over the detonation of the larger 200 kilo explosive package and set it off directly on the magnetic field lines in the middle of the auroral arc.
Two rockets supplied by the University of Houston were then launched through the hole.
The depletion, or reduction, in ionic plasma density was again observed but this time it was not as strong.
The researchers also saw changes in particle distribution and that the changes in auroral formation are different at the centre.
More monitoring at the Universities of Western Ontario and York added further data, but it will be another month before the full data is ready for assessment at the NRC.
Project waterhole has reinforced earlier theories that electric currents in the ionosphere cause the strange phenomenon of the Northern Lights.
The scientists have been able to observe the formation of new electric fields before the plasma recovers from the explosion.
These ionospheric currents are also important to the acceleration mechanism of charged particles.
Thus waterhole has enhanced scientific understanding of the mechanism itself and of how currents interact to accelerate particles Brian Whalen of the NRC points out that auroral activity is known to affect radio waves and power lines and possibly the weather, too,.
‘We're the nuts and bolts people.
It'll be up to someone else to apply the data directly’, he said.
‘None of us believes this is an attempt to control the aurora we just want to understand it.’
The acceleration of charged particles is of ever-increasing importance to astronomers and astrophysicists as they study such phenomena as pulsars and solar flares.
The NRC group is part of a NASA study committee working on proposals for the use of the Shuttle to tow instruments through the upper  atmosphere .
Shuttle delayed by fear of hydrogen blast
Christopher Joyce, Washington DC
A HAIRLINE crack just two centimetres long in an engine manifold of the newest space shuttle has caused NASA engineers to jettison an engine and bring in a replacement.
The leak allowed hydrogen to escape from a highly pressurized coolant system in engine number one (of the shuttle's three onboard engines) and collect in the back end of Challenger, the second shuttle.
The leak was first discovered last December when the Challenger's engines were test-fired in preparation for Challenger's first, and the space shuttle programme's sixth flight.
It showed again on a second test-firing on 25 January and was finally traced to the manifold a few days later.
Shuttle's managers don't know yet what caused the crack.
The hydrogen could accumulate during a launch and spark a serious explosion.
Last week, NASA's engineers in Florida replaced the $30 million leaking engine with another as the shuttle stood on the launch pad.
NASA is working overtime to get Challenger into orbit by 7 March.
The delay of the launch since 20 January has already sparked a wave of inquiries from customers who are concerned that the launch will be pushed back, delaying other flights.
The latest postponement threatens Europe's Spacelab mission, for example (New Scientist , 3 February, p 291).
To meet the current spacelab deadline of 7 March, NASA has decided to launch Challenger without a test firing.
Should more hydrogen problems develop, NASA has built a ‘purge’ system that would drive helium into the rear sections of the shuttle to clear out any pockets of hydrogen.
Head researcher penalised for fixing result
THE US government is barring a former Harvard Medical school investigator in heart disease from obtaining government research funds for ten years.
The researcher, Dr John R. Darsee, was found to have fabricated research data during studies aimed at assessing the effectiveness of drugs in limiting damage to the heart muscle after a heart attack.
The penalty is believed to be the most severe yet meted out to a scientist for falsifying research findings.
It came after a review panel reported to the National Institutes of Health.
This and other recommendations have been made by a four-member panel of distinguished heart experts, forced a year ago by the National Heart Lung and Blood Institute to investigate the episode.
The chairman of the panel was Dr Howard E. Morgan of the Pennsylvania state University College of Medicine in Hershey.
The panel's recommendations have been accepted by the NIH's director, Dr James B. Wyngaarden and forwarded to the assistant secretary for health, Dr Edward N. Brandt, Jr, who said that he will implement them immediately.
An earlier investigation by Harvard of the same charges resulted in
Dr Darsee's suspension as a research fellow in cardiology at the Brigham and Women's Hospital in Boston.
Dr Darsee's present whereabouts is unknown.
Besides denying Dr Darsee government research funds for 10 years, the panel also recommended that Dr Darsee be excluded from serving on any NIH advisory committee for the same period and called for the heart institute to collect from the hospital the estimated cost of the aborted research put at $122 371.
They denied a request by the hospital to repeat the study at their own expense.
One of the more  significant aspects of the panel's report is that it makes a try at preventing repetitions of the affair.
It calls for a review by the heart institute of the supervisory practices and research procedures at the hospital's cardiac research laboratory, where Dr Darsee worked, in about a year to ‘confirm that these procedures are adequate.’
They also recommended that Dr Darsee's earlier studies done at Emory University and elsewhere be reviewed to make certain they contained no fraudulent material.
Weapons row splits particle physicists
MORE THAN positrons and electrons are colliding these days at the Stanford Linear Accelerator Center (SLAC), one of America's king pins in high-energy physics.
The clash has erupted over whether the center should muddy its hands with research related to nuclear armaments.
It's something that the facility has assiduously avoided since its inception in 1962, and most of the faculty is fighting to keep it that way.
An institute established for pure research should not be involved in weapons research, they say.
But where else will the defence establishment turn if it wants to do its own brand of basic research, using spinoffs from the constant advances in high energy physics?
That's one of many questions being raised by the affair which broke into the open last week.
15 faculty members and 280 staff have signed documents objecting to a research proposal from the University of California and from two laboratories that the university administers for the federal government.
They are the nuclear weapons research labs at Lawrence Livermore in California, and Los Alamos in New Mexico.
Sandia Corporation, an organisation in New Mexico that does contract its work for the government, is also involved.
Ironically, SLAC itself would not be directly involved in the research.
It would be done at the Stanford synchrotron Radiation Laboratory (SSRL) which is fed by electron beams from one of SLAC's two storage rings.
But supplying the electron is tantamount to complicity, according to the Staff.
In a letter to the Stanford committee on research, the faculty said: ‘For years we have been able to tell visiting school children, student, engineers, scientists, laymen and the press that we are here to do basic research on the fundamental properties of matter and that we do not do any military work.
It would be most unfortunate if, as a result of this new work, we could no longer make the statement in good faith’.
Scientific, technical and clerical workers have signed a petition calling for the university to make it impossible for ‘nuclear weapons-motivated research’ involving SLAC to be carried out.
After the anti-war protests of the 1960s, the university forbids classified research — but weapons work can be done if it is publishable.
The petition's author, engineering physicist Mary James, says the issue is ‘a crisis of conscience’ for many people who came to SLAC believing weapons work would never be done.
The UC-sandia consortium wants to establish an X-ray calibration and standards facility at SSRL, among other things, to develop high-speed X-ray detectors to  analyse radiation from nuclear weapons tests.
The detectors would have other non-military uses.
The consortium is seeking $5.2 million from the Department of Energy for the research and for  structural alterations to SSRL involving the addition of two beam-lines.
Another $1 million would come from scientists at seven California campuses who would use the lines for non-weapons research.
The work would completely tie up the lines — which is where the  proposal could come unstuck.
The rules say that any beam-line must be available for general use at least one third of the time.
SSRL has 400 general users, and a lot more demand.
The man who must make the final decisions on this and other proposals is Arthur Bienenstock, director of SSRL.
He can weigh up non scientific considerations in his decision, such as the effect of staff morale and relations with SLAC, but Bienenstock is hoping for a scientific solution not a political or philosophical one.
A panel of 10 scientists drawn form universities and private research laboratories will advise Bienenstock.
If the University of California's proposal gets the nod it will be because of the quality of the science involved, Bienenstock indicated.
But he says it has some stiff competition from a consortium of Eastern universities and industry.
Infrared telescope is looking good
Christine Sutton
ASTRONOMY's newest infrared telescope, the orbiting Infrared Astronomy satellite (IRAS), is working better than anyone dared to hope.
Launched successfully from the US on 26 January, IRAS last week began its systematic mapping of the whole sky across a range of infrared wavelengths.
Before observations began the scientists working on IRAS had just one more nail-biting moment to live through.
This came when the satellite threw off the cover that had protected the sensitive detectors during the launch and afterwards, while the team at the operations centre at the Rutherford-Appleton Laboratory checked that the telescope was pointing correctly.
One look at the Moon or even back at Earth could spell disaster for the infrared detectors which have to be sensitive enough to pick up tiny amounts of radiation from distant stars and galaxies.
A huge curved gold-plated shield helps shade the telescope from the Sun's infrared radiation, but it is crucial that the satellite always avoids looking within 60o of the sun.
After the cover had popped off on 31 January, the infrared detectors recorded their first observation: a fluctuating signal due to the receding cover as it spiralled out of the telescope's view.
After a week of checking everything out the survey began in earnest on 9 February.
Scientists working on the survey are now confident that the satellite will complete its mission before its cooling system runs out of helium in about seven months time.
‘It's all going unbelievably well’, summed up Bill McLaughlin, the mission design manager from NASA, at the end of last week.
Silicon spies plead guilty — their bosses stay silent
THE JAPANESE firm, Hitachi has pleaded guilty in San Francisco to charges that the company and two of its employees conspired to send stolen IBM trade secrets to Japan.
The charges arose out of last summer's internationally publicised ‘sting’ operation when FBI undercover agents in California's silicon Valley formed a bogus company and posed as electronics dealers with trade secrets to sell (New Scientist 1 July 1982 p 8–9).
The guilty plea — a surprise about-face by Hitachi which had strongly denied any wrong-doing — follows several weeks of negotiations between the company and the
US Attorney's office to reach a pre-trial agreement.
The company reportedly wanted freedom for its two employees and for the case to be resolved without any further  embarrassment .
‘I hope that this will serve as an example for other corporations throughout the world who deal in electronics,’ the pre-trial judge, Spencer Williams, commented when imposing a $10 000 fine on Hitachi.
The employees were fined a total of $14 000 and placed on probation.
They could have been sentenced to five years in prison.
But questions remain in the case.
Crucially, did the Hitachi hierarchy in Japan know what was going on?
The payment for the secrets was made by a single cheque of £495 000.
According to assistant US Attorney, Herbert B. Hoffman a cheque that big had to come from someone with ‘significant authority’.
But Hitachi's lawyer Peter Fleming told the court that ‘top management in Tokyo had no knowledge of the acts’.
The cheque was not authorised in Tokyo, he said.
Hitachi management had ‘a deep and immeasurable sense of regret’ about what had happened.
Another unanswered question is the extent of IBM's role in the ‘sting’.
Did the company persuade the FBI to ‘set-up’ Hitachi, an international rival in the electronics business?
It's a question that lawyers representing the two Hitachi employees wanted to pursue in open court.
The question may yet be raised in another case, however.
Mitsubishi, the other Japanese company implicated in the ‘sting’ operation, faces similar conspiracy charges and as yet has given no indication of wanting to reach a pre-trial agreement.
The trial is set for May.
IBM and Hitachi are still at loggerheads.
IBM has filed a civil suit against Hitachi and two silicon Valley companies to recover its stolen property and to prohibit them from using any illegally-obtained information in future computer designs.
Meanwhile, Apple Computers has stepped up its campaign against companies allegedly making counterfeit Apple 11 computers (New Scientist , 18 November 1982 p 407).
The company has asked the International Trade Commission to forbid the entry into the United States of counterfeit computers and components.
It accuses 22 companies — most of them in Taiwan — of making counterfeit Apple computers.
The commission is expected to decide this month whether to open an investigation.
Meanwhile, 10 billion years ago…
Nigel Henbest
ASTRONOMERS from the University of California in Berkeley, have discovered the most distant galaxy yet.
It is about 10 000 million light years away.
Berkeley astronomer, Hyron Spinrad, revealed the find during a workshop at the Royal Greenwich Observatory in Britain last week.
The galaxy shows several extraordinary features that may be connected with its extreme youth (we are seeing it as it was when the universe was a third of the age that it is today).
The galaxy — catalogue number 3C 324 — is full of extremely bright glowing gases moving around at very high speeds.
Spinrad has spent 10 years measuring the distance to remote galaxies with strong radio-signals.
He measured their distance by charting the galaxy's spectrum, and measuring how much the wavelengths are stretched by the expansion of the Universe — in other words, how far the spectral lines are ‘red shifted’ along the spectrum.
The larger the red-shift, the greater the distance.
In recent years other radio astronomers in Cambridge and New Mexico have pinned down the positions of previously unidentified galaxies with increasing accuracy.
And other astronomers, notably the Royal Observatory in Edinburgh, have used infra-red detectors to locate the very faint light from the most distant galaxies.
Spinrad has then been able to go to work calculating their distance.
Two years ago, he found the first galaxy with a red-shift larger than 1 — that is, with all its radiation more than doubled in wavelengths.
Spinrad and the Yugoslavian astronomer, Stanislaw ‘George’ Djorgovski, who is now also at Berkeley, have recently used the 4-metre telescope at Kitt Peak, Arizona, to investigate eight more galaxies.
Spinrad announced last week that his team had found two further galaxies with a red-shift greater than I (bringing the total to five).
The newest discovery, 3C 324, comes top with a red-shift of 1.206, which places it 10 000 million light years from earth.
The galaxy is one of the strongest radio-emitters known.
At one side of the galaxy the gas is travelling at 700 km per second faster than it is on the other side.
Spinrad says that such a huge range in velocity is quite unexpected It could be stirred up by the central jets which produce the radio emission.
Or it may be a property of its extreme youth.
Either way, astronomers are regarding it as a fascinating astronomical object and not just a recordbreaker.
Bonanza for Yorkshire buses
SOUTH YORKSHIRE'S low bus fares are persuading drivers to leave their cars at home, according to a study published this week by Oxford University's Transport studies Unit.
The study found that the number of passengers on the buses had increased by 3 per cent over 10 years when it might be expected to have declined by 25 per cent, because of changes in population, employment and the rise in car ownership.
The research has shown, for the first time, that the British will stop using cars if the bus fares are low enough.
Indeed the report says ‘small numbers of households found that the combination of high motoring costs and acceptable levels of public transport such that they were simply not bothered about learning to drive, or getting a car.
There were also small numbers of former car owners without intentions of buying a car again…’
The study found, however, that those with company-subsidised cars were unlikely to be attracted by cheap public transport.
The study also suggests that there may be even more far-reaching effects of low fares.
They may encourage young people to acquire a life-time habit of using buses and not bother with cars.
Scientists win favour in Australian election
AUSTRALIA's snap general election, called for early next month, has proved a bonanza for  Australian scientists.
To bolster his case for being given a second term — and to outflank the opposition Labor Party — prime minister Malcolm Fraser has announced new research initiatives in biotechnology, satellite research, industrial R&D and in the Antarctic.
The package should be worth around A$100 million.
Luck has played a big part.
The Department of science and Technology had a substantial number of submissions before the Cabinet, hoping that one or two might be funded.
But, in a rush of electioneering, all were approved.
The most substantial boost has gone to Antarctic research.
Ministers voted money for a new polar research ship and airstrips at each of Australia's three mainland stations — Casey, Davis and Mawson.
The infusion of cash to the Antarctic should be worth A$50 to 80 million over the next four to five years and will open up the continent to direct flights from Australia.
The move follows years of neglect and a recent rude report from the government's Antarctic research policy advisory committee which maintained that a replacement transport system was essential if Australia was to maintain an effective presence on the continent.
At present, the Australian stations are served by three ageing chartered supply vessels — one of which is to be withdrawn from service in 1987/88.
The new ship, which has been on the drawing boards since the late 1970s, will be a research vessel.
Most passengers will travel to the continent by air.
Cargo will go by chartered vessels.
The ship should be completed by 1987.
The West German ship-design consultant, Schiffko, has already completed design studies for a number of different options.
A compressed-snow runway, similar to that used by the soviet Union at its base at Molodezhnaya, will be built at Casey for between A$3 and 5 million.
It will be capable of handling Hercules aircraft, A rock runway, will be built at Davis, which will take planes up to Boeing 707 size.
It will cost around A$20 million and should take about four years to build.
Flights from Australia will go directly to Case, from where a shuttle service will operate to the other two stations.
The announcement has delighted Australia's Antarctic scientists who say it will transform research and allow many more top-quality scientists to visit the area in summer.
In other moves the government has made an important policy switch with a decision to provide direct support for biotechnology research.
A national biotechnology scheme will fund specialist research groups at universities and research institutes with the aim of establishing five or six world-class research centres soon.
An initial A$2.5 million has been provided this year but it should be increased to A$5 million a year within three years.
Around A$1 million has also been specially earmarked for industrial biotechnology.
A further A$10 million has gone to Australia's main industrial research finding system, the industrial research and development incentives scheme.
It brings funding for this financial year to A$59 million.
In a final act of generosity the government announced it I would provide A$10 million for upgrading the Australian Landsat receiving station.
Australian mining companies have been pressing hard for the upgrade for the past 18 months but, ironically, demand for Landsat data in the last few months has dropped by 60 per cent.
This is partly due to poor data from Landsat 3.
The station has laid off one-third of its staff.
The government funding has pre-empted the announcement of the opposition Labor Party's science and technology policy which strongly favours the support of industry, particularly high technology.
It names 16 ‘sunrise’ industries for support including biotechnology, computer technology, robotics, industrial ceramics and solar technology.
It also includes measures to free venture capital, boost industrial and basic research funding reduce foreign ownership of technology and an increase in finding for Antarctica of 300 per cent.
It now seems certain that, whichever party comes to power on 5 March, the latest funding measures will be upheld.
But one issue that can be expected to divide the parties in the final weeks is whether the controversial Franklin dam in Tasmania should go ahead.
French greens fall out over local polls
Anna Lubinska
Hopes that the French ‘greens’ would follow in the footsteps of their German counterparts by forming a unified Green Party, have  been dashed by a melange of ideological impasses, personal bitterness and struggles for power among the three movements involved.
Their chances of success in the French local elections on March 6 are dashed.
It now seems likely that a ‘joint list’ of green candidates for local government posts will only be submitted in the Paris region, and the Alsace, An agreement reached in October 1982 that would have linked the Amis De La Terre (AT), the Confederation Ecologiste and the Mouvement D'Ecologie Politique (Now VPE) disintegrated last month when it became clear that the VPE wanted to have a dominant say over its two partners.
In an exchange of letters in January, tensions rose so high that the leaders of the two sides are no longer talking to each other.
The VPE insisted that the AT cease separate political activity altogether.
It wanted the new structure to be a financially  independent party with exclusive political loyalty from its members.
The AT, on the other hand, wanted the groups to continue as an association with individual members of each group able to join the new party if they wished.
Meanwhile the third movement, the Confederation, which represents regional associations, wanted grassroots control of the party and opposed the idea of a national executive, favoured by the other two.
The VPE, headed by Solange Fernex, also wants the funds left over from the Presidential campaign of Bruce Lalonde, one of the AT's leading lights, to go into a common kitty, rather than being set aside for a future bid for power by Lalonde — perhaps in the European parliament elections in 1984.
Despite regular clashes between the leading figures in AT and VPE, Lalonde and Fernex, Lalonde is keeping a low profile.
He has much to gain.
Many expect him to emerge from the rubble as the undisputed leader of the French greens.
Speaking  satellite 
A VOICE  synthesiser aboard a British educational satellite is ready to send messages to schools, engineers at the University of Surrey announced this week.
The engineers built the craft, UOSAT, to relay information between radio amateurs and to make science lessons more interesting.
From Saturday, schools — and anyone else with a cheap radio receiver — can ‘listen in’ to the satellite, which broadcasts data about the electronics on the craft with a voice  synthesiser that has a vocabulary of 150 words.
The satellite passes over Britain every' afternoon and the transmission frequency is 145.825 MHz.
For the exact times when the  synthesiser is switched on (normally this will be only at weekends) space buffs can telephone 0483 61202.
The arms race: is it just a mistake?
Conventional game theory shows clearly why East and West compete to out-gun each other.
Hypergames analysis explains why they do this even though both sides say they would prefer peaceful coexistence
P. G. Bennett and M. R. Dando
THE dangers of the present international situation are depressingly obvious.
With the resources devoted throughout the world to military R&D running at five times the total for medical research, ever more lethal weapons systems seem bound to emerge.
Negotiations for  disarmament have in general led nowhere.
The response to this depends largely on your point of view.
For example, many see the problem as one of straightforward communist (or capitalist) perfidy.
For them, there can be no progress until the ‘enemy’ is overcome.
Others point to the rapid growth of military-industrial complexes with vested interests in international hostility.
Others again suggest that the rapid growth of the physical sciences has placed enormous power in the hands of a species,Homo sapiens , that is socially underdeveloped.
Dark psychological explanations can also be found for our plight.
No doubt every reader will be able to add to the list.
Furthermore, different explanations need not be mutually exclusive although some are, of course.
In general, one should be wary of ‘one-factor’ explanations of complex social phenomena.
Even if that factor exists, the outcome may be ‘overdetermined’; that is, produced by several pressures any one of which would be powerful on its own.
Thus it is important to examine critically the different explanations of our present plight, and the different proposals to solve it.
To do so, it may well be helpful to construct formalised and structured models that show how the deadlock in the arms race has arisen.
For example, a particular view may appear convincing at first sight, but become much less so when more rigorous analysis exposes certain implicit assumptions, or hitherto unnoticed consequences.
This is the sort of task that falls under the general heading of ‘conflict analysis’ or ‘conflict research’.
We have spent a lot of time developing a methodology that could contribute to this process.
We aimed to produce models with a clear logical structure, without depending on overly idealised assumptions.
In particular, we wished to incorporate those subjective and probably unquantifiable factors, involving human emotion and motivations, that seem so important in real life.
This led us to develop an approach known as hypergame analysis.
To explain this idea it is easiest to start with the well established structure of game theory.
Game theory, in general, postulates a set of ‘players’(the ‘interested parties’, be they individuals or groups), each of whom has a set of strategies; strategies meaning courses of action that can be taken, which may be conditional on moves made by the others.
A choice of strategy by each player defines an outcome.
Each player is assumed to prefer some of the possible outcomes to others, and indeed to place the outcomes in a preference order.
From analysis of the game system one can draw conclusions about the stability of the various outcomes.
Though game theory has been criticised, we believe it can be helpful if used critically to explore the possible structure of conflicts, rather than to search for neat but probably illusory ‘answers’.
As usually applied, however, game theory has an outstanding limitation; it assumes that all the players see the same game, yet, in many real-world conflicts, this seems far from true.
Typically, the parties may have very different views of the situation, and of the issues at stake.
Thus, there may be as many distinct ‘games’ being played as there are players.
The hypergame concept is based on taking this idea quite literally.
We take as the basic model not a single game, but a system consisting of a linked set of subjective games, one for each player.
In this way, one can model the effects of the participants differing perceptions, including their perceptions of each other.
The analysis consists, in effect, of tracing out the consequences of those beliefs, in terms of the overall system.
As might be expected, this basically simple idea has some fairly involved theoretical ramifications, but we can ignore these for present purposes.
So far, we have applied hypergame analysis to various case-studies and are developing practical methodology, which uses a two-stage approach.
In the first stage we build up a broad structured picture which identifies the main parties, the interactions between them and the main areas where their interests may conflict.
In the second phase, we undertake formal analysis.
This deliberately starts with some very simple models, which are then elaborated step-by-step.
Throughout, the emphasis is on constructing multiple alternative models, which can be explored and tested against each other.
A very simple model will illustrate the hypergame concept.
We start with a common game theory model of an arms race.
Consider the case where there are two ‘players’, Lilliput and Blefuscu (who, according to Jonathan Swift, ‘had been engaged in a most obstinate war for six and thirty moons Past’).
Each side can choose between just two possible strategies: arm and disarm.
The four outcomes that can result are taken to be as labelled in the Box (over page).
Suppose now that the outcome each side would most prefer (labelled 4 in preference order in the figure) is to gain an ‘arms advantage’.
Second in order of preference for each side (labelled 3) is mutual disarmament.
Third in preference order (labelled 2) is the arms race, with each side competing to gain a theoretical advantage; and the least preferred option (1) is an arms advantage to the other side.
Lilliput's preference is shown on the left in each outcome, and Blefuscu's on the right.
Note that only the ordering of the numbers is significant, not their actual magnitudes.
Stable but irrational
Readers familiar with game theory will recognise the resulting system as the game better known as ‘Prisoner's Dilemma’.
The ‘dilemma’, in our present model, is that it appears rational for each individual player not to disarm, but to arm.
By so doing, one player does better whichever choice the other player makes.
If he disarms, you get your best outcome (4 instead of 3).
If he is arming, you must do likewise to avoid your worst outcome (to get 2 rather than 1).
But the paradox is that if both sides were to disarm, they would both do better than by both arming, because each prefers mutual disarmament (3, 3) to the arms race (2, 2).
The moral of this is that rationality, at least as narrowly defined, does not ‘add-up’.
The logic of individual rationality makes the arms race game theoretically stable, even though it makes no sense as a joint choice by the two players.
This simple game based on the Prisoner's Dilemma does seem to capture something of the problem of the arms race in real life.
Clearly, it could be modified or expanded in many ways.
One could add more strategies, defining more specific alternatives.
One could move away from a straightforward ‘bipolar’ system, in which each side has only one of two options.
More fundamentally, one could ask whether the problem should be seen in terms of an interaction between nations at all, for in what sense can a nation be a ‘player’?
This last issue is of just the sort that, in a ‘real’ analysis, should be asked in the first phase of the study.
Let us, however, raise a more direct question.
Does the above game really reflect current ‘national preferences’?
The issue is far from simple, particularly as that phrase can at best be only a convenient label for the resultant of many political pressures and influences.
However, the professed aims of both East and West are rather different from what in practice they are achieving.
Each side claims to want mutual disarmament most of all, and claims, furthermore , that only the aggressiveness of the other side prevents this.
We would be the last to advocate wholesale belief in the utterances of governments, but it is at least possible that these protestations are sincere.
To model the situation implied by this, we need the extra dimension of differing perceptions.
So we move from a game to a hypergame.
Consider, then, two genuinely peace-loving ‘players’, each of whom would most prefer mutual disarmament.
An arms advantage for oneself is now the option that is next most preferred (3), followed as before by the arms race (2), and then by an ‘arms lag’(1).
If each side correctly perceived the other's preferences, the game of Figure 1 would result.
In that case, one would expect the players to have little trouble in arriving at the result they both desire — mutual disarmament.
If they can coordinate their choices this is the only stable outcome.
But suppose now that each player believes that the other's preferences are as in the first game.
This leads to the hypergame of Figure 2.
Analysis of this system shows the arms race to be stable once again.
Each side would like mutual disarmament, but will be deterred from making any move in that direction by the belief that the other side would cheat, and thereby gain an arms advantage.
The obstacle to disarmament in this case is not ambitions of superiority but the fear on each side of being tricked into a position of inferiority.
This suffices to keep the arms race a stable outcome, in the absence of mechanisms for making agreements between the players that are seen as absolutely binding and enforceable (mechanisms that seem conspicuous by their absence in real life).
Also, once this perspective is established, any assurance from the other side of real interest in disarmament will naturally be seen as a trick, or at best as a propaganda move.
Similarly, players can be genuinely frightened of their own weapons, which are of course purely ‘defensive’.
Once again, one could elaborate this basic picture, which may loosely be described as one of ‘mutual paranoia’.
More sophisticated models would also include misperceptions of strategies (not just of preferences), and hence the issue of how each side interprets the other's actions.
This has been only hinted at here.
However, the most important question is whether the ‘mutual paranoia’ theme has any basic plausibility in the context of the current arms race.
The arms race is almost certainly the product of many interactions at different levels, including domestic politics on each side and technical rivalry between defence establishments.
To the extent to which it makes sense to speak of interactions between whole nations at all, we suggest that the two superpowers and their allies may indeed be playing something like the paranoids' hypergame.
At its most general, this view implies that the present perilous state of the world has not come about because its leaders (or shadowy figures supposedly manipulating them) are particularly warlike.
Neither would we suggest that they are particularly stupid.
Rather, something more subtle may be at work.
Specifically, present defence thinking may be based on a too-narrow paradigm of deterrence theory, and a set of assumptions about the world that are at best highly questionable, at worst lunatic.
But because these elements form part of the perceived definition of the problem, they are immune from serious challenge.
Despite all the apparent sophistication of defence analysis, it may thus have produced a set of answers to the wrong question.
Despite serious and sometimes substantial attempts to provide policy-makers with alternatives, the view of the problem that has ‘stuck’ remains that expressed by Hermann Kahn's central question, ‘What price are we prepared to pay to deter the aggressor?’
In itself, Kahn's question is not unreasonable.
But to take it as the question of international policy is to preclude almost by definition the construction of more cooperative approaches.
Its inadequacy is manifested in at least two ways.
First, there is an inability to consider alternative ways in which the ‘other side’ may see the world, and especially the possibility that they may see ‘us’as the aggressor.
Secondly, there is an almost total concentration on ‘individual’ rather than‘system’logic as the basis for making decisions.
As shown even by the very simple models here, narrowly-defined self-interest can be self-defeating once there is more than one ‘player’.
In this case, there seems a clear need to look beyond national interest, not because of some admirable but idealistic notion that one should try to devise policies that are good for everyone, but as a matter of practical necessity.
The Prisoner's Dilemma
ONE OF the most important lessons of game theory is that even if everyone is agreed on the most desirable course of action — for example, nuclear disarmament — a collection of individuals or nations acting independently may find it difficult to achieve the most desirable aims.
The idea is among those that have been taken over successfully into the theory of evolution by John Maynard Smith, who has shown why the evolutionarily stable strategy in a given situation may not be the strategy that would bring most benefit to every individual — the catch being that the only way all individuals could benefit from the best possible strategy, is for all to agree to pursue that strategy.
The dilemma is neatly illustrated by a simple example from game theory called ‘Prisoner's Dilemma’.
The story is told in all the standard texts, and here I have followed the version in Game Theory and Politics by Steven J. Brams (The Free Press, Macmillan, 1975).
Imagine two criminals, partners in crime, who are arrested and placed in separate cells with no means of communication.
The District Attorney (this is an American story) believes the prisoners to be guilty of a serious crime, but has no proof that would stand up in court.
He needs a confession, and attempts to gain one by offering the same deal to each prisoner.
If one suspect confesses and implicates the other, who does not confess, the confessor will go free as a reward for cooperation, while the partner is sent down for the maximum sentence, 10 years.
If both confess, then both will go to jail for five years.
But if neither confesses, all the DA can do is nail them both on a lesser offence, for which the maximum sentence is one year.
The actual numbers in this example are not important.
What matters is simply the ranking of the options available.
What should each prisoner do, if motivated solely by self-interest?
This is the essence of the dilemma.
If each prisoner can trust the other not to confess, the best deal for the two of them is to maintain silence.
But if there is any doubt at all in the mind of one prisoner about his partner's reliability, then it is better to confess.
Confession is the strategy which minimises the maximum jail sentence the prisoner can receive, and this minimax strategy is the best one, even though it ensures that the prisoner cannot receive the lowest sentence allowed by the ‘game’, the one year for a lesser offence.
In this simple example, there is an obvious resolution to the dilemma.
The prisoners can each be sure of benefiting if they have a previously agreed pact never to confess, whatever the circumstances.
This is, allegedly, one of the rules of the Mafia; in that case, a prisoner who confessed would be sure of receiving retribution from the Family, and this outside agency effectively changes the structure of the game.
There are obvious similarities between the arms race and Prisoner's Dilemma, and between political reality and the hypergames discussed in the text.
But in the real world there is no outside agency to enforce cooperation between the ‘players’ in the game, With no kind of ‘super superpower’to make sure that the players stand by any mutually beneficial agreement, the temptation to renege remains a powerful factor.
John Gribbin
Wise owls flourish among the oil palms
Barn owls are extraordinarily successful in Malaysia, where they are changing their habits and becoming valuable friends of the plantation-owners
Graham Lenton
THE CHANCES of seeing 20 wild barn owls together in Britain are virtually nil, but in the oil palm plantations of Peninsular Malaysia such sights are commonplace.
On almost any evening in central and southern Malaysia barn owls congregate after hunting at favoured spots, and do not disperse again until after dawn.
Ubiquitous and successful as they are, the owls could become significant agents of biological control.
The barn owl,Tyto alba , is perhaps the world's most cosmopolitan bird; its members are widely distributed throughout temperate and tropical regions.
But barn owls, famed in folklore for their eerie screech and ghostly white plumage, are declining in Britain.
Stone farm buildings that make ideal nesting sites are becoming scarce.
Pesticides and gamekeepers have also taken their toll.
Until recently, barn owls were also quite rare in Malaysia, although common in neighbouring Thailand and to the south on the islands of Indonesia, especially Java.
But in 1968 a pair of owls bred in the loft of an oil palm manager's house and since then they have spread rapidly, throughout the southern states as far north as Kuala Lumpur.
The birds are now quite common throughout plantations.
Their success is due to a plague of rats, which thrive in the burgeoning oil palm (Elaeis guineensis)plantations of the Peninsular.
Malaysia now produces more than a third of the  world's 's palm oil, and oil palm nuts make an attractive food for the Malayan wood rat (Rattus tiomanicus), the ricefield rat (Rattus argentiventer), and the little rat (Rattus exulans ).
Brian Wood, a pest research officer, estimates that some 240 rats live in each hectare of plantation.
Palm oil earned Malaysia some £344 million in 1979; if left alone, rats would probably have consumed £22 million of the profits.
Poisoning rats with warfarin reduces the damage but is costly and possibly damaging to the environment.
But an average-sized family of owls consumes some 1300 rats each year.
The hope that owls might be effective rat catchers inspired researchers to take a closer look at the habits of these opportunistic birds.
The versatility of the owls is striking.
In most parts of the world, barn owls frequent open country and quarter the ground in search of prey.
But these owls prefer to hunt in dense, forest-like plantations with some 220 palms per hectare, where they ‘perch and wait’ for the rats.
Their diet has changed as well.
Most barn owls eat a variety of small mammals and birds every night, and tell-tale remnants of their meal turn up as undigested bone, hair and feather in regurgitated pellets.
But the Malayan barn owls concentrate on rats.
The Malayan wood rat is their favourite; its remains turned up in 90 per cent or the 2839 pellets examined.
These large (100–150 gram) rats make a substantial meal.
Usually each pellet contains the remains of only one individual.
Malayan barn owls are adapting quickly to this rich habitat and are reproducing much faster than elsewhere.
They lay more eggs than European owls: the mean clutch size in Malaysia is 6.6 compared with around 5.5 in Europe; and while Europeans lay only one clutch each year, the Malayan barn owls reproduce twice or even three times a year.
Broods regularly contain six or seven chicks, and a few parents produced eight or nine young.
In Europe broods of three to six are the norm.
But a shortage of nesting holes probably limits the numbers of owls that can breed on an estate.
Even a large plantation of 200 to 6000 hectares usually has only two or three suitable nest sites.
The researchers tried to bump up numbers by installing large wooden nest boxes (0.5 m x 0.5 m x 1.0m) on the top of telegraph poles throughout three plantations.
Almost immediately barn owls moved in and bred successfully.
On one estate, three owls bred within 50 m of each other.
two in boxes and one in a house loft; between them they fledged a total of 24 chicks in 14 months.
Flocks of owls The population explosion led to new social habits.
Owls started to roost together after hunting each night, forming congregations of up to 40 owls.
The gregarious owls were juveniles less than a year old just coming into breeding condition, but unable to find a place to nest in the plantation.
The nest boxes certainly increased the number of owls in the plantation and the owls responded to the squeeze by defending less rigid territories; the researchers found that the ranges of pairs overlapped considerably.
But even so there is a limit to the number of owls that can be packed into an area; each pair probably insists on at least 20 hectares.
And as rats are so abundant (as many as 5000 may live in a pair's range) many will escape the talons of the nocturnal predators.
On their own, owls cannot cure the rat problem, but they can help with control.
Once poison has temporarily reduced rat populations to almost zero, predation by barn owls can slow the recovery.
By thus increasing the time interval between successive poisonings, the owls can reduce labour and material costs and lessen the impact of pesticides on the environment as a whole.
The research on owls seems to have convinced a number of oil palm companies.
Estate managers are beginning to implement their own integrated biological and chemical control programmes by erecting nest boxes.
They will also be promoting the barn owl invasion of Peninsular  Malaysia .
MONITOR
How the West was one
MOST Californians are outsiders over half have travelled thousands of miles for a place in the sun.
Now scientists are saying that the land itself is foreign and that it too has travelled vast distances across ancient oceans — at the rate of some 25 cm a year.
Only 15–20 per cent of present day California is part of the original North American plate, say three geologists at the United States Geological Survey (USGS) who have been investigating the birth of California.
A fascinating and provocative timetable spanning nearly 200 million years has emerged.
Their work points the way to a revision of plate tectonic theory and ideas about mountain building The USGS geologists, Mark Blake, David Howell and David Jones, have identified six distinct ‘episodes’ gradually creating present-day California Chunks of land came crashing onto the western side of the continent and spread out before fixing themselves to the existing coastline.
Some of the collisions lasted for millions of years before the land was finally in place.
Many separate chunks of land, known as terranes, were involved in each episode.
Their findings dramatically support the idea of accretionary or microplate tectonics advanced by geologists in recent years.
The new theory is a refinement of the theory of plate tectonics, which holds that continents grow steadily and systematically by a process called subduction.
One plate slides under another, scraping off rocks and generating volcanic activity on the overlying plate.
But plate tectonics they say, can not explain the immense diversity of land formation and exotic arrangement of rock in western North America.
For instance, fossils deposited in Asia have turned up in Canadian rock.
Recent  palaeomagnetic studies, which identify the magnetic secrets ‘frozen’ into the rocks when they solidified, have also enabled scientists to decipher when the rocks moved and where they came from.
‘It has taken many geologists a long time to accept plate tectonics’, Dr Blake told New Scientist at the survey's west coast headquarters in Menlo Park, south of San Francisco.
‘We're now asking them to accept a revised model and we expect some opposition.’
The USGS geologists still see plate tectonics as the driving mechanisms, but see it acting rather like a conveyor belt.
‘We're saying that the plates had passengers on them and it is these pieces of flotsam and jetsam, not the crustal plates, that have made the most impact on land formation,’ Blake says.
Between these comparatively shortlived events were long periods of stability when plate tectonics was at work.
‘After the collisions the plates were able to adjust each time — fairly quickly it seems — and things went back to normal until something else came crashing in.’
The ‘something else’ refers to chunks of long-lost continents and pieces of the ocean, ranging in size from the wafer thin — about the width of a football field — to the massive — perhaps an area equal to England and Wales.
After five year's research, the USGs geologists reckon they know what happened in California.
The drama begins about 210 million years ago, when the North American plate finished some 600 km inland roughly where California now meets Nevada and Arizona.
During the next 60 million years, about one-third of California was built up by accretions.
But much of this land is the Sierra Nevada range and the accretionary material has been ‘engulfed’ by younger granite intrusions, such as the spectacular Yosemite area.
The first distinct accretion, according to the USGS team, occurred about 150 million years ago when the collisions added the western flanks of the Sierra and the Klamath mountains to the northeast.
Marine fossils from the Tethys Ocean, a sea that existed 250 million years ago southeast of the Eurasian land mass, were found in the terrane.
About 125 million years ago the foundation for what is now the state's central valley arrived.
It has been added to by sedimentary deposits.
The ensuing 35 million years is offered as an example of what happened between the arrival of the terranes.
The plates realigned and subduction resumed, sparking volcanic activity which spewed forth the granite rock covering much of the Sierras.
The next terrane to arrive, some 90 million years ago, was a mish-mash of rocks referred to by the Indian name ‘Yolla Bolly’, a collection of coarse grained sandstone rock found in various parts of Northern and central California.
Blake speculates that the rocks were probably the upper edge of a continental fragment that plunged under the North American plate.
The rocks were widely scattered in the process.
In the interim before the next collision, plate tectonics resumed but subduction did not.
Instead, the oceanic plate, slid past the North American plate, much like what is happening  today with the San Andreas fault.
The sliding plate brought forth a series of terranes from thousands of miles to the south.
The geologists suggest they coalesced and collided with the coast about 55 million years ago, adding large parts of the San Francisco Bay region to the landscape.
Between 38 to 20 million years ago large sections of the present-day west coast were formed when a chunk of a long-vanished continent adhered to the coastline.
In California, it extends some 400 km along the coast from Sonoma County to Eureka.
A highly significant addition during this time was an underwater ridge from the eastern Pacific; after accretion it pulled apart to form the San Andreas Fault now a few miles inland from the coast.
The fault's western side is moving inexorably northward at a rate of five cm a year and should be part of Alaska in about 50 million years.
The latest arrival — number six on the USGS list — added a small coastal range near Cape Mendocino, northern California, about 15 million years ago.
This detailed knowledge of land formations should help geologists find mineral resources and evaluate geologic hazards such as earthquake zones.
It may also shake up traditional theories on mountain building.
Peter Coney, of the University of Arizona suspects that mountains do not form along the edge of a plate, as traditional plate tectonics would suggest, but within it.
Mimicking photosynthesis in the laboratory
NEW LIGHT is being shed on the molecular mechanics behind photosynthesis — the process by which plants turn sunlight into chemical energy.
Electron microscopist Kenneth Miller, from Rhode Island's Brown University, has revealed the three-dimensional structure of a photosynthetic membrane.
And at Michigan State University, Professor H. Ti Tien and his team have mimicked the all-important event of charge-separation — the generation of photo-excited electrons that leads eventually to the biosynthesis of carbohydrates from carbon dioxide.
Photosynthesis is really two  interrelated processes that biochemists call the light and dark reactions.
First, particles of light (quanta) are collected and used to split water into oxygen and electrons.
The oxygen is released  into the atmosphere while the electrons are used to generate ‘reducing power’ in the form of NADPH (reduced Nicotine Adenine Dinucleotide Phosphate) and ‘biochemical energy’in the form of ATP (adenosine triphosphate).
This is the light reaction.
In the dark reaction, the NADPH and ATP are used to convert carbon dioxide to carbohydrates — a well-known process that forms the ‘Calvin Cycle’.
On a molecular level, photosynthesis begins when light quanta excite banks of chlorophyll molecules that are arranged into networks of microscopic antennae protruding from the photosynthetic membranes.
Like a photochemical spider's web, when one chlorophyll molecule is ‘struck’ by a photon and becomes electronically excited, the excitation spreads rapidly to the rest of the network.
Sitting in the middle of the antennae are specialised chlorophyll molecules that trap the photon energy.
The electrons of chlorophyll molecules become excited and are ferried across the photosynthesis membrane by electron-grabbing molecules called quinones.
The chlorophyll traps are left temporarily positively charged a (so making them chemically highly reactive) while the electrons are put to use in synthesising NADPH and ATP on the other side of the membrane.
In essence, this is the act of photosynthetic charge separation.
The chlorophyll traps make up their electron deficiency by grabbing them from water molecules sited close by.
In chemical terms they oxidise the water.
The antennae-trap web is now ready to catch more photons and produce more oxygen and electrons to continue the cycle.
In plant cells, the photosynthetic mechanism is located deep within the cell's chloroplasts.
Photosynthetic bacteria, on the other hand, have no distinct site for photosynthesis.
Miller separated photosynthesis membranes from the purple bacterium, Rhodopseudomonas viridis, and has now determined their 3-D structure by electron microscopy.
Using a computer-aided image analysis technique (Fourier analysis) the picture he comes up with is of a membrane composed of individual subunits arranged in a hexagonal lattice.
Each subunit is about 100 Angstroms (10A= Inm= 10 -9 m) across with a central region 70A thick surrounded by six satellites.
Miller thinks that the light-collecting antennae (in this case proteins the satellites while the proteins concerned with electron-transport and other reactions of photosynthesis are concentrated in the central region.
(Nature , vol 300, P 53).
Meanwhile, biophysicist Professor H. Ti Trien and his team have been trying to mimic photosynthesis, in particular the charge separation step.
Believing that efficient charge separation could only be possible if the electron donor (that is, chlorophyll) and the electron acceptor (quinone) were in close proximity, they decided to put them into the same molecule.
Following other workers in the field they synthesised porphyrins (close synthetic cousins of chlorophyll) bearing quinone units.
These porphyrin-quinones were then incorporated into ultra-thin (100A) lipid bilayer membranes (Journal of Photochemistry , vol 20, p 139).
On one side of the membrane is a solution of ascorbic acid (a reducing agent) while on the other side is a ferric chloride solution, which is an oxidising agent.
Shining visible light into the membrane excites an electron, from each of the porphyrins, which is then grabbed by the attached quinones.
The resulting charge separation, is in many ways analogous to photosynthetic charge separation.
The ferric-chloride side of the cell then picks up the electron from the quinone reduction while the ascorbic-acid side donates an electron to the porphyrin (oxidation).
In other words, under the action of light, the membrane allows the passage of an electric current when the two sides of the cell are connected up externally.
Tien claims a photocurrent of 22 nanoamps (22 x 10 -9 amp) at a photovoltage of 302mV.
Although efficiencies are tiny compared to solid state solar energy systems ( per cent compared to 12 to 18 per cent) it is the best yet for biomimetic devices.
Besides helping to understand the primary events of photosynthesis, Tien envisages his thin films could be biomimetic solar energy transducers.
If so, he'll have to up his efficiencies dramatically.
On the trail of an elusive stimulant
IT IS NOT unusual for scientific predictions to be made long before techniques are available to test them — and it is particularly satisfying when such predictions are at last fulfilled.
Researchers at the Salk Institute in  California have at last tracked down the last of the ‘releasing factors’ secreted by cells in the hypothalamus that have long been assumed to regulate the release of hormones from the pituitary gland (Trends in Neurosciences .
vol 6, p 1).
The pituitary (located at the base of the brain) is the ‘master gland’ whose hormones stimulate endocrine glands elsewhere in the body to secrete their own hormones.
Thirty years ago Geoffrey Harris, working at the Maudsley Hospital in London, reasoned that the brain's control of the pituitary must be exerted via hormones secreted into the bloodstream from the hypothalamus, a brain region with an important role to play in the regulation of many bodily functions.
Since then, hypothalamic releasing factors have been discovered for all the major pituitary hormones (thyroid stimulating hormone, luteinising hormone and adrenocorticotropin, for example) except growth hormone.
Somatostatin, a hormone that inhibits the release of growth hormone, is now well know, but its opposite proved elusive.
Now Roger Guillemin and Wylie Vale, working separately, have each isolated and characterised a peptide (chain of amino acids) that seems certain to be the missing hormone.
But they did not find it in the hypothalamus.
On examining patients suffering from acromegaly (a disease caused by too much growth hormone, leading to enlarged head, hands and feet) they found that each had tumours of the pancreas.
It was in these tumours that the peptide, called human pancreas growth hormone releasing factor, or hpGRF, was discovered.
Tests have shown that hpGRF is extremely powerful in stimulating the pituitary to produce growth hormone and no other hormones.
But is it the same as the hypothetical hypothalamic growth hormone releasing factor?
Guillemin has already found that labelled antibodies to hpGRF do attach themselves to cells in the hypothalamus of human or squirrel monkey brains.
So it looks as though hpGRF and hypothalamic GRF are the same thing in these species, and it should be only a matter of time before it is from the hypothalamus as well.
Satellite spots neutrons from the Sun
FLARES on the Sun have been known for many years to produce powerful bursts of X-rays, gamma-rays and charged particles.
Astronomers have long suspected that flares also produce neutrons that reach the Earth's orbit, but they lacked the instruments needed to detect them.
Now researchers analysing results from the Solar Maximum Mission satellite have reported detecting solar neutrons from flares two years apart.
The first neutrons detected were generated by a solar flare on 21 June, 1980, although the satellite data was not analysed until later.
A much stronger flare occurred on 3 June, 1982, shortly before Edward Chupp of the University of New Hampshire and eight colleagues from there, the Max Planck Institute for Physics and Astrophysics, and the Naval Research Laboratory, submitted a paper based on their analysis of the earlier flare measurements.
Subsequent analyses of observations from that day, when the satellite was well-placed for observations, showed a short burst of gamma and X-rays followed about a minute later by energetic neutrons that kept on coming until the satellite's orbit carried it behind the earth.
The first neutrons to arrive had the highest energy and the later ones had lower energies.
That is about what would be expected if a short burst of neutrons with a range of energies had to travel a long distance; the slower, less-energetic neutrons would lag behind those with more energy.
Free neutrons are unstable, with a half-life of about 11 minutes, so as observed, the intensity drops off with time as the slower-moving neutrons had more time to decay.
It was possible to correlate the June 1982 neutron burst with a flux of particles detected on the ground by a cosmic-ray detector on top of the Jungfraujoch, a mountain observatory 3–6 kilometres high in the Swiss Alps, operated by the University of Bern.
That marked the first time that solar neutrons were detected on the ground.
Chupp says the correlation of events is important because it lets ground-based observers know what to look for; it is possible that earlier observations of solar neutrons went unrecognised.
The neutron observations may help cast new light on solar flares and solar activity.
The charged particles produced by flares can disrupt communications and present hazards to spacecraft and astronauts.
Because they are deflected by electric and magnetic fields, charged particles can take many hours to reach the earth; neutrons are unaffected by such fields, hence they travel much faster and can serve as an early warning signal.
Measurements of neutron flux at the earth can also help in estimates of carbon-14 production, important because studies of the concentration of that isotope are used in dating and in research on solar activity and climate in the past.
Chupp adds that the existence of strong neutron fluxes may also require re-evaluation of some previously observed solar events.
The observation of solar neutrons has not been accompanied by observation of antineutrons.
Antineutrons are expected by some particle physicists, who believe on theoretical grounds that neutrons could change into antineutrons on their way from the Sun to the Earth.
Antineutrons should be hard to miss; Chupp says they would generate a signal of two billion electron volts, four times the 500 million electron volts of the most energetic neutrons detected.
so far.
Smother tumours
A PROTEIN in blood platelets, called platelet factor 4; can prevent the sudden growth of tiny blood capillaries that feed cancer tumours, Dr Judah Folkman, professor of surgery at Harvard Medical School, told a recent Ciba Foundation meeting in London.
Blood vessels normally grow during the menstrual cycle, embryonic development and wound healing.
But this directed growth of capillaries, known as angiogenesis, also occurs in the early stages of cancer.
The fast-growing capillaries bring blood carrying oxygen and nutrients to the swelling tumour.
If angiogenesis could be stopped, the tumour would in effect, be suffocated.
Capillaries grow in several stages: they sprout from their parent vessels, grow larger, and form hollow and leak-proof vessels.
Blocking any one of these developmental stages stops the whole process.
Dr Folkman found that heparin, released by mast cells around the tumour, causes the capillary cells to elongate very rapidly.
But when heparin is bound to protamine, a substance extracted from the sperm of salmon, new capillaries do not grow longer.
Folkman discovered that tumours in rabbits' ears treated with protamine grew much slower than untreated tumours.
Protamine also slowed the spreading (metastasising) of lung tumours in rats.
The amount of protamine needed to actually shrink the tumours, however, was toxic and killed the rats (Nature , vol 297, p 307).
Platelet factor 4 (PF4), which also binds heparin, is even better at stopping angiogenesis in tissue cultures and is less toxic than protamine, Folkman says.
It can also be readily extracted from out-of-date human blood platelets.
Folkman has plans to test the factor on tumours in animals.
‘PF4 may also be too toxic for use in humans,’ says Folkman, ‘but it will give us a scientific lead to other compounds that might stop tumour growth.’
By finding out how these heparin-binding compounds stop angiogenesis, other more useful drugs may come to light.
Angiogenesis-inhibitors may one day also help treat diseases such as rheumatism, psoriasis and diseases of the eye, which are characterised by an invasion of tissues by blood vessels
New test highlights pre-malignant cells
CANCER may soon be detected in its earliest stages — even before cells begin to appear abnormal.
Cell biologists, Drs Andrew Sincock and Jeff Middleton at Queen Elizabeth College in London can now identify potentially malignant (cancer-causing) cells in cervical smears months or even years before a skilled pathologist can see structural abnormalities in the cells.
The new semi-automated test could at last make massive screening programmes for cervical cancer economically feasible.
Potentially cancerous cells are identified by the vulnerability of their DNA to attack by dilute acid.
DNA briefly exposed to acid takes a certain number of aldehyde (-CHO) groups on board, which are stained purple by Schiff's reagent.
This well-known procedure is known as the Feulgen reaction.
But the nucleus of a pre-malignant cell takes up two to three times more dye than a normal cell, the researchers find.
The nucleus contains more DNA — and its DNA is also more susceptible to hydrolysis by acid.
Apparently, the DNA in an early cancer cell is uncoiled to a greater extent, reflecting heightened metabolic activity.
The research, entirely funded by a charity, Quest for a Test for Cancer, has come up with a way of determining exactly how much stain the DNA has taken up.
The abnormal staining is quantified using an integrating micro-densitometer — across between a microscope and a spectrophoto meter — hooked up to a microcomputer.
A narrow beam of light, about half a micron in diameter, is passed down a microscope focussed on a stained nucleus.
Two oscillating mirrors cause the beam to scan across the nucleus some 60 times.
Even fraction of a second, the optical density of the nucleus is measured and stored in the computer's memory.
The computer can be programmed to display immediately the number of nuclei that are abnormally stained.
The straightforward cytochemical test means that large numbers of samples can be quickly and accurately analysed by relatively inexperienced technicians.
The researchers do not yet know whether the abnormal cells detected by this technique always become cancerous, but long-term clinical trials should give the answer.
They are now following the progress of a group of outpatients at Westminster Hospital screened for cervical cancer by the new test.
Plans are underway to apply the technique to other types of cancer as well.
Birds are deaf to jumbo jets
FASTER, quieter aircraft may be good for people — but not for the world's natural fliers.
Birds roosting and foraging near airports now collide with jumbo jets seven times more often than they did with the early turbo jets.
Birds, like planes, usually face into the wind, so they do not see the plane coming.
Their only hope is to hear the aircraft in time, Joanna Burger of Rutgers University suspected.
But she found that background sound levels at New York's J. F. Kennedy international airport are 51–98 decibels — nearly as loud as an approaching aircraft (Environmental Pollution .
vol 30, p 143).
A bird sitting on a runway can only detect an increase in sound when the plane is just 600 to 800m away.
This gives the bird only about 10 seconds to make its escape from a wide bodied Boeing 747.
Bird strikes have become far more frequent since the jumbo jets were introduced in the late 1970's.
Narrow bodied planes such as the Boeing 707 are slower and more noisy at touchdown, giving the bird more time to get away.
But its chances of evasion are still not good.
Gulls flying into the wind down the runway, at 35km per hour, will soon be overtaken by planes moving at 150km per hour.
And the bird's standard evasive tactic is ill-suited to the airport.
Some gulls react by zig-zagging in front of the plane.
DNA rings may be the key to ageing
WHY DO WE age and die, instead of staying young forever?
The eternal quest for immortality will be in vain until we know the answer — unfortunately, we don't.
A popular theory is that the body's genetic material ‘wears out’, accumulating more and more errors until the body's cells cease to function altogether.
Dramatic evidence of a more active process comes from research into human plasmids: nomadic, circular pieces of DNA which resemble cancer-causing viruses.
The ‘nomads’ are found in normal human cells, with carrying numbers present depending on the type of tissue (New Scientist , vol 94, p 18).
The circular DNA molecules consist of short, repeating sequences called ALU, which flank unique sections that may be genes.
Like the cancer causing retroviruses, the nomads spend much of their time inside chromosomes at various locations in the main body of the DNA: however they often re-form as plasmids to cruise aimlessly about the nucleus.
Retroviruses cause cancer through the action of genes that alter growth; these genes are flanked by short ALU-like repeats.
The function of our own plasmids is unknown, but the unique, ‘inter-ALU’ sequences may be ‘nonsense’or then again, powerful control elements or genes.
Some support for this is the finding that leukaemia cells have unusually few plasmids (Nature , vol 296, p 219).
At an ageing research centre in Arkansas, Bob Schmookler Reis and his colleagues have found that cultured skin cells accumulate plasmids as they multiply and reach their natural life span.
Following this clue, they studied the antibody-producing blood cells of young and old people and found that cells made by older people had four times as many plasmids as ‘young cells’(Nature , vol 301, P 394).
Where do the plasmids come from?
The researchers found that the cells were actually making more plasmids.
The alternative explanation — that more DNA circles were escaping from the cell's genetic material in the nucleus — now seems less likely.
The extra plasmids may appear simply because of the normal instability of the genetic material of the blood cell: cutting out large chunks of DNA is routine when a blood cell makes antibody genes.
However, the association of the nomadic plasmids with ageing is probably more general.
If the ‘inter-ALU’ DNA sequences represent genes for growth modifying proteins (as in retroviruses), then we have in our bodies terrorist elements, which may be part of a normal regulatory system until they become too abundant and cause senescence and death.
Is this accumulation of plasmids an inevitable part of evolution's scheme to kill us off when our breeding days are over?
If so, will the fabled Fountain of Youth be simply an eradicator of nomadic DNA?
Chemists finally crack the alkane bond
SATURATED hydrocarbons (alkanes) such as methane and ethane contain the strongest known carbon-hydrogen bonds; bonds that are notoriously difficult to break in a controlled way.
Now chemists have discovered several compounds of transition metals such as tungsten and iridium that can split an alkane carbon-hydrogen bond to form a carbon-metal bond instead.
Once attached to a metal, an alkane is ‘activated’: it is easier to convert into other useful organic chemicals.
The abundance of alkanes makes them particularly attractive starting materials for industrial processes.
The enormous stocks of methane under the North sea are mainly used for fuel — natural gas.
Industry does convert some methane into methanol by first mixing it with water and using high temperatures to turn this mixture into hydrogen and carbon monoxide.
An ICI catalyst converts the latter to methanol.
A catalyst that could convert methane to methanol or larger unsaturated hydrocarbons directly and at low temperatures would be of enormous commercial interest.
Malcolm Green, an Oxford chemist, speaking at a symposium in London, recently described a tungsten compound that will react with certain carbon hydrogen bonds.
Green's molecules consists of a central tungsten atom to which are attached two five-atom carbon rings (cyclopentane molecules) and two hydrogen atoms.
When the compound is subjected to light it absorbs energy and two hydrogen atoms are lost (elimination).
This leaves a very reactive molecule that can interact with carbon-hydrogen bonds in some chemicals.
The compound will not however react with any simple alkanes.
Another chemist, Robert Bergman from the University of California in Berkeley, has made an organic derivative of iridium that can also split a carbon-hydrogen bond.
Like Green's compound, this iridium compound contains two hydrogen atoms that can be eliminated by light.
The reactive species thus formed will ‘insert’ into a carbon-hydrogen bond — breaking the original bond and replacing it with a carbon-metal and a carbon-hydrogen bond.
This reaction works with a variety of alkanes, including propane, cyclohexane and cyclopropane.
Bergman used his iridium compound in a sequence of reactions to demonstrate how to convert an alkane into an iodine derivative called an alkyl iodide.
Organic chemists can change alkyl iodides into almost any other products they want.
Although these transition metal compounds will activate carbon-hydrogen bonds, they do not behave catalytically, and so are of limited practical value.
A catalyst facilitates a chemical process without being used up or changed itself.
Bergman's iridium compound, for example, is itself chemically altered during the alkyl iodode reaction.
It must be reformed before it will carry out the reaction again, thus making the reaction uneconomical as an industrial process (particularly as iridium is very expensive).
One of the chemists' major aims is to discover why these particular compounds work.
Armed with this information they can design molecules that are even better at the job.
Robin Perutz, from the Inorganic Chemistry Laboratory in Oxford, has looked at the reactive molecule formed when Green's compound is irradiated with light.
He used special low temperature techniques to freeze the intermediate in solid argon at -250°C.
He discovered it has two electrons that could be described as ready and waiting to form chemical bonds.
This type of electronic structure may be a prerequisite for a good alkane activator.
So far no-one has  succeded in activating methane, but if the current rate of progress is maintained that feat must be just around the corner.
TECHNOLOGY
Dumping: Britain faces a cul-de-sac
THIS WEEK'S meeting of the London Dumping Convention brings into the spotlight the problem of dumping nuclear waste at sea.
Yet the only waste that goes into the sea and which has sparked the dramatic protests by the Greenpeace organisation, has been ‘low-level’, or mildly-contaminated materials.
High and intermediate-level waste still waits in storage pools around the world.
This is because higher level waste is initially rich in short-lived isotopes which are highly radioactive.
Its potency declines rapidly, simplifying the problems of ultimate disposal.
Secondly no one has yet tackled the technical problems of finding suitable disposal sites on land or at sea.
The volume of high and intermediate-level waste is increasing rapidly.
Britain alone will have accumulated 600 cubic metres of vitrified high-level waste by 1990, and 2000 cubic metres by the turn of the century.
This quantity, coupled with the lack of recognised and acceptable ultimate disposal sites, is hampering nuclear power programmes in several countries.
Now more people are getting interested in  disposal at sea.
At first sight this option seems the political equivalent of the status quo with low-level waste.
Yet the situation is far from simple.
Disposal at sea is beset by legal ambiguities which offer toeholds for political campaigners.
The dumping of low-level waste is based upon the principle that dilution and dispersal will eliminate hazards.
In contrast, the general view is that disposing of high-level waste must involve isolation and containment.
Disposal at sea has clear advantages for isolation.
Specific sites away from shipping lanes, fishing grounds, submarine cables and so on are isolated in a way difficult to achieve on land.
But disposal sites would have to avoid areas containing resources such as manganese nodules which might be exploited in the future.
Containment would be necessary For tens of thousands of years.
Sites must be chosen for geological stability, and oceanographic and biological inactivity.
From a geological viewpoint using the terminology of plate tectonics, sites must be away from spreading centres, volcanic hotspots and subduction zones.
They should also be sufficiently distant from the continental rise and slope to be safe from ‘slumping’.
This limits suitable sites to the centre of oceans.
Sites would also need to be chosen so that, if wastes did escape, currents would not carry radioactive material too swiftly into zones of human activity.
One key element of ocean currents is the existence of patterns of regional circulation known as ‘gyres’.
Mobility would be at a minimum in the ‘dead’ centres of these patterns.
So the ideal site would be mid-plate, mid-gyre. sedimentation in these places is extremely low, especially in areas below 4 km.
Biological productivity is also very low; so mid-plate, mid-gyre areas are virtually ocean deserts.
The mechanism for containment is the multiple barrier.
For disposal under the seabed, the barriers would be: the waste form itself (vitrified or otherwise); the cannister, the sediment cover and the water column.
The waste form and the cannister should act as barriers for 1000 years each.
The sediment would allow diffusion at a rate of only I metre per 10 years, and ocean circulation would take 100 years to introduce radionuclides into the human environment once radionuclides enter the water column.
Thus the single most important element in the multiple barrier is the sediment cover.
This will form, in terms of delaying release, nearly 100 per cent of the barrier.
It will be enhanced if the containing clay absorbs free radionuclides migrating through it.
Apart from Sweden, Britain alone is studying disposal on the seabed.
Such an approach moves away from the concept of the multiple barrier, substituting isolation and temporary containment followed by dilution and dispersal.
Clearly this approach removes the distinction between the disposal of low-level and high-level wastes.
Instead it transforms the problem into one of concern for the total burden that waste imposes on the oceans, rather than treating low, intermediate and high-level wastes as distinct problems.
Technically, a spectrum of options exists.
For disposal under the seabed the two that scientists are investigating are drilled emplacements and the use of ‘freefall penetrometers’.
These are rocket-shaped projectiles loaded with vitrified waste, and dumped from ships to plunge into soft sediments on the ocean floor.
They are planned to be 1 metre in diameter, 12 metres long, weigh 15 tonnes and have a terminal velocity fast enough to plunge 20 metres into clay.
A variation on the theme is to fire the penetrometers into the ocean floor from just above the surface.
The penetrometer would probably rule out disposal sites within 40o of the poles, because in these areas, ice-rafted boulders could create a hazard.
But penetrometers would be comparatively cheap.
At the other extreme of cost is drilled emplacement.
Coring equipment would drill into lithified sediment to a depth of between 200 and 800 metres.
Each I metre diameter shaft would have waste in 10-metre long containers placed in it, separated by similar lengths of backfill.
In contrast to the other option, it might be possible to monitor and recover the waste.
However, the emplacement vessel or platform would need to be highly sophisticated — perhaps a larger version of the Glomar Explorer.
For disposal on the seabed, waste would be loaded into large concrete vaults.
The level of technology required would be similar to that of drilled emplacement.
Whichever technical option the nuclear industry goes for, the resulting operation will involve both ‘big’ and ‘high’technology.
Drilled emplacement would require massive floating plantships or platforms displacing hundreds of thousands of tonnes, and situated hundreds of kilometres from land.
If these are analogous with the vessels proposed for mining manganese nodules, the capital costs of each operation are likely to run toward $1000 million.
And a number of these plantships would be required.
If the present government's policies continue, it is likely that private companies will run these operations.
The announcement recently that Wimpey Laboratories and G-C Engineers and Consultants are to establish a joint venture company to deal with nuclear waste indicates that private industry sees a role for itself.
The manager of the new company, Wimpey-Gilbert, said he believed it had to start building a business now in order to address future problems of dismantling nuclear plants and disposing of waste.
It is likely that Wimpey-Gilbert will not be the only company to enter this field in the next few years.
But it is possible that all this evaluation of sites and techniques may be wasted.
For the work so far has limited itself to scientific and technical problems, neglecting political and legal issues.
The final decisions will have to take these into account.
Sites will almost inevitably be in international waters outside the 200-mile (390 km) exclusive economic zone and the continental shelf.
Non-nuclear states in the south Pacific, such as Kiribati and its neighbours, are already making their voices heard on the subject of low-level waste dumping, and are beginning to see how the trends are going.
The 1972 London Dumping Convention, to which Britain is party, is not clear on the legality of burying waste under the seabed.
Kiribati and Nauru are seeking an amendment which would ban the dumping at sea of all radioactive waste.
Although this amendment would not apply to any method other than dumping (such as penetrometers or drilled emplacement), it would form an unwelcome precedent for those who look to the ocean as a solution to their problems of managing waste.
If Britain is not to end up in a technological cul-de-sac, with a technical solution to managing nuclear waste confronted by massive political opposition, it must tackle a number of problems.
First, it must present technical data on safety in a way that will encourage public debate.
Secondly, it must evaluate the legal and institutional problems.
For example, the issues of monitoring and possible recovery would prove politically and legally vital.
If these were to be conceded, they would rule out a number of technical options.
Thirdly, dumping on the seabed requires international agreement on the classification of nuclear waste.
The prospects of achieving such a change must decide whether seabed dumping is possible.
HOW an oil platform died
A SERIES of freak events which overwhelmed an inadequately-trained crew caused the wreck last February of the US offshore drilling rig Ocean Ranger.
This was the conclusion last week of a panel of the National Transportation safety Board.
Ocean Ranger lay at anchor east of Newfoundland on 15 February 1982 when the port bow chain locker and upper hull flooded during a storm.
It overturned and sank, killing 84 crew members.
The flooding followed a 10–15 o list to port, due to water pouring into the forward ballast tanks.
The crew lost control of the ballast system after seawater splashing through broken portlights washed over the ballast console and caused it to malfunction.
According to the board, Ocean Ranger was structurally sound and should have been able to weather the storm.
However, contrary to the rig's operating manual, the centre ballast tanks had been kept full instead of empty, resulting in a ‘condition in which a 10–15 o list could develop rapidly’, the board said.
It also noted that the master of the Ocean Ranger was not sufficiently prepared when he took over two weeks before the disaster, and that the operating manual contained too little information on how to use the manual backup to the ballast system.
And neither the rig nor the standby rescue vessels were adequately equipped to handle rescues in adverse weather, or protect crewmen against hypothermia.
The board calls for reforms by the US coast guard, Ocean Drilling and Exploration Company, owner of the rig, and Mobil Oil of Canada, for whom the Ocean Ranger was drilling.
These include tightening coast guard regulations for mobile offshore drilling rigs; revising operating manuals; improving protective equipment on board and on standby vessels; and installing permanent pumps to drain chain lockers on all new and existing rigs.
Solar cells by the million
MATUSHITA of Japan has found a way of printing solar cell material directly onto a glass base.
This will cut the cost of solar cells by more than half, and increase their ability to convert light to electricity.
The company says the process could sidestep some of the problems of conventional solar cells, such as cost and low efficiency.
Conventional solar cells rely on silicon crystals, and energy conversion is around 10 per cent.
Capital cost is nearly £7 for every watt of electrical energy to be produced by solar radiation.
The new technique developed by Matsushita relies on screen printing and brings the mass production cost per watt down to under £3.
Energy conversion is 13 per cent.
A cadmium sulphide paste is printed through a screen onto a glass substrate.
After drying and sintering at high temperature in a nitrogen atmosphere, another layer of cadmium and tellurium is sintered on, followed by a topping of carbon.
Silver electrodes are then overlaid by another printing stage and the material sold in sheets up to 30 cm square.
Matsushita says its new material can power road signs, radio relay stations, irrigation pumps and outdoor clocks.
It will also run domestic appliances in areas of Japan with no mains supply.
Green light for ‘plastic sand’
A PLASTIC grain that could stop deserts spreading has passed its first field trial.
The grain is a polyacrylamide called Agrosoke, which can absorb 40 times its own weight in water.
Its British inventor, Allan Cooke, believes that a mixture of the polymer and sand should help plants to grow in arid areas, preventing soil erosion (New Scientist , vol 95, p 305).
Last week an Egyptian research centre reported that the idea works in a real desert.
Scientists at the Agricultural Research Experimental Station in Ismailia carried out six months of tests with the polymer.
They found that mixing each cubic metre of soil with 3 kg of Agrosoke reduced by 65 per cent the amount of water needed to grow sunflower plants.
And the plants grown in polymer were larger and had more leaves than those in control soils.
Cooke hopes these research findings — along with work carried out at universities near his company's base in Cheshire — will persuade other governments in arid areas to try polymers.
It is good news for his firm, Chemical Discoveries; ‘But perhaps more importantly it will enhance our ambitions for innovating large-scale land reclamation projects to benefit underdeveloped countries.
Monorail metro has a new hang-up
A NEW TYPE of railway could cut by half the cost of new metros to relieve our crowded city streets.
The Flydaway is an extremely simple concept.
Instead of being supported by rails like a conventional train, Flydaway carriages hang off the side of an elevated track.
The system is crudely a cross between a monorail and a conventional railway — a duorail.
Like a monorail it relies on elevated track, but in principle it is much like a train on the Paris metro, except the track is vertical rather than horizontal.
The Flydaway was invented by Commander Francis Perrott, a retired naval engineer.
After he retired from the navy, Perrott was the research manager at a turbine firm.
He has now set up a British company, Flyda Ltd, to exploit his invention.
The company hopes to have a full-scale prototype working by the end of the year.
The heart of the system is its guidance wheels.
The main support for the
‘Flydacraft’, comes from a wheel running on a horizontal ledge.
The trains are prevented from toppling over by another wheel which runs against a vertical concrete track.
The principle is essentially the same as the support for a cupboard on a wall — its main support comes from a horizontal batten and simple screws keep it vertical.
The Flydaway also has a new system of switching trains from track to track, one of the traditional difficulties with monorails.
Train switching is controlled on board the trains themselves.
As the train approaches a junction, a second bottom girder supports the train on both sides.
The overhead track then stops, so the train runs on just the horizontal bottom supports.
A guidewheel on the top of the vehicles, controlled from the cab, moves into the appropriate slot in a ‘V’ -shaped track, and switches the train to right or left, according to the driver's wish.
This system has the advantage that there are no moving parts on the track.
So the maintenance costs are incurred on the vehicle itself, and not the track.
And in the event of a failure the vehicle can be taken out of service, which should increase the reliability of the system.
The main advantage of the Flydaw dramatic reduction in the capital cost of construction.
The two lines can be ‘hung’ on either side of a girder, which is a beam with an ‘I’cross-section.
This beam is then supported by concrete pillars, with spans of up to 50 metres.
The cost of the track alone, excluding signalling and electrical equipment and land cost, should be in the region of £1½ to £2½ million per kilometre.
Flyda says that the cost of Flydaway should be about one tenth that of a conventional underground line, and roughly 40 per cent of elevated systems.
This cost advantage is unlikely to be maintained over systems that can be run at street level.
The initial concept for the Flydaway envisages carriages carrying 5, 12, or 20 people.
The carriages are smaller than most public transport vehicles, with the largest carriage having a width of 2.2 metres, only slightly wider than a bus.
But the carriages will be able to operate in trains, giving a theoretical maximum capacity of up to 30 000 passengers per hour, with 120-metre long trains.
This gives the Flydaway a maximum capacity about the working capacity of an underground line.
(Although these lines are capable of carrying up to 60 000 passengers per hour in one direction.)
Because of its small units the Flydaway also has the theoretical advantage of being able to operate at relatively high frequencies (one minute headways or less) even when trade is slack — although this depends on fully automatic driverless trains.
Flyda claims that the extremely light construction of the track, which will be only 1½ to 2 metres wide, will make the system unobtrusive and environmentally acceptable.
However, there was a row over proposals to put minitrams into the centre of Sheffield 10 years ago.
Technically the Flydaway has much the same characteristics as a tram.
It can climb hills of 1:8, and round bends of just 10 metres in radius.
Despite its low capital costs, the opportunities for such a system are few in Britain.
The last big metro system in Britain was in Tyne and Wear.
This is due to be finished this year and has cost nearly £300 million.
On Thursday, Haringey Council in London is considering a report on using the Flydaway to link Alexandra Palace with the local British Rail station.
The only other opportunity in the foreseeable future is likely to be the new light railway in London's Docklands.
The London Docklands Development Corporation favours an entirely automatic system, such as Flydaway.
But the final decision on which system will operate this £65 million link could well favour an established technology such as a tram.
Engines with a head of glass
JAPANESE engineers claim to be on the verge of building the world's most efficient Stirling, or heat’ exchange, engine.
The secret says Asahi Glass, is to built it out of ceramics.
A Scottish clergyman, Robert Stirling invented the principle of the heat-exchange engine in 1815.
Because it works by external combustion, the engine could in theory power a pollution-free vehicle and run on almost any fuel.
But because of relatively low efficiency, it was  superseded by the internal combustion engine at the  beginning of the century.
The past 10 years have seen a revival in interest.
Government and commercial organisations in the US, West Germany and Japan are all developing Stirling engines.
Philips in the Netherlands has claimed an efficiency of 30 per cent with an engine it built from conventional materials.
Asahi, however, is aiming for at least 37 per cent— roughly that of an internal combustion engine.
The pistons and cylinder head of the 3600 cc prototype are made of fine ceramics.
The company says it will be running some time this year, after more than three years' work.
Modern ceramics such as carbon silicon and nitrogen silicon are almost as strong as steel, and conduct far less heat.
Companies in Japan, Britain and the US are developing conventional diesel engines with ceramic parts.
Such engines would be able to run at very high temperatures, with high efficiencies — but the difference between the expansion rates of metal and ceramic components is a big obstacle.
Asahi is not saying how — or if — it has tackled the problem.
Home video goes hi-fi and auto-reverse
HARDLY a week has gone by in recent months without some new development in home video.
The latest is Sony's plan to introduce a hi-fi modification of its Beta home video system.
Beta Hi Fi recorders will appear first in the US, this summer.
Films with stereo soundtracks, such as Apocalypse now, 2001 and Superman will be the first to appear.
The new system will be compatible with existing equipment, because the new recorders also record and replay conventional soundtracks.
Domestic video recorders have poor sound because the edge track runs at only two centimetres per second.
This is slower than an audio cassette recording.
Beta Hi Fi converts stereo sound into a frequency modulated signal interleaved with the ordinary FM video signal.
The rotating heads record and replay both sound and vision signals.
The frequency response, at 20 hertz to 20 kHz, is twice the range of a conventional domestic video recorder.
And dynamic range, the difference between the loudest and quietest sounds, is 60 decibels, compared with 40 dB in today's recorders.
One lucky spin-off from the system is that it reproduces intelligible sound at normal pitch, when the tape is played back at twice the intended speed.
So a Beta Hi Fi recorder allows someone to scan quickly through a boring interview (or a soap opera).
In addition to feature films, Sony will also release music tapes by Paul McCartney, Elton John and Blondie.
Some of these will be short tapes, which the company sees as the future equivalent to 45 rpm audio discs.
Sony's tape plant in Alabama is already equipped with hi-fi duplicating machines, and is making a stockpile of tapes for the summer launch.
Rival systems, meanwhile, are split on whether to follow suit with a similar system called VHS-Hi Fi.
Matsushita, which makes National Panasonic equipment, has made the system work.
But JVC, the subsidiary company that invented VHS, is not yet happy with it.
Meanwhile in West Germany, Grundig has finally succeeded in building a video recorder with auto-reverse.
It is the first of its kind in the world.
The V2000 format, which Grundig developed jointly with Philips, will record for four hours in each direction.
This is possible because only half of the tape's width travels past the heads on each run.
Normally, the owner must take the cassette out of the machine by hand, and flip it over to record on the other side.
But after two years of struggling, engineers at Grundig have developed a mechanism that manages the same trick while the cassette stays in place.
The auto-reverse recorder, which will go on sale this summer, has an extra pair of video heads on the rotating video drum.
As the tape makes its first pass, the first pair of heads scans half the tape.
When the full length of tape has been used, the drive stops, and starts again in the opposite direction.
At the same time, the video head drum, which rotates 1500 times a minute, stops dead and starts spinning at the same speed in the opposite direction.
The extra pair of heads, now connected, traces the other half of the tape's width.
It sounds simple.
But Grundig had to provide quite separate sound heads, and individual capstans to drive the tape.
The picture and sound disappear for two seconds while the recorder goes into reverse.
The total playing and recording time: eight hours, long enough to record a string of television programmes.
The reversing machine even has a speech synthesiser to tell the owner how to operate it.
Plant silencer
DESIGNERS of noisy factories, and power stations should have no excuse if their creations keep people awake.
Researchers at Batelle's Columbus Laboratories in Ohio have developed a set of microcomputer programs that allow engineering firms to discover exactly how compressors, pumps, oil flares and so on will sound to those living near to them.
The programs, which cost $500 and run on an Apple II computer, compute the direction and level of sound coming from anything up to 15 different sources at selected points on the ground.
Each noise is split into different frequency bands in the 20 hertz to 20 kHz range (equivalent to the range of the human ear), and data about each band is fed into the system.
Background noises can be added for maximum realism.
Results are printed out either as combined sound levels in one place or in the form of a contour map showing the decibel levels
Space sheiks?
THE OIL-RICH Arab states should examine the space scene as the next area of commerce into which to pour petrodollars.
In particular, the area around the Arabian Gulf is an ideal site for a space launch centre that could, ultimately, rival anything in the developed world.
So says Paul Gretton-Watson, a computer consultant from the British firm Scicon.
Writing in the American magazine Astronautics and Aeronautics , Gretton-Watson suggests that the huge volume of natural gas from the Gulf states' oil fields could be turned into liquid hydrogen, an important fuel in high energy rockets.
Every year, says Gretton-Watson, the Gulf states flare off gas equivalent to 92 million tonnes of oil, wasting 42 gigawatts of heat.
The energy source is not the only reason for siting a spaceport in this part of the Middle East; relatively few people live there, so the risk of accidents causing large numbers of deaths or injuries is low.
Black gold
MOZAMBIQUE could be about to join the ranks of the world's oil-producing nations.
The government has announced the opening of the first tender for exploration on its continental shelf.
Mozambique's coastline, of 2000 kilometres, is one of its main assets: it gives the country one of the widest continental shelf areas in Africa.
Surveys conducted on the shelf over the past year suggest that, contrary to surveys in the 1950s and 1960s, hydrocarbons should be there.
But with the present depressed state of the market, any oil may stay underground.
REVIEW
Albert Einstein is a real human being
Paul Davis
Subtle is the Lord by Abraham Paris, Oxford UP, pp 552
HOW CAN one grasp the enormity of Einstein's intellect?
All great scientists seem remote, extra human even.
But Albert Einstein has been deified by the scientific community and society at large.
In the guise of a superbrain, a mind that commanded the most complex and abstract aspects of physical science, he is portrayed as with out equal among scientists.
There were two ways of solving a scientific puzzle, they said.
The first was to use mathematics, the second was to ask Einstein.
The essential folklore is well known.
The young Einstein was unable to find an academic position, went to work in the Bern patent office, and in one incredible year (1905), at the age of 26, wrote a number of research papers that laid the foundations of no less than three major branches of modern physics — statistical mechanics, the quantum theory and special relativity.
Ten years later he produced singlehanded his monumental theory of gravity — the general theory of relativity.
In his later life he wrestled, unsuccessfully, with the idea of a united field theory, and clashed with Niels Bohr over the scope and meaning of quantum mechanics.
He gave the world the bomb — and tried to ban it.
He espoused a variety of scientific, social and political causes, some sound, some controversial, some downright cranky.
He was a legend in his own lifetime.
When Einstein's centenary came along in 1979 there was a flurry of publishing activity.
Several Einstein books appeared, some of the I-knew-the-great-man-personally variety, others of a more scientific nature.
On receiving Abraham Paris's book, I wondered how the market could accommodate yet another.
Having read it, I am convinced it is by far the most important study of both the man and the scientist.
Paris writes with an authority and fastidiousness that is breathtaking.
His research must have been immense.
All Einstein's works, great and small, are discussed and documented in meticulous detail.
In addition, letters to friends, records, certificates, documents, family anecdotes — even occasional remarks — are assembled as part of an elaborate mosaic that actually makes Einstein come alive as a real human being.
He got excited and depressed, dodged the army, fell in and out of love, occasionally suffered ill-health and played the violin.
But he was undeniably withdrawn and pensive.
Though we find him joining the cut and thrust of conference debate, or delivering polemical lectures, we also glimpse a man apart, and in an intellect of such stature, it is tempting to describe him as aloof.
Yet there is no trace of arrogance in his writings or pronouncements, and for those who could track him down, there was a man who was more than ready to mull over the ideas of lesser mortals.
Pais weaves together the strands of Einstein's life skillfully and objectively.
If I am to find a criticism, it is in the rather erratic organisation of the material.
The book is divided into four main blocks, each dealing with one of the four principal contributions that Einstein made to physical theory — statistical mechanics, special relativity, general relativity and the quantum theory.
Unfortunately for chronology, these pursuits were in some cases contemporary with each other, which means that the author has to jump back and forth between different epochs of Einstein's life.
This is done methodically, with elaborate cross referencing, but nevertheless fragments Einstein's personal history and diminishes the reader's empathy.
The reason for this curious approach is the author's intention to fuse two books into one.
Here, in the same volume, is both a sensitive and exhaustive life story, and a balanced, detailed and somewhat formal presentation of hard science.
Messy though this arrangement may be, I think that, on balance, it is unavoidable, for one cannot in reality separate the scientist from the man.
Although in his later years, Einstein was to involve himself more and more in social and political matters — especially pacifism and Zionism — he was fundamentally a theoretical  physicist through and through.
And it was, after all, his science that made him what he was.
During the 1930s, Einstein's resolute distrust of quantum mechanics became crystallised in some famous exchanges with Bohr and others.
The contention concerned whether quantum mechanics with its indeterminism and probabilistic element, was the most complete description of nature that could be attained.
Einstein fervently believed that quantum mechanics, if not inconsistent, was certainly incomplete.
It is ironical that the experimental evidence of the recent past suggests that Einstein's position on quantum mechanics can be sustained only if one is prepared to relinquish his special theory of relativity.
I found the sections in Pais's book dealing with these topics disappointingly terse and dry.
The quantum challenge is probably the last of Einstein's contributions to fundamental physics that has enduring value.
His final years were spent in a search for a supertheory approached from a perspective that was rapidly being overtaken by events.
Pais chronicles the final moments with his characteristic precision.
Part of Einstein's brain lies, grotesquely pickled, somewhere in Princeton.
Will the world ever see the likes of such a man again?
From exhilaration to discouragement
How to grow science by Michael Moravcsik,Universe, pp 206, £8.80 
John Ziman
‘SCIENCE is something that people do…not just an abstract set of laws.’
The direct and humane tone of the first paragraph is sustained throughout.
Professor Moravcsik is addressing himself to a vast audience, from high school students to politically responsible adults.
He explains what science is, why and how it is made, and how it should be fostered in developing countries.
In every chapter, on almost every page, a popular misconception about science is patiently exposed and put right.
Although the picture he gives is over-simplified it is thought out clearly and could scarcely be presented more lucidly or vividly.
For example, his account of the nine different reasons why two scientists might disagree could go verbatim into any elementary course on the social relations of science, whilst the extended analogy between ‘growing science’ and establishing an orchard should be required reading for every would-be science politician in any nation that aspires to modernity.
He is particularly good on the communication processes of science.
Nevertheless, despite much wisdom, and many excellent passages, I would not go along with this book in numerous details, nor in its general attitude.
For example, his typical scientific ‘problem’— how many icebergs would be needed to supply Los Angeles with water — is merely a schoolchild's exercise in numerical estimation, without any research content.
Again, he greatly underplays the ‘moments of discouragement’ that alternate with ‘periods of exhilaration’in a research career.
Like most physicists, he is also too sympathetic to quantitative arguments in the ‘science of science’, by comparison with what can be learnt from qualitative studies.
Where this book mainly fails is in its implied answer to the question: what sort of science is to be grown?
Science is a genus with many species and varieties.
The author insists at several points that he is talking about both basic and applied research, although he would exclude technology.
Yet his conception of science is narrowly academic.
He seldom moves away from the viewpoint of a professor in a well-established university, whose vocation is not only its own reward but also deserves public support for its ultimate benefits to humanity.
He seems almost unaware of the major part of science, which is undertaken by professional employees of large-scale governmental and industrial institutions and is directed toward specific goals of benefit, profit, or power.
He is absolutely right to be scornful of the pretentious organisational plans that often pass for science policy in developing countries.
But he simply does not come to grips with the genuine political and cultural difficulty of establishing effective institutions for research in applied sciences, such as agriculture and medicine, which cannot be seeded entirely by individual commitment and talent.
Above all, he seems to see only good in science, and ignores the dark side, where it has direct connections with exploitation, tyranny, and war.
These connections cannot all be dismissed as technological misapplications of innocent accidental discovery.
Alas, there are some soils on which science grows all too rampant yielding profuse harvests of bitter fruits.
Reformulating evolution
Problems of phytogenetic reconstruction by K. Joysey and A. Friday,Academic, pp 440, £31.40 
Tom Kemp
THE SYMPOSIUM on  phylogenetic construction held in Cambridge in 1980 was novel in one sense, for it consisted entirely of local rather than transatlantic talent, although a number of familiar Americans were certainly present in spirit.
It is a pity that this book contains only half (11) of the papers presented at the symposium, the rest having appeared in the Zoological Journal of the Linnean Society , apparently for logistic rather than logical reasons.
Something of the underlying flavour of the symposium has been inevitably lost, and unfortunately some of the more provocative papers are not in my present remit.
In the past few years, many systematists have been trying to improve the scientific rigour of their subject, mostly by reformulating it in accord with Popperian-type hypothetico-deductive methods.
In this context, we now see the problems of systematics condensing around two major, interrelated questions, which form the two main themes running through most of the papers in this book.
The first question concerns the extent to which preconceptions about evolution should be allowed to influence the creation of classifications.
At one extreme, the ‘transformed cladists’, or ‘natural order systematists’, argue that classification should be based solely upon the perceived patterns of distribution of characters of organisms.
P. L. Forey offers us a somewhat smug polemic directly on this view (’ In itself the scenario is harmless’; ‘the cause of that (taxonomic) pattern if it is considered desirable to know it’etc).
Colin Patterson on the other hand writes what is for me the most scholarly paper in the volume, pointing out that, from the transformed cladist point of view, the much discussed and abused term ‘homology’ actually refers to those characters that define natural groups of organisms and need have no evolutionary connotations.
It strikes me that the transformed cladists have changed little except the words.
For ‘derived character’ or ‘synapomorphy’read ‘homology’; for ‘evolutionary parsimony’read ‘maximal congruence’; for ‘hypothetical ancestor’read ‘morphotype’, and so on.
The logical processes for recognising ‘nested natural groups’ do not differ in form from the more sensible logical processes applied to working out evolutionary relationships.
This much is implied in C. R. Hill and P. R. Crane's (excessively) long discussion of angiosperm relationships.
A. J. Charig however, in the course of his detailed comparison of the main schools of systematics, sees natural order systematics as rather more of a bete noire than it deserves.
The second main theme in the book is the question of the extent to which fossils can contribute data of value to systematics, and it is, of course intimately entwined with the first question.
We see C. R. C. Paul discussing various methods of assessing the degree of completeness of the fossil record, and concluding that ‘we ignore the fossil record at our peril’, on one hand.
On the other we have Forey asserting that ‘Fossils have clouded rather than clarified our attempts to reconstruct phylogeny.’
The last word on this particular problem must, however, go to R. A. Fortey and R. P. S. Jefferies.
Having looked at a number of hypothetical fossil records varying in degree of completeness, they state that ‘Sometimes fossils can be used in phytogenetic reconstruction without reference to stratigraphy: sometimes they can be used in combination with stratigraphy: sometimes they cannot be used at all; and sometimes phylogeny is in principle not  reconstructible .’
Hard to disagree.
It is a pity that the editors have not provided anything in the way of an introduction, links between contributions, or even subdivision of the papers.
People who are not avid readers of Systematic Zoology and like journals that discuss such matters may find this volume a formidable prospect.
They may also find one or two of the papers unnecessarily long for the points they convey.
Nevertheless, there is much good stuff, and it represents a fair view of the present state of the phytogenetic art.
Technological marvels on the move
John Moss
THERE was a time, I suppose, when you could expect to carry the essentials of modern technological developments in your head, and retrieve them pretty quickly and accurately.
But, such is the pace of development, that an idiot's guide to many of the most recent and fast-moving technological marvels, which often bristle with abbreviations, is not to be despised.
On the face of it, the most useful of the three latest guides is The timetable of technology (Michael Joseph, pp 240, £12.95).
Here the editors, Kenneth Baker and Michael Marshall, set each of the amazing achievements of this century in its historical context by timecharts.
But in practice.
this appears as a string of paragraphs, apparently taken from worthy enough technological publications but with little relation to time or place.
It is impossible to place them in context so they are virtually worthless.
Inside modern technology: how it works (Sidgwick & Jackson, pp 185, £9.95) describes the technologies of the 1980s and beyond.
It provides a quick flip around hang gliders, nuclear reactors and instant picture systems in pretty random order, but again there is no recognisable pattern of reference.
Ken Marsh's The way the new technology works (Century, pp 240. £6.95) is potentially the most useful of the three, because of its narrower field.
But for me it fails to provide an easily understandable route to the basic explanations which are lurking there wrapped up in just too much verbiage to be really accessible.
A backdrop of intrigue
The atomic complex Bertrand Goldschmidt,ANS*, pp 479, $31, pbk $24 
Ian Fells
BERTRAND Goldschmidt was appointed Marie Currie's personal assistant in 1933.
After the Nazi occupation of France in 1940 he escaped to the United States where Enrico Fermi and Leo Szilard tried to recruit him to their team at Columbia but were prevented because Washington at that time did not recognise the Free French forces.
Now attached to the British team, he was sent to join the ‘Metallurgical Project’ in Chicago where he worked with Glenn Seaborg on the chemical extraction of plutonium.
A team of over 100 scientists was working to produce a weapon of almost inconceivable power, ‘their moral scruples overcome by the fascination of the research and the haunting fear that the Germans were following the same path’.
In 1942, the British team led by Hans Halban was transferred to Chalk River in Canada where a British-designed pilot uranium isotope separation plant was to be built.
Goldschmidt was transferred to this group.
But by now the American work was going well and President Roosevelt decided that future exchanges with Britain should be kept to a minimum.
The British were dismayed but a further memorandum from the Americans in January 1943 was virtually a declaration of dissociation.
‘Since neither the Canadian or the British governments are in a position to produce the element ‘49’(code name for plutonium) or ‘25’(code name for uranium-235) our interchange has been correspondingly restricted by order from the top.’
The attempt by the Americans to retain control over both military and industrial use of atomic energy, doomed to failure as it was, runs like a thread through this fascinating memoir.
Goldschmidt focuses a penetrating but disinterested eye on this stage of the drama; only when the French decide to develop their own bomb and civil nuclear power programme does his patriotism and pride become engaged but even then his Gallic objectivity is never impaired.
He retails, from personal experience, the development of nuclear explosives and nuclear energy over nearly half a century of political moves, counter moves, international intrigue and manipulation against the personal backdrop of world leaders — from Roosevelt, Truman, Churchill, Joliot-Curie and General Groves to Gaddafi, Hussein, Carter, Reagan and Mitterrand.
Together — with Pierre Auger and Jules Gueron he told General de Gaulle of the weapon that was being developed a year before it was used in 1945 — the General referred to this ‘apocalyptic undertaking’ in his memoirs.
After the war Goldschmidt played a central role in the development of the French nuclear power programme and he has some complimentary, uncomplimentary and candid things to say about the British civil nuclear power programme.
He admires the foresight and thrust of the initial decision in February 1955 to achieve an output of 2000 MW(e) by 1965 and this taken before the reactor (Calder Hall) was completed.
Christopher Hinton he likens to Admiral Rickover; both men had the valuable and unusual ability to provide the essential working  liaison between laboratory and industry.
Hinton was the midwife of the British Magnox programme in the same way that Rickover developed the pressurised water reactor for submarine propulsion, which in turn paved the way for the now dominant pressurised water reactor (PWR).
There is an interesting section on the Russian nuclear programme which is also based on pressurised water reactors.
The French civil power programme was based upon carbon dioxide cooled, graphite moderated, natural uranium fuelled reactors, very similar to the British Magnox stations.
The earlier stations were not as successful as their British counterparts but later designs were thought to be an improvement.
Despite General de Gaulle's go-ahead to build twin units of the new design at Fessenheim in 1968, they were never built and in 1970 PWRs under the Westinghouse licence were ordered.
The decision to abandon French technology and buy American was influenced by the success of the French PWR submarine reactor; falling oil prices which allegedly made the French stations uneconomic; student unrest; General de Gaulle's departure from office in 1969; and dissatisfaction with the performance of the French nuclear industry as well as political infighting.
The author charts this progress and compares it with the British decision to choose the advanced gas cooled reactor (AGR) in 1965.
In a section entitled ‘The British decline’ he criticises the construction of essentially five prototype AGRs, the industrial arrangement of five, later three and now one construction consortia, and the abortive reassessment of reactor choice in 1972 where protagonists of the steam generating heavy water compared it favourably with AGRs and PWRs.
There must be lessons here for those involved in the current debate over whether to build a PWR at Sizewell.
A good deal is said about nuclear proliferation and Goldschmidt staunchly defends France's uncompromising stance on trading in uranium.
The latest move to supply India, a country which exploded a nuclear device in 1974 with material produced from a research reactor, typifies France's ‘go it alone’ approach which has more than a hint of economic expediency in it.
France has certainly been consistent in this stance.
Through the early 1960s, it refused to join a common front of uranium export suppliers as long as it was treated differently by Canada from the United States and United Kingdom over sales of uranium.
These meetings took place every year until 1965, they usually took one day and the French participants used to call them ‘Cashmere meetings’ because, after voicing their disinclination to cooperate, they would leave and go to buy Scottish pullovers.
This is a fascinating memoir from a dedicated French scientist and politician.
A potpourri of microorganisms
Microbes, man and animals by Wan Linton,Wiley, pp 342, £19.50 
Bernard Dixon
JUDGING by its title and subtitle (’ The natural history of microbial interactions’), this is a much-needed work to fill a vacant lot.
While there have been papers and reviews aplenty, no single book has been available until now to embrace the microbial populations which flourish and decline, evolve and stagnate, as co-terrestrials with man and the other animals.
Alas, Dr Linton's book succeeds only partially in filling the gap.
Indeed, he did not set out to be comprehensive.
Rather he began with the course on pathogenicity and epidemiology taught at Bristol by the late Dr Anna Mayr-Harting (to whom the book is dedicated) and invited several colleagues to contribute essays in their fields of interest.
The resulting collection Linton introduces by way of a comment from Michael Montaigne: ‘I have gathered a bouquet of flowers from other men's garden's: naught but the string is my own.’
Such an approach bas inevitably produced a bitty product — becoming bittier as we move from the early sections, which deal systematically with host pathogen interactions, to the latter third of the book.
Why, for example, alongside single chapters on the zoonoses and on food-borne infections, should gonorrhoea and the epidemiology of brucellosis merit individual chapters of their own?
The fact that some of these essays (including an excellent review by Mary English of the epidemiology of fungal disease) are welcome in themselves, does not detract from that criticism.
If anything, it reinforces one's doubts as to how many purchasers in these hard times will be prepared to pay nearly £20 for something which is a potpourri, however delightful, rather than an essential, definitive tome.
Unevenness is also apparent in the topicality of Linton's anthological exercise.
One is struck rather forcibly, for example, by the fact that whereas L. W. Greenham's contribution on wound infection is sufficiently abreast of the literature to embrace one of last year's Communicable Disease Report from Colindale, the only three papers cited in the chapter on pathological interplays between hosts and parasites were published 18, 24 and 27 years ago.
Two of those references are to research by Professor Harry Smith and his colleagues in Birmingham — work which has certainly moved on during the intervening decades.
A second consequence of Alan Linton's biblical approach is that any one of his diseases and other topics is likely to crop up in several different parts of the same book.
Look up rabies, for example, and you are directed to three separate chapters; pre-exposure vaccination being mostly dealt with in one and post-exposure vaccination being mostly in another — with overlap but without cross references.
The same applies to influenza and many other infections.
All in all, one is left with a strong impression that the material could have been marshalled much more effectively.
Linton lists as his perceived audience undergraduates studying microbiology, together with both undergraduate and postgraduate students of medicine and veterinary science.
The publishers, on the dust jacket, add to this list teachers and students of community health.
I can imagine members of each of these fraternities getting something out of some parts of the book — and, by serendipitous sampling, discovering interesting titbits.
But whether this would justify any course planner, supervisor or head of department placing the title on an advisory reading list, I very much doubt.
A window to the world outside
Mind, brain and function edited by J. I. Biro and R. Shahan,Harvester, pp 232. £18.95 
Richard Gregory
AS LONG AGO as the 19th century, Franz Brentano argued that mind is essentially intentional, and that conscious states always require cognitively-held objects, even though these may not exist in the physical world.
In the past two decades, work on theoretical and practical aspects of artificial intelligence has stimulated our concepts of mind.
Such a stimulation accounts for the collection of technical philosophical essays gathered together by J. I. Biro and Robert Shahan in Mind, Brain and function .
The essays are not primarily concerned with consciousness, and indeed ‘consciousness’ does not appear in the index; they are, however, concerned with the relation of brain activity to mental processes, and to the status of psychological laws.
They are concerned with reductionism, and with what it is to be a ‘self’.
The 10 papers originally appeared in the newly-named journal Philosophical Topics , edited by Robert Shahan.
Most have interesting things to say, but why they appear in the order they do is explicable only on some deep theory of random numbers: the editors provide no justification for the selection or their order — or useful introductory remarks or guidelines.
A glossary of technical terms and abbreviations would have been useful, as would also a general bibliography.
The ad hoc collection of topics is saved from nonentity by the stature of the contributors; especially Daniel C. Dennett, who both contributes the interesting paper ‘Making sense of ourselves’ and whose important ideas on intentionality are well described and discussed by Stephen Stich, and by Paul and Patricia Churchland and Colin McGinn.
Dennett's major book Brainstorms (Harvester, 1978) is reviewed here, far less interestingly, by Robert Cummins.
John Searle is not represented directly, but the Churchlands challenge his cat-among-the pigeons argument, purporting to show that computers cannot in principle ever understand , though they can handle effectively symbols or tokens.
Searle presented his argument, which attacks the basic tenet of artificial intelligence that machines can in principle understand symbols and signalled events much as we do, in the form of an analogy in a much-quoted paper: ‘Minds, brains and programmes’(Behavioural and Brain Sciences , vol 3, p 417).
The analogy is his Chinese room, in which two Chinese-speaking people pass Chinese symbols backwards and forwards, in appropriate ways, watched by a non-Chinese speaking Westerner.
According to Searle, the Westerner can come to learn and appreciate the rules for the symbol handling but will never understand meanings of the Chinese symbols.
Searle asserts this, while accepting that we are machines.
He also believes that a Martian might be able to understand symbols (and so apparently assumes that the Martian is not going to be at all like any future (IBM computer).
The Churchlands suggest that Searle's criticism of functionalism is ‘profoundly in error’.
They argue, somewhat surprisingly, that it is a mistake to meet it by trying to ‘upgrade the imagined simulation in hopes of finally winning Searle's concession that at last its states have achieved intrinsic intentionality’.
The Churchlands suggest that the correct strategy is to argue that our own mental states are ‘just as innocent of intrinsic intentionality as are the states of any machine simulation’.
They go on to say that the notion of ‘intrinsic intentionality’ makes no more empirical sense than does the notion of position in absolute space.
There are important issues here.
Personally, I would think that Searle is not particularly concerned about ‘intrinsic intentionality’; but rather with what the machine or the organism as a whole can achieve, in making sense of the senses and of symbols.
It seems to me quite arbitrary to disallow man-made machines this ability, when it is accepted that men are machines, and that such an unknown machine as a Martian may also perceive meanings.
The issue is not the same as issues of consciousness, and fortunately is more amenable to clearly empirical considerations.
Certainly the Chinese people have to discover or learn acceptable meanings for the symbols of their language when they are children.
If this is supposed impossible for the Westerner (assuming he is not too old!) it must be because, and only because, he does not have access to information allowed the Chinese as children.
What is missing in the Chinese room is a window to the world outside.
Surely if associations could be discovered between the use of the symbols and sensed situations, then the Westerner would indeed come to learn Chinese — in the full sense.
This is exactly what language training for children or adults (rather than mere translation) succeeds in doing.
Searle's position can have cogency only if he can show that a computer which is allowed to look through the window (and preferably touch things as well) could not succeed, though in these conditions the Westerner would succeed.
But Searle does not show why the computer would not do as well as the Westerner, in conditions where he or it can learn meanings.
So surely his analogy in no way suggests his conclusion.
It is very curious that almost everybody accepts that Searle's argument is profoundly mistaken and yet there are rich differences of opinion as to why it is wrong!
So, perhaps philosophy even from the sidelines, is of some use, at least as a cognitive stimulant.
A noble rival to gold
A history of platinum and its allied metals by D. McDonald and L. Hunt,Johnson Matthey, pp 450, £20 
Marie Boas Hall
PLATINUM is a metal possessing many different connotations.
It has been used in jewellery on and off for nearly 1000 years, for crucibles in the laboratory and in industry for nearly 200 years, as a catalyst in industry for 150 years, in the electrical industry for 100 years and now, most recently, in modern metallurgy and medicine.
Indubitably a noble metal, non-tarnishing and long wearing, it proved difficult to refine and even more difficult to work in any quantity.
In the past it hovered uneasily between being a rival to gold in jewellery and coinage and being a highly versatile industrial material.
By way of contrast, the Spanish conquerors of northwest South America (modern Colombia), where the first deposits were found, regarded it as a dangerous adulterant of gold and silver and tried to prohibit its export — unavailingly, although its supply to European chemists was made capricious.
Although the pre-Inca inhabitants of Colombia and Ecuador worked the metal satisfactorily, apparently by primitive powder metallurgy, this was naturally in very small quantities.
It tenaciously defied 18th century chemists who struggled to prepare ‘malleable’ platinum in useful quantities, hindered by the fact that, although a noble metal, it combines with phosphorus and arsenic and is seldom found without an admixture of related metals.
In the late 18th century, various French chemists and goldsmiths, notably Janety, prepared platinum in useful quantities.
But it was William Wollaston's process that rendered ‘malleable’ platinum available in quantities large enough for crucibles in which to concentrate lead chamber sulphuric acid, later to be superseded by contact acid made with the aid of a platinum catalyst.
Progress in the 19th century lay in improving refining techniques, in finding new uses (especially in catalysis and electricity) and discovering new sources, principally in the Urals — after which for a time the Russians adopted platinum coinage.
In the 20th century, further deposits were discovered in Alaska in Siberia and in South Africa, this last by agents of Johnson Matthey, a firm founded in the early 19th century and which now controls the South African output.
This book is a complete history of platinum, with much also about the other metals of the platinum group, and an account of the technical achievements of the firm of Johnson Matthey.
The original edition in 1960, ended the story in 1880; the new edition, revised and rewritten by Dr Leslie Hunt, brings the story up to date in all senses, relying on original historical research by Hunt himself and by many contributors to Johnson Matthey's journal Platinum Metals Review (founded 1957), and on Hunt's knowledge of modern industry It is lavishly illustrated, partly in colour.
There is a great deal of interesting biographical material about past workers, and full details of the chemical and metallurgical problems involved.
Very occasionally in going beyond the immediate subject a few minor errors have crept in, but no one else could have told the main story in so authoritative a fashion.
There can be few facts about the history of the platinum metals which cannot be found here, in attractive form.
A Week of good intentions
I HAD a suspicion that it was going to be a nuclear week when I saw Richard Baker examining Raymond Briggs about the bomb on Omnibus .
Brigg's best-selling When the Wind Blows , was being broadcast on radio the following night.
By Monday, Michael Heseltine was being subjected to indignities by a Greenham Common squad at Newbury and Mr George Bush's pro-deterrent visit was being heralded.
The fates were obviously being kind — and not before time they might have thought — to Channel 4, who had Dr Edward Teller scheduled for Opinions (7 February).
Teller, of course, worked on the atom bomb and the H-bomb.
He appeared unrepentant and impassioned in favour of us developing our nuclear muscle — for defence.
He appeared absolutely confident that even if the bombs went off, we would survive.
There was a certain reassurance in this but he didn't say how many or how, and there was the nagging thought that equally well-tutored scientific opinion could be found to argue with him.
His thesis was not new in essence.
The Soviets were cautious in character, not adventurous like Adolf had been (couldn't that change one wondered?) and, while the balance of terror continued, they would behave themselves to an extent.
He admitted the terror but thought the balance could not be taken for granted.
He spent some time expressing his preference, for tactical reasons, for smaller neutron bombs before developing his argument.
What Teller hopes the West will do will be to develop its inter-dependence, eradicate secrecy between its constituent parts so the defence and the knowledge thereto is shared, and behind its nuclear shield build up a society that will prove the point about the superiority of its system.
The other side would be defeated by our success, would erode, one assumed, through envy: the model of our society would be supreme.
Dr Teller didn't claim there was much sign of that happening now but he thought it could, an act of faith pretty much, one thought, unsupported by history.
A programme which one feels one ought to watch because it is so obviously full of good intentions, is Voices, which also comes from Channel 4.
Good intentions, of course, are said to pave the way to hell, but one must, like Edward Teller, be optimistic, so I turned to it (9 February) to watch the rubbing together of two creative intellects, those of John Berger and Susan Sontag who were to discuss/argue/disagree about storytelling.
Both are expert in art criticism, storytelling, and film-making, though their approach is different.
They were alone, face to face across a table over a carafe of water, which gave the camera time — the programme lasted an hour and ten minutes, less advertising breaks — to examine them minutely.
Berger agonises after expression; Sontag, by contrast, is calm, assured and, one must say, extremely logical.
Each was required in turn to break off and tell us the advertisements were coming and it did prove hard, such was the intensity of the exchange, to sustain the intellectual commitment over commercial claims for Whiskas and options .
But they did agree to disagree about the how and why of storytelling and, as Ms Sontag concluded, ‘We are going to go on talking about this for many years but now, alas, we have to stop’.
And alas, I thought, my penitential journey over, they didn't do it separately in the first place.
Is unification in physics necessary?
There has recently been much publicity about the possible ‘unification’ of two of Nature's forces.
But why do physicists want to unify phenomena, should they try to, and do the data justify their claims?
Norman Dombey
LAST MONTH both the popular and the scientific press gave much publicity to the possible discovery of a subatomic particle, the so-called W. Physicists at CERN, the European centre for research in particle physics, had found in their highly sophisticated apparatus evidence consistent with the decay of a W particle (New Scientist, 27 January, p 221).
The existence of the W, if confirmed, is a crucial step in the verification of the unified theory of the electromagnetic force and the weak nuclear force.
But before too much champagne has been drunk, it is pertinent to ask whether the standard unified model is the only possible explanation of the experimental data.
In the past, unification of phenomena that at first sight appear independent has signified great progress in physics.
The classic example is the unification of electricity, magnetism and light (see Box).
In a somewhat similar vein nuclear physics contains examples of processes that appear to have little in common, but which may be unified.
An atomic nucleus, for example, can in certain circumstances emit a gamma ray, or high-energy photon, in a process called gamma decay; this is an electromagnetic effect.
But a nucleus can in other circumstances emit an electron in a process known as beta decay, which occurs via the weak nuclear interaction (figure 1a).
The two decays seem quite different.
The strength of the electromagnetic interaction is much larger than that of a weak interaction, and the range of the electromagnetic force is much greater than that of the weak force.
The probability of the beta -decay of a neutron into a proton, an electron and an antineutrino is given by the so-called fermi weak coupling constant,Gv .
On the other hand, the electromagnetic interaction between an electron and a proton, which arises from the exchange of a photon between the particles (figure 2), is given by the product of electric charges of the particles, that is magnitude e 2 , where e is the unit of charge.
Numerically,e is 10000 times larger than Gv .
Other, more subtle differences show up in the symmetry properties of weak and electromagnetic interactions.
Experiments in the 1950s showed that beta decays are not left-right symmetric; electromagnetic interactions on the other hand preserve left-right symmetry at all times.
The lack of left-right symmetry is known as parity violation.
But even in the 1950s there was one apparent similarity between weak and electromagnetic interactions.
Both interactions occurred between all sorts of subatomic particle: protons, neutrons, electrons, neutrinos and all the more exotic variants of these that occur only in high-energy cosmic rays from outer space, or in experiments with highly accelerated beams of particles.
Furthermore, the weak coupling,Gv , for nuclear decay was measured to be the same as the coupling Gv , for another decay process, that of the muon.
The muon is a particle like the electron, only some 210 times heavier.
It decays into an electron, a neutrino and an antineutrino (figure 1b).
Notice that this decay does not involve particles like protons.
So just as electric charge is ‘universal’ in that all particles have charges that are multiples of e , the weak coupling also appears to be universal.
The theoretical explanation of the universality of electric charge is that the electric current of the charges particles involved in an electromagnetic process must be conserved so that charge cannot be created or destroyed.
Thus the Soviet physicists S. S. Gershtein and Y. B. Zeldovich in 1955 explained that Gv was equal to Gv by assuming that there was a similar conserved current between the particles in beta decay and muon decay.
This new weak current had also to be electrically charged because in beta decay the neutral neutron turns into a positive proton.
Sheldon Glashow writing his thesis at Harvard University a few years later, and Abdus Salam and John Ward independently in Imperial College, London, took this similarity seriously.
They tried to reconcile the different strengths, ranges and parity properties of the weak and electromagnetic interactions, and to convert the two separate theories into a unified theory.
They knew that the electromagnetic interaction takes place via the exchange of a photon (figure 2).
To make the weak interactions ‘look’ the same, they introduced new particles W + and W -.
These two particles would couple to the weak current in the same way that the photon couples to the electromagnetic current (figure 3).
Standard rules of quantum theory show then that Gv = Gv = g 2 /8 M 2 m where g is the new weak charge, analogous to electric charge, and M w is the mass of the W particle.
Glashow, Salam and Ward were thus able to show in 1959 that taking g equal to the electric charge e gave the mass M w in terms of the two  fundamental constants e and Gv , in the same way that Maxwell demonstrated that the velocity of light was given in terms of the two fundamental constants of electricity and magnetism.
Their observation reconciles the differing strengths and ranges of weak and electromagnetic interactions.
The coupling constant in both theories was just the electric charge,e ; weak interactions were weak and of short range simply because M w turns out to be so large.
Glashow then showed in 1961 that the parity properties of weak and electromagnetic interactions could also be reconciled provided that there was a neutral counterpart, Wo, to the W + and the W -.
He assumed in addition that the W o did not exist in its own right.
He argued that ‘quantum mixing’ of the W tum mixing’ake place because both neutral particles had identical quantum numbers.
A new particle Z o resulted from this mixing mechanism in place of the W o .
Glashow was then able to predict that a neutral weak interaction exists due to the exchange of the Z o , for example in electron-proton scattering (Figure 4), and that this process would violate parity in a definite way depending on the amount of mixing.
Many experiments since 1973 have confirmed the existence of weak neutral currents.
Perhaps the most striking have been the experiments that show that the presence of the neutral weak interaction in addition to the electromagnetic interaction does indeed lead to a parity violation in electron proton scattering and in certain transitions in heavy atoms.
The results of these and other experiments fit in well with the predictions of what has become known as the standard electroweak model.
In 1979 Glashow, Salam and Steven Weinberg (now at the University of Texas at Austin) received the Nobel prize for their work on the standard model.
At the time even they were surprised at the honour.
Glashow, for example, was reported to say that the Nobel committee took a ‘bit of a chance’ because ‘nobody has yet built a machine that is capable to check’the new particles predicted (international Herald Tribune, 16 October, 1979).
The parity-violation experiments, however, do not prove the existence of the charged W particles.
Neutral currents with the appropriate properties could still exist in the theory of beta decay that Enrico fermi had proposed in 1933, without the mediation of W or Z particles (figure I).
The test of the standard unified model is the precise prediction of a W + and a W -at mass 82 GeV and a Z of mass 93 GeV. (1 GeV or gigaelectronvolt is roughly the mass of the proton.)
I use my colleague John Cole's  predictions of the masses of W and Z in the standard theory.
Is the standard unified model the only possible explanation of the experimental data it so well describes?
In 1978 the Japanese-American physicist J. J. Sakurai (who died tragically while on a visit to CERN a few months ago) and his student P. Q. Hung answered this question in a remarkably simple way.
They found they could explain all the experimental data on neutral weak currents by assuming quantum mixing between the electromagnetic current and the neutral weak current.
An angle, hw, gives a measure of the amount of mixing.
This feature is present in the standard unified model, so it is not surprising that the experimental data are satisfied.
But in the Sakurai-Hung approach there is no unification of weak and electromagnetic interactions: both interactions exist side-by-side and are independent.
Nevertheless because electrons, protons and other particles  undergo both weak and electromagnetic interactions the effect of each on the other has to be considered.
This is precisely what Sakurai and Hung calculated.
My colleague John Cole and I, along with Alex Calogeracos at University College, London, have taken this approach further.
We are able to show that the mixing angle, hw, that follows from the  considerations of Hung and Sakurai is consistent with that experimentally measured without any assumption of unification.
So the search for the W and Z particles is of great significance.
If the theories are not unified there is no reason for these particles to exist with the masses given above.
The experiments at CERN have not seen a W per se .
What they do see is a high-energy electron (positron) that appears to come from the decay of a W -(W -).
A single electron or positron cannot identify a W -of W + with certainty; there is always a neutrino emitted with the electron in the decay and this cannot be detected.
All that can be said of the experiment at CERN is what the physicists there did say.
What is seen is consistent with the interpretation that nine W particles of mass about 80 GeV were produced, and that these then decayed into electrons (or positrons) and neutrinos.
There is also the proviso that the theory of proton-antiproton collisions with which the experimental results were compared is itself uncertain numerically by a factor of two or three.
In addition, it is always difficult when looking for very rare events — there is supposed to be only one W  secay into electrons or positrons for every 200000 proton-antiproton collisions — to be sure that the large ‘background’ from other processes is well enough understood and that it has not ‘faked’the signal required.
If a Z o were produced which decayed into an electron positron pair, observation of both particles in the pair would give a definite identification of the Z o and its mass.
This is the critical signal that would stop all further speculation about what has actually been seen.
Yet the high-momentum electrons and positrons observed must eventually be explained somehow and it is perhaps worth remarking that it is not new in physics for novel phenomena to be observed when looking for something else.
At present theoretical physicists expect other new heavy particles that could decay in a similar way, such as the so called‘top’ quark.
Further events are needed to disentangle the various possibilities.
There is another kind of experiment being performed at the electron-positron storage rings at DESY, the German particle-physics laboratory in Hamburg.
These experiments are at much lower energies than at CERN, but should give a clearer signal as there is little complication from background.
In electron-positron collisions pairs of negative and positive muons are produced (figure 5).
The presence of the weak interaction as well as the electromagnetic interaction in this process gives rise to an asymmetry whereby an experiment should detect more positive muons in the forward hemisphere (as defined by the direction of the electron) than in the backward hemisphere.
The asymmetry can be calculated, and depends on the mass of the Z particle (figure 5).
The dashed curve shows the result if the mass of the Z is 93 GeV; the straight line is the asymmetry if there is no Z. The most recent results are with a total electron-positron energy of 35 GeV, but at this energy the two curves for the expected asymmetry differ by only about I per cent.
Unfortunately the experiment cannot distinguish the two possibilities to this accuracy.
Later this year, however, there should be a new run at energies up to 43 GeV; this  should give a difference in asymmetry of about 3.5 per cent, which if there will be observable.
These results will thus confirm a Z o particle with a mass around 90 GeV if indeed it is there.
So for another few weeks or months the question of the unification of weak and electromagnetic interactions is still open.
Certainly we already know that both weak and  electromagnetic interactions affect all atoms and molecules in that there is a small intrinsic left-handedness in all matter.
The situation is thus similar now to electromagnetism after Oersted and Faraday but before Maxwell (see Box).
Both weak and electromagnetic interactions need to be included in precise calculations of atomic properties or high-energy electron scattering; but the new era of physics involving the discovery of the X and Z particles with masses given in terms of the electric charge and weak coupling has not yet arrived.
In the meantime it is sensible to be cautious.
Albert Einstein spent the last 50 years of his life unsuccessfully trying to unify the theories of electromagnetism and gravity.
Yet it is a useful general principle in science that distinct and unrelated phenomena exist; not everything is the same as everything else in physics or in other areas of human endeavour.
Freeman Dyson, who with Richard Feynman.
Julian Schwinger and Sin-itiro Tomonaga. was responsible 30 years ago for the development of quantum electrodynamics — the first modern relativistic quantum theory — writes in his autobiography of that period that ‘the ground of science was littered with the corpses of dead unified theories’.
We shall soon see.
The first great unification
AT THE beginning of the 19th century, scientists considered magnetism and electricity to be independent.
But in 1819 Hans Christian Oersted showed that a steady electric current generated a magnetic field, and in 1831 Michael Faraday showed that a time-varying magnetic field would generate an electric current in a conductor.
Together these results produce the combined subject of electromagnetism.
In 1862, James Clerk Maxwell wrote his famous paper in the Philosophical Magazine in which he assumed that a time-varying electric field (the displacement current) would also generate a magnetic field.
This led to his prediction that electromagnetic waves existed and would propagate at a velocity c equal to the ratio of electromagnetic to electrostatic units of measurement.
Numerically c turned out to be remarkably close to ‘the velocity of light in air, as determined by M. Fizeau’ which was ‘70 843 leagues per second’.
Maxwell concluded that ‘we can scarcely; avoid the inference that light consists of the transverse undulations of the same medium which is the cause of electric and magnetic phenomena.’
Thus the theory of light was unified with the theory of electromagnetism, although it took another 30 years before Heinrich Hertz was able to demonstrate positively that electromagnetic waves did exist.
Big revivals for biomass engine
Charcoal-fired gasifiers were common sights on vehicles in oil-starved Europe during the 1940s.
The technology has recently resurfaced in Brazil and the Philippines
Gerry Foley and Geoff Barnard
INDEPENDENT and major revivals of Second World War gasifier technology are under way on opposite sides of the world.
A gasifier is a furnace which is bolted to a vehicle, and produces a combustible gas by burning biomass in a restricted air supply; the gas replaces petrol or diesel.
The technology was used to keep basic transport running in Europe during the 1940s, but it is now being redeveloped in distinct ways by the Philippines and Brazil.
The Brazilians are relying on private enterprise.
Some 60 companies there claim competence and expertise in the technology, each trying to identify its own market niche.
So far about 10 have made sales.
Industria Siquieroli has manufactured and sold the most units — 650 charcoal-burning, vehicle gasifiers at $750 a time.
Other firms specialise in direct-heat gasifiers to make gas to fire furnaces and kilns.
GTI has particular expertise in wood-burning gasifiers for making lime: its largest installation consumes about 0–8 tonnes of wood per tonne of lime produced; traditional methods consume up to 2–5 tonnes of logs for the same result.
The gasifiers provide up to 6 million kcal/hour.
The Philippine government financed the Gasifier Equipment Manufacturing Corporation (GEMCOR) in 1981.
It sold 850 units in its first year.
Of these, more than 450 were for small, inshore fishing boats called bancas.
Nearly 200 were built to power irrigation pumps, and most of the rest were for light commercial vehicles.
GEMCOR now works a three-shift, 24-hour day and employs 125 workers.
Production should reach 4000 units this year, and orders are flowing in.
Gasifiers disappeared in Europe after the war when oil became available again.
They were not considered an attractive, efficient or safe way to power a vehicle.
The oil price rises of the 1970s have created conditions in many Third World countries similar to those of a wartime siege economy.
The world may be awash with oil, but some countries pay half or more of their export earnings to import the stuff.
In rural areas, irrigation pumps and tractors stop because farmers cannot afford fuel.
Despite the problems revealed by wartime experience, gasifiers have tantalised researchers since the early 1970s.
They offer a way to substitute local wood and charcoal for expensive imported diesel and petrol.
More than 100 development groups throughout the world are working on the technology, many of them attempting to update old designs to work with modern engines.
Several of the gasifier systems commercially available are made by Second World War manufacturers.
They have dusted off their old blueprints — or even their old stock — and entered the market again.
Sponsorship by various technical-help agencies has allowed prototype gasifiers to be installed in Tanzania Guyana Tahiti, Seychelles, Indonesia and elsewhere.
Representatives of the EEC's technical assistance programme have been promoting the technology in Pacific islands like Fiji and Tonga.
But, despite such efforts, the technology has refused to take off commercially.
The best installations have demonstrated what is already known — that gasifiers can be made to work; the worst have been total failures.
The reasons for such poor results are simple.
Gasifiers require considerable technical skills for operation and maintenance.
A poorly-run gasifier rapidly becomes unusable and may destroy the engine to which it is attached.
The technology, simple in principle, is not a typical Third World village technology because, for successful deployment, it requires technically sophisticated operators as well as repair and maintenance technicians.
How gasifiers work
A gasifier is a furnace in which the air supply is restricted to prevent complete combustion, hence its more descriptive tag ‘gasification by partial combustion’.
In principle, any reasonably dry biomass can be gasified; but the practical problems of using fuels with a high ash content or of low density are quite severe.
Charcoal, wood chips, coconut shells and maize cobs are the only practical fuels at the moment.
Typical commercial gasifier systems used to run engines consist of a reactor, where the gas is produced; a gas cleaning and cooling system; and a modified carburettor to feed gas and air into the engine.
In most, the suction of the engine draws air into the gasifier and pulls the resulting gases through to the piston chambers.
The typical ‘downdraught gasifier’(there are also updraught and crossdraught models, see figure above) has its combustion zone, or ‘fireball’, in its lower part.
Air is drawn in and the wood or charcoal burns, producing water and carbon dioxide at temperatures of 1100–1700 o C.
The air supply is restricted so the combustion zone can spread only so far.
The hot gases produced are sucked downwards, out of this zone, into a slightly cooler area which contains unreacted carbon from the feed material.
No free oxygen is present so a different set of reactions takes place; the atoms rearrange themselves, seeking the most thermodynamically stable combinations.
Most important to the user, carbon dioxide gets converted to carbon monoxide and water breaks down to give hydrogen.
The oxygen atoms released combine with unreacted carbon to give additional carbon monoxide.
The final gas mixture, called producer gas, contains around 20–30 per cent carbon monoxide, 5–15 per cent hydrogen, along with traces of methane.
The remainder is largely nitrogen, with some carbon dioxide.
Because nitrogen is inert, the calorific value of the mixture is low (3000–5000 kJ/cu.m)— about one-tenth that of natural gas.
The mixture's high carbon monoxide content makes it extremely lethal.
Inhaling it can cause immediate death; the effect has been described as comparable in speed with an acute cerebral haemorrhage or coronary thrombosis.
The combustion zone heats the unreacted feedstock that surrounds it.
Where the temperature exceeds about 400 o C the feedstock begins to break down, giving off water vapour, methanol, acetic acid and — of most concern to the mechanic trying to keep the contraption going — a lot of heavy hydrocarbon tars.
The amount of tar may be as much as 40 per cent of the weight of the original feedstock.
Charcoal has these tars driven off in the course of manufacture, leaving it as a clean, uncomplicated fuel for use in gasifiers.
With other organic fuels the problem is to reduce the quantity of tar in the final gas to a level that can be dealt with by a cleaning system.
Most gasifiers are therefore designed to draw the tars through the combustion zone so that they are cracked or burned.
Excessive moisture, the formation of clinkers, big load-changes, and a variety of other circumstances, however, can let some tar through.
After leaving the gasifier, the gas is cleaned and cooled.
A cyclone is generally used to extract dust and ash.
This is followed by a series of filters and scrubbers.
Cooling is usually done by passing the gas through pipes.
In the case of a vehicle, these may be at the front of the radiator or on top of the cab to take maximum advantage of the  airflow .
The cleaned and cooled gas can be used in diesel or petrol engines.
In the case of compression-ignited diesel engines, a small amount of diesel fuel must be injected in each firing cycle to allow ignition.
Thus the maximum saving of diesel fuel is about 85 per cent.
In practice the saving may be considerably less, as extra diesel is usually needed to take up any temporary increases in the load on the engine.
Gas can substitute entirely for petrol, because ignition is caused by a spark-plug.
In practice.
however, a means of switching to petrol for starting or to meet heavy-load surges is usually retained; so that the fuel saving is less than 100 per cent.
Converted petrol engines suffer a considerable loss of maximum power.
This is usually around 50 per cent, which can be a serious disadvantage.
Running a gasifier requires both patience and mechanical aptitude.
In starting.
a fire must be kindled and the gasifier nursed up to full output.
Filters must be cleaned, cooling systems drained, and the whole assembly checked regularly and carefully for gas leaks.
Above all, operators must be able to adjust and tinker with air, gas and fuel flows to keep the systems running happily.
Gasifiers yield nothing but trouble if they do not get continual care and attention.
This is not to put gasifiers in a realm of esoteric complexity.
They require the care that can be provided by a loving diesel mechanic; unfortunately, such people are rare in developing countries.
Manila seeks efficiency
Reviewing the progress in promoting gasifiers during recent years, it is easy to despair and conclude that, as with many other energy initiatives, the practical difficulties in mounting an effective programme are virtually insurmountable.
Yet clearly the Philippines has made a breakthrough.
Much of the credit must go to Ibarra Cruz of the University of the Philippines and the head of the national non-conventional energy centre.
His first Philippines gasifier model was made in 1967; and through the 1970s his research has resulted in a series of models of increasing practicality, simplicity and cheapness.
These are now the basis of the GEMCOR range.
Three basic types are produced.
The smallest, to power a 12 kW engine for a fishing boat, is light and compact.
It is designed to take minimum space and weighs 55 kg.
A wheelbarrow mounted version can be switched between different small engines in a farm or industrial plant.
Irrigation pump models are for engines of up to 45 kW.
These have to operate for prolonged periods and are robust and solid pieces of equipment, for space and weight are not serious constraints.
The fact that water is available is used to advantage in a simple water-scrubber for cleaning the gas.
A lightweight, modular unit has been designed for vehicles.
It can be fitted to a jeep or truck without extensive modifications to the chassis or bodywork.
The controls are also extremely simple.
An on/off switch on the dashboard controls the petrol pump, and a hand-operated gas-valve regulates the gas flow.
The engine is started on petrol, with just enough suction to bring the gasifier into operation.
Once the gas flow is adequate, the petrol pump is switched off.
On a hill, the petrol pump is switched on and the gas valve is switched off.
The gasifier-powered vehicle is probably at too great a disadvantage to survive the snarling dogfights of traffic in metropolitan Manila, but it seems perfectly adequate for rural use.
The fuel used is almost invariably charcoal made from wood or coconut shells.
Wood chips and other materials may be used, but at this early stage GEMCOR advises users to build experience with charcoal only.
This minimises the risk of tars clogging the filter systems or damaging the engine.
People are not advised to aim for maximum fuel savings, but to concentrate on taking what is readily achievable without affecting efficiency.
The economics appear remarkably good.
Gasifier prices are as low as $50/kW so the advantages of local manufacture and cheap labour rates are immediately apparent.
Models imported from Europe tend to aim for higher levels of fuel saving, a wider range of feedstocks and a greater degree of automation in their operation.
They are usually more complicated and expensive.
Ex-factory prices for such gasifiers may be $400/kW.
The crucial issue seems to be technical abilities, and here the Filipinos score heavily.
They are not just adept; they are irrepressible.
They seem capable of making anything mechanical out of something else.
Manila's public transport is based on a multitude of ‘jeepneys’ the originals of which were surplus US army jeeps left after the war with Japan.
Some are genuine 1940s vintage; others are locally-produced derivatives.
Modern, unofficial vehicular hybrids such as the ‘Toyota-Benz’ are not unknown.
If it is mechanical, Filipinos seem to be able to manipulate it and adapt it.
Gasifier technology is no problem.
The shadow over the future of the Philippines programme — and over any Third World gasifier programme — is the availability of fuel.
Wood depletion and deforestation are already recognised as a problem in the country.
Satellite pictures show that forests cover 30 per cent of the country; the government feels that 46 per cent coverage is a minimum, for economic and environmental reasons.
Attempts are being made to link gasifier deployment with the planting of fast-growing species such as the ipil-ipil (Leucaena leucocephala), which may yield 25 tonnes of dry wood per hectare per year.
A key-role in the tree planting is being played by the farm Systems Development Corporation.
It encourages the formation of farmer cooperatives and advances them loans for irrigation pumps.
Some 1800 of these have now been installed.
Tree planting is also supported by a government campaign.
Some success is being reported, with a net gain in forest area of 34 000 hectares in 1980.
But the basic hope is that market forces will drive wood and charcoal prices high enough to create an incentive for small-scale voluntary replanting and large-scale forestry projects.
Brazil chooses charcoal
A key, coordinating role in the development of Brazilian gasifier technology is played by Florestal Acesita.
This subsidiary of the steel manufacturer Acesita supplies its parent firm with charcoal for steel smelting.
About 250 000 tonnes of charcoal per year are provided from Acesita's eucalyptus plantations.
The average yield from these forests is about 20 cu.m/ha/year, but yields of up to 50 per cent more are now being achieved with new plantations.
Charcoal from planted wood costs around $60 per tonne to produce.
Florestal Acesita, has maintained an interest in gasifier technology.
The firm has acted as a clearing house for information on plantations, charcoal and gasifiers for years.
It collaborates in prototype development and is testing a variety of gasifiers with its transport and planting fleets.
Florestal intends to convert 176 large trucks to gasifier power.
Charcoal is used, almost exclusively, for vehicles and shaftpower applications in Brazil.
This is the major reason for the extremely low cost of the country's gasifiers.
The fuel is clean burning which means the gas-cleaning system can be simple compared with systems burning a different biomass.
No work on fuels other than charcoal appears to be in progress.
Available gasifiers are well-designed and economically competitive, but most have been on sale for 18 months or less.
Private companies have carried out most of the development work, usually to meet specific markets.
They are concerned with the problems of adapting designs to meet individual needs.
To this end, considerable attention is being paid to establishing service and distribution networks: some are doing this in advance of their marketing efforts.
A couple of years observation will show what long-term impact gasifiers will have on Brazil.
It is probable that some units on sale will find applications in other, wood-rich developing countries.
Vanuata experiments with home-built gasifiers
Like Fiji, Tonga and other countries in the South Pacific, Vanuata (the old New Hebrides) faces the problems typical of isolated archipelagoes, where imported energy costs are increasingly high.
The authorities felt that small gasifiers might provide part of the solution to these problems and New Zealand was persuaded to fund and deliver a small downdraught gasifier for Vanuata.
However, this off-the-shelf technology proved unsatisfactory in practice so a local team, headed by Ken Calvert, set about designing building and evaluating their own variant.
It had to be easy and cheap to build, maintain and operate in addition to using local materials where possible.
The fruits of their labours comprised three joined and lined 200 litre drums which delivered producer gas through a simple cloth filter to a Toyota petrol engine driving a generator.
A locally constructed gas-cleaning precipitator was evaluated to reduce the need for frequent filter changing and the gasifier also incorporated a tar condensate trap.
Waste tar was then free for use in wood preservation and other processes.
The engine was started and run solely on gasifier-produced gas and employed another ingenious hybrid device as a speed regulator.
Tony Marjoram 
Stars by the heat of the night
Remarkable new pictures from the Anglo-Australian Observatory in New South Wales show how some familiar objects would look if our eyes were sensitive to infrared radiation IN MODERN astronomy, researchers investigate the sky at all wavelengths from gamma-rays and X-rays to radio waves.
Telescopes such as the orbiting Einstein X-ray observatory and the Merlin array of radio telescopes centred on Jodrell Bank give us views of celestial objects as detailed as optical telescopes can show (New Scientist , vol 93, p 720 and vol 96, p 442).
But one region of the spectrum that has been comparatively little explored is the infrared, or ‘heat radiation’.
This is the name for wavelengths longer than light, from the red edge of the visible spectrum at 0.7 micrometres to the borders of the radio region at 1000 mm (1 mm).
It was the first ‘invisible radiation’ to be discovered, when William Herschel found in 1800 that a thermometer would register heat beyond the red end of the Sun's visible spectrum.
The study of infrared from celestial objects other than the Sun is plagued by three problems.
First, carbon dioxide and water vapour in the Earth's atmosphere absorb all infrared between wavelengths of 30 and 300 mm, and also in narrower bands at shorter wavelengths.
Secondly, the atmosphere (and indeed the telescope itself) is comparatively warm, and emits infrared radiation naturally (just as the hot filament of a light bulb emits light), so the astronomical sources appear only very slightly brighter than the brilliant sky all around.
Thirdly, the semiconducters used as infrared detectors must be cooled to within a few degrees of absolute zero, and they can measure the brightness of only one small patch of sky at a time, rather than‘photograph’ a whole region of sky at once .
The Infrared Astronomy Satellite (IRAS) overcomes some of these problems by carrying a telescope high above Earth's atmosphere, complete with its own complex cooling, system (New Scientist , 27 January, p 226).
Meanwhile, here on Earth, astronomers at the Anglo-Australian Observatory at Siding Spring in New South Wales have evolved a new system that can produce astonishing infrared views such as are shown on these pages.
To achieve this, the astronomers have, for a start, worked only at the shorter infrared wavelengths — 1.2, 1.6, 2.2, 3.8 and 4–8 mm — to which the atmosphere is reasonably transparent.
But the major advance is a new electronic amplifier devised by John Barton and David Allen.
finally, the astronomers have been able to use the world's most precisely controlled telescope.
The 3.9-metre Anglo-Australian Telescope was built as an optical telescope, but its mirror will reflect infrared as well.
And its accurate mounting and sophisticated computer-control allow astronomers to scan the infrared detector along a source with such precision that it can build up a detailed infrared picture, line by line.
The smallest details in these pictures are one arcsecond in size, that is, about  the apparent diameter of the Moon.
Each object shown in these pictures has been scanned at two or three different  wavelengths .
Allen's colleague Jeremy Bailey has produced a computer program that colour-codes the view at each wavelength and then adds them together to produce these multicolour views.
In each picture, the longest wavelength is coded in red, the next-longest green, and the shortest wavelength (where there are three) in blue.
So here we are seeing exactly how the objects would look if our eyes were sensitive to wavelengths about five times longer than they actually detect.
Saturn (figure 1) is pictured here at two wavelengths, 3.8 mm (green) and 4.8 mm (red).
At the shorter wavelength, the rings are lit up by the Sun's radiation and shine brilliantly; but methane in Saturn's atmosphere absorbs at this wavelength and so the planet itself is virtually invisible.
At the longer wavelength of 4.8 mm, solar radiation is much dimmer, so the rings are barely illuminated.
Nor indeed is the planet illuminated, but Saturn has an internal source of heat which makes the planet's globe shine at this wavelength.
Infrared shows us even more about the larger giant planet Jupiter (figure 2).
Blue here corresponds to 1.6 mm, and shows the cloud tops much as we see them at optical wavelengths.
At 2–2 mm (green), a wavelength methane absorbs, the planet is almost invisible except for the methane-poor polar caps.
The longest wavelength (4.8 mm, shown red) is not reflected solar radiation, but is once again the glow from the planet's interior heat source.
Jupiter can be seen radiating mainly through narrow bands parallel to the equator, in particular through the band running just below the centre, where there are gaps in the upper cloud layers.
The greatest advantage of infrared astronomy, however, comes when looking at objects well beyond the Solar System.
This radiation cuts through the dust in space that blocks off the light from distant objects and hides them from the gaze of optical telescopes.
A spectacular example is the giant galaxy Centaurus A, lying some 16 million light years away.
The centre of this galaxy is a prodigious source of radio waves, X-rays, gamma-rays and tremendous jets of high-speed electrons which have shot out almost a million light years on either side.
Yet we can see little sign of the central powerhouse at optical wavelengths, because a dark swathe of obscuring dust lies right across the galaxy (Figure 3A).
Barry Giles of the University of Tasmania has used Allen's system to see Centaurus A in the infrared (figure 3B).
Here the shorter wavelengths of 1.2 and 1.6 mm are shown blue and green, while 2.2-mm radiation is coded red.
The infrared picture is at 10 times the scale of the optical photograph, showing only the very centre of the galaxy.
The dark band below the centre is the innermost dust lane, dense enough to absorb some infrared; the dust lane in fact glows slightly ‘red’ with 2.2-mm radiation from regions within it where stars are forming.
Dominating the infrared view, but dimmed to insignificance in the optical, is the galaxy's central nucleus, shining with the brilliance of 50 million Suns.
Infrared also helps us probe the heart of our own Milky Way Galaxy.
Dust in the Galaxy completely blots out the view for optical astronomers, dimming the light from anything there to an undetectable one million-millionth of its true brightness.
Yet infrared shows this region in spectacular detail (figure 4).
This picture is centred on a tiny radio source that astronomers believe to mark the Galaxy's very centre.
The coding here is blue for 1.6-mm radiation, green for 2.2 mm and red for 3.8 mm.
The brilliant ‘white’ star at the top, IRS 7, is a supergiant star as bright as 100 000 Suns, but with a temperature of only 2500 K. The red objects are cooler, and are not stars but warm clouds of gas and dust.
The ‘blue’ objects are generally stars or clusters of stars that are hotter than IRS 7.
But the blue ‘sources’ in the central triplet are quite different.
Previous lower definition infrared results had shown these as one object, called IRS 16; this picture by Allen and his colleague John Storey shows the triple structure for the first time.
In this case, the two outer (and brighter) objects are probably hot gas clouds, heated up by young hot stars within them.
Storey and Allen suggest that the faint central member of the triplet is different: it is the star cluster at the very centre of the Galaxy.
As we look in closer to the centre, we expect the stars to be more closely packed together, and in this central pan of IRS 16 the infrared radiation comes from thousands of ordinary stars packed into a region only a hundred times larger than the Solar System.
The biology of Loch Ness
Loch Ness and the creatures in it are at last yielding to thorough biological investigation.
Results from the Loch Ness and Morar project suggest that the loch might indeed harbour a large predator
Adrian Shine
IT IS almost exactly 50 years since the world at large was first made aware of the monster of Loch Ness.
Before then (before, that is, the age of the motor roads and the tourists that went with them) it was known only to the Highlanders, and the hunting, shooting and fishing gentlemen who learned of her from their gillies.
Since then hundreds of observers have produced thousands of observations and scores of theories; but it was not until the 1970s, when the Loch Ness and Morar Project formally began work, that the loch itself was subjected to thorough and systematic study.
Even now, almost a decade into the project, we cannot say that the monster does or does not exist.
We do know enough, however, to say that it could; and, although we can lay some of the more romantic if attractive legends to rest, we have to declare that the case is still open.
Something in the loch has produced some very peculiar sonar traces on some very sophisticated equipment, and that something demands explanation.
The Loch Ness and Morar Project itself picked up the threads of the Loch Ness Investigation (LNI), which was founded by David James and Peter Scott in 1962.
The LNI stopped operations in the 1970s, and James and Sir Peter both became patrons of the present project.
The project, with input from the Scientific Exploration Society, brings together amateurs and professionals, including technologists in industry, academics in universities and volunteers.
Finding the monster is only part of the objective.
The main role is to conduct formal systematic limnology — to study the physical features and biology of the loch.
Such basic work, oddly enough, has been largely neglected.
A few comments on this background study will show how the legends are being put into perspective.
Loch Ness was formed originally by the shifting tectonic plates that created the Great Glen fault line, and it was scoured smooth by glaciation.
It was thus that it acquired its unique form: 35 km long and about 1.6 km wide with steep sides sloping to a flat bed.
it is divided into two 220-m deep basins by the outfall of silt from the River Foyers about halfway along its length.
The maximum depth of 230 m is considerably greater than the seas surrounding our shores, and of all British lakes, Loch Ness is second in depth only to Loch Morar, at 310 m.
However, its fault line origins give Loch Ness the greatest mean depth, at 130 m.
These depths were determined by John Murray's Bathymetric Survey of the Scottish Lochs in 1902–08, and were found with a Kelvin wire sounding machine; a sophisticated ratchet device in which a plumb line of piano wire is lowered from a rowing boat.
We have always found these early readings reliable.
They are disputed only by a sonar depth reading of 297 m, recorded in 1969; but we have found no depth over 220 m in the area scanned in 1969.
Echosounders calibrated for sea water over read slightly in fresh water, which may in part explain the exceptional reading.
Echo sounding is also responsible for an erroneous impression given on chart traces, suggesting that the loch walls continue down for 1000 m or so beneath a filling of loose sediment.
But this is due to an effect known as ‘side echoes’; a sonar frequency of 10 kHz or less is required for true sediment penetration.
Loch Ness is quite spectacular enough without such exaggerations.
Its volume of 7443 million cubic metres is greater than any other British lake and could comfortably immerse every man, woman, and child on Earth three times over.
There is clearly enough to conceal a few mysteries as well.
But if there is a monster among those mysteries, how did it get into the loch?
And what about the species we know are there?
Clues must be sought in the post-glacial history.
Ten thousand years ago the ice began its last retreat from Loch Ness.
Melt water temporarily raised the sea level, and rendered the loch more accessible.
Subsequently, relieved of the weight of ice, the land rose and the loch surface is now 16 m above sea level.
We have studied the sediment from the bottom of Loch Ness and Loch Morar to discover whether the sea entered either of them after the last ice age.
Using specially-designed apparatus, we took core samples 2 m long from the bottom of Loch Morar (whose surface is now 9 m above sea level) and 4 m long from Loch Ness; in each case the samples were 2 inches (4.6 cm) in diameter.
We examined these samples for skeletons of marine or freshwater diatoms (microscopic planktonic plants); these skeletons are of silica, and so endure.
In a core from Morar, Dr H. J. V. Birks of Cambridge University found abundant remains of marine algae, providing good evidence that the sea has invaded since the ice age.
But so far we have found no evidence of marine transgression at Loch Ness; though both lochs are still ‘connected to the sea’ via the rivers that drain them.
A 2 m core From Morar is thought, on the basis of pollen count, to have penetrated through more than 5500 years of sediment.
This gives a good idea of the rates of deposition and seems effectively to dispose of the idea that Loch Ness might have acquired 1000 m of sediment since the last ice age.
This, then, is the shape and origin of the habitat that Loch Ness provides for the putative monster.
What food can it supply?
The basis of all food resources, the beginning of all food chains, are plants, which provide the ‘primary productivity’; and primary productivity in Loch Ness is meagre.
High latitude and frequent cloudiness provide only a short growing season, and a suspended peat limits photosynthesis to shallow photic (light) zone of about 6 m.
The hard rocks of the steep hills of the loch's catchment area yield few nutrients to the fast-flowing rivers that feed into it.
Thus the crop of phytoplankton is low and the characteristic planktonic plants known as desmids are poor in variety.
Accordingly, the herbivorous zooplankton, including the crustacean Diaptomus, Cyclops and Daphnia (’ water fleas’) are limited; and so too are the corresponding crustacean predators, such as Leptodora, Polyphemus and Bythotrephes .
Bottom fauna consist mainly of insect larvae and Pisidium (pea mussels).
High acidity (pH 6.5) slows the bacterial decay of organic particles entering the loch and hence the release of their nutrients.
Rooted plants are restricted to within a depth of 3 m around the shore.
Nevertheless, echo sounding shows that fish are abundant over the entire surface to a depth of at least 30 m.
The resident fish consist of brown trout, char, eels and a few pike and stickleback.
Insects falling into the water during the summer provide a substantial proportion of the diet of trout.
But the influx of migratory salmonids prior to spawning is quite independent of the primary productivity.
Salmon and sea trout enter throughout the year and may remain 10 months before spawning, during which time they do not feed.
Weighing less than 100 grams, the young parr leave and spend one to five years at sea before returning, weighing up to 20 kg.
This must be considered a bonus to the food resources available to any larger creatures.
But it is largely because Loch Ness is so biologically unproductive that it is also biologically stable: it is spared the seasonal booms and crashes in the populations of living things that are characteristic of more productive lakes.
The great body of water remains at a uniform 5.6 o C throughout the winter, which prevents the formation of ice.
As summer advances a layer of warmer water (12 o C at the surface and falling with depth), termed the epilimnion, separates from the hypolimnion beneath, which remains at 5.6 o C. The boundary layer where the temperature changes abruptly is the thermocline.
In a lake, photosynthesis is limited to the epilimnion, where the depletion of nutrients cannot be replaced from the hypoplimnion until mixing occurs in winter.
In productive (eutrophic) lakes, the decay of organic matter falling from above robs the hypoplimnion of oxygen, to the detriment of any life that is present there.
In less productive (oligotrophic) lakes, such as Loch Ness, the hypoplimnion does not become depleted in oxygen.
The oligotrophic nature of Loch Ness therefore offers usable and stable living space  throughout the year.
High oxygen levels permit life to extend to the deepest parts of both Ness and Morar.
In 1979–80 we collected an interesting community of invertebrates at 300 m in Loch Morar.
From Loch Ness in 1982 our collection of animals included three char, at 220 m.
A few observations on the conditions of the deep-water sediments are also relevant to some perennial theories explaining ‘monster’ sightings.
Some suggest that the monster is rotting vegetation lifted by bubbles of gas, produced in the loch bed by bacteria.
But gas has been recorded rising from the loch bed only in the shallower water of Urquhart Bay where the releases of small gas bubbles can be provoked by dragging grapples or dropping boulders.
The resulting bubbles can be observed rising, by echo sounder, but are seldom visible at the surface.
Biologists with the Cambridge University Loch Ness Expedition of 1962 found that the mud of the deep water contains few gas-producing bacteria, and resists all attempts to provoke gas by such means.
Freshly extruded cores show no signs of gas and nor does an underwater television camera towed through the sediment.
Tree trunks, too, have been blamed for the monster legends (see New Scientist , vol 95, p 354).
The project's adviser on underwater television is Peter Scoones, who is an extremely experienced underwater photographer (and photographed the first live coelacanth).
Under his expert guidance we have used underwater television extensively in the deep water, and failed to reveal any rotting vegetation or tree trunks.
Tree logs are to be found only in the shallow waters and in Urquhan Bay, Lochend and Dores, where they have been driven by the prevailing wind.
It is hoped that no further speculation on alternative causes of ‘monster sightings’ shall envisage Loch Ness as a stagnant pond.
On the contrary, as we have seen, Loch Ness is a very and scarcely explored environment offering stable conditions and, despite appearances, considerable resources of food.
Most of the fish in the loch originally arrived from the sea and many still migrate to and from it.
The presence of a large predator of fish is not in the least surprising.
There seems to be no overwhelming biological reasons why Loch Ness could not support a ‘monster’.
So what is the evidence that it actually exists?
A series of widely differing photographs have appeared over the years and, in 1960, T. K. Dinsdale produced film of a moving object.
Encouraged by these ‘classic’ pictures, the LNI mounted intensive surveillance of the loch's surface throughout the 1960s.
After a 10-year war of attrition against the law of averages, the resulting films brought realisation that possible surfacings were rare, brief, and exposed little that was useful to photography.
Higher standards introduced towards the end of the 1960s, by Dr Roy Mackal, at the University of Chicago, revealed how photographs could be faked or mistakes made.
The false trail ended and the LNI moved underwater.
There followed a most significant collaboration between the LNI and the University of Birmingham, which deployed a digital sector-scanning sonar in 1968.
This device indicated objects moving at speeds of 3.3 m/s horizontally and 0.5 m/s vertically (Dr Hugh Braithwaite,New Scientist , vol 40, p 664).
Sonar had already produced interesting results in 1962 when used by an Oxford and Cambridge University expedition (Dr Peter Barker,The Observer , 26 August, 1962).
Since then, the LNI, klein Associates, and Partech Ltd have also reported successes.
Unfortunately, 1972 saw the cessation of LNI activities.
In the same year, underwater cameras slung from moored boats in Urquhart Bay by Dr R. H. Rines of the Academy of Applied Science, Concord, New Hampshire, produced photographs which, when enhanced by computer, appeared to show a flipper-like object.
Other pictures followed in 1975.
However, since they are interspersed with shots of debris from the loch bed they are inadmissible as evidence.
Rines and Sir Peter Scott named the animal Nessiteras rhombopteryx in 1975 (Nature , vol 258, p 466).
But they recognised the ambiguities, and based their paper on wider evidence, and were prompted by concern for conservation.
Meanwhile in 1974, revived British effort had moved to the clear waters of Loch Morar (which had a similar tradition of sighting established by The Loch Morar Survey of 1970–72).
The Loch Morar expeditions began with manned observation equipment and progressed to underwater television in 1975.
The objective was to record a full-body profile by viewing upwards from beneath, against the backlit 90 o circle provided by the surface.
In 1976, despite coverage a hundred times greater than can be achieved by conventional flash photography (which is limited by back scatter), no results were achieved in three months of work.
There followed a reassessment of the two principal investigative techniques.
Sonar made interesting contacts but could not identify them directly, while passive underwater photography (photographing whatever passes a given spot) had the potential to reveal essential detail but could not be expected to make contact without a much sounder basis for deploying the cameras.
The Loch Ness Project took on the mantle of the LNI and picked up from the Loch Morar expeditions, and we returned to Loch Ness, whose steep-walled uniformity is more favourable to sonar.
The objective was to repeat previous sonar contacts and, in addition, to establish some pattern that perhaps would lead to active underwater photography.
Hints to identity can also be gleaned from sonar contacts, especially if their movements can be tracked.
After initial forays in 1981, we decided that a major effort was worthwhile.
Thus, in 1982, from the beginning of May to the end of August, we operated two scanning sonars for more than 1500 hours in day-and-night patrols of the deep northern basin.
We decided to begin our search in the deep midwaters on the basis of the results mentioned above, even though, at first sight, biological considerations might have suggested that inshore waters were more promising.
Besides, sonar work is easier in midwater and the results are less ambiguous; we attempted inshore exercises in Urquhart Bay but the results were confused by the loch bed, mooring chains and the wakes of boats.
In fact we found that the entire surface area was well stocked with fish.
We used three kinds or sonar apparatus.
To establish general conditions, we used a Skipper 603 echo sounder (50 kHz, beam width 33 o ).
In May and June we operated a Furuno 106A, with a higher frequency (150 kHz) and a narrower beam (9 o ) from a 20 m barge.
As shown in the colour photographs, the Furuno represents the strength of each trace according to a colour code, with the strongest echoes shown in red, and the weakest in blue.
In July and August we operated a Simrad SY, with a frequency of 80 kHz and a beam-width of 11 o from a motor cruiser made available by Caley Cruisers, Inverness.
We operated from fixed points with the vessels motoring or drifting as near to the centre of the basin as possible between Foyers and Urquhart Bay, normally scanning the beam over a sector of 120 o in the direction of travel at a tilt of 30 o to 40 o , to give an effective search to a depth of 160 m.
We corrected the course on the reception of side echoes, which are clearly identifiable, with the range set to 250 m.
Our objective was to identify and characterise anything unusual in the loch, so we had first to define the sonar characteristics or the things we knew were there: notably to establish the position, strength and movement of signals received from fish.
To do this, we searched specifically for fish with the Skipper 603 echo sounder.
We adjusted the instrument to full sensitivity, and found that a standard fish — a 330-g live trout — barely registered at depths below 150 m.
The alpine char we had already caught at 220 m, weighing around 220 g each, would not have registered at that depth.
The flesh of fish gives only a weak sonar signal; more than half the signal from a fish comes from its gas-filled swim bladder, which is relatively small.
(An air-breathing animal, if such the monster is, would have large lungs and would be expected to give a strong signal.)
To establish a standard against which to measure any strong signals, we calibrated the Furuno sonar against a gas-filled sphere, 24 cm in diameter.
Thus we knew what fish would look like on our sonar, and we knew what kind of signal a standard gas-filled object would produce.
We also established that the fish, abundant over the whole surface, were concentrated in the top 30 m.
Accordingly, we reduced the sensitivity of our scanning sonars to eliminate most of the signals from fish, to the point where only larger fish registered in the surface layers, and the calibration fish, the 330-g trout, was barely detectable at 25 m.
We then set out to look for contacts that come from sources other than the usual lake fish.
We selected contacts with the following characteristics: * Those deeper than the usual limit of the fish echoes (30 m).
* Those stronger than the usual echoes produced by fish.
* Those indicating movement of a kind not normally associated with fish.
We restricted this to vertical movement, because it is more difficult to record horizontal movement accurately.
Positive contact?
We established many such contacts: 12 from the furuno sonar and another 28 from Simrad.
The total number, though far higher than in earlier studies, is not so significant; after all, we spent more time tracking in deep water than all previous studies put together.
What is significant is that the signals had consistent characteristics, and we observed no other kinds of strong echoes in midwater.
Most were beneath the deepest detectable layer of fish (60 m) though some occurred within it.
On some days they seemed absent from the area and then we would detect two or three in the course of the next patrol.
The figures show some of the traces, those from the furuno sonar being in colour.
Note in particular the trace taken in deep water off Urquhart Castle at 17.25h on 16 May, 1982.
The target was demonstrably stronger than the signal from the 24-cm calibration sphere, and we tracked it for 68 seconds, in which time it seemed to dive from 69 m to 114 m (a speed of0–8 m/s or just under 3 km/h).
The strength, depth and probable movement of the target are extraordinary.
Is this the famous and elusive monster?
We can say only that the project has some experienced sonar experts, including Bill Thomas of Kelvin Hughes and Dr Brian Woodward of Loughborough University, and from Simrad of Norway, who have the longest commercial experience of horizontal sonar in the world.
They have taken great pains to eliminate explanations based on freak signals from side walls, or from inanimate objects (although we cannot entirely discount such hypotheses until we have specifically investigated them).
We can say, too, that if there are large creatures in Loch Ness, then they would appear on sonar just as we have recorded them.
However, the excitement is not simply that we have produced data consistent with the presence of large animals.
The point is rather that we now have a strong data base from which to operate, and that the technical means are now available which would reveal precisely what our signals do mean; perhaps to show that there are monsters, perhaps to find, at least beyond reasonable doubt, that their existence is too unlikely to warrant further study.
We have not found monsters, and we have not disproved them, but have shown that there is still a case to answer.
My life with robots
Britain is well behind the US and Japan in making and selling robots.
But events need not have turned out like that.
Here the founder of what was, for many years, Britain's only robot firm tells how the UK missed not one but three golden opportunities to leap forward in this technology
Douglas Hall
THE DEVELOPMENT of robots in Britain really began in February 1966 when I read an article in New Scientist called ‘A robot factory worker’.
The article was written by Joseph Engelberger, the founder of an American company called Unimation (now the world's biggest robot firm).
Engelberger talked about what he called the Unimate, a mechanical arm controlled electronically so it could move like a human's.
It was the world's first true robot.
At that time I was working for Hawker Siddeley Dynamics in Hatfield.
The firm's main work was producing components for aircraft but my job was to investigate new product areas for the company.
I was soon talking on the telephone to Engelberger in Danbury, Connecticut.
And before long Hawker Siddeley's board of directors agreed to approach Unimation for a licence to make its robot in Britain.
Our plans were dashed when I learned that another British company, GKN, had already signed a deal with Unimation.
(GKN actually pulled out less than two  years later.)
But my interest in robots was fired.
I persuaded Hawker Siddeley to sell in Britain the Versatran, another brand of American robot, made by American Machine and foundry (AMF).
Starting in 1967, we tried hard to whip up interest in the robots among potential customers, but with little success.
The economics were very much against us.
A Versatran cost £10 000 (at 1967 prices).
It did the work of a person in a factory occupation such as loading sacks onto a pallet, but then that person cost only an average of £1300 per annum to employ.
Except where working conditions were very bad, it was hard to justify replacing humans with robots.
The sales drive was interrupted by a legal hiccup.
In 1957 a British inventor called Cyril Walter Kenward had filed a patent on an industrial robot that both Unimation and AMf had infringed.
The matter was soon settled by a cash payment.
It did, however, illustrate how people in Britain often have good ideas for inventions but then have to sit on the sidelines as no one is prepared to back them.
The robot business began to look brighter as we built up experience in putting the machines to work.
We adapted the Versatran to handle equipment for spraying paint so it could replace people in paint shops, often extremely nasty places in which to work.
But a few years later, in 1970 the Hawker Siddeley management decided to shift the production of the robots to Hawker Siddeley's factory in Lostock, Lancashire.
This was, and is, one of the largest machine shops in Europe and geared exclusively to aerospace and military manufacture.
(Nowadays the factory operates under the ownership of British Aerospace).
Moving production to Lostock had an immediate effect.
Costs were increased; the quality of the robots went down; and delivery of the machines was disrupted.
The result was that Hawker Siddeley soon stopped making robots altogether and, disenchanted, I left in 1972 to work for Pye Dynamics in Bushey, Hertfordshire, part of the Philips group.
‘No future in robots’
Pye's new robot programme started in January 1973, with myself at the head.
This time, instead of trying to make and sell another company's robot, I started from scratch.
After discussions with Binks Bullows, an American-owned maker of painting equipment, I designed a machine expressly built for spraying paint.
Under the deal, Binks Bullows was to buy a minimum number of the machines each year and sell them to its customers.
Four key members of my original team joined me from Hawker Siddeley and we started work on the first batch of 25 robots.
Binks had given the robot the somewhat unglamorous name RAMP, standing for Random Access Multi Program.
Unhappily, in 1974, Pye's top management began to query the project.
Some of the directors became adamant that there was no future in robots.
In the middle of these difficulties I was invited to visit the Chicago headquarters of Binks Manufacturing (Binks Bullows's parent company) to negotiate selling the machine to the US.
My problems with the management intervened.
After two cancellations that were made at the behest of the managing director, Peter Threfall, I had had enough.
I took a week's holiday and paid for my air ticket myself.
I left a note for Threfall saying, ‘I have bought myself a ticket and gone to Chicago to apologise to Binks for the ill-mannered attitude of the Pye Group management!’
The row strengthened my resolve to start on my own.
Pye eventually agreed to lease me a small factory in Watford, in which I set up my new company.
To my delight, the engineers working with me joined my new venture.
Hall Automation Ltd was formally constituted in July 1974, employing eight people.
In August we delivered the prototype RAMP paint-sprayer to Binks Bullow at its British base in Brownhills near Walsall.
Jim Redding, my chief mechanical designer, and I followed the lorry all the way to make sure nothing went amiss.
Binks Bullows made its first sale to a company in north-east England called Moorlight, which made fluorescent lamp fittings.
This was our baptism of fire and we learned many lessons.
The machine is still operating today.
In 1975, my engineers and I realised we should enter a new area of application — robots for arc welding.
But first we needed a partner with welding expertise, in the same way as with RAMP, we had arranged a deal with a firm established in paint spraying.
We reached an agreement with BOC under which it could provide the welding hardware that we would sell with our robots.
To support the operation, in 1977 BOC purchased 30 per cent of the shares in the company, so giving me a much needed source of cash.
Before this, all the shares in Hall Automation were owned by the Hall family.
Efforts in earlier years to raise money from the City had proved fruitless.
The first few months of 1978 saw two major events concerning the future of Hall Automation.
First, GEC indicated it would like to purchase the company.
But it then transpired that BOC was also interested in taking over the firm by buying the remaining 70 per cent of the shares.
So the GEC approach was blocked while BOC's lawyers got to work on the proposed take-over.
I badly needed more money, so the take-over could not come too quickly as far as I was concerned.
The other decision was to build a new factory.
Work started on a new base close to our rented unit in Watford.
The existing plant, which was none too salubrious, had already attracted much comment.
Indeed an engineer from Volvo, in Sweden remarked that he was surprised that my team could make such good products in such a grotty factory.
Meanwhile, during a visit to Sweden, I discussed robots for painting car bodies on a production line with Volvo engineers.
The problem was that the robots were usually too big and would not fit into the existing booths.
‘Why don't you make a smaller unit?’ said Hans Hermansson, one of Volvo's managers.
On my return to the UK, we decided to do just that.
There was undoubtedly a big market, as many car firms found that without small robots they could not complete the automation of their paint spraying lines.
Before long, my engineers designed a prototype of what was to be called COMPARM (Compact Arm).
It was to prove most successful.
On the acquisition front, however, the course ran anything but smooth.
In 1979, BOC changed its top management and decided it was no longer interested in Hall Automation.
The news was a body blow and for several months the company was on a knife edge.
Ultimately, I renewed contact with GEC which was still interested.
This time, of course, I was going to it cap in hand.
In December, 1979, GEC duly purchased 100 per cent of Hall Automation.
We became part of GEC-Marconi Electronics and as managing director I reported to Sir Robert Telford and continued to run Hall Automation.
We had moved into the new factory in June 1979 and the first effect of our association with GEC was an input of capital to purchase badly needed equipment.
Our only ‘machine tool’ up to that time had been a second-hand pillar drill.
In March 1980 the International Robot Symposium and Exhibition was held in Milan, and for the first time a British robot manufacturer showed its wares in a manner comparable with the best.
With the extra cash, the development of new products, particularly COMPARM, could proceed more quickly and in July 1980 we sold a total of eight COMPARMs to British Leyland, Ford, and General Motors.
In the meantime, GEC-Marconi changed its structure.
In October 1981, Hall Automation became part of GEC Electrical Projects at Rugby.
As a result of this change I was ‘promoted’ chairman and virtually cut off from any effective direction of the company.
I could not tolerate this for long and at the end of February last year I retired from Hall Automation; a sad day.
In May 1982 the Watford factory was closed and the operation transferred to Rugby with the inevitable loss of many valuable members of staff.
Thus the wheel had turned full circle.
It was a similar story to the events at Hawker Siddeley in the early 1970s.
The close-knit robot team of Hall Automation was obliged to move to a much bigger production unit and, as a result, lost much of its identity.
Loss of experience It is  difficult to understand the logic of the move.
The factory at Watford was very suitable and well located.
The move has resulted in the loss of much valuable experience.
GEC has had to pay out hefty sums in redundancy payments, removal expenses and the rent on a factory that is still empty.
Despite the skilled electronic engineers and software writers at Rugby, GEC has sub-contracted further work on the Hall robot's control system to an ex-Hall Automation employee now working on his own.
Clem Jansen, the box of GEC's new automation division, has gone all out to gain a licence to manufacture a Japanese robot.
And as recently announced (with applause from Kenneth Baker, the minister for information technology), GEC has acquired the rights to build the Hitachi Process Robot, a machine which bears more than a strong resemblance to a robot made by the Swedish company ASEA.
I am greatly relieved that the name Hall is now no longer associated with a company which, having all the necessary finance, does not feel it is able to build, for itself, robots that can compete with the world's best.
To end on a happier note, I had a marvellous time in the robot business.
Would I do it all again?
Unhesitatingly yes.
The most memorable thing?
The comradeship, loyalty, support and enthusiasm of all the people who worked for me.
Japan and US lead world's robot league
BRITISH industrialists came under fire last week for being slow to use robots.
Kenneth Baker, the minister for information  technology , said that in the area of robot applications ‘more remains to be done’.
Baker pointed out that ‘the level of robots activity in this country is still below that of major international competitors’.
Meanwhile, the British Robot Association said Britain ‘must do better’ in using robots.
According to the association's annual survey, the number of robots in Britain grew from 713 a year earlier to 1152 at the end of 1982.
The tally compares with 13000 in Japan, 6250 in the US and 3500 in Sweden.
France, Italy, Belgium and other western nations account for 950, 700, 350 and 1200 robots respectively.
The association defines a robot as a programmable device used either to manipulate or transport objects in a manufacturing process.
Of concern to the association is the growing proportion of foreign-made robots in Britain's factories.
Just one quarter of Britain's robots were made in the UK; 37 per cent came from the rest of Europe while 24 per cent and 14 per cent owe their origins to the US and Japan.
In the past year, one in even four of the UK's new robots was made in Japan.
Manufacturers in Britain, the rest of Europe and the US accounted for 23 per cent, 37 per cent and 15 per cent .
The British Robot Association also analysed exactly how the UK's robot owners use their machines.
A startling fact is that the biggest application — accounting for 762 robots — is in ‘non industrial’ applications, which include the use of robots as toys or teaching aids.
An increasing number of companies, some of them British, sell robots for use in this area.
Schools and hobbyists — the kind of people who own home computers — are among the biggest customers.
The biggest industrial application for robots in Britain, accounting for 249 machines, is spot welding.
The next most popular uses for robots, according to the survey are injection moulding of plastics, arc welding, surface coating of materials and painting, loading machine tools and putting things into packets or onto pallets.
Although some 40 firms in Britain supply robots, only half a dozen make them.
A couple of years ago, it seemed that the number of UK robot companies was about to mushroom, but the optimism turned out not to be justified.
Several would-be robot makers fell by the wayside, among them Remek of Milton Keynes.
Tom Brock, secretary of the British Robot Association, says he is disappointed that the number of robot firms has failed to increase.
The Department of Industry is trying to tempt more companies to enter this area.
Since April 1981, the department has given grants off 2.4 million to firms interested in producing new forms of robots.
The government has also handed out £5.5 million to companies using robots in new applications.
In another part of its robots programme, the Department of Industry puts up cash to help firms to pay consultants who advise them on robot applications.
Britain's leading robot manufacturer is the American-owned Unimation, based in Telford.
GEC-Marconi also hopes to become a force in robots.
Besides purchasing Hall Automation, the robot firm started by Doug Hall, GEC-Marconi has reached agreement with Hitachi to assemble the Japanese company's machines.
GEC-Marconi is also doing its own research work into robots.
Engineers are devising a modular robot (see picture) comprising standard sections that can be joined together for specific applications.
Meanwhile, the Science and Engineering Council is conducting its own robot research programme.
It is giving grants to universities to make ‘second-generation’ machines — robots with sense — in conjunction with partners from industry.
FORUM
The Super stroke of genius
P. V. Danckwerts on some of the technical, moral and psychological issues of developing the H-bomb in the light of an article written in 1954 but only just published
SUPPOSING a crash-programme to develop an atomic bomb had been instituted in, say, 1938?
‘If physicists had tried to discover a way to release nuclear energy before 1939, they would have worked on anything else rather than the field which finally led to the discovery of fission, namely radiochemistry.
At that time, concentrated work on any ‘likely’ way of releasing nuclear energy would have led nowhere.’
These are the words of Hans Bethe, leader of the Theoretical Division at Los Alamos in 1943–45 (the period that led to the development of the first atomic bombs) in an article written in 1954 but published in 1982 (’ Comments on the history of the H-bomb’,Los Alamos Science , fall edition 1982).
His article is designed to show that the situation was similar on 31 January, 1950, when President Harry Truman ordered an all-out effort to develop the hydrogen bomb or ‘Super’.
The Super was suggested by Edward Teller in 1942 and work on its development started as soon as Los Alamos was functioning in 1943.
The priority work of the laboratory was, of course, the development of fission weapons based on uranium-235 and plutonium — indeed, the production of a fission device was a precondition for the Super.
Very high temperatures and pressures, which could be produced only by a fission explosion, would be required to ignite the fusion reaction between hydrogen isotopes.
Teller was obsessed by the Super and although he contributed many valuable ideas to the fission project he declined to devote his time to the solid work required to see things through to practical conclusion.
In 1944 Bethe was forced to relieve Teller, at his own request, and his group from work on the war-time development of the fission bomb.
It seems that Teller remained an inexhaustible source of ideas but, as with Winston Churchill, ‘nine out of ten of them were useless and he needed men with more judgement, even if less gifted to select the tenth idea which was often a stroke of genius’.
Partly because of Teller's withdrawal it was necessary to bring in ‘British’ scientists; Klaus Fuchs became a hard-working member of the team.
From 1942 until January 1951 Teller was working on a concept called the ‘classical’ Super.
The objective was to use a relatively small fission device to trigger-off a thermonuclear reaction of arbitrarily large yield.
Two major problems had to be solved; Bethe refers to them as Part I and Part 2.
One might hazard a guess that Part I was concerned with devising a conformation in which the fission explosion would raise the thermonuclear material to the required temperature and pressure and which would contain the material at a sufficient density for a time long enough for a substantial amount to react before being dispersed.
Part 2 may have been concerned with the kinetics of the reactions between hydrogen isotopes under the conditions attainable.
However that may be, there were apparently no serious doubts about Part 2 and the GEORGE shot of 8 May, 1951, in the Pacific showed that a thermonuclear reaction could be produced by a fission explosion — although in this shot an enormous fission explosion was required to ignite a relatively small amount of thermonuclear fuel.
In any event, according to Bethe, the test had become almost irrelevant by that time.
Part I of the problem, on the other hand, had proved quite intractable.
Continuous re-assessment and new modifications and calculations seem never to have conveyed complete confidence, even to Teller.
‘Greatly simplified calculations were done but it was realised that they left out many important factors and were therefore quite unreliable.
Work was therefore concentrated on preparing full-scale calculations ‘for the time when fast computing machines become available’— a sentence which recurs in the theoretical reports of this period’(1946–1948).
Teller went to extreme lengths to rescue Part I, but in the words of Robert Oppenheimer, ‘The programme we had in 1949 was a tortured thing that did not make a great deal of technical sense.
It was therefore possible to argue that you did not want it even if you could have it.’
In 1950 Stanislaw Ulam, an applied mathematician, showed without the use of a high-speed computer that earlier estimates regarding Part 1 had been over-optimistic and that the whole concept was probably impracticable.
‘Teller was desperate between October 1950 and January 1951…
It was evident that he did not know of any solution.’
Some time early in 1951, the principle of a completely novel solution occurred to Ulam.
It did not evolve from the previous work on the classical Super (which would never have led to it) but was a piece of serendipity.
The idea came to Ulam while studying some  aspects of fission weapons.
It seems likely that it involved the utilisation of the X-rays emitted by the fission bomb trigger to propagate the explosion throughout the charge of thermonuclear fuel; travelling at the speed of light they could initiate the fusion reaction in all parts of the charge in a time much less than could be achieved by shock waves (travelling at perhaps 104m/s), so that a substantial degree of reaction Could occur before the material was dispersed by the explosion (New Scientist , 2 September, 1982, p641).
To quote Bethe: ‘It is difficult to describe to a non-scientist the novelty of the new concept…it was to me, who had been rather closely associated with the programme, about as surprising as the discovery of fission had been to physicists in 1939.’
It seems to me that Bethe (writing in 1954) did not give due credit to Ulam for switching ideas onto an entirely new track; judging from accounts given elsewhere (eg The Advisors , H. F. York, San Francisco, 1976) Ulam rather than Teller should be called ‘The Father of the H-Bomb’.
Ulam took his idea to Teller who developed and extended it so rapidly that the outlines of a practicable thermonuclear device were complete before the GEORGE shot (8 May 1951); this had been designed as part of the development programme for the classical Super which by that time had been junked.
It had quickly become clear to everyone that the new concept (the ‘Teller-Ulam configuration’) was likely to prove successful.
Oppenheimer said that it was ‘technically so sweet that you could not argue about that.
The issues became purely the military, the political and the humane problem of what you were going to do about it once you had it.’
The concept was translated into practical terms with such urgency that the first true thermonuclear device (MIKE) was exploded on 1 November, 1952, and gave a yield of 10 megatonnes (about 1000 times as great as the bombs dropped on Japan).
This account says something about the nature of invention.
As Bethe points out, you cannot order miracles to occur.
On the other hand, when one is obsessively searching for the solution to a problem, it may be consciously recognised when it occurs as a fluctuation in the random activity of the subconscious whereas in other circumstances it might not emerge at the conscious level at all.
There seems to be no particular reason why the idea should not have cropped up earlier (or indeed, later) but, according to Bethe, its practical implementation could probably not have been much advanced because this depended on the development of high-speed computers and highly efficient fission triggers; neither of these requirements could have been met at the beginning of 1950.
Bethe originally wrote his article as a polemic in the Oppenheimer-Teller-Super controversy; it was a reaction to the book The Hydrogen Bomb (J. R. Shepley and C. Blair, NY, 1954) which took an extreme anti-Oppenheimer position.
The immediate cause for the declassification and publication of the article was the book J. Roben Oppenheimer..
Shatterer of Worlds , by Peter Goodchild (BBC Publications, 1980), which Bethe considered to be misleading in its account of the development of the Super.
Bethe adds some philosophical footnotes to a primarily technical discussion.
‘I never could understand how anyone could feel any enthusiasm for going ahead.
I am sure that the President and his advisors and most of the scientists who went to work on this project had heavy hearts…
The unquestioning enthusiasm for the thermonuclear programme [among certain scientists]looks to me very much like the enthusiasm that many Germans felt in 1917 when the German government declared unrestricted submarine warfare.
This gave the Germans a temporary advantage, but later on was the main cause which brought the US into the war and thus caused German defeat.’
‘In the course of time the present [1954]conflict between Communism and Democracy, between East and West, is likely to pass just as the  religious wars of the 16th and 17th century have passed.
We can only hope it will pass without an H-bomb war’— a fitting comment, I feel, on the hawks and ideologues on both sides.
Three-D specs
ACCORDING to the Daily Mail , a doctor in Florida has just been busted for charging people £12 a time for a new dieting aid.
It turned out to be a cheap pair of spectacles with one red lens and one blue lens.
Presumably the sight of a red and blue meal puts you off eating it.
Could this be connected with the dismal failure of the latest form of 3D TV, as transmitted by TVS in Britain?
This purported to give viewers colour and depth from a drunken double image viewed through red and blue specs.
But most people just got a headache.
All the signs are that somewhere there is a warehouse full of unwanted red-blue 3D specs, going cheap and looking for an alternative use.
So be on your guard if you are offered a new way to cure gout see in the dark or watch an eclipse of the Sun.
Oh happy day!
Donald Gould on the joy of being 64
I HAVE JUST celebrated, or, rather, I have just had drawn to my attention, the occasion of my 64th birthday.
I awoke well before dawn on the glorious morn, being, as usual, nudged back into the conscious state by a prod in the face from the wet snout of the larger of my ‘two damned dogs’, who can't tolerate layabouts, and who, by the time they're announcing the fat-stock prices on the radio, is eager to be off and away on her early chase after swans and hares and similar misshapen denizens of the Fens.
One day she'll meet a swan who stands its ground (or should I say ‘sits its water’?) instead of just paddling disdainfully away from the natatorial hound, and that, as we country-folk say, will  larn her something proper.
Anyway, I got up, and went about my chores, feeding the cats and brewing tea for the rest of the still-slumbering inhabitants of my humble home in order to get them up and out to school and work, and I was thinking to myself as the kettle boiled that here was the start of yet another ordinary damned day, when the post clanked and slithered through the letter box.
And there, amongst the dung-coloured window envelopes, and the throw away journals, and the solicitations from financial experts offering to procure me a tax-free income for life come my death, was a card from one of my many dear daughters wishing me ‘yet another happy birthday’.
And that, I swear to you, dear reader, was the first moment since my awakening at which I had realised what a very special day it was.
‘Ah!’
I thought, ‘this will make the carrying of the tea to the remaining resident members of my family an uncommonly pleasant event, because they will all smile upon me instead of producing the customary resentful grunt when I stir them from sleep, and they will say ‘Good morning, dear husband/father (as the case might be), and they will reach under their beds for the gift-wrapped goodies so lovingly prepared and concealed the night before, and handing me their tributes they will voice their gratitude to God that I have survived so far, and will express the pious hope that they may be able to repeat such sentiments on this same date for many a year to come.
Not a bit of it.
Do you know, not a single rotten member of my miserable household appeared to be in the least little bit aware of the joyful nature of the dawn which I gaily urged them to acclaim.
If anything their grunts were surlier than usual.
Perhaps they resented my own by then ebullient high spirits.
Oh, I was cast down.
But I said nothing.
I was far too modest, and far too hurt.
I couldn't trust my emotions.
So I just went off quietly to run my bath, a sad and disappointed old man.
If I wasn't going to enjoy a plethora of gifts and compliments, at least I could soak myself in a great comforting pool of self-pity.
Never mind that I hadn't remembered my birthday either.
That wasn't the point.
As I scrubbed away at my remaining teeth I reflected upon the number of obituaries of men in their early sixties which I'd read in The Times over the course of the past few weeks, and I wondered whether the Great Reaper was running a special line in the age group for some divine, mysterious end, and if so, I thought, I might be next, and then they'd be sorry.
I was beginning to have quite a good time, and might have imagined for myself a series of tragic scenes of truly poetic power and solemn grandeur, and was wondering how my dear and attractive wife would look in widow's weeds, when this character started speaking on the radio, and totally ruined my train of thought.
This fellow, whose name I didn't catch, was being interviewed by one of the golden oldies on the Today programme.
He must have been about my age, too, because he'd been in Nagasaki when the bomb went on.
He'd been taken prisoner by the Japs, and had been shipped to Japan, and was working in a warehouse in the docks on the morning of 9 August, 1945.
He described how they heard a plane, and then came the searing flash and tempestuous roar, and then the utter unbroken silence of the grave.
By some freak of physics or geography the warehouse he was in remained standing, and he and his mates remained unseared and intact.
He described how they went outside, and saw the flattened port, and hundreds of bodies lying right around them, their exposed skin scalded, and he remembered the obscene and chilling stillness of it all.
All this was awful enough, but a yet more disturbing revelation was to come.
This fellow, who must be the only, or almost the only, surviving person outside Japan who has been the target of a nuclear bomb, and who knows what it's like for real (as opposed to the criminally bone-headed fantasising about nuclear war indulged in by our sillier soldiers and politicians), was not only matter-of-fact about it all, as though it was the sort of thing that might have happened to anyone, but he actually admitted that he had never given a thought to the possibility that he might be at risk as a result of the radiation he undoubtedly suffered at the time until recent weeks when various busybodies brought the matter to his attention.
If somebody who has been actually shot at by a nuclear weapon is so goddamned complacent about it all, and so pig ignorant of the implications of the happening, what hope have we got that public opinion (the silent majority) will be sufficiently stirred to stop our power intoxicated leaders from brandishing their nuclear swords?
Pride and egoism drives them on, and a shared stupidity stops us from stopping them.
I remember with horror a meeting in our village hall at which boy-scoutish enthusiasm was shown for the knot-tying, camp-fire, make-do-and-mend fun to be had out of getting things going again after the bomb.
Hearing that man made me glad I was 64, which fact gave me a sporting chance of dying a natural death before the inevitable holocaust.
It also made me realise that it was stupid to waste any part of what little time we all have left in silly sulks.
I cheered up at once.
So much so that I was ready for a rich joke when my family got home from their various doings that birthday evening.
The laundry had been delivered during the day, and there was a great, big, brown paper parcel lying on the kitchen table.
I came in from my writing room and said ‘Ah!
Is that my present?
You should have seen their faces!
Ham — the high flying Pongid
John H. Parkinson on the first astrochimp
WHO WAS the first American in space?
No, it wasn't Alan Shepherd or John Glenn.
Not even Neil Armstrong.
It was Ham, the flying chimp.
On 31 January, 1961, before man was brave enough to venture on high, Ham was sent up in a Mercury capsule to investigate the stresses and strains of space flight.
He was trained to operate levers and push buttons in response to flashing lights, each mistake being promptly followed by an electric shock to his feet.
During the night Ham was in fine form, which is more than can be said for the Redstone rocket.
It accelerated more than anticipated with the result that the capsule landed in the Atlantic over a hundred miles away from the target area.
When the recovery crew arrived two hours later they found the capsule was filling with water from the seven foot swell.
Back on the recovery ship, they opened the hatch and there was Ham with his arms crossed, looking gloriously bored by the whole episode, so they gave him an apple!
Last month, at the grand old age of 26, Ham died in a North Carolina zoo, but his troubles don't end there.
His bones will go to Washington for study, his skin will be stuffed and exhibited at the Space Hall of Fame in New Mexico and the rest if there's any left will be cremated.
If ever a chimp deserved a medal it was Ham.
The search for Methuselah
David Challinor reflects on the longevity of animals and plants
A CENTENARIAN'S recollection of a long past event makes it immediate and gives a human perspective not matched by any written account, or artifact.
Some aspects of this immediacy can also be transmitted by our contacts with non-human organisms whose lifespan far exceeds our own.
Although humans may have the longest lifespan of any mammal, giant tortoises have the longest recorded lives among vertebrates.
A well-known tortoise, named Jonathan, lives in the governor's garden on St Helena Island and was once famous for allegedly having seen the exiled Napoleon there.
Unfortunately, Jonathan arrived there in 1882 and missed the emperor by about 60 years.
He is actually the third Aldabra tortoise imported for the governor's garden, each of his predecessors, however, lived for about a century.
Such a lifespan is not uncommon for a captive tortoise, the oldest one recorded being a male, which lived on Mauritius for 152 years.
It may be difficult to age wild animals, but not so temperate trees, because annually they produce growth rings.
The ancient Greeks certainly aged their trees by counting rings and were probably not the first to do so.
The oldest European trees seldom exceed 700 or 800 years, so that European explorers seeing California redwoods for the first time were awestruck by their size and age.
Until this century, a tree had to be felled to count its rings.
Horizontal sections of enormous redwoods were frequently displayed with single rings identified on the polished surface.
The year that Christ was born, for example, was duly marked to help the onlooker bridge time long past.
Not many redwoods, or sequoias, exceeded 3000 years but they were generally considered the oldest living matter until the development of the Swedish increment borer enabled dendrologists (people who study the natural history of trees) to age a tree without felling it.
This elegant device came into practical use in the 1920s and, when screwed into the centre of a tree trunk, extracts a slender core on which the rings are counted.
Using the increment borer, the US Forest Service located some ancient bristlecone pines at the treeline of the White Mountains of east-central California.
Two scientists, Schumman and Went, sampled other pines there in the mid-1950s and found the oldest to be about 4900 years, thereby almost doubling the age of their then nearest competitor, the sequoias.
These twisted, stunted pines are growing at the extreme limits of their tolerance.
Barely staying alive, their rings are very close together.
Despite the ring density, variation in width coincides with irregular rainfall cycles.
By plotting ring patterns of trees in this area, scientists can date ancient building timbers, and other wood artifacts, by comparing ring patterns with those of the bristlecone pines (such studies have been given the name of  dendrochronotogy  .
These pines were considered the holder of the planet's longevity title until about a decade ago, when botanists studying lichens began to develop a new dating technique.
Lichens, unlike trees, have no rings or other manifestations of annual growth.
However, one can determine their age by extrapolation from their growth rate.
This can be done, for example, by locating on a century-old photograph of a receding glacier, a then newly exposed rock surface.
By estimating how long after exposure such a surface would be colonised by lichens, scientists can calculate growth rate, and thus age, by measuring the lichens’ present diameter.
Growth rates vary from 3 to 15 mm/ 100 years.
Lichenologists have estimated that crustose lichens in Alaska and Lappland grow at a rate of 3–4mm/100 years, which would make lichens there of 480 mm diameter at least 9000 years old.
Other ancient lichens have since been discovered.
What was thought to have been an alga growing on sandstone rocks in a dry valley of Antarctica, was identified only two years ago by a Smithsonian botanist as a lichen.
This new species lives below the surface of the rock in the interstices of its sand grains.
These plants are undoubtedly growing in the harshest terrestrial conditions on our planet and can metabolise only for about 300 hours a year.
This environment is far worse for lichen growth than any known in the Arctic and if we conservatively assume that the Antarctic lichens grow only half as quickly as those in the north, the former with a present diameter of 100 mm must be easily 10 000 years old and perhaps considerably older.
The existence of organisms still living after millenia make mere centenarians seem insignificant.
The relatively short human lifespan is thus put in a clearer biological perspective.
The passage of time, however, is a concept seemingly grasped only by humans, so Western man's search for Methuselahs seems to reflect his innate concern to fathom time.
Under steam and getting nowhere
Tam Dalyell joins the parliamentary commuters
NO SOONER had we settled in our seats when the burly Arthur Lewis (West Ham) blew in, waving an order paper, shouting ‘Point of Order, Mr Speaker!’
Unperturbed, Albert Booth (Labour's transport spokesman) raised steam and set the parliamentary engine moving in the great Westminster marshalling yard.
‘Nineteen eighty three,’ he growled into the open firebox, ‘would be the watershed!’
He referred, of course to British Rail which had suffered a drop in investment of £180 million.
That, and unwanted copies of the Serpell report on Britain's railways was something to stoke the fires with.
Sir David Price, who represents Eastleigh Junction and all stations south, just managed to jump  aboard the wagon claiming that the terms of reference of Serpell had been all right, but wrongly interpreted.
Booth agreed that there was a mote in the eye of ministerial beholders preventing them reading the timetable properly.
What about Norwich?
How fares it at Swindon?
The workshops at Shildon, County Durham?
(Railway communities all.)
And what would happen to the east coast route?
Westminster's station master apparent David Howell (first class hons in economics at Cambridge, who happens to represent commuter Guildford as well as being Secretary of State for Transport), woke up with a jolt.
He'd had a bad dream in which he'd been not only head of the Conservative Research Department, but also with the Raj in Belfast, and confronted by coalminers and oil-rig workers to boot.
He rose to his feet, but the thought of a glare from mother, who had shunted him to transport, and he was sitting down again.
Something of a row broke out with an awkward commuter.
Trevor Street had raised the spectre of the Bedford-St Pancras line.
Shocking things had happened there and £160 000 a week was being lost just because there was no agreement on manning.
Station Master Howell signalled annoyance claiming that there ha new investment and ‘a new generation of ticket machinery!’
William Rodgers, a former Labour station master, rose for the ailing Alliance Express.
Like all former managers and station masters, he was keen to justify his own stewardship.
He wanted much more investment — of the kind that was not available, when he was in charge of the station.
The next speaker was the curator of the railway museum — Robert Adley from Dorset.
Who, he asked, would dream of privatising the Royal Navy?
Adley reminded the House that each year 105 000 people were killed and injured on the roads.
One hundred and fifty had been killed on the railway — but many of those were suicides and not passengers.
A diversion was caused by the Faversham Flyer (Roger Moate) pulling against his parliamentary neighbour the Canterbury Chuffer (David Crouch) on unfair rises for the Kent commuters, and their interpretation of Serpell.
Clearly, travelling to and from the Garden of England would henceforth be no bed of roses, to mix a metaphor or two.
Walter Johnson (Derby) in his role as chief booking clerk, wanted to know why the Serpell ticket cost £600 000 before additions.
He challenged anyone to say where the unions had failed to deliver the goods.
Twenty-five thousand men had left the industry in the past two years.
The company-car ‘tax loophole’, he emphasised, was twice the cost to the taxpayer of the subsidy to British Rail.
A satisfactory return on the railway network, claimed my Flying Scotsman colleague Robin Cook (Edinburgh Waverley station) meant a cost-effective system, not an accountancy balance sheet.
What about congestion?
Why not a cost-benefit analysis of trunk roads?
If Serpell went through, not a single power station would be linked to a coal field in Great Britain.
A lack of technical competence shouted from the report, asserted Cook.
Maximise traffic, don't minimise it!
By the time Cook returned to the buffet car, MPs knew that the Commons had shunted Serpell up a rusting siding.
It only remained for Terence Higgins (Worthing) to wrap up Serpell in moth balls, where it will remain until at least after the next election
Fast prayers
DIRECTORS of public bodies like the Central Electricity Generating Board and the UK Atomic Energy Authority do not only arouse controversial views in the press it seems.
Our hack at the train buffet overheard recently just what the workers at Dounreay, where Britain's work on fast breeder reactors is based, thought of one of the UKAEA's director's.
(There are not prizes for guessing which one.)
It would seem that this director was flying in the authority's private, twice daily, shuttle to the site (being a wartime airfield it has its own landing strip) when one of the wheels fell off as the under carriage was being lowered.
The pilot radioed his plight but added that he would try and land anyway.
The scientists and engineers-on site were asked to line the runway and pray.
Despite their prayers, the director's plane landed safely claimed the raconteur.
Tail piece…
POOR OLD HMS Sheffield.
Not only has she attracted more publicity than any other naval vessel since the Mary Rose, but she wasn't even entirely British.
We can now exclusively reveal that part of her was — wait for it — Argentinian.
During her construction an explosion of welding gas damaged the stern section and killed a gang of welders.
But as modern warships are built in modular form, the solution was to cut off the damaged part and weld on another.
Fortunately, there was a stern waiting nearby to be tacked on to another Type 42 destroyer — the Hercules.
It is ironic that the Hercules is now in service with the Argentinian navy.
Adrift in the Atlantic
Dr Chris Lewis on the plight of the Cape Verde Islands
TELL your friends that you are going to the Cape Verde Islands and, in all probability, their eyes will glaze over for several seconds while they frantically try to recall their long-forgotten school geography.
Phone your travel agent to book a night there and the chances are that your request will be met with an equally incredulous silence, before receiving the inevitable, ‘Where?’
Even when you have convinced the travel agent that there is such a place, you are likely to become involved in a series of long telephone conversations.
The outcome will be that he claims he can book you a flight only to the international airport on Sal Island.
This I subsequently discovered to be false.
But from there you will have to make you own arrangements via the local Cape Verde Airline offices.
You may also be told that there is an alternative route via Dakar, some 500 kilometres away, which may give you a connecting flight to Praia, the capital on Santiago Island — provided, of course, that the plane turns up.
I chose the more orthodox route via Lisbon and Sal and arrived safely at my destination at Praia on the appointed day.
Others were less fortunate and spent more than a week exploring the delights of the Senegalese capital before completing their journey.
Not many Britishers, it seems, visit Cape Verde.
Charles Darwin did, but he had no need for a travel agent.
Doubtless he had his own reasons for stopping off at the island — my own were to attend a workshop aimed at formulating a scientific plan for the development of the archipelago over the next two decades.
But as the plane taxied along the runway at Praia airport and I looked out on what could have been a simulated view from a lunar module, I sensed how daunting the task would be.
The main problem with Cape Verde is that it hardly ever rains.
It would be the ideal venue for a timeless Test Match and Geoff Boycott would love it there, as would the ground-staff of Old Trafford and Headingly, for example.
(Though whether Boycott would prefer Cape Verdian escudos to South African rands is another matter.)
The catastrophic effects of the most recent drought can be gauged fact that in 1967 the country imported over 30 per cent of its food requirements and yet only seven years later this figure had risen to a colossal 96.5 per cent in what is nominally an agrarian economy.
On this basis alone the islands have clearly exceeded their carrying capacity, the more so when it is realised that almost all the food imports are in the form of foreign aid.
For many people the only possible escape from their permanent state of poverty and malnourishment is to emigrate.
This has been so for the Cape Verdians for close on 300 years, from the time when the original emigrants first found jobs on the North American whaling fleet.
Even now many families on Cape Verde depend for their existence on money sent back from relations working abroad.
This was brought home to me after witnessing the rapturous welcome given to about a dozen young Cape Verdians as they carried their enormous radios and bottles of duty-free Scotch to their waiting loved ones in the airport arrival lounge.
They celebrate the return of their not-so-prodigal sons in some considerable style.
The economy of the Cape Verde Islands has been precarious ever since the Portuguese discovered the uninhabited archipelago around 1460.
The European settlers gradually increased in numbers and additionally brought over slaves from the Guinea Coast thus bringing pressure to bear on the already sparse natural vegetation.
At the height of the slave trade, Cape Verde provided a transit camp for the shipment of African Negro slaves and various crops and livestock to the New World of Brazil.
In the reverse direction the country received its most abundantly grown foodstuffs of today — maize and cassava.
However, yields have always been low because of the climate.
During this century alone more than 83 000 Cape Verdians have died of starvation — an astonishingly high proportion of the population.
Yet numbers continue to rise.
In 1730 the population numbered 38 000; in 1830 it was 51 500; increasing to 147 500 by 1900; 201 500 in 1960 and marginally under 300 000 today.
The people are bright, reasonably well educated and unusually patriotic considering that they are invariably descended from a mix of European and African Negro forebears.
Their spirit has not yet been extinguished by a virtual 14-year drought.
And, while they are badly in need of such innovations as a massive reafforestation programme, desalination facilities, an organised fishing industry, means to exploit their few mineral resources, and perhaps even tourism, they will not give up easily their island heritage.
The outcome of the workshop was a series of priority recommendations for a development plan.
Reafforestation with genera like Acacia and Prosopis .
which can yield 2 tonnes/hectare year of dry wood even on unfertilised, very dry soil, was considered to be crucial for upgrading the environment as a whole.
Tree cover would prevent further soil erosion, capture some of the precious water vapour in the atmosphere, and eventually furnish a renewable source of fuel and fodder.
The formulation of a watershed management policy linked to searches for subterranean reserves and a desalination programme was considered just as important.
Solar desalination plants, though requiring a great emphasis on cash rather than labour, seem the best long-term bet here.
Indeed, renewable energy sources in general— whether direct solar, biogas, and particularly wind — all have possibilities in Cape Verde.
The establishment of genetically improved crops and livestock: the biological control of insect pests of plants; promotion of the local fishing industry to commercial status; exploitation of minerals, such as limestone, sand, salt and kaolin; the controlled encouragement of tourism; and the extensive training of managerial and skilled personnel to run the various programmes are all much needed.
In turn, they all need money to run them.
Only a secure supply of foreign aid (over US$1 billion up to 1990) will keep the country from going to the wall — and that aid can by no means be guaranteed.
In any event, it will be a long haul for the Cape Verdian people to escape their poverty trap even if things go well.
The portents are not what one would call auspicious.
But my enduring thoughts as I finally flew away from the Cape Verdian sunshine back to Britain was that these people deserve to succeed.
They are not taking things lying down as many other Third World people tend to do.
I for one wish them well!
LETTERS
Working well
Congratulations on your success — at the second attempt!— in moving New Scientist so smoothly into computerised phototypesetting.
Having also personally experienced the problems at Southwark Offset, I realise how daunting the prospect of trying again must have been.
In your natural desire to extol the virtues, however, your leader (’ Moving with the times,’27 January.
p 214) runs the risk of minimising some of the human problems posed by new technology.
It is certainly true that, given the right circumstances, computers can ‘reduce the drudgery, freeing the brain in front of the screen to do more productive things’.
They can also create boring, frustrating mechanical jobs.
As your main article suggests (’ New Scientist says goodbye to Gutenberg,’p 234), information technology does have the unwelcome ability to replace satisfying craft skills with mundane key-tapping tasks.
One of the biggest challenges facing us all, surely, is devising ways of introducing new technology to meet human, as well as organisational, objectives.
A decade from now we might, as you suggest, be wondering what all the fuss was about.
Alternatively, enterprises might be suffering the consequences of poor job design in the form of disgruntled employees in soul-destroying jobs: the information era could mean the equivalent of the production-line chores epitomised in Charlie Chaplin's Modern Times .
The Work Research Unit's experience indicates clearly that, with foresight and determination, the pitfall of dehumanised work can be avoided.
That is if jobs are designed that offer job satisfaction, rather than alienation; challenges rather than boredom; skill enhancement rather than deskilling, so that the technology is seen as a set of tools for people to use in their work rather than the basis of a production system in which people are merely ‘operators’.
But work has to be planned with those aims as clearly spelt out as the financial or operational objectives.
 Richard Smith Work Research Unit Department of Employment 
Not so Super
Your piece on Super-SARA (This Week, 27 January, p 215) is very wide of the mark.
The issue is not whether the UK should back nuclear  safety research at Ispra (those of us in the field in the UK are all in favour of that), but whether the majority of the resources (and even an increase in the planned budget) should be concentrated on a single project which cannot possibly give results until late 1986.
Many technical experts in this country do not believe this to be sensible and know that similar views exist amongst their European colleagues.
Back in 1979, we supported a Super-SARA programme which was then expected to start in 1982.
As your article said ‘Super-SARA has had a long and troubled gestation period’.
Time is passing Super-SARA by.
We needed in-reactor confirmatory data on the clad ballooning phenomenon mentioned in your article.
We could not wait and joined with the US Nuclear Regulatory Commission to commission experiments in Canada.
This removed our interest in the first phase of the proposed Super-SARA programme.
Later phases are also likely to be covered earlier by broad-based  programmes including reactor testing by the US and France with laboratory work in Germany and the UK.
In view of this, European ministers (not just in the UK) are rightly concerned about the cost effectiveness and timeliness of the Super-SARA programme.
 David Hicks UK Atomic Energy Authority London 
Petering out
With reference to your article on the inconsistencies in the Serpell report (This Week, 27 January, p 220) the town of Keith is mostly surrounded by farmland and is certainly not in the middle of a peat bog (Scottish or otherwise).
I suspect that the reason for a terminus in that town has more to do with the presence of several bonded warehouses than with objective computer modelling.
Perhaps the computer model wishes to continue receiving supplies of the ‘creative inspiration’ which it obviously used.
 James Tweedie Wester Ross 
Malaria argument
If anyone has been selective and inaccurate, and consistently so, it is Messrs Chapin and Wasserstom (Letters, 20 January, p 189).
They insist that insecticide resistance, due in the main to their agricultural usage, is the principal cause of failures to reduce malaria transmission by an attack with them on the mosquito vectors.
I think they will find that most authorities will agree that it has proved a major obstacle but by no means the only one.
There are still important vector species which remain susceptible throughout their distribution for example,Anopheles darlingi, A .
nunezovari, A. punctulatus and A. balabacensis ; and many where resistance is confined to relatively small areas.
A. culicifacies , the sole vector in Sri Lanka, remains susceptible to BHC and malathion and malaria persists there; in India, and possibly in Pakistan too, the same species shows ‘pockets’ of resistance to malathion and even to fenitrothion but extensive areas still exist where there is no evidence of organophosphate resistance.
Some species, for example A .
arabiensis in the Sudan, show resistance to malathion only in the adult stage and it seems evident that such resistance was not in fact selected by agricultural usage.
Resistance, other than to the organocholorines, is confined so far to only five important malaria vectors so the potential for chemical control still exists in many parts of the world It is very easy to advocate the use of alternatives to these chemicals but if you examine those for vector control in detail you will find that most are directed against the aquatic stages of the mosquito and few cases of adequate malaria control by such an attack on any significant scale can be cited.
No, the fact remains that malaria is on the increase and has been for a number of years now.
To my mind this is because the present policy of control to an ‘acceptable’ level is subject to different interpretations by different health authorities and the extreme interpretation can be to accept any level as is at present the case in most parts of tropical Africa.
 George Davidson London School of Hygiene and Tropical Medicine London WC1 
ARIADNE
ALL THOSE who doubted, and I was one of them, the existence of the exploding rats, find themselves confounded.
They were made all right and used in even more subtle ways than I explained.
Furthermore, rats were not the only artificial animals used.
It was not only animals, either.
Information has come in from readers, amused, serious and knowledgeable.
For example, the deployment of explosive rats where they would do the most good, or harm (in wartime the two may be synonymous) was aided by accompanying them by leaflets saying that the British were delivering rats infected with bubonic plague and the best thing to do with them, if found, was to throw them into a furnace.
There was also, I am told, an explosive ‘dead dog’ which floated in canals to fetch up against a lock gate.
Nobody took any notice of a dead dog, but it had a timing device that blew a charge against the gate.
I cannot imagine who the people were who thought up such fiendish dodges, but they stopped at nothing.
They invented lumps of explosive coal for railway engines and even false elephant and horse droppings that could destroy vehicles or their tracks or tyres.
A masterwork was the exploding bicycle pump, just like the German army issue.
The real pump could be whipped off and the exploding one clipped on.
When it was removed, it blew.
This was so successful that the army stopped cycling in some districts.
Worse still, there was the development of an exploding saddle, though whether this came to anything is debatable.
I am in two minds about the humour of all this.
It undoubtedly appeals to the sadistic side of human nature and I must confess that while horrified at the idea of maiming or killing people, my first reaction is to laugh.
I like to think that I am laughing at the idea, which has as real a set of consequences as bombs in a Tom and Jerry cartoon.
There is a book on all these stratagems and spoils, called Magic Top Secret by Major Jasper Maskelyne.
It was published by Stanley Paul, but it must be long out of print.
AIRSHIPS offer the most rewarding and comfortable way of travelling, I reckon.
Without going through the long list of advantages that they possess, one or two are extremely appealing — a fast enough speed of about 60 miles an hour, low enough height for passengers to be able to enjoy the view and an ability to stop without falling.
The two blows that scuppered them were, of course, the crash of the R101 and the Hindenburg disaster in New York.
In spite of the revival of interest in airships now they can be filled with cheap helium, they are still unattractive in most circles and it is no good quoting the figures for people killed in airship crashes against those killed every day on the roads and similar statistics, or the successful career of the R100, although filled with hydrogen.
Comparing notes with an airship enthusiast the other day, I learned that the Falklands campaign might bring the airship back into favour because it can be sent up for a good look at the surroundings, especially at sea.
Satellite surveillance is all very well, but it takes time to get information back and in that time a naval force can have moved a fair distance.
Airships are not all that easily shot down, a disconcerting fact to pilots in the First World War who expected tracer to ignite hydrogen gas at once.
The job is made more difficult now because of self-sealing materials and the use of helium.
Even if an airship is shot down, the loss is negligible, or would be if the reconnaissance machines were small, which, apparently is the suggestion.
HEAVEN is my witness that I do not want to be unfair to British Telecom, so I must, with frank and honest gaze, report that I have had several letters saying that I am up the pole in carrying on about the method of charging recently mentioned in this column.
They all make the same point, and it is a reasonable one, that manhours means manhours and evermore shall do so.
But then, I ask, what is the point of the Telecom instruction saying ‘the total number of man-hours (ie not per man)’?
Is that ambiguous or not?
If it is crystal clear and I am convicted of being in my dotage or of going on at half-cock then I shall ask British Telecom to accept my apologies wince at the thought of my next bill, and keep my nose clean.
A GREAT black question mark faced millions of people recently in their newspapers.
In the hollow of the mark were the bold words, ‘Did you know’.
The question, posed by the Tobacco Advisory Council, was asking if readers knew that if smokers did not pay £11.5 million a day in tax, the basic rate of income tax would rise by about 5p in the pound.
This seems an odd argument for smoking to me and, I should have thought, to smokers, too.
On the other hand, for all I know, it may be one of the compensations or rationalisations for giving yourself short breath, a cough and a foul mouth and for making things unpleasant for other people.
But I do not know anyone who would willingly die, for instance, of lung Cancer on the virtuous grounds that he was helping others through his taxes.
The inference to be drawn is that, if we want to keep taxes down we should encourage everyone else to smoke like a forest fire.
The council says that it is speaking up for smokers.
It is, at least, doing its best to raise the price of cigarettes.
Daedalus
MEDICAL radiation treatment is unselective.
The X- or gamma-rays damage all the tissue in their path.
not just the target tissue.
But bacteria can be ‘stained’ with specific dyes, to clarify their structure for microscopy.
So Daedalus is inventing novel ‘X-ray stains’ to make bacteria, etc., absorb X-rays strongly.
The scheme exploits the characteristic energy-spacing between the electron-shells in an atom.
An electron descending from one shell to a lower one emits an X-ray of a characteristic energy (this is the basis of X-ray fluorescence analysis).
So, argues Daedalus, an atom intercepting an X-ray of this energy will absorb it completely by resonance absorption, promoting an electron up to the higher electron-shell.
The energy thus captured by the atom will be discharged into the molecules that surround it.
DREADCO biochemists are devising ‘X-ray stains’ and ‘X-ray antibodies’.
They contain harmless types of atom rare in biological tissues and possessing strong characteristic X-ray fluorescences (gadolinium, dysprosium, etc.).
Swallowed, they will bind to the bacteria or target tissues inside the patient, with little obvious effect.
Then the patient is exposed to X-rays of the correct energy, fluorescently generated from the very element incorporated into the ‘X-ray stain’.
Instead of damaging everything indiscriminately, the X-rays will all be perfectly absorbed by the stained bacteria.
The vast X-ray energies suddenly dumped in their little laps will kill them in a few seconds, giving the fastest cures on record.
Rats and many other creatures can ‘sense’ X-rays.
Daedalus reasons that their nerve-endings must contain some heavy metal that absorbs X-rays by this sort of mechanism; the captured energy then fires the nerve.
So he hopes to come up with a special X-ray stain binding to human nerve endings.
Workers in nuclear and radiographic installations would then not need radiation badges, or monitoring instruments to warn them of the unseen danger all around.
They would feel the disquieting twinges of radiation and take precautions accordingly.