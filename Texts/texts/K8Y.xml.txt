

Published statistics
6.1 THE COLLECTION OF DATA
In the conventional view, data are collected by the sponsor of a study to attempt to answer the specific questions posed.
This may require that an experiment be undertaken where the variables of interest are to a greater or lesser degree under the control of the researcher.
Usually a number of variables are set at predefined levels and the resulting values of one or more ‘response’ variables observed; this would be the situation in an industrial experiment to study the variation in yield of some product resulting from changing levels of temperature, amount of catalyst, process time, and so on.
Such an approach allows us to experience nature's response to the controlled variation of an environment.
Indeed, the French call an experiment ‘une expérience’.
We consider experimental method further in Chapters 15 and 16.
Alternatively, a survey may be used to gather data in circumstances where little or no control over the value of variables of interest is possible.
This is usually the case for studies of human beings whose lifestyles cannot generally be  manipulated except possibly in some short psychological experiment or to further medical research.
But a survey is often the only approach even for the physical sciences; an engineer, investigating the effects of gusts of wind on a suspension bridge must accept the wind speeds that nature happens to provide during the period of observation.
A third means of finding data for certain areas of research, particularly but not exclusively in the social sciences, is to look at information previously collected by someone else for their own purposes.
In this country, the Economic and Social Research Council frequently awards research grants only on condition that data collected will be deposited in the Council's archive together with a full description of the material and any known deficiencies.
A good deal of government-collected data are also held.
Such archived information is frequently used in research because it is more or less free; but, to avoid fallacious conclusions arising from what is usually called secondary analysis, we must be prepared to think critically about the source, reliability and consistency of the data involved.
In particular we need to check that the population defined and the sample extracted by the originator of the material are consonant with the objectives of our own study and that there is no ambiguity in any technical terms or schemes of coding.
Sadly, even information specifically stored for secondary analysis can be marred by obscurities in definition that are not easily resolved.
A somewhat different aspect of the re-use of data involves published summary tables.
As time goes by, more and more information is being routinely gathered on more and more aspects of life by government departments and international agencies.
This ‘growth industry’ feeds on accelerating social change and the increase of centralized planning and control and is aided by the computer's ability to store vast amounts of data and produce extensive tabular summaries.
Because figures published by government bodies are usually of high quality, we shall concentrate on these in what follows although other data sources are mentioned.
We begin with a brief and necessarily incomplete review of UK statistics; the general provisions are typical of what is available in countries of the Western world.
6.2 UK PUBLISHED STATISTICS
Readers are reminded that the term ‘United Kingdom’(UK) includes Northern Ireland, whereas ‘Great Britain' (GB) does not.
Of the constituent countries of the UK, Northern Ireland had a considerable degree of devolved government but this has been modified in recent years pending resolution of Ireland's partition problems.
Scotland has no separate parliament but differs from England so greatly in law and education that a fairly autonomous civil service operates from Edinburgh (the Scottish Office) and there is also a separate Registrar General.
Wales is almost always treated as though it were an offshoot of England although there is a small group of civil servants in Cardiff for specifically Welsh matters.
Needless to say, all this is very confusing to outsiders!
6.2.1 The census (head counting)
The 1920 Census Act specified that population enumerations should take place at intervals of not less than five years, conducted by the Registrars General of England and Wales, of Scotland and of Northern Ireland.
In fact there has been a complete census in the first year of every decade since 1801 (excepting 1941).
Owing to faster population growth and an internal migration rate approaching 10%, an additional 1 in 10 sample was taken in 1966 and this would have been repeated in 1976 but for economic stringencies.
The Act lists basic questions to be asked but permits census-to-census variation and often a census will concentrate on some topic of current interest.
For instance, the censuses of 1971 and 1981 investigated country of origin more fully than had been done previously to extend knowledge about the immigrant population.
In many countries, notably the USA, censuses are de jure, i.e. a person is enumerated in respect of his or her ‘usual’ residence.
Although in 1981 a question was asked about usual residence, UK censuses are de facto, i.e. individuals are enumerated on the basis of where they are at the time of the census (This avoids the need to define the distinction between temporary and permanent residence).
The method is least satisfactory in health resorts, coastal ports, London etc., and  it is obviously necessary to avoid peak holiday times and advisable to choose a weekend.
A Sunday in April is the usual compromise.
It has been estimated that de facto registration inflates the total by about 0.2% because of foreign visitors and that about 2% of the true population are displaced.
In Great Britain there are around 2300 census districts each in the charge of a census officer (usually a member of the local registration staff) who controls specially recruited enumerators — one for each of the 110 000 or so enumeration districts.
All are given extensive training to clarify their duties.
The enumerator must deliver a schedule to the head of each household (including caravans, tents, etc.) in his or her district a few days before census day.
This should be completed by the head of the household on census day and it is collected by the enumerator within two days, any queries being sorted out.
The forms are checked and returned to the census officer for coding.
Enumerators summarize some basic figures and these can be used to prepare preliminary tables for early publication.
Finally external checks on accuracy (e.g. by small-scale postal survey) are made.
6.2.2 Vital registration (event counting)
The registration of births, marriages and deaths has been compulsory since the middle of last century and is used to supplement census figures in intervening years.
Births must be registered within 42 days; the information includes date and place of birth, name (if decided), sex, name of father, maiden name of mother, father's occupation.
(In Scotland, additional information on date and place of marriage allows easy construction of family trees which is vital in genetic research.)
As an encouragement to speedy registration, payment of child allowance is made dependent on production of the birth certificate.
Since 1926 (1939 in Scotland) stillbirths (over 28 weeks gestation) have been compulsorily recorded together with cause.
Death being a fairly serious matter, a body cannot be disposed of until a doctor's certificate as to cause of death has been received by the registrar.
This should be within 5 days of death.
The information recorded is date and place of death, name, sex, age, occupation (of husband for a married woman), cause of death, marital status, age of surviving spouse if married.
Marriage has both religious and legal aspects and this results in a complicated registration system.
The information recorded is date, precise place of marriage and form of ceremony and, for both parties, name, age, marital status, occupation, residence and father's name and occupation.
6.2.3 Special sample surveys
Some examples are given below
(a) Family Expenditure Survey
This involves a survey of about 11 000 households per year from all over the UK  to obtain information on personal income and expenditure.
The interviews take place throughout the year to take account of seasonal variation.
The information collected is used in the construction of the retail price index (see Chapter 8).
(b) General Household Survey (since 1971)
About 13 000 households are interviewed each year in Great Britain, the questions covering constantly varying subjects such as family composition, housing, unemployment, illness, and medical attention, long-distance journeys, educational qualifications, etc.
Once again, the interviews take place throughout the year.
(c) International Passenger Survey
A survey is made of a random sample of about 100 000 passengers into and 125 000 passengers out of main air and sea ports.
(A migrant is one intending to stay out or in for more than 12 months.)
6.2.4 Continuous Population Register
Such a register was available in the UK only from 1939 to 1952, but they have been widely used in, for example, Scandinavia (since the seventeenth century), Belgium, the Netherlands and Switzerland.
A record (nowadays computerized) is generated at birth and information on marriage, children, internal migration, change of occupation, etc., is added throughout the life of the individual.
This is clearly very useful but administratively difficult and expensive.
All the countries mentioned above continue to hold censuses to check against deterioration in quality of data held.
6.2.5 Health statistics
Partly because of a worldwide interest in comparative health studies, statistics on health are a major component of UK government publications.
(a) Hospital Inpatient Survey
This is a 1 in 10 sample in England and Wales — a complete census in Scotland at discharge or death.
It is used for administrative purposes (bed usage, duration of stay, operations, etc.) and studies of illness treated in hospital.
(b) Special databases
Several databases have been established to provide continuous monitoring of the health of individuals, e.g. the Nottingham Psychiatric Case Register, the Medical Research Council General Practice Patient Register (in Edinburgh, for genetic studies).
These are not government controlled.
(Occasional controversy in the press has caused public concern that medical records held by computers are less confidential than those left lying around general practice receptionists' rooms and hospital records offices.
For example, fear exists that growing computerization of personal records might enable a potential employer (particularly the Health Service) to have access to private medical information and, for this kind of reason, the National Insurance Number, which might otherwise be a useful patient identifier, is never used as such.
As a result of these fears, the Data Protection Act which came into operation in 1986 seeks to regulate the ways in which personal data can be stored by electronic means and gives the individuals concerned rights to challenge accuracy.)
(c) General Household Survey
(See Section 6.2.3(b).)
(d) Other special surveys
There are many regular or occasional studies of, for example, psychiatric illness, casualty services, handicapped children, cancer incidence, and so on , sponsored by research councils, health boards, academic institutions and drug companies.
6.2.6 Education
An enormous amount of information is now collected on the performance of individual pupils throughout their school and further education careers and on numbers and grades of teaching staff, e.g. the Universities Statistical Record held on the UCCA computer.
(Elaborate confidentiality precautions are taken.)
6.2.7 Crime and justice
Extensive amounts of data are collected on convictions, the prison population, children in care, etc.
6.2.8 Publications
The publications resulting from these collected data are far too numerous to list individually though such a directory is available.
The main sources are:
1.
Annual Reports of the Registrars General for England and Wales, Scotland and Northern Ireland.
2.
Annual Abstract of Statistics — published by the Central Statistical Office and covering not only current figures but the preceding 10 years.
3.
Monthly Digest of Statistics — mostly economic and financial figures.
4.
Special publications for Scotland, Wales and Northern Ireland, e.g. Highlands and Islands Development Board reports.
6.3 RELIABILITY
Often common sense or investigation of the method of data collection will reveal that not all data are equally ‘hard’(i.e. reliable).
There can be no boundary between soft and hard information — being aware of the problem in relation to the proposed use of the data is what matters.
Generally, government publications (in this country) are reliable, though even civil service reports are censored against the presence of politically sensitive material, e.g. number of coloured people.
Because constituency boundaries are fixed on the basis of figures produced by the Registrars General, these officers are not subject to ministerial control and their publications are more likely to be free of such problems.
Indeed the census total usually turns out to be very close to the estimate derived from the previous census figure corrected for intervening births, deaths and net migration.
Nevertheless, it will be appreciated that, for example, in the census, the main obligation for accuracy rests on the head of the household, who may intentionally or unintentionally provide false information.
Random errors will not matter greatly; what is of concern is the possibility of bias.
For instance, there is a tendency for those with a birthday imminent to return the higher age instead of the attained age, whereas middle-aged women still tend to understate their ages; this phenomenon is sometimes called ‘age-shifting’.
There are inordinately large numbers of ages ending in 0 and 8 and, to a lesser extent, 5; this is known as ‘age-heaping’.
Memory errors resulting from questions such as‘Did you live in your present house five years ago?’ are particularly troublesome.
Occupations tend to be upgraded in status, for example, by implying possession of a special skill or suggesting that the work is in a supervisory capacity.
Those who are unemployed or retired tend misleadingly to give their former occupation.
Of the information collected in vital registration, cause of death is obviously least reliable.
Doctors will often differ as to cause, particularly since ‘died of old age’ is not a permitted entry; the unreliability is therefore greatest for the oldest age groups.
Furthermore, since up to three causes are allowed for in the record, the doctor may indicate, say, pneumonia as ‘primary’(i.e. immediate) cause before a more basic diagnosis of lung cancer.
Indeed fashions in specifying cause of death change from time to time so that the entry may even depend on when the doctor was trained!
A general problem will be apparent in studies of, for example, hospital inpatient statistics where repeated admissions of the same individual are multiply counted.
Thus, an illness that tends to require frequent short spells in hospital will appear to have a high incidence.
In theory, such admissions could be linked by surname, date of birth, address, etc., but, in practice, errors in recording these items make this very difficult.
In any case, hospital admission depends locally on waiting lists and varying medical policy.
Furthermore, since the record is generated on discharge, complications relating to length of stay will disturb calculated sickness rates.
Criminal statistics are notoriously unreliable.
For one thing, we can only guess at the proportion of actual offences known to the police.
This varies with the nature of the offence (murder versus parking on a yellow line), the strength and efficiency of the police and the recording methods employed (under-reporting of offences by young children, pregnant women and old people).
Even with detection and reporting, a prosecution may not follow; failure to prosecute is more marked in Scotland where requirements of corroboration are higher than in England and Wales.
However, this is almost more a problem of consistency.
6.4 CONSISTENCY
It is vital when comparing two sets of published figures to ensure that basic definitions and methods of collection and analysis are consistent.
Even the term ‘population’ requires qualification:
1.
the civilian population of the UK, as might be guessed, excludes all armed forces; for some periods of this century, the merchant navy has also been excluded
2.
the total population includes members of the UK armed forces whether at home or abroad
3.
the home population is the de facto population, i.e. it includes armed forces of any nationality stationed in the UK and merchant seamen in home waters.
We have already mentioned the kind of discrepancies that can exist between Scotland and England; such differences are magnified when world-wide comparisons are to be made.
For example, in Britain, a ‘stillbirth’ is defined to take place only after the 28th week of pregnancy; in the USA, the borderline is the 20th week.
Efforts to harmonize this internationally have merely resulted in the definition of five stillbirth ‘groups’ which no one uses!
Likewise, changes in law may make it difficult to compare figures through time even within one country, e.g. introduction of the breathalyser in 1967 makes it very hard to relate road accident figures before and after this date.
Tables published by government agencies usually carry footnotes indicating such changes.
6.4.1 Standard codes
One area where inconsistency can arise is in the coding of various attributes.
Standard national and international codes exist for many of these but are subject to revision every few years.
Some examples follow.
(a) Standard Industrial Classification
This codes industry of employment rather than occupation.
Thus, a secretary working in a carpet factory (code 438) will be coded differently from the same  grade of secretary working in a neighbouring household textiles factory (455).
The code is an EEC activity classification and the version which came into use around 1980 has 10 divisions, 60 classes, 222 groups and 334 activity headings giving finer and finer detail,
e.g. 3 = manufacture of metal goods, engineering and vehicles
34 = manufacture of electrical and electronic equipment
344 = manufacture of telecommunications equipment
3443 = manufacture of radio and electronic capital goods
Changes of code are minimized, but new activities must be added to the list from time to time.
(b) International Standard Classification of Occupations
This refers to job rather than employing industry but is very long and complicated owing to world-wide coverage.
(c) Social class
This is an artificial five-category grouping based mainly on occupation of head of household.
More than half of the gainfully employed population are in the very heterogeneous Class III but at least the coding is seldom changed.
The social class concept has also been expanded into 17 socioeconomic groups with similar arbitrary divisions.
(d) International Statistical Classification of Diseases (usually abbreviated as ISCD or ICD)
This is a code for diseases and injuries, produced by the World Health Organization and revised approximately every ten years.
It is basically a 3-digit code with an optional decimal figure for greater detail.
Thus 427 is ‘symptomatic heart disease’ and 427.1 is ‘left ventricular failure’.
The codes in the range 800 to 999 relate to accident injuries, e.g. 845 is ‘accident involving spacecraft’.
Of course, such fine detail is seldom required and coarse groupings are usually used for summary, but even these are sometimes disturbed by coding changes.
Psychiatric problems tend to be the most subject to such revision.
To reiterate, a comparison is invalid if we are not comparing like with like.
Fallacious arguments in data interpretation are frequently the result of overlooking this.
Beware!
6.5 OFFICIAL STATISTICS IN THE DEVELOPING COUNTRIES
Clearly, the problems of collecting and interpreting official figures for so-called Third World countries are enormous; reliability and consistency are particularly  questionable.
Since the Second World War, the United Nations Statistical Office has done much to stimulate a global view of economic and social problems particularly in relation to human fertility but the responsibility for gathering the required information is left with individual countries, the majority of which do not have resources of trained manpower to accomplish the task themselves.
The Statistical Office of the European Community is currently planning a ‘twinning’ scheme between certain African centres and selected European institutions in an attempt to remedy this lack.
Skilled personnel are sometimes seconded from the West but there is a danger of imposing a Western framework that is totally unsuitable for societies where agriculture rather than industry is the mainstay of the community.
For instance, in large areas of Africa and Asia, the concept of the nuclear family and therefore of the household is relatively meaningless.
Likewise, ‘employment’ in the sense of earning money may have little relevance in a peasant society where people's needs are met by the labour of their own hands.
Insistence on the use of such terms can introduce great problems of consistency when international comparisons are attempted.
At the same time, international standards may demand tabular formats (for example, involving age groups) that are quite unsuitable for domestic planning.
In general, illiteracy and lack of motivation militate against reliability of figures from Third World countries.
Unsatisfactory communications between capital city and enumeration districts may lead to misunderstanding, exacerbated by language and dialect difficulties as local enumerators proceed into the remoter areas.
Even ‘age’ can cause problems in areas of the world where birthdays are not celebrated and, indeed, there is often a vague response to any question involving the passage of time.
In some cultural environments, information is hard to obtain simply because bad luck is believed to follow upon its disclosure.
Even the simple head counting involved in a census can be difficult and subject to international inconsistencies.
The distinction between de jure counting (practised in countries once in the French or Belgian sphere of influence) and de facto enumeration (the usual method in one-time British colonies) can lead to wildly differing estimates in areas with a high proportion of nomads.
Vital registration, even where it is theoretically compulsory, is also fraught with difficulties.
For instance, there is good evidence that in Ghana in the 1960s only about 22% of births and 17% of deaths were being registered.
This low coverage is, of course, largely due to the considerable distance of much of the population from the nearest registration office.
A lengthy delay in the reporting of a birth increases the chance that the newborn child may die with no registration of either the birth or death; the national birth and death rates will obviously be underestimated in such circumstances.
Naturally, the situation is constantly improving but unfortunately that also means that figures are not comparable over time.
Finally, it has been claimed (as much by statisticians from developing countries as by Western commentators) that official figures from the Third World are often considerably manipulated by the governments producing them.
Sometimes this is done to present a biased picture of progress for domestic approval; sometimes  the opposite is needed to support a request for foreign aid.
Even within a country with ethnic and cultural divisions, false regional statistics may be presented to support claims for greater national representation, increased central funding, and so on.
Interference with the true facts can be achieved by altering the usual format of tables, by selecting a favourable publication date and by adding explanatory notes that create a false impression.
Political bias may also be introduced in, for example, the appointment of the head of the government statistical service or the allocation of research funding to favoured topics such as poverty.
The cynical reader will suggest that these tactics are not peculiar to developing countries.
SUMMARY
Figures published by government agencies are freely available but the source, reliability and consistency of the data must be considered critically.
Variations of definitions and standard coding schemes across national boundaries or through time are a particular problem.
Quality of data in developing countries also presents special difficulties.
EXERCISES
1.
Write an essay discussing the most recent census in your country.
(For a ‘developed’ country, this will probably concentrate on the questions asked in the census form; for other parts of the world the method of conducting the census may be of more interest.)
2.
Find out as much as you can about any one of the special UK sample surveys mentioned in Section 6.1.3; the government publications section of a moderate-sized library should provide most of what you need.
Summarize the main features of the survey.
3.
Try to track down more details of either the EEC Standard Industrial Classification or the International Statistical Classification of Diseases than are given in Section 6.4.1. (you may be lucky enough to have a sight of the complete listings or you may have to scan EEC or World Health Organization publications for information.)
Delving in tables
7.1 INTRODUCTION
Many readers of this book will have become involved with numbers in the belief that they are ‘no good at English’.
This is unfortunate since one of the most important facets of the use of data for scientific research, corporate management or political debate is the communication of the information contained therein to those who will say that they are ‘no good at figures’!
Even a table of results can be frightening, confusing or at least boring to many people unless a summary interpretation of the content is made available.
Clearly a certain amount of skill is involved in the discussion of uncertainties and assumptions in detail sufficient to demonstrate objectivity and yet maintain a clear and convincing line of argument.
Because the presentation of results in lucid and effective language is so important, the qualifying examinations of many professional bodies explore candidates' aptitude for this and some employers (the Civil Service, for example) use similar tests in the screening of job applicants.
The purpose of this chapter is to give some hints on how to comment on tables such as might be found in government summary publications.
Masterly examples of this skill are to be found in the UK Government publication Social Trends and, though usually rather ambitious expositions, are certainly worthy of study and imitation.
In considering a government summary table, it must first be recognized that the raw data were probably collected with no specific use in mind and therefore, almost inevitably, too much detail is given or the format is unsuitable for the purpose in hand.
We consider first how to reorganize a table to highlight the main features of interest.
In particular, we go beyond the introduction to the tabular summary given in Chapter 2 to consider how the presentation of a table can be manipulated to reveal the information content to best effect.
7.2 TABULAR PRESENTATION
The first suggestion is to round the figures to two effective digits since psychologists have shown that the short-term memory need for scanning tables is limited to two digits.
The mental subtractions and divisions necessary for examining the content of a table are thereby facilitated.
This ‘de-focussing’ therefore helps to reveal any real patterns although, of course, it would have been  unacceptable in the original table, the function of which was to.provide an accurate record.
By effective digits we mean those that vary in the list of numbers under consideration.
Thus, entries such as 870, 1076, 1521, 2774, 2163 having four effective digits would become 900, 1100, 1500, 2800, 2200 but 1638, 1797, 1025, 1672 having only three effective digits would be reduced to 1640, 1800, 1030, 1670
On the other hand, rounding must not become an inflexible rule, since numbers like 91, 107, 98, 103 would be considerably perturbed by such treatment.
Naturally, after rounding, marginal or subtotals may be only approximately the sum of their components, but this can be explained in a footnote if necessary.
A phenomenon resulting only partly from use of the above technique is that figures can best be compared when arranged in columns since the eye can then relatively easily ignore unvarying digits and the brain can perform subtractions or divisions more readily.
Long column headings must be abbreviated or made multi-line so that the eye travel for lateral comparisons is not excessive and to avoid the need to present a table side-on.
Another beneficial trick is to re-order the rows and columns of a table in descending order of marginal totals.
This, again, aids the process of mental subtraction.
On the other hand, if several similar tables are to be compared, the same format must naturally be used for all of them.
Likewise, if past convention has made a particular layout familiar (such as time increasing downwards or from left to right), a change should be considered only if a clear advantage is to be gained.
Whatever pattern is adopted, the rag-bag categories of ‘miscellaneous’, ‘others’ and ‘not known’should appear as the bottom or right-most categories of rows and columns.
Where the row and column classifications provide a breakdown of a single population, expressing the body of the table as percentages of one or other marginal total may be helpful.
But, if the information is derived from several populations (e.g. different years), the analagous technique involves setting a base year figure to 100 and scaling the figures for succeeding years relative to this.
(The use of indices is discussed more fully in the next chapter.)
Since we are generally concerned to detect deviations from average, it can be useful to show the row and  column averages where these make sense.
This gives a starting point from which to look for patterns of discrepancy in the subcategories.
Lastly, careful use of space in the table layout may go a long way towards helpful guiding of the eye.
As an alternative to ruled horizontal lines, any subtotals in columns can be offset to the right a little to assist the reader to focus on them or discard them as required.
In printed tables, even the comma used to subdivide large numbers into groups of three digits is often replaced with advantage by a small gap.
7.3 WRITING ABOUT TABLES
There can be no hard-and-fast rules about how to elicit from a table or group of tables the main points of interest; it is possible to offer only some general guidelines.
As a starting point, the methods of tabular reformulation discussed in the last section may form part of an initial familiarization phase.
Notes can be made at this stage and possibly some rough diagrams drawn which may or may not be incorporated into the final document.
Once a reasonably complete distillation has been achieved, writing can begin.
The first consideration must be the nature of the intended audience since this will determine the level of technical jargon (statistical or otherwise) that may be permitted, the format and length of the exposition and the style of writing to be adopted.
For example, whether or not subheadings (possibly numbered) should be used will be partly related to length; they will certainly be of benefit in any text longer than 500 words.
Similarly, the technical expert will be content to read a much more dry but precise report than the apathetic ‘man in the street’.
Occasionally, it can be worth while to vary the style.
In this book, for example, there are passages (e.g. Section 5.5) where a somewhat pompous style is used to present one side of a possibly debatable viewpoint.
By way of contrast, a more informal approach (e.g. Section 1.3) is used with the aim of encouraging readers through a potentially offputting topic.
Sometimes an intentionally condensed presentation (e.g. Section 17.5) may ‘make the reader think’, but such a tactic can be dangerously optimistic.
(You now know, dear reader, how to view the obscure and cryptic sections of this text!)
In any event, do not descend to the clichés of politician and journalist for whom all changes are ‘dramatic’, all majorities are ‘vast’ and all proportions are ‘mammoth’.
Since there is no agency to arbitrate on what is correct and what is not in the use of English, the language is free to mutate quite rapidly.
Nevertheless, current grammatical freedom is unpalatable to many educated in a more rigid era.
It will generally be politic to avoid the antipathy of these readers by eschewing forms such as‘different to’, ‘less people’ and ‘the most numerous sex’.
However, publisher's spelling conventions (e.g. -ize) may have to be accepted.
It will almost certainly be useful to begin your report with some comments on the background to the tabulated information.
For example, the three aspects of  source, reliability and consistency of data discussed in Chapter 6 may provide a useful framework.
Footnotes to a table may indicate inconsistencies (over years, for example) due to changes of definition.
Whatever the overall reliability of the figures, some categories may be less reliable than others due to particular circumstances or simply because the relevant subsamples are very small.
A comment on the source might extend to whether or not the data are routinely collected and, if not, what was the motivation behind the particular study.
In presenting the main commentary, remember that readers will either have the original table before them or a number of tabular extracts presented by you to clarify points of interest.
Thus it will generally not be useful to restate raw figures in the text particularly since comparisons are a vital component of the exercise and these are best expressed as percentages or appropriate rates.
Do not be afraid to aggregate categories that are not (for your purposes) usefully differentiated but, on the other hand, mention parallel or inverse trends in subclasses if you feel these represent more than sampling variability.
While your commentary should naturally be unbiased, it is reasonable that it should include possible reasons for any patterns revealed.
Such explanations could be reasonably well founded or might be hypothetical, in which case this should be made clear to the reader.
It is inevitable that many social and medical phenomena relate to sex, age, social class and race (i.e. genetic or cultural) differences.
The independence of events such as deaths from plane crashes is also important since this may relate to the appropriateness of any rate quoted.
In addition, ‘when?’ and ‘where?’are often important attributes of events as are fluctuating patterns throughout the day, week or year (e.g. road traffic accidents) and discontinuities across regional and national boundaries with varying degrees of legal or cultural similarity (e.g. divorce rates in England and Scotland).
Finally, it is quite in order to conclude with a brief mention of any potentially useful information that is lacking either because it is not available to you or because it has never been collected (but could be).
Such auxiliary data might, for instance, confirm a suggested explanation or help to formulate a more appropriate rate for some event.
SUMMARY
The extraction and re-presentation of information from published tables is an important statistical activity.
Practice develops the skill but a few guidelines provide possible approaches to the task.
EXERCISES
1.
Choose a moderately extensive table from a government publication and summarize its main features in, say, 700–800 words.
Include tabular restatement and/or graphical presentation if these are helpful.
2.
Tables E7.1 and E7.2, reproduced from the Digest of Welsh Statistics (1986), gives information on legal abortions.
Write a report on these data using graphical representation where this is helpful and indicating any other figures that could throw further light on the situation.
Table E7.1 Legally induced abortions reported by hospitals in Wales
Table E7.2 Legally induced abortions to Welsh residents, by place of operation
3.
Table E7.3 comes from the September 1987 Monthly Digest of Statistics.
(The June Census is a compulsory return from all farmers supplying information on crops grown, etc.; φ means figures provisional.)
Describe the main features of the table.
Find information on European Community land use and crop areas and make comparisons.
Table E7.3 Land use and crop areas
Table E7.3 (Contd.)
Changes with time — index numbers
8.1 INTRODUCTION
The use of indicators of current economic performance relative to past performance has become more and more widespread during the last 50 years.
Government decisions, national and international, negotiations with producers over prices and with unions over wages, etc., depend increasingly on such measures.
Typically an index number indicates the current position of prices (e.g. UK index of retail prices) or quantities (e.g. UK index of industrial production) of a list of commodities relative to a specified base point (though the principles can apply in other situations).
(Strictly speaking, ‘current’ is a jargon term that need not indicate present time, as the example in the next section shows.)
Most forms of index number are essentially weighted means such as were described in Section 3.1.5.
Let us see how this comes about.
8.2 SIMPLE INDICES
Suppose we wish to compare someone's purchases of first- and second-class postage stamps in 1981 (current) with the corresponding figures for 1976 (base).
We may be interested in the price comparison or in the quantity of stamps bought.
The relevant figures might be as shown in Table 8.1.
We could compare the changes for each postal rate using the columns headed ‘price relative’ and ‘quantity relative’.
Thus the price relative for first-class stamps is 100 × 14÷8.5 = 164.7, indicating an increase in price of 64.7%.
The quantity relative for second-class stamps is 140.0, indicating an increase in numbers bought of 40%.
To compare the costs over both postal rates, there would appear to be two possibilities:
1.
We could average the relatives.
This would give us overall price and quantity relatives of 341.6÷2 = 170.8 and 260÷2 = 130.0 respectively;
2.
We could calculate the relative of the averages.
The average price moved from 15.0÷2 = 7.5 to 25.5÷2 = 12.75, giving a relative of 170.0; the corresponding quantity relative is 130.8.
Table 8.1 Prices and quantities of first- and second-class stamps in 1976 and 1981
It may be thought disturbing that the two methods give different answers; however, the real argument against both is clearer when they are applied in a situation where the purchases are of different commodities, e.g. coal (in tons), electricity (in kilowatt hours), matches (in boxes of 50).
In such a situation, average price depends on units of measurement and average quantity is meaningless; hence method 2 above is useless.
At least method 1 is independent of the units of measurement (providing they are the same in both years) but the equal weight that attaches to all commodities can give a misleading figure — if the price of matches falls by 50% and the price of coal rises by 50% the combined index indicates no change.
This seems unreasonable in a household budget.
8 3 WEIGHTED INDICES
8.3.1 The Laspeyres index
We must therefore adapt method 1 by weighting the commodities, i.e. taking a weighted mean of the price or quantity relatives (see Section 3.1.5).
What weights should we use?
Clearly, our view of the importance of the 64.7% increase in the price of first-class stamps or the 40% increase in the purchase of second class stamps depends on the value involved.
Thus it could be appropriate to calculate a weighted mean of the relatives using the base values, p B q B , as weights; i.e. for the price index,
Notice that the formula we have used is  which reduces to 
and this is just the relative of the averages of prices weighted by base quantities, i.e. equivalent to a weighted version of method 2.
The result can be simply expressed as 
Thus, prices are compared as a ratio of values of base quantities.
Similarly, quantities are compared as a ratio of values at base prices, i.e. 
These index numbers, using base period value weights, were invented by the German economist Laspeyres.
8.3.2 The Paasche index
We could equally well construct an index in a way suggested by Paasche.
The value weights used are hypothetical and, for price comparisons, represent the value if current quantities had applied in the base year.
For quantity comparisons, the hypothetical value is that which would have held if current prices had existed in the base year.
The calculations are as follows:
Paasche price index =  
Paasche quantity index = 
8.4 COMMENTS ON THE LASPEYRES (L) AND PAASCHE (P) INDICES
The two indices appear to have equal validity in measuring the relative change in price or quantity levels between two periods.
In general L and P are not equal and it is therefore reasonable to ask if there are theoretical reasons for favouring one over the other.
1.
Neither index takes account of the fact that, in a free market, people adjust their buying habits to get greatest value, i.e. if prices change, quantities bought may also change.
Thus, by insisting on a constant quantity pattern, the value of base quantities at current prices overestimates the real current value and hence L overestimates the real effect of price changes.
By a similar argument, P underestimates the effect.
Nevertheless it is still possible to find L less than P, as in our example.
2.
For L, the weights have to be established only once.
This speeds the production of the index number where complex commodity lists are involved so that there is less delay in publication.
On the other hand, the base weights can quickly become irrelevant in a dynamic economy (see 1 above).
For P, the weights have to be recalculated for each current year and, again, current patterns might have been unreasonable in the base year.
Clearly the base point must be one at which reasonable market stability existed.
For example, we should not use a month in which there was a generous round of price increases since that will give an impression of little change in later months.
Such stability may be difficult to achieve for all commodities in a complex list.
Sooner or later the base must be changed and the following example demonstrates the difficulty of relating the new index to the old base for long-term comparisons.
Now, the first- and second-class stamp prices in 1986 were 17p and 12p respectively, so we have:
Then L for price in 1986 relative to 1981 is  and we have already calculated L for 1981 relative to 1976 to be 170.5.
Hence we are suggesting that L for 1986 relative to 1976 might equal 
Unfortunately, L for 1986 relative to 1976 equals  so that the conversion is incorrect for L (and likewise for P).
It would be valid only if
1.
all quantities were to alter by the same factor from 1976 to 1981, or
2.
we use fixed ‘standard’ quantities that belong neither to the base year nor to the current year but are decided on some other basis.
Nevertheless, such a link is often made even when this is not the case.
It is of particular relevance in the construction of chain-linked index numbers where the comparison is made as a sequence of year-to-year (or month-to-month, etc.) indices using either of the index numbers described.
This facilitates frequent changes in the commodity list and, as in the case above, the error is not usually very great.
8.6 UK INDEX OF RETAIL PRICES
This is designed to show month-to-month changes in average prices of goods and services purchased by households.
It is not really a cost of living index since many goods of a luxury nature are included and several items (e.g. income tax, insurance premiums, etc.) for which it is hard to identify ‘units’ of sale are excluded.
Until 1987, the index used 11 major groups: food, alcoholic drink, tobacco, housing, fuel and light, durable household goods, clothing and footwear, transport and vehicles, miscellaneous goods, services, and meals consumed outside the home.
At the start of 1987, a new 14-group system was introduced.
Alcoholic drink, tobacco, housing, fuel and light, and clothing and footwear are as before, but the remaining groups have been restructured as food, catering, household goods, household services, personal items, motoring expenditure, fares etc., leisure goods, and leisure services.
Naturally, these inconsistencies do not affect the comparability of the ‘all items’ index over time and, in part, the changes were introduced to conform more closely with international classifications.
Each group is divided into a number of sections and, for each section, representative goods and services have prices recorded.
These are transaction prices, ignoring hire purchase charges, ‘co-op’ dividends and discounts not given to all customers.
The prices are collected by postal contact with manufacturers or visits to retailers in a representative list of towns.
Table 8.2 Weights for index of retail prices
The weights used were originally calculated from an expenditure survey of 11 638 households in 1953–54.
From 1962 to 1974 the weights were recalculated every February from the results of the latest three Family Expenditure Surveys.
Since 1975, the index has been current weighted by expenditure in the latest available year.
Table 8.2 shows how the weights have altered over the years.
(Notice that the weights in each column sum to 1000.)
The figures reveal fairly consistent reductions in the weights for food and tobacco, and an increase (levelling off) for transport and an unsteady (!) fluctuation for alcohol.
The prices are usually collected on the Tuesday closest to the 15th of the month and the index is published in the third week of the following month in various economic journals.
8.7 UK INDEX OF INDUSTRIAL PRODUCTION
This is a quantity index designed to measure monthly changes in the volume of UK industrial production.
The definition of ‘industrial’ is a little arbitrary — mining, quarrying, gas, electricity and water are included, but service industries, agriculture and fishing are not.
The output is recorded from both private- and  public-sector firms whether the goods are for domestic consumption, export or the armed forces.
The index incorporates 880 items showing the difference between output of products and input of materials, etc., integrated by the Laspeyres base-weighted method.
Where values are supplied by a firm, these have to be adjusted to the price levels for the base period, otherwise the index would reflect value rather than quantity changes.
Where production extends over a long period as in construction industries and shipbuilding, measurement of work in progress has to be achieved by other means.
To take account of different lengths of calendar months, a ‘standard month’ is used, but, even so, public holidays, particularly at Christmas, can distort the index.
The main problem for this index is the considerable delay before publication.
It can take as much as six months for all required figures to be available and it is usual to issue a provisional index six to seven weeks after the end of the month in question and revise this later.
SUMMARY
Indices are available that can be used to compare prices (or, less often, quantities) at two time points.
Different commodities are combined as weighted means of the relative change ratios.
EXERCISES
1.
The table below gives the values (in £million) of gross domestic fixed capital formation in three sectors of UK industry at both current and 1980 prices for three separate years.
(a)
Explain why the entries in columns (b) and (e) are identical and why the entries in columns (a) and (d) differ.
(b)
Explain how to calculate from these figures base-weighted (Laspeyres) price index numbers with 1980 as base year.
(c)
For the three sectors combined and with 1980 as base year, calculate current-weighted (Paasche) price index numbers for 1976 and for 1986.
2.
Table E8.1 comes from the 1987 Digest of UK Energy Statistics (Table 63).
Discuss the patterns revealed by the data, making use of the techniques discussed in this chapter.
(If you have access to the original publication, you might find the explanatory notes on p. 90 of interest.)
Table E8.1 Consumers' expenditure on energy
Demography — introduction
9.1 THE IMPORTANCE OF DEMOGRAPHIC STUDIES
Demography is concerned with description of the size, composition and structure of populations (usually human) and how these change with time.
Thus, we are interested in patterns of births, deaths, marriages and migration (event counts) and the distribution of attributes such as age, sex, social class, family size, etc. (head counts).
The data for such studies come from censuses, vital registration, etc.
This kind of description will be of interest to, for example, geographers (spatial distribution of populations), health service and other planners (future provision of maternity, geriatric, etc., services), sociologists (what decides family size?), economists (changes in per capita income), actuaries (pension arrangements), and many others.
One main problem is that of the delayed effects of population fluctuation.
A ‘baby boom’ now could provide a large number of teachers in 20 years' time; but if in 15 years' time, the birth rate were to be low, there would be insufficient children for these teachers to teach (unless pupil/teacher ratios were to be changed).
Future predictions are not all equally dependable.
For instance, on the basis of last year's births, we can make a fairly accurate assessment of the number of first-year secondary school places required in 11 or 12 years' time, because this depends mainly on fairly stable child mortality patterns.
But we cannot estimate so reliably how many of these will seek university places in 17 or 18 years' time since this will be affected so much by changeable social and economic factors.
Demographic techniques are also of some use in ecological surveillance where there is interest in the breeding stability of a colony of birds or animals or where concern exists over culling programmes or the depletion of fish stocks.
Indeed, some of the methods are of greater validity in these circumstances due to the relative brevity of animal lifetimes and freedom from the rapid temporal changes that human society brings upon itself.
Likewise, many of the methods are useful in manpower studies where the population under investigation is a large workforce, recruitment and leaving (for whatever reason) are the analogues of birth and death respectively, and we are interested in the stability of the age/sex structure or length of service patterns and the prediction of natural wastage.
9.2 DYNAMICS OF POPULATION CHANGE
The year-to-year change in total number in a population depends on two pairs of components:
1.
natural increase or decrease, i.e. the difference between births (B) and deaths (D)
2.
net migration, i.e. the difference between immigration (I) and emigration (E); this can be negative during periods of restricted immigration.
Thus, the population in year t, P t , is related to that in the previous year by 
Ignoring these components for the present, we can simply consider a population growing at say, 0.5% per annum(1 in 200 — roughly the UK rate).
Then  so that, in general , where P was the population size in the starting year.
(This is just the formula for capital growth when compound interest is applied.)
How long will it take for the population to double, i.e. when do we have to feed twice as many people?
We require to find t such that P, = 2P and the answer turns out to be about 140 years.
We have to make the very unreasonable assumption that the growth rate stays constant during all those 140 years.
In some countries, the present growth rate is about 2.5% and the time taken for such populations to double (under the same assumption) is therefore only 28 years!
We shall return to population projection later.
9.3 DEMOGRAPHIC TRANSITION
Before proceeding further, let us consider briefly one aspect of the way in which human populations have been observed to grow.
Little accurate information is available on population sizes in the middle ages but such evidence as we do have for the UK suggests that there was a six- or sevenfold increase between AD 1100 and 1780, with alternating lengthy periods of rapid and slow growth.
These fluctuations were the direct results of famines, plagues, climatic variations and, to a lesser extent, international and civil strife.
From about 1780 onwards, the population grew so steadily and so rapidly that by 1950 the number of inhabitants had again increased by seven times.
With few differences, this pattern was repeated throughout Europe.
Such a relatively  abrupt change in the rate of increase of population size has become known as demographic transition.
This phenomenon was accompanied by a general movement from rural areas to towns and a gradual decline in the death rate, followed by a fall in the birth rate.
The lag in birth rate decrease behind death rate decrease is as characteristic of developing countries today.
It takes some years for the populace to realize that couples need not conceive so many children merely to ensure the survival of two or three.
It is largely this lag that initiates demographic transition but other factors undoubtedly complicate the picture.
For instance, there may be economic and social influences.
The Industrial Revolution in Europe brought about an environment with a considerable element of child labour, making large families advantageous.
At about the same time, in England and Wales, poor relief was, for the first time, made dependent on family size (partly to alleviate the effects of the Napoleonic Wars).
Other changes meant that agricultural workers became less dependent on their employers for lodgings and could therefore marry younger.
Coupled with these economic and social changes, there was a vast improvement in the health of the community.
This was in part because of better diet (due to new crops introduced from abroad and more reliable agricultural methods).
But, above all, better water supplies and the construction of sewers meant improved public hygiene.
At a personal level, the availability of cheap soap and the gradual change from wool to more easily washed cotton clothes did much to make people cleaner.
However, some of the health improvement came about through natural reduction in disease.
For example, bubonic plague had largely disappeared either because rats developed an immunity to the bacillus or due to a change in the flea transmitters.
Likewise, immunity to typhoid and smallpox seems to have increased spontaneously in the human population.
Vaccination, of course, finally eliminated smallpox and there appeared other ‘drugs’ such as mercury to treat syphilis and quinine to prevent malaria (though greater drainage of bogs was as much responsible for control of the latter).
Increase in the use of isolation hospitals and much improved midwifery techniques did much to lower infant and maternal mortality and this is obviously a key aspect of demographic transition.
The relative importance of these factors in different countries (and even in the UK) remains the subject of much debate among economic and social historians.
That the phenomenon of demographic transition exists is unquestionable though, nowadays, the timescale of change is so condensed that the metamorphosis of a developing country may appear superficially different from that of eighteenth-century Europe.
9.4 AGE-SEX PROFILES
A pictorial representation of the age-sex structure of a population at a particular time is provided by an age-sex profile or population pyramid.
Figures 9.1 and 9.2   give some examples.
In drawing such profiles, the male part is conventionally drawn on the left and an arbitrary decision must be made on where to terminate the open-ended top age class (say 100 years).
Five-year age groups are usual but, of course, other groupings could be used.
Fig. 9.1 Age-sex profiles for Great Britain over a period of 130 years.
Horizontal axes show absolute numbers (thousands) in population; vertical axes show age in years.
(a) Total population = 19 million.
(b) Total population = 37 million.
(c) Total population = 45 million.
(d) Total population = 49 million.
(e) Total population = 54 million.
See text for further explanation.
The diagram can portray actual numbers in the age-sex classes so that differences in total population size are apparent.
Figure 9.1 shows the pattern of demographic transition for Great Britain in ‘snapshots’ from 1851 until the present day.
Notice that the pyramid shape is typical only during a period when medical care is deficient; birth and death rates are then high with probability of death being relatively constant at all ages.
As medical care improves, a much more ‘beehive’ shaped profile becomes the standard pattern with death being fairly unlikely before the age of about 60, at which point the remaining scourges of cancer and heart disease maintain the pyramid form for the older age groups.
By 1901, the population had grown considerably but increased use of birth control was narrowing the lower part of the pyramid.
Notice in the 1931 profile the effect of the First World War (WWWWW) on the male population and the drop in births (XX) due to the start of the 1925–35 depression.
The remaining diagrams of    Fig.9.1 show how these phenomena continue to show up in the profile.
The post-1945 ‘baby bulge’ is YY in Fig.9.1(d) and the bulge of the early 1960s boom (‘You've never had it so good’— Harold Macmillan) is shown at ZZ in Fig.9.1(e).
Fig. 9.2 Age-sex profiles for selected countries in the early 1980s.
Horizontal axes represent percentage population; vertical axes show age in years
Fig. 9.3 Age by full-time/part-time profiles for nurses in four different fields.
Horizontal axes are percentages; vertical axes show age in years
Whereas Fig.9.1 shows absolute numbers for the same country at different points of historical time, Fig. 9.2 gives percentages so that structural comparisons can be made among countries with very different total populations at roughly the  same time.
The sequence demonstrates the range of shapes that are found simultaneously in countries at different stages of development.
The Kenyan picture, with 51% of the population under the age of 15, is typical of a Third World country; the fact that the Mauritius pyramid of 25 years ago was very similar to that of Kenya today demonstrates the rapidity with which a population structure can change.
In China the effect of the Cultural Revolution and a recent vigorous birth limitation policy can be seen, as can a massive post-war birth control programme by abortion in Japan to direct resources to industrial development.
The effect of the Second World War on the male Japanese population is also just detectable.
The USA picture shows a slightly distorted beehive shape due to recession followed by a post-war boom reflected in the birth rate.
East Germany shows a sharp fall in the birth rate during the First World War and after the Second and the effect of the Second World War on the males of fighting age.
Simply to demonstrate the usefulness of these diagrams in another context, Fig. 9.3 shows age profiles for trained nurses in a health board area working in four different fields.
Instead of sex, the division about the centre line is between full-time (FT) and part-time (PT).
Notice how the numbers in general nursing (mostly female) swing towards part-time during child-rearing years.
In midwifery, the delay due to a specialized training period and the slightly greater proportion of unmarried women modifies the pattern slightly.
In the psychiatric field the higher proportion of male nurses and hours more conducive to full-time working for women present another pattern.
9.5 COHORTS
A ‘snapshot’ sequence of pyramids may reveal the effects of social (e.g. family planning); historical (e.g. wars, plagues, famines) and economic (e.g. recessions) changes on population structure, but it is also useful to think dynamically in terms of cohorts.
A group of people who entered some state at about the same time and whose life histories are thereafter studied is called a cohort.
A cohort may consist of all persons born in the same year (a birth cohort or generation); but all women married in a particular year or a random sample of all cancer sufferers diagnosed in a certain month are equally valid examples of cohorts.
Cohorts are used to relate events to those that must necessarily precede them, e.g. ‘termination of a marriage’ must precede ‘remarriage’.
Only in this way can we study the probability of the second event occurring.
However, such a probability may change against any of three time scales:
1.
the passage of historical time (‘calendar’ time)
2.
the age of the individual (‘maturing’)
3.
the elapsed time since the prior event.
The probabilities must therefore be expressed in terms of specific cohorts.
SUMMARY
The usefulness of demographic techniques in planning for human societies and large workforces or warning of impending problems in animal populations is considerable.
Demographic change throughout phases of industrialization, etc., is of particular interest.
EXERCISES
1.
Find recent figures giving the age-sex population breakdown for Australia, Canada or New Zealand and draw the population pyramid with the horizontal axis as a percentage.
Compare the pattern with those given in Fig.9.2 and comment on any special features.
2.
Figure E9.1 gives the age-sex profiles for two countries that have experienced a good deal of immigration.
Investigate the recent histories of these countries and try to explain the patterns.
3.
Figure E9.2 shows the age-sex profiles of two island communities.
Discuss possible explanations for any interesting features of the two pyramids.
(Hint: Remember that the entire population of Tristan da Cunha (how many?) was temporarily evacuated to the UK in 1961.)
Fig.
E9.1 Age-sex profiles for Hong Kong and Israel.
Horizontal axes represent percentage population
Fig.
E9.2 Age-sex profiles for the Isle of Man and Tristan da Cunha.
Horizontal axes represent percentage population
4.
Table 2.7 of the Employment Gazettes of 1981 and 1986 give the following figures for unemployment (thousands) in the UK in April of 1979 and 1986: Draw age-sex profiles for the two years.
(Should you use absolute numbers or percentages on the horizontal axis?)
Compare the patterns.
Find out what you can about the definition of ‘unemployment’ in the two years.
Components of population change
10.1 DEATH
We shall begin our discussion of components of population change by looking at mortality since death is a well-defined event that happens to everyone.
We shall initially assume a closed population, i.e. zero migration.
10.1.1 Crude death rates
A simple measure of the incidence of death during some period of time is given by the crude death rate (CDR):
The sign  means that the rate is expressed per thousand.
The period is almost always a calendar year; the average population size may be the estimated midyear figure as in the UK or the average of the 1 January figures in the year in question and the following year.
Scottish figures for 1984 were as follows:
Overall CDR =  
Male CDR = 
Female CDR = 
The last two rates illustrate Male excess mortality, a common phenomenon of most countries this century for all age groups except where death in childbirth reverses the pattern.
Here are some other CDRs (from Tables A1.2 and A2.4 of the 1984 Annual Report of Registrar General for Scotland and Table 18 of the UN Demographic Yearbook, 1984):
Why are Hong Kong and Costa Rica so low?
Why is Spain lower than France, and Puerto Rico lower than USA?
10.1.2 Age-specific death rates
What is obscured by the CDR is the nature of the age structure of the population and the differential mortality involved.
The risk of death is high for the first year of life, falls sharply to reach a minimum during the teens then climbs, especially from middle age onwards.
Other factors are also involved; risks tend to be greater for males than for females at all ages (see Table 10.1); marital state, occupation and social class are likewise important.
Thus, populations with a high proportion of the very young and the elderly can expect (other things being equal) to have higher CDRs than populations largely composed of young and middle-aged people.
We must therefore consider the rates associated with different age groups — age-specific rates.
Suppose two towns A and B, each with a population of 100 000 are as follows:
Notice that in each age group, town B has lower specific mortality than A yet the CDR is higher because of the difference in age structure.
Age-specific rates are often calculated as averages over three years, partly to reduce instability of the statistics in older, almost empty, age classes, and partly to obviate problems due to especially bad weather at the end of a year causing early demise of those who would otherwise die the following year.
10.1.3 Standardized death rates
Age-specific mortality rates (usually in 5-year age groups) are interesting in themselves but cumbersome as a means of summarizing death rate patterns.
How can we take into account the age (and sex) structure of a population in a single figure?
Notice that the CDR can be thought of as a weighted mean of the age-specific  rates, using age class population numbers as weights, i.e. CDR =  
Table 10.1 Age-specific death rates for males and females separately; Scotland 1911–15 (average) and 1984: rates per 1000 population
When we want to compare two populations, as with the example of towns A and B above, we must apply the same (standard) weighting scheme to both; this gives a standardized death rate (SDR).
Which standard?
1.
We could use the age structure of one of the towns (say town A) for both rates.
Thus, the rate for B would be  which is now consistent with the fact of uniformly lower age-specific rates than A. You can check that standardizing A by B's population gives a rate for A of 12.2 against B's 11.0.
2.
We could combine the populations and standardize both rates using this (fictitious) age structure.
Thus, for A we have  and, for B, the rate is 10.2.
3.
For both towns, we could use some weighting scheme derived externally, e.g. the population structure for the whole country.
For international comparability, a standard population fixed by the International Statistical Institute is usually used.
10.1.4 Standardized mortality ratios
It is now only a short step to think of comparing ‘local’ specific mortality patterns (m a ) with a standard (m s ) by means of a weighted average over the age groups of the ratios of m a to m s , using as weights the deaths that would be expected locally if the standard specific mortalities were to apply, i.e. m s x P a This is called the standardized mortality ratio (SMR).
That is, SMR = 
This summary figure is quite widely used by Registrars General in the United Kingdom but others are possible.
Notice how the SMR is analagous with the Paasche index of Section 8.3.2.
10.1.5 Cohort rates and probabilities
Death rates can be criticized in that the denominator population is a somewhat fluid concept.
Suppose we want a death rate specific to 40-year-old males in 1986; then we really need to know, not only how many men aged 40 completed years died in 1986, but the average number of 40-year-old men alive in 1986.
Even if this number could be regarded as reasonably constant throughout the year, it is clear that the group of individuals exposed to the risk of dying is constantly changing.
A man who became 40 just before or just after 1 January 1986 is much more likely to contribute a death to the numerator than a man who became 40 in the early days of 1985 or the last days of 1986.
Such a problem is less serious when 5-year age groups are considered (since  entries and exits form a smaller proportion of the denominator population) and even less serious for the whole-life CDR; but it is at its worst for children in their first year of life.
(In an extreme case we might have a fairly constant number of 0-year-old children (say 5000) in some population, yet averaging 100 deaths each week in the 0–1 age group, so that the ratio of deaths to mean population over a year exceeds 1.01).
Consider as an example the following data:
No. of men aged 40 on 1 January 1986 = 298 000
No. of men aged 40 on 1 January 1987 = 294 000
No. of male deaths at age 40 in 1986 = 1160
Then the ‘average’ male population aged 40 in 1986 can be estimated as 
Hence the male crude death rate at age 40 in 1986 is 
What is generally of more interest is the probability, q 40 , of death between ages 40 and 41 for men in the 1946 birth cohort.
From the available information, we cannot tell how many of the 1160 deaths in 1986 belong to the 1946 cohort so we can only estimate it to be 1160÷2 = 580 and assume that another 580 of the 1946 cohort will die in 1987.
We must assume also that the number of the 1946 cohort alive on 1 January 1987 is 296 000 so that the total number of the cohort who achieved their 40th birthday was 296 000 + 580.
Thus 
It can be shown algebraically (and these calculations verify) that  (where m is the rate per thousand) so that cohort probabilities can be estimated from mortality ratios.
10.1.6 Life tables
Once the cohort probabilities have been established, a cohort life table can be constructed; a birth cohort (or radix) of arbitrary size, say 10 000, is depleted at successive ages according to the cohort death probabilities, q.
Table 10.2 shows such a display for a robin population.
(The robin has a conveniently short lifespan.)
The columns headed q x , l x , and d x present no problem; given the information in any one of them, the other two can be easily found.
Table 10.2 Life table for robins
The column headed e x requires some explanation; it represents the average length of life remaining to a bird that has attained its xth birthday.
Thus, e represents the life expectancy at birth.
The T x column has been included merely to help explain the calculation of e x ; it represents the total ‘robin-years’ of life remaining for all of the original cohort that have reached age x.
Let us assume that deaths occur at completely random points throughout the year.
This is clearly most suspect at the extremes of the age range.
However, if it is accepted, then a bird that dies at age x (in completed years) lives on average (x + ½) years.
Hence, for the last line of a life table, e x must always be 0.5.
The 63 robins that die at random points in their seventh year of life must therefore live a total of T 6 = 63 × 0.5 = 31.5 robin-years.
Now consider T 5 ; the 252 robins that die aged 5 live a total of 252÷2 robin-years, to which must be added one year of life for each of the 63 birds that survive at least another year plus their total life beyond their next birthday (i.e. T 6 ).
In general, thus allowing completion of the T x column from the bottom upwards.
Thereafter, e x is easily found as T x ÷l x Remember that e x is the average remaining life conditional on a robin reaching age x.
It is therefore quite possible (as here) to have 
Cohort life tables are little used for humans because:
1.
it takes too long to follow a human birth cohort through life — the information is of little interest at the end of such an exercise 
2.
mortality depends less on birth cohort and more on year-to-year climatic, economic, epidemiological, etc., variations.
Hence, it is usual to calculate the q's for the current year or a period of years and apply these to what becomes a hypothetical cohort ‘followed’ through a current or period life table.
In such a case, notice that the average length of life, e, in a population is not, in general, equal to the average age at death because all the birth cohorts have been compounded irrespective of their sizes.
10.1.7 Infant mortality
Special problems are associated with calculations of infant mortality because of the rapid decline in the death rate during the first year of life.
We shall not discuss such special calculations.
Fig. 10.1 Infant mortality rates in Scotland, 1984.
The associated jargon and rates may be introduced by looking at the following figures for 1984 extracted from Tables D1.1 and D2.1 of the 1984 Annual Report of the Registrar General for Scotland.
(Stillbirth and perinatal death rates are per thousand births, live or still; the infant death rate is per thousand live births.)
Table 10.3 shows a selection of infant death rates.
Table 10.3 Some infant death rates ()
10.2 MARRIAGE
Many of the summary techniques used to describe mortality carry over into the other components of change.
Strictly speaking, marriage does not contribute directly to population change, but, because of its association with the legitimacy of births in most countries, marriage rates have been of some interest.
The crude marriage rate is thus 
(The rate of actual marriages is half this, but the definition is more consistent with the CDR.
For the same reason, one does not normally restrict the denominator population to those who are of appropriate age and single.)
Naturally, age/sex specific rates and probabilities can be calculated.
In finding such probabilities, some account should be taken of deaths in the cohort during the year in question; we can assume that a person dying lives, on average, six months of the year so that the population at risk of marriage at the start of the year should be reduced by half the deaths.
This correction is usually negligible for the most marriageable age groups.
Marriage tables (analagous to life tables) can be constructed showing, as might be expected, that advancing age first increases the chance of an imminent  marriage but is finally an unfavourable factor.
However, in contradistinction to mortality summaries, period tables have less to commend them in the case of marriage where year-to-year variations may be considerable and past history affects cohorts differentially.
For example, if there is an imbalance of the sexes in an age group either through natural variation in the sex ratio or because men have been killed during war or are at war, the overall marriage rate for that age will tend to fall temporarily and that for the less numerous sex will rise.
In the construction of a marriage table, notice that not all of the radix cohort will marry and that a marriage table can stop at about 55 years (certainly for first marriages) since the marriage probability has become negligible by that age.
10.3 FERTILITY (live births)
Birth statistics can be expressed by a number of different measures.
10.3.1 Crude birth rate (CBR)
Unless there is a very high proportion of females of child-bearing age, the CBR is usually less than 50 .
Table 10.4 gives some typical figures.
Remember that ‘natural increase’ = CBR -CDR.
10.3.2 General fertility rate
This is defined as 
(Sometimes 15–44 years; sometimes married and non-married separately.)
The figure was 51.5  in 1984 in Scotland.
10.3.3 Age-specific fertility rates
These are as for mortality figures.
Standardized rates and ‘standardized fertility ratios’ are sometimes calculated.
Thus, for Scottish legitimate births: Note, however, that in the presence of birth control, fertility is almost more dependent on duration of marriage than on age itself.
Table 10.4 Selected crude birth rates ()
10.3.4 Total fertility rate
This is simply the unweighted sum of the age-specific rates.
It assumes each age to be of equal significance and represents the total number of children born (on average) to each woman of a hypothetical cohort throughout her life.
If specific rates refer to 5-year age groups, we must assume that the rate applies in every year of the group so that the rate derived from the fictitious figures of table 10.5 is   
Table 10.5 Births in some population
10.3.5 Gross and net reproduction rates
Some of these 2.9 children will be male and some female; if we assume a birth sex ratio of, say, 52:48, i.e. probability of female birth = 0.48, then the expected female births per woman is 
This gross reproduction rate measures the population's capacity to replace itself assuming fertility does not alter.
However, this makes no allowance for deaths of the mothers or deaths before 15 among the daughters replacing them.
The ‘survival’ column of Table 10.5 based on current mortality experience, at the time of the calculation, can be used to reduce the fertility figures and give the net reproduction rate 
10.4 MIGRATION
Migration is usually the least important component of population change.
The movement may be international (e.g. UK to USA) or inter-regional (e.g. New South Wales to Western Australia) or ‘local’(e.g. North London to South London).
In all these cases the basic information comes from a census question such as‘Where were you living five years ago?’
But what happens to the five years immediately following a decennial census?
Likewise, some moves during the five-year pre-census period will be missed.
In any event, memory lapses are likely to present major reliability problems.
In the UK, the International Passenger Survey (see Section 6.1.3) provides some information but only on international movements.
Local changes can be detected from employment, taxation and social security records and also Executive Councils' lists of general practitioners'  patients (though these are notoriously out of date especially for age groups having little need of medical care and therefore slow to register with a new GP after moving).
The causes of migration are generally of interest.
There can be economic pressures such as industrialization that cause rural to urban movement.
Career advancement (e.g. promotion to head office) causes drift to the capital city but can also cause urban to suburban movement.
In this country, age-related factors such as ill health and retirement may initiate a move to climatically favoured areas such as the south-coast resorts.
Military service also brings about major and frequent changes of domicile.
International arrangements may be made to cater for refugees such as the Vietnamese ‘boat people’; the insecurity of religious minorities (e.g. the Sikhs in India) also causes redistribution of population.
Net migration (i.e. I -E) for the period between censuses is easily obtained by subtraction of the natural increase (B -D) from the change in total population but will be subject to considerable error in comparison with the other components of change.
Even if in-migration balances out-migration so that(I -E) is zero, the differences in life tables, fertility, etc., of the incoming and outgoing migrants may show up.
This phenomenon is simply but unhelpfully called the demographic effect.
SUMMARY
The four components of population change are birth, death, immigration and emigration.
The first two of these are generally the most important and give rise to rates and methods for temporal and international comparison.
Cohort death rates and probabilities are needed for the construction of life tables which are a vital part of actuarial and insurance work.
In developed countries, the birth rate fluctuates much more than the death rate (due to social and economic changes) and is the major cause of difficulty in population projection.
EXERCISES
1.
Calculate the standardized mortality ratio for barmen aged 25–64 for the quinquennium 1949–53 from the following data:
2.
The populations of two towns A and B and the number of persons who die in them in a certain year are given in the table below.
Describe the populations and their deaths and compare their mortality experience using S as the standard population.
Discuss the result.
3.
The information in Table E10.1, from a publication of the Office of Population Censuses and Surveys, gives all teenage conceptions leading to maternities or abortions (under the 1967 Act) in England and Wales in 1969 and 1983.
The conception rates are given per 1000 women at each age (age 13–15 for ‘under 16’).
Numbers are in thousands.
(a)
Make three different estimates of the number of 16-year-old women in 1969.
Why do they differ and which is best?
Table E10.1 Teenage conceptions resulting in maternities and abortions
(b)
Construct indices to compare overall teenage abortion in 1969 and 1983
(i)
per thousand girls
(ii)
per thousand conceptions.
What does each tell you?
4.
Members of a random sample of 1000 men aged 65 in 1955 were followed up and the numbers still alive after successive 5-year intervals were found to be as follows: Construct a life table for these men including estimates of life expectancy at the start of successive 5-year age periods.
Are your estimates all equally unreliable?
Discuss the disadvantages of using these figures to help determine the provision of old people's facilities in your local community over the next 20 years.
How would you seek better information?
5.
In the little-known country of Academia, it is a well established fact that 1 in 10 undergraduates leave university during or at the end of their first year.
The corresponding figure for second-year undergraduates is 1 in 9.
By the end of the third year only 50% of the original starters have not left.
No undergraduate may stay longer than four years.
Draw up a life table for a cohort of 5000 students showing, for each year of study, the probability of leaving and the expected remaining stay calculated on the assumption that one-tenth of the leavers in any year depart at random times during the year and the remainder go at the end of that year.
6.
An article in the September 1987 issue of Employment Gazette gives the following probabilities of reaching particular durations of unemployment based on benefit claims in 1985.
Construct and compare the two ‘life tables’ for length of unemployment.
What are you assuming?
7.
For Table E10.2 of death rates (OPCS Monitor DHI 80/3).
(a)
explain the terms ‘ICD 180’ and ‘birth cohort’,
(b)
draw a graph to show how death rate changed with year of birth for the 30–34 and 40–44 year age groups,
(c)
draw a graph to show changes in death rate with age at death for the 1901 and 1921 birth cohorts,
(d)
what do these graphs tell you?
What else can you infer from the table?
Table E10.2 Cancer of the cervix uteri (ICD 180), females.
Table of deaths per million population with birth cohorts indicated on the diagonals
Population projection
11.1 RELIABILITY OF PROJECTIONS
Here is an early demographer, Gregory King, writing in 1692 on the future growth of the population of England.
That, when the Romans invaded England, 53 years, before our Saviour's time, the kingdom had about 360,000 people; and, at our Saviour's birth, about 400,000 people;
That, at the Norman Conquest, Anno Christi 1066, the kingdom had somewhat above two millions of people;
That, Anno 1260, or about 200 years after the Norman Conquest, the kingdom had 2,750,000 people, or half the present number; so that the people of England have doubled in about 435 years last past;
That in probability the next doubling of the people of England will be in about 600 years to come, or by the year of our Lord 2300; at which time it will have eleven millions of people; but, that the next doubling after that, will not be (in all probability) in less than 12 or 1300 years more, or by the year of our Lord 3500 or 3600; at which time the kingdom will have 22 millions of souls, or four times its present number, in case the world should last so long,
It is very easy to laugh at such calculations; even King's pre-1692 figures were just wild guesses.
Yet, to this day, the estimation of future population size is a hazardous exercise (though an essential one for local and national governments and indeed for the world as a whole).
A projection can be made only on the assumption that current levels or trends continue.
King lived in an age when growth rates changed only slowly so that his greatest concern was the possible end of the world!
But, today, rapid changes in life expectancy, provision of international aid and the availability of birth control make forecasting for periods greater than five years difficult.
In the longer term, estimates are usually made to cover reasonable extremes of change by presenting three sets of projections — high, central and low.
Of course, publication of  projections is often sufficient in itself to cause changes, e.g. by alteration of government policy.
Basically, a starting population of known (census) size is defined in terms of its age, area of residence, etc., and future developments in that population are estimated by consideration of the components of change in the light of current experience.
The greatest error is likely to derive from the migration component.
Thus, the current life table probabilities of death, q x , are applied to the age groups of the recurrent population plus a term for net inwards migration, i.e. 
Migrant mortality is ignored.
Births have to be estimated from current birth rates but related to marriage trends.
11.2 AN EXAMPLE
Consider these principles as applied to a population of ants.
This will suffice to demonstrate possible approaches.
Assume constant death rate of 250  at all ages except that all ants die before age 6.
Projection I
We are told CBR = 330  and given the starting population in 1987, the bold figures are projected.
Projection II
Now assume that age-specific birth rates are given as age 1 = 850 , age 2 = 715 , zero otherwise.
Then we have:
Projection III
Assume births = deaths, i.e. constant population size of 1640.
This may seem a ridiculous assumption but, where there is an unvarying food supply, nature's response is often somewhat like this.
Furthermore, if we were thinking about projecting the workforce of a large employer, the assumption corresponds to recruitment to replace leavers.
Projection IV
Add to projection II in-migration of 100 ants/year during 1987, 1988 and 1989 having a fixed age structure 
There is to be no fertility in year of arrival then fertility as for remainder of population.
(Human populations can behave in this way.)
Mortality as before.
Add to projection II:
SUMMARY
Population projection is essential for planning but subject to considerable error due to the probable invalidity of assumptions about current and future trends.
Methods are often somewhat ad hoc.
EXERCISES
1.
In October 1987, a new university is opened in Academia (see Exercise 5 of Chapter 10) with 4000 first-year students.
A suggested admissions policy for the next four years is that new first year students should be admitted each October to bring the total student numbers to 10% more than the previous October.
Thereafter the total is to be held constant.
Find the frequency distribution over years of study in 1991.
Does the pattern seem satisfactory in the longer term?
2.
Table E11.1 shows annual changes over the period 1977–85 in the number of live births, the crude birth rate, the general fertility rate, and the total period fertility rate; seasonally adjusted quarterly rates for 1984 and 1985 are also included.
(a)
Why do the live birth figures for the last two quarters of 1985 and for the whole of 1985 appear as rounded thousands?
Why are the corresponding rates not similarly rounded?
Table E11.1 Live births in England and Wales, 1977–1985
(b)
Write a report on the information in the table considering, in particular, future implications of the figures.
Changes with time — time series
12.1 INTRODUCTION
We are frequently presented with data consisting of observations at successive points of time; such data are said to form a time series.
We shall consider here only time series having equal intervals between observations (e.g. numbers of road accidents in a city during successive hours of the day, daily climatic measurements, monthly export totals, quarterly unemployment figures, yearly coal consumption, etc.).
Data of this form usually have the (normally unwelcome) property that each observation is not independent of those that have preceded it.
However, it may be possible to use this dependence to form the basis for short-term predictions of future behaviour.
12.2 SMOOTHING TIME SERIES
Time series data can be represented graphically in a line diagram of observations plotted against time as horizontal axis, successive points being joined by straight lines.
For such diagrams, the time axis is often of considerable length and rules about the size relationship between horizontal and vertical axes can be relaxed.
Often the only striking feature of such a representation is the very erratic behaviour of the observations in relation to time.
Example 12.1
Consider the data on imported softwood given in Table 12.1.
We require to clarify any inherent pattern that might be present in the variation of imports over time.
We can do this by various systems of moving averages, whereby an observation is replaced by the mean of a number of observations centred on the one in question.
For example, if we take 5-year moving averages, the 1956 figure is replaced by the mean volume of imports for years 1954 to 1958, i.e.   
Table 12.1 UK imports of softwood, 1954–84 (in thousands of cubic metres)
Likewise, the 1957 figure is replaced by the mean of years 1955 to 1959, i.e.  and so on.
Naturally we lose some points at the beginning and end (four in this case) but this may not be of great importance in a long series.
The line diagram (see Fig. 12.1) of the averages is seen to be much smoother than that of the original data (recall Section 3.6).
However, a number of comments need to be made:
1.
Clearly the number of observations (the period) used to produce each average (five in our case) is a matter of judgement.
If the period is too short, little smoothing will be achieved; if too long, the irregularities will be flattened out to an almost straight horizontal line and there will be considerable gaps at each end of the series.
Fig. 12.1 UK imports of softwood 1954–84 (thousands of cubic metres)
2.
If there are regular fluctuations (e.g. weekly, seasonal), these will be eradicated by using a period equal to the length of the fluctuation.
On the other hand, if, for example, the data follow a 2-year cycle, a 3-year moving average will follow an inverse cycle since alternate averages will contain two lows and one high or two highs and one low.
3.
Odd-length periods have the merit of being centred on the middle observation of the period; even-length periods provide a smoothed value that must be placed midway in time between the middle two observations of the period.
If we are interested only in smoothing for pictorial representation, this does not matter.
However, we often require smoothed values corresponding to the time points of the original observations yet have no choice but to use 12-month or 4-quarter averages to eliminate seasonal variation.
The next subsection resolves this.
12.2.1 Centred moving averages
Suppose we have quarterly figures y 1 , y 2 , y 3 , y 4 , y 5 , Then the first 4-quarter moving average will be  corresponding to a time point midway between the second and third quarters, and the second will be  corresponding to a time midway between the third and fourth quarters.
Hence an appropriate smoothed value to put in place of y 3 will be the average of these two, i.e.  which is just a weighted average (Section 3.1.5) using weights 1, 2, 2, 2, 1.
The statistic would be called a five-quarter centred moving average.
Although five different quarters are involved, each kind of quarter (1st, 2nd, 3rd, 4th) carries equal weight and so the seasonal effect is eliminated.
12.2.2 Forecasting
Clearly all manner of weighting schemes can be devised for special purposes.
One system that gives rise to exponentially weighted moving averages is constructed  so that greatest weight is given to the most recent observation, the weights applied to previous observations diminishing period by period by a constant factor, a.
This is often used to predict the next observation using the most recent (k + 1) observations of a moderately stable series i.e.  where 
Commonly, a is chosen to be about 0.8 in which case, for a 5-period exponentially weighted moving average, we would have 
Such a weighting scheme would not, of course, eliminate quarterly seasonal effects.
12.3 COMPONENTS OF TIME SERIES
Sometimes there is no coherent pattern in a time series and not much can be done by way of analysis other than elementary smoothing.
Often, however, there are regular fluctuations relating to seasonal variation (S).
These may be superimposed on an upward or downward overall trend (T).
Any residual variation (R) is due to longer-term cycles and/or minor unexplained disturbances.
Thus an observed value (Y) can be modelled (see Section 4.3) as  and it is often useful to isolate these components.
We shall use data on quarterly Scottish marriage rates shown in column Y of Table 12.2 and Fig. 12.2(a) to exemplify the calculation.
12.3.1 Trend
To isolate the trend, we must use a moving average of such period as to eliminate the seasonal effects.
For these data we need a 5-quarter centred moving average so that the smoothed values coincide with quarters.
Thus  and so on, giving the trend T and hence the detrended series 
Notice how we can perform this calculation using 4-quarter moving totals, e.g.   
Table 12.2 Scottish quarterly marriage rate () 1977–84
The trend is presented graphically in Fig. 12.2(b) and shows only minor fluctuations about a constant rate.
Fig. 12.2 Scottish marriage rates () 1977–84:(a) row figures (Y);(b) trend (T);(c) de-seasonalized figures (Y -S)
12.3.2 Seasonality
We are now left with seven years' worth of data containing only seasonal and residual effects.
The four quarterly effects (which we are assuming to be invariant  during the time span of the data) can now be estimated from the averages of corresponding quarters over these seven years.
Thus, for the first quarter, the effect is 
The full set is -2.195, 0.413, 2.148, -0.698.
These add up to -0.332; to arrange that the average seasonal effect is zero, we should subtract -0.332÷4 = -0.083 from each giving  
We can now produce seasonally adjusted figures, i.e.  Figure 12.2(c) shows the de-seasonalized figures.
Except for the two quarters at each end, we can examine residuals  and comment on any that are unduly large, indicating observations badly represented by the proposed model of trend and seasonality.
SUMMARY
Moving averages can be used to smooth time series.
These can be centred if circumstances such as seasonal fluctuation require that the period be even.
Time series may be decomposable into components representing temporal trend and seasonal variation thus permitting production of de-seasonalized figures.
EXERCISES
1.
Table 1 of the Digest of UK Energy Statistics for 1987 gives the consumption figures for coal (millions of tonnes) and petroleum (millions of tonnes of coal equivalent: 1 tonne of petroleum = 1.7 tonnes of coal equivalent) for the years 1960–86 as in Table E12.1.
(a)
Superimpose the consumption figures in a line diagram.
Table E12.1 Coal and Petroleum consumption in the UK, 1960–1986
(b)
Add to your diagram the trend patterns derived from three-year moving averages.
Do you feel justified in smoothing the figures across the sudden oil price rise of 1974 or the miners' strike of 1984?
(c)
Could either of the trends be described as ‘linear’?
Summarize the consumption figures for coal and petroleum in a few sentences.
2.
Table 1.1 of the Monthly Digest of Statistics, September 1987 gives the figures shown in Table E12.2 (£m) for UK Gross Domestic Product:
Table E12.2 UK Gross Domestic Product, 1982–87
Determine trend and seasonal components for these data.
Produce de-seasonalized figures and comment on the overall trend and any anomalous quarterly values.
3.
Table 6.14 of the September 1987 Monthly Digest of Statistics gives the figures shown in Table E12.3 (in thousands of hectolitres of alcohol) for home produced whisky:
Table E12.3 Home production of whisky, 1983–87
Determine trend and seasonal components for these data.
Comment on the seasonal fluctuation and any anomalous monthly figures.
Data analysis and the computer
13.1 THE COMPUTER
Nowadays almost all statistical analysis is accomplished with the aid of electronic computers.
This situation is doubtless taken for granted by today's generation, yet, less than twenty-five years ago, leading statisticians were expressing concern lest interposition of the machine would so distance them from the data under consideration that the quality of analysis would suffer.
What has certainly decreased over the years is the proportion of analyses made by qualified statisticians; the computer has brought to researchers an era of ‘do-it-yourself’.
Unfortunately many still stand in such awe of the computer as to believe that any analysis achieved thereby must be correct.
Nothing could be further from the truth.
The computer can produce great volumes of rubbish at incredible speed.
The quality of its analysis is entirely dependent on how intelligently it is used.
We digress a little at this point to introduce some of the jargon of the computer world.
(a) Hardware
The term hardware refers to the electronic and electromechanical bits and pieces that go to make up the machine.
A computer is controlled by its central processing unit (CPU) operating on the core (or memory) which contains the instructions of the stored program (note the spelling) and also immediately required data.
(The ability to store and hence repeat sequences of instructions is what characterizes the computer as distinct from the desk or pocket calculator.)
The central processor also controls the transfer of information into and out of core as required by
1.
input of data
2.
output of results
3.
intermediate storage of data on some form of backing store — magnetic tape magnetic disc or magnetic drum.
(b) Software
This term covers all the programs needed to accomplish an analysis.
These include:
1.
The operating system which arranges the sharing of the computer among several concurrent users, catalogues any data that are to be preserved in files for later use and provides various ‘housekeeping’ facilities helpful to the user;
2.
High-level languages such as FORTRAN (FORmula TRANslation) or BASIC that are used by trained programmers;
3.
Specialized applications packages that can be used by the relatively unskilled, e.g. SPSS (the Statistical Package for the Social Sciences).
In general, the full analysis of a set of data consists of a sequence of tasks from data validation, through the exploratory production of graphs and summary statistics to more complex techniques of estimation and inference.
Some software is therefore designed to run in conversational mode.
The user, seated in front of a video display unit (VDU), interacts directly with the computer, inspecting the results produced by one request before deciding on the next.
This approach is best suited to the study of small to moderate amounts of data but its merits have probably been exaggerated since even skilled statisticians need minutes rather than seconds to respond sensibly to output from all but the simplest analysis steps.
The alternative to this, essential for large volumes of data, is batch working.
In this case, a sequence of tasks (possibly the complete analysis) is presented to the machine which then detaches the job from the user's control and queues it for running at a convenient time — perhaps overnight — the full results being returned to the user thereafter.
Of course, any errors in the instructions submitted will prevent the machine from producing all that was asked and the job has to be corrected and re-run.
Novice users who are most likely to have ‘bugs’ in their jobs find such delays particularly frustrating.
(c)‘Liveware'
This is the term coined to encompass all the personnel who surround (and fuss over) the modern computer — electronic engineers, systems programmers, applications programmers, duty advisers, information officers and those who give courses to would-be users.
13.2 THE ANALYSIS
Any computing activity can generally be subdivided into three parts:
1.
input of the required data
2.
performing a task — generally a calculation of some kind — using these data
3.
presenting the results of the calculations to the user, i.e. output.
In many areas of the physical sciences, the second of these components is by far the most important.
Two or three numbers might be required as input to a calculation lasting several hours, impossible to accomplish except by computer and producing only a small volume of output.
In statistical analysis, on the other hand, the emphasis is on presenting to the machine information that is exceedingly diverse in nature and often very extensive and which can have a complicated, possibly hierarchical, structure.
Great skill is also required of those who devise the layout of the results, since the output from the analysis many also be extensive, hard to assimilate and capable of misinterpretation.
We now go on to discuss the three aspects of analysis listed above.
13.2.1 Data preparation and input
As many have found, often only in retrospect, a good deal of thought must be given to the preparation of data for presentation to the machine.
Of course, some data are logged automatically and exist only in electronic form.
Time series arising from monitoring a patient's heartbeat or from remote sensing some variable at the bottom of an oil well are obvious examples.
Usually such data are so extensive that considerable compression or smoothing is a vital preliminary to analysis.
In psychological experiments, data are often derived directly by presenting the experimental subject with the required test via the screen of a computer terminal; this is clearly most useful for visual perception studies.
Over the years, portable devices of greater and greater sophistication have been devised to facilitate easy and accurate recording of information at the point of collection.
Use of a tape recorder in an interview is a crude example; automatic transfer of tree girths directly from the measuring tape to some magnetic storage medium is another.
The main object of all such instruments is the reduction of transcription errors.
Anyone who has tried to record information outdoors by pencil and paper on a wet and windy winter's day will appreciate this problem!
Optical scanning devices have become increasingly used for data capture either via some sort of bar coding system such as is commonly found at library issue desks, or by means of equipment able to read pencil markings at specified positions on a sheet of paper (‘mark sensing’).
The desirable goals of accurate reading of handwriting or indeed recognition of spoken words have almost been achieved.
For the time being, it remains true that most studies require some form of typing activity (key punching) in the preparation of data for the computer.
Obviously, during this process, errors can be introduced due to misreading of badly written figures, misunderstanding about data layout or simple mistyping.
Verification of the transcription phase is usually achieved by arranging that the data be punched twice by different individuals, the two versions being then compared by some automated means.
Even this is not proof against all errors.
If the volume of data will permit, ‘calling over’ the information as received by the  computer against the earliest written documents can be helpful.
In addition the computer itself can readily be made to perform certain checks.
The once ubiquitous 80-column punched card has now almost disappeared but, even though data are commonly input via a VDU, much of the flavour of the earlier era remains.
This is particularly true in the case of questionnaire data from surveys (discussed more fully in the next chapter).
Such data are usually extensive and a good deal of compression can be achieved by working in fixed format mode where each item of data for a sample unit occupies its own particular field (position) in the line being typed.
The width of the field is determined by the maximal number of characters required for that variable.
For greatest compression, each field must be as small as possible so that, for example , it may be acceptable to use a width of two for an ‘age’ variable by lumping together all ages of 99 and over.
Use of fixed format input means that many coded questionnaire responses need occupy only a single character field.
(Most often, a digit is used with 9 for ‘don't  know ’ or 0 for ‘don't  know ’and 9 for ‘other’.)
The alternative (and now much more common) free format mode requires separators such as a space or a comma to appear between items of data; this slows down data entry for a skilled keypuncher but is probably preferable for the non-expert since problems of field misalignment cannot occur.
As an example, consider the layout of data on age of mother, weight (kg), average number of cigarettes smoked per day, number of days gestation, sex of baby, weight of baby (g),…:
A modern technique known as forms mode entry eliminates many data preparation problems and undoubtedly represents the way ahead.
Basically, a general program, supplied with the data layout and validation requirements of the study, guides the keypuncher page by page through the questionnaire form (or similar document) in a helpful way so as to facilitate error-free transfer of the information and to give immediate warning of any data anomalies that can be detected as well as providing a mechanism for correcting mistakes.
This process is usually mounted on a small computer dedicated to data entry, thus avoiding unnecessary use of the main machine; the transfer takes place only when the data are complete and free from error.
Even though more flexible ways of data preparation are welcome, past rigidity at least had the benefit of encouraging disciplined thinking about the relationship between the information to be collected and its analysis.
In all the discussion of this subsection, we are assuming that, after input to the computer, the data will form a named file in the backing store.
From there it can be made available to the required analysis program(s) as often as necessary.
Three decisions need to be made about how the information is to be held in the file.
1.
Character versus binary.
In the former, the data are held as coded characters exactly reflecting what would appear on a printed page.
This is the usual method and amendments can readily be made to such a file if errors are detected.
In a binary file, information has been converted to the machine's internal number representation and can be scanned much more rapidly by analysis programs.
This is important for extensive data sets that are to be accessed often.
2.
Sequential versus random.
Data records held in sequential mode can be read only from first to last and all steps in any analysis run must be accomplished in a single ‘pass’ through the file.
In a random (or direct) access file, records are individually identified by a key and can be read or re-read in any order.
This is seldom needed for analysis purposes but random access mode is often used where the records are held over a long period and have to be updated with the passage of time.
3.
Flat-file versus database.
This is really an extreme form of the distinction made in 2 above.
In a simple flat-file, the data are thought of as a two-way array of m variates (columns) measured on each of n units (rows), thus 
This array is usually called the data matrix.
From what was said in Chapter 4, the reader will appreciate that there could be several hundred variates and, of course, in the case of a census, n might be of the order of millions.
Where data have a hierarchical structure or where units do not all have the same number of variates recorded (due, for example, to different numbers of visits to a clinic), the more elaborate structure of a database will be needed.
Basically, this is achieved by the provision of software to facilitate the updating of and access to such data.
One considerable advantage is that units satisfying specified conditions can be extracted for analysis without the need to scan the whole file; on the other hand, there are considerable overheads in setting up the database.
Since 1986, the Data Protection Act has required that all information about identifiable people stored by any electronic means is subject to a stringent registration scheme.
Holders of such data must adhere to the very reasonable legal, moral and ethical principles laid down by the Act and the data must be held securely and confidentially.
The Act is very wide ranging particularly in relation to the adjective ‘identifiable’ and subjects have considerable rights of inspection and amendment of their own records.
13.2.2 Statistical calculations
As we have implied above, statistical calculations are generally not as onerous as those of physics or chemistry.
The greater benefit of the computer to statisticians is therefore that it does not become ‘bored’ by a large number of simple tasks.
Data checking is a particular instance, e.g. in a study of obstetric history, the machine can be made to check that, for each woman,
Computers are likewise good at some kinds of coding (e.g. social class from occupation), grouping (e.g. allocation to age class) and calculation of elapsed times (e.g. date of survey-data of birth).
Any such calculation should be done ‘manually’ only if a useful check on data preparation would thereby be provided.
However, problems can and do arise even with statistical calculations, one of the commonest being the build-up of rounding errors.
It must be remembered that computers are usually accurate to only six or seven decimal figures (whereas the average desk calculator can work to ten- or eleven-place accuracy).
The following little example is instructive.
Example 13.1
We are to find the corrected sum of squares of three numbers using the formulae
(a)
(b)
The computer we are using has a precision of six decimal figures.
Suppose the data are 1, 2 and 3.
For (a),
For (b),
There is no problem with the calculation and both answers are the same because the maximal precision of any of the numbers involved is two, i.e. well within the capabilities of the machine.
Suppose, instead, that the data are 1001, 1002, 1003 
For (a), and  so that, as before, the answer is correctly evaluated as 2 since no number exceeds six decimal figures of precision.
But consider what happens with (b).
The squares of the data values require more than six decimal figures and must therefore be truncated by the computer.
Thus   
Further, and hence  We now have the embarrassing impossibility of a negative sum of squares and must conclude that method (b) is unsatisfactory for computer use even though it is the more practicable for hand calculation.
The data used in the example may look unusual and have indeed been chosen for simplicity, but readers who suppose that these values are exceptionally troublesome are encouraged to experiment with others to confirm the reality of the problem.
However simple-minded the above example may appear, it underlines the need for caution in examining the results of statistical computations.
There are undoubtedly items of software in use today that could produce negative sums of squares and many others giving answers that are undetectably wrong.
A problem peculiar to statistical analysis is the existence of missing values in data due, for example, to unexpected death of an animal or human subject, refusal of a respondent to answer all questions or even clerical carelessness.
Sometimes the incompleteness need merely be noted and its extent reported, but other calculations are sensitive to incomplete data and the missing entries must be estimated.
The way in which such imputed values are found varies from case to case and is seldom straightforward.
One statistical activity that often does give rise to lengthy calculations is simulation, a technique mentioned in Section 5.3 as a possible means of determining the form of complex probability distributions.
However, simulation finds wider application nowadays in the many situations where it is necessary to  model the behaviour of some process before its introduction or before a change is made.
For example, a hospital, wishing to evaluate the possible benefits of introducing a pneumatic tube system for the transfer of blood samples between wards and laboratory, could use a computer to mimic the behaviour of the network under varying routine and emergency demand patterns and thus explore potential overload problems.
Calculations of this kind can be particularly time-consuming (especially since there is generally no end to the scenarios that might be worth examining) and considerable ingenuity must be employed in writing software that will run efficiently.
Automated techniques for studying data have been provided in some software.
These aids are generally of the ‘try all possible combinations of…’ variety and therefore demand excessive processing time besides inhibiting the use of common sense — the most valuable of all data analysis tools.
However, current developments in the area of artificial intelligence do hold some promise of making available an efficient blend of human intuition and computer slog.
13.2.3 Output
In many ways, this is also a troublesome part of the analysis sequence.
The difficulty faced by the software designer is to establish the correct level of detail to be provided, particularly in batch mode.
It is tempting always to produce all the information that can be extracted by a particular statistical technique.
This has the advantage of keeping the task request simple and the standardized layout of the output is easily scanned for points of interest by those familiar with the software.
On the other hand, such an approach generally gives rise to very voluminous output neither easy to absorb from the screen of a VDU nor conducive to constructive contemplation when transferred to the continuous stationery produced by line printers.
For users not expert in statistics, a great deal of the output will be meaningless or open to misunderstanding.
No solution to this difficulty has been found and it remains a fundamental problem of ‘do-it-yourself’ data analysis.
For, whereas a user of X-ray crystallography software is generally an X-ray crystallographer, most users of statistical packages are not statisticians.
Following the adage that ‘a picture is worth a thousand words’ one might suppose that graphical methods would provide an exceptionally useful form of output.
For elementary exploration of a set of data, this is indeed true, but a great deal of work is needed to develop helpful graphical presentation of the results of the more sophisticated statistical analysis.
The difficulty lies principally in that, whereas most scientists and engineers operate in a three-dimensional world capable of reasonable representation on a television screen, the multivariate nature of data analysis goes far beyond this into a space of m dimensions conceivable to the human mind only as an abstraction.
In the area of graphical methods as well as in other areas of the interface between computing science and statistics, a great deal of exciting research work remains to be done.
SUMMARY
In the use of computers for statistical analysis, the greatest difficulty relates to data input and validation.
Since the value of the resulting output is highly dependent on these components, they should be given early and careful consideration.
The quality of even the latest software is not above reproach and much remains to be done to ensure that the computer is the servant rather than the master of its users.
EXERCISES
1.
Experiment with the use of some statistical package to which you have access.
(MINITAB is ideal for beginners.)
Try repeating some of the exercises at the ends of Chapters 2, 3 and 4.
Have you any criticisms of the output layout?
How useful are the graphical facilities?
2.
If you know how to write in some high-level language like Basic, Fortran or Pascal, construct programs to
(a)
find the mean and standard deviation of a set of numbers,
(b)
compute the correlation coefficient for n values of x and y,
(c)
(more difficult) de-trend and de-seasonalize a time series.
Social surveys
14.1 THE BASIC FRAMEWORK
In the widest sense, a survey consists of a number of organizational steps which need not occur in the order given below, which may often need to be retraced and which must usually be related to a preceding sequence of research or to comparable contemporaneous studies.
Nevertheless, it is reasonable to pick out the following headings.
1.
Aims.
Are we interested only in summaries or descriptions or are hypotheses to be tested and inferences made?
To what population are the results to apply?
2.
Review.
Available and relevant knowledge must be reviewed.
Discussions with experts and interested bodies will be needed with possible consequent revision of 1.
3.
Research methods.
These must be designed or adapted from other studies bearing in mind available resources; a questionnaire of some kind will often be involved.
A sampling scheme must be proposed (see Chapter 17).
Provisional tables, etc., for the final report should be specified.
4.
Pilot study.
This is sometimes called a ‘pre-test’ and checks the feasibility of the scheme.
A pilot is almost always essential.
A revision of 3 may result.
5.
Staff training.
Personnel for interviewing, coding, etc., must be recruited and trained.
6.
Selection of sample.
7.
Data collection.
8.
Processing of results.
The information collected must be checked and corrected where possible.
Coding will probably be required in preparation for computer analysis.
9.
Statistical analysis.
The interpretation of the output from the computer is basically as required by 1 but, almost always, new questions arise during the course of the study.
10.
Report.
This must be written at a level suited to the expected readership.
Does the interpretation make sense?
Is there support or contradiction from previous research?
What might be done next?
And so back to 1.
14.2 PILOT STUDY
Let us assume that we already know the size of sample, the type of respondent  (child, housewife, air traveller, etc.), whether or not questions are to be asked on more than one occasion, whether there are likely to be seasonal fluctuations, whether the study is to be factual or attitudinal or both, and so on.
The pilot study is a trial run on a small number of ‘guinea pigs’ who should be as like the true respondents as possible in age, intelligence, social class, etc.
Naturally, most pretesting effort should be put into areas thought likely to present problems and areas most crucial to the purposes of the survey.
After background reading and talks with experts (possibly the sponsors of the study) the initial pilot could be simply a number of lengthy unstructured interviews.
If a formal questionnaire is to be involved one can test among other things:
1.
the wording of any introductory letter
2.
how to reduce non-response (e.g. colour of paper, age/sex of interviewer)
3.
wording of questions (if ambiguities are found a further pilot is required to check the re-wording).
It may be possible as a result of the pre-test to change certain questions from free response to a more structured format.
A re-pilot will be required.
For large, frequently repeated studies, each is the pilot for its successor but, on the other hand, changes militate against comparability over time.
14.3 METHODS OF DATA COLLECTION
The decision that must be made is between a postal questionnaire and some form of face-to-face interview.
14.3.1 Postal questionnaire
Arguments For
it is cheap (requires only printing of form, stamped addressed envelope, etc.)
hence a larger sample is possible for the same expenditure
the questionnaire form will almost certainly reach the desired respondent
a frank response to sensitive questions may be obtained
Against
the questions must be simple because no further explanation is possible
the scheme is no use for respondents of low intelligence or limited educational background
‘step-by-step’ question responses cannot be obtained because the respondent usually reads all the questions before starting
the main problem is low response rate.
This is typically 40% but may rise to 80% for groups especially interested in the study.
The non-response is likely to be non-random so that bias is introduced; a  further complication is that the desired respondent may pass the form to someone he or she thinks is more interested.
Government surveys generally achieve a good response rate.
It is usually found (from pilot studies) that ‘personalization’ of the letter of introduction is of no benefit, nor is use of a printed letterhead; neither the colour of paper nor the day (of the week) received seem to make any difference.
Free gifts that can be claimed by return of the form can help.
It has been found that stamped addressed envelopes give a better response than business reply envelopes (people do not steam off the stamps!).
Reminders are sometimes helpful.
To maintain rapport in a postal survey we must always put ourselves in the position of the respondent.
Apart from an explanation of the reason for the survey, how he or she came to be selected, a guarantee of anonymity, etc., each question must maintain the respondent's cooperation by treating him or her with courtesy and gratitude — a few ‘pleases’ and a ‘thank you’are essential.
It is worth avoiding official-sounding phrases such as‘marital status’, ‘length of service’ and also slang unless it is really likely to be helpful.
The questionnaire should be attractively printed and spaced to achieve a good layout and make things easy for the respondent.
Whatever the degree of response finally obtained it is good practice to investigate possible non-randomness and hence bias.
Because it has been found that late returns come from people who are similar to non-responders, comparison of early and late returns may be useful.
We can also relate the achieved sample to known facts about the population being sampled e.g. age/sex structure, geographical distribution.
14.3.2 Formal interview
(Face-to-face, possibly in the street but usually in the home, occasionally by telephone particularly for any follow up needed).
Arguments
For
it is flexible in skilled hands, and allows explanations and probes
pictures, lists, etc., can be shown (possibly in a randomized order) and samples can be handed over the inspection
some degree of immediate coding of responses may be possible
Against
cost of training interviewers; supervisors are needed to give assistance to interviewers and check against possible interviewer dishonesty; travelling and subsistence must be paid even when the respondent proves to be unavailable or uncooperative interviewers may leave or become stale
collection of data may take a long time especially if the survey covers a wide geographical area 
there is always a danger of interviewer bias; interviewers may be tempted to express agreement with the respondent to maintain rapport; the interviewer's opinion may be conveyed by dress, accent, pauses or voice inflection in the reading of questions, thus revealing boredom, surprise etc.
(Remember, interviewers are not a random sample of the general population.)
14.3.3 Self-administered questionnaire
The typical situation is a hospital out-patient department; the receptionist hands a questionnaire to the patient who completes it before or after treatment and returns the form there and then.
Air travellers on a long flight provide another example of ‘captive respondents’.
Arguments For
it is cheap because an already-paid official is used
sampling is usually accurate, e.g. every tenth patient
response rate is good because the respondent is captive and likely to be interested
personal contact and explanation are available where necessary but the risk of interviewer bias is small
Against
the questionnaire has to be fairly short
response may be incomplete when the patient is removed by doctor or ambulance, or the plane lands.
etc.
14.3.4 Group-administered questionnaire
Here, the respondents are typically a class of students or an invited audience.
(An examination is a sort of group-administered questionnaire.)
Questionnaire forms can be distributed and collected or the questions can simply be read out and the responses written on blank sheets of paper.
Arguments For
the respondents are again ‘captive’ and moderately interested
if the questions are read out, the time for each response is the same for all and can be predetermined
respondents can seek clarification
Against
there may be contamination through copying, discussion or even any request for clarification.
14.3.5 ‘Hybrids'
All sorts of variations on these patterns and mixtures of the techniques are useful in specific situations.
For example, the UK Census questionnaire is essentially  hand-distributed, self-administered with an opportunity for clarification at collection time.
14.4 CONFIDENTIALITY
All information should be confidential in the sense that it cannot be traced back to an individual, i.e. anonymity must be guaranteed to achieve frankness.
Thus, even for the Census, small random errors are introduced into district summary tables to prevent ‘recognition’ of unusual (e.g. 104-year-old) people.
In general, therefore, it is best if a questionnaire does not request name unless a future follow-up is anticipated.
14.5 QUESTIONNAIRE DESIGN
Any but the simplest of questionnaires consists of a series of question sequences, each dealing with a different area of interest.
(Occasionally it is useful to ask essentially the same question in different ways at different points as a check.)
14.5.1 Question sequence
Sequencing is at its most important in the interview situation where the respondent cannot ‘look ahead’; but, whatever the method of data collection, it is sound policy for questionnaire designers to give some thought to this aspect both to clarify their own thinking and to simplify the respondent's task.
The order of sequences should be considered first, then the question order within sequences.
Within a sequence it is customary to move from the more factual to the more attitudinal questions, but whatever scheme appears natural and logical (to the respondent) should be used.
In some cases, care must be taken not to put ideas into a respondent's head early in an interview lest these are reflected back at a later point.
For attitude surveys, a good order is:
1.
questions to find out if the respondent has thought about the issue at all 
2.
open questions concerning general feelings
3.
multiple-choice questions on specific matters
4.
questions designed to find out reasons for views held
5.
questions to find out how strongly views are held.
The funnel effect can be used to ‘narrow down’ a response step by step, e.g. ‘Did you choose to study statistics to improve your job prospects?’is too leading a question.
If this is indeed the respondent's belief, we want him or her to advance the idea spontaneously, so we might have: ‘Had you any difficulty in making the choice of subjects that you study?’;
‘Were short-term considerations such as timetabling or published pass rates the only factors that influenced you?’;
‘Do you feel your study of statistics will be of long-term benefit to you in the job market?’
A filter question is one that diverts respondents past a question sequence if it is irrelevant to them.
Excessive use of this technique in a self-administered or postal questionnaire can irritate respondents so much that they give up.
14.5.2 Open questions
An open question invites a ‘free’ answer, recorded in full with the detail limited only by the space on the form.
Coding is required but a selection of responses may be used verbatim in the final report to add interest.
An open question gives great freedom to respondents but we may get just what happens to be uppermost in their minds.
(This may be exactly what is wanted.)
An open question is useful as a probe to clarify or expand the answer to a closed question, e.g. ‘Why?’.
Open questions are easy to ask, difficult to answer and much more difficult to analyse.
Coding is time consuming and usually cannot be planned before the start of data collection; when it can, field coding may be feasible.
(With field coding, the interviewer asks an open question but codes the response immediately from a checklist not shown to the respondent.)
Even for open questions, there will perhaps be loss of information if the interviewer does not use a tape recorder or due to the crudeness of coding required or the need to pool categories during analysis.
We must naturally avoid any compression that causes bias.
As usual, much of this can be sorted out in the pilot phase.
With questions on behaviour such as magazines read or television programmes watched the previous week, the open question produces lists that tend to be abbreviated due to forgetfulness.
On the other hand, checklist questions sometimes inflate the response through confusion of ‘last week’ with ‘previous week’, etc.
Diary techniques may be useful for longer-term studies provided they do not alter the respondent's habits.
14.5.3 Closed questions
A closed question is one with a fixed number of alternative replies.
Because closed questions are easy and quick to answer, there can be more of them.
The danger is that the respondent may become irritated by rigid categories constraining freedom of reply.
There is also a risk that a closed question may bias response to a later open question by directing thought in a particular way.
There are various forms of closed question.
(a) Checklist
A list of options is presented which should include ‘other’ and ‘don't know’categories.
The ‘other’ category may require a ‘please specify’probe.
The selection of more than one category may be permitted.
(b) Rating
A rating gives a numerical value to some kind of judgement usually assuming equal intervals in the rating scale.
Alternatively, a graphical scale can be used, e.g. ‘Please mark your position on the following scale of satisfaction with this book’
For a rating scale, the number of classes varies from 3 or 5 to perhaps 10; it is usually difficult to make distinctions finer than a 10-point scale requires.
An odd number of classes provides a neutral mid-point.
The extreme categories tend to be under-used; we can try to make them sound less extreme or (particularly for a 10-point scale) we can pool responses from a group of end categories.
We have to make clear to the respondent what words like ‘very’, ‘often’, etc., are to mean.
There are almost always problems of defining the end points of scales relating to, for instance, ‘honesty’.
Different respondents may use different frames of reference unless we tell them the purpose of the rating procedure.
Another problem is that of the halo effect; where a group of questions relates to different aspects of, for example, a particular statistics course, respondents with an overall feeling of like or dislike will tend to give high or low ratings to all the features.
We can make it difficult for the respondent to do this by randomizing the direction of the successive ratings so that‘desirable’ is sometimes on the left and sometimes on the right.
(c) Ranking
The respondent is simply asked to place a list in order.
In survey conditions it is unreasonable to ask for a ranking of more than ten objects.
If we must, we can use classed ranks — a hybrid between rating and ranking — in which we ask for assignment to a top class, second class, etc., there being relatively few classes.
A large number of preference pairs gives the same kind of information as a rank ordering.
(d) Inventories
An inventory is a list about, for example, leisure interests or emotional feelings.
Respondents are asked to tick those statements that apply to themselves or to give a mini-rating such as‘usually/sometimes/never’.
Respondents should be told to read all statements before answering so that they have a ‘feel’ for such a rating scale.
A variation is the adjective checklist in which respondents are asked to indicate any of a list of adjectives they feel apply to themselves or to others, or to a product, etc.
(e) Grids
A grid is essentially a two-way inventory allowing rapid collection of a great deal  of information, e.g. ‘Tick the textbooks you would use for each of the following topics’
14.5.4 Dangers in question wording
Whether closed or open questions are used there are many problems of question wording.
(a) Vagueness
Consider, for example, the question ‘Which textbooks have you read in the last six months?’.
Does ‘read’ mean ‘studied assiduously from cover to cover’or ‘borrowed to look at one or two chapters’or ‘glanced at in a library’?
The purpose of the questionnaire will help us to tighten up the wording.
Social and/or geographical ambiguities can cause problems of interpretation even with factual questions, e.g. ‘When do you usually have tea?’
The word ‘tea’ could mean the beverage or a meal varying widely in size and time over different parts of the country and different social classes.
A particularly dangerous form of ambiguity is exemplified by ‘Do you know if your mother ever had measles?’
What will the answer ‘No’ mean in this case?
(b) Loaded words/phrases
These are emotionally coloured and may suggest approval or disapproval of a particular point of view, e.g. ‘racist’, ‘aristocrat’, ‘deprived’, ‘ambitious’.
One problem is that a word may be loaded in a special way for some part of the sample, e.g. ‘disabled’ is a much more acceptable term than ‘crippled’for those in such categories.
In interviewing, different word stress can cause trouble through the  interviewer's misunderstanding.
Try asking the probe ‘Can we eliminate football hooliganism?’ with the accent on each of the five words in turn.
(c) Leading questions
These also suggest what the answer should be or indicate the questioner's point of view, e.g. ‘Are you against excessive cuts in university funding?’.
The term ‘excessive’, however it might be quantified, must invite the answer ‘Yes’.
(d) Prestige bias
When asked in general terms, people claim to include a great deal of high-fibre food in their diets, to read only quality newspapers, never ever to drink and drive, etc.
To avoid this we can emphasize the need for honesty, or provide a filter question so that a low-prestige answer seems equally acceptable.
For example, instead of ‘Have you watched any of the following television documentaries?’, try the preliminary filter ‘Do you ever watch television documentaries?’
Prestige bias can also result from questions introduced by, for example, ‘Experts believe that…’
‘It is often said that…’
(e) Occupation bias
The question ‘What is your job?’ often produces considerable prestige bias (so that a ‘plumber’becomes a ‘heating and sanitary engineer’) or vagueness, e.g. ‘businessman’.
This can be clarified only by requesting a brief job description.
(f) Ignorance/failure of memory
Respondents are always reluctant to admit such deficiencies; questions must be phrased so that they seem quite acceptable, e.g. ‘Which brand of washing powder did you buy last?’ might become ‘Can you remember which brand…?’
For lists, the ‘don't know’ category must always be made available.
(g) Embarrassing questions
These ask about attitudes or behaviour that are not socially approved, e.g. drug abuse, truancy.
The anonymity guarantee must be supported by an atmosphere of permissiveness in the question, e.g. someone may find it easier to admit that they have broken copyright law when faced with the options.’
1.
never
2.
once or twice
3.
on many occasions
‘.
(h) Habit questions
Examples are:
1.
‘When did you last…?’
2.
‘How often in the last week…?’
3.
‘How often do you…on average?’
1 and 2 may be subject to seasonal variation: 3 is vague about the period covered — the average may be changing.
(i) Over-long lists
In the interview situation, shyness or nervousness of respondents can cause them to forget items from a long multiple-choice list.
Keep lists short or show the respondent a card with the relevant items.
There is always a danger of order bias — people remember and therefore choose items near the beginning or end of a list.
Even in a list of two items, the second will be favoured.
Either the list can be presented in random order to each respondent or each interviewer has the list in a different order.
(j) Double-barrelled questions
An example is ‘Are you a vegetarian for ethical or health reasons?’
Does ‘yes’ imply ‘ethical’or ‘health’or ‘both’?
(k) Double negative questions
An example is ‘Would you rather not live in a  non-fluoridated water area?’.
14.6 ANALYSIS
At all stages of a survey, but particularly analysis of the data, considerable cooperation is required among:
1.
the sponsor
2.
the interviewers and coders
3.
the data preparation team
4.
the statistician
5.
the computer programmer.
Usually the sponsor or the statistician is ‘in charge’.
The analysis phase covers conversion of the information collected into a form suitable for computer input, validation, calculation of appropriate statistics, interpretation of these and preparation of a report.
The coding frame for each question should have been constructed at the pilot stage except for open questions where we may need to look at all responses to  decide on groupings.
The coding frames are collected together in a ‘code book’ that contains details of all codes, interviewer instructions, etc.
Copies of the book will be required by all those connected with the survey and all copies must be kept up to date by the incorporation of any coding additions, new conventions and unresolved problems.
Coding can be done by interviewers to give them a break and to sustain their interest, but is more usually done by special staff.
Either a whole form is completed by a single coder who then has an overall picture of the response and may thus spot inconsistencies, or each coder concentrates on all responses to a particular question, thus developing considerable skill at that particular task.
14.7 THE REPORT
Much of what was noted in Chapter 7 is relevant here.
One has to assume that the report will be read critically (though this may be a rather optimistic view).
Establishing the ‘required’ length at the outset will help to get the balance right.
The title must be brief yet reflect the precise area covered by the report; a longer sub-title can be included if necessary.
A useful framework is:
1.
Introduction: outline of previous knowledge plus research team's initial perception of problem.
2.
Review of the literature.
3.
Methods used, described in sufficient detail to enable the research programme to be repeated, possibly elsewhere.
Mention unforeseen problems.
4.
Results section: should include all useful new information provided by the survey summarized by tables, graphs, etc.
5.
Discussion: including any inferences resulting from statistical tests together with tentative explanations and unresolved questions.
6.
Summary: sometimes placed at the beginning — often the only part read.
7.
Acknowledgements of help in the form of advice or assistance in the conduct of the survey.
8.
Bibliography: using either numbered references or the Harvard system, e.g. Yates, F. (1981) Sampling Methods for Censuses and Surveys, 4th edition, Griffin, London.
Yates, F. and Anderson, A.J.B. (1966) A general computer program for the analysis of factorial experiments.
Biometrics, 22, 503–524.
With this system, references in the text are quoted as ‘…
Yates and Anderson (1966)…’ or ‘…
Yates (1981, p. 123)…’
SUMMARY
Organizing a survey is a non-trivial exercise.
Several methods of data collection are possible and a pilot study will be necessary to check that the operation runs  smoothly.
In particular, questionnaire design calls on many talents and the proposed format requires extensive testing in the pilot.
EXERCISES
1.
Aberdeen Royal Infirmary patients are to be surveyed in a bid to provide a better service.
The survey begins on Sunday and the first 10 patients to be discharged from each of 20 wards chosen at random will be given a comprehensive questionnaire to complete anonymously.
The answers will then by analysed to find out what changes or improvements, if any, are thought appropriate.
Patients will be asked such diverse questions as their first reaction to hospital, the food, ward temperature and how they were treated.
Aberdeen Press & journal , 22/1/88
Discuss this study critically, considering
(a)
how the sample is to be chosen (Aberdeen Royal Infirmary is a general teaching hospital with about 650 beds in 30 wards.)
(b)
the advantages and disadvantages of the proposed study scheme
(c)
an outline of the required questionnaire.
2.
A college department wishes to find the views of students who have just completed its first-year course on various aspects such as content, presentation, work load, etc., relative to other departments.
Suggest an appropriate method of investigation, justifying your choice, and suggest any problems you foresee.
3.
State any biases you would expect to find, and what measures you would suggest (either in the planning of the enquiry or in the statistical analysis) for remedying them in the following situations.
(a)
In a study of the use of British Rail for long journeys, a random sample of households is selected and each member is asked to list all railway journeys over 40 miles taken in the last 12 months.
(b)
In a study of the association between family size and university education, each member of a random sample of all who graduated from a certain university in the last 20 years is asked for information on the number of brothers and sisters he or she has.
(c)
In a study of consumer preferences for brands of tea, a random sample of telephone numbers is selected and a telephone enquiry made as to the brand in current use at each address.
(d)
In a study of the relationships between road accidents and the sizes and ages of cars involved, a hospital casualty department questions all accident patients about the cars in which they were travelling.
4.
Explain fully how you would plan a study of the use of leisure recreational reading (i.e. not directly required by courses of study) among students of a college of higher  education.
Mention the most important points about the questionnaire you would use, but do not show it in full.
5.
Criticize briefly each of the following methods for evaluating viewers' appreciation of a television company's programmes:
(a)
tape recording interviews with passers-by at a busy street corner;
(b)
scrutinising letters in the press and letters received by the company from viewers;
(c)
door-to-door enquiries by trained interviewers in randomly chosen areas;
(d)
employment of a number of paid viewers who are required to report on reception, interference, etc., as well as on the programmes themselves.
Schemes of investigation
15.1 INTRODUCTION
Science often proceeds by observing the response to some experimental disturbance, reform or treatment.
Denote this intervention by X. In the case of agriculture, biology, medicine and even psychology, formal laboratory or field experiments can usually be done; in the social sciences this is generally not possible but a good survey can often be thought of in quasi-experimental terms.
We met this distinction between controlled and uncontrolled X in Section 6.1.
Because comparison is generally more important in science than the determination of an absolute value, we are generally involved in observing either the same sample at two different points of time between which X occurs, or two different samples at the same point in time, only one having been subjected to X.
15.2 PROBLEMS
What problems beset this process?
(Because research on human beings presents the greatest difficulties we shall concentrate initially on this area.)
To start with, we shall assume that the same sample of subjects is observed at times T; and T 2 .
15.2.1 History (i.e. changes external to the subjects)
Any number of unknown external events may have affected the status, attitude, behaviour, etc., of a subject between T 1 and T 2 .
This is greater, the longer the interval between T 1 and T 2 .
In a laboratory context, the isolation of the respondent during a T 1 — T 2 period of a few hours should remove this effect; even changes such as sunlight may need to be eliminated.
For longer intervals there may be a seasonal effect, e.g. depression in winter, anxiety as exams approach.
15.2.2 Maturation (i.e. changes within the subjects)
Between T 1 and T 2 a respondent will have grown older, possibly more tired, more hungry, etc.
This includes the possibility that, for example, hospital patients or backward children could improve spontaneously even without X.
15.2.3 Mortality (a special case of the previous problem)
Some respondents may die or otherwise be lost to the study between T 1 and T 2 .
Suppose that over a six-month period, we were investigating changes in attitude of a sample of single girls to contraception, some of the group might get married.
Removal of these from the T 1 observations could bias the findings.
15.2.4 Changes in response at T 1 due to observation at T 2 
The very fact that subjects know that they are being observed may change their responses.
This is the so-called reactivity or Hawthorne effect, named after its discoverer.
15.2.5 Changes in response at T 2 due to observation at T 1 
The T 2 observations may be affected simply by the fact that this is at least a second experience of being observed, i.e. there may be a learning effect due to T 1 .
It may be that questions asked at T 1 cause respondents to develop and shift attitudes during the T 1 — T 2 period possibly through private discussion or, if the study is sufficiently large, public controversy.
If possible, the T 1 observation should be of such a nature as to minimize reaction of this kind.
15.2.6 Changes in instrumentation
This relates to bias introduced by the use of different observers/interviewers at T 1 and T 2 or simply the same people becoming more skilled/bored, or the researcher may be tempted to use ‘improved’ scales of IQ, occupational prestige, etc., at T 2 .
15.2.7 Selection bias
Suppose that on the basis of the observations at T 1 , we select a group of children of low reading ability for experimental remedial education, X. Even if X is useless, a child's performance at T 2 cannot (or is unlikely to) deteriorate since it is already at an extreme low but, through chance variation, it may move upwards giving an overall appearance of improvement.
The data in Table 15.1 are simulated observations from a trivariate distribution in which the population means of all three variables are 55, the standard deviations are 15 and the correlations are ρ yz = ρ xy = 0.5 and ρ xz = 0.25.
It will be seen from the histograms, scatter diagrams (Fig. 15.1) and sample statistics that the random sample of 50 observations reflects these properties reasonably well.
Suppose we think of these data as representing the marks of 50 students in statistics examinations; the variable x, gives the marks at a Christmas examination and y gives the marks of the same students at Easter, the overall correlation being 0.523.
If we consider the ten poorest students at Christmas (i.e. those with marks equal to or less than 40) we find an average increase in marks from 33.6 to 43.6.
This observed 10-mark increase (with a standard error of 4.41) appears to indicate a real improvement.
Table 15.1 A random sample of 50 observations on variables x, y and z
Fig 15.1 Scattergram of the data from Table 15.1:(a) y vs. x (r=0.523);(b) z vs. x (r=0.218)
Thus, if the ten students had been given additional tuition, however ineffective, it would have appeared to be of benefit.
All we have shown is that ‘when you're at the bottom, the only way is up’.
Clearly, therefore, there are dangers in the selection of extreme groups for special treatment since a change is to be expected even if the treatment is completely useless.
This change is greater, the lower the correlation between the before and after measurements.
For instance, if the Easter marks had been as in column z rather than y, showing a correlation with the Christmas performance of only 0.218, the average increase in marks of the lowest ten students would be 18 (standard error = 3.63)— an even more noteworthy ‘improvement’.
Even when subjects are selected for experiment not on the basis of extreme scores in the attribute being studied but on the scores of some correlated attribute, the same problem can arise.
Suppose for the same data the x-variable was a measure of reading ability in fifty 7-year old children, the y-variable a test of spelling ability and the z-variable the response to the same type of test given six months later.
The average overall spelling mean is virtually unchanged.
However, children with poor reading ability will tend to have lower spelling scores on the first occasion (since the correlation is 0.523) than on the second (for which the correlation is only 0.218).
The result will be an apparent average increase in spelling ability for the ten poorest readers of 8 points (standard error = 3.21).
If these ten poorest readers had been selected for remedial teaching during the six-month period a (probably) spurious improvement in spelling would be inferred.
15.2.8 Interaction between observation at T 1 and X
Those who have undergone measurement at T 1 may no longer be typical of the population in their reaction to X, the effect of which may be magnified or diminished.
For example, someone who has just completed a questionnaire (at T 1 ) on nutrition may have a response to a film on food additives different from that which they would have had in the absence of the questionnaire.
When the first observation is a routine measurement (e.g. a school examination) this problem is diminished.
15.2.9 Self-selection
Many of the above comments do not apply to a design of the form  where S 1 and S 2 now relate to observations (made simultaneously) on two different samples of individuals, the first having received treatment X and the second forming the so-called control group.
(The arrow simply indicates that X precedes the time of observation.)
This is, of course, the special case of the design   where X 2 means ‘no special treatment’.
However, a new problem arises — that of selection due to differential recruitment of those receiving or not receiving X. For example if, after a TV programme on violence in society, we sample to compare the opinions of those who watched it with those who did not on the question of bringing back corporal punishment, can we assume that the subpopulations of watchers and non-watchers are otherwise similar?
15.3 MATCHING
The main trouble with the scheme  is that we cannot be sure that an apparent X effect is not due to a differential influence of some extraneous factor(s) on the S 1 and S 2 groups.
However, for such of these variables as we can conjecture may have such an influence, we may be able to match the experimental and control groups.
This matching (usually, but not necessarily, on a one-to-one basis) might be done prospectively at the start of the study, but, if that degree of control is possible, it is usually preferable to use randomization as discussed later (in Section 15.4).
More usually, therefore, the matching takes place retrospectively by rejection of non-matched subjects at the time of selection of S 1 and S 2 .
In such studies, the argument is cause-to-effect, i.e. we match a group that has received X with a control group and look to see if the observed responses in the S 1 and S 2 samples differ.
We could also set up an effect-to-cause study in which we match two groups for which the response measurements differ and look to see if they also differ in some previous X-type phenomenon.
For example, in studying some congenital abnormality we could match a group of such children with a group of normal children and find out if, for instance , the mothers' smoking habits in pregnancy differed.
In medical studies, this is sometimes the only ethical way to proceed; furthermore, the rarity of most abnormalities would demand a very large initial sample if the study were done by initial matching or by a cause-to-effect argument.
If we are interested in matching for age and educational level in the TV example of Section 15.2.9, two approaches are possible:
1.
Precision control: a 1: 1 (or 1: k) pairing is made between S 1 and S 2 in respect of all combinations of the levels of age and educational attainment.
The more extraneous variables involved, the harder it is to find matched subjects for many of the combinations;
2.
Frequency distribution control: match each variable separately.
This means we could have in the (unrealistic) case of two subjects per group, TV group: young and little-educated, old and well-educated non-TV group: young and well-educated, old and little-educated.
We have matching for age and for education but it is possible that people who are older and well-educated have special opinions; such a person happens to receive X and may give rise to a spurious effect.
The object of matching is to increase the internal validity of the study, i.e. to try to ensure that any S 1 -S 2 difference is due to X alone.
This is done at the expense of the external validity, i.e. the applicability of the results to the population as a whole.
The external validity may be less for at least two reasons:
1.
The need to reject unmatchable subjects because they are in some rare category of the extraneous variables means that the sample groups do not truly represent the population;
2.
By insisting that the two groups have the same distribution of some extraneous variable(s) we may be introducing unreal constraints, e.g. smokers and non-smokers do not have the same age distribution in the population.
15.4 RANDOMIZATION
The following techniques use randomization prior to the start of the study to remove most of the problems cited earlier.
However, few survey situations allow such a randomization (R) between control and experimental (X) groups of subjects.
i.e. the sample of subjects is divided into two (not necessarily equal) groups by random allocation at the start of the study.
Both groups are then observed (S 1 T 1 , S 2 T 1 ), the experimental group receives X and the control group no treatment, then both are observed again (S 1 T 2 , S 2 T 2 ).
The correct analysis is to test the average change measured by (S 1 T 2 — S 1 T 1 ) against the average change measured by (S 2 T 2 –S 2 T 1 ).
If neither observers nor subjects know which subjects are receiving X and which are controls (a double blind trial see Section 16.5), the interaction effect discussed in Section 15.2.8 will be removed.
If this is not possible we can achieve a similar result by the more complicated design  
We can measure the effects of some of the problems listed earlier, by using only the observations at the second time point.
We can arrange these post-X observations as 
The X effect is 
The effect of the initial observation is 
The interaction between initial observation and X is given by  i.e. the difference between the X effects in the presence and absence of preliminary observation.
It will be seen that the selection difficulty presented by the design in Section 15.2.9 could be removed by randomization.
Thus the following scheme is both useful and relatively simple: As against(b) we learn nothing about the (possibly interesting) history, etc., effects but the scheme is reasonably practicable.
SUMMARY
In any scientific investigation, the extent to which an observed difference is due only to some deliberate perturbation (internal validity) and the extent to which such a difference reflects a real difference in the parent population (external validity) is dependent on avoidance of a variety of pitfalls.
These are most often found where human subjects are involved.
Matching and randomization are two fundamentally important design techniques to enhance the validity and efficiency of a study.
EXERCISES
1.
A social worker has developed a means of measuring ‘emotional stability’ in children by means of a short interview with the child.
She now wishes to investigate whether the level of stability changes in the six months after children are moved from local authority  homes to foster parents.
Write a report, warning her briefly of any problems and difficulties of interpretation that might occur and suggest how she might best proceed with the study.
2.
New legal provisions on sex equality suggest that male nurses must be permitted to act as midwives.
Since health service planners are concerned about the acceptability of this to mothers-to-be, they are to employ both male and female midwives for a trial period of 12 months in a certain area.
How would you conduct a survey to investigate any change of opinion during the experimental period?
3.
In recent years a policy of discharging long-stay psychiatric patients into the community has been introduced.
Describe how you would conduct a study to investigate the well-being of such ex-patients.
Controlled experiments
16.1 TWIN STUDIES
One type of ‘natural’ matching has been used in some studies of identical (I)(about 1 in 5) and non-identical (N) twins.
Twins, being of identical ages, are usually even better matched on environmental variables during upbringing than are siblings.
Furthermore, I twins are genetically identical, coming from the same ovum, whereas N twins arise from two ova and are therefore no more similar genetically than ordinary siblings.
We can sometimes use this to separate genetic and environmental effects —‘nature versus nurture’.
The concordance between Is for some attribute (e.g. IQ) may be higher than that between Ns so that we can infer a genetic effect.
Suppose we measure smoking concordance as the percentage of cases where if one twin has the smoking habit then so does the other.
If we find that the concordance is higher for Is than Ns, there could be a genetic component.
But, for such an attribute, the difference could also be due to the greater psychological affinity between Is.
In the case of diseases, the effect of any genetic component is more clear cut.
For instance, the Danish Twin Registry (using 4368 twin pairs of the same sex born from 1870 to 1910) found the following concordances.
Because the sample is so large, even the last of these shows evidence of a genetic component.
However, notice that twins are unusual (about 1% of births) and this militates against external validity.
Twins are more likely to be born to older mothers who already have several children; they are likely to be born prematurely and/or have difficult births.
Some of these drawbacks to external validity diminish with the age of the twins being studied.
Of course, in a controlled experiment, where randomization is possible, we are perfectly entitled to combine the advantages of randomization and matching.
This chapter is really about how to achieve this.
The presentation is somewhat terse as the principles are more important than the details.
16.2 ‘BLOCKING'
As a first example, suppose we test the effect of some treatment, X, on pairs of identical twins, randomly allocating which is to receive X and which is to be the control.
Genetic, sex, and environmental effects are eliminated in each within-twins difference and the mean of the these differences is a good measure of the X-effect even if the average response varies greatly over the twin pairs.
We know this (Chapters 3 and 5) as a paired comparison study.
Such a design can be generalized to allow comparisons among more than two experimental treatments, though the rarity of human triplets, quadruplets, etc., means that the techniques are of use primarily in animal or plant studies.
Suppose we have three drugs X 1 , X 2 , X 3 (or two drugs and a control) that are to be compared in their effect on rats.
It will be easy to find litters of three (or more) and, even though such rats may not be ‘identical triplets’, the variability among siblings should be less than among rats in general.
Thus:
1.
comparisons of the drugs within litters reveal differences due only to the effects of X 1 , X 2 , X 3 , i.e. there is good internal validity;
2.
if the within-litter comparisons are replicated for a wide variety of rat parents, the experiment will possess good external validity;
3.
if the treatments are randomly allocated (i.e the firstborn rat does not always receive X 1 ) there should be no bias in the comparisons.
As another example, suppose that, in some sloping field, the effects of nitrogen (N), phosphate (P) and potash (K) on yield of barley are to be compared among themselves and with the yield from unfertilized (C) ground.
The field is sufficiently large for six plots down the sloping length by two plots across; thus, there are 12 plots and we can replicate each of the four treatments three times.
A random allocation of the treatments might give rise to the following:
Notice that, by chance, the N plots tend to be near the top (? dry) end of the field and the K plots are towards the bottom (? wet) end so that we cannot be sure that any observed yield difference between N and K is not due to the moisture  differential or to natural soil nutrients being washed by rain towards the low end of the field, etc.
Recognizing that plots of similar natural fertility, moisture, exposure, etc., are likely to be adjacent in the field, we can adopt a system of local control in which the area is conceptually divided into three blocks each of four contiguous plots:
Within each block, the four treatments are allocated to plots at random so that we might have: For this layout, the treatment comparisons within a block are less subject to unwanted external variation (both suspected and unknown).
A measure of this variation will be available from discrepancies among the mean yields of the blocks.
In summary, to compare t treatments, we require a number of blocks each consisting of t experimental units; for one reason or another the units within each block are expected to behave reasonably similarly in the absence of any treatment, e.g. t mice from the same litter, t plots in the same area of a field.
The t treatments are applied within each block by random allocation to the experimental units.
Because each treatment occurs once and only once in each block, the response to a treatment averaged over the blocks is a valid measure of the effect of that treatment (and, less important, the block averages are valid measures of the block effects).
The less the natural variation within blocks, the greater is the internal validity of the experiment; the greater the natural variability among blocks, the greater the external validity (though a single experiment is unlikely to be adequate in this respect); and randomization of the treatments within each block eliminates bias in the comparisons.
Such a scheme is called a randomized block experiment.
More complicated designs are commonly used, particularly in agricultural research, but the basic principles are the same.
16.3 CONFOUNDING
One problem of the simple randomized block experiment arises when the number of treatments, t, is large.
The straightforward scheme may be
1.
impossible, e.g. we may be unable to get sufficiently large litters;
2.
unsatisfactory since larger blocks will tend to have greater natural variability which runs contrary to the fundamental reasons for blocking.
One possible way round this is called confounding.
Suppose we wish to study eight fertilizer treatments (X 1 to X 8 ) but can find uniform areas of ground (= blocks) for only four plots each.
For just two blocks, we could have:
We can readily compare among X1 to X4 and among X5 to X8; but a comparison of any of X, to X4 with any of X5 to X8 will be confounded with (i.e. inseparable from) the natural difference between the two blocks; this difference is unknown but could be large.
It is possible that these 16 treatment comparisons across blocks are of much less interest than the 12 within-block comparisons and we can be satisfied with what are effectively two separate four-plot experiments.
Alternatively, if enough additional blocks can be found, it may be possible to allocate the treatments in such a way that any treatment comparison can be made within at least one block.
Such a scheme is described as partially confounded.
For example, we could compare three treatments in three blocks each of size two by:
16.4 FACTORIAL TREATMENT COMBINATIONS
Let us return to our fertilizer experiment on barley involving N, P, K and C. You will notice how, on their own, these four factors (as we shall now call them) tell us nothing about how the fertilizers act in combination, e.g. does the effect of N differ according as P is present or absent?
To do this, we need to extend the list of treatments to give C, N, P, K, NP, NK, PK, NPK all eight appearing in each block (assuming there is no confounding).
Such a scheme has a factorial treatment structure and is very commonly used in biological research.
Let the observed yield from the plot receiving a particular treatment combination be denoted by the corresponding small letter(s).
Thus the overall effect of N versus C — the so called main effect of N — will be  averaged over blocks.
But we can also consider, for example, the difference between the effects of N in the presence and in the absence of P — the NP interaction (effect): averaged over blocks.
This is, of course, the same as the differential effect of P in the presence and absence of N, i.e. 
Example 16.1
Suppose that, for some block, we observe (in arbitrary units):
Then, for this block,
Factorial experiments are very efficient, since each experimental unit is used in the estimation of every main effect and interaction.
The only problem is that the number of treatment combinations resulting is usually considerable so that large blocks must be tolerated or confounding introduced.
This last is less of a restriction than might at first be supposed.
Remember that, although there are eight factorial combinations in Example 16.1, only seven comparisons are of interest, namely:
1.
the main effects (N, P and K)
2.
the two-factor interactions (NP, NK and PK)
3.
the three-factor interaction (NPK)
Thus, if only four-plot blocks are available, we could have the following: The reader can check that the only one of the seven comparisons that is confounded is the NPK interaction and this is the least likely to be important.
If it were thought to be worth estimating then a design involving more blocks and partial confounding would have to be used.
16.5 CLINICAL TRIALS
In the design of medical experiments (for the assessment of new drugs, for example) two additional problems generally appear.
The first is the ethical difficulty associated with the need to give a patient the treatment that is believed to be best for him or her as an individual rather than what the statistician's random allocation might assign.
We shall not pursue this further except to remark that a subject's consent to be part of an experiment is always obtained.
The second problem relates to biases introduced by the patient's or physician's preconceptions of the merits of a treatment.
The danger is most marked where the measurement of improvement is subjective.
For example, headaches and colds are very variable and virtually all disappear even without treatment.
Furthermore, illnesses of a psychosomatic nature can be ‘cured’ by any treatment that the patient believes in.
An objective physiological measurement is therefore preferable to asking people how they feel.
Where this cannot be done and the chance of subjective bias remains, experimental subjects are best divided into two groups — one group to receive the ‘active’ drug (D), the other to receive a placebo (P); this is literally a ‘provider of comfort’and is an inactive imitation of the true drug that is identical in appearance, taste, etc.
Until the experiment is over, neither the subject nor the doctor knows which is being administered in any particular case, the random allocation of patients to D or P being made at some remote centre.
Such a design is called a double blind trial.
Clearly some treatments cannot be disguised (e.g. surgical operations) and we must also be sure that some side effect does not reveal the true drug to the doctor.
(If the doctor knows whether a patient is receiving D or P, the trial is described as single blind.)
If patients entering an open-ended trial in order of referral to some clinic were to be assigned to D or P by, for example, tossing a coin, the numbers in the two  groups could become unsatisfactorily discrepant.
The reader can confirm from Table 5.1 that 10% of such trials would result in an imbalance of at least 20:10 after 30 patients.
A technique known as permuted block randomization resolves this.
The chronological patient sequence is divided into blocks of, say four.
Within a block, six orders of equal D/P assignment are possible:
The decision as to which allocation is to apply to any block of four patients is simply made by random selection of a digit in the range 1 to 6.
SUMMARY
Matching can be used in twin studies or, more generally, in paired comparison experiments to reduce unwanted variation.
The principle can be extended for larger blocks of inherently similar experimental units and can thus increase the sensitivity of an experiment to detect a real effect if it exists.
Factorial treatment structure brings considerable advantages of interpretation but may also require over-large blocks so that confounding becomes necessary.
Medical experiments must, if possible, be designed to eliminate psychological biases in patient and physician.
EXERCISES
1.
A drug company is developing a new drug likely to be useful in the alleviation of migraines and other headaches.
Unfortunately it may conceivably cause liver disease if taken over a long period, and this is to be tested by administering it to pigs.
(Pigs are biologically very similar to humans.)
Its merits as a headache cure are to be evaluated using the patients in two general practices.
Advise on how these two aspects of the development programme should be conducted, indicating any reservations you may have about the methods or possible findings.
2.
Distinguish the benefits of matching and randomization in designed experiments.
Sixteen tomato plants are grown on a greenhouse bench in two rows of eight.
Make a list of reasons why the yields of the plants will not be identical.
Four growing regimes are under consideration involving two amounts of water supplied daily each with or without the addition of a trace element.
How would you allocate the treatments to the plants if you were told that the temperature gradient along the bench was 
(i)
nil,
(ii)
moderate,
(iii)
considerable?
3.
Using the method of comparing population proportions given in Exercise 5 of Chapter 5 and the fact that about one in five twins are identical, check the assertion that the Danish Twin Registry data given in Section 16.1 show evidence of a genetic effect even for ‘other acute infections’.
4.
Thirty elderly patients are available for a study of hip joint replacement.
Twenty need replacement of both joints (bilateral); the remainder need only one.
The question at issue is whether it is ‘better’ for the bilateral cases to have both joints replaced in a single operation or to have each joint treated separately in operations, say, six months apart.
Two surgeons will perform all the operations.
It is proposed to quantify the patients' recovery and rehabilitation by measuring the oxygen intake required during a walk of 100 metres ten days after the operation.
Write a report commenting on the experiment.
Sampling schemes
17.1 INTRODUCTION
The simple random sampling scheme described in Chapter 1 is widely used and forms the basis of more sophisticated procedures that are particularly needed in the social sciences and medicine.
In these circumstances we are usually investigating a finite population and therefore a sampling frame (i.e. a list of population members) may well exist.
For some of this chapter we require additionally that such a list contains subsidiary information about each unit such as age and sex.
Such attributes will certainly be known if the frame is derived from some kind of register, of patients, employees or students, for example.
A note about choosing the sample size is in order here although the decision about how large n should be is usually difficult.
In general terms, some idea of the standard deviation of a variate has to be available from past experience and the acceptable width of confidence interval must be stated.
This latter limits the permitted size of standard error and the relationship between standard deviation and standard error then allows determination of the appropriate n for that variable.
For example, for a mean, we would have (see Section 3.6)
The sample size must be the largest required for any of the required estimates or tests.
In practice,
1.
prior knowledge of basic variability is often inadequate particularly in a novel research area and/or
2.
the value of n is limited by resources of time, money or experimental material.
The Greek letter φ is used to denote the sampling fraction, i.e. the proportion (n/N) of the population that is included in the sample.
17.2 SYSTEMATIC SAMPLING
Suppose the sampling fraction is to be 10%.
Then, for systematic sampling, we select the sample members to be every tenth unit on the list starting from a randomly selected point in the first ten.
In general, if the sampling fraction is φ, take every (1÷φth element.
There are thus (1÷φ possible samples each defined by its first member and all equally likely if the starting member is chosen by selecting  a random integer from the range 1, 2,…,(1÷φ).
(We are assuming for convenience that (1÷φ) is an integer.)
If the sampling frame was constructed in such a way that the population members are listed in random order, then systematic sampling is equivalent to simple random sampling.
The method is clearly convenient and cheap; for instance, it is very easy to sample every tenth new patient passing through a hospital clinic even though the population of patients is gradually changing.
The problem is that we cannot be sure that there is no hidden cyclic variation in the list sequence that could cause bias.
For example, if we pick every tenth house number in a street then all those selected will be on the same side of the street; this could affect some aspect of the study.
Likewise, if we sample every fifth fruit tree along a row in an orchard, we cannot be sure that the resulting gaps of, say, 20 metres are not coincident with some long-forgotten drainage scheme.
However, the organizational advantages of being able to sample at a regular interval are so considerable that such anxieties are often set aside.
17.3 SIMPLE RANDOM SAMPLING FROM A SEQUENCE
Simple random sampling can be achieved even when no frame is available but where the population members present themselves one at a time as potential sample members.
The penalty is that the procedure for determining whether or not a particular unit is to be in the sample is a little more complex than with systematic sampling or straightforward random selection of n out of N.
Let us suppose that the kth member of the population sequence is being considered for sample inclusion and that thus far in the sampling process, we have obtained n k of the n sample members that we require.
We consider three possible methods.
Each requires us to be able to produce a (pseudo-) random value, u k equally likely to lie anywhere in the range 0 to 1.
(Some pocket calculators will do this; otherwise one obtains a random integer of 5 or 6 digits from a random number table and precedes it with a decimal point.)
Method 1
Simply select unit k for the sample with probability φ, i.e. generate a random u k in (0,1) and if u k is less than φ, include this unit in the sample.
The only disadvantage is that the final sample is unlikely to have exactly n members though this may not be very important.
Method 2
Select unit k for the sample with probability , stopping once the desired sample size has been attained, i.e. when n k = n.
A few moments' reflection, perhaps considering explicit values for N and n, will convince the reader that the final sample size must be n.
Method 3
This method can be used even if N, the size of the population, is not known in  advance.
It is most useful where records are to be sampled by computer from an unknown number held on, say, a magnetic tape.
When the kth record is encountered, a random value, u k , in the range 0 to 1 is generated; the final sample is to consist of those records associated with the n smallest values of u produced.
As the sampling proceeds, therefore, a continual shuffling of proposed sample members into and out of an auxiliary subfile is required, a procedure ideally suited to the computer.
The sample size is, of course, exactly n.
Although it may not be immediately obvious, all three methods produce simple random samples, i.e. every possible sample of size n out of a population of N has an equal chance of selection.
17.4 STRATIFIED SAMPLING
It will surely have occurred to the reader that a simple random sample, however ‘correctly’ taken, can be unrepresentative of the population from which it is drawn.
Thus all members of a simple random sample of 30 pupils from a mixed school could be girls.
It is an unlikely occurrence but it could happen and we would feel concerned if it did; indeed, we would probably be unhappy with any sample where the sex ratio differed greatly from that of the whole school.
Likewise, we might be disturbed if the social class distribution in the sample turned out to be very unlike the overall pattern.
If the sampling frame contains information on important qualitative attributes such as sex, social class, etc., that split the population into subpopulations or strata, then we can enhance representativeness by constructing the overall sample from the amalgamation of simple random samples from within every stratum, e.g. within each sex stratum or within each sex-by-class combination.
By this means we may greatly enhance the external validity of the study (see Section 15.3).
Furthermore, in practice, we are often interested in making comparisons among the strata so that it is sensible that each be well represented.
Another reason for stratification is akin to that for local control in field experimentation, i.e. variance reduction.
(Indeed stratification is to the survey what local control is to the experiment.)
For, suppose we know that the main variable of interest in the study varies much less within strata than overall, then much more precise estimates are possible for this variable.
The apparent advantage of variance reduction is diminished if, as is much more often the case with surveys than with experiments, many somewhat disparate characteristics are under review rather than just a few correlated variables.
17.4.1 Allocation of sample units to strata
Once a decision to adopt a stratified sampling scheme is made it becomes necessary to decide how the n units in the sample should be allocated to strata.
Suppose that the ith stratum contains N i population members () and we are to select n i sample units () from it.
Two approaches are possible:
(a) Proportional allocation
In this case the overall sampling fraction, φ, is applied to each stratum, i.e.  Thus  The n i values thus calculated will generally have to be rounded and adjusted so that their sum is n.
(b) Optimal allocation
This procedure assumes that, from past experience (possibly a pilot study), we know the standard deviation, d i , of the main variable of interest within stratum i.
Then the sampling fraction in stratum i is made proportional to d i , i.e.  (This assumes once again that there is a single variable of primary interest.)
The more variable strata will therefore be more intensively sampled, leading to estimates of the population mean or total for this variable that are ‘optimal’ in the sense of having smallest possible variance.
We can perhaps clarify the last two methods by means of a simple example.
Example 17.1 Suppose there are three strata as follows:
We require a sample of 100, i.e. an overall sampling fraction of φ=1/6.
Then proportional allocation determines the strata sample sizes to be    
On the other hand, optimal allocation would give  so that i.e.    
17.5 MULTISTAGE SAMPLING
Multistage sampling is used in circumstances where a hierarchy of units exists.
For example, we could have a number of herds of cattle; these herds would be called primary sampling units and the individual cattle are secondary sampling units.
Any number of levels is possible but, in what follows, we shall restrict discussion to such two-level hierarchies.
The important point is that the two-stage sampling process involves a sample of the primaries from each member of which is taken a sample of secondaries.
It is the lower-level sampling units that are of interest.
It is usually the case that the primaries are geographically distinct and, since not all are to appear in the sample, there is a clear administrative advantage over a sampling scheme involving the whole population of secondaries.
For example, we might have 10000 cattle distributed over 100 farms; a simple random sample of 200 cattle is likely to involve most of these herds, resulting in a great deal of travelling by the investigators.
There is an obvious reduction in travel if we sample 10 herds and then 20 animals within each.
A further gain is that we do not need a sampling frame for the 10 000 cattle but only for each of the 10 herds; such a frame could be constructed on the investigator's arrival at a farm.
17.5.1 Cluster sampling
A special case of this two-stage approach requires that a complete census be taken in each of the primaries selected; this is called cluster sampling.
In our example, we would consider all animals in each of the herds sampled.
It is because the individual animal is the unit of study that this is a cluster sample; if herds were the unit of study we would simply have a simple random sample of herds.
This distinction is of some importance.
A random sample of herds might indicate that 40% of them are grazing on marginal land; but if animals were the unit of study we might find only 20% of animals to be on marginal grazing, i.e. the 40% of herds are the smaller ones accounting for only 20% of the cattle.
The clusters need not be natural.
They can be defined arbitrarily for the study, for example, by imposing a grid on a map, in which case the technique is called area sampling.
17.5.2 Two-stage sampling schemes
If we are to have a true two-stage sample, i.e. a sampling scheme for the secondaries within the sampled primaries, we have to face the fact that the primary sampling units can contain very different numbers of secondary sampling units.
However, we require that every secondary sampling unit has an equal chance of inclusion in the sample.
Two possible ways of doing this are now described.
To exemplify these let us consider just three herds of cattle, A, B and C, and assume for simplicity that only one herd is to be sampled.
The numbers of cattle in these herds are 
Method 1
We could simply choose the herd at random(i.e. with probability of 1/3 that any given herd is selected) and then use the same secondary sampling fraction for the animals whichever herd is involved, say φ = 20%.
The probability that any particular animal is included in the study is thus 1÷3 × 1÷5 = 1÷15, i.e. the probability of its herd being the one selected times the probability of that animal being then sampled at the secondary stage.
Although this method fulfils the requirement that all animals are equally likely to be part of the study, we have no control over sample size; if herd A is chosen then there will be 36 cattle in the study whereas for herd C there will be only 6.
Method 2
We could sample the herds with probabilities proportional to their sizes, i.e.
Prob.
A is sampled =  
Prob.
B is sampled = 
Prob.
C is sampled = 
Then, whichever herd is sampled, a fixed number of cattle, say 20, will be chosen at random; the number of secondary sampling units is thus predetermined.
Furthermore the probability that any animal in A is selected must be 180÷300 × 20÷180 = 1÷15; for an animal in B the probability is 90÷300 × 20÷90 = 1÷15; and likewise, for C, the probability is 30÷300 × 20÷30 = 1÷15.
The equiprobability requirement is therefore satisfied.
Although this second method is the more complicated to administer and requires that we have at least a rough idea of the size of the primary sampling units, it has some advantages that become obvious when we recall that several primaries are generally sampled:
1.
It seems reasonable that larger and therefore, in some sense, more important primary sampling units should have a greater chance of inclusion in the study than small ones;
2.
If interviewers or investigators are each allocated the same number of primary sampling units, they will all have the same workload;
3.
If it is later decided to compare primaries, then equal-sized samples from each are likely to give greatest precision.
But how are we to sample the primaries with probability proportional to size?
Suppose we now consider six herds from which two are to be sampled in this way.
The numbers of cattle and the cumulated numbers are as follows:
All that is necessary is to select two numbers from random number tables in the range 1 to 586.
For example, suppose we choose 289 and 423; then 289, being in the range 271 to 300, corresponds in the cumulative total to herd C; likewise 423 corresponds to herd E. 
The difficulty is that we are quite likely to pick the same herd twice and if that were to happen we would have to sample it twice.
An alternative selection method which generally avoids this problem is to take a systematic selection of 2 from the range 1 to 586.
Thus we pick a random number in the range 1 to 586÷2 = 293.
Suppose we get 204, giving herd B; the second of the systematic sample will be 204 + 293 = 497 which gives herd F. By this means we always obtain two different primaries unless one of the primaries accounts for more than half the secondaries, in which case that primary has to appear at least once.
17.6 QUOTA SAMPLING
This technique is particularly associated with street interviewing for public opinion and market research surveys as an alternative to calling on randomly selected households.
A stratification requirement is specified, such as age-by-sex, and the numbers to be sampled from the strata as discussed in Section 17.4.1.
Interviewers are then assigned quotas for interview, e.g. ‘3 men aged 20–29, 4 women aged 45–64,…’ and they choose these sample units in any convenient way, usually by judging that an approaching individual fits one of their required categories and confirming this before the interview.
The quotas, combined over interviewers, thus form a non-random stratified sample.
It is worth noting that a matching problem arises akin to that discussed in Section 15.3; the sample can be selected to mimic the population distribution of stratifying variables either individually or in combination.
The first is clearly easier but less likely to be representative.
Quota sampling is obviously less costly than a simple random sample partly because no sampling frame is needed and no call-backs to unlocated sample members are involved.
The greatest advantage, however, is speed so that an up-to-date measure of opinion on, say, a recent government action is rapidly available.
Against this must be set a non-response about twice that for a random sample involving house calls.
Bias may be introduced by the time and place of the quota selection unbalancing the sample in respect of some unconsidered attribute such as employment status.
The need to make subjective judgement of, say, age before approaching a potential respondent tends to produce ages unevenly clustered around the middle of the age ranges which must, in any case, be broad to make the technique feasible.
Above all, there is no easy way of checking on the honesty of an interviewer who might, at worst, sit comfortably at home inventing questionnaire responses.
17.7 STANDARD ERRORS
Calculation of standard errors of sample statistics is generally more complicated for the sampling schemes described in this chapter than for the simple random sample.
Even for this latter, the standard formulae must be amended if φ is  anything over, say, 0.1.
The non-randomness of the quota sample (and, to some extent, the systematic sample) prevent computation of any measure of variability.
SUMMARY
Although systematic and quota sampling provide samples that are not truly random, they can be used in circumstances where no sampling frame exists.
If, on the other hand, a frame contains additional information about the population members, stratified sampling can improve both external validity and precision.
Where a hierarchy of units exists, multistage sampling is appropriate and often has administrative advantages.
EXERCISES
1.
The tutorial groups for students taking one of three courses are arbitrarily labelled A, B,…,
P and the numbers of students in the groups found to be as follows:
Explain in detail how to take each of the following samples of these students:
(a)
a simple random sample of 45 students,
(b)
a systematic sample of 28 students,
(c)
a sample of 25 students stratified by course; sample sizes in strata are to be proportionally allocated and a simple random sampling scheme used,
(d)
a cluster sample using tutorials as clusters; four tutorials are to be selected with probability proportional to cluster size, two from course 1 and one from each of the other courses.
2.
You are asked to plan a survey to investigate the consumption of sweets by school children.
Write a preliminary memorandum, embodying your proposals for a sampling scheme with a consideration of possible frames and indicating any difficulties of execution or interpretation that occur to you.
3.
To investigate the attitude of residents in a residential area of a city to a number of civic amenities, an agency decides to sample 200 adults from the area.
It proposes to do this by taking a random sample of 200 households, obtained from a listing of all households in the district and then selecting at random one adult from each such household.
The sample members will then be interviewed in their homes during the evening.
Comment on this study proposal, discussing any possible sources of bias likely to be  encountered, as well as such problems as non-response.
What improvements to the scheme can you suggest?
4.
The households in a town are to be sampled in order to estimate the average value per household of assets that are readily convertible into cash.
Households are to be divided into two strata, according to rateable value.
It is thought that for a house in the high-rate stratum the assets are about four times as great as for one in the low-rate stratum, and that the stratum standard deviation is proportional to the square root of the stratum mean.
It is also known that the low-rate stratum contains two-thirds of the population of households.
Resources are available for a sample of 300 households.
Distribute this number between the strata, using
(a)
proportional allocation,
(b)
optimal allocation.
Indicate which method you consider to be more suitable, justifying your choice.
Longitudinal studies and interrupted time series
18.1 PREVALENCE
When the members of a sample are observed once at a single point of time, the study is termed cross-sectional.
Suppose we are interested in deafness defined as the inability to hear some standard sound through headphones.
From such a snapshot study, we can estimate the prevalence of deafness in, for instance, different age/sex categories.
The prevalence of a state is defined as the proportion of a population so categorized at a given point of time regardless of when those affected entered the state.
18.2 INCIDENCE
Of greater interest is incidence — the probability of entering the state in a certain interval of (usually) age.
To determine, say, the incidence of deafness in men between 50 and 55 years of age we really need more of a cine film approach.
If we were to attempt an estimate from a cross-sectional study, it would have to be 
But this ignores the possibility of short-lived deafness, i.e. not all of y will be included in x so that the estimate will be too low.
Such a bias may be reinforced by the fact that those who are deaf in the 45–50 age range are less likely than their fellows to survive to the following age group (because the deafness is related to some general malady or because hearing deficiency increases their risk of fatal accident).
A further complication is the possibility of secular drift, i.e. prevalence changing with the passage of historical time.
This, however, is unlikely to be a serious problem with five-year age bands.
A cross-sectional study might supply some true incidence information if subjects were asked when they first noticed the onset of the condition of interest.
However, unless an accident or acute illness was associated with onset, memory problems are likely to confuse the result.
For example the gradual deterioration often associated with deafness will make it impossible to determine the point at  which the required criterion of deafness was first satisfied.
A longitudinal study involves a pattern of the form  where the same sample (sometimes called the panel) is observed (O i ) at (usually) equal intervals of time — say five years.
Some of the problems noted in Chapter 15 can therefore appear.
Sometimes ‘continuous’ monitoring is possible in the sense that a change of state triggers an observation; this can be the case in, for example, psychiatric follow-up studies.
Returning to our simple example of male deafness, for n men aged 45–50 on a previous survey five years earlier and now, therefore, aged 50–55, we might have numbers such as:
The number, b, in the deaf→not deaf cell tells us how many revert to normal hearing; the estimated probability of reversion is thus .
Prevalence of deafness in the 50–55 age range is given by .
This could be compared with an estimate of the same prevalence from some previous survey to give a measure of secular change.
The incidence is, of course, simply given by .
This simplified presentation takes no account of subjects dying or otherwise being lost to the study between the two observation points.
18.3 CROSS-SECTIONAL VERSUS LONGITUDINAL SURVEYS
Cross-sectional studies take place at a single point of time (although the ‘point’ may in practice cover several months).
We have to assume that this point is in some sense typical.
If comparisons of prevalence are to be made with other populations or other times, there must obviously be reasonable similarity in the research methods used.
However, for a cross-sectional study, a body of field workers, equipment, working accommodation, etc., must be available for a relatively short time on a one-off basis.
Since few agencies can readily bring such expertise together unless they frequently make different cross-sectional studies, there is considerable demand for the resources of firms providing market research and attitudinal survey expertise.
Longitudinal studies done in ‘rounds’(e.g. observation at, say, five-year intervals of historical time) can suffer in the same way but, with continuous monitoring, the problem disappears.
Any cross-sectional type of analysis can also be made with a longitudinal study but, in addition, the aggregation of changes in individuals can be looked at.
This  may make the additional expense worth while, though after the initial O 1 , the sample becomes less and less representative of the population as individuals are selectively lost through death, migration, etc.
Quite apart from the availability of incidence information, a longitudinal study makes it possible to see if certain events regularly precede others, e.g. ‘Do people with a certain laboratory report consistently develop liver disease?’
Knowledge about this time-sequencing may help in cause-effect arguments.
To be set against the longitudinal study is the greater cost although, of course, the basic need to define population and sample is common to both.
At the report stage it may prove difficult to summarize the random sample of histories in a useful way.
The problem of consistency of measurement techniques over a long period of time will have to be faced and may not have a satisfactory solution.
Even with the most careful preliminary organization, unavoidable disintegration of the original project team through retiral, migration, etc., usually occurs to some extent.
This may bring about degeneration of standards either through communication failures or because succeeding team members lack the pioneering zeal of the original staff.
Frequent interim reports may help to maintain interest.
The overall length of a longitudinal study may be shortened somewhat by making it semi-longitudinal.
In the typical case, a series of age cohorts is followed until the cohorts overlap.
On the other hand, many studies become open-ended because more and more uses are found for a cohort whose past history is well documented.
The National Child Development Study involving all 17 000 children born in Great Britain during the first week of March, 1958 is an example; although the initial interest centred on the effects of gestational experience on childhood development the cohort is still intermittently used in a variety of studies.
Another variant of the basic longitudinal study is exemplified by the British Labour Force Survey.
Interviews are conducted throughout the year at the rate of 5000 households per month.
Each sampled household is surveyed five times at quarterly intervals so that, every month, 1000 new households enter the study and 1000 households leave it.
Thus, successive quarterly samples (of 15000 households) have 12000 households in common and the sampled households in two quarters one year apart have 3000 households in common.
18.4 INTERRUPTED TIME SERIES STUDIES
Many data are collected regularly and routinely, particularly by government agencies, local authorities, etc., so that, again, we have  These observations are best thought of as a time series rather than a longitudinal study since the whole community of a country or region (rather than a sample) is generally involved.
If, at some point, a legal reform, a ‘crackdown’ or a relatively  abrupt social change, X, takes place, we have  It may be possible to use this pattern sometimes called an interrupted time series to measure the effect of X, if any.
Sometimes a ‘control’ is available as when, for example, a change is made in England and Wales but not Scotland or when laws diverge in adjacent states of the USA.
This allows estimation of history, etc., effects; but, even without such a control, the existence of observations extending on either side of X may permit elimination of some of the problems inherent in the simple O 1 →X→O 2 design of Chapter 15.
If nothing else, we have a measure of the natural variability, but detrending and de-seasonalization as discussed in Chapter 12 are also possible.
The problem of subjects maturing seldom applies since we are usually looking at whole populations.
Reactivity, learning and the O 1 X interaction are eliminated because of the routineness of the O i .
By considering a few examples we can isolate some of the problems that remain:
(a) Orlando Wilson
One well-known paradox relates to relative stability in the number of reported thefts in Chicago during the 20 years prior to the appointment (X) of one Orlando Wilson as police chief, followed by an abrupt increase and continued climb immediately after X, suggesting a considerable growth in crime as a result of Wilson's appointment.
In reality, Wilson had tightened up reporting procedures so that fewer thefts escaped the record due to carelessness, corruption, etc.
If we look at the corresponding figures for murders (which were much less likely to have been concealed), there is no sign of any change due to X. This is an example of an artefact due to change in instrumentation (more complete recording, in this case).
It is always liable to be found when a ‘new broom’ arrives.
(b) Connecticut speeding crackdown
Another well-known example from the USA arises from an apparently striking fall in fatalities due to road accidents from one year to the next following a police drive against speeding motorists in Connecticut.
However, when these two years' figures are compared with those of previous years, it becomes obvious that the crackdown was introduced as a result of a particularly bad year for accidents.
We would therefore expect the following year(s) to be nearer the average.
We have an example of selection bias as described in Section 15.2.7.
The bad year in question could have had unusually severe winter weather but, even without information on this, figures from adjacent (and therefore similarly affected) states would provide controls.
Fig. 18.1 Great Britain road casualties and car licences 1950–72
(c) The ‘breathalyser'
A similar sort of crackdown in the UK involved introduction of the breathalyser in (September) 1967.
Figure 18.1 shows how the total casualties fell over this period even though the number of vehicles on the road was increasing.
But the fall began almost at the beginning of 1966!
This was almost certainly due to drivers’ increased awareness of the dangers of drunken driving resulting from publicity as the enabling act went through parliament.
Of course, such prior advertisement of a new law is usually necessary for it to be an effectual deterrent.
Were there drivers who changed their habits only through fear of personal detection?
We can get some sort of control reference by comparing figures for fatalities and serious injuries (all of which would be reported) during commuting hours (when drivers are most likely to be sober) with those during weekend nights.
Such a comparison leaves the effect of the breathalyser in little doubt.
(d) Compulsory wearing of car seat belts
Figure 18.2 is reproduced from Statistical Bulletin No. 2 (1985) of the Scottish Development Department and shows various death and injury patterns in  Scotland spanning the introduction of compulsory wearing of seat belts by front-seat occupants of cars and light vans.
The benefit of the legislation for these front-seat occupants is obvious.
If drivers were simply being more careful, there would be a corresponding decline for pedestrians and rear seat passengers; a fall is indeed to be seen but is not nearly so marked.
Indeed, it had actually been argued  that the feeling of security induced in drivers by the wearing of a seat belt would increase the danger to other categories of road user; this is clearly not so.
Fig. 18.2 Road accidents in Scotland: deaths and serious injuries, seasonally adjusted, February 1979-January 1985
(e) Whooping cough vaccination
Fig. 18.3 Whooping cough notifications and deaths, England and Wales 1940-.
Whooping cough vaccine is known to be effective in preventing the illness (or at least lessening its effects) in an individual.
But it occasionally causes death or brain damage.
It is therefore of some interest to ascertain whether the introduction of the vaccine in the UK round about 1957 was of benefit to the community as a whole.
Figure 18.3 shows the notifications and deaths for the diagnosis since 1940.
Notifications give a poor indication of incidence — witness the ‘increase’ when the National Health Service was introduced in 1947 and when notification became compulsory.
Deaths, on the other hand (apart from epidemic  years such as 1941), show a falling trend due to better treatment of the illness, with little evidence of any considerable life-saving effect resulting from mass vaccination  
Fig. 18.4 Five-year moving average of death rates since 1850. x represents the start of an immunization programme Heavy line = whooping cough; D =  diphtheria ; M = measles; S = scarlet fever; T = tuberculosis.
For tuberculosis; standard mortality rates for all ages are given; for other diseases points represent death rates per million (ages < 15).
Figure 18.4 shows, by means of five-year moving averages of deaths per million in England and Wales, how the disease has behaved over a long period in relation to other serious illnesses.
Tuberculosis, for which no vaccine is available, has shown a decline similar to whooping cough, as has scarlet fever which seemed to ‘burn itself out’.
Diphtheria and measles, against which mass vaccinations have also been instituted, showed fairly abrupt falls immediately after introduction of the respective vaccines and indicate what one might expect of a successful programme.
One reason for the apparent failure of whooping cough immunization may lie in the ability of the virus to mutate to new forms which evade destruction until a new vaccine can be devised and introduced; this problem does not arise with diphtheria and measles.
SUMMARY
If incidence rather than prevalence is of interest, a longitudinal panel study is essential even though the administrative burden can be heavy.
On the other hand, official statistics in the form of an interrupted time series present one of the  simplest forms of investigation of social change, provided care is exercised in interpretation.
EXERCISES
1.
Students' attitudes on university funding are to be studied by means of the following design: where S 1 and S 2 are random samples of students starting their second and first years respectively interviewed at time T 1 .
Those S 2 students who survive into second year (i.e. S' 2 ) are interviewed one year later at T 2 .
Thus, X and X' represent ‘one year's experience of university life’.
Discuss the design.
Could it be made longitudinal?
2.
Investigate the extent to which capital punishment is a deterrent to murder.
Explain what statistical evidence you might examine (the experience of New Zealand is interesting) and how you would use the figures to come to as clear a conclusion as possible.
Smoking and lung cancer
19.1 INTRODUCTION
The object of this chapter is to show how the ideas presented in the earlier chapters can be used to collect and collate information on a specific question: ‘Can smoking cause lung cancer?’
Right from the introduction of the smoking habit to Europe, there have been doubts about the effect on health; only since the 1920s has evidence been gathered in any purposeful way.
19.2 EXPERIMENTS
In a case such as this, involving humans, it is not possible to perform a properly designed cause-to-effect experiment.
It would be necessary to allocate at random a group of people to smoke (say) 50 cigarettes per day and another group to be non-smoker controls.
But
1.
there are obvious ethical problems
2.
the experiment would need to run for very many years to demonstrate any difference.
However, animal experiments have shown that, if tobacco extracts are painted on rabbits' ears, cancerous growths tend to appear on the skin of the ear.
On the other hand, beagles, taught with difficulty to ‘smoke’, appeared to suffer no ill effects to the lungs.
19.3 PUBLISHED FIGURES
Death certificates are not particularly useful since, as we have noted, the reported cause of death may be somewhat unreliable; in any case, no indication is given of the deceased's smoking habits.
It is possible to compare lung cancer death rates across countries, sexes, classes, etc., where there are known to be smoking differences.
For instance, the Scottish death rate from lung cancer is at least twice that in countries like Eire and Iceland with a similar genetic background but lower tobacco consumption.
But, of course, the Irish and the Icelanders tend to live in smaller, cleaner towns.
The rate is increasing with time (1 in 900 in the UK in 1903  against 1 in 400 in 1973); but this may be an artefact of more accurate recording of the cause of death — before the arrival of modern terminal care, people usually died of pneumonia.
Likewise the excess rate for males over females, for town dwellers over country dwellers, for social class V over social class I, all of which coincide with smoking differences, can be explained by other means.
‘Odd’ statistics may provide useful clues.
For example, cancer of the hard palate is found worldwide but is fairly rare; however, the prevalence is very high in a certain area of South America.
The explanation appears to be that shepherds at high altitudes reverse their cigars in their mouths to prevent them blowing out in the wind!
19.4 RETROSPECTIVE STUDIES (effect-to-cause)
In the 1920s and 1930s, doctors in the UK and USA began to realize that a very high proportion of their lung cancer patients had been heavy smokers.
Because these comparisons were only among patients, the external validity was poor since patients are not a random sample of the general population.
Furthermore, we have only the patient's word for what he or she smoked; such estimates are often wildly ‘out’ and can be affected by what the patient thinks the doctor wants to hear.
Some tobacco companies even suggested that people might become smokers to relieve the irritation caused by a developing cancer!
This possibility of precedence confusion can be dealt with as follows.
There is a rare form of lung cancer, distinguishable from the usual type only under the microscope.
It is found that people with the rare form are no more likely to be smokers than are those with no lung cancer.
Since the irritation theory would apply equally to both types of cancer, the argument is refuted.
19.5 PROSPECTIVE STUDIES (cause-to-effect)
Raymond Pearl, a Baltimore general practitioner, followed the health records of hundreds of his white male patients for many years and, in 1938, published life tables for smokers (‘moderate’ and ‘heavy’) and non-smokers.
The survivor curves are shown in Fig. 19.1
This suggested effects far beyond what could be accounted for by lung cancer deaths alone.
But Pearl's study was criticized for the non-randomness of his sample.
It might be thought that one could look at lung cancer death rates among groups of people (e.g. Mormons, Seventh Day Adventists) who never smoke.
But such people are abstemious in many other ways, e.g. they do not drink tea.
Furthermore, suppose tendency to want to smoke and tendency to lung cancer are linked by inheritance; a smoking craving might cause some to leave the Mormon faith, but these are also the people with a tendency to lung cancer so that those left behind show a low incidence.
Fig.19.1 Survivor curves for smokers and non-smokers
In 1951, two medical statisticians (Doll and Bradford-Hill) sent questionnaires to all doctors in the UK and received 40 000 replies (a two-thirds response rate) giving details of age, eating and exercise habits and, almost incidentally, smoking habits.
(Because this was a prospective study there is no apparent reason why doctors would have lied about these.)
With the aid of the Registrars General, the survival of these doctors was monitored for ten years and causes of all deaths determined.
The results showed that only about 1 in 1000 non-smoker doctors died of lung cancer, whereas, for heavy smokers, the figure was 1 in 8.
(The study also showed that the death rate from heart attacks was about 50% higher for smokers; this may help to explain Pearl's high death rates.)
At the time of the study, R.A. Fisher complained that there remained a logical loophole; the association could be the result of, say, a common genetic tendency.
If this were so, giving up smoking would not alter an individual's chance of developing lung cancer.
However, as a result of the Doll/Bradford-Hill report, many doctors did give up smoking and since 1960, the rate of death due to lung cancer has declined among doctors.
This refutes Fisher's argument.
SUMMARY
In many statistical investigations, information from a variety of sources must be sifted and collated to produce a consistent and logical explanation of some phenomenon.
The relationship between smoking and lung cancer is a classic study from the field of epidemiology.
EXERCISES
1.
It is said that vegetarians (who do not eat meat or fish) are healthier and live longer than  meat-eaters.
Discuss the difficulties involved in investigating these claims scientifically.
How would you attempt to gather reliable evidence on the matter?
2.
‘Heart disease’ arises from the build-up of fat on the artery walls.
The resulting blood pressure increase can rupture a blood vessel (haemorrhage) or a globule of fat can become detached and block a vessel (thrombosis).
If either of these events happens, the result is a stroke or a heart attack according as the event is in the blood system of the brain or the heart muscle.
Both can be severely disabling or fatal.
The following causes have been suggested:
(a)
stress
(b)
soft water
(c)
lack of exercise
(d)
heredity
(e)
fat in the diet.
Explain how you might seek evidence to evaluate these hypotheses.
3.
Search out information about and write an essay on one of the following:
(a)
damage to the urban and rural environment due to‘acid rain’,
(b)
clusters of cancer cases near nuclear power stations,
(c)
migration as a levelling influence on regional unemployment differentials.
An overview
In the original sense of the word, an ‘interpreter’ is one who negotiates, who is a broker between two parties.
This book is therefore about the two-way process of seeking and assimilating the information that the world around is prepared to give — the data.
We opened with the simile of a wobbling jelly; another parallel that may be helpful is the relationship between a shadow and its origin.
The data are the shadow and, though description of that shadow can be of interest, it is the understanding of the source — the state of nature — that is the object of statistical analysis.
To identify a fluttering shadow as the outline of a bird is not so revealing as the recognition that it is actually cast by a butterfly.
This process can be assisted by trying to change the positions of the light source and the surface on which the shadow falls and then observing the changed pattern.
Although the reader may be entertained by debating the correctness of this analogy, its inclusion is intended to underline the principle that the design of a study and the analysis of the resulting information are intimately related.
The chapters of this book switch erratically between these two aspects but they must come to blend in readers' minds so that their practical implications control the course of an investigation.
The aims of any programme of research should be fully specified at the outset.
Often they are not.
Such are the weaknesses of human nature that initial enthusiasm sometimes overcomes care and prudence in a desire to get the show on the road.
Also, the task can be quite daunting; assumptions and approximations may be required that the originator of the project cannot accept, leading to revision of the goals or abandonment of the study.
Just as students are often unwilling to check arithmetic because they would ‘rather not know about any errors’, so researchers are reluctant to examine their aims, hoping rather that all will become clear in the writing of the final report.
If they are very lucky, it will.
But the time to plan the final report is right at the start.
Of course, such insistence on exploring the logical steps of design and analysis may seem a gospel of perfection.
Nevertheless, critical evaluation must be attempted and, if difficulties remain, they should at least be identified and their likely effects recognized and, if possible, quantified.
Naturally, professional statisticians have considerable skill and experience on how to go about this and enjoy the challenge of participation, particularly if help is sought from an early stage.
In the application of its methods, statistics is both an art and a science.
But the roots of the subject itself are in the realm of cognitive philosophy.
Modern  developments in artificial intelligence are bringing a new light to past perceptions of how the human mind interacts with nature.
Certainly the most difficult ideas in statistics have little to do with mathematics but are rather about what is knowable and how it may be known.
Although this may seem somewhat esoteric it is the first step towards understanding the concepts of population and sample with which this text began.