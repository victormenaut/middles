

Action now for Scientific literacy
WHILE MANY PEOPLE disagree violently about accommodating Cruise missiles, subsidising British Leyland, and membership of the European Community, there can be few who would argue about the value of providing high quality science education in schools.
Report after report issued under the aegis of British governments and parliaments of all flavours, as well as weighty learned institutions, have stressed that the balanced education of the future would put more emphasis on the numerate skills.
These underlie an appreciation of scientific and technological issues, such as those needed by civil servants and industrial managers as well as scientists and engineers.
The Royal Society has now come up with some concrete recommendations for broader-based science education and postponement of specialisation.
All fourth and fifth year pupils in maintained schools would have nine weekly science lessons, three each in physics.
chemistry and biology, equivalent to 22.5 per cent of total teaching time.
At present, choices made at age 13 may require science to be dropped completely, and only 11 per cent of pupils take all three science subjects.
Pupils would need to continue with maths, physics, chemistry and biology in sixth forms to keep all science options open to 18+.
Achievement would be measured by an ‘I-level’ examination, equivalent to half an A-level.
Statistics and mechanics might be suitable subjects.
Pupils would continue with up to six subjects in the sixth form, in varying proportion of I- and A-level.
The Department of Education and Science has welcomed the report and has already promulgated a similar view in its own Green Paper Science Education in Schools , issued in June 1982.
But the steps it is now taking will fall far short of producing the necessary number of teachers for England and Wales.
We already have a shortage of science teachers: the Royal Society estimates at least 400 unfilled vacancies in 1981.
An increase of up to 20 per cent in the number of science teachers would be required to provide the extra lessons recommended for 11–18 year olds.
Nobody has counted science teachers for many years but there are thought to be about 30 000, so 6000 more could be needed.
Science teaching is also undermanned in less overt ways, such as teaching by unqualified teachers and those whose knowledge is out of date, adding perhaps another 10000 to this figure.
The number of secondary science teachers emerging in 1981 was 1654.
It is not clear what future levels will be, though the total of trained secondary teachers for each year from 1983 to 1985 will be just over 3000.
Whichever way these figures are compared, they are not very encouraging.
Current teacher training programmes will at best fill vacancies and make good existing shortcomings.
Ministers should take the opportunity offered by falling school rolls to act seriously now to increase the quality and quantity of science education in schools.
The hundreds of millions of pounds it is poised to spend on science and technology at higher levels, including Britain's IT industry, may be largely wasted if it does not.
Cut the biotechnology cackle
Far more  money is still being made in talking about biotechnology than in actually doing it.
The audience attending the recent joint annual congresses of recombinant DNA and hybridomas, held in Philadelphia, learned this gem at a cost of $750 a head.
Conference organisers have not been slow in jumping on the bandwagon.
But their efforts to provide interesting material are dogged by the reticence of some speakers to elaborate, for commercial reasons, on their methods.
Clearly it is prudent to question carefully the credentials of a biotechnology conference before signing up.
The three-day thrash in Philadelphia, which coincided with an exhibition of products called Biotech ‘83, was not a great success, and the proceedings left a bad taste in several hundred mouths.
Privately, dissatisfied participants questioned whether the organiser — a New York company that sells advertising space in journals, publishes minor journals and headhunts biotechnologists — was truly qualified to assess the scientific merit of the work to be presented.
Details, too, were not ideally arranged.
Instead of a free subscription to an obscure journal (which came with the registration fee) many listeners would have preferred a decent abstract list — not to mention fewer unimaginative and time-worn presentations.
Others might have appreciated a list of participants — usually a routine feature of such meetings.
A scientist who had the temerity to ask at Philadelphia for one was severely reproved.
The purpose of the meeting was to bring together young scientists, businessmen and journal publishers.
The high cost of the meeting, though, kept away many bright young people, and the older established scientists who did come presented familiar work.
The business people, who constituted more than half the audience, also felt cheated because the young scientists they had come to hear and proposition failed to materialise.
If there is anyone still pondering why he trekked out to Philadelphia and paid all that money for Biotech '83, he may contemplate the image of one of the organisers departing from his hotel on the last day of the meeting, wrapped first in a fur coat, and then in a Jaguar.
Science is robust enough to survive such perversions of its purposes, but the conference business may not be.
Conference organisers may well be cutting their own throats in the long run if they try to wring the last penny (or cent) out of the latest bandwagons.
Meanwhile, let the buyer beware.
THIS WEEK
China Syndrome gets Student expelled
THE FIRST American anthropologist to enter rural China since the communist revolution has been expelled from Stanford University after writing about the barbaric birth control methods he witnessed in the Pearl River delta of south-east China.
The anthropologist, Steven Mosher is a doctoral student at Stanford.
He claims that Stanford has been leant on by the Chinese government and by American academics, who were scared that the door to China would be closed unless he was punished.
‘Since fall 1980 each American academic exchange delegation to visit Peking has had to sit through a lengthy recounting of the research improprieties I am alleged to have committed,’ Mosher said, in a statement last week.
China has now tightened up on the access of social scientists to rural areas.
The Mosher case is said to be the main reason.
Mosher was in Shunde county of Guangdong Province from September 1979 until June 1980.
He was on leave from his doctoral programme in Taiwan and was funded by the Washington-based Committee on Scholarly Exchange with the People's Republic of China.
The Chinese charge that Mosher tried to smuggle antique coins out of the country, violated travel regulations, brought a female secretary from Hong Kong to China without permission and used photographs of abortions to mount a political attack on China.
Mosher claims the charges have no merit and can be refuted.
All the fuss is over the fourth charge — should Mosher have published an article in a Taiwan Sunday newspaper condemning Chinese birth-control methods and accompanied by photographs of women 7½ months pregnant and about to be given an abortion?
The photographs have especially riled the Chinese: ‘The photographs…had nothing to do with any conceivable social science research topic and were an insult to those who were photographed’.
His American critics say he has ‘jeopardised his informants’.
But Mosher is unrepentant.
He says the duty of an anthropologist is to record the truth.
He resumed his attack last week labelling the population control programme as ‘sheer barbarity’.
He said: ‘I found that abortions were regularly carried out on women seven, eight, and even nine months pregnant, often against their will, and that there were even cases of officially-instigated infanticide.
Statutes which prohibit these and other inhumane practices were simply ignored by communist cadres in their drive to meet the stringent production quotas they were given.’
Mosher claims the abortions are still going on, but are fewer in number — women are taking to the hills and giving birth under primitive conditions.
The Chinese academy of social science, in a letter to Stanford, ‘hoped that the university would deal with this matter severely’.
But Clifford Barnett, the head of the anthropology department which voted in closed session last week to expel Mosher from his PhD candidature, denies that the department was subjected to any pressure.
Stanford is being extraordinarily tight-lipped about the affair.
The eleven faculty members who made the decision are sworn to secrecy.
They will not reveal the substance of a confidential 47-page report on the reasons for Mosher's expulsion.
Barnett said the charges against Mosher involved ‘professional misbehaviour’.
The university says that to release the report would be injurious, to innocent parties’.
Mosher will appeal against the department's decision and may sue the university.
Lead man deserts the government's camp
Fred Pearce
ANOTHER prop in the British government's case for retaining some lead in petrol gave way yesterday.
Professor Michael Rutter, a prominent member of the Lawther committee which reported on lead three years ago, told a conference at the Royal Institution that ‘it now seems’ that the Lawther committee ‘very substantially underestimated the risk from lead in petrol.’
Ministers still stand by Lawther's conclusion that only 10 per cent of lead in the human body is likely to come from additives in petrol.
But ‘there can be no doubt,’ says Rutter, that ‘the estimate of 10 per cent was much too low.’
Rutter, who was speaking at a conference organised by CLEAR, the anti-lead group, went on to question the way committees such as the Lawther committee are set up.
‘We lacked the breadth of scientific backup needed when dealing with such a complicated set of questions,’ he said.
He praised the US Environmental Protection Agency —‘an independent body of scientists who undertake research and research evaluation as a service to government (but not as a spokesman for government).
We have no equivalent organisation and there is no question that the type of expertise provided by the EPA was not available to us.
Maybe it should be in future.’
He went on to attack scientists on both sides of the lead debate ‘who ally themselves to political campaigns…
. All too often they discredit themselves by engaging in character assassination…and by their transparent acceptance of bad research when it happens to suit their case.’
And he cited two papers, co-authored by Derek Bryce-Smith, professor of organic chemistry at the University of Reading, as being the result of ‘individual scientists who have got rather carried away in a flush of enthusiasm.’
He concluded that he ‘had no doubt’ that lead should be totally removed from petrol.
‘The point is that we do not need any further research to resolve the policy issues.
No good can come from keeping lead in petrol and harm may result.
Do we need to know more than that?’
Britain is set to abandon nuclear reprocessing
Fred Pearce and Roger Milne
THE BRITISH nuclear industry is considering abandoning its commitment to reprocessing spent nuclear fuel from power stations before disposal.
Instead, the fuel rods from the planned new generation of pressurised water reactors (PWRs) may be put into store for more than 100 years, before being buried in deep-rock formations, unreprocessed.
This new twist to the ever-more complicated story of Britain's plans for dealing with nuclear wastes, was confirmed this week by the Central Electricity Generating Board's chief witness at the Sizewell PWR inquiry, John Baker.
He told New Scientist that ‘fortunately neither the AGRs nor the PWRs have fuels which require early reprocessing, as the Magnox fuel does’.
This means that the board will not have to reprocess as soon as in the past ‘or ever at all’.
Reprocessing is becoming an increasingly costly item in the fuel cycle of Britain's ageing Magnox reactors.
Reprocessing of fuel from advanced gas reactors and PWRs will cost even more.
The plan to abandon reprocessing for the oxide fuels used in PWRs, such as the CEGB wants to build at Sizewell, is backed by Britain's biggest engineering firm, GEC.
GEC Energy Systems has pioneered the design of dry-storage ‘warehouses’ that could store the most radioactive wastes, such as spent fuel, for many decades.
The dry stores, says GEC, are cheaper than conventional water pools, leak less radioactivity and can be used for much longer periods.
David Deacon of GEC Energy Systems designed and built the world's first large air-cooled dry-store (essentially an air-conditioned concrete box) at the Wylfa nuclear power station in North Wales in 1976.
He wrote recently in the GEC Journal of Science and Technology: ‘It can be demonstrated that it is significantly cheaper to store fuel for medium to long periods and then to commit it directly to a geological repository, rather than to commit fuel to the reprocessing cycle.’
Storage leaves open the option of reprocessing later, he says, if the value of the uranium and plutonium that could be reclaimed in the process, made it worthwhile.
Deacon's plan would involve every nuclear power station in Britain becoming a ‘mini-Windscale’, storing many years worth of spent fuel on-site.
But the CEGB prefers the idea of a central store.
The board says it needs such a store in any case in the 1990s because there may be a shortage of reprocessing capacity at British Nuclear Fuels' Sellafield plant (New Scientist , 3 February, p 289).
But the new admission that Britain is considering (forgetting) about further reprocessing plant opens up the possibility that the temporary store may become a long-term dumping ground for the industry's most dangerous waste.
Meanwhile other details about the plans of NIREX, Britain's new nuclear-waste authority, to dispose of nuclear waste emerged at the Sizewell inquiry itself this week.
Ronald Flowers, a director of NIREX, revealed that, despite last month's vote at the London Dumping Convention against the dumping of nuclear waste at sea, he thought that ‘it is reasonable to plan for continuations of the UK operation with some increase in quantities disposed of’.
NIREX still hopes to dispose at sea of the massive quantities of contaminate rubble and components that will be produced when Britain's first nuclear power stations are demolished.
Flowers confirmed New Scientist's report last week that new land dumps to take ‘intermediate’ nuclear waste would be in operation by 1995.
How drug addicts unlocked a key to Parkinson's Disease
Ian Anderson, San Francisco
A BIZARRE tale of heroin addiction, drug peddling, police raids and the missing page from an old chemistry journal lie behind the chance discovery in California of a chemical that may be the key to unlocking the mysteries of Parkinson's Disease — a debilitating and incurable disease of the nervous system that afflicts the elderly.
Scientists have stumbled across a chemical that caused Parkinson-like symptoms in six heroin addicts who were admitted, desperately ill, to the Santa Clara Valley Medical Center's neurology unit last summer.
The addicts thought they were injecting a synthetic heroin, instead they had given themselves a permanent Parkinson-like condition.
Two have suffered serious side-effects from the standard Parkinson's treatment.
Ironically their misfortune may lead to a cure for others.
The scientists now believe it will be possible to duplicate the effects of Parkinson's in laboratory animals, opening the door to finding successful treatments.
Early indications from the National Institute of Mental Health in Maryland, which has been testing the Californian findings, suggest that the drug does produce Parkinson-like symptoms.
The story began last summer when a 42-year-old man who couldn't talk and could barely move his fingers was admitted to the medical center.
In the succeeding weeks five more patients, all in their twenties and thirties, were admitted with similar Parkinson-like symptoms.
‘We were stunned,’ said J. William Langston, a neurologist at Stanford university who conducts clinical research at Santa Clara.
‘They had rigidness of posture, slow movement, and tremors — the classical Parkinson's symptoms’.
Three who were hospitalised could not speak intelligibly, had a fixed stare, and were constantly drooling.
But these were young people.
What was happening?
The common link was that they were long-term drug abusers, and they all bought their heroin from pushers in the streets of San Jose and Watsonville, in June and July.
1982.
The researchers eventually stumbled on a 1979 article in a little-known medical journal,Psychiatry Research , which described the plight and eventual suicide of a 23-year-old graduate student.
Here was another case of a heroin-user with Parkinson-like symptoms.
The student, disabled by his disease, was referred in 1976 to the National Institute of Mental Health.
He kept meticulous notes of his drug habit, including the formula he used to obtain his ‘target’ drug, MPPP, a synthetic heroin.
But the student, in trying to speed up the reaction, took short-cuts in his formula.
He succeeded in producing 1-methyl-4-phenyl-1,2,5,6-Tetrahydropyridine — MPTP for short — rather than MPPP.
Langston was finally able to pin MPTP as the culprit.
His patients had MPTP in their bodies and varying degrees of MPPP.
Moreover, narcotics agents tracked down the supplier to an illicit laboratory in Morgan Hill, California — there they found pure MPTP.
The most bizarre aspect of the medical sleuthing was to come later.
Langston's team wondered where the student got his formula?
Drug assayist Ian Irwin from Stanford scoured the literature and located reference to the formula in a 1947 edition of the journal of organic chemistry.
He went scurrying to the dusty shelves in the basement of the chemistry department at Stanford: the vital page had been chopped out by a razor blade.
Police believe the thief was the ‘kitchen chemist’ in Morgan hill whose ‘heroin’sparked off the research in the first place.
Genes watchdog faces takeover
BRITAIN's independent watchdog for genetic manipulation and the growing biotechnology industry is on the verge of losing its independence and being consumed inside the Health and safety Executive (HSE).
The plans for the Genetic Manipulation Advisory Group (GMAG) will be revealed in a consultation paper to be published by the Department of Education and Science (DES) this month.
The move reflects the changing face of genetic engineering as it moves from the university lab to the factory floor.
But some members of the group fear that, inside the HSE, they will lose their teeth.
The HSE takes a generally sanguine view of the dangers to health from biotechnology.
It is usually no more dangerous than, say, the antibiotics industry, the executive says.
The consultation paper from the DES (under whose departmental umbrella the group currently resides) will set out four options for the future of the group.
One would involve its immediate dissolution.
But officials at both the DES and the HSE want to turn it into a specialist advisory group to the HSE.
This would put it, ultimately, under the control of the Department of Employment.
The terms of the transfer could, GMAG members fear, leave them at the beck and call of the parent executive.
At present, research proposals involving Genetic manipulation are referred to both GMAG and the HSE.
But in future GMAG could be by-passed and the dialogue between GMAG and scientists, which has built over six years could be lost.
Similarly, GMAG's unique constitution which includes a representative of the ‘public interest’, would be bound to go.
It would probably be reconstituted along the lines of the Advisory Council for Dangerous Pathogens.
This is composed of 50 per cent scientists and a leavening of employers' and employees' representatives.
Suspects bulge the immigrants' computer
Steve Connor
A BRITISH minister has revealed that some 300 000 names of people who have committed, or are suspected of committing, offences under immigration law have been entered onto a computerised index used by the immigration service.
The  existence of the computer was first revealed last month in New Scientist (10 February, P 351).
The computer was installed in August 1981.
It is run by the intelligence unit of the immigration service and is at Harmondsworth near Heathrow Airport, where the service runs its detention centres.
Liberal peer, Lord Avebury, pressed the government last week for further information about the computer.
Home affairs spokesman, Lord Elton, said that, in addition to the names, the records will include other personal details such as nationality, date of birth and address.
It will note one or more of the ‘clearly defined categories within which the person is considered to fall,’ he said.
These categories include people who have been refused leave to enter Britain, refused entry clearance, deported, or considered to be involved in abuse or attempted abuse of the immigration laws, Elton told the House of Lords.
He went on: ‘Before any particulars are entered on the computer, they are carefully checked with a view to ensuring that they are accurate and, where information has been received from individuals, that it has not been furnished maliciously’.
Avebury questioned the use of an index of suspects.
‘Who considers those persons to have been involved in abuse or attempted abuse of the immigration laws?
What checks are there to be on the reliability of the information.
At last, the (legal) Monty Python video
AFTER nine years of Pythonesque wrangling, the BBC last weekend reached agreement with Equity and the Musicians’ Union on a formula for the release of Old TV programmes on video-tape or disc.
At last, videos of Monty Python and other TV evergreens will be on sale — legally.
The deal is a blow to the video-pirates, who have cashed in by selling illegal copies.
The BBC recognised early on that there was money to be made from selling archive programmes on video.
The corporation first suggested a deal with the Musicians Union in January 1974.
But wrangles followed.
Three years ago the independent ITV companies agreed to pay a royalty of between 15 per cent and 35 per cent to musicians and actors on all tapes sold.
The BBC, with an expensive backlog of  video programmes ready for sale, has settled now more generously.
Radioactive laundry
A FORMER employee of British Nuclear Fuels at Sellafield (formerly Windscale) goes before an industrial tribunal today to claim that he was forced into resigning from the Sellafield works because he insisted on answers to his allegations that the plant's laundry was inefficient and turned out ‘clean’ overalls that still contained traces of radioactivity.
John Taylor claims that a report he wrote that called for improvements was not acted on and research was stopped.
He says that automatic alarms that warn of radioactivity on clothes have been ‘arbitrarily’ adjusted so that the alarms do not go off too often.
And he says that the company has tried, since 1980, to get  approval from the Nuclear Installations Inspectorate to slacken the regulations on how much radioactivity is allowed on clothing.
He says that staff ‘suffer a significant radiological exposure which could easily remain completely undetected’.
This is because the alarm sensors look at overall levels of contamination of clothes, whereas the contamination is often ‘highly localised and correspondingly intense’.
British Nuclear Fuels says that radioactivity on overalls is within safety limits and that Taylor was a difficult man to work with.
The company denies that a swingeing attack on Taylor in an internal staff report amounted to ‘constructive dismissal’, as Taylor has claimed since he resigned.
The NII says its staff ‘do not see the laundry as representing significant safety problems’.
Boom time for  bonsai bugs
AFTER VIDEO-recorders the Japanese invasion continues in the form of  bonsai trees.
A forest of almost 20 000 of these miniature, old and very expensive  trees enter the European Community each year.
It is hardly a crisis.
There is no European  bonsai industry.
But along with the miniature conifers come miniature hitch-hikers.
Popillia Japonica is an insect found in the soil which is brought from Japan with the tree.
Scientists fear that the Oriental bugs could attack European conifers, which have no immunity against the pest.
The EEC has laws to stop harmful organisms in plants coming into the community.
But imports have continued, mostly via Holland.
Now two years of talks with the Japanese aimed at weeding out the insects before they set sail, may be about to bear fruit.
A deal will be signed this month.
Animal front spreads
THE MILITANT defenders of animals' rights have been as active in Canada as in Britain.
The latest letter-bomb attack on the Canadian High Commission in Trafalgar Square, London comes after a spate of raids in Canada in the past two years.
In a raid on the J. M. Schneider meat-packing plant at Kitchener, Ontario trucks had their tyres slashed.
Other attacks have been made on meat plants in the Montreal area and on research laboratories at McGill University and the Hospital for Sick Children in Toronto.
Canadian animal rights groups take different views on this kind of activity.
The Federation of Humane Societies in Canada says the anarchists are destroying the work of the moderates.
More militant groups welcome the publicity it brings.
Exosat launch exported
EUROPEAN X-ray satellite, Exosat, is to be launched aboard an American rocket, the Delta, the council of the European Space Agency agreed last week.
The launch, set for May, has been switched from Europe's Ariane rocket because of continued problems with its turbopump and lubrication system.
(New Scientist , 13 January, p 72).
Toxic dump slush-fund is in the mire
THE US government last week offered to buy the dioxin-contaminated town of Times Beach, Missouri, lock, stock and barrel.
The chief of the Environmental Protection Agency, Anne McGill Burford pledged $33 million from the agency's $1600 million ‘superfund’, which was created in 1980 to finance the clean up of hazardous waste dumps.
Times Beach unwittingly became contaminated ten years ago, when trucks sprayed the town's unpaved streets with waste oil containing dioxin, an extremely toxic byproduct of a common herbicide.
Last December, floods ravaged the town, forcing emergency evacuations and renewing worries about dioxin levels in soil and silt (New Scientist .
27 January, p 218).
The unprecedented offer to buy the 800 homes and 50 businesses in the town followed the publication of the results of the analysis of soil samples taken after the flood by the National Centers for Disease Control.
A majority of the 248 samples taken from road dirt and drainage ditches showed dioxin levels of one part per billion; some samples had 300 parts per billion of dioxin.
According to CDC spokesman David Ferney, the post-flood results confirmed the pre-flood analyses made in December.
On 23 December, CDC advised Times Beach residents to leave the town because the dioxin posed a long-term risk to human health.
But the EPA wanted to wait for the results of the post-flood analysis in the hope that the floods would dilute the dioxin.
They did not.
The announcement last week was the first bit of positive news to come out of the beleaguered EPA for weeks.
The agency is thrashing about in the grip of an angry Congress that is bent on uncovering mismanagement and conflicts-of-interest at the agency.
Last December, Congress charged that money set aside for cleaning up several of the 400 ‘top-priority’ dumps had been held up last autumn so as not to benefit local Democratic officeholders, who were fighting off Republican challengers in the November elections.
Meanwhile, the fund's top hand, Rita Lavelle, was accused of harassing an EPA official who criticised the agency's toxic waste programme, and then of lying about her conduct to inquiring congressmen at later hearings.
The House of Representatives took the unprecedented step of citing Burford for ‘contempt of Congress’— a charge that, if proved, could put her behind bars.
Burford's crime, they said, was to refuse (at President Reagan's request) to turn over scores of EPA documents relating to deals EPA made with businesses that dumped hazardous wastes.
The EPA insists that deals are better than lengthy lawsuits, but some in Congress suspect that polluting companies are getting off lightly.
Lavelle then accused the agency's top lawyer of being too tough on polluting companies.
That got her fired on 7 February.
And word got out that a paper shredder was busily eating superfund documents.
Last week, however, the White House relented.
Reagan fired a handful of top EPA officials and replaced them with five fresh-faced outsiders.
But congressmen are not satisfied.
Last week, they roasted Lavelle for eating at exclusive restaurants at the expense of company officials who were, at the time, under investigation by her department for dumping toxic waste in harbours and unprotected dumps.
Not surprisingly, Reagan's opponents in Congress are using the superfund scandal to attack the President's overall record on the environment.
Since its inception in 1980, the superfund has moved slowly on the country's 11 000 dangerous dumps.
The White House, in turn, is so jittery about the scandal's political reverberations that the President's lawyer is now reviewing all contacts that Burford has had with White House officials in the past.
Some have intimated that these contacts, while not illegal, raise suspicions that the superfund's huge cache for money may have turned into a political slush-fund, spread around to fertilise the candidates of loyal Republicans.
Market for baby-seal skins dries up
CANADIAN seal-hunters have decided to spare baby harp and hooded seals in this spring's seal cull on the ice off Newfoundland.
They will take only the adults, which are shot.
The babies have traditionally been clubbed to death.
The decision came as the EEC dithered again this week over whether to impose an import ban on seal products.
Since the EEC first voted against the imports, in December, West Germany has decided to impose a ban regardless of the final vote.
Norwegian and West German firms have told the sealers that the market for whitecoats (baby harp seals) is too ‘unsure’ for a hunt this year.
The view is confirmed by the Norwegian company Carino, of Dildo, Newfoundland, the largest processor of seal Furs and skins.
Public protest has eroded the market for seal products, said company spokesman Bernhard Nygaard, and ‘we cannot purchase things we cannot sell’.
Nygaard said that Carino was offering sealers only half the usual prices for adult harp and hooded seals.
70–80 000 adults are killed in the Canadian hunt off Newfoundland every year.
Acid talks go sour
THE WAR of words between the US and Canada over acid rain reached new heights last week.
The Reagan administration ruled that two films, produced by the Canadian National Film Board that portrayed the effects of acid rain on the country's lakes and forests should begin with a warning that they are the opinion of a foreign agency and do not please the US government.
Reagan also ruled that a third film, on nuclear war, should bear the same ‘propaganda’ label and that anyone who hires the films should be reported to the government.
The move to counter the effects of the films came as the two governments last week published separate interpretations of a two-year investigation by scientists into acid rain.
The 1000-page study took two years to prepare.
But the two sides could not agree on what to do to control the emissions of sulphur that cause acidified rain.
Canada wants action.
The US does not.
Canada's minister for the environment, John Roberts, said ‘We are at the end of an exhaust pipe…which starts in the US.’
Europe to monitor trade in toxic waste
THE EUROPEAN Commission has published proposals to monitor the shipment of toxic waste across the borders of the 10 member-states.
Three million tonnes of toxic waste is transported for treatment or disposal across the borders of the member states each year.
At the moment supervision of these shipments in most countries stops at the national border.
Only West Germany has specific legislation, dating back to 1974, obliging international and regional shippers to obtain prior permission.
A few other countries have ad hoc bilateral agreements with particular neighbours.
Under the new draft directive, a shipper will be obliged to notify the shipment in the countries of origin transit and destination.
It must also confirm that a suitable waste disposal facility will accept the shipment.
The member states will also have to report  annually to the commission on the amount and type of waste shipped.
The draft directive is expected to be adopted by the EEC council of ministers by the end of the year.
Californian schools reap bumper harvest of Apples
AN APPLE for the teacher is about to take on a new meaning in California.
Apple Computer, the Cinderella company that has risen from the backyard to a worldwide name in six years, is going to give a computer to every public and private school in the state — that's about 10 000 computers with a retail value of almost $20 million.
The gesture is likely to be surrounded by much ballyhoo when it is officially announced in April.
The computers will be in the schools for the start of the 1983–84 school year.
Not to be outdone, Hewlett-Packard, Atari and IBM are also committed to donations, according to Phil Daro, California's administrator for technological education, Radio Shack and Texas Instruments may also join in.
But Apple is going to make the loudest splash.
All told, it will more than double the number of computers in Californian schools.
The grand plan, according to Apple, is to help make the state's youngsters ‘computer literate’.
‘If the United states is to remain technologically strong, our schools must begin training our children in today's technology,’ the company says.
But will the flood of computers be of much use?
Educators have their doubts.
Sandy Wagner, coordinator of computer curriculum in Santa Clara County, says computers aren't much help without trained teachers.
However, Daro says, the point is being recognised by the manufacturers.
‘To dump the machines in schools and then see them not used will look very bad,’ he told New Scientist .
Manufacturers don't want computers to suffer the same fate as educational TV — a lot of promise but precious little worthwhile application.
The companies are likely to provide basic instruction on how to use the computer.
Some will send their staff to help in programming and classroom instruction.
The state is also planning to pitch in.
In January it opened 15 centres which will up-date teachers' skills in maths, science and computer education.
There are plans to enlist students who can already handle computers — about 50 per cent of California's high schools and 20 per cent of its elementary schools already have computers.
But some schools just don't know what to do with the computers.
Daro says there is a tendency to misuse computers, with too much emphasis on drill and practice.
A survey by his department found that this is especially true in working class areas.
‘What this says to me is that the middle and upper classes are being taught how to control the machine, and the lower class is being controlled by it,’ he said.
Why are the manufacturers being so free with their products?
The reason is Assembly Bill 9194, which is set to run until July 1984.
Under the legislation, manufacturers can claim a credit of 25 per cent of the market value for each computer they give away to schools.
Apple is hoping that the event won't be lost on Washington either.
The company is still smarting from the rebuff it received in the senate finance committee.
Apple wanted to donate a computer to every school in the country — more than 100 000 machines.
But two powerful senators, Robert Dole and Howard Metzenbaum, scotched the idea, saying it was a special interest tax break that would cost the Treasury $64 million over four years.
Apple's altruism was also labelled as a smart ploy to introduce the nation's youngsters to its products.
The massive computer giveaway is the most grandiose sign to date of the increasing involvement of the state's high-tech industry with education.
Atari wants to link home computers to school computers via telephone lines.
Based in an affluent school district of Silicon Valley, the system will be used initially to exchange information and advice between students, parents and the local high schools.
The transmission of instruction, homework and class papers is also possible.
But Hewlett-Packard is sceptical that the strategy of one computer per school will work.
‘We don't want to be giving these things away for publicity reasons,’ a company spokesman told New Scientist last week.
‘How can you teach a class on one computer?’
So Hewlett-Packard's approach is to set up what amounts to a high-tech classroom in a select group of schools.
Last week Hewlett-Packard announced that 14 Californian schools — six of them in Silicon Valley — would each receive 10 computers with video monitors and printers, 12 disc drives, two graphics plotters and a selection of educational software.
The total retail value is $714 000.
Why Britain is well-schooled in computers
BRITAIN has stolen a march on the US in one of the most important aspects of information technology — getting computers into schools.
The Department of Industry's £4 million scheme to put a microcomputer into every secondary school in Britain has closed with 6400 computers distributed.
Only 600 schools did not take up the offer of half-price computers.
The government has now turned its attention to primary schools.
So far a quarter of Britain's 27 000 schools for the under-11s have taken up the offer of cut-price computers and training material.
The two computers on offer from the DoI for secondary schools were the BBC Acorn microcomputer and Research Machines 380Z.
Primary schools are being offered the Acorn, a smaller version of the Research Machines computer and the Sinclair Spectrum.
The Department of Education and Science provides the software and training for teachers.
So far the department has sent 7000 teachers on computer courses and reckons to train 50 000 teachers by the end of this year.
The DES has nevertheless been criticised for not giving enough training to teachers and for not providing adequate computer programs for the classroom.
At present the department has a library of some 400 pieces of educational software.
But the Department of Industry recently threw £½ million into the ring in an effort to get firms to adapt their software for use in schools.
The schools meanwhile have been critical of the hardware on offer.
Both the BBC microcomputer and the Spectrum have been plagued by delays and faults.
The Inner London Education Authority, in a circular to schools, says that the computers on offer to primary schools are ‘in many respects inadequate’.
The Sinclair Spectrum came in for particular criticism.
Its keyboard design was ‘not easy to use’ and the ease of damaging the machine  coupled with the cost of repairs meant that a ‘relatively short life should be expected’.
The ILEA recommended the Research Machines 380Z or 480Z.
John Lamb
Keeping the weather eyes open
WESTERN EUROPE is to decide this month on a series of satellites that will provide data for weather forecasts until well into the 1990s.
But the nations are worried by a threat from the US to reduce its own weather satellites This could hit plans, particularly in Britain, to expand meteorological services.
The world relies for much of its weather data on a network of satellites, which are either stationed in a fixed position above the equator or which travel over the poles to scan the globe.
The craft are owned by the US, Japan, USSR and the 11-nation European Space Agency (ESA).
The countries swap data freely.
This permits them to build up a picture of how the weather is changing virtually anywhere on Earth.
But a dark cloud hangs over the future of the data-swapping  arrangement , which is coordinated by the World Meteorological Office of the United Nations.
First, Meteosat, ESA's contribution to the satellite network, is near retiring age and will stop sending information next year.
After that, there could be a gap of at least three years during which the European nations will play no part in satellite meteorology.
To decide on craft that will fill the gap from 1987 onwards, leading west European nations are meeting in Paris on 21–23 March.
They are considering setting up an organisation called Eumetsat to plan three new satellites that will cost £250 million over eight years.
The new craft like the existing Meteosat, would be positioned in geostationary orbit above Africa.
Who will pay for the satellites?
Britain will stump up 14 per cent, France and West Germany both say they will pay 22 per cent .
Other nations have agreed smaller contributions.
Unless France and West Germany — traditionally western Europe's biggest spenders on space technology — can be persuaded to pay more, the project may not go ahead.
One carrot is the siting of the Eumetsat headquarters.
The other nations may agree to place this at ESA's satellite operations centre just outside Frankfurt — if the Germans agree to shell out more cash.
A second problem concerns the two polar-orbiting weather craft run by the US's National Oceanic and Atmospheric Administration (NOAA).
The US says that it intends to axe one satellite.
But the news has dismayed European countries.
They require data from the polar craft as the geostationary vehicles cannot ‘see’ very far north.
One polar craft would provide coverage of a spot on the Earth only every 12 hours, twice the period obtained with a two-satellite system.
This could have a big effect on weather forecasts.
Britain is especially worried as the Meteorological Office in Bracknell requires data from both polar craft to provide world-wide weather forecasts.
The Met office has just agreed a prestigious deal with the International Civil Aviation Organisation to provide weather data to airlines all over the world.
One option would be for Europe to fund some of NOAA's satellite activities.
Alternatively, countries such as Britain could provide space hardware or equipment for ground stations.
Complicating the whole issue is the fact that the US is still deciding on the future for all its remote-sensing satellites.
These include the Landsat Earth-mapping vehicles.
The US government is considering selling off all the satellites to a private company.
Comsat, the Washington satellite firm, has put in a bid for all the craft.
As another option, the US administration could combine the civilian weather craft with another series of meteorological vehicles operated by the US Air Force purely for defence.
Fate of Tees sands may be sealed
AN INTERNATIONALLY important wildfowl refuge is threatened with destruction if a bill now before parliament is passed.
The site is on Seal Sands part of the flats and marshes at the mouth of the River Tees in Cleveland.
Seal Sands is a valuable winter stopping off point for the shelduck.
Many of Europe's population of the species depend on the site, according to the Royal Society for the Protection of Birds.
Seal Sands is also an important staging post on the migration route of at least 11 species of waders, including the knot and redshank, which feed on the plants that grow on the marshes.
It is a designated site of special scientific interest (SSSI) and the British government has listed it as eligible for protection under the Convention of Wetlands of International Importance.
But the Tees and Hartlepool Port Authority has introduced a private member's bill asking for a ten-year extension of its powers to reclaim Seal Sands.
The present powers, granted in 1920, have lead to massive encroachments of industry.
Conservationists were hoping for a halt in 1984, when those powers expire.
In the mid-19th century, before reclamation began, Seal Sands covered 2500 hectares.
By 1969 the port authority had reduced this to 700 hectares.
Today only 140 unreclaimed hectares capable of supporting wildfowl and waders, remain.
Most of the reclaimed land has been developed for oil refineries and chemical works, but half of the area drained since 1969 stands empty.
The port authority says it has no immediate plans for developing more of Seal Sands.
But it says that the surviving mudflats may be needed in future because they are near deep water and could make good sites for factories that are served by ships.
John Tholen, chief executive of the authority, told New Scientist : ‘I am a conservationist myself, but in the end you've got to make up your mind as to which is more important — people or birds.’
Computer spies give thanks to Personal computers
THE NEW generation of personal computers that plug into company computer-networks are proving a boon to computer spies.
So concluded a gathering of international experts on computer security that met in Cannes last week.
The increased use of personal computers means that files of secret information may be left lying around on a cassette or floppy disc in unlocked offices or at the homes of executives.
Theft is easy.
Industrial spies may even be able to plug their own personal computers into confidential data-networks.
On top of this the micro-revolution is bringing a massive rise in computer literacy.
Many more people can become computer spies.
‘Companies can't tell you how many machines they have so how can they possibly protect them?’ said Ronald Berg, of the Scandia risk-management company in Sweden.
Leslie Goldberg, a consultant spoke of civil servants in the US Defense Department who took their own personal computers into the office ‘because it was easier to use them than the department's system.’
‘It's too late to do much about it except to make sure you always lock up your personal computer.’
Goldberg told New Scientist .
Andrew Lloyd Cannes
Chemical companies in West Germany claim that the Eastern Bloc may be tapping the communications networks of government agencies and stealing their commercial secrets.
They told the Cannes conference that new legislation that requires them to give more information on toxic chemicals to federal safety watchdogs could leave them wide open to spies.
To police the new safety standards much more commercial information will have to be passed to local officials in West Berlin.
That means sending the data along microwave links that cross East German territory.
Now a data processing research institute is to study ways of coding such sensitive data.
The West builds up its metals mountain
Peter Marsh
Following the American lead, Britain has created a stockpile of strategic materials to safeguard industry if supplies run short.
The industrialised world is also looking to developments in technology to help it out of materials shortages — and China may lend a hand too
ANYONE who feels like investing in a few grams of silver should call up a certain Carroll Jones in his office in Washington.
Jones, an official in the US government's General Services Agency, has $2000 million worth of the stuff that he would dearly love to get rid of.
Likewise, if it is tin that you hanker for, Jones is in charge of 150000 tonnes of the material that his political masters have decided they no longer need.
The tin — worth another $2000 million — will reside in government warehouses until the US Congress decides to sell.
The gargantuan quantities of tin and silver are part of the stockpile of strategic materials that, since 1950. the American government has stored away, squirrel-like, in case of an emergency such as a world war.
The current policy is to keep 94 materials in quantities sufficient to supply American industry for up to three years.
Among the substances are important metals such as aluminium, cobalt, chromium and manganese that the US has to purchase almost exclusively from abroad.
The stockpiled items — worth a total of $12000 million, a figure equivalent to the gross domestic product of Ecuador — also include more esoteric substances such as talc, iodine, sapphires, rubies and mercury.
As the official in charge of the stockpile, Jones administers the 114 depots and warehouses that store the materials.
One of the more spectacular sites is in Gregory, Texas, where some 4 million tonnes of bauxite are kept in dumps, some of them the height of a six storey building.
Last April, President Reagan announced he wanted to build up the stockpiles, to insulate the US from shortfalls in foreign supplies and to bolster the American defence industry, which relies on some of the more exotic metals.
Thus Congress has authorised Carroll Jones to spend an extra $157 million on the stockpile this financial year.
At the same time, the nation's rulers have reduced their requirements for particular items.
This is why Jones is stuck with a large surplus of silver and tin, which he must sell only in small quantities for fear of upsetting the world's metal markets.
As in other aspects of economic policy, Mrs Thatcher's government in Britain has been eager to follow the American example.
For some months, officials have been secretly creating their own stockpile of key materials that Britain imports, particularly from politically sensitive areas of the world, and which are important in keeping industry's engines running.
The government has given few details of its hoarding scheme.
But it is much smaller than the American store.
According to reports in the metals industry the government plans to spend about £100 million over the next six months.
The metals probably include chromium, manganese, cobalt, vanadium, nickel, tungsten and tantalum.
The places where they are stored are secret.
Sources indicate that the most likely sites are government warehouses in the Midlands or RAF bases.
Britain and the US are not the only Western countries to worry about supplies of key materials.
In 1975, France set up a stockpile, administered by the director general of mines in the French Ministry of Industry.
The stockpile will be worth some £400 million by 1985, if spending targets are met.
The French policy is to keep two years' supply of metals; the government has admitted to hoarding copper, lead, tungsten and chromium, although it has kept to itself exactly what else the stockpile contains.
Japan, Spain, Sweden, South Korea and Italy all operate stockpiles or are considering them.
West Germany abandoned plans for a similar scheme in 1980 because of lack of cash.
Several factors are behind the move toward stockpiling.
Over the past 20 years, factories in the West have become increasingly dependent on materials which, though used in relatively low quantities, are vital to many of today's industrial, consumer and defence products.
Tungsten, molybdenum, chromium and manganese, for instance, are used to harden steel.
Production of cars.
industrial machinery — indeed anything that includes hardened steel — would grind to a halt if trade in chromium stopped.
A study in West Germany in 1978 said that, with a 30 per cent shortfall in supplies of the metal over a full year the country's output of goods and services would fall by a quarter.
Platinum is another metal on which the industrialised world relies, mainly for its role as a catalyst in chemical reactions.
All the platinum ever mined would fill no more than the average living room.
Yet without the material, the oil, chemical and food industries of the West would come to a standstill.
And in the US and other countries with stringent rules governing air pollution, car firms would run short of catalytic converters for fitting to exhausts.
In weaponry, tungsten is a vital component in armour plating.
Titanium is used as a lightweight element in aircraft and other military vehicles.
In the US, the defence industries account for 44 per cent of the country's consumption of the metal.
Cobalt is important in the jet engines of aeroplanes because it makes turbine blades resist high temperatures.
Alexander Haig.
no less, has testified to the US Congress that a shortfall in cobalt supplies could have a ‘devastating’ effect on aircraft production.
Related to this dependence is that many of the metals are imported from countries which either have a near-monopoly, are politically unstable or which are ideologically opposed to the capitalist West.
Thus South Africa and the USSR provide between them 94 per cent of the world's platinum-group metals, 65 per cent of its vanadium, 60 per cent of its chromium and 58 per cent of its manganese.
One mine in Brazil provides virtually all of Britain's niobium.
Likewise, the West depends heavily on Zaire and Zambia for cobalt and on Malaysia and Thailand for tantalum.
The linkages between the political, economic and geographical aspects of the world's metals trade have provided plenty of material for conspiracy theorists, particularly right-wing politicians in the US who have voiced fears about a ‘metals cartel’.
According to this argument, the Soviet Union could join forces (either through persuasion or through a military takeover) with other important metals producers, in southern Africa for example.
The cartel could then clamp down on shipments of key materials, slowly bleeding the industries of the developed world.
The move to build up stockpiles in the US is a result, at least partly, of such fears.
Another reason is that the US will simply need more of the world's more unusual metals if it wants to meet its targets of producing more military equipment.
According to the government's plans, defence spending will grow from $187 000 million in 1982 to $274000 million in 1984.
And the share of military expenditure devoted to purchases of new equipment will grow from 23 per cent last year to 39 per cent in five years' time.
Hoarding key materials is not the only way in which Western nations can safeguard key industries.
Indeed.
Philip Crowson, economic adviser to Rio Tinto-Zinc, points out that stockpiles can have unwanted side effects.
First, changes in technology and in demand can mean the kinds or grades of metals in the stores are no longer required.
As a result, a nation could find itself with huge quantities of substances which it does not need — which is precisely the American experience.
Secondly, says Crowson, the very existence of a stockpile can dull the incentive among both governments and companies to find new sources for materials or to replace the materials with something else that is cheaper or easier to obtain, due to new technology.
An obvious route to reducing dependence on outside sources is to become more self-sufficient.
As part of his policy statement on materials last year, President Reagan said the government would promote domestic mining, for instance by tax incentives or by simplifying planning procedures.
The American government owns one-third of the nation's land, much of it rich in resources.
But under existing environmental legislation, some two-thirds of this land cannot be exploited.
This applies particularly to ‘wilderness’ states such as Montana where large quantities of cobalt and platinum are lying under the ground.
President Reagan's rhetoric has, however, yet to be converted to action.
The Department of Interior has only just begun a two year study to streamline the fiendishly complex sets of planning rules that govern new mining projects.
Another problem is that the administration has given few details on the cash that it will put up to induce mining companies to begin new ventures.
President Reagan failed to support an effort in Congress to provide a five-year, $1000 million package of loans and other measures to tempt the mining firms into action.
Proposals for a Defense Industrial Base Revitalisation Act, which would also have provided cash for retooling over a wide range of American industry, had widespread support from Democrats and Republicans.
But hampered by the lack of enthusiasm from the White House, the measure failed to complete its legislative programme.
Other nations have tried to open up their land areas.
In 1975 France started a detailed evaluation of the minerals it could obtain from its own soil.
The Bureau de Recherches Geologiques et Minieres, the state-owned minerals agency, spends about £5 million per year on prospecting and has found deposits of copper, tungsten, lead and zinc.
Japan also encourages domestic mining with government grants and loans that in the late 1970s totalled some £5 million annually.
Results in Britain from a similar scheme, started in 1972, have been disappointing.
Of £50 million that the government then made available to encourage mining companies, only about £10 million had been taken up by the middle of last year (see above).
Another technique to reduce the risks from a cartel is to find new overseas suppliers of key minerals.
The deposits may not be economic to mine now, though they could be either if the world recession lifts and demand grows, or if supplies from elsewhere are cut short.
Thus mining companies in the West are eyeing Morocco and Australia as possible sources of cobalt.
Bulgaria, Switzerland and northern Greece could, one day, be pockmarked by chromium mines.
Prospectors have discovered titanium in many of the world's tropical areas, for instance central America, the east coast of Africa, and Brazil.
And some time next century, metals companies will probably start mining the bottom of the Pacific for potato-sized nodules containing manganese, copper and cobalt.
(That is assuming that the United Nations sorts out the regulatory problems.)
Then there are the technological routes out of the materials bottleneck.
Substitution of new materials for existing ones, increased recycling or simply reduced demand for materials due to technical innovations could all make a nonsense of predictions that the world may run short of key metals.
Over the past few decades, the world's demand for tin has fallen dramatically as packagers have reduced the thickness of the metal in tinplate or have turned to other materials for their containers, aluminium for instance.
Ceramic materials may take over the role of cobalt in magnets, a move stimulated by the political upheavals in Zaire in 1978, which forced up the price of the metal and made many manufacturers look for substitutes.
In telecommunications, optical fibres made of glass will gradually replace the traditional copper cables in new equipment.
Engineers could also reduce their dependence on strategically important metals by redesigning components.
At NASA's Lewis Research Center in Ohio, for example, workers are trying to substitute small quantities of nickel for the cobalt used in alloys in jet engines.
This would be a step forward as the US has its own supplies of nickel and the metal is relatively common elsewhere.
Another way to reduce the consumption of strategic metals is to design components to better tolerances, perhaps with computerised techniques.
For instance, in a high-strength steel section in an engine, only those parts exposed to the greatest stresses would require sophisticated alloys containing a metal such as tantalum or vanadium, with the remaining components made of something less exotic.
Governments around the western world are looking to some of these technical ideas to provide a form of long-term insurance against problems with metals supplies.
Thus last year, the council of ministers of the EEC agreed to a £20 million research programme in materials, parts of which should boost recycling techniques and produce substitutes for the strategically important metals.
Over the next decade, several factors could complicate the metals scene still further, making it even more difficult for governments to weigh up their approach to issues such as stockpiling.
One factor is that China could become an important supplier of metals to the West.
China is already a big exporter of tungsten but it has major reserves of titanium, tin, copper.
lead, zinc, manganese and platinum.
The country could provide a third key area for metals production to add to southern Africa and the Soviet Union.
This would ease worries in the West about supply shortages.
A related development concerns a class of metals that hitherto few have considered ‘strategic’, but which could become increasingly important.
The metals are the ‘rare earths’, such as yttrium, lanthanum, neodymium and europium.
According to estimates, China's reserves of the rare earths amount to 100 million tonnes, about 70 per cent of the world total.
At present the country's exports of these metals are small but the volume could be stepped up as uses for the materials grow.
The metals are already important in colour TVs, fluorescent tubes, laser devices and ceramics.
Samarium (alloyed with cobalt) produces very powerful magnets, used in lightweight headphones for example.
And lanthanum could become extremely valuable if the world decides to turn to hydrogen as a novel fuel; a mixture of lanthanum and nickel absorbs hydrogen safely and easily and so could become an important storage mechanism for the gas.
In another change, in the past couple of years wealthy investors have realised that relatively exotic metals, such as cobalt, chromium and manganese, represent a very good hedge against inflation.
Over the next decade, the price of the metals will go only one way — up.
A network of brokers in London and New York do business in these metals.
In the past six months one London dealer says he has bought $2.5 million worth of the metals for a single client.
Another dealer said that to do business worth $1 million in a week is not uncommon.
Exactly who the purchasers are is often not known, not even by the broker.
Requests for the metal are made in telexes in the names of banks or solicitors acting for the buyer.
Most of the metals bought and sold in this manner never find their way to their purchasers.
Instead, the metals reside in warehouses in Rotterdam, a piece of paper bearing his name being the investor's only proof of ownership.
The companies that administer the warehouses keep details of their contents a closely guarded secret.
Nonetheless, it may be sobering for the European governments considering stockpiles that the private sector has already beaten them to it — and that the stores of the materials themselves are tantalisingly close to the industries that, some day, may be desperate for supplies.
Satellite data provide clues to deposits
MODERN technology has given geologists a variety of methods with which to find new deposits of metals.
In less sophisticated times, people searching for minerals picked up clues simply by walking over the ground and examining it visually.
Rocks stained a particular colour, sudden changes in vegetation, material thrown up by borrowing animals — all these could provide evidence of a mineral deposit.
Nowadays, the exploration work often starts with images of the ground obtained by an orbiting satellite such as Landsat.
With computer techniques, a researcher can analyse the image to look for changes in geology.
Aerial surveys can achieve the same thing, although the maps so obtained cover a smaller area.
From aeroplanes, too, researchers can deploy instruments that measure the Earth's magnetic field.
Fluctuations can point to ore deposits.
In a similar way, electromagnetic probes can help in the search.
For more detailed work, geologists rely either on geochemical surveys or drilling.
The surveys normally use samples of soil or water from streams.
Chemical analysis can indicate elements that may be present deep in the ground near the stream's source.
Drilling can take several forms.
In percussive drilling, the least sophisticated method, a bit cuts a hole through the rock and compressed air blows out chippings.
This technique will investigate no further than about 50 m under the ground; and there is no way of knowing from exactly which point in the hole the samples emanated.
Diamond drilling cuts a solid core from rocks as much as 2 km beneath the surface.
The core, which may be as much as 25 cm in diameter, can later be analysed chemically.
Britain in the pits over strategic metals
A HOLE in the ground that would engulf St Paul's Cathedral 30 times over is at the centre of a row over Britain's application to join the minor league of strategic-metals producers.
The hole would appear over the next 20 years at Hemerdon, just outside Plymouth — the scene of western Europe's biggest deposit of tungsten.
Hemerdon contains about 40 000 tonnes of the metal, enough to satisfy the UK's needs for 25 years.
At present, Britain imports virtually all its tungsten at a cost between £10 million and £20 million per year.
The hole, or rather the volume of waste material that is dug from it, has greatly bothered conservationists.
They point out that the proposed mine is just on the fringe of Dartmoor, an area of outstanding natural beauty..
According to the consortium proposing the mine, digging an open pit is the only way to get at the tungsten, which is  embedded as a wolframite (iron manganese tungstenate) in a deposit of granite.
The waste from the pit would be dumped on one side of a hill a short distance away.
Despite extensive plans by the consortium for landscaping, the conservationists claim the dump would turn into an eyesore.
The wolframite is scattered extremely thinly throughout the granite.
Thus to obtain 1 kg of metal more than 2 tonnes of rock must be mined.
This includes both the granite and the ‘country rock’ or killas, which surrounds the deposit.
That means that, over the life of the project, some 100 million tonnes of rock must be dug, the great bulk of it waste material.
The consortium behind the project comprises the American mining firm AMAX and Hemerdon Mining and Smelting, a company registered in Bermuda.
The issues surrounding the project were explored at a public inquiry in Plymouth last autumn.
Tom King, the Environment Secretary, will announce whether the plan will go ahead within the next few months.
If the scheme receives the green light.
Billiton, the minerals exploration subsidiary of Shell, may join the venture.
The firm has an option on purchasing the share in the project of Hernerdon Mining and Smelting.
AMAX has pointed out that the mine could create 350 new jobs.
The whole project would cost £50 million, of which some £6 million has been spent.
The partners have built a processing plant at the site to separate the tungsten ore and to recover a small amount of tin as a byproduct.
Tin is the only member of the metals loosely bracketed in the ‘strategic’ category in which Britain is even approaching self sufficiency.
The nation obtains something like a quarter of tin from four mines in Cornwall, historically an important tin producing area.
Britain is geologically interesting because it contains many different rock formations containing a large range of metals.
But the minerals themselves are rarely concentrated enough to support mining projects.
This is particularly so after deposits near to the surface have been worked out.
Last century, Britain was the leading producer ‘of tin, copper, lead and iron.
At one time there were 400 tin mines in Cornwall alone.
In 1972, the government tried to promote new mining ventures with the Mineral Exploration and investment Grants Act.
This finances exploration work to the tune of 35 per cent of the total cost.
But companies have been slow to take up the money.
Of the £50 million that the act set aside, by the middle of last year the government had allocated only about £10 million.
Pressure by environmentalists is just one of the problems facing mining companies.
Another difficulty is that in Britain mining rights for minerals are held by individuals; not, as in many countries, the state.
Whoever owns a piece of land may not necessarily own the mineral rights.
The landowner may have sold these rights centuries ago to someone else.
There is no record of who owns what in the area of mining rights; thus a mining company may have to spend years discovering to whom it has to pay royalties even after it has ascertained that a piece of Britain contains valuable metals.
Another difficulty is that the geological maps of Britain are out of date.
The Institute of Geological Sciences is slowly improving matters by analysing the country's rocks with modern geochemical methods.
The work started with the mapping of Scotland and has now reached northern England.
At the current rate, the job may not be finished until the 1990s.
Britain does not have to rely solely on its own mines for some kinds of metals.
It can import ores and then use metallurgical knowhow to obtain the finished metal.
In this way, Britain is an important refiner of platinum, which it buys from mines in South Africa.
In another metals area, Britain is gambling to become western Europe's only supplier of titanium, at a time when the market for the lightweight material is already oversupplied.
But the management of Deeside Titanium, which commissioned the world's most modern titanium plant at Deeside, north Wales, last November, feels that the risk is justified.
The £25 million plant is a joint venture between Billiton, Rolls-Royce and IMI.
It should produce 5000 tonnes of the metal per year.
Rolls-Royce needs the metal for its own products — the fans in the company's engine for the Boeing 757.
The process uses a lot of energy.
In it, sodium and titanium tetrachloride are mixed in a steel vessel at 1000°C, producing salt and titanium.
Small explosive charges break up the mixture, which is then washed and centrifuged to leave pure titanium in the form of granules.
Science students go back to school
Ros Herman
Undergraduates at Imperial College, London, guide and stimulate local pupils by acting as tutors to small groups within classes
JASON, in spite of his tendency to go to the loo in the middle of lessons and to complain of ‘guts ache’ after eating five KitKats, is a neat operator.
It takes him less than 30 seconds to assemble an asbestos mat, a bunsen burner, a wire mat on the top of a tripod and a beaker full of water Full of bright blue supersaturated copper-sulphate solution.
Bored as he waits for the liquid to boil, eyed admiringly by his pint-sized partner, he plays with wooden spills, dipping them into the solution and passing them through the flame beneath.
‘Do you know why part of the flame goes green?’ asks Tony, who has been hovering nearby.
‘Yes’, says Jason, ‘it's from the copper’.
Tony explains for the wide-eyed accomplice that the green colour shows the presence of copper in the flame, and because the colour doesn't appear with an undipped spill, the substance must be in the liquid.
Tony Raynham is not a science teacher but a 3rd-year chemistry undergraduate at Imperial College in London's fashionable South Kensington.
Every Wednesday afternoon for the past 15 weeks Tony has ‘tutored’ 1st year pupils at nearby (and almost equally fashionable) Holland Park Comprehensive.
The idea is for a number of undergraduates in science and engineering (about 80 this year) to assist teachers by working with small groups of kids in the classroom, usually helping with some form of practical work, answering their questions and guiding them, if necessary, through the experiments.
To take part, students have to join a society called the ‘Pimlico Connection’(after one of the schools in the scheme), and go through a day of ‘orientation’ to the kind of situations they will face in the classrooms.
The following week they go off to their respective schools, to meet the teachers and the group of children they will be working with.
The scheme was the brainchild of Dr Sinclair Goodlad, who runs a course on the ‘communication of scientific ideas’ in the humanities department at Imperial College.
The course is only loosely related to the Pimlico Connection scheme, but 15 students each year choose tutoring in schools as their fieldwork for the course.
Most of the student tutors however are undergraduates of science or engineering.
What, then, makes them want to do it?
Some are interested in becoming teachers, and use the scheme to help them decide whether teaching is right for them.
Sandra Cairns, for example, a shy girl with a grammar school background, has had her eyes opened to the realities of teaching groups of mixed ability and dealing with difficult situations.
Nevertheless, she still wants to be a teacher, and has begun to develop more social confidence through her work as a tutor.
Vera Shroff, a mathematics student, found that tutoring brought back the fun of a subject she was having some teething troubles in coping with at university level.
And it brought home to her the fact that some children cannot cope with maths, however hard they try — a realisation she may well be glad of when she fulfils her ambitions to become a fully-fledged teacher.
Jonathan Wheeler, an engineering undergraduate, has no intention of becoming a teacher.
Sponsored by Lucas Industries, he plans to become a practising electrical  engineer , though he might do casual teaching, perhaps at evening classes.
He went into the scheme because he thought it would be fun.
And he says he has acquired some skill in communicating technical ideas that he uses in presentations for Lucas.
This year's chairman of the Pimlico Connection, Giancarlo Marcheselli, was curious to know what teaching would be like, and thought it would be a way of doing something useful to complement the years as a student ‘where everything you do is for yourself’.
When the scheme began, eight years ago, Goodlad was afraid that there would be all sorts of objections from teachers and teaching unions.
But his first probes to local schools met with an enthusiastic response.
As Ruth Aplin, teacher of a class in the final year at Fox Primary, one of the feeder schools to Holland Park, put it, the scheme makes it possible for a class to tackle far more ambitious and varied projects.
Aplin's class was living proof of this.
Three groups of 10-to-11 year olds sat at tables round the room, heads bent over balsawood boats, a paper merry-go-round, and magnetised bits of metal.
A rather taller head towered above the others at each table, usually bent in equal concentration.
Voices raised with excitement came from a fourth group, clustered round the sink in the corner of the room.
One little boy stood poised with a transparent plastic hose over a bucket, whose sides supported a rod that formed the axle of a paddle wheel.
A string led from the axle, via a pulley held by another patient pupil, to a dangling weight.
Supervised by a competent and occasionally firm John Hughes, the group ironed out some initial hitches, and finally the water flowed, the wheel turned, and the weight rose.
While the keen (but slightly damp) experimenters went on to test different designs of paddle wheel, Ruth Aplin showed me some of the pupil's science folders.
Many of the ‘tutored’ activities had involved ‘tasks’that the children had to work out how to do.
These included measuring the volume of a hand or the ‘stretch’ of an elastic band.
Not all the teachers look on tutoring as an unmixed blessing.
Ash Sethi, Jason's teacher at Holland Park, has had some  unpleasant experiences, though last year's tutoring group was extremely good, he says.
The students seem reluctant to stop after the 15-week session (running from October till early February, after which exams and job interviews must take first place with students).
One even insisted on continuing for longer.
Holland Park and Fox are not the only schools served by the Pimlico Connection.
Students from Imperial College help out with Nuffield-based general science at Stockwell Manor School and Sir Walter St John School and Pimlico School as well as Holland Park.
The scheme also offers engineering (metal and woodworking) at the first two, in addition to electronics and maths at the last.
As the only permanent fixture in a constantly changing group, Sinclair Goodlad maintains continuity and lays down the scheme's philosophy.
The prime aim is to help the pupils.
In Goodlad's own words, ‘In large inner city schools, many of whose pupils are immigrants, and where mixed-ability ‘teaching is used, able pupils value the extra stimulus tutoring can offer; less able pupils often need help in order to survive.’
Tutors can give the pupils more individual attention, and encourage them to ask questions they might be reluctant to ask the teacher for fear of being thought stupid.
The personal relationships built up over the 15 weeks often encourage longer discussions on how the subjects the pupils are learning about relate to the outside ‘grown-up’ world of industry and academe.
Pupils can come to identify with the students — sometimes the age difference is not more than a few years — and ask how they can emulate them.
This may be particularly important for girls with male science teachers.
Female science students may make the girls think again about their decision not to continue with science.
Some learn in this way for the first time about what goes on inside a university.
Sometimes the pupils can indulge their curiosity further, on tours, organised by the students, round the laboratories at Imperial College.
Mixed reactions temper overall success
Goodlad has been careful to see that the reactions of all participants — pupils, students and teachers — are recorded to measure the success of the scheme.
Comment from all parties is, on balance, favourable, but by no means free from individual criticisms.
Some pupils find it off-putting to have someone looking over their shoulder the whole time; others would like to see fewer tutors per class.
Some teachers have found it difficult to integrate the tutors into the way they teach, and as a result the close relationship with the pupils has failed to materialise.
But on balance, the children think they understand the work better, and so do the teachers.
Tutoring ideas could be used much more widely as a way of helping both older and younger students, for example with older pupils helping younger ones in their own schools.
As Goodlad points out, tutoring is not new.
Invented in the late 18th century, it made possible large schools with a wide range of age and ability, that could be run by a small complement of teachers.
Developing countries and large urban areas in America use the technique now for roughly the same reason.
And research, Goodlad says, has shown that both parties benefit.
It is too early to pinpoint any long-term effects of the Pimlico Connection.
However, there are plans afoot to see if tutoring in primary schools has any effect on performance later on.
Goodlad's ideas have received some outside encouragement: the Royal Society of Arts gave his course on communication of scientific ideas an Education for Capability award for providing an effective combination of academic reflection and the learning of practical skills in communication of ideas.
As I walked out of another final-year class at fox Primary, the pupils were building complex models out of balsawood or working on the theories of gearing with big pieces of plastic Meccano.
Approaching the tube station at Notting Hill Gate, I saw a number 88 bus go by with a bright yellow advertisement proclaiming ‘Britain needs its universities’.
Perhaps people would get the message if the Pimlico Connection had more imitators up and down the country.
MONITOR
Polarisation brings a new twist to  fusion research
RESEARCHERS at Princeton University are now working on an experiment that will, later this year, try out a way of polarising atoms of some of the isotopes of hydrogen.
If the scientists succeed, they will have taken a small step toward improving the efficiency of nuclear fusion devices.
They will also have developed a new technique for controlling reactions within a fusion reactor.
The work on producing the polarised atoms is being carried out by William Happer in the physics department at Princeton.
So far Happer has concentrated on producing polarised xenon atoms.
The nuclei of certain atoms resemble tops in that they have an intrinsic angular momentum — or spin, such atoms are said to be polarised when they are all spinning in the same direction.
To polarise the xenon atoms, Happer and his colleagues T. McLelland of Case Western Reserve University, and N. D. Bhaskar, first mix some rubidium atoms with xenon gas.
Because the atoms have spin they behave like tiny magnets and the two species of atoms are attracted to each other to form so-called van der Waals molecules.
The rubidium atoms become polarised when they absorb light at a specific wavelength from a dye laser, and then swap spins with their xenon partners before the short-lived molecules dissociate.
The end result is polarised xenon.
The xenon atoms remain polarised for about half an hour.
They gradually depolarise in collisions with the walls of the container that holds the xenon: the collisions allow the nuclei of the xenon atoms to change their spin states.
Polarised atoms of xenon are useful because they can provide the reference direction for very accurate gyroscopes.
Polarised hydrogen, on the other hand.
which may be easier to make than polarised xenon, could help fusion research.
In fusion reactions, light atoms are brought together sufficiently energetically to overcome their natural repulsion.
At the energies required for fusion the atoms are fully ionised, creating what is known as a ‘plasma’.
The light ions ‘fuse’ to yield larger ions, releasing energy in the process.
Today most fusion research aims to create the conditions under which deuterium (D) and tritium (T)— two isotopes of hydrogen — can fuse to yield a neutron and an ion of helium-4.
Both particles created in a single fusion reaction carry energy that can be turned into heat and, ultimately, into electricity.
Russell Kulsrud of the Princeton Plasma Physics Laboratory (PPPL) believes that by polarising the deuterium atoms before feeding them into a fusion device, it may be possible to accelerate the rate at which D-T reactions occur.
Kulsrud has calculated that it might be possible to increase the reaction rate by as much as 50 per cent.
Scientists have known for some time that fusion reactions between nuclei of deuterium and tritium are more likely if the nuclear spins line up in the same direction.
Until recently, however, fusion researchers thought that because so little energy is needed to polarise — and therefore to depolarise — a deuteron it would be pointless to think about polarising the fuel in a fusion reactor.
The belief was that the energetic motion of the ions and electrons in the ionised gas, the plasma, would soon destroy any polarisation.
Kulsrud now believes that ‘the mechanisms for depolarisation of nuclei in a magnetic fusion reactor are surprisingly weak’.
According to his calculations, the spin polarisation lasts longer than the particle is likely to.
In other words, a deuterium ion is likely to take part in a fusion reaction long before its polarisation is destroyed (Physical Review Letters , vol 49, p 1248).
The reason why fusion is more likely between spin polarised nuclei is straightforward.
Deuterium nuclei (deuterons) have a spin that will, in a magnetic field, line up either with the magnetic field or perpendicular to it.
D-T fusion reactions take place predominantly between particles with spins parallel to the magnetic field.
(This is because of the spin characteristic of the intermediate nucleus, helium-5, produced fleetingly during the fusion reaction.)
There are three polarisation states for deuterium nuclei, so only a third of the atoms in unpolarised deuterium will line up perpendicular to the magnetic fields.
(Some nuclei that are not aligned with the field also take part in fusion reactions, although at a slower rate.)
By lining up all the deuterons with the magnetic field, it is, in theory, possible to increase the fusion rate by about 50 per cent.
The influence of spin polarisation on fusion does not end with the DT reaction.
This may well be the first fusion process used in a reactor, but there can also be fusion between deuterons alone or between deuterons and nuclei of helium-3.
Spin polarisation can also affect these reactions.
There is some uncertainty about the extent to which aligning the deuterons' spin could improve the D-D reaction rate — it could be between 1–5 to 2–5 times higher with polarised deuterons.
But perhaps a more interesting effect — and the one that might offer scientists the first chance to test Kulsrud's work experimentally — is the ability to suppress D-D reactions by polarising the deuterons so that all the ions line up parallel to the magnetic field.
The ability to suppress D-D reactions has two implications.
One possible fusion fuel cycle involves D-He 3 reactions.
Unlike D-T reactions, D-He 3 fusion does not produce energetic neutrons and a helium-4 nucleus, instead it would yield a helium-4 nucleus and a proton.
Not only does this offer a fuel cycle that gives energetic charged particles — raising the hope of generating electricity without going through the heat/steam-raising cycle — but also it does away with the fast neutrons that can damage the materials of a fusion reactor and at the same time produce a radiation hazard for workers.
But there would be some D-D reactions in a reactor filled with deuterium and helium-3, which do produce neutrons.
Thus one benefit of the D-He 3 fuel cycle would be lost unless D-D reactions are suppressed — and this could be done by polarising the neutrons.
The second implication of the ability to reduce dramatically the D-D process is in testing the influence of spin polarisation on fusion reaction rates.
Polarisation may also vary the way in which fusion takes place.
For example, the alignment of the nuclear spin relative to the magnetic field in a fusion device influences the direction in which the products of fusion emerge from the plasma.
When the deuterons are polarised in a D-T reaction the fusion products — helium-4 nuclei (alpha particles) and neutrons — come out of the plasma perpendicular to the magnetic field.
In the fusion device known as a tokamak the field runs around the doughnut-shape (toroidal) container.
This means that neutrons produced in polarised-deuterium reactions would travel perpendicularly through the walls of the reactor, and so follow the shortest possible path through the material of the wall and create less radiation damage.
On the other hand, polarising the deuterons differently can force the plasma into pushing out alpha particles parallel to the  magnetic field.
This would have no impact on the fusion rate, but a tokamak can contain alpha particles more easily if they start their life travelling in the same direction as the magnetic field that contains the plasma.
Thus spin polarisation can influence the ability of a fusion device to contain a fuel.
As fusion research today is all about containing the fuel, any benefit here would be more than welcome.
Acid flux triggers the Venus Fly-trap
RAPID leaf movements are usually put down to changes in turgor pressure.
But new research by Alan Bennett of Cornell University and Stephen Williams of Lebanon Valley College in Pennsylvania suggests that leaf closure in the Venus fly-trap (Dionaea muscipula is due to rapid growth triggered by a drop in the pH of the cell walls of the leaf (Science .
vol 218, p1120.
Studies on plant movement have centred around plants such as Mimosa pudica (the sensitive plant).
Movement in such species is known to be due to turgor changes in the specialised hinge-like regions of the leaf, known as pulvini.
But the Venus fly-trap and its relatives, the sundews (all Droseracea) do not possess pulvini so movement must be caused by some other mechanism.
Rapid growth was thought to be a likely candidate so the researchers measured the expansion of cells in the trap leaves by marking leaves with evenly spaced ink dots.
During closure, they found, the outer surface of the leaf expands; but during the 10 hours of reopening, the inner surface slowly expands.
Irreversible cell expansion — very rapid growth — caused the movement, not turgor change.
In fact the closed traps were less turgid than open ones — the opposite of what would be expected if changes in turgor pressure springs the trap.
Many plant cells rapidly expand when their cell walls are  acidified ; acid activates enzymes that increase the flexibility of cell walls.
The researchers suspected that such ‘acid growth’ could be at work in the fly-trap.
To test the idea, they infiltrated the trap leaves with buffers of varying p H.
Leaves closed best at pH 3–4.
As p H rose, leaves closed more slowly and by p H 5 the traps were virtually paralysed.
The trap has to be quite acid before it can close.
Bennett and Williams suggest that the cells of opening or closing traps acidify their own walls by releasing hydrogen ions.
The cells would have to expend energy, in the form of adenosine triphosphate (ATP), in order to pump hydrogen ions out of the cell.
So the biologists measured the levels of ATP in traps, by adding cells to an extract of the bioluminescence system of the firefly.
The number of scintillations produced is directly proportional to the number of ATP molecules present.
They found that moving traps were indeed consuming energy — cells used up 29 per cent of their ATP in the three seconds of closure, Venus fly-traps clamp down on their prey as a result of irreversible cell expansion, which is itself caused by the activation of wall loosening enzymes by a lowered p H in the cell wall.
This is produced by active release of hydrogen ions.
Acid growth, not turgor pressure, moves the Venus fly-trap.
Laser-light: the shortest wavelengths yet
A LASER-BASED technique that can produce extreme ultraviolet radiation hits been developed independently by two American research groups.
Studies of such wavelengths, shorter than about 104  nanometres , have been difficult, but they are important to chemists because of the actions they can have on atoms and molecules.
One of the groups which developed the technique is using it to investigate chemistry; the other is claiming a record for generating the shortest wavelength of coherent (laser-like) light.
The extreme ultraviolet is part of what physicists call the vacuum ultraviolet, wavelengths shorter than about 200  nanometres which are absorbed by air so strongly that experiments have to be performed in a vacuum.
At wavelengths shorter than about 104  nanometres , there are no materials suitable for use as windows to transmit the radiation into the vacuum chamber.
The most practical source of such radiation has been synchrotron radiation, which is of limited spectral purity and is available only at a few facilities.
The new technique makes use of the non-linear interaction of intense light with matter, a phenomenon first discovered about two decades ago.
Such interactions can multiply the frequency of the light by an integer, or equivalently divide the wavelength by the same number.
The interaction of intense laser light with certain gases can produce extreme ultraviolet radiation, but the windows needed to keep the gas contained would absorb it, some ingenious ways were devised to amid windows, but they generally were too cumbersome for practical use.
The American researchers got around the problem by synchronising a pulsed laser and a pulsed gas jet so the pulse of laser light hit the gas just as it emerged from the jet into the vacuum chamber.
This avoids the need for the extreme ultraviolet radiation generated by the laser pulse to pass through a window (the gas from the jet is quickly pumped out of the vacuum chamber).
The first public report of the idea came from Andrew H. Kung, managing scientist of the San Francisco Laser Center at the University of California at Berkeley.
After demonstrating it himself, he teamed with Richard N. Zare, a Stanford University chemist, and two postdoctoral fellows, Ernesto E. Marinero and Charles T. Rettner.
Working with Kung, they used a wavelength-tunable dye laser to  illuminate jets of argon, generating a third harmonic output tunable continuously between 97.3 and 102–3  nanometres .
The extreme ultraviolet power was only a few watts, but it was adequate to detect molecular hydrogen.
Zare is delighted to have a simple and comparatively inexpensive extreme ultraviolet source he can use in his laboratory; both the laser and the pulsed value are standard commercial products.
His only complaint is that the technique is so simple that other researchers will be able to duplicate it quickly enough to keep up with his group.
The technique was developed independently by Jeffrey Bokor, P. H. Bucksbaum, and R. R. Freeman at Bell Telephone Laboratories in New Jersey.
They used a different laser that produces shorter, more powerful pulses at a shorter wavelength, which let them generate third, fifth and seventh harmonics.
The power produced drops off as the harmonic number increases, so to generate the higher harmonics requires much higher input intensity.
The 35–5-  nanometre seventh harmonic they  observed breaks the previous record of 38  nanometres for shortest wavelength coherent light set in 1977 by a team at the Naval Research Laboratory in Washington using another harmonic-generation technique.
Strictly speaking the extreme ultraviolet light does not come from a laser, but it does retain the coherence of the original.
Although laser gain has been observed at a shorter wavelength — 18–2  nanometres by Geoff Pert's group at the University of Hull — no claims have been made that those experiments produced coherent light; the emission included radiation generated by other processes.
No details on coherence of the supersecret X-ray laser demonstrated two years ago at the Lawrence Livermore National Laboratory have been reported in public, but it seems that that device probably would not have generated coherent output for similar reasons
Cancer threat lurks in normal cells
THE MUTANT gene that late last year was incriminated by American scientists in human bladder cancer (New Scientist .
vol 96. p 41 8) has now turned up in the normal as well as the tumour tissues of a bladder cancer patient.
Ruth Muschel and her colleagues at the US National Cancer Institute and at Yale, reporting this discovers in Science (vol 219, p 853), suggest that this may mean that some people have an inborn tendency to develop certain kinds of tumour.
The original discovery of the cancer gene was made by fragmenting the DNA of human bladder tumour cells and feeding the fragments to cultured mouse cells.
Some of the mouse cells became cancerous in consequence, and the human DNA retrieved from them turned out to contain a mutant version of a normal cellular gene.
Since the normal, non-mutant gene does not cause the cultured mouse cells to become cancerous, the cancer-causing properties of the bladder tumour gene were naturally attributed to the mutation.
But the work of Muschel and her collaborators, George Khoury, Richard Koller and Ravi Dhar at the National Cancer Institute and Paul Lebowitz at Yale, now makes it look as though the mutation may not have occurred in the tumour cells alone — instead, the mutant gene may be a rare variant that is actually inherited by some people.
Muschel and her co-workers analysed DNA from 14 normal people who did not have cancer, and from the tumour cells, the normal bladder tissue, and some blood cells of a patient with a bladder tumour.
All 14 normal people had the non-mutant version of the gene, whereas the cancer patient had the mutant gene not only in his tumour cells, but in his normal bladder and blood cells as well.
So if the mutant gene causes cancer, why had the patient not developed cancer until relatively late in life, and then only in the bladder?
The answer is that it takes, fortunately for us, more than one mutant gene to make a normal cell into a tumour cell.
The cultured mouse cells that are the universal test for cancer-causing genes work only because they are very far from normal.
Most experts think normal cells probably have to undergo several mutations in order to become cancerous; and it is generally suspected that the mouse cells — a laboratory line known as NIH 3T3 cells — have already undergone numerous mutations and are hovering on the brink of becoming cancerous.
So without doubt one of the most important and difficult problems in cancer research is that of identifying the other mutations.
But in the meantime, some relatively easy questions arise from Muschel's discovery.
For example, how many members of the cancer patient's family have the mutant gene, and do those who have it show a greater tendency to develop cancer?
There is no reason to expect that they will necessarily have developed cancer of the bladder: the same gene seems to be associated with tumours of the lung, gut, and other tissues (Nature vol 300, p 539).
Probably, the presence of a mutant gene just brings an individual one step closer to the possibility of cancer.
Whether he develops it, and in what tissue, must depend on other factors — such as exposure to chemical carcinogens, for example in food or cigarette smoke.
Nerves need vitamin E
SINCE vitamin E was first discovered in the rat in 1922 so many ill-founded claims for its role in human nutrition have been made that cynics have dubbed it ‘E for everything’.
Success in treating a rare inborn error of metabolism called abetalipoproteinaernia (ABL) has renewed interest in vitamin E, and suggests it is essential for normal nerve function.
Vitamin E is available from so many food sources that no normal diet could possibly be deficient in it.
And yet serious degeneration of nerves in young children with ABL can be halted by massive doses of the vitamin.
The results are dramatic — but how does the body become deficient in vitamin E, what is its role in human nutrition and why does its absence sabotage the nervous system?
Professor Otto Wolff and Dr David Muller at the Institute of Child Health and Professor June Lloyd at st George's Hospital Medical School believe they have some of the answers and have recently advanced their theories (Lancet , 29 January, p 225) by drawing together clues from a number of apparently unrelated diseases.
Children with ABL cannot absorb fat because they lack the lipoproteins in which fats are carried from the small intestine into the bloodstream.
Since vitamin E is fat soluble, this shortcoming in turn prevents it from entering the circulation, even though it may be present at normal levels in the gut.
Support for this thesis comes from the act that other diseases in which fat malabsorption occurs are also accompanied by vitamin E deficiency.
Conditions in which there is depletion of its, which are essential for emulsifying fats in the gut, are also characterised by low levels of vitamin E. Examples are people with liver disease, or who have had a large part of the ileum (’ lower’end of the small intestine) removed.
Professor Wolf and his colleagues point out that all these diseases may show symptoms of nerve degeneration which are similar to those found in ABL and can also be treated by supplements of vitamin E. For example, a striking reduction in axonal degeneration has been seen in patients treated with vitamin E, for pancreatic insufficiency associated with cystic fibrosis.
Although the role of vitamin E in nerve function is unknown, Diplock and Lucy from the Royal Free Hospital Medical school have suggested that it acts as an anti-oxidant and protects the membrane of the cell by mopping up reactive free radicals.
The chemical structure of the vitamin consists of a chroman nucleus substituted with a single hydroxyl group and a long-chain hydrocarbon.
It is thought that the side-chain  inter-reacts with poly-unsaturated fatty acids in the membrane, leaving the hydroxyl group in a position to scavenge free radicals and so maintain the integrity of the cell.
Why sex declines in marriage
THIRTY years ago Alfred Kinsey's famous report on the sexual behaviour of the human female provided statistical evidence for a decline in the frequency of sexual intercourse in marriage as time passed.
This well-known phenomenon was attributed to a decline in the male's sexual ability and motivation with age.
So traditional wisdom and scientific studies of human sexual behaviour agreed that sexual intercourse in marriage declined as the male lost interest and capacity.
A rather different view was then put forward by J. R. Udry, F. R. Deven, and S. J. Coleman of the University of North Carolina, and the Population and Family Study Centre in Brussels (Journal of Biosocial Science , vol 14, p 1).
They found that in the United states, Belgium, Thailand and Japan, the decline in the rate of sexual intercourse in marriage is more closely associated with the wife's age than that of the husband.
This seemed to be the case in all four countries in spite of their considerable cultural differences and distinctive marriage customs.
The most probable explanation, the researchers concluded, was the decline in female, rather than male, testosterone levels between the ages of 20 and 35.
Kinsey and almost everyone else had got it wrong.
The male was not losing interest or ability.
He was simply responding to changes in his wife's hormone levels.
Now W. H. James of the Medical Research Council's Mammalian Development Unit at University College, London, casts doubt on Udry's theory (Journal of Biosocial Science .
vol 15, p 83).
He shows that the decline in the frequency of sexual intercourse in marriage is not simply linear with age or the duration of the marriage.
The frequency of intercourse approximately halves in the first year of marriage but takes about 20 years to halve again.
After an initial burst of enthusiasm the male slowly loses sexual ability and interest over the years.
So perhaps Kinsey was more or less right after all.
TECHNOLOGY
Photons smash cancer cells
SURGEONS are testing an extraordinary new way of destroying cancer tissue with lasers.
The treatment, called photoradiation  therapy , solves some of the problems with using conventional lasers to cut out tumours, and is more accurate than radiation therapy.
It could also be a useful was of spotting the disease early — when chances of containing it are best.
Surgeons in the US, Europe and Japan have been removing tumours with powerful carbon dioxide lasers since the early 1970s.
A laser ‘knife’ has many advantages over the scalpel — it can make a much finer cut, and causes less bleeding.
There is also less risk of infection.
But lasers have snags: even with optical fibres, lasers cannot reach every part of the body.
And, like conventional scalpels, the beams will cut through tissue whether it is healthy or diseased.
The conventional treatment for large tumours, deep within the body, is to bombard them with powerful doses of gamma radiation.
This will kill of the cancer cells but it will also kill off anything else in the vicinity.
And radiation, like many of the drug treatments used for cancer, can cause side-effects that are almost as unpleasant as the disease itself.
But a remarkable chemical, called hematoporphyrin derivative (or HPD), has enabled scientists to devise a weapon which combines the power and penetration of radiation therapy with the pin-point precision of the surgical laser.
The chemical comes from degraded  haemoglobin in cows' blood.
When it is injected into the human body, it disperses throughout the living tissue, until it is removed by normal functions.
However, it remains in tumour cells significantly longer than in healthy cells.
So if doctors find concentrations of HPD a few days after the injection, they know there are tumours there.
They find the concentrations by exploiting HPD's second useful property — its sensitivity to light.
When illuminated with a violet light from a laser, the tissue containing HPD glows red.
As long as doctors can see the glow, they can find tumours anywhere in the body.
To destroy the tumours, they bring into play HPD's third useful property.
When it is bombarded with red light, it undergoes a photochemical reaction.
The reaction produces oxygen radicals (single oxygen atoms) which react instantly with any tissue within reach, killing it.
The whole operation sounds too neat to be true — but it seems to be working at least as well as other last-ditch cancer treatments.
It was pioneered by Dr Thomas Dougherty, of the Roswell Park Memorial Institute in New York.
Dougherty has been doing the basic research and clinical studies for the past five years.
He has claimed success with every kind of tumour that he has treated — including lung cancer at a late stage.
However, there are still practical  difficulties with observing the fluorescence in the chemical, and in getting the laser light to the right spot.
More importantly, doctors have doubts about the safety of HPD.
For obvious reasons, people receiving injections of the drug must stay in the dark for a few days, and keep out of direct sunlight for a month, or longer.
In the United States, doctors have to obtain a special ‘investigative drug licence’ from the government's Food and Drug Administration before injecting a patient with HPD.
Despite the doubts, doctors are giving photoradiation therapy a try.
The laser company Coherent, which makes the equipment, says at least eight hospitals in the US are using photoradiation.
And doctors in Canada, Australia, Italy, Holland, Norway and Japan are also carrying out research.
At least one British surgeon, John Carruth of Southampton, is interested.
The world's experts meet in Milan in May to report their findings.
The meeting's proceedings could determine whether photoradiation therapy becomes a part of every cancer hospital — or whether it joins the ever-mounting pile of discarded ‘miracle cures’.
Universal appeal of satellite communications
JUNIOR education minister William Shelton did not know whether he was coming or going last week when he gave an experimental communications system known as Project Universe its first public airing.
Shelton's photograph was fed into the network and transmitted to a scanning device.
This reconstituted his image and then sent it back again to the exhibition at London's Barbican Centre.
The whole thing was accomplished with the help of a satellite.
Project Universe is a £5 million scheme involving the Orbital Test Satellite (OTS) which will eventually link some 150 computers attached to local area networks at seven research centres in Britain.
The idea is to test the hardware and software associated with high-speed communications and investigate the kind of services that could be supplied over such a network.
British Telecom is putting up some £1 million toward the project including three hours free time each day on OTS.
Dustbin-lid sized receiving dishes, of the type that will be used with BT's satstream service due to start next year, are at each of the sites taking part.
The network operates at 2 megabits per second, fast enough to send a copy of The Oxford Dictionary every minute.
The research teams, led by the Science and Education Research Council (SERC), have built into the network special routines which send a bit of error-checking data for each bit of original data.
The result is that the network only sends duff data once every twenty minutes on average.
Apart from computer data, Project Universe can also carry telephone conversations, slow-scan TV pictures and facsimile transmissions.
The system has enough bandwidth to carry a low quality, moving black-and-white TV picture.
The project is already over a year into its three-year life.
The network will study how to manage a large collection of computers, which are constantly being switched in and out of the Universe network.
One research team keeps a record of which computers are attached to the network at any time.
The network also brings together separate groups of people working on different aspects of the same software project.
Although a network based on landlines would be more economic for most of the work, project coordinator Peter Linington of SERC pointed out that Project Universe will act as an important test bed for satellite products and services.
‘Many of these will have export potential,’ he said.
Project Universe is due to end in 1985, but there are moves to have the whole scheme extended, perhaps to researchers who just want to use the network to do day-to-day work.
Nature's barriers are best
PLANTING trees along the side of the roads can shield people from traffic noise at a fraction of the cost of conventional techniques.
A team of scientists Margaret Williams and Keith Attenborough, at the Open University, and Jeanette Brooks of Bradford University, find that a dense, narrow belt of trees and shrubs can reduce sound level by up to 10 decibels.
This brings noise from the busiest roads to an acceptable level.
Tree belts are usually regarded as ineffective barriers to noise because they are superficially similar to a layer of porous material — they are full of holes through which sound is transmitted very efficiently.
Planners have estimated that belts of trees would have to be some 100 metres deep to reduce noise significantly.
Such estimates come from measurements in forests with little undergrowth, or single lines of trees.
But narrow belts of trees with shrubs are much better at absorbing sound.
A sound wave has a much greater chance of being scattered and absorbed by such dense vegetation.
It is not surprising that single lines of trees or hedges have proved ineffective — these simply do not provide enough material to scatter or absorb much sound.
Dense, narrow belts of trees, which David Cooke and David Van Haverbeke of the US Forest Service studied, reduce noise levels by 3 to 7 decibels: wider belts can dampen noise by up to 10 decibels.
Many purpose-built barriers are no better at reducing noise.
Screens some three metres high, for instance, reduce noise by 5 decibels, and a five-metre cutting by 6 decibels, says the British Road Federation.
Trees can make solid barriers even more effective.
The American researchers find that planners can improve the noise reduction of an earth mound (around 15 decibels) by up to 5 decibels by planting trees on or around it.
Existing tree belts are usually wind breaks, which are not encouraged to grow densely.
With proper management it should be possible to produce denser belts specifically designed to reduce noise.
The choice of species in tree plantings is also important.
Broad leaved species are probably more effective, but evergreens may give better protection in winter.
Research is under way at the Open University to find out which species of trees and shrubs and which management regimes are best.
Tree belts offer many benefits over conventional techniques of controlling noise.
Not the least of these is the low cost.
One estimate puts the cost of planting a 15-metre wide belt of trees on both sides of the road and initial maintenance at £15000/km.
Such a belt may reduce noise by 5 decibels.
An equally effective screen would cost at least £1 60000/km, according to the Department of Transport.
The same source puts the cost of building earth mounds along both sides of a road at between £80000 and £190 000/km.
Building the road in a cutting would cost up to £10–5 million per kilometre.
Tree belts are also more effective windbreaks, and provide more protection from the glare of low sun than mounds or fences.
They can also help purify the air, stabilise embankments with their roots, provide habitats for wildlife, and improve the aesthetic appeal of the roads.
ET's phone 
WITH a little help from ET, Texas Instruments is about to put its Magic Wand reading machine on the market.
The firm showed the Magic Wand at the Chicago Consumer Electronics Show last year.
The device, which costs around £40, uses bar codes to trigger chips that synthesise speech.
Books for the system, which will cost around £8 each, have conventional pictures and text alongside printed bar codes.
When the light pen is moved over the code, it triggers the chips to make sound through the loudspeaker.
In the film ET , the visitor from outer space learns to talk with the help of another Texas speaking toy — so the company is launching the Magic Wand with a book of words from ET's vocabulary.
Powders pave the way
MAKING objects such as gear wheels out of compressed, powdered metal rather than cut or molten metal saves both energy and materials.
It is also opening up a rare advance in the ancient art of making metal alloys.
A company in Wales, Mixalloy, is exploiting a process that the British Steel Corporation invented to mix metal powders with other powders to make all sorts of chemicals with new properties.
The process starts by mixing metal powder with a liquid containing a cellulose binder.
The mix forms a slurry which is rolled out and heated to remove water, then thickened into a solid strip.
The strip goes through a rolling mill and a furnace which burns off the cellulose binder, leaving a strip of metal alloy.
More rolling and heating finishes the alloy off.
Because the process does not involve handling molten metal, there is no contamination from ladles and tanks.
And the process gives a more even mix than conventional melting because the more dense metals do not sink.
Most important of all, the process does not involve the high temperatures of a conventional foundry.
So the company can make use of ingredients, including organic ones, that could not survive melting.
The result: a whole range of new alloys, tailored to meet new needs.
Because the process is extremely flexible, customers no longer need to buy a minimum of one tonne of alloy.
The company will supply a few hundred grams at a time.
Customers include makers of heating elements and bimetallic strips for thermostats.
Walking takes a load off Parisian feet
THE WORLD'S first accelerating pedestrian walkway will begin running in Paris next year.
It will be at the busy 1.75-metre interchange between the Paris metro and the railway at Les Invalides.
If, as trials suggest, the system is a success, the foot-weary pedestrian will breathe a sigh of relief.
The accelerating walkway has been a gleam in the eye of town and transport planners for some time.
Conventional moving walkways such as those at London's Heathrow Airport and in a number of Paris metro stations travel at less than normal walking pace.
Most travellers consider them irritatingly slow over distances of more than 150 metres.
But higher speeds than the normal three kilometres per hour pose problems getting on and off the moving walkway.
The new accelerating system that the French developed speeds up from three to 12 km/h without knocking passengers off their feet.
The system, called Trax, has already undergone 2000 hours of tests.
It is based on a design from the RATP Paris metro authority, but a private company, Ateliers et Chantiers de Bretagne, is making and selling it.
The company is a subsidiary of Alsthom Atlantique, which makes France's TGV high speed train.
Overlapping plates form the surface of the walkway.
Acceleration comes as the plates are made to overlap less, while deceleration is achieved by increasing the overlap.
A tongue-and-groove arrangement allows the plates to comb or mesh with each other, making a smooth surface for pedestrian's feet, suitcases or prams.
Guide units running under the surface on rails control the amount of overlap.
These quadrilateral guide units change their geometry at acceleration and deceleration points.
The elongation or contraction of one side of the guide unit is transmitted to the plates.
The system also manages to incorporate a handrail running at the same speed as the walkway — an advance compared with most single speed systems.
The whole thing contains 700 000 parts.
The designers have paid a lot of attention to safety, says Robert Chevillon, deputy general manager.
He says he is confident that the system will meet all rules that transport and public health authorities lay down — good news for passengers changing trains at Les Invalides next year.
In the final trials, a team of actors, under medical guidance, will try to simulate the behaviour of the aged and infirm.
The trials are also needed to gain acceptance from New York's Port Authority.
This recently selected Trax for close study as a future method of transporting passengers.
Trax costs about twice as much as a conventional walkway.
But once the costs of other engineering and construction work common to both types of system are included, the difference is much less significant.
Ateliers et Chantiers de Bretagne hopes Trax will not, however, be used only to ease the lot of weary car, rail and subway passengers.
Says Howard S. Goldberg, the Trax representative for the US: ‘It'll change urban geography.
By providing fast links between car parks, residential , office and shopping centres, real-estate planners will have a whole new range of options open to them.’
Goldberg sees Trax as suitable for up to 500 metres.
At this point, cars or bicycles come into their own.
Enter the robot librarian
STUDENTS at a Japanese university have ‘robots’ to fetch video tapes for them.
Seated in one of Kanazawa Industrial University library's viewing booths, they just pick the tape they want to see front the list displayed, punch in its number, then sit back and wait for the programme to start.
The average wait is usually about 40 seconds.
The legwork takes place in the library's core, where video and audio tapes are stacked in what looks like a multi-store car park.
Between stacks weave ramps and runways fronted by columns of video-tape recorders and audio cassette players, 30 of each.
Shuttling between stacks and players are ‘intelibots’, battery-driven, wheeled robots no bigger than a shoe box.
Each has a ‘magic hand’, a manipulator which swings from vertical, for fishing tapes out of stacks, to horizontal, for slotting them into players.
When not making deliveries, the intelibots wait in a parking area, where they charge up their batteries.
The name is not well chosen — intelibots are about as intelligent as the players they serve.
A central computer tells them where to go, calculating the shortest routes and making sure they don't crash into each other as they beetle along at a brisk 5 to 6 kilometres per hour.
Kanazawa Industrial University has 34 of the tiny automatons fetching and replacing some 2000 video and 1000 audio tapes for its 4500 students.
The computer also takes care of other jobs, such as channelling one programme to several booths simultaneously, or providing data on current system usage and tape popularity.
Sony, the system's developer, reckons that pictures carry more efficiently than print the increasing weight of information people have to assimilate.
The company has also been rethinking other aspects of library management.
The library of the future would be able to handle all types of electronic media, with users blissfully unaware of whether they were watching, say, a laser disc or a video tape.
And communications lines would take the effort out of exchanges between libraries as well as removing the need for viewing places to be adjacent to stores.
Shot in the arm for tin mines
A GLIMMER of light has appeared on the horizon for Britain's hard-pressed tin-mining industry.
For much of this century, the mines in Cornwall have been in the doldrums.
From a peak of 400 mines around 1850, the number has fallen to just four.
But Wheal Jane, Cornwall's biggest mine, has announced that it wants to extend its workings.
The mine, owned by Carnon Consolidated, a subsidiary of Rio Tinto-Zinc, is a few kilometres south west of Truro near the tiny village of Baldhu.
Carnon Consolidated has applied for planning permission to extend underground by 2 kilometres toward another small settlement, Carharrack.
The company thinks that the ore in the new area runs to about 2 million tonnes, which doubles the mine's existing reserves and would guarantee a total of 15 years' production.
Miners would start digging out the new supplies in 1985.
At present, Wheal Jane produces about 1600 tonnes of tin per year, about the same as South Crofty mine near Redruth.
Geevor, the third biggest mine, brings to the surface some 800 tonnes annually.
If the plan goes through, the mine would push further west from the workings acquired when Wheal Jane lined up with a second mine, Mount Wellington, a couple of years ago.
Rio Tinto bought both mines in 1979 after its previous owners pulled out.
For Rio Tinto, the new scheme extends the interest it has shown in tin mining in Cornwall since acquiring Wheal Jane.
In that time, the giant multinational has bought a 40 per cent stake in the company that runs south Crofty (which also operates a small mine at Pendarves) as well as 18 per cent of the shares in Geevor.
Carnon Consolidated spends about £500000 a year on exploration, and says it is interested in opening up new, small mines in Cornwall, feeding the ore to the Wheal Jane processing plant.
So far, however, the policy has still to come to anything.
It was interested in taking over a small mine called Wheal Concord but pulled out because financial prospects were poor.
Wheal Concord, at Blackwater, near Redruth, closed down in November and has yet to reopen.
Cable debate Signals confusion
Barry Fox
Britain will begin to lay cables for TV and home communication along the length and breadth of the country in the next year or two.
But before work can begin, a raging debate on technology must be resolved
THIS MONTH the government will publish both a policy White Paper and a draft of technical standards for the cabling of Britain in the next two or three years.
This will inevitably generate heated controversy, fuelled by continuing confusion over the technologies.
Opposite view points can be made to sound equally valid on the main issues of controversy: the architecture of the network to be laid, and whether copper coaxial or optical glass cables should be used.
Capital cost is the crucial factor.
British TV viewers will not pay more for a cable system just because one day it will be able to provide interactive services which they have never heard of, let alone want.
The government may have to offer a subsidy, or tax relief, to cable operators and subscribers willing to invest in the most adaptable system.
The government's intention is clear.
The cabling of Britain is to begin as soon as possible, so that at least major conurbations will be hooked up by 1986, when the BBC starts direct broadcasting by satellite (DBS).
Many households will be unable or unwilling to erect and accurately locate the large dish aerials necessary for DBS reception.
Instead they will rely on a local cable station to provide the DBS signals.
The Prime Minister has said that she is ‘determined to encourage cable systems’.
Of those who resist cable she says ‘just remember that the same arguments were put forward against ITV over 25 years ago; they were wrong then and they will be wrong again.’
Kenneth Baker, the Minister for Information Technology, has laid down basic technical requirements.
He has also promised some incentives to operators who invest in systems that can be upgraded to provide services like ‘teleshopping’ and ‘telebanking’.
In any cable system the programme signals are fed from the ‘head end’ down cable links into TV sets in homes.
Currently more than two million British households, one in ten homes, rely on some form of cable system for TV reception.
In some cases a single aerial serves a block of flats or a valley area where off-air reception is poor.
Kenneth Baker has decreed that all new cable systems must be compatible, so that a system laid in one part of the country will interface with the systems laid in other parts.
Baker has also said that every system must be compatible, through its head end, with the trunk networks laid by British Telecom and its private sector competitor, Mercury.
As an incentive for investment, the government will give 20 year licences to operators who install ‘upwards-compatible-’ or adaptable, systems.
Operators who cut corners will get a licence for only 12 years.
Although these requirements are now clear, classification of other technical issues await the White Paper and the technical report by a working party set up last year by the Department of Industry.
This group, under the chairmanship of Dr Toni Eden, has been drafting standards for every service that is likely to be piped down a British cable system.
Only voice telephony was excluded from the group's brief because this is to remain the monopoly of British Telecom.
The working party's work is ‘on schedule’.
It will report to the DoI this week.
Tree and branch is the oldest type of cable architecture.
As the phrase suggests, a main trunk of cables splits off into branches which supply individual subscribers.
Although the system is relatively cheap to install, because there is no electronic switching.
it has the disadvantage that every signal is available in every home.
Some signals must be scrambled and some homes supplied with unscramblers to limit access, for instance to extra subscription services.
Also it is more difficult to provide interactive services because the trunk lines become cluttered with return signals from each branch.
A system of ‘polling’ is necessary, with connections  rapidly switched on a time sharing basis.
Some firms that make TV sets like the idea of the tree and branch system because it means that profitable electronics must be built into the receivers which they sell.
A switched-star network is the alternative approach.
In a switched-star system, only main trunk lines need handle the full range of signals, to and from subscribers.
This is made possible by adding switching stations, for instance at the end of every street, where the trunk lines terminate.
Secondary links connect the switch points to individual homes.
Control signals sent by subscribers down the lines to the local switch point call up whichever of the services on the main trunk line they wish, or are entitled, to receive.
So the expensive electronics is paid for by the cable company.
The main trunk lines can be of optical fibre and the secondary links of coaxial cable, but this is not an essential feature of the switched-star system.
The system can rely entirely on coaxial cable, or entirely on optical fibre, just as a tree and branch network can be based on either technology.
Coaxial cable is made from a central core of copper spaced by plastic or air insulation from an outer sheath of copper braid protected by a plastic sleeve.
The outer conductor ‘Coax’ is widely used to carry radio.
television audio and video signals because there is less loss of signal than through a twisted pair of wires or flex.
But there is some loss of signal in coaxial cable, and this increases with frequency because the inner and outer conductors behave like a capacitor which is transparent to high frequencies.
A signal which has a frequency of 10 MHz will be halved in strength after travelling one kilometre along a coaxial cable.
The losses increase dramatically as the frequency rises.
They are, for instance, twice as large at UHF frequencies as they are at VHF frequencies.
And high frequencies must be used to carry a number of TV channels because each is 8 MHz wide and they are ‘stacked’ one over the other in the frequency spectrum.
Where a coaxial system serves numerous homes over a large area, high quality low-noise cable must be used with amplifiers to boost the signal and compensate for losses.
Selected equalisation, which boosts high-frequency signals more than low frequency signals, may be necessary.
Because every colour TV channel soaks up another 8 MHz of bandwidth there is a trade-off between the number of channels carried and the distance between equalisers and amplifiers.
This is why unqualified reports which cite the large, or small, number of TV channels which a coaxial cable can carry are meaningless.
The length of run, the quality of cable and the money spent on equalised booster amplifiers all affect quality.
It is possible to ‘prove’ that coaxial cable can carry more TV channels than optical fibre.
But it is at a high price.
As a guide, a coaxial cable can handle a bandwidth of 450 MHz, around 30 TV channels in practice.
To double the number you just lay another cable alongside the first.
But the signals need boosting, ideally after 250m and certainly every 2 km.
It's impractical to boost all channels simultaneously, through a wide band amplifier, so separate amplifiers are need different channel bands.
This is why coaxial cable can more easily be used for a switched-star system, where the cable runs a short distance from switching station to TV set.
This is why Kenneth Baker has promised longer franchises for operators prepared to install systems that rely on switched-star technology.
It also explains why it is virtually certain that Britain's main trunk lines will rely on optical fibre.
Moreover British Telecom's interest and experience, in using fibres for telephony and data transmission, is likely to guarantee it an option on providing optical trunk lines between the switch points of star networks.
If it were a perfect world the cable links into subscribers homes would also be of optical fibre, and upwards compatible in the future.
Unfortunately, the real world of cable commerce is far from perfect.
An optical fibre uses total internal reflection to carry a light beam over long distance and around corners.
The fibres need to be about the diameter of human hair.
The obstacle to optical fibre communication is lift loss due to impurities in the glass.
The recent enthusiasm for optical fibres is the result of dramatic advances in optical purity.
High-temperature gas cleaning techniques remove the water and metal ions which are a natural constituent of silica glass, and attenuate light.
Ten years ago a light beam passed down an optical fibre was reduced by 99 per cent to one per cent of its original strength after travelling just one  kilometre .
Now, with purer glass, light loss over the same distance is 7 per cent.
Another snag with optical fibre is spurious reflection inside the fibre.
As the waves or pulses pass down the fibre they mix with their delayed echoes.
This distorts an analogue waveform and introduces errors into a digital pulse chain.
The problem is solved if the central core is very thin so that there is no room for reflection.
This is ‘monomode’ fibre, but it is obviously more difficult to inject light into the end of a fibre whose usable core is only five or six micrometres wide.
The source must be aligned to an accuracy of 0.5 micrometres.
It is difficult to join monomode fibre because the tiny cores must be accurately aligned.
Although this is practical in a laboratory, it is (as engineers from Rediffusion working near Hastings found out) impractical on a wet and windy day in a trench in a field.
On abnormally hot days, and abnormally cold days, the jointing block can change size sufficiently to spoil the join.
So Rediffusion engineers do not see a future for monomode fibre except for trunk lines.
But British Telecom thinks the problems can be licked.
The alternative is to use a ‘multimode’ fibre which has a wide optical core (around 50 micrometres).
The snag with multimode fibre is that, because it carries a wider beam of light, it suffers more from spurious internal reflections.
This limits the length of run which can be used between booster stages which receive light, reconstitute and amplify the electrical signals, and re-inject light into the next stage of fibre.
Obviously this puts up the system cost over long distances.
So multimode fibre is better for long runs.
But the balance may change as optical fibres become cheaper with mass production.
A single fibre of multimode will soon cost around 7p a metre.
But the fibre needs protection, because it must be pulled through ducts.
The addition of a steel strain-wire and outer sleeve puts another 10p a metre on the price.
If two fibres are used in the same sleeve, the price comes to 20–25p a metre.
By comparison coaxial cable, of the type need to cable homes, will cost the cable company around 16p a metre.
The unknown factor is the cost of copper.
The cost equation complicates further because coaxial cable needs only simple connections at each end, for instance a plug to fit a TV set.
An optical fibre link needs a photo detector to convert the incoming light pulses into electrical signals.
If the system is interactive, then a light transmitter in the home is needed as well.
There are two ways of injecting light into a fibre: a solid state laser or a solid state light emitting diode (LED).
In both methods the light source is either varied in intensity, as an analogue of the signal, or switched on and off in digital code.
For analogue transmission, the light output must vary in linear fashion with the electrical inputs.
A LED or laser which behaves in linear fashion is more difficult to make, and thus more expensive, than a device with a light output only roughly corresponding to the power input.
A laser produces a beam which is easier to focus into a monomode fibre because its light does not spread in frequency as much as a LED.
For digital operation, where the light is simply switched on and off to generate pulses, linearity is not essential.
But a digital system is more expensive because coding circuitry is needed to convert the analogue picture and sound signals into digital code and back again.
However this circuitry will get cheaper.
The other factor in the confusion is wavelength.
At some frequencies even the best optical fibre available will absorb or scatter light, whereas at other frequencies they will pass light with almost no attenuation or scattering.
It is essential to avoid the light wavelength 1.4 micrometres because, at this value, the residual water molecules which reman in even the most pure fibre drastically attenuate the signal.
At higher frequencies, and shorter wavelengths, below 0.7 micrometres, spurious reflections cause such serious scattering that the usable signal is soon lost.
At longer wavelengths, around 1.8 micrometres, there is serious water absorption again but around 1.6 micrometres there is a peak of performance.
Unfortunately it is more difficult to make light sources and light receptors that work at this longer wavelength.
Both STC and Plessey are working on this technology, for use in submarine telephone fibre, but the current best bet for domestic cable is a wavelength of around 0.8 micrometres.
This avoids the water absorption point and is not too seriously affected by scattering.
Costs and qualifications
All-optical cabling is an attractive proposition.
Although the full potential of coaxial cable has not yet been exploited, the long term potential of optical fibre is far greater.
Bandwidth and attenuation are the key factors.
Whereas loss of signal in coaxial cable makes runs of over one kilometre disproportionately expensive, because of the need to amplify signals on the way, the low loss of fibres makes long runs cheap; but fibres need conversion hardware at each end.
Already losses in fibre are so low that a light signal can travel well over 16 km before it halves in intensity (a 3 dB loss).
By 1990 light signals will be travelling under the Atlantic, to North America, with only the same 3 dB loss.
There is much talk of potential bandwidth in optical fibres.
But reference to bandwidth is meaningless unless it is qualified by distance of the run.
A fibre link which has a bandwidth of 5 GHz/km will have a bandwidth of a GHz for a 5 km run and so on.
Bandwidth is also limited by the performance of the terminations at the end of the fibre.
Although in theory the bandwidth of a light link can approach infinity, in practice all lasers, photo receptors and digital conversion circuits have a finite bandwidth of operation.
There is a limit to how fast they can operate.
Also the faster they operate the more expensive they are.
Any shift of frequency, for instance, as the laser is driven hard into high frequency switching.
creates scatter in the fibre and reduces bandwidth.
The problem is worse with LEDs.
In practice, it is possible to send a single digitally encoded TV channel only down an optical fibre.
This has generated some confusing comparisons between the capability of optical fibre and coaxial cables.
You can say that optical fibre is able to carry only one TV channel whereas a coaxial cable can carry up to 30.
But the qualifications are numerous.
Mr Bell found optics but he needed fibres
THE IDEA of transmitting signals by light is old.
In 1880 Alexander Graham Bell filed a patent application on an ‘apparatus for signalling and communicating called a Photophone’.
This described a  primitive system of optical communication.
Bell took light from the sun, a lamp or even a candle, and used a lens to focus it into a beam.
He passed this beam through holes in a rotating wheel, or oscillating miniature venetian blind, to chop it up or ‘modulate’ it.
His photophonic receiver was a lens which focused the modulated beam onto a very thin, hard rubber diaphragm mounted on a large telephone ear-piece.
According to the patent (Us 235 199), when the chopped light beam was focused onto the diaphragm it created sympathetic movements which could be heard as sound.
Bell soon realised that attempts to convert light energy direct into acoustic energy at the receiver were impractical.
So he replaced the diaphragm with a piece of light-sensitive selenium, connected in series with a battery and telephone earpiece.
This incoming beam was focused onto the selenium which changed its resistance according to the strength of the light.
So a fluctuating beam changed the resistance in the circuit to produce a varying current in the ear-piece which in turn created sound.
He built a prototype which for nearly a hundred years lay in the Smithsonian Institution, Washington DC.
Recently a curator realised what it was.
The obvious snag with free space communication by light is that the atmosphere attenuates the beam.
Rain, fog, snow, smoke or pollution all reduce the strength of, or completely block, the light beam.
The next idea, which was tried at Bell Laboratories, was to send a beam through closed channels, like wave guides.
But these of course must be straight, because the light beam cannot pass round corners.
The answer is to send the light down a pipe which guides it round corners.
The wild dogs of Italy
James Hansen
Wolves, last of the great native predators, are now rare in Italy.
Feral dogs are taking their place
NOT SO LONG AGO it seemed that we had very nearly lost our fairy tale beasts.
Bears are exceedingly thin on the ground in European forests and these days ravening packs of wolves rarely come howling across the Danube when the winter is hard enough to freeze the river over.
Whether the relative scarcity of wolves is an abhorrent vacuum is a matter of personal taste, but there is now compelling evidence that nature is moving to fill the gap.
Southern European countries, particularly Italy, are beginning to face serious problems with another big new predator.
This is the feral dog; the domestic animal adapted to the wild and living as a wild animal.
According to a game  survey recently performed by a group at the Istituto di Zoologia of the University of Rome, Italy's population of fully wild, feral dogs has exploded to above 80 000 and is fuelled by a pool of perhaps ten times as many stray and free-ranging animals.
Feral dogs in the Italian environment behave in much the same fashion as wolves.
Running mainly at night in packs of up to 20 or 30 members, they avoid the sight of man, and prey by preference on the larger herbivores.
Since game in the form of deer and roebuck is scarce, the herbivores in question are often man's cattle, horses and especially sheep.
Though reliable comparative data are lacking, the Italian researchers believe the feral dog population has at least doubled since 1975.
This extraordinary increase is due in part to the fact that the dogs breed twice a year and it seems that all females give birth whereas in wolf society only dominant females do so.
Competition in the wild favours larger dogs: mastiffs, setters, German shepherds, the bigger hunting and herding breeds.
Documentary evidence of the economic damage caused by these animals has only recently become available with the institution in Italy of a programme to refund farmers for losses caused by wolves, which are legally protected.
Last year farmers and herders claimed for losses of more than £650 000 in just three Italian regions — Abruzzo, Latium and the Campania.
Even allowing for the traditionally light-hearted Italian attitude toward obtaining government money, Italy's wolves number at best only about 150, including puppies.
Most official wolf damage is certainly caused by wild dogs.
Dogs without masters and the damage they cause are.
of course, no novelty.
But the quantitative explosion in the feral dog population in Italy has been accompanied by a qualitative change.
‘Certainly, we've always had strays,’ comments Luigi Boitani, the Roman zoologist who directed the wild dog study, ‘but this time were facing something different.
These animals are moving into the ecological niche left vacant by the disappearance of the last big predators from Western Europe: wolves, bears and lynx.’
The behaviour of stray dogs, according to Boitani, is quite different from that of feral animals.
Strays range alone or in small groups of two to four members and prefer to live in and around villages and small towns.
Active during the day as well as at night, they appear to adjust their pace to the human by-products on which they depend, such as scraps from butchers shops and other handouts.
The chance to study a geographically convenient dog population which is not dependent on man is attracting researchers from other parts of Europe.
David MacDonald of Oxford University and his student, Jeff Carr, have begun a long-term behavioural study of these non-domesticated dogs.
The Italian group has been principally interested in following the dynamics of population growth among the feral animals.
The Italians see many reasons for the sharp increase in numbers.
One is the disappearance of the traditional predators themselves.
They kept the feral dog population in check.
Indeed, dog still makes up about 3½ per cent of the diet of Italian wolves.
Another factor contributing to the wild dog problem is the progressive abandonment of the countryside as people retreat to big and medium sized cities.
Large parts of Italy (the  Apennine foothills are a prime example) are wilder now than they were a century ago.
The army of herders and small farmers that once destroyed wild dogs as a question of principle has gone off to work at FIAT.
The increasing prosperity of those humans that have remained in the countryside is also contributory.
Their garbage dumps are the single most important source of nutrition for feral dogs.
Whatever the reasons, the dogs are there and in growing numbers.
Aside from the economic damage they cause to herders, they are-beginning to cause other problems.
Actual attacks on humans are rare so far and more likely to be the work of strays than of truly feral animals.
Genuinely wild dogs avoid the company of men.
One of the problems created by the new population of feral dogs is exotic, but instructive.
The Italian branch of the World Wildlife Fund sponsors a wolf group, which is attempting to preserve that animal.
According to the group, the biggest single menace to the Italian wolf is competition from wild dogs.
This competition is not so much for food and territory as it is genetic.
Wolves and dogs can interbreed, though in an ideal natural state they prefer not to.
Still, when a mere handful of wolves is submerged in an enormous population of wild dogs, these encounters necessarily become more common.
The danger, according to the wolf group collaborator.
lies in the fact that ‘the offspring behave like wolves and look like dogs…this ‘camouflage’ gives them a tremendous competitive advantage because, as ‘dogs’, they can approach garbage dumps in populated areas or herds of sheep without anyone taking special notice, much less reaching for a rifle’.
All this means that the Italian wolf, which is perilously close to extinction, may survive after all — but as a dog.
It also means that the feral dogs are acquiring genetic material that could turn them into dangerous carnivores in grand style.
How does this happen?
What turns man's best friend into a dangerous renegade?
‘The better question is what makes a dog into a household animal,’ comments Francesco Francisci, a participant in the wild dog study.
‘We think it depends on a kind of imprinting.
Dogs raised in the presence of man behave in the way we expect of them.
Dogs which reach, say, six months of age without human contact may behave as wild animals.’
Support for this explanation comes from the geographical distribution of the feral dog population, concentrated in central and southern Italy.
It is, in a sense, a question of latitude.
The further south you go, the more wild dogs there are.
The determining factor seems to be a variation in the rapport existing between man and the dogs he keeps.
Laura Fabbri helped conduct the dog census recently performed by the group at the University of Rome.
The finding is that roughly 25 per cent of Italy's total dog population of around 3.5 million is made up of animals which, if they are not all precisely strays, are at least‘free-ranging’ and not under immediate human supervision.
‘It's an incredible figure,’ she says, ‘and it hides a very evident gradient from the northern regions to the south of a progressively looser relationship between man and dog.
‘We think this depends above all on a different social structure in the villages of central and southern Italy.
In that warmer climate, a great part of life takes place in the streets or in the piazza and the dogs are left to wander as they wish.’
Interchanges between free-ranging town dogs and countryside strays become common.
The bitch that whelps her pups beyond the margins of village life has given birth to potentially feral dogs.
All this is as old as history.
What is new is that the controls which held this population in check no longer exist.
For the moment, Italian government authorities are more inclined to see the feral dog question as a potential concern rather than a dramatic problem.
The Ministry of Agriculture collaborated on the dog study and circulars have gone out urging villages to fence in their garbage dumps to deprive the animals of at least that source of food, but there is no wave of popular enthusiasm for slaughtering dogs wholesale.
That could happen though.
It turns out that the wild dog business is intimately bound up with another coming problem; the spread of silvan rabies out of eastern Europe.
The disease front has been moving ahead by fits and starts for most of the last two decades and three years ago crossed the Italian frontier into the provinces of South Tyrol and Venice.
Wild dogs are a nearly perfect vector for silvan rabies and when the disease front reaches the centres of population in central and southern Italy the result will almost certainly stir public opinion toward sharp action to reduce the feral population.
Conservationists are concerned that when this happens frightened local authorities may resort to indiscriminate poison baiting, a step which would undoubtedly kill many wild dogs, but could spell the end for several other declining species as well.
Italy, in fact, had another serious bout with a rabid dog population in the aftermath of the Second World War.
The crisis, which mostly concerned urban strays, was successfully met with poison baiting, but this was not without environmental cost.
Among other things, it led to the extinction of a distinctive species of Sicilian vulture.
This time the country would likely lose its last remaining wolves.
Not everyone is moved to tears by the fate of vultures and wolves, but it is possible to think that the old crops of predators and scavengers were somehow more attractive than the new lot.
Red Riding Hood could conceivably be taken in by an Irish Setter.
A poodle tucked up in Granny's bed would be only pathetic.
The curious case of the shrinking Sun
John Gribbin
Claims that the Sun is shrinking have led researchers to discover a more immediate danger.
The Earth's climate could change significantly in the next century
ASTRONOMERS were startled, and laymen amazed, when in 1979 Jack Eddy, of the High Altitude Observatory in Boulder, Colorado, claimed that the Sun was shrinking, at such rate that, if the decline did not reverse, our local star would disappear within a hundred thousand years.
Eddy was working with mathematician Aram Boornazian on a study of measurements of the Sun's diameter which had been made at the Royal Greenwich Observatory between 1836 and 1953.
Together they found evidence of a decline in solar angular diameter of two seconds of arc — equivalent to 0.1 per cent— per century (American Astronomical Society Bulletin , vol 11, p 434).
We have very good evidence indeed that the Earth and the Sun have existed with essentially the same relationship to each other as at present for at least four thousand million years.
So Eddy's discovery should imply that the Sun is merely in a temporary phase of contraction, which must soon be halted and reversed.
However, Eddy's initial claims quickly turned out to be overstating the case.
The observations at Greenwich had been made by different observers at different times, using techniques that depended, to some degree, on the observer's skill and expertise in his chosen technique.
One approach is to time the passage of the Sun, from one side (or — limb’) to the other, across a fixed meridian (north-south) wire, and calculate its size from the known rotation rate of the Earth; another depends on measurements of the solar disc using a micrometer screw at the eyepiece of a telescope.
Both techniques proved to have been subject to persistent human errors which accounted for at least part of the apparent change in the size of the Sun.
Theorists pointed out that any such change in the size of the Sun ought to produce a detectable change in the solar constant — the amount of energy output by the Sun.
But measurements of the solar constant since 1850 implied that the solar radius must have been constant to within 0.3 seconds of arc per century, less than half the variation in the records at Greenwich.
And in mid-1980 Irwin Shapiro at the Massachusetts Institute of Technology published an analysis of observations of the transits of Mercury across the Sun's disc, in which he concluded that the Sun had maintained a constant size since the end of the 17th century (Science , vol 208. p 51).
This technique depends on timing the passage of Mercury across the face of the Sun in those years when, in May and November, the Earth, Mercury and the Sun are appropriately aligned.
This happens only about 13 times in each century, but such transits have now been the subject of detailed astronomical scrutiny for more than two hundred years.
By late 1980, the issue seemed clear cut.
The old records at the Royal Greenwich Observatory contained a consistent error, and Eddy had been wrong to take them at face value.
But then the plot began to thicken.
In mid-December, 1980, both of the two leading weekly science journals,Nature and Science , carried, in the same week, scientific papers reporting work stimulated by the claims of Eddy and Boornazian.
As far as the numbers that came out of their calculations were concerned the two papers agreed.
Yet the interpretation of those numbers by the two teams led to diametrically opposed conclusions, as typified by the titles of the two papers.
In Science (vol 210, p 1243), David Dunham, Sabatino Sofia, Alan Fiala.
David Herald and Paul Muller presented their analysis under the heading ‘Observation of a probable change in the solar radius between 1715 and 1979’.
At the same time in Nature (vol 258, p 548) John Parkinson, Leslie Morrison and Richard Stephenson were proclaiming to the scientific world ‘The constancy of the solar diameter over the past 250 years’.
A close look at those two intriguing papers shows how even the most objective scientist can unwittingly colour his conclusions to suit his expectations; but they leave the question of variations in solar size very much still open.
The argument for solar constancy is essentially the one I have already outlined.
Parkinson and his colleagues pointed out that out of seven regular observers using the Royal Greenwich Observatory's meridian circle since 1851, five produced self-consistent observations of the Sun's diameter over each of their periods as observer.
The other two were full of ‘strong, erratic personal biases’.
Together with the data from the transits of Mercury, this re-interpretation of the old records led to the conclusion that the maximum extent of any change on the Sun's radius since 1850 was no more than a decline of 0.08 seconds of arc per century, with a possible error range of plus or minus 0.07.
Within the range of possible error, the Sun's diameter might be constant — the conclusion proclaimed in the Nature headline — or it might be shrinking, although at only one tenth of the rate claimed by Eddy.
Despite the opposing headlines, this is not out of line with the evidence published in the paper in Science .
For the conclusion reached by Dunham's team was that the Sun has indeed been shrinking and that its radius — or, strictly speaking, half the measured angular diameter — has contracted by 0.34 seconds of arc over 264 years.
This result is almost exactly in line with the decline, in round terms, of some 0.1 seconds of arc per century suggested by the re-examination of the records at Greenwich.
The technique used by Dunham's team is disarmingly simple in concept.
It depends upon the precise accuracy of some observations made in 1715 under the direction of Sir Edmond Halley, later Astronomer Royal and known to this day for the comet that bears his name.
In that year, there was a total eclipse of the Sun visible from Britain on 3 May.
Halley organised observations of the eclipse from different parts of the country.
The duration of totality at each observing site provides an important clue to the size of the Moon's shadow on the Earth, and thereby to the size of the Sun at the time of the eclipse.
An observer only a kilometre inside the edge of the path of totality, for example, would see total eclipse lasting for a mere 15 seconds.
Thanks to Halley's efforts valuable observations of this kind were made at both the northern and southern edges of the path of the eclipse in 1715.
In the north of England, Theophilus Shelton, Esquire, recorded at Darrington (a small town located just north of 53 degrees 40 minutes latitude, near the city of Leeds) that the Sun — was reduced almost to a Point, which both in Colour and Size resembled the planet Mars’.
He concluded that the northern limit of totality was just south of his location.
In the south of England, the edge of totality was bracketed by observations on either side of the line, south of the village of Cranbrook (which lies in Kent, just south of latitude 51 degrees 10 minutes l.
Together the results gave a precise guide to the extent of the Moons shadow.
The path of the edge of totality depends on the precise geometrical alignment of the Sun, Moon and Earth at the 1976 (visible from Australia) and 1979 (watched by hundreds of amateur astronomers across North America).
Dunham and his colleagues concluded that there was no measurable change in solar radius between 1976 and 1979, but that between 1715 and the 1970s the Sun had shrunk by 0.34 seconds of arc, with an uncertainty of plus or minus 0.02 seconds of arc.
This disagrees with the earlier claim by Eddy and Boornazian, but falls within the limits set by theorists (including Sofia, himself a member of the group headed, alphabetically by Dunham) on the basis of measurements of the solar constant.
It is also consistent with the data on transits of Mercury that Parkinson's team used as evidence for the constancy of the solar radius; according to their data any change has to be smaller than 0.15 seconds of arc per century.
To put these figures in perspective, the Sun's angular diameter is 32 minutes of arc, a little over half a degree.
Its linear diameter is I 392 000 km; it looks so small only because it is at a distance of nearly 150 million km from us.
If the two papers had appeared out of the blue, with no preparation of the scientific ground, they would surely have made an enormous impact both among astronomers and in the wider world.
Even if the Sun is shrinking at a rate of merely 0–01 per cent per century it would totally disappear in a million years; and it would have been twice its present size a million years ago.
(Both these are ludicrous suggestions according to everything that has been learned over the past half century about how stars work.)
Alternatively, the present contraction could be simply one phase of a long, slow pulsation which might have a cycle time of hundreds or thousands of years.
The two papers did not cause an immediate stir.
The headlines had already been written when Eddy and Boornazian suggested that the Sun was shrinking at a rate of 0.1 per century.
Contraction at just one tenth of this rate seemed small beer to astronomers, and was presented by the popular media (where they took any notice at all) as another example of a way-out scientific idea that had been undermined by more careful studies.
Metaphorically, we can imagine theoretical astronomers breathing a sigh of relief and saying ‘Oh, so the Sun is only shrinking by a tenth of a second of arc per century, not a full arc second after all.
Nothing to worry about.’
It seems to have taken several months for the message to sink in that here was plenty to worry about, with deep implications for the inhabitants of planet Earth.
The immediate cause for concern is that changes in the Sun's diameter are linked with changes in its heat output.
And changes in the solar constant by even a fraction of one per cent can have a pronounced influence on the climate of the Earth.
Eddy's own interest in the old records from the Royal Greenwich Observatory had developed from his study of the changing level of solar activity as indicated by the numbers of sunspots, or dark spots in its surface.
He had proved during a thorough re-analysis of old observations that the coldest decades in recent history; the ‘Little Ice Age’ of the second half of the 17th century, coincided (if that is the right word) with an interval when the Sun was remarkably free from  sunspots .
Since then the spots have come and gone with a period of roughly 11 years.
Might changes in the size of the Sun be linked both to the surface activity manifested by sunspots and to small-scale climatic changes on Earth?
Curiously, in their paper suggesting the constancy of the solar diameter, Parkinson and his colleagues did mention that the data from transits of Mercury show hints of a periodic variation in the Sun's size with a cycle time of about 80 years.
The special significance of this number is that a similar long-term cyclic pattern shows up in the record of the changing number of sunspots, modulating the stronger 11 year rhythm.
A cycle of the same length also occurs in climatic patterns, revealed by historical records of temperature, the width of the annual growth rings in trees, and so on.
Some powerful hints of what was to come were there to be seen in December 1980.
But it was not until the autumn of 1981 that the bombshell finally exploded.
The breakthrough came from one of Eddy's colleagues, Ronald Gilliland, who also works at the High Altitude Observatory.
Rather surprisingly, it was published not in the pages of Nature or Science , where hot news is usually aired for rapid communication among scientists, but between the sober covers of the Astrophysical Journal (vol 248. p 1144), a pillar of respectability among the astronomical establishment.
Gilliland based his study on no less than five sets of data, including the old records from the Royal Greenwich Observatory with the correction for systematic observing errors, similar measurements with a meridian circle from the US Naval Observatory in Washington, two sets of Mercury-transit observations, and the solar eclipse data.
His first conclusion, from a battery of statistical tests, was that the overall decline in solar diameter of about 0.1 seconds of arc per century since the early 1700s is real.
And when standard statistical tests aimed at revealing small, regular changes in the pattern of variability were turned on the data from the meridian circle they showed an unambiguously clear trace of a periodic variation with a repeating rhythm of 76 years.
This almost exactly matches up with the periodic variation present in the Mercury-transit data.
The range of this regular pulsation is only 0–02 per cent of the Sun's radius, but it has been stable over the full 250-year span covered by the various sets of observations.
It also shows a clear relationship with sunspot activity; by and large there are fewer sunspots when the Sun is bigger.
If the same anticorrelation can be applied to the longer-term decline in the Sun's diameter, it may provide a clue to the dearth of sunspots during the height of the Little Ice Age, 300 years ago.
Then the Sun's angular diameter was about two thirds of a second of arc greater than it is today.
And as the icing on the cake, Gilliland also reported a smaller, but clearly present, fluctuation in solar size which ties in with the 11-year sunspot cycle.
The periodic variations are unambiguous.
As for the longer-term decline in solar diameter, the discovery that started the whole ball rolling, Gilliland was cautious in his claims.
‘Given the many problems with the data sets,’ he said, ‘one is not inexorably led to the conclusion that a negative secular solar radius trend has existed since AD 1700, but the preponderance of current evidence indicates that such is likely to be the case.’
With rhythmic variations 11 and 76 years long now identified in the measurements of solar diameter, it seems straightforward to interpret the longer trend as part of a similar but longer cycle, which would pose no real problems for astrophysicists' and geologists' faith in the long-term stability of our nearest star.
Gilliland, however, had a parting shot to fire in his paper in Astrophysical Journal .
At present, if his analysis is correct, the Sun is approaching a maximum of the 76-year cycle, and will start to decline in size once again at about the end of the present decade.
But the accepted value of the solar radius, 959–63 seconds of arc, is based on 19th-century observations, and became enshrined in the text books at a time when the Sun was close to a minimum of the 76-year cycle.
If Gilliland is correct, the true value of the mean solar radius is more nearly 959.8 seconds of arc.
However, his claims are testable.
Existing techniques are accurate enough to measure such changes in the Sun's size directly, and monitoring programmes set up in the wake of these various claims and counter-claims will resolve the issue, one way or another, before the end of the decade.
So where do we go from here?
One path, clearly, leads back to the puzzle of just how the Sun works.
Another path leads inexorably in a different direction, into the detailed study of the workings of the Earth's atmosphere and climatic systems.
In his search to determine more about solar variability, Gilliland has now been forced to unravel the complexities of recent climatic changes on Earth.
It is difficult to see how these changes in solar diameter, both the 76-year cycle and the long-term decline, could fail to have affected the temperature of the globe.
But so many other factors also affect the climate that the only hope of detecting the solar ‘signal’ and thereby, ultimately, learning more about what makes the Sun tick — is to subtract out the other main influences From the historical record of changes in temperature in the northern hemisphere.
It might seem a daunting task, but in recent years climatologists have begun an intensive study of temperature changes on Earth.
Their main concern has been that the build-up of carbon dioxide in the atmosphere — a by-product of our  dependence on  energy from fossil fuels — could trap infrared radiation near the surface of the Earth and warm the planet through the so-called ‘greenhouse effect’.
By and large, climatologists agree that two ‘perturbations’ have been affecting trends in temperature during the present century: the warming influence of this increase in carbon dioxide, and the variable cooling influence produced when great volcanic eruptions spread dust high into the stratosphere, blocking some of the heat from the Sun.
Most of the changes in temperature over the past hundred years can be broadly explained by these two processes at work.
But Gilliland found that the real temperature record could be matched much more closely by adding a third factor to the calculations: varying solar heat output tied to the 76-year cycle of variations in solar size (Climatic Change , vol 4, p I 1 1).
Gilliland is at pains to point out that this does not prove anything about the way eternal influences affect the workings of the weather machine.
His study certainly provides food for thought, though.
He has taken the well-established record of annual changes in the average temperature of the northern hemisphere since 1881, and obtained the best possible fit to this pattern by combining the three external influences: the greenhouse, volcanic and solar effects.
Each of these is modified by a scale factor to adjust the agreement between the model and the real world.
And the model incorporates an adjustable time lag in the volcanic and solar influences to allow for the time it takes the temperature of the atmosphere and oceans to respond to outside influences.
With so many variable factors, it ought to be possible to provide a ‘fit’ to almost any curve; the interesting point of the study is whether or not the scale factors put into the equations seem to be saying anything that would lead to an elucidation of the influences at work.
The main features that any such model has to explain are a slight warming of the world (at least, of the northern hemisphere, for which good records are available) from the late 19th century up to the 1940s. and a subsequent cooling up to the 1970s.
Gilliland gets a reasonable fit between records of actual temperature and his model if he leaves solar variations out altogether and works only with the build-up of carbon dioxide and influence of volcanic products on the atmosphere.
The early part of the present century was quiet in volcanic terms, and the warming might be explained as a result of dust clearing from the stratosphere, while the recent cooling trend coincides with increasing volcanic activity.
But in such a variation on the theme, the influence of carbon dioxide has to be set very small, only one tenth of the strength most climate modellers currently accept, otherwise it would have overwhelmed the volcanic influence and caused the Earth to continue warming through the 1940s, 1950s and 1960s.
The best fit of all between theory and reality comes, however, when Gilliland adds the third factor, solar variability.
Now the warming trend is explained as due to a combination of solar and volcanic influences, with a 24-year lag between the maximum of the solar diameter in 1911 and the peak warmth of the 1930s — the dustbowl era in North America.
From about 1940 to the 1970s, in this picture, both solar and volcanic influences were acting to cool the Earth, more than compensating for the rapid buildup of carbon dioxide, even with the standard greenhouse effect numbers.
All this is very intriguing.
It resolves the puzzle of why the Earth cooled even while the concentration of carbon dioxide was continuing to grow exponentially, and all it requires is a peak change in solar luminosity of just 0.28 per cent over the 76-year cycle, producing a maximum influence on surface temperatures on Earth of only 0.28°C.
These figures are well within the range of possibilities set by observations of the solar constant from the Earth: as Gilliland says in this picture ‘low temperatures of the last two decades result primarily from a minimum of the solar 76-year cycle’.
But while resolving one puzzle about the greenhouse effect.
his analysis also raises new concern about its future influence on mankind.
If the standard calculations of the greenhouse effect are indeed correct, then over the next 30 years temperatures are likely to rise by a full degree (centigrade) as ever-increasing quantities of fossil fuel are burnt.
But now the solar influence is beginning to turn around to contribute a further warming influence up to the year 2010, boosting the greenhouse effect where for the past 30 years it has been counterbalancing it.
The result is a forecast of much more rapid and pronounced warming of the globe than has previously been thought likely, setting in by the end of the present decade.
At first, this might seem beneficial as the 1990s see a return of the excellent conditions for world agriculture that prevailed in the 1950s.
Beyond the turn of the century, however, the forecast implies a rapid warming into conditions unseen on Earth for a thousand years or more, heralding a super dustbowl era far worse than the 1930s across the Great Plains of North America.
So, in the space of three years, the curious puzzle of the shrinking Sun has taken researchers from the studies of dusty old records locked away in the files of the world's most famous observatory to speculations about the internal workings of the Sun and a grim warning of the possible climatic future that will confront the next generation of human beings on an already overcrowded planet.
Halley and his successors at the Royal Greenwich Observatory would surely have been fascinated to learn of the unexpected uses to which astronomers of the late 20th century would put their solar observations.
A century or two ago, though, who could have imagined any practical benefit to mankind from such erudite and scholarly historical research as studies of the exact path of an eclipse, or painstaking measurements of the angular diameter of the Sun?
REVIEW
Old Man River challenges Old Faithful
The myths of human evolution by Niles Eldredge and Ian Tattersall Columbia UP, pp 197, $22.50 
Adrienne Zihlman
ONE OF THE hottest ‘debates in evolutionary studies is the issue of ‘punctuation’ versus gradualism.
Did morphological change in a lineage — the human lineage, for example— take place slowly, like a steady stream, or were long periods of stasis punctuated by bursts of rapid change, like Old Faithful?
The many gaps in the fossil record can be interpreted both ways — as being due to the intrinsic incompleteness of a record based on random serendipitous preservation, or as reflecting the true historical sequence: what you see is all there is.
Niles Eldredge and Stephen Jay Gould, two invertebrate palaeontologists, suggested in 1972 that the abrupt transitions in the fossil record reflect the way evolution actually works, by fits and starts, or in their latinate phrase, ‘punctuated equilibria’.
The Myths of Human Evolution is an application of the punctuationist viewpoint to human evolution.
The main myth being punctured, or punctuated, is the old erroneous view that humanity evolved gradually over the aeons from an ape forebear.
According to Niles Eldredge and Ian Tattersall, this transformation, like those of the marine invertebrates, the bites and brachiopods, followed Old Faithful rather than Old Man River.
As one who was teethed on the works of George Gaylord Simpson, I've been bemused for several years by the supposed ‘revisionist’ implications of punctuation.
Simpson called it quantum evolution and described it as one of three evolutionary modes.
The authors discuss neither Simpson nor the important paper by John Cronin et al in 1981 (Nature , vol 292, p 113) which summarises the mass of morphological and molecular data which contradict the punctuational argument as applied to human evolution.
It therefore appears, as the old saying goes, that in the authors' thesis what is true is not new, and what is new is very likely not true.
The fatal defect in the punctuational hypothesis is the failure to consider all the evidence of the past two decades from studies of molecular evolution.
A mere description, that organisms stay the same for millions of years and then rapidly change, does not explain anything in this time of our rapidly growing understanding of genetic mechanisms.
Dozens of studies of the genetic material itself, the DNA, and the proteins it encodes, indicate that these change in a random statistical fashion, but at fairly steady rates over evolutionary time.
Each genetic change can result in either much morphological change, little morphological change or no morphological change.
Thus a stream of water percolating down into the Earth can result in Old Faithful, a little stream, or no surface manifestation.
The molecular discoveries about primate relations — namely, that humans, chimpanzees and gorillas share 99 per cent of their genetic material — has profoundly affected current ideas about human origins.
The data strongly imply a threeway divergence of these species not longer than 5.6 million years ago.
They also tend to rule out Ramapithecus , a fossil hominid dated at 8.14 million years.
as a human ancestor, even though most palaeontologists, prior to the molecular observations, considered it the earliest hominid.
Palaeontologists almost universally denied both these implications of the molecular research and continued denying them for almost 15 years.
They have come around to accepting them, but on the dubious grounds that the  palaeontological  evidence now proves that the earliest hominids arose in Africa about 5–6 million years ago and that Ramapithecus was not a hominid.
This example clearly demonstrates that morphology can be made to fit any preconception, even those diametrically opposed to each other.
In a recent article in Current Anthropology , one palaeontologist went even further and stated that the molecular anthropologists were right for the wrong reasons, while the palaeontologists were wrong for the right reasons.
A justly famous early hominid is the remarkably complete Australopithecus afarensis known as Lucy.
The binomial has been applied by Don Johanson and Tim White to a group of fossils from Laetoli, Tanzania, dated about 3.6 million years, and from Hadar, Ethiopia, dated about the same age by some palaeontologists though recent evidence indicates an age nearer 3 million.
The authors present Lucy as an example of punctuation: rapidly becoming human and bipedal from the ape ancestor, then not changing for a million years.
But the gradualist argument could be applied just as tellingly: chimp-like skull, body size and body build, feet retaining chimp-like features even though the pelvis and lower limbs indicate bipedalism.
Aside from its logical and evidential deficiencies.
The Myths of Human Evolution has a number of technical shortcomings.
There are no charts or diagrams to help even the experienced reader to follow the argument, nor references, and only the occasional illustration of a skull.
It reads more like a series of loosely joined essays than an integrated work.
The forefront of evolutionary understanding today is our growing knowledge of the molecular biology of genomes.
To ignore this and focus on the false antithesis of punctuation versus gradualism, with no mechanism to explain either, is to engage in a scholastic war of words largely devoid of scientific content.
In my opinion, this empty disputation will eventually take its place with the many other myths of human evolution.
The language of time
On time by Michael Shallis,Burnett, pp 208, £8.95 
David Bohm
IN THE first part of this book Michael Shallis gives an interesting non-technical account of how modern physics has gone on from the common-sense notion of time to a whole series of fundamental changes.
These include the relativistic dependence of time on velocity and the strange new features of time in black holes, as well as the ambiguity of direction of time that has emerged in the mathematical description of the interactions of elementary particles.
The main conclusion drawn is not only that modern scientific notions of time are in many ways puzzling and paradoxical; even more important for Shallis is that the essential quality of our actual experience of time is left out of such treatments altogether.
Indeed, time as treated in physics is at best a series of abstractions.
These may be very useful in certain domains, but they have little to do with that actual flowing movement of life which concerns us deeply and intimately.
This leads on to the second part of the book, in which the author begins by showing that there is a deep ambiguity in our basic concepts of causality and chance.
The notion of randomness is especially unclear, in the sense that it has never been defined in any consistent way.
Within this ambiguity and unclarity, Shallis sees room for an entirely different approach to the whole question of time.
Briefly, this is to consider that the connections and orders of various sets of events are not exhausted either by the categories of causality or of randomness (or by both together).
In addition we must consider their meanings .
As I understand it, one can illustrate the proposal in terms of a language.
If we look at an array of words as a self-referential order, we may abstract certain regularities, such as rules of grammar, sentence construction, statistics on the relative frequencies of various letters, words, etc.
But this would leave out of account the main factor that was involved in determining the order of words, which is the meaning of the whole communication.
That is to say, the order of a language is determined in some domain that transcends the language, rather than within the language itself, which has only a symbolic relationship with the domain in question.
Moreover, the regular repetitive features of a language have only a secondary kind of importance, whereas what is primarily important is the meaning which is not related significantly either to a causal order or to a statistically treatable random order.
Shallis's proposal is then that events that happen in the Universe may similarly combine in ways that have meaning, in a domain or context that transcends time as we know it.
This proposal he supports by citing many particular examples of experiences of individuals with coincidences, synchronicity of various happenings, time slip,deja-vu , precognition, omens, divination, and so on.
There is nothing regular or reproducible about these experiences, which have indeed a certain quality of uniqueness.
And, of course, the scientific approach through general laws and formulae has nothing to work on in this sort of context.
Nevertheless, he suggests that a kind of valid knowledge can be developed here, based on careful and consistent description rather than on explanation .
To be sure, science has never found meaning in the phenomena that it studies.
But Shallis argues that this is hardly surprising, because its very terms of thought are such as to exclude it from the content of all discussion, at the very outset.
(Evidently, a study of language through the categories of causality, chance, measurement, statistics, etc could also miss the all important factor of meaning, in a similar way.)
He seems to be saying that we should open ourselves up to this possibility, and that if we do not do so, we may miss something that may ultimately be much more important to us than all that science is able to capture in its net.
Those who accept the general orientation of modern science may well find considerable difficulty in coming to grips with this main point.
Nevertheless,On Time is definitely worth reading seriously, because it may indicate a way of thinking that does not totally exclude the question of meaning, not only from the studies of phenomena of the kind discussed here, but also from all sorts of other studies, in such diverse fields as biology, psychology, and human relationships.
Thus, we may be helped to avoid the trap of ruling out as irrelevant, even before we start, whatever forms of knowledge may not fit into the currently accepted general scientific approach to reality as a whole.
A major challenge to schools
New information technology in education by David Hawkridge,Croom Helm, pp 238, £11.95 
Robert Campbell
THE application of information technology to education represents a major market for the manufacturers of microcomputers and the producers of software or ‘courseware’.
It also represents a major challenge to educationalists: not only should children be taught to use information technology, a subject that has emerged suddenly from nowhere with no teaching tradition, but all disciplines could profit from incorporating this new technology into their teaching practice.
There are plenty of books on the applications of information technology and several quite good introductory texts on computing for courses in schools but this is the first to take a general look at information technology in education in its broadest sense.
This important subject needed a balanced up-to-date review and David Hawkridge, professor of applied educational sciences and director of the Institute of Educational Technology at the Open University, has supplied one with great clarity.
The potential readership of this book is huge and it is, therefore, a pity that it is available only as a hardback.
We must hope that the publishers will bring out a paperback.
Hawkridge starts with survey of new information technology and its jargon aimed at readers new to the field, and which could be skipped by those who know about it.
The second part is a review of examples of how the technology has already been used in education.
His emphasis is, naturally, on microcomputers: but videotext, videodiscs and other technologies are also considered.
In places the text is a little unsatisfactory but this reflects the early stage of development rather than the author's own expertise.
In the third section, Hawkridge discusses the issues arising from the work reviewed in the previous part, revealing many problems and constraints.
He then enjoys himself at the end of the book, looking to the future from various standpoints.
The difficulties in making forecasts for a fast-growing technology can be seen here.
Professor Hawkridge quotes, for example, a 1981 prediction that the number of installed microcomputers in the United States in 1984 will be 3 million of which 750000 will be in homes and 300 000 in educational institutions.
This far exceeds more recent estimates for 1983 given at the On-line conference in December.
But, generally, one of the strengths of the book is its unusually current information, despite the time it must have taken to produce the book.
This was achieved in places by using reports from newspapers and magazines, which can be a weakness for such reports are often unreliable.
The statement that the Adonis project for supplying copies of scientific articles on demand is now operating in libraries in many countries (taken from the Observer)is unfortunately erroneous: this system is unlikely to be operational until mid-1984.
The slightly naive view of copyright problems could be a minor criticism but generally the treatment is authoritative with the coverage stronger on schools than higher education.
A serious concern of the author's is that governments' commitment to education, so strong up to the mid-1970s, has waned.
This has left large international organisations and other commercial interests to take the initiative in information technology for schools without the necessary control to ensure balanced development with universal provision of essential hardware and courseware.
David Hawkridge finishes by stating that all of us who declare an interest in education should increase our understanding of the new technology, and whether education can take advantage of it will depend on us.
With books like this we shall be much better equipped lo make a contribution.
A scientific case against dieting
The dieter's dilemma by William Bennett and Joel Gurin,Basic Books, pp 314, $14.95 
Katy Bridges
MANY people feel that they are overweight.
Yet in the long run, diet failures vastly outnumber the successes.
Predominant myths in this society are that thin people are somehow better than fat people — in health and in image — and that a failure to diet comes from weak will or subconscious fears.
In The Dieter's Dilemma , William Bennett and Joel Gurin, two respected (and thin) science writers, set out to dispel these myths and to explain, with the help of science and common sense, why it's virtually impossible to lose weight through dieting.
The authors put it to us that every individual has a comfortable, or ‘setpoint’, weight, around which our bodies naturally fluctuate.
The setpoint is probably maintained by a chemical substance, perhaps a hormone, that is released by fat cells and monitored by the brain.
The setpoint mechanism is likened to a thermostat.
Upon weight gain or loss (in this case during dieting), the setpoint device pressures the conscious mind to change behaviour so as to reattain the set body weight.
This pressure may be experienced as agitation or inclination to eat more.
As dieting progresses, the feeling hardens into voracious hunger; restlessness gives way to energy-conserving lethargy.
Long-term dieters, although thin, are often under permanent stress from their bodies, a stress that is relieved only after compulsive ‘binges’ help regain the lost kilograms.
Thus, dieting can be bad for the body and the mind.
Reviewing other medical studies of large populations, the authors dispel another myth, that fat people consume more calories than thin people.
In addition, counting calories to balance daily metabolic activity, a popular dieting trick, is useless, as the setpoint mechanism can override the conversion of calories to fat or energy.
Assuming that dieting is a fruitless and frustrating task for people who think that they are too fat, the alternatives are fairly limited.
Cigarettes and amphetamines ‘turn down’ the thermostat-setpoint mechanism, but weight is regained when these are discontinued.
Exercise can also lower the setpoint, ‘and it is not simply a matter of burning more calories or raising the metabolism’.
Having described the setpoint theory in convincing detail, Bennett and Gurin then put it in a historical context.
One of the most interesting chapters of the book concerns the evolution of setpoint and its advantages to survival.
In another chapter, they show that slightly fat people run no greater risk of illness or death than thin people — indeed, the reverse is true.
Then follows a fascinating history of dieting and a scathing analysis of contemporary diet aids.
Throughout the book, each opinion is backed up by reputable studies or everyday examples.
Although The Dieter's Dilemma occasionally reads like a journal article, choc-a-bloc with references, it is alleviated by a good sense of humour and an active communication with the reader.
More importantly, the roles of hormones, fat cells, muscle tissue, glycogen and other factors involved in body weight regulation are clearly and accurately described.
These medical aspects are sorely needed in most of the current bestseller diet books.
Unfortunately, the inclusion of these issues may make it difficult for a dieting lay-reader.
This should not be taken for ‘yet another diet book’; if anything it is the opposite.
The authors close: ‘The endless quest for thinness has done far more harm than good.
For many people, it has represented an expensive, unnecessary, and unsuccessful war against the body.
A truce is long overdue.’
Although a whole fashion cannot be reversed by a single book, Gurin and Bennett provide a scientific analysis of dieting and why most people shouldn't do it.
From science to metaphor
Psychological life by Robert Romanyshyn,Open University, pp 209, £11.95 
Neil Bolton
THIS IS a delightful book, for it succeeds in being both scholarly and entertaining.
Drawing from the phenomenological philosophy of Maurice Merleau-Ponty and Ortega Gasset and from the history of science, the author sets out to define the nature of psychological life which, he believes, has been obscured by too-ready an acceptance of psychology as part of the natural sciences.
His conclusions are original and thought-provoking, and deserve to be widely discussed for they challenge the major assumptions of traditional academic psychology.
Robert Romanyshyn's central thesis is that psychological life is not part of the material world or of an inner subjectivity separate from that world but exists as a form of mediation between the two.
We can understand this by considering a person's reflection in a mirror which is experienced as a re-figuring and deepening of the reflected image: the depth of the reflection, for example, shows that the reflection is not the mirror itself, although, clearly, without the mirror there is no reflection.
In this case we may say that the reflection shows itself through the mirror, and this illustrates the general principle that consciousness can be understood only by an examination of how it reveals itself through the material world.
Likewise, what we find in the reflection is not a person, by which Romanyshyn means the empirical events that define an individual's life, but a figure in a story.
Thus, the true nature of psychological life is the ‘refiguring’ of the world of fact as story and the person as a figure in it.
Another way of stating this thesis is to say that psychological life is essentially metaphorical in character.
As Ortega and Paul Ricoeur have argued, a metaphor affirms and denies an identity between two things: in likening a girl's cheek to a rose we may be impressed by the similarity but are aware of the difference.
Indeed, the awareness of the difference is crucial to the power of the metaphor.
William Harvey's metaphor of the heart as a pump has exerted a profound influence in science because we are aware that the heart is not just a pump.
Now, metaphor is central to the life of the mind because this joint assertion of identity and difference is the means by which consciousness identifies itself as figure, possessing a certain symbolic meaning, in its reflection upon the material world.
Metaphor is a perspective which re-figures the world and the person's experience of himself in the world.
A striking metaphor is always a disruption of the habitual and conventional.
This argument leads to two main conclusions.
First, Thomas Kuhn's paradigms receive a psychological rationale: a major metaphor, such as the pumping heart, signifies a new perspective on man and his world which can be understood only from within that perspective.
Secondly, influenced by the new metaphors of natural science, psychology has distorted the nature of psychological life by subjectivising and literalising experience and it needs to recover the metaphorical character of psychological life.
In this respect, Romanyshyn's work is a rich extension of Edmund Husserl's critique of psychologism, the reduction of experience to its subjective aspect.
Scientific psychology has remained indifferent to this critique for the past 80 years.
Romanyshyn's exposition of the critique is, however, forceful and lively and may stimulate those working within the traditional paradigm at least to justify their indifference, if not to question their perspective.
Herbivores break through cell walls
Nutritional ecology of the ruminant by P. J. van Soest,O & B Books, pp 373, $30 
Digestive physiology and nutrition of marsupials by I. D. Hume,Cambridge UP, pp 256, £25 
Andrew Loudon
SOME 70 million years ago at the end of the Cretaceous and beginning of the Palaeocene the climate of the Earth underwent a profound change.
As the climate became cooler and more seasonal, the dominant vegetation of the Cretaceous, the gymnosperms, was replaced by an extensive radiation of angiosperms, ancestors to today's modern flowering plants.
These plants probably had high levels of insoluble carbohydrates in their cell walls, principally cellulose, and may also have produced a wide variety of toxic plant secondary compounds.
Cellulose represents an important potential energy source for herbivores but the breakdown of cell fibre presents herbivores with difficulties.
Unexpectedly, vertebrates apparently do not produce the enzyme cellulose and the process of cellulolytic degradation can occur only with the aid of populations of anaerobic bacteria in specialised fermentation chambers in the alimentary tract.
Thus, the development of the angiosperm flora of the Palaeocene was paralleled by an extensive radiation of specialised herbivorous mammals with the capacity to ferment and break down tough plant material.
These two books are about the ways in which two important groups of mammals — the ruminants and the marsupials — utilise different food resources.
Although written from the different viewpoints of the agriculturalist and the physiological ecologist, both books have running through them the strongly common theme of comparative physiology and nutrition.
Peter van Soest starts Nutritional Ecology of the Ruminant with a brief historical resume of the study of ruminant nutrition and the development of food analysis and then moves through a series of chapters which describe with great clarity the mathematics involved in the measurement of throughput rate and digestibility.
The centre section deals with gastrointestinal fermentation and metabolism leading to a final section on the whole animal.
It is, however, more than just another good textbook on ruminant nutrition for students of agriculture.
It represents a major synthesis in what has become an increasingly complex field and, because the approach is founded so solidly on comparative nutrition, should be of interest to biologists who work on wild and domesticated herbivores.
The book is perhaps especially valuable for the way in which the author is prepared to extend onto the thin ice of speculation and comparison with other animals, a trait sadly lacking in so many agricultural scientists.
In particular, the final chapter presents an intriguing model for the theoretical upper and lower limits to body size in ruminants, based, as the author cheerfully admits, on some speculative assumptions but surprisingly accurate nonetheless.
Van Soest has achieved a considerable feat in guiding the reader through the complexities of ruminant nutrition with great clarity and at times a fine sense of humour.
For instance, the section on nitrogen metabolism is prefaced by that famous anonymous ditty which starts How inoffensive are the faeces Of all the graminivorous species, That grind on grain and graze on grasses.
Like sheep and horses, mules and asses…and finishes: From sulphurous eggs and oily fishes, And all the highly seasonal dishes: such is the  odourous part of a man Devoted to this frying pan.
Ian Hume's Digestive Physiology and Nutrition of Marsupials is the first in a series of Monographs on Marsupial Biology edited by Hugh Tyndale-Biscoe.
The main emphasis of Hume's book is the inter-relationships between nutrition, reproductive performance and life histories of marsupials.
He starts with a clear discussion of the problems of temperature regulation.
metabolic rate and water metabolism followed by two short chapters on carnivorous and omnivorous marsupials.
However, the great strength lies in the detailed consideration of the nutrition and ecology of the marsupial herbivores.
To all those who think that the only herbivorous mammals of Australia are kangaroos, and introduced sheep, this book will come as something of a revelation.
It is in short a most lucid account of an extraordinary assemblage of mammals and there are many fine stories.
For instance, many of the small arboreal folivores (leaf eaters) are able to eat diets of eucalyptus that contain up to 30 per cent of phenolic compounds (enough to kill any domesticated herbivore) and the description of their nutritional physiology is quite fascinating.
The final part of the book is concerned with the nutrition and ecology of the main grazing mammals of Australia, the kangaroos and wallabies, and the author draws detailed comparisons between data obtained from nutritional measurements made in captivity to the nutrition and performance of wild populations.
This is a fine book with a message for nutritional physiologists and ecologists, well illustrated with clear figures and diagrams and written in simple direct style.
If the following books in the series are as good they will serve well the cause of marsupial biology.
Create engineers for the future
Who's going to make it? by Engineering Careers Information service, minute film 
Barry Fox
THE RECESSION has shrunk British industry.
So there are fewer engineering jobs, less opportunities for training, and Britain's engineering industry becomes less competitive and shrinks further.
Everyone has their own idea on how to stop the rot.
The Engineering Careers Information service's answer was to spend £35000 on a film to show school leavers.
At a preview it was introduced live by John Butcher MP, Under-secretary of state for Industry, asserting that futurologists who write off Britain as an industrial nation are ‘talking a load of hooey’.
On film HRH the Prince of Wales offers school leavers more reasoned advice.
‘Engineering makes other careers possible,’ he explains.
‘Nurses need equipment, so do office workers and drivers.’
In the film and TV trade HRH is known as ‘One take Charlie’.
His polished performance for this film was shot in a single take, in just half an hour in Kensington Palace.
If he decides against becoming King, Prince Charles would make a slick TV anchor man.
The body of the film looks at four young engineers.
Paradoxically the choice of subjects only goes to prove that most new jobs in British engineering are paid for by defence contracts.
A girl working on quality assurance at Thorn EMI is checking radar circuits.
An Asian boy at Fairey is helping to make Harrier strike aircraft.
A young stress engineer, at British Aerospace, is working on satellites.
Only a young development engineer, tucked away in a corner of a small private factory, is developing a wholly peaceful gadget.
It injects insulin automatically into the bloodstream of a diabetic.
The film, although hammy in places, will certainly stimulate interest in engineering among school leavers.
But is the stimulus needed?
Most school leavers are surely already desperate for a job.
‘What industry needs is money to create more of these jobs’ said an engineer in the panel discussion that followed the preview.
But by then John Butcher, the man with the power to make such things happen, had left for another appointment.
Intelligible minds and fair's fare
If I had something important to say on television I don't think I would choose a Sunday afternoon.
It is a time when people, putting off the reality of Monday morning, don't want to be disturbed.
This, however, is not the thinking — and it may be presumptuous — that moved BBC2 to hit us between the frontal lobes with Jonathan Miller's states of mind (20 February), a series that will march on for 15 weeks in the cause of bringing us up to date with what is happening in psychology.
Now Dr Miller can be a brilliant presenter, the energy manifest in his long, wiry frame making for admiring and compulsive viewing.
But in this case, apart from his introduction, we weren't to see much of him.
We did see a lot of another Miller, Professor George Miller, of Princeton University, a stolid man of undoubted erudition but with none of Jonathan Miller's expository dynamism.
Professor Miller, for such he has to be called for identification purposes, was first asked to give a definition of what psychology was, though the other Miller probably knew what the answer would be.
There were, said the professor, two kinds of science, inductive and deductive.
Psychology was of the first so it was ‘still learning what the definition of psychology should be’.
There was then a complex discussion of reflexes which both Millers enjoyed but wasn't all that easy to follow for the uninitiated.
Professor Miller proceeded to the changes in psychology wrought by the Second World War, which was interesting.
What had happened, he explained, was that psychologists had been pushed into the laboratories to work with other scientists and technologists who were there to develop complex machines needed for more efficient slaughter.
This had introduced them to  mechanisms which suggested a terminology applicable to their own science making it possible for them at last to give the behaviourists, who had been giving those not of their ilk a hard time, some of their own medicine.
So, as Jonathan Miller put it, ‘man came in on the back of the machine’.
‘It's ironic,’ said George.
And so it is.
It's also ironic that the whole discussion could have taken place on radio, which would have enabled one to look for a dictionary or just take a walk round the room.
It should have been very much a psychological Sunday.
Channel 4's Crucible was scheduled to show a programme called Behaving Ourselves .
It didn't happen, which had nothing to do with the state of mind we might have been in after Jonathan, but everything to do with another independent channel showing a related subject.
Crucible sought a way from this impasse by showing Fun Fare (sic).
It was partly about fairs but mainly about Theme Parks, which is what big business has made them into in the US and what they are becoming here.
These parks programme you round the hurdy-gurdies and all with as little inconvenience to the owners as possible.
It had little to do with science in society, and writers Lawrence Moore and Robert Young seemed slightly self-conscious about this, using the words ‘science’ and ‘technology’as much as possible in relation to management, roller coasters et al to compensate.
They also used the word ‘primitive’ a lot.
‘There is a primitive sexuality lurking here,’ the narrator, who was very pleased with his voice, said at one point, catching my interest briefly.
There were many other primitives to come but by the time they arrived I knew it was just an attempt to revive attention.
Actually it was a primitive attempt at passing off a filler as a science programme.
Not fair at all really.
FORUM
The balance of Green power in Europe
Debora MacKenzie takes a jaded view of European elections
THERE will be two big elections in Europe on 6 March.
The Germans choose a new national government.
The French, in city and hamlet, will elect new town councils.
About the only thing the races have in common is ecology parties, the Greens.
Les Verts could make their debut in elected office.
Die Gruenen could hold the balance of power in the federal government of Europe's richest state.
So what?
By far the most noise has been made about Die Gruenen.
They are a collection of communists, environmentalists, libbers and German romanticists, choruses the press.
They have also put members in 7 of Germany's 11 regional governments, and brought down that of Hamburg.
The January polls gave them 7 per cent of the vote, guaranteeing federal seats.
This would not matter, were it not for the apparent demise of the Liberals, who used to hold the balance in the Bundestag and whose defection brought down Helmut Schmidt's government in October.
Die Gruenen seem poised to step into the third party gap.
It is the prospect of national political stability in the hands of the ‘anti-party party’ that is scaring everyone.
But what can the Greens do, if elected?
First, all depends on the Social Democrats getting just enough votes to form a government, but only with Green help.
The Social Democrats have already started to deny that they would ask for this help — a denial spurred perhaps by some mistrust of the Greens by their own mainstream supporters.
The right-wing coalition knows better than to ask for any help from the Greens.
If the Social Democrats do after all cooperate with the Greens, and the Greens hold the balance of power, can they use it?
To do so, they would have to threaten defection, toppling the government, if concessions are not made to their programme.
But the last time the Social Democrats were brought down, in October, the right gained strength.
The Greens certainly do not want that.
Moreover, such a move would probably spell the end of the Greens' day in the political sun.
German memory of Weimar instability, and what it led to, is strong.
Paradoxically, fear of a reprise of dictatorship drives the German electorate to seek political stability, and widespread commemoration of the fiftieth anniversary of Nazi power has revived those fears.
If Die Gruenen seemed to spell political instability, the next election would dispense with Die Gruenen.
So without risking political suicide, all the Greens can hope for is more visibility, credibility, and their campaign expenses back, if they get federal seats.
They may not even get that — the social Democrats have been stealing Green fire with a recent anti-nuke stance.
Moreover, potential Green sympathisers are terrified of the ultra-right Franz Strauss, and will vote social Democrat to forestall a right-wing government.
Die Gruenen had dropped below the 5 per cent needed to enter the Bundestag in the last polls.
There has also been a miraculous rise of the Liberals from the ashes — they may just squeak past 5 per cent themselves.
In third parties, Germans may prefer a low card to a wild one.
Meanwhile in France, reform of the election laws may give Les Verts their first electoral victories.
Instead of the high scoring party forming the whole town council, it will now form half, with the other half split among all parties in proportion to votes.
Apart from creating a catastrophe point where an extra per cent of votes can theoretically send a party from 15 to 66 per cent of the council (the French have never been comfortable with simple democracy) this, also theoretically, gives minority parties a voice.
The Greens won 10–30 per cent in some 1977 municipals, so they expect to get onto some councils.
What will this give them?
Precious little.
With the rules guaranteeing one party a clear majority, it is uncertain what any minority voices will be worth.
Then there is a real question of municipal power, now that socialist ‘decentralisation’ has set up regional councils.
When Paris approved a nuclear plant near the town of Golfech in 1981 and Golfech refused, the socialist —(read Paris)— dominated regional council overturned the refusal.
Why do the Continent's environmentalists, unlike the British or American, need to go to the trouble and expense of political parties at all?
In France, the centralisation of power renders both local action and lobbying Paris fairly useless.
In both countries, press attention varies directly with involvement in mainline politics.
But the political process is risky.
It requires a degree of unification which environmentalists can rarely achieve with dignity.
Once elected, the pressure group spokesman becomes a politician, whose business is compromise, not ideological purity.
Any transition from bearing witness to holding power, however limited, will probably alienate the idealistic Green rank and file from the people they have elected.
Where this will leave either is hard to say.
so there may well be eco-freaks in office in France and Germany come Monday.
Not to panic.
But also not to stop worrying about acid rain or nuclear holocaust, just yet…
Hazards of misunderstanding
‘ALTHOUGH the contribution of the world we live in to the levels of risk is most obvious for accidents, developments in medical thinking throughout the last two centuries have indicated how by modifying the way we live — originally through general improvements in diet and hygiene, more recently through changes in education and social welfare — we can contribute substantially to prolongation of healthy life or its reduction by increasing smoke pollution and cigarette consumption’.
Risk Assessment: a study Group Report The Royal Society, 1983 
Dr H. A. H. Boot and the cavity magnetron
Sir John Randall FRS pays tribute to his collaborator and friend
THE DEATH of Harry Boot on 8 February at the age of 65 removes a distinguished figure from those who contributed in an outstanding way to the successful application of British science to radar during the Second World War.
Boot will always be remembered for his share in our joint invention of the cavity magnetron which at an early stage produced pulses of radiation of many kilowatts at wavelengths of 9–10 centimetres.
Such powers were a necessity for advanced warning, as were the short wavelengths for the much greater precision in the location of targets than was possible with the 11-m waves of the coastal radar stations set up during the late 1930s.
Sir Bernard Lovell FRS has recently vividly recalled in New Scientist the development of centimetric radar systems (vol 96, pp 246 and 315 1982).
Here I wish solely to describe the genesis of the corresponding transmitter — the magnetron — and Harry Boot's part in it.
Boot was educated at King Edward's High School, Birmingham, where he was a near-contemporary of M. H. F. Wilkins (later Professor M. H. F. Wilkins FRS of DNA fame).
Harry obtained his Honours BSc in physics in 1938 and became a research student under Dr P. B. Moon (later Professor P. B. Moon FRS).
There was no lengthy research leading to the discovery of the cavity magnetron.
(Indeed it can be vouched that the idea was worked out in considerable detail one afternoon in November 1939.)
However there were special circumstances: Boot and I together with many others were members of Professor M. L. E. Oliphant's laboratory; and this without question fostered a scientific atmosphere of great energy, excitement and discussion.
As the threat of war increased, so Oliphant's thoughts of building a 60-inch cyclotron for nuclear physics diminished and the involvement with government and its need for microwave sources became paramount.
Words such as klystron and rhumbatron were heard in the land and the papers of William Hansen on resonators were a continual topic of discussion.
The award of an Admiralty contract to Oliphant's laboratory for the production of centimetric wave (CW) transmitters made it virtually certain that the principal topic of research would be the klystron, a then recent brilliant invention by the Varian brothers of California.
And in a very short time James Sayers had constructed a continuously pumped CW klystron operating at about 20 kV anode voltage.
Boot and I had been allocated the less spectacular task of making and using miniature Barkausen-Kurz tubes as possible receivers.
But we were unsuccessful and our thoughts turned to other possible centimetric wave transmitters.
For it seemed to us there might well be problems with klystrons, such as, for example , focusing a suitable beam-current through the klystron cavities and drift tube.
We also considered sparks and split-anode magnetrons, especially the latter as their geometry would allow large input power if oxide-coated cathodes could be used.
A magnetron would almost certainly give us power.
But how could the advantages of the klystron with its enclosed resonators be combined with the more favourable geometry of the magnetron?
How Harry Boot and I achieved our end is shown in the photograph of one of the original anode blocks machined in December 1939.
This reveals neither cathode (tungsten 0.75 mm diameter) nor output loop, but makes clear the method of incorporating the coupled resonators into a small solid copper anode block.
Note that these resonators were cylindrical extensions of Hertzian loops, the resonant wavelength of which had been calculated by Hector Macdonald of Aberdeen in 1902 as 7.94 x the diameter of the loop.
Our cavities were 12 mm in diameter and the slots I x I mm in section.
The specifications of November 1939 became a reality just 43 years ago, when a CW pump evacuated model with sealing wax joints was switched on for the first time at the end of February 1940, and operated successfully.
Visible streams of what turned out to be high frequency radiation flowed from the output lead and burnt out all the de-capped car headlamps we could find!
A little later we used a Lecher-wire system to determine the wavelength at 9.8 cm.
From then on, development of the magnetron became a cooperative effort between university and industry in the UK: E. C. S. Megaw of the Research Laboratories of the GEC was let into the secret and went back to Wembley to construct the first sealed-off model.
In September 1940 the Tizard Mission, including J. D. Cockcroft, E. G. Bowen and R. H. Fowler, visited the Us and showed the magnetron to the authorities and to the Bell Telephone Company who got it working in a single weekend.
This led to the founding of the MIT Radiation Laboratory solely for magnetron and radar research, and to the employment of many American electronics companies in its manufacture.
Such distinguished theoreticians as Douglas Hartree and Edmund stoner in the UK and John Slater in the US spent most of the war in working out its mechanism.
Although by mid-1941 peak powers of about 50 kW at 10 cm wavelength were obtainable, upper limits to power and efficiency were set by mode jumping.
This was overcome by James Sayers who carried out ‘cold-test’ resonance experiments in order to measure the principal resonance frequencies of the Cavity system.
As a consequence, Sayers introduced the concept of metal straps between alternate segments which allowed the achievement of still greater power at  efficiencies of some 50 per cent.
There is no question that the cavity magnetron had a crucial bearing on the outcome of the war.
Perhaps I may be allowed to paraphrase for brevity from Scientists against Time by J. P. Baxter in his official account of the American Office of Scientific Research and Development:
‘…a radically new and immensely powerful device which remains the heart of every modern radar.
(It) was the first tube capable of producing power enough to make radar feasible at wavelengths of less than 50 cms.
When the members of the Tizard Mission brought one to America in 1940, they carried the most valuable cargo ever brought to our shores.
It sparked the whole development of microwave radar and constituted the most important item in reverse Lease-lend.’
It is to the cavity magnetron that Harry Boot made his greatest contribution and by which he will be long remembered.
Harry and I collaborated closely from 1939–43, discussing every step together.
Consequently it was as hard then as it is now to separate our various individual contributions.
I do not think outsiders realised this sufficiently.
He played a vital part in the whole development both with his ideas and his supremely practical outlook: no setback discouraged him; a man of great integrity, discerning in his judgments and withal a sardonic sense of humour.
In recent years we met less often and yet he could conjure up an incident which made us both feel it had all happened a few days ago.
He later worked on high-power magnetrons both at Birmingham and at SERL, Baldock, where he became a senior principal scientific officer.
He is gone too soon and will be sadly missed by his family and friends.
Axe hacks' snacks
One hard-pressed organisation seems  determined to keep up its standards despite its financial problems in these hard times.
The British Association for the Advancement of Science insists that it will be ‘business as usual’ for the press at this year's annual meeting.
That means pouring food and drink down the throats of the journalists — and odd hangers on — who take the trip to Brighton this August.
This is an odd thing to do as the company sponsoring the press facilities in recent years, ICI, has withdrawn its support.
On more than occasion New Scientist — and at least one science editor from a leading newspaper, to our knowledge — has told the BA that the press can afford to pay for their own drinks and for those of their less wealthy friends who don't have the same access to expenses.
All the journalists need in the way of facilities are typewriters, telephones, and copies of the papers to be presented at the meeting.
That means not only that the BA could save money, but that it could make a bob or two by catering for the assembled hacks.
It seems that our advise has been ignored.
A new sponsor is in the winds — Robert Maxwell's Pergamon Press — but even if this verbal offer comes to nothing, the BA will blow its own hard-won cash on food and drinks.
The association would do better to invest some of that money in advertising to attract a larger audience to the annual meeting.
Freud's fatness test
Colin Brewer finds that the scalpel is oft mightier than the couch
ONE OF THE basic principles of the psychodynamic school of psychiatry (founder S. Freud) is that things are not what they seem.
Even though the old man himself is supposed to have rebuked one of his more imaginative pupils with the phrase: ‘There are times when a cigar is only a cigar’, his disciples have generally been reluctant to embrace the more prosaic and parsimonious explanations for life's little difficulties.
One of the commoner of these difficulties is obesity and some people are so fat that the term ‘superobese’ has been applied to them.
Although most fat people simply eat too much by normal standards, there are many who are equally gluttonous but remain thin, and some fat people remain fat in spite of eating much less than normal.
This suggests that individual variations in physiology and metabolism must be an important factor.
It might also suggest that if fat people have psychological problems, these may well be a result of the obesity, or at most a contributory factor.
Naturally, for the people who gave us oral fixation and penis envy, among other contributions to psycho-babble, this is far too simplistic an idea.
For them, obesity is seen as a reflection of unmet oral needs, assorted sexual hang-ups, or an unconscious desire for recognition which is satisfied by becoming bigger than everyone else.
More important than these theories of causation are the ideas about treatment which flow from them.
If, as the psychodynamic school believes, obesity is fundamentally a psychological problem, it follows that treatment should ideally be aimed at the mind rather than at the body, and that treatment aimed at the body will leave the underlying psychological problem unaltered or even aggravated, similar objections were and still are levelled against behavioural treatments which allegedly deal only with ‘symptoms’, leaving the underlying problem to spring up anew.
Unlike many psychodynamic theories, this one is at least susceptible to proof or disproof and it has just been comprehensively disproved.
Of all the treatments for obesity, none could be more physiological or less psychological than the operation known as jejeunoileostomy or ileal bypass, in which most of the small intestine is short circuited thus greatly reducing the amount of intestine available for absorbing food.
It results in very rapid weight loss in most cases and generally the weight stabilises at around the ‘ideal’ level.
It is a major procedure with a small but not insignificant mortality rate and some patients experience severe and persistent side-effects, although many of them seem to regard these as preferable to being ‘superobese’.
Jejeunoileostomy has now been largely replaced by a simpler and safer operation in which the capacity of the stomach is considerably reduced by placing a row of surgical staples across it.
However, some American researchers (P. Castelnuovo-Tedesco et al.
Amer ,J. Psychiat .
vol I 39, p 1248, 1982) have followed up 24 patients who had bypass surgery, some of them as long as 12 years ago, and none less than four years.
They report that in general, the psychiatric outcome was good and that the patients who were not happy about the result were those few in whom the operation had not led to the expected weight loss.
Furthermore, although there was no shortage of surgical complications which might have formed a focus for psychological discontent, the researchers noted that ‘even those who had experienced complications tended to underrate their seriousness and to express satisfaction.’
Those patients with a good postoperative result ‘maintained it for many years, gained occupationally and vocationally, were pleased with the results of the operation, and were willing to recommend it to others’.
Surgery for superobesity is obviously a last resort, but this study confirms the conclusion of earlier short term follow-up studies which found a good psychological outcome even in patients with a history of psychiatric treatment.
‘Symptom substitution’ was notable for its absence, and the ‘underlying psychological problems’seem to have been a result rather than a cause.
Fighting the common code
CODE FEVER is loose in Geneva.
Ever since the World Health organisation approved an infant-formula marketing code in 1981, everyone else wants one too.
It happens like this: a pressure group asserts that promotion of product X causes health hazards and demands a marketing code.
The industry howls, then writes its own code to pre-empt any that could change its marketing practices.
WHO throws up its hands at the whole, loud confrontation.
For ‘X’ read pesticides, pharmaceuticals, food additives, cigarettes, alcohol, blood, et al.
The question is whether UN agencies can form a check-and-balance to the multinational corporation that has yet to admit that profit is not (quite) enough to control activities affecting millions of lives.
Who tells Nestle not to promote the bottle feeding that may give a slum child fatal diarrhoea?
WHO?
Well, it tried.
But after 18 months of the infant formula code, WHO executive chairman Maureen Law wonders if all the fuss was worth it.
Little has been done by countries to implement the code and industry's feathers remain ruffled.
And WHO needs industry cooperation, especially in drug research.
A secretariat staffer involved with the latter says multinational industry should police itself.
And if it doesn't?
Well, he doesn't know who else could…but please don't write that WHO wants to confront industry.
All right, we won't.
But we know who does want to.
The infant formula brouhaha created a monster which may yet check and balance the dread corporation: the multinational pressure group.
If nothing else, the formula code gave protesters an effective rallying point.
But other righteous causes are going to have to look for another way to organise.
WHO certainly isn't going to write any more codes for a while.
Case of the midwife toad revisited
Robin Dunbar offers a new interpretation of some strange claims
THE history of science is a sequence of conflicts between incompatible theories in which each side vigorously defends its own theory until conclusive evidence persuades a majority to accept one view.
The discredited theory finally dies out, as the German physicist Max Planck once remarked, with the demise of its remnant adherents.
Yet not all instances of theory replacement are quite so straightforward, for some have rumbled on for decades.
Few, however, can have been as tragic as the case of the midwife toad (Alytes obstetricians)and its reverberations in the 1920s.
The centre piece of the act is the onymous toad, unique among the frogs and toads because it has lost the horny pads (the so-called nuptial pads) that grow on the male's hands during the breeding season and because it mates on dry land rather than in water.
Its name derives from the fact that, instead of the eggs being spawned in water and left to their own devices, they are carried by the male wrapped in strings around his legs.
Loss of the nuptial pads is usually assumed to be a consequence of mating on land because, in other frogs and toads, their function appears to be to help the male to grip onto the female's slippery back while mating in water.
The midwife toad came to public notice when, in the early years of this century, the Austrian biologist Paul Kammerer announced that he had persuaded the toads to breed once more in water and that, as a consequence of this, they had re-evolved nuptial pads.
This claim was thrown at a biological community that had come down heavily only recently in favour of Gregor Mendel's theory of inheritance.
Mendel's theory argued that only characters already present in the genes (or introduced into the genetic material through random mutations) could be inherited.
If one accepted this view one had to reject its main rival, the theory of the inheritance of acquired characters proposed by the French biologist Jean Baptiste Lamarck.
Lamarck, and many of his contemporaries (including Charles Darwin himself in the later stages of his life), supposed that repeated use of a trait could itself change the genetic material that was responsible for the transmission of that trait so that characters acquired or improved by repeated use could be inherited directly by the offspring.
Now, just as the long-standing debate had been settled, here was Kammerer adducing experimental evidence in favour of Lamarck's discredited theory, for he was claiming that the pads had been ‘re-evolved’ in a few generations as a direct consequence of the parents being persuaded to mate once more in water!
It was too much.
The ensuing debate, however, does little credit to the Mendelians, the more bigoted of whom refused even to believe in the existence of the pads despite the eye-witness testimony of a number of biologists who were later to become household names in their own rights — a list of eye-witnesses that includes British biologists as diverse in their biological contributions as J. B, S. Haldane, W. H. Thorpe, L. Harrison Matthews and G. E. Hutchinson.
A vigorous, and at times vitriolic, debate rumbled on in the pages of Nature and elsewhere, culminating in 1926 with an accusation by the American biologist Gladwyn Noble of outright fraud.
Noble had been allowed to dissect the last of Kammerer's nuptial pad specimens (the rest had been destroyed some years earlier in a fire).
He asserted that the black pads had been manufactured through injections of Indian ink under the skin.
Kammerer's protestations that the specimen must have been ‘touched up’ by an over-zealous museum assistant notwithstanding, the Mendelians rested their case, well assured that truth had prevailed in the face of religious fervour.
Kammerer was left to ponder a life's work in ruins.
The shame was too great and he shot himself on an Austrian hillside a few weeks later.
What is truly bizarre about this tragic episode is that the Mendelian camp had, in Kammerer's extraordinary results, dramatic evidence in support of their own theory.
It seems that they were side-tracked by Kammerer's Lamarckian claims and forced into the position of assuming that, since the theory was wrong, the data had to be wrong too, and this left them with fraud as the only possible explanation.
The clue is given in one of Kammerer's rare personal replies to one of his critics, a certain Dr G. Boulenger of the British Museum, who had reported the complete failure of an attempt to replicate the midwife toad experiment.
The eggs had all died before hatching, and Boulenger was led to question whether Kammerer had ever managed to get eggs to hatch at all, let alone re-evolve nuptial pads in adults.
Stung to the quick, Kammerer retorted that: ‘Boulenger was lucky for apparently his eggs developed normally for as long as 5 days before they died.
For as I have already explained…
, I would have expected that under these ‘natural’ conditions [that is, trying to raise the eggs in pond water]the eggs would have been totally infested with fungi…
Each affected egg must be carefully removed, cut away from the ball [of tangled strings]with a pair of fine scissors, and the sterilisation of the whole container must be repeated.
In spite of these precautions I had to throw away many more balls, after the embryos they contained died, than Boulenger has ever handled ’(my italics).
Significantly, Kammerer goes on to add: ‘Later on,…the results improve:in successive generations the mortality of water-eggs is hardly greater than that of other anura who depose their eggs normally in water ’(my italics).
(Translation taken from Arthur Koestler's The Case of the Midwife Toad , Pan Books.)
The clue is in the italicised sentences.
Kammerer must have been inadvertently imposing massive selection pressure on the eggs, weeding out all those eggs that were unlikely to survive and develop in a watery environment.
Those eggs that did survive would have done so only because they contained, as hang-overs from the species' earlier aquatic way of life, genes pertinent to a more aqueous existence, including perhaps the genes for nuptial pads.
Kammerer must have been imposing such intense selection on such a large sample of eggs that he was able to filter out the odd egg that carried the remnant complex of genes still present in the species' gene pool at very low frequencies.
That this must be the explanation for Kammerer's disturbing results is attested to by the discovery of a wild midwife toad with rudimentary nuptial pads as much as two years before Kammerer took his own life.
Unfortunately, the report had been published in one of the more obscure German journals, and none of the protagonists in the Kammerer debate was aware of its existence.
Had they known of it, the course of events might have been different and a tragedy averted.
Paul Kammerer had unwittingly fallen into an invidious trap, partly as a result of the circumstances of his times and partly as a consequence of the vicious narrow-mindedness that thinkers of every age and generation are prone to in defence of their personal commitment to particular theories.
What is instructive in terms of the psychology of science is the willingness with which Kammerer's detractors were prepared to resort to fraud as an explanation for facts that had been misinterpreted as support for a counter-theory, facts which, at that point, their own theory was insufficiently well developed to account for.
It is here, in the process of the replacement of one theory by another, that the greatest danger lies.
For the adherents of the new theory are faced with the prospect of having to abandon a theory that they feel to be right in the face of evidence which, at that stage, they are unable to reconcile with the new theory — when in fact in a decade or two they might well be able to do so.
The response at such times of frustration is not infrequently a response governed by emotion alone.
Obvious is best
SUCH is the glamour of television that when the BBC's news department wanted a new science correspondent for TV, the corporation's advertisements produced nearly 400 applicants.
The advertisement — as featured in New Scientist — said that the BBC wanted an experienced journalist, but that did not stop a flood of academics from applying.
And after all that effort they chose the most obvious candidate — a man with experience of television and the BBC — James Wilkinson.
He is, of course, the corporation's watcher of science (along with medicine, aerospace, and aviation, not to mention street sieges when he happens to be in the wrong place at the wrong time) for radio's news department.
‘The airstrip now landing at Port Stanley…‘
Tam Dalyell follows a fearful night of fancy
WHERE is Britain going to build its next major airport?
Are we to replace the sands of Maplin with concrete plains?
Will Stansted succumb to major expansion?
But, hold on — here's a new option we should consider: Port Stanley.
Until last week this latter option might have been considered a possible piece of government lunacy.
But that Monday's Panorama programme revealed it as a piece of lunacy under definite consideration.
My old friend Fred Emery, who presented the programme from the Falklands, told us — and I have confirmed this from other sources — that the Cabinet is struggling to decide whether to build a new airstrip alongside Stanley's existing facilities or, as has been hinted at in the Commons, on an entirely new site.
Either way, it was asserted, the cost would approach £350 million and the whole project could take on the same proportions as providing London with its third airport.
I record the exchange between Emery and General David Thorne, the officer commanding in the Falklands.
‘In theory,’ said Emery, ‘defence costs might fall once there's a proper strategic airfield.
But building one is going to cost an enormous amount, and produce an asset which we will not find it easy to leave.
The original airstrip was designed only for short nights to the mainland in the days when the Foreign Office was encouraging the seduction of the Falklands by Argentina.’
It was then explained that the present airstrip was improvised by the engineers as a front-line RAF base to take the Hercules, the slow transport which drones between Ascension Island and Stanley at £250000 per trip.
The big Tristars, crucial to large scale reinforcement just cannot get in.
General Thorne felt that he would rather defend one airport than two.
If he had two, his force level would inevitably have to go up.
Three days after the riveting Panorama, the £350 million had been upped to £880 million by the Daily Express on the assumption that it would be built by contractors.
(Or downed to £110 million if built by Sappers, another 1000 of whom would have to be recruited.)
I must call a halt here to brain-numbing estimates of expenditure.
Estimate, instead, the sheer physical task that is being contemplated: to build a ‘Stansted at Stanley’ would be to do something that has never been done before in the whole of history.
Never has anyone even considered building an international airport, moving all the necessary supplies over a distance of 13 000 kilometres between hemispheres.
Now we are asked to consider this proposition in the face of local opposition which ranges from sullen resentment to armed threat — said local opposition being only some 25 supersonic minutes away for that bogeyman of the south Atlantic airways, the Super Etendard jet fighter.
I would so like to be the proverbial fly on the wall at those Cabinet subcommittees which are currently engaged on passing judgement on so far-fetched a proposition.
Have they any notion as they sit around Downing street of the horrendous costs involved for a nation that finds difficulty in paying its water workers properly?
What kind of technical briefing are they being offered?
Are they told, for example, that it is near impossible to build a sound airstrip for heavy transport jets in conditions of wicked cold and chilling damp, such as the Falklands suffer for seven months of the year?
Do they understand that for years to come, our treasure is to be poured into maintaining round-the-clock, full-alert, protection of the airstrip works, at a time when we are seriously discussing the future of student loans, and putting the millstone of debt round the necks of our students in higher education?
There are a number of high technology problems that need answering.
But there is also the simple problem of water.
In March and April 1982 well-meaning planning experts back in Whitehall had obviously jumped to the conclusion that in the wet, cold, boggy desolation of East Falkland, dehydrated Arctic rations would be the ideal fare.
All they needed was water to reconstitute them and there was definitely going to be no shortage of that particular commodity.
Alas, the conclusion jumped back.
There is indeed a great deal of water in the Falklands.
Most of the time it falls on the troops from the sky, while they try to sleep in some half-filled trench.
But the water becomes virtually undrinkable on contact with the soil because of a local parasite called liver-fluke, which creates growths on that particular organ of the unwary and thirsty, if imbibed.
Sterilisation tablets, boiling, and straining leave the dreaded fluke unruffled.
It is a strange business to build an international style airport on an Ultima Thule, to which fresh water is going to have to be brought for the foreseeable future from the northern hemisphere.
For pity's sake, before we commit ourselves to ‘Stansted at Stanley’, let's pause and contemplate the costs and the consequences!
LETTERS
Medical quarrel
As former colleagues of Drs A. C. Allison and I. A. Clark we were dismayed to see their relatively minor quarrel aired in public (Lancet .
12 February).
Having read your disgracefully biased article ‘Cheating charge rocks malaria’(This Week, 17 February, p 423) we feel we must protest.
Statements like ‘malaria specialists have rallied round Clark’, and ‘nine other scientists…. back the former pupil’ are grossly unfair.
Firstly it should be said that the contention is over the origin of an idea.
The dispute involves speculative claims far beyond the available data.
The idea is not ‘new’, but is a limited aspect of the role of free radicals in cytotoxicity.
It is certainly a long way from proving ‘enormously significant to…. 200 million malaria sufferers’, as your article claims.
To describe the affair as ‘cheating’ is farcical.
The development of an idea is a tortuous process involving many outside influences.
It is not surprising that two people, who have worked closely together over a period of time, come up with a similar approach to a problem.
To claim priority over an idea in these circumstances is to take a risk indeed.
To side with one or other of the protagonists, as your informants seem to have done, is foolhardy.,
Warts and all
During the 30 years I have lived in the US, as a student and university teacher, I have never seen the large carbuncles and boils described by Robert Mandeville (’ The natural history of boils’, 20 January, p164), although they were common during my school days and army service in England from 1944 to 1952 and I suffered from them myself on three occasions.
Apparently they are still quite frequent in Britain, but surely the really interesting question is why ?
I have always thought boils were a symptom of the first signs of  scurvy , caused by a lack of ascorbic acid, vitamin C. Surely diet, rather than lesions and subsequent infection, is a much more important factor?
Fresh fruit and vegetables are always available here, and are relatively inexpensive compared to northern Europe.
There is a well-established habit of drinking fresh or reconstituted fruit juices.
Do they still boil cabbage to a soupy pulp in Britain, and pour the vitamin C down the drain?
Peter Gould  Pennsylvania  State University 
Fringe medicine
I read with interest Robin Monro's article (’ Medicine beyond the fringe,’20 January, p 151).
He seems to imply in this article that therapies such as acupuncture cannot be tested within the context of controlled trials.
I would like to make two points to dispel this illusion.
Medicine (conventional or complementary) deals with a very significant interface between the psychological and the physical.
This is necessarily part of the unwritten effect involved in any clinical trial.
Secondly, although acupuncture does not tit readily into the currently available clinical trial models, there is no reason why such therapies cannot be assessed within the context of clinical trials, by developing new trial methods to facilitate this assessment.
As a physician  practising several of the complementary therapies I feel that it is wrong to defend their lack of scientific evaluation by making the case that they can't be assessed.
With the appropriate investment of thought, time and money many of these therapies can be evaluated within the constraints of modern science.
To claim otherwise would be to place these therapies on a mystical pedestal, which would clearly open them to justifiable derision by those who doubt their efficacy.
Nuclear facts
In his review of the new Science Museum nuclear power exhibit (Review, 20 January, p 187) Walt Patterson manages to leave the impression that, first, nuclear power plants achieve — only’ 53–7 per cent capacity factors, and secondly that the British Magnox or the Canadian CANDU systems achieve ‘somewhat higher’figures (implying ‘not by much’).
J. A. Brink, from Paris, in a letter published in the same issue, is similarly vague, suggesting that the PWR has an availability of ‘below 80 per cent’.
As Patterson pointed out (but did nothing to rectify) the term ‘capacity’ factor’needs to be explained, and ‘availability’is not quite the same.
Availability is the percentage of time for which the plant is mechanically capable of producing full rated output of electricity, whether or not such output was called for to meet demand at the time.
Capacity factor is the electrical energy actually produced as a percentage of the theoretical maximum possible, 24 hours a day every day.
Availability is less meaningful as a measure of performance since it is not possible to be sure the plant would be available to produce full load unless it is actually in operation, which proves whether or not it is working satisfactorily.
It is certainly not as good as capacity factors as an indication of the value for money the owner gets from the plant: each kilowatt hour the reactor produces helps to pay off the capital invested in the power station.
The weighted average capacity factor apportions a higher figure to better  performance from bigger reactors, since it is better to produce 85 per cent of the rated output from an 800 MW power station than the same fraction from a 400 MW station.
To set the record for 1981 straight, the weighted average capacity factors for the year 1981 were 75.1 per cent for CANDU; 74.3 per cent for all pressurised heavy water reactors (PHWRs, CANDUs plus two other reactor units): 58.0 per cent for Magnox reactors; 53.7 per cent for pressurised water reactors (PWRs); and 53.3 per cent for boiling water reactors (BWRs).
These figures are taken from Nuclear Engineering International , March 1982.
For comparison with non-nuclear plant, comparable coal-fired power stations operating in Ontario in 1981 had capacity factors of about 50 per cent and produced electricity costing some 60 per cent more than the nuclear generated electricity.
School science
We feel obliged to respond to your very critical and ill-informed review of the three school science survey reports produced by the Assessment of Performance Unit (Forum, 10 February, p 391).
We owe this both to the two teams of experienced science teachers responsible for the survey, which we direct, and to the several hundred science teachers who have generously given their time and advice to our teams.
Three aspects are of particular concern.
The first is the reviewer's clear implication that we have not consulted ‘real teachers’.
At an early stage over 1500 copies of a first consultative document were circulated to teachers and others.
A report on the extent and nature of the feedback was included in a second document circulated to all schools in 1980.
In developing both our general policy and our test items, we have had invaluable help from many teachers.
The second concerns the ‘blunders’ that your reviewer sees in our questions.
Each use of symbols and phrases that he quotes has been debated with ‘real teachers’ who have advised us on the choice that will cause the least confusion with pupils.
The third point is the reported absence of conclusions.
We are sorry there was no index to help in the search for these.
The review gives a list of such banal items as ‘older pupils do better…. than do younger ones’ and calls these our ‘shattering conclusions’.
This list is not ours — it was composed by your reviewer.
The conclusions that we think significant are not hard to find.
They are presented in the three closing chapters — one in each volume.
Yellow peril
I wonder if Daedalus, or anyone else for that matter, has thought about the annoying glare given off by those ubiquitous orange sodium street lamps.
They are used because of their alleged efficiency, the two main spectral lines of sodium being emitted at nearly the peak of the colour response curve for the human eye.
But has their subjective efficiency ever been measured?
The lines are most visible on a clear night, in excess of about 25 metres from a sodium light, and are seen to radiate in planes parallel to the main axis of the lamp fitting.
Although the lines are exceedingly fine (ie, just resolved) at this distance, increasing the separation between source and observer can produce very strong lines which account for the glare I have noticed.
These details and a few rough calculations raise an interesting possibility.
The phenomenon of ‘beats’ is well known in acoustics and radio.
The two sodium spectral lines (589 and 589.5 nanometres) could be interfering to make a ‘beat note’ whose modulation is the difference frequency between the two spectral lines.
Such a ‘note’ would have a frequency of about 400 GHz and a corresponding wavelength of 0.7 mm, that is to say it is a microwave of about the same order of size as the ones used in microwave ovens Now the law relating to microwave transmission is very strict, and microwave ovens are suitably shielded to prevent the unwanted escape of potentially lethal radiation.
Has anyone looked at the amount of sodium spectral orange that is transformed to microwaves in this way?
Is there yet, another health hazard in our midst?
Rooting for Ginseng
I am writing with regard to the letter in which John Brunner asserts that taking Ginseng over eight to ten days resulted in acute hypertension and ‘pressure headaches’(Letters, 6 January, p 44).
There is a great deal of sound evidence of the fact that Ginseng normalises blood pressure.
Those suffering from hypertension benefit.
There is no sound evidence of Ginseng inducing harmful side effects.
It would be interesting to know exactly what preparation Mr Brunner ingested.
Analytic tests of products on the market in the UK — and even more so in the US — have shown that many contain no Ginseng at all.
Many include other dubious elements and most, even if they are an extract of the Ginseng root, are useless.
Reputable firms marketing high quality Ginseng will support the fact that no reaction to the plant drug — taken in proper quantities — can be obtained within eight to ten days.
Twenty-eight to thirty days is more reasonable.
Starry eyed
The article entitled ‘Angry astronomers gag their critics’(This week, 17 February, p 424) that you published contains a number of factual errors that need to be corrected.
First, the publication of the paper on the Isaac Newton Telescope was not delayed by ‘political discussions’.
It had to be thoroughly discussed with all interested parties before being submitted for publication, and the speed of publication was about average for this sort of paper.
Secondly, the paper on radio astronomy is not ‘now bogged down in legal proceedings’.
There was some question of possible legal action in the past, but this has been resolved.
The paper is in fact due to be published in Research Policy in the near future.
Thirdly, as regards the Science Policy Research Unit's proposal to assess  engineering research, no member of the unit has claimed that the project was turned down by the Social Science Research Council because of‘lobbying by outsiders’.
Nor is this view held by the unit.
Finally, the paper does not ‘suggest that the new observatory at La Palma…should not be run by the Royal  Greenwich Observatory’.
Rather, it identifies the way in which the decision as to who should operate the telescope was taken and reports that many university astronomers felt the possibility of alternative organisational arrangements should have been publicly debated.
Waste not…
The recent article on sea dumping of nuclear waste by Christopher Joyce (This Week, 10 February, p 352) contains several potentially misleading statements.
For example, although a curie of any radioactive element disintegrates at the same rate as I gram of natural radium (as found in sea-water) there is no connection between this and the relative toxicity of the element as compared to radium.
Radium is readily absorbed into the body where it concentrates in the bone marrow and gives off very damaging alpha particles.
Only its natural decay product, polonium, is more toxic.
Radium is tens of times more hazardous than plutonium and thousands of times more hazardous than tritium.
Tritium is widely used in medical diagnosis and research, and comprises the largest individual component of the UK low-level sea dump.
Again, the imploded drums found at early US sea disposal dump sites were not made to the much tighter standards pioneered in the UK and embodied in current national and international codes of practice.
Those drums have been shown to reach, intact, the much deeper depths required for sea disposal by the London Dumping Convention and the Nuclear Energy Agency of the Organisation for Economic Cooperation and Development.
However, it is expected, and intended, that the radioactivity will slowly escape from these drums and be diluted and dispersed.
Any concentration of radioactivity in adjacent sediments reduces the amount of radioactivity entering the sea since it will allow more time for the radioactivity to decay.
ARIADNE
IN THE longueurs of the Sizewell B inquiry, now retained by public demand at The Maltings in Snape for a long run, there must be some lighter moments or the people concerned would have run screaming into the sea at Aldeburgh by now.
For one thing, as I can report, having kept an eye on proceedings since they disappeared from the newspapers and TV, all the witnesses are affected by the disease of official language.
For example, ‘an unexpected deleterious effect on one component often may be counteracted by adjusting one or more of the other components’.
It is a long way from swings and roundabouts.
Of course, the witnesses, so far mainly from the CEGB, have written what they are going to say beforehand and are described as ‘reading proofs’.
The goings on are hardly lightened by the rule that cross-examination questions have to be submitted beforehand as well, so the answerers know what's coming.
One bemused attender started to shout that he could not understand the answers to such questions.
Sir Frank Layfield, the official inspector, decided that the man was not entitled to interrupt the proceedings even though he was seeking clarification, and he was escorted outside.
Demonstrations have quietened down, I gather, and presumably will not flower again in strong colours until the TV cameras and reporters turn up again and I should think that is unlikely before the objectors start seriously to pitch in.
One good result so far, though, is that so many people were mystified by acronyms being bandied about (SNUPPS, NII, AGR and the like) that the secretariat for the inquiry had to produce a list of them.
I would like a copy, having been puzzled at the outset by TCPA's representative asking questions on DCF.
DON'T look now, but the picture this week needs an explanation and an apology.
It is, of course, really a serious matter but the photograph was too good a subject to ignore.
It comes from RHM Research and it shows how the company rescued valuable 16th and 17th century books that had been waterlogged in a flood at Sudbury Hall in Derbyshire.
The Bodleian Library was asked to help with advice on how to save the volumes and the conservator of books there remembered an after-dinner conversation a couple of years ago with Dr Roger Angold, of RHM.
He had thrown in the idea that accelerated freeze drying might be a good process to use in drying out books without damage.
The seventy eight saturated books were put through it, after trials.
The ice sublimed away, leaving the books, dry and undamaged, to be returned to Sudbury.
Horse-whipping used to be, by some accounts, a favourite method of dealing with columnists, though I have never run across an actual case.
The above explanation is intended to keep blood pressures low, for people have demonstrated astonishing reactions to captioned pictures.
Everyone of my age probably remembers a quotation from an MP in school textbooks exemplifying the mixed metaphor.
’ I smell a rat.
I see it floating the air and I shall nip it in the bud.’
I was reminded of this, fresh as I was from reading some of the slanging that made the Bermondsey bye-election so appealing, when looking at a joint release from six trades unions.
It was protesting about the proposed sale of British Telecom to private industry and the threatened guillotine on the committee stage of the Bill.
Putting on a guillotine, the release said, meant that, like the Gadarene swine, the government was rushing headlong into the daddy of them all.
WE ALL, I reckon, feel a nostalgia for the days before the First World War when we read that travel was simpler and you could go down to Victoria and light out for the Continent without bothering about a passport.
It is not enough compensation to read on your contemporary one that Her Majesty's Secretary of State for Foreign Affairs requests and requires.
So, on the whole I am cautiously in favour of machines inspecting passports at great speed, a scheme experimentally working at Heathrow.
It seems to me to reduce the passport to what it generally is in practice, a not too efficient identification card, of which few authorities take much real notice.
On the other hand there are, in some people's opinions, sinister overtones of being able to keep tabs on a person's movements, though how that differs from the present system I am not quite sure.
Of course you do not have a legal obligation to show a passport to get out of the country; you do have, to get in.
So I think I am right in saying that all that queuing and handing over at desks on the way out is nonsense.
What I find a bit puzzling is that, as far as I can see, automatic inspection of passports will eliminate the keen-eyed, guilt-inducing gaze from the man behind the desk who presumably, is looking for doctored documents, suspicious photographs and forged visas and stamps.
That was the point, I thought, but perhaps machines are more versatile than I have been informed they are.
DAEDALUS
IN THE field-ion microscope, a fine metal point is charged to high voltage in a vacuum.
Ions are repelled from it; they radiate out in straight lines as a sort of radially-expanding ‘image’ of the fine structure of the metal point.
By the time it hits a distant screen, this ion-image has expanded millions of times.
Single atoms can be resolved.
Daedalus now points out that the solar wind is an exactly similar phenomenon.
Ions repelled from the solar surface rush radially outwards.
They too must carry a steadily-expanding ion-image of the Sun's surface.
Since the Earth is about 216 solar radii from the Sun, the solar-wind image must be enlarged about 216 times when it reaches us.
By capturing it, we might hope to resolve sub-millimetre details of the solar surface!
A suitable satellite in high orbit should do it nicely.
At first Daedalus thought of orbiting a huge phosphor-coated plastic sheet on which the solar-wind image would scintillate on TV-screen principles.
But he then realised that the Sun is rotating.
The solar-wind image must rotate too: in fact it is sweeping past us at about 400 km/s.
So a narrow vertical phosphor strip is all you need: rapid luminous sampling will reconstruct the image as it sweeps past.
Daedalus's solar-wind satellite accordingly resembles one of those joke unrolling tube whistles.
Once launched, it unrolls and inflates in space into a long sausage-skin full of low-pressure gas.
A voltage across opposite sides of the sausage makes the whole thing into a huge Geiger-counter.
Solar ions hitting the sunward side of the sausage initiate avalanche-breakdown across the interior, causing a bright flash from the phosphor on the other side.
An associated space-camera relays the data back down to Earth.
Resolution is limited to about the sausage-diameter — a metre or so, corresponding to 5 mm on the solar surface.
Solar theory should be enormously stimulated.
And if ion-winds can also be detected from the stars, their vastly greater distances should give even higher magnifications.
Subatomic details on stellar surfaces could easily be scrutinised!