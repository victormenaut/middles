

EDITORIALS
Cardiac rehabilitation
Should be available to all who would benefit
Cardiac rehabilitation aims to restore patients with cardiac disease to their optimal physical, psychosocial, emotional, and vocational status.
Coronary artery disease remains the main cause of premature disability in the United Kingdom, and patients who have suffered a myocardial infarction form the largest group considered for rehabilitation, although rehabilitation is equally important for patients who have undergone cardiac surgery or suffer from congestive heart failure.
Nevertheless, fewer than half the health districts in the United Kingdom have established cardiac rehabilitation programmes.
Cardiac rehabilitation programmes differ from centre to centre and in patients with different cardiac illnesses.
In general, they consist of exercise, relaxation, and discussion.
Patients join four to eight weeks after a myocardial infarction, although flexibility of timing of entry is desirable.
Both light and heavy exercises have been shown to be of benefit in improving physical conditioning.
Discussion sessions concentrate as necessary on patient education, risk factor modification, and individual counselling.
Programmes are supervised by physiotherapists, nurses, or occupational therapists with doctors and other professionals available as necessary.
They usually run for four to 12 weeks with one to six sessions a week.
The availability of such programmes to cardiac patients seems justified in terms of cost:benefit.
In the mid-1980s in the United Kingdom coronary artery disease was estimated annually to cost £500m in treatment and £1800m in lost production.
It also accounted for 11.6% of sick leave.
The running cost of a cardiac rehabilitation session is only £4-£15 per patient.
The financial benefits gained in terms of productivity and maintaining an occupational income by returning to work are clear.
In a Swedish study 51.8% of patients who had undergone cardiac rehabilitation remained in active employment five years after myocardial infarction compared with 27.4% of a control group.
This was due to reduced anxiety, depression, and recurrent cardiac events.
The rate of readmission with cardiac disease, and hence the cost was also lower in the patients who had been rehabilitated.
This finding was supported by a recent American study, although in neither study were patients randomised.
Apart from reconditioning of patients, does cardiac rehabilitation reduce mortality and morbidity?
Exercise promotes beneficial changes in the body.
Heart rate and blood pressure are lowered at rest and at exercise.
Increased high density lipoprotein cholesterol concentrations, improved glucose tolerance in diabetic patients, and enhanced fibrinolysis in response to thrombotic stimuli have been documented, as have improvements in electrocardiographic ST segment changes, ejection fraction, and ischaemic threshold.
Among the randomised trials of exercise based rehabilitation after myocardial infarction performed in the 1970s and 80s only one reported a significant reduction in cardiac mortality, but some of these trials offered only short term follow up while others were small and vulnerable to type II statistical error.
In the late 1980s two meta-analyses reported significantly lower pooled odd ratios for cardiovascular death (0.75 and 0.78).
Overall mortality and cardiovascular mortality, defined as fatal reinfarction or sudden death, were reduced by 20% to 25% in patients randomised to an exercise programme.
There was no significant difference in non-fatal reinfarction rates.
Exercise based rehabilitation programmes therefore seem to have some prognostic value, perhaps in part due to risk factor modification, increased patient surveillance, and exercise training.
Large clinical trials to confirm the conclusions of the meta-analyses are awaited.
Who should be eligible for cardiac rehabilitation programmes?
In theory they should be available to all suitable patients, but the current lack of facilities precludes this.
Patients who may benefit most include those who have multiple cardiac risk factors, a low exercise capacity, are slow to adjust psychologically to a new lifestyle, or have had recent cardiac surgery.
Contraindications to exercise training include unstable angina pectoris, ventricular arrhythmias, and severe aortic stenosis.
Early mobilisation in patients who have had uncomplicated myocardial infarction and subsequent predischarge exercise testing are routine practice nowadays.
The testing also serves to select patients suitable for rehabilitation (excluding those at extreme ends of the fitness spectrum) and in itself forms an important aspect of rehabilitation.
Age is not an absolute contraindication, although there are obvious constraints in elderly patients.
As the effects of training increase in an S shaped curve and plateau in about 13–14 weeks, patients leaving supervised programmes should continue long term exercise after adequate counselling.
This also applies to low risk patients, who need not participate in formal programmes but can perform exercises at home with the help of rehabilitation manuals and audiotapes.
The cost of myocardial infarction and other cardiac illness is high and is in part the result of vocational disability.
Against  this background, it is reasonable to conclude that cardiac rehabilitation programmes are cost effective and should be made available to all who would benefit.
In terms of their prognostic value, however, further research is needed.
Designer hips
Don't let your patient become a fashion victim
The number of total hip replacement operations performed in Britain is now starting to stabilise at about 40000 a year.
The implants used to replace the hip are manufactured for this market by a dozen or so companies, many of which are multinationals with their main marketing base in the United States.
The sales for each company in Britain are very small and commercially unimportant by manufacturing standards.
Until recently the profit margin has been tight, with an implant selling in Britain for less than half the price charged in the rest of Europe and the United States.
A so called‘rationalisation’ of the market is now leading to the price charged in Britain rising at over twice the rate of inflation towards what is charged in other countries.
A perfectly satisfactory standard total hip replacement could be marketed for not much over £100, but the price in Britain now varies from £250 to well over £1000 for each implant.
The situation with knee replacements is similar but the margins, if anything, are even greater, with knee replacements costing around £300 to manufacture selling for between £700 and £1500.
Many units have had to stop replacing joints again this year because they have run out of money — most unsatisfactory when cheaper hips are available and waiting lists are so long.
Budgets for prostheses have been used for years to limit elective orthopaedic activity because blocking the purchase of further implants stops joint replacement surgery, the ‘bread and butter’ of current elective orthopaedic practice.
The cost of the implant should be only a fraction of the total cost of surgery and inpatient care — overall cost £3000-£5000 per joint replacement (Nuffield Orthopaedic Centre NHS Trust costings, 1992).
It is surprising how few units have changed to a cheaper implant in the face of a limiting budget for prostheses.
Why then do some units use expensive implants and risk closure when the money runs out?
The answer is that the surgeons in those units believe that the expensive implants are better than the cheaper models.
They are encouraged in this by the manufacturing companies, which scarcely let a year go by without introducing a ‘new improved’ joint replacement which offers hitherto undreamt of (and unproved) advantages over the older designs.
The cost of the new designs includes development, tooling, advertising, and marketing and is contributing (the manufacturers claim) to the rapid rise in the cost of implants in Britain.
The implant has to be manufactured in many different sizes, and in Britain often fewer than 200 are sold in each size before the design becomes obsolete (S M Hamilton, personal communication).
Development costs have to be recouped rapidly if the company is to make a profit and introduce an even newer hip next year.
The long term clinical results (over 10 years) obtained with some implants (some of the cheapest on the market today) are very good, with failure rates of less than 1% a year.
Frequently, the newer and more expensive designs prove, after a few years of use (and a few hundred hapless patients), to give results substantially worse than this and are quietly withdrawn from the market despite initial optimism over their theoretical advantages.
In Sweden the Christiansen hip was used in over 5000 total hip replacement operations before its high failure rate was appreciated.
If a more reliable hip replacement had been used it is estimated that the Swedish community would have saved $20m, never mind the human suffering.
If every surgeon in Britain used the most reliable prosthesis and achieved the same results as the best units the annual cost of revision operations would be £200m.
If less reliable prostheses or techniques were used the cost at 10 years would be over three times as much.
This ‘fashion trade’ in joint replacements is costing the health service many millions of pounds each year and, even more important, is causing patients unnecessary pain and distress through early failure of unproved implants.
Currently the need for a revision is the most commonly  used definition of a failed operation.
When using this definition the results of conventional designs seem so good that it is effectively impossible to show that a new design is significantly better than conventional ones.
With current failure rates of around 1% a year a new design that was a 30% improvement over an older one (a truly radical improvement) would require a trial of many thousands of patients followed up for at least a decade before a significant difference could be shown.
By then the new implant being tested would have disappeared, replaced by a design with even more untried ‘features.’
Is there any point in continuing the development of new implants if we are using revision as the criteria of failure?
The answer is almost certainly no.
To escape the pernicious influence of fashion we need three things.
Firstly, we need a national register of hip replacements and revisions to provide an accurate measure of revision rate.
This would allow the identification of implants that are doing badly and would allow individual units to compare their performance with national figures.
Secondly, we need outcome measures for assessing new implants that are more sensitive to failure than revision.
These would take into account pain and disability at levels that do not currently warrant revision.
And, finally, we need ways of assessing early how well an implant is functioning.
One of the most promising techniques depends on the very accurate measurement of the sinkage of implants into bone.
Although its predictive value has not been proved, it warrants urgent evaluation.
In future such techniques may help to identify design features that improve survival.
Drugs are subjected to rigorous testing before they can be marketed, and the same should be a statutory requirement for implants.
Manufacturers would be prepared to foot the bill if testing became law.
In its current form the implant industry remains a haven for all the excesses of free enterprise.
Does Crohn's disease have a mycobacterial basis?
The case is unproved
Crohn's disease in an indolent chronic inflammatory disorder that may affect the entire alimentary tract, with inflammation through all layers of the intestine.
Its pathogenesis is a mystery.
Treatment remains non-specific and consists predominantly of immunosuppression.
The original description by Crohn suggested mycobacteria as a possible cause, given that chronic enteritis in animals, termed Johne's disease, was caused by tubercule bacilli.
The many types of mycobacteria vary in their pathogenicity; some are saprophytic and rarely cause disease, while others are potentially pathogenic, particularly in immunocompromised patients.
The pathogenicity of Mycobacteria paratuberculosis remains uncertain.
Research workers have tried for many years to culture mycobacteria from tissue affected by Crohn's disease, although generally without success.
To unravel the complex mycobacteriology of chronic enteritis, investigators have developed sensitive methods to detect and characterise relevant pathogenic strains.
Hermon-Taylor's group identified IS900, a DNA repetitive element in an uncharacterised mycobacterial isolate from a patient with Crohn's disease.
This was shown by DNA fingerprinting to be indistinguishable from M paratuberculosis , which causes enteritis in animals.
The group suggests that assays based on polymerase chain amplification of highly specific DNA sequences from these insertional elements, and recombinant and synthetic peptides from their predicted proteins, will revolutionise the detection and characterisation of these agents.
These findings should be balanced, however, with a report from another group, which showed that DNA from M paratuberculosis is present not only in Crohn's disease but also in ulcerative colitis and other non-inflammatory bowel disorders.
Furthermore, the mycobacteria detected in Crohn's disease were not identical, suggesting that a single pathogenic mycobacterium does not cause the disease.
Elsaghier and colleagues explored a possible role for M paratuberculosis in the pathogenesis of Crohn's disease.
They reported that 57% of their patients with the disease had antibodies to M paratuberculosis antigens, although only 18% concurrently had raised titres to three different M paratuberculosis antigens.
The authors believed that their serological results supported the view that infection with M paratuberculosis might cause Crohn's disease.
Their failure to include suitable controls with other small intestinal disorders or to perform assays for other bacterial antigens, however, makes interpretation of their results difficult.
Stainsby and colleagues' finding of no significant difference in serum concentrations of antibodies to various mycobacteria, including M paratuberculosis , between patients with Crohn's disease and ulcerative colitis and healthy controls argues against the hypothesis that Crohn's disease is caused by M paratuberculosis .
The potential role of mycobacteria in the pathogenesis of Crohn's disease has been investigated with monoclonal antibodies to a glycolipid specific to M avium .
The antibodies were tested in specimens obtained at bowel resection from patients with Crohn's disease and controls.
A positive reaction was seen in the bowel wall, both in patients with Crohn's  disease and in controls.
In controls the positive reaction was limited to the lamina propria, while in Crohn's disease reactivity was also found in the submucosa and subserosa.
Although the authors concluded that the results indicated that mycobacteria might have a role in the pathogenesis of Crohn's disease, a further search for mycobacteria in the affected intestinal tissue with antibodies to M paratuberculosis strain linda,M tuberculosis , and the common mycobacterial antigen lipoarabinomannan failed to find a positive reaction in any of 67 specimens from 30 affected patients.
One case report has described a 38 year old homosexual man who developed fever, diarrhoea, and weight loss and in whom radiography showed terminal ileilitis.
Microbiological culture of the resected terminal ileum showed numerous M avium intracellulare.
Antimycobacterial treatment had previously resulted in weight gain, loss of fever, and resolution of diarrhoea, implying that this patient's terminal ileitis was due to a mycobacterial infection.
Clustering of Crohn's disease in several members of certain families supports the hypothesis of an infective cause.
Antituberculous treatment has been tried in Crohn's disease, although the drugs used have a low sensitivity to atypical mycobacteria including M paratuberculosis .
Rifabutin and ethambutol in combination were assessed in 16 patients with recurrent Crohn's disease who had undergone an ileal resection with a partial colectomy and ileocolonic anastomosis.
Ten patients were treated for six months; six patients withdrew because of adverse effects.
In none of the 10 patients who completed the six months' trial and none of the five patients who completed 12 months' treatment was endoscopic improvement of the lesions seen.
A pilot study at St George's Hospital in London has suggested that quadruple treatment with rifampicin, ethambutol, isoniazid, and pyrazinamide or clofazamine may be of value, with 10 of 20 patients remaining in remission after nine months' treatment.
The authors concluded that controlled trials of antimycobacterial treatment, using four or more agents, should be conducted.
Other theories of aetiology continue to be investigated.
A group at the Royal Free Hospital in London has implicated a vasculitic process possibly initiated by the measles virus since encoding mRNA has been found in affected tissue.
Such agents could initiate an immunological process that becomes self perpetuating long after living organisms have disappeared.
Some cases of granulomatous terminal ileitis are caused by mycobacteria, albeit in immunocompromised people.
The presence of antibodies to certain M paratuberculosis proteins in this condition and the finding, with sophisticated polymerase chain reaction techniques, of DNA associated with mycobacterium in the tissue of two thirds of patients with Crohn's disease argues for the involvement of mycobacteria in the pathogenesis of the disease.
The failure to find organisms in affected tissue with sensitive immunohistochemical methods and the variable responses of the condition to antituberculous drugs suggest that a definite role for mycobacteria in the pathogenesis of Crohn's disease remains unproved.
Night visits in general practice
An acceleration, with the new contract, of an underlying rising trend
A steep rise in claims for night visits followed the introduction of the general practitioner contract in 1990 (p 762), mostly resulting from the one hour extension to each end of the period of eligibility (2300–0700 hours).
The underlying rising trend that existed before the introduction of the contract was, however, maintained.
Deputising services undertook relatively fewer visits in 1990–1 — at 28%, the proportion was down from 46% in 1989–90, the last year of the old contract.
Although the overall number of night visits increased sharply in the first year after the introduction of the contract (for example, in Berkshire it rose by a half over the previous year p 762), the absolute number of night visits made by the deputising service may have altered much less, without much cost saving.
The extended hours of eligibility and the greater rewards for general practitioners who make their own night visits readily explain these changes in activity.
Whether the proportion of night calls dealt with by telephone advice fell as a result of the greater financial incentive to visit is unknown.
Why there is an underlying rising trend in night visiting, especially when average list sizes have fallen steadily, is less easily explained.
In part, it reflects the secular increase in overall consultation rates due to sociodemographic changes such as the increase in numbers and proportions of elderly people and the very young.
Rates of out of hours calls for these age groups, as for general practitioner consultation rates in general, are higher, and relatively more calls from elderly patients result in home visits.
Also, the proportion of households containing one person living alone increased between 1981 and 1991 from 22% to 26%, and the proportion of households containing only one adult and at least one child increased from 2% to 4%.
In these often lonely circumstances  people's desire for medical advice may be acute, and with nearly nine tenths of households having a telephone in 1990 (up from three quarters in 1981), advice is becoming more accessible.
The trend to early retirement may create expectations of good health, and greater unemployment is accompanied by increased levels of ill health.
This may raise call rates, both generally and at night.
Other new incentives to call have arisen, such as the prospect of meningococcal infection in a child with fever and headache — a fear more intense at night.
General practitioners' threshold for visiting is probably lower for patients on lists that they cover than for their own patients.
With the growth in the size of practices and better practice organisation the likelihood of general practitioners covering many other patients increases.
With no disincentives for patients to request night visits and incentives for general practitioners to visit can we expect the increased level of activity to yield health or welfare benefits or avoid costs elsewhere in the system?
Some people believe that it helps in a night consultation to see a doctor you recognise.
Taylor long ago argued that the advantages emerged over several years of contact and that in an emergency most experienced doctors could successfully manage their patient's problem.
Others, especially those with an interest in the psychodynamics of the doctor-patient relationship, have argued that such situations present diagnostic opportunities fully exploitable only by doctors aware of their patients' relationships with those about them, and their previous behaviour and reactions.
As for patients, most are satisfied by all aspects of emergency consultations with deputising service doctors, most of whom they do not know.
Although there is evidence to the contrary, another benefit attributed to general practitioners undertaking their own visits is that, through their familiarity with the situation, they are more selective in referring patients to hospital, and hospital costs are therefore reduced.
It will be important to know whether the trend in nocturnal hospital admissions has changed since the new contract was introduced.
General practitioners are experiencing more stress, less job satisfaction, and poorer mental health with the new contract than before.
The opportunity to enhance income through night visits may soften less agreeable effects of practice with the contract.
Paradoxically, most general practitioners think that opting out of the responsibility for out of hours care altogether should be made possible.
There has been an explosion of night visiting.
Whether this has resulted in better clinical decision making, patient satisfaction, or use of resources is as yet unknown.
Role models and patronage
Greater awareness needed of their capacity for harm
In a more rational world some of the central processes of medical education and career development would be less mysterious — or at least more subject to rational scrutiny.
No one who has gone through medical school would deny the influence of role models, and no one who has pursued a career in medicine, particularly hospital medicine, would discount the part played by patronage.
Both are pervasive and powerful.
Both are underdocumented.
Compared with other university courses, clinical medicine is distinguished by the contribution of teachers who are themselves practitioners of what they teach.
As role models these teachers do more than they know, and they start early.
Medical students in prolonged contact with junior doctors learn attitudes by example, for better or for worse.
Though less in contact with students, the more senior and perhaps more influential members of the clinical teaching staff also offer a wide range of role models, which students may choose to emulate or reject.
Although such little attention as has been paid to the role of the role model emphasises the positive, the sometimes dramatic impact and value of the negative should be recognised too.
Students may choose quite early which behaviours to avoid for ever— or try to.
Teachers are probably more conscious of teaching knowledge and skills than they are of passing on attitudes, yet if, as former medical students themselves, they were to identify ideals in teaching the chances are that those ideals would be closely associated with one or two of their own teachers whose attitudes and behaviour made the most favourable impression at an impressionable age.
We try to follow where such teachers led, but the accumulation of cynicism as students progress through the course is well recognised.
Perhaps we teach what we are.
On an important topic where anecdotes far outnumber studies, a 1988 conference report from Indiana at least provides a concise distillation of local — apparently anecdotally derived — wisdom, noting that role modelling is inseparable from teaching and may do harm as well as good.
It goes on to recommend that role modelling should itself be the subject of teaching so that its functions may become more explicit and better recognised.
Even if we are not quite ready for that, we should as teachers at least attempt to promote our exemplar role to the realms of awareness and practise as conscious role models, because whether we like it or not that is how we function.
Patronage, by contrast, has always been a conscious activity.
Defined as ‘the encouragement given to an individual  by a patron who favours, protects, and gives influential support,’ it has a long and generally inglorious history in the development of careers in medicine.
Medical students and junior doctors seek it.
Their seniors bestow it with increasing effectiveness as their seniority increases.
It is informal in operation, by its nature unequal in its distribution and in practice a means of subverting both unredeemed meritocracy and the more complex goals of equality of opportunity.
Women lose out, as do ethnic minorities, which renders the effects of patronage both wasteful and unfair.
Its principal practitioners and beneficiaries do not, of course, complain, but a BMA document just published expresses grave concerns about current realities and proposes far reaching reforms which, if implemented, would offer a fairer, better deal for all junior doctors (p 798).
The document recognises that informal patronage could be seen as a response to widespread deficiencies in medical education, postgraduate training, appraisal, careers guidance, and appointments procedures.
Individual and covert arrangements offer help and guidance to a favoured minority in overcoming a range of obstacles which nevertheless are faced by all.
More open, more supportive, and more accountable mechanisms are now proposed.
Were they to be realised, patronage might not disappear entirely but it would wither substantially as a result of being seen as increasingly unnecessary and underhand.
Financial and administrative constraints permitting, the proposed more open and formal systems of careers guidance, support, and appraisal could achieve much especially if, as proposed, they were reinforced by explicit fairness in appointments procedures from preregistration level upwards and monitored for ethnic and gender neutrality under the auspices of the NHS Management Executive.
And if self aware role modelling in medicine were to include a declared abstention from all the quiet words, phone calls, and fixings of the present highly selective mechanisms of patronage then the expectations of medical students and junior doctors, especially women and those from ethnic minorities, could be transformed, perhaps within only a generation or two.
NHS trusts
Patients will lose out if some trusts continue making unsafe appointments
The procedures used by advisory appointment committees to appoint consultants have long been governed by successive statutory instruments of parliament for one reason only — to protect the public.
The system has proved a remarkably cost effective way of ensuring that people are not appointed to consultant posts unless they have reached appropriate levels of training and experience.
The royal colleges have a crucial role in ensuring the maintenance of standards through these procedures.
Some other countries have created vastly expensive bureaucratic accrediting agencies to achieve the same ends.
Previously, regional health authorities were responsible for most consultant appointments, but since April 1991 more and more consultants are being employed directly by trusts.
Unfortunately, there is increasing evidence that some trusts, particularly those in the ‘first wave,’ are trying to circumvent the current statutory instrument by such manoeuvres as appointing locums to non-existent posts or by using temporary consultant appointments.
Similarly, although trusts were reminded just one year ago of their obligation to abide by the manpower approvals procedures relating to non-consultant career grades, some have established posts clearly intending them to be outside the controls incorporated in Achieving a Balance .
This has ranged from posts being created without manpower approval to trusts inventing their own titles for grades, which results in grave confusion not only from a manpower perspective but also because different titles may imply competence not possessed by the post holder.
Furthermore, because of their freedom to set their own terms of service, trusts are able to offer contracts that are potentially exploitative, requiring hours of work beyond those considered acceptable for doctors in the training grades.
Unfortunately, outposts of the NHS Management Executive, which are supposed to monitor trusts, have proved totally ineffectual in taking any action on this matter.
But, sooner rather than later, trusts will have to realise that they can ignore neither the law of the land nor agreements to which ministers are cosignatories.
The secretary of state has given the Joint Consultants Committee (JCC) an assurance that the Department of Health will deal firmly with specific cases of abuse that are notified to the department, and, indeed, certain trusts have already been made to reverse inappropriate appointments.
It is unlikely that the JCC knows of all the instances in which either Advisory Appointments Committee procedures have been bypassed or inappropriate non-consultant career grade appointments have been made.
Doctors who are aware of such appointments in their hospitals (or plans for them) should notify the JCC's secretariat of the circumstances so that they may be taken up with the department without delay.
The anonymity of doctors who provide these details will be guaranteed.
If these practices are not stamped out it is the public who will be the ultimate victims.
PAPERS
Rural population mixing and childhood leukaemia: effects of the North Sea oil industry in Scotland, including the area near Dounreay nuclear site
Abstract
Objective —
To determine if any excess of childhood leukaemia and non-Hodgkin's lymphoma was associated with certain striking examples of population mixing in rural Scotland produced by the North Sea oil industry.
Design —
Details were traced for over 30000 workers involved in the construction of the large oil terminals in the Shetland and Orkney islands in northern Scotland or employed offshore.
Home addresses of the 17160 Scottish residents were postcoded, integrated with census data, and then classified as urban or rural.
Rural postcode sectors, ranked by proportion of oil workers, were grouped into three categories with similar numbers of children but contrasting densities of oil workers.
The incidence of leukaemia and non-Hodgkin's lymphoma was examined in these rural (and also in urban) categories in the periods 1974–8, 1979–83 and 1984–8.
Setting —
Scotland.
Subjects —
Young people below age 25.
Results —
A significant excess of leukaemia and non-Hodgkin's lymphoma was found in 1979–83 in the group of rural home areas with the largest proportion of oil workers, following closely on large increases in the workforce.
The area near the Dounreay nuclear installation, where an excess of leukaemia is already well known, was within the rural high oil category.
Conclusion —
The findings support the infection hypothesis that population mixing can increase the incidence of childhood leukaemia in rural areas.
They also suggest that the recent excess in the Dounreay-Thurso area is due to population mixing linked to the oil industry, promoted by certain unusual local demographic factors.
Introduction
Evidence has mounted that the incidence of childhood leukaemia can be increased by population mixing, particularly in a rural area.
This implies an increase in contacts between susceptible and infected individuals for some unidentified transmissible agent whose transmission is more likely in areas of low population density where the prevalence of susceptible people tends to be higher than average.
A striking example of rural population mixing has been produced in northern Scotland by the North Sea oil industry.
The Sullom Voe oil terminal in Shetland, which can handle 70% of the United Kingdom's crude oil production, was Europe's largest construction site — moreover it is sited in Britain's remotest region.
Many thousands of men were transported and housed in specially built camps, drawn from places as diverse as the sparsely populated Scottish highlands and the densely populated conurbations of Britain.
Similar arrangements applied to the smaller terminal at Flotta in Orkney.
In addition, offshore work involves many thousands of men travelling to Aberdeen from all over Britain, to be ferried to installations by helicopter.
Few children live close to these work sites (and, obviously, none offshore), but the regular home visits of these workers might occasion indirect exposure of their home communities to any effects of the mixing in those unusual worksites.
We therefore studied leukaemia and non-Hodgkin's lymphoma in young people in the rural home areas of workers at Scottish oil terminal and offshore sites.
Methods
oil industry workers
We concentrated on the three largest groups of workers in the oil industry (‘oil workers’) in northern Scotland in the late 1970s: construction workers of the oil terminal at Sullom Voe (Shetland); construction workers of the Flotta oil terminal (Orkney); and offshore workers on rigs and platforms.
We could not identify all such workers as complete records have not survived and instead used the closest approximations possible — namely, records incorporating home address of all those (more than 17000) attending the medical centre at the Sullom Voe oil terminal in Shetland during its construction phase (believed to represent a high proportion of all but short stay workers); 3500 construction workers at the Flotta oil terminal in Orkney (incomplete data); and more than 10000 offshore workers, being all those who obtained an offshore survival certificate (required for such work) in Scotland in the period from June 1976 (the earliest date for which records have not been destroyed) to 1980.
Substantial sections of the three workforces were covered in this way.
For the purposes of this study, the three groups of oil workers have been combined.
Many of these workers lived outside Scotland, particularly in the Tyneside and Teesside areas of England.
After eliminating duplication due to men working at more than one site, the postcodes of all Scottish addresses were determined.
leukaemia and non-hodgkin's lymphoma
Details of leukaemia and non-Hodgkin's lymphoma in patients aged under 25 were provided by postcode sector by the Scottish Cancer Registration Scheme for the 1970s and 1980s (from the start of the North Sea oil industry).
Expected numbers for each sector were calculated by multiplying the five year age specific population of each sector in different calendar periods (see below) by the corresponding Scottish incidence rates for these diseases.
study categories: areas and periods
By integrating the postcoded details of oil workers with census small area statistics by postcode sector we calculated the proportions of oil workers among economically active men (those in work at the time of the census) in each postcode sector of Scotland.
‘Urban’ areas were arbitrarily defined as those postcode districts that embrace Dundee, Aberdeen, and Scotland's central industrial belt stretching from Edinburgh to the Firth of Clyde west of Glasgow.
The remainder of Scotland was regarded as ‘rural.’
To estimate the proportion of oil workers needed to influence the incidence of childhood leukaemia in a rural population, we created three categories containing different proportions of oil workers but with similar populations of children.
We ranked rural postcode sectors in ascending order of proportions of oil workers among economically active men.
By accumulating the expected numbers of cases of leukaemia and non-Hodgkin's lymphoma below age 15 we then created three categories with (as far as possible) similar expected numbers.
The ranges of density of oil workers in these categories were <=4.81; oil workers per 1000 economically active men (low), 4.82–10.56 (medium), and >=10.57;(high).
These ranges were also used to examine urban areas.
Aberdeen was examined separately because of its special place in the North Sea oil industry.
The three periods mainly considered were 1974–8, 1979–83 and 1984–8.
The period 1979–83 centres on a census year, and 1974–8 and 1984–8 are of similar length (1988 is the latest year for which cancer registration data were available).
For the period 1974–8, populations for 1976 were used, estimated as the mean of the relevant age specific numbers from the 1971 and 1981 censuses.
For 1984–8, postcode sector populations by age group in 1986 were estimated by adjusting the 1981 figures by the proportionate regional changes in the 1986 age specific estimates of the registrar general's data.
(When all sector populations estimated by this method were summed across ages 0–24, the total was 0.56% below the published Scottish figure for 1986.)
There was a substantial increase in terminal and offshore activity in 1977–8, reaching unprecedented levels shortly afterwards.
In terms of their potential effects, the three periods may therefore be seen as pre-mixing, early post-mixing, and later post-mixing.
specific investigations
The incidence of leukaemia and non-Hodgkin's lymphoma was also investigated with respect to factors that may influence the prevalence of individuals susceptible to infective agents or the intensity of exposure.
These include relative isolation, taken as more than 20 km from urban areas; social class, defined as the proportion of the population in the registrar general's classes I and II; density of children, taken crudely as the number of enumeration districts in a given sector having 100 or more children (postcode sectors are fairly large, and acreages of enumeration districts are not available); and increases from 1971 to 1981 in the numbers of men working away from home in the construction and energy industries (‘recent oil impact measure’).
This measure of increases in men working away from home is unduly conservative but is the closest possible from census data, being the numbers of men in these industries who were working outside their local government district of residence in 1981 minus all men in the construction industry in 1971, expressed as a percentage of economically active men in 1981.
(Details of men working outside their local government district in 1971 are not available.)
It is relevant that these districts are large, in some cases similar to old counties, and that rural areas contained negligible numbers of oil workers in 1971 when the energy industry was not a census category.
statistics
Significance levels (two sided) for the relative risks were calculated, based on an assumed Poisson distribution.
Point estimates for the relative risks were computed by using the observed to expected ratios in the individual exposure groups.
Confidence intervals for these were calculated on the basis of the associated binomial probability.
Results
Table I shows details of the 17160 oil workers resident in Scotland, by category, together with the corresponding numbers of economically active men.
Observed and expected numbers for leukaemia and non-Hodgkin's lymphoma were combined and are shown in table II for ages 0–4, 5–14, 15–24, and 0–24 by oil worker category in urban and rural Scotland and in  the three periods.
Of 18 categories only the rural high oil group in 1979–83, the early post-mixing period, showed a significant excess.
This reflects an excess at ages 0–4 (31 observed, 16.60 expected) and is associated with a significant trend (p<0.01) across the three rural categories, assuming a log linear model (table III).
It is largely due to leukaemia (27 observed, 15.16 expected), mainly acute lymphatic leukaemia.
The rural high oil category showed no significant excess in the other two periods in any age group, nor was there any (significant) excess in any period in the rural low or medium groups or any urban group.
Incidence was also examined in 1969–73 (data not shown), but as in 1974–8 there was no excess in the rural high oil group.
An excess at 0–4 years was present in the high category when the rural oil categories were reconstituted on the basis of ranked prevalence of workers at Sullom Voe only (28 observed, 16.45 expected; p<0.01), at Flotta only (24 observed, expected 16.27; p<0.08), and offshore (29 observed, expected 16.55; p<0.01).
When examined by single years of age from 0 to 4 (table IV), the rural high oil category showed excesses below age 1, at age 1, and particularly at age 2, when annual incidence peaked at 38/100000/year.
This peak at age 2 contrasts with that at age 3 (in all three periods) in all urban areas and in the rural low and medium areas, as was found in the rural high oil group in the earlier period, 1974–8.
In 1984–8, when incidence at ages 0–4 had declined, the rural high oil group continued to show a peak at age 2.
Details of father's occupation in cancer registration and mortality records did not suggest any excess of oil related jobs among the fathers of cases living in rural high oil areas.
Moreover, a comparison of the names of all rural oil workers with those of fathers of children with leukaemia or non-Hodgkin's lymphoma in 1979–83 (ascertained for another study) produced only a single match.
Incidence in the rural high oil category was examined in more isolated (>20 km from urban areas) and less isolated sectors, and at higher and lower levels of certain factors, for which the division was made after ranking the sectors on the factor (disregarding isolation) at the point that as closely as possible halved the cumulative expected values at ages 0–14.
The increased incidence at ages 0–4 was restricted to postcode sectors more than 20 km from urban areas, where it was greater in sectors of higher social class (table V).
There was also some indication of a higher incidence in sectors with a higher child density measure both at ages 0–4 and 5–24, and at both greater and lower degrees of isolation.
In more isolated areas, where increases of construction workers working away from home in the 1970s (the recent oil impact measure) were greater, there was a higher incidence of leukaemia and non-Hodgkin's lymphoma at ages 5–24 (but not at 0–4) than in remaining areas.
When these differences were investigated by this approach using three subgroups of similar size, the recent oil impact measure in more isolated areas showed a significant trend at ages 5–24 (p<0.01; table VI).
Social class in more isolated areas showed a slight (but non-significant) trend in both age groups (table VI).
Discussion
The hypothesis underlying this study was confirmed by finding a significant increase of childhood leukaemia in those rural areas of Scotland that were most affected by population ‘mixing’ associated with the North Sea oil industry.
Moreover, the effect occurred in the early post-mixing period and mainly in the 0–4 age group, in keeping with findings in rural new towns in their early growth period.
There can be concern in a study of this type that multiple testing may have resulted in certain conclusions being due to chance findings.
However, the prior hypothesis led us to the finding in the rural high oil worker category (table II, both for all ages and the 0–4 age group.
The only multiple testing therefore is within the time periods after population mixing (a total of two cells).
However, the unusual nature of the mixing itself, as well as the areas it affected, warrants further discussion.
mixing: work sites and camps
Much has been written about the technological challenge of North Sea oil, said to be second only to the American space programme in scale.
The development of this industry by 1981 (costing £25000m at 1980 prices) made a great impact on northern Scotland, a region of longstanding unemployment and depopulation.
The building of oil terminals and platforms in the area, together with work offshore, provided unprecedented opportunities for employment.
The most extensive operation was the construction in Britain's remotest region, Shetland, of the Sullom Voe oil terminal, the largest such site in Europe.
The workforce (well over 20000 men) contained many from the Highlands and Islands, though the largest numbers were drawn from industrial parts of Scotland (Clydeside) and England (Tyneside and Teesside) that were severely affected by unemployment.
For all this to take place in so remote an area required not only payment of high wages but elaborate logistic arrangements on a massive scale, particularly with the strict conditions laid down by the Shetlands Islands Council to protect both the environment and the local way of life.
These conditions severely limited off site recreation.
Non-local workers were transported by plane and housed in large, specially built camps or in adjacent accommodation ships.
Work hours were long, 10–12 hours per day, 6½days a week, with a one week break taken away from the islands every four weeks.
Weather was often poor, and recreation centred around the bars provided.
The high effective population density was further increased, as on any construction site, by the frequent arrival of new workers as others left.
Conditions were often crowded.
The bringing together of workers was therefore not only on a larger scale but also more continuous than at other oil industry sites, including the isolated platform construction yards at Kishorn, Nigg, and Ardeseir in Highland Region, where many workers travelled from home daily or at least went home at weekends.
These differences may be significant given the potency of large and relatively prolonged aggregations for producing epidemics in experiments.
The workforce at Sullom Voe increased from around 1500 early in 1977 to over 6500 in 1979 and over 7200 in 1980; in 1981 the numbers fell to over 3000 with further declines thereafter.
These numbers are exceptional for a construction site.
Indeed a survey found that few such sites in rural Britain over the past 45 years reached even 3000.
(Sellafield is the most striking exception in the numbers of nuclear and construction workers working together on the same site over most of a 40 year period.)
Oil related activity in the North Sea also increased at this time, and helicopter flights to offshore sites reached record numbers in 1978.
Such activity would further promote contacts between individuals either susceptible to or infected with micro-organisms.
leukaemia
Ages 0–4 — Leukaemia in Scottish rural areas showed a peak slightly later than in urban areas, at age 3 instead of age 2.
This may be due to the very low population density — below 0.03 per acre; similar (slight) differences between rural and urban areas are well known in certain childhood infectious diseases.
In the rural high oil category, where incidence increased significantly in 1979–83 (table II), the peak moved during that period to age 2 as in urban areas, remaining there in 1984–8; in other rural areas it continued to be at age 3.
The recent peak at age 2 in the rural high oil category might therefore be seen as an ‘urbanising’ effect.
Within this category, incidence at ages 0–4 was greatest in areas of relative isolation and higher social class (table VI) as in another recent study.
Here the prevalence of people susceptible to infectious agents would tend to be higher than in urban areas or in adjacent rural areas, which show no excess (tables II and VI).
Ages 5–24 — There was no excess of leukaemia and non-Hodgkin's lymphoma at ages 5–24 in the rural high oil category.
However, their incidence did show a highly significant trend with increasing oil impact measure among rural high oil sectors (p<0.01; table V).
All this is consistent with the absence of any effect of oil work in urban areas (table II): rural post code sectors already supporting many construction workers (and Highlands Region has one of the highest levels of such workers in Britain) would be expected to be less affected by oil work than similar areas without such earlier ‘exposure’(that is, those in which the recent impact score was high).
Areas with a relatively high density of children also seemed to have a higher incidence (table V), but a combined analysis was dominated by the recent oil impact measure.
The absence of an increase [in cases of leukaemia]in the rural high oil category overall in 1979–83 is due to a deficiency that is almost significant in areas with a lower child density or a lower oil impact score — and which was not present in the previous period, 1974–8.
Deficiencies at ages 5–24 were noted in new towns and the possibility was considered that they were due to immunising effects of a milder epidemic of the underlying infection.
If they were not due to chance, however, this may apply here.
The subgroup analyses are, to a certain extent exploratory.
communicable diseases
A severe epidemic of measles among local people in Shetland occurred in 1977–8 with more than 30% of people affected aged over 14 and 10% aged over 24.
Conditions at Sullom Voe were considered to have contributed to this epidemic as well as to unusual outbreaks of whooping cough, scarlet fever, and influenza (J D MacGregor, International Epidemiological Association, Edinburgh, 1981).
The role of large aggregations of individuals for appreciable periods is probably of central importance here.
Such aggregations not only promote transmission of micro-organisms but through repeated exposure allow large doses of these.
Large doses may be particularly important in relation to diseases that are uncommon responses to infection, such as feline leukaemia.
ascertainment of oil workers
Construction workers at the Sullom Voe oil terminal formed the largest group in this study.
The proportion of the workforce covered by medical records (the source of our data) is probably greater than on most sites because a visit to the medical centre was one of the few legitimate reasons for breaking off work, so often  made irksome by harsh weather.
The other two groups were also not fully ascertained (in particular offshore workers who obtained their survival certificate before mid-1976).
It is relevant, however, that their proportions in the populations were used only to rank the sectors (that is, for internal comparison).
When we grouped the sectors approximately into local government districts and then ranked them on density of oil workers, this ranking coincided with that based on data for offshore workers in the 1981 census, suggesting that our data was not affected by geographic bias.
The rural high oil category showed broadly similar excesses whether defined on the basis of all oil workers or of those only at Sullom Voe, at Flotta, or offshore.
This probably reflects the fact that all such work tended to attract men from similar areas — those that were conveniently situated or with low levels of local employment.
The excesses in the analyses of Flotta or offshore may therefore be indirectly due to the effects of Sullom Voe.
The above findings on childhood leukaemia provide further evidence for horizontal transmission of some underlying infection of which leukaemia is a rare response.
but made more likely in conditions of heavy exposure to the relevant agent, as in feline leukaemia.
They also suggest that the relevant agent (or agents) can be transmitted among adults, and thence to children, as suggested by other recent work involving occupational settings.
The excess of leukaemia and non-Hodgkin's lymphoma was not concentrated in the children of oil workers themselves, consistent with this being a community (or ‘herd’ effect.
The effects of the oil industry observed in this study are also consistent with long established observations on both experimental and human epidemics which Topley summarised 50 years ago as indicating that outbreaks of many infective diseases can be produced simply by ‘movements of susceptible and infected hosts in relation to one another, and aggregations or dispersals of human or animal herds.’
dounreay-thurso area
Among the rural areas with a high proportion of oil workers is the Dounreay-Thurso area, where a recent excess of childhood leukaemia is well known.
The relation of this excess to the present study has a rather complicated history — with the excess near Sellafield, it originally suggested the population mixing hypothesis, but what was then in mind was the mixing connected with the nuclear industry; an analogy with a new town was drawn.
Subsequent studies have supported the hypothesis that population mixing can influence the incidence of childhood leukaemia.
At the start of the present study and, indeed, until its analysis stage we did not associate the Dounreay-Thurso area with the oil industry, and finding that it contained a high proportion of oil workers was unexpected.
The 20 year interval between the first influx of nuclear workers in the late 1950s and the cluster of cases near Dounreay beginning in 1979 could not readily be explained since the excesses usually followed fairly quickly on other types of population mixing.
On the other hand, it would be incorrect to suppose that the first influx of nuclear workers in Thurso did not produce the twofold or greater increase noted in rural new towns: the only two cases at ages 0–14 in the period 1951–67 occurred in the five years following the influx in 1958 (expected 0.41; not significant).
The well known excess of leukaemia began around 1979, at the same time as the increases found by this study in rural areas far removed from any nuclear installation — but all affected by population mixing associated with the oil industry.
The (second) Dounreay-Thurso excess therefore followed closely on a striking example of population mixing.
Leukaemia
The excess of leukaemia and non-Hodgkin's lymphoma at ages 0–4 in 1979–83 in the area within 25 km of Dounreay was appreciable (8.8-fold), only six sectors in the rural high oil category having higher values (from 10.4-fold to 500-fold).
The excess at ages 5–24 in this area is particularly noticeable and persistent, continuing up to 1988.
The relation with the recent oil impact measure table IV has obvious relevance here and also to why all the Thurso cases are in the west of the town, with none in the east: west Thurso is largely composed of nuclear workers not native to the area (locally known as ‘atomics’), making it until about 10 years ago rather separate from the rest of the town.
Compared with sector KW14.8 covering the east part of the town, a smaller proportion of residents in sector KW14.7, which covers the west part worked in the construction industry in the pre-oil era (table VII).
Indeed, the east part of Thurso has a tradition of ‘travelling workers’— men regularly working away from home on construction.
The availability of work at terminals just to the north led to a much greater increase in construction workers (82% compared with 18%) in the western sector (KW14.7) than in the eastern sector (KW 14,8), so that by 1981 their levels had become similar.
The recent oil impact was therefore greater in the west, which had twice as high a rank as the east (table VII).
Also relevant may be the greater density of children in west Thurso, given the evidence suggesting such an effect (table V) and its importance in epidemics.
Within the rural high oil category only three (small) sectors more than 20 km from urban areas had a higher rank than west Thurso when the recent oil impact and child density scores were combined.
In contrast, east Thurso (with no cases) may be seen as resembling the urban areas in our study in which the incidence of leukaemia was unaffected by the oil industry.
The fewer susceptible people in urban areas would reflect the prior sporadic exposure to the widespread relevant agent, which would make them less vulnerable to an epidemic caused by new and sudden population mixing.
Intensity of exposure: incomers
The factor that may be decisive in explaining why the oil industry produced in west Thurso so great and prolonged an excess at ages 5–24 is the large number of incomers already associated with the nuclear industry.
(An appreciable increase had also occurred in the late 1970s in the numbers of US servicemen and their families in the area.)
The 1971 census showed that the relevant postcode sector held almost the highest proportion  (23%) of residents not born in Scotland of any sector in the rural high oil worker category, and indeed one of the highest in Scotland.
Children who moved into west Thurso from other (and often distant) places would have been exposed quite suddenly (at school and elsewhere) to an unusual infective burden.
Any such move makes probable the sudden exposure to new agents (or at least new strains) to which children born in the area, even if their parents had moved in, would already have developed some immunity.
In the case of west Thurso such effects were likely to be compounded by the scale of exposure to infections among other incoming children, including possibly the infection of which childhood leukaemia is postulated to be a rare consequence.
In a recent study of schools within 25 km of Dounreay, the proportion of incomers (born outside the county) among children was highest (namely, 50%) for the birth cohort 1969–74 (R Black, personal communication).
The proportion would have been even higher in the Pennyland Estate in west Thurso, for it was built by the nuclear authority specifically for incomers.
All the cases of leukaemia in Thurso were centred on this estate, which lies just within 12.5 km of Dounreay, so that the excess there was much higher than in the next cumulative zone, which included the eastern half of the town (within 16.6 km), a difference that has been the subject of much comment.
It may not be a coincidence, therefore, that four of the five individuals who developed leukaemia at ages 5–24 in Thurso in the years 1979–88 were incomers — and also that four were born in the years 1969–74, the birth cohort with the greatest proportion of incomers.
All three children with leukaemia at ages 0–4 within 25 km of Dounreay were born in the area.
The absence of cases in west Thurso above age 4 among children born locally may at least partly reflect the immunity conferred by their recent exposure at earlier ages.
As mentioned previously, in its unusual level of incomers west Thurso resembles a new town, albeit on a smaller scale.
Here, this earlier population mixing would have represented a leukaemogenic influence — and in a part of the town in which shortly afterwards a second and different type of mixing was occasioned by the oil industry.
Other evidence has suggested that a double influence operates in the development of leukaemia at ages 5–24, part early, the other late (possibly analogous to the way that early and persisting pestivirus infection in cattle alters the response to later infection by a different strain.
The probable importance of large doses of the relevant agent (from either repeated exposures or particularly heavy exposures) has been emphasised previously in relation to childhood leukaemia.
This is established in feline leukaemia, of which the greatly increased incidence in households with many cats is due mainly to exposure of kittens to large doses of a virus that usually causes merely an immunising infection.
However, older cats are also at risk if introduced into those special households, but the exposure has usually to be longer.
The cases of leukaemia in Thurso at ages 5–24 seem to be analogous, for these occurred later (in 1983–8) than those at ages 0–4 (1980–1).
We cannot test independently the relevance of the high proportion of children not born locally in the isolated area that is west Thurso since no area within the rural high oil category is known to approach its high level.
However, the only area (comprising three adjacent sectors) in this category with a comparable proportion not born in Scotland (though it is much less isolated) also showed an excess in the period 1979–83 at ages 5–24 (2 cases observed, 0.54 expected).
Previous studies have not investigated the role of population mixing in the Dounreay-Thurso area, though they have shown that the excess of leukaemia and non-Hodgkin's lymphoma there cannot be attributed to paternal nuclear employment or irradiation before the child's conception.
Furthermore, other evidence weighs strongly against radioactive discharges as the cause.
The present findings suggest that the excess is due to the population mixing associated with the oil industry causing an epidemic of the infection, of which childhood leukaemia is an uncommon response.
Certain factors specific to the Dounreay-Thurso area would have tended to promote transmission and heavy exposure.
Cigarette smoking and human papillomavirus in patients with reported cervical cytological abnormality
Abstract
Objective —
To assess the relation between two risk factors for cervical neoplasia: smoking and infection with oncogenic human papillomavirus.
It has been suggested that smoking causes a local immunological defect, which could facilitate the infection and persistence of human papillomavirus.
Design —
Cross sectional epidemiological study.
Completion of a structured questionnaire by the patients, analysis of cervical scrapes for human papillomavirus, and morphological examination of biopsy specimens.
Setting —
Outpatient gynaecological clinic.
Subjects —
181 women with a report of cervical cytological abnormality.
Main outcome measures —
Prevalence of infection with oncogenic human papillomavirus and smoking habits.
Results —
Oncogenic human papillomavirus was found in the cervix of 26 (41%) of the 63 women who did not smoke, 22 (58%) of the 38 who smoked 1–10 cigarettes a day, 28 (61%) of the 46 who smoked 11–20 cigarettes a day, and 26 (76%) of the 34 who smoked >=21; cigarettes a day.
The prevalence of the virus thus increased in accordance with the number of cigarettes smoked (p=0.001).
This relation remained after adjustment for age at first intercourse and lifetime number of sexual partners.
Of the 63 non-smokers, 23 had previously smoked at least 10 cigarettes a day at some time.
Of these 23 women, 14 (61%) had oncogenic human papillomavirus in their cervix.
Of the 40 women who had never smoked at least 10 cigarettes a day, 12 (30%) had the virus.
The prevalence of oncogenic human papillomavirus in non-smokers therefore depended on previous smoking habits (p=0.03).
Conclusion —
The dose dependent effect of cigarette smoking on the occurrence of oncogenic human papillomavirus favours a causal relation between these risk factors for cervical neoplasia.
Introduction
Cervical intraepithelial neoplasia is a morphologically defined lesion associated with the development of cervical carcinoma and is conventionally separated into three grades according to the severity of cytological abnormality and disturbance of the epithelial architecture.
It has been established that cervical infection with human papillomavirus is important in the aetiology of neoplasia.
Human papillomavirus shows considerable genetic heterogeneity, and the list of different types is growing.
A subset of the virus types, called the oncogenic types, includes types 16, 18, and 33 and is found in 80–90% of cervical carcinomas.
Of carcinomas positive for human papillomavirus, types 16 and 18 account for about 90%.
Compared with invasive carcinoma, intraepithelial neoplastic lesions harbour a greater diversity of human papillomavirus types.
The benign types, 6 and 11, are found in condylomas and low grade intraepithelial neoplasia but are seldom if ever the only virus types in invasive carcinomas.
The malignant potential of many other virus types mainly found in association with mild to moderate dysplastic smears is uncertain.
Cigarette smoking is a risk factor for cervical neoplasia.
It has been associated with the presence of cervical human papillomavirus in patients at student health clinics.
Another risk factor for cervical neoplasia is sexual behaviour: a woman's having intercourse before the age of 17 was the most powerfully discriminating variable in nearly all the studies where this has been investigated, and a history of multiple sexual partners is an important supporting variable.
The association between smoking and cervical neoplasia, however, remains after adjustment has been made for such sexual risk factors.
We performed a cross-sectional epidemiological study on 181 women with a report of cervical cytological abnormality in order to find out whether the presence of oncogenic human papillomavirus was associated with the grade of the neoplastic lesion; whether smoking was associated with the grade of the neoplastic lesion; and whether smoking was associated with the presence of oncogenic human papillomavirus.
Patients and methods
patients
We recruited patients from the outpatient clinic of the department of gynaecology, University Hospital, Groningen.
Some had been referred by their general practitioners because of a report of cervical cytological abnormality, and in others a cytological abnormality had been discovered during gynaecological examination.
Patients were eligible for participation in the study if two cervical smears indicated mild or moderate dysplasia or one smear indicated severe dysplasia or carcinoma in situ.
These cytological criteria for eligibility correspond with the grounds for colposcopy as agreed by cytopathologists and gynaecologists in the Netherlands.
The interval between the two smears indicating mild or moderate dysplasia was a maximum of one year.
We requested smears that had not been initially examined at our cytology department for review, and our cytologist's readings were definitive.
Patients were ineligible if they had previously undergone a colposcopic examination because of a report of abnormal cytology, if their cervical smear was taken during pregnancy, or if their cervical smear indicated atypical endocervical cylindrical epithelium.
From 1 September 1988 to 1 May 1991, 194 consecutive patients were eligible for participation in the study and were invited to the outpatient clinic for an extra visit to answer a structured questionnaire and to have a cervical scrape taken for analysis for human papillomavirus.
Thirteen patients did not enter the study: two did not want to, two were pregnant at the time of colposcopy, and nine were not treated in accordance with the study protocol (no cervical scrape or no biopsy specimen was taken or no treatment was given when the biopsy specimen showed cervical intraepithelial neoplasia).
Therefore, 181 patients were included in the study.
questionnaire
We asked the women the average number of cigarettes they smoked a day and asked those who did not smoke whether they had ever smoked at least 10 cigarettes a day.
We also asked the patients their age at their first sexual intercourse and their lifetime number of sexual partners.
We had told the women beforehand that the questionnaire contained intimate questions and that they were not obliged to answer.
analysis for human papillomavirus
We scraped the cervix with the blunt and pointed end of a wooden cervical spatula and with an endocervical brush.
We suspended the scraped cells in 5 ml phosphate buffered saline, pH 7.2, supplemented with merthiolate 1:10000 vol/vol and sent the cell suspension to the laboratory for processing the next morning.
The polymerase chain reaction was performed for human papillomavirus types 6 and 11, 16, 18, and 33 as described previously.
The laboratory staff were unaware of the histological reports.
morphological examination
Four weeks after the cervix had been scraped, we took colposcopically directed biopsy specimens and, in the case of an endocervical position of the neosquamocolumnar junction, performed endocervical curettage.
If cervical intraepithelial neoplasia was diagnosed from the tissue specimens we excised the whole transformation zone about 4–6 weeks later by low voltage diathermy ablation or cold knife conisation.
The diathermy loop was used when the neosquamocolumnar junction could be visualised entirely and did not extend up into the canal more than 5 mm from the anatomical os externum.
One pathologist who was unaware of the virus types found in the cervical scrapes examined all the biopsy specimens and the excised tissues from the patients.
Cervical intraepithelial neoplasia was diagnosed and graded according to the criteria of the World Health Organisation.
The cervical neoplasia was classified according to the most severe lesion found by histological examination.
statistical analysis
Using SPSS software, we analysed differences between two groups of patients in the frequency of smokers and the presence of human papillomavirus by means of the χ 2 test with Yates's correction for two independent samples.
We analysed differences between multiple groups in age distribution and the number of cigarettes smoked daily by means of the Kruskal-Wallis one way analysis of variance.
We performed 2×k table analyses of the presence of oncogenic human papillomavirus in relation to the categories of cervical neoplasia and the presence of the virus in relation to the number of cigarettes smoked per day.
For this we used the EGRET software, in which the formula for the χ 2 test for trend in stratified and unstratified analysis was taken from Breslow and Day.
The test was used with continuity correction, and values of p<0.05 were considered to be significant.
Results
diagnoses
The final histological diagnoses were no neoplasia (15 patients), cervical intraepithelial neoplasia grade I (19), cervical intraepithelial neoplasia grade II (28), cervical intraepithelial neoplasia grade III (108), and microinvasive carcinoma (11).
The patients were aged 20–67 with a mean age of 34.4 (SD 7.9) years.
There was no difference between the categories of the diagnosis with regard to age distribution (χ 2 =3.11, df=4, p=0.54; Kruskal-Wallis one way analysis of variance).
human papillomavirus and cervical neoplasia
Table I shows the virus types detected in the histological categories.
A benign human papillomavirus (type 6 or 11) was the only virus type found in one patient and was found in conjunction with oncogenic virus types in another three patients.
One or more oncogenic human papillomavirus (type 16, 18, or 33) was found in 4 (27%) of the 15 patients without neoplasia, 6 (32%) of the 19 with cervical intraepithelial neoplasia grade I, 12 (43%) of the 28 with cervical intraepithelial neoplasia grade II, 73 (68%) of the 108 with cervical intraepithelial neoplasia grade III, and 7 (64%) of the 11 with microinvasive carcinoma.
Thus the prevalence of oncogenic human papillomavirus increased with the severity of the neoplastic lesion (χ 2 =14.97, df=1, p<0.001; χ 2 test for trend).
Of the 102 patients positive for oncogenic human papillomavirus, 80 (78%) had cervical intraepithelial neoplasia grade III or a more severe lesion.
smoking and cervical neoplasia
Altogether 118 (65%) of the 181 patients smoked at least one cigarette a day, and the proportion of smokers in each of the histol0gical categories was similar: 10 (67%) of the 15 without neoplasia, 12 (63%) of the 19 with cervical intraepithelial neoplasia grade I, 18 (64%) of the 28 with cervical intraepithelial neoplasia grade II, 71 (66%) of the 108 with cervical intraepithelial neoplasia grade III, and 7 (64%) of the 11 with microinvasive carcinoma.
No significant difference was found between the histological categories with regard to the number of cigarettes smoked a day (χ 2 =5.07, df=4, p=0.27; Kruskal-Wallis one way analysis of variance).
smoking and human papillomavirus
We found oncogenic human papillomavirus in the cervixes of 76 (64%) of the 118 smokers and 26 (41%) of the 63 non-smokers, a significant difference (χ 2 =8.02, df=1, p=0.005; χ 2 test for two proportions).
To look for a dose-response relation we grouped the patients according to the number of cigarettes they smoked a day: none, 1–10, 11–20, and >21.
Table II shows that the frequency of oncogenic virus increased significantly  in accordance with the number of cigarettes smoked a day.
We analysed the dose-response relation with adjustment for the patients' age at first intercourse and lifetime number of sexual partners.
Nine patients did not give their number of sexual partners and one of them did not tell her age at first intercourse.
We therefore performed the analysis on 172 patients: the median age at first intercourse was 17 (interquartile range 16–19) years, and the median number of sexual partners was 4 (interquartile range 2–10) partners.
For the analysis we grouped patients according to age at first intercourse (<17 or >18) and lifetime number of sexual partners (1–2, 3–4, 5–10, and >11).
After adjustment for these sexual risk factors we found that the dose-response relation between the number of cigarettes smoked a day and the presence of oncogenic human papillomavirus was still significant (χ 2 =10.90, df=1, p<0.001).
We also investigated whether this dose-response relation depended on the histological severity of the lesion.
Since we had found that the analysis was hardly affected by adjustment for sexual risk factors we performed a non-adjusted analysis on the entire group of 181 women.
We grouped patients into those with no or low grade lesions (up to and including cervical intraepithelial neoplasia grade II) and those with more severe lesions (cervical intraepithelial neoplasia grade III or microinvasive carcinoma).
Table III shows that the dose-response relation between the number of cigarettes smoked a day and the occurrence of oncogenic human papillomavirus in the cervix was significant in both groups.
Of the 63 women who did not smoke, 23 women had smoked at least 10 cigarettes a day at some time.
Of these 23 women, 14 (61%) had oncogenic human papillomavirus in their cervix.
Of the remaining 40 women who did not currently smoke and who had never smoked at least 10 cigarettes a day, only 12 (30%) had oncogenic papillomavirus.
This difference was statistically significant (χ 2 =4.54, df=1, p=0.033; χ 2 test for two proportions).
Discussion
We studied only newly diagnosed cases of cervical abnormality because the persistence or recurrence of cervical neoplasia might also be affected by the factors under study.
Pregnant patients were excluded because biopsy and treatment are usually postponed until after delivery.
Because of possible biological differences in the development of squamocellular and adenocellular cervical neoplasia we also excluded any patients with atypical endocervical cylindrical cells in the smear.
For the detection and typing of human papillomavirus we used cervical scrapes instead of biopsy specimens.
In situ hybridisation studies have shown that human papillomavirus can be detected only within lesions and not in adjacent normal epithelium.
The probability of missing a cervical infection is possibly higher when using biopsy specimens because of inevitable sampling errors.
Our data cannot be compared directly to those of other investigators because the reported frequencies of human papillomavirus in cervical lesions vary widely because of differences in selecting patients, in the materials analysed, in detection techniques, and in the geographical occurrence of virus types.
The frequencies of the virus types observed in our patient group match the findings of another group of Dutch investigators.
We found that 80/102 (78%) patients positive for oncogenic human papillomavirus had cervical intraepithelial neoplasia grade III or a more severe lesion.
Since patients with all three grades of cervical intraepithelial neoplasia had similar ages this suggests that many infections with oncogenic human papillomavirus apparently led directly to cervical intraepithelial neoplasia grade III.
Campion et al suggested that human papillomavirus infections progress rapidly from grade I to grade III of cervical intraepithelial neoplasia, but their study was methodologically flawed in that grade I lesions were diagnosed by colposcopy while grade III lesions were later diagnosed by histology.
The features that can be seen by colposcopy do not, however, correspond well with the severity of histological abnormality.
We found that 65% of the patients smoked at least one cigarette a day, a proportion nearly twice as high as that in the general population.
According to Dutch health statistics from 1990, 38% of women aged 20–34 and 36% of women aged 35–49 smoke.
There are two possible explanations for the association of smoking with cervical neoplasia.
Firstly, chemical carcinogens in tobacco smoke might directly induce cervical carcinogenesis.
By means of the Ames test, Holly et al have shown that women who smoke are more likely to have mutagenic cervical fluids than non-smokers, and nicotine and cotinine have been found in the mucous secretions of the uterine cervix of smokers.
Secondly, cigarette smoking might produce a local immunological defect.
Smoking is associated with a significant and dose dependent decrease in the concentration of antigen presenting Langerhans' cells in the normal cervical epithelium.
We did not find an association between smoking and the grade of cervical intraepithelial neoplasia, and so our data do not support the hypothesis that cigarette smoke has a direct mutagenic effect on the cervical epithelium.
We did, however, find a significant dose-response relation between the number of cigarettes smoked daily and the presence of oncogenic human papillomavirus in the cervical lesion.
Such a graded response according to exposure can strengthen inferences of causality.
The relation remained after adjustment for sexual risk factors, indicating a significant independent effect of smoking.
We also found that the presence of human papillomavirus in the cervix of non-smokers was significantly associated with a history of smoking.
Our findings support the suggestion that smoking might produce a local immunological defect, which could facilitate the infection and persistence of human papillomavirus.
The dose-response relation between the number of cigarettes smoked daily and the presence of oncogenic human papillomavirus was found both in the group with no or low grade lesions and in the group with more severe lesions.
This confirms that lesions associated with one particular virus type may show diverse morphological characteristics.
Possible explanations for this diversity are the physical state of the virus (whether it is extrachromosomal or integrated into the host cell chromosomes), the topographical  position of the lesion on the cervix, and the presence of possible cofactors such as herpes viruses in carcinogenesis.
Randomised comparison of combination chemotherapy plus supportive care with supportive care alone in patients with metastatic colorectal cancer
Abstract
Objectives —
To compare the length of survival and quality of life in patients given combination chemotherapy in addition to supportive care and in patients given only supportive care.
Design —
Randomised study.
Setting —
Gastrointestinal oncology departments.
Patients —
40 previously untreated patients with histologically confirmed, measurable colorectal cancer that was locally recurrent or metastatic.
Interventions —
Patients were allocated randomly to receive chemotherapy or only supportive care in a ratio of 2:1 according to performance status, metastatic disease of the liver, and weight loss in the six months before entering the study.
Chemotherapy consisted of four week cycles of intravenous leucovorin (200 mg/m/day) followed by 5-fluorouracil (550 mg/m/day) and cisplatin (20 mg/m/day), each drug being given on the first four days of the cycle.
Main outcome measures —
Length of survival and quality of life score with an optimised functional living index — cancer scale.
Results —
Overall survival was significantly longer for patients given chemotherapy (11.0 months) than for those receiving supportive care alone (5.0 months; p=0.006).
Despite common association of chemotherapy with mild to moderate gastrointestinal symptoms, there was no significant difference between the two groups in global or subgroup quality of life scores.
In patients with abnormal scores before treatment, quality of life seemed better in the chemotherapy arm.
Conclusions —
In this sample of patients with disseminated colorectal cancer the chemotherapy regimen was an effective form of palliative treatment.
Introduction
Chemotherapeutic management of advanced colorectal cancer has been a challenge to medical oncologists for the past three decades.
Although tumours in 15–20% patients have responded to fluorinated pyrimidines, in particular 5-fluorouracil, there has been no evidence of improved survival.
Empirically derived combinations of chemotherapeutic drugs have given disappointing results.
Recent attempts to enhance the therapeutic activity of 5-fluorouracil have focused on biochemical modulation: several randomised studies have shown that its effect is greater when used in combination with leucovorin.
The optimal dose, schedule, and route of administration, however, have not been established.
Complete response is rare, and the improvement in median survival seems small.
Furthermore, most 5-fluorouracil and leucovorin dose schedules have a high incidence of severe gastrointestinal side effects and may thus interfere with the patients' quality of life, one of the most important aspects to be considered in palliative treatment.
Because of uncertainty about the true palliative benefit of combined regimens we conducted a randomised study of the effects of chemotherapy and supportive care on survival and quality of life of patients with colorectal cancer.
The chemotherapeutic regimen chosen was a combination of 5-fluorouracil and leucovorin with cisplatin.
Cisplatin was included  because of experimental evidence that it further potentiates inhibition of thymidylate synthase, encouraging therapeutic results in pretreated breast and advanced head and neck cancer, and the recent demonstration of a good therapeutic index in metastatic colorectal cancer.
Patients and methods
Patients who had inoperable, measurable, histologically confirmed metastatic or locally recurrent adenocarcinoma of the colon or rectum were eligible for entry into the study.
Additional requirements for inclusion in the study were age under 75 years, life expectancy over two months, Eastern Cooperative Oncology Group performance status<3, no previous chemotherapy, and adequate haematological (leucocyte count >4×10/l, platelet >150×10/l), hepatic (no jaundice and serum aminotransferase concentrations <100 IU/l), and renal (serum creatinine <132.6 µmol/l, creatinine clearance >1 ml/s) functions.
After fully informed consent was obtained eligible patients were registered by phone at the central statistical office at the University of Vienna.
They were allocated randomly to receive supportive care plus chemotherapy (arm A) or supportive care only (arm B) in the ratio of 2:1 (which was based on the assumption of a higher refusal rate in arm B).
The assignment was determined by randomisation in blocks of six (as defined by a computer generated random number list) according to performance status (score 0–1 v 2–3), metastatic disease of the liver (assessed by ultrasonography or computed tomography, or both), and weight loss in the six months before entering the study.
Supportive care consisted of analgesics, nutritional support, blood transfusions to correct severe anaemia, and psychosocial support.
Chemotherapy consisted of leucovorin 200 mg/m/day by intravenous push followed 30 minutes later by bolus 5-fluorouracil 550 mg/m/day and cisplatin 20 mg/m/day, given as a two hour infusion with adequate hydration.
An antiemetic regimen comprising dexamethasone, metoclopramide, and lorazepam was routinely used.
All chemotherapeutic drugs were given on four consecutive days at four week intervals for a total of six months or until there was evidence of tumour progression.
Quality of life was assessed at entry to the study and every two months with the functional living index for cancer (FLIC).
This is a contemporary, well validated, 22 item self report scale developed for repeated use by patients with cancer.
It provides a single quality of life score based on indexes of perceived physical wellbeing, psychological state, and sociability.
To counteract difficulties in collection and methodological evaluation of the data we used several refinements of the method, including designation of a central data coordinator and use of a simple 10 point (rather than a continuous) scale.
Furthermore, serial results in individual patients at successive time points have been evaluated according to the criteria proposed by Presant et al for assessing palliative response by quality of life (box).
Analyses of changes in quality of life were performed separately for patients whose scores were all normal before treatment and for patients who had at least one score (global score or subgrouping) that was abnormal — that is, more than two standard deviations above results in normal people.
Twenty healthy volunteers completed the questionnaire.
They had a mean score of 41 (SD 16), and this was used to define a normal value.
The duration of palliative response was measured from the time that the response was first observed until a significantly lower score was obtained on two successive measurements.
A significant change in score was defined by the 95% statistical interval of the instrument on test-retest analysis assessed during the early part of this trial.
Treatment related toxicity and antitumour responses were assessed periodically in both groups according to World Health Organisation standard criteria.
The length of time to disease progression and survival time, both calculated from the date of randomisation, were analysed by the Kaplan-Meier product limit method.
Differences between groups were evaluated by the log-rank test.
Results
Between April 1988 and September 1989 40 patients were accrued to the study.
Two patients in each treatment arm refused to accept the treatment assigned or participate in the research study or both.
Thus, 36 patients were eligible for analysis of response and toxicity.
Of these, 24 were randomised to receive chemotherapy with 5-fluorouracil, leucovorin, and cisplatin (arm A) and 12 to receive no chemotherapy (arm B).
During the study two patients randomised to no chemotherapy were treated with 5-fluorouracil alone when symptomatic tumour progression occurred.
This decision was made by the patients' doctors; neither patient responded, but both have been included in this analysis.
The pretreatment characteristics of patients in the two groups were similar (table I), although the median time from original diagnosis of colorectal cancer to entry was slightly longer for patients with chemotherapy (8.5 (range 0 to 61) months) than for those who received supportive care alone (5.5 (0 to 22) months).
Table II summarises the results of chemotherapy or supportive care in the two groups.
Of the 24 patients randomised to chemotherapy, eight partially responded.
The median number of treatment cycles administered was 4.5 (range 1 to 8), and the median duration of response was 35 (16 to 56) weeks.
Nine patients (38%) had stable disease and seven (29%) progressive disease.
Three (25%) of the 12 patients randomised to no chemotherapy were classified as  having stable disease and nine (75%) as having progressive disease.
Median time to progression was 6.0 (2 to 14) months for treated patients and 2.3 (1.5 to 8.0) months for controls; this difference was significant (p=0.0008).
Thirty three of the 36 patients in the study had died at the end of the study period, 21 (87%) in the chemotherapy group, and all 12 in the supportive care group.
The minimum follow up of the three survivors was 28 months.
Figure 1 gives the actuarial survival curves for all subjects by randomisation status (including the four patients who refused the assigned treatment) the curves for the two groups are significantly different (p=0.006).
Median survivals were 11.0 (4.0 to 37.0) months for patients randomised to chemotherapy and 5 (1.5 to 23) months for those receiving supportive care.
The sample size was too small to rule out any differences in survival between patients of different prognostic groups.
Toxicity was common in the chemotherapy group though symptoms were generally mild to moderate.
No patient stopped chemotherapy because of side effects, and only two had to have the dose reduced (by 25%) because of grade 3 haematological and gastrointestinal side effects (table III).
In the supportive care group, mild nausea, diarrhoea, and infection were indicated by two, three, and one patient, respectively; no other systemic toxicities were recorded.
Eighteen (75%) patients who received chemotherapy and eight (67%) symptomatically treated patients completed at least two questionnaires on quality of life, including one at baseline, and were thus considered evaluable.
The remaining patients were either unwilling to complete the form (two), had problems with reading because of poor vision or language (two), or died early (six).
An average of five evaluations was available for all patients (range 3 to 10).
The mean total scores in the chemotherapy group (87.5 (SD 44)) and in the supportive care group (80.2 (40)) were similar at baseline, as were mean factor scores.
Table IV gives the quality of life in the two treatment groups.
Overall, there was no difference between the two patient groups, though in patients with abnormal scores before treatment the quality of life seemed better in the chemotherapy group.
In the chemotherapy group a transient slight deterioration in quality of life was noticed during treatment with cylotoxic drugs (fig 2), thereafter, quality of life improved compared with baseline and with patients in the supportive care group.
In the supportive care group supportive drugs resulted in an initial improvement of the quality of life.
The improvement was short lived, however, because of disease progression.
No significant difference was found between the scores in the two groups.
Discussion
Many doctors and their patients reject conventional anticancer chemotherapy for disseminated colorectal cancer in favour of unproved alternatives because of uncertainty about therapeutic gain and concern about toxic effects.
Whether and how much chemotherapy prolongs survival in advanced colorectal cancer, despite its use for several decades, is not known.
5-Fluorouracil, which was the standard treatment until one or two years ago, was not considered to prolong median survival, although a controlled trial against no chemotherapy has never been done.
Several trials comparing 5-fluorouracil alone with combinations based on biochemical modulation have indicated better objective tumour response with the combination treatment.
An improvement in survival with combinations of 5-fluorouracil and leucovorin, however, has been observed in only two of seven randomised studies, and the improvement was only moderate.
We found that combination chemotherapy with 5-fluorouracil, leucovorin, and cisplatin in patients with metastatic colorectal cancer increased time to progression and length of survival compared with those in patients given only supportive care.
Because of the small numbers of patients studied, our findings are only preliminary.
Nevertheless, the median increase in survival in patients receiving chemotherapy was six months, during which time, overall quality of life, was at least as good as in patients receiving supportive care.
In patients with symptomatic disease, the quality of life, in fact, seemed better in the chemotherapy arm despite its common association with mild to moderate gastrointestinal side effects.
The advantage of chemotherapy in previously asymptomatic patients might be questioned in view of the side effects and the initial, though only minor and transient, decrease in patients' subjective wellbeing.
However, our study was conducted before ondansetron became available, which in our experience would probably have prevented the initial decrease in quality of life in patients receiving cisplatin.
Furthermore, the overall gain in time to progression and length of survival was seen in patients born with and without tumour related symptoms.
The potential advantage of early treatment in asymptomatic patients with advanced colorectal cancer seems to have been confirmed by a trial from the Nordic Gastrointestinal Tumor Adjuvant Therapy Group.
Further advances in the knowledge of prognostic factors should help to identify subgroups of (asymptomatic) patients with a more prolonged clinical course, in whom a wait and see policy seems adequate.
Whether the addition of cisplatin to 5-fluorouracil and leucovorin had any influence on the apparent beneficial effect of palliative chemotherapy in this study remains uncertain.
Comparison of objective tumour response rate and median survival time in our chemotherapy group in phase II and III studies with conventional 5-fluorouracil and leucovorin regimens suggest that this is not the case.
Severe toxicity was less common in our study, however, and the three-drug combination seems to have a better therapeutic index.
In conclusion our data indicate that chemotherapy with 5-fluorouracil, leucovorin, and cisplatin improves quality of life in symptomatic patients with metastatic colorectal cancer and prolongs survival.
Although the small numbers studied reduces the strength of our results, they support previous, indirect evidence of a beneficial effect of chemotherapy in this disease.
In addition, the distribution of characteristics known to affect survival in colorectal cancer was similar in the two study groups.
Criteria for assessing palliative response through quality of life proposed by Presant et al 
Patients with normal pretreatment scores
Maintained complete palliative response — No global or subgrouping score drops below normal range for two or more successive measurements
Worse quality of life — A global or subgrouping score drops below normal range for at least two successive measurements.
The score is significantly below the baseline value or patient dies
Patients with abnormal pretreatment scores
Complete palliative response — All global and subgrouping scores increase to within normal range for at least two successive measurements
Partial palliative response — At least one global or subgrouping score increases significantly on at least two successive measurements, with no score decreasing significantly
Stable quality of life — No significant change in any quality of life score during study for at least two successive measurements
Worse quality of life — At least one global or subgrouping score decreases significantly on two successive measurements, or patient dies
Spontaneous resolution of severe chronic glue ear in children and the effect of adenoidectomy, tonsillectomy, and insertion of ventilation tubes (grommets)
Abstract
Objective —
To measure the time to spontaneous resolution of severe chronic otitis media with effusion (glue ear) in children and study the effects of adenoidectomy, adenotonsillectomy, and ventilation tubes (grommets).
Design —
Randomised controlled study over 12 years.
Setting —
Paediatric otorhinolaryngology clinics and inpatient unit.
Subjects —
228 children aged 2–9 years with pronounced hearing loss from glue ear and persistent bilateral middle ear effusions confirmed on three occasions over three months.
Interventions —
Children were randomly allocated to adenotonsillectomy, adenoidectomy, or neither procedure.
In all groups a Shepard type ventilation tube was inserted in one randomly chosen ear.
Follow up was annually for five years and then less often for up to seven years four months.
For analysis the two operated groups were combined.
Main outcome measures —
Otoscopic clearance of fluid, change in tympanogram, and improvement in mean audiometric hearing threshold.
Results —
Survival analysis showed appreciable otoscopic and tympanometric resolution of fluid with ventilation tubes alone and adenoidectomy alone compared with no surgery.
Further improvement was seen after combination of both treatments.
Mean audiometric hearing thresholds improved with fluid resolution.
Resolution was delayed in younger children and in those whose parents smoked, irrespective of treatment.
Whereas a single insertion of a Shepard tube resolved the glue for a mean (SD) period of 9.5 (5.2) months, the effect of adenoidectomy was sustained throughout follow up.
Conclusions —
Treatment of glue ear considerably shortened the time to fluid resolution, combined adenoidectomy and tube insertion being better than either procedure alone.
Resolution was longer in younger children and those whose parent(s) smoked, irrespective of treatment.
Introduction
Chronic otitis media with effusion, or glue ear, is the commonest reason in children for elective admission to hospital for surgery.
Possible treatment includes myringotomy with or without insertion of a ventilation tube (grommet) either alone or with adenoidectomy and occasionally tonsillectomy.
The criteria for surgical intervention are uncertain, and the decision to operate has always been considered difficult in view of the high rate of spontaneous resolution.
This is particularly associated with the child's age and the presence or absence of intercurrent infection, which is often seasonally related.
No data are available to show how long fluid persists in severe cases that would normally have been listed for surgery.
This randomised controlled study began in August 1979, and recruitment continued until June 1986.
Most of the children were referred by senior clinical medical officers (audiology).
Some were direct referrals from general practitioners.
The study was designed to show the outcome of chronic glue ear not submitted to any surgical treatment compared with that treated by adenoidectomy and adenotonsillectomy.
Insertion of a ventilation tube in only one ear enabled us to assess this procedure both alone and in combination with adenoidectomy and adenotonsillectomy.
In 1983 the initial results after 12 months were reported, which showed the effects with assessment by otoscopic examination.
In 1986 the results at 12 months were reported with tympanometry and audiometry.
The design of this study is now well known and has been analysed by other workers.
This report presents the long term findings up to 12 years after inclusion in the study.
Subjects and methods
The methods have previously been reported.
District ethical committee approval was given and informed consent obtained from the parents.
There were five entry criteria:(a ) age between 2 and 11 years (but none were over 9);(b ) pronounced subjective hearing loss;(c ) pneumatic otoscopic confirmation of fluid in the middle ear of both ears;(d ) tympanometry not showing a type A peaked curve (98% type B, 2% type C1 or C2); and (e ) in excess of 25 dB pure audiometric or free field hearing loss in each ear at one or more frequencies.
From an initial population of 472 children, 228 satisfied the five criteria at each of three preoperative assessments separated by intervals of six weeks.
The remainder were not followed up further.
By using tables of random numbers the children were allocated to three groups: adenotonsillectomy, adenoidectomy, and no surgery.
Each child then received a Shepard ventilation tube in a randomly chosen ear.
This allowed the child to ‘hear’ while the study progressed.
The contralateral unoperated ear received no treatment but was examined under magnification (6×) to confirm the presence of fluid.
For ethical reasons a new tube had to be inserted when the original tube extruded, fluid recurred in the operated ear, and the unoperated ear still contained fluid and showed subjective and audiometric loss of hearing.
New tubes were always inserted in the same ear.
The homogeneity of the three groups has been reported for a wide variety of preoperative variables including age, sex, type of fluid, allergy, seasonal variation, radiographic determination of the size of the adenoids and postnasal space, and duration of preoperative loss of hearing.
Assessment of the first 150 cases after one year in April 1983 confirmed that tonsillectomy had no additional benefit compared with adenoidectomy alone.
Children were then allocated at random to the two remaining groups of adenoidectomy or no surgery in addition to unilateral insertion of ventilation tubes until June 1986, thus accounting for the reduced numbers at follow up in the later years.
The unique design of this study allowed examination of  outcome in an unoperated ear in a group of children who did not receive surgery to either the tonsils or adenoid.
In effect, this group of ears represents the natural history of the untreated condition.
These data can be compared with the outcome in ears treated with a ventilation tube or adenoidectomy or both.
For the purposes of analysis the children who received adenotonsillectomy were grouped with those receiving adenoidectomy alone.
Homogeneity was maintained.
During follow up of the 228 children six were excluded because they moved out of the area or because of poor attendance.
Of these, four were from the no surgery group and two from the adenoidectomy group.
Of the final 222 children, 139 were in the adenoidectomy and adenotonsillectomy group and 83 in the no surgery group.
Table I shows the details of times of follow up.
The analysis focused on three procedures: otoscopy, tympanometry, and audiometry.
With few exceptions all otoscopic examinations were performed blind by a single validated otoscopist (RN).
The ears were examined and the presence or absence of fluid was noted before inspection of the case notes or pharynx.
Tympanometry was performed to assess the presence (type A, C1, or C2) or absence (type B) of a tympanometric peak.
Pure tone audiometry was carried out by an audiological technician or scientist under sound proof conditions unless, according to age, free field audiometric assessment was required.
Free field audiometry was carried out preoperatively in only 25 children, who were thus excluded from the audiometric analysis.
This did not notably affect the age distribution of either group.
A further six were excluded from the audiometric analysis because of unreliable preoperative audiometric data, leaving 118 in the adenoidectomy group and 73 in the other.
Audiometry and tympanometry were also performed blind.
The improvement in the mean of the pure tone audiometric thresholds at six frequencies from 250 Hz to 8 kHz was calculated for each follow up time.
The results of the postoperative otoscopic and tympanometric assessments enabled estimation of the duration of glue ear to be made.
From a questionnaire given to parents the reported duration of hearing loss preoperatively was added to the three months of observation and to the duration of the condition postoperatively to give an overall estimate.
Because of missing data on preoperative loss of hearing three cases of adenoidectomy and six of no surgery were excluded from the survival analysis.
Hence 135 cases of adenoidectomy and 77 of no surgery were considered.
Because of the fluctuating nature of the condition strict criteria were required for defining the time when the fluid ceased to persist; when type B tympanograms changed to type A, C1, or C2; and when an observation was stopped.
Each ear of each child was investigated separately, thus enabling the comparison of four different types of ear: unoperated, ventilation tube alone, adenoidectomy alone, and adenoidectomy in combination with a tube.
Fluid was said to persist until the ear was reported otoscopically dry or there was persistence of A, C1, or C2 tympanometric peaks on consecutive visits for a period in excess of 12 months.
The time was taken to be half way through the interval incorporating the change — for example, for a child in whom fluid was reported at year 3 but not at years 4 and 5 the fluid survival time was estimated to be 3.5 years.
Data from children whose ears contained fluid at the final assessment or at the last assessment before they were lost to follow up were excluded.
Data were also excluded when the child developed severe obstructive symptoms from enlargement of the adenoids or tonsils, often with sleep apnoea that required surgical treatment, and when surgery was required to the unoperated ear — for example, insertion of a ventilation tube for severe collapse of the tympanic membrane or for treatment of cholesteatoma.
Eleven children were excluded because of severe problems in the operated ear, which was treated by insertion of a long stay Goode type tympanostomy tube instead of a Shepard tube.
Obviously excluding more severe cases results in bias because the remainder are less severe.
Table II, however, shows that most excluded cases were from the no surgery group.
The principle of intention to treat was not used in this analysis as the purpose was evaluation of the untreated condition and the effects of treatments rather than their feasibility.
During the follow up period there were only five ears in which fluid recurred and persisted for at least one year after they had been clear for at least one year.
All had been operated on, four of them by combined adenoidectomy and insertion of a tube and one by insertion of a tube only.
statistical analysis
The mean number of tubes inserted and the mean follow up time for each group were compared by using Student's t test.
Survival analysis was carried out with the Kaplan-Meier product limit method, and the log rank test was used to test the significance of differences between the treatment groups.
Potential prognostic factors were investigated with Cox's proportional hazards regression model.
Regression coefficients were exponentiated to produce estimated relative hazards.
We calculated 95% confidence intervals for these hazards and used χ 2 to test the overall significance of the association of the categorical variables with survival.
Prognostic factors for mean hearing thresholds were tested for significance with analysis of covariance.
All the statistical computations were performed with the SAS software system.
Results
The number of insertions of Shepard ventilation tubes was recorded for each child, and it became apparent that ears treated with tubes alone required significantly more tubes (mean 2.48, SD 1.39) than  those receiving tubes in combination with adenoidectomy (1.52, 0.85; p=0.0001).
This was in spite of a longer mean time till exclusion or end of follow up for the adenoidectomy cases.
Follow up irrespective of exclusion was equivalent in the two groups (non-adenoidectomy cases mean (SD) 7.0 (2.5) years and adenoidectomy cases 7.2 (2.6) years; p=0.7).
duration of glue ear
The survival curves for both otoscopic (fig 1) and tympanometric assessments (fig 2) clearly show that fluid remains longest in the no surgery group.
This is followed by the ventilation tube only group and the adenoidectomy only group, whose survival curves are not significantly different.
The combined adenoidectomy and ventilation tube groups had the shortest duration of glue ear.
Table III shows the p values for the log rank tests of equality of survival between each pair of treatments.
There is a clear benefit of all three treatments over no surgery in terms of duration of fluid.
There is no difference overall between adenoidectomy only and ventilation tubes alone but a significant benefit of adenoidectomy and tube insertion in combination above any other treatment.
prognostic factors
By using forward stepwise Cox regression analysis, age at operation, tonsillectomy, and radiographic size of adenoids and airway were not significantly related to the duration of glue ear in either of the surgery or no surgery groups.
Parental smoking, age at onset, and duration of hearing loss before operation, however, were significant and are therefore included in the models.
Sex is also included as a controlling factor, although it was not significant.
The variables were categorised (table IV), and hazard rates were estimated relative to a baseline category (table V).
Regression analyses were carried out based on the results of both otoscopy and tympanometry but only those for otoscopic findings are reported to avoid repetition.
The only factor affecting the no surgery group was parental smoking.
Fluid in the ears of children whose parents did not smoke was more than twice as likely to clear than fluid in the ears of those whose parents did smoke.
For ears treated surgically there was no significant advantage of adenoidectomy alone over the baseline category of tube insertion alone.
Fluid in the ears of children receiving adenoidectomy and a tube, however, was more than twice as likely to clear than fluid in those receiving either procedure alone.
Once again parental smoking was detrimental to the clearance of fluid, and the sex of the child was not significant.
Those aged 2½years or less at the onset of hearing loss fared considerably less well than their older counterparts.
Table V also shows that the longer glue ear had been present before the operation the longer the overall duration of the condition.
This  suggests that early surgery would substantially reduce the overall time with fluid.
hearing thresholds
The improvement in mean hearing threshold since the preoperative assessment was calculated for each child at each follow up.
Table VI shows the mean results and the maximum possible number of valid observations for each group at each time point.
These decline due to exclusions made during follow up.
Analysis of covariance for possible factors affecting mean hearing thresholds was carried out for each of the four different types of ear.
Age at onset of hearing loss, sex, parental smoking, postnasal airway size measured by radiography, and duration of preoperative hearing loss were not significant.
Each model included presence or absence of fluid, presence or absence of tympanometric peak, age at operation (mean (SD) 5.48 (1.12) years), and size of adenoids measured by radiography,(1.48 (0.29) cm).
Follow up at seven and 10 years was excluded from this analysis because of the small amount of data for the models.
For all groups the main determinant of improvement in mean hearing threshold was the otoscopic presence or absence of fluid.
The resolution of fluid made a difference of between 4.2 (SE 4.4) dB and 21.2 (4.2) dB.
The improvement was significant at the 5% level on at least four of the six follow ups considered for each type of ear.
The presence of a peak was also shown to be significantly beneficial on several occasions, particularly in the adenoidectomy only group (maximum effect 7.6 (SE 2.4) dB).
Age at operation was found to be significant only for the adenoidectomy only group, with those who were older at operation facing a minor disadvantage (about 2 dB) between three and five years postoperatively.
Size of adenoids affected the gains in mean hearing thresholds of only those children receiving adenoidectomy.
An improvement of up to 13.1 (SE 2.6) dB was shown per cm size of adenoids removed.
Discussion
Our previous work and that of others has shown the spontaneous resolution of glue ear in the short term.
Our previously reported resolution rate at two to three years was similar to that of Leiberman and Bartal, who found clearance in 31% of 158 children with middle ear effusions untreated for two and a half years.
No data exist for outcome of the untreated chronic condition followed for more than five years.
Our study shows the long term spontaneous resolution of severe glue ear in children.
They had a history of hearing loss for an average of 18 months.
Fluid was present in both ears at three examinations over three months and was confirmed by a validated otoscopist and by tympanometry.
Nevertheless, without any treatment the condition improved as time passed.
As seen from table VI, the recall rates at five years were between 74% and 84%, at seven years between 51% and 64%, and at 10 years between 29% and 35%.
They were low in the later years partly because recruitment continued until 1986 and partly because of non-attendance.
Survival analysis showed similar improvements in otoscopic clearance and tympanometric change as a result of treatment with a ventilation tube alone or after adenoidectomy alone compared with no surgery.
Further improvement resulted from the combination of both adenoidectomy and insertion of tubes.
The median duration of glue ear assessed otoscopically reduced from 6.1 years without surgery to 3.5 years with tubes, 3.4 years after adenoidectomy, and 2.3 years with the combination of adenoidectomy and tubes.
Objective assessment with tympanometry shows longer durations because of its greater sensitivity compared with otoscopy (7.8 years, 4.9 years, 4.0 years, and 2.8 years, respectively).
Otoscopic and tympanometric improvement are shown to be associated with improvements in mean hearing thresholds.
Mean hearing thresholds are related mainly to the presence or absence of fluid in the middle ear.
The gain in hearing for each treatment group at six and 12 months was similar to that shown in other comparable studies.
Some relation between size of adenoids and thresholds of hearing has also been shown.
Children who have had large adenoids removed develop slightly better hearing thresholds than those who have had smaller adenoids removed.
Assessment of the effect of possible prognostic factors showed that children with parents who smoke and, in the case of surgical intervention, children who are younger (<2½ years) at onset of hearing loss have a less satisfactory outcome otoscopically and tympanometrically.
Because surgery reduces the postoperative duration of the fluid the earlier it is performed the shorter is the overall duration of the condition.
Unlike in other studies we did not find any perceptible effect due to sex.
These results agree with our other previously reported short term data in which the effect of adenoidectomy was related to the age of the child and, to a lesser extent, to the degree of nasopharyngeal obstruction caused by the adenoids.
More recently we have shown morphological differences in the  nasopharynx and base of the skull in children with glue ear, and these are probably related to age and growth.
This study was designed to evaluate the effect of adenoidectomy and adenotonsillectomy compared with no surgery and thus the data on effectiveness of treatment with ventilation tubes are somewhat a by-product.
During the follow up a mean of 2.48 (1.39) tubes were required to maintain adequate hearing ability in those treated with tubes alone compared with a mean of 1.52 (0.85) in the children also receiving adenoidectomy.
The two ears may not react identically with respect to insertion and reinsertion of tubes.
In this study the overall need for reinsertion may have been higher, particularly in the no surgery group, than if bilateral tubes had been used instead of one tube.
recommendations
Based on the data from this and other work we can suggest certain recommendations for the clinical management of children with severe established glue ear.
Owing to the spontaneous resolution seen before and after operation all children with glue ear should be examined with pneumatic otoscopy and tympanometry on at least two occasions over three months before the decision to operate is made.
The decision to combine adenoidectomy with insertion of tubes should consider the additional morbidity and mortality and also the probable need for an overnight stay in hospital for adenoidectomy compared with a day case admission for tube insertion alone.
The child's age is important in relation to adenoidectomy.
With increasing age the operation is more effective in resolving glue ear than in younger children, in whom such surgery may also be contraindicated for more general reasons of increased operative risk.
Initially insertion of tubes alone would be advisable in younger children with established glue ear unless there is coexisting gross adenoidal enlargement and chronic nasopharyngeal obstruction.
Parents should, however, be advised that repeat treatment may be required, particularly when insertion is carried out alone.
Adenoidectomy will considerably reduce the overall duration of glue ear.
Furthermore, it can be recommended more confidently in older children.
We have shown that the combination of adenoidectomy with insertion of tubes provides immediate restoration of subjective hearing ability because of the tube; longer term sustained resolution because of adenoidectomy; and also a possible reduction in the need for subsequent insertions of new tubes.
If bilateral cases of glue ear are treated with adenoidectomy and tube insertion there may be a case for bilateral myringotomy and aspiration of fluid but with insertion of only a unilateral tube.
This would reduce by half the complications due to tubes.
Finally, parents of children with glue ear should be advised to stop smoking.
Effect of influenza B virus infection on human performance
Anecdotal evidence suggests that influenza reduces people's safety and efficiency in their workplace.
Studies of experimentally induced influenza B in volunteers at the Medical Research Council Common Cold Unit showed that infection increased reaction times to stimuli occurring at varying intervals but that hand-eye coordination was unimpaired.
We report the results of two studies to determine whether natural influenza B similarly prolongs human reaction times and whether other performance indicators are affected.
Subjects, methods, and results
Altogether 178 volunteers aged 18–30 were studied.
The studies were approved by the ethics committee, and all volunteers gave written informed consent.
On recruitment to the first study 92 healthy subjects rated their mood and performed various tasks (a variable fore period simple reaction time task, five choice serial response task, repeated numbers detection task, free recall test, delayed recognition memory test, logical reasoning test, focused attention task, and categoric search task) that have been described previously.
Subjects who developed a symptomatic infection of the upper respiratory tract were retested while ill and again one month later when asymptomatic.
Subjects who remained healthy (controls) were retested two and three months after recruitment.
On recruitment to the second study 86 subjects, some of whom had symptomatic infections, rated their mood and performed 10 minute versions of the simple reaction time task and serial response task.
Subjects were retested one month later.
A nasopharyngeal swab and blood sample for virological testing were collected from all symptomatic subjects, and a blood sample was collected from convalescent subjects four to six weeks later.
A nasopharyngeal swab or paired serum samples, or both, were collected from healthy subjects.
All specimens were coded before virological testing.
Analyses of covariance were carried out in the first study, and t tests were used in the second study.
In the first study four of the 26 subjects with symptomatic infections had influenza B and showed a 38% increase in the mean time taken for the variable fore period simple reaction time task, from 320 (SD 24) ms when recruited to 440 (SD 90) ms when symptomatic.
One month later, when the subjects were asymptomatic, their reaction time had fallen  to 385 (SD 75) ms.
Their reaction time when symptomatic was also significantly longer than the repeat reaction time of the subjects who remained healthy (recruitment value 360 (SD 69) ms, repeat value 367 (SD 72) ms, second repeat value 373 (SD 124) ms)(p<0.05).
The subjects with influenza were 13% slower in performing the repeated numbers detection task than they had been on recruitment (recruitment value 555 (SD 79) ms, symptomatic value 627 (SD 132) ms, asymptomatic value 550 (SD 32) ms) and were significantly slower than the healthy controls (recruitment value 565 (SD 60) ms, repeat value 544 (SD 88) ms, second repeat value 549 (SD 72) ms)(p<0.05).
The infected subjects were also less accurate than the controls in the categoric search task (88% v 93%; p<0.05).
Influenza B had no effect on mood or on performance of the five choice serial response and focused attention tasks and free recall, delayed recognition memory, and logical reasoning tests.
In the second study seven of 72 subjects with symptomatic infections had influenza B, and there were 14 asymptomatic controls.
The subjects with influenza B showed a 19% increase in the reaction time for the 10 minute version of the variable fore period simple reaction time test when compared with the controls (symptomatic value 440 (SD 82) ms, control value 369 (SD 79) ms, p<0.05).
One month later the two groups had similar reaction times.
As before, influenza B had no effect on the five choice serial response task or mood.
Comment
The two studies showed that natural influenza B selectively impaired tasks in which the subjects were unaware when or where the target stimulus would appear.
Memory, logical reasoning, and hand-eye coordination were unimpaired.
These results could not be explained by expectations about the effects of influenza or by changes in mood, and they confirm findings at the Medical Research Council Common Cold Unit with experimentally induced illnesses.
Influenza B impaired performance of the simple reaction time task by about 20–40%, which is comparable to the 5–10% deterioration seen with alcohol consumption or work at night.
The subjects with influenza B were not so ill that they were confined to bed, so influenza could possibly endanger people's safety at work.
GENERAL PRACTICE
Visiting through the night
Abstract
Objective —
To describe the time distribution of visits at night and to evaluate trends in night visits from 1982 to 1992.
Design —
Analysis of a sample of one in 12 claim forms for night visits submitted over one year beginning 1 July 1991, and estimation of the number of night visits in previous years from data on payment.
Further information was obtained from performance indicators from the Department of Health.
Setting —
General practices responsible to Berkshire family health services authority.
Main outcome measures —
Times of night visits, proportion performed by deputies, and trend in number of night visits after adjusting for the increased hours during which visits are claimed.
Results —
The change in the hours for which payment may be claimed accounted for 33.8% (536/1584) of all night visits in the sample.
After visits during these extra two hours were excluded, claims increased by 38.7% from 1989 to 1992 and more than doubled in the past 10 years.
Use of deputies both in Berkshire and in England and Wales dropped by more than half since 1989.
General practitioners in Berkshire claimed 31.5 night visits per 1000 population in 1992.
Conclusions —
The increase in the number of night visits is only partly due to the change in hours during which visits may be claimed.
It is also due to a long term and possibly accelerating rise in demand.
This is despite a major reduction in the proportion of calls performed by deputising services, the use of which had been said to be the main factor increasing the numbers of night visits.
Introduction
There has been a considerable increase in the number of night visits claimed for by general practitioners since the introduction of their 1990 contract, which brought two changes to the relevant payment structure.
Firstly, visits could be claimed if they were requested and made between 2200 and 0800 (previously 2300 and 0700).
Secondly, a differential payment was introduced with a lower rate for visits made by deputising services and a higher rate for visits made by doctors in a partnership or as members of a non-commercial rota.
The higher rate of payment was set at more than twice the previous fee for night visits.
Does the extension in claimable hours alone account for the increase in the number of night visits?
The change in hours may compound an underlying increase in the number of claims, either because of increasing demand by patients or because the higher fee might encourage doctors to visit when previously they offered advice by telephone.
An increasing number of claims for night visits was recorded during the 1970s, and anecdotal evidence suggests that this trend continued both before and after the new contract, although no figures have been published.
The differential fee for night visits is intended to discourage general practitioners from using deputising services, but it is not known whether it has had this effect.
No studies have reported the proportion of night visits carried out by deputies before and after the 1990 contract.
During 1991 new forms were introduced which allowed details of up to 10 night visits to be claimed on a single form.
Berkshire family health services authority amended the new form from July 1991 to ask for the time the visit was made.
This makes it possible to calculate the number of visits performed within the previously claimable hours of 2300 to 0700 and comparisons can therefore be made with historical numbers of claims for night visits and with earlier published research.
My aims were to record the time distribution of night visits and to show the trend in claims in Berkshire over the past 10 years after adjusting for the change in claimable hours.
I also considered the number of claims made for visits by deputies before and after the 1990 contract and the ages of patients visited at different times of night.
Methods
Claim forms for night visits, each containing details of up to 10 visits, are processed for payment and then filed by practice and by the quarter of the year.
I estimated that a sample of one in 12 night visits performed by general practitioners in Berkshire in the year beginning 1 July 1991 would provide details of about 2000 visits.
I used a stratified sample technique in two stages to ensure an adequate representation of singlehanded and group practices.
Random numbers were used to identify one in six of each type of practice.
This identified five out of 30 single handed practices and 14 out of 85 group practices.
All item of service claim forms received from these practices by 1 September 1992 were retrieved and alternate forms were analysed, with details of night visits performed between 1 July 1991 and 30 June 1992 being recorded.
If there were an odd number of claim forms from any practice half of the visits from the outstanding form were entered.
The following information was obtained for each visit: practice code number, the quarter of the year in which the form was received, the patient's year of birth, the date and time of the visit, and the relationship of the visiting doctor to the claiming doctor — that is, partner, locum, assistant, trainee, member of a rota, or a commercial deputy.
Time was recorded in terms of the preceding whole hour — for example, 0750 was recorded as 07.
These data were recorded on a computer and analysed with the Excel (Microsoft) package and a statistics package for personal computers (Timberlake Clark).
Historical data on claims for night visits were derived from the manual payment ledgers of the family health services authority for each quarter for the 10  years beginning October 1982.
Payment quarters mainly reflect visits done in the preceding quarter but also include visits made and claimed for in the first month of the payment quarter.
The rates of payment for previous years were obtained from old amendments to the statements of fees and allowances.
The number of night visits made in each quarter was estimated by dividing the total sum paid by the fee payable for each visit in the preceding quarter.
There will be slight inaccuracies in these estimates as a few claims will be received in April each year and be paid at a new rate after a rise in fees, and some very late claims may be paid at an old lower rate.
These inaccuracies, however, are probably small and apply similarly in each year and therefore are unlikely to affect the overall results.
For quarters since April 1990 separate calculations were made for claims at high and low rates to calculate the total number of night visits.
The annual number of night visits was obtained by adding up the four quarterly totals ending in September in each year.
Further information about claims for night visits in Berkshire and in England and Wales was obtained by using the health service indicators supplied to family health services authorities by the Department of Health.
Results
The 14 group practices and five singlehanded practices identified by the sampling technique represented 66 or 16% of the 403 general practitioners responsible to Berkshire family health services authority, and the sample consisted of half of the claims for night visits from these doctors, a total of 1988 visits.
This was 8.0% (1988/24803) of all claims received from general practitioners in Berkshire in the four quarters.
Of these claims, 1535 (77.2%) were paid at the higher rate and 453 (22.8%) at the lower rate paid for visits done by deputising services.
The percentage of visits done by deputies varied widely between different practices, ranging from 0% to 66%.
The figure of 22.8% of claims at the low rate in the sample is higher than the rate for Berkshire as a whole (14.8%) in the same period because the largest practice in the sample used a deputy for 53% of its night visits.
The table shows the times at which visits were made.
Doctors did not always provide this information on claim forms so times were available for only 1591 (80.0%) of the visits analysed.
A disproportionate number of visits in the late evening were to young children.
After 2200 and before 0100, 217 (26.8%) of the visits were to children aged under 5 years whereas only 110 (14.2%) of the visits later in the night were to this age group (χ 2 =37.47, df=1, p<0.001).
Of the calls for which times were available (and excluding the seven visits which occurred outside the claimable hours), 536 (33.8%) were done between 2200 and 2300 between 0700 and 0800 (SE proportion 0.0119, 95% confidence interval 31.5% to 36.1%).
The trend in numbers of night visits claimed is shown in the figure.
Assuming that the proportion of visits done in the sample between 2300 and 0700 can be applied to the whole of Berkshire, it becomes possible to estimate the number of night visits which would have been claimable by using the definition of a night visit which applied before April 1990.
The figure shows this estimated number of visits with 95% confidence intervals.
The change in hours accounts for 64.7% (8393/12975) of the increase in the number of claims since 1989.
Analysis of the health service performance indicators shows a large drop in the proportion of night visits performed by deputising services.
In the quarter ending June 1989, 37.5% of night visits in Berkshire and 45.6% nationally were done by deputies; these proportions dropped to 14.8% in Berkshire in the year ending September 1992 and 28.1% nationally in the year ending March 1991 (calculated from payment data as the health service indicators no longer record the percentage of visits done by deputies).
Of all practices in Berkshire, 28.5% use deputies, which compares with the national average of 38.1%.
General practitioners in Berkshire claimed 31.5 night visits (20.8 visits between 2300 and 0700) per 1000 population in the payment year to September 1992.
In total the 403 general practitioners in Berkshire claimed 24803 night visits in the year to 30 September 1992, an average of 61.5 visits per doctor per year.
Discussion
Adding two hours to the period within which night visits may be claimed has caused a clear rise in claims, and this accounts for most, but not all, of the increase observed since the 1990 contract.
The change in hours compounds a long term upward trend in the number of night visits.
Even after visits made in the extra two hours are excluded there has still been an increase of nearly 39% in night visits from 1989 to 1992, and the number of night visits has more than doubled in the 10 years since 1982.
The data suggest that the underlying increase has accelerated since 1990.
This may reflect doctors' increasing willingness to visit or increasing consumer demand.
Fry noted that night visits had trebled in the seven years from 1967 to 1974.
Buxton et al showed an increase between 1967 and 1975 from 4.3 to 10.1 night visits per 1000 patient population per year.
The equivalent estimated figure from my research is 20.8 visits, which represents an almost fivefold increase in 25 years.
In terms of individual doctors this means an average practitioner will do a visit between 2300 and 0700 every nine days instead of less than once a month.
Several authors have described wide variations in the number of night visits, which have proved difficult to explain.
The most important factor was said to be a high use of deputising services associated with higher rates of visiting and less use of giving advice by telephone.
Two studies have shown that general practitioners can deal with a high proportion of night calls on the telephone, but Sheldon and Harris found that fewer than 3% of night calls received by two deputising services were dealt with by giving advice by telephone without a visit.
It has been argued that factors in medical organisation, such as the use of deputies, may be more important than variations in patient demand in shaping the number of night visits.
Since the introduction of a differential fee, however, the number of visits in Berkshire undertaken by deputies has dropped by more than half, yet overall numbers have risen steeply.
A national study is needed to see whether these trends are representative but little information about night visits is collected centrally.
The performance indicators which are produced consist mainly of data on payment, which makes useful analysis difficult.
Why a differential fee was introduced to encourage general practitioners to do their own night calls is unclear.
It seems ironic at a time of mounting concern about the excessive hours of junior hospital doctors.
It may have been from a belief that deputies provide a less good service, although published research has shown high patient satisfaction with deputies.
Alternatively it may have been because a rising number of night visits, increasingly performed before 1990 by deputies, increases health service expenditure as the cost of deputies is eventually reimbursed in the expense element of general practitioner remuneration.
Escalating numbers of night visits highlight the  problems of a pool system for paying doctors.
The new fee structure deters the use of deputies and means that increasing claims effectively lead to decreased payment for each visit.
I have shown that most night visits are made before 0100.
Between 0100 and 0600 on an average night only about 12 visits are made across Berkshire.
Hobday analysed night work in Maidstone in 1983 and showed that on an average night 26 general practitioners were on duty but did only 3.3 calls between them.
He argued that this was inefficient and that the workload at night in Maidstone could be dealt with by only two doctors.
Since about 35% of contacts at night are dealt with by telephone the amount of disturbance for individual doctors is considerably higher than the number of claims, and the stress created by night work is out of proportion with the small number of contacts with patients.
These issues have recently been discussed by Iliffe and Haug.
They predict a continuing rise in demand but argue that work out of hours is an essential part of a general practitioner's role.
They suggest that the boundaries of this work need reorganising and that a 24 hour commitment for most doctors is neither necessary nor justifiable.
A responsibility to the practice of 17 hours is proposed, with an emergency service being provided at other times by health authorities.
My proposal would be that general practitioners continue to provide 24 hour availability by telephone when their particular knowledge of an individual patient is needed.
A service between midnight and 0700 should be the responsibility of family health services authorities, which could arrange for visits and perhaps a night time surgery.
These authorities would contract with doctors who wished to work a night shift.
There would be no financial incentive to visit or the perverse effect of more night visits leading to decreasing rates of payment.
Few doctors would be needed at night by the family health services authority, and only a small proportion (less than 1%) of all patients' contacts with general practitioners would be affected.
If the trends shown by my research continue and the numbers of night visits double again within another eight years then a reorganisation of this sort will become essential.
EDUCATION & DEBATE
Six authors in search of a citation: villains or victims of the Vancouver convention?
Abstract
Objectives —
To analyse trends in the number of authors per article over the past 10 years.
Design —
Analysis of articles from random volumes of eight biomedical journals.
Subjects —
Cell, Nature, Proceedings of the National Academy of Sciences of USA (PNAS), Journal of Clinical Investigation (JCI), Biochemical and Biophysical Research Communications (BBRC), Journal of Clinical Oncology (JCO), New England Journal of Medicine (NEJM), Lancet.
Main outcome measures —
Median and modal numbers of authors.
Results —
All journals except Cell and Nature showed a trend towards increasing authorship numbers over the study period.
The trend was most noticeable in journals such as JCO which feature clinical research.
General medical journals (Lancet, NEJM ) with a median of six to seven authors per article published far fewer seven author than six author studies, which suggests that author number may be influenced by the Vancouver convention which precludes citation of more than six authors.
Conclusions —
The phenomenon of expanding authorship in biomedical journal articles is not explained by the hypothesis that newer research technologies have necessitated more extensive collaboration.
Rather, the data suggest that conferral of authorship may sometimes have a volitional component which contributes to rising author numbers.
It is proposed that replacement of the Vancouver convention with a ‘first author, last author’ citation system may help stem this rise in author numbers.
Introduction
Success in biomedical careers has long been associated with authorship of publications in peer reviewed journals, and this association may partly explain the exponential increase in the number of articles published over the past two decades.
A parallel trend which has been widely perceived but less well documented is that of increasing numbers of authors per article.
A large number of authors may be required in a study for various reasons:(a ) the need for a large team of technically specialised laboratory workers;(b ) the need to accrue scarce resources for study, such as clinically derived material (human tissue, blood samples) or patients with rare diseases; or (c ) the inherent labour intensiveness of a given project, such as cloning an unmapped gene or cooperative management of a large clinical trial.
Nevertheless, it is difficult for journal editors to assess the credentials of each author of a study.
The Vancouver convention (originated by Index Medicus and the National Library of Medicine ), which states that articles contributed by more than six authors are cited in reference lists as the first six authors et al , offers an indirect method of assessing the stringency of criteria for selecting authors.
This convention results in the last authors — who usually include the principal investigator — being dropped from the citation.
Since principal investigators may be assumed to influence the selection of coauthors, the popularity of the Vancouver convention raises a testable hypothesis — namely, that senior authors may favour six author articles over seven author articles.
I analysed the authorship number in eight journals over 10 years to test this hypothesis.
Methods
I selected random volumes of eight biomedical journals covering a broad spectrum of biomedical research:Cell, Nature, Proceedings of the National Academy of Sciences USA (PNAS), Journal of Clinical Investigation (JCI), Biochemical and Biophysical Research Communications (BBRC), Journal of Clinical Oncology (JCO), New England Journal of Medicine (NEJM), and the Lancet .
Volumes were selected from the period 1982–92 and only original articles were eligible for analysis (including letters but not correspondence in Nature ).
Short reports, reviews, editorials, and hypotheses were excluded, but original articles of non-biomedical interest (such as occur in Nature and PNAS ) were included.
Articles citing multi-institutional groups in the title or author attribution were designated ‘large group collaborative studies,’ and such studies were grouped with multiauthor (>12 authors) studies for analysis.
These multiauthor and collaborative studies were accorded 13 authors in the analyses.
Results
The table summarises the data obtained from the selected journals.
A total of 3366 original articles with over 18279 authors were evaluated.
Figure 1 shows  that distribution of authorship number was similar for Nature and Cell , although Cell had a higher proportion of studies with three to five authors.
The number of authors per paper did not increase between 1985 and 1991 in Cell and a small increase only in the proportion of articles with 8–12 authors was seen in Nature during 1982–92.
Two author studies made up the highest proportion of articles in both journals.
All journals other than Nature and Cell showed increases in median and modal author numbers over the study period.
The median and modal author numbers for PNAS and JCI increased by 20% or more.
The proportion of studies with two to three authors fell while the proportion with six to 10 rose, shifting the curve to the right.
The shifting of the distribution of authorship number towards the right was more pronounced in BBRC between 1982 and 1992 (fig 2).
A similar trend towards increasing authorship number was seen in JCO (fig 2).
This journal had the highest median and modal number of authors and published a high proportion of multiauthor or collaborative studies (21% in 1988).
Since neither the quality nor the acceptance rate of foreign submissions can be gauged from available data, however, the significance of this finding remains speculative.
This increased number of multiauthor studies is not confined to journals publishing clinical trials.
The NEJM , a general medical journal that publishes both clinical research and medically relevant basic research, also had a threefold to fourfold increase in the proportion of multiauthor or collaborative articles between 1985 and 1991 (table).
These studies accounted for 22% of published original articles by 1991.
This increase in large group studies is associated with about a 50% fall in articles by one to three authors (data not shown).
As with most of the other journals surveyed,NEJM had a general increase in authorship number as shown by the increase in mode from 5 to 6 (20%) and in median from 5 to 7 (40%) between 1985 and 1990 (table).
The distribution of authorship number showed a sigmoidal increase up to six authors followed by a steep fall to seven authors and more gentle fall to 11 authors (fig 3).
Similar trends were seen in the Lancet , another general medical journal.
The Lancet had a steeper fall from six to seven authors, perhaps because they published a smaller proportion of large group studies (and hence had a higher modal ‘6’ value) than NEJM .
Figure 4 shows how the rapid fall from six to seven authors is influenced by alterations in the overall distribution of authorship numbers.
Only a 3% absolute fall in the number of papers with seven authors was seen in the NEJM during 1985–9, when the modal author number was 5.
A 10% fall in seven author papers occurred during 1989–91 when the modal author number rose to 6.
This amounts to a relative frequency decline of over 50%.
Similarly, for the Lancet in 1985–9 the modal author number was 5 and there was an absolute fall in seven author papers of 6%, whereas in 1989–91 the mode was six authors and the fall in seven author papers was papers was 10%.
Again, this absolute reduction represents a relative fall of seven author articles of more than 50% when compared with the frequency of six author articles (fig 4).
Discussion
Three patterns of distributions of authorship number are suggested by these my data:(a ) the ‘left shifted’ pattern seen in high profile basic research journals (Nature ,Cell ), in which over 90% of articles have one to five authors (mode 2);(b ) the ‘right shifted’pattern characterised by few small group studies and many large group studies (mode 7) seen in low profile basic research (BBRC ) and clinical research (JCO ) journals; and (c ) a rapid fall from six to seven authors (mode 6) in general medical journals (NEJM ,Lancet ).
A threefold increase in the proportion of large group (>12 authors or multi-institutional) studies and a reduced proportion of small group (1–3 author) studies was seen in both general and specialist clinical journals.
Increased complexity of analytical methodology seems an unlikely explanation for this trend.
An alternative hypothesis is that clinical studies have evolved towards a larger format over the past decade, with a corresponding increase in the number of contributors.
A further possibility is that either the prestige or the style of the journals has altered over the study period, thus encouraging submission (or acceptance) of larger studies.
The rapid fall from six to seven authors seen in both general medical journals suggests a preference by some principal authors for six rather than seven authors.
This raises the possibility that in some instances the number of authors may be negotiable: by limiting the number of cited authors to six, last authors receive acknowledgment in citation listings.
If this theory were  true a similar pattern might be expected in other journals.
The different modal authorship numbers could explain why this was not seen.
JCO in 1991 is unlikely to exhibit a rapid fall from six to seven authors — for example, because the distribution is already shifted far to the right; conversely, basic research journals such as Nature and PNAS cannot exhibit the pattern until their modal author number increases to 6.
Moreover, the ongoing trend towards increasing author numbers in journals such as NEJM (in which median authorship had already reached 7 by 1989) indicates that the pattern may disappear again.
To determine whether this ‘Vancouver effect’ is real, a more exhaustive analysis is needed of all volumes of a wider spectrum of journals.
Despite its limitations, however, this study helps clarify some aspects of the phenomenon of expanding biomedical authorship.
Firstly, the hypothesis that increasing author numbers reflect the development of more labour intensive research technologies becomes untenable given the continuing high quality small group productivity seen in Nature and Cell .
Secondly, the apparent occurrence of a rapid fall from six to seven authors seen in some journals suggests that conferral of authorship may sometimes have a volitional component.
This phenomenon may either favour or oppose the career interests of potential coauthors.
In the case of an article written by five authors, for example, the availability of one extra slot may work to the advantage of a colleague whose goodwill is valued but whose involvement with the project had been only marginal; such circumstances may help create a dip in the number of articles with five authors (figs 2 and 4) which further accentuates the peak at six authors.
For papers with seven potential authors the principal investigator may decide to omit one author and instead acknowledge his or her help at the end of the article.
The excluded authors lose a publication, which may adversely affect their academic career.
Longer term consequences of this trend could include an increased counterproductive emphasis on first author publication.
The uncontrolled increase in the number of authors might be remediable to some extent by journals devising a collective policy.
One such strategy would be to cite references by first and last author et int (and intervening).
This would reduce the number of authors listed while still providing unambiguous access to the publication, acknowledgment of the primary researcher, and identification of the department or laboratory (usually that of the last author).
This should not reduce the usefulness of the reference, since the prime function of a citation is that of referring an interested reader to another work rather than that of publicly applauding the contribution of individual authors.
Paradoxically, then, reduced citation visibility for authors could result in middle authorship continuing to be regarded as an earned (and hence valued) privilege rather than as a right, a favour, a payback, or an inconsequential  bagatelle .
Countdown to Community Care
Care management and mental health
This is one of a series of articles looking at the forthcoming changes to community care
The community care reforms will produce a new kind of key worker who will organise and budget for packages of care: the care manager.
Care management goes live in April 1993 but is still poorly rehearsed and its performance may yet disappoint.
This overview sets out the origins of case management, its transformation into care management, and the principles guiding its practice.
To spell out how the concept works, plans for care management in Southwark's mental health services are described.
The roots of case management lie in social case work.
Within the specialty of mental health the central coordinating function was first recognised formally in the United States by the Community Mental Health Centers Act (1963) and its 1975 amendments, which explicitly required the centres to link with other agencies providing care for long term patients.
There has, however, been an increasing recognition in the United States over the past 25 years that community based services for people with long term mental illness have too often been fragmented.
Thus, methods of drawing together the components of care were developed, especially in federally funded initiatives such as the Community Support Program.
The principles most often ascribed to the concept of case management are outlined in box 1.
Continuity of care refers both cross sectionally, to a comprehensive range of services for people with long term mental illness, and longitudinally, to emphasise the need for enduring and possibly indefinite care for a substantial proportion of this group.
In practice case management for people with long term mental illness has developed into a range of techniques that can be described along 12 different axes (box 2), which aim to ensure that patients with long term psychiatric disorders receive consistent and continuing services for as long as they are required and that services do not focus inappropriately on patients with less severe conditions.
Case managers might give direct care to clients, in a model that emphasises the staff-patient relationship as the key component through which effective care is channelled, in the tradition of social case work.
Brokerage models, however, give the case manager a central and more distant coordinating function without any necessary direct contact with the patient.
Whichever model is used, case management offers the same range of tasks to individuals.
In Britain the concept of case management gained currency rapidly after 1985, when the House of Commons Social Services Committee's report on community care recommended that ‘the government give high priority to encouraging and monitoring the developing use of keyworkers.’
Sir Roy Griffiths took up the idea, under a different name, in 1988 in specifying that ‘no person should be discharged without a clear package of care devised and without being the responsibility of a named care worker.’
From case to care management
The 1989 white paper on community care,Caring for People , took the implementation of these ideas further: ‘Where an individual's needs are complex or significant levels of resources are involved, the government sees considerable merit in nominating a ‘case manager’ to take responsibility for ensuring that the individual's needs are regularly reviewed, resources are managed effectively and that each service user has a single point of contact.’
The provisions of the 1990 National Health Service and Community Care Act make the following statutory requirements of case managers: ‘Where it appears to a local authority that any person for whom they may provide or arrange for the provision of community care services may be in need of any such services, the authority (a ) shall carry out an assessment of his needs for those services and (b ) having regard to the results of that assessment, shall then decide whether his needs call for the provision by them of any such services.’
In 1991 case management was renamed ‘care management’ in a guidance document from the Department of Health and Social Services Inspectorate, on the grounds that the term ‘case’was demeaning to the individual and misleading in that it is the care, and not the person, that is being managed.
At the same time, however, central guidance made it clear that the version of care management now officially sanctioned was one in which direct service provision was not included, and that a brokerage model was therefore being endorsed.
This important change marked the introduction of the purchaser/provider split in social services practice, with the care manager clearly identified as a purchaser but not as a provider of services.
Implementing care management in Southwark
In Southwark the processes of assessment and care management have been planned through an interagency group comprising local authority officers, and representatives from all the health providers and purchasers and the voluntary sector.
The model of assessment which has been agreed has three levels (fig 1).
Firstly, there is a screening stage, which can take place in many community settings — for example, at social services offices, general practitioners' surgeries, and centres of voluntary organisations, or in hospital before discharge.
Information is recorded on  standard forms at screening and these are sent to the appropriate care management and assessment team, which is responsible for ensuring that each stage of the assessment and care management cycle is completed (box 3).
Decisions on who needs assessment and who has priority will usually be made within the social services department, but in hospital such decisions can be made with health care staff.
The team member making the main assessment of needs for community care uses a structured form with headings covering all areas of potential need.
Assessors can use an accompanying checklist to make a more detailed assessment of certain factors, allowing the resulting completed form to reflect more closely and personally the assessed person's needs.
If a more detailed assessment is necessary evaluation by a specialist can be requested, to be completed within an agreed timescale.
Throughout the process any informal carers should be consulted and should be offered their own assessments when appropriate.
People being assessed are also offered an advocacy service in case they feel that they would benefit from additional support.
The model of care management planned for Southwark is mainly one of brokerage.
In practice, however, social workers will still be able to offer personal help to their clients when this seems useful within the agreed care plan.
The local authority's obligation to provide services after assessment is a complex issue, with confusing messages coming from the Department of Health.
Nevertheless, Southwark will provide services based on clear and publicly available eligibility criteria.
Given the levels of need and the amounts of money transferred to Southwark for community care, there will probably be some unmet needs.
Information on unmet needs will be collated to inform future allocation of resources.
Assessing mental health in Southwark
This model of assessment and care management will be used for all client groups in Southwark, but in the mental health services some additional features, both legislative and professional, will be incorporated into the overall structure.
Some mental health assessments are covered by separate legislation in the Mental Health Act 1983 and these will continue as at present.
If such assessments result in admission to hospital the discharge policies under the NHS and Community Care Act 1990 will still apply.
Services offered to people with mental health problems after discharge from hospital may have to take into account the care programme approach, the NHS and Community Care Act, and section 117 of the Mental Health Act.
Southwark's model attempts to combine all the legislative requirements in one structure.
It does this, for example, by defining three broad levels of service support (each with a corresponding type of service response): low (individual care), medium (care programme approach), and high (care management), where access to each of these is determined by the results of specialist mental health needs assessment.
Access to mental health services often bypasses social services departments (unlike services such as those for the elderly); thus the bulk of the initial screening will probably be undertaken by staff not working for the local authority.
A high percentage of clients and patients will need complex interdisciplinary assessments.
These specialist assessments may be performed by a variety of mental health professionals including social workers, psychiatrists, community psychiatric nurses, and psychologists.
To make the process as uniform as possible the professionals will use standardised methods covering a range of possible problems, such as those included in the Camberwell assessment of need, which is being developed at the Institute of Psychiatry (box 4).
Discharge planning
Adequate discharge planning will be a key factor in introducing successfully both care management and the accompanying changes in residential care funding.
The past decade has seen an increasing amount of guidance from the departments of health and social security about the need for adequate discharge policies.
This guidance has become increasingly prescriptive, to the extent that the most recent document strongly reinforces the view that no discharge from hospital should occur without a clearly agreed and implemented discharge plan in place for each patient.
This guidance has almost certainly improved substantially the extent and quality of discharge planning.
In many cases, however, there are still important gaps between the services required and those available.
This may improve as arrangements for care management are gradually introduced but for the foreseeable future plans will have to reflect a realistic view of available resources.
This is particularly important if hospitals are to avoid becoming blocked with people awaiting the implementation of satisfactory discharge arrangements.
When patients no longer need inpatient care but still have complex needs one of the key roles of care managers will be to facilitate rapid transfer home or to an alternative community setting.
Residential care
One of the key motives behind the current legislative changes was to find a way of limiting expenditure on residential and nursing home care, which increased from £10m in 1979 to £1 billion in 1989.
This increase was accompanied by a substantial rise in the provision of residential and nursing home places by the private and voluntary sectors.
Given the projected growth in the number of elderly people likely to need support from health and social services, this increasing cost looked set to continue.
When legislative change was being considered, attention was often drawn to the so-called ‘perverse incentive’ that saved local services money if they supported people in residential care rather than in their own homes.
A clear intention of the changes was to enable care managers to buy domiciliary support, rather than residential care, where this was both more appropriate and inexpensive.
Research in Southwark, however, has suggested that only about 15% of recent referrals to residential care were inappropriate.
In April responsibility for funding residential and nursing home care will be transferred from the Department of Social Security to local authority social services departments over a phased period of four years.
Thus these funds immediately become cash limited.
Doubts about the adequacy of the total transfer to social services departments have already been expressed, and it is becoming clear that there may well be insufficient resources for some client groups.
This is most noticeable in the case of residential care services for drug and alcohol misusers, which have a high turnover of clients.
In turn, this has raised concerns about the viability of such services, and what will happen if many projects have to close while still caring for people.
This will probably be one of the flash points as the new joint arrangements for assessment and care management become established.
Two other potential problems loom large.
Firstly, fundholding practices may find their obligation to buy community care services a disincentive from pressing for early discharge.
Secondly, patients in hospital may also avoid hasty discharge to residential or care homes if they face means tested charges.
Earlier and more thorough planning may be needed before discharge, and interagency troubleshooting arrangements may have to be set up should difficulties arise.
Care management budgets
The formula dictating how social security budgets are transferred to local authorities discriminates against Southwark and most other inner city authorities in London which have few existing private and voluntary providers within their boundaries.
The formula rewards authorities with large numbers of local independent providers and does not allocate resources to the authorities from which residents using that independently run care originated.
Thus Southwark, which exports 70% of adults needing residential care, will not receive adequate funding to pay for future placements.
This is compounded by the relatively high level of psychiatric morbidity in areas such as Southwark and by a 11% reduction in NHS funding resulting from a new weighted capitation system, which is diverting money from inner south London to Kent and Sussex.
Guidelines from the Department of Health say that 85% of the social security element of the funding transfer should be spent on  residential care for elderly people with a further 5% on their day care.
Clearly, this leaves the other adult groups starved of resources for residential placements.
Southwark, therefore, proposes spending less — 80% — of the transferred monies on services for elderly people.
Of the remaining 20% of the total transfer people with mental health problems will receive approximately 6%.
This sum is inadequate and, even with rigorous prioritisation, will lead to some patients and clients having to be supported in the community when their needs could be met more adequately in residential care.
Southwark's model of care management will not devolve financial responsibility for community care to care managers.
In the first year, budgets will be held by more senior staff and decision making on a day to day basis will be delegated to team managers.
Very clear gatekeeping procedures and stringent setting of priorities will be needed to prevent overspending.
Care management as a purchasing function
Purchaser and provider divisions have been established in both health and social services authorities, but the splits occur at different levels.
In social services departments purchasing work is oriented much more towards individuals than in the NHS, where it is oriented around services.
This difference will not affect care management much in cases where social services teams are the main purchasers — for example, in services for people with learning disabilities.
It is, however, much more important where the health  authority is the main purchaser of community services — for example, in services for mental health, particularly in those for people with long term and severe mental illness and complex needs.
For such patients decisions by care managers to commit resources will have to be integrated with the decisions about the deployment of health resources made by doctors, usually consultant community psychiatrists with specific responsibilities for discharge and aftercare under the Mental Health Act and care programme approach working in mental health multidisciplinary teams.
Mechanisms must be set up to ensure that commissioners in both health and social services receive feedback about needs assessment.
This will be necessary if services in the NHS and those bought by care managers are to relate more closely to individual needs.
This feedback will be particularly necessary in districts where care managers do not hold their own budgets.
New perverse incentives
The implementation of care management in April could rapidly illustrate how the admirable intentions of the government's community care policy might founder on the unintended consequences of more powerful forces and contradictions.
Firstly, although the transfer of social security funds will allow more needs driven services, it will also cash limit expenditure.
Secondly, this transfer punishes local authorities with few residential care homes and cuts off money for future expansion.
Thirdly, the directive that care managers should not themselves give direct care runs counter to the core of good social work practice and creates a new corps of care administrators, thereby reducing the number of staff available to give direct care.
Fourthly, no central guidance has emerged on how to coordinate at the local level, care management, the care programme approach, and hospital discharge procedures, thus inviting triplication of planning effort.
Fifthly, conflicting central guidance is emerging about the statutory requirements to provide services for people whose assessments show up unmet needs, or even to inform people about the results of assessment.
Finally, the distinction between health and social care is proving much less clear in practice than in concept, and long running boundary disputes between agencies could erupt unless the problem is considered specifically in joint planning forums.
Such joint planning is taking place now in Southwark.
Managing care management carefully
There is the ever present danger that insufficient overall funding will drown the potential benefit of the community care reforms.
And, for the current volleys of reforms to hit their targets, several extra initiatives will be required.
When needs assessment information is fed into discussions on commissioning and planning it will probably highlight the need for district health authorities and local authorities (and, increasingly, general practitioner fundholders) to commission many community services jointly.
Joint commissioning arrangements will allow specific gaps in service provision to be filled.
Variations in joint commissioning practice between social services and health services will have to be piloted and monitored carefully.
Agencies will have to agree on definitions of needs and how people with different degrees of need will be prioritised when services are rationed.
The division between health and social needs can be narrowed by joint training.
Agreed procedures for appeals, complaints, and arbitration should be set up for users, and for authorities in dispute.
Finally, models of care management must be tracked carefully and evaluated to show whether brokerage is the hub or the rub of community care.
Box 1 — Principles of case management
Continuity of care
Accessible services
Staff-patient relationship
Titrating support to need
Facilitating independence
Patient advocacy
Advocacy for services
Box 2 — Twelve axes to define case management in practice
1 Individual/teamcase management
2 Direct care/ brokerage
3 Intensity of interventions
4 Degree of budgetary control
5 Health/social service function
6 Status of case manager
7 Specialisation of case manager
8 Staff to patient ratio
9 Degree of patient participation
10 Site of contact
11 Level of intervention
12 Target population
Box 3 — Stages in care management and assessment
Stage 1: Publishing information — Making public the needs for which assistance is offered and the arrangements and resources for meeting those needs.
Stage 2: Determining the level of assessment — Making an initial identification of need and matching the appropriate level of assessment to that need.
Stage 3: Assessing need — Understanding individual needs, relating them to agency policies and priorities, and agreeing the objectives for any intervention.
Stage 4: Care planning — Negotiating the most appropriate ways of achieving the objectives identified by the assessment of need and incorporating them into an individual care plan.
Stage 5: Implementing the care plan — Securing the necessary resources or services.
Stage 6: Monitoring — Supporting and controlling the delivery of the care plan on a continuing basis.
Stage 7: Reviewing — Reassessing needs and the service outcomes with a view to revising the care plan at specified intervals.
Box 4 — Problem areas covered by the Camberwell assessment of need
Accommodation
Occupation
Specific psychotic problems
Psychological distress
Information about condition and treatment
Non-prescribed drugs
Food and meals
Household skills
Self care and presentation
Safety to self
Safety to others
Money
Childcare
Physical health
Alcohol
Basic education
Company
Telephone
Public transport
Benefits
Acute bacterial meningitis in young adults mistaken for substance abuse
Bacterial meningitis can present with acute disturbance of behaviour which may closely mimic substance abuse.
Patients admitted to the casualty department with disordered behaviour present a considerable diagnostic challenge.
An organic cause (acute confusional state or brain syndrome) may be differentiated from an acute functional psychosis by the presence of abnormal neurological signs, particularly clouding of consciousness.
Psychiatric symptoms, however, are poor diagnostic discriminators.
In urban casualty departments an important cause of acute brain syndrome occurring in adolescents and young adults is the abuse of therapeutic or recreational substances, including alcohol.
In south east Queensland the use of hallucinogenic leaves and petals of ‘angels' trumpets’(Brugmanasia sauveolens ) is a particularly common cause of acutely disturbed behaviour.
We present the case histories of two young adults in whom acute behavioural disturbance, initially diagnosed and treated as substance abuse, was the presenting manifestation of acute bacterial meningitis.
Case 1
A previously well 15 year old, who had been living with friends, became acutely violent and confused shortly after his return home.
Recreational drug abuse was suspected by his family and, with the help of the police, he was taken to the casualty department at the Princess Alexandra Hospital.
During the initial examination he remained aggressive and confused.
The axillary temperature was 37.3°C and he had a tachycardia of 110 beats/min.
Detailed neurological examination was impossible but no focal abnormalities or neck rigidity were noted.
To facilitate further investigations the patient was sedated and ventilated.
As he was thought to have abused an hallucinogenic drug, he was given activated charcoal and sorbitol by nasogastric tube.
A white cell count of 30.1×10/l, a negative urine drug screen, and normal results on cranial computed tomography prompted examination of the cerebrospinal fluid.
Lumbar puncture, performed three and a half hours after admission, showed turbid cerebrospinal fluid containing 1500 white cells/µl (100% polymorphs), protein 5.7 g/l (normal 0.15–0.45 g/l), and glucose 1.2 mmol/l.
The Gram stain showed intracellular diplococci, later confirmed as Neisseria meningitidis .
He was given benzylpenicillin with cefotaxime and transferred to the intensive care unit, where he subsequently made an uncomplicated recovery.
Case 2
A previously healthy 34 year old man was arrested after he was discovered defaecating in a neighbour's living room.
He required physical restraint and was then taken to the local psychiatric hospital by the police.
On examination the patient was extremely agitated and appeared to be hallucinating.
His axillary temperature was 37°C, and his heart rate was 90 beats/min.
Further assessment was interrupted when the patient had a seizure.
Acute self poisoning was suspected, and he was transferred to the Princess Alexandra Hospital.
Following admission he remained aggressive and required physical restraint.
He was then sedated and ventilated to facilitate further examination and investigation.
Activated charcoal with sorbitol was administered by nasogastric tube.
Subsequent investigations showed normal results on cranial computed tomography with only alcohol present in the urine drug screen.
A full blood count, measured in blood taken on admission, however, showed a leucocytosis of 36.9×10/l (89% neutrophils).
A lumbar puncture, performed five hours later, showed turbid cerebro-spinal fluid containing 2880 white cells/µl (100% polymorphs), protein concentration 6.4 g/l (normal 0.15–0.45 g/l), and glucose 5.1 mmol/l (blood glucose 8.8 mmol/l).
Gram staining and culture gave negative results.
Following penicillin and cefotaxime administration he was transferred to the intensive care unit, where he made a complete recovery.
Discussion
Patients admitted to casualty departments with acutely disturbed behaviour present a major diagnostic challenge.
The differential diagnosis includes both primary psychiatric illness and a wide range of organic acute brain syndromes, including substance abuse.
Certain toxic syndromes indicating a specific substance may become familiar to medical staff owing to frequent local abuse.
An infusion of B sauveolens , regularly used in the Brisbane region, causes a central anticholinergic syndrome with wildly disturbed behaviour, visual hallucinations, and dilated pupils.
There was an initial suspicion that this poison had been ingested by our two patients.
Many infections of the central nervous system may have prominent psychiatric manifestations, leading to initial misdiagnosis, psychopharmacological intervention, and delay in starting appropriate treatment.
Both the young adult patients described above had pyogenic meningitis.
Both developed wildly disturbed behaviour of sudden onset with clouding of consciousness but without other, more typical, features of central nervous infection such as fever or neck rigidity.
In each case the primary diagnosis was substance abuse, and initial treatment was given accordingly.
An important clue, however, to the true diagnosis in both was the finding of leucocytosis.
Diagnostic lumbar puncture and administration of antibiotics were delayed while cerebral computed tomography was performed.
To facilitate this investigation both patients were sedated and ventilated, thus preventing further neurological assessment.
There is disagreement among neurologists as to whether lumbar puncture should proceed without tomography in these circumstances.
However, if an immediate scan is available then most would advocate its initial use to minimise the risk of coning.
To avoid potentially disastrous delays in treatment clinicians should be aware that bacterial meningitis can present quite atypically, with the sudden onset of severe behavioural disturbance closely mimicking substance abuse.
Previously well patients presenting with acute behavioural disturbance and leucocytosis without clear evidence of substance abuse require urgent lumbar puncture and appropriate antibiotics.
If immediate cerebral computed tomography is available it may be used before lumbar puncture, although the patient should be given broad spectrum antibiotics first.
This is particularly important should short term ventilation be required, and antibiotic therapy should never be withheld while the results of investigations are awaited.
Tackling inequalities in health: the Australian experience
Federal and state governments in Australia have embarked on a series of national initiatives which show a firm commitment to tackling social inequalities in health.
The development of national goals and targets for health, for example, covers social and environmental conditions and sets differential targets for specific social groups with very poor health status.
In a complementary initiative, a wide ranging analysis of the health care system — the National Health Strategy — has as one of its main objectives to improve the equitable impact of the health system.
Where problems of access to and quality of services have been exposed, policies have been devised to deal with them.
The exceptionally poor health of the Aboriginal community has elicited cross party support for action.
Resources have been allocated to implement the National Aboriginal Health Strategy: to improve living and working conditions, education, and employment opportunities.
Britain can glean much from the Australian experience.
For over a decade the medical and nursing professions in Britain have taken a leading role in calling for a clearer national commitment to tackling social inequalities in health.
Sadly, these calls have not been heeded because in the United Kingdom this has been seen as a party political issue instead of an important pointer to priorities for action.
The latest missed opportunity on this front was the 1992 Health of the Nation English health strategy, issued by the Department of Health.
This said practically nothing to stimulate action in relation to the systematic health differentials between social groups in Britain.
Yet nobody, of whatever political party, can seriously want these differentials to continue if they can be changed.
What might a ‘national commitment’ look like, and how might the rhetoric of concern for inequalities be translated into concrete action?
Recent developments in Australia suggest that in some respects that country is further along the road than Britain in formulating a policy response to the obvious inequalities found there.
Australian initiatives are therefore worth closer inspection.
Stark contrasts
Australia can be described as one of the healthiest countries in the world, but also one with some of the most dramatic social inequalities in health.
In 1990 the expectation of life at birth stood at 80 years for women and 73.9 years for men, higher than that expected in New Zealand, the United States, and the United Kingdom.
In the same year infant mortality was 8.2 deaths per 1000 live births, comparable with countries such as Norway and the United Kingdom.
Other measures of health show promising improvements.
Yet huge inequalities in health exist between different social groups within the population.
The most striking differences are to be found between Aborigines and the Australian population as a whole.
For example, expectation of life at birth for Aborigines is some 15–17 years less than that for the total Australian population, and infant mortality is three times greater.
Differences in adult mortality reach a peak for the 35–44 year age group, with Aboriginal men dying at a rate more than 11 times that of the total male population.
Aboriginal women in that age group have death rates around nine times those of the total female population.
Aborigines form a relatively small subgroup of the population (1.4%).
Substantial differentials can, however, be observed if comparing the top 20% of the population with the bottom 20% on a scale of ‘occupational prestige.’
For example, men in the lowest 20% of the population on this scale have death rates almost double those of the top 20%.
Using equivalent family income, the poorest third of working age men reported 65% more serious chronic illness than the richest third, and for women there was a 33% difference between rich and poor.
The Australian health care system is immensely complex, at least to outsiders.
A universal system of health insurance known as Medicare, funded from general taxation, was introduced in 1984.
All residents of Australia are eligible for benefits under the scheme, and services are provided on a fee for service basis.
In primary care a national schedule of fees has been established.
Patients are reimbursed for 85% of the schedule fee for each item of general practitioner service and for specialist consultations outside hospitals.
Diagnostic services attract lower benefits, while some services provided by professions allied to medicine are not covered by Medicare.
Doctors do not have to stick to the schedule fee but can choose to bill the Medicare administration directly, rather than the  patient.
If they do this, the doctor is paid the 85% of the fee covered by Medicare and the patient pays nothing.
This bulk billing, as it is called, is becoming increasingly common, with 70% of bills for general practitioners' services settled accordingly.
For secondary care there is a mix of public, private, and charitable hospitals.
About 40% of funding for the public hospitals comes from the federal government, nearly 50% from state governments, and the remainder from private health insurance.
The public hospitals are managed by the states, and the federal government has very little direct say in them.
For hospital services the Medicare benefit is only 75% of the schedule fee, but patients can insure privately to cover the gap between benefit and fee.
In practice, people who choose to be public patients in public hospitals pay nothing and the hospital receives a capped grant to provide these services.
People who choose to be private patients in public hospitals have 75% of the schedule fee reimbursed by Medicare and pay any additional fees through private insurance, if they have it.
Since the introduction of Medicare in 1984 the proportion of the population covered by private insurance has dropped from 62% in 1983 to 42% in 1992, with a large decrease in coverage among young people.
In contrast, in Britain only about 11% of the population has private insurance and coverage is restricted to a limited range of medical services.
A special feature of the Australian system is compulsory community rating.
In this, insurers have to pool good and bad risks and charge a standard premium to all subscribers.
One problem is that as younger, fitter people have dropped out of the pool, premiums have had to rise dramatically.
Setting goals and targets
Against this background, several policy initiatives suggest that the issue of inequalities in health has been taken seriously in Australia.
For example, tackling inequalities has been an aim of successive Australian efforts to set national goals and targets for health.
Considerable advances have been made compared with other countries.
As early as 1985 the Better Health Commission was set up to report on the current health status of the Australian population and to identify underlying health problems and strategies for dealing with them.
An early report from the commission stated that: ‘in determining health needs and priorities for action, the inequalities in health due to social, economic and environmental factors must be recognised, and, when possible, addressed.’
Following the commission's recommendations, the Health Targets and Implementation Committee was set up by the state and federal health ministers to develop national goals and targets.
Its report,Health for All Australians , published in 1988, also emphasised the importance of wider determinants of health and of reducing inequalities, though it was not able to set goals and targets in these areas at that time.
Like the Better Health Commission, it concentrated on targets for preventable mortality and behavioural risk factors.
In 1991, however, a year long review and revision of existing targets was initiated, and the results were published in February 1993.
These latest goals and targets have been used to make more explicit the challenge of achieving greater equity in health.
This is done in two ways: by setting specific targets for disadvantaged population groups and by spelling out the need to change key determinants of health beyond the health care sector.
Proposals on these key determinants range from targets to improve overall literacy, employment, and housing through to describing the unequal health impact of the physical environment on different social groups.
Box A lists the sectors covered under the heading of ‘healthy environments’ and gives an illustration of goals and targets for certain Aboriginal communities and for low income private tenants.
In contrast, most countries, including England, have set universal targets which run the risk of masking important health differentials.
The way in which the goals and targets were developed in Australia also differs noticeably from the English experience, where the whole process was controlled from within the Department of Health.
In Australia a group of academics acted as consultants to the federal Department of Health, Housing, and Community Services and were given the freedom to consult and encourage participation by as wide a range of people as possible.
Formulation of each target involved many people and agencies with experience in the relevant sectors.
They linked in, for example, with Aboriginal community leaders working on the National Aboriginal Health Strategy and with the housing and transport sector during the development of the healthy environment targets.
The target setting exercise has already had modest success in convincing ministers to incorporate some of the goals and targets in the five yearly hospital funding agreement drawn up between state and federal governments.
All state and federal health ministries  will also be required to review their work in the light of these goals and targets.
Though a bold start, the exercise has still to tackle the problems of how the targets are to be achieved or the deeper involvement of other sectors with an influence on the key determinants of health.
In this respect it is at a similar stage to the British initiative.
National strategy
A complementary development has been the National Health Strategy initiative.
This is a two year review and strategy development programme set up in November 1990 by the deputy prime minister, Brian Howe.
He took over the health brief with a reputation as a social policy reformer, having previously reviewed the social security system with the aim of making it fairer.
He replaced a health minister who had been widely regarded as extremely successful in increasing access to health services through the introduction of Medicare.
Nevertheless, new problems of access were appearing on the horizon, as were issues of cost containment.
These concerns, together with the need for the new minister to make his mark, resulted in Howe commissioning a wide ranging analysis of the health care system.
Box B summarises the terms of reference of the strategy initiative.
Attention was not restricted to curative and caring services but extended to activities fostering good health, including health education, promotion and public health.
From the beginning, the importance of considering equity issues was emphasised by the director of the strategy unit, Jenny Macklin:
One major objective will be to improve the equitable impact of the health system.
As poor health is linked to socio-economic circumstances, the strategy will work to reduce inequalities in terms of cost, access and health status.
Options for change in other areas to be covered by the Strategy will be analysed to ensure that any changes maintain or improve equity.
Equity was seen primarily in terms of access to services on the basis of need rather than social status, and financing of services according to means.
As the initiative has progressed, some undoubted strengths of having a national commitment of this nature have emerged.
There is clearly an advantage in having the initiative linked to the political process at federal level, with the personal backing of the deputy prime minister, and with a director, Jenny Macklin, possessing respected political and analytical skills.
This has guaranteed not only resources but also access to national databases not always open to others.
It has also ensured that some of the proposals, particularly those concerned with improving access to health services, have been incorporated into the federal budget plans for 1992–3 and the five yearly hospital funding agreements between state and federal governments.
Another strength of the initiative has been the sophisticated analyses it has been able to produce.
Eleven background papers and four major issues papers have been prepared in this way.
True to the director's stated objective, all the analyses and policy options stemming from them have paid attention to the effects of the various proposals on access to services and costs borne by patients.
The strategy unit has marshalled the efforts of several Australian research groups to carry out new analyses on socioeconomic factors.
From this work, a substantial research report was published in September 1992 on how income and environment affect health, on a scale and depth approaching that of the Black report in Britain.
Unlike the Black report, however, the Australian document received official blessing, launched by the deputy prime minister himself.
The report concludes that, in addition to making health services more equitable, policy needs to be focused on five broad areas: the distribution of economic resources; education; living conditions; access to and conditions of work; and the provision of social support.
It is too early to say what the government response will be to this report in terms of committing resources and policy initiatives.
But it has already had an impact and led to some innovative developments at other levels — for example, on the need to improve working conditions and workplace reform.
The strategy unit has been engaged in raising awareness and educating managers and trade unionists all over Australia.
A national workshop on the issue brought together management and union representatives, and similar activities have taken place in each state.
The aim of the exercise is to encourage commitment to reform of working arrangements, so that joint work on practical strategies could then begin.
One proposal, put forward and agreed by participants at the national workshop, was the establishment of a ‘best practice’ programme to which agencies could apply for funds to redesign work.
Guidelines have been agreed by management and union representatives and have been put to federal ministers.
In the meantime, the government of Western Australia has started such a scheme.
Funds have been allocated to facilitate workplace reform and the unions in Western Australia have put forward a job redesign package as part of their current wage discussions.
Another innovation has been in the area of public participation.
The strategy unit employed the Australian Council of Social Services and the Consumer Health Forum to conduct a ‘consultation on health inequalities’ around Australia.
Focus group discussions were held with Aborigines, people with disabilities, lone parents, consumer advocates, and other special needs groups.
This was an attempt to gather the views of people who are often underrepresented in formal consultation processes.
The possible long term influence of the National Health Strategy is difficult to predict.
It needs time to develop its policies on tackling inequalities in health, but with the Australian general election on the 13 March, its days may be numbered.
This is one of the dangers of its strong political ties to one party and one politician in particular.
Improving access
The National Health Strategy initiative has been particularly valuable in documenting the evidence on inequalities in access to and quality of healthcare.
The introduction of Medicare in 1984 improved access to  services, but underlying problems remain.
Two examples illustrate some of the dilemmas the strategy has exposed and the policies initiated to deal with them.
Firstly, although every Australian has an equal right to be treated in a public hospital, the income that can be derived from treating a privately insured patient is greater than that for treating a public patient in the same facilities.
Fees can be obtained from Medicare and topped up with private insurance.
This incentive, combined with a period of cost containment in which public hospital funding from state and federal governments has been under increasing pressure, has created even more of a temptation to attract privately funded patients in preference to patients solely supported by public funds.
In these circumstances, there is evidence to suggest that patients are sometimes selected on the basis of insurance status rather than need.
In 1990, for example, 73% of all acute hospital beds were public, but only 61% of total hospital bed days for acute services were allocated to public patients.
There is also evidence that privately insured patients are being offered a higher quality of clinical service in public hospitals in terms of shorter waiting times for elective surgery and in gaining the attention of more experienced doctors.
In an attempt to deal with this discrimination against public patients, the 1992–3 federal budget plan is introducing bonus payments to those states which increase the proportion of public patients treated.
There are also penalties that will apply if the proportion falls below a particular level.
The aim is to ensure that state governments and hospitals do not have a financial incentive to take private patients over public patients.
An additional £600m ($A1.36bn) over five years has been allocated for this.
Secondly, the strategy has identified that access to outpatient and non-urgent casualty services run by public hospitals also seems to be declining for public patients.
State governments have been closing these hospital based services to help save money on their capped budgets, on the assumption that patients will be able to obtain such services in the community, funded by the open ended Medicare budget from the federal government.
The problem is that the patient often has to pay a portion of the cost for the community based services.
This puts services which used to be available free from the hospital out of reach of patients on low income and those with chronic health conditions who require multidisciplinary care.
Now that the problem has been identified, federal and state health ministers have agreed in principle that this type of service should be a federal responsibility and discussions have started on the practicalities.
It is assumed that if all medical services are the responsibility of one level of government, then there will be no perverse incentive to close outpatient departments and thereby shift the cost from state to federal funds.
Tackling Aboriginal issues
Heightened awareness of Aboriginal health inequalities, coupled with acknowledgement of how disadvantaged Aborigines are in other aspects of life, has elicited cross party support for action to improve the situation.
Policy development has come from two main sources.
Firstly, a National Aboriginal Health Strategy has been developed by a 19 member working party, 14 of whom were leading members of the Aboriginal community.
They undertook exhaustive consultation with Aboriginal people and others to involve them in the policy formulation.
Secondly, a royal commission, originally set up in 1987 to investigate the alarmingly high rates of Aboriginal deaths in police and prison custody, expanded its brief to become a wide ranging inquiry into the conditions under which Aboriginal and Torres Strait Islander communities live.
Recommendations from these two sources were in broad agreement on what was needed: improvements in living and working conditions, better educational and employment opportunities, improved access to and quality of services, control by the Aboriginal community over services, and a say in making policies that affect them.
There were also calls for better information systems to monitor conditions.
In response to the National Aboriginal Health Strategy the federal government has allocated an additional £108m ($A232m) over a five year period from 1990 to 1995.
Three quarters of this is to be spent on improving housing, water supplies, sewage, roads, and communication networks to Aboriginal communities.
The remainder is to establish community controlled health services and facilities and includes substance abuse, education, and prevention projects.
The royal commission, reporting in 1991, noted that the money allocated was only one fifth of the estimated full cost of implementing the strategy and was therefore inadequate.
The federal government's response to the royal commission's recommendations included allocating further resources for job creation and encouraging economic independence in Aboriginal communities.
Recommendations on appointing representatives from Aboriginal communities to various local and national policy boards have been implemented.
A start has therefore been made on improving the social and economic conditions influencing Aboriginal health, but of course much more remains to be done.
Perhaps the most interesting feature of the experience so far for Britain is to recognise the care with which the health problems of minority populations have been investigated and to some extent acted on.
Conclusion
There is certainly evidence, on paper, of a national commitment in Australia to reducing inequalities in health.
In official pronouncements there is none of the reluctance found in Britain to acknowledge the existence of a problem and the need to deal with it.
This can be seen in the setting of goals and targets, in the National Health Strategy, and in initiatives with the Aboriginal community.
It is more difficult to judge how far the commitment has been translated into action, though it is clear that progress has been made in some areas.
Britain has much to learn from the Australian experience.
The setting of goals and targets for health has advanced considerably in Australia, through focusing attention on specific social groups in the population with very poor health and through formulating targets for the wider social and environmental determinants of health.
Although discussions about translating targets into action plans are at a preliminary stage, the succession of target setting exercises has been based on a broad concept of the influences on health.
This is essential for successfully addressing inequalities in health.
The Welsh Health Planning Forum has developed differential targets along these lines for the NHS in Wales, but in general the rest of the United Kingdom has not progressed so far.
Local and national Health of the Nation initiatives would benefit considerably from such an approach.
One of the strengths of the Australian National Health Strategy initiative has been its ‘equity audit’ on the health care sector — identifying problems over access and use of services for different groups in the population.
The initiative has also made concerted attempts to tackle these problems.
Whether the action  proposed will solve them is another matter.
The analyses by the strategy unit revealed difficulties in maintaining the principle of access to services on the basis of need, against incentives to select patients on the basis of their insurance status.
In Britain we are likely to come up against similar, if not identical, perverse incentives as the NHS reforms alter the way in which services are funded.
For example, there have already been reports of hospitals giving preference to patients of fundholding general practitioners.
Continued vigilance will be necessary to monitor and to protect the principle that patients should have access on clinical rather than financial grounds.
Aboriginal communities have been so disenfranchised in the past that giving them a say in policies and decisions affecting them is seen as a key element in improving their health
The example of cost shifting between federal and state governments reveals how services for low income patients can inadvertently disappear in such a process.
In Australia the scope for cost shifting of this nature may be greater than in Britain, but it still goes on here.
For example, savings on capped hospital drug budgets may be made by transferring costs to the more open ended general practice budget when discharging patients.
Savings by the NHS have been made by switching funding for respite care and long stay nursing care to the social security budget.
The basic problem with this shift is that previously free NHS services have become means tested services along the way.
New funding arrangements for community care will not necessarily put a stop to such perverse incentives and may indeed provide further scope for cost shifting.
The Australian experience provides a timely reminder of the importance of monitoring the effects of reforms on different social groups.
The Australians show considerable inventiveness in involving other sectors and groups in policy development.
Indeed, the Aboriginal communities have been so disenfranchised in the past that giving them a say in decisions and policies affecting them is seen as a key element in improving their health.
It looks as though this is being done in a serious, not just a token, way.
This is something that has been advocated for disadvantaged communities across Europe but rarely gets past the rhetoric stage.
Health inequalities in Britain remain pervasive.
If they are to be taken more seriously and tackled effectively in the future, there is much of practical value to be gleaned from the Australian experience.
Box A: Healthy environments: goals and targets
Goals and targets have been developed in six separate sectors covering the physical environment (global pollution, air, water, and soil contamination, indoor environment); housing, homes and community infrastructure; transport; work and the workplace; schools; and health care settings.
An example: adequate housing 
Goal:To increase the number of people living in adequate housing
Proposed targets:
Priority population — Aboriginal and Torres Strait Islanders in rural communities and settlements
To reduce exposure to risks to health associated with poor living conditions.
Intermediate indicator — to increase the proportion of Aboriginal and Torres Strait Islanders living in remote and rural communities who live in dwellings which have
Potable water for drinking/cooking
Adequate water supply
Electricity
Bathing and laundry facilities
Sewage
Waste disposal
Adequate drainage.
Baseline —to be derived from ATSIC study into housing and community infrastructure.
Priority population — low income private tenants
To reduce exposure to health risks associated with poor living conditions.
Intermediate indicator — to reduce the proportion of low income tenants living in substandard accommodation.
Baseline — to be derived from ABS housing survey in 1994.
Box B: National Health Strategy: summary of terms of reference
To analyse evidence and provide strategy options concerned with
Demand for medical services
Delivery of services
Financing of services and its effect on effective health care delivery
Distribution of health costs across the population and their impact on individuals and families
The role of the private sector
The balance between supply and demand of health workers
Methods to stimulate an increased focus on preventive services and the integration of these services with primary and secondary care
LETTERS
Microphthalmos and anophthalmos and environmental pollutants
Editor ,— Stuart Handysides draws attention to recent reports of anophthalmia.
In 1984 there was public anxiety in the area covered by Forth Valley Health Board in Scotland about emissions from a chemical incineration plant that was disposing of polychlorinated biphenyls.
A review of morbidity in the area of the plant showed sufficient cases of microphthalmos to warrant further investigation.
A working party was set up to establish the prevalence of microphthalmos and anophthalmos in Scotland.
It found no increased prevalence in the Forth Valley, nor any association to link the cases there.
The working party identified liveborn infants with the condition in six health board areas during 1971–85.
Ninety nine cases were found: 33 were associated with other eye anomalies, 41 were associated with other congenital anomalies, and 16 were part of a syndrome or chromosomal abnormality.
The investigation was hampered by difficulties in ascertaining cases, particularly for children who had died.
Twenty four sources of information were tapped.
The yield from centrally recorded data — the Scottish neonatal discharge record (SMT 11), Scottish hospital inpatient statistics (SMR 1), and the school health service medical record card (SMR 10)— varied among the health boards; these three sources contributed 50% of the cases.
Greater Glasgow Health Board, with 31% of the total population, contributed 45% of the cases.
It was the only health board with a well organised congenital malformation register.
As well as defects apparent at birth being recorded, developmental screening of children by a health visitor on three occasions before their 4th birthday allowed newly detected abnormalities to be recorded.
The register contained 90% of cases identified in this health board and was the sole source of identification for 40%, whereas the combined SMR data identified only 38%.
The register identified the high mortality (40%) of microphthalmic children in this health board compared with a combined mortality of 13% in the five other health boards, two of which recorded no deaths.
A congenital anomaly register has now been set up in Scotland based on all the centrally recorded data.
It will be as good as the various methods of recording data allow.
It was only a matter of time before another environmental pollutant came under suspicion of causing a congenital anomaly, raising calls for a proper epidemiological study.
Until systems exist that allow accurate recording of congenital anomalies present at birth and of those recognised after the neonatal period such a study faces insurmountable difficulties.
The system used by Greater Glasgow Health Board has much to commend it.
Conclusions drawn from applying statistical techniques to incomplete data may be very misleading.
Ethical issues in randomised prevention trials
Editor ,— Nicholas Wald shows that there is no inherent ethical conflict in setting up randomised controlled trials in clinical practice.
He does not mention, however, the choices that have to be made during execution of such trials, the ultimate allegiance of the clinician being to each patient separately while that of the researcher is to the overall trial design.
In the ever changing conditions of real life the clinician would adjust any regimen to the patient's particular needs; in contrast, the researcher would do his or her best to make the patient conform with the standard set down by the protocol.
Perhaps the practitioner conducting the trial should not be the patient's own doctor.
Editor ,— Nicholas Wald rightly emphasises the importance and the ethical nature of randomised multicentre prevention trials, but he makes two statements that cannot go unchallenged.
Doctors do not, as he claims, exaggerate the distinction between research and medical practice.
It is fundamental.
Patients believe that their doctor, to the best of his or her ability, is following two rules: everything that is beneficial is being done, and nothing is being done that is not directly beneficial.
In randomised prevention and therapeutic trials these rules are not followed.
Patients are randomly assigned to treatments even if the doctor suspects that one form of treatment might be better.
Observations are made that are not directly intended to benefit the patient.
It is true that medical advice may be little more than an educated guess that proves wrong and that close supervision in a therapeutic trial may benefit the patient.
This does not affect the ethical difference.
Patients subjected to research need the twin protection of informed consent and surveillance by ethics committees.
I take issue with the view that district ethics committees are superfluous once central committees have approved a multicentre project.
A single committee, even of the great and good, is not the source of all wisdom.
For example, the research ethics committee of the Royal College of General Practitioners does not include a dietitian.
Recently, this hospital's dietitian requested changes in the patient information sheet for a protocol approved by that committee.
There are many in Bradford who read Urdu but not English, a point not always considered in London.
No doubt circumstances vary in other districts in ways not obvious to outsiders.
Local patients, investigators, and resources are known only to local people, and assessments must be made locally.
Better communication between the organisers of multicentre research and district ethics committees is needed.
This is not a reason why district ethics committees should yield to pressure to abdicate their responsibilities to local citizens.
Assessment of students
Editor ,— In her review of methods of assessing students Stella Lowry argues the need for methods of assessment that match learning objectives.
Considerable advances have been made in the assessment of knowledge and clinical skills — for example, multiple choice questions and objective structured clinical examinations.
In addition to providing the knowledge and skills needed for medical practice the new curriculum at the medical schools of the Royal London Hospital and St Bartholomew's Hospital aims to foster lifelong learning and awareness of strengths, weaknesses, and learning needs.
As a part of this the development of skills in self reflection and self critique of performance is important for continuing learning and personal development.
A new formative assessment for third year students has been introduced this year — namely, an integrated workbook assignment.
This entails interviewing a patient, tape recording the interview, selecting a section to transcribe, and analysing the communication process involved in taking the history.
The student must also write up the medical history and examination findings and discuss the psychosocial, ethical, legal, and nursing considerations of the patient's case.
This task, which follows on from a programme covering these subjects, incorporates the strands of communication skills, behavioural sciences, ethics, and law in the assessment.
Students must complete a satisfactory workbook assignment before entering parts 5 to 10 of the MBBS examination.
To overcome the problem of case specificity, assessment of the interview section is based not on how well the students did but on their ability to evaluate their communication with the patient.
Their critique is verifiable by reference to the tape recording of the interview.
Students do not have to search for the ‘ideal’ patient.
Even if they were not satisfied with their history taking, their analysis of their performance, the difficulties or constraints perceived, and ideas for improvement are the most important material for assessment in this task.
The first cohort of 240 students has just completed this assignment.
The students have reported useful insight from their self assessments and have been able to identify things they did well, problems, and how they could improve.
This method of assessment matches two key  aims in our new curriculum.
Firstly, in relation to communication skills part it addresses the development of students' skills in self awareness and reflective learning.
Secondly, the workbook encourages the students to understand their patient as a complex person whose health and wellbeing depend on more than biomedical considerations.
The depth and extent of this understanding have been shown in some work of exceptional quality and insight.
We believe that the integrated workbook assignment embodies the ideals of our curriculum and, in particular, has considerable potential for continuing the General Medical Council's recommended strands of ethics, law, behavioural science, and communication skills throughout the clinical course.
Harvard's ‘new pathway’
Editor ,— As one who taught on Harvard's ‘new pathway’ during the two pilot years, as well as on its traditional courses, I would like to make some comments.
The success of any programme rests on the faculty's enthusiasm and support.
Teaching well takes time and often yields little tangible reward.
Harvard's new pathway got through its pilot years relying on the motivated staff and fellows.
This staff may not be available at many medical schools.
The greatest change in the curriculum with courses similar to the new pathway is seen in the preclinical faculty.
Preclinical staff usually have busy schedules and may not be particularly well oriented to clinical matters.
For example, teaching, say, the pharmacology of tetracycline in the traditional way is usually fairly easy for a preclinical pharmacologist with a related scientific interest.
Less easy for (and possibly of less interest to) preclinical staff is dealing with a case study for the new pathway; such a case might start with the pharmacology of tetracyclines, pass through their therapeutic use in general, and end on a debate about whether oxytetracycline should be used as prophylaxis for traveller's diarrhoea in Mexico.
Team teaching, with both preclinical and clinical staff present at each session, may be a feasible alternative, given the staff available at most medical schools.
It is true that the new pathway was oversubscribed in both pilot years.
During the first pilot year, however, there was a sense amongst the ‘traditional’ class that their colleagues in the new pathway were taking an extraordinary gamble with their medical education.
During the second year this feeling persisted, but less strongly.
I do not agree with Stella Lowry that ‘special arrangements that had been made for the new pathway students had caused resentment among other students, who felt that they were being treated like second class citizens.’
Lastly, the success rate at Harvard in the national board examinations has always been extremely high (as it is at most American medical schools).
These examinations are probably a poor instrument for measuring the quality of medical education because they concentrate on factual retention.
Community based medical education
Editor ,— Dr Nigel Oswald, cited by Stella Lowry, is correct in implying that learning skills in clinical decision making requires seeing large numbers of patients in a short space of time.
This, however, is an argument against rather than for community based learning.
This is illustrated by an example from our practice.
An average general practice of 10000 patients refers 34 patients a year for assessment of breast lumps.
A student attending a well directed breast clinic may personally see this number of patients in less than a month and be taught to make an accurate clinical assessment.
She or he would have to spend a year in general practice to have the opportunity to acquire similar skills.
To paraphrase Oswald, ‘It is more important to see 30 patients who might have breast cancer than five who do (but it is useful and likely that you will see them too).’
Teaching how to elicit and interpret physical signs
Editor ,— John R Hampton may be right to lament the decline in doctors' abilities to elicit and interpret physical signs, but I believe that he is wrong to conclude that training in the setting of general practice will sound the deathknell of these skills.
My memories of cardiac teaching rounds are of a dozen students queueing to listen to a murmur while the registrar stood at the end of the bed swinging a stethoscope and staring out of the window.
Aware of restive colleagues, one listened hurriedly and joined the whisper going round the group: ‘What did you hear?’
Coming back later on one's own was rarely useful: even if the relatives weren't round the bed there was rarely a doctor prepared to give guidance.
‘We don't spoonfeed you here’ was one of the less excusable reasons given for declining to help floundering students.
Traditionally, doctors were trained by being apprenticed to established physicians.
In hospitals the system has broken down under the pressure of numbers and new teaching methods are only slowly being found, but teaching in general practice has remained close to the tradition in which older generations of doctors learnt their skills.
Move a medical school to Milton Keynes
Editor ,— Why not move one of London's medical schools to Milton Keynes?
Designated to receive most of its population from London, the city could now adopt one of its medical schools as well.
There are precedents for such a move: during the second world war some students and staff from University College Hospital, London, relocated to Cardiff.
Milton Keynes has its own hospital; consultants and senior staff could move there with the medical school.
Even the name of the medical school could be retained with just the postcode changed.
Antenatal diagnosis of Down's syndrome
Editor ,— The increase in antenatal diagnoses of Down's syndrome suggests a more widespread use of biochemical screening.
This, and analysis of the results by David E Mutton and colleagues, is to be encouraged but raises some interesting points which have potential implications for resource allocation.
The two main reasons for antenatal screening are (a ) to plan the most appropriate place and mode of delivery to minimise the hazard to neonatal life and (b ) to offer termination of pregnancy if the diagnosis is made before 24 weeks' gestation (previously 28 weeks').
From Mutton and colleagues' raw data, assay of serum α fetoprotein concentration detected 21% of the detected cases in women under 35 while triple testing detected only 17%.
This might suggest that assay of α fetoprotein concentration alone is better at detecting Down's syndrome than triple testing.
The converse, however, is the case, and the difference can probably be explained simply by the more widespread use of assay of α fetoprotein concentration during the period studied.
Although the total proportion of diagnoses seems to be rising, the rise is steepest in those who historically have fallen into a high risk group — that is, woman aged 35 and older.
This is not surprising as the algorithm to assign risk is weighted in favour of such cases.
Unfortunately, around three quarters of cases of Down's syndrome occur in fetuses of women under this age, and in 1991 biochemical testing detected only 6.5% of all cases of the syndrome.
From the analysis we do not know the proportion of pregnant women who participated in this form of screening, but it seems that around 48% of cases might be detected if triple screening was universal.
Detailed ultrasound scanning detected 7.2% of all cases of the syndrome, though, again, the same rules apply — that is, what proportion of all antenatal patients underwent detailed scanning?
Recently, however, Luck reported that in an unselected population detailed ultrasound scanning detected all of the cases of Down's syndrome when a physical abnormality was present.
At least half of all fetuses with the syndrome have a congenital heart defect, and many others have bowel atresias.
Perhaps of greater importance, however, is that only half of liveborn infants with aneuploidies have Down's syndrome.
Many of the other common aneuploidies (such as trisomy 13, trisomy 18, and Turner's syndrome-XO) are associated with physical abnormalities that are more readily appreciated on ultrasound scanning than the subtle ones associated with Down's syndrome.
Furthermore, ultrasound scanning detects other physical anomalies, of which some are associated with genetic abnormality.
Many represent a hazard to neonatal life.
If these anomalies are detected in good time the parents can receive counselling and the subsequent management of the pregnancy can be planned, so reducing the national perinatal mortality rate.
Detailed ultrasound scanning has been shown to be cost effective as it detects most cases of Down's syndrome as well as other life threatening conditions.
Perhaps its wider implementation in early pregnancy should be an aim of all obstetric departments.
Sexually transmitted diseases and HIV infection among homosexual men
Editor ,— B G Evans and colleagues conclude the discussion of their paper by stating, ‘Because of the high background prevalence of HIV-1 infection risks to homosexual men practising unsafe sex are greatest in London.’
They further suggest that ‘safe sex information aimed at…homosexual men in London needs special emphasis.’
This statement might be misunderstood by readers to imply that unsafe sex elsewhere is somehow less risky than it is in London.
This is certainly untrue of Brighton, where the high prevalence of HIV-1 infection in homosexual men is similar to that in London.
Indeed, the rate of infection in Brighton is probably the highest in Britain (90 new infections in homosexual men reported in 1992 (23% of those tested)).
By any statistical configuration, the concentration of HIV infected homosexual men in Brighton is higher than that in London or elsewhere in Britain.
The chance of infection through unsafe sex in Brighton is therefore relatively higher.
Information on safe sex aimed at homosexual men in Brighton is needed.
Statutory and voluntary organisations have recently intensified education and information programmes and increased the educational involvement of outreach groups with homosexual men in Brighton.
Editor ,— We have conducted a similar study to that of B G Evans and colleagues of sexually transmitted diseases and HIV-1 infection among homosexual men in England and Wales.
Our results reinforce their findings to a certain extent, but our experience in 1992 shows a further reduction in new cases of gonorrhoea in men and in the diagnosis of HIV positivity in male homosexuals.
We looked at the total incidence of gonorrhoea; the number of cases of gonorrhoea found in men overall and in homosexual men; the male to female ratio of cases of gonorrhoea; and the number of new cases of HIV infection diagnosed in homosexual men (table).
Like Evans and colleagues, we found that new cases of gonorrhoea in all men and in homosexual men showed an increase in 1988–90.
Our study, however, showed a reduction in 1991 and 1992.
The male to female ratio of cases was lowest (1.2:1) in 1987, subsequently rising to 2.1:1 in 1990.
The number of new cases of gonorrhoea in homosexuals was lowest in 1987 and then gradually increased until 1990.
Our study indicates that unsafe sexual practice may have increased in men from 1987.
The decline in gonorrhoea and other sexually transmitted diseases in the mid-1980s may have been due to safer sex practices after health education through the mass media and various other local activities.
The reduction that we found in 1992, in both gonorrhoea and HIV infection, is heartening, but vigorous and continuing health promotion will be necessary to continue this trend.
Editor ,— B G Evans and colleagues present compelling evidence that unsafe sexual behaviour and transmission of HIV have increased among gay and bisexual men in England and Wales after a decline in the 1980s.
They suggest that this may partly be due to a failure to sustain the successful community based health education activities of the early and mid-1980s.
To ascertain the level of HIV prevention activity specifically targeting gay or bisexual men in Britain staff and volunteers at the National AIDS Manual, North West Thames Regional Health Authority HIV project, the Terrence Higgins Trust, and Gay Men Fighting AIDS undertook a survey between November 1991 and April 1992.
Two hundred and forty organisations with a remit for HIV prevention work were identified.
Answers to a standard telephone questionnaire were obtained from 226 (94%); 202 respondents were statutory organisations and 24 were voluntary agencies.
Altogether 149 respondents reported that they had never undertaken or funded any HIV prevention work specifically aimed at gay or bisexual men.
Of the remaining 77, only eight had ever offered a ‘substantial’ programme of such work; this was a relatively unexacting definition, requiring only a written needs assessment and the employment of a whole time or part time worker with a specific remit for this work.
Only three agencies had ever offered a ‘comprehensive’ package of HIV prevention work for gay and bisexual men, defined as needs assessment, the production of local health education resources, one or more public education events, staff training, and the employment of a worker.
At a time when it is increasingly popular to search for complex explanations for continuing or increasing levels of unsafe sexual behaviour among gay and bisexual men the most obvious explanation — lack of continuing education about safer sex — must not be overlooked.
Evans and colleagues' concern about the failure of AIDS educators to target gay and bisexual men is well founded: it seems that those most at risk from HIV have also been the most neglected in recent years.
HIV prevention workers must ensure that they prioritise their work according to epidemiologically demonstrable need if the alarming trends in surveillance data on sexually transmitted diseases and HIV infection are to be arrested or reversed.
Editor ,— We recently highlighted an increase in unsafe sexual behaviour and transmission of HIV among homosexual men in England and Wales after a period of decline.
Our data included documented seroconversions to the end of 1991 — that is, newly diagnosed HIV-1 infection in men for whom the year and month of a previous negative result of an HIV-1 test were available.
The table summarises revised data, including the seroconversions reported during 1992.
The number of reported seroconversions has risen steadily since 1986; the number of cases in which transmission of HIV-1 was known to have occurred during 1990–2 (157) was more than double the number recorded during 1987–9 (74).
Despite widespread recognition in 1991 of the unfavourable trends in sexually transmitted diseases among homosexual men transmission of HIV during 1992 seems not to have declined but may have intensified further.
Relation of birth variables to death from cardiovascular disease
Editor ,— D J P Barker and colleagues' study puts a further nail in the coffin of those who doubt that the intrauterine environment influences later health — in this instance, death from cardiovascular disease.
A theme running through the Southampton group's many studies on this topic is that maternal nutrition is primarily responsible for reduced prenatal growth.
Though there can be no doubting the importance of maternal malnutrition as a cause of reduced fetal growth in poor countries and even perhaps in Preston, Sheffield, and Hertford in the early part of this century, where Barker and colleagues' cohorts were born and brought up, there is no strong evidence of undernutrition now being responsible for restraining intrauterine growth in developed countries.
Maternal diet is only one of the many factors that can lead to fetal growth retardation.
To begin to understand mechanisms that might link the environment of fetal life and infancy with later disease, influences other than maternal nutrition need to be considered — for example, Edwards et al have recently proposed that links between the fetal environment, adult hypertension, and low birth weight could be mediated through dysfunction of the placental barrier to maternal cortisol.
Paradoxical pain
Editor ,— David Bowsher defines paradoxical pain as chronic nociceptive pain that does not respond to morphine.
It is more generally understood as pain that is made worse rather than better by increasing doses of morphine.
It has been reliably reported with large doses of intrathecal morphine and diamorphine and probably occurs occasionally with large daily doses of the same drugs intravenously.
Bowsher and his colleagues have made a good case for paradoxical pain being the result of a genetic inability to metabolise morphine to the potent morphine-6-glucuronide, leaving large quantities of morphine-3-glucuronide (a putative morphine antagonist or a non-specific cerebral stimulant, or both) unopposed.
It is difficult, therefore, to understand why Bowsher has opted for an alternative definition.
It is also disturbing that he has used ‘overwhelming pain’ as a synonym for paradoxical pain.
Overwhelming pain is a term used to emphasise a common result of chronic unrelieved severe cancer pain.
It almost always responds to adequate amounts of morphine, coanalgesics if appropriate, and, usually, an anxiolytic.
A comparable situation is sometimes seen despite large doses of morphine when the patient's anxieties and fears have not been addressed.
Thus, in one case, a patient with inoperable cancer of the oesophagus was still in pain despite receiving 12 g of oral morphine a day when he was admitted to a hospice; a week later he was free of pain when taking 60 mg of morphine a day and 10 mg of diazepam at night.
His seemingly morphine resistant nociceptive cancer pain responded to listening, explanation, and the setting of positive rehabilitation goals.
Nociceptive pain is also relatively resistant to morphine and other opioids when there is peripheral or central neural sensitisation.
Sensitisation occurs in damaged tissue and the surrounding area and in areas subserved by either an injured peripheral nerve or an injured part of the central nervous system.
Pain associated with inflammation is a typical example of peripheral sensitisation, hence the need to use a non-steroidal anti-inflammatory drug in most patients with painful soft tissue and bone metastases.
Morphine alone is often inadequate, but there is nothing paradoxical about this.
Central sensitisation may also occur in such cases as part of a secondary ‘wind up’ phenomenon in the dorsal horn.
Occasionally this requires specific correction — for example, with an n-methyl d-aspartate receptor blocker such as ketamine.
Central sensitisation in neuropathic pain is possibly more complex and, as Bowsher points out, demands a range of alternative measures.
Editor ,— Paradoxical pain is a new and confusing term that has been defined in different ways.
David Bowsher  describes it as nociceptive pain that is not receptive (does he mean responsive?) to opioids.
Yet in an earlier publication, in which the term was first coined, he and his colleagues used it to describe ‘pain [which]ceases to be relieved or is worsened by further administration’ of morphine or diamorphine (our italics).
We have not seen any patients whose physical pain has been made worse by morphine or diamorphine, nor are we aware of any good evidence that this occurs.
More importantly, we fear that the suggestion that this may happen may deter some doctors from giving adequate doses of these drugs when they are properly indicated.
It is well recognised that opioid analgesics do not always relieve pain, and there are already several unsatisfactory ways in which such pain is described, including ‘opioid insensitive,’‘opioid non-responsive,’ and ‘opioid resistant.’
As we have written elsewhere, these terms have subtle differences in meaning, which are partly semantic but partly reflect different views.
The introduction of yet another term will add confusion.
We believe that what has been described as paradoxical pain is what we would refer to as ‘opioid poorly responsive’ pain and that opioid responsiveness is a continuum that may be influenced by any of a large number of factors related to the patient and the drug as well as the pain.
The pharmacokinetics of morphine may provide at least part of the explanation, but there are too few data to justify the editorial's subheading (morphine-3-glucuronide does not, by the way, bind to opiate receptors).
In 1967 Cicely Saunders described the concept of total pain, which encompasses the psychological, emotional, and spiritual turmoil of some patients with severe pain.
Might this be what Bowsher refers to as overwhelming pain?
Editor ,— David Bowsher's editorial oversimplifies a complex and contentious issue.
Paradoxical pain may well exist but is neither well documented nor common; it does not account for the majority of cases of uncontrolled pain, and we are not aware of any evidence that it was an important factor in the care of the patient in the recent highly publicised court case.
The hypothesis that paradoxical pain is caused by abnormal metabolism of morphine is plausible but built on shaky foundations.
The evidence in rats that morphine 3-glucuronide may antagonise the analgesic actions of morphine is unsubstantiated and is hard to explain given that morphine 3-glucuronide has a much lower binding affinity for opioid receptors than either morphine or the active morphine metabolite, morphine 6-glucuronide.
Furthermore, large interspecies variations exist not only in the metabolism of morphine but also in the distribution of opioid receptors.
Thus animal data on this subject cannot, and should not, be extrapolated to humans and many questions remain.
Though recognition of this potential therapeutic problem is welcome, until the clinical importance of the morphine metabolites in humans is completely understood these rare cases of paradoxical pain will remain unexplained.
Editor ,— The concept of paradoxical pain and its relation to morphine metabolites raises many questions.
There are several conceptual errors inherent in this description.
One of the most fundamental is that the pain syndromes as described should at any time actually respond to opioids.
This makes the assumption that so called paradoxical pain is nociceptive pain, with the second assumption that all nociceptive pain responds to opioids.
If the cases of so called paradoxical pain were opioid sensitive why not test that hypothesis directly?
This can be done by giving, for example, the highly lipid soluble opioid fentanyl by incremental intrathecal injection.
This will differentiate between opioid ‘resistant’ pain — when tolerance or metabolic problems may intervene to inhibit the analgesic effect — and opioid ‘insensitive’pain.
In a considerable number of pain syndromes no response will occur: these are opioid insensitive pains.
Neither opioid resistant nor opioid sensitive pain is paradoxical.
Neither is there a clear distinction between the concept of paradoxical pain and that of overmorphinisation and overwhelming pain syndrome.
A hyperaesthetic overwhelming pain state can occur in patients with chronic pain and is just as likely to occur in patients receiving buprenorphine, when the morphine metabolic pathways are irrelevant.
In these patients a reduction in opioid is associated with a reduction in hypersensitivity and in the pain.
Again, this type of pain is not paradoxical since there is no indication that it should respond to opioid.
A difficulty may occur in the interpretation of metabolite concentrations and pain response.
The subjects in one of the trials referred to were patients in whom pain control was inadequate or who had appreciable side effects, which is why the cerebrospinal fluid was accessed.
The metabolic concentrations cannot be taken to be representative of analgesic ratios.
There are indeed some paradoxical pains — that is, pain that is aggravated by the use of analgesics or relieved by the use of antalgesics.
One such is biliary colic induced or aggravated by morphine used to treat biliary colic.
Another is pain after stroke that does not respond to narcotics but in some cases is reduced by the opioid antagonist naloxone.
These are true paradoxes, and morphine metabolites are of no importance.
Editor ,— The important moral of David Bowsher's editorial on paradoxical pain must be that if any analgesic drug is not effective it is always worth trying a chemically dissimilar drug of the same class before deciding that the pain will not respond to that form of analgesia.
The reminder that this is also true of the opiates is well taken.
I do not think, however, that the distinction between nociceptive and neuropathic pain is as clear as Bowsher suggests.
In cancer, for instance, the pain may be purely nociceptive early on, but if the tumour begins to damage a nerve by pressure or invasion the resulting pain must have a neuropathic element.
In addition, there is now much evidence that a brief episode of pain may induce long term changes in the excitability of spinal nociceptive neurones.
In inflammatory arthritis, and possibly in many other types of inflammatory pain, these changes in central excitability may be enhanced by the activation of ‘silent’ nociceptors.
On the basis of current knowledge, such excitability of central nociceptors is unlikely to be controlled by opiates alone.
Effective pain control, irrespective of the type of pain, is often best achieved by a combination of drugs, including non-steroidal anti-inflammatory agents, tricyclic antidepressants, and anticonvulsants, as well as opiates.
I believe that the term paradoxical pain is misleading and that the old term, morphine resistant pain, is a more exact description of the phenomenon.
It also leads the doctor to the obvious conclusion — try something different.
Neonatal group B streptococcal infections
Editor ,— The prevention and appropriate management of neonatal infection remain a problem.
Use of chemoprophylaxis to prevent group B streptococcal infections, especially in high risk groups (those with premature rupture of membranes or premature labour), seems to be a logical means of minimising the risk of this condition.
It is important, however, to consider the possible detrimental effects of giving antibiotics to the mother during a high risk pregnancy because of the risk of selecting antibiotic resistant flora.
These organisms may then colonise the neonate during vaginal delivery, and if infection subsequently develops, establishing effective antibiotic treatment may be difficult.
The problem of neonatal infections due to antibiotic resistant organisms (for example, coagulase negative staphylococci, Gram negative bacilli) in special care units may be related to the widespread use of broad spectrum antibiotics in neonates.
The possibility of increased virulence in these antibiotic resistant strains (which may be related to the presence of plasmids encoding antibiotic resistance and virulence factors) suggests that attempts to limit their selection by judicious use of narrow spectrum antibiotics should be considered at all times.
The development of immunoprophylaxis for group B streptococcal infections may  ultimately result in the most appropriate intervention with the least effects on the neonatal microbial flora.
Treating hypertension in elderly patients
Editor ,— B C Campbell asserts that there is no consistency in the results of trials of antihypertensive treatment in elderly people.
Campbell supports this with results from a subset of patients from the Australian national blood pressure study, saying that these ‘show no reduction in fatal strokes but a 75% reduction in ischaemic cardiac death.’
This statement is not correct.
There were only two fatal strokes and five fatal heart attacks in this study.
One does not have to be a statistician to discount these results.
Among the five substantial trials of treatment of hypertension in elderly people the reduction in stroke was 47% in the Swedish trial, 42% in the hypertension in elderly patients study, 37% in the systolic hypertension in the elderly programme, 36% in the European working party trial, and 25% in the Medical Research Council's trial.
The smaller reduction in the Medical Research Council's trial may be accounted for by the high rate of drop outs and deviations from the protocol.
In contrast to these reductions in the rate of stroke, the effect on heart attacks was much less: only in the systolic hypertension in the elderly programme did it achieve significance.
Diuretics seem to have had a better protective effect than β blockers.
These results harmonise remarkably well and suggest that treatment of patients aged between 60 and 80 with systolic blood pressure persistently above 160 mm Hg will prevent 30–50% of strokes in these patients.
Inherited prion disease
Editor ,— John Collinge and colleagues report two cases of Creutzfeldt-Jakob disease with a heterozygous missense mutation at codon 200 of the prion protein gene causing substitution of lysine in place of glutamate in the prion protein.
They were also homozygous for methionine at the polymorphic codon 129.
One of the two patients was of British origin without any known Sephardic or central European ancestry, which has been reported to be associated with the mutation at codon 200.
We have also encountered patients with Creutzfeldt-Jakob disease in France who were not apparently linked to any previously described ethnogeographic cluster of this mutation but bore it, suggesting the existence of separate foci.
During 1992 three patients with the disease carrying a heterozygous mutation at codon 200 were recorded in a survey of human spongiform encephalopathies by the Institut National de la Santé et de la Recherche Medicale.
Two patients in whom the disease was confirmed (aged 46 and 54 at onset; duration of illness six and four months) were of French ancestry (tracing back three and five generations).
Undefined neurological diseases, which in retrospect might have been Creutzfeldt-Jakob disease, had affected the patients' fathers, who had died at age 68 and 71 respectively.
The third patient was a Jewish woman who had been born in Tunisia (a cluster of cases of the mutation has occurred in such patients); she did not have any known familial history of Creutzfeldt-Jakob disease).
She developed the disease at age 67 and died nine months later.
Whereas our two first patients were homozygous for methionine at codon 129 of the prion protein gene, the last patient seemed to be heterozygous, encoding a methionine and a valine from each allele.
Accumulating data indicate that heterozygosity at codon 129 plays an important part  in the phenotypic expression of familial prion diseases.
It delays the onset of the disease associated with a 144 base pairs insertion and modulates the phenotypic expression of fatal familial insomnia and the form of Creutzfeldt-Jakob disease associated with a mutation at codon 178.
If our last observation shows that heterozygosity at codon 129 does not protect against the disease associated with a mutation at codon 200 it then suggests a possible delaying effect on its onset, as Collinge and colleagues propose.
As in the patient of British descent reported on in detail by Collinge and colleagues, the anticipation phenomenon — that is, earlier onset of the disease with progress through generations — appears clearly in our first two patients' families.
Whether only the codon 129 genotype contributes to the molecular basis of this anticipation phenomenon is still unknown.
Editor ,— It is good to read a clear and concise exposition of the current position on the enigmatic and horrible spongiform encephalopathies and their mysterious transmissible agent(s).
These universally fatal chronic brain infections of mammals (including humans) are untreatable and will remain so until more is known about this unique agent.
To label them prion diseases, which implies that the causal organism is a self replicating protein containing no nucleic acid, is to obstruct research.
This hypothesis not only flouts accepted biological principles but is unnecessary.
There is good evidence that this transmissible agent is a virus containing DNA but that it is embedded in a tough proteinaceous capsule.
The presence of the capsule explains the agent's extraordinary resistance to all the manoeuvres that inactivate viruses, such as gamma radiation, ultraviolet light, high temperatures, and even enzymes that digest nucleic acid.
Instead of devoting more time and money to the inherently unlikely possibility that this organism alone among replicating particles contains no nucleic acids, researchers should concentrate on cracking its tough proteinaceous capsule.
About 100 years ago Robert Koch was confronted by a similar situation, albeit at the microscopic rather than the ultramicroscopic level.
His efforts to make progress with the tubercle bacillus came to a standstill until he tackled its tough (lipid) capsule.
Hopes of treating these awesome ‘slow virus’ infections of the brain may rise when the scrapie agent's capsule is breached.
Interpreting fluoroscein angiograms
Editor ,— Barbara Harney has misunderstood the point of the case that we reported, and we disagree with one of her assertions.
The fact that there is leakage of dye on fluoroscein angiography in patients with raised intracranial pressure is axiomatic.
It was for this reason that the procedure was undertaken at this regional neurology centre, to investigate the young woman, who had headache and bilateral papillo-oedema and in whom cranial imaging showed no abnormality and the opening pressure on lumbar puncture was normal on two occasions.
In this case the fluoroscein angiogram showed definite blurring of the disc margins at 6.5 and seven minutes.
This was not thought to be hyperfluorescence of the discs by our ophthalmological colleagues.
Our contribution to Minerva included a photograph of the angiogram, but this was not published.
We were, like Harney, puzzled by this finding.
After the discovery of buried disc drusen, however, we reviewed the literature, and it emerged that late leakage of dye is recognised in this condition.
Punctate fluorescence of the drusen can corroborate the diagnosis but was absent here.
Contrary to what Harney says, the disc margins in this condition need not remain well defined.
The phrase ‘a recognised diagnostic pointer’ was not our own and was perhaps an unfortunate choice by the technical editor.
A reference for these observations was included in our original but, again, was omitted from the published report.
We agree that care should be taken over terminology in the interpretation of fluoroscein angiograms but contend that the use here was accurate.
The language of health
Editor ,— R Buckland states that the Read codes will be ‘applied automatically by computer software.’
In fact, the software requires the coder to input a term before it can produce a list of possible codes from the database.
The coder then selects the codes to apply.
Analysis of other code systems has shown that the process of coding is time consuming and probably at least 20% inaccurate.
The Read codes were designed to be used with a computer and are therefore organised on a hierarchical structure.
They can be used as a single axis system, but for a detailed description a multiaxial code is recommended.
This means that even individual procedures can require several codes.
As coding with the Read codes can be more detailed than with other codes it will be more prone to error and may take longer.
The clinical terms project is a fundamental part of the NHS information management and technology strategy and is financed as one of the national facilitating projects.
The Read codes, being the common origin of the proposed electronic data interchange, provide the potential to improve the distribution of information to clinicians for patient management.
Additional benefits include information for research, audit, epidemiological studies, and management.
While the NHS Management Executive recognises the clinical terms project as fundamental, considerably less thought and budgeting have been given to the inputting of the codes.
The increased detail of these codes means that hospital coders are likely to have more difficulty in coding clinical work accurately.
The probable consequence is that doctors will be given the task.
This will be practical only if they are provided with the necessary training and time.
Good quality clinical information systems are expensive, will often need adjustments for individual units, and, to make them usable, require computer terminals at every site where information might be recorded.
The investments required are obviously large, but the strategy makes clear that the choice of whether to invest, as well as the amount and source of the investment, is a local one.
The management executive must give more thought to the development of improved methods of applying Read codes and the budgetary implications of this.
Editor ,— As R Buckland points out, the clinical terms project is set as the foundation of the NHS's information technology strategy and is supported by professional bodies.
The project is working to identify all the terms required for recording medical records and for other uses, such as medical audit.
Coding schemes and medical record models are, however, only a means to an end and have no value if they fail to support effective clinical information systems.
Clinicians can make use of clinical information only if they are provided with systems that are useful in their job of delivering patient care.
It is their use in supporting clinical systems that must form the test of the Read codes.
The implementation of the information technology strategy must avoid this relationship's being reversed: the coding scheme must not form the test of clinical systems.
Buckland is wrong to dismiss technical details.
The Read codes adopt a particular approach to the representation of medical concepts.
This is based on enumerative classification, which is deeply rooted in the traditions of epidemiology and vital statistics.
It is a huge step from these uses to the recording of all embracing clinical descriptions for multiple purposes.
To take but one of many examples, there is a single code to represent all the variations on the Charnley hip replacement procedure — quite adequate for most statistical purposes but falling short of the detail required for audit and for many of the uses to which medical records are put.
To date the representation of detailed clinical descriptions for more than a narrow area of medicine has never been achieved by use of an enumerative approach.
There are strong theoretical and empirical arguments, from both doctors and librarians, why it is not possible.
It is not surprising that, since Buckland's editorial was written, the neatly expressive classificatory structure illustrated  in his example has now been discarded to cope with the problems of fixed length codes and a single hierarchy.
It is also a mistake to underestimate the challenge of moving from individual terms to the complexity of the contextual information in a medical record.
The broader patient centred aims of the information technology strategy are to be welcomed.
As with all scientific endeavour, however, it is essential to have a clear vision of the true goal and to have critically appraised the steps required to achieve it.
Topical metronidazole Editor,— I wish to correct Minerva's mistaken assertion that there is no preparation of topical metronidazole for treating rosacea in Britain.
As she rightly points out, the effectiveness of metronidazole in rosacea was first reported in Britain and topical metronidazole has become a market leader in the United States.
Some years ago Bioglan Laboratories, a small independent British pharmaceutical company, started developing a topical formulation of 0.75% metronidazole gel (Metrogel) for rosacea.
Metrogel received marketing authorisation in Britain in December 1991 and was launched by Sandoz Pharmaceuticals, Bioglan's licensee, in April 1992; it has already established itself as the leading topical treatment for rosacea in Britain.
This shows that it is still possible to produce a useful and innovative medicine without the huge financial and technical resources that are at the disposal of the multinational corporations.
Public health medicine after district mergers
Editor ,— Dr Lindsay Davies, the chairman of the Committee for Public Health Medicine and Community Health, has emphasised the difficulties for directors of public health after the merger of health districts and consequent loss of some director posts.
As a director of public health, and having been chief officer to a health authority for some 17 years and having experienced more reorganisations than I care to remember, I can see the career turbulence that the current reforms are producing for some senior doctors in public health medicine.
The question needs to be asked, however, whether directors of public health are still needed at district level.
Could not public health medicine be organised in the same way as clinical specialties, where a coordinator is appointed for a limited period and then the role passes on, with the financial supplement, to another?
Salaries for existing directors would, of course, have to be protected.
The temporary coordinator could draw on the skill of all his or her colleagues in an enlarged district in giving advice to the health authority.
Being a consultant in communicable disease control, I know that many trainees turn away from this vocation when they realise that making control of infection their special interest would diminish their chances of becoming a director.
If the senior manager role cannot be rotated consultants in communicable disease control may be better served in another role — for example, as consultants in the many community health departments that now have trust status.
Alternatively, consultants in communicable disease control could be based in public health laboratories and become their community arm, with the benefit of speedy transmission of microbiological information.
The debate on directorship could be applied in this field too, but the appointment to the service of a national director who is not a microbiologist illustrates the innovativeness of the organisation.
When your time's up
Editor ,— Reviewing our book, Emily Grundy wrote: ‘They…contend that there are no grounds for speaking of aging at the cellular level.’
This general conclusion is not true.
Instead, our precise conclusion was that ‘the probability of cell death does not increase with the age of the culture; ie, there are no grounds for speaking of aging at the cellular level’(p 216).
It is obvious that our conclusion is limited by cell cultures only and is not equivalent to the general conclusion that there is no cell aging at all.
Our second comment deals with the quotation from the Dictionary of Demography , published in the form of a ‘box’ attached to the review.
Although we consider this dictionary as a respectable edition, the definition of the Gompertz-Makeham law is not correct there.
Mortality force instead of the probability of dying should be used in this definition.
A detailed explanation of this correction is given in our book and was approved recently by Age and Ageing : ‘If you do not feel confident that you understand the Gompertz-Makeham equation, nor the distinction between age-specific mortality and the force-of-mortality, then this book is a must for you.’
Nicotine substitutes for hospital patients who smoke
Editor ,— I am writing to correct factual inaccuracies in the letter from Patrick Magennis and Anne Begley concerning the difficulty in providing nicotine substitutes in hospital.
There has never been a ban on supplying nicotine substitutes to individual patients.
If the treatment is requested by a consultant it will be supplied.
A system exists to ensure that at any time, day or night, the consultant's wishes are followed.
The new deal in Trent
Editor ,— As training grade doctors participating in the task force process in Trent region, and as executive members of the Junior Doctors Committee, we have seen the intolerable pressures placed on medical staffing numbers in pursuit of the controls on working hours.
Sometimes it seems that a destructive tension has been created between the new deal, achieving a balance, and the forthcoming reforms of specialist training.
To many the only perceived way to reduce junior doctors' working hours is through an unbalanced expansion in non-consultant posts.
This stems largely from a failure to embrace new work practices and is no part of the new deal.
This tension is manifest in the proposed creation of hundreds of extra staff grade and senior house officer posts in Trent region alone to achieve the maximum 72 hour week for junior doctors.
This is in addition to the many illicit senior house officer posts uncovered by the task force process and ratified retrospectively.
There has been no proportionate increase in consultant numbers despite Higginson's work showing the benefits of this approach.
Trent has become notorious in this respect.
As representatives of junior doctors we cannot condone the unilateral abandonment of agreed manpower targets, but we believe that the Trent regional task force has been singled out unfairly.
Trent is not alone among regions; it is merely that Trent does not conceal its statistics.
There are rumours from other regions of two manpower lists, one for local consumption and one for the Department of Health.
Adherence to agreed manpower limits is complicated by the fact that the Department of Health counts doctors in post, whereas counting the establishment is the only realistic way of planning locally.
These two figures differ by about 4.6% (100 posts) in this region.
Trent regional task force has worked with exemplary commitment to achieve both the agreed deadline next year and the interim deadline of 1 April this year.
An experienced general manager has been retained in a consultancy role and has been present on practically all the visits to units since August 1991.
His skill has resulted in accurate information on actual hours of work rather than the, often lower, contracted hours.
Sir Duncan Nichol has recently moved in this direction (letter to regional general managers, 22 Dec 1992).
It has become obvious that the units where the new deal is not yet succeeding are those in which local management, whether medical or professional, has failed.
Those units where management has ignored or abdicated its responsibility are those that are now panicking in the approach to 1 April.
It is acknowledged that the deadline of this April will largely be achieved, if only on paper.
Moving to actual rather than contracted hours, examining work intensity, and achieving the 1994 controls depend, however, on a coalition of juniors, managers, and consultants assessing the working practices of all staff.
Any failure of the new deal to achieve its aspirations will be a failure not only of local management but of the management  executive in not exercising effective and comprehensive data collection, monitoring, and implementation procedures.
Costs of screening are important
Editor ,— Recent issues of the journal have included four articles on screening programmes.
The outcome of a screening programme (as with many other health care activities) depends on its effect on three main variables: length of life, quality of life, and costs.
It is therefore surprising that none of the articles includes any mention of the costs that might be incurred.
D M Bradley and colleagues' paper on a screening programme for Duchenne muscular dystrophy in newborn infants describes the benefits for the nine affected families that were identified.
It would be interesting to be able to weigh these against the costs of over 34000 blood tests, pretest information, and more sophisticated analysis and counselling in those in whom the result of the test was positive.
Fritz H Schröder's editorial on screening for prostatic cancer makes the point that the specificity and positive predictive value are too low for any of the available screening tests to be recommended.
This statement needs to be expanded in relation to the cost disadvantages of screening as well as the physical and psychological disadvantages.
Finally, A L Clark and A J S Coats's editorial on screening for cardiomyopathy suggests that improved test validity and proved treatment would be sufficient to justify echocardiography on a mass scale.
Though this may be the case, it would be helpful to be given some idea of the potential cost-benefits and disadvantages.
A screening programme is a public health measure, and any balanced discussion should make at least some reference to its overall impact on the use of health care resources.
Increase in staff grade posts
Editor ,— The number of advertisements in the BMJ for staff grade positions seems to have increased in recent months.
Funding for such posts seems to be easier to obtain than funding for additional consultant posts, and the  restrictions imposed by the Joint Planning Advisory Committee do not apply.
This trend has important implications.
There may be a change in the balance between junior staff posts and consultant posts, which is the basis of the advisory committee's calculation of the number of trainees required.
The trend will probably further delay the creating of new consultant posts, which is critically important for some specialties, including oncology, where unrealistic workloads are carried.
It is important that proper consideration is given to the educational needs of people appointed to such posts, which could quickly become dead end jobs.
Experience suggests that these posts will mostly be occupied by women wishing to work less than full time, particularly since the full time contract in many specialties implies a considerable amount of unpaid overtime work.
It is possible that there will be a trend to suggest such posts for women, rather than efforts being made to develop training and working schemes that take into account the professional potential of all qualified doctors.
Those interested in postgraduate and continuing medical education and in the best use of all qualified medical practitioners should be concerned at these developments.
They should continue to seek solutions for the needs of the servide that take into account equality in opportunity and education rather than just short term financial considerations.
Health services research
Editor ,— The dialogue between a sociologist and a director of research, by Catherine Pope and Nicholas Mays, sounds all too familiar.
The unproductive conflict between ‘soft’ science and ‘hard’science obscures their complementary contribution to our understanding of the world.
The general form of their relationship has been expressed in Hammond's cognitive continuum, on which can be mapped any of the processes by which we gain knowledge of our environment.
At one extreme lie true experimental procedures, which demand a high degree of control over possible confounding factors.
At the other end lie experiential and non-interventional studies of human behaviour that use the sort of ‘let it all hang out’ approach that so exasperates directors of quantitative research.
Pope and Mays's cognitive approaches fit well into this framework (figure).
What is clear from their paper and from everyday experience is that the potential for dispute, although considerable at the experimental end of the continuum, becomes seemingly limitless at the observational end.
Where most of life actually happens, however, is in the subjective, soft, and statistically unclean world of human experience and judgment.
Our challenge is to make explicit the basis for the intuitions and judgments that happen there and to make applicable the results of experimental interventions gained under more constrained conditions.
For example, the informal decision making that is a component of ‘surgical signatures’ and ‘practice style’can be made explicit by use of clinical judgment analysis and related techniques, which set the basis for variations in diagnostic and therapeutic judgment on a statistically firm footing.
Once made explicit in this way judgment policies can be used to help reach a consensus when unaided discussion fails.
This failure may be a consequence of the inability of most experts to describe the policies they operate and consequently the systematic basis for their skill.
Clinical judgment analysis and related approaches have allowed us to probe why doctors' diagnostic and therapeutic decisions vary, what is really meant by a treatment success, and what it is that patients really value (and fear) about the treatments they are given.
These are the questions and concerns of the moment.
Some of the necessary tools for opening the black box of human judgment are now available.
The means of hardening the soft end of the cognitive continuum are now tried and tested.
All that remains is for their critical application to be supported by imaginative funding.
Aspiration of earwigs from metered dose inhaler
Editor ,— V S Taskar and colleagues' article on foreign body aspiration as a hazard of metered dose inhalers appropriately emphasises aspects of good inhaler technique.
I experienced the problem about five years ago, when I had a bout of wheezing in the late summer.
At about 2 am I fumbled across the bedside table for my bronchodilator.
After a sharp deep draught on the inhaler I was horrified to experience a disagreeable and wriggly sensation somewhere in the centre of my chest.
Rapid and explosive coughing followed, with the production of first one earwig and then, after further  excruciating coughing and slight haemoptysis, a second.
Nowadays I am obsessional about checking the gadget, particularly as I use it infrequently.
Dust, grit, and ‘wee beasties’ seem to be able to gain admission to the vestibule of the inhaler through the open top of the canister housing, even though the safety cap has been religiously applied.