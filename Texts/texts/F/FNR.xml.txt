

SOME BASIC IDEAS
Summary
Herein are summaries of some of the techniques which will be used later.
They include
logic and graphs
search spaces, with some classic examples
the notion of a description language, and how it affects search
different varieties of search: forward or goal-directed, search for a goal or for a path to a goal, and the distinctions between monotonic, reversible and irreversible search
some basic search algorithms
heuristic guidance of search
metric spaces
experimental backing
complexity.
The algorithms we shall investigate will call for quite careful description.
Plain text could get us a long way, but words are an imprecise medium, and pictures and other symbolic descriptions often help; so we shall need a few mathematical ideas and notations.
Several good references are listed in the further reading, but you need not worry about them now.
You will find this material easier to read if you know the names of some letters of the Greek alphabet.
Also, do not be surprised if you see conventional symbols used in odd ways.
For instance, the symbol’ <’may represent any partial order, not just the standard order on numbers.
2.1 Logic
This is not the place to explain logic, but there are a few bits of terminology which occur often in this subject which are not entirely standard.
an atomic formula is one of the form p (t, t,…t) where each t is a term, and p is a predicate symbol which expects k arguments (i.e. p has  arity k).
a literal may be either an atomic formula or a negated atomic formula * p (t, t,…t).
a (disjunctive) clause is a sentence composed only with the connectives v and * in which all variables are universally quantified.
The quantifiers are usually omitted.
Thus, the sentence
Ax (Ey likes (y, x) 6 happy(x)) is equivalent to the clause * likes (y, x) v happy(x).
Both can be read as’ Anyone who is liked is happy’.
2.2 Graphs
The graphs we are concerned with are quite unrelated to the sort which physicists and engineers draw on graph paper.
To a professional, a graph consists of a set V, called the set of vertices or nodes, and another set E of arcs or edges:
G = (V, E)
An edge is an unordered pair of nodes.
True pedants add the proviso that an edge cannot also be a node.
Imagine what the graph would look like if it was!
Most graphs which you will come across in these references are finite.
We shall require infinite ones too, because almost all interesting search spaces are infinite.
There is always a trivial algorithm for searching in a finite space — just list all nodes, and examine them in turn.
That is the simplest case.
Next, we come to the notion of a directed graph, sometimes called a digraph.
This is a set of nodes V, as before, but now an edge is an ordered pair of nodes: G = (V, E) where E c VxV
In pictures, a graph consists of points and lines as in Figure 1, while in a digraph, Figure 2, the lines have arrows on them.
The arrow points from the first node a of the pair (a, b) towards the second, b.
Note by the way that a picture of a graph, like Figure 1, is not itself the graph.
It is an embedding of the graph in the plane of the page.
One graph has lots of possible pictures.
Figure 3 is another picture of the same graph as Figure 1.
In any graph, each node is different from all the other nodes.
If you like, each node has its own name.
One often meets graphs in which the arcs have names too.
This is called a labelled graph.
The name attached to an arc is called its label.
A label can be more than just a sequence of characters.
We shall come across labelled graphs in which the labels are elaborate operators.
When arcs have labels, there can be more than one arc between two nodes.
Of course, two arcs with the same ends must have different labels.
A path in a graph is a sequence of distinct nodes a b c…d e with arcs between them from a to b, b to c,…d to e.
Its length is the number of arcs.
If e is the same as a, and the length is at least 1, then it is a cycle.
The graph is connected if any two nodes in it are the ends of a path.
The same ideas apply in digraphs, except that all the edges must point in the same direction.
To a true graph theorist, a tree is a connected graph with no cycles.
However, nobody thinks of a tree like that.
Every tree which occurs in computing has a distinguished node, called its root.
When we have decided on a root, then the tree automatically becomes directed: Each arc is given an arrow pointing away from the root, as in Figure 4.
Then, to each node e, we can associate its set of children which are the far ends of the arcs pointing away from e.
A leaf is a node with no children.
Any node with children is said to be internal.
The depth of a (rooted) tree is the length of a longest path from the root to a leaf.
The depth of any node is the length of the path from the root to it.
Thus, the root itself is at depth 0.
The typical number of children of each internal node in a tree is called its branching ratio.
One often comes across trees in which every node is either a leaf or else has just two children.
Such a tree is called a binary tree, and of course its branching ratio is 2.
Often, there is no fixed number of children per node.
In that case, the’ branching ratio’is a suitable number which makes the calculations work the way we want.
For instance, it is often convenient to be able to estimate how many nodes there are in the whole tree.
If all leaves are at roughly the same depth d, and the number of children per internal node is fixed, say n, then this is 1 + n + n + n +…+ n
If the number of children varies, then the branching ratio n can be taken to be the number which makes this sum give the right answer.
In fact, computing people add yet another layer to this stack of definitions, though they very rarely make it explicit.
In practical cases in computing, in a tree, the children of each node are often not just a set.
They are ordered, so they form a list.
Any picture of such a tree, with ordered lists of children, conveys a much more thorough image than does a picture of an average graph, because the picture can exhibit the order in each list of children.
The root is at the top, as in Figure 5 (not at the bottom as in botany).
Each node's children appear below their parent, in order from left to right.
This is standard convention.
Since almost everyone who writes about computing does not usually bother to make all these distinctions, I shall too.
Whenever you come across a graph or a tree, it will be up to you to recognise what kind it is.
2.2.1 Covering trees
Figure 6 shows a graph and part of its cover.
If this picture makes some sense to you, then you need not read the rest of this paragraph.
Suppose you are presented with any connected graph G with vertices V and edges E, and a root r.
(Even when G is not a tree, we can still choose a root for it.)
Unless it is a tree, there will be some cycle in it.
There is a standard way of constructing a tree called the (universal) cover of G, or’ gee twiddle’
G– = (V–, E–, r–) with the following pleasant property:
There is a projection from the tree G– onto G. That is,
every node of G– is’ above’some node of G; and
every edge of G– is above some edge of G; and
G–'s root r– is above r.
Furthermore, G– is the largest possible such tree.
Every node and every edge of G is underneath a matching object in G–;
If you were an ant crawling around in G–, it would seem at each node as if you were on G. The only way to tell the difference is that in G, if you go far enough in the right direction, then you arrive back where you began; whereas in G–, the only way to return to the start is by retracing your steps.
All such covering trees look the same.
They are isomorphic graphs, and they go by a common name: G–.
For the technically minded, there is a standard way of constructing one such G– from G. It involves the notion of a walk.
A walk is like a path, except that it may cross itself:(a b c b…a d e)
If the walk does not have any triple of successive nodes like…b c b…so it never goes from one node to another and then straight back again, then let us say the walk is reduced.
You can always convert any walk to a reduced walk, by deleting pairs of nodes:…f g b c b h…changes to…f g b h…
Given any node a in G, the nodes of G– above a are reduced walks in G from r to a:(r s t…u a)
This may seem a curious idea.
The point is, mathematical notation gives us complete liberty, unless it explicitly states otherwise.
You can make any set into the set of nodes of a graph.
The set chosen here, the reduced walks from r in G, fulfils all we require of V–.
The root r– of the cover is represented by (r), which is a walk of length 0.
If there is an arc in G from a to another node b, then (r s t…u a b) is a walk in G, so when reduced if necessary, it yields a node of G–.
The arc (a, b) in G is covered by the arc ((r s t…u a),(r s t…u a b)) in G–.
If you want an idea of what is going on, look at Figure 7.
This shows a graph G which is actually a tree, and its cover G–.
Since G is already a tree, it and G– are isomorphic; but the nodes of G–are unlike those of G.
Similarly, any connected digraph with a root has a covering tree.
However, all the paths in the construction are directed; that is, all the arrows on a path point the same way, away from the root.
This means that there are fewer paths, so the cover of a digraph may be smaller than the cover of the undirected graph.
In fact, the cover of a digraph may even be smaller than the original digraph.
The cover of a digraph may be finite, while if you leave out all its arrows, its undirected cover is infinite.
Figure 8 shows an example.
As an exercise, you may care to draw a picture of a labelled directed rooted graph G; and then draw a picture of part of its cover.
The cover is a labelled tree.
Each edge in the cover will be labelled with the same label as the edge it covers.
2.3 Search
Searching consists of trying to find a way to change the present state of affairs into some desired state.
This desired state is called a goal state.
The change is made in a finite sequence of discrete steps.
An operator is a process or action which makes one step.
It generates a new state from an old one.
Example: Binary chopping, again
When you perform this algorithm, the states are pairs of numbers (x, x) where x <x.
Given any such state, there are two operators which act on it.
The first produces a new state (x, x) and the other produces (x, x) where x = (x +x) /2.
The goal is any state (x, x) in which x — x s 0.0001.
Example: Learning to recognise an arch, again
In this case, the important aspect of a state is what is in the two sets of properties.
Remember, the first set contains properties which are common to arches, and the other holds properties which arches don't have.
One step in the search consists of examining a pile of wooden blocks, and modifying the contents of the sets accordingly.
The operator adds a property to one of the two sets.
The goal is a state in which the set of properties common to all arches includes the assertions: there are two columns there is an extra block the extra block rests on both columns and the other set of properties contains the assertion: the two columns touch.
Search is hard because there are (usually) many paths from the start state.
The art of search lies in choosing a short sequence of operators which will lead the search to a goal quickly.
Choice of operator is often made using rules.
There are various forms of rule.
The standard form is
IF conditions
THEN action
This means that, if the conditions are all true in some state, then the operator called action can be applied to the state, and what is more, this operator may help the search.
Example: The’ blocks world’
There is a well known setting for search called the’ blocks world’.
In it, there are three wooden blocks labelled A, B and C, and a table.
Any state is described by saying what is on what.
Initially, we can imagine that A is on top of B which is on the table, and C is also on the table.
The goal is the state in which A is on B and B is on C, on the table:
Thus, there are four constants in the blocks world:
A B C table and one relation which is specific to this task: on (x, y).
Using just it, the start state is described as on (A, B) ∘ on (B, table) ∘ on (C, table) ∘
Ax * on (x, A) ∘ Ax * on (x, C) and the goal is on (A, B) ∘ on (B, C) ∘ on (C, table) ∘ Ax * on (x, A).
In practice, the blocks world is more conveniently described if we introduce a second predicate: clear(x) which, by definition, means Ay * on (y, x).
Using it, the start state is represented as the statement on (A, B) ∘ on (B, table) ∘ on (C, table) ∘ clear(A) ∘ clear(C) and the goal state is on (A, B) ∘ on (B, C) ∘ on (C, table) ∘ clear(A).
The natural rules for this task have conditions which also include the predicate which checks that two things are different.
One does not usually bother to mention it, but we shall make it explicit this time.
There are two operators.
The first, stack (x, y), takes a block x off the table and puts it on another block called y.
The second, unstack(x), does the reverse.
Thus the operator stack forms a new state from the current one by deleting the facts on (x, table) clear(y) adding the fact on (x, y) and unstack works by deleting the fact on (x, y) adding the facts on (x, table) clear(y)
They are applied by the two rules:
IF on (x, table) ∘ clear(x) ∘ y table ∘ clear(y) ∘ x y
THEN stack (x, y)
IF on (x, y) ∘ clear(x) ∘ y table
THEN unstack(x)
In certain states, a rule can be applied in more than one way.
A particular choice for the variables x, y and z in a rule gives an instance of the rule.
In the start state of the blocks world, this does not happen.
The two rules can each be applied in only one way.
For instance, the first rule can only be applied when x is C and y is A. It produces the state on (A, table) ∘ on (B, table) ∘ on (C, table).
In this new state, the rule for unstack cannot be applied at all, but the rule for stack has six instances: x is any one of the three blocks; y is any block other than x.
In this example, the shortest path to the goal is via the instance x is B y is C which produces the state on (A, table) ∘ on (B, C) ∘ on (C, table)
Example: Finding a forced win, in Os and Xs
In this game, the board is a 3*3 array of squares, like a b c 1 X
The individual squares will be called’ a1’,’b2’, etc.
The structure of the board can be described by eight facts stating which triples of squares form lines: linear (a1, a2, a3)…linear (a3, b2, c1)…
(The order of arguments in linear doesn't matter.)
The other predicates which we use to describe the game are empty(x) which signifies that the square x is empty; filled (x, C) where x is a square and C is a player; to-move(C) which is true if it is C's turn to play; win(C) which is short for the formula
Ex Ey Ez (linear (x, y, z) ∘ filled (x, C) ∘ filled (y, C) ∘ filled (z, C)).
If you are playing with O then the goal is win(O).
Once the board's structure has been given, the position in this picture is completely described by the facts empty(a1) empty(a2) empty(a3) empty(b3) empty(c1) empty(c2) empty(c3) filled (b1, X) filled (b2, O) and, if it is O's turn to play, to-move(O).
In the game, there is just operation, called play (x, C).
It deletes the facts empty(x) and to-move(C) which are this operation's necessary conditions, and adds filled (x, C) and to-move(D) where D C.
The task of this example is not to play the game but to discover forced sequences of moves.
In any such forced sequence, moves will occur in pairs: First player O plays, and then player X plays.
Hence, an operation in this task's search space consists of a pair of play operations.
An operation play (x, O); play (y, X) of the search deletes the facts empty(x) and empty(y) and adds filled (x, O) and filled (y, X).
Here are two rules which may help to plan a forced win.
IF to-move(O) ∘ linear (x, y, z) ∘ filled (x, O) ∘ empty(y) ∘ empty(z)
THEN play (y, O); play (z, X)
IF to-move(O) ∘ linear (x, y, z) ∘ linear (x, y', z') ∘{y, z}{y', z'}∘ empty(x) ∘ filled (y, O) ∘ empty(z) ∘ filled (y', O) ∘ empty(z')
THEN any move beginning play (x, O) will lead to win(O).
Strictly speaking, this second is not a proper rule because it does not end with a single well defined action.
You can consider it as an abbreviation for the two rules with this condition and the actions play (x, O); play (z, X) and play (x, O); play (z', X).
A search space is a labelled directed graph.
It depicts the present situation, the desired situation, all ways of obtaining all possible situations, and the possible steps between them.
Each possible situation is a’ state’.
Its nodes are the possible states.
Its edges are labelled with the operators between states.
an edge is directed from the old state to the new state which its operator generates.
In these notes, an edge will often be identified with the operator labelling it.
In addition, a search space has
a goal.
a start state.
The start state is the unique state where search begins.
It is the graph's root.
The goal is a characterisation of the set of goal states.
It may be
the particular goal state, if there is just one, or
a predicate which defines goal states.
This predicate is sometimes given in terms of an entity which occurs in goal states and not in others.
The artificial intelligence community sometimes uses terminology a bit loosely.
Example: The eight tiles problem
You can buy versions of this in toy shops.
It consists of a plastic 3*3 square containing eight little tiles and one empty space.
The tiles have grooves and ridges on them so that they do not fall out.
There is one empty position.
Let us suppose that the tiles have numbers 1, 2, 3,…8, and that the goal is the configuration where the empty space is in the middle.
If, when you first pick it up, the tiles are in the positions then one path to the goal proceeds:
Some of the search space's side branches have also been shown.
The whole search space is big — much too big to be drawn on one page.
This task has four operators: A tile may be slid Up, Left, Right or Down.
The edges in the search space are marked accordingly.
In binary chopping, a state is a pair of numbers.
A goal is any pair close enough together.
By contrast, in the blocks world, there is a unique goal.
There are various algorithms for searching in different cases.
Some of them will be described soon; but first, as always, it behoves us to study their data.
The principal datum input to any search algorithm is a description of its search space.
Examples: Characteristics of search spaces
1 Binary chopping
The start state is the pair (0,1).
A goal is any pair of numbers (x, y) for which x s S s y and y-x s 0.0001
2 Inventing the notion of an arch
The start state is the pair of two empty sets ({},{})
A goal is any pair of sets (L, L) for which every property in L is fulfilled by every arch, and anything which is not an arch has at least one property in L.
An operation adds a single property to one of the two sets.
3 The eight queens problem
A state of this task consists of a chess board, with up to eight pieces on it so that no two pieces are on a common row or file or diagonal.
The start state is the empty board, with no pieces on it.
An operation consists of adding one extra piece.
Naturally, it must not be put on a square which is on the same row or file or diagonal as another piece.
A goal is any state with eight pieces on the board.
4 The eight tiles problem
The start state is whatever state the toy is in when you find it.
An operation involves sliding a tile into the empty position.
This changes the positions of the moved tile and the empty space.
The tiles are numbered or painted, and the goal is a particular arrangement of them.
There is just one goal.
5 The crates problem
A warehouse is nearly full of heavy crates.
One at the back has to be delivered, so it must be moved to the front door, but others are in the way.
There are two views of this task, the real one and the naive one.
A naive state is some arrangement of crates.
A naive operation consists of pushing one crate into an adjacent free area.
It is raining, so no crate may ever be moved outside.
Also, stacking crates on top of each other is not allowed.
A naive goal is any state in which the crate to be delivered is by the door.
The naive start state is the state of the crates when you arrive.
This description does not depict the real problem.
What is really wanted is not to get the important crate to the door, but to get it there without expending more effort than necessary.
Hence, the real goal is a path of least effort through the naive space.
A real state consists of a naive state and a sequence of naive operations which produces it from the naive start state.
A real operation is almost the same as a naive one.
It involves pushing one crate into an adjacent space, thus producing a new naive state, and also extending the sequence of naive operations by this extra push.
A real goal is some state consisting of a naive goal, with the crate to be delivered at the door, and a sequence of pushes getting it there which involves least effort.
The real start state is the naive start state and the empty list of pushes.
These spaces have quite different characteristics.
1:
The search space is a tree.
No two different paths from the start will ever arrive at the same state.
2:
The search space is not necessarily a tree, because properties may be added to the two sets in various orders depending on the order of training instances.
However, once a property has been added to either set, it is never removed.
Hence the space does not have cycles.
3:
As for 
2
, the search space is not a tree.
Pieces can be put on the board in any order, so one position can arise via several sequences of operators.
One a piece is on the board, it is never removed; so the search space does not have cycles.
4:
This space has cycles.
It is easy to slide tiles around and find they are back in some position they were in before.
5:
The naive space has cycles.
The real search space does not because a real operation extends the list of naive operations which move crates.
(This is not much consolation, if ever you find you have pushed all the crates in circles.)
The real search space is almost the cover of the naive one.
It would be the cover, if we were clever enough never to push a crate and then decide to push it straight back to where it was before.
A similar situation arises in any task which involves finding an easiest solution.
There is a naive search space, whose states are the situations which you could produce, and the real space whose states record ways of obtaining naive states.
Here are some more examples, with summaries of their properties:
6 Composing crossword puzzles
a state is a partly filled form of a crossword.
an operation consists of entering a new word.
the start state is the empty form.
a goal is any form with all positions for letters filled in.
The task space is a finite digraph without cycles.
Objective: Find a goal
Branching ratio: about 100
Depth: constant (depending on the crossword)
Ratio of goals to all leaves: maybe s 1/10000
7 Theorem proving
There are various approaches.
For the sake of argument, consider the tableau method which is described in the book by Wilfrid Hodges mentioned in the further reading.
a state is a tree.
Each path from the root to a leaf includes all hypotheses, the negation of the statement to be proved, and various consequences of them.
an operation consists of choosing a composite statement in some branch, and extending the branch by expanding this statement into simpler ones.
a goal is any tree in which every branch contains two contradictory statements.
the start state is the minimal tree containing just the hypotheses and the negation of the statement to be proved.
The task space is a digraph without cycles.
Objective: Find a goal
Branching ratio: finite (may be unbounded)
Depth: may be infinite, depending on the task
Ratio of goals to all leaves: may be very small or zero.
8 Exploring the world
Five hundred years ago, the map of the known world had large gaps in it.
Many learned people still thought that the world was flat.
Each time a new expedition returned from foreign parts, a little more of the map could be filled in.
a state is a state of the map of the known world.
an operation consists of mounting an expedition and then filling in a bit more of the map when it returns.
the goal is the completed map.
the start state is the current state of the map, just before Columbus first sailed west.
The task space is a digraph without cycles.
Objective: Find a goal
Branching ratio: r 100 (as there were some hundred possible directions for a new expedition)
Depth: r 100
Unique goal
2.3.1 Description languages
The language in which a task is posed can have a crucial bearing on its solution.
In most of the examples above, this language is not specified.
Really, it should be made explicit.
Different languages lead to different task formulations, and half the art of searching lies in how you pose the task.
Example: The eight queens problem in more detail
Here are three different ways to represent a state.
Representation 1: As a set of coordinates of filled squares.
In this representation, the state 
The start state, the empty board, is represented as{}.
Representation 2: As a list with one entry for each row of the board.
If there is a queen on the board on the square in the ith row and jth column, then the ith entry in the list is the number j.
If there is no queen in the ith row, then this list entry is 0.
Thus, the empty board is [0, 0, 0, 0, 0, 0, 0, 0].
In this representation, the above board appears as [8, 6, 4, 2, 0, 0, 0, 0].
Representation 3: This also portrays the board as a list, but it can only represent boards which have queens in the bottom i rows for some i, and none in higher rows.
Thus, the empty board is []and the board above is represented as [8, 6, 4, 2].
This representation cannot depict the state 
Example: The’ blocks world’again
The obvious way to describe this task is with constants: A B C table relations: on = connectives: ∘ v 6 * variables: x y…quantifiers: A
This is the apparatus which we first assumed, in the task's original definition.
Any state, and the condition for any operation, can be written in this language.
In practice, though, states and conditions can be described in a simpler language containing only the symbols constants: A B C table relations: on clear connectives: ∘ variables: x y…
If we select a more suitable set of relations, then we can get by with fewer connectives.
Also, since all variables are assumed universally quantified, there is never any need for an explicit quantifier.
This second language is less expressive than the first.
All the ideas expressible in the second can be reproduced in the first, but, for instance, the notions of disjunction (v) and implication (6) and negation (*) which occur in the first cannot be expressed in the second.
Doing without negation is a particularly helpful feature, for a reason which will be explained below.
The process of choosing a language for a task is sometimes called’ adjusting the task's bias’.
Usually, we shall take the language for granted.
Adjusting bias will be discussed in Chapter 6 and again in Chapter 8.
Notice that the language discussed here is only intended to describe the individual states and the goal of the task, and the conditions for applying operators.
It is not adequate for describing the task's operators, nor does it suffice for description of the whole search space.
Thus, in the crates problem, the naive states can be described just by stating which crate is where, but the states of the real search space involve subtler relations.
It is often a good idea to use an unexpressive language.
The reason is simple.
The more expressive the language, the more possible states can be described in it; and hence, the larger will be the space of states that a solver may have to search through for a goal.
Also, an unexpressive language permits few operators, so the search's branching ratio is small.
Example: Eight queens again.
Representation 1 allows us to depict any set of pairs of coordinates.
There are 2 possible states which can be described in this representation.
This is a vast set of states.
Most of them are illegal because two queens conflict.
In addition, the branching ratio at the start is 64, and it never falls below 57.
Searching this space directly would be totally impractical.
Representation 2 can depict 9 possible states (8 entries in the list, each of which can take one of 9 possible values).
It can depict all legal states, so all the excluded ones are illegal.
It is a great improvement over representation 1.
However, a state with k unoccupied rows has 8k children, so the branching ratio starts at 64 and remains high for most of the search.
Searching in this space would be difficult.
Representation 3 can depict 8 + 8 + 8 + 8 +…8 = 19,173,961 states.
There are some legal arrangements of queens on the board which this representation cannot depict.
However, it can depict all goals (because in any goal there is a queen in every row).
Also, this representation suggests a particular limited set of just eight operators: Given any incomplete state, the jth operator adds a queen in the jth column.
In this representation, the search space is a tree with branching ratio no more than 8.
When illegal states are excluded, the branching ratio is less.
Search in this space is easy on a small computer, and is even practical manually.
Half the art of search lies in choosing a language which suffices to describe the task and an adequate solution but very little more.
If you can find such a language, then there will be very few alternatives and so your search will probably not go astray.
Your task may cease to be a search task at all.
There are circumstances when it pays to use an expressive language.
Sometimes a task can be posed in a simple language but the best search path leads beyond the space describable in that language.
However, these cases are hard.
(It is tempting to say that they are rare, but the truth is that they only seem rare because we avoid them.)
As a matter of principle, it is better to use an unexpressive language.
Heuristic information (that is, suggestions about how best to search) is often more easily  exploited when the language is enlarged, but that does not necessarily mean that it has to be made more expressive.
For instance, the predicates clear and’’ were used in the conditions for the operations in the blocks world, but adding them did not extend the descriptive power of the language.
They just make these conditions appear simpler.
Predicates and functions
You may have noticed that the description we have given of the blocks world is quite unlike all the descriptions of the eight queens task.
A state of the blocks world is portrayed as an assertion, formed with predicate symbols.
By contrast, in each of the three representations of the eight queens problem, each state is a term.
Terms are constants or are constructed using function symbols.
For instance, the empty list []is a constant.
Any longer list [a, b, c,…]is a term built from the list's head (in this case, a) and its tail (the rest:[b, c,…
]). The list notation does not make any function symbol explicit; but in prolog, for instance, one can write such a list as.
(a,[b, c,…])
The ‘.’
is actually a function symbol.
Usually, there is no great difficulty in translating from a description using predicates to one using functions, or the other way round.
In section 6.5, we shall see that the choice of one style of description, rather than the other, can have a substantial effect on learning.
For the time being, just bear in mind that there are these two styles.
2.3.2 Reversible and irreversible search
In most of the examples above, every operator can be reversed.
The eight little plastic tiles can always be pushed back into their original state (if you can remember the sequence of moves you have made so far).
A queen can be put on the chess board, and then taken off again.
If you really want to, you can push the crates around and then push them all back again, and nothing will have changed except the dust on the floor and your state of health.
You can erase words from a partly designed crossword, and you can prune a partly completed tableau by just forgetting some of it.
Exploring the world is different.
Once part of the map of the world is known, and that knowledge has been disseminated, there is no way that it can be censored.
Once Columbus had found his new continent, and the discovery was common knowledge, anyone else mounting an expedition westward would have a rough idea what he would find.
The common pool of knowledge grows inexorably.
Extensions of the map are irreversible.
Note: Saying that an operator is reversible is not the same as saying that it is an invertible function on the set of states.
For instance, all four operators U, D, L and R of the eight tiles problem are reversible, but they are partial functions on the set of states and so they do not have inverses.
Another real life example of irreversible search occurs while
9 Cooking
a state is an arrangement of cooking utensils and ingredients in a kitchen.
the operations of cooking include moving things, heating, mixing ingredients, cutting, and breaking eggs.
the goal is supper.
the start state is the state of your kitchen when you arrive home after work.
Moving something is a reversible operation, but mixing, cutting and breaking are irreversible.
Heating is usually irreversible, because foods change their textures irreversibly when heated.
Planning involves simulating a difficult or irreversible search.
A planner is given some such task, where any venture into search would be rash.
It constructs a second more tractable search space.
There are two ways whereby the planning search may be easier than the real one:
the operators in the planner's simulation may be reversible, or
the planner may be able to remember several states.
We shall soon see that some good search algorithms involve remembering many states.
Thus, if one search path turns out to be wrong, the planner can recover and try a different path.
A plan is a path in the simulation from its start to a goal.
If the plan is faithful to the original task, then
there will be one operator for each original operator;
the search graph in the simulation will be isomorphic to that of the original task.
Planning need not be faithful.
When cooking, a plan can be invented in the cook's head.
He imagines the sequence of states which he will have to construct on the way to supper.
When he has invented such a plan, if he is wise, he will check that it really is faithful.
Suppose that the goal is a mushroom  omelette .
The initial plan might consist of the operations:
a
break eggs into a bowl;
b
whisk eggs;
c
heat fat in frying pan;
d
pour whisked eggs into hot fat in the pan;
e
chop mushrooms;
f
mix the chopped mushrooms with the eggs; etc.
This plan is not faithful to reality.
As soon as the eggs meet the hot fat, they will harden and the mushrooms could never be mixed in with them.
Also, by the time operation e is complete, the eggs will be burning.
A more nearly  faithful plan would be
a
break eggs into a bowl;
b
whisk eggs;
c
heat fat in frying pan;
d
pour whisked eggs and mushrooms into hot fat in the pan; etc.
e
chop mushrooms;
f
mix the chopped mushrooms with the eggs;
Planning is valuable for solving irreversible search, as in cooking.
It is also useful when some operators in real life are reversible but’ expensive’, and they can be simulated by cheaper ones.
Pushing heavy crates expends a lot of energy, so planning would be appropriate in that task too.
A fourth case where planning helps occurs when the real task involves a deep subtle search and there is a simpler simulation of it.
The search space of the simulation is formed by partitioning the real search space's set of states into disjoint subsets.
Each subset is a single’ state’of the simulation.
For example, when cooking, the whole solution can be abbreviated into three major steps: a
a
prepare ingredients b
b
cook ingredients c
c
lay table and serve.
If this abbreviated plan is convincing, then it can be refined to a more nearly faithful one.
The process of constructing a sequence of plans like this, each a refinement of the one before, is called hierarchical search or stepwise refinement.
It is often quicker than direct search in the original task's space.
2.3.3 Goal directed search
Study the search space which is drawn in Figure 9a.
This directed graph has a significant property which is common in real tasks: At almost all nodes, the node has at least as many children as parents, and several nodes have more children than parents.
Also, there is only one goal state.
Let us suppose that
on average, each state has more children than parents;
all operators are reversible;
there is just one unique goal state;
the task specification includes an adequate description of the goal.
When this happens, we can convert the search of Figure 9a into another simpler search as follows:
The states of the new space are the same as those of the original one.
Its start state is the (unique) goal of the original.
Its goal is the start state of the original.
Operators of the new space are the reverses of operators of the original.
The new space has as many nodes as the original, but often a lot of its nodes will not be accessible.
In Figure 9a, there is no directed path backwards from the node marked’ Goal’to any other leaf of the space.
Hence, the relevant part of the new space is smaller, so search is easier.
Figure 9b shows the space which a naive search algorithm will explore for this task.
It is the cover of the original space.
A naive algorithm may generate some state twice by two different paths, and not detect that this state occurs twice.
Figure 9c shows the space which the same algorithm would explore if it were given the backwards version of the task.
Observe that 9b has fourteen nodes but 9c has only nine.
An algorithm which performs this trick before it starts searching is said to be goal directed, or a backwards searcher.
There are cases when backwards search is no use.
In the examples, 1, 2 and 3:
The specifications of these tasks do not tell us what a goal state is.
The objective is to find out.
There are many goals.
4: In the eight tiles problem, there is just one goal state, and we know what it is, and the objective is to construct a path to it.
However, every state has just as many parents as children, so searching backwards won't help.
5: As in 4, each state has many parents.
Also, there are many naive goal states, differing in the positions of all the other crates.
However, in the case of cooking, goal directed search can help.
If you can make up your mind what you want to eat then there is a unique goal.
The real task does not have reversible operators, but any planning simulation of it will.
Goal directed search is useful much more often than this discussion suggests.
The eight queens and eight tiles tasks are not typical of real life search situations.
They were adopted by academics for the purpose of studying search.
They are good little theoretical exercises because they are easily described and algorithms for solving them are easily programmed, but they have a lot of symmetry, which is most uncommon in real life.
2.3.4 Monotonic operators and theorem proving
In this section, we shall assume that any task is described by assertions formed with predicates.
The notion of monotonicity does not mean anything if states are portrayed as terms.
There is a class of search tasks, monotonic search, in which each operator establishes some new truth compatible with all other previously known truths.
An operator which adds some new knowledge, and which does not undo any fact already established, is said to be monotonic.
The two operators of the blocks world are not monotonic.
Stack (x, y) deletes the facts on (x, table) and clear(y) and unstack(x) deletes the fact on (x, y) for some block y.
Example: Letter arithmetic
There is just one way to replace all the letters by digits in the multiplication
ABA so that the calculation is correct and no two letters stand for the same digit.
This task begins from the start state
A B ∘ A C ∘…
A H ∘ B C ∘…
B H ∘…
G H ∘{A, B, C, D, E, F, G, H}c{0,1,2,3,4,5,6,7,8,9}∘
AC mod 10 = E ∘ (BC + AC div 10) mod 10 = A ∘
AC + (BC + AC div 10) div 10 = D ∘…
The children of the start state consist of the above assertions and one additional one, such as E<D.
Each state is a conjunction of facts extending its parent.
The goal is a state containing eight assertions of the forms
A=d B=d…
H=d where the symbols d are all digits.
Each operator adds a single new fact which is a direct consequence of what is known already.
Problems in the form of this example are called constraint satisfaction tasks.
This particular example has a unique solution, but a constraint satisfaction task can have many goal states which all fulfil the task's goal.
Monotonic search is usually efficient in space, because the searcher does not have to construct each state's representation from scratch.
Apart from the start state, which must be presented in full, any state can be described as its parent plus a few more details.
Hence, in monotonic search, one can often keep many states easily.
Since constructing new states is simple, monotonic search can sometimes be time efficient too.
Almost all monotonic operators are reversible: If the operator adds a new truth, then its reverse just forgets that truth again.
Exploring the world is intriguing because its operator is monotonic but not reversible (among other reasons).
Note: Some authors describe monotonic search slightly differently.
They say that search is monotonic if, whenever two operators can be applied to a state, either can still be applied to the state produced by the other.
The definition used here is closer to the notion of monotonicity in logic.
Monotonicity is a sensitive property.
It depends critically on the description language.
Consider, for instance, exploring the world.
Describing a map with words and symbols, such as appear on this page, is not very easy; but you can imagine that each entry on the map is recorded as a fact of the form here-found (x, y, thing) where x and y are coordinates, longitude and latitude, and thing is any feature such as a mountain or river or city.
A state of knowledge is a set of facts of this form, and so the search can be described adequately in a language containing just the names of things which might be found, all possible coordinates, and the relation here-found.
When this is the chosen language, exploring the world is indeed monotonic.
However, suppose that we extend the language with negation,’ *’.
In the extended language, the fact * here-found (…,…
, America) is true in the start state but it is contradicted in some later states.
Hence, depending on what facts we choose to assert about states, this task in the extended language may not be monotonic.
Search is never monotonic if the description of a state can include negations of assertions which are true in its children.
For this reason, the description languages used in artificial intelligence do not usually include’ *’.
It would be too tedious, each time a new state is formed, to contradict all the assertions of ignorance in its parent.
If a negated conjunct is really necessary in some condition, then the standard trick is to introduce a new predicate which stands for it.
The two predicates clear and’’ of the blocks world serve just this purpose.
Theorem proving is a special case of monotonic search.
Its distinguishing feature is that it uses only a very few basic operators — typically just one, called modus ponens, or some equivalent — which are very well understood and reliable.
All the searcher's other knowledge is in the form of rules about how best to apply this operator, and of facts about states which this operator can be applied to.
The searcher may go astray because either some supposed fact is wrong or it applies modus ponens in an irrelevant way, but it will never arrive at any result which is not a strict consequence of its start state.
Theorem proving is a particularly easy form of search.
All its operators are both monotonic and reversible.
Furthermore, the states of a theorem prover are sets of axioms and proved assertions, and these are easily remembered, so it is easy to keep many states simultaneously.
The major difference between theorem proving and other forms of monotonic search is just the choice of basic operators.
From the point of view of searching, this difference is not very great.
When it comes to learning, there is a more significant difference because a general monotonic learner may be permitted to invent new basic operators, and these may be ill understood and unreliable.
However, in most of what follows, we shall treat theorem proving and other cases of monotonic search as if they were the same.
If you know enough about a task, you can always translate it into an exercise in theorem proving.
In the translation, each assertion is about the space which the original task searches.
A state of the new task is a set of assertions.
An operator is a deduction step.
Hence a child node consists of all the assertions of its parent and one extra consequence of them.
The start state is the specification of the original task's search space.
A goal is a set of assertions which includes one of the form’ A certain sequence of operations will lead to a goal of the original task’.
If ever the theorem prover discovers one of its goals, then back in the original task, you can run off this sequence of operations without any search at all.
The limitation of theorem proving is, the description of the original task may be inadequate.
For instance, it is no use trying to prove that a certain sequence of steps will lead to supper if you don't know what is in your larder.
2.3.5 Summary properties of search tasks
Of any search task, one can ask the following questions:
How big is the search space?
Is the search space a tree?
If not, does it have directed cycles?
(A space without directed cycles is a partial order.)
What is its maximum branching ratio?
What is its depth?
Is there a unique goal?
If not, among all states,
Roughly what is the proportion of goals?
Is the search for a naive goal or for a path to a naive goal?
Are all operators reversible?
Are all operators monotonic?
(We assume some standard representation of the task by predicates.)
For the examples we have seen so far, the answers to these questions are:
Binary chopping (to some given precision)
How big is the search space? finite; depends on the precision.
Form of the search space? tree
What is its maximum branching ratio? 2
What is its depth? small (proportional to the log of the precision)
Proportion of goals? unique goal
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? yes
Learning to recognise an arch
How big is the search space? small
Form of the search space? partial order
What is its maximum branching ratio? low
What is its depth? low
Proportion of goals? unique goal, we hope
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? yes
The’ blocks world’
How big is the search space? 13 states
Form of the search space? has cycles
What is its maximum branching ratio? 6
What is its depth? low
Proportion of goals? unique goal
Is the search for a goal or for a path? a path
Are all operators reversible? yes
Are all operators monotonic? no
Finding a forced win, in Os and Xs
How big is the search space? small
Form of the search space? partial order
What is its maximum branching ratio? low
What is its depth? low
Proportion of goals? may be high
Is the search for a goal or for a path? may be either
Are all operators reversible? if searching for a goal: no if searching for a path: yes
Are all operators monotonic? no
The eight tiles problem
How big is the search space? 181440 states
Form of the search space? has cycles
What is its maximum branching ratio? 4
What is its depth? large
Proportion of goals? unique goal
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? no
The eight queens problem (in representation 3)
How big is the search space? less than 4000000
Form of the search space? tree
What is its maximum branching ratio? 8
What is its depth? 8
Proportion of goals? perhaps about 1 in 1000
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? no
The crates problem
How big is the search space? finite but large
Form of the search space?
The naive space has cycles
What is its maximum branching ratio? perhaps about 20 depending on the number of crates
What is its depth? perhaps about 10
Proportion of goals? perhaps about 1 in 10000
Is the search for a goal or for a path? a path
Are all operators reversible? yes (in the naive space)
Are all operators monotonic? no
Composing crossword puzzles
How big is the search space? finite but large
Form of the search space? partial order
What is its maximum branching ratio? large
What is its depth? perhaps about 50
Proportion of goals? low
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? no
Theorem proving
How big is the search space? may be infinite
Form of the search space? a partial order
What is its maximum branching ratio? depends on the example
What is its depth? may be infinite
Proportion of goals? usually negligible
Is the search for a goal or for a path? a path
Are all operators reversible? yes
Are all operators monotonic? yes
Exploring the world
How big is the search space? finite but large
Form of the search space? partial order
What is its maximum branching ratio? about 100
What is its depth? large
Proportion of goals? unique goal
Is the search for a goal or for a path? a goal
Are all operators reversible? no
Are all operators monotonic? yes
Cooking
How big is the search space? finite but large
Form of the search space? may have cycles
What is its maximum branching ratio? perhaps about 20
What is its depth? large
Proportion of goals? low
Is the search for a goal or for a path? a goal; but the planning version is for a path
Are all operators reversible? no
Are all operators monotonic? no
Letter arithmetic
How big is the search space? perhaps a few hundred states
Form of the search space? partial order
What is its maximum branching ratio? perhaps about 5
What is its depth? perhaps about 100
Proportion of goals? unique goal
Is the search for a goal or for a path? a goal
Are all operators reversible? yes
Are all operators monotonic? yes
2.3.6 Some primitive search algorithms
Be warned: Most of the algorithms described in the next few sections are rarely used in practical systems.
The search algorithms commonly used in the real world are almost invariably the very simplest.
They are instances of a method called hill climbing, which will be explained shortly.
For all that, it is wise to know a little more about search than just that method, so here are a few relatively elaborate algorithms which are also sometimes useful.
The algorithms listed in this section are’ blind’: They cannot take advantage of any prior knowledge of the search space.
They all involve storing several states.
Hence they would be no good for making supper directly, though they could construct a plan for making supper.
Breadth first search
This algorithm works when the objective is to find a goal in the search space, rather than a path to the goal.
It keeps a list, called OPEN, of nodes which the algorithm has found and which are not goals but whose children might be goals.
It also has a variable called N whose value is the current node of interest.
OPEN: = []
N: = start
WHILE N fail and N is not a goal
FOR each child C of N add C to the back of OPEN
IF any such child C is a goal
THEN
N: = C
ELSE
IF OPEN = []
THEN
N: = fail
ELSE
N: = the head of OPEN
OPEN: = the tail of OPEN
When this stops, either N is a goal or else it is fail and the task has no solution.
The algorithm examines every shallow state, which can be constructed by a short path from start, before it examines deeper states.
Hence, if all goals are deep or the branching ratio is large, it will be very slow.
It also takes up a lot of memory for OPEN, remembering all states which might lead to a solution.
The process of constructing all N's children is called expanding N. Note that the order of children in the FOR loop is not specified.
This order will depend on the representation of the task.
Depth first search
This is a variant of breadth first search.
It differs in just one step: Instead of add C to the back of OPEN it reads: add C to the front of OPEN
Depth first search searches all descendants of one state before considering the state's siblings.
If the first state among siblings always leads to a goal, it will be much more efficient than the breadth first version; but otherwise it may be worse.
In fact, if the search space has unlimited depth, then depth first search may never stop whereas breadth first search would succeed.
Depth first search depends critically on the order in which the children C of N are added to the front of OPEN.
Thus it depends on the order of children in the FOR loop:
FOR each child C of N…
It is very useful if, at each node on a path to a goal, the child leading to the goal comes first (or at least, none of the children preceding it leads to a large subtree).
If you can design the task's representation so that this is so, then depth first search is better.
Breadth first search with back pointers
This is the corresponding algorithm which finds a path to a goal.
It differs from naive breadth first search, much as the naive version of the crates problem differs from the real version.
Each node is given a pointer to its parent.
Thus, a node of this algorithm's search space is a pair:(N, p) where
N is a node of the naive task p is a pointer to the parent of (N, p).
The algorithm has a new variable, S, whose value is such a pair.
It goes
OPEN: = []
N: = start
S: = (N, p) where p points to S itself.
WHILE N fail and N is not a goal
FOR each child C of N p: = a new pointer to S add (C, p) to the back of OPEN
IF any such child C is a goal
THEN
N: = C
ELSE
IF OPEN = []
THEN
N: = fail
ELSE
S: = the head of OPEN
N: = the naive node in S
OPEN: = the tail of OPEN
When this stops, either N is fail or else the pointer p in the current S points back along the sequence of operators which leads to N. There is a version of this which corresponds to the basic depth first algorithm.
Backtracking depth first search
This is a modification of depth first search which avoids finding all children of N straight away.
It is more economical with space.
The nodes of its search space are a little more elaborate than the task's basic states.
Each node consists of a state of the original task's search, and a list of operators which can be applied to this state.
Thus, the algorithm searches a space in which each node is a pair:(N, LOp)
In one cycle, the algorithm takes one operator out of the list LOp and applies it to N, so producing just one child.
avoiding duplicate states
This is a simple modification of the basic algorithms.
A node is only added to the OPEN list if it has never been created before.
The algorithm keeps a second list, called CLOSED, of nodes which have been expanded and removed from OPEN.
Whenever a child is created, it is only added to OPEN if it is not already in OPEN or CLOSED.
Back marking
Sometimes a searcher can detect that a large branch of its search space does not contain a goal.
For instance, in the eight queens problem, the position  Q cannot be extended to a goal because no queen can ever be put in the left file.
However, a naive searcher might explore children and  children's children of this state, such as before discovering this fact.
As soon as the back marking algorithm finds a proof that some state N does not lead to a goal, it tests to see if the same proof applies to ancestors A of N. When the same proof works for A, it prunes A and all its other descendants too.
2.3.7 Heuristic functions
Breadth first search employs no knowledge about the task.
Depth first search can employ a little knowledge, if you can represent the search space so that the searcher always tries the best child first.
However, the chances are that, if you can really represent the search space optimally before calling the algorithm, then there is no need for search at all.
You could solve the task without it.
The key to clever search lies in choosing the most likely state in OPEN.
If the algorithm can detect a short path to a goal, and always choose the N from OPEN which is on this path, then it will be efficient.
Heuristic evidence is any guiding information which suggests how to choose a good node N from OPEN.
A heuristic rule may be  helpful , but it may be wrong.
A good heuristic function is a function f on states which tends to be large on nodes on paths to goals.
The rule of thumb is that, whenever you choose a next node, the one you choose has as large a value f(N) as possible.
There are good and bad heuristic functions.
A function is bad if it is large on nodes which lead nowhere.
Part of the art of search lies in designing good heuristic functions.
Examples of heuristic functions:
3a eight queens: number of children
Suppose N is a state of the eight queens problem, and r is any row on the board without a piece in it.
If N can be extended to a goal, then it must be possible to put a piece in row r somewhere.
Say nr is the number of positions in this row where one might put a piece, according to the rules.
Then fa(N) is the smallest value of nr, for all such rows r.
For instance, suppose N1 is the left of the two positions shown below, and N2 is the one on the right.
In them,
Q represents a piece.
* represents a square where a piece cannot be put.
q represents a square where one could put a piece.
The numbers beside rows are the numbers of free positions.
3b eight queens: minimum number of extensions in any row
Suppose
cN is the number of rows in N which already contain a piece;
for each unoccupied row, r, say nr is the number of positions in this row where one might still put a piece.
Let fb(N) = cN * fa(N)
It is easy to estimate the worths of a few significant nodes: fa(N) fb(N)
Node N: start 8 0 any child of start r 5 s 7 any grandchild of r 2 s 24 start any grandparent of s 2 r 36 a goal any parent of a goal 1 49 fb is a better heuristic function than fa because it assigns relatively large values to states near goals.
The values of a heuristic function need not be numbers.
Here is another even better function for the eight queens problem whose values are lists of numbers.
3c eight queens: ordered lists of numbers of vacancies
If N is any state, fc(N) is a list with as many entries as N has empty rows.
The entry in the list corresponding to row r is the number of positions in this row where one could put a piece.
The subtlety is that fc(N) is arranged in increasing order.
Thus fc(N1) = [2,2,3,3]fc(N2) = [1,1,2,3]
If M and N are any two positions, then say fc(M)>fc(N) if either all entries in fc(M) are strictly positive, and fc(M) is shorter than fc(N) or fc(M) and fc(N) have the same length, and the first few entries in fc(M) and fc(N) are equal and the next entry of fc(M) is larger than that of fc(N).
This is a novel use of the symbol’ >’.
Generally, people think of inequality as a test which is performed on numbers.
Here we are following Humpty Dumpty's maxim: A word (or symbol) can mean whatever we want it to mean.
In this case, fc(N1)>fc(N2).
The state N1 is more promising than N2 because for each row in N2, there is a row in N1 with at least as many free positions.
The novel definition of’ >’was chosen to reflect this fact.
Of all three heuristic functions, fc is almost certainly the best, because it contains most information.
Both fb(N) and fa(N) can be derived from fc(N).
2.3.8 Some heuristic search algorithms
Best first search
In this algorithm, the collection called OPEN of untested nodes is a set, not a list.
The algorithm is supplied with a heuristic function, called f, and it chooses nodes from OPEN in order of their f values.
It is designed to find a goal as quickly as possible.
OPEN: = the rest of OPEN (Traditionally, the new node N chosen from OPEN is the one with least f value.
The version given here suits the above examples.)
The A algorithm
This is the heuristic  analogue of uninformed search with back pointers.
Given a few reasonable assumptions about f, it is guaranteed to find a best path from the start to a goal.
It has to extend its search one stage beyond the point where it finds the best path, in order to check that it is the best.
A is based on the assumption that each step in the search has a cost, and the best path is one which costs least.
Say cost (N, C) is the cost of stepping from a node N to one of its children, C. Since the aim is to find a path, the space which the algorithm explores has nodes consisting of pairs
S = (N, p) where N is a naive state, and p is a pointer to another such pair.
Each such node S is at the end of a path from the start, and this path can be traced back from S by following pointers.
The cost of this path is a function g of S. If C is a child of N, and
T = (C, q) where q points to S then g(T) = g(S) + cost (N, C)
The heuristic component of the algorithm is given by some estimate h(N) which is a guess at the cost of a cheapest path from N to a goal.
Thus, h(N) = 0 whenever N is a goal, and h(start) is an estimate of the cost of a cheapest solution.
As well as the set OPEN of known nodes which might lead to goals, and whose children have yet to be explored, the algorithm keeps a set called CLOSED of other known nodes whose children have been constructed.
OPEN: ={}
CLOSED: ={}
N: = start
S: = (start, p) where p points to S itself.
WHILE N fail and N is not a goal add S to CLOSED
FOR each child C of N
IF C is not the naive state of any node in OPEN or CLOSED
THEN add a new node (C, q) to OPEN where q points to S
ELSE{say OPEN u CLOSED contains some state T = (C, q)}
IF g(T) s g(S) + cost (N, C)
THEN do nothing
ELSE discard T
T': = (C, q') where q' points to S put T' on OPEN
IF OPEN{}
THEN
S: = the entry in OPEN, and
N: = the naive state in S, for which g(S) + h(N) is least
ELSE
N: = fail
This algorithm is guaranteed to find a cheapest path to a goal, if
there is a goal somewhere, and a path in the search space from start to it; and
for every naive state N, h(N) is not more than the least cost of any path from N to a goal; and
there is a number k>0 which is less than the cost of any step: cost (N, C) r k for every node N and child C.
Recall the warning which you read above: All these fancy search algorithms which you have just read about are not really much use for algorithmic learning.
They are good educative examples of the search process, but it can be argued that they have little to do with practical intelligence.
These algorithms are most suited to tasks with small branching ratios and reversible operators, and with simple states so that the algorithm can remember several states at a time.
Real life is harder.
Hill climbing
This algorithm is so simple that it is often not taken seriously.
It sets out to find a state N with the greatest possible worth, f(N).
Such a state is treated as a goal.
Hill climbing is so called because it is like what one would do to find the top of a hill in a Scottish mist: Keep going up.
N: = start
WHILE N has a child C for which f(C)>f(N)
N: = the child C of N for which f(C) is largest
The result is the final value of N. Note that the value f(C) is assumed to be the true worth of the state C, at least when C is a goal.
Hill climbing can be adapted to a range of situations.
The other algorithms are all designed as if the children of each state form a discrete set with no structure.
When the set of children of N is infinite, it will not be feasible to calculate f for them all.
It can happen that the children of N form a continuum, C where tnR, and N = C, and f (C) is a smooth function of t.
Then the natural approach is to find the slope f' of f at N. The algorithm's step then becomes
N: = C where k is a suitable positive number if f'>0 k<0 if f'<0
The magnitude of k is arbitrary.
Perhaps it could be large if the second derivative of f is small, and vice versa.
Hill climbing has several limitations.
Anyone who has done much climbing in Scotland will be familiar with the first two.
The foothill problem.
There may be several states N with greater worth than any of their children.
The algorithm may arrive at the wrong one, and miss the true goal with maximum worth.
This error is like climbing below a col, and coming up on the wrong side of it.
The true goal is a summit to the right of the col, but the climber may begin a bit too far to the left and go up the other way.
The plateau problem
A node and its children may all have almost identical f values, so the algorithm lacks sensible guidance.
This is analogous to wandering around in a misty marsh in a river valley surrounded by invisible summits.
The other algorithms presented above do not suffer so seriously from this problem because they can all examine alternative paths for several steps.
The canyon problem
The worth function f may change abruptly.
Any analogy with climbing a gentle hill breaks down.
This is particularly awkward if the states form a continuum and f is smooth and the step length, k, is arbitrary.
Then the algorithm may opt for a particular child C on the basis of the gradient of f at N, but the gradient is misleading and in fact f (C) is much less then f(N).
When each state has only a finite set of children, this won't happen.
The knife edge problem
This is a bit more subtle.
The analogy is that you are a robot walking up a hill, and your designer has built you so that you can only step north, south, east or west.
You find yourself on the crest of a steep ridge running gently north east towards the summit.
For a human, the task of finding the summit would be easy because he could see north east; but the robot can only test the gradient along the principal points of the compass, and in all those four directions it slopes down.
The robot concludes that it is at the summit, and stops.
Another similar case occurs no doubt every evening in thousands of homes.
A television set has three colour controls: red, green and blue.
Twiddling any one of them makes the colour worse; but if all three, or perhaps just two, were adjusted simultaneously then the colour of the picture would improve.
Despite that, there are many cases when hill climbing is the best or perhaps the only practical approach:
when the children of each node form a continuum (see above).
when each state is complex, and so only one state can be remembered at a time.
when any two states have a common descendant goal, so choosing a child which is not optimal does not matter too much.
This often happens in theorem proving.
when the task's operators are irreversible.
In that case, there is no point in remembering alternative branches of the search, because when one has been chosen, the others are no longer attainable.
This last situation is common in real life.
For example,
10 Politics
Once something has been said, it cannot be unsaid.
Hill climbing with planning
The usual strategy in real irreversible searches is the following: Choose some function f of the operators of the search task.
f is meant to be an estimate of the benefit gained by applying an operator.
This f is used only to detect when the algorithm has reached a summit or plateau.
It is not used to choose between alternatives.
To do that, at each step, the algorithm constructs a planning sub-task.
It simulates the bit of the search space which is rooted at the current state, and plans an acceptable next move.
The major limitation of this algorithm is the quality of the simulation in the planning stage.
If not much is known about the properties of operators which could act on N and its children, then the plan may be misleading.
N: = start
WHILE any operator Op applies to N, and f(Op)>0
Set up a subsidiary planning task to simulate a search for a path starting at N, leading to a better state
Explore this planning task until it has found
either a path to some state C which is substantially better than N
or a child C of N such that every path to any better state begins through C
N: = C
In real life, all the clever algorithms with OPEN lists are verging on the irrelevant.
Hill climbing with planning is used almost universally.
Even the planning step is commonly done by pattern matching or some similar technique which does not involve keeping alternatives.
Heuristic rules
So far, we have taken the operators for granted.
In practice, finding which operators apply to some node is often a substantial part of the searching process.
Commonly, each operator is incorporated into a rule.
This is a data structure with a condition an action and maybe a name other details.
The action is the operator, which is applicable to some states N but not to all.
The condition is a predicate which is true of just those states N that the action can be applied to.
Rules of this form are often called’ IF…
THEN…’ rules: If the condition is true of a state, then the action can be applied.
There is a simple searcher which uses such rules.
Say the set of all rules available to the searcher is called KB, for’ knowledge base’.
The searcher goes:
N: = start
TRACE: = []
WHILE N is not a goal and N fail
CS: = conflict-set (N, KB)
IF CS ={}
THEN
N: = fail
ELSE
R: = best-rule (CS, Goal-conditions, N){say R has action Op name Name}add Name to the back of TRACE
N: = do-action (Op, N)
The TRACE is a record of which rules were applied to find a goal.
It is often useful for later learning.
The three major sub-functions are
conflict-set.
This finds which rules have conditions satisfied by N.
best-rule.
This makes a heuristic choice of one rule R out of the conflict set, CS.
(The word’ heuristic’means that the choice is based on incomplete or unreliable information.)
It is this rule's action which generates the child of N for the next cycle.
do-action.
This applies the action Op of the rule R to the current state N, and produces the next state of the search.
The’ other details’in the rule, which were left unspecified above, are to help best-rule to make a good choice of rule.
best-rule can also make its choice depend on the node N itself and on the properties which characterise a goal, Goal-conditions.
The process of choosing a best rule is called conflict resolution.
Many learning systems have a problem solving component of this form.
Learning is relatively easy with such systems, because the searcher's knowledge is accessible in the rules.
Generate and test
There is a particular form of best-rule which runs thus:
Children: ={do-action (Op', N) | E R' n CS (Op' is the action of R')}
C: = the state in Children which most nearly satisfies
Goal-conditions
R: = a rule in CS whose action Op produces C from N.
This method of search, which creates all children of N and opts for a best one, is called generate and test.
It is a sort of hill climbing.
Of course, if the searcher is really written thus, then it creates the chosen next state twice, which is inefficient, so one would never actually write a searcher thus.The optimised code goes:
N: = start
TRACE: = []
WHILE N is not a goal and N fail
CS: = conflict-set (N, KB)
IF CS ={}
THEN
N: = fail
ELSE
Children: ={do-action (Op', N) | E R'nCS (Op' is the action of R')}
N: = the state in Children which most nearly satisfies Goal-conditions
Generate and test only makes sense when we have some means of measuring how nearly each child C satisfies Goal-conditions.
Often, there is such a measure.
Also, this form of search is only possible when the searcher can generate and then retract states, or store several states simultaneously, so it is not suitable with irreversible operators.
2.4 Metric Spaces
A metric space is a set, commonly called X, or perhaps S. The  elements of X are commonly called’ points’, since they are supposed to have no size.
Instead, there is a notion of distance between points.
This distance is a function d: X * X DL R which satisfies the following properties:
For any points x and y in X,
1
d (x, y) r 0
2
d (x, y) = d (y, x)
3
d (x, y) = 0 5 x = y
4
d (x, y) s d (x, z) + d (z, y) for any z in X
The last condition, 4, is called the triangle inequality.
If you draw a triangle with corners x, y and z, then it says that the length of the side between x and y is no more than the sum of lengths of the other two sides.
Example: Normal distance — the Euclidean metric
If you choose a particular position, such as the doorknob on your front door, you can describe any position in your house by saying how far it is a) behind the front door (its x1 coordinate) b) to the right of the line perpendicular to the front door (its x2 coordinate) c) above the doorknob.
Thus you can describe it by the three numbers (x1, x2, x3)
The set X is the set of all positions in your house.
If (y1, y2, y3) is a second such point, then the Euclidean distance between x and y is d (x, y) = r(x1-y1) + (x2-y2) + (x3-y3)
This function d is just the usual distance between any two places.
It satisfies the five conditions on a metric.
Example: Manhattan distance, in the eight tiles problem
This is a metric on the space of states in the game studied above, with eight tiles which have to be moved to the goal position:
Consider the positions 5 3 6 7 1 6 5 3 6
If tile 4 were out of the way, as in c, the position b could be obtained from a by sliding the tiles 7, 8, 3, 5, and 1 in a loop.
If they are moved in that order then it would take twelve operations to move from a to b.
The Manhattan distance Md (a, b) is 12.
In general, for any state x, the position of each tile t can be described by its horizontal and vertical coordinates:(t, t).
Thus in a the coordinates of tile 5 are (1,3), while in b, 5 is at coordinates (2,1).
The Manhattan distance Md (x, y) between any two states x and y is s
It is the number of operations needed to move from x to y if no tile got in the way of any other.
(You should check that this function Md really is a metric.)
Example: Venn diagram metric
Take any set S, and let X be the set of finite subsets of S; so a point in X is a finite set{s, s, s,…s}of elements of S. The number of elements in x is sometimes written |x|
In this case, it is the number N.
Say y is another such finite subset.
The Venn diagram distance between x and y is the number of elements of S which are in either x or y, but not in both:
Vd (x, y) = |x| + |y| — 2 * |x o y|
See Figure 10.
We should check that this really is a metric.
You can convince yourself that it satisfies the conditions 1…4.
To check the triangle inequality, we need to know that
Vd (x, z) + Vd (z, y)— Vd (x, y) is not less than 0.
This is most easily seen by drawing a picture like Figure 11.
The expression above is twice the number of points which are a) in z, but not in x or y or b) in x and y, but not in z.
Thus, it cannot be negative.
Example: The Hamming distance
This is a metric between N-tuples of 0s and 1s.
Here, N is a number.
An N-tuple is a sequence of N digits (0 1 1 1 0 0 1 0 0 0 0 1…1)
The set X is the set of all such sequences.
There are 2 points in X. If N is 3, then you can think of a cube, drawn in the corner of the three axes of Cartesian geometry.
A point is a corner of this cube.
The Hamming distance between two tuples
Hd (x, y) is the number of positions where the two tuples have different entries.
For example, if x = (0 1 1 0 0 0 1 0 1 1) y = (0 1 0 0 1 0 1 1 0 1) * * * * then
Hd (x, y) = 4
Again, you should check that this function Hd satisfies the five conditions of a metric.
We shall use the Venn diagram metric and the Hamming distance later.
2.5 Experimenting with chance
There is very little that we can be absolutely certain about.
Descartes said
I think, therefore I am.
The point of this statement is not that it is true.
It is just about the only fact which is unquestionably true.
Anything else, for instance the’ fact’that Socrates talked with Plato, depends (from Socrates' point of view) on the assumption that there really was someone called Plato who often visited him.
Socrates might have dreamed about Plato.
For all we in the 20th. century know, Plato may have dreamed all the conversations with Socrates.
You may have dreamed all the’ experiences’in which you ever heard mention of Socrates and Plato.
If you find this hard to swallow, try to work out what is reality in the more fanciful novels of Daphne du Maurier.
Ontology is the branch of philosophy concerned with existence: How can you be certain that anything exists, apart from you yourself?
What does it mean to say that something exists?
It is an aspect of metaphysics, which is the study of what we are justified in believing from first principles.
By its nature, metaphysics almost never provides us with certainties; so rather than bother with it, most people’ believe’what to them seems most plausible.
Since Plato (dreamed that he) saw Socrates most days, and this was among the most vivid and consistent of all his experiences (dreams?), and since Socrates said things which he, Plato, had not thought of before and which sometimes surprised him, Plato found it most consistent and convenient to’ believe’that Socrates really did exist.
Probability is all about justifying beliefs.
Our belief in Mendel's theory of genetics is based on experiments, with fruit flies and similar organisms, whose interpretation depends on probabilities.
Almost all learning is similarly based.
We observe a set of events, and decide that they have one explanation which is vastly more probable than all others; so we come to believe that this explanation is correct, and we use it to make predictions in future.
When you have seen an egg broken into a frying pan of hot fat so that it turns from a clear fluid into a rubbery opaque solid, you are inclined to believe that this is quite a good way to cook an egg.
When you have seen it done twice, and furthermore your mother does it with complete confidence and expects you to eat the result, then you are sure.
You have learned about frying eggs.
You simply do not admit the possibility that your mother is being whimsical, and that these two eggs were exceptional.
It would be just too unlikely.
However, if you were completely rational, you would admit the possibility — and your mother would go frantic.
Let us analyse the fruit flies experiment more carefully.
Some results fit the prediction very well (e.g. if the numbers of offspring with red eyes is just three times the number with white eyes); some fit it quite well (e.g. if 61 have red eyes and 23 have white eyes), and some would fit it very badly (e.g. if all but one had white eyes).
To make the numbers a bit easier, let us suppose that there are just three offspring.
If n have red eyes, then (assuming Mendel's model of inheritance) we can tabulate how many ways this can happen: n no. of ways this can happen 
However, these are not really the numbers which matter.
What we should consider are all the alternative ways that a fruit fly can inherit individual switch settings.
There are four possible outcomes: It gains red from the mother, and red from the father; red from the mother, and white from the father; white from the mother, and red from the father; white from the mother, and white from the father.
According to Mendel's theory, all these situations are equally likely.
Let us tabulate how many ways each can occur, among three offspring: rr rw wr ww no. of ways this can happen 
DDDD total: 64
Out of these 64 possible outcomes, the 27 marked’+ ’ have just one fruit fly with both switches set to’white’.
These are the cases which lead to two offspring with red eyes and one with white.
Suppose the student actually found that 2 have red eyes and 1 has white.
According to the theory, the chance of this result is (27/64).
The idea of confidence runs thus: Rather than look at the probability of getting this precise result, we find the probability of getting a less likely observed result.
The probabilities of all possible observable results are: no. with red eyes probability 
The chance of getting a less likely result is 10/64, or about 15%.
This is reasonable.
In a typical experiment, the chance of getting a less likely result is no more than 50%.
This number, the chance of getting a less likely result, is called the backing which the experiment offers the theory.
If an experiment's possible outcomes are O1, O2, O3,…and a theory predicts that each outcome Oj may occur with probability Pj, and the outcome actually observed is On, then this observation gives the theory a backing of Pj
By various tricks which save us from the full load of naive combinatorics, one can show that the student's original result (61 with red eyes, 23 with white) gives Mendel's explanation a backing of nearly 100%; so the professor was right.
In fact, the calculation above hides a significant assumption: We have taken it for granted that there are precisely 3 offspring.
If you allow that there might be any number of offspring, the backing is not quite this value.
2.6 Complexity of descriptions
The student's experiment provides good backing for Mendel's theory.
The questions arise: Does the same experiment provide equally good backing for any other theory?
If so, why do we opt for Mendel's?
The answer to the first is, Yes.
There are lots of theories (e.g. that fruit flies have a  probability of 61/84 of inheriting red eyes rather than white).
The reason for opting for Mendel's is that it not only has good experimental backing.
It is also simple.
This simplicity can be quantified (in principle).
It is done thus.
We choose once and for all some language for describing theories.
English might do, if you can decide just what is English; but theoreticians prefer something more precise.
Each candidate theory is described in this language.
We then measure the complexity of each description, and choose the one with adequate experimental backing and the simplest description.
Solomonoff and Kolmogorov independently invented a systematic way of measuring complexity.
Choose any programming language.
If D is a description of some theory, then observe that D is a string of characters.
Its complexity
K(D) is the length of any shortest program which prints D and then stops.
Kolmogorov showed that, if you choose a different programming language, the difference in the values K(D) is less than some constant which depends on the languages but not on D. More details will appear in Chapter 8.
Even this is an unduly myopic attitude.
We do not just consider Mendel's theory in isolation.
Science is a unity, and the test of simplicity should really be applied to the whole of science.
We do not believe that fruit flies have a chance of 61/84 of red eyes because on its own this is a silly idea; but also, it does not fit with the rest of our accumulated knowledge.
Mendel's idea fits.
Further reading
Many relevant techniques are described in detail in the compendium edited by Bibel & Jorrand (1986).
There are plenty of good books on the propositional and predicate calculi.
If these topics are new to you and you feel the urge to investigate them, try the ones by Binmore (1980) and Hodges (1977).
If you are more adventurous, look at Johnstone (1987) or Mendelson (1987).
There are several good books on graph theory too.
However, the kind of graph theory which mathematicians investigate is almost totally irrelevant for computing, and even the basic definitions which computing people use are not always the standard graph theoretic ones.
The proper approach appears in Bollobas (1979) and Wilson (1972/85) and various other references.
Rich & Knight (1991) present a good introduction to the theory of search, including a discussion of the various features characterising search spaces.
For a thorough introduction to the subject of search algorithms, you can study the books by Nils J. Nilsson (1980) or Judea Pearl (1984).
Experimental backing is an aspect of probability theory and statistics.
For more details, see any one of many good books, such as the one by N C Barford (1967).
The original references on complexity are Kolmogorov (1965); Solomonoff (1964, 1978).
More references and a discussion of Kolmogorov's contribution appear in Razborov (1990).
Applications to machine learning are discussed at greater length by Li and Vitsnyi (1993).
LEARNING ALGORITHMS WITH NUMERIC INPUT
Summary
This chapter describes learning algorithms which handle individual real numbers.
The particular examples discussed are
control theory for differential equations
remembering and extrapolating
adjusting heuristic weights, for conflict resolution and also for evaluating a position during game playing
discovery of algebraic relations between numeric parameters.
In the process, we shall discover an instance of a phenomenon called
the credit assignment problem and we shall see that the classification scheme set out in Chapter 1 does not always work perfectly.
Recall, we shall classify algorithms by their forms of input.
The first class consists of those which handle real numbers.
Real numbers are both very versatile and very simple, so it is not surprising that learning processes use them in many different ways.
Very broadly, the algorithms summarised here fall into three groups: those that optimise a parameter; those that record numeric observations and extrapolate from them; and those that use real numbers as weights, to choose between alternatives.
Weights also occur in many neural nets, which will be described in Chapter 4.
The essential difference between the weights in neural nets and the numbers used here is that, in a neural net, each datum conveys information by virtue of its position among other similar data, whereas each number in the algorithms of this chapter conveys a distinct piece of information.
3.1 Optimising methods
The very simplest example of this kind is the binary chopper, presented in Chapter 1, which finds a root of a polynomial.
Some of the very earliest’ clever’programs were elaborations of this method.
Chemical engineering companies found that their very large complex expensive process plants could be controlled better by computer than by human operators.
Typically, such a plant's behaviour can be described by a differential equation which depends on coefficients.
The optimum values of these coefficients characterise the plant.
When engineers build the plant and install the computer control, they do not know the precise values of these coefficients; so they start the process running with approximate values, and write control programs which improve the values.
This is a simple form of learning.
If the plant's behaviour depends continuously on a single coefficient, then binary chopping is a safe standard approach.
If it is known that behaviour depends smoothly, and the second derivative of the dependence is never zero and its first derivative is bounded away from zero, then the Newton-Raphson method is better.
More commonly, the plant can only be characterised by several coefficients which must all be optimized simultaneously.
The science of adjusting these coefficients goes by the name of control theory.
3.2 Observing and Extrapolating
The crudest form of learning imaginable consists of just remembering.
The learner notes what has happened in the past, and assumes that in like circumstances, the same will happen in future.
This algorithm, if it deserves the title, becomes valuable if it is used together with one simple assumption: We assume that in almost identical circumstances, almost the same will happen again.
In jargon, we say that effects depend on causes continuously.
It will take us even further if we assume that the dependence of effects on causes is smooth.
Example: The birth of a robot
A new robot is screwed down onto its workbench.
It has a single arm with seven joints in it.
Three joints let it move its gripper to any point in 3-dimensional space; and then two more let it point its gripper in any direction; the sixth lets it twist its gripper round the gripper's axis; and the last opens and closes the gripper.
Each joint can move through an angle within fixed limits, say up to 180 degrees.
As yet, the robot has no way of knowing whereabouts its gripper will be, when its joints are set at particular angles.
Even more to the point, how can it move its gripper to a given position?
What joint angles will get it there?
Two ordinary TV cameras are fixed so that they both scan the robot's work area, from different perspectives.
The robot holds a small torch in its gripper.
Thus, the gripper's position can be chosen by selecting settings for joint angles, and measured by reading the torch's images on the TV pictures.
Any position in the robot's reach can be described by its two images in the cameras' fields of view.
We can specify a point by saying what its coordinates are in the first field of view
say (x, y)— and its coordinates in the second —(x, y).
Of course the field of view is 3-dimensional, so in principle we need only state three of these four coordinates, but there is no point in discarding information and so we may as well keep them all.
Problem solving: Given a point in the robot's work area, what settings for its joint angles will move the gripper to this point?
The point is described by the four image coordinates (x, y, x, y), called I. A person can provide I by holding a torch at the desired point.
The problem solver should find a corresponding 7-tuple of joint angles,(j, j, j, j, j, j, j) which we shall call J.
Learning: Given a set of solutions to past problems, find information which will help the problem solver in future.
Observe that this task can be solved by just altering the first three joints.
There is no need to twist the gripper or alter the direction the torch points in, and the gripper should most certainly not be opened.
Hence j, j, j and j are redundant.
Simple method, depending on continuity: The robot keeps a set, called R, of all the solutions it has found.
Each entry in R is a pair (I, J) of a point's image coordinates on the TV pictures and of joint settings which take the gripper to this point.
Problem solving: Given some image coordinates I, the robot looks up in R for the pair (I', J') with I' closest to I. It moves all joint angles to J'.
If this leaves the gripper too far from the desired point, the robot tries adjusting each joint a little in turn to see which adjustment moves the torch's image most quickly towards the point I. It selects the most significant joint and keeps moving this one until the torch's image is as close as it can get to I; then it tries adjusting another joint, and so on, until the gripper is close enough.
This process of fine adjustment is a form of hill climbing.
Learning: This consists just of adding a new pair (I, J) to the set R every time the robot moves to a new position.
Elaborate solution, depending on smoothness: This keeps the same set R as the simple solution.
It also keeps a similar set R2, explained below.
Problem solving: This is almost exactly as in the simple solution.
The only difference is that the problem solver searches in the combined set R u R2 for a pair (I', J') with I' closest to I.
Learning: In this second approach, learning takes three forms.
a
As before, whenever the robot moves its gripper to a new position, it records the joint settings J and the torch's image coordinates I as a new entry (I, J) in its set R.
b
In order to gain reasonable competence by the simple approach, the robot has to record more than 1000 entries in the set R. This takes a long time.
To overcome the delay, the robot's software’ dreams’approximate solution pairs (I', J') and uses them like observed entries.
It finds these approximate solutions by linear interpolation from the first few genuine observations.
The interpolated points are recorded in the set R2.
The details appear below.
c
The’ dreamed’entries in R2 are inevitably a bit inaccurate.
When there is time, the robot's joints are switched on again, and for each entry (I', J') in R2, the robot's arm angles are set to J'.
Then the cameras record the torch's true image coordinates I, and the true observed pair (I, J') is added to the set R. The’ dreamed’entry (I', J') is removed from R2.
Here are the details of the’ dreaming’stage.
After the robot has accumulated several observations (I, J) in R, the joint motors are switched off and the software chooses new joint angles J' at random, or perhaps it chooses J' in a poorly explored region.
The software finds the four entries (I, J)(I, J)(I, J)(I, J) in R for which the joint settings J are closest to J'.
(In practice, it may be best to choose the four settings J first, and then let J' be any point between them.)
Unless the points J,…
J all happen by accident to be linearly dependent, there will be four numbers w, w, w, w for which
J' = w J + w J + w J + w J and w + w + w + w = 1
These relations define the numbers w uniquely.
(We choose four points J because the space of Js is 3-dimensional.
The J s form the corners of a tetrahedron.)
The dreamed approximate solution I' is the point w I + w I + w I + w I
This 4-tuple I' of coordinates in the TV images may not actually correspond to any real point in the 3-dimensional space of possible positions for the torch, but that does not matter.
Clocksin and Moore, who performed this experiment, found that the robot could indeed learn its way around its workspace in a fairly short time (between a quarter and half an hour).
The details of their experiment were a little more subtle.
The pairs (I, J) were not stored as a simple list, but as a k-D tree, so that search for closest points is faster.
Moore says that the cameras were accidentally jogged a little during their tests, without much effect.
The software included a second k-D matrix which recorded errors in the first matrix.
An error found at one position was assumed to apply over a large region of the workspace.
The old alternative approach to robot control involved some very complicated equations describing the kinematics.
These equations are hard to solve, and even if one can find a solution, it may be inaccurate.
The Clocksin-Moore approach to problem solving and learning has nothing to do with a robot or visual images.
The software behind the robot need not embody any knowledge of the robot at all.
It is basically a way to associate tuples of real numbers
I JL J
Strengths of the method:
It is very general.
It will work for any robot with any number of joints.
It is also independent of the positions of the TV cameras.
The same logic will work in any situation where one set of real parameters depends smoothly on another set.
It uses all the available information from experiments.
It does not depend on any description of the robot's structure.
Such descriptions are complex, and only approximate.
Learning is fast enough for realistic applications.
Performance is also fast enough.
Learning is incremental.
The algorithm implemented with this particular robot could re-learn after its TV cameras were jogged.
Thus, obsolete learned information can be forgotten or over-ridden.
Limitations
There may be more than one joint setting which brings the gripper to a desired point.
The relation
I JL J is one-to-many.
The algorithm may not find the’ best’answer.
Clocksin and Moore chose joint angles which were most’ comfortable’for their robot.
3.3 Controlling Search With Weights
Optimization of a few parameters is not what people think of, when they speak of learning.
However, simple adjustments to numbers can produce worthwhile learning behaviour.
A prime example occurs during search with heuristic rules, in the choice of a rule from a conflict set.
Recall the version of hill climbing which depends on rules, in Chapter 2.
Suppose we use it to decide how to travel from home to somewhere.
The possible operations are walk; bicycle; drive by car; take a train; fly.
Taking the train is the pleasantest method, so we invent a rule:
IF the destination is near a station
THEN go by train
NAME train rule
WEIGHT 10
The last entry, the rule's weight, should be large for reliable rules and small for unreliable ones.
It often happens that the initial choice of weights is poor, and the program should learn improved weights.
An extremely simple algorithm which does this runs thus.
Suppose there are altogether N rules.
Start with given weights.
Search with these weights.
IF the search reaches a goal by a path of length k
THEN add 1/k to the weight of each rule whose name appears in the Trace; subtract 1/ (N-k) from the weight of every other rule
ELSE subtract 1/k from the weight of each rule whose name appears in the Trace; add 1/ (N-k) to the weight of every other rule.
Strengths of this algorithm:
It is simple.
If it is run twice on similar data, it will probably work better the second time.
Limitations:
It only learns at the end of a search.
A truly incremental algorithm would improve its performance during a single search, so that information learned early can be used later in the same search.
Weights will grow indefinitely.
It reduces the weights of rules which may never have occurred in any conflict set.
Some of these rules may be good in other situations, and deserve high weights.
If they are not employed, then their weights will fall too low.
The last flaw is an instance of the credit assignment problem.
Although a search may be successful, we do not know which key rule led to its success.
There may be just one rule whose weight deserves to be raised, but this algorithm raises all weights of rules in the trace indiscriminately, and  suppresses all others.
Similarly, if the search fails, this algorithm penalises all the chosen rules, when perhaps only one is flawed.
We do not know which aspect of the search was significant.
3.3.1 The Bucket Brigade Algorithm
This algorithm is an alternative way of learning weights.
It is incremental, and it only credits and penalises rules which were actually involved in the search.
It gets its name from an analogy with a chain of firefighters passing buckets of water.
Each time a bucket is passed from hand to hand, a little water is spilt.
The algorithm passes credit between rules, similarly.
Each time credit is passed from rule to rule, the rule which passes it keeps back a little for itself.
It works as follows.
We fix, once and for all, two numbers: C, the value of a reward, which is any positive number, and K which is between 0 and 1.
Any rule chosen from the conflict set has its weight reduced by the factor K. If it is the last rule of a successful search, so its action leads immediately to a goal, then the reward C is added to its weight.
Any other rule used during search is rewarded by the amount of weight which the next rule loses.
Thus, suppose a search reaches a goal in three steps, by rules R1, R2 and R3.
To start with, their weights are W1, W2 and W3.
After the search,
R1 will have weight (1-K) *W1 + K*W2
R2 will have weight (1-K) *W2 + K*W3
R3 will have weight (1-K) *W3 + C.
In a similar search which fails, R1 and R2 will end up with the same final weights, but R3 is not rewarded:
R1 will have weight (1-K) *W1 + K*W2
R2 will have weight (1-K) *W2 + K*W3
R3 will have weight (1-K) *W3.
Usually, all rules begin with the same or nearly equal weights.
After one search, all have nearly unchanged weights except the last rule, R3.
After two such searches along the same path, R2 will have a substantially changed weight.
The three rules will then have weights:
R1 will have weight (1-K) *W1 + 2(1-K)*K*W2 + K *W3
R2 will have weight (1-K) *W2 + 2(1-K)*K*W3 + K*C
R3 will have weight (1-K) *W3 + (1-K) *C + C.
The code of the hill climbing rule-based searcher can be modified easily, so that it incorporates this form of learning.
In this version of the algorithm, we shall forget about names and the trace.
The learner is incremental.
In fact, its code is incorporated into the searcher, so that the two are rather hard to tell apart.
The algorithm includes an extra variable called LR, for’ Last Rule’, which is set to the rule which generated the current state N.
N: = start
LR: = a dummy value
WHILE N is not a goal and N fail
CS: = the conflict set
IF CS ={}
THEN
N: = fail
ELSE
R: = a rule in CS with greatest weight
Weight of LR: =
Weight of LR + K * Weight of R
Weight of R: = (1-K) * Weight of R
LR: = R
N: = Action of R (N)
IF N is a goal
THEN
Weight of LR: = Weight of LR + C
Strengths of the Bucket Brigade algorithm:
It is incremental, even to the extent that learning can take place during a search.
It does not require any memory.
Past examples are not saved.
In fact, it does not even require memory for the trace.
Some learning algorithms do.
It accepts noisy data.
If one search misleads it and sets the weights a bit wrong, then later learning efforts can correct the mistake.
The weights remain bounded.
Limitations:
It is slow.
If a search path to a goal takes n steps, then the first rule on the path will gain no credit until the nth traversal of the path.
It is even slower at learning from failures at search.
A poor rule will not be penalised until the Nth attempt at search, where N is the number of states in the entire search subspace accessible through this rule.
Until then, the poor rule may even gain weight.
Even when the first rule eventually gains credit, the amount of credit it gains is only
K * C where n is the depth of the goal.
This is not much, if n is large.
It is sensitive to the order of examples.
It never stops learning.
It is only good for learning weights, and weights are not always a reliable way to resolve conflicts.
If a rule is good in some circumstances and bad in others, it will sometimes be missed when it should be used if its weight is low; but it may be mis-applied if its weight is high.
What we really want is an algorithm which learns improved conditions for a rule.
We shall meet some such algorithms, but not yet.
3.3.2 The Hierarchical Bucket Brigade Algorithm
This next version of the bucket brigade algorithm seems to be the best way to solve the credit assignment problem when conflicts between rules are resolved with simple weights.
Several flaws in the simple bucket brigade algorithm arise only if the search path is long.
When paths are short, every rule on the path will gain an improved weight after just a few searches, and for a good rule, the gain in weight will be significant even if the rule is first on the path.
The hierarchical algorithm is designed to keep all paths short.
The idea is this.
The basic search algorithm is as before, except that actions are of two sorts.
Basic actions are the primitive operations which change state in the search space.
Hierarchical actions are themselves search problems, which can be solved just like the main search.
Whenever the search selects a rule whose action is hierarchical, it sets up a new search problem, and solves it by calling search recursively.
When an action is hierarchical, of course its rule R1 will gain weight
K * W2 from the following rule R2, just as before.
The trick is, this same gain in weight is passed as the credit C to the recursive call of search.
Thus, at each lower level of the hierarchy, the last applied rule also gains credit proportional to the weight of R2.
Example:
Let us suppose that
the whole problem is solved by rules R1 and R2, which both have hierarchical actions;
R1 has constituents R1.1 and R1.2, which are both basic;
R2 has constituents R2.1 and R2.2;
R2.2 is basic;
R2.1 is hierarchical, with constituents R2.1.1 and R2.1.2 which are both basic.
To start with, all rules have weight 16.
To make the numbers simple, suppose that C = 8 and K is a quarter.
After successive successful searches, the various rules' weights become
R1: 16 R2: 20
R1.1: 16 R1.2: 16 R2.1: 16 R2.2: 20 R2.1.1: 16 R2.1.2: 16
R1: 17 R2: 23
R1.1: 16 R1.2: 17 R2.1: 17 R2.2: 23 R2.1.1: 16 R2.1.2: 17
R1: 18.5 R2: 25.25
R1.1: 16.25 R1.2: 18.5 R2.1: 18.5 R2.2: 25.25
R2.1.1: 16.25 R2.1.2: 18.5
Thus, every rule gains credit after at most three searches.
If searching was done with just the basic actions, then R1.1 would only gain its first bit of credit after the fifth search, and then it would gain a mere 1/64th. of a point of credit.
If we are cunning, we can design the hierarchy so that every path at every level is just two or three steps long.
Then at each level, credit is passed back after just a few attempts at search.
Suppose each path is just two steps long, and there are D levels in the hierarchy; then every rule will have gained credit after (D+1) successful searches.
By contrast, there are 2 basic operations at the lowest level; so if instead all the basic operations were applied directly, at a single level, the first of them would only gain credit during the 2 th successful search.
What is more, in the hierarchical algorithm, early rules gain much more credit.
A rule whose action is basic will appear just like the rules above for the straightforward version.
A rule with hierarchical action will have a form:
IF Conditions
THEN SubgoalConds
NAME Name
WEIGHT Weight where SubgoalConds is a condition describing the goal of the subtask which this rule sets up.
This hierarchical version can be encoded quite easily.
All the essential work is done by a function called hbba whose value is the goal, if the algorithm finds one, or else false.
The other major difference from the straightforward version is that the variable LR now stores a list of all rules which helped to set up the current state.
The rule at the back of this list will have a basic action.
All the others before it will have hierarchical actions, each setting up the subsidiary search which was solved by the following rule.
hbba accepts as inputs
the current state, N
a specification of the current (sub) goal and answers
the goal, if it finds a sequence of rules which produce a goal fail, otherwise.
It has a subsidiary output,
the list LR of rules which  should gain credit C.
Top level:
N: = hbba (start, GoalConds, LR)
IF N fail
THEN
FOR each R in LR
Weight of R: = Weight of R + C
RETURN N
Recursive function hbba (CurrentNode, Goal/SubgoalConds, LR):
N: = CurrentNode
LR: = []
WHILE N does not satisfy Goal/SubgoalConds and N fail
CS: = the conflict set
IF CS ={}
THEN
N: = fail
ELSE
R: = a rule in CS with highest weight
FOR each rule R' in LR
Weight of R': =
Weight of R' + K * Weight of R
Weight of R: = (1-K) * Weight of R
IF Action of R is basic
THEN
LR: = [R]
N: = Action of R on N
ELSE
LR: = [R|SLR]
N: = hbba (N, SubgoalConds of R, SLR) hbba: = N
3.4 Board Games
One can play board games with rules, rather like the rules used for searching.
Such a rule is called a weighted pattern.
It takes the form
IF a part of the board position matches a pattern P
THEN credit this position with weight W
The weights W may be positive or negative.
The key to analysis of a position is what is called its static value.
The static value of a position is found by a) finding all ways in which all patterns can match the board; b) adding the weights.
Each pattern's weight W is added once for each occurrence of the pattern P on the board.
The form of a pattern depends on the particular game.
In Os & Xs, a pattern may contain
some squares filled with O
some squares filled with X
some squares which are empty.
A pattern need not cover the whole board; thus in Os & Xs, a pattern may cover just a row, or just a diagonal.
In a more complicated game like chess, a square in a pattern may be labelled as just’ filled’, or’filled with a white piece’, rather than stipulating precisely which piece.
Also, some squares in the pattern may only match a square on the edge of the board.
Example: Playing Go by pattern matching
There is another board game called Go which has been played in China for millennia.
The old Chinese army used it to train their generals in strategy.
The objective is to win empty space.
You have won a bit of space if your opponent cannot play into it without eventually being captured.
The rules are beautifully simple, but its branching ratio is 300 or more and it is more subtle than chess.
In this game, opposing groups attack each other by surrounding and touching.
The board can be represented in two ways: the low level of board positions and pieces, and the higher level of groups.
This higher level is a graph.
Each group is portrayed by a node, which is labelled with the group's essential characteristics, such as its size and how much free space it has round it.
Between any two interacting groups, there is an arc which is labelled with the degree of interaction.
In addition, an area of free space may also be represented by a node, with arcs from it to the groups surrounding it.
Since the branching ratio is so great, search at the lower level is a poor playing strategy.
An alternative is to choose a’ sensitive region’at the higher level, and then only explore low level moves there.
A player can find sensitive centres by pattern matching in the high level graph.
Suppose a program contains such patterns.
Assume we are at a board position called MyPos, and it is the program's turn to play.
We adopt a standard strategy called two-ply minimax search.
Nilsson's book describes it.
The program considers every possible move which the laws of the game allow it to make, and arrives at various positions which we may call HisPos.
Then, for each such move, it considers each possible move that the opponent might make, leading to positions called MyNewPos.
(In practice, there are ways of avoiding calculation of all possibilities.)
The program calculates the static value of each position MyNewPos.
From these, it estimates a backed up value for each HisPos:
The backed up value of HisPos is the minimum of all static values of positions MyNewPos derived from HisPos.
The move which the program chooses is the one which leads to a position HisPos with maximum backed up value.
As an exercise, try writing some such weighted patterns for Os & Xs, and find out what moves they will lead the minimax algorithm to choose.
3.4.1 Learning by Deeper Search
That is the playing part.
Now we come to the learning algorithm.
As with the bucket brigade algorithm, we want to learn a better set of weights for the patterns.
The principle is this.
We assume to start with that the weighted patterns provide a rough guide to playing the game.
If the original patterns and weights are no good, then this algorithm cannot learn.
The program can estimate the value of the position MyPos in two ways.
The first estimate is its static value, Vs.
The second value we can assign to it is the backed up value of the position HisPos which the program chooses to move to.
This is the double backed up value, Vdb, of MyPos.
One would expect that Vdb is a better estimate of the true worth of MyPos.
The static value Vs depends solely on the patterns, while Vdb exploits both the patterns and some search.
The search is likely to reveal more about the future course of the game than the patterns alone.
The algorithm runs thus: IF Vs>Vdb
THEN the static value of MyPos is probably too high, so decrease the weight of each pattern which matches MyPos.
IF Vs<Vdb
THEN increase the weight of each pattern which matches MyPos.
This algorithm is applied every time it is the program's turn to move.
It is extremely crude.
As always, there is a credit assignment problem.
At any one position, there may well be a key pattern which captures the critical features of play — but we have no way of finding it.
For lack of a better alternative, all patterns matching the position are given equal credit.
The amount of the increase or decrease is a matter of preference.
It might be proportional to (Vs-Vdb).
3.4.2 Learning by Watching the Opponent
Needless to say, there are better learning algorithms for weights of patterns.
One such is based on the following idea.
If the opponent is a weak player and the program is strong, then the opponent's choice of move will usually be worse than the one leading from HisPos to MyNewPos.
The program's static estimate of MyNewPos will be nearly right, and the program can ignore the opponent's play.
However, if the opponent is strong and the program is weak, the opponent may realise that his best move from HisPos is not the one leading to MyNewPos.
Suppose instead, he chooses to move to position CorrectNewPos.
The program will then calculate Vdb for CorrectNewPos, and discover that the opponent's evaluation of it is more accurate than its original static value.
If
Vdb(CorrectNewPos) < Vs(MyNewPos) then the program made a mistake.
Both Vs(MyNewPos) and Vs(CorrectNewPos) should be corrected.
The algorithm is:
After each of the opponent's moves,
IF CorrectNewPos is not MyNewPos
AND Vdb(CorrectNewPos) < Vs(MyNewPos)
THEN increase the weight of each pattern which matches MyNewPos; decrease the weight of each pattern which matches CorrectNewPos.
Of course, learning weights on their own is not enough.
A skilled learner will invent new patterns as well.
That is beyond the scope of this section.
3.5 Discovering Physical Laws: Where The Classification Is Flawed
The last example in this section is of a program called Bacon.
Our classification system suggests that it should be here, because its input is numeric, but it does not fit into the optimizing paradigm.
The reason is that, although its input is numeric, its output is a formula which contains discrete whole numbers.
Hence, the program's search space is an array with integers for subscripts.
Bacon's search involves discrete steps in a finite set of directions, rather than steps through a continuum.
Bacon is intended to find’ laws’of nature from the sort of quantitative experimental data which were available to the people who first discovered these laws.
One typical law is Kepler's relation between the period P of a planet's motion around the sun, and the radius R of its orbit:
P = c * R or
P * R = c where c is a constant for all planets in the solar system.
Given data on individual planets, Bacon finds the constant c and the powers, 2 and -3, of P and R.
Bacon searches through the space of such formulae.
The more interesting aspect of its search is the part which finds the whole numbers, the powers of P and R. During the search, Bacon constructs intermediate approximations to the right powers, all of the form i j where i and j are whole numbers.
In a grossly simplified form, the essential rules are
IF a known quantity A is constant in all cases
THEN conjecture a law:
A = c and stop.
IF two known quantities A and B are linearly related
THEN conjecture a law:
A = k * B + c and stop.
IF two known quantities A and B increase together
THEN invent a new quantity
C = A /B
IF one known quantity A increases while another B decreases THEN invent a new quantity
C = A * B
If the input is the (fictitious) data for three planets:
P 1 8 27
R 0.3 1.2 2.7 then the program's steps will generate
A 3.333 6.666 10 (P /R)
B 11.11 5.555 3.704 (A /R)
C 37.04 37.04 37.04 (A * B) and declare the law:
P/R * (P/R) /R = 37.04 in a suitably tidied form.
Even the first version of Bacon is subtler than this summary suggests.
It contains sixteen basic rules, which include some for overall control and some which collect data.
It avoids re-inventing a quantity.
The entire system is goal-directed, although the search through the space of formulae is forward.
Conflicts are resolved by choosing a rule instance which refers to the most recently created quantity.
This leads to a depth first backtracking search.
In its later versions, Bacon is much cleverer than this.
It could
invent dependent parameters.
If given three or more parameters as input, it can hold all but two constant and discover a law relating these two; then treat the constants (c, k) within this law as new parameters, and relate them to the other inputs.
This involves a recursive hierarchical search.
exploit expectations.
In the version with dependent parameters, once Bacon has found a form of law relating two inputs while others have particular settings, it will expect that these two are always related by a law of the same form in which only the values of c and k vary.
invent intrinsic properties.
In the example above, the constant 37.04 depends on the mass of the sun.
If we could observe several different solar systems around other stars, the resulting constants would vary.
Later versions of Bacon can recognise that each star has an intrinsic property which manifests itself as this’ constant’.
perform what physicists call dimensional analysis, and what programmers call type checking.
Two quantities can only be equal if they contain the same powers of space, time and mass.
In addition, some versions could find periodicity and other forms of regularity.
Bacon is unusual because its input and output are both from spaces with exceptional structure.
The input space is given by real numbers.
The output is a cross between real numbers (the constants c and k) and integers (powers of P and R).
There are very few published accounts of learning programs which search in spaces parametrised by integers.
Bacon could be discussed here, since its input is numeric, or in the section on correlation and clustering since it searches for correlations.
Perhaps it is best viewed as a correlator.
It is described here so that you can see as soon as possible: A learning program may have many facets, and classification is fraught with subtlety.
Further reading
Siddall (1982) describes in detail several methods of control theory, with examples and implementations in Fortran.
The experiment with a robot, on learning by remembering, was performed by Bill Clocksin and Andrew Moore, who describe it in their joint paper (1989).
Other details appear in Moore's thesis,(1990).
Learning by remembering is applied in many situations and under different names.
The method of chunking, described in Chapter 7, is one form of it.
Another has been tested by P T Breuer (1991), who used it to speed up a game playing program.
The bucket brigade algorithm appears, among other places, in Booker, Goldberg & Holland (1989).
The hierarchical version is due to Stewart W Wilson (1987).
Bacon is described by Langley, Simon, Bradshaw & Zytkow (1987) who give an unusually thorough account of their experiments.
Later developments appear in Langley & Zytko (1989).
ASSOCIATION AND NEURAL NETWORKS
Summary
This chapter presents a selection of learning algorithms which handle large sets of similar simple attributes.
The ones presented are
an individual neural node
layered perceptrons, and the back propagation algorithm
WISARD
logical neural nets
Boltzmann machines and annealing
Kohonen's feature maps
reproduction from a single parent, with mutation
genetic algorithms.
These algorithms have very different characteristics.
We shall see how the behaviour of each depends on
the space it searches
its degree of heuristic guidance.
We shall also see how they compare with natural minds.
We come now to the class of learning algorithms which, its proponents claim, most closely depicts the workings of human minds.
The input to such an algorithm is a set of values for very many attributes.
All the attributes are of the same type.
The only distinguishing feature of two different attributes is that they are in different positions, or something like that, which has no bearing on their possible values.
Each value is simple — typically either 1 or 0, or’ yes’or’no’, or a real number.
The prototype for such a situation is the input to the nervous system behind a human eye.
This comes from the retina, which consists of very many cells which each’ fire’when illuminated.
(Retinal cells are not all the same, so the eye is a bit more complicated than our idea of the algorithm, but the difference is not too great.)
Behind the retina is a cascade of other cells, neurones, which are all physiologically of just a few types.
Each can receive impulses from other cells, and each will’ fire’and so stimulate other neurones if its own inputs trigger it.
The subtlety of mental processing is due to the layout of links between neurones.
This chapter will discuss seven kinds of device, representing the major computational models in the field.
The first, the simple perceptron, is one of the oldest in the class.
Its output depends on a weighted sum of its inputs.
It is indeed simple, but its basic form is limited.
The simple perceptron is the basic ingredient of multi-layer perceptrons.
The WISARD machine and logical neural nets have structure and function resembling multi-layer perceptrons, but the individual nodes in them are made from 1-bit direct access memories.
The so-called Boltzmann machine is more elaborate and perhaps more versatile, but slow.
 Kohonen nets are designed to invent classifications.
The genetic algorithms form a separate family which do not involve anything resembling neurones, but they are included here because they take their data in a very low level form, like the input to a neural net.
4.1 Perceptrons
The term’ perceptron’has been used by different people to mean different things, but its definition seems to have stabilised by now.
A simple perceptron is intended as a model of a single neurone.
It is a simple computational unit, sometimes called a’ node’.
One node has many inputs and a single output.
There are various kinds of node which differ in the forms of their inputs and output, and how the output depends on the inputs.
We shall start with the simplest.
In this kind, each input is 0 or 1, which you can interpret as’ no’or’yes’.
Its output is similarly 0 or 1.
When its output is 1, we say that it fires.
You can visualise the inputs as coming from an array of photo-electric cells, which we call its retina.
Note that in this picture, each photo-cell yields just 0 or 1, not values in between.
Thus, it is as if the image on the array of photo-electric cells is a silhouette.
(This paradigm is very common, but there is no reason why the inputs should lie in a two dimensional array.
They might form an array of some higher dimension, or they might not have any array-like structure at all.
In the brain, the olfactory system appears to be composed of a sequence of layers of nodes with no obvious geometric structure.)
Let us call the node's inputs s1, s2,…sN where N is some number which may be large.
Each input has an associated weight w1, w2,…wN
These weights are real numbers.
The node also has a’ threshold’t which is another real number.
The node's output is 1 if s1*w1 + s2*w2 +…+ sN*wN > t 0 otherwise
Within the node, knowledge is encoded by the weights and the threshold.
In fact, the threshold is redundant.
You can always replace it by an extra’ input’s0 whose value is always 1, and a corresponding’weight’w0 whose value is -t.
Thus the modified node's behaviour depends on the sum
D = s0*w0 + s1*w1 +…+ sN*wN and it produces 1 if D>0 0 otherwise.
Any perceptron recognises a certain class of all its possible inputs.
This class, usually called C, is{(s0, s1,…sN) | D>0 and sj n{0,1}for each jsN}
It is the class of retinal images which make the perceptron fire.
A node learns by having its weights changed.
There is a well known simple algorithm for doing this.
Its input is an infinite sequence of images
I1, I2, I3,…on the array of inputs, each supplied with’ yes’if it is in the class C which the node should learn to recognise’no’if not.
We assume that the node is in the second form, so its threshold is 0.
The algorithm runs thus.
REPEAT, for each input Ij in turn,
IF D(Ij)>0 but Ij is not in the desired class C
THEN replace each wn by wn — sn(Ij);
IF D(Ij)<0 but Ij is in the desired class C
THEN replace each wn by wn + sn(Ij).
The first alternative corrects all weights which are too high, and so cause the node to recognise an image not in C. The second corrects weights which are too low.
Whenever the node classifies an input correctly, its weights do not change.
A weight may be modified by both alternatives several times before it approaches a stable value.
This learning algorithm has the following pleasant property:
Convergence Theorem
Suppose that there do exist weights w'0, w'1, w'2,…w'N such that the sum s0*w'0 + s1*w'1 +…+ sN*w'N is>0 whenever the input I is in the class C, and is<0 whenever I is not in C; then, for any initial weights wi and for any training sequence Ij, there is some number k for which the perceptron classifies Ij correctly for all j>k.
That is to say, if any node is capable recognising the concept C, then after some limited time, the given node will complete its learning.
However, the theorem does not let us recognise when this stage is reached.
This theorem should be used with care.
Recall, the node has just a finite (though large) set of inputs si, and each input can only be 0 or 1.
Hence there is only a limited set of possible instances I. The sequence Ij may be infinite, but after some point, no new instances can occur.
This theorem cannot be relied on in other cases.
4.2 Multi-layer perceptrons
The very simplest kind of perceptron consists of a single node.
The simplest form which is commonly considered has two layers of nodes.
The lower layer contains many nodes.
Their inputs come directly from the retina.
A single photo-cell on the retina may be an input to several nodes.
The second layer consists of just one node, and its inputs are the outputs of all the nodes in the first layer.
The perceptron's output is the output of this top node.
It is intended to yield 1 if the image on the retina is of a particular kind.
For example, it might be 1 if the scene on the array of photo-cells is convex, and 0 otherwise.
The word’ perceptron’can mean just one isolated computational node, like a single neurone; but it is often taken to mean an entire device with many layers and many nodes in each layer.
A single node will recognise something if at least a certain number of its inputs are on, so it is basically a counting device.
A two-layer perceptron can recognise convex areas on its retina.
A three-layer perceptron can recognise arbitrary polygonal areas, which may be neither convex nor even connected.
It is not clear a priori what classes can be recognised by any perceptron.
One perceptron on its own is limited.
For instance, no single node can recognise the class of images consisting of just one dot anywhere on the retina.
Minsky and Papert studied in some detail how elaborate a perceptron must be in order to recognise a particular class.
There are some simple and interesting restricted forms of layered perceptron.
For instance, one can place conditions on each node in the first layer.
The device is said to be
order limited with limit k if each node in the first layer has at most k inputs;
diameter limited with limit d if each node in the first layer can have any number of inputs, but all its input photo-cells lie in some disc of diameter d.
Example: Recognising convex areas
There is a 2-layer order limited perceptron with limit 3 which can recognise the class of convex images.
An image is convex if every straight line with both ends in the image is actually entirely in the image.
The perceptron's upper layer contains just one node, with an input from each node in the lower layer.
It fires if every node in the lower layer does not fire.
Thus, all weights on its inputs might be set to -2, and its threshold t may be -1.
In the lower layer, there is one node for each set of three distinct collinear points in the retina.
Its weight on the middle input is -2 and its two outer inputs both have weight +2, and its threshold t is 3.
This node fires if the two ends are illuminated but the central one is not.
Even a simple device like this is large.
If the retina is a square array with n photocells along each edge, then there are n(n-1)/2 pairs in it which can be the ends of a line.
On each such line, there are on average some c*n possible middle points, where c is a number independent of n.
Hence K, the number of nodes in the lower layer, is of order 3
Example: Recognising sets of rectangles
There is a 3-layer diameter limited perceptron which can recognise the class of sets of non-overlapping solid rectangles whose edges are oriented horizontally and vertically.
As before, the top layer consists of a single node which fires if and only if all nodes in layer 2 do not fire.
Layer 2 has a node for each 2*2 square of four adjacent photocells in the retina.
It will fire if the input from this little square could not be inside or outside or part of an edge or corner of any rectangle.
Its inputs come from 10 nodes in layer 1, all with inputs from the same 2*2 square.
Among these ten nodes, one fires iff its inputs are all on; one if its inputs are all off; four recognise the four possible orientations of an edge; and four recognise the four possible corners.
By contrast, it is generally accepted that a diameter limited perceptron cannot recognise the class of connected images.
The argument runs thus: Consider the four figures
The top two are connected but the bottom two are not; but any small portion of either of the bottom two also appears somewhere in one of the top two, so any node whose inputs are restricted to some small region cannot behave differently for connected and disconnected inputs.
It is possible to build layered perceptrons with many nodes in the top layer.
Such a device does not’ recognise’any particular concept.
Rather, it calculates a function: The function's argument is the set I of inputs to the bottom layer, and its value is the resulting set of values of all nodes at the top layer.
4.2.1 Other forms of perceptrons
When the inputs to a perceptron s0, s1,…sN can only be 1 or 0, then the possible inputs that it can be shown are the corners of an N-dimensional cube.
There is no particular reason why input should be so restricted.
We can also let any point inside the cube be input too.
We just let each sj be a real number: -0 s sj s 1
Many people design perceptrons thus.
Which kind of input is best depends on what the perceptron is for.
Plain 0/1 input is better if the perceptron is supposed to classify silhouettes, since then, each point on the retina is black or white.
If the image has grey shading then each sj can take intermediate values.
Recall, the output of our first simple perceptron is a step function: f(D) = 0 if D s 0 = 1 if D>0 where, as before, D is the sum of weighted inputs:
D = s0*w0 + s1*w1 + s2*w2 +…+ sN*wN
Another variety of perceptron can have any number in between -1 and 1 for its output too.
There is a common model used by theorists, called a sigmoid nonlinearity.
The output of a sigmoid perceptron is e
This is always between 0 and 1, and never quite either.
The function f has the pleasant property that f(-D) = 1 — f(D)
We say that this perceptron recognises the class of inputs for which f(D) > ½
One can invent other varieties, with different forms of output.
The essential feature is that the output function f(D) should be
monotonic increasing
at or near 0 for large negative values of D
at or near 1 for large positive values of D.
The virtue of the step function perceptrons we discussed first is that they are very simple, so it is possible to derive theoretical results for them, such as the training theorem.
However, a sigmoid  perceptron can embody more information.
4.2.2 The back propagation algorithm
This is intended for a multi-layer perceptron in which each node is sigmoid.
It will work for a recogniser, with just one node in the top layer, but it is phrased as if the top layer contains many nodes.
Suppose that there are M layers.
The input’ bottom’layer is called layer 1, and the output’top’layer is numbered M. There are Nk nodes in the kth layer.
Each training instance consists of inputs to the bottom layer: s0, s1,…sN1 and the corresponding desired outputs d0, d1,…dNM from the top layer.
REPEAT
FOR each training instance
FOR each layer, starting at layer M and working down to layer 1
FOR each node in this layer
Adjust the weights of this node
UNTIL the output is sufficiently close to the desired output for every instance.
Weights are adjusted according to the following rule.
wj: = wj + K * diff * xj
Here, wj is the value of the node's jth weight.
K is a positive constant, called the gain.
xj is the jth input to this node.
When the node is in the first layer, then xj is sj.
In higher layers, xj is the output from a node in the previous layer.
diff is a measure of the error in this node's output.
There is a general method for calculating diff, outlined in the survey by Hinton (see Further reading).
The following is a simple approach.
Suppose the node's actual output is y.
If this node is in the top layer, and its desired output is d, then diff = y * (1-y) * (d-y)
If it is in some lower layer, then diff = y * (1-y) * Diff where Diff is a weighted sum of the diffs of the nodes in the layer above.
Say wk is the weight on the arc from the current node to the kth node in the layer above, and diffk is the diff for this upper node; then
Diff = diff0 * w0 + diff1 * w1 +…diffm * wm where the sum is over the layer above the current node.
The algorithm calculates diff recursively for nodes in successive layers, starting at the top output layer and working back down to the input layer — hence its name, back propagation.
As usual, we have treated the threshold of each node as an extra input from some extra node which is permanently switched on.
This is a hill climbing algorithm.
It is meant to minimise the mean square error of all the output nodes.
The gain, K, controls its speed of convergence.
If K is large, then the algorithm will’ jump to conclusions’quickly, but it may overshoot the optimum weights.
Hence one can expect that it is prone to the canyon problem.
There is no certainty that it will produce the best weights, and there is no knowing when to stop it, but it seems to work well in practice.
Example: A different robot
The robot described in Chapter 3 learned to relate coordinates of its gripper with its joint angles.
This is just one kind of learning which can go on inside a robot's workings.
A different kind, studied by David Fraser and his student Farid Azhar, concerns the precise signals which the robot's electronics should send to its stepper motors.
A simple kind of control, which does not work perfectly, consists of a feedback loop.
In outline, a feedback loop consists of
the input, which is the required setting of a joint;
the stepper motor which moves the joint;
a sensor, which detects the actual setting of the joint;
a control circuit, whose inputs are the required setting and the actual setting.
Its output depends on the difference between these two inputs.
The disadvantage with this design is that, when the robot's joint is loaded, it is never in quite the right position.
The circuit only sends the motor an impulse when the sensor's output is different from the required setting.
The joint always has to sag a bit before the control starts to push it.
A standard improvement on this feedback loop is shown next.
It includes an additional feedforward control.
This anticipates the inadequacy in the feedback control, and provides a supplementary signal.
The output from the feedforward control is intended to provide the motor with just the extra push which will bring it to the required setting.
The controls are run periodically, three times each second.
Both the input required setting, and the sensor, are sampled once for each cycle.
In their implementation, Fraser and Azhar gave the feedforward control a simple memory.
The feedforward control calculates its output from the latest value of the required setting and the previous four values too.
Thus, the feedforward control can provide one value when the required setting is constant, and a different value when the required setting is changing steadily, and yet a third value when the required setting is accelerating.
Both control circuits are implemented as layered perceptrons with sigmoid nodes.
Their structures are shown in the first diagram.
The robot used by Fraser and Azhar actually has three motors for its three joints: waist, shoulder and elbow.
(The gripper was controlled separately.)
Hence, each control circuit has three sets of inputs and three output nodes, one for each motor.
Internally, each net has nodes with interconnections from all inputs and to all outputs.
This allows the output for each motor to depend on the inputs for all three joints.
The feedback net has six inputs.
Three are the required settings of the motors, and the other three are the observed settings measured by the sensors.
These six inputs form the net's retina.
Between it and the output layer, there is a single layer of 18 nodes.
Each node in this middle layer is connected to all the retinal nodes and all the output nodes.
Before this net was connected to the robot, it was trained by the back propagation algorithm to mimic a standard controller.
The feedforward circuit has a retina consisting of fifteen inputs.
At any instant, three of them are the current required settings of the three motors; three are the previous required settings, a third of a second earlier; three are the required settings a third of a second before then; and so on.
In the diagram of this net, the little rectangular boxes on the input lines signify delays.
The middle layer also consists of fifteen nodes, each connected to all the inputs and all the outputs.
This net was trained while the robot worked.
Its weights were originally set to small random values.
The training algorithm is essentially back propagation.
It cannot be exactly the algorithm given above because the desired outputs d from the top layer are not known, so it cannot calculate y * (1-y) * (d-y).
Instead, diff for each node in the top layer is the output from the corresponding node in the top layer of the feedback net.
Thus, the feedforward net is trained to minimise the output from the feedback net.
The graphs show how learning proceeded in the feedforward net.
Learning was quickest for the motor which moved the waist.
The desired torque on this motor does not depend much on the settings of the elbow and the shoulder.
The torque at the elbow depends a bit on the setting of the shoulder, so learning to control the elbow took a little longer.
The torque at the shoulder depends a lot on the angle at the elbow, and so as one would expect, learning to control the shoulder took longest.
Fraser and Azhar experimented to see how the feedforward net reacted when they fixed a piece of metal weighing 450 grammes onto the gripper.
The graphs show that the feedforward net's output was less close to the (new) ideal output, and also that it varied quite a lot.
This is not surprising.
What is surprising is that, when they took the weight off again, the feedforward net's output was better than it had ever been before.
4.3 Logical neural nets
This is the name chosen for another family of devices which are built from simple nodes, connected in a network.
The input, output and internal structure of such a device are all very like those of a multi-layer perceptron, but the individual nodes, and the functions they perform, are quite different.
A node is a memory device, a RAM chip.
It does not do any arithmetic.
Its principal inputs are its address lines, which may each be set to 0 or 1.
If the chip has N address lines then it will contain 2 memory cells which can each store 0 or 1.
Perhaps N might be 10, in which case the chip is a 1 Kbit memory.
When some bit pattern arrives on the node's input lines, this pattern is treated as the address of one of its memory cells.
The RAM chip has three other lines:
an input data line
an output data line
a control line.
The control line tells the chip whether to’ read’or to’write’.
When it is told’ read’, it copies the contents of the addressed cell to the output data line.
If it is set to’ write’then the addressed cell is over-written with the value supplied on the input data line.
It might seem that such a node, incapable of any computation, would be a useless building block for a computing device, let alone a learner, but this is not so.
4.3.1 WISARD
This is the name of a machine designed and built by a team led by Aleksander.
The name WISARD is short for’ WIlkie, Stonham and Aleksander's Recognition Device’.
Some of the nodes in it do a bit of computation, but the learning part is built just of RAM chips.
The inputs to WISARD are taken from a retina, just like the inputs to a perceptron.
Each input from the retina is 0 or 1.
You can imagine that the inputs to each RAM chip are taken from a random selection of points on the retina.
(In a practical implementation, maybe the inputs to the different chips are chosen nonrandomly, so that the different chips survey a carefully chosen range of features.)
WISARD consists of three layers, and is capable of three operations: learning to recognise a class of images, recognising whether a new example is in a learned class, and distinguishing between classes.
These three operations are performed in the successive layers.
The whole device is connected as a tree.
The top node is a’ calculator’which distinguishes between classes.
This has one input from each subtree below it.
Each of these subtrees is a recogniser for one class, whose top node is a’ summing device’.
Below it, the leaves of the tree are the RAM chips, whose principal inputs (address lines) are connected to the retina.
I
Learning:
Each recogniser learns to recognise just one class.
It is taught separately, with its own training set.
The training set need only contain positive examples of this recogniser's class, but it should be clean.
The learning procedure for one recogniser is
Initially, all cells in all RAM chips are set to 0.
An image is projected onto the retina.
Each RAM chip treats its inputs from the retina as the address of one of its cells.
Each chip is instructed to’ write’the input value 1 to this cell.
Recognising:
An image is projected onto the retina, as when learning.
The chip is instructed to’ read’, so it copies the value in its addressed cell to its output line.
This value will be 1 if the current image matches some training instance on this cell's N inputs 0 otherwise.
In each recogniser, all the RAM chips' outputs are fed to the summing device.
The  output from the summing device is just the number of 1s fed into it.
Suppose that a recogniser contains K RAM chips.
If the example presented for recognition is exactly the same as a training instance, then all this recogniser's RAM chips will output 1, so the summing device will output the number K.
If the example is quite unlike any training instance, then the values on the N input lines to each RAM chip will be uncorrelated with any of the values it was trained from.
The cell addressed inside the chip will be chosen at random, and since they were all set to 0 originally, the vast majority of RAM chips will output 0.
The output of the summing device will be much less than K.
If the example is similar to a training instance, then there is a good chance that its inputs to some RAM chips will match the inputs from this instance exactly.
These RAM chips will output 1.
For an example which approximates training instances in most respects, most RAM chips will output 1, so the summing device will output a number close to K.
Thus, a recogniser built to this design can generalize from its training set.
The generalized class of examples which it recognises consists of all which produce a sum close to K.
The class of a recogniser has a degree of fuzziness: Anything which produces exactly the sum K is certainly in the class.
An example which produces a smaller sum k is probably in the class if k is nearly K, but progressively less likely to be in it as k decreases.
Distinguishing between different classes:
WISARD combines several of these recognisers.
It can recognise one class with each recogniser.
The top layer takes for its inputs all the sums produced by all the recognisers, and calculates which ratio R = k/K is greatest.
WISARD's output consists of
the name of the class corresponding to this greatest ratio;
the absolute confidence, R, that the example belongs to this class;
the relative confidence that the example is in this class rather than the next most likely class.
Say the next largest ratio is R'.
The relative confidence is taken to be (R-R') /R
When clean training data are available, the WISARD design has substantial advantages over conventional multi-layer perceptrons.
It can recognise classes, and distinguish between them, and generalize from its training set, just like a perceptron.
It can also provide measures of confidence in its classification, which a conventional perceptron cannot.
Its sensitivity is only limited by the number of RAM chips built into it.
With enough chips and a large enough training set, it could in principle recognise any class of retinal images with absolute confidence.
Its designers have calculated how its reliability increases with more memory.
Last but not least, the learning process is fast.
During learning, each training instance is presented just once, and the learning process consists of writing once to each RAM chip.
That takes less than a microsecond!
4.3.2 A logical net which learns responses
This device is half way between WISARD and a multi-layer perceptron.
Its nodes are all like WISARD's RAM chips, but the interconnections between them are arranged so that the nodes are in a layered partial order, not necessarily a tree.
The arcs between nodes just convey bits of data.
They do not have any associated weights.
They could be copper wires.
As well as the nodes and their interconnections, the net has a separate circuit which can generate random bit patterns.
If the net has K nodes, then there will be K bits in each pattern.
The K lines from this circuit are linked to the input data lines on the K nodes.
If there is just one node in the top layer, then the device is a recogniser which outputs 1 for examples in its recognised class and 0 for all other inputs.
If the top layer contains several nodes, then the device computes a more elaborate function of its input.
Problem solving:
An example is projected onto the retina.
In each node of the bottom layer, the inputs are treated as the address of a cell.
The value stored in this cell is the node's output.
The same procedure is followed in progressively higher layers.
The outputs from each layer are taken as inputs to the next layer up, as far as the top layer, whose outputs form the output of the whole device.
Learning:
As for WISARD, the training set should be clean.
It consists of correctly matched input/output pairs.
Thus, if there is just one node in the top layer, so that the device is a recogniser, the training set should contain both positive examples (with expected output 1) and counter-examples (with expected output 0).
Before learning starts, every cell in every node is set to 0 or 1 at random.
This is done by writing a random bit from the generator to each cell in turn, in every node.
After that, the learning procedure is much like the one above for problem solving.
The only difference is that, if the output from the whole device is wrong, then every node in the whole device is instructed to reset its addressed cell to some random value again.
When the device classifies some training instance correctly, then all the cells it refers to for this instance are unchanged.
When it mis-classifies an instance, all the cells which were addressed are reset to a random value.
This is a rather rough and ready technique.
The device makes no attempt at credit assignment.
Some of those same cells may also be referred to when the device is presented with an instance which it has already classified correctly, so later learning can upset what was learned before.
Example: Backing up a truck
The task is to reverse an articulated truck into a loading bay.
Anyone who has watched this being done will realise that it is difficult.
The back end of the truck should end up just abutting the door of the bay.
The centre of the back end should be near the centre of the door, and the axis of the rear half of the truck should be perpendicular to the door.
A state of the task is described by giving
the coordinates (x, y) of the centre of the back end, relative to the centre of the loading bay door.
In a goal state,(x, y) should be near (0,0);
the angle y of the axis of the rear half of the truck away from a line perpendicular to the door.
In a goal state, y should be near 0;
the angle v between the axis of the driver's cab and the line perpendicular to the door.
(This angle does not matter in a goal state.)
Thus, a task state is described by four real numbers.
Within the net, these numbers are represented as bit patterns and their numerical values have no significance.
A system which learns to solve this problem consists of two parts:
a logical neural net, as above.
Its input is a current state of the truck.
Its output is a setting for the steering wheel in the driver's cab.
an’ emulator’, which is a program whose inputs are a representation of a current state of the truck and the output of the logical neural net.
Its output is the state of the truck, after the truck has been reversed a short distance with the steering wheel set as instructed by the net.
The training set consists of a variety of initial positions of the truck.
The net and the emulator are run alternately.
Each time they are run, the net's output is considered’ good’if the truck's coordinates, x, y and y, are all closer to 0.
Otherwise, the net's addressed cells are reset at random.
An example like this was tested as a student project, devised and supervised by Teresa Ludermir.
In the project, the net has three layers.
The lowest layer contains four nodes, one for each of the four parameters x, y, y and v. In each node, its parameter is treated as the address of a cell.
The middle layer contains 24 nodes, each connected to all four outputs from the bottom layer.
Each node's output in the middle layer is connected to just one node in the top layer.
These connections are arranged’ randomly’.
The top layer has six nodes, each with four inputs.
Their outputs are interpreted as a 6-bit signed binary number, which is to be the setting of the steering wheel.
This device was trained with instances of increasing difficulty.
The first few instances were close to a goal, while later ones had larger values of x, y and y.
It succeeded in learning how to reverse the truck into the bay.
This task is similar to one described by Kong and Kosko, who credit it to Nguyen and Widrow.
Their version of the problem is easier because the truck is not articulated.
The steering wheels are mounted directly on the trailer.
Kong and Kosko describe an implementation of the task using fuzzy rules, and another using a traditional layered perceptron trained by the back propagation algorithm.
The implementation with fuzzy rules worked, but it was provided with substantial prior knowledge in the form of the rules, and I am not sure how much of its success was due to learning.
It still worked quite well when up to half its original rules were removed.
The neural net controller contained 24 hidden sigmoid neurones.
Learning in this net needed more than 100,000 iterations.
Kong and Kosko write that the learning algorithm for it did not always converge, and the resulting system did not always back the truck up smoothly.
Design of logical nets:
In modern digital computers, it is cheaper to use large RAM chips with many address lines, but in a logical net, small RAM chips may work better.
In any learning task, the proportion of cells in each chip which should be set to 1 is likely to be independent of the number of cells in the chip; so the number of learning cycles will be roughly proportional to the size of a chip.
If each chip has N input address lines then the chip will contain 2 cells.
Hence, the time needed to train the device is likely to rise roughly exponentially with N.
Another advantage of small chips with few address lines is that each cell of a chip only stores a very crude fragment of knowledge about the image on the retina.
In logical nets, generalization depends on chance exact match between some portion of the retinal image and a training instance.
The more bits surveyed by each chip, the more precise will be the required match, and the proportion of matches among all the features surveyed by all nodes will be smaller.
Hence the scope in the net for generalizing will be better if N is small.
4.4 Boltzmann Machines
Within a perceptron, the flow of information is just one way, from input to output.
If perceptrons are layered, then similarly, the flow is just one way.
This is not always desirable.
A computational device of this sort cannot perform any calculation involving steps equivalent to iteration or recursion.
For that, one needs some kind of feedback loop.
Perceptrons can be connected in loops.
Rather than study them, we here describe a kind of network in which information can flow in both directions.
This particular sort is called a Boltzmann machine.
A Boltzmann machine (or BM for short) is a finite graph, with various additions.
Each node n is a switch which may be’ on’or’off’, 1 or 0.
The setting of this switch will be written sn.
Note that a switch is not like a simple perceptron.
In addition, each node has an attached threshold, tn.
Each arc is labelled with a weight, a real number.
The weight on an arc between nodes i and j will be called wij.
The nodes are partitioned into two sets: the visible nodes V, which behave as input/output links, and the hidden nodes H which are involved only in calculation.
For practical applications, we also assume that the graph is connected, and that V is nonempty.
Each arc is symmetric in i and j.
It is not directed, and the weight on it may be denoted as either wij or wji.
The BM is designed to accept as input a setting of some of its visible switches, and provide as output settings of the remaining visible switches.
Note that there is no fixed division between inputs and outputs.
The BM is rather like a Prolog predicate.
Each visible switch can be treated as input for one application, and output for the next.
The BM tries to find the best settings of all its switches, subject to any restrictions placed on any in V. What counts as’ best’depends on the values of its weights.
Each setting of the nodes in V can be regarded as a’ fact’or a’typical situation’.
For instance, one can imagine a BM intended to recognise dogs and lamp posts.
All but two of its visible nodes will be tied to the output of cells on a retina.
Of the remaining two, one should be on if there is a dog on the retinal image.
The other should be on if the image includes a lamp post.
The machine can be used in different ways.
You could show it a scene, while its two special nodes are free, and see if either or both of them come on.
Alternatively, you could show the retina an incomplete image of a dog, with some retinal nodes free, and tie the special’ doggy’node on.
In that case, the machine should complete the image.
The free retinal nodes should be switched so that the final image on the retinal nodes is a better image of the dog.
The problem-solving aspect of a BM is a classic case of hill climbing.
It actually looks for a lowest point, not a summit.
The’ height’which it tries to minimize is called the machine's energy, and is defined to be
E = si * ti — si * sj * wij
As with perceptrons, the threshold can be replaced by an extra’ switch’called s0, which is permanently on, and which is connected to each other node i by an arc with weight w0i = -ti.
Then, the first sum for E can be omitted.
We shall do this:
E = — si * sj * wij
Suppose that some switch si is initially 0, at which point the BM has energy E0; and then this switch is turned on, and the machine's new energy is E1.
The increase in energy is
E1 — E0 = — sj * wij
This gives a simple rule for deciding how to alter each switch, to lower the energy.
If si is originally 0 and E1<E0 then turn this switch on;
If si is originally 1 and E1>E0 then turn this switch off;
Otherwise, leave this switch unchanged.
To begin with, all switches except the inputs are set at random.
The BM attempts to minimize E by repeatedly adjusting the settings of all switches except the inputs, according to this rule.
When E is at a minimum, the output is the settings of the other visible switches.
The references on BMs leave open the issue of how one chooses which switch to consider when.
One might either consider changing each switch in turn, cyclically, until none is changed; or, one might consider changing them all simultaneously.
The machine's behaviour will be slightly different for different options.
The design is well suited for parallelism, with one processor for each node, all switching together (synchronously) or at random(asynchronously).
4.4.1 Annealing
As with all hill climbing, a BM may end up at a local minimum.
To try to avoid this, the above simple rule is changed.
The BM introduces some randomness into its switch settings, which is intended to make it sometimes jump out of local minima.
The degree of randomness is controlled by a parameter T, called the BM's temperature.
The rule for changing switch settings becomes:
If si is initially 0, then change it to 1 with probability
DDDDDDDDDDDDDD
If si is initially 1, then change it to 0 with probability
DDDDDDDDDDDDDD
When T is very small, this rule is almost the same as the first simple form.
It only differs significantly when the change in energy,(E1-E0), is very small too.
When T is progressively larger, so too is the random element.
This particular form is symmetric about the case E1 = E0, at which the switch has an even chance of going either way.
The annealing algorithm runs thus:
Fix the input visible units.
All other switches are initially random.
FOR progressively smaller values of T, until T is small alter the switches repeatedly according to this probabilistic rule.
The output is now on the free visible nodes.
At any stage, the machine may jump away from any local minimum; but as T gets smaller, it is more and more likely to stay near a particular minimum.
The intention is that while T is large, it will jump often between the catchment areas of different minima.
As T falls, it will still be able to jump readily out of the catchment of undesirable local minima, but it will be progressively less likely to jump all the way out of the neighbourhood of the global minimum, which is where we want it to finish.
4.4.2 Learning in a Boltzmann Machine
Within a BM, knowledge is stored in the values of its weights.
The kind of knowledge it can hold consists of preferred settings of all its visible nodes.
Suppose that there are several such preferred settings,
S1, S2,…
Sk,…which occur naturally with different frequencies.
You can regard each setting Sk as depicting some fact.
A BM can be taught many such facts.
Say, the natural frequency with which Sk occurs is Fk.
If the BM is running’ free’at some fixed temperature T, with no restrictions on any of its units, then different settings will appear on its visible nodes.
Ideally, we would like just these settings Sk to appear, and no others; and each should appear with its natural frequency Fk.
The basis of learning in a BM is, we force it so that when it runs free, this is what it does.
The BM's learning algorithm is a cyclic process which never ends.
The BM can never be guaranteed to have learned perfectly, so learning continues until the machine's performance appears to be good enough.
Each cycle runs as follows:
Fix the temperature of the machine to some constant value T.
Run the annealing algorithm, lowering its temperature.
While it runs, make the input on the visible units jump between the various desired settings.
The proportion of time that the input is any particular Sk is Fk.
Also while the machine runs, observe each node.
Let pij be the proportion of time for which si*sj = 1.
When the values of the pijs are known fairly accurately, let the machine run free.
While it runs free, say p'ij is the proportion of time during which si*sj = 1.
When the frequencies p'ij are known fairly accurately stop the machine and adjust its weights.
If pij>p'ij then increase wij.
If pij<p'ij then decrease wij.
The amount by which one changes weights is somewhat arbitrary.
One could say that, whatever the values of pij and p'ij,
change wij to wij + (pij — p'ij).
In practice, real BMs sometimes just add or subtract 1.
The logic behind this rule is that if si and sj are switched on together more frequently in the desired state than while the machine is free, then we want them to correlate more; so we increase the weight of the link between them.
In fact, the learning rule can be given a firmer foundation.
While the machine runs, it passes through a sequence of states.
A state is a set of values for all its switches; so if the BM has N nodes, then it has 2 possible states.
We shall denote any state by the letter G. Suppose that in the first phase of a learning cycle, while its inputs are the desired settings Sk, each state G occurs with frequency P(G).
Then in the second phase while the machine is running free, G occurs with frequency P' (G).
Ideally, this should be the same as P(G) for each G. The discrepancy between them can be quantified as a number, commonly called C. This is zero when each P' (G) equals the corresponding P(G).
Otherwise, C is greater than zero.
The learning algorithm is designed to find a minimum of C. It is another example of hill climbing, with height C.
Ludwig Boltzmann was an Austrian physicist.
The name’ Boltzmann’is chosen for these devices because their algorithms can be justified by a statistical analysis of the sort first devised by Boltzmann.
His method shows how one can explain many physical phenomena, including the speed of sound in air and the colour of light from a glowing furnace.
However, the analogy should not be taken too far.
Boltzmann did not assume that energy is minimized.
On the contrary, he assumed that it is constant, as have all physicists before and since.
His principle is that another quantity called entropy is maximized.
Entropy measures randomness; so Boltzmann's physical principle is that nature maximises chaos.
Intelligence minimises it.
4.5 Kohonen's feature maps
Both the varieties of networks considered so far can learn, but they cannot invent new concepts.
If you want a perceptron or a BM to learn to recognise some class of examples, you have to provide it with a) examples of the class b) counter examples c) for each training instance, the fact that it is an example or a counter example.
Kohonen's nets are quite different.
They invent clusters.
A cluster is not the same as a concept, but if a learner can discover natural clusters then this is progress towards finding a concept.
The training set does not consist of labelled examples and counter examples.
Rather, it contains a mixture of instances of all sorts, without any distinction between examples and other instances; and the net discovers natural groupings among them.
We shall investigate other clustering algorithms later.
A node in a Kohonen net is simpler than nodes in perceptrons and BMs.
Every node is connected to all the inputs s0, s1,…sN and for each input, each node has a weight w0, w1,…wN and nothing else.
Each input si is a real number.
However, the weights are not used to multiply the node's inputs.
They are used to measure the’ distance’of the node from an input, as we shall see.
When the algorithm begins, nodes are’ scattered’randomly or evenly across a space described by the values of their weights w.
As it progresses, the nodes' weights are changed a little at a time so that the nodes drift into clusters.
The clusters form in areas where there are lots of inputs.
Other regions where inputs are rare become empty of nodes.
Thus, a Kohonen net correlates inputs by forming clusters of nodes.
Typically, the net contains many nodes, but each node only has a few inputs: N is quite small.
The nodes are laid out in some sort of space.
The natural way to do this is as an N-dimensional rectangular array; but all that matters is that there is some metric D on the set of nodes, so that we can talk about the distance
D (n1, n2) between any two, and about neighbourhoods of a node.
Kohonen's clustering process involves two separate metrics.
The one we have just discussed is a distance between nodes.
It is given before the algorithm begins.
The other, d, is a distance from a node's weights to an input instance, and is calculated at run time.
The D metric is a computational convenience.
It is the d metric, calculated from weights, which is of most interest.
The algorithm involves two numbers, K and c.
K is used to decide when two nodes are close.
c is a gain term, and specifies how much is learned from each input.
It proceeds as follows: Choose random values for all the weights of all nodes.
No two nodes should have all their weights the same.
Choose a positive value for K and a value for c in between 0 and 1.
WHILE K and c are not negligible
FOR each training instance s1…sN
Compute the distance d of each node from this instance: 5DDDDDDDDDDDDDD6
Select the node n whose distance d from this input is least.
FOR each node n' close enough to n: D (n, n') <K
Update its weights: w'i: = w'i + c * (si — w'i)
Reduce K and c a bit.
The effect of one cycle of this algorithm is that one node and its near neighbours are moved closer to the current input.
Other more distant nodes stay where they were.
Thus, over several cycles, the nodes spread out.
Each follows the inputs nearer to it and tends to drift away from other nodes.
After a while, the nodes will be spread over their metric space according to a distribution close to the probability density of instances in the training set.
If you wish, you can consider that each node corresponds to a cluster of inputs.
4.6 Optimizing Simple Behaviour
Some quite subtle effects can be produced by adjusting a few parameters.
You can show how on a personal computer.
The method involved forms half of a genetic algorithm.
The full genetic algorithm will be described below.
The computer screen displays a few white dots, called’ protozoa’, and lots of purple dots called’bacteria’.
The program adds bacteria to the screen in a steady stream, scattered at random.
Once a bacterium has appeared on the screen, it remains there until a protozoon approaches it and eats it.
Each protozoon makes a biassed random walk about the screen.
It has a certain store of energy.
This store is boosted by 40 units each time it finds and eats a bacterium.
It loses one unit of energy for each step of its walk.
If its energy level falls to zero, it dies and disappears.
If its energy ever exceeds 1000, it divides into two offspring.
A protozoon has a head, labelled’ F’.
Each step consists of a rotation through some multiple of 60 degrees, followed by movement forward one unit.
The six possible rotations are labelled F (for no rotation — just move forward), L (left), HL (hard left), RV (reverse), HR (hard right) and R (right).
Its motion is characterized by an array of six whole numbers, one for each possible angle that it might turn through.
On any move, the chance that it might turn through one angle is large if the number for that angle is large in comparison to the numbers for other angles.
If the number for direction X is x, say, then the probability that it will turn hard left is
When a protozoon divides, its two offspring have almost the same numbers as the parent, but one of them has just one number augmented by 1 and the other has just one number decreased by 1.
The system begins with its protozoa jiggling about at random, but after a few generations, they evolve into a race of protozoa which tend to move in straight lines.
This is a good foraging tactic when the bacteria, their food, are scattered at random.
There is a variant of the experiment: Bacteria drop at random over most of the screen, but in one small region (the Garden of Eden) they appear much more frequently.
The result is evolution into two distinct populations.
The protozoa outside the Garden of Eden tend to travel in straight lines, as before.
However, inside it, they evolve into a race of protozoa which spin in tight circles.
Thus they stay in the garden.
4.7 Genetic algorithms
This is another family of learning algorithms which, its supporters claim, has biological verisimilitude.
It works when
the learning task can be described as hill-climbing;
each state in the domain of learning can be described by a (long) string of similar entries, such as real numbers or binary bits, and each such string describes some state;
there is lots of processing power at hand.
It has been used successfully, for example in the design of jet engines.
The idea runs thus:
The algorithm keeps a set of several strings.
It is simpler if they all have the same length.
Each string portrays one state of a target system (although the state of the learner is the set of all the strings).
The initial choice of strings may be random, but the algorithm works better if some of the ones it starts with are varied but fairly sensible.
There is some’ height’function H of states of the target system, which we want to optimize.
In each learning cycle, the algorithm calculates H, for each of its strings.
When it has done this, it replaces the worst few strings with new strings, formed by randomly blending a random selection of the best strings.
This cycle repeats, until the best string is good enough.
Such an algorithm can provide an alternative learning mechanism in a neural network.
Let the arcs of the net be ordered.
This can be done in any way you please, but once the order is chosen, it stays fixed.
Each arc has a weight, which is a number.
Any state of the net is given by specifying its list of weights (in order).
If each weight is written in binary form, all to the same precision (say 8 bits), then we can form a bit string by simply concatenating them.
Thus, the weights 15 3 8 -37 corresponds to the bit string 00001111000000110000100011011011
Suppose that this is a good string, and another good string is 00010011101110010100011101110001 which encodes the weights 19 -71 71 113
We combine these by choosing just a few random points in them, say two or three, and copying from the first string up to the first point; then copying from the second up to the second point; then copying from the first again; and so on, switching between them at each point.
If the chosen points are as shown: 00001111000000110000100011011011 x x 00010011101110010100011101110001 then the resultant string will be 00001111000010010100011111011011 which encodes 15 9 71 -37
If you prefer that the algorithm should not invent new weights, but only select existing weights from the parent strings, then the crossover points marked’ x’may only be at the ends of 8-bit sequences.
The idea of genetic algorithms is to mimic natural Darwinian selection of genetic codes.
The chance of a given string being chosen to’ reproduce’is related to the worth of its state, H. When the two good strings are similar, then any feature which occurs in both will inevitably appear in their’offspring’.
Any feature where they differ is chosen from one of them at random.
Thus, features which lead to good states are reproduced, and others drop out.
In principle, it would seem that the performance of a genetic algorithm may depend significantly on the order of the basic data in each string.
When a particular order is chosen, then any cross between two strings which keeps a weight w from one parent will keep w's neighbours in that parent, unless there is a crossing point (x) just before or just after w.
Thus, the choice of initial order in the strings imposes correlations between the weights, and these may be unnatural.
Enthusiasts for genetic algorithms do not seem worried about this.
Nature does the same.
Such a method was tested by Whitley on three neural nets.
He reports that the algorithm does indeed usually converge.
It is often better than a standard learner, such as the Boltzmann algorithm, because the genetic algorithm keeps note of a large set of states, one for each string.
The Boltzmann machine may miss the best solution and climb a foothill, or be stuck on a knife edge; but the genetic learner has a broader view of many possible states, and is less likely to be so caught.
Genetic algorithms have proved their worth commercially.
One simple way to design a good fan blade for a turbine is to design a mediocre blade, describe its shape with some numeric parameters, and then test variants of it.
The parameters can be put together to form a’ genetic code’for the blade.
The variants can be tested by simulations in a large computer, and the codes of successive variants can be optimised by genetic recombination and selection.
After several cycles, the mediocre blade will be replaced by a better one.
Dr. Steve Jones cited this example in his 1991 Reith lectures on the BBC — and he is a real biological geneticist.
Genetic algorithms involve a lot of computation.
30000 recombinations is considered’ a very small number’.
The number of strings which the algorithm keeps may range up to a few thousand, and each bit string may be a few hundred long.
In addition, there is the overhead of calculating the height function, H. For a neural net consisting of layered perceptrons, this may not be so heavy; but if it involves annealing, the total process demands a lot of computing time.
The benefit of a broad view over many states only applies if the set of strings is large and varied.
The genetic learner may go astray if its strings are mostly similar.
As the learner is designed to favour good features, the population will eventually become monoclonal, however big it is and however it starts.
The genetic theory postulates two modes of evolution.
One works by crossing two parents with slightly different genetic codes, and mixing their genes randomly.
This is the essence of the algorithm above.
The other mode depends on occasional small mutations, like the changes in the parameters of protozoa.
Whitley tested a variant of the above algorithm which incorporated both modes.
It only (!) keeps 50 strings.
When they seemed to resemble each other rather too closely, he introduced random mutations in the offspring.
The number of mutations increased as the parents became more alike.
Compared with the original algorithm, this often produced fast, accurate results, but it did sometimes fail to produce any acceptable result at all.
4.8 Properties of neural nets and genetic algorithms
At the end of the last chapter, it was apparent that the classification scheme which we are using is fallible.
For neural nets and genetic algorithms, it is not so much fallible as crude.
There is an immense difference between Wisard and logical nets at one extreme, and Boltzmann machines at another.
The reason becomes clear when we consider these devices' search spaces for learning.
Any learning neural net explores a space in which each state is described by a large set of simple parameters.
In a logical net, the parameter values are the settings of the bits in the cells of its RAM chips.
In a classical perceptron or Boltzmann machine, the parameters are the weights on edges between nodes.
The learning method in Wisard consists of remembering examples.
It does no search.
Hence, learning takes just as long as it takes to present and record the examples.
A logical net searches while learning, so its learning process is slower than that of Wisard.
The learning search of a logical net falls into two levels.
The simpler level is a search for the correct output for some particular instance.
The higher level is a search for correct outputs for all instances in the training set.
Its search has two interesting features:
Most neural nets search by some process which involves a form of convergence by successive approximations.
In a logical net, at the simpler level of search, there is no notion of approximation to a correct answer from given inputs.
The learner makes no use of anything like a metric on its set of possible states.
It uses absolutely no heuristic guidance.
When an instance is mis-classified, all bits used in that classification are reset at random.
Given these curious aspects, it is a bit surprising that a logical net's learning process succeeds.
Perhaps it does because its search space contains very many goal states.
Also, although the simpler level of search has no metric and involves random jumps, there is a notion of proximity at the higher level: Two states are close if the proportion of RAM chip cells with different entries is small.
In this sense, the search at the higher level only takes small steps.
All other kinds of neural nets have learning search spaces parametrised by weights, which are real numbers.
(In a simple perceptron, even though the output of a node is discrete, the weights on edges are real numbers.)
Their learning processes involve convergence of these weights.
One simple distinction is between layered nets which learn by back propagation, in which the search optimises each node's output locally, and nets such as the Boltzmann machine which optimise a global feature such as the quantity called C. Local optimisers usually learn faster than global optimisers.
Actually, the sluggishness of the BM learning process has another cause.
It is a hill climbing process which should avoid foothills, so the learning steps have to be small.
The back propagation algorithm can also be described as a hill climber, but it is not so prone to climbing false foothills.
It is designed to optimise weights in one layer of the perceptron at a time, and the procedure for optimising weights in one layer is controlled by properties of the layer above which is already partly optimised.
Thus, each layer provides heuristic information about how to optimise the next layer down.
At each node of a BM, there is no such heuristic guidance.
Kohonen's method minimises the distance between an input vector and the vectors of nearby nodes in the net.
This is a form of local optimisation.
The degree of locality is controlled by the parameter called K, which bounds the set of nodes whose weights change in one learning step.
The search steps taken by a genetic learner are of two kinds.
The process of crossing between two genetic codes is random.
The selection of relatively successful codes is heuristic.
All the onus of learning falls on the heuristic part.
The random part is what makes genetic algorithms slow.
Strengths:
Algorithms of this sort are very widely applicable.
The form of input is so basic that almost any application can be encoded as input to a network.
Except for the ones based on RAM chips (WISARD and logical neural nets), they tolerate noisy training sets.
In fact, noise can be very helpful because it gives the network a chance to jump out of local minima.
Training examples are not saved.
Perceptrons solve problems quickly, although Boltzmann machines do not.
Perceptrons require both examples and counter-examples.
Boltzmann machines require only positive examples.
Kohonen nets do not involve any notion of a concept, so there is no notion of a positive example.
Genetic algorithms require some measure H of the quality of each example.
They are suitable for parallel processing, with one process for each node.
Limitations:
In these designs, learning is not incremental.
(Other designs of learning neural nets may be able to learn incrementally.)
These algorithms are typically quite good at learning how to respond to one kind of input.
In principle, a perceptron should be able to respond correctly to several varieties of input; but in practice, learning later responses may upset what was learned before.
Although there is a well developed theory of how these devices learn and solve problems, there isn't any very useful theory of how they represent their learned knowledge.
The knowledge is not in an accessible form.
This is particularly true of BMs.
As a result, it is very hard to predict the behaviour of a BM in detail.
It is an essentially probabilistic’ black box’.
A user of a BM has to live with whatever the BM tells him, which will be correct for most of the time but which is never totally reliable.
(The one exception is Wisard.)
As a consequence of the last remark, these devices are incapable of returning’ right’answers.
For the same reason, learning is never complete.
The user must decide when the machine's knowledge is reliable enough, if it ever is.
Training sets are typically large, for any kind of neural net.
Learning in neural nets is notoriously slow.
4.9 Comparison with Biological Systems
The resemblance between genetic algorithms and Darwinian evolution seems close, although natural genetic reproduction may be more subtle.
There is a suggestion that offspring do not have an even chance of inheriting a trait from either parent.
Each natural gene may come with accompanying genetic material which exists solely to block the corresponding gene from the other parent; and furthermore, there may be even more genetic material which sometimes inhibits the blocking mechanism!
The Economist (March 21st. 1992, pages 212–122) summarises the idea.
Nothing like this occurs in the artificial form of the algorithm.
The human brain is a network of neurones.
A neurone is a cell with a small centre from which stem long strands called axons and dendrites.
There are links, called synapses, between axons and dendrites.
When a neurone’ fires’, it releases chemicals called neurotransmitters at each synapse on its axons.
These stimulate (or inhibit) the dendrite on the other side of the synapse.
Thus, the flow of information across a synapse is one way, as in a perceptron.
Quite a lot is known about how neurones are interconnected in different parts of the brain, and how they work together as a computing device.
A lot of research in neural nets appears to be driven by researchers' imaginations and the desire to make devices which work, with very little reference to biology.
For example, no neuron behaves like a RAM chip, and synapses do not appear to be symmetric as in a Boltzmann machine.
This may be a wise strategy.
Some comparison between the brain and artificial machines can be made purely by scale.
Each brain contains some 10,000,000,000 neurones.
Each neurone is linked by some 10,000 synapses.
There are about 100 known kinds of neurotransmitter.
In addition, neurones are not linked randomly.
There are signs of organized layout among the axons and dendrites.
From all this, it will be clear that there is no prospect of building artificial nets with anything like the subtlety of the brain.
However, neurophysiology does offer some encouragement.
For a start, the brain of a bee contains only about 300,000 neurones.
That sort of number is within the range of current technology.
Many observed functions within the brain resemble what can be done with layers of perceptrons.
In the parts which deal with smell and taste, lower layers of neurones show only a little specialization between different stimuli.
Later layers show progressively more specialization.
The human vision system can recognise a face in about 150 milliseconds.
This sort of speed is typical of some score layers of neurones.
By contrast, the process of learning is slow and subtle.
Learning a manipulative skill, such as typing, takes hours.
This is more typical of a Boltzmann machine.
There is a region of the brain called the hippocampus which has something to do with recording new memories.
Within it, some neurones link back on themselves, as in an autocorrelator.
There are some aspects of psychology which have not yet been fitted into the neural net paradigm.
One is storage of memory for events.
Nobody knows whereabouts this occurs in the brain.
Another involves the process of recall.
When you remember a scene, it is as if what you once saw reappears before your eyes.
If the memory is particularly vivid, you can actually fail to see what is really in front of you.
It seems that the remembered visual image replaces input from the retina.
A third is the question of awareness, emotion and reasoning.
There is a phenomenon called’ blind sight’which sometimes occurs after brain damage.
The person involved may look at something and not be aware of it; but if he is asked to guess where the object is in his field of vision, he guesses correctly.
Evidently, all the intermediate levels of visual processing still work, but there is a broken link between the output of vision and the seat of awareness.
That seat of awareness is not understood at all.
Further reading
Over the past half decade, there has been a flood of designs and theories for different devices of this sort.
This short chapter is only intended to offer a taste of the topic.
There is no space here to attempt a full survey.
Anyone who wants a thorough view should study the Neural Computation journal and some of the many new textbooks on neural nets, such as Aleksander & Morton (1990), Judd (1990), Kosko (1992) and Zurada (1992).
Hinton (1989) and Lippmann (1987) give good introductory surveys.
For more details of perceptrons, see Minsky & Papert (1969) which is reviewed by Block (1970).
The account here is adapted from Bundy et al.(1980).
Variants of the learning method for a single node, and others, appear on pages 59–73 of Zurada (1992).
Nilsson (1990) provides two proofs of the convergence theorem.
There have been studies of perceptrons with other styles of output function.
One such is based on a node whose output is near zero if the sum of inputs is either large positive or large negative.
The graph of the output is a Gaussian hump.
Details and further references appear in Hartman, Keeler & Kowalski (1990), Park & Sandberg (1991), and Poggio & Girosi (1990).
The account of WISARD given here follows Aleksander & Morton (1990).
I am grateful to Teresa Ludermir who introduced me to it, and to logical neural nets.
She has published various papers about them, such as(…
). Her version of the truck reversing problem is similar to the one given by Kong and Kosko; see Kosko (1992), although her solution is different.
Boltzmann machines and other similar devices have foundations in information theory.
This can justify both the formula for C and the expression for the probability of switching sj while annealing.
For more discussion, see Hinton et al.(1984).
The theory of neural nets has been developed much further.
It now covers stability of the convergence process.
This work derives from the theory of dynamical systems which was originally developed by Anosov, Smale and others in the 1960s.
Ruelle (1989) describes some of the underlying mathematics.
For more details of Kohonen's nets, see T Kohonen (1982, 1984).
The experiment with protozoa is described by A K Dewdney (1989).
Booker, Goldberg and Holland (1989) describe genetic algorithms as a form of rule-based systems, but this seems an unduly narrow view.
Forsyth (1989), Chapter 4, gives an extended discussion with reference to applications.
The account here is taken from Whitley (1989).
Rolls (1987), referred to in Chapter 1, gives an account of real neural processes.
The seat of awareness is a subject of philosophical speculation.
For some suggestions on what it might do, if we can ever find it, see Johnson-Laird (1983).
CLUSTERING AND CORRELATION
Summary
The subject matter of this chapter falls into three broad sections.
The first covers the basic principles of clustering:
top-down and bottom-up clustering techniques
bias: the effect of description language on clustering
extensional clusters
intensional descriptions of clusters
names for clusters
the contrast between statistical and conceptual clustering
simple hierarchies and ontological hierarchies.
The second contains descriptions of five clustering algorithms:
a method for inferring context-free grammars
a top-down decision tree constructor depending on entropy, which is explained
a more sensitive incremental tree constructor
a way to decide which properties of a system depend directly on each other an elaborate algorithm which builds a directed graph of concepts incrementally, and which can decide where to concentrate its effort to best effect.
The final section is a comparative study of these algorithms and the methods they incorporate.
It describes
features of the above clusterers, and how properties of a clusterer affect its behaviour
a general procedure for inventing a clustering algorithm for any particular situation.
There is an appendix on
the design of statistical experiments
estimating entropy
composite properties formed from simple observable ones
expected success rates, and the relative merit of entropy.
The paradigm underlying this family of methods is that patterns or clusters appear nonrandomly in the training set, for some unknown underlying reason.
The learner discovers the clusters among examples by noticing nonrandom correlations among their features.
Usually, he (or it) takes absolutely no account of any possible causes of such features.
This sort of learning ignores underlying structure of examples, and it does not try to explain why the regularities occur.
The differences between these methods and neural nets are both qualitative and quantitative.
All the inputs to a net are simple and similar, and there are typically lots of them.
A correlator's input consists of examples with just a few features which can be quite elaborate.
This means that the correlator can use statistical methods, or approximations to statistics, which would be impractical in a large neural net.
Perhaps the most satisfying kind of learning combines this sort of statistical observation with later analysis.
That is what Mendel did.
He noticed statistical trends in inherited traits; he postulated a statistical model with nice properties which fitted his data; and then he suggested a simple discrete model which would behave like the statistical model.
Many years later, biochemists showed that his discrete model really does occur in the molecular structure of living cells.
5.1 How Clustering Works
A cluster is a set of examples which are close together.
The very word’ close’only makes sense when the examples are in a metric space, or something like it.
If you want to do clustering of any sort, you need a pseudo metric.
A pseudo metric is a function like a metric, which satisfies all the axioms of a metric except that the distance between two distinct points can be zero.
Every metric is a pseudo metric.
The natural prototype for a metric space is the world we live in, where the metric is Euclidean distance which you can measure with a ruler.
This is a bit misleading, for the following reason.
Each position in Euclidean space can be described by three coordinates, and coordinates are real numbers which can be added.
This means that you can take an average of several positions.
Hence if, say, you are watching a flock of starlings pecking for worms on a lawn, you can talk about the centre of the flock.
This is a point on the lawn where there may not be any starling.
In fact, if the starlings are all in a circle (perhaps the lawn has a fairy ring which the worms find congenial), then the centre of the flock will be quite a long way from any of the birds.
See figure 5.1.
The Euclidean metric is unduly specialised.
If you think instead of a Venn diagram metric, then you will understand that not all metrics permit averages.
Example: Classifying species of British birds
This is a complicated process, with at least two levels.
To start with, imagine you are given just the physical outline of adults of several species.
You might look for clusters, using the properties
size
shape of beak
position of eyes
shape of wing.
Each of these properties is described by a number, or a few numbers.
The shape of a beak is described by the ratio of its length to its depth, and its curvature; similarly, the position of eyes may be characterised as the angle which they subtend at the centre of the bird's head.
For each of these properties, there will be a few clusters which can be assigned average characteristics.
Thus, a bird's size may be
small
medium
large
These three values are the names of three clusters of birds' sizes.
Similarly, beaks may be
short
long and straight
curved
Again, these values are the names of clusters of beaks.
There are many species with short stubby beaks; some with very long beaks; and some with curved beaks (Figures 5.2, 5.3).
Clustering does not tell us why.
We know why.
Different bill shapes suit different eating habits, but never mind that for the moment.
Similarly, there will be a few significant clusters of wing shape.
Again, from our vantage point, we know that each is suited to a particular kind of flight and life style; but let us ignore that fact, and continue with the primitive analysis.
We can now assign to each species the various clusters which it is in.
Thus, a sparrow
is small;
has a short beak;
has a wide field of vision; and
has stubby wings.
A kestrel
is large (as birds go);
has a curved beak;
has eyes which look forward; and
has long wings.
These properties are discrete.
In any further analysis, we cannot take averages.
Let us try a different pseudo-metric on the space of species.
Say the distance between two species is the number of characteristics which they don't have in common.
Thus, the distance between a sparrow and a kestrel is 4.
(This distance function is a pseudo-metric rather than a metric, because there are different species with identical values for these four properties.
Sparrows and bluetits have the same four characteristics, so the distance between them is 0.)
If we look for clusters in this space of species, differentiated by coarse discrete characteristics, then we do indeed find quite striking clusters.
There are many small birds with short bills, short wings and all-round vision.
There is a small but distinctive group of larger birds with longer wings, hooked bills and forward vision.
These are the hunting birds, the owls, hawks and falcons.
There are also other possible groups of characteristics which are strikingly absent among real species — for example, there are very few large birds with short wings and short straight beaks.
This classification could be refined by adding more discrete properties, such as foot shape and habitat, and whether the bird is brightly coloured.
5.1.1 A Simple Clustering Algorithm
This particular algorithm has no great virtue.
It is presented here so that there is no confusion about the clustering process.
Its input is a finite set of examples (’ points’) in a space with a (pseudo) metric called d.
Its output is a partition of these points into subsets called clusters, and an association of some unique name with each subset in the partition.
Clustering algorithms often name their clusters.
Later, any other example can be allocated to a cluster by specifying that name.
A name is in effect an extension of the language for describing examples.
Associated with any cluster C, there is a number called its diameter.
This is the largest distance d (y, z) between any two points in C. We shall say that a point x is nearly central in C if for every point z in C, the distance d (x, z) is less than two thirds of C's diameter.
Form a list of all unordered pairs of distinct input points [{x, y}{x, z}{y, z}…]
Sort this list of pairs, in increasing order of the distance between the points.
WHILE the list is not empty take the first pair{x, y}from the remainder of the list and
IF neither x nor y is as yet in a cluster
THEN form a new cluster C.
To start with, C ={x, y}
IF one of the two points, say x, is in some cluster C
AND the other point y is not yet in any cluster
THEN add y to C.
IF x and y are in different clusters C1 and C2
AND x is nearly central in C1
AND y is nearly central in C2
AND d (x, y) < (diameter(C1) + diameter(C2)) /2
THEN amalgamate C1 and C2 into a new combined cluster.
(Otherwise, do nothing to the clusters.)
Note that the operations performed in this algorithm are not part of mathematics.
The first form a new cluster C.
To start with, C ={x, y}really means:
Invent a new name C;
Associate the name C with the set{x, y}.
Similarly, add y to C means
Associate C with the union of{y}and the set previously associated with C;
Forget C's previous association.
and amalgamate C1 and C2 into a new combined cluster means
Invent a new name C;
Associate C with the union of the sets previously associated with the names C1 and C2;
Forget the names C1 and C2.
The names may not be textual.
In this sense, a’ name’may be nothing more than an address in a computer's memory, such as a pointer in a Pascal data structure.
Although the naming process is basic to clustering, I shall not always say what the names are.
They are just the means by which one refers to clusters.
Kohonen's net algorithm does not invent a partition of the training set into clusters.
Rather, it invents a sort of simplified version of the training set: When the net has finished learning, the nodes in it form clusters of just a few points which each reflect much larger clusters in the training set.
Each node acts as a name for the set of examples near it.
At the end of this little statistical clustering algorithm above, every point is in some cluster of two or more points.
That may be undesirable.
Some points may be isolated, and are best left as such.
When x is in a cluster C and y is not in a cluster, then perhaps y should only be added to C if d (x, y) < twice diameter(C)
Also, the choice of the number two thirds, in the notion of being nearly central, is arbitrary.
This number could be anywhere between a half and 1.
It could even be dependent on the cluster: Say the number is k, so a point in C is nearly central if
A z n C d (x, z) <k * diameter(C)
Then k could be quite an elaborate function of C, such as This algorithm depends a bit too much on the sorting stage.
If two pairs{x, y}and{z, w}are the same distance apart, then the sorter could put them in either order.
Different choices of order may lead to different clusters.
This algorithm is an instance of bottom up clustering.
The algorithm begins with a few small clusters, and it enlarges them and creates new clusters as it is presented with more data.
There is a contrast between this sort and the so-called top down algorithms.
They start with all data in one large set, and they subdivide this set into smaller clusters.
Below, we shall study a top down algorithm which presents its output in a form called a’ decision tree’.
This particular algorithm is manifestly not incremental.
All the training data have to be collected and paired and then sorted before the main clustering stage can begin.
There are incremental approaches.
In outline, they proceed thus:
Start with an empty set of clusters.
WHILE there is still some training data x: = the next instance;
IF there is a cluster C containing points near x
THEN add x to C;
IF C is unbalanced
THEN subdivide C into new clusters C1 and C2
ELSE form a new cluster whose sole point is x.
The notion of an unbalanced cluster, and the operation of subdividing a cluster, are complicated.
We shall come across specific forms of them later.
This algorithm is incremental because it creates clusters from the first few data, and these early clusters can be used at once, even though they may be improved during later learning.
Clustering algorithms can also be distinguished by the natures of the clusters which they form.
An optimization technique constructs a partition of the training set.
Each component of the partition is a cluster, and the algorithm is intended to find clusters which are as far apart as possible.
Clumping techniques, by contrast, form clusters which may overlap.
Any one instance may lie in several clusters.
A hierarchical technique generates a tree whose leaves are labelled with individual instances.
Each internal node represents a cluster whose constituent points are the instances labelling the leaves below it.
5.1.2 Aspects of the Clustering Process
The clustering process can consist of as many as three stages:
1
adapting the representation of the training set;
2
finding extensional descriptions of clusters;
3
inventing intensional descriptions of clusters.
Adapting the representation
This is a case of adjusting bias.
With luck, you won't have to do it.
For species of birds, the data are given in a form which will make any natural clusters obvious.
A change of representation is necessary, though, in the case of a circular flock of starlings; for if the basic clustering algorithm were applied to the cartesian coordinates of the birds, then the information that they are in a circle would be lost.
Figure 5.4 shows three different examples of training sets.
The first is not suitable for clustering.
The points in it are distributed randomly, so it contains no information which could be learned by any process.
The second example has two clusters, and the basic algorithm will find them easily.
The third example is the hard case.
The distribution of points in it contains quite a lot of information, but the basic clustering algorithm will lose it because this algorithm will lump all the points into a single cluster.
However, the basic algorithm will find the right clusters if the example is transformed into polar coordinates centred at the place marked’ +’; for then, all the points on the circle are transformed onto a line which projects to a small region on the r-axis (Figure 5.5).
The difficulty is to find a good description language in which to express the learning situation.
I do not know of any recipe.
The human eye is exceptionally good at recognising patterns such as circles.
Computers are much less versatile.
Even if an algorithm embodied knowledge that perhaps the training set should be transformed to polar coordinates, still the algorithm would have to search a 2-dimensional continuum of possible centres.
This is a case of a more general problem:
The ease of solution of any task depends on the language in which it is expressed.
In this example, the possible description languages include all possible Cartesian frames of reference and all possible polar frames of reference, and perhaps a variety of other kinds of coordinate systems as well.
The task of finding a good language for clustering depends in turn on a good language for describing possible languages!
In the following examples, we shall always assume that the training set is already represented suitably.
Finding extensional descriptions of clusters
This amounts to finding clusters in the training set by an algorithm such as the one above.
When the data are presented suitably, it is easy.
Otherwise it is virtually impossible.
This stage produces a family of subsets of the training set, the clusters, but it does not identify each cluster except by stating which example is in which cluster.
Inventing intensional descriptions of clusters
An extensional description of a cluster just states what are its elements.
An intensional description provides a way to decide whether any particular thing is in the cluster by examining its properties.
One assumes that there is some class{x | p(x)}where p is a predicate formula which is true of elements in the cluster and false for all instances not in it.
Any object x should be assigned to the cluster if it is in this class, and not otherwise.
Usually, the purpose of clustering is to discover intensional descriptions, so that any new example not in the training set can be allocated to a cluster without doing the clustering all over again.
This classical view is sometimes too rigid.
One cannot always decide sensibly what is in the cluster and, more to the point, what is not.
Therefore, some people prefer to say that the defining predicate p is only a sufficient condition:
If p(x) then x is in the cluster but there may also be objects x, in the cluster, for which p is false.
I shall assume that all intensional descriptions are classical.
Intensional descriptions are usually invented by some sort of generalizing method.
However, there is a simple algorithm which invents intensional descriptions from raw clusters:
Given a finite cluster C, described extensionally, c: = a point in C for which max{d (x, c) | x n C}is least
D: = max{d (x, c) | x n C}
The intensional description of the class corresponding to C is then{x | d (x, c) s D}
The selected element c is called a prototype for C. The intensional descriptions which we shall come across do not involve prototypes.
Perhaps this is a weakness in our presentation of the subject.
Prototypes are often very natural ways to describe clusters.
In practice, stages 2 and 3 are often rolled into one.
An intensional description of a cluster is developed during the clustering process.
5.1.3 Statistical and Conceptual Clustering
Consider again the classification of birds.
It falls into two parts.
The first clusters according to numeric properties, and the second according to the names of the clusters invented during the first part.
These two levels of classification exhibit two quite different sorts of clustering.
The first level, which discovers general trends in the forms of wings and beaks, is a case of statistical clustering.
The other, which depends on discrete features, is often called conceptual clustering.
These are two extreme points on a spectrum of algorithms.
In practice, almost any useful algorithm incorporates aspects of both approaches, but for the moment, we shall explain them by emphasising their differences.
Statistical clustering is intuitively simple.
The learner observes extensional clusters of traits in examples, and may perhaps name them.
The learned information just makes these traits explicit, so that they can be used for prediction.
The traits themselves are basic observed numeric features.
The metric used for clustering is derived directly from these traits.
The term’ conceptual clustering’has been adopted because the class{x | p(x)}of an intensional description is often referred to as a’concept’.
There is a continuing discussion as to what a concept really is.
The concepts which we humans are aware of are certainly much more subtle; but let that pass.
Conceptual clustering involves more detailed analysis of relationships between traits.
Each cluster has an associated intensional definition.
For instance, if there are two functions f and g defined on the set of all possible examples, and f can take the value a and g can take the value b, then one such cluster might be defined as the set of all examples x for which f(x) = a ∘ g(x) = b
In the case of birds, f and g might be lengthofbeak and shapeofbeak.
The cluster defined by the property lengthofbeak(x) = short ∘ shapeofbeak(x) = straight includes sparrows and finches.
The learner's search space is a space of sets of definitions, in some suitable description language, and the goal is a set of succinct definitions which partition the training set suitably.
The properties of examples are not always described with functions.
For instance, one can either describe a species of bird with functions called lengthofbeak and shapeofbeak, so lengthofbeak(sparrow) = short shapeofbeak(sparrow) = straight or alternatively one can use binary relations called beaklength and beakshape: beaklength (sparrow, short) beakshape (sparrow, straight)
The second style is common in the literature.
Since definitions are tidier when they only involve discrete alternatives, the languages used for these definitions almost always contain only functions f with finite ranges.
One does not meet so-called conceptual clusterers which generate definitions of the form a<f(x)<b.
This feature of the language makes the clustering algorithms simpler.
The ease with which one can learn a concept depends critically on the structure of the predicate p, which in turn depends on the description language.
Let us assume that we have fixed on a particular language.
The concept is called conjunctive if p is a conjunct of atomic formulas, perhaps preceded by quantifiers: p p A…
E…
(A ∘ B ∘…
C)
If p contains the connective’ v’as well as’∘’then it is called disjunctive.
Many of the methods presented in later chapters only work with conjunctive concepts.
One virtue of clustering is that it can often invent disjunctive ones.
It is possible to imagine concepts in which A, B,…could be negated atomic formulas, but in practice, learning algorithms do not invent such concepts.
Note that the input to a statistical clusterer is a set of instances which are not classified in advance.
Perhaps the instances are all examples of some large class which we suspect may have meaningful subclasses, and the statistical clusterer should invent the subclasses; or perhaps there is no given initial class at all.
There is not usually any notion of a counter-example, when the clustering is statistical.
By contrast, the input to a conceptual clusterer consists of examples which have already been labelled with the names of the clusters they lie in.
The conceptual clusterer's input contains both examples and counter-examples of each concept.
Another essential difference between conceptual and statistical clustering lies in the form of the metric.
In conceptual clustering, the metric is not always made explicit.
Its clustering process is usually equivalent to straightforward clustering using a pseudo-metric d, where for any two examples x and y, d (x, y) = the number of functions f in the description language for which f(x) f(y)
This d resembles the Hamming distance.
When people speak of conceptual clustering, they often mean more than just a clustering process.
The possible properties of examples form a partially ordered set: Property P is less than property P' if A x (P(x) 6 P' (x)).
Conceptual clustering often generates a tree of properties.
The top node is the property which is true of all possible examples.
Each node defines a cluster.
The cluster defined by a leaf is a subset of the cluster defined by any node above this leaf.
Thus, the property f(x) = a ∘ g(x) = b might define a leaf below an internal node f(x) = a, and this in turn might lie below the nodes f(x) n{a, c, d}and true.
Given any such tree of concepts, it is easy to construct a pseudo metric whose clusters are the nodes of this tree.
Say, if two definitions p1 and p2 describe nodes n1 and n2 in the tree, then the distance d (p1, p2) is the number of edges on a shortest path in the tree from n1 to n2.
However, such metrics do not always appear to be natural.
Usually, conceptual clusterers employ the natural hierarchy of predicates directly, and make no mention of a metric.
Some conceptual clusterers invent more general partial orders, not just trees.
The term’ conceptual’arises because each node is a’concept’which is defined by some predicate.
A good conceptual clusterer is one which finds a succinct meaningful hierarchy of succinct definitions of meaningful concepts.
Both bottom up and top down methods have been used.
5.1.4 Ontological clusters
Clustering is easier when every function and predicate which we might consider applies to every conceivable training instance.
In the cases considered so far, this is so.
Every bird has a beak and wings, so it is meaningful to ask what are the lengths of beaks and wings for all birds.
If the training set also includes cats, dogs and cows, then we could not always ask for these values.
Suppose that there is a set called A of all functions and predicates which we might consider using in tests on instances.
Each subset S of A has an associated class of all conceivable things x which all the functions and predicates in S can be applied to:
C ={x | A fnS it is meaningful to ask, what is f(x)?}
If S is empty then C contains absolutely everything.
If S contains’ length of wing’then C includes all birds, but not cats and dogs.
C is the (extensional) ontological cluster associated with S.
Within a single ontological cluster C one can use all the elements of S to distinguish subclasses of C. When a training set TS is a subset of C, and all the tests made on instances use only functions and predicates from S, then the invented clusters are called simple.
All the clusters discussed above are simple.
Two simple clusters C1 and C2 are distinguished by some function (or predicate) f which takes a value v1 on all points x in C1, and a different value v2 on all x in C2.
Two ontological clusters C and C' are distinguished by some f which has values on all x in C but which is meaningless on all y in C'.
The subsets of A form a natural lattice.
The top element is the empty set, and the bottom element is A itself.
If S' c S c A, so S' is above S in this lattice, then the ontological cluster C is a subclass of C. Most clustering algorithms restrict attention to a single ontological class, but some try to construct a more elaborate hierarchy of concepts.
In simple cases, this hierarchy may consist of two layers.
The nodes of the upper layer are ontological classes.
The lower layer is a conventional classification consisting of simple classes.
Any simple class C1 lies below an ontological class C if all the functions and predicates used to describe C1 are in the set S for C.
Example: Classification by form of coat.
Suppose that we are classifying things according to parameters in a set A which contains the functions form-of-fur length-of-feathers
These two functions are only partial.
Birds do not have fur, and cats and dogs do not have feathers.
Therefore, the classification must have an ontological layer, consisting of the concepts above the dotted line.
The concepts below the line are simple.
There are classification tasks for which even this view is too elementary.
Suppose that we are classifying living creatures.
The top level of the classification is by’ genus’.
’ genus’is a function on animal species, whose values include mammal bird insect arachnid fish
The top few levels of such a classification would go
The complication in this example is that the concept birds appears to be simple, because it is the subclass of animals on which the function’ genus’has the value bird; but at the same time, the concept of birds is ontological because the function length-of-feathers is defined only on it.
In general, it seems that an ontological concept can occur below a simple one.
The lattice of concepts does not always fall neatly into two layers.
Whether we regard a concept as ontological or simple depends on our choice of description language.
This example is based on the assumption that A includes the functions form-of-fur and length-of-beak, but no function such as kind-of-coat.
If we choose a different set A, containing kind-of-coat but omitting form-of-fur and length-of-beak, and if the possible values for kind-of-coat are shaggy fur straight fur short feathers long feathers then the whole classification can be completed with just simple concepts because the function kind-of-coat is total.
5.1.5 Combining concepts and statistics
Having emphasised the differences between these two approaches, statistical and conceptual, we shall now see how they can be used together.
Statistics is the natural method for coping with noise.
In fact, it is almost the only way.
(The alternative is to guess that some training examples are erroneous, and leave them out of the training set.)
Statistical clustering depends on values which may be approximate or prone to error.
When you measure the sizes of birds' beaks, there may be some scatter among the readings.
Conceptual clustering cannot handle noise so easily.
In the conceptual case, each example's properties are supposed to be precise and discrete.
The prime example discussed by Stepp and Michalski involved classifying simple pictures of goods trains.
Their aim was to partition the set of pictures by the shapes of the engines and wagons.
The pictures show just a few shapes of wagon.
There is no possibility of blurring between them.
Similarly, Fisher and Langley use anatomical data to classify phyla.
Again, their data consist of clean small sets of attributes.
This situation is not very realistic.
In real life, blurring is common.
Among birds, where does one draw the distinction between short beaks and medium sized beaks?
This is a source of noise which the learner must tolerate.
When classification takes place in two stages, first statistically and then conceptually, then with luck all the noise will be eliminated in the first stage.
It can happen, though, that the first stage mis-classifies some instance.
Analysis during the second stage may suggest revision.
This actually happened while chemists were classifying the elements.
Dobereiner, Newlands and Mendeleev grouped the then known elements by chemical properties, and found that their masses formed a steady sequence, with very few exceptions.
Among the exceptions were chromium, indium, platinum and gold.
Mendeleev suggested that the given masses might be in error.
The experimenters repeated their tests, and found that their first results were indeed wrong, and Mendeleev was right.
Clustering is rarely used in isolation.
A learner is usually searching for some property which fulfils criteria which have nothing to do directly with the observed data.
This is particularly true of conceptual clustering.
In the case of birds, we know that meaningful clusters will be associated with birds' life styles.
In Chapter 7, on learning in rule based systems, we shall come across a technique called explanation based generalization (EBG).
This relates an example's features to its function and purpose.
Using EBG, one might observe just one species, a sparrow, and deduce that any bird with short wings and a short beak could survive comfortably on a diet of seeds and insects.
EBG would then suggest that there may be many bird species with these characteristics, and that they all have such a diet.
Such deductive methods complement observation and statistics.
Each approach on its own is somewhat unreliable.
The statistics may have low confidence levels, and the assumptions for deduction may be ill founded; but if they both lead to the same classification, then a learner can trust it.
What Mendel did was rather different.
His analysis was more like the algorithm embodied in one of the later versions of Bacon.
Given his statistical results, he conjectured the existence of some otherwise completely unknown feature — the genetic code.
Pat Langley calls such a conjectured feature an’ intrinsic property’.
For many years, until biochemists performed their wonders, there was no other way to justify Mendel's theory.
It had two foundations.
The first was its close fit with observed inheritance.
The other was, and is, its elegance.
In the rest of this chapter, we shall study five algorithms.
They are chosen because they are practical and valuable, and they are relatively clearly described in the literature.
All five show some aspects of both statistical and conceptual clustering.
They all classify according to discrete properties, but they also have statistical features and so they can all handle noisy data.
The first, due to Gerry Wolff, synthesises context-free grammars.
Such grammars are used to define languages, which themselves are necessary for describing other tasks.
The second builds decision trees.
Variants of it have been used in many situations.
The third, following work of Al-Mathami, builds somewhat more elaborate trees which can predict properties of examples which are not completely described.
The fourth constructs a graph of properties in which there is an arc between two properties if they are essentially correlated.
It was invented by Fung and Crawford.
The fifth algorithm, due to Scott and Markovitch, invents an efficient hierarchical classification of objects according to their behaviours under actions.
All these algorithms are examples of a class of clusterers studied by Buntine.
He has sketched a procedure for inventing classifiers in many learning situations, and he suggests a Bayesian approach which helps find good heuristic components for them.
Any clusterer should be founded on some kind of theory based on probabilities.
Among these five, the one with least justification is the grammar learner.
It is a hill climbing process which attempts to find a compact encoding of a language which is not too dissimilar from its training set.
Al-Mathami's method attempts to maximise the chance that its predictions will be correct.
the Fung-Crawford graph constructor chooses constructions with the greatest experimental backing.
The decision tree learner can be justified by a notion called entropy, which is also used in the theory of Boltzmann machines and in the Scott-Markovitch learner.
Entropy measures the expected surprise of a situation, and all these algorithms are designed to search for explanations which make the world appear more coherent and less surprising.
This notion will be explained further.
5.2 Inference of grammars
This case of clustering was investigated by Gerry Wolff.
His method invents a grammar for a written language.
In principle, it might be applied to English.
The grammar is used to describe the structure of a passage of text.
It consists of context free productions, which are rules of the form
A DDL X Y…
Z
The symbols X, Y,…on the right may be symbols which occur in the text (terminal symbols); or they may be other symbols which are invented for use just within the grammar (variables).
The symbol on the left of the arrow, A, is always a variable.
Example:
Say the terminal symbols are
Jane John likes chases dog ball the and the variables are
NP Proper Common Verb Art S
The productions might be
Proper DDL Jane
Proper DDL John
Common DDL dog
Common DDL ball
Verb DDL likes
Verb DDL chases
Art DDL the
NP DDL Proper
NP DDL Art Common
S DDL NP Verb NP
Given any symbol, A, we can speak of all the sentences generated from A by the grammar.
Here, the term’ sentence’does not mean what you were taught in school.
In this sense, a sentence can be any finite sequence of terminal symbols.
If A is a terminal symbol, then this set of sentences consists of just A itself.
Otherwise, A may give rise to lots of sentences:
Choose any production for A
A DDL X Y…
Z
For each symbol on the right side, choose some sentence generated from this symbol by the grammar (Sx for X, Sy for Y, etc.)
Concatenate all these sentences, in the order of their symbols in the production:
Sx Sy…
Sz
This combined sentence is now a sentence generated from A. We sometimes write
A DDL Sx Sy…
Sz
Example:
With the grammar above,
NP DDL John
NP DDL the ball
S DDL John likes the ball (Note: Such a grammar is almost the same as a Chomsky context free grammar.
The only difference is that in a CFG, one variable symbol is singled out and called the start symbol.
The only sentences generated by a CFG are from its start symbol.
Wolff originally described a way of learning CFGs.
The algorithm discussed here omits the start symbol.)
Each way of generating a sentence gives rise to a rooted tree, called its parse tree.
The root is labelled by the variable you generate from, A. All the internal nodes are labelled with variables, and the leaves are labelled with terminals.
At each internal node, labelled X say, there is one child for each symbol on the right side of the chosen production for X. These children are ordered in the same way as their labels are ordered in X's production.
Thus, a parse tree for
S DDL John likes the ball is
In this case, the grammar permits only one parse for this sentence.
The whole process can be turned round.
Rather than start from a symbol and find what sentences can be generated from it, one can begin with a sentence and a grammar, and find a way of generating that sentence.
This process is called parsing the sentence.
A parser is a program which takes as input a sentence and a grammar, and which constructs such a parse tree.
Grammars and parsers were invented for the study of human languages.
They are now also ubiquitous throughout computing.
They are basic to the growing subject of computational linguistics, which aims to let computers talk and make sense of human language; but before that arose, they proved invaluable for computer languages like Basic, Pascal and Prolog.
They have other applications too.
5.2.1 Wolff's algorithm
The input to the algorithm consists of
a text
a grammar, like the ones described above.
Its output is
another grammar extending the input grammar.
While processing, it also produces
a sequence of parse trees for abutting segments of the input text.
The input grammar may be completely empty to start with.
By repeating the algorithm cyclically, it can form ever larger, more comprehensive grammars.
The grammar found by Wolff's method is a bit special.
It has two distinct kinds of variables: AND symbols and OR symbols.
An AND symbol A has just one production, but otherwise it is unrestricted.
A DDL B C D…
An OR symbol X may have lots of productions, but they are all unit productions.
That is, each has only one symbol on its right side:
X DDL P
X DDL Q
X DDL R…
All these different productions are sometimes summarised by writing them thus on a single line:
X DDL P | Q | R |…
Each OR symbol acts as a name for the set of symbols{P, Q, R,…}on the right sides of its productions.
Wolff's algorithm is a clusterer because it forms and names these sets.
This form of grammar does not involve any essential loss of generality.
Any set of context free productions generates the same sentences as one of Wolff's grammars.
The algorithm begins:
Find a sequence of abutting parses of the input text, using the input grammar:
WHILE some text remains unparsed,
IF the grammar is inadequate for parsing any initial segment of the remaining text
THEN skip one terminal symbol of the text
ELSE find the longest possible initial segment of the remaining text which can be parsed with this grammar, and a parse tree for it.
Record properties of the resulting sequence of parses
There are two sorts of information recorded.
1
the sequence of top symbols of parse trees (For this purpose, a skipped terminal counts as such a top symbol.
If the algorithm begins with an empty grammar, then this recorded sequence of’ top symbols’is simply the original input text.)
2
within all parse trees, all the contexts in which each symbol occurs.
Suppose some symbol B occurs in a parse tree whose top symbol is A. Within this tree, B is the top node of a smaller parse tree.
Let us say, S is the sentence covered by the whole tree, and S is the shorter sentence generated within it from B. S is a substring of S. Say L is the part of S to the left of S, and R is the part to the right.
There are various notions of context for B in this parse tree.
Different versions of the algorithm employ different notions of context.
There is room for experiment here.
The simplest is the pair (L, R).
For some purposes, this is too crude.
L and R are strings of terminals.
If the algorithm is to do all we want, contexts should be allowed to include some variables too.
Suppose that X is an OR symbol generating S in the tree, and S is disjoint from S.
Say L' is formed from L by replacing each such string S in L by the variable symbol X which spans it.
Similarly, R' is got by substituting OR symbols for their spans in R. The second kind of context is the pair (L', R').
As the grammar grows larger and larger, with repeated cycles of the algorithm, the parse trees also grow.
When we seek a context for B, it may not be wise always to work within the largest parse tree containing B. The contexts found from large trees are too specific.
Instead, suppose the symbol A is taken to be an AND symbol; and furthermore, within the tree for A, all the ancestors of B (symbols between B and A) are also AND symbols.
For our parse, we can choose the largest which satisfies this condition.
Now within this tree, we can find the second kind of context, as above.
Suppose it is (L’, R’).
This is called the AND-tree context of B.
Once it has recorded features of the sequence of parses, the algorithm continues:
Revise the grammar
Build new AND symbols;
Fold new OR symbols;
Rebuild over-general OR symbols.
Building is simple.
The algorithm scans the sequence of top symbols of parses, and looks for pairs M, P which crop up together unexpectedly often.
Whenever it finds such a pair, it invents a new AND symbol, say A, and a production for it:
A DDL M P
Folding invents new OR symbols, and inserts them into previous productions.
If several symbols (say P, Q, R…) occur unexpectedly often in a context, then this step invents a new OR symbol X. Its productions are
X DDL P | Q | R |…
(There are variants on the folding process.
It can be made to depend on several contexts, not just one.)
Whenever a new OR symbol X is folded, the algorithm modifies every AND production which contains an alternative for X. In the right hand side of each such production, it replaces every occurrence of P or Q or R or…by X. Thus, the production
A DDL M P would become
A DDL M X
Wolff calls this process generalizing.
Rebuilding is necessary because folding sometimes groups too many symbols together as alternatives for a single OR symbol.
Maybe P and Q and R behave alike, but there may be times when the algorithm folds a set{P, Q, R}where, say, Q sometimes occurs in a context which never contains P or R. The algorithm looks for any context which contains some of X's children but not the others.
If it finds just one such context, then it forms a new OR symbol Y with productions
Y DDL P | R and it revises the productions for X:
X DDL Y | Q |…
The first stage, parsing the text, is a preliminary to learning.
It compiles information from which one can calculate a metric (although no explicit metric is ever actually calculated).
The main clustering activity is folding.
Rebuilding is a corrective measure, required because the clustering process is flawed.
Wolff's algorithm works well with artificial data.
It has also been tried with natural text, with some success.
It builds sensible AND symbols, but finding meaningful OR symbols is rather harder.
One notable feature of it is, it has succeeded in discovering recursive productions such as A DDL X l
X DDL A | a
Within this grammar, A can generate a whole family of strings al all alll allll alllll…
Building extends the grammar, by correlation; but it can also be looked on as a way of extending the vocabulary of the learner.
When the algorithm starts with an empty grammar, the only symbols it has are those in the text.
These are not enough.
It also needs to study short strings within the text.
Building gives it a way to refer to these strings.
In this algorithm, the clustering process is folding.
It depends on contexts in parse trees, and the algorithm can only calculate parse trees if it has enough AND symbols; so clustering depends on the set of available AND symbols.
Any initial restriction on the form of a problem is a kind of bias.
In Wolff's algorithm, the building stage adjusts the bias so that it is more suitable for folding.
There are some rigorous theorems which prove that context free grammars cannot be learned, in a certain sense.
However long a text you take, you can never be certain that a grammar you find by any algorithm is the one the text is generated by.
If we only allow ourselves plain text as input, the output is always something of a guess.
These theorems, due to Gold, will be discussed further in Chapter 8.
Besides that, some features of natural human languages cannot be captured by any context free grammar.
They require more subtle description.
In practice, people do succeed in learning to talk and understand each other.
There is an old argument between so-called rationalists and empiricists.
The empiricists believe that language should be learnable by experience, without any prior knowledge.
The only prior knowledge a baby has is what is in its chromosomes, and chromosomes do not encode that sort of knowledge.
Rationalists argue that this is impossible.
Language is just too subtle.
Never mind what we think is in chromosomes — if babies can learn to talk and listen, then some sort of knowledge must be in there somewhere.
Wolff's approach tends to the empiricist view.
It incorporates some knowledge, to wit the algorithm itself, but no more.
In real life, the situation is complicated by other factors.
For a start, nobody is quite sure what is English, or whatever language it is we are trying to learn.
Native speakers of English all think they know, but almost any two will differ somewhat in their usage and idiom.
One person's usage may be another