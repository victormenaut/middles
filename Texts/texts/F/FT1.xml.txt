

Editorials
ACE inhibitors after myocardial infarction
May benefit patients with left ventricular dysfunction
Studies in animals and humans have shown that giving angiotensin converting enzyme inhibitors after myocardial infarction reduces left ventricular remodelling, and this effect has been linked to benefits in clinical outcome.
The recently published results of three large randomised, double blind, placebo controlled trials focus on the effects of angiotensin converting enzyme inhibitors on mortality and morbidity caused by ventricular remodelling after myocardial infarction.
Ventricular remodelling comprises infarct expansion and global ventricular dilatation.
Infarct expansion, or thinning and lengthening of the infarcted myocardium, is seen within hours after myocardial necrosis and is usually complete within three weeks after infarction.
Global ventricular dilatation, which results from a progressive increase in myocyte length with sarcomere recruitment and resulting hypertrophy of the non-infarcted myocardium, continues for a long time.
Infarct expansion may result in cardiac rupture, aneurysm, and thrombus formation.
Global ventricular dilatation results in progressive increase in the volume of the ventricular cavity, which is a powerful predictor of mortality after myocardial infarction and a recognised precursor of symptomatic heart failure.
In the SAVE (survival and ventricular enlargement) and the SOLVD (studies of left ventricular dysfunction) prevention trials an angiotensin converting enzyme inhibitor was given to patients with asymptomatic left ventricular dysfunction with the aim of reducing mortality and the incidence of heart failure.
The SAVE trial enrolled 2231 patients who had suffered myocardial infarction, had ejection fractions equal to or less than 40%, and had no symptoms of heart failure.
The mean time after myocardial infarction to treatment was 11 days.
The SOLVD prevention study included 4220 patients, nearly 80% of whom had history of myocardial infarction, with ejection fractions equal to or less than 35%.
Entry into this trial (as opposed to the SOLVD treatment trial) required that the patient had no symptoms and was taking no treatment for heart failure.
These patients were randomly assigned to placebo or captopril (SAVE) or enalapril (SOLVD prevention).
The mean follow up was 42 and 37.4 months respectively.
In the CONSENSUS II (cooperative north Scandinavian enalapril survival study II) an angiotensin converting enzyme inhibitor was given within 24 hours of myocardial infarction in addition to standard treatment and irrespective of baseline ventricular function (though patients with clinical evidence of heart failure at the time of entry into the study were excluded).
This trial had the potential for identifying a distinct effect of angiotensin converting enzyme inhibitors on the early, infarct expansion, phase of remodelling.
It enrolled 6090 patients and assigned them to enalapril or placebo.
The mean time to treatment was 15 hours.
Interim analysis showed no reduction in mortality and an increased incidence of hypotension in the enalapril group, so the trial was discontinued.
The follow up varied from 41 to 180 days, and because the trial was stopped only 48% of the patients were followed up for six months.
In the SAVE study all cause and cardiovascular mortality were significantly reduced in the captopril group, by 19% and 21% respectively.
A reduction in the incidence of progressive heart failure was primarily responsible for the reduced mortality.
In the SOLVD prevention trial trends toward reduction in all cause and cardiovascular mortality in the enalapril group did not reach statistical significance.
In CONSENSUS II there was no significant reduction in all cause and cardiovascular mortality with treatment.
The lack of survival benefit was consistent in all predefined subgroups (anterior myocardial infarction, age over 70 years, and concomitant baseline treatment).
In the SAVE and SOLVD prevention trials development of symptomatic heart failure, progression of heart failure, rate of admission with heart failure, and incidence of recurrent myocardial infarction were all reduced in the treatment group.
In the CONSENSUS II study there was no significant reduction in any of these end points, although the rate of change of treatment due to worsening heart failure was significantly reduced in the treatment group.
Why did the SOLVD prevention and CONSENSUS II trials show no apparent benefit on mortality?
In the SOLVD prevention study nearly 19% of the placebo group received open label treatment with  angiotensin converting enzyme inhibitors during follow up and this crossover may have resulted in underestimation of survival benefit.
In addition, patients entering this study were more likely to have had no symptoms of heart failure than those in the SAVE study, in which treatment for heart failure was permitted.
The apparent lack of benefit of early angiotensin converting enzyme inhibitor treatment in the CONSENSUS II trial may have been related to the design or to theoretical issues.
Patients with non-Q wave, inferior, and previous myocardial infarction have a reduced risk of infarct expansion.
Thrombolytic agents might reduce the stimulus to remodelling by  limiting infarct size and augmenting blood flow to additional jeopardised zones.
Moreover, since the use of thrombolytic agents and aspirin in patients with myocardial infarction improves short and long term survival we would expect to have to give any additional treatment for a longer period to a larger number of patients to show a further reduction in mortality.
On the other hand, an adverse influence of giving angiotensin converting enzyme inhibitors early after myocardial infarction might result from systemic or direct myocardial effects of these agents.
Systemic hypotension may reduce coronary perfusion; in the CONSENSUS II trial there was an increased incidence of first dose hypotension in the enalapril group (10.5% v 2.5% in placebo group) and mortality was increased in patients who showed this effect.
Myocardial angiotensin II increases the rate of myocardial protein synthesis, promotes growth of myocytes, and promotes collagen expression by fibroblasts, and large increases in angiotensin converting enzyme activity are found in the scar tissue after myocardial necrosis.
In the short term these effects may serve to maintain cardiac structural integrity and normalise cardiac performance, in part by augmenting chamber compliance.
Thus angiotensin converting enzyme inhibitors may actually worsen clinical outcome by inhibiting an early protective effect of angiotensin II on cardiac structure.
Do the data from the CONSENSUS II study provide conclusive proof that giving an angiotensin converting enzyme inhibitor early produces no survival benefit?
The findings certainly highlight the risk of hypotension; elderly people and patients with inferior myocardial infarction have a greater propensity for this complication.
On the other hand, an echocardiographic substudy showed that the increase in ventricular volume was significantly reduced in the six months after myocardial infarction, presumably because of attenuation of remodelling — as supported by the data from the SOLVD prevention and the SAVE studies.
The factors that could potentially have been influenced by early angiotensin converting enzyme inhibitor treatment are infarct size and expansion, and we have no specific data on these.
Indeed, the effect of early treatment on infarct size and expansion may be seen only in subgroups of patients, and future studies should be directed at identifying such groups.
Though many issues remain to be resolved, data from the SOLVD prevention and SAVE trials provide compelling evidence that patients recovering from myocardial infarction with asymptomatic left ventricular systolic dysfunction (ejection fraction equal to or less than 40%) benefit from long term administration of an angiotensin converting enzyme inhibitor.
While routine, early administration of angiotensin converting enzyme inhibitors cannot presently be recommended in patients with acute myocardial infarction, this strategy needs further evaluation in high risk subgroups, especially patients with large, first anteroapical infarction.
The thrifty genotype in non-insulin dependent diabetes
The hypothesis survives
In 1962 Neel proposed that the persistence of diabetes mellitus must mean that the diabetic genotype holds some survival advantage.
Two decades later, after the clearer recognition of the distinction between insulin dependent and non-insulin dependent diabetes mellitus, Neel modified his hypothesis so that it related specifically to non-insulin dependent diabetes.
Despite some scepticism, the ‘thrifty genotype’ remains a convenient explanation for the extremely high incidence of non-insulin dependent diabetes that develops in many populations which have experienced rapid socioeconomic modernisation during this century.
The thrifty genotype is thought to have offered a survival advantage to individuals in hunter-gatherer and early agricultural societies, who were subject to periods of nutritional hardship, by favouring fat deposition during periods when food was abundant.
In modern times, when physical activity has decreased and calories are in constant supply (usually in an energy dense form high in fat and simple carbohydrates), the genotype has become disadvantageous and favours the development of obesity and non-insulin dependent diabetes.
Central to the need for the thrifty genotype hypothesis is the assumption that, when expressed, non-insulin dependent diabetes was associated with diminished fertility (at least before the advent of modern medicine).
Certainly, a reproductive handicap has recently been shown in young Nauruan women with early onset non-insulin dependent diabetes.
Whether or not humans with a thrifty genotype did in the past (or continue to) better survive periods of famine has not been directly proved.
This assumption rests largely on the observation of high susceptibility to non-insulin dependent diabetes in populations known historically to have experienced such episodes.
North American Indians, Polynesian and Micronesian Pacific islanders, Australian Aborigines, and Asian Indians are usually considered in these terms, but it is not known whether all human populations previously subject to severe ‘feast and famine’ conditions have a thrifty genotype.
Recently some migrant African and Chinese populations have also been found to be susceptible to non-insulin dependent diabetes, and people of European origin might be the only true ‘low susceptibility’ group.
As Weatherall suggested in his 1992 Harveian oration, this implies either that the positive selective forces favouring the thrifty genotype were not as strong in Europeans or that the negative selective forces of a sedentary, well nourished lifestyle have operated for sufficiently long to diminish the frequency of the genotype.
Whatever the case, non-insulin dependent diabetes in Europeans tends to occur in middle or older age, after completion of the reproductive phase in women, and contrasts with the early age of onset and severity of the disease observed in populations such as Nauruans.
Experimental support for the thrifty genotype comes from work in rodents showing that strains susceptible to obesity and diabetes have lower metabolic rates, defective thermoregulatory mechanisms, and an enhanced ability to store food as fat and to survive prolonged fasting.
Prospective studies in Pima Indians indicate that ‘thrifty’ lower metabolic rates in humans may favour weight gain.
The metabolic expression of the thrifty genotype favouring non-insulin dependent diabetes would appear to be via selective tissue insulin resistance and relatively high basal and stimulated insulin concentrations.
Under modern conditions these promote obesity and a vicious cycle of increasing insulin resistance and compensatory hyperinsulinaemia, leading ultimately to pancreatic β cell decompensation and frank diabetes.
Adapting data from known insulin resistant states such as obesity and non-insulin dependent diabetes, O'Dea has recently theorised on the nature of metabolic adaptations which might have favoured survival in Australian Aborigines in the context of their traditional hunter-gatherer diet, where ‘feasts’ on wild animals such as kangaroos were characteristically high in protein and relatively low in fat and carbohydrate.
In this case, selective insulin resistance in liver would promote conversion of dietary protein to glucose and fat via active hepatic gluconeogenesis, which is not sensitive to insulin suppression, and lipogenesis, which is insulin sensitive.
Meanwhile, fat accumulation would be encouraged by insulin sensitive adipose tissue and the relative resistance to glucose uptake in skeletal muscle.
This concept of selective insulin resistance is not new and has been well demonstrated in animal models.
In a recent publication Wendorf and Goldfine have related apparent variation in susceptibility to non-insulin dependent diabetes among North American Indian tribes to the timing of migrations into the continent.
They have hypothesised that a ‘thrifty genotype’ may have been selected around 10000–11000 years ago in a subgroup of ancestral Indians with a particular reliance on vanishing big game species such as mammoths.
While difficult to prove, the development of such hypotheses at least indicates that researchers are attempting to justify reliance on the thrifty genotype to explain variation in non-insulin dependent diabetes frequency.
In Pima Indians a considerable body of work favours a selective insulin resistance in muscle as the metabolic expression of the thrifty genotype.
This includes the demonstration that insulin action appears to aggregate in families and that its distribution is compatible with a single gene codominant mode of inheritance.
None the less, except for specific mutations to key enzymes in rare syndromes of familial diabetes and the recently identified link between some kindreds of maturity onset diabetes of the young and mutations to the glucokinase gene in liver and β cells, the gene(s) causing the bulk of cases of non-insulin dependent diabetes remain obscure.
While a single defect may be responsible in small unique populations such as Pima Indians and Nauruans, in larger high prevalence populations a polygenic causation is most likely.
It would be too much to expect convergent evolution to have produced a single thrifty genotype in all populations by chance, and the most likely situation is that several genes have been selected in different combinations, in different populations, to produce a phenotypically similar syndrome.
The search for the genes responsible for non-insulin dependent diabetes continues and may one day offer the option of high risk or even general population screening, and perhaps specific gene manipulation therapies.
However, even now the epidemiological evidence for the disadvantages of a sedentary lifestyle and Western diet in causing current epidemics of obesity and non-insulin dependent diabetes in the developing world provides a compelling basis for promoting primary prevention of these diseases.
Certainly, preventive strategies and modern medicine will tend to counteract negative selective forces and will maintain the frequency of the thrifty genotype in human populations, but, as Neel soberly pointed out, ‘efforts to preserve the diabetes genotype through this transient period of plenty are in the interests of mankind,’ for some time in the future we may again be glad to have it.
Deprivation payments
Still awaiting change
Deprivation payments were introduced with the 1990 general practitioner contract with the intention of putting more resources into practices where deprivation might be expected to increase the demand for services.
Laudable though the aim might be, the results have been much criticised.
Arguments over using the ‘Jarman index’ to assess demand for services and the method of payment have obscured the principle underlying the payments.
This was that a proportion of the pool funding for General Medical Services would be redirected to practices in high demand, deprived areas.
Using scores derived from the Jarman underprivileged area index for this purpose seems to have face validity — the index was developed from a survey of one in 10 British general practitioners who were asked to weight social factors according to how much they thought they increased workload.
Using census data, scores were derived for areas of the country either on the basis of ward or enumerator district (with average populations of 5327 and 450 respectively) and extra money is provided to practices with a higher proportion of factors perceived by a representative sample of doctors to equate to high patient demand.
Critics have argued that no evidence exists that Jarman's weighted factors actually create extra workload; indeed, some state that doctors in high scoring areas may actually work fewer hours.
Such arguments are themselves spurious since general practitioners are able to ration their workload despite potential demand by controlling access through measures such as rapid consultations, short surgery hours, or other barriers to visits.
They can thereby create patient expectation of the level of demand that will be serviced.
The actual workload of doctors does not necessarily reflect accurately the potential demand for their services, as unmet need is not taken into account.
Calculating any extra resource using an index based on doctors' own perceptions of what creates work seems the more valid approach as these will presumably be factors deterring doctors from working in underprivileged areas or causing them to limit access so as to avoid overwork.
Despite almost a decade of critical review and the existence of alternative formulas for identifying deprivation the Jarman index remains as good an indicator of potential demand for general practitioner services as exists.
Major problems, however, arise over its application.
Census data are used to calculate the scores, but these may be as much as 12 years out of date(current data were collected in 1981; scores based on the 1991 census are expected later this year).
Averaging scores to a ward can exclude payments to practices in deprived parts of otherwise affluent wards and put unnecessary resources into practices in pleasant parts of underprivileged wards.
Graduating payments and particularly the high score needed to trigger any payment are problems.
Most important is this extra income might not result in any attempt to improve care.
The Department of Health has talked about refining elements of the general practitioner contract.
Basing the deprivation payments on the Jarman index still seems the best option, although it is time the Department of Health supported research into its validation.
Such studies should focus on the workload in comparable practices — for example, practices in high and low deprivation areas with similar levels of commitment.
Considerable fine tuning of the eligibility criteria is needed.
Payments could be limited to a maximum number of patients per general practitioner which would stop the protests that a single handed general practitioner with a list of 3000 in an area of high deprivation receives an extra £28 800 a year without any obligation to provide extra services.
That maximum should be as low as 1300 to 1500 patients per full time principal.
Perhaps the most important function of deprivation payments was to compensate practices with low lists after the introduction of the new contract, which linked income much more closely to capitation.
The payments should not encourage high lists in areas of deprivation.
Eligibility according to ward is a further problem that using enumeration districts would partially overcome, although a much greater margin of error (approaching 50%) comes from converting patient postcodes to enumeration districts than to wards.
Perhaps some of the total expenditure on deprivation payments could be reserved by the family health services authorities for appeals, providing that the money was ‘ring fenced.’
Authorities could, within clear guidelines, make discretionary payments — for example, to practices on the boundaries of ‘deprived’ wards who could show that they would receive payment if enumeration districts were used.
Similarly, practices could appeal if they had evidence that the population characteristics had altered substantially since the last census.
Furthermore, it could be possible to update locally certain factors in the Jarman index more regularly, especially unemployment rates, which change so rapidly.
Lowering of the entry point for payment from the 30 point on the Jarman scale — Jarman has suggested that 16 is more appropriate — and the use of more payment bands might be considered.
This would increase the proportion of patients attracting some deprivation payment to their practice from the current 9% to about 20% of the population.
Current total spending on deprivation payments could be maintained despite increasing the eligibility for payments if the average per capita fee was lowered.
Spreading the benefits too thinly, however, might undermine the viability of the most needy practices.
An important priority is to ensure that extra income is spent on extra activity.
Limiting maximum eligible patients per doctor would help, as would requiring some broad declaration by the practice as to how the sum would be spent (such as will occur with the revised health promotion payments).
Practices receiving substantial payments should be expected to employ sufficient ancillary staff — for example, nearer two full time equivalent staff per doctor rather than the one to one ratio so often seen.
Pressure could also be exerted on practices to adopt 10 minute consultation bookings, given the relation between length of consultation and quality of performance.
It is, however, difficult to see how ambitious practice aims could be realised from payments made and therefore subject to change on a quarterly basis.
We need to canvas opinion and collect data from innovative practices in deprived areas to find out how payments have been best used to support their greater patient demand.
Clearly without the deprivation payments some good inner city practices would have been bankrupted by the new contract.
Unfortunately, because of anomalies in the system, the needs of other deserving practices are still not being met.
Improving London's health service
Now comes the hard part
After a century's diagnosis of the ills of London's health service many in Britain will welcome almost any prescription that stops 15% of the population consuming 20% of NHS resources.
But for those living and working in London the decisions made last week by the government (see p 537) have to face a harder test.
Will they improve or worsen the standard of care given to Londoners — and to a wider group of patients who use London's specialist services and benefit from its research and teaching?
Many people have been disappointed that last week's decisions were not more precise — defining, for example, exactly what should happen to St Bartholomew's or Charing Cross Hospital.
But London's pattern of hospitals is such a historical muddle that no one can wave a wand and transform everything overnight.
Restructuring will inevitably entail compromises, and it is important that those who have to make the compromises work should be involved in fashioning them.
In general Mrs Bottomley has set clear boundaries and timetables for the decisions and told the relevant authorities to work out their own salvation within them.
In return she should ensure that her mechanisms for protecting research and education are robust enough to enable London's hospitals to operate fairly within the internal market.
The boundaries make sense, but there is an air of indecent haste about the timetables.
Even though many of Sir Bernard Tomlinson's recommendations, now endorsed by the government, worked with the grain of changes already occurring, some of the timetables are short.
The specialty reviews, for example, have to assess current and projected needs, define criteria for tertiary services, and advise on the most cost effective and clinically effective locations — all by the end of May.
This work will not be done in a vacuum, but for a problem that has existed for at least a lifetime a few more months of thought might lead to better outcomes, and ones that will be more readily accepted.
Likewise, the agent of change might have been better devised to give more confidence in the outcome.
At present the London Implementation Group smells too much of hole in the corner deals: the working part of the group consists only of two named people, one of them a former Thames regional chairman, and the ordinary members of the specialty review committees (see p 589) and of the all important Primary Health Care Forum had not yet been announced as we went to press.
The group might knock heads together but it doesn't seem designed to do the equally important job of explaining what is happening to staff and patients.
And Londoners need explanations and reassurance.
Over the past few months they have been told that their primary and community health care is awful; now the government tells them that accident and emergency departments will shut, along with 2000-odd hospital beds, when their immediate evidence is that it is hard to get into hospital.
Likewise, the implementation group's human resources subgroup sounds too low key to convince staff that the NHS will tackle the problems of maintaining morale and preserving skills while changes go on.
Part of the problem for Londoners is that the notion of transformed primary and community care, breaking down the boundaries between hospital and community, is still unclear.
The government has accepted Tomlinson's recommendation for investment in premises and has set up an initiative zone to encourage new ideas.
There are plenty of good ideas around, but the worry is that the sheer difficulty of practising in inner cities will overwhelm even the most enthusiastic practitioners and managers and that they will ‘settle for more of the same…but out of better buildings.’
Much will depend on the leadership and vision of the Primary Health Care Forum.
Perhaps the biggest gap in both the Tomlinson report and Making London Better concerns research and postgraduate teaching.
The government has already decided to bring special health authorities into the internal market.
It also wants a market in research, and it promises a mechanism for funding the excess costs of academic teaching and clinical research.
But there is little inkling of how the mechanism will work, and we have to wait until December for the management executive and the Department of Education to come up with ideas.
They are badly needed: as Green has said, ‘It would be easy for clinical research to become the first and indeed the greatest casualty of reductions in size in London.’
This is particularly true since London has a pressing need for a shift in acute beds towards those serving elderly Londoners with multiple diseases and away from younger patients with interesting single diseases (J James, MSD Foundation symposium, 1992).
London may just be carrying too big a burden of research and teaching for its population — but the means by which research and education get redistributed, rather than simply shut, remain unclear.
It is easy to criticise the government's decisions: after a century of inertia, making changes in London was always going to be difficult, and Mrs Bottomley deserves credit for getting the process started.
But she needs to remember that deciding strategies is easy; implementing them is hard.
She has given a lot of commitment to the strategy; she now needs to give as much to the process of change and to ensuring that the public understands it.
Londoners have for long been perversely proud of a health  service that does not serve them well.
They now have a chance of a better one — but they need to be able to recognise that.
Treating bed wetting
Bladder exercises, star charts, enuresis alarms, and now ERIC
Bed wetting is not the most glamorous of paediatric problems; treating it, however, can be highly rewarding because treatment is so often successful.
There are an estimated 500000 sufferers aged between 6 and 16 in Britain.
From reports of management within enuresis clinics it is reasonable to predict that 400000 might be curable.
Nocturnal enuresis alone, particularly if primary, is both a disorder of maturation and a genetic trait.
Compared with controls, patients are three to four times more likely to have a parent who had been enuretic.
They are also more likely to have excessive diffuse slow background activity on electroencephalography and are significantly shorter than controls.
Minor neurological dysfunction is common, including mild hypotonia, problems with coordination or fine manipulation, and mild dyskinesia.
Many such ‘clumsy children’ have learning problems, emotional difficulties, and problems with relationships.
These may be provoked by the lowered self esteem that many suffer as a result of teasing and criticism by peers, parents, and teachers.
Within this group the wetters do even worse.
Add to these handicaps serious life changes or stress, such as parental separation, and the setting is complete for the misery, embarrassment, and shame of bed wetting.
Daytime and mixed day and night wetters have certain aetiological differences from night wetters, although there is much overlap: they are more likely to have had an adverse perinatal history, their bladder capacity tends to be smaller, and they are more likely to suffer urinary tract infection.
It has been suggested that their urological dysfunction may be a marker of perinatal neurological damage, but in a study of 191 wetters those with dyssynergic bladders and increased residual urine were also those with the most severe behavioural problems.
The most effective treatment is dry bed training and an enuresis alarm.
Devlin and O'Cathain found that of 122 children consecutively referred, 22 became dry after a structured interview (with the child alone as well as with a parent), advice on personal hygiene and cleanliness training, daily bladder exercises, and provision of a ‘star chart.’
These authors supplied a further 96 children with an enuresis alarm; 81 achieved initial success (42 consecutive dry nights) at a mean of 7.3 weeks (range 1–26 weeks).
Eleven required a second try, of whom nine achieved dryness.
Success is more likely when the problem is maturational and less likely when there is psychiatric disorder of the child, severe family stress, absence of concern by child and parents, urological dysfunction, and developmental delay.
Other poor prognostic signs may be coexisting day wetting and maternal intolerance of the enuretic child.
Drugs have a minor role: tricyclic antidepressants have little effect and are not suitable to be kept in households where there is a combination of young children and stress.
Oxybutynin is unhelpful except, perhaps, in children whose enuresis is part of a syndrome including frequency, urgency, dysuria, and covert bacteriuria.
Desmopressin, 20–40 mg nightly intranasally is effective during treatment in 12% to 70% of cases; only up to 31% remain dry after the course of treatment.
There is no long term advantage of three months' over one month's treatment.
It may be of most value for occasional use, in known responders, to save embarrassment when sleeping away from home.
It must not be prescribed without a careful history; if the child's real problem is not enuresis but psychogenic polydipsia, desmopressin may provoke water intoxication and hyponatraemic fits (RAF Bell et al , personal communication).
Patient and parent support groups are often valuable in paediatric practice.
The Enuresis Resource and Information Centre (ERIC) is a registered charity providing advice and information to children, parents, and professionals.
It runs a confidential telephone counselling service and publishes material for teenagers, parents, and others.
Taking a leaf from the book of other lobby groups, it is in the process of publishing guidelines on minimum standards of clinical practice and offering a consultative service to purchasers on their implementation.
PAPERS
Interaction and dose equivalence of salbutamol and salmeterol in patients with asthma
Abstract
Objective —
To examine the pharmacological interaction of salmeterol and salbutamol and to derive an estimate of dose equivalence of salmeterol for airway and systemic effects in patients with asthma.
Design —
Randomised double blind crossover study.
Subjects—
12 patients with mild asthma.
Intervention —
Placebo or salmeterol 50, 100, 200 µg given on separate days followed two hours later by inhaled salbutamol in cumulative doses up to 3600 µg.
Main outcome measures —
Change in forced expiratory volume in one second (FEV 1 ), heart rate, plasma potassium concentration, QTc interval, tremor amplitude, and creatine kinase myocardial isoenzyme concentration.
Results —
Compared with placebo, the mean (95% confidence interval) changes in FEV 1 and heart rate after salmeterol 200 µg were 0.61 (0.32 to 0.90) l and 7.0 (3.8 to 10.2) beats/min.
Adding salbutamol caused a large increase in FEV 1 after placebo (0.69 l) with progressively smaller changes after increasing doses of salmeterol (0.19 l after salmeterol 200 µg).
Heart rate and QTc interval increased and plasma potassium concentration decreased roughly in parallel on the four study days with a suggestion of convergence at higher doses of salbutamol.
Geometric mean dose equivalences for salmeterol 50 µg and 100 µg compared with salbutamol were 4.9 and 7.8 (mean 6.4) for FEV 1 and ranged from 7.1 (2.9 to 17.0) to 12.6 (4.4 to 36.4) for heart rate, plasma potassium, and tremor (mean 9.5).
Conclusions —
The effect of adding salbutamol to salmeterol is largely additive.
Weight for weight salmeterol may be up to 10 times more potent than salbutamol.
Considering its longer duration of action salmeterol 50 µg twice daily could be equivalent to salbutamol in doses up to 500 µg four to six hourly.
Introduction
Salmeterol, a new long acting β 2 agonist, produces bronchodilatation for at least 12 hours in patients with asthma and when given in doses of 50 µg twice daily provides more effective control of asthma symptoms than salbutamol 200 µg four times daily.
Whether this difference represents an important additional effect of salmeterol or is simply due to a higher relative dose is uncertain.
The dose equivalence of salmeterol compared with salbutamol determined from single dose studies of acute changes in lung function has ranged from 1 to 16 — that is, doses of salmeterol from 12.5 µg to 200 µg have produced similar bronchodilatation as 200 µg salbutamol.
This shows the insensitivity of single dose comparisons as a method of estimating relative potency.
The importance of knowing the dose equivalence of β 2 agonists is highlighted by the recent experience in New Zealand, where the excess mortality associated with fenoterol has been attributed to marketing of a metered dose inhaler that contained a two to four times higher relative dose of fenoterol compared with salbutamol.
It is recommended that salmeterol be taken regularly twice daily and that a shorter acting β 2 agonist such as salbutamol be added for additional relief.
The effect of combining the two agonists, however, has received little attention.
Salmeterol is a partial agonist in vitro compared with salbutamol and could therefore reduce the access and effectiveness of salbutamol by occupying βreceptors.
We studied the effect of adding increasing doses of salbutamol to salmeterol 50, 100, and 200 µg to examine the drugs' interaction and estimate dose equivalence.
Subjects and methods
We studied 12 subjects (three women) aged 18–54 years with asthma but no other medical problems.
All were lifelong non-smokers, had a normal electrocardiogram, a baseline forced expiratory volume in one second (FEV 1 ) greater than 60% predicted, and at least a 15% rise in FEV 1 after 400 µg inhaled salbutamol.
All were taking an inhaled short acting β 2 agonist as required and nine a regular inhaled steroid.
Subjects gave informed written consent and the study was approved by the Nottingham City Hospital ethics committee.
FEV 1 was measured by dry bellows spirometer with the subject seated and the better of two successive measurements was recorded.
Heart rate, QTc interval, tremor, and plasma potassium were measured as described; plasma creatine kinase concentration by the N-acetyl cysteine activated method; and creatine kinase myocardial isoenzyme by immunochemical separation (Isomune-CK, Roche, Welwyn Garden City, Hertfordshire).
Subjects were studied at the same time of day on four occasions at least seven days apart.
Inhaled β 2 agonists and drinks containing caffeine were withheld for 12 hours before each study.
After 20 minutes' rest baseline heart rate, QT interval, tremor amplitude, FEV 1 , and plasma potassium and creatine kinase myocardial isoenzyme concentrations were measured.
Subjects then received placebo or salmeterol 50, 100, or 200 µg by metered dose inhaler according to a randomised, crossover, double blind design (by Latin square randomisation).
Two hours later inhaled salbutamol was given in doses of 100, 500, 1000, and 2000 µg at 20 minute intervals, to give a cumulative dose of 3600 µg.
Doses of salbutamol other than the 100 µg dose were administered by putting multiples of 500 µg into a spacing device (Volumatic, Allen and Hanburys, Uxbridge, Middlesex) and asking the  subject to inhale deeply three times.
Measurements were taken in the same order at baseline, two hours after placebo or salmeterol, and 15 minutes after each dose of salbutamol.
Symptoms were documented before each dose of drug.
The study was stopped if symptoms became severe or if heart rate rose above 140 beats per minute.
Patients stayed in the department until symptoms settled and heart rate had fallen below 110 beats per minute.
analysis
The study had 90% power to detect a difference in heart rate of 6.5 beats/min according to our previous study.
Baseline, maximum changes, and final measurements on the four study days were compared by analysis of variance with the generalised linear interactive modelling (GLIM) statistical package and expressed as means with 95% confidence intervals.
Symptoms were compared by the χ 2 test.
Dose equivalence was estimated from the salbutamol dose which caused the same effect as that seen with each dose of salmeterol.
This was done by plotting each person's response to each dose of salmeterol on the log dose response curve after placebo and salbutamol for each index.
Raw data were used and geometric means and 95% confidence intervals were calculated with the confidence interval analysis (CIA) statistical package.
When the change with salmeterol was above the highest or below the lowest change seen with salbutamol the highest or lowest salbutamol dose was used in the analysis as a censored value.
Results
Baseline, FEV 1 , heart rate, plasma potassium concentration, QTc interval, tremor amplitude, and creatine kinase concentration (including myocardial isoenzyme) did not differ significantly at the four visits (table I).
All patients completed the full study on all four occasions.
Salmeterol caused a largely dose dependent increase in FEV 1 , heart rate, QTc interval, and tremor amplitude and a fall in plasma potassium concentration (table I, figure).
Salbutamol caused a large increase in FEV 1 after placebo and progressively smaller increases after salmeterol 50, 100, and 200 µg (table I, figure).
Heart rate and tremor amplitude increased and plasma potassium concentration decreased roughly in parallel on the four study days (figure).
No significant difference was found in the maximum change seen for any of the measures on the four study days (table I).
Creatine kinase concentration (including the myocardial isoenzyme) did not increase significantly after salmeterol or salbutamol.
When salmeterol was compared with salbutamol the geometric mean dose equivalence for FEV 1 , heart rate, plasma potassium concentration, QTc interval, and tremor amplitude ranged from 3.1 to 12.6 (table II).
The dose equivalence tended to fall with increasing doses of salmeterol indicating that the responses were tending to plateau with the 200 µg dose.
The geometric mean dose equivalences for the 50 and 100 µg doses were 4.9 and 7.8 (mean 6.4) for FEV 1 and ranged from 7.1 to 12.6 (mean 9.5) for heart rate, tremor, and plasma potassium.
The dose equivalence for QTc interval was consistently lower than that for other measures ranging from 3.1 to 5.8 (table II).
One patient developed lower right chest pain and nausea 48 hours after receiving salmeterol 200 µg, which was attributed to biliary colic.
Other symptoms (tremor, headache, and ‘others’) occurred more often with salmeterol 200 µg (five, four, and 16 subjects respectively) than with salmeterol 100 µg (one, two, 10), salmeterol 50 µg (one, two, six), and placebo (one, two, eight), though the differences were not significant.
Discussion
The British National Formulary recommends that salmeterol is taken twice daily and that a shorter acting β 2 agonist such as salbutamol be added for further relief.
Since few studies have examined the efficacy  and safety of combining salmeterol and salbutamol we examined the effects of adding salbutamol to salmeterol on the airway response and potential side effects in patients with mild asthma.
Measurements were taken when maximal changes in airway and systemic responses would be expected.
A 200 µg dose of salmeterol was included in addition to the recommended doses of 50 and 100 µg to extend the dose range of salmeterol.
Salmeterol caused a largely dose dependent increase in airway and systemic effects, although only salmeterol 200 µg increased tremor amplitude and QTc interval.
The addition of salbutamol caused a roughly parallel fall in plasma potassium concentration and increase in heart rate, tremor amplitude, and QTc interval confirming a largely additive effect; the dose response curves for salbutamol seemed to converge at higher doses.
Although more potent than salbutamol on human airways, salmeterol is a partial agonist achieving about 70% of the maximum effect seen with salbutamol.
When a partial and fuller agonist are combined the partial agonist, by occupying receptors, acts as a partial antagonist, thus reducing the effectiveness of the fuller agonist.
Salmeterol could therefore reduce the efficacy of salbutamol when the two are given in combination.
In vitro, the dose response curve for a fuller agonist in the presence of a partial agonist is shifted to the right with an elevated baseline and a tendency for the curves to converge and cross over with increasing dose.
Experiments in vitro permit maximum stimulation of βreceptors and relaxation of smooth muscle, which are unlikely to occur in vivo.
Although the suggestion of convergence of the dose response curves for salbutamol is in keeping with an interaction between an agonist and a partial agonist, our data suggest that this interaction is unlikely to be large enough to cause a clinically important reduction in beneficial or adverse effects of salbutamol in patients taking salmeterol.
Such an interaction, however, remains possible in patients with severe asthma.
dose equivalences
This is the first study to compare the beneficial and adverse effects of increasing doses of salmeterol and salbutamol.
Single dose comparisons of salmeterol 50 µg and salbutamol 200 µg have generally shown similar effects on FEV 1 , heart rate, and tremor, leading to the view that 50 µg salmeterol is roughly equivalent to 200 µg salbutamol.
One study looking at the protection provided by the two drugs against histamine induced bronchoconstriction suggested a more than fourfold difference in potency.
Our study allowed us to estimate dose equivalence from the dose response findings with salbutamol and salmeterol.
The response to cumulative doses of salbutamol was compared with that to non-cumulative doses of salmeterol.
The estimate of dose equivalence for FEV 1 is probably conservative since the airway response to salbutamol is greater with cumulative doses.
The use of a spacing device for the higher salbutamol doses increases the amount delivered to the airways and might also underestimate dose equivalence since airway and systemic effects of β 2 agonists are mainly due to inhaled drug.
The dose equivalence for plasma potassium concentration, tremor, and particularly FEV 1 fell with increasing doses of salmeterol as would be expected if the responses were reaching a plateau.
The values for the 50 and 100 µg doses of salmeterol (generally between 8 and 10) may therefore be better estimates of dose equivalence.
The lower dose equivalence for QTc interval remains unexplained but may reflect compensatory mechanisms or greater β 2 selectivity of salmeterol.
Salmeterol seems to be up to 10 times more potent weight for weight than salbutamol for effects on heart rate, plasma potassium, and tremor.
Thus for peak effects salmeterol 50 µg is equivalent to up to 500 µg salbutamol.
The dose equivalence over 24 hours is more difficult to estimate but since salmeterol has a longer duration of action salmeterol 50 µg twice daily may be equivalent to salbutamol 500 µg four to six hourly.
Our findings have two important implications.
Firstly, studies comparing salmeterol 50 µg twice daily with salbutamol 200 µg four times a day are not comparing equi–effective doses and differences between the two drugs may be due to differences in dose rather than differences in drug.
Secondly, high doses of β agonist are associated with an increased risk of death and epidemics of death from asthma have been associated with marketing of high doses of β agonists.
Whether the relation between high doses of β agonists and mortality is causal remains uncertain but until the question is settled the high dose of salmeterol relative to salbutamol must raise some concerns.
Further studies are needed to address the dose equivalence of salmeterol and the optimum dose for long term treatment.
Mycobacterium tuberculosis DNA in tissue affected by sarcoidosis
Abstract
Objective —
To investigate the prevalence of Mycobacterium tuberculosis DNA in granulomatous tissues from patients with sarcoidosis and from controls matched for age, sex, and tissue by using the polymerase chain reaction.
Design —
Single blind control trial.
Subjects —
16 patients with sarcoidosis who had undergone diagnostic biopsy of lung, skin, or lymph node and 16 patients with squamous cell carcinoma or Hodgkin's disease to act as controls.
In addition, four lung specimens infected with M tuberculosis were included as positive controls.
Results —
M tuberculosis DNA was present in sarcoid tissues containing granulomas from seven of the 16 patients and one of the 16 matched controls.
Two of the four specimens known to be infected with M tuberculosis were positive in the controlled experiment.
Conclusion —
These figures suggest that M tuberculosis DNA is detected as readily in patients with sarcoidosis as in patients with frankly tuberculous tissues and imply that M tuberculosis may be linked to the cause of sarcoidosis.
Introduction
Despite considerable circumstantial evidence that the causal agent in sarcoidosis is likely to be a mycobacterium no conclusive association that might influence treatment has yet been shown.
Both are multisystem granulomatous disorders, and patients commonly present with such similar symptoms that sarcoidosis was once held to be a variant form of tuberculosis.
In addition to the clinical and histological similarities between the two diseases, there is much evidence from immunological, epidemiological, and microbiological studies to suggest a common cause.
Patients, however, still have no specific treatment option and rely on being in the majority of patients who spontaneously recover.
For the remainder with pre-fibrotic lung infiltration, about 7% will progress to respiratory failure, right ventricular failure, and death within 10 to 15 years.
One reason for the controversy surrounding the association between sarcoidosis and Mycobacterium tuberculosis is the absence of acid fast organisms on routine Ziehl-Neelsen staining of sarcoid tissues.
Yet it is now becoming well recognised that pathogens may exist in forms without cell walls and may generate ‘slow bacterial infections’ in which the organism will not be easily cultured.
The tubercle bacillus has long been recognised to exist in various guises and seems able to exist interchangeably with and without its cell wall.
When a concerted and invasive effort has been made to find acid fast rods in sarcoid tissue they seem to be present, and acid fast bacteria without cell walls and tuberculostearic acid have also been isolated from lesions of patients with sarcoidosis.
With the advent of the ultrasensitive polymerase chain reaction for detecting DNA in clinical samples the opportunity to unravel the debate should be available.
The polymerase chain reaction was pioneered for diagnostic use, and the detection of as few as two genomes of M tuberculosis in clinical samples is now possible.
This technique has been successfully applied to lung washings from patients with sarcoidosis, and M tuberculosis DNA was detected in half of the samples.
In addition, a further 20% of such patients had non-tuberculous mycobacterial DNA in their lung washes.
A third of controls with negative cultures, however, were also positive for M tuberculosis complex.
This result could have been due to the inclusion of mostly elderly patients in the control group who may have been harbouring M tuberculosis from a primary asymptomatic tuberculous infection, which was common in their youth.
Another disadvantage of this study was that bronchial washings may not always sample only those parts of the lung specifically affected by the disease.
We therefore set out to design an experiment with a younger, more appropriately matched control population.
In addition, samples containing granulomatous lesions from patients with acute disease should be most likely to contain a causative pathogen.
By using archival specimens embedded in paraffin to produce a large and immediate specimen pool, perfectly matched samples from the time of diagnosis can be easily generated.
We therefore applied these techniques to archival tissues containing sarcoid granulomas and further investigated the viability of this widely available tissue source in research based on polymerase chain reaction techniques.
Patients and methods
patients
Samples from 36 patients who had undergone surgery and biopsy at the Middlesex Hospital between 1988 and 1991 were used.
Sixteen of these were from patients with sarcoidosis who underwent biopsy of the lung (nine patients), lymph node (six patients), or skin (one patient).
The diagnosis was therefore based on the histological presence of non-caseating granulomas in the tissue and failure to culture M tuberculosis in addition to the clinical and radiological findings.
Sixteen control subjects were matched for age, tissue, and racial origin (table I).
In addition, four patients' lymph nodes seen to contain acid fast bacilli on Ziehl-Neelsen staining were included as a positive control of DNA extraction and sample inhibition in our study samples.
M tuberculosis had been cultured from one of these.
For all tissues the preservative method was immersion in buffered neutral formalin for up to 24 hours followed by embedding in paraffin.
preparation of sample dna
Sections of 25 µm were cut from each block with a sterile no touch technique, and new blades were used for each to reduce contamination.
These sections were then placed in sterile 1.5 ml Eppendorf containers and allocated a code number by the histopathology department at Middlesex Hospital.
The samples were then analysed at Surrey University, where they underwent deparaffinisation, DNA extraction, and purification before polymerase chain reaction techniques.
Four sections were cut from each block and analysed separately.
Deparaffinisation was performed as described elsewhere.
The samples were incubated in xylene at 56°C for 10 minutes and vortexed.
They were then washed once in 80% ethanol and once in acetone and left to dry for 1 hour.
To avoid contamination sterile reagents were used with new disposable pasteur pipettes for each addition and removal of washing solution.
The steps were carried out in a fume hood subjected to ultraviolet light and hydrochloric acid washes between experiments.
The samples were dried with loose lids on.
To detect any contamination of samples before breaking the code, negative controls of sterile water were included with the samples in a ratio of one negative control per three samples and treated identically at each stage.
DNA extraction — Sterile water (50 µl) was added to each sample, which was then boiled for 30 minutes and a 3:2 (vol:vol) ratio of autoclaved 1 mm and 0.1 mm glass beads added to half fill each Eppendorf container (Stratech Scientific, Luton).
After vigorous beating for 5 minutes in a minibead beater (Stratech Scientific, Luton) the tubes were pierced at the bottom with a 16 gauge sterile needle and nested in a 1 ml microfuge tube.
This was placed in a sterile universal tube and centrifuged at 14000 revolutions per minute for 4 minutes as previously described to elute sample DNA.
The sample was then purified by using diatoms and guanidinium thiocyanate, but the procedure was adapted by washing only once with each reagent, thereby reducing cross contamination and still abolishing sample inhibition.
The absence of inhibiting components was confirmed in a separate experiment.
The sample was finally resuspended in 50 µl of TRIS-EDTA buffer.
polymerase chain reactions
Two sets of primers were used.
IS 6110 is specific for the M tuberculosis complex and is present in multiple copies in M tuberculosis , in a single copy in M bovis BCG , and in 1–5 copies in M bovis .
It has been validated for early diagnosis of tuberculosis.
Primers were designed to amplify a 305 base pair product as INS 1: 5' CGTGAGGGCATCGAGGTGGC 3' and INS 2: 5' GCGTAGGCGTCGGTGACAAA 3'.
Primers were made from a section of mycobacterial specific (65 kilodalton gene, TB1 and 2)groEL gene which is conserved among known mycobacteria but not specific for M tuberculosis .
They produced a product of 625 base pairs: TB 1: 5' GAGATCGAGCTGGAGGATCCGTACG 3' and TB 2: 5' GCGGATCTTGTTGACGACCAGGG 3'.
Amplification was performed as described elsewhere but with a 25 µl reaction volume with 5 µl of sample added.
The amplified products were electrophoresed through a 1.5% agarose gel stained with ethidium bromide and viewed under ultraviolet light.
Positive controls of 12.5 fg and 36 fg of M tuberculosis DNA were run respectively for IS 6110 and 65 kilodalton primers to reflect their differing sensitivities.
For greater sensitivity we then performed Southern hybridisation and probing.
By blotting the gel containing amplified product on to a positively charged nylon membrane and probing (or ‘hybridising’) this with a heterologous labelled strand of DNA the sensitivity of detection can be increased 10-fold.
Thus a gel which appears to contain no product may reveal the presence of 10 fg of mycobacterial DNA, equivalent to less than 2 genomes.
This procedure was performed for all specimens and both DNA products.
Probes were prepared by random oligonucleotide labelling with digoxigenin of the products of the polymerase chain reaction electrophoresed through a 1.5% agarose gel, excised, and purified with Gene-Clean (Bio 101, La Jolla, California).
statistics
We used double classification χ 2 testing to assess the significance of the differences in positivity for the patients with sarcoidosis and controls.
Results
The specificity of the polymerase chain reaction primers has previously been validated but the sensitivity was reassessed.
We found we could detect 12.5 fg M tuberculosis DNA with the IS 6110 primers, but only 36 fg with the 65 kilodalton based primers.
Both products required Southern hybridisation and probing for detection.
Our earlier technique of preparing samples for polymerase chain reactions from bronchial washings entailed a simple single tube protocol that minimised handling and the possibility of contamination.
When this was applied to tissue sections, however, most samples were found to inhibit the polymerase chain reaction.
To overcome this problem we used a DNA extraction procedure with guanidinium thiocyanate and diatom particles to purify the DNA.
This procedure was found to abolish inhibition of the polymerase chain reaction for all specimens examined, as confirmed by spiking M tuberculosis DNA with 5 µl of each purified sample, performing polymerase chain reaction techniques, and detecting amplification as described.
Four sections from each block were analysed separately.
Tissue homogenisation, DNA extraction, and polymerase chain reactions were performed in batches of 10 samples.
To detect contamination occurring during preparation of samples, negative control samples containing only water were treated exactly as the tissue samples and examined by polymerase chain reaction techniques.
One negative control sampled was processed for every three tissue samples examined.
Despite stringent precautions taken to avoid contamination in some batches of samples processed occasional negative control samples gave positive amplification on the polymerase chain reaction.
We also found that, although samples from the same section gave consistently positive or negative amplification, serial sections from the same tissue block sometimes gave discordant results, presumably due to non-uniform distribution of mycobacteria within the tissue block.
We therefore applied the following criteria to define a positive result.
Any sample for which a single section gave positive amplification for the IS 6110 primers in an experiment in which all negative controls in the batch were negative was recorded as presumptive positive for M tuberculosis complex DNA.
Additionally, a presumptive positive result was recorded for any sample if three out of four of the sections examined were positive for the IS 6110 polymerase chain reaction.
As M tuberculosis DNA should give positive amplification with the 65 kilodalton polymerase chain reaction we confirmed each positive result by amplification with this reaction.
A final positive result was recorded only if the sample was positive with both the IS 6110 and the 65 kilodalton polymerase chain reactions in an uncontaminated experiment.
The code was then broken, and the results are presented in table II.
Three of the six lymph nodes affected by sarcoid and  four out of nine of the lung specimens affected by sarcoid were positive for both M tuberculosis complex specific DNA and the sequence from the 65 kilodalton gene.
Only one of the control subjects was similarly positive, and this sample was from the upper lobe of a lung with clear apical capping suggestive of old asymptomatic infection with M tuberculosis .
Neither of the skin specimens was positive, and sections of lymph node from matched controls were never positive with either set of primers.
Of the four infected tissues containing acid fast bacilli, two were positive for IS 6110 and 65 kilodalton sequences.
These included the only sample positive on culture for M tuberculosis .
Discussion
We have presented data showing that a significant proportion of granulomatous tissues from patients with sarcoidosis contain M tuberculosis DNA (p<0.01).
This applies to both affected lung and lymph nodes.
Only one control tissue fulfilled our criteria for positivity.
These results agree with those recently presented of tests that examined bronchial lavage fluid, in which half of patients with sarcoidosis were positive for M tuberculosis DNA.
The sensitivity of 50% for samples of tuberculous tissue roughly concurs with the results from much larger studies on various clinical samples that give a sensitivity for the polymerase chain reaction of 37–80% and culture of 29–67%.
The positive controls in our experiment that were negative for the polymerase chain reaction were also culture negative, and our apparent low sensitivity is consistent with previous work on clinical samples.
This is disappointing for such a theoretically sensitive technique, but we found that tissue itself potently inhibits the polymerase chain reaction if not adequately purified, and despite our spiking experiment we may not have abolished sample inhibition.
The incorporation of a modified template may have detected this.
This study has confirmed our previously reported prevalence of such DNA in sarcoid samples but had only 6% ‘false positivity’ compared with our lavage study, in which 32% of elderly control patients were positive for M tuberculosis DNA.
Our one positive control specimen was lung tissue taken from a carcinoma in a region of apical fibrosis in a patient aged 45 years.
This patient may have had asymptomatic M tuberculosis infection in this classic anatomical site, as is well recognised in patients with chronic bronchitis, in whom it may be unmasked by treatment with steroids.
Other studies that used similar techniques for diagnostic purposes give a similar genuine false positive rate.
We attribute our low rate of false positivity to the use of age matched controls.
Other recent research is consistent with ours.
Mycobacterial rRNA has been shown to be raised in sarcoid spleens.
Other authors have recently cultured sarcoid skin lesions and grown mycobacteria that on sequencing appear to be M avium spp.
Not all studies using molecular techniques have produced evidence for M tuberculosis as the pathogen in sarcoidosis.
Bocart et al found mycobacterial DNA in only two of 16 tissues affected by sarcoid, and an accompanying editorial to their paper suggested this was strong evidence against a link.
We suspect, however, that the reason for the low positivity in their experiment could be due to differences in sensitivity, and this study failed to include positive control tissue infected with tuberculosis to detect this.
In our experience the technique used for tissue disruption and DNA extraction is critical to the success of subsequent DNA amplification.
Mechanical disruption of tissue and microbial cells is far more efficient in our hands than enzyme methods.
We used glass beads for mechanical disruption and included tissues infected with mycobacteria as positive controls for DNA extraction and sample inhibition.
We estimated that each positive sarcoid section had as few as six genomes by comparison with positive control DNA samples, and Southern hybridisation was essential for detection.
In addition, the viable DNA in tissue embedded in paraffin decreases according to the nature and duration of exposure to preservative.
The different techniques used in hospital pathology departments before samples are tested may, therefore, affect results.
Our results suggest that M tuberculosis DNA is associated with tissues affected by sarcoid with a similar prevalence to tissue infected with M tuberculosis .
Taken in conjunction with the wealth of circumstantial evidence based on epidemiological, immunological, and microbiological data, the implication that M tuberculosis is the infecting organism in this disease is becoming convincing.
The results of trials of antituberculous drugs have been discouraging, but these are generally small studies that use older regimens based on isoniazid, aminosalicylic acid, and streptomycin.
In addition, the tubercle bacillus in patients with sarcoidosis may be present in the form without a cell wall as a slowly growing bacterial infection so agents directed toward the cell wall mycolic acid such as isoniazid would be ineffective.
In addition, the bacteria may exist in a ‘stationary phase,’ which is well recognised in other genera.
These points may also explain their failure to grow in culture media but do raise the possibility that another, as yet unidentified organism, may be involved.
In conclusion, we have demonstrated the presence of M tuberculosis DNA in sarcoid lung and lymph tissue and shown that archival specimens seem to be suitable for research based on polymerase chain reaction techniques.
The implications of this for any study in which sample recruitment is time consuming or invasive are obvious.
The evidence is now sufficient to justify a new approach that uses the polymerase chain reaction to identify patients infected with M tuberculosis .
A multicentre controlled trial of modern antituberculous drugs may then be performed in patients positive for the polymerase chain reaction to assess the potential for a specific, non-steroidal, curative treatment.
Demographic and social characteristics of adults with cystic fibrosis in the United Kingdom
Abstract
Objective —
To obtain information about social and demographic characteristics and lifestyle of adult patients with cystic fibrosis, including those who do not attend major specialist clinics.
Design —
Confidential self completion postal questionnaire to adult patients with cystic fibrosis, asking about social and demographic characteristics, social class and occupation, employment, education, insurance and social security benefits, symptom severity, and medical care.
Setting —
National association for adults with cystic fibrosis.
Subjects —
1052 adult members of the Association of Cystic Fibrosis Adults UK, accounting for 68% of those with cystic fibrosis in the United Kingdom population over 16 years of age and over 80% of those over 25 in June 1990.
Results —
The response rate was 82% (397 women, 423 men).
Most adults with cystic fibrosis were found to be living fulfilling lives into adulthood.
Significantly fewer men were married or cohabiting than women (110 (26%) men, 175 (44%) women).
420 (55%) responders were working, and of these 235 (56%) had less than two weeks' sick leave a year.
Half of those not employed gave ill health as the reason.
Revealing that they had cystic fibrosis at job interviews reduced likelihood of being employed for those with mild to moderate disease.
People with cystic fibrosis had been less successful than the general population in achieving O level or equivalent qualifications, but more successful in achieving A level or higher qualifications.
Achievement of any qualifications enhanced employment prospects irrespective of disease severity.
Conclusion —
Contrary to an image of chronic ill health and disability, a high proportion of adults with cystic fibrosis are living full and productive lives.
Introduction
Cystic fibrosis is the commonest serious inherited disease among white people, affecting 1 in 2500 live births in the United Kingdom.
After its recognition in 1936 and its description as a clinical entity in 1938, the prognosis has improved from a few months of life to a median age of survival of over 20 years.
Over a third of cystic fibrosis patients are now over the age of 15, and the number of patients is increasing by 100 to 120 patients a year, most of whom are adults.
Considerable rises in the number of adult patients are forecast by the year 2000.
There have been few studies of adults with cystic fibrosis with respect to employment, education, housing, insurance, and other features of lifestyle, and most such studies have been based on patients attending large clinics, who may be unrepresentative of the total population with the disease.
Patients attending the cystic fibrosis clinic in Melbourne were found to have a good outlook with respect to employment and social life, and a study at the Brompton Hospital characterising clinical and social features of patients attending its cystic fibrosis clinic found a similarly good outlook.
Our study aimed to describe the social and demographic characteristics of adults with cystic fibrosis in the United Kingdom and the medical care they receive and to provide baseline data from which the impact of changes recently introduced into the National Health Service can be assessed.
Subjects and methods
subjects
All 1052 people with cystic fibrosis aged over 16 who were alive and known to the Association of Cystic Fibrosis Adults UK (ACFA) on 1 July 1990 were studied.
Adults with cystic fibrosis become known to ACFA by transfer from the Cystic Fibrosis Research Trust's mailing list when they reach the age of 18, or by direct referral.
Many paediatricians inform the research trust of new patients with cystic fibrosis, with their family's consent, on diagnosis.
ACFA membership is biased towards patients attending larger treatment centres where there is awareness of the cystic fibrosis trust, but it contains a substantial proportion of patients who would not be included in a study based on large clinics.
Estimates of the number of adults with cystic fibrosis in different age groups alive at the middle of 1990 were obtained from the United Kingdom Cystic Fibrosis Survey (J Dodge, personal communication).
The ACFA mailing list represents approximately 68% of patients with cystic fibrosis over the age of 16 in the United Kingdom, and over 80% of those aged 25 or more.
methods
Data were collected by a self administered multiple choice questionnaire covering social and demographic characteristics, employment, education, life insurance, social security benefits, general practitioner services, and hospital care.
Severity of disease was assessed by calculating percentage of predicted weight for height and by a standard symptom questionnaire.
Breathlessness, cough, sputum, abdominal discomfort, and fatigue were each rated on a five point scale ranging from none to very severe, and a summary score was calculated as the average of these five symptoms.
The score was strongly associated with other measured indicators of health status such as percentage of predicted weight for height, unemployment due to ill health, receipt of medically assessed benefits, use of health services, and patient's own perception of disease severity.
Symptom scores were normally distributed for both sexes around a mean of 2.20 for men and 2.39 for women.
The questionnaire was mailed by the ACFA executive officer at the Cystic Fibrosis Research Trust with a reply paid envelope and explanatory letter.
Confidentiality was maintained by using a reference number system.
Non-responders were sent two further copies of the questionnaire with reminders.
Age, sex, and regional health authority of residence were obtained from the ACFA mailing database for all non-responders after the final mailing for comparison with responders.
analysis
All questionnaires were coded by two people.
Social class was assigned using the patient's occupation, the father's if the patient was not employed, and the mother's if no other occupation was available.
On this basis, social class could be assigned to 98% of subjects.
Data were analysed with the Epi-Info and BMJ Confidence Intervals Analysis statistical packages.
95% Confidence intervals for single proportions and the differences between two proportions were calculated with the normal or exact methods where appropriate.
Where analogy is made to the general population, comparison figures are taken from Social Trends .
Results
response
Questionnaires were returned by 866 subjects.
Not all respondents answered all questions.
Analysis for each question was based on responders to that question.
There were 186 non-responders, giving a response rate for ACFA members of 82%, and for the whole British population with cystic fibrosis of 56% (table I).
The response rate increased with age and was significantly higher in women (397, 85%) than men (423, 79%)(95% confidence interval for difference 1.1% to 10.6%).
The median age of responders was 22 and the maximum age was 67 for men and 56 for women (fig 1).
There was no significant difference in age or region of residence between responders and non-responders.
household details
Overall, 237 (56%) men and 191 (48%) women with cystic fibrosis lived with their parents, this difference being significant (difference 0.3% to 14.0%).
The proportion living with parents fell with increasing age, from 82% of those under 20 to 15% of those over 30 and none of those over 40.
A total of 34% (31.1% to 37.8%) of cystic fibrosis adults were married or cohabiting, compared with 61% in the general population over 16 years of age.
Women were significantly more likely to be married or cohabiting than men, with 44% of women and 26% of men in such relationships (difference 11.2% to 24.5%).
In the general population in 1987, 11% of households contained a non-dependent child, defined as a child over 16, or over 19 if in full time education or training up to this age.
In this study, in 52% (48.6% to 55.3%) of households the subject was a non-dependent child.
educational achievements
More responders left school without any qualifications (116/800, 15%; 12.2% to 17.4%) than in the  general population (11%).
The proportion was higher in manual social classes (58/198, 29%) than in non-manual social classes (55/683, 8%, difference 14.6% to 27.9%) and was related to disease severity.
In those without qualifications the mean symptom score was 2.59 compared to 2.36 in those with qualifications (difference 0.087 to 0.375).
However, 188/730 26% (22.6% to 28.9%) of responders had A level qualifications or their equivalent (fig 2) compared to 19% of the general population, and this was also related to symptom score, the mean in those with A levels being 2.26, and without 2.44 (difference 0.053 to 0.287).
In all, 348/742 (46%; 42.8% to 50.1%) responders had taken a further qualification after leaving school (fig 2).
Twelve per cent of men and 12% of women were graduates, compared with 13% of men and 6% of women in the general population.
There was a significant social class difference (53% from non-manual social classes had higher qualifications compared with 32% in manual social classes; difference 13.1% to 28.7%).
There was no significant difference in severity of disease between the groups who had obtained and had not obtained higher qualifications.
Those with qualifications of any sort were more likely to be in paid employment (59% of those with school leaving qualifications, 37% of those without; difference 16.7% to 34.9%) at all levels of disease severity.
employment
At the time of the questionnaire 468 (54% (50.3% to 57.4%)) responders were in paid employment compared with 69% of the general population over 16.
Men were significantly more likely to be employed than women (60% v 51%; difference 2.0 to 16.1) and the proportion in employment rose with age to 40 years and decreased thereafter (table II).
Weekly hours worked by adults with cystic fibrosis were not significantly different from those worked by the general population.
Severity of disease as measured by symptom score was significantly higher in those who were unemployed due to ill health (table III).
Over half (235, 56%) of 419 employed adults with cystic fibrosis had had less than two weeks' sick leave in the previous year, and 34% (144) had had less than 1 week.
Only 6% (24) had had more than 12 weeks' sickness absence.
Absence was not related to age, sex, or social class.
Of those who were not in paid employment, 30% (106) were students.
Of the remainder, 70% (182/352) gave ill health as a reason.
The proportion of students decreased and the proportion who cited ill health increased with age, and there were significant sex differences in the reasons given for unemployment — more women than men were not in paid employment because of either ill health or homemaking (table III).
At job interviews, 77% of respondents (585/757) tell the lay interview panel that they have cystic fibrosis, either always or sometimes.
Those who said they never revealed that they have cystic fibrosis were more likely to be employed (64% (110/172)v 53% (311/585), difference 2.5% to 19%), than those who revealed their cystic fibrosis at job interviews, but the questionnaire did not ask about responses to confidential medical questionnaires.
This difference was significant for those with mild to moderate disease but not those with severe disease.
Another household member was not working because of the cystic fibrosis of 63/737 (9%) respondents, of whom 37 (58%) were parents, 13 (21%) partners, and 8 (13%) siblings or other relatives.
Of the respondents, 393 (45%; 41.8% to 48.3%) gave an occupation other than housewife for their mother, whereas 69% of women aged 45–60 in the general population are employed.
social class and occupation
Table IV shows the social class distribution for those adults with cystic fibrosis who are employed.
In comparison to the general population, significantly higher proportions of patients with cystic fibrosis of both sexes were following non-manual, particularly clerical, occupations.
In comparison to their parent of the same sex, those who responded to the questionnaire were more likely to be engaged in non-manual occupations (difference 4.2% to 28% for men and 2.7% to 28.7% for women).
Discussion
This study is the first to provide data on lifestyle from a large sample of adults with cystic fibrosis outside a major clinic: although inevitably influenced by survival effects, the survey shows that a substantial proportion of patients with cystic fibrosis now live fulfilling and productive lives well into adulthood.
Both the sample and the responders are likely to be biased in favour of those of higher social and educational status.
No other appropriate sampling frame was available outside major clinics.
Although responders may be biased towards independent, articulate individuals in employment, unemployed people may have had more time to reply, and this is supported by a higher response rate among women, who had a lower employment rate.
The possibility of sampling bias and responder bias should be borne in mind when considering the rest of the results.
Overall, adults with cystic fibrosis seem to have difficulty in achieving independence, with only one third married or cohabiting, and a higher proportion than in the general population remain in the parental home.
Men were significantly less successful in this respect.
Men may have more difficulty with delayed physical maturity, and in fulfilling the traditional male societal role of ‘provider’ when faced with chronic medical problems.
The proportion of responders achieving independence from parents rose with age and suggests that independence is delayed rather than prevented by the presence of cystic fibrosis.
Nevertheless, the high proportion who are still dependent on their parents may have important implications for housing and health and social services needed for those who remain at home as their parents become elderly and frail.
The employment record of adults with cystic fibrosis is good.
On average, both sexes achieve 80% of the employment rate in the general population over 16 years, with no reduction in working hours and only a minority reporting prolonged sick leave.
To some extent this must represent a survivor effect, with those in better health entering employment.
Ill health is an important cause of unemployment, particularly with increasing age.
The high proportion of adults with cystic fibrosis in non-manual occupations may reflect selection or response bias.
They may, however, choose non-manual occupations because they are unable to fulfil more physically demanding jobs, and their preference may also reflect trends towards clerical, service, and information technology jobs within society as a whole.
One finding of concern is the increased likelihood of adults with cystic fibrosis who said they never reveal their cystic fibrosis at job interviews being employed, compared with those who said they reveal their cystic fibrosis sometimes or always.
Those who do not reveal their cystic fibrosis are less severely affected and may be better able to disguise their disease to lay observers.
But even those with mild and moderate disease were more likely to be employed if they did not reveal their cystic fibrosis, suggesting that doing so places the job seeker at a disadvantage.
Failing to reveal cystic fibrosis may be disadvantageous or even illegal.
It is important to educate employers with facts so that discrimination is prevented.
People with cystic fibrosis might be advised to confine information about their disease to confidential medical questionnaires.
People with cystic fibrosis are less likely to achieve O level or equivalent qualifications than the general population but more likely to have A levels, and women with cystic fibrosis are more likely to have degrees.
Those who did not obtain O level or A level qualifications had more severe disease.
These findings may reflect survival bias, with severe disease resulting in lack of qualifications and premature death, the survivor population going on to get higher qualifications.
However, the finding that school leaving qualifications are associated with disease severity highlights the importance of ill health at this age and suggests the need for flexibility in the education of those who suffer health problems during study for examinations.
Different study models of reduced intensity could be considered for those experiencing health difficulties.
This is important because qualifications of any sort enhance subsequent employment independent of disease severity.
The success of adults with cystic fibrosis in conducting their lives, education, relationships, and employment has been bought at some cost to the family.
Although fewer than one in 10 identified another family member as unable to work due to their cystic fibrosis, the employment rate among mothers of adults with cystic fibrosis is less than that expected in women aged 45–60.
This suggests considerable and largely unrecognised sacrifice by mothers of people with cystic fibrosis.
When cystic fibrosis was first described it was a fatal disease of children.
Increasingly, adult patients are achieving independence, careers, and impressive educational attainments.
This study has identified some areas in which adults with cystic fibrosis may need extra help, notably in obtaining school leaving qualifications and for young men to achieve independence.
It has also shown that treatment of children with cystic fibrosis has produced adults who are fit, active, and integrated for most of their lives.
Clinical diagnosis of pyloric stenosis: a declining art
Abstract
Objective —
To assess whether diagnostic imaging of pyloric stenosis has made a difference in rapidity of diagnosis, duration of pre-operative hydration, and length of stay in hospital.
Design —
Chart review of infants with confirmed diagnosis of pyloric stenosis.
Setting —
Paediatric teaching hospital.
Subjects —
215 infants with a confirmed diagnosis of pyloric stenosis seen during 1974–7 and 187 infants with pyloric stenosis seen during 1988–91.
Main outcome measures —
State of hydration on admission, time between presentation to hospital and a definite diagnosis, techniques used to make the diagnosis, whether a pyloric mass was felt before or after an imaging procedure, time between presentation and surgery, and length of stay in hospital.
Results —
The average age at presentation was 40 days in both groups.
A pyloric mass was palpated either without or before an imaging study in 87% (187/215) of cases during 1974–7 but in only 49% (91/187) during 1988–91.
The use of barium meal examination and ultrasonography increased from 20% (42/215) of cases during 1974–7 to 61% (114/187) during 1988–91.
There were no significant differences between the two groups in the time taken to establish a diagnosis, the mean duration of preoperative treatment, or the length of stay in hospital (after allowance for the decline in average length of stay of all hospital patients between the two periods).
Conclusions —
An increased use of diagnostic imaging for pyloric stenosis did not lead to earlier diagnosis or better management.
While imaging is important in identifying pyloric stenosis in difficult cases, increasing reliance on imaging has reduced doctors' skills in diagnosing pyloric stenosis clinically.
Introduction
Hypertrophic pyloric stenosis occurs in about 3/1000 live births.
Few other paediatric illnesses are as gratifying to diagnose and as uniformly responsive to treatment.
The diagnosis can be made clinically by careful abdominal examination after a suggestive history has been obtained.
Visible gastric peristalsis is a supportive finding, while palpation of an olive shaped mass in the right upper quadrant of the infant's abdomen is diagnostic.
The availability of barium studies and ultrasonography, however, has raised questions as to the best method of diagnosis.
A study at the Children's Hospital of Pittsburgh from 1912 to 1967 found that a palpable pyloric mass was detected in 80% of cases.
From a study of 200 infants Stunden et al suggested that diagnosis by means of clinical methods alone is possible in 84% of cases.
Such patients should be operated on without undergoing imaging studies.
In 1934 it was shown that pyloric stenosis could be confirmed by barium meal examination, and in 1977 Teele and Smith first described the use of ultrasound in the diagnosis.
These techniques have been relied on increasingly to evaluate vomiting infants.
Scharli et al noted a 23% rise in the use of barium meal examinations over the 55 years of their study, while Breaux et al reported a 34% increase in the use of imaging techniques between 1980 and 1984, largely attributable to the use of ultrasound.
In both studies the authors emphasised that most of the pyloric stenoses could have been diagnosed clinically and that many of the imaging studies performed were not only unnecessary but also not cost effective.
It has been recommended that radiologicial examination should be reserved for patients who present with confusing or indistinct diagnostic features.
This study compares the methods used to diagnose pyloric stenosis at Camperdown children's hospital during two periods, 1974–7 and 1988–91, to determine whether imaging is being used more frequently and whether it has led to earlier diagnosis and better management of pyloric stenosis.
Methods
The medical records for all cases of pyloric stenosis seen at the Children's Hospital, Camperdown, during 1974–7 and 1988–91 were reviewed.
Cases were included only if the diagnosis was confirmed at surgery.
Because we were specifically looking at the assessment of children who presented to hospital with vomiting and a possible diagnosis of pyloric stenosis we excluded children admitted for other conditions who subsequently developed symptoms in hospital that led to a diagnosis of pyloric stenosis.
There were four such exclusions in 1974–7 and three in 1988–91.
An example was a child admitted for surgical repair of a tracheo-oesophageal fistula who developed pyloric stenosis in hospital postoperatively.
Each patient's record was reviewed to obtain the following information: age and sex; history; state of hydration on admission; whether a pyloric mass was felt; whether a test feed was given during abdominal examination; and whether any diagnostic imaging was used.
We also measured the time between presentation to hospital and a definite diagnosis of pyloric stenosis being made; time between presentation to hospital and surgery; length of preoperative rehydration treatment; and total length of stay in hospital.
The χ 2 test was used to compare proportions between the groups, and the 95% confidence interval of the difference is given when appropriate.
The Mann-Whitney test was used to compare continuous variables (Minitab data analysis software release, State College, Pennsylvania).
Results
Altogether 406 infants met the study criteria, and the records of 402 were available for study: 215 during 1974–7 (group A) and 187 during 1988–91 (group B).
Age, sex, and clinical presentation
The ratio of male to female infants in group A (3.8:1) was similar to that in group B (4.8:1).
The average age of infants at presentation was 40 days in both groups, and 188 (87%) of those in group A and 168 (90%) of those in group B were aged between 2 weeks and 2 months on admission.
The median duration of vomiting before admission was also similar in both groups, being seven days in group A and eight days in group B (W=39799.5, p=0.34 (95% confidence interval -2.0 to 0.0)).
Thirty nine infants (18%) in group A and 44 (24%) in group B had been vomiting for three weeks or more before admission.
About half of the infants in each group were not clinically dehydrated on presentation to hospital (102 (47%) in group A and 99 (53%) in group B), 102 (47%) in group A and 75 (40%) in group B were mildly dehydrated (by up to 5%), 10 (5%) in group A and 11 (6%) in group B were moderately dehydrated (5–10%), and only one child in group A and two in group B were dehydrated by 10% or more.
Palpation of pyloric mass
An infant suspected of having pyloric stenosis would usually be assessed by a house officer in the casualty department and then referred for a surgical consultation.
In group A 212 (99%) of the 215 infants had a pyloric mass palpated preoperatively compared with 148 (79%) of the 187 infants in group B. This difference was significant (χ 2 =38.4, p<0.001 (0.1 to 0.3)).
There were also significant differences between the two groups in the technique and results of abdominal palpation by the first hospital doctor to assess the patient, usually the medical resident or registrar.
In group A 101 (47%) of those first to see the infant gave a test feed compared with 41 (22%) in group B (χ 2 =26.4, p<0.001 (0.2 to 0.3)).
In group A 94 (44%) of those first to see the infant could palpate a pyloric mass compared with 61 (33%) in group B (χ 2 =4.7, 0.02<p<0.05 (0.02 to 0.2)).
Diagnostic imaging
Table I shows that 20% of infants in group A had at least one imaging study whereas 61% in group B had such studies, a more than threefold increase (χ 2 =70.5, p<0.001 (0.3 to 0.5)).
The 42 patients imaged in group A had 46 barium meal examinations, while the 114 infants imaged in group B had 86 barium meal examinations and 67 ultrasound scans.
The number of imaging studies performed outside the hospital increased from 19 (9%) in group A to 59 (32%) in group B, an almost fourfold increase (χ 2 =31.9, p<0.001 (0.2 to 0.3)).
Of these imaging studies performed before presentation to hospital, four (21%) barium meal examinations gave false negative results in group A while six (14%) barium meal examinations and 10 (31%) ultrasound scans gave false negative results in group B. Of the studies done at the hospital, one (4%) barium meal examination in group A and two (5%) barium meal examinations and one (3%) ultrasound scan in group B gave false negative results.
Combinations of diagnostic methods
Table II shows that a clinical diagnosis — that is, palpation of a pyloric mass either without or before an imaging study — was made in 187 (87%) cases in group A and 91 (49%) in group B (χ 2 =67.0, p<0.001 (0.3 to 0.5)).
A diagnosis of pyloric stenosis was based entirely on the result of imaging in 1% of cases in group A compared with 21% in group B (χ 2 =35.7, p<0.001 (0.1 to 0.2)).
Time taken to reach diagnosis
Twenty eight (13%) infants in group A and 71 (38%) in group B were referred to the hospital with a diagnosis of pyloric stenosis, made either clinically or by diagnostic imaging.
We compared the times taken to establish a definite diagnosis for the remaining infants from the time of presentation at the hospital.
In group A 114 (62%) were diagnosed within six hours, 124 (67%) within 12 hours, and 140 (76%) within 18 hours.
This compares with 56 (48%), 68 (59%), and 83 (72%) respectively in group B. The proportions were not significantly different between the groups (χ 2 =5.8, 0.05<p<0.1).
Preoperative management
On average the infants in both groups were operated on on their second day in hospital.
After exclusion of those who had been rehydrated intravenously before admission to the hospital the median duration of preoperative rehydration treatment was similar in both groups (21 hours in group A and 22 hours in group B; W=35314.5, p=0.2 (-4.3 to 0.8)).
Duration of stay
The average stay in hospital fell from 7.2 days in group A to 4.9 days in group B. This reflects the declining length of stay for all patients in this hospital, from an average of 7.8 days during 1974–7 to 4.1 during 1988–91.
Discussion
Abdominal palpation is an accurate method of diagnosing pyloric stenosis, and 80% of the infants proved to have pyloric stenosis at surgery during 1974–7 had a pyloric mass palpated without an imaging study.
Another 7% had a mass palpated before imaging, giving a total of 87% of cases that were diagnosed clinically.
Although a clinical diagnosis can usually be made, the rate of clinical diagnosis by palpation declined between 1974–7 and 1988–91, and the reliance on imaging studies increased more than threefold.
There has been a trend for the clinical diagnosis of pyloric stenosis to be preceded by an imaging study: during 1974–7 only 25 (12%) infants were imaged before a mass was palpated compared with 57 (31%) during 1988–91.
Other authors have noted high rates of use of imaging studies to diagnose pyloric stenosis and have suggested that some of these studies are unnecessary.
Forman et al suggested that if pyloric stenosis is suspected the child should be examined by an experienced doctor before an imaging study is ordered since the high sensitivity of abdominal palpation achieved by experienced doctors should limit imaging of infants.
One criterion for a patient's inclusion in this study was that pyloric stenosis had to be confirmed at  surgery.
Thus false positive cases, when a diagnosis of pyloric stenosis was not confirmed at surgery, were not included.
The proportion of false positive diagnoses with physical examination is generally accepted to be low.
Scharli et al noted two false positive diagnoses (both an annular pancreas) in a series of 1215 infants examined by experienced surgeons, and other studies support this finding.
From clinical and economic viewpoints a careful abdominal examination should be the first assessment of a vomiting child.
Not only is it cheaper than other methods but, with experience, it is also sensitive.
If the diagnosis of pyloric stenosis is not confirmed at the first test feed the examination can be repeated or imaging performed.
A further clinical examination is time consuming since it entails preparing another feed and watching or palpating the abdomen throughout a feed.
Ultrasonography may be more economical in terms of medical time in cases in which abdominal palpation gives initially negative or equivocal results.
During 1988–91 most imaging studies were performed in general hospitals or radiological practices before the infant was seen at our hospital.
The high proportion of false negative results obtained emphasises that imaging studies on infants are best done by those who perform them regularly and have developed skill in paediatric diagnostic imaging.
Ultrasonography is ideal for infants since it does not entail ionising radiation, allows parents to be present during the examination, and is non-invasive.
For these reasons it is the imaging procedure of choice.
This study, however, showed increased use of barium meal examinations, from 46 during 1974–7 to 86 during 1988–91.
While barium studies may exclude other causes of vomiting such as gastro-oesophageal reflux and intestinal malrotation, barium adds volume to the obstructed stomach of an infant with pyloric stenosis, increasing the risk of further vomiting and aspiration.
The high standard of diagnostic imaging skills at our hospital has led to a decreased reliance on clinical findings.
Test feeding was used less often, and the ability of junior house staff to palpate a mass was considerably less, during 1988–91 than during 1974–7.
The art of palpating for a pyloric mass should be taught by experienced senior staff to junior staff and students.
During 1988–91, 38% of infants were referred to our hospital after pyloric stenosis had been diagnosed, a larger proportion than during 1974–7 (13%).
Despite the increased use of imaging there was still a similar delay in recognising the condition before admission since there were no significant differences between the two periods with regard to age on admission, duration of vomiting, and state of hydration on presentation.
Imaging has an important place in confirming or excluding pyloric stenosis in difficult cases.
During 1974–7, however, the diagnosis could be made clinically in 87% of cases without the use of radiology or ultrasonography.
The increasing reliance on imaging suggests that the technique of palpating a pyloric tumour is a declining art.
Antibiotic resistant propionibacteria in acne: need for policies to modify antibiotic usage
Antibiotics have been used to treat acne for over 30 years and are still widely prescribed.
Their efficacy is due partly to their inhibitory effect on cutaneous propionibacteria — the micro-organisms implicated in the pathogenesis of the disease.
Until the late 1970s these organisms were uniformly sensitive to therapeutically useful antibiotics.
Since then an association has been found between the carriage of resistant strains and failure to respond to treatment with the corresponding antibiotic.
We report the results of a survey of the incidence of resistant propionibacteria in patients with acne.
Patients, methods, and results
We studied 468 patients with acne attending the outpatient clinic at Leeds General Infirmary between 1 March 1991 and 29 February 1992.
There were 229 new referrals from general practice, most of whom had previously received one or more courses of antibiotic treatment; 155 existing patients who showed little or no clinical improvement after at least three months' antibiotic treatment; and 84 patients sampled for other reasons (mainly anxiety and screening after treatment with isotretinoin).
Resistant propionibacteria were detected by rubbing facial skin with a moistened swab, which was used to inoculate a series of plates containing selective antibiotics.
Plates were examined for growth after seven days' anaerobic incubation at 37°C.
Resistance profiles of purified isolates were determined in conventional disc tests.
Overall, 178 patients carried propionibacterial strains resistant to one or more antibiotics.
Resistance to erythromycin was the most common (124 patients), and most of these strains (73 out of 110 tested) were cross resistant to clindamycin.
Tetracycline resistant propionibacteria were carried by 61 patients; such isolates are cross resistant to doxycycline but sensitive to minocycline.
Thirty patients carried strains with reduced sensitivity to trimethoprim.
Importantly, 27 patients carried two or more strains with different antibiotic resistances.
The resistance patterns of 171 resistant propionibacteria obtained from 143 patients were analysed in detail (table).
Overall, 153 strains were resistant to trimethoprim or erythromycin alone or cross resistant to tetracycline and doxycycline or to erythromycin and clindamycin.
Thirteen strains were resistant to three or more antibiotics.
Comment
In 1976 Leyden reported the complete absence of antibiotic resistant propionibacteria in over 1000 patients with acne.
The high prevalence of resistance in our study highlights the need for policies to restrict the use of antibiotics to limit the spread of resistant strains and hence preserve the effectiveness of antibiotics in treating acne.
There are 23 ways of prescribing antibiotics for acne based on oral, topical, or combined use of available preparations.
The relative risk of developing resistance with each regimen is unknown.
Minocycline is the only antiacne antibiotic to which cutaneous propionibacteria have not acquired resistance, although it is widely prescribed.
Patients who respond to antibiotics do not harbour propionibacteria resistant to the therapeutic drug, except for a few of those treated with topical erythromycin.
We recommend the following guidelines for antibiotic treatment of acne in both hospital and general practice.
Firstly, do not prescribe antibiotics if a non-antibiotic topical preparation will suffice.
Secondly, continue treatment for no longer than is necessary (accepting that six months is the minimum).
Thirdly, if further treatment is required reuse the same drug whenever possible.
Short intervening courses of topical treatment with an antibacterial agent such as benzoyl peroxide may help to eliminate any resistant propionibacteria that have been selected.
Finally, avoid concomitant oral and topical treatment with chemically dissimilar antibiotics to reduce the risk of resistance developing to both.
The concept of long term rotational antibiotics for acne is outdated and this practice should be avoided.
Patients must not expect an endless supply of alternative drugs.
General practitioners and dermatologists should now consider propionibacterial resistance as a cause of therapeutic failure.
Whenever possible, patients who do not respond to antibiotics should be screened for resistant strains.
A protocol based on the methods used in this study may be obtained from us.
Knowledge and use of secondary contraception among patients requesting termination of pregnancy
Studies of the contraception used before an unplanned pregnancy indicate that an increasing proportion of such pregnancies are due to condom failure.
It is vital that people know of secondary methods of contraception, and we investigate here the current extent of this knowledge.
Patients, methods, and results
We interviewed 769 of the 808 patients attending our clinic between 1 April 1991 and 31 January 1992 for whom termination of pregnancy was agreed under the Abortion Act; those interviewed included 62 (8%) Afro-Caribbean and 69 (9%) Asian patients.
The simple structured interview covered patients' knowledge and previous use of contraceptives and knowledge of secondary methods of contraception to be used if failure of a primary method was suspected.
The source of advice on family planning was also recorded.
In 307 cases the method used at conception was known to be less effective than one previously used by the same patient.
This usually represented a change from the combined pill to condoms.
A potential failure of contraception had been recognised by 171 patients: 93 reported a split or leaking condom; 13 reported a condom coming off during intercourse; 32 admitted inconsistent use of condoms; 32 reported forgetting to take contraceptive pills or taking antibiotics concurrently with the pill; and one reported a late injection of medroxyprogesterone acetate.
The 138 patients who had recognised a potential condom failure represented only 45% of the 309 who had conceived while using this method.
Only 20 of them had attempted to use a secondary or emergency contraceptive method; this was the postcoital pill in all cases.
An adequate knowledge of the existence, timing, and source of postcoital pills was shown by 52 (30%) of the 171 patients who had recognised a potential contraceptive failure and 25 (12%) of the 210 who had not used any contraception — that is, 77 (20%) of the 381 patients who might have benefited from their use.
Knowledge of postcoital insertion of an intrauterine contraceptive device was shown by four (2%) of the 171 and two (1%) of the 210 respectively — that is, only six (2%) of the 381 who might have benefited.
Most patients (718/769) would have preferred to use postcoital contraception than experience an unplanned pregnancy.
Of the patients surveyed, 501 had received advice on contraceptives from a general practitioner and 102 from a community family planning clinic, and 163 had not received medical advice (no information was available from three patients).
Among those who had received medical advice, knowledge of postcoital contraceptive methods differed significantly according to the source of that advice (table).
We asked 422 patients who had used the combined pill at some time about their knowledge of appropriate secondary methods of contraception to be used in the event of forgotten pills, severe vomiting, severe diarrhoea, and concurrent antibiotic treatment.
Four correct replies were given by 70 (19%) of the 372 patients instructed by general practitioners and 25 (50%) of the 50 instructed in community family planning clinics.
This difference was significant (p<0.001) and was not explained by age, parity, marital status, or social class, which were similar in both groups.
Comment
Self selection bias — for example, because of better informed and motivated patients seeking advice from specialist clinics — cannot be excluded from this study.
Our results are, however, consistent with those of previous studies which suggest that patients attending general practitioners (the main source of contraceptive advice) are less knowledgeable than those attending specialist clinics.
An audit of unplanned pregnancies seen in one practice also emphasised the need for great care in counselling people using the pill.
Efforts to encourage the use of condoms as protection against sexually transmitted infections should emphasise their use as well as rather than instead of more effective contraceptive methods, but greater knowledge of postcoital methods and their use is also vital.
Weak link in vaccine cold chain
The efficacy of vaccines, especially live vaccines, may be compromised unless they are kept at 2–8°C (or 0–4°C for polio vaccine) at every stage of the cold chain from manufacturer to recipient.
In tropical countries the importance of maintaining the cold chain is recognised, but in temperate countries it has been given little attention until recently.
Results from Hungary indicated that vaccines were exposed to heat during transport and that facilities for refrigeration were  inadequate , and recent British studies have shown that the storage of vaccines is inadequate in general practices and health authority premises.
We assessed the conditions in which vaccines were transported in Birmingham during the summer and surveyed the modes of transport used.
Methods and results
Vaccines were transported from a central distribution point to 53 general practices and six health clinics in the former Central Birmingham Health District.
Staff from the general practices collected their vaccines from the central office, while health clinics received theirs from the district's transport service.
Each of the 138 orders for vaccines studied was made up into an ordinary envelope that contained a Monitor Mark Time-Temperature Integrator Tag (3-M).
This is a laminated strip that, once activated, indicates exposure to temperatures over 10°C by the progression of a blue dye along the strip.
It is accurate to within 1°C of the actual temperature in the package (information supplied by manufacturer).
Thus any breakdown in the cold chain could be quantified.
Data on exposure to high temperatures and on transport (such as mode, duration, distance, and refrigeration) were obtained by means of a short questionnaire completed by the recipient on opening the package of vaccine.
The ambient temperature for each day of the study was determined from local weekly weather statistics.
Altogether 104 questionnaires were usable.
The table shows the results; some results were excluded because information was lacking or, in three cases, the spread of blue dye along the indicator strip was extensive and clearly inconsistent with other details.
Sixty six of the vaccine packages had been exposed to temperatures >10°C.
All vaccines transported by the district transport service, which does not use refrigerators and is not provided with cool boxes, were exposed to excessive temperatures.
Over half of those transported by car were similarly exposed.
The proportion of vaccines exposed to >10°C was significantly higher when the journeys lasted more than 30 minutes or were greater than 3.2 km.
Temporary refrigeration of vaccines during or after their transport had no benefit and may be falsely reassuring.
The mean maximum daily temperature during the study was 21 (range 11–27) °C.
The daily temperatures were not related to exposure of the vaccines to temperatures >10°C.
Comment
The only successful means of transporting vaccines without excessive warming was by car, and this depended on speed of transit.
The use of cool boxes was so rare that we cannot comment on their potential effectiveness, and it is difficult to obtain small cool boxes of known effectiveness in Britain.
We welcome the Department of Health's takeover of the distribution of vaccines.
This will not, however, address the deficiencies of local storage facilities on those occasions, such as immunisation of children at school, when vaccines need to be transported to other sites.
The World Health Organisation recommends that all polio vaccines that are exposed to temperatures over 10°C for more than a day (five days for measles vaccines) should be used within three months.
Thus the cumulative effect of inadequate transport, poor storage, and exposure in consulting rooms during clinics may be considerable.
Using cool boxes of known effectiveness, improved staff training, and random monitoring exercises similar to this one would improve a potentially hazardous situation.
Risk of vertebral fracture in women with rheumatoid arthritis
Rheumatoid arthritis commonly affects postmenopausal women, and osteoporosis and resultant fractures may contribute to the morbidity of the condition.
No large case-control studies have assessed the rates of vertebral fracture in a standardised manner.
We compared women with rheumatoid arthritis with population controls for the presence of vertebral fractures.
Subjects, methods, and results
The cases were drawn from 191 postmenopausal women aged 45–65 with rheumatoid arthritis who consecutively attended clinics in five London hospitals.
All were white, were not taking replacement oestrogens, and had agreed to have their bone density measured before entering a drug study.
They were asked if additional radiographs of the spine could be taken, and 149 (78%) agreed.
The controls were 713 postmenopausal women aged 45–65 not taking hormone replacements.
They were obtained from the age-sex register of a large general practice in London, and those who responded (77%) were similar to the national average in height, weight, and smoking habits.
The socioeconomic status of the women was estimated from their postcode (ACORN classification system).
Radiographs of the women's lateral thoracic and lumbar spine were taken in a standardised manner.
They were examined with a semiautomated morphometric technique with high specificity by a single, blinded observer using a digitising board, and anterior, central, and posterior heights were recorded.
The predicted posterior height for each vertebra was calculated from four adjacent vertebrae, and an algorithm compared the ratios with normal ranges of vertebral heights from T4 to L4.
A woman was classified as having a fracture if she had at least two minor deformities (reduction of two vertebral height ratios by at least 2 SD) or a single severe deformity (reduction of two vertebral height ratios by at least 3 SD).
The arthritic women were also assessed for disease activity and bone mineral density (at the hip and spine, with a dual energy x ray absorptiometer).
Differences between the groups were assessed with the χ 2 test for proportions and unpaired t tests (after log transformation of non-normal distributions).
Odds ratios were calculated for the risk of fracture in the arthritic women.
The cases and controls were similar in age, years since the menopause, and weight (table).
Most controls were from lower to mid-middle class, while the cases had a greater spread of social class.
Eighteen arthritic women (12.1%) had a vertebral fracture compared with 44 (6.2%) of the controls (odds ratio 2.1 (95% confidence interval 1.2 to 3.7)).
Among the arthritic women five (28%) of those with a fracture had used steroids (mean dose 10.8 mg/day for 8.9 years) compared with 46 (35%) of those without a fracture (7.9 mg/day for 4.3 years).
Those with fractures were older (59.0 v 56.3 years, p=0.02) and more likely to have smoked (10/18 (56%)v 39/126 (31%), p=0.04)(table).
No significant differences were found in height, weight, disease activity, duration of disease, socioeconomic class, or years since the menopause.
Bone density was lower, but not significantly, in the women with fractures (by 4.9% in the lumbar spine and by 9.9% at the hip).
Results were similar when only severe fractures were analysed.
Comment
The rate of vertebral fracture in the women with rheumatoid arthritis was over twice that in the controls.
The only other large studies with controls examined hip fractures and rheumatoid arthritis and found odds ratios of 1.5 and 2.1.
These results suggest that there is an increased risk of osteoporotic fractures at the spine and hip.
In our study bone densities were not uniformly low at all sites, and fractures cannot easily be explained by the use of steroids or disease activity.
Ankylosing spondylitis is also associated with increased rates of vertebral fracture.
The prevalence of vertebral fractures in women with rheumatoid arthritis aged over 65 is probably higher than the 12% found in the younger women in this study, and clinicians should be aware of the possibility of vertebral fractures in all patients with rheumatoid arthritis.
GENERAL PRACTICE
National asthma attack audit 1991–2
Abstract
Objective —
To describe the frequency and characteristics of asthma attacks in the United Kingdom and to compare actual management with recommended guidelines for the management of attacks.
Design —
Correspondence survey.
Setting —
218 general practices in the United Kingdom.
Subjects —
1775 patients of all ages who had a total of 1805 asthma attacks over three months.
Main outcome measures —
Patient characteristics, place of management of attacks, comparison of actual management with recommended guidelines.
Results —
Of the 1805 attacks, 300 occurred in boys aged 0–9, 144 in girls aged 0–9, and 118 in women aged 20–29.
The estimated frequency of attacks in the community was 14.3 per 1000 patients per year.
1546 (86%) patients with attacks were managed within general practice, 225 (12%) were admitted to hospital, and 34 (2%) were discharged from an accident and emergency department.
Two patients died.
On initial presentation, 248 (14%) patients were ‘not breathless,’ 900 (50%) were ‘moderately breathless,’535 (30%) were ‘breathless and distressed,’68 (4%) were ‘too breathless to talk,’and 2 were ‘moribund.’
Recording of clinical data was variable.
Underuse of nebulised bronchodilators and systemic steroids was apparent in all grades of clinical severity.
Contrary to current guidelines for asthma management, ‘step up’ in  maintenance therapy after an attack was often not practised.
Conclusion —
Reported management was at variance with recommended guidelines.
This has major implications for the design and distribution of future guidelines.
Introduction
Asthma kills 2000 people in the United Kingdom every year; it is a major cause of hospital admission in all ages and an important cause of work and school absence and has economic and quality of life implications for individual sufferers, their families, and society.
Optimal treatment of attacks can be life saving but suboptimal treatment or unnecessary delay in the provision of care can be fatal.
Hospital studies on asthma attacks have concentrated on specific groups of patients (adults or children) or have been confined to specific geographical areas.
Studies from the community are few and predominantly based on single practices.
No published British studies encompass all ages, all regions, and all sectors of medical care.
Medical audit has a key role in negotiating and monitoring standards of asthma care.
The British Thoracic Society's confidential inquiry and the work of Bucknall et al and others have been instrumental in encouraging local and national guidelines for the management of acute severe asthma.
Progress on improving the provision and delivery of acute medical services depends on knowledge of how asthma attacks are actually managed, rather than on how attacks should be managed.
The General Practitioners in Asthma Group, formed in 1987, exists to encourage good quality care of asthma in primary care, support educational initiatives, and encourage research and audit.
The group's research unit was established in 1991 with the remit to study the quality of asthma care in general practice and to provide objective data to contribute to the debate on how and where asthma should be managed.
The unit links research with practitioners' educational needs for personalised feedback on clinical performance.
This paper reports the main findings from the first national audit of asthma attacks (1991–2), which had two aims — to describe the characteristics of asthma attacks in the community, and to compare current management with recommended guidelines.
Method
A package of booklets for recording asthma attacks linked to a distance learning programme (accredited for one half day disease management of postgraduate educational allowance) was developed and piloted.
The 380 members of the General Practitioners in Asthma Group were invited to participate in the audit.
In addition, a medical mailing agency was commissioned to send invitations to participate to 2000 general practitioners stratified to be representative of all regions in the United Kingdom.
Practitioners who returned a preprinted postcard expressing further interest were sent the package of recording booklets and distance learning programme.
Each practitioner was invited to record details of all patients who presented with an asthma attack during a predetermined three month period.
Data were collected in four overlapping blocks of three months starting with September 1991 and ending with January 1992.
Practitioners were asked to include all asthma attacks from patients on their lists whether or not they were seen by themselves, a partner, a deputy, or in hospital.
In the absence of a universally agreed published definition for attacks or, for that matter, for asthma, consensus was reached within the steering group to define an attack according to three criteria, as ‘an episode of respiratory symptoms which prompts an urgent consultation with a doctor, is of sufficient severity to prevent the patient working or attending school or performing domestic duties or playing, and results in increased use of antiasthma medication.’
Practitioners were asked to provide details of practice size and whether or not their practice operated an asthma clinic, owned a portable nebuliser, had a register of asthma patients, operated a practice or local protocol for asthma care, or employed a practice nurse with special knowledge of asthma.
Details of patients included age, smoking habits, drugs taken before the attack, severity of symptoms, use of a self management plan, or possession of a peak flow meter.
Consultation details, recording of clinical signs (pulse rate, blood pressure, etc), treatment given, drugs taken after the attack (at the time of final medical contact), and follow up arrangements were sought whether or not the patient was seen by a general practitioner or an accident and emergency department, was admitted to hospital, or experienced a combination of all three.
Information was sought on all patients registered with participants, whether seen by them or not.
At the end of the three month study periods the practitioners returned completed booklets to the research unit.
The research data were entered on to computer for analysis with the SPSS package and the management of each patient was compared with published recommended guidelines for adults or children.
Each practitioner was sent a personalised, handwritten critique of their management together with copies of published guidelines and the remainder of the distance learning package.
Results
participants
Of the 380 members of the General Practitioners in Asthma Group, 183 expressed an interest in the project, of whom 103 returned recording booklets for analysis and personalised feedback; 352 practitioners out of the 2000 who received an invitation returned a postcard expressing interest and 115 returned study booklets.
In total, 218 general practitioners from all regions of the United Kingdom participated.
In all, 159 (73%) participants reported that their practices had an asthma clinic; 167 (77%) practices had a practice nurse with special knowledge of asthma, 204 (94%) owned a nebuliser, 135 (62%) operated a practice asthma protocol, and 179 (82%) had a register of asthma patients.
patient characteristics
A total of 1748 patients had one attack within the three month study period, 24 had two attacks, and three had three attacks, giving a total of 1805 attacks included in the study.
Accurate data on age and sex were supplied for 1389 patients with attacks, 300 of which occurred in boys aged 0–9 and 144 in girls aged 0–9.
There were 118 women aged 20–29 who experienced an attack.
Table I shows the age-sex distribution of patients, including those admitted to hospital.
A total of 129 practices supplied an accurate ‘denominator’ for the practice population from which patients were derived.
Their combined list size was 250739 and from this population a total of 897 attacks was reported over a three month period.
The rate of asthma attacks in this subsample is estimated at 14.3 per 1000 patients per year.
Two hundred and forty one adults were known to be active smokers and 261 children passive smokers.
In total, 516 (29%) patients possessed their own peak flow meter and 615 (34%) had been issued with a self management plan.
In the previous year 1106 (61%) patients had experienced an asthma attack, of whom 415 (38%) possessed a peak flow meter and 457 (41%) a self management plan.
place of management
In all, 1738 (96%) patients in the survey were seen initially by their general practitioner; 41 (2%) referred themselves and were admitted to hospital, and 26 (1%) referred themselves and were managed and discharged by an accident and emergency department.
Of those initially seen by the general practitioner, 184 (11%) were referred and admitted to hospital, 8 (<1%) were referred and discharged by an accident and emergency department, and 1546 (89%) were managed solely by the general practitioner (table II).
Five patients (2% of those admitted) were mechanically ventilated in an intensive care unit.
Two deaths were reported in the survey; both occurred outside hospital.
In each case the patient was reported to be ‘moribund’ when the general practitioner arrived, and resuscitation proved unsuccessful.
recording of clinical data
In the 1805 asthma attacks studied there was a written record of pulse rate in 968 (54%), blood pressure in 266 (15%), respiratory rate in 1143 (63%), presence or absence of cyanosis in 1275 (71%), and peak flow rate in 1473 (82%).
The best documented measure of severity was a simple classification based on each patient's initial state of distress on presentation.
This was available in 1749 (97%) patients with asthma attacks: 248 (14%) were ‘not breathless,’ 900 (50%) were ‘moderately breathless,’535 (30%) were ‘breathless and distressed,’68 (4%) were ‘too breathless to talk,’and two were ‘moribund,’both of whom died (table III).
treatment
Of the 1546 patients managed exclusively by general practitioners, 477 (31%) received nebulised bronchodilators.
Twenty eight (12%) of those in the not breathless category, 197 (23%) of those who were moderately breathless, 217 (54%) of those who were breathless and distressed, and 27 (82%) of those who were too breathless to talk were given nebulised bronchodilators.
Nebulised bronchodilators were given to 21 (62%) of the 34 patients managed and discharged from accident and emergency departments and 126 (56%) of the 225 patients admitted to hospital.
Systemic steroids (oral prednisolone or intravenous or intramuscular hydrocortisone) were given to 863 (56%) of the 1546 patients managed exclusively by general practitioners: 54 (22%) of those not breathless, 556 (66%) of those moderately breathless, 208 (52%) of those breathless and distressed, and 28 (85%) of those too breathless to talk.
Systemic steroids were given to 14 (41%) of the 34 patients managed in and discharged from accident and emergency departments and 116 (52%) of the 225 patients admitted to hospital (table II).
General practitioners prescribed antibiotics to 489 (32%) patients, accident and emergency departments to six (18%), and hospitals to 49 (22%) patients.
Each patient's maintenance treatment before the asthma attack was classified by the research staff according to British Thoracic Society steps 0–5 (children were placed in accordance with recent published consensus papers).
Details of drugs taken before and at the time of the final medical contact after the attack were incomplete owing to poor quality of discharge letters in 64 patients managed in accident and emergency departments or hospital.
In the 1546 patients managed exclusively by the general practitioner in whom details of maintenance treatment before and after the attack were supplied, 349 (23%) had been receiving no treatment (step 0) before the attack, and 51 (15%) of these were not prescribed any maintenance treatment after their attack.
A total of 298 (85%) had their treatment ‘stepped up’; 131 (38%) received bronchodilators alone (step 1) and 161 (46%) received some form of prophylactic maintenance treatment (step 2 or beyond).
Before their attack 374 (24%) patients had been receiving bronchodilators alone (step 1); in 147 (39%) maintenance treatment was unchanged or reduced after their attack, and 227 (61%) were prescribed medication from step 2 or beyond.
Before their attack 356 (23%) patients had been prescribed treatment at step 2, 275 (77%) of whom remained at step 2 or lower after the attack, and 81 (23%) had their treatment stepped up according to recommended guidelines.
Before their attack 303 (20%) patients had been prescribed step 3 medication; 281 (93%) remained at step 3 or lower after the attack and 23 (7%) had their maintenance treatment stepped up.
A total of 107 (7%) patients were classified as step 4 and 51 (3%) as step 5 (receiving maintenance oral steroids) before the attack (table III).
Discussion
The audit of asthma attacks represents an attempt to study the access to, process of, and outcome of asthma attacks in all age groups in all regions of the United Kingdom.
The findings confirm that in the early 1990s asthma attacks are a common life threatening problem of concern to hospitals and general practitioners alike.
Recruiting the practices through a special interest group and through self selection is likely to have biased results.
About a quarter of participants, however, did not operate an asthma clinic or employ a practice nurse trained in asthma care, and a small proportion of participants did not possess a portable nebuliser.
The survey was linked to a distance learning package accredited for postgraduate allowance, and this may have encouraged practitioners with varying degrees of expertise in asthma management to participate.
The distance learning package included a copy of the British Thoracic Society's guidelines for the management of asthma but was issued after the recording booklets for the study had been completed and returned.
The educational impact and subsequent effect on patient management will require further study.
Attacks were reported more commonly in boys than girls (ratio 2:1), gradually decreasing in frequency through early adult life, but becoming more common at ages 50–59 and above.
An exception to this was the large number of women aged 20–29 included in the study.
This group has a high doctor-patient contact rate but also includes a proportion of patients with unstable or brittle asthma.
The two deaths recorded during this study both occurred in patients who were moribund when first assessed and did not survive long enough to reach hospital.
The deaths serve as a reminder that asthma is responsible for a continuing toll of personal tragedy.
The frequency of asthma attacks presenting in the community was estimated at 14.3 per 1000 patients per year.
No comparable figures have been published, although the third national morbidity survey reported ‘asthma presentations.’
The great majority of attacks, 1546 (86%) of 1805, were managed exclusively outside hospital by general practitioners.
Published research on the patterns of care of attacks are dominated by hospital based studies.
Self referrals to accident and emergency departments or direct admissions to hospital accounted for only 67 (3.7%) attacks.
It should be emphasised that participating practitioners were asked to return details of all patients on their lists who experienced an attack and not simply those whom they saw personally.
It is possible, though, that some self referrals and patients who did not consult their own doctor during the course of their attack may not have been notified to participating practitioners and thus were excluded from the study.
The definition used for the survey was a pragmatic attempt to encourage participants to include patients of all grades of severity of attack.
Until a definition for asthma itself can be agreed the problem of defining attacks will continue to generate controversy.
The definition used here is independent of the variables under study (age, clinical signs, investigations, place of treatment, and clinical outcome).
Recording of objective clinical signs such as pulse rate, blood pressure, and peak flow could be considered suboptimal.
Low standards of recording rather than of measuring may be the problem.
Some doctors may not see the relevance of recording details of clinical signs.
However, a simple grading of severity of attack based on each patient's level of distress with breathing was completed in 97% of cases.
Our findings, in common with those of other audit surveys, are a strong argument in favour of simple standardised recording systems for certain medical conditions such as acute chest pain, review of diabetes, major trauma, and burns.
The British Thoracic Society's guidelines recommend that the treatment of acute severe asthma includes nebulised bronchodilators and systemic steroids.
The survey highlights possible underuse of nebulisation, which was given to 35% of patients, and of steroids, given to 55%.
This may represent a failing by doctors to initiate adequate therapy, but there are other possible explanations.
Many of the mild or ‘not breathless’ cases in this survey, particularly children, may have responded to less aggressive intervention.
The proportion of attacks treated with nebulised bronchodilators and systemic steroids increased with increasing severity of clinical presentation (table II).
Guidelines for acute severe asthma based on hospital experience may not be an appropriate yardstick by which to judge how asthma attacks are measured in the community.
It is important to note, though, that a sizable proportion of patients graded as ‘breathless and distressed’ and ‘too breathless to talk’did not receive the recommended lifesaving antiasthma treatment: nebulised bronchodilators and systemic steroids.
The pattern of maintenance treatment before and after attacks was also at variance with published guidelines.
This survey shows that many patients received no follow up and no change to treatment despite experiencing an asthma attack.
The issuing of peak flow meters, availability of self management plans, and follow up appointment procedures were other aspects of care where actual practice differed from recommended practice.
General practitioners' management of asthma attacks may differ from published recommendations for several reasons.
The guidelines may not have been sufficiently widely disseminated, or they may have been viewed as impractical or unrealistic.
The standard of clinical care reported is less than acceptable.
A simple, relevant, and attainable set of goals for the management of acute asthma attacks in the community and in hospital is clearly needed.
This needs to be supported by educational initiatives and a means whereby doctors can receive relevant personal feedback on how individual patients have been managed.
The first national asthma attack audit shows that access to medical care for people having asthma attacks continues to be mainly through general practitioners.
The process of care differs considerably from recommended guidelines, and the outcome of attacks is a continuing cause for concern.
This survey is a baseline against which attempts to effect change can be measured.
EDUCATION & DEBATE
Ethical issues in randomised prevention trials
Randomised controlled trials have made important advances in preventive medicine.
It is unfortunate, therefore, that launching such trials can be a lengthy affair because of protracted discussions on ethical issues that are often misunderstood.
Good ethics and good science
One of the sources of misunderstanding arises from the failure to recognise that the key ethical question in a randomised prevention trial is also the key scientific question — namely, ‘Is X an effective and safe way of preventing disease Y?’
If the answer to the question is known, there is no scientific purpose in conducting the trial, and it is also unethical to do so.
If the answer to the question is not known, it is both scientifically worthwhile and ethically correct to investigate the issue, provided that the question is not a trivial one.
In research in preventive medicine, as in research in therapeutic medicine, the ethics are driven by the science; good science is a prerequisite for good ethics.
The answer to the question ‘Is X an effective and safe way of preventing disease Y?’ depends on making a scientific judgment based on all the relevant information available.
The quality of that judgment is crucial to the ethical position.
Usually, the claim that a randomised trial (whether of a preventive or a therapeutic regimen) is unethical presumes that the answer to the question that the trial is designed to answer is already known.
If it is known, the claim is valid.
This is not the case if there is uncertainty.
It may not be an easy judgment to make but it is a scientific one that rests not only with researchers but with all those responsible for providing preventive and therapeutic services.
The ethical debate is in fact a scientific debate.
It is impossible to divorce the two.
To represent an intervention as effective when its efficacy is uncertain is itself unethical.
In Non-preventive or non-therapeutic research the experimental subject is simply assisting the research to obtain biological data
There are difficulties in answering a question by saying that we don't know.
The answer fails to distinguish between being, on the one hand, uninformed and, on the other hand, aware of enough knowledge to conclude that the question cannot be answered.
The problem is exacerbated in medical practice, where the pressure to do something may be compelling and judgment may be impaired by a well intentioned but misguided presumption of benefit.
For example, it has seemed self evident that if less advanced disease has a better prognosis, the earlier the disease is detected, the better the outcome.
That this is not necessarily true (or if so, only to a minor extent) is illustrated by screening for lung cancer.
The presumption of benefit bedevils much of the ethical thinking affecting proposed randomised trials.
The misunderstanding that good medicine and good science are in conflict (or worse, that there is a clash between compassion and science) may arise from a failure to distinguish medical trials from human biological research that is neither preventive nor therapeutic.
In a trial each subject stands to benefit (though in the event no benefit may arise or there may even be risk of harm), but in non-preventive or non-therapeutic research the experimental subject is simply assisting the researcher to obtain biological data — for example, how a particular drug is excreted.
There is a possibility, albeit unlikely, that knowledge may be advanced at the expense of the subject.
Unlike in trial based research, the subject could be harmed but cannot benefit and therefore, unlike in trial based research, there is potential conflict between advancing knowledge and ensuring the safety of the individual.
Evidence of efficacy
A related misunderstanding stems from the fallacy that it is the aim of medical researchers to obtain absolute evidence of efficacy.
It is perceived that researchers wish to dot the i's and cross the t's.
In fact, the aim is to obtain reasonable evidence; if reasonable evidence already exists, further research is unnecessary.
The legitimate debate hinges on what constitutes reasonable evidence.
In formulating a view, it is helpful to examine this in two ways.
Firstly, if more than one explanation could reasonably account for an apparent effect then sufficient doubt remains.
Secondly, if knowledgeable doctors in good faith adopt differing practices, there is a pragmatic basis for regarding the issue as unresolved.
A trial is then simply marshalling the diversity of practice in a disciplined manner capable of resolving the issue when otherwise it would remain open.
The great advantage of such  trials is that they resolve uncertainty so that an effective measure is more likely to be adopted and an ineffective one more likely to be abandoned.
Another difficulty, and indeed one implied by the original question ‘Is X an effective and safe way of preventing disease Y?’, is the practice of judging the efficacy and safety of treatments as either positive or negative, rather than in quantitative terms.
Estimating the extent of any benefit is as important as determining whether there is a benefit.
One needs to know by how much, for example, screening for breast cancer reduces mortality from the disease in order to determine whether a screening programme is worthwhile.
In terms of hazard, a procedure causing a serious complication in one out of 1000 individuals may be acceptable in clinical practice, but if in screening a procedure with the same complication rate reduced the incidence of the disorder sought by 50% in a population with an incidence of that disorder of two per 1000, then there would be no net benefit.
If the same procedure were used in a ‘high risk’ group with an incidence of, say, 100 per 1000 then 50 would benefit for each person harmed.
It should be the objective of a randomised trial to estimate the magnitude of an effect, not simply its presence.
Harm
Then there is the question of harm.
Few, if any, effective interventions come without any risk of harm — but in preventive medicine the risk may be very small, whether it is a small risk of a serious adverse effect or a larger risk of a mild one.
Perversely, this ‘low risk’ situation poses its own problems.
For example, the view that taking extra vitamins may be harmful does not ring true; the possibility of harm is  acknowledged but it is one that is regarded as remote and pedantic.
The problem is well illustrated by the debate that preceded the Medical Research Council's study on the prevention of neural tube defects.
Even allowing for doubt about whether the vitamin supplementation was effective, administering it seems to be a one way bet — at worst it is harmless, but at best it may cure or prevent the disorder in question.
This view assumes that an intervention can be biologically potent and yet non-toxic.
If a chemical can affect the differentiation and growth of the developing neural tube, it could, in principle, also affect the differentiation and growth of tissues in less desirable ways.
The difficulty in acknowledging the possibility of harm from a method of prophylaxis that seems natural is that the potential hazard is unspecified and hypothetical whereas the potential benefit is specific.
Even if vitamin supplementation did not prevent neural tube defects, what specific harm would result?
The answer that we don't know seems unconvincing.
An intervention with a low probability of harm given to millions can do more damage than one with a high probability of harm given to a few
Pointing out that there have been blunders in the past through the introduction of inadequately evaluated methods of prophylaxis (for example, oxygen for retinopathy of prematurity) which, with the benefit of hindsight, would not have occurred had they been investigated properly, has all the conviction of a sermon in a secular world.
It is important that our colleagues and the public recognise that although, in general, preventive strategies by their nature are likely to involve minimal risk, this does not mean zero risk; an intervention with a low probability of harm given to millions can do more damage than one with a high probability of harm given to few.
Although one of the reasons for conducting trials is the possibility of harm, trials are principally designed to test efficacy.
They are weak instruments for detecting harm for two reasons.
Firstly, they often recruit subjects with a high risk of the disorder under study so as to minimise the number of subjects needed to show an effect.
This will reduce the statistical power of detecting adverse effects that could be quantitatively important when used in low risk situations.
Secondly, since the nature of the harm is usually unknown, larger numbers are needed than if it could be specified in advance to avoid spurious statistically significant adverse effects.
A two step approach is needed.
A trial is first performed to show efficacy and, within limits, to quantify the effect.
If efficacy can be shown then the question of harm needs to be considered by using information from the trial itself (however few the data) together with other available data; it may be necessary to set up special surveillance methods.
If efficacy cannot be shown and the proposed intervention is abandoned then the question of harm becomes irrelevant.
Ironically, in the same way that the denial of the risk of harm associated with a possible preventive measure undermines the basis for performing a randomised trial designed to investigate the efficacy of that measure, the exaggeration of the risk of harm tends to undermine public health measures designed to introduce the preventive measure if it is shown to be effective.
The fortification of staple foods with folic acid to prevent neural tube defects may be unwisely delayed on this account.
A hypothetical risk of harm is a good reason to investigate efficacy; it is not a good reason to block the introduction of a proved benefit.
Ethics and resources
Another problem arises from the ‘low risk’ situation in prevention trials.
The general ethical approach applied to therapeutic trials, namely that there should be an approximate balance between expected benefit and possible risk to each individual in the trial (so justifying the allocation of patients at random to a new or existing treatment) is less relevant in certain prevention trials.
Though the possibility of hazard should never be ignored, however remote it may seem, the ethical position in prevention trials often stems mainly from a balance between the extent of benefit (if any), and the cost of the intervention.
An example is the trial currently underway to see whether breast cancer screening by mammography is better yearly than every three years.
Given that mammography every three years is effective in reducing mortality from breast cancer, it follows that performing examinations yearly will be more effective.
There is an increase in risk due to irradiation of the breast, but this is likely to be small and is best estimated from a woman's age and  dose of irradiation.
The issue is to estimate the marginal benefit from the increased frequency of screening examinations against the marginal increased cost.
It is not an argument over which of the two screening strategies will confer the greater medical benefit.
That is a foregone conclusion.
That the ethics of prevention trials is often a resource driven argument is tacitly assumed but rarely explicitly stated.
It is important that the profligate use of resources is avoided; otherwise some patients will be denied a benefit because of the misdirection of limited resources.
This is not to say that economy is paramount; indeed, economy is secondary to what should be an overriding principle in public health — namely, equity.
We all expect to benefit equally from clean water, clean air, access to screening facilities, and so on.
Given that resources are limited, a screening policy, for example, should be designed to ensure that, whatever resources are available, screening is available to all and that a fair method is adopted to designate those at highest risk as screen positive and ensure that they are offered appropriate prophylactic intervention.
That this is not always the case is apparent from our policy in cervical cancer screening, the bulk of which has occurred among young women, in whom the risk of dying of cervical cancer is substantially less than among older women, who have not been the focus of public screening policies for this disease.
Also, current recommendations for screening for stroke and major coronary events by blood pressure measurements are not rational in that they take little account of the absolute risk of these diseases, specifying cut off levels for blood pressure screening that take little or no account of age.
The formulation of research, as well as the practice that may follow, needs to recognise explicitly the issues of equity and cost.
They are important ethical considerations.
Medical practice and medical research
Ethics affect both medical practice and research, but research usually falls under the closest scrutiny.
There is an important distinction between practice and research that stems principally from intent.
If an intervention is established as beneficial and worthwhile it forms part of medical practice.
If we are uncertain about its value and apply it mainly to assess its worth then it is research.
We have, in recent years, exaggerated this distinction and created an awkward divorce between practice and research.
Much of medicine is uncertain, and the admission of that uncertainty creates the obligation to research the matter further.
Practice and research are intertwined; good doctors have an obligation to inquire as well as to provide a clinical service, and codes of conduct and ethical guidelines that recommend different rules for practice and research create an undesirable dichotomy that itself has ethical implications.
For example, if a particular method of prevention (or treatment) is of uncertain value, there are many obstacles to be passed before the necessary research can be conducted, from seeking funds and securing the necessary collaboration to obtaining consent from many local research ethics committees.
The difficulties tend to encourage the denial of the uncertainty and the assumption that the intervention is effective.
It is easier to presume the benefit and call the intervention a service activity than to remain agnostic and call it research together with the inconvenient implications.
It would be desirable if randomised trials in general, and preventive trials in particular , were an integral part of medical practice, adopted whenever a new approach needed assessment and a trial was judged to be the best way of accomplishing this objective.
This should be supported by the health service as part of the programme of development that any large organisation must adopt if it is to remain up to date and use its resources cost effectively.
The country would consist of a network of multicentre prevention trials funded by the NHS.
Apart from the need to provide the necessary explanation about each trial, they need not be considered substantially differently from other activities of the health service.
A serious obstacle to the conduct of such research, which is often necessarily large scale, is the need to consult many ethical committees.
Clearly, the ethical aspects of each preventive trial need consideration, but this should be the responsibility of a central ethical committee and not a matter to be considered separately by many (perhaps over a 100) district research ethical committees that are, in the main, concerned with local clinical research projects.
The view that the ethical aspects of preventive trials can only be properly considered locally is unjustified.
In general, the opposite is the case: the scale of the research and the kind of expertise required to assess the scientific, and therefore the ethical, validity are such that the research is best considered centrally.
Underlying principles
Four principles therefore underlie randomised prevention trials.
Much would be achieved if these were more widely acknowledged.
Firstly, the ethics derive from the interpretation of science as they do in all randomised trials; secondly, quantitative estimates of efficacy are needed, to be balanced against reasonable estimates of possible adverse effects; thirdly, in prevention trials the ethics can be largely a resource based argument with equity an overriding consideration; and, fourthly, there should not be one law for practitioners and another for researchers.
The obstacles to preventive research are found in many countries, including Britain, which has a distinguished history in epidemiology, public health, and preventive medicine.
In spite of this Britain still has some of the highest rates of preventable disease in the developed world, such as for ischaemic heart disease and lung cancer.
Too little is done to implement existing preventive knowledge or to develop preventive strategies known to be effective.
Britain has a relatively poor record of instituting randomised prevention trials; the Scandinavians, by comparison, have been more enlightened and more active in breast cancer screening.
The importance of such research is not recognised in Britain.
Individuals need to be persuaded that a concerted approach and a specific commitment of resources are required.
In the United Kingdom, the NHS plans to spend about £36 billion in the calendar year 1993 (about 15% of public spending, or 5.7% of the gross domestic product); a small percentage of this should be devoted to the conduct of prevention trials and the implementation of worthwhile new preventive medical services.
The scale of the venture means that a nation's health service must accept a major responsibility for the work.
Countdown to Community Care
Newcastle: ‘If it doesn't work here, it can't work anywhere’
Newcastle upon Tyne, a city of 273 000 people and regional capital of the north east, starts off with several advantages for community care.
Some are structural — the health authority, family health services authority, and local authority are coterminous — while others stem from tradition and a strong sense of community.
Despite the familiar fears about the implementation of community care — not enough money, buck passing, and bed blocking — I was struck by the optimism of social services staff.
Even so, there's a clear split between the planners and the practitioners: the former see the new framework as strengthening existing good professional relationships; the practitioners, in the health service particularly, fear it as a layer of bureaucracy that may undermine those relationships.
Longstanding good relationships
Newcastle's Labour council has long had a policy of supporting its community — a policy that has informed many aspects of city life.
Thus the underground transport system, the Metro, was built with disabled people in mind and council residential homes buy all their supplies locally, to support their communities.
The council spends heavily on social services, partly as a general policy and partly to provide targeted services to vulnerable groups.
Brian Roycroft, director of social services, is clearly proud of the standards of city provision and its responsiveness to users and carers.
Roycroft also claims — and many in the health authority agree with him — that relationships between the social services department and the health authority, both managerially and professionally, have long been good and have helped underpin a high level of community care.
Hence his belief that if community care can't work in Newcastle ‘it can't work anywhere.’
Implementing community care
Possibly because of this widespread feeling that relationships between health and social services were good, planning for April 1993 got left behind, and the city has spent the past few months frantically catching up.
‘If you had asked me a year ago,’ said Roycroft, ‘I'd have said everything was going wonderfully well — but then it all fell apart.’
The main problem was rate capping for the city council, which meant that the social services department had to save £4m and lose 400 staff.
Homes for the elderly were shut, and formerly nominal charges increased and extended.
Three of the four people on the community care implementation team left.
At the same time the chairman and chief executive of the Newcastle Health Authority left amidst acrimony about restructuring within the Northern region.
Realising that things were getting behind, Roycroft pulled together a policy group consisting of himself, Gary Smith, and Clare Dodgson (chief executives of the health authority and the family health services authority).
Soon afterwards the chairman of the local medical committee, Dr Frankie Walters, joined the group — a move that many in Newcastle think important both symbolically and practically.
The community care plan
The core work on client groups for the 1993 community care plan is being done by the six joint care planning teams (covering aging, physical disability, learning difficulties, mental health, HIV/AIDS, and drug and alcohol problems).
These have representatives of the independent sector, users and carers, ethnic minorities, and the housing department as well as social services, family health services authorities, and health authority members.
Although the teams have an important role in constructing the plan, they have tended to be marginalised because they control few resources.
As John Harvey, director of public health, put it, many of the teams take a very global view for their client group, consult widely, come up with good ideas, and are responsive to users, but their influence is tiny.
‘They have become forums for airing aspirations.’
Harvey would like to see them bound more closely into the decision making process.
As an example he cites a review that the health authority and the mental health trust have just completed of their mental health strategy, using the planning team as an important part of the consultation mechanism.
One of the recommendations is that the planning team should implement the review by analysing different ways of providing particular services — crisis intervention, for example— and making recommendations to the purchasing authority.
The purchaser-provider split
To implement a purchaser-provider split within Newcastle social services department the plan is to let the fieldwork section, based in area social work teams, act as purchasers and the residential and day care section (which includes home care) to act as providers.
Few people in the department see this split as being very real at the moment, particularly as it will be some time before budgets are devolved.
Roycroft doesn't particularly like the idea; he thinks assessment and provision go together for individuals — a view shared by most of his social workers.
Meanwhile the problem for the health service is that although the health authority sits on the policy group and is a coauthor of the community care plan, the work gets done — and the problems arise — in provider units.
Both Gary Smith and Barry Dowdeswell, chief executive of the Royal Victoria Infirmary, Newcastle's oldest teaching hospital, agreed that the acute hospitals had only just woken up to community care.
Only now are they starting to organise training for their staff.
As a result, said Dowdeswell, they had realised how lucky they were with their hospital social workers: ‘The hospital teams have been very well protected within social services.’
Nevertheless, there are fears.
Roycroft and Smith worry about perverse incentives: the fact that hospital treatment costs nothing, while people have to pay for the home care that will keep them  out of hospital.
Dowdeswell worries about meeting contract commitments and the costs of having a geriatrician concerned in all complex assessments in elderly people.
Doctors and nurses worry about bed blocking.
The ‘real backbone’ of Newcastle's social services; home care…
Within the hospital
Nurses at the infirmary have just been praised by the Audit Commission for their work on discharge planning, and a group of sisters in the orthopaedic unit confirmed that they had long taken this seriously.
They discuss any problem cases with their hospital social worker each week, occupational therapists routinely make home visits before discharge, and if minor adaptations are needed a hospital technician will do them.
Nevertheless, there are delays in discharge at the moment, often caused by silly things like a lack of commodes, and the nurses worry that if everything has to be in place before discharge then there will be more delays.
But the blame is always with the system, not the individuals: ‘We're lucky with our social worker — she works wonders,’ said one of the sisters.
The strongest criticism I heard came from Dr Alistair Brewis, consultant chest physician and medical director of the Royal Victoria Infirmary.
Not realising that the documents on discharge and assessment had been produced in a hurry to meet the government's suddenly imposed deadline of 31 December, he criticised the two weeks he had been given to consult his 200 consultant colleagues.
‘The act is all about communications — but here's a bad way of communicating at the outset.’
He found it difficult to get consultants interested in community care because it was only one on a long list of issues.
Terry Sangwin, the nurse manager for the medical unit, thought staff in the hospital still needed much more information about what would happen, and she feared the planners didn't realise the impact community care might have on acute hospitals.
Both she and Dr Brewis were also concerned about the many patients on their wards who came from outside Newcastle: they knew even less about the arrangements for community care within those districts.
Jackie Marston, manager of the hospital social workers, was still negotiating funding for social work links with these surrounding districts when I met her and some of her team at the hospital.
They were very positive about community care and saw it as natural development of their work.
Marsden explained that hospital social workers had negotiated their role in discharge in the 1970s, when the Freeman Hospital had been built —‘discharges don't happen without social services input’— and had then spread the same working practices to Newcastle's other two acute hospitals.
They were very aware of the fears about bed blocking.
Indeed, they had just finished a three month study, half funded by the hospital ‘at a time when it didn't have a penny’ to assess bed blocking on one medical/geriatric ward that had particular problems.
The delays were caused mainly by patients waiting for long term care (because private provision hadn't kept up with demand).
As a result Anne Wilson, the ward's social worker, is now funded half by the hospital to work at the Sanderson rehabilitation centre to liaise with community social workers and families.
Anne Wilson and her colleagues were as concerned as anyone that assessments are done promptly.
‘Not delaying assessment and placement is important to the user and family — it maintains their confidence.’
They too mentioned things like delays in supplying commodes that currently kept patients waiting.
All these examples, they thought, simply highlighted the need for better coordination.
Indeed, Kate Weightman, programme director at the community health unit, thought that the planning envisaged by the community care plan would actually speed throughput by instilling the need to plan discharge from the outset of a hospital episode — and before when possible.
Her unit is already used to joint working with social services.
It manages the joint inspection unit for nursing and residential homes and runs the joint loan equipment service (the one that never has enough commodes).
She is also running awareness training in community care which brings together community health staff, general practice nurses, and social workers, and her district nurses already provide some joint training for their own auxiliaries and social service home carers.
Assessment
The fear about bed blocking is acknowledged in Newcastle's assessment procedures, which speak repeatedly about the need not to delay discharge.
Three levels of assessment are envisaged, and most people think that only the last of these — a complex assessment — will be new.
The first level is simply to provide people with information on services — through hospitals, general practitioners' surgeries, social services offices, and public libraries.
Anyone can then request an initial assessment of needs, which will be provided if the person falls into one of the client groups (elderly; mentally ill; with learning disabilities; with physical or sensory disability or chronic illness; with drug or alcohol problems; or at risk of infection with HIV) and is experiencing difficulty with daily living.
Carolyn Stephenson, principal assistant (community care) in the social services department, doesn't think any of this is different from what staff do already.
People may, for example, be referred by general practitioners, who would be expected to provide any relevant information.
In future any referrer will have to provide an agreed minimum data set.
Moreover, ‘if a person has been assessed by a general practitioner or a nurse for health reasons and they have identified social needs at the same time then we are committed to accepting that assessment,’ said Stephenson.
She said  that some social workers were uneasy about that, and she agreed that it involved trust, but it was also to avoid abandoning the problem to someone else.
What is also new, she emphasised, is that the assessment is of a need, not for a particular service.
It will then be up to a care manager to decide on the level of service.
This will probably involve a social worker in an area team putting together a package that might consist of a few hours of home care, a couple of days of day care, and night sitting for one night.
‘Probably these people don't need a formal care manager, though someone will be responsible for them,’ says Stephenson.
The impact of the new legislation will be felt most for people who have complex needs identified as a result of a comprehensive assessment.
The key criteria are that a person has a range of needs that cannot be met by one agency alone and his or her ability to live independently is in jeopardy.
This is the level where care managers will come into their own, thinks Stephenson.
They need not always be social workers, and indeed, there is one care management pilot going on in the city that has a district nurse as a care manager.
Here too, eventually, is where there will be budgets for care.
Budgets and services
But what will those budgets buy?
Before the allocations were known the social services department had worked out that ideally it would need £5.75m.
It has got £3.2m.
Already some social services have long waiting lists because demand outstrips supply.
Nevertheless, Graham Armstrong, assistant director of social services responsible for finance and administration and now leading the community care project team, is more worried about the second and third years.
Ninety per cent of people in residential care in Newcastle are on income support and the average time to death or discharge in residential care is three years, so his department will be picking up the whole burden by the third year.
The main shift as he sees it is that the service has become cash limited overnight.
‘So we have to get the eligibility criteria tied down very tight.
Yet if you establish criteria for nursing and residential care you are adjusting to resources, not need.’
In the first year Armstrong's concern is to go for a minimum safe agenda —‘make sure clients are safe and then build on that.’
He has not therefore got very far with devolving budgets.
In 1993 they will be allocated to area managers but not yet to care managers.
This fits in with the traditionally tight control that local authority finance directors like to keep.
Armstrong's other concern is that budgets should be tightly tied to the information strategy, and his department is not far advanced with that.
Another concern is to ensure that new sorts of responsive services are there to meet identified needs.
As well as managing the market the social services department is having to nurture it.
Stephenson thinks that unless there are no informal carers (in which case providing continuous support at home is much more expensive than residential care) much can be done to keep even very dependent people at home.
But first those services need to exist.
A night sitting service, for example, needs a certain number of permanent trained staff.
‘So there will have to be some block contracts to provide predictability for the provider,’ says Stephenson.
Likewise, voluntary bodies need funding to enable them to bid for contracts.
The department had hoped to reach flexible service agreements with many voluntary bodies, but the city's lawyers told them they either had to give grants (guaranteeing nothing in return) or set binding contracts.
Care management
Legal problems have also dogged some innovative experiments within the care management pilots the city is running.
Lynn Boyle, a social worker from area 2, told how the Inland Revenue had scuppered a plan to get unemployed women to provide informal care in return for expenses and small payments.
The tax authorities insisted they put the women on the payroll.
Like her boss, Boyle doesn't see why assessment and care management should be separated.
She sees assessments as involving a dialogue with the client, with social workers sometimes doing therapeutic work as part of the process.
She thought enforcing a distinction would simply become bureaucratic.
She also told of how social workers could stimulate resources very locally.
Her area has many people with learning disabilities; the community is tolerant and the housing stock suits conversion.
Both the housing department and housing associations had helped with accommodation, and the social work team had started luncheon clubs and drop in centres for people with learning disabilities who did not go to day centres.
They then persuaded user groups, voluntary bodies, and churches to take these services over.
Boyle also spoke of her colleagues' traditionally mixed views about general practitioners.
Whether much good would came out of closer formal links between social workers and general practices very much depended on the general practitioners' attitude.
‘GPs don't understand what we do’— they referred inappropriate patients, and were often reluctant to pass on information.
She also conceded, however, ‘We tend not routinely to inform GPs.
There isn't a sense  of coresponsibility, and we need to change that.’
When the two groups do work together the outcome is better.
Boyle cited a recent example of an elderly woman who was being physically abused.
The doctor had helped assess the problem because she had a good relationship with her patient and had documented all her injuries.
Another case in which good relationships would have helped was that of an attention seeking woman who had spent most of her life in hostels and was constantly getting struck off general practitioners' lists.
‘It would have been nice to have worked through the issues with the GPs and negotiated with the patient.’
The Metro, a symbol of Newcastle's commitment to supplying people in the community
General practitioners
Dr Frankie Walters, chairman of the local medical committee and a general practitioner in west Newcastle, agrees with Boyle's analysis that relationships between general practitioners and social workers have not always been good.
She too is optimistic that they are changing: ‘We are learning to talk much more openly without scoring points.’
As a result of her involvement in the policy group all 46 practices in Newcastle were consulted over the draft discharge and assessment arrangements and had an opportunity to influence them.
The family health services authority has funded Walters' locum costs (‘14 meetings in two months’) and the costs of training for general practitioners and their staff, and Brian Roycroft came and spoke to well attended meetings of fundholders and non-fundholders.
One thing he did was to reassure them that planning was not as far advanced as they had feared.
Nevertheless, most general practitioners remain concerned about the lack of detail in the assessment and discharge documents — and are waiting to see the more detailed information that is promised before April.
They also worry that assessment procedures will raise expectations that can't be met.
Walters knows well enough not to promise to deliver for general practitioners.
‘The most I can do is say, ‘This is what GPs are thinking’.’
Also, both she and Clare Dodgson, chief executive of the family health services authority, worry that, despite the well attended meetings, there are still some general practitioners who know nothing about community care.
‘If they don't know by 1 April, it won't be through want of trying,’ says Walters.
That so much effort has gone into getting general practitioners on board is a tribute to the widely held view within social services that general practitioners are not central to community care, but their capacity for throwing a spanner in the works is considerable.
Yet both Clare Dodgson and John Harvey, director of public health, see beyond that.
Dodgson thinks that the debate over community care has brought out a demand among general practitioners for attached social workers, particularly among fundholders.
Harvey also sees general practitioners as having a key role in the information strategy for community care.
Despite the firm figures in the community care plan (see table), and social services managers' confidence that they know the level of need in the city, he thinks they know little.
‘Need is not a static concept: we need a longitudinal view and to understand how needs change.’
He wants to know, for example, the risk among 100 people aged over 80 of any of them needing care over the next five years — because a carer dies, they have a stroke, or they develop dementia.
As part of a locality purchasing project in the east end of the city he is working with several practices to see how general practitioners can provide this sort of information.
The future
The locality purchasing project also raises questions about joint purchasing.
Everyone agrees that if the service is to be seamless there are enormous benefits in having people providing care across the health-social care boundary —‘one person providing bathing, bandaging, and hoovering’— and not bothering about who pays.
But there are huge political difficulties with the latter.
‘Councillors are not going to give up their responsibility for how money is spent,’ says Roycroft.
Themselves accountable to an electorate, councillors are sceptical about the accountability of health authorities — and about health authorities knowledge of the community: to them it's simply what happens outside hospital.
Nevertheless, most of the planners in Newcastle think that the issue of joint purchasing and a wider debate about dissolving the boundary between health markets and care markets is one that the community care arrangements will increasingly force upon them.
But all that is some way off, and not confined to Newcastle.
For the time being Brian Roycroft assures everyone that they won't fall off a precipice on 1 April and that community care is going to work.
Although Newcastle is having to run very hard to catch up with the mechanics of community care, Roycroft thinks that in terms of the spirit of the act, the city is already way ahead.
‘On things like consulting with patients and carers — we've been doing that for years.’
Some guidelines on the use of cost effectiveness league tables
Decisions to allocate resources in health care are increasingly influenced by relative cost effectiveness.
To warn decision makers of some of the pitfalls currently found in cost effectiveness league tables and to suggest how meaningful comparisons may be made between health care technologies a published league table was scrutinised by examining its sources.
This showed some of the methodological problems surrounding such tables and how such difficulties could be reduced in future.
The source studies in the table featured different years of origin, discount rates, health state valuations, settings, and types of comparison programmes; all of these differences may raise problems for meaningful comparison.
Decision makers need to assess the relative value for money of competing health care interventions.
In the absence of systematic comparisons such assessments are likely to take place informally.
This will probably have a worse risk-benefit trade off than the formalised use of league tables.
In recent years comparisons between health care interventions in terms of their relative cost effectiveness in cost per life year or per quality adjusted life year (QALY) gained have become fashionable.
The principal motivation for this comparison is to help decisions about the allocation of health care resources.
For example, Williams calculated the cost per QALY of a range of health care interventions and divided them into ‘strong candidates for expansion’ and ‘less strong candidates for expansion’1 to illustrate what could be achieved by using available data and to argue for further refinements.
Recently, Laupacis et al proposed that the adoption and use of new health technologies should be classified into five grades of recommendation based on their cost per QALY, although they also acknowledged that many other issues other than cost effectiveness, such as ethical and political considerations, affect the implementation of a new technology.
Cost effectiveness league tables have recently attracted the interest of decision makers in health care, such as those concerned with the allocation of public resources in the state of Oregon.
The role of league tables has also been discussed in the British Department of Health (H Neuberger, personal communication), although a final decision on the appropriateness of their use has yet to be made.
Many argue that a league table of costs per QALY, if properly constructed, can provide comprehensive and valid information to help decisions on the allocation of resources.
The potential dangers of the unthinking use of league tables, however, as currently presented have been pointed out by several authors.
Despite such reservations, comparisons of cost effectiveness between health care programmes will inevitably be made by both analysts and decision makers, either formally or informally.
We report some inappropriate comparisons and make some recommendations for constructing future league tables.
We examined a recently reported league table and identified methodological deficiencies.
We have presented guidelines to help decision makers interpret league tables as currently presented.
Problems with cost effectiveness league tables
The paper by Maynard contains a discussion typical of league tables.
He presents a table containing 21 health care interventions (see table) and writes: The methods used to produce such estimates are crude but do facilitate the production of a ‘league table’ which ranks the cost of producing a QALY by investing in competing treatments.
An example of these ‘guesstimates’ is set out in Table 1.
The implication of these data is that resources should be invested by purchasers in treatments which produce QALYs at low cost.
Note that there is not necessarily presupposition that activities should be funded in the order they appear in the list nor that the same ranking holds for every location.
There is a risk, however, that this inference may be drawn.
The mere act of assembling data on a range of interventions gives greater prominence to the cost effectiveness data than does the reporting of the studies individually.
It also implies a certain degree of homogeneity in study methods.
That is, the reader is invited to believe that the ranking of interventions in the table is a function of their relative value for money, independently of the methods of the original economic evaluation study or the quality of the clinical evidence on which it was based.
Closer examination of published league tables shows that the methods of the source studies are rarely homogeneous.
We found that 20 of the estimates originated from six source studies, with one being of unknown origin.
In interpreting the league table a number of methodological features are particularly important; these are discussed in turn.
year of origin
The years of analysis in the source studies ranged from 1982 to 1989, although all estimates were converted to 1990 prices for the comparison.
The analysis in each source study reflects the then current state of knowledge about costs and benefits.
Technological change may mean it is inappropriate to use results for  which the ‘sell by’ date has expired — that is, the reflating of older values may be inaccurate.
In addition to technological change, shifts in the relative prices of different interventions over time may affect the ranking.
discount rate
The discounting of costs and benefits occurring in the future was mostly consistent at 5% a year in real terms.
This reflects the consensus in reports on economic evaluation and, as all the source studies were from the United Kingdom, the discount rate recommended by the Treasury.
One of the six sources, however, responsible for three of the 21 estimates of cost per QALY did not discount either costs or benefits.
As this study was concerned with preventing death, or as much as 30 years of long term serious disablement, by neurosurgical intervention the effect of this omission is considerable.
In some of the other source studies discounting was quite appropriately not applied as all costs and benefits occurred within a year.
The issue of discounting has been debated recently after a Treasury decision to apply a zero rate in the case of health benefits.
Therefore there may not be as much agreement on this point in economic evaluations carried out in the future.
The onus is on authors of the original studies to provide cost effectiveness ratios with a range of discount rates, a minimum requirement being 5% and 6% a year on costs and 0%, 5%, and 6% on benefits.
preference values for health states
Preference values for health states were most commonly derived from the matrix developed by Kind et al .
Other of the source studies, however, used patient values, clinician values, or educated guesses.
Different approaches to the estimation of utilities for health states generate different values; this has the potential to reduce greatly the comparability of the six source studies.
The degree of homogeneity in this league table, however, is greater than many because of the relative prominence of the ‘Rosser matrix’ in cost utility analyses in the United Kingdom.
If league tables from North America were being reviewed these would probably show greater variation in analytical approach as there has been considerably more research into, and validation of, measures of health state preference.
(See, for example, the table reported in Torrance and Zipursky.)
At present it is difficult for users of league tables to satisfy themselves of the validity of the estimates of benefit used once these estimates have been concealed in a league table.
Limiting the studies included to those that use the same measure of quality of life might lend a table some face validity.
If in future, however, reporting of each estimate is extended to include descriptive profiles of quality of life — that is, health states and the valuations placed on them for the new and comparison treatment — then this may enhance credibility and allow reworking in alternative settings.
Writing in the context of quality of life measurement, Cox et al pointed out that in reporting data the analyst has to serve unknown future users with unknown future interests.
range of costs
Detailed investigation of the range of costs considered in each of the six source studies was hampered by inadequate descriptions of methods of costing.
This problem is likely to be encountered often by decision makers wishing to assess the methodological quality of published studies.
In most of the source studies, however, only direct health care costs were included in the ratio of cost per QALY.
Only one estimate included consideration of patients' time, which biased the ranking of this procedure downwards in the league table.
If more detailed information were available on costing methods further discrepancies in analytic approach would probably be exposed, with further consequential shifts in the league table.
If future league tables report disaggregated profiles for resources and costs for each study then the costs may be validated or recalculated for the decision maker's own setting.
comparison programme
The source studies reported in the table differed in respect of the type of comparison programme from which the incremental cost per QALY is assessed.
The ‘baseline’ for comparison may have a major impact on these costs.
If a comparison programme of no formal intervention is chosen it is important to establish that the costs and consequences of this have been valued credibly.
‘Doing nothing’ is unlikely to have zero costs and zero benefits.
Even in the case of a screening programme (when doing nothing is a real option) some cases would be found even in the absence of any organised service and there would also be the costs of treating disease as and when it occurred.
Thus, for most economic evaluations, the notion of an option with zero costs and zero benefits is not relevant.
A central problem for the choice of a comparison programme lies in the perception of the ‘margin.’
In other (non-health) areas of social policy economic evaluations usually start with the presumption that all current activities are efficient — that is, worthwhile when compared with all alternatives.
Additional money is allocated (at the margin) to the next most worthwhile activity.
No such margin exists for health care provision, however, much of which has never been scientifically evaluated.
Instead, health care provision has a political margin with budgets allocated on criteria other than cost effectiveness.
Decision makers should satisfy themselves that current practice is itself worth having before using it as a comparison for a new treatment.
If the comparison programme is inefficient the analysis will be misleading.
Some of the source studies used to create the table compared with an alternative of ‘doing nothing,’ others compared with a minimum intervention (which may differ among programmes), and yet others considered the incremental cost per QALY of expanding services to other groups of patients.
In such a case, with restricted and non-standardised comparisons, interpretations across the programmes in the league table are fraught with difficulty.
setting of study
All the studies reported in the league table by Maynard originated from the United Kingdom.
Additional complexities are involved if league tables contain studies from several countries.
Gray pointed out the need to make adjustments for exchange rates by using appropriate purchasing power parities for medical goods and services (Health Economists Study Group, Aberdeen, 1991).
A comparison of this approach with simple exchange rate conversions changed the rankings.
More importantly, however, differences also exist between countries in clinical practice, the availability and relative prices of health care resources, and the incentives given to health professionals and institutions.
In a recent analysis that compared economic evaluations of the same intervention (a new drug) performed by identical methods in four countries, Drummond et al showed that although the drug was 36% more expensive in the United States than in the three European Community (EC) countries considered, it was also more cost effective in the United States because of the relative price and use of other health care resources compared with those in the EC (such as the higher charges for surgical operations).
Considerable care should be exercised when extrapolating the results of cost effectiveness from one country to another or including them in the same league table.
quality issues for construction of tables
The descriptions of treatments listed in league tables are sometimes inadequate.
Cholesterol testing and treatment seems good value at £1480 per QALY.
This value, however, presumes opportunistic screening by general practitioners and the use of a series of laboratory tests on blood samples.
It says nothing about the setting up of programmes of screening by invitation or the use of desk top analysers.
Many authors have used estimates generated by others in their own league table; did they check the original analyses?
In the table the value for continuous ambulatory peritoneal dialysis is not attributable to any of the source studies.
The whole table has been reported here and elsewhere.
Given the authoritative ‘halo’ effect that such tables evidently acquire, accurate reporting is a most basic requirement.
Finally, the representativeness and quality of contributions to the league table may be questionable.
For instance, Teeling Smith's value for antihypertensive treatment to prevent stroke is clearly an outlier, with other analysts giving much higher values.
Unfortunately, often it is not possible to comment on a given source study's assumptions as only summary calculations are presented.
Studies in which essential information is not available should not be included in league tables.
Some guidelines for using current league tables
Users of league tables should compare their own setting with that for the source studies included.
In particular, they should pose three related questions.
Firstly, in my setting could the interventions listed be applied with the same likely success as in the time period and setting reflected in the league table?
(For example, are the outcomes generalisable to my local need and clinical practice, and are the same service structures in place?)
Secondly, are the comparisons embodied in the various estimates of cost effectiveness relevant to my local situation?
(For example, if a comparison of a new intervention is made with current practice, minimum practice, or doing nothing do I agree that in my setting the same comparison programme would apply?)
Thirdly, are the resource implications and consequent costs similar to those pertaining in my setting?
(For example, how does my local availability of health care resources compare with that in the source study?)
It may reduce contention if comparison in a league table is restricted to those interventions targeted at one condition (such as cardiovascular disease) or at one group of patients (such as elderly people).
Although, inevitably, broader comparisons need to be made when allocating the whole health care budget, the homogeneity of the underlying studies is undoubtedly greater within more restricted boundaries, such as focusing on the mix of services for a given group of clients.
Currently league tables present only point estimates; this conveys a false sense of precision and encourages unwarranted conclusions.
Possible changes would include the reporting of sensitivity analyses in league table documents or, as more economic evaluations of health care interventions are undertaken alongside controlled clinical studies, the use of confidence intervals around estimates.
Even a methodologically sound table is unlikely to feature estimates with a high degree of precision; they should be viewed in this light.
Cost effectiveness estimates should not be used in a mechanistic fashion; at best they provide a useful aid for decision making.
Other factors — for example, equity — legitimately influence decisions.
In particular a given league table is unlikely to contain all the relevant comparisons of programmes of the appropriate intensity or scale to enable a budget to be allocated.
To approach this in a more formal sense would also require mathematical programming techniques.
Conclusions
Although league tables of costs per QALY may help decision makers in the allocation of resources, potentially they can also mislead.
One approach would be to abandon the construction and use of league tables altogether.
Whether or not league tables are reported, however, there is a natural tendency for evaluators to compare their study results with those for other health care programmes.
Also, there is a need on the part of decision makers to assess the relative value for money from competing health care interventions.
In the absence of systematic comparisons, as embodied in league tables, these assessments of relative value for money are likely to take place informally.
This will probably have a worse risk-benefit trade off than the formalised use of league tables.
Against this background we have outlined some of the problems with current league tables and made some suggestions for improving their construction and interpretation.
Even if our recommendations were adopted by those undertaking economic evaluations and those constructing league tables, however, decision makers should still exercise the appropriate caution, care, and intelligence.
Foreign body aspiration: a hazard of metered dose inhalers
Metered dose inhalers should be carried with their safety caps on to prevent foreign bodies entering them and being propelled into the bronchial tree when they are discharged.
Metered dose inhalers have a well established role in the management of bronchial asthma.
If inhalers are carried in bags or pockets without their safety caps on foreign bodies may enter their mechanism and be expelled forcefully into the bronchial tree.
The ensuing symptoms are often difficult to distinguish from those of an acute attack of asthma.
We describe two cases of accidental aspiration of a foreign body after use of a metered dose inhaler.
Case reports
case1
A 24 year old woman who had had well controlled asthma since childhood developed acute chest tightness and wheeze.
She took her salbutamol inhaler from her pocket and used it.
She then felt a choking sensation with increasing wheeze, tightness, and cough and rushed to the nearest accident and emergency department.
She was 13 weeks pregnant, and her asthmatic attack was attributed to her pregnancy.
She was treated with nebulised salbutamol, which relieved the symptoms, and was reassured and asked to attend the general medical unit of her hospital for follow up.
She developed recurrent exacerbations of her asthma and regular use of salbutamol ipratropium inhalers and high doses of inhaled steroids were prescribed.
After her delivery the symptoms did not decrease and a maintenance dose of oral prednisolone, 12.5 mg/day, was also prescribed.
Despite this she continued to wheeze daily and thought that her inhalers were of no benefit.
Almost a year after presenting to the accident and emergency department she was referred to our chest unit for observation and treatment.
On arrival she complained of cough with purulent sputum, haemoptysis, and pain in the right side of the chest in addition to her asthmatic symptoms.
A chest radiograph was normal, but a ventilation-perfusion lung scan showed a matched defect in the right lower lobe.
Staphylococcus aureus was isolated from her sputum but she failed to respond to antibiotics, so a fibreoptic bronchoscopy was performed.
This showed a conical hard object lodged firmly in her right intermediate bronchus, granular tissue, and purulent secretions.
A valve cap from a car tyre (13 mm long) was removed at rigid bronchoscopy.
Her peak flow rates, which had improved from 200 to 350 l/min with nebulised bronchodilators, increased to 450 l/min, and her oral steroids were reduced.
The time between her accident and the correct diagnosis was one year.
case 2
A 20 year old woman with mild asthma requiring intermittent inhalation of salbutamol developed acute chest tightness.
She took out her inhaler, which was in her pocket without its safety cap on, and used it.
She immediately choked, coughed, and felt that she had swallowed something.
A one penny coin was recovered from her stools two days later.
Discussion
Aspiration of a foreign body during use of an inhaler has been infrequently reported since the first report in 1981.
Small objects like pennies, tablets, and screw caps can easily lodge in the mouthpiece of a metered dose inhaler and, when the inhaler is used, be forcefully expelled into the pharynx or larynx and inhaled into the bronchial tree.
The importance of replacing caps on the mouthpieces of inhalers to prevent this must be emphasised.
In both the reported cases the patients stated that they had inhaled or ingested a foreign body.
In case 1 the similarity of the symptoms to those of an acute asthmatic attack and the fact that a nebulised bronchodilator resolved the acute event were misleading.
The patient's persistent and frequent respiratory tract infections and the inadequate control of her unstable asthma with maximal treatment could, however, have suggested the presence of a foreign body.
Such a possibility should be considered in all patients with unstable asthma that is difficult to control with conventional treatment.
In case 2 the symptoms resolved rapidly and the patient did not require further  treatment.
Doctors should take a careful history and be aware of this problem in the regular follow up of patients using metered dose inhalers long term.
Careful instruction in the use of metered dose inhalers is an essential part of educating asthmatic patients.
Patients should take a slow sustained inhalation when using their inhalers rather than a rapid deep breath as this may reduce the likelihood of inhaling a foreign body so deeply.
Dry powder devices such as Diskhalers and, particularly, Turbohalers are possibly less likely to accept foreign bodies because of the fairly small sizes of their mouthpieces.
Nevertheless, in view of the risks of inhaling a foreign body an inhaler's safety cap should always be replaced when it is not in use.
LETTERS
Umbilical cord clamping in preterm infants
Editor ,— The possibility that late clamping of the umbilical cord may lead to a reduction in respiratory distress syndrome is not a particularly new finding, but S Kinmond and colleagues offer a more scientific approach in evaluating this phenomenon.
However, they give little information about the obstetric features of their cases, which might be quite important since matching is essential with such small numbers — for example, were the babies predominantly born during preterm labour or were they delivered electively for some obstetric complication?
Also not clear is whether any of the mothers received steroids to accelerate fetal maturity before delivery.
Although these matters may not be particularly important as far as the haematological features are concerned, they certainly are with respect to ventilation.
I believe that more information on them would be helpful in the interpretation of the paper.
Editor ,— S Kinmond and colleagues' results require further consideration.
Immediate cord clamping may deprive the infant of placental blood, and previous reports suggest that a long delay may lead to adverse effects due to hypervolaemia.
Holding the infant 20 cm below the introitus for 30 seconds before clamping the cord probably enables an intermediate and beneficial volume of placental blood to enter the fetal circulation.
The question arises as to why a longer (60 second) delay should allow a larger and thus excessive transfusion.
Almost all women in Britain receive oxytocin-ergometrine (Syntometrine) at delivery, which causes an increase in plasma oxytocin concentration within 45 seconds.
The resulting contraction could force an excessive volume of blood into the neonate.
If so, the cord should be clamped before the increase in plasma oxytocin concentration.
The time between administration of an oxytocic agent and cord clamping rather than fetal delivery and cord clamping may thus be critical.
The authors do not state whether their patients received an oxytocic agent or document its time of administration.
Further consideration of the physiological process of labour at term raises an additional point.
Even when an oxytocic agent is not given a small proportion of patients show an increase in plasma oxytocin concentration during the late second or early third stage of labour.
This increase in endogenous oxytocin could theoretically cause uterine contraction and excessive transfusion within 30 seconds of delivery.
Did the authors record the time of uterine contraction after delivery and, if so, was there any relation to neonatal outcome?
Editor ,— S Kinmond and others' study of early versus late umbilical cord clamping shows an apparent improvement in outcome with a simple intervention but raises some questions.
Firstly, is the position of the baby or the timing of the clamping of the cord the primary factor?
If the position of the baby is important maybe the two babies in the intervention group who were not held below the introitus should not have been included in the final analysis.
Equally, one infant in the random group was held below the introitus, and the position of the others is not stated.
Over the whole study was position a significant variable independent of the study group?
We note that the unit's policy was consistent during the study.
One of the significant outcomes was the volume of blood transfused, and therefore it is important for the transfusion policy to be stated, especially as the text does not suggest that the staff were blind to the study groups.
On the basis of the figures given for the number of infants requiring blood transfusion there is no significant difference between the two groups for either ventilated infants or all infants (χ 2 test, two by two analysis).
There were three babies with chronic lung disease in the random group and none in the regulated group, but the total in the study is too small to say if either figure is different from the expected incidence of about 10% in this population of babies.
The blood pressure stability is said to have been no different between the two groups.
Was this an overall effect on blood pressure or specifically the beat to beat variability?
Beat to beat variability has been used as an indicator of hypovolaemia in preterm infants at risk of intraventricular haemorrhage.
The proposed mechanism of improvement of placental transfusion of volume and red cells is attractive.
As the authors have pointed out previously, packed cell volume is a poor indicator of red cell mass and a better measure would have been useful.
The improvement may well be largely related to an increase in volume leading to an increase in blood volume in the lungs — an effect shown by transthoracic impedance techniques.
As the authors state, it is important for neonatal research to look at simple interventions such as this.
We believe, however, that further study is needed before firm recommendations about umbilical cord clamping can be made.
Editor ,— The paper by S Kinmond and colleagues suggests that modification of a common practice — clamping of the umbilical cord — may have important implications for the wellbeing of immature babies.
Most of the previous research into the timing of cord clamping has concentrated on babies born near term.
Rigorous steps to avoid selection biases seem to have been taken in only two of the four studies of immature babies — namely, those of Hofmeyr et al .
The first of these two studies is referred to in Kinmond and colleagues' paper as showing a reduction in the incidence of periventricular and intraventricular haemorrhage with delayed cord clamping.
In addition, however, this policy was associated with more neonatal deaths.
As the authors explain, the small sample size (38 babies) led to imbalances in the baseline variables, which could explain the differences in outcome.
Because of this the investigators subsequently carried out a larger trial (in 86 babies), which did not confirm either the reduction in haemorrhage or the increase in mortality.
There also seem to have been considerable baseline imbalances in the trial reported by Kinmond and colleagues, judged on the few characteristics described in table I, and so the same concerns must apply.
The inclusion of birth weight as one of the enrolment characteristics is probably inappropriate.
Increased placental transfusion in the regulated group should surely have resulted in an increased birth weight, as found in other studies in which cord clamping was delayed.
Birth weight is therefore more reasonably considered to be an outcome variable.
The principal outcome, and the measure on which the trial sample size was based, is the number of transfusions.
In appraising the trial's results it would therefore be helpful to know the indications for transfusion that applied at the Queen Mother's Hospital during the trial and whether the decision to transfuse was made without knowledge of the policy on cord clamping.
Immature babies are even more likely than others to have their umbilical cords clamped and cut immediately after birth to allow resuscitation and transfer to neonatal intensive care.
Kinmond and colleagues' study suggests that a short delay  may improve outcome without any obvious deleterious effects.
But, on the basis of currently available evidence, uncertainty remains.
Before widespread changes in practice are made this finding needs confirmation in a larger trial with mortality and major respiratory and cerebral morbidity as the primary outcomes.
Authors' reply ,— We agree that umbilical cord clamping at preterm delivery warrants further study of large numbers of infants, including those delivered by caesarean section and those below 27 weeks' gestation.
We believe that it is important to establish the management that results in optimal outcome for these infants and also to examine the complex mechanisms concerned, including blood volume changes and haemodynamic effects on systemic and pulmonary circulations.
With regard to the specific questions raised, there were no significant differences between the groups in maternal or obstetric histories.
Onset of labour was spontaneous except in two cases in the regulated group, in which induction was performed for prolonged rupture of membranes.
Suppression of labour was attempted with ritodrine in four cases in the regulated group and two cases in the random group; only one mother in each group received steroids antenatally.
Oxytocin-ergometrine (Syntometrine) was given after delivery of the anterior shoulder except in the case of first twins (four in the regulated group and two in the random group).
Uterine contractions before cord clamping and beat to beat variability in blood pressure were not recorded and would both be interesting in future studies.
The policy regarding transfusion of red cells was as previously described; staff present at delivery could not be blinded to the treatment group but were seldom responsible for ordering transfusion.
Our results were analysed according to intention to treat.
Only two infants could not be held as low as intended, and omitting these from analysis does not substantially alter the results.
Both time and gravity are important in determining the volume and rate of placental transfusion.
Diana Elbourne is correct that the volume of placental transfusion received will contribute to the recorded birth weight; theoretically this could mean a difference of up to 6%.
Reanalysis of our data, with the assumption that all infants in the regulated group received a placental transfusion of 50 ml/kg (probably a gross overestimate), showed no significant difference in weight between the groups, and results of stepwise regression analysis were not altered significantly.
We believe that a multicentre trial with clearly defined outcome measures is necessary to recruit an adequate number of premature infants.
Investigation of physiological effects and analysis of outcome for infants in various categories will point to optimal management of cord clamping and enable the establishment of practical guidelines.
As June Alexander pointed out, the benefit of allowing foals a placental transfusion was established by veterinary specialists long ago.
Problem lists in letters
Editor ,— As B W Lloyd and P Barnett emphasise, communication between general practitioners and hospital specialists is of utmost importance — the letter remains the most significant factor in this and often it is this initial form of contact which may determine the care a patient receives and set the tune of the partnership between the two doctors.
Although Margo concluded that general practitioners were generally satisfied with the initial outpatient letter, most studies have found that communication between the hospital specialist and the general practitioner could be better.
Defects which have been reported include absence of essential details, omission of information about treatment, no mention of what information has been imparted to the patient, and time delay.
It may now be time for the different royal colleges to examine the possible use of a standard hospital letter for all specialties to general practitioners.
Such a proposal would allow all doctors, whether hospital based or in the community, to focus quickly on important areas — for example, medication prescribed; it would improve communication and reduce the risk of medical mishaps.
It is highly embarrassing and totally inappropriate for doctors to shuffle through sheets of notes to locate one item of information while the patient sits waiting expectantly.
Editor ,— The short paper by B W Lloyd and P Barnett provides confirmation that problem list letters are welcome by their recipients, but it should not be taken to imply that these are a new invention.
I learnt their use in the early 1970s from Dr P B L Muldoon, physician at the North Staffordshire Hospital Centre, and have personally used them since 1974–5.
They are, in various forms, routine practice for senior members in our department and, I am sure, many others.
They may also include several additional items of useful information: the duration of disease (for example, diabetes, malignancy) or date of operation, the certainty or otherwise of a diagnosis, the existence of an unexplained or undiagnosed symptom or sign, and involvement of other hospitals or carers.
Others include a complete therapeutic list at the end, particularly useful with multiple drugs and forgetful patients.
An example could thus be:
Insulin-treated diabetes with obesity (1971)
Active proliferative retinopathy (recent laser at Moorfields)
Diabetic nephropathy (biopsy proven, last creatinine 237 µmol/l)
Ischaemic heart disease (CABG 1991)
Unexplained low back pain
Recent bereavement
Current treatment:
Human Mixtard 24 units am, 16 units pm
Captopril 25 mg bd
Frusemide 40 mg mane
Aspirin 75 mg daily
Unknown analgesics
As the authors state, such letters are also of benefit to hospital clinicians, who can focus on the problems much more quickly than by scanning notes (often untidy or illegible) plus related correspondence and results.
The subsequent familiarity with the problems is often welcome by the patient.
To write such letters well does indeed require a critical review of the evidence.
There are other additional benefits.
Copying of a single letter to all those involved in the care of a patient ensures that issues of overlap and interaction are less often overlooked.
Such a recent letter, easily faxed, may provide sufficient data for an unfamiliar doctor or team to cope until notes are found, and if letters are stored as a cumulative file on a word processor a fairly complete history may easily be obtained from a secretary.
It is surely time that the old style of letter, ‘Your patient is doing well and should continue on the same treatment,’ was consigned to the history books.
Few of us would be happy with a bank whose statements read ‘Your account is in credit but will not remain so for much longer.’
Editor ,— I entirely agree with B W Lloyd and P Barnett's view that a problem list in letters from hospital doctors to general practitioners is invaluable.
I have been using it for some years and encourage my junior staff to do the same.
I believe that the general practitioners find a problem list more informative (although I have never investigated this formally) and that the copy letters in the notes make a more useful case summary than the handwritten record.
I take the system further in that I split the text of the letter up into sections pertinent to each problem.
The sample letter would therefore become:
Dear Dr Smith,
Re: John Jones 1.1.85
456 Any Street, London N17 33X
Problems:
1 Poorly controlled asthma — Thank you for referring this boy with frequent attacks of cough and wheeze.
He sleeps badly and is short of breath on exertion.
I was generally optimistic but emphasised the potential for serious attacks.
I prescribed sodium cromoglycate 10 mg (2 puffs) three times a day.
His relief drug is terbutaline 1 mg (4 puffs) four hourly.
Both are to be taken via a nebuhaler (he has excellent technique).
His peak flow was 180 today and I have issued a peak flow meter for his parents to establish the best he can manage.
I have given them the danger peak flow value of 100, and if it falls below this they will come to casualty.
Passive smoking — I have advised his parents to stop smoking as it is likely to be aggravating the child's asthma.
3 Brother with Down's syndrome — Close family involvement in the management of this child's asthma is important, and this will be less easy because of the care that his brother also needs.
4 Poor housing — I have not addressed this problem today.
Review — 1 month.
I believe that this arrangement of letters is even easier to read and to scan for items of particular interest and relevance, both for the general practitioner and during further hospital follow up.
Editor ,— Problem lists in hospital letters are an excellent way to improve communication, and they also enable quicker and more focused care for patients when they are reviewed at subsequent hospital appointments.
I hope that general practitioners will increasingly use problem lists in their own letters.
There are two traps for the unwary.
Firstly, the problem list can be overwhelmingly long if all the minor problems are included.
It is often better to be selective or list the problems in order of priority.
This tailors the list to the condition that is under review.
Secondly, there is a temptation to attach a diagnostic label to each condition so that it fits neatly on the problem list.
This can lead to unquestioning acceptance of what is really a hypothetical diagnosis.
It is better to use the symptom as ‘the problem’ if the diagnosis is unclear.
My final point is a secondary issue relating to the sample letter.
This was high quality until the last sentence, which suggested attendance at casualty if the peak flow fell.
Surely in this fictional consultation the child's own general practitioner would be able to assess the child and administer inhaled or oral steroid as effectively, if not more effectively, than a busy house officer in a casualty department?
The laryngeal mask
Editor ,— Moira E O'Meara and J Gareth Jones state that a laryngeal mask may prove life saving when a patient is impossible to intubate or ventilate by traditional methods.
One such incident occurred recently in our theatres.
A young man underwent uvulopalatopharyngoplasty and trimming of inferior turbinates to prevent him snoring.
After induction of anaesthesia tracheal intubation proved difficult owing to an anteriorly placed larynx (Mallampati grade II-III) and the patient's great size (he was 201 cm tall and weighed 118 kg).
Once tracheal intubation had been achieved with the aid of a stylet, surgery proceeded uneventfully.
Two hours after the operation he developed a reactionary haemorrhage from the right  tonsillar fossa and was returned to the theatre.
The bleeding into his pharynx was so massive that he was unable to maintain his own airway while lying supine.
With the patient on his left side anaesthesia was induced with halothane in oxygen and small increments of propofol intravenously.
The airway became increasingly difficult to maintain.
Removing the nasal pack and inserting a nasopharyngeal tube did not improve matters.
Direct laryngoscopy was performed and the posterior third of the larynx visualised, and suxamethonium was given to facilitate intubation.
Several attempts to force the endotracheal tube over a gum elastic bougie were unsuccessful.
Preparations were made to perform an emergency tracheostomy, but spontaneous respiration returned and a size 4 Brain laryngeal mask was inserted, allowing easy maintenance of the airway and confining further haemorrhage to the pharynx.
There was adequate room to position a Boyle Davis gag into the oral cavity and control the tonsillar haemorrhage, and at the end of the procedure careful deflation of the laryngeal mask cuff under direct laryngoscopic vision allowed passage of an endotracheal tube into the trachea.
The patient was transferred to the intensive care unit and was extubated the next morning.
Williams and Bailey recently described using a reinforced laryngeal mask for adenotonsillectomy.
We do not advocate routine use of a laryngeal mask in anaesthesia to control tonsillar haemorrhage as it does not guarantee protection from aspiration of blood and gastric contents.
It can, however, be life saving when traditional methods of intubation fail, and we recommend its inclusion with equipment for difficult intubations.
Editor ,— Moira E O'Meara and J Gareth Jones mention the life saving potential of the laryngeal mask when the patient is impossible to intubate or ventilate by traditional methods.
The mask is useful before the patient reaches hospital, particularly when ventilation is required in suspected cervical spinal injury and movement of the patient is minimised, and for facial injuries, in which using a face mask is difficult.
It can be passed when the operator is positioned in front of the patient, such as in a road traffic accident victim trapped in a vehicle, and the insertion technique is relatively simple and much less traumatic than endotracheal intubation.
Although the risk of aspiration has not been completely eliminated, massive regurgitation is unlikely to occur, but avoiding ventilatory pressures greater than 20 cm H 2 O (not always practical in a trauma situation) avoids leaks around the cuff and minimises gastric dilatation.
A I J Brain reported in an address to the Royal Society of Medicine that work is currently underway on developing a laryngeal mask that incorporates a separate channel through which a nasogastric tube can easily be passed.
Editor ,— Moira E O'Meara and J Gareth Jones comprehensively describe many aspects of the laryngeal mask pertinent to anaesthesia.
We would like to add comments about its use relevant to non-anaesthetists.
Although the laryngeal mask does not protect the airway against gastric contents, it has potential for use in resuscitation and emergency medicine when intubation is impossible and a patent airway cannot be maintained with a face mask.
In this situation during anaesthesia the laryngeal mask has been life saving.
Several studies have shown that unskilled staff rapidly acquire the skill to insert the mask.
It is also important, however, to appreciate when attempts at insertion are in-appropriate and to be able to recognise and manage failure of insertion.
Studies are currently under way to determine the role of the laryngeal mask in emergency medicine and the degree of training required to ensure its safe application in this area.
The laryngeal mask is being modified to produce a version that may afford better protection against aspiration, and this may extend its applications still further.
These would include its use in emergency airway management in the field and in cervical spine injury as the neck may be maintained in the neutral position during insertion and laryngoscopy is not needed.
A further potentially widespread application is in diagnostic fibreoptic bronchoscopy in awake patients.
The hypopharynx is well adapted to the presence of a foreign body, and insertion of the laryngeal mask can easily be achieved in awake patients under topical anaesthesia and sedation.
When a fibreoptic bronchoscope is passed down the tube of the laryngeal mask a dynamic view of the vocal cords is possible in most patients.
Bronchoscopy can be readily performed and has the added advantage of allowing the patient's respiratory function to be monitored and 100% oxygen to be administered if required.
We believe that formal training in the use of the laryngeal mask would be beneficial to any physician dealing with such cases.
A laryngeal mask should be available in any hospital setting where airway management is carried out.
Editor ,— We recently encountered an interesting complication related to use of a laryngeal mask.
After successfully excising a branchial cyst in the right neck the surgeon was faced with a deeper non-pulsatile cyst under considerable tension, with fine nerves and blood vessels traversing its wall.
It was lying laterally between the greater cornu of the hyoid bone and the thyroid cartilage.
A fine needle on a syringe was introduced through the cyst wall, and air under pressure pushed the syringe barrel outwards.
At this point the surgeon and the anaesthetist realised that this pharyngocele had resulted from the inflated cuff of the laryngeal mask used.
It was more prominent through the head having been turned to the opposite side.
The ‘diagnostic’ puncture of the cuff resulted in a slow deflation, which eventually necessitated the replacement of the mask owing to the development of laryngeal spasm.
The surgeon was relieved of the need for further investigation of the swelling.
Editor ,— I am much less enthusiastic about the wholesale introduction of the laryngeal mask in this country than Moira E O'Meara and J Gareth Jones.
An anaesthetist's ability to maintain airway patency by using a conventional facemask is not a skill easily acquired, nor is it made redundant by this new piece of equipment.
There are circumstances — and I include cases of upper airway obstruction, epiglottitis being an obvious example — when such expertise is crucial to successful management.
There is a real danger that trainees who in their early years depend too much on the laryngeal mask will find themselves insufficiently experienced in this vital aptitude.
It behoves those responsible for training junior anaesthetists to postpone any introduction to this novel device until the good old standby of bag and mask anaesthesia has been mastered.
Gestational diabetes mellitus
Editor ,— I am concerned about some of the analyses in Professor Jarrett's review of gestational diabetes.
A lack of consensus about terminology and definitions hampers progress, but does not imply that a problem is non-existent.
He omits the work of the Diabetes Pregnancy Study Group of the European Association for the Study of Diabetes.
This group has carried out a large multicentre study on glucose tolerance in normal pregnancy.
It showed that more than 10% of normal pregnant women had two hour glucose concentrations after a 75 g glucose tolerance test of more than 8 mmol/l and were thus labelled as having impaired glucose tolerance.
Pregnancy is clearly having an effect on glucose tolerance, a fact Jarrett dismisses.
Indeed, in one longitudinal study in normal women, significant metabolic differences could be found over only one month in the third trimester.
In view of the changes in glucose tolerance in pregnancy Lind has recommended that for a 75 g glucose tolerance test the two hour cut off should be 9 mmol/l and for the one hour test, 10.5 mmol/l.
He further recommends the use of the term gestational impaired glucose tolerance for those women whose two hour value lies between 9 and 11 mmol/l, reserving the term gestational diabetes for those women who have a result indicating diabetes on a glucose tolerance test in pregnancy.
Jarrett dismisses excessive birth weight as being secondary to maternal obesity rather than glucose tolerance abnormalities.
Although obesity seems to be the major factor, he fails to cite studies showing that glucose tolerance remains a significant factor even after allowance for maternal obesity.
Jarrett ignores studies that have investigated perinatal morbidity in association with abnormal results on a glucose tolerance test.
In a study of over 200 women with abnormal results, each woman was compared with a control matched for obesity, age, ethnicity, and parity and who had screened negative for glucose intolerance.
Results showed that stays on the neonatal care unit of more than 48 hours (brief admissions because of the label diabetes were excluded) were significantly more common in those with abnormal results.
Furthermore, with worsening glucose abnormalities the neonatal morbidity in terms of hypoglycaemia and polycythaemia was increased.
Maternal obesity was not a significant factor.
Jarrett finishes by recommending that screening for abnormalities of glucose tolerance in pregnancy be discontinued.
Even if one ignores any effects on the fetus and neonate, what about the mother?
The longest follow up reported is by O'Sullivan.
Reviewing women at between 22–28 years after the index pregnancy (when they had had abnormal glucose tolerance, but reverted to normal tolerance afterwards), he found a 50% prevalence of diabetes and 25% of abnormal glucose tolerance, whereas the controls had rates of 7% and 3%.
The stress of pregnancy on the maternal pancreas provides a most powerful predictor of the subsequent development of impaired glucose tolerance and diabetes.
Thus women with a high probability of developing such problems can be separated out for preventive therapy research programmes, in an effort to decrease the morbidity and mortality these women subsequently suffer.
Editor ,— R J Jarrett has put forward the epidemiologist's view of gestational diabetes, pointing out the lack of a clear definition, the lack of consensus on treatment, and evidence suggesting that maternal glycaemia is not important in determining fetal size.
All of these points are well taken.
Nevertheless, most clinicians would not view a mother with impaired glucose tolerance in pregnancy with the same sangfroid as epidemiologists, who view the data from afar.
Why the discrepant views?
We recently retrospectively reviewed the data on mothers who had mild degrees of impaired glucose tolerance at Central Middlesex Hospital.
It is policy to screen all mothers with risk factors for gestational diabetes (family history of diabetes, previous gestational diabetes, poor obstetric history, clinical polyhydramnios, maternal obesity, or previous macrosomic infant (>4000 g) with a 75 g oral glucose tolerance test at 30 weeks' gestation.
In 260 pregnancies (3.7% of the total) the two hour glucose value on testing was >6.9 mmol/l, and clinical details of the mother and baby were obtained from the case notes in these cases.
Of relevance to the present argument were the correlations with the babies' birth size.
On regression analysis with birth weight as the dependent variable and maternal body mass curve, maternal weight gain during pregnancy, maternal age, and area under the glucose curve as independent variables, only maternal body mass curve reached significance as a factor predicting birth weight.
This is in keeping with the findings of others.
It was apparent, however, that babies of mothers of Asian Indian origin were smaller than those of the other ethnic groups.
Therefore, the babies' body mass curve was calculated in the same manner as for adults, from birth length and birth weight, and this measure was substituted for birth weight in the regression equation.
In this model both maternal body mass curve and the area under the glucose curve emerged as significant factors in predicting body mass curve at birth (r =10.5,t =3.2 and 3.1 respectively, p<0.01 for both).
Birth length is notoriously difficult to measure.
Nevertheless, we have attempted to derive a better index of fetal adiposity than simple birth weight and have found it to have a significant bearing on results of regression analysis, suggesting that maternal glycaemia does have an impact on fetal size independent of maternal weight.
Though macrosomia cannot be directly equated with morbidity, as long as maternal glycaemia during pregnancy can be shown to influence any aspect of fetal outcome the entity of gestational diabetes cannot be dismissed out of hand.
We believe that screening programmes are still necessary to learn more about the condition.
With the current interest in fetal nutrition and its influence on health in later life, now is not the time to be turning our backs on this important issue.
Editor ,— R J Jarrett documents the case against screening for diabetes in pregnancy.
As studies have focused on the immediate pregnancy related complications of gestational diabetes it is not surprising that the value of such screening has not been properly investigated as far as long term complications are concerned.
In the long term, the offspring may develop diabetes and the woman may suffer from progression of the disease to complications such as diabetic nephropathy and renal failure.
Evidence on the value of screening has been obtained mostly from countries with well developed health care systems.
The prevailing conditions in other settings may be quite different: higher levels of perinatal morbidity and mortality as well as poor access to antenatal care and other health services.
Therefore, it is inadvisable to extrapolate from those studies to disadvantaged populations.
Furthermore, disadvantaged groups, both within our society and in developing countries, have been shown to be at high risk from gestational diabetes.
In the current economic climate it is not justifiable  to provide screening services of doubtful value.
But it is imperative to ensure that research is carried out on the value of screening for gestational diabetes in those hitherto neglected settings.
We shall then be in a much better position to state when and where screening does more good than harm.
Alternative allergy and the GMC
Editor ,— It is disingenuous of Robert Kilpatrick to claim that the General Medical Council was not created ‘to protect the public against quacks.’
The objective defined in 1858 was to enable ‘persons requiring medical aid…to distinguish qualified from unqualified practitioners’ and we need only read some early issues of the Lancet to learn that the medical register was regarded at the time as a list of doctors who set themselves higher standards than the multitude of quacks who then preyed on the sick.
From the beginning, the register sought to record the names of doctors whose education and behaviour conformed to standards of which the GMC approved.
It may soon, thanks to the reforming energy of its president, be able to demand that registered doctors conform to standards of competence and performance laid down by the council.
As a patient, I would like to think that the presence of a doctor's name on the list would also offer me some sort of guarantee that he or she would not subject me to unnecessary or unjustifiably harmful treatment — whether that treatment be labelled alternative or mainstream, labels that serve only to confuse the issue.
My definition of harmful treatment would embrace not just treatment that exposed me to risk without any hope of compensating benefit but treatment of unproved efficacy that diverted me from having other treatments that were of proved value.
To satisfy that humble demand the GMC, pace its president, would not have to set itself up as ‘an investigation bureau to assess the scientific validity of new treatments before allowing doctors to use them.’
It need only do what it does now when it examines doctors' behaviour by asking them to justify that behaviour before a panel of their professional peers.
That surely is the principle of self regulation, and its justification.
I find it sad that the business of getting practitioners, medical or alternative, to provide evidence of the efficacy of the treatments they use is left largely to consumer organisations, and to a financially hard pressed charity like HealthWatch, which has to step in where the professionals seem afraid to tread.
It is irrelevant to argue, as Robert Kilpatrick does, that the GMC has no power of regulation over unqualified practitioners.
If people want to prescribe unproved forms of treatment, or to receive them, that is their choice.
All that some of us ask is that ‘GMC attested’ doctors should be prepared to justify the treatments they use with evidence that is acceptable to their peers.
If they don't want to do that they can still practise as unregistered practitioners, using an adjective that might offer their clients the same frisson as ‘alternative.’
To say that the GMC should not get involved in assessing the acceptability of a treatment a doctor uses in the way it is now prepared to assess a doctor's professional behaviour is to cede the argument to those who claim that self regulation has become an introspective, bureaucratic activity that has grown detached from its original purpose of protecting patients.
Editor ,— The alleged beliefs and practices of clinical ecology portrayed by A B Kay bear only a superficial and pejorative resemblance to the actual published positions of the field's representative organisation, the American Academy of Environmental Medicine, known as the Society For Clinical Ecology until 1985.
Responsible leaders in this field have dubbed Kay's artificial entity the ‘straw man of clinical ecology.’
It was created by sewing together carefully chosen bits and pieces of abstractions from various articles culled from the field's diverse literature.
It is a pejorative conglomeration of selected perceptions that characterises the field and its practitioners as bizarre, irresponsible, and unscientific, lacking any reason to give them any credibility.
Various threatened vested interests seem to have designed this portrayal specifically to discourage any honest, in depth inquiry into the actual tenets of the field.
The actual official published positions of the field provide a new and powerful model of environmental medicine that contains numerous valuable insights and perspectives into the dynamic causes of many acute and chronic illnesses and provide powerful tools to vastly improve both the quality and cost effectiveness of the health care of applicable illnesses.
A B Kay is entirely correct when advocating that the medical establishment is obligated to take action that is in the best interest of the general public.
This interest can best be served by all responsible members of the scientific community beginning and sustaining an objective, scientific, honourable, and openminded face to face dialogue based on the actual and official tenets of clinical ecology/environmental medicine.
For the opening steps in this dialogue, interested readers are urged to obtain the official and responsible positions of the field as outlined in four documents: the official philosophy paper and practice guidelines of the American Academy of Environmental Medicine; the official bibliography of the founder of the field, Theron G Randolph, MD; and Chemical Sensitivity: Volume 1 , the first volume of a four volume work that is destined to become the magnum opus on the topic of chemical sensitivity, which is an important component of the larger discipline.
Editor ,— There have basically been two criticisms of my article and Richard Smith's editorial on alternative allergy and the General Medical Council (GMC)— firstly, that it is not the GMC's job to censor alternative practitioners and, secondly, that the neutralisation-provocation technique has in fact been validated.
Both points deserve comment.
In 1989 the Medical Practitioners Disciplinary Committee of New Zealand found Dr D W Steeper guilty of professional misconduct for his use of the Vega testing machine in diagnosing allergy.
Unorthodox or alternative medicine was not on trial.
The committee, however, did not accept that Dr Steeper had adopted any ‘theory of medicine or surgery’ in the use to which he put his machine.
Since he was practising alternative medicine under the guise of orthodox medicine the committee believed that he had misled his patients and that this amounted to professional misconduct.
Part of the disciplinary action taken was to ensure that patients were made aware when he employed alternative medical practices and that written consent was obtained in advance.
Perhaps the GMC could consider a similar requirement.
The question of validity of the methods of clinical ecologists is essentially what constitutes good science.
Good science involves collegiality and consensus and the long and arduous process of replication and verification.
Pivotal to medical scientific progress is the scientific literature, with the discipline imposed by writing and the reasoned critical argument in which the strengths and weaknesses of the scientific case are stated.
If published claims are of any real importance others will want to repeat, verify, contradict, or confirm them.
Papers written by clinical ecologists for clinical ecologists in clinical ecology journals have little scientific impact.
In fact, neither Clinical Ecology nor the Journal of Nutritional Medicine is even mentioned in the Science Citation Index Journal Citation Reports .
A report by the Royal College of Physicians of London concluded that studies of clinical ecologists were seriously flawed.
More serious is the fact that conventional doctors do not accept that there is such a thing as ‘environmental illness’ or the ‘total allergy syndrome.’
They find the definition (‘a polysymptomatic disorder…caused by environmental excitants’) absurd, not least because it fails to exclude other polysymptomatic diseases — that is, psychiatric illness.
Sexual behaviour
Editor ,— Colin Francome repeats an assertion made by Gurman and others.
We agree that in broad terms the mean number of heterosexual partners reported by men must be equal to the mean number reported by women over a specified time interval in a closed population.
In datasets from different countries men consistently report more partners than women, and the British data show smaller differences than many others (T W Smith, paper presented to American Sociological Association, Cincinnati, 1991).
Several reasons have been proposed to explain some of this discrepancy.
The mean is an unstable statistic influenced by a small proportion of the population with large numbers of partners; sampling strategies may under-represent minorities with multiple partners (for example, prostitutes); and sexual partnerships occur outside the sampled population through migration and travel.
Women have male partners who are, on average, older than themselves; thus the age distribution of men is not necessarily representative of the age distribution of the male partners of the women in the sample (and vice versa).
‘Lifetime’ partners do not refer to a finite time period but to partners since first sexual intercourse ‘so far’; this may be the most inaccurate measure because of difficulties of recall, whereas discrepancies over shorter time intervals are much smaller.
Although men and women are remarkably consistent about the frequency and type of sexual practices reported, strong cultural forces may nevertheless influence men and women in different ways with respect to accurate reporting of the number of sexual partners.
We intend to examine this apparent anomaly more fully in future work.
Francome suggests that the only way to obtain reliable data about heterosexual behaviour is to ask men about their previous experiences and the ages of their partners and from the responses construct the data for women.
For those men with many partners this would require a superhuman feat of memory.
More importantly, it is unacceptable to construct data that are available by direct inquiry.
John Bancroft has highlighted an important observation that recently there seems to be a greater willingness to disclose socially censured sexual behaviour.
Improved survey design (such as careful definition of a sexual partner) may result in more accurate information, but there is ample scope for further research into gender differences in reporting by detailed inquiry into the way people recollect their sexual behaviour.
Increased acceptance of studies of sexual behaviour could indicate a change in the social acceptability of different lifestyles, which would lead to greater similarity between men and women in the interpretation of past behaviour.
Editor ,— In his editorial on studies of sexual behaviour in France and Britain John Bancroft raises two particularly important issues — namely, the training of interviewers and the importance of understanding rather than counting.
In our study special attention was paid to the selection, training, and psychological support of the interviewers.
All had volunteered for the study and received two full days' training before its start.
This training was to elucidate the objectives of the study, to familiarise interviewers with the questionnaire, and to prepare them for conducting interviews on intimate and possibly sensitive questions.
There were 110 interviewers (48 men and 62 women) working in two private survey institutes.
Only six resigned from the study on the grounds of stress related to the subject.
While the data were being collected (from 16 September 1991 to 15 February 1992) at least one of the researchers was present in each telephone room, in addition to the usual supervisory staff of the institute.
At all times it was possible to listen in on interviews and to monitor the interviewer's work on a control screen.
This enabled us not only to check the quality of the work but also to help if problems arose and to show the importance we attached to the role of the interviewers.
At the midpoint of the study all interviewers were seen in small groups for a psychological debriefing.
Key variables were monitored statistically, and a weekly meeting was held with senior institute staff.
Acceptance rates were computed and surveyed every week.
With regard to Bancroft's second point, our research was designed not only to quantify sexual practices but also to understand why people put themselves at risk of contracting sexually transmitted diseases, especially AIDS.
More than half of the questionnaire was devoted to questions aimed at understanding the psychological and sociological mechanisms in risk taking.
These include social norms, the familial environment, verbal communication with new partners, attitudes towards body fluids, social networks, and an exploration of the ‘locus of control.’
Academic medicine
Editor ,— Richard Smith has drawn attention to the concern expressed by academic physicians over the process of accreditation.
The royal colleges and the Joint Committee on Higher Medical Training (JCHMT) are often accused of inflexibility in their interpretation of requirements for accreditation in the case of academics, whereas in fact such applications are treated sympathetically.
So far as training for consultant practice in the NHS is concerned, all specialties accept one year of research towards higher medical training.
One year in four seems to be a reasonable compromise between the acquisition of research experience and the need to acquire clinical skills.
Academics may also find it more practical to obtain accreditation in a single specialty rather than dual accreditation including general medicine, because the experience in emergency admissions and continuing care may be difficult to obtain in some academic units.
Others may obtain approval in advance for training in unusual specialties where no formal JCHMT programme exists.
Academic trainees should receive proper counselling before starting a research post in order that future career options are fully understood.
The type of research, including its clinical component, may well influence future career choice and opportunity.
Above all, trainees and their supervisors should discuss matters concerning accreditation with the JCHMT early in the training period.
Accreditation is of little relevance to those who compete for senior academic posts, which are decided on the basis of academic achievement.
Problems may arise when trainees decide to switch from an academic to a NHS career.
Accreditation is then perceived as a safety net, but here again consultant appointment depends less on accreditation than on possession of the required skills and training.
The colleges have always taken the view that accreditation was not a prerequisite for appointment; a recent survey revealed that approximately one third of physicians were not accredited at the time of their appointment.
A different form of accreditation for academics would not help.
For those who wish to make the transition back to the NHS, the possession of a certificate testifying to their clinical adequacy only when part of a team of physicians is likely to be a stigma rather than of benefit.
The JCHMT continually revises its training programmes and awards accreditation to those who satisfactorily complete the prescribed period in training.
The colleges have, however, always held the view that accreditation was less important for appointment to consultant status than that the skills and training of the individual should match the requirements of the post.
Poor growth in children
Editor ,— Gary Butler and N A Bridges and colleagues have missed the fundamental point of our paper on poor growth in school entrants as an index of organic disease.
We do not deny the value of long term repeated measurements of height for all children and have advocated that these should start at the earliest possible age.
We had previously reported, however, that the imprecision of measurement of height is such that it is not possible, in the short term, to label a child a ‘good’ or ‘poor’grower.
The widespread practice, therefore, of delaying investigations in those children with very short stature for six or 12 months in order to estimate velocity is of no value.
Once height at school entry has been measured nothing further is to be gained from such a delay.
We have shown that a single measurement of height is of greater value in identifying disease in short children than a single measurement of velocity over 12 months.
Circumcision
Editor ,— Andrew Gordon and Jack Collin's editorial is centred on the correct indication for circumcision — that is, true phimosis — and notes that most, if not all, of these cases could be the result of balanitis xerotica obliterans.
The histological characteristics described in detail in the editorial are more appropriately called lichen sclerosus, as indeed they were in the article by Clemmensen et al which the authors cite.
Balanitis xerotica obliterans is a descriptive term and not a  diagnosis, as it represents the end stage of several skin diseases, including, rarely, cicatricial pemphigoid and lichen planus as well as lichen sclerosus.
Similar confusion has arisen in females whereby lichen sclerosus has been described by various now obsolete terms such as leucoplakia and kraurosis vulvae.
The recognition of lichen sclerosus as a clear entity is a prerequisite for good management and follow up.
Unfortunately, these aspects are relatively neglected in males, particularly boys, as few surgeons seem to send the ablated foreskin for histological examination.
Genital dermatoses are seen by specialists from disciplines other than dermatology, and the evolution of multidisciplinary teams running combined clinics for vulval diseases has led to improved diagnoses and management of patients with these problems.
Perhaps similar clinics should be organised for penile problems.
Initiation of hypertension in utero
Editor ,— C M Law and colleagues, in their interesting paper on the initiation of hypertension, suggest that the relation between birth weight and blood pressure is amplified through life.
Although this assertion may be correct, the evidence that they present requires cautious interpretation.
The four study populations reported on differ in their dates of birth as well as in their ages, with the oldest study populations being born earliest.
Separating the effect of age from the effect of time of birth on the strength of the relation between birth weight and blood pressure is thus difficult.
To show conclusively that the association between birth weight and blood pressure is amplified with age, longitudinal study of individuals is needed.
Only one of the four groups studied by Law and colleagues, the Farnborough (Brompton study) children, was followed over time.
However, the regression coefficients relating birth weight and systolic pressure in those children, once adjusted for variation in the standard deviation of systolic pressure at different ages, do not become consistently stronger with age (figure).
Moreover, within the Brompton study data, it is particularly difficult to interpret the regression coefficients (which are adjusted for concurrent weight throughout) in the youngest age groups.
The relation between birth weight and blood pressure in the youngest age groups are thus being adjusted for measurements of weight only days or months after birth.
Is it plausible that birth weight and weight four days after birth can be used to distinguish the effects of fetal and postnatal growth on blood pressure?
The limited value of such an analysis is illustrated by the large standard error in the birth weight-blood pressure regression coefficient at 4 days, which reflects the high correlation of the two weight measures.
We have recently produced direct evidence supporting the possibility of amplification of the birth weight-blood pressure relation in childhood.
In a cohort of 540 children, the association between birth weight and blood pressure increased in strength almost twofold between 5–7 years and 9–11 years.
The absence of any relation between birth weight and change in blood pressure percentile rank between 5–7 and 9–11 years suggested that the relation between birth weight and blood pressure is established by 5–7 years and that the subsequent increase in the strength of the association is due to blood pressure tracking and to the increasing dispersion of the blood pressure distribution with age.
Primary prevention of neural tube defects with folic acid
Editor ,— At Christmas we all received the HMSO circular dated 17 December from Dr Calnan and his colleagues at the Department of Health.
We are exhorted to get all women planning a pregnancy to consume a supplement of 400 µg folic acid or folate daily and to continue through the first trimester.
Unless you are addicted to Brussels sprouts and fortified cereals, you will find taking this amount of folic acid unachievable solely through eating the right food.
Imagine our horror at the thought that we might now be held legally responsible for the tragic results of an inadequate diet.
Add to this our initial difficulty in knowing what to recommend, when the very same paper said that there was no such folic acid supplement to give.
What a relief when a second circular dated 18 December arrived, saying that 0.4 mg supplements are available singly or as B group tablets.
Keen to work proactively, we asked eight local pharmacists what they could provide for our patients.
Three responded.
We could prescribe approximately 0.4 mg folic acid by using one of several iron and folate combinations recommended for pregnancy anaemias.
But they had no low dose folic acid supplements.
One pharmacist belonging to a national firm contacted the head office, which said that a suitable product can be made by special order at £100 for 100.
Another asked the National Pharmaceutical Association, which said that there is no medicinal product licensed for sale but referred to one small firm's product, a 400 µg folic acid tablet, licensed only as a food supplement.
It seems extraordinary that circulars should be issued with no public health promotional material and no infrastructure available for the delivery of the recommended supplements.
The obstacles to appropriate preconceptual and conceptual care of women in deprived urban areas such as ours are great; a pregnancy may be a welcome but unplanned event and many social and economic factors are beyond individual control.
How can we help poor women to avoid neural tube defects when the appropriate supplements, even when they become available, are not going to be prescribable on FP10s but will have to compete with the rest of the family's needs?
Lunchtime admissions
Editor ,— Recently, as a trainee general practitioner, I went out on an urgent call at lunchtime on a weekday.
It turned out that the patient had probably had an intracranial bleed.
I contacted the local neurosurgical unit, and one of the team agreed to admission.
When I asked where I should send the woman I was told to contact the neurosurgical bed manager, who would tell me which ward to send her to.
I eventually got back to the switchboard and asked for the neurosurgical bed manager.
It was 1.45 pm by this time.
I was told that the manager had gone to lunch, and when I asked for him to be bleeped I was told again that he had gone to lunch and that he had left his bleep at the switchboard — all in a tone that suggested that this was standard behaviour.
I then asked to speak to his superior and was told that he didn't have one as he worked on his own and there was no way of contacting him.
I spoke again to the neurosurgical surgeon and explained that I had no choice but to send the patient, who needed urgent admission to casualty.
He eventually suggested sending her to one of the wards and said that he would sort things out from there.
I find it indefensible that a bed manager — especially for neurosurgery, a specialty in which admissions are often urgent — is allowed to go to lunch without a bleep and that no one covers for him.
I wonder whether eventually admissions will be possible only during working hours but not during lunch or coffee breaks.
Radiology guidelines
Editor ,— The paper from the Royal College of Radiologists Working Party would have had more impact if details of results of radiographs had been included.
The introduction of the guidelines appears to have led to around 1615 patients not being x rayed (we are not told whether there were confounding variables such as fundholding).
Were these patients disadvantaged in any way?
If not, one presumes that radiography would have either shown no lesion or not contributed to management of disease.
If that is the case, the percentage of abnormalities should have increased in the reduced number who were referred.
This report is valuable, but incomplete.
The suggestion that generation of referral data should be specified in contracts is welcome, but should be extended to feedback (through the medical audit network rather than through management) of the results of radiography referral in terms of proportions of abnormal findings.
Although many negative results will be clinically useful, an unduly high negative percentage would suggest that the referral threshold needed to be reviewed.
Editor ,— The design of the study reported by the Royal College of Radiologists Working Party seems flawed.
Using an uncontrolled study to report a reduction in referrals from general practice in the year after introduction of the guidelines and then to ascribe this drop to the guidelines themselves is somewhat naive.
A control group is needed to exclude a secular trend contributing to the reported reduction in referral rates.
This could easily have been a series of practices such as our own in areas where no education or communication with the local general practitioners took place.
Such a control group could also have been identified easily as all practices have been keeping records of their referrals for x ray examinations since the introduction of the new contract in April 1990.
Our practice shows a reduction in referrals for x ray examinations of 14 per 1000 patients between the last two full annual report years of 1990–1 and 1991–2.
If this behaviour represents a national trend then the influence of these guidelines on the sample doctors' behaviour may not have been as pronounced as the study suggests.
Civil rights for disabled people
Editor ,— Congratulations to the BMJ on taking such a strong stand on such an important issue.
How about the BMA really setting the government an example by (a ) encouraging all its disabled staff to register as disabled with guarantees of no discrimination, and (b ) taking prompt action to move from its current level of 1.5% of workforce disabled, towards the 3% target as laid out in the 1944 Disabled Persons Employment Act?
Let no one accuse us of idle posturing.
Treatment of asymptomatic carotid artery stenosis
Editor ,— Minerva quotes a paper from the New England Journal of Medicine that compared carotid endarterectomy with optimal medical treatment for asymptomatic carotid artery stenosis.
Unfortunately, her summary is misleading.
Her message was that the data favour operative intervention.
Although there was a significant reduction in ipsilateral neurological events in the group who had endarterectomy, these events included transient monocular blindness and transient ischaemic attacks, which do not necessarily have a major impact on patients' wellbeing.
When stroke was considered alone there was merely a trend favouring the surgical group; this trend disappeared when perioperative deaths and three strokes associated with arteriography were included in the analysis.
Importantly, deaths overall, including postoperative deaths, were mainly due to coronary artery disease, and when this was combined with stroke as a measure of outcome there was no difference between the two groups.
This study does not show a clear advantage for the operative group.
Hospital patients who smoke
Editor ,— Substance misusers make up a considerable proportion of patients admitted to hospital, yet during our work as housestaff we have noticed a curious anomaly.
While patients dependent on alcohol who suffer withdrawal are given sedation and heroine addicts receive opiate substitutes, only tobacco users are expected to go ‘cold turkey’ or leave their ward to smoke.
On one occasion a confused patient with subarachnoid haemorrhage harangued nurses and caused chaos on the ward until she was wheeled out of the high dependency unit for a cigarette, after which she settled down.
Yet when an attempt was made to obtain nicotine patches or chewing gum to help the patient over her withdrawal and reduce the chaos none were available.
A few telephone calls to other units showed that our hospital was not an exception.
When patches were obtained from an outside pharmacy some peace returned to the ward.
All doctors are aware of the hazards of smoking.
A period as a hospital inpatient is an opportunity for patients to break the habit, particularly those who have disease related to smoking or are at high risk, such as those with diabetes.
Even when the habit cannot be broken, stopping a patient smoking temporarily — for example, preoperatively — will often to be to the patient's and the hospital's advantage.
Profits from the sales of patches or chewing gum might even provide a source of income, which could fund an antismoking counsellor.
Epidemiology of endometriosis
Editor ,— The data of M P Vessey and colleagues should be interpreted with caution.
The data were collected over a 22 year period during which the frequency of diagnosis of endometriosis increased dramatically, with, for example, the reported incidence at time of laparoscopic sterilisation rising from <5% to 18%.
This occurred for a number of reasons: laparoscopy became a widely used diagnostic tool, with diagnosis no longer being restricted to patients with disease severe enough to warrant laparotomy, and gynaecologists became aware of the myriad visual appearances of endometriosis which represent evolution of the disease through visually distinct stages to the relatively inactive classic blue-black lesions.
The changes outlined above and in the editorial by Eric J Thomas have significant implications for the interpretation of Vessey and colleagues' data.
Firstly, most patients nowadays have the diagnosis made at laparoscopy, yet in this study only 15% of cases were diagnosed at laparoscopy and this increased to only 29% when endometriosis was the principal diagnosis.
This suggests that only relatively severe cases of endometriosis were diagnosed or reported and that the actual incidence in this population might be greater.
Secondly, under-reporting can be further suspected because no cases of endometriosis were reported in the 14% of the population in whom female sterilisation was the method of contraception.
From the figures quoted above, between 119 and 429 additional cases of endometriosis would be expected.
Finally, until recently, gynaecologists recognised only the classic blue-black lesion as being endometriosis and if diagnosis was restricted to this appearance and to severe cases this would influence the effect of age on the risk of endometriosis reported in this study.
The lack of epidemiological information about endometriosis is lamentable and can be improved only by studies in which an accurate visual diagnosis is made.
Vessey and colleagues are unfortunate that over the period of their study the goal posts have moved significantly, with a greater use of laparoscopy as a diagnostic tool and a change in our understanding of what visually constitutes the diagnosis of endometriosis.
Hypersensitivity to dexamethasone
Editor ,— A T C Chan and M E R O'Brien describe episodes of bronchospasm and urticaria that developed four days after treatment with oral dexamethasone in a patient with pleural and liver metastases.
The patient was also receiving substantial doses of ibuprofen, cimetidine, and hydrocortisone.
The attacks continued after ibuprofen and cimetidine had been stopped and recurred three days later, after chemotherapy in conjunction with intravenous dexamethasone, metoclopramide, and oral dexamethasone.
Sensitivity to dexamethasone was suggested by the fact that the adverse reaction stopped on withdrawal of the drug, although an intradermal test to dexamethasone sodium phosphate gave a negative result.
This case report seems to describe a pseudoallergic drug reaction for which there is no satis-factory explanation.
I suggest that release of vasoactive mediators from plasma proteins should be considered as a possible mechanism for pseudoallergy on the basis of recent studies on mediator displacement by a wide range of therapeutic drugs.
Non-steroidal anti-inflammatory drugs, ionic cholegraphic contrast media, and intravenous preparations of hydrocortisone, benzylpenicillin, and sulphonamides release prostaglandin F 2 a from serum proteins.
Intravenous preparations of basic drugs — for example, neuromuscular blocking agents, metoclopramide, procainamide, and desferrioxamine — displace histamine.
If relevant to clinical practice these in vitro observations suggest that pseudoallergic drug reactions may be regulated not by pharmacological specificity but by relatively non-specific protein binding properties.
The displacement properties of acidic or basic protein binding drugs and effects of released vasoactive mediators are probably additive.
The adverse reactions experienced by the patient reported on may therefore be attributable to the drug regimen and not solely to dexamethasone.
Business travel and sexually transmitted diseases
Editor ,— Recent correspondence regarding travel and sexually transmitted diseases has focused mainly on holidaymakers rather than business travellers.
In October last year the oil industry held an occupational health conference in Scotland.
Most of the large oil companies were represented.
Many of their employees had travelled to sub-Saharan Africa, south or South East Asia, and Latin America, where HIV infection and other sexually transmitted diseases are prevalent.
We asked medical and nursing delegates if their companies helped employees to avoid infection when travelling abroad.
All 60 delegates responded to a short self administered questionnaire.
Advice and protection were more commonly given for diseases such as tetanus and malaria than for sexually transmitted diseases such as gonorrhoea and syphilis (table).
Interestingly, 39 respondents said that their companies gave out emergency treatment kits to protect against non-sexual  transmission of HIV but only six said that their companies gave out condoms to protect against sexual transmission.
Fifty three respondents thought that British business travellers were at an increased risk of acquiring HIV infection through heterosexual contact abroad.
Fifty five thought that British companies should take more responsibility for raising their employees' awareness of being infected with HIV through heterosexual contact abroad.
Only 22, however, thought that British companies should take more responsibility for providing them with condoms.
These findings, although based on a small sample from one industry, suggest a need for programmes on preventing sexually transmitted diseases for business travellers.
These diseases could at least be given the same priority as other diseases.
Emphasis should be placed on raising awareness of sexually transmitted diseases including HIV infection and, when possible, providing the means of protection against them.
Community dermatology
Editor ,— In recent months dermatologists have been under increasing pressure from fundholding general practitioners to conduct clinics in the community.
Some general practitioners have even banded together and threatened a wholesale withdrawal of funding unless the local hospital provides dermatological services in the community.
At a recent meeting of the North West London Dermatology Audit Group we examined the diagnostic accuracy of general practitioners' referral letters to one dermatological department in a London teaching hospital.
In one month, 70 new patients were seen by the consultant.
In only 28 cases (40%) did the general practitioner offer a diagnosis, and in only 18 cases was the diagnosis correct — a diagnostic accuracy of 27%.
It is, of course, possible that general practitioners refer only difficult cases for a specialist opinion, but most of these patients were not particularly complex from the diagnostic point of view.
Twenty two required biopsies but in only three cases was the final diagnosis different from that initially suggested by the consultant.
One response to these data might be to argue that community based dermatology is therefore essential, but this view is held only by those whose ignorance of the specialty is total.
Certainly there is a need for education, but facilities for this exist already for those general practitioners who are minded to avail themselves of the opportunities.
Furthermore, the practice of good dermatology does not depend solely on adequate eyesight and clinical expertise.
As in any other specialty, the transition from guesswork to proper science depends on a range of hospital based diagnostic services such as immunofluorescence, mycology, patch testing, and photo testing, and on close liaison with the pathology department.
From a therapeutic point of view, facilities for daily outpatient treatment, phototherapy, and, where necessary, isolation are minimal requirements.
None of these facilities can be easily duplicated in the community.
Indeed, those dermatologists who have undertaken clinics in general practitioners' surgeries find that over 40% of the patients seen have to be followed up in a hospital clinic, thus leading to unnecessary duplication of notes and considerable wastage of consultant time and resources.
It may, of course, be reassuring for general practitioners to have consultants available in their surgeries, and doubtless they will be able to find ‘skin specialists’ who are retired, redundant, or otherwise unemployable, but to pretend that this represents proper dermatology or will lead to an improvement in patient care is dangerous nonsense.
General practitioners in partnership with management
Editor ,— I have followed with interest the continuing debate on the future of primary care.
N Starey and colleagues' article is particularly challenging in its radical approach to the funding of all health care and the formation of a primary care provider unit.
A similar model is already functioning in Lyme Regis, Dorset, and we are exploring its virtues as an organisational model for the delivery of a comprehensive and integrated health and social care service.
Lyme Community Care Unit was established on 1 April last year to serve the population registered with the two general practices in the area.
It sprang from two beliefs: that a radical review of provision of primary care was required and that no established procedure should remain unquestioned.
A business plan was formulated and a general manager appointed to implement its proposals.
In essence the business of this unit is to provide what it sensibly can and to purchase that which it cannot provide.
Provision is local wherever practicable, and users' views on services strongly influence provision.
Dorset Health Authority approved the plan and appointed the unit as its community services provider with funds transferred from the existing NHS trust units.
A comprehensive range of staff is employed by the unit, including a hospital at home team, community nursing, physiotherapy, occupational therapy, social worker, chiropody, casualty service, community psychiatric nursing, health visiting, and community health adviser.
The general practitioners act as the directors, assessing need and instructing the general manager.
Dorset Health Authority monitors activity in the usual manner, and quality criteria are no less rigorous than for any other provider unit.
The second stage of development, to be implemented on 1 April this year, is for the unit to become the purchaser of health care for all patients resident in the area.
During this year purchasing will be confined to fundholding procedures, but it is planned to extend this to all secondary health care from 1 April next year.
The third phase of development is to incorporate social care within  the primary care team's activities.
This is proving more taxing, but there have been encouraging signs of support from some people within social services and negotiations are continuing.
Development of this exciting innovation is continuing, firmly rooted in the concept that the primary care team must be at the centre of health and social care planning and provision.
It must, of course, be responsive to the strategic imperatives of both purchasers and central government, and this is reflected in the annual business plan submitted to purchasers for approval and funding.
I believe that the logical extension of this organisational model is the establishment of a primary care NHS trust which would reflect local needs in its planning, use the considerable skills of general practice in needs assessment, and provide greatly increased job satisfaction for those providing health and social care.
Part time working and job sharing
Editor ,— Vivienne van Someren does not state in her editorial (5 December) the Department of Health criteria for part time senior registrar training, but Sunita Shrotria says that to work part time ‘one must be looking after small children, or caring for a disabled relative.’
I found that the strictness with which these criteria are applied varies from region to region, and that the Department of Health is prepared to be flexible if you can argue a cogent case.
I met and married my husband just at the point when I was beginning to apply for senior registrar posts.
He was vicar of a rural parish in which he was very happy and had no desire to move.
The village is 70 miles from the nearest senior registrar post in my specialty, and on the boundary between two NHS regions.
I was not prepared to travel 140 miles to and from work each day, nor to live away from home on a long term basis, so I inquired about part time training.
I was advised to speak initially to the region's director of public health, who told me that geographical limitations and the pastoral duties of a vicar's wife were not a good enough reason to train part time, and he would not support my application.
He told me to come and see him again when I was pregnant.
I was very discouraged and did not approach anyone in the other region, fearing the same response.
However, after a year of locum posts I did approach the regional postgraduate dean of the other region and met a much more sympathetic response.
After a telephone call to the senior medical officer in charge of part time training schemes at the Department of Health, who agreed that my application would be allowed, I successfully jumped all the hurdles.
A part time post was created for me at a busy hospital 40 miles from home, and the arrangement worked very well.
I have since heard of a Roman Catholic nun who was also a doctor, who was allowed to train part time to fit in with her religious duties.
So my message is to shop around and not to be downhearted.
Ask, and you may receive.
BMA's evidence on specialist training
Editor ,— Linda Beecham's report on the BMA's submission to the chief medical officer's working party on specialist training related to training requirements in the European Community includes the statement: ‘Since the work of public health doctors in the United Kingdom has changed considerably since 1975 the requirements for specialist training have no relevance.’
We do not understand the logic behind this assertion, and its implications are worrying.
There have indeed been great changes to public health practice over the past 20 years.
But surely these imply even greater need for a solid grounding in the principles of public health medicine and well structured training for aspiring practitioners?
This serves to emphasise the relevance to the specialty of the EC directives which require ‘Theoretical and practical instruction…supervised by the competent authorities…in a health establishment approved for this purpose…involving the personal participation of the doctor training to be a specialist in the activity and in the responsibilities of the establishments concerned.’
Each member country must determine the detailed content of training for the various specialties it chooses to recognise under the directives.
In the United Kingdom the Faculty of Public Health Medicine is charged with the responsibility of maintaining and developing postgraduate training and education in the specialty.
The faculty has regularly reviewed both its membership examination regulations and syllabus and its training recommendations over the past 20 years to ensure that they keep up to date, most recently with a complete revision last year, and it is continually striving to improve education and training in the specialty.
Public health medicine has in many respects been in the vanguard of specialty training, with a unified training grade combining registrar and senior registrar grades since 1982 and a well structured training programme, which many regard as a model of its kind.
Following a very successful workshop in September 1991 supported by the Nuffield Provincial Hospitals Trust, which representatives of most EC countries attended, the faculty has been active in promoting its model of training to other EC countries.
It is hoped that many of them will develop qualifications which can be recognised under the EC directives.
British qualifications in public health medicine fully meet the requirements of the directives and the faculty looks forward to a time when the specialty will be formally recognised in all member states and not only in Britain, France, and Ireland.
Medicolegal fees
Editor ,—BMA News Review recently carried suggestions for hourly rates for doctors retained as expert witnesses by lawyers.
The hourly rate for doctors called on occasionally was set at £93, an increase of 12% over the old fee.
A new category of ‘doctors regularly engaged as experts’ is referred to, who are advised to charge £140.
For doctors who have overheads comparable with those of other professionals (solicitors, for example, typically have overheads running to 75% or more), the fee is perhaps not unreasonable.
Such doctors, however, are comparatively few.
For those whose overheads are modest (and for those who add an extra sum for secretarial, postage, or other expenses), I do not believe that either rate is sustainable in terms of a client's ability to afford it or the likelihood of its recovery by the successful litigant at the conclusion of a case.
Such figures represent a huge leapfrogging of rates formerly charged and need justifying in an economic recession.
A rate for an experienced expert for, say, 1990 might have been typified by £60 and for 1993 by £90.
I wholly support the concept of a ‘going rate’ for experts to avoid misunderstandings and to promote financial planning.
To arrive at such a figure, however, the BMA must consult with solicitors and the courts to avoid unsustainable advice being given to doctors who accept this sort of work.
For those regular expert witnesses who feel the need to charge more than £90 an hour (or £500 for their initial report in a medical negligence case) an explanation should be proffered to avoid subsequent substantial difficulty on taxation, and the words ‘as recommended by the BMA’ would not normally suffice for either the retaining lawyer or the court.
Eligibility for legal aid
Editor ,— The two dominant elements of care for people with a mental disorder are social and medical.
A less conspicuous but vital third element is a robust legal structure to protect the rights of this vulnerable group as regularised in the Mental Health Act 1983.
Pivotal to the security of people with a mental disorder who are detained in hospitals is the right to apply to a mental health review tribunal for discharge from their sections and to be represented at the tribunal by a lawyer of their choice.
The importance of representation has been shown by research published by the lord chancellor's department: the chances of applicants being successful rises from 20% to 35% when they are professionally represented.
The lord chancellor claims that the revised assistance by way of representation scheme due to come into effect on 1 April will cover the costs of professional representation at a tribunal for almost all those currently covered by the scheme.
Many of those who previously would also have been entitled to two hours of free legal advice and assistance under the green form scheme, however, will now have to pay for it.
The requirement to pay £75 an hour is likely to deter some patients from seeking expert opinion about the justification for their detention and the advisability of seeking discharge.
In consequence some patients who should apply for discharge will not, while others who should not will apply inappropriately and be subjected to the rigours of the adversarial process without proper preparation.
The medical analogy is of a patient either not being treated because of the cost of investigations or undergoing treatment in the absence of data showing which treatment, if any, is indicated.
Non-entitlement under the green form scheme will apply to patients even when they are entitled to representation under the new assistance by way of representation scheme, albeit on a contributory basis.
Everybody, but especially people with a mental disorder, needs expert legal advice when faced  with statutory confinement.
The plan to limit entitlement under the green form scheme may result in detained people who would otherwise have secured a discharge remaining on a section.
One of the most important components of the legal framework created by parliament for their protection will have been rendered ineffectual for them.
Health insurance for doctors
Editor ,— In 1980 I took out a BMA-BUPA policy for myself, my wife, and my daughter.
We were then aged 45, 36, and 6 years and in good health.
Since then my annual premium has risen steadily, from £97.98 to £1793.45 in 1992 — a rise of 1700%.
The figures include the discount for BMA members, which has fluctuated between 30% and 40% between 1980 and 1992.
Over the same period the outpatient cover has risen from £300 to £440 (49%).
Inpatient room charges have risen between 150 and 200%.
Despite careful inquiry of BUPA and the BMA, no one has been able satisfactorily to explain the astronomical rise in the premium, especially in comparison to the benefits.
I have made only a couple of minor claims over the period in question.
I have not been supplied details of the BMA/BUPA scheme, but the chief executive of BUPA tells me that between 1981 and 1991 total BUPA benefit payments rose 280% while BUPA membership rose 10%.
One in eight members now claim benefit, compared with one in 10 a decade previously.
The Private Practice and Professional Fees Committee of the BMA agrees that none of this adequately explains the rise in my premium and it has expressed astonishment at the figures I have provided.
Furthermore, while inquiring into possible alternative schemes, I learned from the managing director of Norwich Union Health Care that the seemingly more reasonable ‘express care policy’ I was about to take out was barred to doctors.
This exclusion is not mentioned anywhere in their policy literature, but the explanation, I was told, is that ‘medical professionals as a group tend to draw very considerably in excess of the levels generated by members of the lay public.’
It was implied that this might explain the enormous rise in my BMA/BUPA premium.
This information from Norwich Union Health Care was also greeted with surprise by the Private Practice and Professional Fees Committee.
The matter has been referred to the council of the BMA.
English test for doctors qualified in US and Canada
Editor ,— The General Medical Council has decided that doctors who qualify in the United States and Canada are no longer exempt from the PLAB test.
The Council recently decided that all such doctors should pass the PLAB test as a precondition of obtaining limited registration unless they have obtained exemption from the test by one of the routes that the council has approved for this purpose.
The reason given for this change is that there was recent wide coverage in the press of the case of an American paramedic who succeeded by fraudulent means in obtaining limited registration from the council.
Surely this decision is preposterous.
How can we condemn the majority of American and Canadian graduates to taking the PLAB examination on the basis of one misdemeanour by an unqualified American citizen?
It is quite clear that most Canadian and American graduates speak perfect English, and it seems rather pointless to expect them to take an examination that consists of a test of English and of basic medical knowledge that they already have.
The more likely reason for this move is a ‘tit for tat’ reaction.
American and Canadian licensing authorities require British graduates to take the ‘FEMGEMS’ and the evaluating examination of the Council of Canada respectively.
It seems rather ironic that we are going to make life more difficult for the vast majority of English speaking North American graduates when under European Community regulations we now grant full registration to doctors from several European countries.
Such doctors are able to work in Britain with full registration even though their English may be poor.
I wonder how long it will be before Australian graduates are required to take a test of English?
This action seems to be a retrograde step that will hinder the exchange of medical graduates between the English speaking communities on either side of the Atlantic.
Shanghai declaration on non-communicable diseases
Editor ,— This declaration evolved from a meeting of selected WHO directors of collaborating centres in Shanghai, China, in March 1992.
Since the 1940s there has been a global transition in health, with a rapid emergence of non-communicable diseases.
For developing countries, this has been especially troublesome as we are faced with infectious and nutritional diseases concurrent with a new imposing force of non-communicable diseases.
The twenty first century looms with a growing and greying world population; non-communicable diseases will tax the resources of all nations unless actions are taken now.
As directors of WHO non-communicable disease collaborating centres and key officials in centres for non-communicable diseases we declare that a concerted global action needs to be initiated to develop national and regional infrastructures to alleviate the financial expense and cost in human suffering from non-communicable diseases in the next millennium.
Historically, countrywide health improvements have begun with the public health system.
Improved sanitation, infection and vector control, betterment of nutrition, vaccinations, and maternal and child health programmes have been under the purview of local health authorities in the ministries of health.
In contrast, non-communicable diseases have been virtually ignored by local health departments.
Hence, the prevention and control of non-communicable diseases have been out of the mainstream of public health.
Tragically, there are few local or national data concerning the incidence of non-communicable diseases.
The limited monitoring is based on information on death certificates, but this has marginal value for control programmes as the occupational, environmental, and genetic factors that precede death are not documented.
Moreover, death is just ‘the tip of the iceberg,’ the most severe form of disease but a delayed and infrequent manifestation.
To begin scientifically based national non-communicable disease programmes, standardised integrated disease monitoring systems need to be established.
Considerable global assets have been spent to prevent non-communicable diseases — for example, with major investments to improve the environment.
However, as there has been no simultaneous monitoring of incidence, it is extremely difficult to determine the effects of these expensive actions to human health and wellbeing.
A global system to monitor disease and risk factors is needed to truly understand the effects of environmental and genetic factors on health in the next century.
We therefore declare that nations begin to consider the development of scientifically based national non-communicable disease plans.
A foundation to these programmes should be standardised cost effective integrated disease monitoring systems.
Piscatorial epistles
Editor ,— Acute respiratory obstruction due to live fish lodged in the airway is extremely rare.
A J Trevett and colleagues reported on two patients who developed acute respiratory obstruction from swallowing live fish.
Although they were not aware of similar case reports, four cases of acute respiratory obstruction due to swallowing live fish have been reported, all from India.
In 1954 a fish was retrieved from the lower trachea and right bronchus of a 14 year old boy who had presented with acute severe dyspnoea after swallowing a live fish.
In 1973 Tarasia and Mishra reported on a 10 year old boy who caught a fish in a pond and attempted to kill it by biting; unfortunately the fish slipped inside his mouth.
The boy presented with shortness of breath and husky cough.
A 10 cm long fish was retrieved from the trachea and the boy made an uneventful recovery.
In 1978 the 10 year old son of a fisherman presented with acute respiratory distress and surgical emphysema in upper thorax, neck, and face after accidentally swallowing a live fish.
The boy had held the fish between his teeth while trying to catch another.
A 10 cm fish was removed from the right bronchus and the boy made a full recovery.
In 1983 Banerjee and Singh reported that an 8 month old child who put a fish in his mouth aspirated it and choked on it; unfortunately he did not survive.
The fishermen and their children have a habit of holding fish between their teeth to prevent their escape while searching for another.
Rarely, the fish slip inside the mouth and can cause acute respiratory obstruction with fatal consequences.
As Trevett and colleagues pointed out, only swift action can save these patients' lives.