

THE POWER AND THE ARCHIVES
It is raining DNA outside.
On the bank of the Oxford canal at the bottom of my garden is a large willow tree, and it is pumping downy seeds into the air.
There is no consistent air movement, and the seeds are drifting outwards in all directions from the tree.
Up and down the canal, as far as my binoculars can reach, the water is white with floating cottony flecks, and we can be sure that they have carpeted the ground to much the same radius in other directions too.
The cotton wool is mostly made of cellulose, and it dwarfs the tiny capsule that contains the DNA, the genetic information.
The DNA content must be a small proportion of the total, so why did I say that it was raining DNA rather than cellulose?
The answer is that it is the DNA that matters.
The cellulose fluff, although more bulky, is just a parachute, to be discarded.
The whole performance, cotton wool, catkins, tree and all, is in aid of one thing and one thing only, the spreading of DNA around the countryside.
Not just any DNA, but DNA whose coded characters spell out specific instructions for building willow trees that will shed a new generation of downy seeds.
Those fluffy specks are, literally, spreading instructions for making themselves.
They are there because their ancestors succeeded in doing the same.
It is raining instructions out there; it's raining programs; it's raining tree-growing, fluff-spreading, algorithms.
That is not a metaphor, it is the plain truth.
It couldn't be any plainer if it were raining floppy discs.
It is plain and it is true, but it hasn't long been understood.
A few years ago, if you had asked almost any biologist what was special about living things as opposed to nonliving things, he would have told you about a special substance called protoplasm.
Protoplasm wasn't like any other substance; it was vital, vibrant, throbbing, pulsating, ‘irritable’(a schoolmarmish way of saying responsive).
If you took a living body and cut it up into ever smaller pieces, you would eventually come down to specks of pure protoplasm.
At one time in the last century, a real-life counterpart of Arthur Conan Doyle's Professor Challenger thought that the ‘globigerina ooze’ at the bottom of the sea was pure protoplasm.
When I was a schoolboy, elderly textbook authors still wrote about protoplasm although, by then, they really should have known better.
Nowadays you never hear or see the word.
It is as dead as phlogiston and the universal aether.
There is nothing special about the substances from which living things are made.
Living things are collections of molecules, like everything else.
What is special is that these molecules are put together in much more complicated patterns than the molecules of nonliving things, and this putting together is done by following programs, sets of instructions for how to develop, which the organisms carry around inside themselves.
Maybe they do vibrate and throb and pulsate with ‘irritability’, and glow with ‘living’ warmth, but these properties all emerge incidentally.
What lies at the heart of every living thing is not a fire, not warm breath, not a ‘spark of life’.
It is information, words, instructions.
If you want a metaphor, don't think of fires and sparks and breath.
Think, instead, of a billion discrete, digital characters carved in tablets of crystal.
If you want to understand life, don't think about vibrant, throbbing gels and oozes, think about information technology.
It is this that I was hinting at in the previous chapter, when I referred to the queen ant as the central data bank.
The basic requirement for an advanced information technology is some kind of storage medium with a large number of memory locations.
Each location must be capable of being in one of a discrete number of states.
This is true, anyway, of the digital information technology that now dominates our world of artifice.
There is an alternative kind of information technology based upon analogue information.
The information on an ordinary gramophone record is analogue.
It is stored in a wavy groove.
The information on a modern laser disc (often called ‘compact disc’, which is a pity, because the name is uninformative and also usually mispronounced with the stress on the first syllable) is digital, stored in a series of tiny pits, each of which is either definitely there or definitely not there: there are no half measures.
That is the diagnostic feature of a digital system: its fundamental elements are either definitely in one state or definitely in another state, with no half measures and no intermediates or compromises.
The information technology of the genes is digital.
This fact was  discovered by Gregor Mendel in the last century, although he wouldn't have put it like that.
Mendel showed that we don't blend our inheritance from our two parents.
We receive our inheritance in discrete particles.
As far as each particle is concerned, we either inherit it or we don't.
Actually, as R. A. Fisher, one of the founding fathers of what is now called neo-Darwinism, has pointed out, this fact of particulate inheritance has always been staring us in the face, every time we think about sex.
We inherit attributes from a male and a female parent, but each of us is either male or female, not hermaphrodite.
Each new baby born has an approximately equal probability of inheriting maleness or femaleness, but any one baby inherits only one of these, and doesn't combine the two.
We now know that the same goes for all our particles of inheritance.
They don't blend, but remain discrete and separate as they shuffle and reshuffle their way down the generations.
Of course there is often a powerful appearance of blending in the effects that the genetic units have on bodies.
If a tall person mates with a short person, or a black person with a white person, their offspring are often intermediate.
But the appearance of blending applies only to effects on bodies, and is due to the summed small effects of large numbers of particles.
The particles themselves remain separate and discrete when it comes to being passed on to the next generation.
The distinction between blending inheritance and particulate inheritance has been of great importance in the history of evolutionary ideas.
In Darwin's time everybody (except Mendel who, tucked away in his monastery, was unfortunately ignored until after his death) thought that inheritance was blending.
A Scottish engineer called Fleeming Jenkin pointed out that the fact (as it was thought to be) of blending inheritance all but ruled out natural selection as a plausible theory of evolution.
Ernst Mayr rather unkindly remarks that Jenkin's article ‘is based on all the usual prejudices and misunderstandings of the physical scientists’.
Nevertheless, Darwin was deeply worried by Jenkin's argument.
It was most colourfully embodied in a parable of a white man shipwrecked on an island inhabited by ‘negroes’:
grant him every advantage which we can conceive a white to possess over the native; concede that in the struggle for existence his chance of a long life will be much superior to that of the native chiefs; yet from all these admissions, there does not follow the conclusion that, after a limited or unlimited number of generations, the inhabitants of the island will be white.
Our shipwrecked hero would probably become king; he would kill a great many blacks in the struggle for existence; he would have a great many wives and children, while many of his subjects would live and die as bachelors…
Our white's qualities would certainly tend very much to  preserve him to a good old age, and yet he would not suffice in any number of generations to turn his subjects' descendants white…
In the first generation there will be some dozens of intelligent young mulattoes, much superior in average intelligence to the negroes.
We might expect the throne for some generations to be occupied by a more or less yellow king; but can any one believe that the whole island will gradually acquire a white, or even a yellow population, or that the islanders would acquire the energy, courage, ingenuity, patience, self-control, endurance, in virtue of which qualities our hero killed so many of their ancestors, and begot so many children; these qualities, in fact, which the struggle for existence would select, if it could select anything?
Don't be distracted by the racist assumptions of white superiority.
These were as unquestioned in the time of Jenkin and Darwin as our speciesist assumptions of human rights, human dignity, and the sacredness of human life are unquestioned today.
We can rephrase Jenkin's argument in a more neutral analogy.
If you mix white paint and black paint together, what you get is grey paint.
If you mix grey paint and grey paint together, you can't reconstruct either the original white or the original black.
Mixing paints is not so far from the pre-Mendelian vision of heredity, and even today popular culture frequently expresses heredity in terms of a mixing of ‘bloods’.
Jenkin's argument is an argument about swamping.
As the generations go by, under the assumption of blending inheritance, variation is bound to become swamped.
Greater and greater uniformity will prevail.
Eventually there will be no variation left for natural selection to work upon.
Plausible as this argument must have sounded, it is not only an argument against natural selection.
It is more an argument against inescapable facts about heredity itself!
It manifestly isn't true that variation disappears as the generations go by.
People are not more similar to each other today than they were in their grandparents' time.
Variation is maintained.
There is a pool of variation for selection to work on.
This was pointed out mathematically in 1908 by W. Weinberg, and independently by the eccentric mathematician G. H. Hardy, who incidentally, as the betting book of his (and my) college records, once took a bet from a colleague of ‘One half penny to his fortune till death, that the sun will rise tomorrow’.
But it took R. A. Fisher and his colleagues, the founders of modern population genetics, to develop the full answer to Fleeming Jenkin in terms of Mendel's theory of particle genetics.
This was an irony at the time, because, as we shall see in Chapter 11, the leading followers of Mendel in the early twentieth century thought of themselves as anti-Darwinian.
Fisher and his colleagues showed that Darwinian selection made sense, and  Jenkin's problem was elegantly solved, if what changed in evolution was the relative frequency of discrete hereditary particles, or genes, each of which was either there or not there in any particular individual body.
Darwinism post-Fisher is called neo-Darwinism.
Its digital nature is not an incidental fact that happens to be true of genetic information technology.
Digitalness is probably a necessary precondition for Darwinism itself to work.
In our electronic technology the discrete, digital locations have only two states, conventionally represented as 0 and 1 although you can think of them as high and low, on and off, up and down: all that matters is that they should be distinct from one another, and that the pattern of their states can be ‘read out’ so that it can have some influence on something.
Electronic technology uses various physical media for storing 1s and 0s, including magnetic discs, magnetic tape, punched cards and tape, and integrated ‘chips’ with lots of little semiconductor units inside them.
The main storage medium inside willow seeds, ants and all other living cells is not electronic but chemical.
It exploits the fact that certain kinds of molecule are capable of ‘polymerizing’, that is joining up in long chains of indefinite length.
There are lots of different kinds of polymer.
For example, ‘polythene’ is made of long chains of the small molecule called ethylene — polymerized ethylene.
Starch and cellulose are polymerized sugars.
Some polymers, instead of being uniform chains of one small molecule like ethylene, are chains of two or more different kinds of small molecule.
As soon as such heterogeneity enters into a polymer chain, information technology becomes a theoretical possibility.
If there are two kinds of small molecule in the chain, the two can be thought of as 1 and 0 respectively, and immediately any amount of information, of any kind, can be stored, provided only that the chain is long enough.
The particular polymers used by living cells are called polynucleotides.
There are two main families of polynucleotides in living cells, called DNA and RNA for short.
Both are chains of small molecules called nucleotides.
Both DNA and RNA are heterogeneous chains, with four different kinds of nucleotides.
This, of course, is where the opportunity for information storage lies.
Instead of just the two states 1 and 0, the information technology of living cells uses four states, which we may conventionally represent as A, T, C and G. There is very little difference, in principle, between a two-state binary information technology like ours, and a four-state information technology like that of the living cell.
As I mentioned at the end of Chapter 1, there is enough information  capacity in a single human cell to store the Encyclopaedia Britannica, all 30 volumes of it, three or four times over.
I don't know the comparable figure for a willow seed or an ant, but it will be of the same order of staggeringness.
There is enough storage capacity in the DNA of a single lily seed or a single salamander sperm to store the Encyclopaedia Britannica 60 times over.
Some species of the unjustly called ‘primitive’ amoebas have as much information in their DNA as 1,000 Encyclopaedia Britannicas.
Amazingly, only about 1 per cent of the genetic information in, for example, human cells, seems to be actually used: roughly the equivalent of one volume of the Encyclopaedia Britannica.
Nobody knows why the other 99 per cent is there.
In a previous book I suggested that it might be parasitic, freeloading on the efforts of the 1 per cent, a theory that has more recently been taken up by molecular biologists under the name of ‘selfish DNA’.
A bacterium has a smaller information capacity than a human cell, by a factor of about 1,000, and it probably uses nearly all of it: there is little room for parasites.
Its DNA could ‘only’ hold one copy of the New Testament!
Modern genetic engineers already have the technology to write the New Testament or anything else into a bacterium's DNA.
The ‘meaning’ of the symbols in any information technology is arbitrary, and there is no reason why we should not assign combinations, say triplets, from DNA's 4-letter alphabet, to letters of our own 26-letter alphabet (there would be room for all the upper and lower-case letters with 12 punctuation characters).
Unfortunately, it would take about five man-centuries to write the New Testament into a bacterium, so I doubt if anybody will bother.
If they did, the rate of reproduction of bacteria is such that 10 million copies of the New Testament could be run off in a single day, a missionary's dream if only people could read the DNA alphabet but, alas, the characters are so small that all 10 million copies of the New Testament could simultaneously dance upon the surface of a pin's head.
Electronic computer memory is conventionally classified into ROM and RAM.
ROM stands for ‘read only’ memory.
More strictly it is ‘write once, read many times’ memory.
The pattern of 0s and 1s is ‘burned’ into it once and for all on manufacture.
It then remains unchanged throughout the life of the memory, and the information can be read out any number of times.
Other electronic memory, called RAM, can be ‘written to’(one soon gets used to this inelegant computer jargon) as well as read.
RAM can therefore do everything that ROM can do, and more.
What the letters RAM actually stand for is misleading, so I won't mention it.
The point about RAM is that you  can put any pattern of 1s and 0s into any part of it that you like, on as many occasions as you like.
Most of a computer's memory is RAM.
As I type these words they are going straight into RAM, and the word processing program controlling things is also in RAM, although it could theoretically be burned into ROM and then never subsequently altered.
ROM is used for a fixed repertoire of standard programs, which are needed again and again, and which you can't change even if you wanted to.
DNA is ROM.
It can be read millions of times over, but only written to once — when it is first assembled at the birth of the cell in which it resides.
The DNA in the cells of any individual is ‘burned in’, and is never altered during that individual's lifetime, except by very rare random deterioration.
It can be copied, however.
It is duplicated every time a cell divides.
The pattern of A, T, C and G nucleotides is faithfully copied into the DNA of each of the trillions of new cells that are made as a baby grows.
When a new individual is conceived, a new and unique pattern of data is ‘burned into’ his DNA ROM, and he is then stuck with that pattern for the rest of his life.
It is copied into all his cells (except his reproductive cells, into which a random half of his DNA is copied, as we shall see).
All computer memory, whether ‘ROM’ or ‘RAM’, is addressed.
This means that every location in the memory has a label, usually a number but this is an arbitrary convention.
It is important to understand the distinction between the address and the contents of a memory location.
Each location is known by its address.
For instance the first two letters of this chapter, ‘It’, are at this moment sitting in RAM locations 6446 and 6447 of my computer, which has 65,536 RAM locations altogether.
At another time, the contents of those two locations will be different.
The contents of a location is whatever was most recently written in that location.
Each ROM location also has an address and a contents.
The difference is that each location is stuck with its contents, once and for all.
The DNA is arranged along stringy chromosomes, like long computer tapes.
All the DNA in each of our cells is addressed in the same sense as computer ROM, or indeed computer tape, is addressed.
The exact numbers or names that we use to label a given address are arbitrary, just as they are for computer memory.
What matters is that a particular location in my DNA corresponds precisely to one particular location in your DNA: they have the same address.
The contents of my DNA location 321762 may or may not be the same as the contents of your location 321762.
But my location 321762 is in precisely the same position in my cells as your location 321762 is in your cells.
‘Position’ here means position along the length of a particular chromosome.
The exact physical position of a chromosome in a cell doesn't matter.
Indeed, it floats about in fluid so its physical position varies, but every location along the chromosome is precisely addressed in terms of linear order along the length of the chromosome, just as every location along a computer tape is precisely addressed, even if the tape is strewn around the floor rather than being neatly rolled up.
All of us, all human beings, have the same set of DNA addresses, but not necessarily the same contents of those addresses.
That is the main reason why we are all different from each other.
Other species don't have the same set of addresses.
Chimpanzees, for instance, have 48 chromosomes compared to our 46.
Strictly speaking it is not possible to compare contents, address by address, because addresses don't correspond to each other across species barriers.
Closely related species, however, like chimps and humans, have such large chunks of adjacent contents in common that we can easily identify them as basically the same, even though we can't use quite the same addressing system for the two species.
The thing that defines a species is that all members have the same addressing system for their DNA.
Give or take a few minor exceptions, all members have the same number of chromosomes, and every location along the length of a chromosome has its exact opposite number in the same position along the length of the corresponding chromosome in all other members of the species.
What can differ among the members of a species is the contents of those locations.
The differences in contents in different individuals come about in the following manner, and here I must stress that I am talking about sexually reproducing species such as our own.
Our sperms or eggs each contain 23 chromosomes.
Each addressed location in one of my sperms corresponds to a particular addressed location in every other one of my sperms, and in every one of your eggs (or sperms).
All my other cells contain 46 — a double set.
The same addresses are used twice over in each of these cells.
Every cell contains two chromosome 9s, and two versions of location 7230 along chromosome 9.
The contents of the two may or may not be the same, just as they may or may not be the same in other members of the species.
When a sperm, with its 23 chromosomes, is made from a body cell with its 46 chromosomes, it only gets one of the two copies of each addressed location.
Which one it gets can be treated as random.
The same goes for eggs.
The result is that every sperm produced and every egg produced is unique in terms of the contents of their locations, although their addressing system is identical in all members of one species (with minor exceptions that  need not concern us).
When a sperm fertilizes an egg, a full complement of 46 chromosomes is, of course, made up; and all 46 are then duplicated in all the cells of the developing embryo.
I said that ROM cannot be written to except when it is first manufactured, and that is true also of the DNA in cells, except for occasional random errors in copying.
But there is a sense in which the collective data bank consisting of the ROMs of an entire species can be constructively written to.
The nonrandom survival and reproductive success of individuals within the species effectively ‘writes’ improved instructions for survival into the collective genetic memory of the species as the generations go by.
Evolutionary change in a species largely consists of changes in how many copies there are of each of the various possible contents at each addressed DNA location, as the generations pass.
Of course, at any particular time, every copy has to be inside an individual body.
But what matters in evolution is changes in frequency of alternative possible contents at each address in populations.
The addressing system remains the same, but the statistical profile of location contents changes as the centuries go by.
Once in a blue moon the addressing system itself changes.
Chimpanzees have 24 pairs of chromosomes and we have 23.
We share a common ancestor with chimpanzees, so at some point in either our ancestry or chimps' there must have been a change in chromosome number.
Either we lost a chromosome (two merged), or chimps gained one (one split).
There must have been at least one individual who had a different number of chromosomes from his parents.
There are other occasional changes in the entire genetic system.
Whole lengths of code, as we shall see, may occasionally be copied to completely different chromosomes.
We know this because we find, scattered around the chromosomes, long strings of DNA text that are identical.
When the information in a computer memory has been read from a particular location, one of two things may happen to it.
It can either simply be written somewhere else, or it can become involved in some ‘action’.
Being written somewhere else means being copied.
We have already seen that DNA is readily copied from one cell to a new cell, and that chunks of DNA may be copied from one individual to another individual, namely its child.
‘Action’ is more complicated.
In computers, one kind of action is the execution of program instructions.
In my computer's ROM, location numbers 64489, 64490 and 64491, taken together, contain a particular pattern of contents — 1s and 0s which — when interpreted as instructions, result in the computer's little loudspeaker uttering a blip sound.
This bit pattern is 101011010011000011000000.
There is nothing inherently blippy or noisy about that bit pattern Nothing about it tells you that it will  have that effect on the loudspeaker.
It has that effect only because of the way the rest of the computer is wired up.
In the same way, patterns in the DNA four-letter code have effects, for instance on eye colour or behaviour, but these effects are not inherent in the DNA data patterns themselves.
They have their effects only as a result of the way the rest of the embryo develops, which in turn is influenced by the effects of patterns in other parts of the DNA.
This interaction between genes will be a main theme of Chapter 7.
Before they can be involved in any kind of action, the code symbols of DNA have to be translated into another medium.
They are first transcribed into exactly corresponding RNA symbols.
RNA also has a four-letter alphabet.
From here, they are translated into a different kind of polymer called a polypeptide or protein.
It might be called a polyamino acid, because the basic units are amino acids.
There are 20 kinds of amino acids in living cells.
All biological proteins are chains made of these 20 basic building-blocks.
Although a protein is a chain of amino acids, most of them don't remain long and stringy.
Each chain coils up into a complicated knot, the precise shape of which is determined by the order of amino acids.
This knot shape therefore never varies for any given sequence of amino acids.
The sequence of amino acids in turn is precisely determined by the code symbols in a length of DNA (via RNA as an intermediary).
There is a sense, therefore, in which the three-dimensional coiled shape of a protein is determined by the one-dimensional sequence of code symbols in the DNA.
The translation procedure embodies the celebrated three-letter ‘genetic code’.
This is a dictionary, in which each of the 64 (4 x 4 x 4) possible triplets of DNA (or RNA) symbols is translated into one of the 20 amino acids or a ‘stop reading’ symbol.
There are three of these ‘stop reading’ punctuation marks.
Many of the amino acids are coded by more than one triplet (as you might have guessed from the fact that there are 64 triplets and only 20 amino acids).
The whole translation, from strictly sequential DNA ROM to precisely invariant three-dimensional protein shape, is a remarkable feat of digital information technology.
Subsequent steps by which genes influence bodies are a little less obviously computer-like.
Every living cell, even a single bacterial cell, can be thought of as a gigantic chemical factory.
DNA patterns, or genes, exert their effects by influencing the course of events in the chemical factory, and they do this via their influence on the three-dimensional shape of protein molecules.
The word gigantic may seem surprising for a cell, especially when you remember that 10 million bacterial cells could sit on the surface of a pin's head.
But you will also remember that each of these  cells is capable of holding the whole text of the New Testament and, moreover, it is gigantic when measured by the number of sophisticated machines that it contains.
Each machine is a large protein molecule, put together under the influence of a particular stretch of DNA.
Protein molecules called enzymes are machines in the sense that each one causes a particular chemical reaction to take place.
Each kind of protein machine churns out its own particular chemical product.
To do this it uses raw materials that are drifting around in the cell, being, very probably, the products of other protein machines.
To get an idea of the size of these protein machines, each one is made of about 6,000 atoms, which is very large by molecular standards.
There are about a million of these large pieces of apparatus in a cell, and there are more than 2,000 different kinds of them, each kind specialized to do a particular operation in the chemical factory-the cell.
It is the characteristic chemical products of such enzymes that give a cell its individual shape and behaviour.
Since all body cells contain the same genes, it might seem surprising that all body cells aren't the same as each other.
The reason is that a different subset of genes is read in different kinds of cells, the others being ignored.
In liver cells, those parts of the DNA ROM specifically relevant to the building of kidney cells are not read, and vice versa.
The shape and behaviour of a cell depend upon which genes inside that cell are being read and translated into their protein products.
This in turn depends on the chemicals already in the cell, which depends partly on which genes have previously been read in the cell, and partly on neighbouring cells.
When one cell divides into two, the two daughter cells aren't necessarily the same as each other.
In the original fertilized egg, for instance, certain chemicals congregate at one end of the cell, others at the other end.
When such a polarized cell divides, the two daughter cells receive different chemical allocations.
This means that different genes will be read in the two daughter cells, and a kind of self-reinforcing divergence gets going.
The final shape of the whole body, the size of its limbs, the wiring up of its brain, the timing of its behaviour patterns, are all the indirect consequences of interactions between different kinds of cells, whose differences in their turn arise through different genes being read.
These diverging processes are best thought of as locally autonomous in the manner of the ‘recursive’ procedure of Chapter 3, rather than as coordinated in some grand central design.
‘Action’, in the sense used in this chapter, is what a geneticist is talking about when he mentions the ‘phenotypic effect’ of a gene.
DNA has effects upon bodies, upon eye colour, hair crinkliness, strength of aggressive behaviour and thousands of other attributes, all of which are called phenotypic effects.
DNA exerts these effects initially locally, after being read by RNA and translated into protein chains, which then affect cell shape and behaviour.
This is one of the two ways in which the information in the pattern of DNA can be read out.
The other way is that it can be duplicated into a new DNA strand.
This is the copying that we discussed earlier.
There is a fundamental distinction between these two routes of transmission of the DNA information, vertical and horizontal transmission.
The information is transmitted vertically to other DNA in cells (that make other cells) that make sperms or eggs.
Hence it is transmitted vertically to the next generation and then, vertically again, to an indefinite number of future generations.
I shall call this ‘archival DNA’.
It is potentially immortal.
The succession of cells along which archival DNA travels is called the germ line.
The germ line is that set of cells, within a body, which is ancestral to sperms or eggs and hence ancestral to future generations.
DNA is also transmitted sideways or horizontally: to DNA in non-germ-line cells such as liver cells or skin cells; within such cells to RNA, thence to protein and various effects on embryonic development and therefore on adult form and behaviour.
You can think of horizontal transmission and vertical transmission as corresponding to the two sub-programs called DEVELOPMENT and REPRODUCTION in Chapter 3.
Natural selection is all about the differential success of rival DNA in getting itself transmitted vertically in the species archives.
‘Rival DNA’ means alternative contents of particular addresses in the chromosomes of the species.
Some genes are more successful than rival genes at remaining in the archives.
Although vertical transmission down the archives of the species is ultimately what ‘success’ means, the criterion for success is normally the action that the genes have on bodies, by means of their sideways transmission.
This, too, is just like the biomorph computer model.
For instance, suppose that in tigers there is a particular gene which, by means of its sideways influence in cells of the jaw, causes the teeth to be a little sharper than those that would be grown under the influence of a rival gene.
A tiger with extra-sharp teeth can kill prey more efficiently than a normal tiger; hence it has more offspring; hence it passes on, vertically, more copies of the gene that makes sharp teeth.
It passes on all its other genes at the same time, of course, but only the specific ‘sharp-teeth gene’ will find itself, on average, in the bodies of sharp-toothed tigers.
The gene itself benefits, in terms of its vertical transmission, from the average effects that it has on a whole series of bodies.
DNA's performance as an archival medium is spectacular.
In its capacity to preserve a message it far outdoes tablets of stone.
Cows and pea plants (and, indeed, all the rest of us) have an almost identical gene called the histone H4 gene.
The DNA text is 306 characters long.
We can't say that it occupies the same addresses in all species, because we can't meaningfully compare address labels across species.
But what we can say is that there is a length of 306 characters in cows, which is virtually identical to a length of 306 characters in peas.
Cows and peas differ from each other in only two characters out of these 306.
We don't know exactly how long ago the common ancestor of cows and peas lived, but fossil evidence suggests that it was somewhere between 1,000 and 2,000 million years ago.
Call it 1.5 billion years ago.
Over this unimaginably (for humans) long time, each of the two lineages that branched from that remote ancestor has preserved 305 out of the 306 characters (on average: it could be that one lineage has preserved all 306 of them and the other has preserved 304).
Letters carved on gravestones become unreadable in mere hundreds of years.
In a way the conservation of the histone-H4 DNA document is even more impressive because, unlike tablets of stone, it is not the same physical structure that lasts and preserves the text.
It is repeatedly being copied and recopied as the generations go by, like the Hebrew scriptures which were ritually copied by scribes every 80 years to forestall their wearing-out.
It is hard to estimate exactly how many times the histone H4 document has been recopied in the lineage leading to cows from the common ancestor with peas, but it is probably as many as 20 billion times.
It is also hard to find a yardstick with which to compare the preservation of more than 99 per cent of information in 20 billion successive copyings.
We can try using a version of the game of grandmothers' whispers.
Imagine 20 billion typists sitting in a row.
The line of typists would reach right round the Earth 500 times.
The first typist writes a page of a document and hands it to his neighbour.
He copies it and hands his copy to the next one.
He copies it again and hands it on to the next, and so on.
Eventually, the message reaches the end of the line, and we read it (or rather our 12,000th great grandchildren do, assuming that all the typists have a speed typical of a good secretary).
How faithful a rendering of the original message would it be?
To answer this we have to make some assumption about the accuracy of the typists.
Let's twist the question round the other way.
How good would each typist have to be, in order to match the DNA's performance?
The answer is almost too ludicrous to express.
For what it is worth, every typist would have to have an error rate of about one  in a trillion; that is, he would have to be accurate enough to make only a single error in typing the Bible 250,000 times at a stretch.
A good secretary in real life has an error rate of about one per page.
This is about half a billion times the error rate of the histone H4 gene.
A line of real-life secretaries would degrade a text to 99 per cent of its original letters by the 20th member of the line of 20 billion.
By the 10,000th member of the line, less than 1 per cent of the original text would survive.
This point of near total degradation would be reached before 99.9995 per cent of the typists had even seen it.
This whole comparison has been a bit of a cheat, but in an interesting and revealing respect.
I gave the impression that what we are measuring is copying errors.
But the histone H4 document hasn't just been copied, it has been subjected to natural selection.
Histone is vitally important for survival.
It is used in the structural engineering of chromosomes.
Maybe lots more mistakes in copying the histone H4 gene occurred, but the mutant organisms did not survive, or at least did not reproduce.
To make the comparison fair, we should have to assume that built into each typist's chair is a gun, wired up so that if he makes a mistake he is summarily shot, his place being taken by a reserve typist (squeamish readers may prefer to imagine a spring-loaded ejector seat gently catapulting miscreant typists out of the line, but the gun gives a more realistic picture of natural selection).
So, this method of measuring the conservatism of DNA, by looking at the number of changes that have actually occurred during geological time, compounds genuine copying fidelity with the filtering effects of natural selection.
We see only the descendants of successful DNA changes.
The ones that led to death are obviously not with us.
Can we measure the actual copying fidelity on the ground, before natural selection gets to work on each new generation of genes?
Yes, this is the inverse of what is known as the mutation rate, and it can be measured.
The probability of any particular letter being miscopied on any one copying occasion turns out to be a little more than one in a billion.
The difference between this, the mutation rate, and the lower rate at which change has actually been incorporated in the histone gene during evolution, is a measure of the effectiveness of natural selection in preserving this ancient document.
The histone gene's conservatism over the aeons is exceptional by genetic standards.
Other genes change at a higher rate, presumably because natural selection is more tolerant of variations in them.
For instance, genes coding the proteins known as fibrinopeptides change in evolution at a rate that closely approximates the basic mutation rate.
This probably means that mistakes in the details of these proteins  (they are produced during the clotting of blood) don't matter much for the organism.
Haemoglobin genes have a rate of changing that is intermediate  between histones and fibrinopeptides.
Presumably natural selection's tolerance of their errors is intermediate.
Haemoglobin is doing an important job in the blood, and its details really matter; but several alternative variants of it seem capable of doing the job equally well.
Here we have something that seems a little paradoxical, until we think about it further.
The slowest-evolving molecules, like histones, turn out to be the ones that have been most subject to natural selection.
Fibrinopeptides are the fastest-evolving molecules because natural selection almost completely ignores them.
They are free to evolve at the mutation rate.
The reason this seems paradoxical is that we place so much emphasis on natural selection as the driving force of evolution.
If there is no natural selection, therefore, we might expect that there would be no evolution.
Conversely, strong ‘selection pressure’, we could be forgiven for thinking, might be expected to lead to rapid evolution.
Instead, what we find is that natural selection exerts a braking effect on evolution.
The baseline rate of evolution, in the absence of natural selection, is the maximum possible rate.
That is synonymous with the mutation rate.
This isn't really paradoxical.
When we think about it carefully, we see that it couldn't be otherwise.
Evolution by natural selection could not be faster than the mutation rate, for mutation is, ultimately, the only way in which new variation enters the species.
All that natural selection can do is accept certain new variations, and reject others.
The mutation rate is bound to place an upper limit on the rate at which evolution can proceed.
As a matter of fact, most of natural selection is concerned with preventing evolutionary change rather than with driving it.
This doesn't mean, I hasten to insist, that natural selection is a purely destructive process.
It can construct too, in ways that Chapter 7 will explain.
Even the mutation rate is pretty slow.
This is another way of saying that, even without natural selection, the performance of the DNA code in accurately preserving its archive is very impressive.
A conservative estimate is that, in the absence of natural selection, DNA replicates so accurately that it takes five million replication generations to miscopy I per cent of the characters.
Our hypothetical typists are still hopelessly outclassed by DNA, even if there is no natural selection.
To match DNA with no natural selection, the typists would each have to be able to type the whole of the New Testament with only one error.
That is, they would each have to be about 450 times more accurate  than a typical real-life secretary.
This is obviously much less than the comparable figure of half a billion, which is the factor by which the histone H4 gene after natural selection is more accurate than a typical secretary; but it is still a very impressive figure.
But I have been unfair to the typists.
I assumed, in effect, that they are not capable of noticing their mistakes and correcting them.
I have assumed a complete absence of proofreading.
In reality, of course, they do proofread.
My line of billions of typists wouldn't, therefore, cause the original message to degenerate in quite the simple way that I portrayed.
The DNA-copying mechanism does the same kind of error-correction automatically.
If it didn't, it wouldn't achieve anything like the stupendous accuracy that I have described.
The DNA-copying procedure incorporates various ‘proofreading’ drills.
This is all the more necessary because the letters of the DNA code are by no means static, like hieroglyphs carved in granite.
On the contrary, the molecules involved are so small — remember all those New Testaments fitting on a pin's head — that they are under constant assault from the ordinary jostling of molecules that goes on due to heat.
There is a constant flux, a turnover of letters in the message.
About 5,000 DNA letters degenerate per day in every human cell, and are immediately replaced by repair mechanisms.
If the repair mechanisms weren't there and ceaselessly working, the message would steadily dissolve.
Proofreading of newly copied text is just a special case of normal repair work.
It is mainly proofreading that is responsible for DNA's remarkable accuracy and fidelity of information storage.
We have seen that DNA molecules are the centre of a spectacular information technology.
They are capable of packing an immense amount of precise, digital information into a very small space; and they are capable of preserving this information — with astonishingly few errors, but still some errors — for a very long time, measured in millions of years.
Where are these facts leading us?
They are leading us in the direction of a central truth about life on Earth, the truth that I alluded to in my opening paragraph about willow seeds.
This is that living organisms exist for the benefit of DNA rather than the other way around.
This won't be obvious yet, but I hope to persuade you of it.
The messages that DNA molecules contain are all but eternal when seen against the time scale of individual lifetimes.
The lifetimes of DNA messages (give or take a few mutations) are measured in units ranging from millions of years to hundreds of millions of years; or, in other words, ranging from 10,000 individual lifetimes to a trillion individual lifetimes.
Each individual organism should be seen as a temporary  vehicle, in which DNA messages spend a tiny fraction of their geological lifetimes.
The world is full of things that exist…!
No disputing that, but is it going to get us anywhere?
Things exist either because they have recently come into existence or because they have qualities that made them unlikely to be destroyed in the past.
Rocks don't come into existence at a high rate, but once they exist they are hard and durable.
If they were not they wouldn't be rocks, they would be sand.
Indeed, some of them are, which is why we have beaches!
It is the ones that happen to be durable that exist as rocks.
Dewdrops, on the other hand, exist, not because they are durable, but because they have only just come into existence and have not yet had time to evaporate.
We seem to have two kinds of ‘existenceworthiness’: the dewdrop kind, which can be summed up as ‘likely to come into existence but not very durable’; and the rock kind, which can be summed up as ‘not very likely to come into existence but likely to last for a long time once there’.
Rocks have durability and dewdrops have ‘generatability’.
(I've tried to think of a less ugly word but I can't.)
DNA gets the best of both worlds.
DNA molecules themselves, as physical entities, are like dewdrops.
Under the right conditions they come into existence at a great rate, but no one of them has existed for long, and all will be destroyed within a few months.
They are not durable like rocks.
But the patterns that they bear in their sequences are as durable as the hardest rocks.
They have what it takes to exist for millions of years, and that is why they are still here today.
The essential difference from dewdrops is that new dewdrops are not begotten by old dewdrops.
Dewdrops doubtless resemble other dewdrops, but they don't specifically resemble their own ‘parent’ dewdrops.
Unlike DNA molecules, they don't form lineages, and therefore can't pass on messages.
Dewdrops come into existence by spontaneous generation, DNA messages by replication.
Truisms like ‘the world is full of things that have what it takes to be in the world’ are trivial, almost silly, until we come to apply them to a special kind of durability, durability in the form of lineages of multiple copies.
DNA messages have a different kind of durability from that of rocks, and a different kind of generatability from that of dewdrops.
For DNA molecules, ‘what it takes to be in the world’ comes to have a meaning that is anything but obvious and tautological.
‘What it takes to be in the world’ turns out to include the ability to build machines like you and me, the most complicated things in the known universe.
Let us see how this can be so.
Fundamentally, the reason is that the properties of DNA that we  have identified turn out to be the basic ingredients necessary for any process of cumulative selection.
In our computer models in Chapter 3, we deliberately built into the computer the basic ingredients of cumulative selection.
If cumulative selection is really to happen in the world, some entities have got to arise whose properties constitute those basic ingredients.
Let us look, now, at what those ingredients are.
As we do so, we shall keep in mind the fact that these very same ingredients, at least in some rudimentary form, must have arisen spontaneously on the early Earth, otherwise cumulative selection, and therefore life, would never have got started in the first place.
We are talking here not specifically about DNA, but about the basic ingredients needed for life to arise anywhere in the universe.
When the prophet Ezekiel was in the valley of bones he prophesied to the bones and made them join up together.
Then he prophesied to them and made flesh and sinews come around them.
But still there was no breath in them.
The vital ingredient, the ingredient of life, was missing.
A dead planet has atoms, molecules and larger lumps of matter, jostling and nestling against each other at random, according to the laws of physics.
Sometimes the laws of physics cause the atoms and molecules to join up together like Ezekiel's dry bones, sometimes they cause them to split apart.
Quite large accretions of atoms can form, and they can crumble and break apart again.
But still there is no breath in them.
Ezekiel called upon the four winds to put living breath into the dry bones.
What is the vital ingredient that a dead planet like the early Earth must have, if it is to have a chance of eventually coming alive, as our planet did?
It is not breath, not wind, not any kind of elixir or potion.
It is not a substance at all, it is a property, the property of self-replication.
This is the basic ingredient of cumulative selection.
There must somehow, as a consequence of the ordinary laws of physics, come into being self-copying entities or, as I shall call them, replicators.
In modern life this role is filled, almost entirely, by DNA molecules, but anything of which copies are made would do.
We may suspect that the first replicators on the primitive Earth were not DNA molecules.
It is unlikely that a fully fledged DNA molecule would spring into existence without the aid of other molecules that normally exist only in living cells.
The first replicators were probably cruder and simpler than DNA.
There are two other necessary ingredients, which will normally arise automatically from the first ingredient, self-replication itself.
There must be occasional errors in the self-copying; even the DNA system very occasionally makes mistakes, and it seems likely that the  first replicators on Earth were much more erratic.
And at least some of the replicators should exert power over their own future.
This last ingredient sounds more sinister than it actually is.
All it means is that some properties of the replicators should have an influence over their probability of being replicated.
At least in a rudimentary form, this is likely to be an inevitable consequence of the basic facts of self-replication itself.
Each replicator, then, has copies of itself made.
Each copy is the same as the original, and has the same properties as the original.
Among these properties, of course, is the property of making (sometimes with errors) more copies of itself.
So each replicator is potentially the ‘ancestor’ of an indefinitely long line of descendant replicators, stretching into the distant future, and branching to produce, potentially, an exceedingly large number of descendant replicators.
Each new copy must be made from raw materials, smaller building blocks knocking around.
Presumably the replicators act as some kind of mould or template.
Smaller components fall together into the mould in such a way that a duplicate of the mould is made.
Then the duplicate breaks free and is able to act as a mould in its own right.
Hence we have a potentially growing population of replicators.
The population will not grow indefinitely, because eventually the supply of raw materials, the smaller elements that fall into the moulds, will become limiting.
Now we introduce our second ingredient into the argument.
Sometimes the copying will not be perfect.
Mistakes will happen.
The possibility of errors can never be totally eliminated from any copying process, although their probability can be reduced to low levels.
This is what the manufacturers of hi-fi equipment are striving towards all the time, and the DNA-replication process, as we have seen, is spectactularly good at reducing errors.
But modern DNA replication is a high-technology affair, with elaborate proofreading techniques that have been perfected over many generations of cumulative selection.
As we have seen, the first replicators probably were relatively crude, low-fidelity contraptions in comparison.
Now go back to our population of replicators, and see what the effect of erratic copying will be.
Obviously, instead of there being a uniform population of identical replicators, we shall have a mixed population.
Probably many of the products of erratic copying will be found to have lost the property of self replication that their ‘parent’ had.
But a few will retain the property of self-replication, while being different from the parent on some other respect.
So we shall have copies of errors being duplicated in the population.
When you read the word ‘error’, banish from your mind all pejorative associations.
It simply means an error from the point of view of high-fidelity copying.
It is possible for an error to result in an improvement.
I dare say many an exquisite new dish has been created because a cook made a mistake while trying to follow a recipe.
Insofar as I can claim to have had any original scientific ideas, these have sometimes been misunderstandings, or misreadings, of other peoples' ideas.
To return to our primeval replicators, while most miscopyings probably resulted in diminished copying effectiveness, or total loss of the self-copying property, a few might actually have turned out to be better at self-replication than the parent replicator that gave rise to them.
What does ‘better’ mean?
Ultimately it means more efficient at self-replication, but what might this mean in practice?
This brings us to our third ‘ingredient’.
I referred to this as ‘power’, and you'll see why in a moment.
When we discussed replication as a moulding process, we saw that the last step in the process must be the new copy's breaking free of the old mould.
The time that this occupies may be influenced by a property which I shall call the ‘stickiness’ of the old mould.
Suppose that in our population of replicators, which vary because of old copying errors back in their ‘ancestry’, some varieties happen to be more sticky than others.
A very sticky variety clings to each new copy for an average time of more than an hour before it finally breaks free and the process can begin again.
A less-sticky variety lets go of each new copy within a split second of its formation.
Which of these two varieties will come to predominate in the population of replicators?
There is no doubt about the answer.
If this is the only property by which the two varieties differ, the sticky one is bound to become far less numerous in the population.
The non-sticky one is churning out copies of non-sticky ones at thousands of times the rate that the sticky one is making copies of sticky ones.
Varieties of intermediate stickiness will have intermediate rates of self-propagation.
There will be an ‘evolutionary trend’ towards reduced stickiness.
Something like this kind of elementary natural selection has been duplicated in the test-tube.
There is a virus called Q-beta which lives as a parasite of the gut bacterium Eschenchia coli.
Q-beta has no DNA but it does contain, indeed it largely consists of, a single strand of the related molecule RNA.
RNA is capable of being replicated in a similar way to DNA.
In the normal cell, protein molecules are assembled to the specification of RNA plans.
These are working copies of plans, run off  from the DNA masters held in the cell's precious archives.
But it is theoretically possible to build a special machine — a protein molecule like the rest of the cellular machines — that runs off RNA copies from other RNA copies.
Such a machine is called an RNA-replicase molecule.
The bacterial cell itself normally has no use for these machines, and doesn't build any.
But since the replicase is just a protein molecule like any other, the versatile protein-building machines of the bacterial cell can easily turn to building them, just as the machine tools in a car factory can quickly be turned over in time of war to making munitions: all they need is to be fed the right blueprints.
This is where the virus comes in.
The business part of the virus is an RNA plan.
Superficially.it is indistinguishable from any of the other RNA working blueprints that are floating around, after being run off the bacterium's DNA master.
But if you read the small print of the viral RNA you will find something devilish written there.
The letters spell out a plan for making RNA replicase: for making machines that make more copies of the very same RNA plans, that make more machines that make more copies of the plans, that make more…
So the factory is hijacked by these self-interested blueprints.
In a sense it was crying out to be hijacked.
If you fill your factory with machines so sophisticated that they can make anything that any blueprint tells them to make, it is hardly surprising if sooner or later a blueprint arises that tells these machines to make copies of itself.
The factory fills up with more and more of these rogue machines, each churning out rogue blueprints for making more machines that will make more of themselves.
Finally, the unfortunate bacterium bursts and releases millions of viruses that infect new bacteria.
So much for the normal life cycle of the virus in nature.
I have called RNA-replicase and RNA respectively a machine and a blueprint.
So they are, in a sense (to be disputed on other grounds in a later chapter), but they are also molecules, and it is possible for human chemists to purify them, bottle them and store them on a shelf.
This is what Sol Spiegelman and his colleagues did in America in the 1960s.
Then they put the two molecules together in solution, and a fascinating thing happened.
In the test-tube, the RNA molecules acted as templates for the synthesis of copies of themselves, aided by the presence of the RNA-replicase.
The machine tools and the blueprints had been extracted and put into cold storage, separately from one another.
Then, as soon as they were given access to each other, and also to the small molecules needed as raw materials, in water, both got back to their old tricks even though they were no longer in a living cell but in a test tube.
It is but a short step from this to natural selection and evolution in the  laboratory.
It is just a chemical version of the computer biomorphs.
The experimental method is basically to lay out a long row of test-tubes each containing a solution of RNA-replicase, and also of raw materials, small molecules that can be used for RNA synthesis.
Each test-tube contains the machine tools and the raw material, but so far it is sitting idle, doing nothing because it lacks a blueprint to work from.
Now a tiny amount of RNA itself is dropped into the first test-tube.
The replicase apparatus immediately gets to work and manufactures lots of copies of the newly introduced RNA molecules, which spread through the test-tube.
Now a drop of the solution in the first test-tube is removed and put into the second test-tube.
The process repeats itself in the second test-tube and then a drop is removed and used to seed the third test-tube, and so on.
Occasionally, because of random copying errors, a slightly different, mutant RNA molecule spontaneously arises.
If, for any reason, the new variety is competitively superior to the old one, superior in the sense that, perhaps because of its low ‘stickiness’, it gets itself replicated faster or otherwise more effectively, the new variety will obviously spread through the test-tube in which it arose, out-numbering the parental type that gave rise to it.
Then, when a drop of solution is removed from that test-tube to seed the next test-tube, it will be the new mutant variety that does the seeding.
If we examine the RNAs in a long succession of test-tubes, we see what can only be called evolutionary change.
Competitively superior varieties of RNA produced at the end of several test-tube ‘generations’ can be bottled and named for future use.
One variety for example, called V2, replicates much more rapidly than normal Q-beta RNA, probably because it is smaller.
Unlike Q-beta RNA, it doesn't have to ‘bother’ to contain the plans for making replicase.
Replicase is provided free by the experimenters.
V2 RNA was used as the starting point for an interesting experiment by Leslie Orgel and his colleagues in California, in which they imposed a ‘difficult’ environment.
They added to their test-tubes a poison called ethidium bromide which inhibits the synthesis of RNA: it gums up the works of the machine tools.
Orgel and colleagues began with a weak solution of the poison.
At first, the rate of synthesis was slowed down by the poison, but after evolving through about nine test-tube transfer ‘generations’, a new strain of RNA that was resistant to the poison had been selected.
Rate of RNA synthesis was now comparable to that of normal V2 RNA in the absence of poison.
Now Orgel and his colleagues doubled the concentration of poison.
Again the rate of RNA replication dropped, but after another 10 or so test-tube transfers a strain of RNA had  evolved that was immune even to the higher concentration of poison.
Then the concentration of the poison was doubled again.
In this way, by successive doublings, they managed to evolve a strain of RNA that could self-replicate in very high concentrations of ethidium bromide, 10 times as concentrated as the poison that had inhibited the original ancestral V2 RNA.
They called the new, resistant RNA V40.
The evolution of V40 from V2 took about 100 test-tube transfer ‘generations’(of course, many actual RNA-replication generations go on between each test-tube transfer).
Orgel has also done experiments in which no enzyme was provided.
He found that RNA molecules can replicate themselves spontaneously under these conditions, albeit very slowly.
They seem to need some other catalyzing substance, such as zinc.
This is important because, in the early days of life when replicators first arose, we cannot suppose that there were enzymes around to help them to replicate.
There probably was zinc, though.
The complementary experiment was carried out a decade ago in the laboratory of the influential German school working on the origin of life under Manfred Eigen.
These workers provided replicase and RNA building blocks in the test-tube, but they did not seed the solution with RNA.
Nevertheless, a particular large RNA molecule evolved spontaneously in the test-tube, and the same molecule re-evolved itself again and again in subsequent independent experiments!
Careful checking showed that there was no possibility of chance infection by RNA molecules.
This is a remarkable result when you consider the statistical improbability of the same large molecule spontaneously arising twice.
It is very much more improbable than the spontaneous typing of METHINKS IT IS LIKE A WEASEL.
Like that phrase in our computer model, the particular favoured RNA molecule was built up by gradual, cumulative evolution.
The variety of RNA produced, repeatedly, in these experiments was of the same size and structure as the molecules that Spiegelman had produced.
But whereas Spiegelman's had evolved by ‘degeneration’ from naturally occurring, larger, Q-beta viral RNA, those of the Eigen group had built themselves up from almost nothing.
This particular formula is well adapted to an environment consisting of test-tubes provided with ready-made replicase.
It therefore is converged upon by cumulative selection from two very different starting points.
The larger, Q-beta RNA molecules are less well adapted to a test-tube environment but better adapted to the environment provided by E.coli cells.
Experiments such as these help us to appreciate the entirely  automatic and non-deliberate nature of natural selection.
The replicase ‘machines’ don't ‘know’why they make RNA molecules: it is just a byproduct of their shape that they do.
And the RNA molecules themselves don't work out a strategy for getting themselves duplicated.
Even if they could think, there is no obvious reason why any thinking entity should be motivated to make copies of itself.
If I knew how to make copies of myself, I'm not sure that I would give the project high priority in competition with all the other things I want to do: why should I?
But motivation is irrelevant for molecules.
It is just that the structure of the viral RNA happens to be such that it makes cellular machinery chum out copies of itself.
And if any entity, anywhere in the universe, happens to have the property of being good at making more copies of itself, then automatically more and more copies of that entity will obviously come into existence.
Not only that but, since they automatically form lineages and are occasionally miscopied, later versions tend to be ‘better’ at making copies of themselves than earlier versions, because of the powerful processes of cumulative selection.
It is all utterly simple and automatic.
It is so predictable as to be almost inevitable.
A ‘successful’ RNA molecule in a test-tube is successful because of some direct, intrinsic property of itself, something analogous to the ‘stickiness’of my hypothetical example.
But properties like ‘stickiness’ are rather boring.
They are elementary properties of the replicator itself, properties that have a direct effect on its probability of being replicated.
What if the replicator has some effect upon something else, which affects something else, which affects something else, which…eventually, indirectly affects the replicator's chance of being replicated?
You can see that, if long chains of causes like this existed, the fundamental truism would still hold.
Replicators that happen to have what it takes to get replicated would come to predominate in the world, no matter how long and indirect the chain of causal links by which they influence their probability of being replicated.
And, by the same token, the world will come to be filled with the links in this causal chain.
We shall see those links, and marvel at them.
In modern organisms we see them all the time.
They are eyes and skins and bones and toes and brains and instincts.
These things are the tools of DNA replication.
They are caused by DNA, in the sense that differences in eyes, skins, bones, instincts, etc. are caused by differences in DNA.
They exert an influence over the replication of the DNA that caused them, in that they affect the survival and reproduction of their bodies — which contain that same DNA, and whose fate is therefore shared by the DNA.
Therefore, the DNA itself exerts  an influence over its own replication, via the attributes of bodies.
DNA can be said to exert power over its own future, and bodies and their organs and behaviour patterns are the instruments of that power.
When we talk about power, we are talking about consequences of replicators that affect their own future, however indirect those consequences might be.
It doesn't matter how many links there are in the chain from cause to effect.
If the cause is a self-replicating entity, the effect, be it ever so distant and indirect, can be subject to natural selection.
I shall summarize the general idea by telling a particular story about beavers.
In detail it is hypothetical, but it certainly cannot be far from the truth.
Although nobody has done research upon the development of brain connections in the beaver, they have done this kind of research on other animals, like worms.
I am borrowing the conclusions and applying them to beavers, because beavers are more interesting and congenial to many people than worms.
A mutant gene in a beaver is just a change in one letter of the billion-letter text; a change in a particular gene G. As the young beaver grows, the change is copied, together with all the other letters in the text, into all the beaver's cells.
In most of the cells the gene G is not read; other genes, relevant to the workings of the other cell types, are.
G is read, however, in some cells in the developing brain.
It is read and transcribed into RNA copies.
The RNA working copies drift around the interior of the cells, and eventually some of them bump into protein-making machines called ribosomes.
The protein-making machines read the RNA working plans, and turn out new protein molecules to their specification.
These protein molecules curl up into a particular shape determined by their own amino-acid sequence, which in turn is governed by the DNA code sequence of the gene G. When G mutates, the change makes a crucial difference to the amino-acid sequence normally specified by the gene G, and hence to the coiled-up shape of the protein molecule.
These slightly altered protein molecules are mass-produced by the protein-making machines inside the developing brain cells.
They in turn act as enzymes, machines that manufacture other compounds in the cells, the gene products.
The products of the gene G find their way into the membrane surrounding the cell, and are involved in the processes whereby the cell makes connections with other cells.
Because of the slight alteration in the original DNA plans, the production-rate of certain of these membrane compounds is changed.
This in turn changes the way in which certain developing brain cells connect up with one another.
A subtle alteration in the wiring diagram of a particular part of the beaver's brain has occurred, the indirect, indeed far-removed, consequence of a change in the DNA text.
Now it happens that this particular part of the beaver's brain, because of its position in the total wiring diagram, is involved in the beaver's dam-building behaviour.
Of course, large parts of the brain are involved whenever the beaver builds a dam but, when the G mutation affects this particular part of the brain's wiring diagram, the change has a specific effect on the behaviour.
It causes the beaver to hold its head higher in the water while swimming with a log in its jaws.
Higher, that is, than a beaver without the mutation.
This makes it a little less likely that mud, attached to the log, will wash off during the journey.
This increases the stickiness of the log, which in turn means that, when the beaver thrusts it into the dam, the log is more likely to stay there.
This will tend to apply to all the logs placed by any beaver bearing this particular mutation.
The increased stickiness of the logs is a consequence, again a very indirect consequence, of an alteration in the DNA text.
The increased stickiness of the logs makes the dam a sounder structure, less likely to break up.
This in turn increases the size of the lake created by the dam, which makes the lodge in the centre of the lake more secure against predators.
This tends to increase the number of offspring successfully reared by the beaver.
If we look at the whole population of beavers, those that possess the mutated gene will, on average, tend therefore to rear more offspring than those not possessing the mutated gene.
Those offspring will tend to inherit archive copies of the self-same altered gene from their parents.
Therefore, in the population, this form of the gene will become more numerous as the generations go by.
Eventually it will become the norm, and will no longer deserve the title ‘mutant’.
Beaver dams in general will have improved another notch.
The fact that this particular story is hypothetical, and that the details may be wrong, is irrelevant.
The beaver dam evolved by natural selection, and therefore what happened cannot be very different, except in practical details, from the story I have told.
The general implications of this view of life are explained and elaborated in my book The Extended Phenotype, and I shan't repeat the arguments here.
You will notice that in this hypothetical story there were no fewer than 11 links in the causal chain linking altered gene to improved survival.
In real life there might be even more.
Every one of those links, whether it is an effect on the chemistry inside a cell, a later effect on how brain cells wire themselves together, an even later effect on behaviour, or a final effect on lake size, is correctly regarded as caused by a change in the DNA.
It wouldn't matter if there were 111 links.
Any effect that a change in a gene has on its own replication probability is fair game for  natural selection.
It is all perfectly simple, and delightfully automatic and unpremeditated.
Something like it is well-nigh inevitable, once the fundamental ingredients of cumulative selection — replication, error and power — have come into existence in the first place.
But how did this happen?
How did they come into existence on Earth, before life was there?
We shall see how this difficult question might be answered, in the next chapter.
ORIGINS AND MIRACLES
Chance, luck, coincidence, miracle.
One of the main topics of this chapter is miracles and what we mean by them.
My thesis will be that events that we commonly call miracles are not supernatural, but are part of a spectrum of more-or-less improbable natural events.
A miracle, in other words, if it occurs at all, is a tremendous stroke of luck.
Events don't fall neatly into natural events versus miracles.
There are some would-be events that are too improbable to be contemplated, but we can't know this until we have done a calculation.
And to do the calculation, we must know how much time was available, more generally how many opportunities were available, for the event to occur.
Given infinite time, or infinite opportunities, anything is possible.
The large numbers proverbially furnished by astronomy, and the large timespans characteristic of geology, combine to turn topsy-turvy our everyday estimates of what is expected and what is miraculous.
I shall build up to this point using a specific example which is the other main theme of this chapter.
This example is the problem of how life originated on Earth.
To make the point clearly, I shall arbitrarily concentrate on one particular theory of the origin of life, although any one of the modern theories would have served the purpose.
We can accept a certain amount of luck in our explanations, but not too much.
The question is, how much?
The immensity of geological time entitles us to postulate more improbable coincidences than a court of law would allow but, even so, there are limits.
Cumulative selection is the key to all our modern explanations of life.
It strings a series of acceptably lucky events (random mutations) together in a nonrandom sequence so that, at the end of the sequence, the finished  product carries the illusion of being very very lucky indeed, far too improbable to have come about by chance alone, even given a timespan millions of times longer than the age of the universe so far.
Cumulative selection is the key but it had to get started, and we cannot escape the need to postulate a single-step chance event in the origin of cumulative selection itself.
And that vital first step was a difficult one because, at its heart, there lies what seems to be a paradox.
The replication processes that we know seem to need complicated machinery to work.
In the presence of a replicase ‘machine tool’, fragments of RNA will evolve, repeatedly and convergently, towards the same endpoint, an endpoint whose ‘probability’ seems vanishingly small until you reflect on the power of cumulative selection.
But we have to assist this cumulative selection to get started.
It won't go unless we provide a catalyst, such as the replicase ‘machine tool’ of the previous chapter.
And that catalyst, it seems, is unlikely to come into existence spontaneously, except under the direction of other RNA molecules.
DNA molecules replicate in the complicated machinery of the cell, and written words replicate in Xerox machines, but neither seem capable of spontaneous replication in the absence of their supporting machinery.
A Xerox machine is capable of copying its own blueprints, but it is not capable of springing spontaneously into existence.
Biomorphs readily replicate in the environment provided by a suitably written computer program, but they can't write their own program or build a computer to run it.
The theory of the blind watchmaker is extremely powerful given that we are allowed to assume replication and hence cumulative selection.
But if replication needs complex machinery, since the only way we know for complex machinery ultimately to come into existence is cumulative selection, we have a problem.
Certainly the modern cellular machinery, the apparatus of DNA replication and protein synthesis, has all the hallmarks of a highly evolved, specially fashioned machine.
We have seen how staggeringly impressive it is as an accurate data storage device.
At its own level of ultra-miniaturization, it is of the same order of elaborateness and complexity of design as the human eye is at a grosser level.
All who have given thought to the matter agree that an apparatus as complex as the human eye could not possibly come into existence through single-step selection.
Unfortunately, the same seems to be true of at least parts of the apparatus of cellular machinery whereby DNA replicates itself, and this applies not just to the cells of advanced creatures like ourselves and amoebas, but also to relatively more primitive creatures like bacteria and blue-green algae.
So, cumulative selection can manufacture complexity while single-step selection cannot.
But cumulative selection cannot work unless there is some minimal machinery of replication and replicator power, and the only machinery of replication that we know seems too complicated to have come into existence by means of anything less than many generations of cumulative selection!
Some people see this as a fundamental flaw in the whole theory of the blind watchmaker.
They see it as the ultimate proof that there must originally have been a designer, not a blind watchmaker but a far-sighted supernatural watchmaker.
Maybe, it is argued, the Creator does not control the day-to-day succession of evolutionary events; maybe he did not frame the tiger and the lamb, maybe he did not make a tree, but he did set up the original machinery of replication and replicator power, the original machinery of DNA and protein that made cumulative selection, and hence all of evolution, possible.
This is a transparently feeble argument, indeed it is obviously self-defeating.
Organized complexity is the thing that we are having difficulty in explaining.
Once we are allowed simply to postulate organized complexity, if only the organized complexity of the DNA/ protein replicating engine, it is relatively easy to invoke it as a generator of yet more organized complexity.
That, indeed, is what most of this book is about.
But of course any God capable of intelligently designing something as complex as the DNA/protein replicating machine must have been at least as complex and organized as that machine itself.
Far more so if we suppose him additionally capable of such advanced functions as listening to prayers and forgiving sins.
To explain the origin of the DNA/protein machine by invoking a supernatural Designer is to explain precisely nothing, for it leaves unexplained the origin of the Designer.
You have to say something like ‘God was always there’, and if you allow yourself that kind of lazy way out, you might as well just say ‘DNA was always there’, or ‘Life was always there’, and be done with it.
The more we can get away from miracles, major improbabilities, fantastic coincidences, large chance events, and the more thoroughly we can break large chance events up into a cumulative series of small chance events, the more satisfying to rational minds our explanations will be.
But in this chapter we are asking how improbable, how miraculous, a single event we are allowed to postulate.
What is the largest single event of sheer naked coincidence, sheer unadulterated miraculous luck, that we are allowed to get away with in our theories, and still say that we have a satisfactory explanation of life?
In order for a monkey to write ‘Methinks it is like a weasel’ by chance, it needs a  very large amount of luck, but it is still measurable.
We calculated the odds against it as about 10 thousand million million million million million million (104 40 ) to 1 against.
Nobody can really comprehend or imagine such a large number, and we just think of this degree of improbability as synonymous with impossible.
But although we can't comprehend these levels of improbability in our minds, we shouldn't just run away from them in terror.
The number 10 40 may be very large but we can still write it down, and we can still use it in calculations.
There are, after all, even larger numbers: 10 46 , for instance, is not just larger; you must add 10 40 to itself a million times in order to obtain 10 46 What if we could somehow muster a gang of 10 46 monkeys each with its own typewriter?
Why, lo and behold, one of them would solemnly type ‘Methinks it is like a weasel, and another would almost certainly type ‘I think therefore I am’.
The problem is, of course, that we couldn't assemble that many monkeys.
If all the matter in the universe were turned into monkey flesh, we still couldn't get enough monkeys.
The miracle of a monkey typing ‘Methinks it is like a weasel’.
is quantitatively too great, measurably too great, for us to admit it to our theories about what actually happens.
But we couldn't know this until we sat down and did the calculation.
So, there are some levels of sheer luck, not only too great for puny human imaginations, but too great to be allowed in our hard-headed calculations about the origin of life.
But, to repeat the question, how great a level of luck, how much of a miracle, are we allowed to postulate?
Don't let's run away from this question just because large numbers are involved.
It is a perfectly valid question, and we can at least write down what we would need to know in order to calculate the answer.
Now here is a fascinating thought.
The answer to our question — of how much luck we are allowed to postulate — depends upon whether our planet is the only one that has life, or whether life abounds all around the universe.
The one thing we know for certain is that life has arisen once, here on this very planet.
But we have no idea at all whether there is life anywhere else in the universe.
It is entirely possible that there isn't.
Some people have calculated that there must be life elsewhere, on the following grounds (I won't point out the fallacy until afterwards).
There are probably at least 10 20;(i.e. 100 billion billion) roughly suitable planets in the universe.
We know that life has arisen here, so it can't be all that improbable.
Therefore it is almost inescapable that at least some among all those billions of billions of other planets have life.
The flaw in the argument lies in the inference that, because life has  arisen here, it can't be too terribly improbable.
You will notice that this inference contains the built-in assumption that whatever went on on Earth is likely to have gone on elsewhere in the universe, and this begs the whole question.
In other words, that kind of statistical argument, that there must be life elsewhere in the universe because there is life here, builds in, as an assumption, what it is setting out to prove.
This doesn't mean that the conclusion that life exists all around the universe is necessarily wrong.
My guess is that it is probably right.
It simply means that that particular argument that led up to it is no argument at all.
It is just an assumption.
Let us, for the sake of discussion, entertain the alternative assumption that life has arisen only once, ever, and that was here on Earth.
It is tempting to object to this assumption on the following emotional grounds.
Isn't there something terribly medieval about it?
Doesn't it recall the time when the church taught that our Earth was the centre of the universe, and the stars just little pinpricks of light set in the sky for our delight (or, even more absurdly presumptuous, that the stars go out of their way to exert astrological influences on our little lives)?
How very conceited to assume that, out of all the billions of billions of planets in the universe, our own little backwater of a world, in our own local backwater of a solar system, in our own local backwater of a galaxy, should have been singled out for life?
Why, for goodness sake, should it have been our planet?
I am genuinely sorry, for I am heartily thankful that we have escaped from the small-mindedness of the medieval church and I despise modern astrologers, but I am afraid that the rhetoric about backwaters in the previous paragraph is just empty rhetoric.
It is entirely possible that our backwater of a planet is literally the only one that has ever borne life.
The point is that if there were only one planet that had ever borne life, then it would have to be our planet, for the very good reason that ‘we’ are here discussing the question!
If the origin of life is such an improbable event that it happened on only one planet in the universe, then our planet has to be that planet.
So, we can't use the fact that Earth has life to conclude that life must be probable enough to have arisen on another planet.
Such an argument would be circular.
We have to have some independent arguments about how easy or difficult it is for life to originate on a planet, before we can even begin to answer the question of how many other planets in the universe have life.
But that isn't the question we set out with.
Our question was, how much luck are we allowed to assume in a theory of the origin of life on Earth?
I said that the answer depends upon whether life has arisen only  once, or many times.
Begin by giving a name to the probability, however low it is, that life will originate on any randomly designated planet of some particular type.
Call this number the spontaneous generation probability or SGP.
It is the SGP that we shall arrive at if we sit down with our chemistry textbooks, or strike sparks through plausible mixtures of atmospheric gases in our laboratory, and calculate the odds of replicating molecules springing spontaneously into existence in a typical planetary atmosphere.
Suppose that our best guess of the SGP is some very very small number, say one in a billion.
This is obviously such a small probability that we haven't the faintest hope of duplicating such a fantastically lucky, miraculous event as the origin of life in our laboratory experiments.
Yet if we assume, as we are perfectly entitled to do for the sake of argument, that life has originated only once in the universe, it follows that we are allowed to postulate a very large amount of luck in a theory, because there are so many planets in the universe where life could have originated.
If, as one estimate has it, there are 100 billion billion planets, this is 100 billion times greater than even the very low SGP that we postulated.
To conclude this argument, the maximum amount of luck that we are allowed to assume, before we reject a particular theory of the origin of life, has odds of one in N, where N is the number of suitable planets in the universe.
There is a lot hidden in that word ‘suitable’, but let us put an upper limit of 1 in 100 billion billion for the maximum amount of luck that this argument entitles us to assume.
Think about what this means.
We go to a chemist and say: get out your textbooks and your calculating machine; sharpen your pencil and your wits; fill your head with formulae, and your flasks with methane and ammonia and hydrogen and carbon dioxide and all the other gases that a primeval nonliving planet can be expected to have; cook them all up together; pass strokes of lightning through your simulated atmospheres, and strokes of inspiration through your brain; bring all your clever chemist's methods to bear, and give us your best chemist's estimate of the probability that a typical planet will spontaneously generate a self-replicating molecule.
Or, to put it another way, how long would we have to wait before random chemical events on the planet, random thermal jostling of atoms and molecules, resulted in a self-replicating molecule?
Chemists don't know the answer to this question.
Most modern chemists would probably say that we'd have to wait a long time by the standards of a human lifetime, but perhaps not all that long by the standards of cosmological time.
The fossil history.of earth suggests that we have about a billion years — one ‘aeon’, to use a convenient  modern definition — to play with, for this is roughly the time that elapsed between the origin of the Earth about 4.5 billion years ago and the era of the first fossil organisms.
But the point of our ‘numbers of planets’ argument is that, even if the chemist said that we'd have to wait for a ‘miracle’, have to wait a billion billion years — far longer than the universe has existed, we can still accept this verdict with equanimity.
There are probably more than a billion billion available planets in the universe.
If each of them lasts as long as Earth, that gives us about a billion billion billion planet-years to play with.
That will do nicely!
A miracle is translated into practical politics by a multiplication sum.
There is a concealed assumption in this argument.
Well, actually there are lots, but there's one in particular that I want to talk about.
This is that, once life (i.e. replicators and cumulative selection) originates at all, it always advances to the point where its creatures evolve enough intelligence to speculate about their origins.
If this is not so, our estimate of the amount of luck that we are allowed to postulate must be reduced accordingly.
To be more precise, the maximum odds against the origin of life on any one planet that our theories are allowed to postulate, is the number of available planets in the universe divided by the odds that life, once started, will evolve sufficient intelligence to speculate about its own origins.
It may seem a little strange that ‘sufficient intelligence to speculate about its own origins’ is a relevant variable.
To understand why it is, consider an alternative assumption.
Suppose that the origin of life was quite a probable event, but the subsequent evolution of intelligence was exceedingly improbable, demanding a huge stroke of luck.
Suppose the origin of intelligence is so improbable that it has happened on only one planet in the universe, even though life has started on many planets.
Then, since we know we are intelligent enough to discuss the question, we know that Earth must be that one planet.
Now suppose that the origin of life, and the origin of intelligence given that life is there, are both highly improbable events.
Then the probability of any one planet, such as Earth, enjoying both strokes of luck is the product of the two low probabilities, and this is a far smaller probability.
It is as though, in our theory of how we came to exist, we are allowed to postulate a certain ration of luck.
This ration has, as its upper limit, the number of eligible planets in the universe.
Given our ration of luck, we can then ‘spend’ it as a limited commodity over the course of our explanation of our own existence.
If we use up almost all our ration of luck in our theory of how life gets started on a planet in  the first place, then we are allowed to postulate very little more luck in subsequent parts of our theory, in, say, the cumulative evolution of brains and intelligence.
If we don't use up all our ration of luck in our theory of the origin of life, we have some left over to spend on our theories of subsequent evolution, after cumulative selection has got going.
If we want to use up most of our ration of luck in our theory of the origin of intelligence, then we haven't much left over to spend on our theory of the origin of life: we must come up with a theory that makes the origin of life almost inevitable.
Alternatively, if we don't need our whole luck ration for these two stages of our theory, we can, in effect, use the surplus to postulate life elsewhere in the universe.
My personal feeling is that, once cumulative selection has got itself properly started, we need to postulate only a relatively small amount of luck in the subsequent evolution of life and intelligence.
Cumulative selection, once it has begun, seems to me powerful enough to make the evolution of intelligence probable, if not inevitable.
This means that we can, if we want to, spend virtually our entire ration of postulatable luck in one big throw, in our theory of the origin of life on a planet.
Therefore we have at our disposal, if we want to use it, odds of 1 in 100 billion billion as an upper limit (or 1 in however many available planets we think there are! to spend in our theory of the origin of life.
This is the maximum amount of luck we are allowed to postulate in our theory.
Suppose we want to suggest, for instance, that life began when both DNA and its protein-based replication machinery spontaneously chanced to come into existence.
We can allow ourselves the luxury of such an extravagant theory, provided that the odds against this coincidence occurring on a planet do not exceed 100 billion billion to one.
This allowance may seem large.
It is probably ample to accommodate the spontaneous arising of DNA or RNA.
But it is nowhere near enough to enable us to do without cumulative selection altogether.
The odds against assembling a well-designed body that flies as well as a swift, or swims as well as a dolphin, or sees as well as a falcon, in a single blow of luck — single-step selection — are stupendously greater than the number of atoms in the universe, let alone the number of planets!
No, it is certain that we are going to need a hefty measure of cumulative selection in our explanations of life.
But although we are entitled, in our theory of the origin of life, to spend a maximum ration of luck amounting, perhaps, to odds of 100 billion billion to one against, my hunch is that we aren't going to need more than a small fraction of that ration.
The origin of life on a planet can be a very improbable event indeed by our everyday standards, or  indeed by the standards of the chemistry laboratory, and still be sufficiently probable to have occurred, not just once but many times, all over the universe.
We can regard the statistical argument about numbers of planets as an argument of last resort.
At the end of the chapter I shall make the paradoxical point that the theory we are looking for may actually need to seem improbable, even miraculous, to our subjective judgement (because of the way our subjective judgement has been made).
Nevertheless, it is still sensible for us to begin by seeking that theory of the origin of life with the least degree of improbability.
If the theory that DNA and its copying machinery arose spontaneously is so improbable that it obliges us to assume that life is very rare in the universe, and may even be unique to Earth, our first resort is to try to find a more probable theory.
So, can we come up with any speculations about relatively probable ways in which cumulative selection might have got its start?
The word ‘speculate’ has pejorative overtones, but these are quite uncalled for here.
We can hope for nothing more than speculation when the events we are talking about took place four billion years ago and took place, moreover, in a world that must have been radically different from that which we know today.
For instance, there almost certainly was no free oxygen in the atmosphere.
Though the chemistry of the world may have changed, the laws of chemistry have not changed (that's why they are called laws), and modern chemists know enough about those laws to make some well-informed speculations, speculations that have to pass rigorous tests of plausibility imposed by the laws.
You can't just speculate wildly and irresponsibly, allowing your imagination to run riot in the manner of such unsatisfying space fiction panaceas as ‘hyperdrives’, ‘time warps’ and ‘infinite improbability drives’.
Of all possible speculations about the origin of life, most run foul of the laws of chemistry and can be ruled out, even if we make full use of our statistical fall-back argument about numbers of planets.
Careful selective speculation is therefore a constructive exercise.
But you do have to be a chemist to do it.
I am a biologist not a chemist, and I must rely on chemists to get their sums right.
Different chemists prefer different pet theories, and there is no shortage of theories.
I could attempt to lay all these theories before you impartially.
That would be the proper thing to do in a student textbook.
This isn't a student textbook.
The basic idea of The Blind Watchmaker is that we don't need to postulate a designer in order to understand life, or anything else in the universe.
We are here concerned with the kind of solution that must be found, because of the kind of problem we are faced with.
I think that this is best explained, not by looking at lots of particular theories, but by looking at one as an example of how the basic problem — how cumulative selection got its start — might be solved.
Now, which theory to choose as my representative sample?
Most textbooks give greatest weight to the family of theories based on an organic ‘primeval soup’.
It seems probable that the atmosphere of Earth before the coming of life was like that of other planets which are still lifeless.
There was no oxygen, plenty of hydrogen and water, carbon dioxide, very likely some ammonia, methane and other simple organic gases.
Chemists know that oxygen-free climates like this tend to foster the spontaneous synthesis of organic compounds.
They have set up in flasks miniature reconstructions of conditions on the early Earth.
They have passed through the flasks electric sparks simulating lightning, and ultraviolet light, which would have been much stronger before the Earth had an ozone layer shielding it from the sun's rays.
The results of these experiments have been exciting.
Organic molecules, some of them of the same general types as are normally only found in living things, have spontaneously assembled themselves in these flasks.
Neither DNA nor RNA has appeared, but the building blocks of these large molecules, called purines and pyrimidines, have.
So have the building blocks of proteins, amino acids.
The missing link for this class of theories is still the origin of replication.
The building blocks haven't come together to form a self-replicating chain like RNA.
Maybe one day they will.
But, in any case, the organic primeval-soup theory is not the one I have chosen for my illustration of the kind of solution that we must look for.
I did choose it in my first book, The Selfish Gene, so I thought that here I would fly a kite for a somewhat less-fashionable theory (although it recently has started gaining ground), which seems to me to have at least a sporting chance of being right.
Its audacity is appealing, and it does illustrate well the properties that any satisfying theory of the origin of life must have.
This is the ‘inorganic mineral’ theory of the Glasgow chemist Graham Cairns-Smith, first proposed 20 years ago and since developed and elaborated in three books, the latest of which, Seven Clues to the Origin of Life, treats the origin of life as a mystery needing a Sherlock Holmes solution.
Cairns-Smith's view of the DNA/protein machinery is that it probably came into existence relatively recently, perhaps as recently as three billion years ago.
Before that there were many generations of cumulative selection, based upon some quite different replicating entities.
Once DNA was there, it proved to be so much more efficient as a replicator, and so much more powerful in its effects on its own  replication, that the original replication system that spawned it was cast off and forgotten.
The modern DNA machinery, according to this view, is a late-comer, a recent usurper of the role of fundamental replicator, having taken over that role from an earlier and cruder replicator.
There may even have been a whole series of such usurpations, but the original replication process must have been sufficiently simple to have come about through what I have dubbed ‘single-step selection’.
Chemists divide their subject into two main branches, organic and inorganic.
Organic chemistry is the chemistry of one particular element, carbon.
Inorganic chemistry is all the rest.
Carbon is important and deserves to have its own private branch of chemistry, partly because life chemistry is all carbon-chemistry, and partly because those same properties that make carbon-chemistry suitable for life also make it suitable for industrial processes, such as those of the plastics industry.
The essential property of carbon atoms that makes them so suitable for life and for industrial synthetics, is that they join together to form a limitless repertoire of different kinds of very large molecules.
Another element that has some of these same properties is silicon.
Although the chemistry of modern Earth-bound life is all carbon-chemistry, this may not be true all over the universe, and it may not always have been true on this Earth.
Cairns-Smith believes that the original life on this planet was based on self-replicating inorganic crystals such as silicates.
If this is true, organic replicators, and eventually DNA, must later have taken over or usurped the role.
He gives some arguments for the general plausibility of this idea of ‘takeover’.
An arch of stones, for instance, is a stable structure capable of standing for many years even if there is no cement to bind it.
Building a complex structure by evolution is like trying to build a mortarless arch if you are allowed to touch only one stone at a time.
Think about the task naïvely, and it can't be done.
The arch will stand once the last stone is in place, but the intermediate stages are unstable.
It's quite easy to build the arch, however, if you are allowed to subtract stones as well as add them.
Start by building a solid heap of stones, then build the arch resting on top of this solid foundation.
Then, when the arch is all in position, including the vital keystone at the top, carefully remove the supporting stones and, with a modicum of luck, the arch will remain standing.
Stonehenge is incomprehensible until we realize that the builders used some kind of scaffolding, or perhaps ramps of earth, which are no longer there.
We can see only the end-product, and have to infer the vanished scaffolding.
Similarly, DNA and protein are two pillars of a stable and elegant arch, which persists  once all its parts simultaneously exist.
It is hard to imagine it arising by any step-by-step process unless some earlier scaffolding has completely disappeared.
That scaffolding must itself have been built by an earlier form of cumulative selection, at whose nature we can only guess.
But it must have been based upon replicating entities with power over their own future.
Cairns-Smith's guess is that the original replicators were crystals of inorganic materials, such as those found in clays and muds.
A crystal is just a large orderly array of atoms or molecules in the solid state.
Because of properties that we can think of as their ‘shape’, atoms and small molecules tend naturally to pack themselves together in a fixed and orderly manner.
It is almost as though they ‘want’ to slot together in a particular way, but this illusion is just an inadvertent consequence of their properties.
Their ‘preferred’ way of slotting together shapes the whole crystal.
It also means that, even in a large crystal such as a diamond, any part of the crystal is exactly the same as any other part, except where there are flaws.
If we could shrink ourselves to the atomic scale, we would see almost endless rows of atoms, stretching to the horizon in straight lines — galleries of geometric repetition.
Since it is replication we are interested in, the first thing we must know is, can crystals replicate their structure?
Crystals are made of myriads of layers of atoms (or equivalent), and each layer builds upon the layer below.
Atoms (or ions; the difference needn't concern us) float around free in solution, but if they happen to encounter a crystal they have a natural tendency to slot into position on the surface of the crystal.
A solution of common salt contains sodium ions and chloride ions jostling about in a more or less chaotic fashion.
A crystal of common salt is a packed, orderly array of sodium ions alternating with chloride ions at right angles to one another.
When ions floating in the water happen to bump into the hard surface of the crystal, they tend to stick.
And they stick in just the right places to cause a new layer to be added to the crystal just like the layer below.
So once a crystal gets started it grows, each layer being the same as the layer below.
Sometimes crystals spontaneously start to form in solution.
At other times they have to be ‘seeded’, either by particles of dust or by small crystals dropped in from elsewhere.
Cairns-Smith invites us to perform the following experiment.
Dissolve a large quantity of photographer's ‘hypo’ fixer in very hot water.
Then let the solution cool down, being careful not to let any dust drop in.
The solution is now ‘supersaturated’, ready and waiting to make crystals, but with no  seed crystals to start the process going.
I quote from Cairns-Smith's Seven Clues to the Origin of Life:
Carefully take the ha off the beaker, drop one tiny piece of ‘hypo’ crystal onto the surface of the solution, and watch amazed at what happens.
Your crystal grows visibly: it breaks up from time to time and the pieces also grow…
Soon your beaker is crowded with crystals, some several centimetres long.
Then after a few minutes it all stops.
The magic solution has lost its power — although if you want another performance just re-heat and re-cool the beaker…to be supersaturated means to have more dissolved than there ought to be…the cold supersaturated solution almost literally did not know what to do.
It had to be ‘told’ by adding a piece of crystal that already had its units (billions and billions of them) packed together in the way that is characteristic for ‘hypo’crystals.
The solution had to be seeded.
Some chemical substances have the potential to crystallize in two alternative ways.
Graphite and diamonds, for instance, are both crystals of pure carbon.
Their atoms are identical.
The two substances differ from each other only in the geometric pattern with which the carbon atoms are packed.
In diamonds, the carbon atoms are packed in a tetrahedral pattern which is extremely stable.
This is why diamonds are so hard.
In graphite the carbon atoms are arranged in flat hexagons layered on top of each other.
The bonding between layers is weak, and they therefore slide over each other, which is why graphite feels slippery and is used as a lubricant.
Unfortunately you can't crystallize diamonds out of a solution by seeding them, as you can with hypo.
If you could, you'd be rich; no on second thoughts you wouldn't, because any fool could do the same.
Now suppose we have a supersaturated solution of some substance, like hypo in that it was eager to crystallize out of solution, and like carbon in that it was capable of crystallizing in either of two ways.
One way might be somewhat like graphite, with the atoms arranged in layers, leading to little flat crystals; while the other way gives chunky, diamond-shaped crystals.
Now we simultaneously drop into our supersaturated solution a tiny flat crystal and a tiny chunky crystal.
We can describe what would happen in an elaboration of Cairns-Smith's description of his hypo experiment.
You watch amazed at what happens.
Your two crystals grow visibly: they break up from time to time and the pieces also grow.
Flat crystals give rise to a population of flat crystals.
Chunky crystals give rise to a population of chunky crystals.
If there is any tendency for one type of crystal to grow and split more quickly than the other, we shall have a simple kind of natural selection.
But the process still lacks a vital ingredient in order to give  rise to evolutionary change.
That ingredient is hereditary variation, or something equivalent to it.
Instead of just two types of crystal, there must be a whole range of minor variants that form lineages of like shape, and that sometimes ‘mutate’ to produce new shapes.
Do real crystals have something corresponding to hereditary mutation?
Clays and muds and rocks are made of tiny crystals.
They are abundant on Earth and probably always have been.
When you look at the surface of some types of clay and other minerals with a scanning electron microscope you see an amazing and beautiful sight.
Crystals grow like rows of flowers or cactuses, gardens of inorganic rose petals, tiny spirals like cross-sections of succulent plants, bristling organ pipes, complicated angular shapes folded as if in miniature crystalline origami, writhing growths like worm casts or squeezed toothpaste.
The ordered patterns become even more striking at greater levels of magnification.
At levels that betray the actual position of atoms, the surface of a crystal is seen to have all the regularity of a machine-woven piece of herringbone tweed.
But — and here is the vital point — there are flaws.
Right in the middle of an expanse of orderly herringbone there can be a patch, identical to the rest except that it is twisted round at a different angle so that the ‘weave’ goes off in another direction.
Or the weave may lie in the same direction, but each row has ‘slipped’ half a row to one side.
Nearly all naturally occurring crystals have flaws.
And once a flaw has appeared, it tends to be copied as subsequent layers of crystal encrust themselves on top of it.
Flaws can occur anywhere over the surface of a crystal.
If you like thinking about capacity for information storage (I do), you can imagine the enormous number of different patterns of flaws that could be created over the surface of a crystal.
All those calculations about packing the New Testament into the DNA of a single bacterium could be done just as impressively for almost any crystal.
What DNA has over normal crystals is a means by which its information can be read.
Leaving aside the problem of read-out, you could easily devise an arbitrary code whereby flaws in the atomic structure of the crystal denote binary numbers.
You could then pack several New Testaments into a mineral crystal the size of a pin's head.
On a larger scale, this is essentially how music information is stored on the surface of a laser (‘compact') disc.
The musical notes are converted, by computer, into binary numbers.
A laser is used to etch a pattern of tiny flaws in the otherwise glassy smooth surface of the disc.
Each little hole etched corresponds to a binary 1 (or a 0, the labels are arbitrary).
When you play the disc, another laser beam ‘reads’ the pattern of flaws, and a special-purpose computer built into the player turns the binary  numbers back into sound vibrations, which are amplified so that you can hear them.
Although laser discs are used today mainly for music, you could pack the whole Encyclopaedia Britannica onto one of them, and read it out using the same laser technique.
Flaws in crystals at the atomic level are far smaller than the pits etched in a laser disc's surface, so crystals can potentially pack more information into a given area.
Indeed DNA molecules, whose capacity for storing information has already impressed us, are something close to crystals themselves.
Although clay crystals theoretically could store the same prodigious quantities of information as DNA or laser discs can, nobody is suggesting that they ever did.
The role of clay and other mineral crystals in the theory is to act as the original ‘low-tech’ replicators, the ones that were eventually replaced by high-tech DNA.
They form spontaneously in the waters of our planet without the elaborate ‘machinery’ that DNA needs; and they develop flaws spontaneously, some of which can be replicated in subsequent layers of crystal.
If fragments of suitably flawed crystal later broke away, we could imagine them acting as ‘seeds’ for new crystals, each one ‘inheriting’its ‘parent's’pattern of flaws.
So, we have a speculative picture of mineral crystals on the primeval Earth showing some of the properties of replication, multiplication, heredity and mutation that would have been necessary in order for a form of cumulative selection to get started.
There is still the missing ingredient of ‘power’: the nature of the replicators must somehow have influenced their own likelihood of being replicated.
When we were talking about replicators in the abstract, we saw that ‘power’ might simply be direct properties of the replicator itself, intrinsic properties like ‘stickiness’.
At this elementary level, the name ‘power’ seems scarcely justified.
I use it only because of what it can become in later stages of evolution: the power of a snake's fang, for instance, to propagate (by its indirect consequences on snake survival) DNA coding for fangs.
Whether the original low-tech replicators were mineral crystals or organic direct forerunners of DNA itself, we may guess that the ‘power’ they exercised was direct and elementary, like stickiness.
Advanced levers of power, like a snake's fang or an orchid's flower, came far later.
What might ‘power’ mean to a clay?
What incidental properties of the clay could influence the likelihood that it, the same variety of clay, would be propagated around the countryside?
Clays are made from chemical building blocks such as silicic acid and metal ions, which are in solution in rivers and streams having been dissolved —‘weathered' — out of rocks further upstream.
If conditions are right they crystallize out of solution again downstream, forming clays.
(Actually the ‘stream’, in this case, is more likely to mean the seeping and trickling of the groundwater than a rushing open river.
But, for simplicity, I shall continue to use the general word stream.)
Whether or not a particular type of clay crystal is allowed to build up depends, among other things, upon the rate and pattern of flow of the stream.
But deposits of clay can also influence the flow of the stream.
They do this inadvertently by changing the level, shape and texture of the ground through which the water is flowing.
Consider a variant of clay that just happens to have the property of reshaping the structure of the soil so that the flow speeds up.
The consequence is that the clay concerned gets washed away again.
This kind of clay, by definition, is not very ‘successful’.
Another unsuccessful clay would be one that changed the flow in such a way that a rival variant of clay was favoured.
We aren't, of course, suggesting that clays ‘want’ to go on existing.
Always we are talking only about incidental consequences, events which follow from properties that the replicator just happens to have.
Consider yet another variant of clay.
This one happens to slow down the flow in such a way that future deposition of its own kind of clay is enhanced.
Obviously this second variant will tend to become common, because it happens to manipulate streams to its own ‘advantage’.
This will be a ‘successful’ variant of clay.
But so far we are dealing only with single-step selection.
Could a form of cumulative selection get going?
To speculate a little further, suppose that a variant of a clay improves its own chances of being deposited, by damming up streams.
This is an inadvertent consequence of the peculiar defect structure of the clay.
In any stream in which this kind of clay exists, large, stagnant shallow pools form above dams, and the main flow of water is diverted into a new course.
In these still pools, more of the same kind of clay is laid down.
A succession of such shallow pools proliferates along the length of any stream that happens to be ‘infected’ by seeding crystals of this kind of clay.
Now, because the main flow of the stream is diverted, during the dry season the shallow pools tend to dry up.
The clay dries and cracks in the sun, and the top layers are blown off as dust.
Each dust particle inherits the characteristic defect structure of the parent clay that did the damming, the structure that gave it its damming properties.
By analogy with the genetic information raining down on the canal from my willow tree, we could say that the dust carries ‘instructions’ for how to dam streams and eventually make more dust.
The dust spreads far and wide in the wind, and there is a good chance  that some particles of it will happen to land in another stream, hitherto not ‘infected’ with the seeds of this kind of dam-making clay.
Once infected by the right sort of dust, a new stream starts to grow crystals of dam-making clay, and the whole depositing, damming, drying, eroding cycle begins again.
To call this a ‘life’ cycle would be to beg an important question, but it is a cycle of a sort, and it shares with true life cycles the ability to initiate cumulative selection.
Because streams are infected by dust ‘seeds’ blown from other streams, we can arrange the streams in an order of ‘ancestry’and ‘descent’.
The clay that is damming up pools in stream B arrived there in the form of dust crystals blown from stream A. Eventually, the pools of stream B will dry up and make dust, which will infect streams F and P. With respect to the source of their dam-making clay, we can arrange streams into ‘family trees’.
Every infected stream has a ‘parent’ stream, and it may have more than one ‘daughter’stream.
Each stream is analogous to a body, whose ‘development’ is influenced by dust seed ‘genes’, a body that eventually spawns new dust seeds.
Each ‘generation’ in the cycle starts when seed crystals break away from the parent stream in the form of dust.
The crystalline structure of each particle of dust is copied from the clay in the parent stream.
It passes on that crystalline structure to the daughter stream, where it grows and multiplies and finally sends ‘seeds’ out again.
The ancestral crystal structure is preserved down the generations unless there is an occasional mistake in crystal growth, an occasional alteration in the pattern of laying down of atoms.
Subsequent layers of the same crystal will copy the same flaw, and if the crystal breaks in two it will give rise to a sub-population of altered crystals.
Now if the alteration makes the crystal either less or more efficient in the damming/drying/erosion cycle, this will affect how many copies it has in subsequent ‘generations’.
Altered crystals might, for instance, be more likely to split (‘reproduce').
Clay formed from altered crystals might have greater damming power in any of a variety of detailed ways.
It might crack more readily in a given amount of sun.
It might crumble into dust more readily.
The dust particles might be better at catching the wind, like fluff on a willow seed.
Some crystal types might induce a shortening of the ‘life cycle’, consequently a speeding up of their ‘evolution’.
There are many opportunities for successive ‘generations’ to become progressively ‘better’at getting passed to subsequent generations.
In other words, there are many opportunities for rudimentary cumulative selection to get going.
These little flights of fancy, embellishments of Cairns-Smith's own concern only one of several kinds of mineral ‘life cycle’ that could have  started cumulative selection along its momentous road.
There are others.
Different varieties of crystals might earn their passage to new streams, not by crumbling into dust ‘seeds’, but by dissecting their streams into lots of little streamlets that spread around, eventually joining and infecting new river systems.
Some varieties might engineer waterfalls that wear down the rocks faster, and hence speed into solution the raw materials needed to make new clays further downstream.
Some varieties of crystal might better themselves by making conditions hard for ‘rival’ varieties that compete for raw materials.
Some varieties might become ‘predatory’, breaking up rival varieties and using their elements as raw materials.
Keep holding in mind that there is no suggestion of ‘deliberate’ engineering, either here or in modern, DNA-based life.
It is just that the world automatically tends to become full of those varieties of clay (or DNA) that happen to have properties that make them persist and spread themselves about.
Now to move on to the next stage of the argument.
Some lineages of crystals might happen to catalyse the synthesis of new substances that assist in their passage down the ‘generations’.
These secondary substances would not (not at first, anyway) have had their own lineages of ancestry and descent, but would have been manufactured anew by each generation of primary replicators.
They could be seen as tools of the replicating crystal lineages, the beginnings of primitive ‘phenotypes’.
Cairns-Smith believes that organic molecules were prominent among non-replicating ‘tools’ of his inorganic crystalline replicators.
Organic molecules frequently are used in the commercial inorganic chemical industry because of their effects on the flow of fluids, and on the break-up or growth of inorganic particles: just the sorts of effects, in short, that could have influenced the ‘success’ of lineages of replicating crystals.
For instance, a clay mineral with the lovely name montmorillonite tends to break up in the presence of small amounts of an organic molecule with the less-lovely name carboxymethyl cellulose.
Smaller quantities of carboxymethyl cellulose, on the other hand, have just the opposite effect, helping to stick montmorillonite particles together.
Tannins, another kind of organic molecule, are used in the oil industry to make muds easier to drill.
If oil-drillers can exploit organic molecules to manipulate the flow and drillability of mud, there is no reason why cumulative selection should not have led to the same kind of exploitation by self-replicating minerals.
At this point Cairns-Smith's theory gets a sort of free bonus of added plausibility.
It so happens that other chemists, supporting more conventional organic ‘primeval soup’ theories, have long accepted that  clay minerals would have been a help.
To quote one of them (D. M. Anderson), ‘It is widely accepted that some, perhaps many, of the abiotic chemical reactions and processes leading to the origin on Earth of replicating micro-organisms occurred very early in the history of Earth in close proximity to the surfaces of clay minerals and other inorganic substrates.’
This writer goes on to list five ‘functions’ of clay minerals in assisting the origin of organic life, for instance‘Concentration of chemical reactants by adsorption’.
We needn't spell the five out here, or even understand them.
From our point of view, what matters is that each of these five ‘functions’ of clay minerals can be twisted round the other way.
It shows the close association that can exist between organic chemical synthesis and clay surfaces.
It is therefore a bonus for the theory that clay replicators synthesized organic molecules and used them for their own purposes.
Cairns-Smith discusses, in more detail than I can accommodate here, early uses that his clay-crystal replicators might have had for proteins, sugars and, most important of all, nucleic acids like RNA.
He suggests that RNA was first used for purely structural purposes, as oil drillers use tannins or we use soap and detergents.
RNA-like molecules, because of their negatively charged backbones, would tend to coat the outsides of clay particles.
This is getting us into realms of chemistry that are beyond our scope.
For our purposes what matters is that RNA, or something like it, was around for a long time before it became self-replicating.
When it finally did become self-replicating, this was a device evolved by the mineral crystal ‘genes’ to improve the efficiency of manufacture of the RNA (or similar molecule).
But, once a new self-replicating molecule had come into existence, a new kind of cumulative selection could get going.
Originally a side-show, the new replicators turned out to be so much more efficient than the original crystals that they took over.
They evolved further, and eventually perfected the DNA code that we know today.
The original mineral replicators were cast aside like worn-out scaffolding, and all modern life evolved from a relatively recent common ancestor, with a single, uniform genetic system and a largely uniform biochemistry.
In The Selfish Gene I speculated that we may now be on the threshold of a new kind of genetic takeover.
DNA replicators built ‘survival machines’ for themselves — the bodies of living organisms including ourselves.
As part of their equipment, bodies evolved onboard computers — brains.
Brains evolved the capacity to communicate with other brains by means of language and cultural traditions.
But the new milieu of cultural tradition opens up new possibilities for self-replicating entities.
The new replicators are not DNA and they are not  clay crystals.
They are patterns of information that can thrive only in brains or the artificially manufactured products of brains — books, computers, and so on.
But, given that brains, books and computers exist, these new replicators, which I called memes to distinguish them from genes, can propagate themselves from brain to brain, from brain to book, from book to brain, from brain to computer, from computer to computer.
As they propagate they can change — mutate.
And perhaps ‘mutant’ memes can exert the kinds of influence that I am here calling ‘replicator power’.
Remember that this means any kind of influence affecting their own likelihood of being propagated.
Evolution under the influence of the new replicators — memic evolution — is in its infancy.
It is manifested in the phenomena that we call cultural evolution.
Cultural evolution is many orders of magnitude faster than DNA-based evolution, which sets one even more to thinking of the idea of ‘takeover’.
And if a new kind of replicator takeover is beginning, it is conceivable that it will take off so far as to leave its parent DNA (and its grandparent clay if Cairns-Smith is right) far behind.
If so, we may be sure that computers will be in the van.
Could it be that one far-off day intelligent computers will speculate about their own lost origins?
Will one of them tumble to the heretical truth, that they have sprung from a remote, earlier form of life, rooted in organic, carbon chemistry, rather than the silicon-based electronic principles of their own bodies?
Will a robotic Cairns-Smith write a book called Electronic Takeover?
Will he rediscover some electronic equivalent of the metaphor of the arch, and realize that computers could not have sprung spontaneously into existence but must have originated from some earlier process of cumulative selection?
Will he go into detail and reconstruct DNA as a plausible early replicator, victim of electronic usurpation?
And will he be far-sighted enough to guess that even DNA may itself have been a usurper of yet more remote and primitive replicators, crystals of inorganic silicates?
If he is of a poetic turn of mind, will he even see a kind of justice in the eventual return to silicon-based life, with DNA no more than an interlude, albeit one that lasted longer than three aeons?
That is science fiction, and it probably sounds far-fetched.
That doesn't matter.
Of more immediate moment is that Cairns-Smith's own theory, and indeed all other theories of the origin of life, may sound far-fetched to you and hard to believe.
Do you find both Cairns-Smith's clay theory, and the more orthodox organic primeval-soup theory, wildly improbable?
Does it sound to you as though it would need a miracle to make randomly jostling atoms join together into a self-replicating molecule?
Well, at times it does to me too.
But  let's look more deeply into this matter of miracles and improbability.
By doing so, I shall demonstrate a point which is paradoxical but all the more interesting for that.
This is that we should, as scientists, be even a little worried if the origin of life did not seem miraculous to our own human consciousness.
An apparently (to ordinary human consciousness) miraculous theory is exactly the kind of theory we should be looking for in this particular matter of the origin of life.
This argument, which amounts to a discussion of what we mean by a miracle, will occupy the rest of this chapter.
In a way it is an extension of the argument we made earlier about billions of planets.
So, what do we mean by a miracle?
A miracle is something that happens, but which is exceedingly surprising.
If a marble statue of the Virgin Mary suddenly waved its hand at us we should treat it as a miracle, because all our experience and knowledge tells us that marble doesn't behave like that.
I have just uttered the words ‘May I be struck by lightning this minute’.
If lightning did strike me in the same minute, it would be treated as a miracle.
But actually neither of these two occurrences would be classified by science as utterly impossible.
They would simply be judged very improbable, the waving statue much more improbable than the lightning.
Lightning does strike people.
Any one of us might be struck by lightning, but the probability is pretty low in any one minute (although the Guinness Book of Records has a charming picture of a Virginian man, nicknamed the human lightning conductor, recovering in hospital from his seventh lightning strike, with an expression of apprehensive bewilderment on his face).
The only thing miraculous about my hypothetical story is the coincidence between my being struck by lightning and my verbal invocation of the disaster.
Coincidence means multiplied improbability.
The probability of my being struck by lightning in any one minute of my life is perhaps 1 in 10 million as a conservative estimate.
The probability of my inviting a lightning strike in any particular minute is also very low.
I have just done it for the only time in the 23,400,000 minutes of my life so far, and I doubt if I'll do it again, so call these odds one in 25 million.
To calculate the joint probability of the coincidence occurring in any one minute we multiply the two separate probabilities.
For my rough calculation this comes to about one in 250 trillion.
If a coincidence of this magnitude happened to me, I should call it a miracle and would watch my language in future.
But although the odds against the coincidence are extremely high, we can still calculate them.
They are not literally zero.
In the case of the marble statue, molecules in solid marble are  continuously jostling against one another in random directions.
The jostlings of the different molecules cancel one another out, so the whole hand of the statue stays still.
But if, by sheer coincidence, all the molecules just happened to move in the same direction at the same moment, the hand would move.
If they then all reversed direction at the same moment the hand would move back.
In this way it is possible for a marble statue to wave at us.
It could happen.
The odds against such a coincidence are unimaginably great but they are not incalculably great.
A physicist colleague has kindly calculated them for me.
The number is so large that the entire age of the universe so far is too short a time to write out all the noughts!
It is theoretically possible for a cow to jump over the moon with something like the same improbability.
The conclusion to this part of the argument is that we can calculate our way into regions of miraculous improbability far greater than we can imagine as plausible.
Let's look at this matter of what we think is plausible.
What we can imagine as plausible is a narrow band in the middle of a much broader spectrum of what is actually possible.
Sometimes it is narrower than what is actually there.
There is a good analogy with light.
Our eyes are built to cope with a narrow band of electromagnetic frequencies (the ones we call light), somewhere in the middle of the spectrum from long radio waves at one end to short X-rays at the other.
We can't see the rays outside the narrow light band, but we can do calculations about them, and we can build instruments to detect them.
In the same way, we know that the scales of size and time extend in both directions far outside the realm of what we can visualize.
Our minds can't cope with the large distances that astronomy deals in or with the small distances that atomic physics deals in, but we can represent those distances in mathematical symbols.
Our minds can't imagine a time span as short as a picosecond, but we can do calculations about picoseconds, and we can build computers that can complete calculations within picoseconds.
Our minds can't imagine a timespan as long as a million years, let alone the thousands of millions of years that geologists routinely compute.
Just as our eyes can see only that narrow band of electromagnetic frequencies that natural selection equipped our ancestors to see, so our brains are built to cope with narrow bands of sizes and times.
Presumably there was no need for our ancestors to cope with sizes and times outside the narrow range of everyday practicality, so our brains never evolved the capacity to imagine them.
It is probably significant that our own body size of a few feet is roughly in the middle of the range of sizes we can imagine.
And our own lifetime of a few decades is roughly in the middle of the range of times we can imagine.
We can say the same kind of thing about improbabilities and miracles.
Picture a graduated scale of improbabilities, analogous to the scale of sizes from atoms to galaxies, or to the scale of times from picoseconds to aeons.
On the scale we mark off various landmark points.
At the far left-hand end of the scale are events which are all but certain, such as the probability that the sun will rise tomorrow — the subject of G. H. Hardy's halfpenny bet.
Near this left-hand end of the scale are things that are only slightly improbable, such as shaking a double six in a single throw of a pair of dice.
The odds of this happening are 1 in 36.1 expect we've all done it quite often.
Moving towards the right-hand end of the spectrum, another landmark point is the probability of a perfect deal in bridge, where each of the four players receives a complete suit of cards.
The odds against this happening are 2,235,197,406,895,366,368,301,559,999 to 1.
Let us call this one dealion, the unit of improbability.
If something with an improbability of one dealion was predicted and then happened, we should diagnose a miracle unless, which is more probable, we suspected fraud.
But it could happen with a fair deal, and it is far far far more probable than the marble statue's waving at us.
Nevertheless, even this latter event as we have seen, has its rightful place along the spectrum of events that could happen.
It is measurable, albeit in units far larger than gigadealions.
Between the double-six dice throw, and the perfect deal at bridge, is a range of more or less improbable events that do sometimes happen, including any one individual's being struck by lightning, winning a big prize on the football pools, scoring a hole-in-one at golf, and so on.
Somewhere in this range, too, are those coincidences that give us an eerie spine-tingling feeling, like dreaming of a particular person for the first time in decades, then waking up to find that they died in the night.
These eerie coincidences are very impressive when they happen to us or to one of our friends, but their improbability is measured in only picodealions.
Having constructed our mathematical scale of improbabilities, with its benchmark or landmark points marked on it, let us now turn a spotlight on that subrange of the scale with which we, in our ordinary thought and conversation, can cope.
The width of the spotlight's beam is analogous to the narrow range of electromagnetic frequencies that our eyes can see, or to the narrow range of sizes or times, close to our own size and longevity, that we can imagine.
On the spectrum of improbabilities, the spotlight turns out to illuminate only the narrow range from the left-hand end (certainty) up to minor miracles, like a hole-in-one or a dream that comes true.
There is a vast range of mathematically calculable improbabilities way outside the range of the spotlight.
Our brains have been built by natural selection to assess probability and risk, just as our eyes have been built to assess electromagnetic wavelength.
We are equipped to make mental calculations of risk and odds, within the range of improbabilities that would be useful in human life.
This means risks of the order of, say, being gored by a buffalo if we shoot an arrow at it, being struck by lightning if we shelter under a lone tree in a thunderstorm, or drowning if we try to swim across a river.
These acceptable risks are commensurate with our lifetimes of a few decades.
If we were biologically capable of living for a million years, and wanted to do so, we should assess risks quite differently.
We should make a habit of not crossing roads, for instance, for if you crossed a road every day for half a million years you would undoubtedly be run over.
Evolution has equipped our brains with a subjective consciousness of risk and improbability suitable for creatures with a lifetime of less than one century.
Our ancestors have always needed to take decisions involving risks and probabilities, and natural selection has therefore equipped our brains to assess probabilities against a background of the short lifetime that we can, in any case, expect.
If on some planet there are beings with a lifetime of a million centuries, their spotlight of comprehensible risk will extend that much farther towards the right-hand end of the continuum.
They will expect to be dealt a perfect bridge hand from time to time, and will scarcely trouble to write home about it when it happens.
But even they will blench if a marble statue waves at them, for you would have to live dealions of years longer than even they do to see a miracle of this magnitude.
What has all this to do with theories of the origin of life?
Well, we began this argument by agreeing that Cairns-Smith's theory, and the primeval-soup theory, sound a bit far-fetched and improbable to us.
We naturally feel inclined to reject these theories for that reason.
But ‘we’, remember, are beings whose brains are equipped with a spotlight of comprehensible risk that is a pencil-thin beam illuminating the far left-hand end of the mathematical continuum of calculable risks.
Our subjective judgement of what seems like a good bet is irrelevant to what is actually a good bet.
The subjective judgement of an alien with a lifetime of a million centuries will be quite different.
He will judge as quite plausible an event, such as the origin of the first replicating molecule as postulated by some chemist's theory, which we, kitted up by evolution to move in a world of a few decades' duration, would judge to be an astounding miracle.
How can we decide whose point of view is the right one, ours or the long-lived alien's?
There is a simple answer to this question.
The long-lived alien's  point of view is the right one for looking at the plausibility of a theory like Cairns-Smith's or the primeval-soup theory.
This is because those two theories postulate a particular event — the spontaneous arising of a self-replicating entity — as occurring only once in about a billion years, once per aeon.
One and a half aeons is about the time that elapsed between the origin of the Earth and the first bacteria-like fossils.
For our decade-conscious brains, an event that happens only once per aeon is so rare as to seem a major miracle.
For the long-lived alien, it will seem less of a miracle than a golf hole-in-one seems to us — and most of us probably know somebody who knows somebody who has scored a hole-in-one.
In judging theories of the origin of life, the long lived alien's subjective timescale is the relevant one, because it is approximately the timescale involved in the origin of life.
Our own subjective judgement about the plausibility of a theory of the origin of life is likely to be wrong by a factor of a hundred million.
In fact our subjective judgement is probably wrong by an even greater margin.
Not only are our brains equipped by nature to assess risks of things in a short time; they are also equipped to assess risks of things happening to us personally, or to a narrow circle of people that we know.
This is because our brains didn't evolve under conditions dominated by mass media.
Mass reporting means that, if an improbable thing happens to anybody, anywhere in the world, we shall read about it in our newspapers or in the Guinness Book of Records.
If an orator, anywhere in the world, publicly challenged the lightning to strike him if he lied, and it promptly did so, we should read about it and be duly impressed.
But there are several billion people in the world to whom such a coincidence could happen, so the apparent coincidence is actually not as great as it seems.
Our brains are probably equipped by nature to assess the risks of things happening to ourselves, or to a few hundred people in the small circle of villages within drum-range that our tribal ancestors could expect to hear news about.
When we read in a newspaper about an amazing coincidence happening to somebody in Valparaiso or Virginia, we are more impressed by it than we should be.
More impressed by a factor of perhaps a hundred million, if that is the ratio between the world population surveyed by our newspapers, and the tribal population about whom our evolved brains ‘expect’ to hear news.
This ‘population calculation’ is also relevant to our judgement of the plausibility of theories of the origin of life.
Not because of the population of people on Earth, but because of the population of planets in the universe, the population of planets where life could have originated.
This is just the argument we met earlier in this chapter, so  there is no need to dwell on it here.
Go back to our mental picture of a graduated scale of improbable events with its benchmark coincidences of bridge hands and dice throws.
On this graduated scale of dealions and microdealions, mark the following three new points.
Probability of life arising on a planet (in, say, a billion years), if we assume that life arises at a rate of about once per solar system.
Probability of life arising on a planet if life arises at a rate of about once per galaxy.
Probability of life on a randomly selected planet if life arose only once in the universe.
Label these three points respectively the Solar System Number, the Galaxy Number and the Universe Number.
Remember that there are about 10,000 million galaxies.
We don't know how many solar systems there are in each galaxy because we can only see stars, not planets, but we earlier used an estimate that there may be 100 billion billion planets in the universe.
When we assess the improbability of an event postulated by, for instance the Cairns-Smith theory, we should assess it, not against what we subjectively think of as probable or improbable, but against numbers like these three numbers, the Solar System Number, the Galaxy Number and the Universe Number.
Which of these three numbers is the most appropriate depends upon which of the following three statements we think is nearest the truth:
1.
Life has arisen in only one planet in the entire universe (and that planet, as we saw earlier, then has to be Earth).
2.
Life has arisen on about one planet per galaxy (in our galaxy, Earth is the lucky planet).
3.
The origin of life is a sufficiently probable event that it tends to arise about once per solar system (in our solar system Earth is the lucky planet).
These three statements represent three benchmark views about the uniqueness of life.
The actual uniqueness of life probably lies somewhere between the extremes represented by Statement 1 and Statement 3.
Why do I say that?
Why, in particular, should we rule out a fourth possibility, that the origin of life is a far more probable event than is suggested by Statement 3?
It isn't a strong argument, but, for what it is worth, it goes like this.
If the origin of life were a much more probable event than is suggested by the Solar System Number we should expect, by now, to have encountered extraterrestrial life, if not in (whatever passes for) the flesh, at least by radio.
It is often pointed out that chemists have failed in their attempts to duplicate the spontaneous origin of life in the laboratory.
This fact is  used as if it constituted evidence against the theories that those chemists are trying to test.
But actually one can argue that we should be worried if it turned out to be very easy for chemists to obtain life spontaneously in the test-tube.
This is because chemists' experiments last for years not thousands of millions of years, and because only a handful of chemists, not thousands of millions of chemists, are engaged in doing these experiments.
If the spontaneous origin of life turned out to be a probable enough event to have occurred during the few man-decades in which chemists have done their experiments, then life should have arisen many times on Earth, and many times on planets within radio range of Earth.
Of course all this begs important questions about whether chemists have succeeded in duplicating the conditions of the early Earth but, even so, given that we can't answer these questions, the argument is worth pursuing.
If the origin of life were a probable event by ordinary human standards, then a substantial number of planets within radio range should have developed a radio technology long enough ago (bearing in mind that radio waves travel at 186,000 miles per second) for us to have picked up at least one transmission during the decades that we have been equipped to do so.
There are probably about 50 stars within radio range if we assume that they have had radio technology for only as long as we have.
But 50 years is just a fleeting instant, and it would be a major coincidence if another civilization were so closely in step with us.
If we embrace in our calculation those civilizations that had radio technology 1,000 years ago, there will be something like a million stars within radio range (together with however many planets circle round each one of them!.
If we include those whose radio technology goes back 100,000 years, the whole trillion-star galaxy would be within radio range.
Of course, broadcast signals would become pretty attenuated over such huge distances.
So we have arrived at the following paradox.
If a theory of the origin of life is sufficiently ‘plausible’ to satisfy our subjective judgement of plausibility, it is then too ‘plausible’to account for the paucity of life in the universe as we observe it.
According to this argument, the theory we are looking for has got to be the kind of theory that seems implausible to our limited, Earth-bound, decade-bound imaginations.
Seen in this light, both Cairns-Smith's theory and the primeval-soup theory seem if anything in danger of erring on the side of being too plausible!
HaYing said all this I must confess that, because there is so much uncertainty in the calculations, if a chemist did succeed in creating spontaneous life I would not actually be disconcerted!
We still don't know exactly how natural selection began on Earth 
This chapter has had the modest aim of explaining only the kind of way in which it must have happened.
The present lack of a definitely accepted account of the origin of life should certainly not be taken as a stumbling block for the whole Darwinian world view, as it occasionally — probably with wishful thinking — is.
The earlier chapters have disposed of other alleged stumbling blocks, and the next chapter takes up yet another one, the idea that natural selection can only destroy, never construct.
CONSTRUCTIVE EVOLUTION
People sometimes think that natural selection is a purely negative force, capable of weeding out freaks and failures, but not capable of building up complexity, beauty and efficiency of design.
Does it not merely subtract from what is already there, and shouldn't a truly creative process add something too?
One can partially answer this by pointing to a statue.
Nothing is added to the block of marble.
The sculptor only subtracts, but a beautiful statue emerges nevertheless.
But this metaphor can mislead, for some people leap straight to the wrong part of the metaphor — the fact that the sculptor is a conscious designer- and miss the important part: the fact that the sculptor works by subtraction rather than addition.
Even this part of the metaphor should not be taken too far.
Natural selection may only subtract, but mutation can add.
There are ways in which mutation and natural selection together can lead, over the long span of geological time, to a building up of complexity that has more in common with addition than with subtraction.
There are two main ways in which this build-up can happen.
The first of these goes under the name of ‘coadapted genotypes’; the second under the name of ‘arms races’.
The two are superficially rather different from one another, but they are united under the headings of ‘coevolution’ and ‘genes as each others’environments'.
First, the idea of ‘coadapted genotypes’.
A gene has the particular effect that it does only because there is an existing structure upon which to work.
A gene can't affect the wiring up of a brain unless there is a brain being wired up in the first place.
There won't be a brain being wired up in the first place, unless there is a complete developing embryo.
And there won't be a complete developing embryo unless  there is a whole program of chemical and cellular events, under the influence of lots and lots of other genes, and lots and lots of other, non-genetic, causal influences.
The particular effects that genes have are not intrinsic properties of those genes.
They are properties of embryological processes, existing processes whose details may be changed by genes, acting in particular places and at particular times during embryonic development.
We saw this message demonstrated, in elementary form, by the development of the computer biomorphs.
In a sense, the whole process of embryonic development can be looked upon as a cooperative venture, jointly run by thousands of genes together.
Embryos are put together by all the working genes in the developing organism, in collaboration with one another.
Now comes the key to understanding how such collaborations come about.
In natural selection, genes are always selected for their capacity to flourish in the environment in which they find themselves.
We often think of this environment as the outside world, the world of predators and climate.
But from each gene's point of view, perhaps the most important part of its environment is all the other genes that it encounters.
And where does a gene ‘encounter’ other genes?
Mostly in the cells of the successive individual bodies in which it finds itself.
Each gene is selected for its capacity to cooperate successfully with the population of other genes that it is likely to meet in bodies.
The true population of genes, which constitutes the working environment of any given gene, is not just the temporary collection that happens to have come together in the cells of any particular individual body.
At least in sexually reproducing species, it is the set of all genes in the population of interbreeding individuals — the gene ‘pool’.
At any given moment, any particular copy of a gene, in the sense of a particular collection of atoms, must be sitting in one cell of one individual.
But the set of atoms that is any one copy of a gene is not of permanent interest.
It has a life-expectancy measured only in months.
As we have seen, the long-lived gene as an evolutionary unit is not any particular physical structure but the textual archival information that is copied on down the generations.
This textual replicator has a distributed existence.
It is widely distributed in space among different individuals, and widely distributed in time over many generations.
When looked at in this distributed way, any one gene can be said to ‘meet’ another when they find themselves sharing a body.
It can ‘expect’ to meet a variety of other genes in different bodies at different times in its distributed existence, and in its march through geological time.
A successful gene will be one that does well in the environments provided by these other genes that it is likely to meet in lots of different  bodies.
‘Doing well’ in such environments will turn out to be equivalent to ‘collaborating’with these other genes.
It is most directly seen in the case of biochemical pathways.
Biochemical pathways are sequences of chemicals that constitute successive stages in some useful process, like the release of energy or the synthesis of an important substance.
Each step in the pathway needs an enzyme — one of those large molecules that is shaped to act like a machine in a chemical factory.
Different enzymes are needed for different steps in the chemical pathway.
Sometimes there are two, or more, alternative chemical pathways to the same useful end.
Although both pathways culminate in the identical useful result, they have different intermediate stages leading up to that end, and they normally have different starting points.
Either of the two alternative pathways will do the job, and it doesn't matter which one is used.
The important thing for any particular animal is to avoid trying to do both at once, for chemical confusion and inefficiency would result.
Now suppose that Pathway 1 needs the succession of enzymes A1, B1 and C1, in order to synthesize a desired chemical D, while Pathway 2 needs enzymes A2, B2 and C2 in order to arrive at the same desirable end-product.
Each enzyme is made by a particular gene.
So, in order to evolve the assembly line for Pathway 1, a species needs the genes coding for A1, B1 and C1 all to coevolve together.
In order to evolve the alternative assembly line for Pathway 2, a species would need the genes coding for A2, B2 and C2 to coevolve with one another.
The choice between these two coevolutions doesn't come about through advance planning.
It comes about simply through each gene being selected by virtue of its compatibility with the other genes that already happen to dominate the population.
If the population happens to be already rich in genes for B1 and C1, this will set up a climate favouring the A1 gene rather than the A2 gene.
Conversely, if the population is already rich in genes for B2 and C2 this will set up a climate in which the A2 gene is favoured by selection rather than the A1 gene.
It will not be as simple as that, but you will have got the idea: one of the most important aspects of the ‘climate’ in which a gene is favoured or disfavoured is the other genes that are already numerous in the population; the other genes, therefore, with which it is likely to have to share bodies.
Since the same will obviously be true of these ‘other’ genes themselves, we have a picture of teams of genes all evolving towards cooperative solutions to problems.
The genes themselves don't evolve, they merely survive or fail to survive in the gene pool.
It is the ‘team’ that evolves.
Other teams might have done the job just as  well, or even better.
But once one team has started to dominate the gene pool of a species it thereby has an automatic advantage.
It is difficult for a minority team to break in, even a minority team which would, in the end, have done the job more efficiently.
The majority team has an automatic resistance to being displaced, simply by virtue of being in the majority.
This doesn't mean that the majority team can never be displaced.
If it couldn't, evolution would grind to a halt.
But it does mean that there is a kind of built-in inertia.
Obviously this kind of argument is not limited to biochemistry.
We could make the same kind of case for clusters of compatible genes building the different parts of eyes, ears, noses, walking limbs, all the cooperating parts of an animal's body.
Genes for making teeth suitable for chewing meat tend to be favoured in a ‘climate’ dominated by genes making guts suitable for digesting meat.
Conversely, genes for making plant-grinding teeth tend to be favoured in a climate dominated by genes that make guts suitable for digesting plants.
And vice versa in both cases.
Teams of ‘meat-eating genes’ tend to evolve together, and teams of ‘plant-eating genes’tend to evolve together.
Indeed, there is a sense in which most of the working genes in a body can be said to cooperate with each other as a team, because over evolutionary time they (i.e. ancestral copies of themselves) have each been part of the environment in which natural selection has worked on the others.
If we ask why the ancestors of lions took to meat-eating, while the ancestors of antelopes took to grass-eating, the answer could be that originally it was an accident.
An accident, in the sense that it could have been the ancestors of lions that took up grass-eating, and the ancestors of antelopes that took up meat-eating.
But once one lineage had begun to build up a team of genes for dealing with meat rather than grass, the process was self-reinforcing.
And once the other lineage had begun to build up a team of genes for dealing with grass rather than meat, that process was self-reinforcing in the other direction.
One of the main things that must have happened in the early evolution of living organisms was an increase in the numbers of genes participating in such cooperatives.
Bacteria have far fewer genes than animals and plants.
The increase may have come about through various kinds of gene duplication.
Remember that a gene is just a length of coded symbols, like a file on a computer disc; and genes can be copied to different parts of the chromosomes, just as files can be copied to different parts of the disc.
On my disc that holds this chapter there are officially just three files.
By ‘officially’ I mean that the computer's operating system tells me that there are just three files.
I can ask it to read one of these three files, and it presents me with a  one-dimensional array of alphabetical characters, including the characters that you are now reading.
All very neat and orderly, it seems.
But in fact, on the disc itself, the arrangement of the text is anything but neat and orderly.
You can see this if you break away from the discipline of the computer's own official operating system, and write your own private programs to decipher what is actually written on every sector of the disc.
It turns out that fragments of each of my three files are dotted around, interleaved with each other and with fragments of old, dead files that I erased long ago and had forgotten.
Any given fragment may turn up, word for word the same, or with minor differences, in half a dozen different places all around the disc.
The reason for this is interesting, and worth a digression because it provides a good genetic analogy.
When you tell a computer to delete a file, it appears to obey you.
But it doesn't actually wipe out the text of that file.
It simply wipes out all pointers to that file.
It is as though a librarian, ordered to destroy Lady Chatterley's Lover, simply tore up the card from the card index, leaving the book itself on the shelf.
For the computer, this is a perfectly economical way to do things, because the space formerly occupied by the ‘deleted’ file is automatically available for new files, as soon as the pointers to the old file have been removed.
It would be a waste of time actually to go to the trouble of filling the space itself with blanks.
The old file won't itself be finally lost until all its space happens to be used for storing new files.
But this re-using of space occurs piecemeal.
New files aren't exactly the same size as old ones.
When the computer is trying to save a new file to a disc, it looks for the first available fragment of space, writes as much of the new file as will fit, then looks for another available fragment of space, writes a bit more, and so on until all the file is written somewhere on the disc.
The human has the illusion that the file is a single, orderly array, only because the computer is careful to keep records ‘pointing’ to the addresses of all the fragments dotted around.
These ‘pointers’ are like the ‘continued on page 94’pointers used by the New York Times.
The reason many copies of any one fragment of text are found on a disc is that if, like all my chapters, the text has been edited and re-edited many dozens of times, each edit will result in a new saving to the disc of (almost) the same text.
The saving may ostensibly be a saving of the same file.
But as we have seen, the text will in fact be repeatedly scattered around the available ‘gaps’ on the disc.
Hence multiple copies of a given fragment of text can be found all around the surface of the disc, the more so if the disc is old and much used.
Now the DNA operating system of a species is very very old indeed, and there is evidence that it, seen in the long term, does something a bit like the computer with its disc files.
Part of the evidence comes from the fascinating phenomenon of ‘introns’ and ‘exons’.
Within the last decade, it has been discovered that any ‘single’ gene, in the sense of a single continuously read passage of DNA text, is not all stored in one place.
If you actually read the code letters as they occur along the chromosome (i.e. if you do the equivalent of breaking out of the discipline of the ‘operating system’) you find fragments of ‘sense’, called exons, separated by portions of ‘nonsense’ called introns.
Any one ‘gene’ in the functional sense, is in fact split up into a sequence of fragments (exons) separated by meaningless introns.
It is as if each exon ended with a pointer saying ‘continued on page 94’.
A complete gene is then made up of a whole series of exons, which are actually strung together only when they are eventually read by the ‘official’ operating system that translates them into proteins.
Further evidence comes from the fact that the chromosomes are littered with old genetic text that is no longer used, but which still makes recognizable sense.
To a computer programmer, the pattern of distribution of these ‘genetic fossil’ fragments is uncannily reminiscent of the pattern of text on the surface of an old disc that has been much used for editing text.
In some animals, a high proportion of the total number of genes is in fact never read.
These genes are either complete nonsense, or they are outdated ‘fossil genes’.
Just occasionally, textual fossils come into their own again, as I experienced when writing this book.
A computer error (or, to be fair, it may have been human error) caused me accidentally to ‘erase’ the disc containing Chapter 3.
Of course the text itself hadn't literally all been erased.
All that had been definitely erased were the pointers to where each ‘exon’ began and ended.
The ‘official’ operating system could read nothing, but ‘unofficially’I could play genetic engineer and examine all the text on the disc.
What I saw was a bewildering jigsaw puzzle of textual fragments, some of them recent, others ancient ‘fossils’.
By piecing together the jigsaw fragments, I was able to recreate the chapter.
But I mostly didn't know which fragments were recent and which were fossil.
It didn't matter for, apart from minor details that necessitated some new editing, they were the same.
At least some of the ‘fossils’, or outdated ‘introns’, had come into their own again.
They rescued me from my predicament, and saved me the trouble of rewriting the entire chapter.
There is evidence that, in living species too, ‘fossil genes’ occasionally come into their own again, and are re-used after lying dormant for a million years or so.
To go into detail would carry us too  far from the main pathway of this chapter, for you will remember that we are already out on a digression.
The main point was that the total genetic capacity of a species may increase due to gene duplication.
Reusing of old ‘fossil’ copies of existing genes is one way in which this can happen.
There are other, more immediate, ways in which genes may be copied to widely distributed parts of the chromosomes, like files being duplicated to different parts of a disc, or different discs.
Humans have eight separate genes called globin genes (used for making haemoglobin, among other things), on various different chromosomes.
It seems certain that all eight have been copied, ultimately from a single ancestral globin gene.
About 1,100 million years ago, the ancestral globin gene duplicated, forming two genes.
We can date this event because of independent evidence about how fast globins habitually evolve (see Chapters 5 and 11).
Of the two genes produced by this original duplication, one became the ancestor of all the genes that make haemoglobin in vertebrates.
The other became the ancestor of all the genes that make myoglobins, a related family of proteins that work in muscles.
Various subsequent duplications have given rise to the so-called alpha, beta, gamma, delta, epsilon and zeta globins.
The fascinating thing is that we can construct a complete family tree of all the globin genes, and even put dates on all the divergence points (delta and beta globin parted company, for example, about 40 million years ago; epsilon and gamma globins 100 million years ago).
Yet the eight globins, descendants as they are of these remote branchings in distant ancestors, are still all present inside every one of us.
They diverged to different parts of an ancestor's chromosomes, and we have each inherited them on our different chromosomes.
Molecules are sharing the same body with their remote molecular cousins.
It is certain that a great deal of such duplication has gone on, all over the chromosomes, and throughout geological time.
This is an important respect in which real life is more complicated than the biomorphs of Chapter 3.
They all had only nine genes.
They evolved by changes in those nine genes, never by increasing the number of genes to ten.
Even in real animals, such duplications are rare enough not to invalidate my general statement that all members of a species share the same DNA ‘addressing’ system.
Duplication within the species isn't the only means by which the number of cooperating genes has increased in evolution.
An even rarer, but still possibly very important occurrence, is the occasional incorporation of a gene from another species, even an extremely remote species.
There are, for example, haemoglobins in the roots of plants of the pea family.
They don't occur in any other plant families, and it  seems almost certain that they somehow got into the pea family by cross-infection from animals, viruses perhaps acting as intermediaries.
An especially important event along these lines, according to the increasingly favoured theory of the American biologist Lynn Margulis, took place at the origin of the so-called eukaryotic cell.
Eukaryotic cells include all cells except those of bacteria.
The living world is divided, fundamentally, into bacteria versus the rest.
We are part of the rest, and are collectively called the eukaryotes.
We differ from bacteria mainly in that our cells have discrete little mini-cells inside them.
These include the nucleus, which houses the chromosomes; the tiny bomb-shaped objects called mitochondria (which we briefly met in Figure 1), filled with intricately folded membranes; and, in the (eukaryotic) cells of plants, chloroplasts.
Mitochondria and chloroplasts have their own DNA, which replicates and propagates itself entirely independently of the main DNA in the chromosomes of the nucleus.
All the mitochondria in you are descended from the small population of mitochondria that travelled from your mother in her egg.
Sperms are too small to contain mitochondria, so mitochondria travel exclusively down the female line, and male bodies are dead ends as far as mitochondrial reproduction is concerned.
Incidentally, this means that we can use mitochondria to trace our ancestry, strictly down the female line.
Margulis's theory is that mitochondria and chloroplasts, and a few other structures inside cells, are each descended from bacteria.
The eukaryotic cell was formed, perhaps 2 billion years ago, when several kinds of bacteria joined forces because of the benefits that each could obtain from the others.
Over the aeons they have become so thoroughly integrated into the cooperative unit that became the eukaryotic cell, that it has become almost impossible to detect the fact, if indeed it is a fact, that they were once separate bacteria.
It seems that, once the eukaryotic cell had been invented, a whole new range of designs became possible.
Most interestingly from our point of view, cells could manufacture large bodies comprising many billions of cells.
All cells reproduce by splitting into two, both halves getting a full set of genes.
As we saw in the case of the bacteria on a pin's head, successive splittings into two can generate a very large number of cells in rather a short time.
You start with one and it splits into two.
Then each of the two splits, making four.
Each of the four splits, making eight.
The numbers go up by successive doublings, from 8 to 16, 32, 64, 128, 256, 512, 1,024, 2,048, 4,096, 8,192.
After only 20 doublings, which doesn't take very long, we are up in the millions.
After only 40 doublings the number of cells is more than a trillion.
In  the case of bacteria, the enormous numbers of cells produced by successive doublings go their separate ways.
The same is true of many eukaryotic cells, for instance protozoa such as amoebas.
A major step in evolution was taken when cells that had been produced by successive splittings stuck together instead of going off independently.
Higher-order structure could now emerge, just as it did, on an incomparably smaller scale, in the two-way branching computer biomorphs.
Now, for the first time, large body size became a possibility.
A human body is a truly colossal population of cells, all descended from one ancestor, the fertilized egg; and all therefore cousins, children, grandchildren, uncles, etc. of other cells in the body.
The 10 trillion cells that make up each one of us are the product of a few dozens of generations of cell doublings.
These cells are classified into about 210 (according to taste) different kinds, all built by the same set of genes but with different members of the set of genes turned on in different kinds of cells.
This, as we have seen, is why liver cells are different from brain cells, and bone cells are different from muscle cells.
Genes working through the organs and behaviour patterns of many-celled bodies can achieve methods of ensuring their own propagation that are not available to single cells working on their own.
Many-celled bodies make it possible for genes to manipulate the world, using tools built on a scale that is orders of magnitude larger than the scale of single cells.
They achieve these large-scale indirect manipulations via their more direct effects on the miniature scale of cells.
For instance, they change the shape of the cell membrane.
The cells then interact with one another in huge populations to produce large-scale group effects such as an arm or a leg or (more indirectly) a beaver's dam.
Most of the properties of an organism that we are equipped to see with our naked eyes are so-called ‘emergent properties’.
Even the computer biomorphs, with their nine genes, had emergent properties.
In real animals they are produced at the whole-body level by interactions between cells.
An organism works as an entire unit, and its genes can be said to have effects on the whole organism, even though each copy of any one gene exerts its immediate effects only within its own cell.
We have seen that a very important part of a gene's environment is the other genes that it is likely to meet in successive bodies as the generations go by.
These are the genes that are permuted and combined within the species.
Indeed, a sexually reproducing species can be thought of as a device that permutes a discrete set of mutually accustomed genes in different combinations.
Species, according to this view, are continually shuffling collections of genes that meet  each other within the species, but never meet genes of other species.
But there is a sense in which the genes of different species, even if they don't meet at close quarters inside cells, nevertheless constitute an important part of each others' environment.
The relationship is often hostile rather than cooperative, but this can be treated as just a reversal of sign.
This is where we come to the second major theme of this chapter, ‘arms races’.
There are arms races between predators and prey, parasites and hosts, even — though the point is a more subtle one and I shan't discuss it further — between males and females within one species.
Arms races are run in evolutionary time, rather than on the timescale of individual lifetimes.
They consist of the improvement in one lineage's (say prey animals') equipment to survive, as a direct consequence of improvement in another (say predators') lineage's evolving equipment.
There are arms races wherever individuals have enemies with their own capacity for evolutionary improvement.
I regard arms races as of the utmost importance because it is largely arms races that have injected such ‘progressiveness’ as there is in evolution.
For, contrary to earlier prejudices, there is nothing inherently progressive about evolution.
We can see this if we consider what would have happened if the only problems animals had had to face had been those posed by the weather and other aspects of the nonliving environment.
After many generations of cumulative selection in a particular place, the local animals and plants become well fitted to the conditions, for instance the weather conditions, in that place.
If it is cold the animals come to have thick coats of hair, or feathers.
If it is dry they evolve leathery or waxy waterproof skins to conserve what little water there is.
The adaptations to local conditions affect every part of the body, its shape and colour, its internal organs, its behaviour, and the chemistry in its cells.
If the conditions in which a lineage of animals lives remain constant; say it is dry and hot and has been so without a break for 100 generations, evolution in that lineage is likely to come to a halt, at least as far as adaptations to temperature and humidity are concerned.
The animals will become as well fitted as they can be to the local conditions.
This doesn't mean that they couldn't be completely redesigned to be even better.
It does mean that they can't improve themselves by any small land therefore likely! evolutionary step: none of their immediate neighbours in the local equivalent of ‘biomorph space’ would do any better.
Evolution will come to a standstill until something in the  conditions changes: the onset of an ice age, a change in the average rainfall of the area, a shift in the prevailing wind.
Such changes do happen when we are dealing with a timescale as long as the evolutionary one.
As a consequence, evolution normally does not come to a halt, but constantly ‘tracks’ the changing environment.
If there is a steady downward drift in the average temperature in the area, a drift that persists over centuries, successive generations of animals will be propelled by a steady selection ‘pressure’ in the direction, say, of growing longer coats of hair.
If, after a few thousand years of reduced temperature the trend reverses and average temperatures creep up again, the animals will come under the influence of a new selection pressure, and will be pushed towards growing shorter coats again.
But so far we have considered only a limited part of the environment, namely the weather.
The weather is very important to animals and plants.
Its patterns change as the centuries go by, so this keeps evolution constantly in motion as it ‘tracks’ the changes.
But weather patterns change in a haphazard, inconsistent way.
There are other parts of an animal's environment that change in more consistently malevolent directions, and that also need to be ‘tracked’.
These parts of the environment are living things themselves.
For a predator such as a hyena, a part of its environment that is at least as important as the weather is its prey, the changing populations of gnus, zebras and antelopes.
For the antelopes and other grazers that wander the plains in search of grass, the weather may be important, but the lions, hyenas and other carnivores are important too.
Cumulative selection will see to it that animals are well fitted to outrun their predators or outwit their prey, no less than it sees to it that they are well fitted to the prevailing weather conditions.
And, just as long-term fluctuations in the weather are ‘tracked’ by evolution, so long-term changes in the habits or weaponry of predators will be tracked by evolutionary changes in their prey.
And vice versa, of course .
We can use the general term ‘enemies’ of a species, to mean other living things that work to make life difficult.
Lions are enemies of zebras.
It may seem a little callous to reverse the statement to ‘Zebras are enemies of lions’.
The role of the zebra in the relationship seems too innocent and wronged to warrant the pejorative ‘enemy’ But individual zebras do everything in their power to resist being eaten by lions, and from the lions' point of view this is making life harder for them.
If zebras and other grazers all succeeded in their aim, the lions would die of starvation.
So by our definition zebras are enemies of lions.
Parasites such as tapeworms are enemies of their hosts, and hosts are enemies of parasites since they tend to evolve measures to  resist them.
Herbivores are enemies of plants, and plants are enemies of herbivores, to the extent that they manufacture thorns, and poisonous or nasty-tasting chemicals.
Lineages of animals and plants will, in evolutionary time, ‘track’ changes in their enemies no less assiduously than they track changes in average weather conditions.
Evolutionary improvements in cheetah weaponry and tactics are, from the gazelles' point of view, like a steady worsening of the climate, and they are tracked in the same kind of way.
But there is one enormously important difference between the two.
The weather changes over the centuries, but it does not change in a specifically malevolent way.
It is not out to ‘get’ gazelles.
The average cheetah will change over the centuries, just like the mean annual rainfall changes.
But whereas mean annual rainfall will drift up and down, with no particular rhyme or reason, the average cheetah, as the centuries go by, will tend to become better equipped to catch gazelles than his ancestors were.
This is because the succession of cheetahs, unlike the succession of annual weather conditions, is itself subject to cumulative selection.
Cheetahs will tend to become fleeter of foot, keener of eye, sharper of tooth.
However ‘hostile’ the weather and other inanimate conditions may seem to be, they have no necessary tendency to get steadily more hostile.
Living enemies, seen over the evolutionary timescale, have exactly that tendency.
The tendency for carnivores to get progressively ‘better’ would soon run out of steam, as do human arms races (for reasons of economic cost which we shall come to), were it not for the parallel tendency in the prey.
And vice versa.
Gazelles, no less than cheetahs, are subject to cumulative selection, and they too will tend, as the generations go by, to improve their ability to run fast, to react swiftly, to become invisible by blending into the long grass.
They too are capable of evolving in the direction of becoming better enemies, in this case enemies of cheetahs.
From the cheetahs' point of view the mean annual temperature does not get systematically better or worse as the years go by, except in so far as any change for a well-adapted animal is a change for the worse.
But the mean annual gazelle does tend to get systematically worse more difficult to catch because better adapted to evade cheetahs.
Again, the tendency towards progressive improvement in gazelles would slow to a halt, were it not for the parallel tendency to improvement shown by their predators.
One side gets a little better because the other side has.
And vice versa.
The process goes into a vicious spiral, on a timescale of hundreds of thousands of years.
In the world of nations on their shorter timescale, when two enemies each progressively improve their weaponry in response to the  other side's improvements, we speak of an ‘arms race’.
The evolutionary analogy is close enough to justify borrowing the term, and I make no apology to my pompous colleagues who would purge our language of such illuminating images.
I have introduced the idea here in terms of a simple example, gazelles and cheetahs.
This was to get across the important difference between a living enemy, which itself is subject to evolutionary change, and an inanimate non-malevolent condition such as the weather, which is subject to change, but not systematic, evolutionary change.
But the time has come to admit that in my efforts to explain this one valid point I may have misled the reader in other ways.
It is obvious, when you come to think about it, that my picture of an ever-advancing arms race was too simple in at least one respect.
Take running speed.
As it stands so far, the arms-race idea seems to suggest that cheetahs and gazelles should have gone on, generation after generation, getting ever faster until both travelled faster than sound.
This has not happened and it never will.
Before resuming the discussion of arms races, it is my duty to forestall misunderstandings.
The first qualification is this.
I gave an impression of a steady upward climb in the prey-catching abilities of cheetahs, and the predator-avoiding abilities of gazelles.
The reader might have come away with a Victorian idea of the inexorability of progress, each generation better, finer and braver than its parents.
The reality in nature is nothing like that.
The timescale over which significant improvement might be detected is, in any case, likely to be far longer than could be detected by comparing one typical generation with its predecessor.
The ‘improvement’, moreover, is far from continuous.
It is a fitful affair, stagnating or even sometimes going ‘backwards’, rather than moving solidly ‘forwards’ in the direction suggested by the arms-race idea.
Changes in conditions, changes in the inanimate forces I have lumped under the general heading of ‘the weather’, are likely to swamp the slow and erratic trends of the arms race, as far as any observer on the ground could be aware.
There may well be long stretches of time in which no ‘progress’ in the arms race, and perhaps no evolutionary change at all, takes place.
Arms races sometimes culminate in extinction, and then a new arms race may begin back at square one.
Nevertheless, when all this is said, the arms-race idea remains by far the most satisfactory explanation for the existence of the advanced and complex machinery that animals and plants possess.
Progressive ‘improvement’ of the kind suggested by the arms-race image does go on, even if it goes on spasmodically and interruptedly; even if its net rate of progress is too slow to be detected within the lifetime of a man, or even within the timespan of recorded history.
The second qualification is that the relationship that I am calling  ‘enemy’ is more complicated than the simple bilateral relationship suggested by the stories of cheetahs and gazelles.
One complication is that a given species may have two (or more) enemies which are even more severe enemies of each other.
This is the principle behind the commonly expressed half-truth that grass benefits by being grazed (or mown).
Cattle eat grass, and might therefore be thought of as enemies of grass.
But grasses also have other enemies in the plant world, competitive weeds, which, if allowed to grow unchecked, might turn out to be even more severe enemies of grasses than cattle.
Grasses suffer somewhat from being eaten by cattle, but the competitive weeds suffer even more.
Therefore the net effect of cattle on a meadow is that the grasses benefit.
The cattle turn out to be, in this sense, friends of grasses rather than enemies.
Nevertheless, cattle are enemies of grass in that it is still true that an individual grass plant would be better off not being eaten by a cow than being eaten, and any mutant plant that possessed, say, a chemical weapon that protected it against cows, would set more seed (containing genetic instructions for making the chemical weapon) than rival members of its own species that were more palatable to cows.
Even if there is a special sense in which cows are ‘friends’ of grasses, natural selection does not favour individual grass plants that go out of their way to be eaten by cows!
The general conclusion to this paragraph is as follows.
It may be convenient to think of an arms race between two lineages such as cattle and grass, or gazelles and cheetahs, but we should never lose sight of the fact that both participants have other enemies against whom they are simultaneously running other arms races.
I shall not pursue the point here, but it can be developed into one of the explanations for why particular arms races stabilize and do not go on for ever— do not lead to predators pursuing their prey at Mach 2 and so on .
The third ‘qualification’ to the simple arms-race is not so much a qualification as an interesting point in its own right.
In my hypothetical discussion of cheetahs and gazelles I said that cheetahs, unlike the weather, had a tendency as the generations go by to become ‘better hunters’, to become more severe enemies, better equipped to kill gazelles.
But this does not imply that they become more successful at killing gazelles.
The kernel of the arms-race idea is that both sides in the arms race are improving from their own point of view, while simultaneously making life more difficult for the other side in the arms race.
There is no particular reason (or at least none in anything that we have discussed so far) to expect either side in the arms race to become steadily more successful or less successful than the other.
In  fact the arms-race idea, in its purest form, suggests that there should be absolutely zero progress in the success rate on both sides of the arms race, while there is very definite progress in the equipment for success on both sides.
Predators become better equipped for killing, but at the same time prey become better equipped to avoid being killed, so the net result is no change in the rate of successful killings.
The implication is that if, by the medium of a time machine, predators from one era could meet prey from another era, the later, more ‘modern’ animals, whether predators or prey, would run rings round the earlier ones.
This is not an experiment that can ever be done, although some people assume that certain remote and isolated faunas, such as those of Australia and Madagascar, can be treated as if they were ancient, as if a trip to Australia were like a trip backwards in a time machine.
Such people think that native Australian species are usually driven extinct by superior competitors or enemies introduced from the outside world, because the native species are ‘older’, ‘out of date’ models, in the same position vis-d-vis invading species as a Jutland battleship contending with a nuclear submarine.
But the assumption that Australia has a ‘living fossil’ fauna is hard to justify.
Perhaps a good case for it might be made, but it seldom is.
I'm afraid it may be no more than the zoological equivalent of chauvinistic snobbery, analogous to the attitude that sees every Australian as an uncouth swagman with not much under his hat and corks dangling round the brim.
The principle of zero change in success rate, no matter how great the evolutionary progress in equipment, has been given the memorable name of the ‘Red Queen effect’ by the American biologist Leigh van Valen.
In Through the Looking Glass, you will remember, the Red Queen seized Alice by the hand and dragged her, faster and faster, on a frenzied run through the countryside, but no matter how fast they ran they always stayed in the same place.
Alice was understandably puzzled, saying, ‘Well in our country you'd generally get to somewhere else — if you ran very fast for a long time as we've been doing.’
'A slow sort of country!’ said the Queen.
‘Now, here, you see, it takes all the running you can do, to keep in the same place.
If you want to get somewhere else, you must run at least twice as fast as that!’
The Red Queen label is amusing, but it can be misleading if taken (as it sometimes is) to mean something mathematically precise, literally zero relative progress.
Another misleading feature is that in the Alice story the Red Queen's statement is genuinely paradoxical, irreconcilable with common sense in the real physical world.
But van Valen's evolutionary Red Queen effect is not paradoxical at all.
It is  entirely in accordance with common sense, so long as common sense is intelligently applied.
If not paradoxical, however, arms races can give rise to situations that strike the economically minded human as wasteful.
Why, for instance, are trees in forests so tall?
The short answer is that all the other trees are tall, so no one tree can afford not to be.
It would be overshadowed if it did.
This is essentially the truth, but it offends the economically minded human.
It seems so pointless, so wasteful.
When all the trees are the full height of the canopy, all are approximately equally exposed to the sun, and none could afford to be any shorter.
But if only they were all shorter; if only there could be some sort of trade-union agreement to lower the recognized height of the canopy in forests, all the trees would benefit.
They would be competing with each other in the canopy for exactly the same sunlight, but they would all have ‘paid’ much smaller growing costs to get into the canopy.
The total economy of the forest would benefit, and so would every individual tree.
Unfortunately, natural selection doesn't care about total economies, and it has no room for cartels and agreements.
There has been an arms race in which forest trees became larger as the generations went by.
At every stage of the arms race there was no intrinsic benefit in being tall for its own sake.
At every stage of the arms race the only point in being tall was to be relatively taller than neighbouring trees.
As the arms race wore on, the average height of trees in the forest canopy went up.
But the benefit that the trees obtained from being tall did not go up.
It actually deteriorated because of the enhanced costs of growing.
Successive generations of trees got taller and taller, but at the end they might better, in one sense, have stayed where they started.
Here, then, is the connection with Alice and the Red Queen, but you can see that in the case of the trees it is not really paradoxical.
It is generally characteristic of arms races, including human ones, that although all would be better off if none of them escalated, so long as one of them escalates none can afford not to.
Once again, by the way, I should stress that I have told the story too simply.
I do not mean to suggest that in every literal generation trees are taller than their counterparts in the previous generation, nor that the arms race is necessarily still going on.
Another point illustrated by the trees is that arms races do not necessarily have to be between members of different species.
Individual trees are just as likely to be harmfully overshadowed by members of their own species as by members of other species.
Probably more so in fact, for all organisms are more seriously threatened by  competition from their own species than from others.
Members of one's own species are competitors for the same resources, to a much more detailed extent, than members of other species.
There are also arms races within species between male roles and female roles, and between parent roles and offspring roles.
I have discussed these in The Selfish Gene, and will not pursue them further here.
The tree story allows me to introduce an important general distinction between two kinds of arms race, called symmetric and asymmetric arms races.
A symmetric arms race is one between competitors trying to do roughly the same thing as each other.
The arms race between forest trees struggling to reach the light is an example.
The different species of trees are not all making their livings in exactly the same way, but as far as the particular race we are talking about is concerned — the race for the sunlight above the canopy — they are competitors for the same resource.
They are taking part in an arms race in which success on one side is felt by the other side as failure.
And it is a symmetric arms race because the nature of the success and failure on the two sides is the same: attainment of sunlight and being overshadowed, respectively.
The arms race between cheetahs and gazelles, however, is asymmetric.
It is a true arms race in which success on either side is felt as failure by the other side, but the nature of the success and failure on the two sides is very different.
The two sides are ‘trying’ to do very different things.
Cheetahs are trying to eat gazelles.
Gazelles are not trying to eat cheetahs, they are trying to avoid being eaten by cheetahs.
From an evolutionary point of view asymmetric arms races are more interesting, since they are more likely to generate highly complex weapons systems.
We can see why this is by taking examples from human weapons technology.
I could use the USA and the USSR as examples, but there is really no need to mention specific nations.
Weapons manufactured by companies in any of the advanced industrial countries may end up being bought by any of a wide variety of nations.
The existence of a successful offensive weapon, such as the Exocet type of surface skimming missile, tends to ‘invite’ the invention of an effective counter, for instance a radio jamming device to ‘confuse’the control system of the missile.
The counter is more likely than not to be manufactured by an enemy country, but it could be manufactured by the same country, even by the same company!
No company, after all, is better equipped to design a jamming device for a particular missile than the company that made the missile in the first place.
There is nothing inherently improbable about the same company producing both and selling them  to opposite sides in a war.
I am cynical enough to suspect that it probably happens, and it vividly illustrates the point about equipment improving while its net effectiveness stands still (and its costs increase).
From my present point of view the question of whether the manufacturers on opposite sides of a human arms race are enemies of each other or identical with each other is irrelevant, and interestingly so.
What matters is that, regardless of their manufacturers, the devices themselves are enemies of each other in the special sense I have defined in this chapter.
The missile, and its specific jamming device, are enemies of each other in that success in one is synonymous with failure in the other.
Whether their designers are also enemies of each other is irrelevant, although it will probably be easier to assume that they are.
So far I have discussed the example of the missile and its specific antidote without stressing the evolutionary, progressive aspect, which is, after all, the main reason for bringing it into this chapter.
The point here is that not only does the present design of a missile invite, or call forth, a suitable antidote, say a radio jamming device.
The antimissile device, in its turn, invites an improvement in the design of the missile, an improvement that specifically counters the antidote, an anti-antimissile device.
It is almost as though each improvement in the missile stimulates the next improvement in itself, via its effect on the antidote.
Improvement in equipment feeds on itself.
This is a recipe for explosive, runaway evolution.
At the end of some years of this ding-dong invention and counter-invention, the current version of both the missile and its antidote will have attained a very high degree of sophistication.
Yet at the same time — here is the Red Queen effect again — there is no general reason for expecting either side in the arms race to be any more successful at doing its job than it was at the beginning of the arms race.
Indeed if both the missile and its antidote have been improving at the same rate, we can expect that the latest, most advanced and sophisticated versions, and the earliest, most primitive and simplest versions will be exactly as successful as each other, against their contemporary counter-devices.
There has been progress in design, but no progress in accomplishment, specifically because there has been equal progress in design on both sides of the arms race.
Indeed, it is precisely because there has been approximately equal progress on both sides that there has been so much progress in the level of sophistication of design.
If one side, say the antimissile jamming device, pulled too far ahead in the design race, the other side, the missile in this case, would simply  cease to be used and manufactured: it would go ‘extinct’ Far from being paradoxical like Alice's original example, the Red Queen effect in its arms-race context turns out to be fundamental to the very idea of progressive advancement.
I said that asymmetric arms races were more likely to lead to interesting progressive improvements than symmetric ones, and we can now see why this is, using human weapons to illustrate the point.
If one nation has a 2-megaton bomb, the enemy nation will develop a 5-megaton bomb.
This provokes the first nation into developing a 10-megaton bomb, which in turn provokes the second into making a 20-megaton bomb, and so on.
This is a true progressive arms race: each advance on one side provokes the counter-advance on the other, and the result is a steady increase in some attribute as time goes by — in this case, explosive power of bombs.
But there is no detailed, one-to-one correspondence between the designs in such a symmetric arms race, no ‘meshing’ or ‘interlocking’of design details as there is in an asymmetric arms race such as that between missile and missile-jamming device.
The missile-jamming device is designed  specifically to overcome particular detailed features of the missile; the designer of the antidote takes into account minute details of the design of the missile.
Then in designing a counter to the antidote, the designer of the next generation of missiles makes use of his knowledge of the detailed design of the antidote to the previous generation.
This is not true of the bombs of ever-increasing megatonnage.
To be sure, designers on one side may pirate good ideas, may imitate design features, from the other side.
But if so, this is incidental.
It is not a necessary part of the design of a Russian bomb that it should have detailed, one-to-one correspondences with specific details of an American bomb.
In the case of an asymmetric arms race, between a lineage of weapons and the specific antidotes to those weapons, it is the one-to-one correspondences that, over the successive ‘generations’, lead to ever greater sophistication and complexity.
In the living world too, we shall expect to find complex and sophisticated design wherever we are dealing with the end-products of a long, asymmetric arms race in which advances on one side have always been matched, on a one-to-one, point-for-point basis, by equally successful antidotes (as opposed to competitors) on the other.
This is conspicuously true of the arms races between predators and their prey, and, perhaps even more, of arms races between parasites and hosts.
The electronic and acoustic weapons systems of bats, which we discussed in Chapter 2, have all the finely tuned sophistication that we expect from the end-products of a long arms race.
Not surprisingly, we  can trace this same arms race on the other side.
The insects that bats prey upon have a comparable battery of sophisticated electronic and acoustic gear.
Some moths even emit bat-like (ultra-) sounds that seem to put the bats off.
Almost all animals are either in danger of being eaten by other animals or in danger of failing to eat other animals, and an enormous number of detailed facts about animals makes sense only when we remember that they are the end-products of long and bitter arms races.
H. B. Cott, author of the classic book Animal Coloration, put the point well in 1940, in what may be the first use in print of the arms-race analogy in biology:
Before asserting that the deceptive appearance of a grasshopper or butterfly is unnecessarily detailed, we must first ascertain what are the powers of perception and discrimination of the insects' natural enemies.
Not to do so is like asserting that the armour of a battle-cruiser is too heavy, or the range of her guns too great, without inquiring into the nature and effectiveness of the enemy's armament.
The fact is that in the primeval struggle of the jungle, as in the refinements of civilized warfare, we see in progress a great evolutionary armament race — whose results, for defence, are manifested in such devices as speed, alertness, armour, spinescence, burrowing habits, nocturnal habits, poisonous secretions, nauseous taste, and (camouflage and other kinds of protective coloration); and for offence, in such counter-attributes as speed, surprise, ambush, allurement, visual acuity, claws, teeth, stings, poison fangs, and (lures).
Just as greater speed in the pursued has developed in relation to increased speed in the pursuer; or defensive armour in relation to aggressive weapons; so the perfection of concealing devices has evolved in response to increased powers of perception.
Arms races in human technology are easier to study than their biological equivalents because they are so much faster.
We can actually see them going on, from year to year.
In the case of a biological arms race, on the other hand, we can usually see only the end-products.
Very rarely a dead animal or plant fossilizes, and it is then sometimes possible to see progressive stages in an animal arms race a little more directly.
One of the most interesting examples of this concerns the electronic arms race, as shown in the brain sizes of fossil animals.
Brains themselves do not fossilize but skulls do, and the cavity in which the brain was housed-the braincase — if interpreted with care, can give a good indication of brain size.
I said ‘if interpreted with care’, and the qualification is an important one.
Among the many problems is the following.
Big animals tend to have big brains partly just because they are big, but this doesn't necessarily mean that they are, in any interesting sense, ‘cleverer’.
Elephants have bigger brains than humans  but, probably with some justice, we like to think that we are cleverer than elephants and that our brains are ‘really’ bigger if you make allowance for the fact that we are much smaller animals.
Certainly our brains occupy a much larger proportion of our body than elephants' brains do, as is evident from the bulging shape of our skulls.
This is not just species vanity.
Presumably a substantial fraction of any brain is needed to perform routine caretaking operations around the body, and a big body automatically needs a big brain for this.
We must find some way of ‘taking out’ of our calculations that fraction of brain that can be attributed simply to body size, so that we can compare what is left over as the true ‘braininess’of animals.
This is another way of saying that we need some good way of defining exactly what we mean by true braininess.
Different people are at liberty to come up with different methods of doing the calculations, but probably the most authoritative index is the ‘encephalization quotient’ or EQ used by Harry Jerison, a leading American authority on brain history.
The EQ is actually calculated in a somewhat complicated way, taking logarithms of brain weight and body weight, and standardizing against the average figures for a major group such as the mammals as a whole.
Just as the ‘intelligence quotient’ or IQ used (or it may be misused) by human psychologists is standardized against the average for a whole population, the EQ is standardized against, say, the whole of the mammals.
Just as an IQ of 100 means, by definition, an IQ identical to the average for a whole population, so an EQ of 1 means, by definition, an EQ identical to the average for, say, mammals of that size.
The details of the mathematical technique don't matter.
In words, the EQ of a given species such as a rhino or a cat, is a measure of how much bigger (or smaller) the animal's brain is than we should expect it to be, given the animal's body size.
How that expectation is calculated is certainly open to debate and criticism.
The fact that humans have an EQ of 7 and hippos an EQ of 0.3 may not literally mean that humans are 23 times as clever as hippos!
But the EQ as measured is probably telling us something about how much ‘computing power’ an animal has in its head, over and above the irreducible minimum of computing power needed for the routine running of its large or small body.
Measured EQs among modern mammals are very varied.
Rats have an EQ of about 0.8, slightly below the average for all mammals.
Squirrels are somewhat higher, about 1.5.
Perhaps the three-dimensional world of trees demands extra computing power for controlling precision leaps, and even more for thinking about efficient paths through a maze of branches that may or may not connect farther on.
Monkeys are well above average, and apes (especially ourselves) even higher.
Within the monkeys it turns out that some types have higher EQs than others and that, interestingly, there is some connection with how they make their living: insect-eating and fruit-eating monkeys have bigger brains, for their size, than leaf-eating monkeys.
It makes some sense to argue that an animal needs less computing power to find leaves, which are abundant all around, than to find fruit, which may have to be searched for, or to catch insects, which take active steps to get away.
Unfortunately, it is now looking as though the true story is more complicated, and that other variables, such as metabolic rate, may be more important.
In the mammals as a whole, carnivores typically have a slightly higher EQ than the herbivores upon which they prey.
The reader will probably have some ideas about why this might be, but it is hard to test such ideas.
Anyway, whatever the reason, it seems to be a fact.
So much for modern animals.
What Jerison has done is to reconstruct the probable EQs of extinct animals that now exist only as fossils.
He has to estimate brain size by making plaster casts of the insides of braincases.
Quite a lot of guesswork and estimation has to go into this, but the margins of error are not so great as to nullify the whole enterprise.
The methods of taking plaster casts can, after all, be checked for their accuracy, using modern animals.
We make-believe that the dried skull is all that we have from a modern animal, use a plaster cast to estimate how big its brain was from the skull alone, and then check with the real brain to see how accurate our estimate was.
These checks on modern skulls encourage confidence in Jerison's estimates of long-dead brains.
His conclusion is, firstly, that there is a tendency for brains to become bigger as the millions of years go by.
At any given time, the current herbivores tended to have smaller brains than the contemporary carnivores that preyed on them.
But later herbivores tended to have larger brains than earlier herbivores, and later carnivores larger brains than earlier carnivores.
We seem to be seeing, in the fossils, an arms race, or rather a series of restarting arms races, between carnivores and herbivores.
This is a particularly pleasing parallel with human armament races, since the brain is the on-board computer used by both carnivores and herbivores, and electronics is probably the most rapidly advancing element in human weapons technology today.
How do arms races end?
Sometimes they may end with one side going extinct, in which case the other side presumably stops evolving in that particular progressive direction, and indeed it will probably even ‘regress’ for economic reasons soon to be discussed.
In other cases, economic pressures may impose a stable halt to an arms race, stable even though one side in the race is, in a sense, permanently ahead.
Take running speed, for instance.
There must be an ultimate limit to the speed at which a cheetah or a gazelle can run, a limit imposed by the laws of physics.
But neither cheetahs nor gazelles have reached that limit.
Both have pushed up against a lower limit which is, I believe, economic in character.
High-speed technology is not cheap.
It demands long leg bones, powerful muscles, capacious lungs.
These things can be had by any animal that really needs to run fast, but they must be bought.
They are bought at a steeply increasing price.
The price is measured as what economists call ‘opportunity cost’.
The opportunity cost of something is measured as the sum of all the other things that you have to forgo in order to have that something.
The cost of sending a child to a private, fee-paying school is all the things that you can't afford to buy as a result: the new car that you can't afford, the holidays in the sun that you can't afford (if you're so rich that you can afford all these things easily, the opportunity cost, to you, of sending your child to a private school may be next to nothing).
The price, to a cheetah, of growing larger leg muscles is all the other things that the cheetah could have done with the materials and energy used to make the leg muscles, for instance make more milk for cubs.
There is no suggestion, of course, that cheetahs do cost-accounting sums in their heads!
It is all done automatically by ordinary natural selection.
A rival cheetah that doesn't have such big leg muscles may not run quite so fast, but it has resources to spare for making an extra lot of milk and therefore perhaps rearing another cub.
More cubs will be reared by cheetahs whose genes equip them with the optimum compromise between running speed, milk production and all the other calls on their budget.
It isn't obvious what the optimum trade-off is between, say, milk production and running speed.
It will certainly be different for different species, and it may fluctuate within each species.
All that is certain is that trade-offs of this kind will be inevitable.
When both cheetahs and gazelles reach the maximum running speed that they can ‘afford’, in their own internal economies, the arms race between them will come to an end.
Their respective economic stopping points may not leave them exactly equally matched.
Prey animals may end up spending relatively more of their budget on defensive weaponry than predators do on offensive weaponry.
One reason for this is summarized in the Aesopian moral: The rabbit runs faster than the fox, because the rabbit is running for his life, while the fox is only running for his dinner.
In economic terms, this means that individual foxes that shift resources into other projects can do better than individual foxes that spend  virtually all their resources on hunting technology.
In the rabbit population, on the other hand, the balance of economic advantage is shifted towards those individual rabbits that are big spenders on equipment for running fast.
The upshot of these economically balanced budgets within species is that arms races between species tend to come to a mutually stable end, with one side ahead.
We are unlikely to witness arms races in dynamic progress, because they are unlikely to be running at any particular ‘moment’ of geological time, such as our time.
But the animals that are to be seen in our time can be interpreted as the end-products of an arms race that was run in the past.
To summarize the message of this chapter, genes are selected, not for their intrinsic qualities, but by virtue of their interactions with their environments.
An especially important component of a gene's environment is other genes.
The general reason why this is such an important component is that other genes also change, as generations go by in evolution.
This has two main kinds of consequences.
First, it has meant that those genes are favoured that have the property of ‘cooperating’ with those other genes that they are likely to meet in circumstances that favour cooperation.
This is especially, though not exclusively, true of genes within the same species, because genes within one species frequently share cells with one another.
It has led to the evolution of large gangs of cooperating genes, and ultimately to the evolution of bodies themselves, as the products of their cooperative enterprise.
An individual body is a large vehicle or ‘survival machine’ built by a gene cooperative, for the preservation of copies of each member of that cooperative.
They cooperate because they all stand to gain from the same outcome — the survival and reproduction of the communal body — and because they constitute an important part of the environment in which natural selection works on each other.
Second, circumstances don't always favour cooperation.
In their march down geological time, genes also encounter one another in circumstances that favour antagonism.
This is especially, though not exclusively, true of genes in different species.
The point about different species is that their genes don't mix — because members of different species can't mate with one another.
When selected genes in one species provide the environment in which genes in another species are selected, the result is often an evolutionary arms race.
Each new genetic improvement selected on one side of the arms race — say predators — changes the environment for selection of genes on the other side of the arms race — prey.
It is arms races of this kind that have  been mainly responsible for the apparently progressive quality of evolution, for the evolution of ever-improved running speed, flying skill, acuity of eyesight, keenness of hearing, and so on.
These arms races don't go on forever, but stabilize when, for instance, further improvements become too economically costly to the individual animals concerned.
This has been a difficult chapter, but it had to go into the book.
Without it, we would have been left with the feeling that natural selection is only a destructive process, or at best a process of weeding-out.
We have seen two ways in which natural selection can be a constructive force.
One way concerns cooperative relationships between genes within species.
Our fundamental assumption must be that genes are ‘selfish’ entities, working for their own propagation in the gene pool of the species.
But because the environment of a gene consists, to such a salient degree, of other genes also being selected in the same gene pool, genes will be favoured if they are good at cooperating with other genes in the same gene pool.
This is why large bodies of cells, working coherently towards the same cooperative ends, have evolved.
This is why bodies exist, rather than separate replicators still battling it out in the primordial soup.
Bodies evolve integrated and coherent purposefulness because genes are selected in the environment provided by other genes within the same species.
But because genes are also selected in the environment provided by other genes in different species, arms races develop.
And arms races constitute the other great force propelling evolution in directions that we recognize as ‘progressive’, complex ‘design’.
Arms races have an inherently unstable ‘runaway’ feel to them.
They career off into the future in a way that is, in one sense, pointless and futile, in another sense progressive and endlessly fascinating to us, the observers.
The next chapter takes up a particular, rather special case of explosive, runaway evolution, the case that Darwin called sexual selection.