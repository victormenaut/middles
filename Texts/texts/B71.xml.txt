

Researching the researchers
EXPERIENCED practitioners of the arcane ‘science’ of ‘research evaluation’revealed this week yet another shortcoming in Britain's machinery of science policy.
The Science Policy Research Unit at Sussex University says that a substantial part of the science-policy machine — the part that operates under the auspices of the Department of Education and Science — relies on peer review and ‘informed prejudice’.
Not for our policy makers the quantitative techniques — such as citation indexes and patent records — that have made inroads into the US's science policy system.
Should British researchers hold up their hands in horror at this further shameful symptom of the failure of the United Kingdom to innovate?
Or should they be grateful that Britain's science machine is too poor and backward to buy a computer to take its decisions for it?
Certainly Britain lags other counties in its use of quantitative method of assessing research output.
The US's National Science Foundation now employs such methods routinely to quantify various ‘indicators’ of activity and productivity in different areas of science and technology and different types of research institute.
Using data from the Science Citation Index, produced by the Institute for Scientific Information in Philadelphia, the NSF produces annual reports that have become indispensable to the US Congress in its deliberations on government policy.
It was the NSF's statistics that led President Carter's administration to feed more funds to basic science, though it is doubtful whether the numbers help in decision making at a more detailed level — whether to increase or cut a laboratory's budget, for example.
Quantitative information on the British scene is hard to come by, especially if you are looking for detail on a national, ‘transdisciplinary’ or ‘transministry’basis.
So Britain is poorly equipped to even consider making any comparisons of the productivity or usefulness of research.
There are some attempts under way to rectify this.
The British Association for the Advancement of Science and the Technical Change Centre are trying to produce a national ‘science audit’.
And in its reply to the report on science and government by the House of Lords Select Committee on Science and Technology, the government promised to publish a regular review.
Such information is essential if there is to be any meaningful debate on science policy in Britain, while we should also welcome the natural reluctance to use  statistics as the sole basis for planning, this should be no excuse for not producing the information.
The problem of research planning is neatly highlighted in other reports, recently produced by SPRU, on the effectiveness of large laboratories, such as the Royal Greenwich Observatory and Daresbury's electron synchrotron, NINA.
SPRU's results, based on a historical use of scientific indicators, does not show how such indicators could help Britain to avoid such problems in the future.
what these reports have shown is that speed is of the essence and such projects should be chosen and approved far more quickly, in time for the researchers to be able to compete with rival projects.
It would be interesting to hear the views of the planners on how this shift of pace might be brought about.
We should welcome the initiative of the Advisory Board for the Research Council in looking at quantitative measures of research output.
But, like observers of cricket, we must not become so engrossed in our analysis of the statistics of the game that we lose sight of the fundamental issues of strategy.
Concrete foundations
MOST GOVERNMENTS now accept that technology is the key to a prosperous economy.
For rich industrial nations, the problem of microelectronics and other new technologies boils down to managing the transition while many workers either change their employment, or join the ranks of the permanently unemployed.
For the developing world, the impact of new technology is less obvious.
Mostly, it has negative effects, enabling the richer nations to race away with an increasing share of the world's economic wealth, often ‘earned’ by exploiting the resources of the poor nations (see p. 226).
while the rich countries get richer, the poor ones stay firmly saddled with the customs and practices of the 19th century.
These were among the topics on the programme this week in London at yet another ‘megathrash’ on the social consequences of new technology.
Conferences used to be a British strength, but this one was sponsored by Honda — the company that made it socially acceptable to ride a motorcycle without donning ‘leathers’.
Electronics usually grab the attention when talk turns to new technology.
It was heartening, therefore, to hear Gunnar Hambraeus, chairman of the Royal Swedish Academy of Engineering Sciences, consider the social consequences of concrete.
This material, ready mixed, possibly on the instructions of a computer, and hardened with ingenious new chemicals, daily changes millions of lives.
The implications of the new technological revolution go far beyond the silicon chip and its offspring, a message that may have penetrated as far as Kenneth Baker, Britain's Minister for Information Technology.
He wants to extend the government's support for research into areas beyond disciplines related to computers.
Baker has pinpointed surface chemistry, sensors and materials as areas ripe for government support (This week, p. 203).
It will be some time before these initiatives lead to new products and jobs.
Before that happens Britain will be in no position to do much more than talk about the implications of new technology.
But at least talking, as at this week's conference, keeps the topic in the public domain.
THIS WEEK
AIDS hunt homes in on Haitian pigs
ACQUIRED immuno-deficiency syndrome, otherwise known as AIDS, the disease that is mystifying doctors and spreading rapidly among American gays, may be caused by a new strain of African swine fever virus that is infecting pigs in Haiti.
Dr Jane Teas, of the Harvard School of Public Health in Boston, Massachusetts, believes that AIDS could have originated with just one Haitian homosexual eating infected pork.
He could have passed the disease on to American gays touring Haiti.
AIDS was first recognised in 1978.
It suppresses the immune system so that infected people are prone to almost any infection that happens to come their way.
In addition.
AIDS victims often develop a rare type of cancer, known as Kaposi's sarcoma.
The severity and virulence of the disease is causing great concern in the US.
The American watchdog for communicable diseases.
the  Centers for Disease Control in Atlanta, Georgia, had received 1339 notifications of the disease up to mid-April, of which 505 had died.
They have set up an AIDS task force, which has been searching for the bug that causes the disease; they suspect a virus.
In a letter to The Lancet Teas suggests that the AIDS virus may be a modified strain of the virus that causes African swine fever (vol i. p 923).
Teas told New Scientist : ‘I believe the case is even stronger than my paper suggests.’
There are many similarities between AIDS and African swine fever (ASFV).
The first cases of AIDS were reported in 1978.
They were closely followed by the confirmation of ASFV in Haitian pigs in 1979.
Both cause fever and loss of appetite.
ASFV also modifies the immune system in a similar way to AIDS.
AIDS patients have swollen lymph nodes (these are the sites of antibody production and of the removal of invading organisms).
In pigs with ASFV, cells of the lymphatic system multiply abnormally rapidly.
This also resembles what happens in AIDS.
Teas suggests that infection of human immune cells with a variant of ASFV could explain the immuno-deficiency seen in AIDS patients.
ASEV can be spread through contact with faeces, blood and urine.
The virus was first discovered to infect two species of tick and three species of wild swine in Africa.
By 1909 it had spread to domestic African pigs.
It has since migrated to Europe, Latin America and the Caribbean, leaving a trail of slightly different strains in its wake.
The only way to combat the disease is to kill all the pigs, as was done in Cuba when the disease was confirmed in 1976.
The American government has recently announced that it is to spend some $18 million to wipe out the pig population of Haiti in an effort to control the virus.
Teas believes that AIDS could have started with a Haitian eating infected pork.
The person would have been a homosexual with a stomach ulcer or  abscess through which the virus could have penetrated his circulation.
he could then have transmitted the disease through sexual contact with American gays holidaying in Haiti.
Since the virus can spread through blood, transfusions could account for the 25 per cent of AIDS victims who are not gay.
Teas is planning to test her suspicions by infecting pig blood with blood from AIDS patients.
and comparing it with pig blood infected with ASFV and looking for similarities.
Hominoid collarbone exposed as dolphin's rib
Ian Anderson, San Francisco
A FIVE million year-old piece of bone that was thought to be the collarbone of a humanlike creature is actually part of a dolphin rib, according to an anthropologist at the University of California-Berkeley.
Dr Tim White says the discovery of the blunder may force a rethink of theories about when the line of man's ancestors separated from that of the apes.
He puts the incident on a par with two other  embarrassing faux pas by fossil hunters:Hesperopithecus , the fossil pig's tooth that was cited as evidence of very early man in North America.
and Eoanthropus or ‘Piltdown Man,’ the jaw of an orangutan and the skull of a modern human that were claimed to be the ‘earliest Englishman’.
‘Seldom has a bone been hyped as much as this one.’
White told New Scientist .
Dr. Noel Boaz from New York University who made the original classification of the bone, is counter-attacking.
‘I have not gone any further than the evidence allowed,’ Boaz said.
The fragment was must likely the left clavicle or collarbone of a hominoid, he said. Boaz says that he has done no more than suggest that the 75–5 mm bone, found in March 1979 at Sahabi in Libya, was from the ‘superfamily’Hominoidea 
Last weekend White publicly criticised Boaz's work at a conference on locomotion in Berkeley, California, organised by the Institute of Human Origins.
White claims that weighty issues are involved.
If it is really from a hominid, then the date when man's ancestors first stood up would recede by one to two million years and place the divergence between man-s ancestors and great apes beyond five million years ago.
White is especially critical of an article and illustration by Boaz and Dr Douglas L. Cramer of New York University in the August 1982 issue of Natural History a monthly magazine published by the American Museum of Natural History.
‘The problem with a lot of anthropologists is that they want so much to find a hominid that any scrap of bone becomes a hominid bone.’
White claims the hone is from something like the modern Pacific white-sided dolphin.
Its size and shape shows it is ‘one of the posterior ribs of a small cetacean similar to species of the modern dolphin genera Tursiops or Lagenorhynchus .’
Boaz and White are at loggerheads over another palaeontological minefield — the age of ‘Lucy’, the half-complete-skeleton found by Johanson in Ethiopia.
Boaz has claimed it is ½ million years younger than stated by White.
But White says that row has nothing to do with his vehement stance on the Sahabi bone issue.
Sparks flew at last weekend's Berkeley conference when Boaz turned up unexpectedly.
The dispute boiled down to two main anatomical points.
White says that to be a clavicle, the specimen should have an S- or sigmoid curie, but it does not.
Also, a tiny opening in the bone, called the  neutrient foramen, should open laterally, but it opens in the opposite direction.
Seveso's designer claims modification caused explosion
THE FACTORY explosion at Seveso, northern Italy, which  polluted the town with dioxin in 1976, was caused by negligence, according to the engineer who designed the chemical plant.
Fritz Moeri, one of five people charged in the trial of company officials which began on 18 April, has told New Scientist that the disaster would have been impossible if the system he built in 1970 had not been modified and if ‘fashionable’ exhaust valves had not been added at the insistence of company management.
The accident happened when workers at the ICMESA plant neglected to add water to stop a reaction producing trichlorophenol (TCP) as they went off shift early one Saturday morning.
The reactor's contents spontaneously over-heated, expanded, blew valves on vents and emptied the contents of the cauldron into the air above the plant.
The heat also produced up to a kilogram of lethal dioxin, some of which still contaminates the surrounding area.
Moeri, who left the firm in 1971, stands accused of having designed equipment which permitted this.
However, Moeri says that, with his system, the explosion should have been impossible.
Last week,New Scientist showed Moeri a picture taken of the equipment at the time of the accident.
He said he hardly recognised it.
By reconstructing the changes made to the Seveso plant after Moeri's departure, it has been possible to recreate what probably caused the blast.
Givaudan, ICMESA's Swiss owner, had patented a process in which tetrachlorobenzene would react with soda to produce TCP at low temperatures.
The reaction is heat-producing.
If it is allowed to get too hot it also produces dioxin, an unacceptable contaminant in the TCP used for manufacture of hexachlorophene.
In Moeri's design for the plant, heat was continuously removed from the reaction by evaporation of xylene.
When it was complete, a second solvent, ethylene glycol, was evaporated by applying a vacuum to the reactor vessel, again keeping the pressure and temperature of its contents low.
Those contents were then tipped into a second vessel containing cold water, which both stopped and cooled the reaction.
However, according to scientific consultants now at the Seveso special office, Herwig Von Zwehl, Moeri's successor and co-defendant, instead removed the ethylene glycol by adding water and letting the solvents separate.
Besides letting the mix stay hot longer, the job of adding water to a 170o pot until it stopped steaming would take several hours, says Moeri.
This is the operation that plant workers decided they could do without on the morning of the accident.
Jorg Sambetn, a Givaudan engineer who investigated the accident after it happened, says it should only have taken 15 minutes, and cannot understand why the workers omitted it.
Moeri's original procedure, of adding the chemicals to the second vat, would however have taken only a few minutes and would have left the reaction in no condition to explode.
He claims that after his departure no one seems to have made a serious effort to keep workers informed on the dangers of the process, and especially of the need to cool it.
Company officials have emphasised that.
even without adding water, the reaction should have cooled by itself.
They say that the explosion was due to hitherto unknown chemical or physical events for which it cannot be held responsible for failing to predict.
Moeri told New Scientist , however.
that a simple explanation for the explosion probably lies in the condenser, which cooled evaporating xylene as it left the reaction and was specially designed to accommodate up to 50 kg of deposited tetrachlorobenzene, without blocking.
when the reactor was left hot on that July morning, heat would have crept up the metal to the condenser and melted the crystals, letting tetrachlorobenzene flow back into the reactor and lie in a concentrated layer on top of its contents.
The reaction would start anew, but this time with no way to remove its heat.
Over the next six hours, heat built up until eventually the reaction ran out of control.
Studies have shown that in fact the over-heating did take place in the top of the pot.
Moeri has never published this hypothesis, however.
He says that it needs to be tested.
All might still have been well were it not for the rupture-discs on the vents above the reactor.
Moeri considered these discs unnecessary for his low-pressure reaction, but he says Givaudan insisted on them ‘because they were fashionable on all their equipment at the time’.
Rupture-discs, however, simply open and let all the pressure escape.
If proper security valves, which close after initial pressure is dissipated, had been used the pollution would have been much less, says Moeri.
Moeri's account of the accident could have a profound effect on the defence the company makes at the trial, which resumes on 11 May.
His simple explanation of the accident appears to undermine the company's case that ‘unknown processes’ were involved.
And it seems undeniable that ICMESA was in possession of a plant which, had it not been modified, would not have exploded.
Moeri has no intention of attending the trial, since another defendant, Paolo Paoletti the production supervisor, was killed by terrorists in 1980.
He intends to sit the trial out at his home in Switzerland.
Fastest pulsar flashes for Australia
ASTRONOMERS in Australia have detected flashes of light from the fastest-rotating star, the ‘millisecond pulsar’ discovered by radio astronomers last November (New Scientist , vol 96, p 362).
This pulsar is a compact neutron star, which rotates 642 times every second, and the detection makes it only the third pulsar known to emit light.
One of the radio astronomers keeping track of the pulsar's radio waves is Dick Manchester, who has used the 64-metre radio telescope at Parkes in Australia.
Manchester, with colleagues Bruce Peterson and Pat Wallis, has now switched to the nearby Anglo-Australian optical telescope to look for pulses of light from this region of the sky.
By searching for flashes repeating at exactly the same rate as the radio pulses, they were able to investigate much fainter pulsations than if they had been looking for pulses occurring at an unknown rate.
In the hour before dawn, when the pulsar had risen above the horizon and the sky was still dark, they picked up pulsations at the correct rate — and, as confirmation, found them again the following morning.
The flashes of light average out to a magnitude of 24.5, making the pulsar one of the faintest objects ever detected, about one hundred-millionth as faint as the dimmest stars visible to the unaided eye.
One of the pulsar's original discoverers, Stanislav Djorjovski, reported finding a brighter star coinciding with the pulsar last November.
He did not, however, detect any pulses.
The newly-discovered flashes could be weak fluctuations in this object.
But Manchester's team has remeasured the position of this star and find it does not coincide with the radio position after all.
The pulsar is probably a different object.
Andrew Lyne, who has been studying the millisecond pulsar from Jodrell Bank, told New Scientist ‘What will be interesting is to compare the arrival times of the optical pulses with the radio’.
The measurements require an accuracy in timing which is good to ten millionths of a second.
Nuclear watchdog goes on trial
BRITAIN'S Nuclear Installations Inspectorate, which is  responsible for ensuring the safety of nuclear power, this week puts its credibility on the line at the public inquiry into the Sizewell B reactor.
The inquiry, now ending 14 weeks in session, is examining the Central Electricity Generating Board's plan to build a pressurised water reactor (PWR) at Sizewell in Suffolk.
The pressure group, Friends of the Earth, wants the safety part of the hearing to be adjourned until the inspectorate is prepared to issue a licence for the PWR.
But the inspectorate says it cannot possibly reach that stage until well into next year, long after the end of the inquiry.
The inquiry inspector, Sir Frank Layfield, must decide if Friends of the Earth's challenge is valid.
The group based its objection on the assurance that had been given to parliament that the hearing would not be held until the inspectorate was satisfied with the PWR's safety.
The chief inspector of nuclear installations, Ron Anthony, now admits that he expected the inquiry to open later in the licensing procedure — which is independent of the inquiry's deliberations.
He also says that the Central Electricity Generating Board has not satisfied the inspectorate on a number of issues.
In recent weeks there has been a war of claim and counter-claim between the board and the inspectorate about how far some of the key issues, such as fuel clad ballooning, the integrity of the steam generator and the reactor's protection, have been resolved.
Anthony admits, that the inspectorate's judgement is on trial.
France steps up search for cheap breeder
Andrew Lloyd, Paris
THE FRENCH authorities expect to announce details of the design of the next generation of fast-breeder reactors at a nuclear safety conference next week.
The plans will show a more powerful and less costly product than existing prototypes.
The aim is to bring the generating costs of breeders closer to those of the traditional light-water reactor.
To achieve savings of around 30 per cent, the French will relax certain safety constraints observed in Super-Phenix, the 1200 megawatt breeder due to come into operation next year.
Plans include the elimination of the dome over the reactor.
Other savings will come from increasing the power of the reactor from 1200 to 1500 MW, the use of a smaller diameter reactor vessel, a rectangular rather than cylindrical reactor building, a thinner concrete structure, fewer pipes and the possible scrapping of the system to remove very hot items of equipment from the reactor vessel.
No official go-ahead from the national safety authorities has yet been given.
But French nuclear experts are convinced that the ‘reference accident’(a core melt-down in which the molten fuel reacts with the sodium coolant) is so improbable that the secondary containment dome can be done away with.
In light-water reactors (including PWRs) the concrete shell of the reactor building needs to withstand a pressure of five bars, whereas a fast breeder has to cope with a mere 40 millibars, according to an Electricite de France engineer.
One advantage of a rectangular reactor building will be a much simpler overhead crane.
The revolving crane in Super-Phenix is still not working properly.
A thinner blanket of uranium oxide is one technique being considered to reduce the reactor vessel's diameter.
It would, however, make the reactor breed plutonium less efficiently.
Further savings could come if countries that are developing breeder technology worked together on the next generation of reactors.
At least this is what the head of the Commissariat a L'Energie Atomique, Michel Pecqueur said last week.
Another reason for renewed French interest in co-operation might be the need for reassurance, argues one non-French source in Paris.
‘Few people realise just how far ahead the French have gone’, he says.
Their isolation has engendered self doubt in some quarters.
Current predictions for the cost of generating electricity with the Super-Phenix put it at about the same as coal but twice as expensive as a conventional reactor.
The new plans, which have yet to receive official blessing, would drop this to 1–6 times.
Probreeder sources in France say that the premium is worth paying because of the energy independence given by a breeder, which ‘breeds’ its own plutonium fuel.
But reduced forecasts of energy needs.
coupled with French spending cuts mean that  government funds for future breeders will be that much more difficult to win.
Alvey follow-ups planned
MORE BIG collaborative projects involving the government, academic researchers and industry are in the pipeline following the go-ahead given by British ministers this week to a £350 million programme in information technology.
The five-year project, mooted by a committee of electronics specialists chaired by John Alvey of British Telecom, will draw together the country's top researchers.
Kenneth Baker, the minister for information technology, said this week the government will contribute £210-£250 million of the project's cost.
Civil servants are already examining what could follow the Alvey proposals.
Baker is keen on more collaborative projects in key technologies.
He has earmarked surface chemistry.
sensor technology and materials research as candidates for the joint approach.
Like the project on information technology, this work could be administered by small directorates within the Department of Industry.
Baker also plans schemes that help industry to exploit technologies developed by universities.
He visits a university a month, talking to academics, and is impressed by the ‘tremendous potential of the work.
‘Britain has not been very good at getting the ideas out of universities.
I want to do what I can to open the gateways,’ he told New Scientist .
Depo's side-effects repealed
UPJOHN, the maker of the controversial injectable contraceptive, Depo-Provera faces much more than the unspecified objections of health minister, Kenneth Clarke, if it is to reverse Clarke's decision to ban the use of the drug as a long-term contraceptive in Britain.
A pressure group, which calls itself the Coordinating Committee on Depo-Provera, has submitted 400 pages of evidence to the appeal panel, which began hearings this week.
The CCDP argues that the known side-effects of the drug, medroxyprogesterone, have been played down by Upjohn and the medical experts it has called.
And it says that prospective Depo patients need more detailed information of the drug's method of action and side-effects if they are to give truly informed consent.
The committee bases its views on the experience of Depo's use in conditions for which it already has a licence.
It cites cases in which the use of the drug to prevent medically-undesirable conception after childbirth has resulted in more or less continuous bleeding.
This may only cease on withdrawal of the drug.
Such bleeding has also occurred in women given Depo after vaccination against German measles.
Other evidence shows that, as the level of Depo in the body falls, conception may occur, making the fetus susceptible to the drug's effects.
Experiments on animals have shown that Depo causes feminisation of male fetuses and  masculinisation of female fetuses in the same litter.
The drug also finds it way to the mother's milk and so could exert its effects on newborn children.
Since the company made its appeal — and contrary to their current data sheet for the drug — Upjohn has recommended delaying Depo injections until the sixth week after childbirth.
The coordinating committee suggests that the appeal panel ask why this change has been made, and when Upjohn proposes to inform doctors of the change.
Metro fleur-de-Lille
THE £230 million Lille metro was opened last week by France's President Mitterrand.
The metro is the first full-scale railway system to have driverless trains and it can run up to 60 two-car trains an hour during peak periods (New Scientist .
3 June 1982, p 644).
The trains run on special rubber tyres, which have solid insides to prevent accidents caused by a train getting a fiat tyre.
The first 13 kilometre line of the metro has opened on time.
Lille eventually hopes to have a 70 kilometre network.
American court upholds ban on nuclear plants
AMERICA'S highest court jolted the nuclear power industry last week by allowing individual states to ban new reactors until a way is found to dispose permanently of highly radioactive spent fuel.
No permanent repository for spent nuclear fuel will be available until the late 1990s.
So the ruling by the Supreme Court in Washington could be devastating for the industry.
The decision affects only new plants, however — not the 80 or so stations now working or the 57 under construction.
And, since no one has ordered a new nuclear plant since 1978 and none are in the offing, the decision is academic, say industry spokesmen.
Not so, according to elated opponents of nuclear power.
The courts now hold that any of America's 50 states can snub new nuclear plants for economic or technological reasons unrelated to the safety or cost of the reactor itself.
‘Even though the plant may be okay,’ explains David Berick of the Environmental Policy Center in Washington, ‘it's [viability]is intimately linked to other technology.’
Local power authorities have only recently considered what waste disposal will cost ratepayers, when they compare nuclear energy to coal or hydroelectric alternatives.
The court's decision came in a case brought by two utilities, that provide power in California.
against the state's six-year-old ban on new plants.
The Reagan administration had cast its lot with the power companies.
But the Supreme Court decided that only questions of safety or public exposure to radiation are within the federal government's domain.
No country has yet settled on how to rid itself forever of the thousands of tonnes of highly radioactive nuclear waste that is piling up at power plants.
In the US, plants are storing spent fuel assemblies in pools near their reactors.
But the federal Department of Energy has helped to accelerate research on waste disposal by arguing that these pools are becoming dangerously crowded — an argument the plants' owners echo.
A law was passed last year to establish a timetable for building a national dump that could store radioactive waste safely for thousands of years.
The law says the president must nominate a site, from several now being considered, by 1987.
The country must build a working repository by about 1998.
Candidates include underground salt domes, basalt or granite formations and deposits of tuff (volcanic ash).
Finding ways of soothing political opposition to the dumping of highly radioactive waste could prove more difficult than solving the scientific problems.
Governors of two of the states say they won't take other people's nuclear garbage.
California is not alone in its pessimism over the future of waste disposal.
Five other states have passed similar moratoria, citing the economic uncertainty of nuclear power's future.
The federal government fears that other states are likely to follow suit.
A spokesman for an industry lobby group, the Atomic Industrial Forum, assured New Scientist that, as the economy picks up and demand for electricity grows, states will reconsider their gloomy assessment of the atom.
No matter which way the economy goes, however, the Supreme Court has made it clear that nuclear power can no longer depend on Washington's paternal hand when its competes with other forms of energy for the public's blessing.
Britain to appoint new head for satellite mapping
Peter Marsh
THE DEPARTMENT of Industry (DoI) is about to appoint a new recruit to knit together Britain's activities in satellite mapping.
The department's manager for remote sensing will try to persuade different parts of the government to spend more in this area.
The extra cash would fund research related to the new generation of satellites planned for later in the 1980s.
The manager will coordinate the efforts of the DoI, the Natural Environment Research Council and the Ministry of Defence.
Also involved are the Ministry of Agriculture, Fisheries and Food and the Department of the Environment.
All these bodies use data about the Earth's surface or the oceans.
The information is collected by remote-sensing craft such as the American Landsat vehicles.
The  government already pays for a unit at the Royal Aircraft Establishment, in Farnborough, that uses powerful computers to  analyse signals from the satellites.
The information is helpful in, For example, following the growth of crops or urban sprawl.
But a new generation of satellites will carry particularly sensitive instruments giving more detailed information.
France, the US and Japan plan such craft.
And the European Space Agency wants to launch a craft called ERS. 1 that will scan the Earth's oceans with radar instruments from 1987.
Britain will contribute about a sixth of the £250 million cost.
The new craft, however, causes problems for researchers.
They need a different set of computer techniques to make sense of the more detailed data that will be produced.
So far, government departments have hardly been speedy in coming forward with extra cash.
A working party of remote sensing specialists last year proposed a five year programme of work costing about £2 million a year.
The Natural Environment Research Council has already provided more funds for its own remote-sensing work.
This year it will spend about £1 — 5 million on research related to satellite mapping.
Sir Hemann Bondi, the council's chairman, says that it is in Britain's interest to spend more on what he calls the ‘ground segment’ of remote sensing.
Ultimately, in the view of the Department of Industry, this could stimulate new businesses that sell data from satellites.
Energy' Cinderella should step out of the grate
MPS HAVE called on the government to force electricity boards to use the heat that goes up the chimneys of Britain's power stations.
A House of Commons select committee, which has been looking at the feasibility of using ‘combined heat and power’(CHP), last week reported that the idea is about to come into its own in Britain.
The report is the strongest boost yet for CHP which, until now, has been the Cinderella of the energy industry.
The MP's report draws heavily on an earlier study by consultants, W. S. Atkins, which said that CHP and district heating schemes would be viable in no fewer than nine cities in Britain.
Barking, east London, is a leading candidate for a scheme.
The main proposal is that the government should support a move that the Energy Bill should give electricity boards responsibility for providing heat.
The main problem is money.
The MPs hope that talks now going on in the City between consultants and finance houses might help to raise cash — but at least one MP, Labour's energy guru, Arthur Palmer, is not optimistic.
The committee also recommended that the government clarify the rules covering the supply of heat and energy so that anyone planning a CHP scheme would not need to carry out long negotiations with the electricity industry.
Shuttle fleet is too small to do the job
Christopher Joyce
DESPITE the sparkling performance by the first two of America's four planned space shuttles, the fleet's goal of 30 flights a year by 1990 is ‘impossible or highly improbable,’ according to a study by leading aerospace experts.
A panel, assembled by the National Academy of Sciences, predicted last week that NASA won't be able to meet its ambitious schedule with its present fleet of four orbiters.
The news is not necessarily unwelcome at the space agency: NASA's administrator.
James Beggs, has been lobbying hard for money to build a fifth Shuttle-orbiter, and the academy's assessment could help loosen the purse strings at the white House.
The best NASA can hope for with four orbiters is 17 to 25 flights per year by 1990, says the panel.
A fifth orbiter would allow 35 flights.
NASA's programme for a steady now of shuttle spare pans has also ‘lagged seriously’, the academy's report concludes.
The two rocket boosters attached to the shuttle's belly drop back to sea and are reused, but too few will be built to support the NASA manifest.
There will be 19 main engines for the shuttle — each orbiter carries three, the rest are spares.
The panel thinks this level of back-up is ‘marginal’ for even a programme of 24 nights a year.
The report, entitled Assessment of Constraints on Space Shuttle Launch Rates , also notes that additions to the maintenance system and equipment will be needed to bring down ‘turnaround’ time — the period from landing to readiness for the next flight.
Should an accident seriously damage one of the craft it could take years to gear up specialised production lines to build the larger replacement parts.
The most recent launch, the first flight of the second orbiter, Challenger, was delayed from January to April because of persistent fuel and coolant leaks from the main engines.
The delay and uncertainty over the fate of Challenger's principal cargo, the tracking and data relay satellite, which is now aloft but in the wrong orbit, means that Europe's Spacelab may have to operate at half-speed when it is launched in September.
That possibility has raised blood pressures at the European Space Agency, but the lowered expectations for the shuttle fleet can't help but brighten prospects for more business for Ariane, Europe's own satellite launching service
Why Britain does not get value for money in research
MODERN methods of assessing the value of scientific research might just as well not exist as far as British policy makers are concerned.
So say three researchers at Sussex University's Science Policy Research Unit.
Their report, based on interviews with some of Britain's most influential planners at the Department of Education and Science, the University Grants Committee and the research councils, is published this week.
It calls for the tradition of assessing research by peer review to be augmented with techniques such as counting the number of papers produced and how often those papers are cited by other scientists.
But changes could be on the way.
Sir David Phillips, chairman on the Advisory Board for the Research Councils, this week told New Scientist that he has ‘detected a reluctance on the part of the research councils to get involved with such methods.
The French and the US National Science Foundation have been doing this for some time.’
He is keen that Britain should get involved too, and would like to see whether the council could help.
John Irvine, Ben Martin and Geoff Oldham at SPRU have found that most decisions about research grants are still based on the time-honoured ‘peer review’ system: a researcher's proposals are sent to a handful of experts engaged in similar research.
If their assessment of the scientist's standing and contribution to his discipline is favourable, the project goes ahead.
The system applies to individual research projects, expensive central facilities and university departments alike.
Peer review works fairly well, the report points out, when science budgets are growing and there is room for most disciplines to develop at their own pace.
But when funding stays level and subjects begin to compete for resources, the system breaks down.
It cannot, for example, produce a list of research priorities.
And, as one of the interviewees said, ‘peer review is an essentially conservative process, which tends to encourage the reproduction of establishment views’.
The quest for sharper and more quantitative ways of making decisions on research is widespread.
The French government asked SPRU to look at what role such methods play in the British system.
The answer seems to be that they are barely used at all.
The team says this is not so because administrators dislike them, or that they are hardly aware of them.
Irvine and Martin have themselves developed a method of assessing the performance of some of Britain's most expensive scientific investments.
They have found that the output of researchers using Manchester University's radio telescopes at Jodrell Bank compares unfavourably with that of those working at the Mullard Radioastronomy Laboratory at Cambridge University.
They reported last week that the NINA electron accelerator at Daresbury in Cheshire was built too slowly and too late to produce research comparable with that of similar machines elsewhere (New  Scientist  21 April, p 133).
Philips lends support to these conclusions.
But, he says, the studies have the advantage of hindsight.
He would like indicators that can help planners to avoid problems in the early stages of a project.
The SPRU researchers believe that the lack of quantitative information can mean that the research output of, say, a university department, is not taken sufficiently into account.
They cite the UGC's decision on how to make cuts in university spending in 1981.
‘Clearly the members of the UGC had some informal qualitative knowledge of the relative research strategies of different university departments, but they had no systematic evidence to support these views.
As a result, the main criterion was based on estimated teaching activity with past research performance apparently being accorded relatively little weight.’
The research councils also ‘rely almost totally on qualitative peer review judgements, and delegate responsibility for more detailed evaluation further down the line to research institute directors and university department heads’.
Some, particularly in smaller institutes, do the job well: but others ‘have little clear notion of where they stand nationally or internationally.’
Tasmanian conservation boned up by government
AN IMPORTANT new archaeological site, showing evidence of human habitation more than 20 000 years ago, has been discovered on the Franklin River in South west Tasmania, lending weight to new moves by the Australian federal government to stop the dam there.
Last week, the government introduced into parliament the world Heritage Properties Conservation Bill, specifically aimed at stopping work on the dam.
It was introduced at a special one-day sitting of parliament and relies on a number of commonwealth powers, including its right to make special laws for any race of people.
This is aimed at protecting the aboriginal sites on the Franklin.
The legislation will be used to back up the federal government's high court case, due to be heard at the end of May, which is seeking a permanent injunction against the power project.
The new archaeological discoveries, including artifacts, bones, charcoal and hearths, have been made at Deena Reena cave.
Charcoal and bone remains have already been dated at 20 500 years old and further remains found beneath these are now being analysed.
Military urged to transfer its technology
THE IDEA that Britain's leading position in military technology automatically helps its industrial base has taken a new blow.
A report by the electronics committee of the government's National Economic Development Council says that the type of companies that lead the military electronics market are precisely the companies that are unable or unwilling to develop products for civilian customers.
The report's author, Sir Ieuan Maddock, a former chief scientist at the Department of Industry, says the Ministry of Defence is partly to blame for this stagnation in ‘technology transfer’.
The bulk of the ministry's technically-sophisticated work goes to companies or divisions of companies that deal almost exclusively with the Ministry of Defence.
And most companies that deal in both military and civilian products admitted to Maddock that transfers between the two ‘did not happen to anything like the extent that was desirable’.
Maddock visited defence contractors such as Ferranti, GEC-Marconi, Mullard, Plessey, Racal and Thorn-EMI in his investigation.
Maddock reports that most firms are hostile to the idea of moving people between civil and military sectors: ‘There already exists a large culture gap and it is getting even wider.’
In the long run, this situation is bad for Britain's industry, Maddock argues, because the technology gap is growing all the time between companies working on advanced defence electronics (in which Britain leads the world) and these struggling to retain some of the consumer electronics business.
In contrast, before the Second World war, British civil industry was a leader in products such as radios and cathode-ray tubes which could be adapted quickly for military purposes.
‘The brilliant inventiveness of the engineers and scientists…would have been of little avail if this strong industrial base had not existed.’
The Ministry of Defence's own research establishments also came in for criticism for playing too great a role in specifying, designing and managing defence projects.
Maddock urged the laboratories to set up ‘industrial applications units’ to find civil uses for their projects.
According to the report, the only way for Britain to benefit from its military strengths in rebuilding an electronics industry is for a massive, government-sponsored effort in technology transfer.
This cannot be achieved by existing military contractors moving into civilian markets for which they have few skills, but by giving the market-oriented companies more access to ‘front-line’ technology.
Drivers to pay
HONG KONG will announce next week a £3–5 million experimental pay-as-you-drive scheme for taxing the colony's 300 000 motorists.
If the trial is successful the Hong Kong government will extend it to cover the entire colony.
The scheme could help to curb traffic on Hong Kong's crowded roads as well as distributing the cost of paying for the roads more fairly.
The technology has been developed by scientists at Britain's Department of Transport and could easily be used to cut traffic in cities in Britain and elsewhere.
It is the first practical way of introducing ‘road pricing,’ a concept recommended by a committee under the late Professor Reuben Smeed nearly 20 years ago.
Smeed said that traffic in cities should be cut by having electronic tolls.
Whenever motorists used a particular section of road they would pay for the upkeep of the road and for the congestion they caused.
In the Hong Kong scheme each car will have an ‘electronic’ number plate.
As it passes over a toll point in the road (essentially a wire loop buried in the surface) a radio beam from the loop will interrogate the number plate and feed back the vehicle number to a central computer.
As the vehicle passes over successive toll points, charges will clock up on the vehicle's account, rather like a telephone bill.
Charges will be geared to cut traffic to a desirable level.
Like telephone charges, they can be varied from day to day and between evenings and rush hours.
The Hong Kong contract is a boost for Transpotech, the marketing company set up by Britain's Transport and Road Research Laboratory.
One senior Hong Kong government official told New Scientist : ‘One of the things we hope to find out is whether the system can be fiddled.
We've got a lot of electronics wizards over here you know.’
Dockland airport faces planning row
THE FIRST serious row over plans to redevelop London's docklands seems certain to erupt soon.
It concerns a scheme, backed by the London Docklands Development Corporation (LDDC) to build an airport in the middle of an area scheduled for new housing.
The LDDC decided last week to support the plan for a commuter airport that could be in operation by 1985 and handle more than 100 aircraft movements every day from the port by 1990.
The airfield is designed for short take-off and landing airliners (STOLS).
The construction company, John Mowlem, Hants to build it on the central pier dividing the Royal Albert dock and the King George V dock.
A public inquiry into the proposal is due to start on 8 June.
A report to the LDDC last week said that landings and take offs could mean that a ‘small but significant section of the local community’ would suffer noise levels above the level at which houses and schools should not be allowed (40 on the NNI index).
But the corporation says that 5000 jobs could be created by the scheme, although few of them would be at the airport itself.
The Civil Aviation Authority, which is drawing up safety and environmental guide lines for STOLports say that it sees ‘no problem in  accommodating ’ the docklands plan.
Local groups complain that the guidelines are bring drawn up expressly to meet the requirements of the port — the first of its type in Britain — and the Dash 7 aircraft that will fly to it From Britain's regional airports and, later, abroad.
Chemical Industry flirts with an old flame
Coal tar was the feedstock for all kinds of chemicals until it was replaced by oil in the 1940s.
Now the wells are running dry, the chemical industry is once again making eyes at coal
Clare Bishop and Peter Maitlis
THE petrochemicals industry is one of the world's largest with annual sales of over £40 billion — excluding sales of fuel.
It manufactures basic organic chemicals on vast million-ton-per-year scales for every purpose: plastics and fibres, solvents and paints, and even some pharmaceuticals.
Many of these basic substances are ‘small’ molecules containing up to six carbon atoms as well as hydrogen, oxygen and sometimes nitrogen.
In order to make them, industry needs a source of carbon in a usable form together with hydrogen.
Oil is the most economic source at the moment.
Last month's record-breaking talks between OPEC members (it was their longest meeting) did not increase the amount of recoverable oil in the world, all it did was lower costs about $30 a barrel.
Oil will go up in price again.
In turn, the increase will add to the value of current research into alternatives, such as coal.
Last century, most materials for the chemicals industry were derived from coal tar, made by heating coal to high temperatures in the absence of air and distilling out the volatile products.
This is a complex, variable mixture and the efficiency of conversion is very low.
Coal-tar technology could not cope with the huge expansion in the chemical industry that took place after the Second World War.
Instead, the petrochemicals industry turned to oil as a raw material.
Oil is easier to obtain and to transport, and it has the advantage of being more homogenous as well as containing hydrocarbons with a higher hydrogen to carbon ratio.
Coal has a typical ratio 0–8:1 while that for oil is about 1–75:1.
Crude oil is thermally ‘cracked’ in a  refinery to give petrol.
In chemical terms, this is the chopping-up of larger, more vaned hydrocarbon molecules into simpler ones.
An important by-product of cracking is the two-carbon molecule ethylene.
Ethylene is the feedstock from which other molecules are built by the chemical industry.
Estimates of the amount of oil in proven recoverable reserves (92 x 10 9 tonnes) suggest that at current rates of use (3 x 10 9 tonnes per year) we should have enough, both as fuel and a feedstock, for 30 years.
There is, of course, substantial uncertainty about this figure.
A small increase in the amount of oil which it is economical to extract (currently around 35 per cent) would improve the picture substantially.
Nevertheless, it is likely that it will become cost-ineffective to extract the remaining residues earlier rather than later.
Natural gas is likely to be the next raw material of choice.
This is also composed of hydrocarbons and is similar to oil except that the hydrocarbon molecules are smaller and thus more volatile.
The main constituent of natural gas is methane, CH 4 , which has only one carbon attached to four hydrogens.
Proven natural gas reserves (of which 40 per cent are in the USSR) are estimated to be about 82 x 10 12 cubic metres (equivalent in energy to about 740 x 10 9 tonnes of oil).
At the current rate of use of some 1.5 x 10 12 cubic metres per year, this should be enough for another 50 years or so.
Although the reserves are vast, this supply will also run out in the not-too-distant future.
A further problem is that, although methane is an excellent fuel, it resists the changes needed to make it into a useful feedstock.
Biotechnology will surely play a vital role in any future chemicals industry.
However, there is little sign that it will ever produce compounds on the scale required by the heavy organic chemicals industry.
For example, alcohol (ethanol, CH, CH, OH) is invariably made by fermentation for human consumption.
The presence of as much as 90 per cent of water, because the fermentation must be carried out in water, is no real hindrance.
However, industry needs about 1.5 million tonnes of pure ethanol, free from water, each year.
To remove water from fermented alcohol on this scale would require quite uneconomic amounts of energy.
So it's usually made by stoichiometrically adding water to ethylene over an acid catalyst.
How about coal?
World reserves of coal of various types are estimated to be around nine hundred billion (9 x 10 11 ) tonnes.
That will last nearly 250 years at current consumption and no one can see beyond that.
Coal is a surprisingly complicated substance.
Details of its chemical make-up and formation are still incompletely understood.
We do know that most coal fields began life as swamps about 300 million years ago in the Carboniferous period.
When dead plant matter is exposed to oxygen in air or water it can be completely decomposed by micro organisms.
If the plant falls into stagnant water (containing anaerobic bacteria) it will not decay completely.
This is what happened in the Carboniferous swamps.
Dead matter was only partially decomposed and vestiges of the woody parts (such as spores, pollen bark and twigs) remained forming a layer of peat.
Periods of peat accumulation were followed by periods of rapid subsidence in which the peat swamp was submerged and covered with silt or sand.
The next stage is coalification: the very slow process in which peat is converted to coal.
High pressures and temperatures drive off volatile gases from the peat over millions of years and compact it to form coal.
Different coals have undergone coalification to different extents.
Brown coals and lignites have experienced only mild heat and pressure; anthracite, the highest quality coal, requires the extremes that only occur during the formation of mountains.
Since coal is formed from plants, which are chemically very complicated, its own chemical make-up is very elaborate.
It is impossible to describe coal with a simple chemical formula in the way that sugar (sucrose), for example, is represented by the formula CHO.
Even two lumps of coal from the same seam may be chemically quite distinct.
In general coals contain carbon, hydrogen, oxygen, nitrogen and sulphur with various other trace elements.
Their typical proportions in a particular coal are given in Table 1.
The percentage carbon content of a coal is important because the calorific value depends on it.
Coals are ‘ranked’ depending on their carbon content.
Brown coals and lignites (only 60 per cent carbon) are of low rank; anthracite (which is virtually pure carbon) is a very high rank.
Box A shows a hypothetical ‘coal molecule’.
Much of the structure consists of all-carbon rings.
In a few rings one of the carbons is replaced by an atom of oxygen, nitrogen or sulphur which can cause problems.
When coal is burnt any sulphur is oxidised to sulphur dioxide which is a poison both in the atmosphere and to a feedstock.
Coals must be desulphurised or the sulphur dioxide removed after burning.
Coal's complex structure causes a further problem.
It contains large raolecular units while the chemical industry wants small carbon units for feedstocks, so coal must be broken down, usually to make a gas.
There are two main techniques, carbonisation and gasification; both require very vigorous conditions, especially high temperatures.
Coal  carbonisation (its conversion to coke for steel making) is a long established process in which the coal is pyrolysed (split by heat) in the absence of air.
The main products are coke, coal tar, light oils, ammonia liquor and coke oven gas.
Early last century, long before the advent of natural gas, coke oven gas provided domestic heating and lighting in large industrial towns such as Birmingham and Sheffield.
The demand for gas soon became too great for the coke ovens to supply it, so gas works were set up.
The first gas works opened in London in 1812.
It produced coal gas.
Coke ovens were geared to efficient coke production and worked at about 1100°C.
At this temperature some of the hydrocarbons break down to give hydro gen.
The gas works operated at lower temperatures (about 800°C), so coal gas was slightly richer in hydrocarbons and had a better calorific value than coke oven gas (see Table 2).
Only about 20 per cent of the carbon in the coal ends up in the gas in either process, so  carbonisation is not an efficient method for producing a carbon-based gas from coal.
Future processes are more likely to use gasification.
Gasification involves not only heating the coal (as in  carbonisation ) but also the subsequent reaction of the coke or char produced with air, oxygen, steam or various mixtures of them.
The end product contains mainly carbon monoxide and hydrogen from the gasification step plus a little methane from the  carbonisation .
Gasification was first used in the US in 1873 to supplement coal and coke oven gas.
The main type of gas made was called blue water gas (see Table 2).
Blue water gas was made by a cyclic process.
The chemistry of this process with a few modifications, will probably form the basis of future gasification plants.
First hot coke is blasted with air.
The coke burns with the oxygen in the air to give carbon dioxide C + O2 = CO2 and becomes even hotter since this is a highly energy-releasing (exothermic) process in which 97 kilocalories are released for every gram-molecule (mole) of carbon dioxide formed.
This is commonly expressed as, H = -97 kcal/mol.
Eventually a temperature is reached where the endothermic (heat absorbing) reaction of carbon dioxide with the hot coke to give carbon monoxide starts, CO 2 + C = 200 (H = +39 kcal/mol)
The air flow is then stopped and the second stage of the cycle is begun in which the hot coke is blasted with steam.
This gives a mixture of carbon monoxide and hydrogen in another endothermic reaction,
C + H 2 0 = CO + H 2 (H = +29 kcal/mol)
At this point the blue water gas is collected and the cycle repeated.
The gas was given its name because steam is used in its preparation and because it burns with a bright blue flame.
It is more commonly known today as water gas.
The combination of the chief components, carbon monoxide and hydrogen, are usually referred to as synthesis gas (or syn-gas).
The technology of making water gas improved steadily until the 1940s.
During the Second World War the Germans used coal gasification to make syn-gas which was then used to make oil in a process developed by two chemists named Fischer and Tropsch.
The discovery of large quantities of natural gas, in the late 1950s made gas from coal uncompetitive.
The renewed interest in alternative fuels and feedstocks has revived coal gasification and the old technology is rapidly being brought up to date.
Future coal gasification will probably take place in a gasifier — the Lurgi or the Koppers-Totzek for example.
Both produce syn-gas using the same chemical reactions used to make water gas.
However oxygen, rather than air, is used to give a more concentrated syn-gas.
The process is very efficient and almost all of the carbon in the coal ends up in the gas.
Syn-gas is also made from natural gas (methane) by the related reaction,
CH 4 + HO= CO + 3H,
This reaction is more endothermic (H = +49 kcal/mol) but gives three times as much hydrogen.
The principles of making syn-gas, then, are well established.
The present challenges are: to design plants capable of making it economically on a large scale from coal; to explore the potential of syn-gas; and to develop its use in the production of basic chemicals.
In order to use syn-gas directly it must be possible to vary the ratio of carbon monoxide to hydrogen.
For example, to make methyl alcohol (methanol, which used to be called wood alcohol), a 1:2 ratio is required,
2H 2 + CO = CH 3 OH
Enrichment of the hydrogen content is accomplished by passing the syn-gas mixed with steam over an iron catalyst, when the mildly exothermic ‘water gas shift reaction’ occurs,
One problem remaining is the economic one.
Natural gas (at a price of £2.30 per 250000 kcalories), can make 1000 cubic metres of syn-gas for £44.
Coal selling at £8 per tonne will also make 1000 cubic metres for between £35 and £53.
The price range reflects the uncertainty involved in the novel technology that will be needed to build large syn-gas plants.
The price of 1000 cubic metres of syn-gas rises sharply to between £70 and £105 if the cost of coal rises to £35–45 per tonne.
The current price in Britain of coal suitable for this purpose is higher still, around £50 per tonne.
In South Africa the oil embargo coupled to the availability of large reserves of low quality but very cheap coal have made production of oil from coal attractive.
Its SASOL plant has now been making oil from syn-gas using the Fischer-Tropsch process since 1955.
It is hoped it will shortly be supplying 40 per cent of South Africa's motor fuel requirements.
In outline, the catalysed reaction that occurs in making hydrocarbons is,
However, the reaction is extremely complex and current ideas about it are the subject of hot debate.
Syn-gas and carbon monoxide have been a mainstay of the heavy organic chemicals industry around the world for some years.
This trend is very likely to continue.
Indeed, any shift from an oil-based to a natural gas or coal-based feedstock is likely, on present indications, to increase the use of these gases.
An extensive chemistry is being built up involving the use of carbon monoxide, hydrogen, and other simple building block molecules in interconversions catalysed by metals.
This is discussed in the next article.
New routes to simple compounds
Chemists are hunting new ways to create basic compounds from simple substances such as synthesis gas.
This mixture of carbon monoxide and hydrogen is obtained from methane, higher hydrocarbons or coal
Peter Maitlis
WHAT IS the cheapest way to make acetic acid?
It is the chief active ingredient in vinegar but the world's consumption of about two and a half million tonnes per year is used for more than sprinkling on fish and chips.
In fact vinegar and acetic acid for human consumption are still made by fermenting alcohol, but the biotechnology involved cannot supply the amount required by the chemical industry.
Other routes are needed.
Acetic acid (CH3, sometimes known as ethanoic acid) is a two-carbon compound comprising a methyl (CH 3 —) linked to a carboxylic acid group (-COOH).
There are many methods for making it.
The older, classical industrial syntheses were based on the interconversion of other two-carbon compounds.
One such route was the chemical oxidation of ethanol (’ alcohol’CH hol’-H) to acetaldehyde (or ethanal) which is in turn oxidised to acetic acid.
Later, alcohol was bypassed entirely.
One popular route started off from the highly reactive unsaturated hydrocarbon, ethylene (or ethene), H 2 C=CH 2,which was readily and cheaply available from petroleum refineries.
The oxidation route proceeded,
However, even this process has been made uncompetitive by a method developed by Monsanto in the US.
This process abandons the old idea that it is necessary to start with a two-carbon feedstock to make the two-carbon acetic acid.
Instead, it relies on coupling two C units, both of which are cheaply available from very convenient sources such as natural gas, coal or the otherwise useless ‘heavy ends’ of crude oil.
This process used some novel inorganic chemistry developed in this country, in Italy and in the US during the late 1950s.
The key is that acetic acid can be put together out of carbon monoxide and methyl alcohol (methanol, wood alcohol).
CH3OH + CO — CH 3 C OH Methanol costs about £140 a tonne.
This gives a route to acetic acid, selling for about £310 a tonne, that is more economic than the one from say, ethanol, at around £470 a tonne.
Convenience and economics mean that virtually all acetic acid plants being built rely on the Monsanto process.
A very large one, producing 150 000 tonnes of acetic acid per year has recently been commissioned by BP Chemicals at Hull.
This route would have been wishful thinking to the classical organic chemist hut the reaction proceeds readily in the presence of rhodium, one of the rarest metals, and of iodine, which act together as a catalyst system.
Catalysts lie at the very heart of the chemicals industry.
They speed up the rates of interconversions of compounds that would otherwise proceed too slowly to be useful.
Heating also generally increases the rate.
The faster a reaction proceeds, the more economical the process (within limits of course).
However, by using a good catalyst a reaction can be run at a high rate and at a lower temperature, thus saving energy.
The simplest catalysts are acids and bases.
The more complex, non-biological transformations use either homogeneous or heterogeneous catalysts.
Homogeneous catalysts are compounds of metals that dissolve in the reaction mixture and which must be separated from it at the end.
They give high selectivity for one product, but so far only a limited range of reactions falls within their scope.
Heterogeneous catalysts are insoluble in the reaction medium.
Indeed, the reaction is frequently performed by passing the reacting gases over a bed of the catalyst supported on some inert material.
Heterogeneous catalysts promote many different kinds of reactions and they are easy to recycle.
They often show lower selectivity (more undesired by-products are often produced) and little is understood about how they work.
However, although heterogeneous catalysis is still more an empirical art than an exact science, its usefulness cannot be overstated.
The conversion of methanol and carbon monoxide into acetic acid is catalysed  homogeneously by a soluble rhodium compound.
The  catalytic cycle (Figure 1) illustrates well how a modern inorganic chemist thinks.
The key compound is the negatively charged ion [Rh(CO) 2 I 2 ]—(1).
This is a square arrangement of two carbon monoxide and two iodide ligands attached to the central rhodium, which is in the rather low formal oxidation state of +1.
For this reason and because metals like to pack as many ligands around themselves as space permits, the ion readily undergoes ‘oxidative addition’ by methyl iodide (CH 3 — I).
This gives a new ion (2) in which the rhodium has the higher formal oxidation state of +3 and its desired complement of six ligands.
The very fast reaction (2) → (3) then occurs.
This is called a ‘methyl migration’ and the methyl which has just oxidatively added in reaction (1) → (2) moves onto the carbon of an adjacent carbon monoxide on the metal.
It is then replaced by an incoming carbon monoxide (feedstock) to give (4).
The inorganic part of the process (involving the metal) ends with the ‘reductive elimination’(opposite of oxidative addition) of acetyl iodide (CH 3 C) from the metal and the reformation of the square planar rhodium ion [Rh (CO)2 I 2 ]—(1).
However there is still an important piece of organic chemistry needed to complete the cycle.
Acetyl iodide is a very reactive substance and immediately combines with water, also present in the reaction mixture, to give acetic acid and HI (hydriodic acid), CH 3 C + H 2 O→CH 3 C + HI The HI in turn reacts with the methanol (feedstock) to give more methyl iodide, CH 3 OH + HI→CH 3 I + H 2 O which can then react to [Rh(CO) 2 I 2 ]— and so on.
A cycle such as this is a series of stoichiometric reactions.
Under ideal conditions, as in this process, steps proceed in sequence and incredibly quickly.
The only raw materials needed are carbon monoxide and methanol.
The rhodium and the iodide just continue cycling as long as feedstocks are supplied.
An important example of a  heterogeneously catalysed process is the synthesis of methanol from carbon monoxide and hydrogen (syn-gas).
Variants of this reaction have been known for many years.
The most successful process was developed by ICI Agricultural Division.
In this, the mixed gases are passed over a solid zinc oxide-copper catalyst.
The significant advance here was the development of a long life catalyst effective under  relatively mild conditions (250°C and 50–100 atmospheres pressure).
The stoichiometry of the reaction is given by, CO+2H 2 →CH 3 OH but this equation gives no indication of what happens on the catalyst surface: for example, carbon dioxide is almost certainly involved as an  intermediate at one stage.
This process requires extremely pure syn-gas, which may be either from natural gas (methane) or the higher hydrocarbons (naphtha).
It can also be made from coal but in this case the syn-gas must be carefully purified to remove sulphur compounds which poison the catalyst.
Methanol, largely as a result of this simple synthesis, is cheap and is likely to become an all-important petrochemicals feedstock.
Since it can also be catalytically broken down to carbon monoxide and hydrogen, it is also a convenient and easily transportable source of syn-gas.
The search is therefore on in laboratories around the world for new ways of obtaining useful chemicals from C 1 building blocks such as carbon monoxide, formaldehyde (obtained by oxidising methanol), perhaps carbon dioxide, but especially methanol.
The acetic acid process described above is an example.
The  catalysted conversion of methanol to ethanol by arrangement of two carbon monoxide and two iodide ligands attached to the central rhodium, which is in the rather low formal oxidation state of + 1.
For this reason and because metals like to pack as many ligands around themselves as space permits, the ion readily undergoes ‘oxidative addition’ by methyl iodide (CH 3 — I).
This gives a new ion (2) in which the rhodium has the higher formal oxidation state of +3 and its desired complement of six ligands.
The very fast reaction (2)—(3) then occurs.
This is called a ‘methyl migration’ and the methyl which has just oxidatively added in reaction (1)—(2) moves onto the carbon of an adjacent carbon monoxide on the metal.
It is then replaced by an incoming carbon monoxide (feedstock) to give (4).
The inorganic part of the process (involving the metal) ends with the ‘reductive elimination’(opposite of oxidative addition) of acetyl iodide (CH 3 C) from the metal and the reformation of the square planar rhodium ion [Rh (CO)2 I 2 ](1).
However there is still an important piece of organic chemistry needed to complete the cycle.
Acetyl iodide is a very reactive substance and immediately combines with water, also present in the reaction mixture, to give acetic acid and HI (hydriodic acid).
The HI in turn reacts with the methanol (feedstock) to give more methyl iodide.
which can then react to [Rh (CO)2 I 2 ]— and so on.
A cycle such as this is a series of stoichiometric reactions.
Under ideal conditions, as in this process, steps proceed in sequence and incredibly quickly.
The only raw materials needed are carbon monoxide and methanol.
The rhodium and the iodide just continue cycling as long as feedstocks are supplied.
An important example of a heterogeneously catalysed process is the synthesis of methanol from carbon monoxide and hydrogen (syn-gas).
Variants of this reaction have been known for many years.
The most successful process was developed by ICI Agricultural Division.
In this, the mixed gases are passed over a solid zinc oxide-copper catalyst.
The significant advance here was the development of a long life catalyst effective under relatively mild conditions (250°C and 50–100 atmospheres pressure).
The stoichiometry of the reaction is given by, but this equation gives no indication of what happens on the catalyst surface: for example, carbon dioxide is almost certainly involved as an intermediate at one stage.
This process requires extremely pure syn-gas, which may be either from natural gas (methane) or the higher  hydro carbons (naphtha).
It can also be made from coal but in this case the syn-gas must be carefully purified to remove sulphur compounds which poison the catalyst.
Methanol, largely as a result of this simple synthesis, is cheap and is likely to become an all-important petro chemicals feedstock.
Since it can also be catalytically broken down to carbon monoxide and hydrogen, it is also a convenient and easily transportable source of syn-gas.
The search is therefore on in laboratories around the world for new ways of obtaining useful chemicals from C 1 building blocks such as carbon monoxide, formaldehyde (obtained by oxidising methanol), perhaps carbon dioxide, but especially methanol.
The acetic acid process described above is an example.
The catalysed conversion of methanol to ethanol by reaction with syn-gas, is also possible but not yet commercial.
An interesting development is the direct synthesis of acetic anhydride, used to make cellulose acetate for photographic film base.
Tennessee Eastman, a branch of Kodak, will shortly commission a plant in Tennessee to make 225 million kilograms a year of acetic anhydride from coal.
It will first produce syn-gas from the coal which will be converted into methanol, which in turn will be converted into acetic acid.
Methanol and acetic acid can be combined (esterified) to give methyl acetate, Methyl acetate can then be reacted with carbon monoxide to acetic anhydride, in a variant of the acetic acid process, This allows the production of a C 4 chemical in four easy stages from the basic building blocks of carbon monoxide and hydrogen.
It is a glimpse into the future.
Nor is this the only way that methanol can be used.
Mobil in the US has made an aluminosilicate zeolite mineral called ZSM-5 which converts methanol almost effortlessly into high octane petrol.
Water is also produced but can be easily separated.
This remarkable transformation is believed to occur because this zeolite has highly acidic surfaces; they create carbonications from the methanol that polymerise.
It also has narrow channels; through which only molecules having dimensions within certain limits can pass.
ZSM-5 and related materials are known as shape-selective catalysts.
The mode of operation can be adjusted so as to produce a high content of aromatic (benzene-like) molecules which give the product its characteristically high octane number.
A plant based upon this Mobil process is being set up in New Zealand.
It will use local natural gas as a feedstock which it will use for the production of syn-gas, methanol and then petrol.
One of the topics currently fascinating chemists of many persuasions is the direct reaction of carbon monoxide and hydrogen to give other products.
One of these is the methanation reaction which occurs over a nickel metal catalyst and which was originally discovered by Sabatier and Senderens 80 years ago, Although this is widely used to make synthetic natural gas for example, there is even more interest in the production of compounds containing higher numbers of carbon atoms.
Reactions of this type take place over metal catalysts (typically iron) containing — promoters’(such as potassium) which play an essential though quite unexplained role.
These reactions, first discovered by Fischer and Tropsch in Germany in the 1920s were used to make poor quality petrol during the Second World War.
Many of the problems associated with hydrocarbon production by this method have now been solved: the SASOL plants in South Africa are an example.
Under appropriate conditions, Fischer-Tropsch type catalysts will also promote the direct formation from syn-gas of simple organic compounds containing oxygen (alcohols, ethers, aldehydes, for example), which are more valuable.
Unfortunately however, current catalysts mostly give a mix of products often including hydrocarbons.
The aim of a viable chemical process is to add value to the raw material in making the product.
It is desirable to have the highest possible selectivity towards one product to minimise the need for separation plants.
Clearly we need to find out how and why Fischer-Tropsch reactions work and how we may control and tune them.
There is substantial interest in the possibility of promoting these reactions with homogeneous catalysts.
One such process, developed by Union Carbide in the States, makes ethylene glycol (for antifreeze and to make polyesters such as terylene) directly from syn-gas.
This uses a highly ingenious homogeneous catalytic system.
based again upon rhodium compounds.
Unhappily, the conditions needed for the reaction to proceed with reasonable rates and good selectivity are so stringent (300C and 500–1000 atmospheres pressure) that commercialisation is probably a long way off.
The potential prizes in this area look very tempting and the search for fundamental understanding of the condensation of carbon monoxide both with homogeneous catalysts and on metal surfaces continues.
A current popular model (Figure 2) for the hydrocarbon formation reaction on a surface proposes dissociation of CO to a metal carbide () and metal oxide (O).
The carbide then reacts with hydrogen on the surface to give a methylene (CH,).
Chemists have begun to develop models for these processes and for the formation of other compounds using soluble metal compounds which are more easily studied than catalyst surfaces.
Many of the individual steps have been identified and considerable efforts are under way to take the steps and combine them into efficient catalytic cycles.
Heart disease — action now, proof later
Physicians, like scientists, set great store by the definitive experiment — the crucial investigation that will show the direction for the future.
Those dealing with heart disease have looked for such clean-cut results in vain.
So they have learned to do without them
Clive Wood
FOR ANY-ONE with an interest in coronary heart disease (CHD), the early 1980s is an exciting time.
Within the next two or three years, seven major studies, all aimed at reducing coronary risks on a massive scale, will have reported their findings.
Between them they involve tens of thousands of volunteers.
Total costs are in the region of tens of millions of dollars.
But the results are so late in arriving that they will have only a limited influence on national policies for coronary prevention.
Expert groups in many Western countries have already decided to lobby for changes towards a healthier lifestyle without waiting for results.
Worse still, the results do not even provide compelling evidence that intensive intervention on its own actually reduces the frequency of coronary disease anywhere in the world.
The findings from a number of these intervention studies were recently discussed at a symposium at Hohenreid Castle, outside Munich.
In the castle grounds is the famous Hohenreid Clinic, one of the first centres in Europe to introduce programmes of ‘secondary prevention’— changes in living patterns to save patients with one heart attack from having another.
Everywhere delegates mixed with the clinic's own patients in their blue track-suits.
Through the walls of the conference hall, occasional noises could be heard from the gymnasium next door as patients completed daily exercise routines.
And, as if to emphasise the difficulties that any prevention programme has to face, you had only to walk through any exit to find little groups enjoying a quiet cigarette.
But the main interest is in trying to reduce the rate of first heart attacks among middle-aged men prone to CHD (men have maybe four times as many heart attacks as women).
The key question is whether, after decades of exposure to the various ‘risk factors’, it is possible to reduce the frequency of coronary events and the deaths they may cause in men in the highest-risk categories, by altering the way of life of these men, and perhaps also by giving them appropriate drugs.
The commitment to try to answer this question was initiated by Professor Jeremiah Stamler of Chicago's North Western University Medical School in 1959, when a group of cardiologists led by Dr Paul Dudley White first urged both physicians and the American public to fight four major coronary risk factors: raised blood pressure (hypertension), raised serum cholesterol, obesity and diabetes.
Within two years, the American Heart Association (AHA) suggested that cigarette smoking was an additional risk.
It also published its statement on nutrition which, after reappearing in various forms, has probably done more to confuse the public than any other single pronouncement in the whole heart-disease field.
What the AHA said was that levels of total fat, saturated fat (which largely means animal fat) and cholesterol in the American diet were too high.
Accordingly, they should be reduced throughout the whole population.
It was this last bit that caused the problem, and critics have poured scorn on the advice ever since.
Their objections are: how is it possible to have a single dietary policy for old and young, rich and poor, manual workers and pregnant women?
And as the relationship between fat intake and cholesterol levels in the blood serum of any particular individual is very variable, why should we even try?
If the objectors miss the point, it is because the ‘prevention’ lobby expressed it badly.
The intention in changing the eating habits of the whole community is to bring the average serum cholesterol levels down and so reduce the overall risk in the population.
There is no guarantee that such a change will benefit you, me, he or she.
We all vary in our capacity to synthesise cholesterol and in the speed at which we break it down.
Genetic differences may mean that although we eat the same diet, the level of cholesterol in your blood may be, say, 200 mg/100 ml (5–17 mmol/litre) while mine is 300 mg/mI (7–76 mmol/l), giving me more than twice your risk of a heart attack.
Bringing the population average down makes us all more like the Japanese, with their low cholesterol and low coronary rates than, say, the Finns who have high levels of both (see Figure 2).
The fact that some Japanese have heart attacks and some Finns do not is no argument against the overall benefit of a low national average serum cholesterol.
How low?
An expert group convened by the WHO in 1981 noted that in populations where CHD was uncommon, average adult cholesterol levels did not rise above 200 mg/ 100 ml (5–17 mmol/l).
So that was the WHO's recommended optimum mean level for the populations of Britain, America, Europe and elsewhere.
The group admitted that the benefits for the individual were — uncertain and depended on his set of risk factors.
It also concluded that:— There seems little likelihood that scientific experiments will be able to demonstrate the effectiveness of primary prevention in the population as a whole, and thus decisions on preventive policy must be reached without prior experimental tests.’
But even the groups chairman, Professor Geoffrey Rose of the London School of  Hygiene and Tropical Medicine, admitted that he had thought that when all the trials were completed, the benefits of intervention would be certain.
It is not so.
What the trials do provide is a sufficient assurance of safety and a sufficient probability of major benefit to urgent action in the whole population.
Certainty eluded them, and it eludes us still.
So what went wrong?
The answer is that nothing actually went wrong.
But the hopes of those who designed the intervention trials, and particularly the hope that they could produce unambiguous results, were simply too naive.
Take, for example, the Multiple Risk Factor Intervention Trial (known in the trade as ‘Mr Fit’), a study of over 12 000 men in 22 clinical centres in the US started 10 years ago at an annual cost of $12 million.
Its origins go back to 1970 when a specially commissioned task force of the National Heart and Lung Institute (as it then was), was asked to look into the feasibility of a trial which would settle, once and for all, the question of whether dietary change could, on its own, reduce the frequency of heart attacks in the American population.
The task force decided against attempting a trial.
Although coronary disease was, and still is, the number one killer of American men, in any particular year it claims perhaps only 1 per cent of the male population.
A trial to show a clear difference in death rates between men on the old and new dietary patterns would have needed somewhere between 25 000 and 115000 volunteers and would have cost between $0.5 and 1 billion, even at 1971 prices.
Not surprisingly, the task force concluded that such a trial was not feasible.
What it recommended, instead of simply pursuing the diet/cholesterol link, was a trial to control cholesterol, blood pressure and smoking all at once.
Since the individual risks are multiplied together (so that someone with a double risk on each factor has an overall eight-fold increased risk) the trial could be done on a much smaller number of volunteers.
And that is how ‘Mr Fit’ began.
As yet, not all the results are in.
But the data that Stamler described confirmed the fact that risk levels for CHD can be changed.
After screening more than 360000 men, the 12 000 who were eligible were placed into an experimental or control group.
The criteria for inclusion are complicated, but basically the doctors were looking for men in the top 10 per cent of overall risk.
So, for example, a man with a diastolic blood pressure (the pressure when the heart is filling) of 90 mm of mercury (90 mm Hg) and who smoked 30 cigarettes a day, would be entered if his serum cholesterol was 295 mg/ 100 ml (7–63 mmol/l) or more.
Hypertensives in the experimental group received intensive advice about their blood pressure, including guidance on weight control and salt restriction, and if that didn't work they were given anti-hypertensive drugs.
They also received an equally intensive programme designed to change life-long shopping, cooking and eating patterns, and hence bring their cholesterol down as well.
A whole battery of measures was tried in an attempt to get them to give up cigarettes.
The other 6000 men, the ‘controls’ were not simply left to their own devices.
But instead of such intensive interest, they received their usual level of medical care from their own personal physicians, with an annual visit to the project clinics to have their progress monitored.
Six years of detailed attention produced a fall in blood pressure of some 12 per cent.
Cholesterol fell by 5 per cent and nearly half of the men gave up smoking.
But the control group didn't remain static either.
They also improved their risk factors — an 8 per cent fall in blood pressure, serum cholesterol down by 3 per cent and nearly a third giving up smoking.
The hoped-for physiological differences that resulted from an intensive intervention programme and routine medical care were marginal.
Differences in the rate of deaths from heart disease were also small.
The six year mortality from CHD in a population like this, getting only their usual physician's care, was expected to be about 29 per 1000.
Instead it was only 19.3 per 1000.
But the result of the intensive programming over the same six years lowered coronary death rate by only 7 per cent, when they were needed.
And in the meantime public health workers have learned to live without them.
Turkish nursery rhyme to test theory
A delightfully simple idea for an experiment to test Rupert Sheldrake's hypothesis of formative causation wins the New Scientist's first prize of £250
LAST October we invited readers to take part in testing an idea which, if established, would require re-assessment of large areas of biology and re-adjustment of much of the rest of science.
The idea was Rupert Sheldrake's hypothesis of formative causation, which he described in New  Sciente  of Life in 1981 and outlined in New Scientist in 18 June of that year (vol 90, p 766).
The hypothesis states that the form taken by any natural organised object or living thing, whether a crystal or an embryo, is not determined solely or indeed initially by the physical laws at present recognised by science.
Instead, the shape that a crystal first assumes is determined by chance.
Having once formed, it emanates ‘morphogenetic fields’ that influence the form taken by all other crystals of the same kind: and the fields of each new individual combine to provide a ‘morphic resonance’, active across both space and time.
By comparable mechanisms, an animal (or human) will find a task easier to learn if others of the same type have learned the same task before.
Not every-one shared our enthusiasm for Sheldrake's proposition.
The incomparable Nature declared that his was ‘a book fit for burning’.
But we pointed out, first, that the hypothesis of formative causation, if true, would be extremely important; and secondly that it was, in contrast to the crackpot ideas with which Nature (vol 90, p 749) confused it, open to experiment.
And what is theoretically important and testable deserves a scientist's consideration, Events then moved quickly.
In New York, Robert Schwartz, chairman of the Tarrytown Group of New York, offered $10000 to the person who actually carried out the best test of the hypothesis.
And we offered a mini-prize, of £250 (plus two £ 100 prizes for the runners-up), not for carrying out such an experiment but for providing an idea for a critical experiment.
The first prize has now gone to Dr Richard Gentle, a specialist in fluid mechanics at Nottingham University.
His idea is beautifully simple.
He sent us a Turkish nursery rhyme, in Turkish, along with a jumbled version that still rhymes.
Sheldrake's hypothesis predicts that it should be easier for people who do not speak Turkish to learn the real rhyme, even though it seems no less opaque to non-Turkish speakers than the fake one, because millions of Turks have already learned the rhyme in the past.
Our judges were four distinguished scientists: Professor Martin Rees, director of the Institute of Astronomy, Cambridge; Dr Alan Gauld of the Department of Psychology of the University of Nottingham; Professor Steven Rose of the School of Biological Sciences at the Open University; and Dr Peter Davies of the Department of Zoology, University of Nottingham.
They liked the elegance of Dr Gentle's idea, and the fact that it required no great resources.
Any interested person (a school-teacher?) could carry it out.
However, critics have already refined Dr Gentle's idea.
They point out that nursery rhymes are not nursery rhymes for nothing; it could be that the genuine Turkish nursery rhyme is intrinsically more memorable than the nonsense rhyme; nursery rhymes, after all, catch on in part because they have an innately pleasing rhythm.
But a leading Japanese poet, Shuntaro Tanikawa, has provided Rupert Sheldrake with an old Japanese rhyme, and has specially written a genuine poem and a nonsensical poem, all with the same rhyme and metre.
Will Westerners find the old rhyme easier to learn than the equally, euphonious ‘control’ rhymes, since the Japanese have already learnt it?
Two other experiments share second place and win £100 each.
Dr Susan Blackmore, of the Brain and Perception Laboratory of the University of Bristol, suggests that a group of babies be trained to use a ‘baby-operated tape player’ invented by Tom Troscianko and herself, which will shortly be released on to the market.
Six months later, after thousands of babies have learned to operate this device, more babies are introduced to the tape player.
Sheldrake's hypothesis predicts that the second group should learn to operate the machine more quickly.
But this test is rather more costly and time-consuming than Gentle's idea, and one judge felt that the variability in the way babies are normally cared for could introduce a great deal of ‘noise’ into the experiment.
Dr Vernon Neppe, a visiting lecturer at NY Hospital Cornell Medical Center, also wins £100.
He suggested inducing seizures in rats by repeatedly dosing them with the drug lignocaine, in small amounts that do not at first induce seizures.
Eventually this procedure permanently enhances the sensitivity of the rats to the drug; the phenomenon is known as kindling.
Once one group of rats has been treated, kindled seizures should occur more rapidly in other rats of the same strain, according to Sheldrake's hypothesis.
This experiment is rather costly, however, and one judge felt that the immense amount of work already done on chemical kindling in rats would muddy the waters.
Our own competition is ended: our congratulations to the winners, and our thanks to the judges and to the many entrants, who  produced some excellent ideas and criticisms.
The Tarrytown offer continues, however, until the end of 1985.
Those wanting more information should write to Robert Schwartz at the Tarrytown Conference Center, East Sunnyside Lane, Tarrytown, NY 1059 1, USA.
The Tarrytown competition might incidentally be unique (and our prize vicariously so); For when in the past has money been offered for an experiment to test an idea?
MONITOR
New proof that  neutrinos do not oscillate
Neutrinos, those ghostly elementary particles produced in radioactive decays, have a reputation for being elusive: they interact so weakly with matter they can pass right through the Earth unimpeded.
In recent years there has been some evidence that they are even more evasive, changing identity by oscillating from one type of neutrino to another.
But the latest results from an experiment designed to detect neutrino oscillations show that it has failed to find any such effects, as the group leader Felix Boehm announced at the Spring Meeting of the American Physical Society last week in Baltimore.
Boehm, from the California Institute of Technology (Caltech) told the audience.
‘We believe that our results show conclusively that neutrinos do not undergo oscillation and thus are not mixed states in a quantum-mechanical sense.’
Neutrinos belong to the group of particles known as leptons — particles that do not feel the strong nuclear force which binds the atomic nucleus together.
The best-known lepton is the electron, but as well as the neutrinos the lepton family also includes the muon and the tau, both of which are like the electron but are respectively 210 and 3500 times heavier.
The neutrinos appear to come in three varieties, one associated with each of the electron, muon and tau.
They have generally been assumed to be massless, particularly as measurements of the electron-neutrino mass showed it to be less than a few ten thousandths that of the electron.
Only if neutrinos do have some mass, however small, can they oscillate from one type to another.
In that case, the electron-, muon-, and tau-neutrinos would be quantum-mechanical mixtures of some ‘base states’ of differing mass.
Because of their different masses the base states would move at slightly different velocities and mix in an oscillatory fashion, producing first one type of neutrino and then another.
Thus oscillating neutrinos must have mass; but massive neutrinos need not oscillate.
The first signs that neutrinos might oscillate came in 1980 from an experiment by Fred Reines and colleagues at the University of California, Irvine.
They studied the interactions of electron-antineutrinos produced in the fission reactions in the nuclear power reactor at Savannah River in South Carolina.
Interest in the possibility that neutrinos might oscillate intensified later that year with the result from an experiment at the Institute for Theoretical and Experimental Physics at Moscow, which implied a small mass of 34 electron volts for the electron-antineutrino (the electron's mass is 0–5 million electron volts).
But since then the evidence against oscillations has been mounting.
It has come from experiments with high-energy muon neutrino beams at particle accelerators, and from lower-energy neutrinos at two nuclear reactors.
This latter work has been by Boehm's team of physicists from Caltech, the Technical University of Munich, the Institute for Nuclear Science at Grenoble and the Swiss Institute for Nuclear Research at Villigen.
Boehm's group began work in 1979 at the research reactor at the Institut Laue Langevin (ILL) in Grenoble, but the researchers have since moved their apparatus to take advantage of the higher flux of neutrinos from the more powerful commercial reactor at Goesgen in Switzerland.
Their technique is to look for the process known as ‘inverse beta decay’, in which an electron- anti neutrino reacts with a proton to produce a positron (positive electron) and a neutron.
By measuring the energy spectrum of the positron, the researchers effectively measure the spectrum of the antineutrinos arriving at the detector.
A comparison of the measured spectrum with that expected for antineutrinos issuing from the fission reactions in the reactor's core reveals whether the antineutrinos have changed in any way en route to the detector; muon- or tau-antineutrinos would not produce inverse beta decays.
At ILL the researchers found the ratio of the measured yield of positrons to the expected yield to be 0–955, with a statistical error of 0.035 (Physical Review D , vol 24, p 1097).
Last autumn they reported an improved figure, after moving the apparatus to Goesgen, of 1.05 + 0.02, in other words, a result consistent with there being no oscillations (CERN Courier , vol 22, p 371)— In its latest work the team has been able to dispense with having to make a comparison with the expected antineutrino spectrum, thereby removing one source of uncertainty.
The researchers have repeated their measurements with the apparatus moved from its initial distance of 38 m from the reactor's core to 48 m.
A comparison of data from the same apparatus at different distances is a crucial test for oscillations.
If neutrinos do oscillate then their spectrum should vary with distance from the reactor core, the actual result depending on the length between oscillations.
The new data put tight limits on the difference in mass,m , of the base states.
When analysis is complete it should set (m)2 as less than 0–01 V 2 , a factor ten smaller than the upper limit found in the experiment at ILL.
The new results do not exclude the possibility that neutrinos have mass, and so cosmologists still have the freedom to consider the role of the massive neutrino in the evolution of the Universe.
Honey bees can recognise kin
DARWIN'S theory predicts that animals will strive to maximise their genetic contribution to the next generation.
In the 1960s, W. D. Hamilton suggested that an individual should sacrifice itself to help not only its own children, but also its close relatives that share many genes.
The search for animals which can identify kin has now led to the discovery that honey bees discriminate between full and half sisters.
The bees are significantly more likely to bite half sisters than full sisters, even though all the bees grew up in the same hive (Nature , vol 302, p 147).
Wayne Getz and Katherine Smith of the University of California at Berkeley set up three hives with a queen fertilised by two unrelated drones — the matings produced worker bees of differing colours.
Worker bees are three times more closely related to their full sisters than to their half sisters, so workers could enhance their fitness by raising a full sister as the new queen when the colony swarms.
The biologists watched how groups composed of 10 full sisters reacted to a new worker.
The bees, they found, were more  aggressive towards half-sisters.
The workers, it seems must be using genetically-based cues, such as colour or odour, to discriminate between full and half sibs.
But it is not yet clear whether they base their judgements on their own features, or whether they have become habituated to the characteristics of their full-sister group.
Researchers hope that observations of behaviour within the hive will provide an answer.
An enzyme with many faces
THERMODYNAMICALLY speaking organisms are improbable.
They are only made possible by enzymes — biological catalysts that speed up their chemical reactions to a respectable level.
Enzymes are remarkably discriminating.
Each catalyses a single reaction of a particular chemical, which is recognised by its shape.
This ‘lock and key’ relationship has led to the view that the enzymes' own structure must of necessity be unique and unchanging, and so it is — usually.
The notion that an enzyme might exist in a number of forms decided purely on probability is anathema to many scientists.
Yet this is exactly what has recently been proposed by Professor David de Rosier and his co-workers at Brandeis University, in Massachusetts,(Journal of Molecular Biology .
vol 165, p 523).
The enzyme in question is the a-keto glutarate dehydrogenase complex (KGDC for short).
It is one of the several enzymes involved in the central process of a cell's metabolism known as the citric acid or Krebs cycle.
Like all enzymes it is a protein and like some it is made up of a number of polypeptide subunits (so forming a ‘complex’).
But like no other yet characterised it seems able to exist and function in a large number of structural permutations.
Therefore no unique structural formula describes all molecules of KGDC.
The complex is made up of three different types of subunit each with its own enzymic activity.
The first type forms a core in the shape of a cube.
The two remaining types bind to the surface of the cube.
There are four binding sites symmetrically arranged on each of its six faces.
The distribution of sites has the same symmetry as the cubic core, but the occupancy of sites by the two subunits is fairly haphazard (see Figure).
Combinatorial analysis has shown that there are no fewer than 124896 structural isomers possible for KGDC.
Enzymatic catalysis involves the breaking and making of different chemical bonds.
The received wisdom of biochemistry is that active sites in complex enzymes are close to one another, so as to facilitate the movement of intermediate structures among the active sites, rather like a production line in a factory.
The de Rosier team argue that this need not be true if an active site on one part of the molecule can affect other sites on the complex in such a way as to facilitate movement of intermediates between sites.
Any advantage bestowed on KGDC by its peculiar structure remains a mystery — but one possibility is that a molecule with 24 separate binding sites will be much more effective at picking up very small quantities of substrate.
Why lions are not jealous lovers
MALE lions are not the solitary, ferocious beasts that inhabit the jungles of Hollywood.
Whereas females cooperate in hunting and in rearing cubs, male lions cooperate with one another in forming coalitions which try to gain and keep possession of groups of females.
Despite their weaponry, males within a coalition rarely fight over oestrous females.
Dr Brian Bertram, now curator of mammals at the London Zoo, first suggested explanations for this surprising harmony.
A male who stands by and allows another companion member of the coalition to mate with a receptive female is not really being altruistic, because that companion is probably a relative.
As William Hamilton, now at the University of Michigan, pointed out in the 1960s, a male can perpetuate its genes in the next generation, not only by fathering offspring himself, but also by assisting the reproductive efforts of near relations who share many of his genes.
Evolution can thus favour apparently ‘altruistic’ behaviour which increases the reproductive output of relatives, if this behaviour really does increase the individual's inclusive fitness — the total representation of its genes in the next generation.
This evolutionary driving force was dubbed ‘kin selection’, to distinguish it from the additional (and inevitable) shaping of an individual's own reproductive efforts by natural selection.
In the case of lions, not only was a rival male likely to be a close relative, but the costs of physically competing with him were high and the benefits low.
Costs were high because injuries to a male can easily be crippling or fatal, whereas serious injuries to his companion can leave him without an essential partner in combating rival coalitions.
Benefits were low because only a minute proportion of copulations result in a reared cub in the next generation (Journal of Zoology , vol 177, p 463).
Brian Bertram, working with David Bygott and Jeannette Hanby of the University of Cambridge, showed that males actually did increase their lifetime fitness by cooperating with one another (Nature, vol 
282, p 839).
They followed the fates of individually-known lions living in the Serengeti and Ngorongoro Crater in Tanzania over several years.
Their data showed that the larger the coalition, the greater was the fitness of each male within it despite the fact that he had more companions with whom he had to share paternity.
The effect resulted mainly from the fact that larger coalitions could maintain tenure of a pride for longer.
Because, they argued, the males in coalitions were almost always close relatives, kin selection enhanced the benefits of cooperation.
But is kinship really an important driving force behind cooperation in male lions?
Craig Packer and Anne Pusey of the University of Chicago have continued to follow the life and loves of these Tanzanian lions (Nature , vol 296, p 740).
Now, it seems, non-relatives are not so uncommon in coalitions: 5 out of 12 (42 per cent) of the coalitions they observed contained non relatives — considerably (but not significantly) higher than the 10 per cent (2 out of 21) recorded in Bygott, Bertram and Hanby's study.
Packer and Pusey suspect that the likelihood of finding non-relatives in a coalition probably varies with the average age of its members, because young related males may die and be replaced by singleton males from other prides.
Packer and Pusey observed oestrous females attended by rival males, sometimes relatives and sometimes not.
They showed that disputes between the males were some what commoner than had previously been reported.
They also noticed that males within a coalition did not discriminate between kin and non-kin: they did not compete more intensely with non-relatives.
This observation, they believe, throws doubt on the importance of kin selection.
The researchers suggest that game theory, rather than kin selection, can account for the general absence of fighting over females within a coalition.
Game theory suggests that contests over oestrous females will be settled ‘conventionally’, by the recognition of asymmetries such as‘owner versus rival’.
The costs of fighting, which often leads to serious wounds or blinding, would outweigh the benefits of mating with a particular female in the long term, they argue.
Males compete mainly, say Packer and Pusey, by trying to anticipate oestrous in a female to be the first to consort with her.
Bertram argues that game theory and kin selection should not be seen as alternative explanations (Nature , vol 302, p 356).
‘The role of kin selection is that in competition between related males, it makes the costs higher and the benefits lower,’ he says.
Nor is kin selection the only force promoting cooperation in lions; but it can amplify the initial advantage of being in a group.
Wild potatoes mimic aphid alarm signal
ONE type of wild potato has developed a novel way of protecting itself from the ravages of aphids — by producing the chemical that aphids themselves use as an alarm signal.
R. W. Gibson and J. A. Pickett of Rothamsted Experimental Station in Harpenden believe their discovery to be the first example of a food crop plant using insect pheromones in this way, and suggest that this mode of protection could be introduced to cultivated species (Nature .
vol 302, p 608).
Gibson and Pickett studied the wild tuber-bearing potato,Solanum berthaultii .
which shows resistance to a wide range of pests including aphids, leafhoppers, mites and thrips.
Their resistance lies in the tiny glandular hairs which cover the foliage of the wild potato, but not that of cultivated species.
The hairs come in two types, designated A and B .
Type A hairs are short, and have a four-lobed head which ruptures on contact to produce a quick-setting fluid.
Type B hairs are longer and secrete a sticky exudate.
Together they provide a formidable barrier which effectively immobilises unsuspecting insects.
But trapping appears to be only the second line of defence.
Analysis of the exudate of type B hairs and leaf-washes taken from S .
berthaultii revealed the presence of (E) -B farnesene, the main ingredient of the alarm  pheromone of most species of aphid.
If this chemical were released into the air surrounding the leaves, it would act as a first line of defence, repelling aphids with an eye on predation.
The Rothamsted researchers confirmed their suspicions in a series of delicate experiments involving the aphid,Myzus persicae , the wild potato S. tuberthaultii and its cultivated relative S. tuberosum .
They found that air expelled from a syringe which contained leaflets of the wild potato disturbed 54 out of 99 aphids.
In a similar test using leaflets of cultivated potato, only one of 113 aphids was disturbed.
The researchers then set themselves the unenviable task of positioning 48 aphids about 1 cm away from the leaves of both the wild and cultivated plants, making sure that the insects were pointing towards the leaves.
Only six aphids were daring enough to walk onto S. berthaultii leaves compared with 34 which strolled happily across S .
turberosum leaves.
Researchers have already managed to produce hybrids of S. berthaultii and cultivated species.
A quick way to sequence proteins
SCIENTISTS are beginning to catch on to a technique with the space-age name of FAB-MS (fast atom bombardment mass spectrometry).
FAB-MS has already made some impact as a probe for finding the composition of the surfaces of materials — now it is breaking into the business of measuring the molecular mass of large organic molecules, such as chains of peptides, the constituents of proteins.
The trick is to dissolve or suspend the molecule in drop of matrix material such as glycerol.
Then the machine bombards the molecule with a beam of fast atoms, which knocks a stream of charged ions off from the molecule.
Many of these will be the molecule under study, slightly modified by the addition or loss of a proton.
The mass spectrometer imposes electric and magnetic fields on the ions in such a way as to separate and count ions according to the ratio of mass to charge, and thus to establish the mass of the molecule.
Inside the mass spectrometer, the beam can be directed towards a target where the ions are broken up into their constituent parts, whose individual masses are also measured.
The machine includes  software that can look at the very precise estimates of molecular mass and relate this figure to a certain combination of atomic masses, thus establishing which amino-acid groups are present.
The software can also compare much data of this type to determine the amino-acid sequence.
Exact measurement of molecular weight works for molecules up to 10 000 times the mass of the hydrogen atom — among the molecules that have been  analysed is the hormone glucagon with a molecular weight of 3482.
Amino-acid sequencing is limited to smaller molecules, for example the endogenous opioid met-enkephalin with a molecular weight of 573.
FAB-MS has also succeeded in sequencing peptides that could not be  analysed any other way — for example a toxin from Australia that a group at the Department of Inorganic and Organic Chemistry at Cambridge University sequenced recently.
Mr Richard Smith, who works in Dr Dudley Williams's group at Cambridge, said that the technique was quicker and easier to use than its major competitor, field desorption mass spectroscopy.
One drawback is the price — machines on the market cost between £100 000 and £500 000.
Lizard bone shakes world of taxonomy
MUCH of that part of the classification of the animal kingdom which is based on the study of homologous structures in living creatures may have to be reexamined if not demolished and rebuilt, following the publication of a letter in Nature from two distinguished comparative anatomists (vol 302, p 243).
The letter concerns formation of an obscure bone in the heads of a group of burrowing lizards called the amphisbaenians.
‘This scientific paper would have riveted the anatomical and zoological world of the 1920s.
It should do so today,’ says anatomist Dr Robert Presley of University College Cardiff, commenting on the paper.
Comparative anatomy, however, is an unfashionable science in the West today.
The excitement concerns a bone in the floor of the skull, that lies between the eyes of amphisbaenians, called the orbitosphenoid bone.
In amphisbaenians, well adapted to their burrowing lives, with vestigial or non-existent limbs and much reduced eyes, the skull is very heavily ossified and box-like to enable it to be rammed through the soil, and so the orbitosphenoid is a thick plate of bone.
In surface-living lizards, thought to be the nearest relatives of amphisbaenians, the orbitosphenoid is a mere wisp of flimsy, soft cartilage.
It has always been assumed that the bony orbitosphenoid of amphisbaenians evolved from the flimsy cartilage of lizards by the straightforward ossification of cartilage — the process whereby nearly all bones are formed during the embryonic development of any animal.
In other words, two homologous structures were formed in the same way in two closely-related groups, though their development diverged, in the course of evolution, to adapt the structure for two different lifestyles.
But two distinguished comparative anatomists with a long-time interest in amphisbaenians, Professor Angus d'Alberi BelIairs of St Mary's Hospital Medical School in London and Dr Karl Gans of the University of Michigan, decided to take a closer look.
They obtained some preserved embryo amphisbaenians — a difficult thing to do as specimens are scarce — and sectioned them at various stages of development.
The results showed that the orbitosphenoid in amphisbaenians, though apparently homologous to the same structure in lizards, is formed in quite a different way.
It is formed directly from soft tissue (mesenchyme) and not by ossification of cartilage and it belongs to the rare class of bones known as membrane bones.
Bellairs himself is cautious about the wider significance of his findings, suggesting in his letter only that the classification of the amphisbaenians and their relationships with related groups deserve new attention.
But Presley points out: ‘With the discovery that a membrane bone can totally mimic a basic cartilaginous constituent of the developing skull, we have to ask ourselves how widespread such a phenomenon may have been in vertebrate evolution.’
The enormous edifice of presumed relationships built up from careful study of homologous structures,(the supreme example is the study of bones in the skulls of reptiles) may have to be re-examined, and if necessary dismantled and rebuilt from the ground up.
TECHNOLOGY
Slow start for private lines
MERCURY Communications, the new British telecommunications company, is running into problems launching its rival to British Telecom's trunk lines.
A dispute with Telecom engineers is holding up the company's first link in London, and Mercury's first inter-city line — which should have been operating by now— will not open until next year.
The company is the first, and so far the only, firm that has won permission from the government to compete with British Telecom, now that the  government has removed Telecom's monopoly over telecommunications services.
Mercury wants to install a network of fibre-optic cables, to carry telephone calls and computer data, inside the ducts that run along railway tracks.
The network will eventually link Manchester, Birmingham, Bristol, London and towns en route in a figure-of-eight formation, centred on Birmingham.
But Mercury is making a slow start.
It will not say how many subscribers it has — only that it expects to sign up no more than a couple of dozen by the autumn.
The first customers will all be in London, where microwave transmitters on roof-tops will carry voice and data traffic coded in digital form.
It is this microwave link, rather than a fibre, that Mercury will extend in stages toward Birmingham later this year.
Gradually, however, fibre-optics will take over.
By 1985 Mercury plans to install 1000 kilometres of cable to link Britain's business centres, but work on laying the cables has not yet begun.
One reason was the company's decision, at the end of last year, to switch from multimode fibre optics to a newer type of cable called monomode.
Monomode cables carry light signals on a longer wavelength, and will give Mercury the chance to carry four times as much data as is possible on multimode, and over a greater distance.
The cables will need repeaters only every 25 kilometres.
Contracts to supply the 10000 km of fibres and associated equipment are still out to tender.
One of the big British cable-makers — BICC, GEC or STC — looks likely to pick up the £20 million order.
British Telecom, meanwhile, plans to install 100000 km of fibre by 1990, most of it multimode.
Mercury's immediate problem is getting the limited London service working.
The company wants to open a microwave link between its headquarters in Long Acre and its main transmitter in Britannic House, less than 3 km away.
Mercury is waiting for Telecom to connect its equipment with a private telephone exchange (Telecom's contracts specify that its engineers should carry out any work on telephone exchanges).
But the engineers, members of the Post Office Engineering Union, are refusing to do any work on the Mercury network as a protest against the government's plan to turn Telecom into a private company.
A spokesman for Mercury said this week that he was waiting for engineers to fit boards to hook up the exchange.
Although the spokesman claimed that the Mercury network can operate independently of British Telecom, the blacking has stopped the launch of the Mercury service.
The company has already had to win a hard-fought battle with Telecom to get access to international telephone lines — a must for the type of customers it hopes to attract.
The network will also need Telecom's cooperation to distribute traffic in areas without microwave links.
So far, Mercury, which is backed by a consortium of Cable and Wireless, Barclays Merchant Bank and BP, has either committed or spent £60 million.
The network will cost £200 million to complete, considerably more than the £50 million reported as the company's original funding.
The final network will include a £2¼ million satellite-Earth station in London's docklands (see left).
British Telecom has been putting on the pressure since Mercury was granted a licence in February last year.
That same month, the corporation announced new high-speed cable and satellite services of the kind that Mercury plans to offer.
Telecom will also have the edge over Mercury in its ability to provide switching.
At first Mercury's customers will have to rely on fixed, leased lines, and will not be able to talk to other subscribers.
Not until a new switching centre in Birmingham comes on stream will the 1000-km network be up to the mark.
Mercury is evaluating switching centres from both British and overseas suppliers.
Early indications are that Britain's system X, which Telecom is gradually introducing is not particularly favoured.
Eventually, Mercury will carry telephone calls, computer data, slow-scan television pictures, and provide services such as video-conferencing and electronic mail.
But it seems unlikely that many customers will rely solely on the private network, instead using it in tandem with British Telecom's lines.
Mercury eyes space privateers
MERCURY could be involved in an unusual satellite link across the Atlantic.
The firm is talking to Orion Satellite Corporation, in Washington DC, about linking up to the US via two satellites which the American firm plans to station above the Atlantic in 1986 and 1987.
Legislators in the US are still deciding whether to permit Orion to go ahead.
The company's spacecraft would be the first commercial communications satellites above the Atlantic that are not owned by Intelsat, the international satellite organisation in which 108 nations have shares.
Orion's plans have caused consternation among Intelsat officials, who are worried that the craft could become the first of many operated by private companies to ‘cream off’ traffic from the lucrative telecommunications route between North America and Europe.
Meanwhile, Mercury has more plans for transatlantic services using an existing Intelsat-V satellite.
Starting next year, the company intends to operate an Earth station in London's docklands.
This would send signals via the craft to American telecommunications companies such as AT&T.
Later in the 1980s, Mercury may also lease transmitters on a European satellite to carry telephone calls to other parts of the continent.
Mercury may also lease channels on a satellite to carry TV programmes.
The material would be broadcast to antennas on the ground and from these by cable to people's homes.
British Telecom, Mercury's competitor, has already announced similar plans using Unisat, Britain's national satellite due to be launched in 1986.
Robot turns into a work-mate
YES-MAN, a prototype table top robot, unveiled this week needs human company.
The two-armed machine's designers at Patscentre, Cambridge, say it will take over routine laboratory tasks such as assay work, culture transfer and chemical sampling.
If they are right, technicians' overtime sheets will never be the same again.
The prototype Yes-Man was made for Prutec, the venture capital arm of the Pru, which is now looking for a manufacturer.
Patscentre made Yes-Man after it decided that robots were getting too complicated: manufacturers were trying to make them both cognitive and manipulative.
The designers decided to supply the cognitive power of this machine by giving it a human.
No one says you could train a monkey to use it; but ‘you could train a very unintelligent operative’.
It can be programmed by machine code, by taking it through the motions, or by keyboard.
The arms have three degrees of freedom, and it is possible to fit wrists.
The kind of tasks it might excel at are assembling keyboards and putting gearboxes or electric motors together.
In fact, any sequence of tasks that requires testing before each step.
The human hands Yes-Man a part, the robot fits and tests it.
Yes-Man has built-in safety features.
Screwdriver blades might have a retractable sheath or ultrasonic guards, for example.
And the arms cringe when they touch human flesh.
Why haven't heat pumps caught on?
HEAT PUMPS turn out more usable energy than they consume — but their impact on Britain's homes has fallen short of what many pundits predicted.
The idea of recovering waste heat from air or water, and using it to heat buildings cheaply, is very attractive.
So why have these devices not caught on?
In practice, however, heat pumps are expensive to install, they have long payback periods at current fuel prices, and there are doubts about their reliability and performance.
Discussion among delegates at the Heat Pumps for Buildings conference at Nottingham University this month revealed that, far from being a fully developed technology as their long history might suggest, their evolution is not complete.
Heat pumps fall down on their development from the refrigeration equipment industry — none of the components of a heat pump has been designed specially.
Compressors in heat pumps need to give high efficiency over a wide range of operating temperatures, but those developed for refrigeration tend to be designed for a comparatively narrow range.
No manufacturer has yet made a compressor specifically for heat pumps, although one American company is studying the problem.
On top of this, no one has developed a way of controlling heat pumps with microprocessors.
Both techniques could considerably improve the efficiency of electrically-driven heat pumps, making them a more worthwhile proposition.
One paper at the conference predicted that the heat pump companies can cut overall costs by at least 25 per cent in the next 20 years.
An improvement in coefficient of performance (the ratio of heat output to total power consumption) to about 5:1 along with standardisation and simplified equipment could cut costs by half.
Because of their high cost and complexity, new refrigerants and/or mixtures are unlikely to cut the cost of heat pumps in the home.
Heat pumps are economical only when they exploit a low temperature energy source costing little or nothing.
The commonest sources are ambient ones, such as outside air, the ground or running water.
All are effectively stores of solar heat.
Two rules must be obeyed when exploiting them: local cooling, close to the heat pump or its heat exchanger, must be no faster than heat can flow in from the surroundings; and the source temperature must not be lowered more than a few degrees below its undisturbed level.
One quality that a good source should have is a relatively high temperature throughout the heating season, making ground-water pumped from shallow depths the best.
The worst ambient source from the point of view of performance is outside air.
Because of its low thermal capacity, large amounts must be used, which creates a problem of noise.
No one has managed to stop outdoor heat exchangers frosting up during winter, but the availability of air makes it the most likely source for domestic, mass-produced heat pumps in Britain.
The Department of Energy's research station at Harwell estimates that industry could exploit a heat pump market of between £500 million and £1000 million over the next 10 to 15 years.
But it predicts massive sales and energy savings only if heat pumps penetrate the space-heating market.
Shipshape: but why?
TWO THEORIES surfaced this week to explain the ‘uncorroded condition’ of Holland 1, the submarine that sank off Cornwall in 1913 and was recovered last year (New Scientist , 31 March, p 905).
‘She was protected within weeks by rapid colonisation of a cold-water coral,’ argued Michael Schofield of UMIST's Corrosion Protection Centre.
‘The corrosion rate is not out of line with what you would expect of a steel hull lying in quiet sea water,’ claimed John Bernie, head of the National Physical Laboratory's National Corrosion Service.
Schofield said: ‘You can scotch all the ideas about magnetic fields or different metals in the hull preventing corrosion.
We think she corroded for only a matter of weeks before being colonised.
There is a 3–4 mm deposit of calcium carbonate on top of the coral skeletons.
This probably prevented galvanic corrosion by covering all internal and external surfaces.’
Bernie believes some parts, mainly the rivets, had corroded and so protected the hull because they were tightly bound to the 11 mm steel.
Scientists at NPL thought that the non-ferrous towers caused the ferrous decking to rot.
Safety foam
A NEW TYPE of polyurethane foam developed for mattresses in prisons could cut the number of deaths from fires in homes.
Standard polyurethane foam ignites rapidly, forming dense clouds of smoke and toxic vapour as it does so.
The new foam, which the  government Property Services Agency developed, is impregnated with a high-alumina product which forms water vapour as soon as a flame touches it.
During tests at the government's Fire Research Station, an ordinary mattress reached a temperature of 108°C after catching fire, and produced 313 cubic metres of smoke.
The safety mattress took a shorter time to reach a lower maximum temperature — 84°C — and formed only 50–3 cubic metres of smoke.
The safety mattress also produced only half as much carbon monoxide and carbon dioxide.
Best of all, the safety mattress does not cost much more than standard foam.
America assesses prospects for viewdata
COMMERCIAL viewdata services aimed at domestic customers are starting up in the US later this year, but it is not clear that the public is waiting with bated breath.
In viewdata (known in the US as videotex), information is transmitted to terminals such as TV screens along telephone lines.
British Telecom has pioneered in this area with its Prestel service, but so far this has appealed mainly to business customers.
Leading the pack in the US is the Viewdata Corporation of America, a subsidiary of the Knight-Ridder newspaper chain.
It plans to offer the service in the Miami area this September and take viewdata nationwide by 1985.
‘We expect to be the McDonald's of videotex, opening up franchises around the country,’ said a spokesman.
Knight-Ridder sponsored one of the early viewdata tests with a system called Viewtron.
Some 200 householders in the Miami suburb of Coral Gables participated in the trials between July 1980 and September 1981.
The telecommunications giant AT&T.
which provided the telephone lines in these trials, is now involved in another experiment in which it has teamed up with the CBS television network.
In this, a couple of hundred homes in the New York suburb of Ridegewood, New jersey, are receiving viewdata.
Both sets of tests aim to discover what consumers want.
Knight-Ridder is accenting three specific areas for its commercial services: local information, home banking and ‘teleshopping’.
The system will use a display format derived from the Canadian Telidon standard.
Terminals will cost around £500; on top of this, subscribers will have to pay for their information at the rate of about £30 per month.
Marketing will focus on what Americans call the ‘upscale’ consumer: ‘youthful, affluent, success and status-oriented’.
But will such people be caught dead keying into ‘the McDonald's of videotex’?
Market analyst Steven Weissman of the International Resource Development  Comoration is sceptical, expressing doubt that consumers will find the service worth the money.
Optimists remain, however.
Newspaper publishers in Boston, Kansas City, Fort Worth, Baltimore and Seattle have signed preliminary agreements for joint ventures with Knight-Ridder.
Dennis Sullivan, AT&T's assistant vice president, says his company is ‘committed to the development of videotex as a future consumer communication service.
Meanwhile, a few visionaries are assessing prospects for still more extensive computerised information services.
Theodor Nelson, who works in Texas, is developing what he calls Project Xanadu.
He says this is ‘a computer system seriously intended to become…a universal instantaneous publishing system and library’.
In essence, anyone linked to the system could store information and make it available to others with suitable terminals.
So far, however, Nelson is struggling to find backers for his ideas.
Aussies buy Canadian
CANADA's Telidon viewdata system has received two big boosts.
Infomart of Toronto is to sell Telidon-based equipment to Australia the first major incursion into a market that Britain's Prestel has dominated.
The sale to Westpac Banking Corp of Sydney will provide banking and shopping services in association with Myer Emporium, Australia's largest chain store operator.
Also the Canadian Broadcasting Corporation (CBC) announced the start of its national trial for IRIS (information relayed instantly from source) the first big North American teletext trial to be operated by national bases.
The test will run until mid summer and involve several hundred homes and public locations in Toronto, Montreal and Calgary.
The Norpac decoders for the trial are expensive, at about £1600, but the price will come down considerably if the trial is a success.
Telidon is already in use in several commercial view-data systems in Canada but this is the first big broadcast operation.
The information is sent on a regular TV signal and occupies the vertical blanking-portion.
Unlike cable systems, which provide a continuous scroll of information, the IRIS system allows the viewer to select any page at random.
IRIS contains some 150 pages of information covering weather, agriculture markets prices, television schedules and a continuously updated news service staffed by CBC editors.
Microfirms jockey for a hit disc
INDUSTRIAL wrangling over a new type of small floppy disc for storing data is holding up the development of useful portable computers.
At least six types of microfloppy, as the discs are called, of varying size and format are competing for the market.
One thing the microfloppies have in common is the fact that they are all under 10 cm in diameter, making them better suited for briefcase-size computers than the existing 5¼-inch (12–5 cm) and 8-inch (20 cm) diameter floppy discs.
There the similarity ends.
Sony, which has been furiously lobbying standards organisations and other makers of disc drives (the machinery that ‘plays’ the discs) to accept its design, makes a 3-inch (7–2 cm) disc encased in a hard plastic cover.
Hitachi, with its partners Maxwell and Matsushita, has come up with a disc the same size, while a third combine headed by Seagate has opted for 3¼ inch (7–8 cm) in an attempt to head off the Japanese competition.
Up to now the Japanese have not been leaders in disc design.
From behind the Iron Curtain, a Hungarian firm called Budapest Radio Engineering has produced a 3-inch (7.2 cm) microfloppy encased in a traditional cardboard cover.
The drive for this disc is cheaper than the others at £150.
This disc is being sold in Britain, but it seems unlikely to win favour amongst large microcomputer manufacturers.
Sinclair Research has promised to produce a small magnetic storage unit for its Spectrum computer.
The device is called the Microdrive, but it has yet to go on sale, and in any case will be based on a tape rather than a disc.
The tape is said to run in a continuous loop, a method of speeding up access time to data.
This arrangement is known as a stringy floppy.
IBM too is working on a small disc which will break with tradition by being a metric size.
The company's 100 mm disc (just under 4 inches) is destined for use with new personal computers.
At the moment computer manufacturers and disc drive makers who do not want to invest in their own design are uncertain about whose bandwagon to jump on.
Hewlett-Packard, Sord, RCA and Dot have all plumped for Sony, which is generally reckoned to be ahead in the standard stakes.
Some 13 American drive manufacturers, after some hedging have also undertaken to build discs and drives to Sony's brief.
But the rest have to make up their minds soon, because portable computer owners are short of good software for their computers.
An industry standard microfloppy would encourage software companies to turn out better programs.
Arriving at a standard size for the microfloppies is only the first step towards a standard product.
Although the discs will fit any drive, this is no guarantee that they can be read by the computer.
PATENTS
ITT's digital slip
THE mighty ITT is running into legal problems with the name Digivision which it chose for its £20 million investment in an all-digital television set.
The sets, which will come on the market this winter, handle picture and sound signals in digital, rather than analogue code.
The company is busily registering Digivision as a trade name around the world, but has found that a small British firm is already using the name for electronic equipment.
Division Broadcast of Leicester has spent 10 years building up goodwill for its Digivision range of colour and monochrome display monitors for computers.
Worse still for ITT, the company is now starting to sell a Digivision television set for broadcast and television studios.
Although the sets made in Leicester do not have digital circuits, ITT would need Digivision Broadcast's permission to use the name.
This is out of the question, says British company.
The only way out now is for ITT to register a different name for its digital television sets in Britain.
The Japanese firm Trio has a similar problem.
In most countries, it sells its hi-fi equipment under the name Kenwood.
But in Britain, Thorn EMI owns the name Kenwood for electrical equipment — so Trio has to re-label all its ‘Kenwood’ equipment that it sells in Britain as ‘Trio’.
DeLorean's investment takes off
JOHN Z. DeLOREAN, facing drug charges in the US following the collapse of his Northern Ireland sports-car firm, this week asked for a reduction in bail because he is short of money.
Perhaps he can raise some cash on a patent granted last week or an application he filed in happier days.
US patent 4378658 covers the famous gull-wing door for the DeLorean car.
The patent names DeLorean as the inventor, although the rights have been assigned to DeLorean Research Ltd Partnership.
Since last October, inventors have had to pay renewal fees to keep US patents in force, so any firm interested in copying the gull-wing design will be keeping an eye on the status of the DeLorean patent.
It will also need to overcome some of the gull wing's snags — for example it is impossible to open in a crowded car park.
 Quadrophonics earns its keep at last  QUADROPHONIC sound is alive and well and making money for US inventor Wesley Ruggles thanks to a string of patents for his improvements on the old CBS SQ system.
Ruggles, who used to work for Sinclair in Britain, developed a circuit for steering sound signals into the four channels of a  quadrophonic system.
When quad discs disappeared from the shops, he licensed Dolby Laboratories to use his technology to make a surround-sound effect from cinema films with stereo soundtracks.
By a happy accident, all the Dolby sound films now available on home video tapes and discs have surroundsound buried in the soundtrack.
It's there for the taking by anyone with a suitable decoder.
Ruggles is now licensing electronics companies around the world to sell add-on quad decoders for home video.
His biggest break comes in May when Sony starts to sell Beta hi-fi video tape recorders.
These offer superior quality sound because the stereo sound is recorded in FM, along with the video waveform.
The high quality will encourage people to buy surround-sound units.
Ironically, the British patent on the system, no 1 514 162, lapsed in May 1982 because no one paid the annual renewal fees.
But the US patent, 3944 735, is still in force.
Luckily for Ruggles it was filed well before the US Patent Office began to ask inventors to pay renewal fees.
Safe memories
COMPUTER users, both professional and amateur, dread power cuts because even a short interruption can lose all the data in the machine's memory.
Businesses use battery back-ups, but high-capacity batteries are expensive, and take hours to re-charge.
Now the Precise Power Corporation of Florida, has filed a European patent application (69 568) on a power back-up system which does not rely on batteries.
Instead, in an ingenious way, it uses a flywheel generator.
The inventor says that a steel flywheel around half a metre in diameter ran store the equivalent of around 5 kilowatt hours of energy.
With this it can supply the computer with power of accurate mains frequency and voltage for several minutes as the wheel slows down.
When mains power comes back, the wheel gathers speed again in around 15 minutes.
This compares very favourably with a battery system, which can take several hours to re-charge.
Loneliness of the long-distance recorder
WESTERN ELECTRIC, the manufacturing branch of Bell Laboratories, has filed a British patent application (No 2 100 551) on a new kind of directional loudspeaker or microphone.
So-called ‘gun’ microphones are well-known tools for bugging distant conversations or picking up sound from a point source in a crowded room or studio.
Existing gun microphones, for example those made by Sennheiser of West Germany, consist of sound sensors at one end of a tube perforated like a flute.
Sound entering the tube from the side cancels out before it reaches the sensor because waves arriving through different holes are out of phase.
Traditionally, the holes have been spaced at regular intervals along the tube — but Western Electric says it has discovered that this is not the ideal approach.
The patent contains eight pages of mathematical formulae which purport to prove that a microphone will have much better directional characteristics if the holes are spaced at irregular or non-linear intervals.
Western Electric's engineers says that with their technique a tube three feet (I metre) long performs as well as conventional microphones twice the size.
The same technique could also enable a loudspeaker to pump sound into a very tightly defined area.
So some sections of an audience would hear one sound, and others something quite different.
DIY  progams 
Barry Fox
A SIMPLE but clever idea for helping people learn computing comes from Research Machines, in European application 69 522.
The student has a stock of colour-coded labels which carry computer key words (such as‘GOTO’) or numbers or letters of the alphabet.
The legends are printed in conventional type, but the labels also have the same message printed in binary code, like the bar-code price-tag on supermarket goods.
To write a computer program, the student selects appropriate labels and puts them in a tray.
In this way, the program flow is easily depicted and can easily be altered by swopping labels.
And when the program is ready, the student enters it simply by moving a light pen across the bar codes.
So there is no need to type instructions laboriously into the keyboard.
The poor world needs chemists
Developing countries will remain poor unless they can exploit natural resources to their own advantage.
To do this requires indigenous skills in chemistry
Michael Freemantle
FOUR-FIFTHS of the world's scientists and technologists work in Europe, the USSR and North America.
Roughly four-fifths of the world's people live elsewhere.
According to the Science Policy Research Unit at Sussex University, less than 1 per cent of research carried out in the developed countries has any significance for the developing world, and half that research effort is devoted to military and related activities.
In order to redress this global imbalance in scientific expertise, Third World countries need to build up their own capabilities in science, technology and especially in chemistry.
The chemist plays three essential roles in the Third World: the productive role, the protective role and the problem-solving role.
Third World countries are poor by many criteria.
They are poor in terms of their provision of health and education services, the average calorific intake of their people, their average per capita incomes, and their gross national products.
They are also poor in terms of their production and consumption of chemicals.
The United Nations Industrial Development Organisation estimates that from 1960 to 1970 developing countries increased their share of the world's chemical production from 4–7 per cent to 5–2 per cent.
In the same period, developing countries increased their share of world consumption of chemicals from 7.2 per cent to 7.6 per cent.
The average per capita consumption for all developing countries was about an eighth of the world average.
Yet many of these countries are rich in organic and mineral resources.
For example, 80 per cent of our tin and 75 per cent of our bauxite come from the Third World.
But because expertise in chemistry and related sciences and technologies is often scarce in poor countries, they cannot develop natural resources to their own advantage, and they remain dependent on experts from industrialised countries.
If the countries of the poor world are to escape from the most serious forms of dependence, and if they are to get the best social and economic benefit from their resources, it is essential that they have control over their resources.
And this is possible only if they have control over the processes that convert raw materials to products of value.
Such control requires expertise in research and development, production and quality control — and all these require skills in chemistry.
Trade between the rich world and the poor consists mainly of developing countries selling cheap raw materials for processing in industrial countries for consumption there.
What is left over often goes hack to the Third World.
President Julius Nyerere of Tanzania has said that poor countries enjoy two basic rights: ‘To sell cheap and to buy dear.’
Fertilisers provide a case in point.
About four-fifths of all phosphate raw material comes from the Third World.
Yet the Third World makes only 8 per cent of all fertiliser, and consumes about 15 per cent.
About one-fifth of the world's people live in the developed world, yet they use about 85 per cent of all fertiliser for their own crops.
A tonne of fertiliser applied to a previously unfertilised area (and most farming land in the Third World is unfertilised) can produce up to 10 extra tonnes of grain.
Previously well-fertilised areas produce no more than 3 extra tonnes.
Some countries now plan to make their own fertilisers.
For example, Indonesia and Malaysia propose to make ammonia and urea.
But developing a large chemical industry needs capital, and to run it needs energy.
For Third World countries, with limited domestic markets or limited natural resources, the development of multi-million dollar industrial projects is neither feasible nor desirable.
The alternative is a small-scale industry, based on renewable resources — but designing this requires chemical expertise too.
One of the biggest challenges to a chemist concerned with development is minimising the bad effects of development.
The chemist has a key role to play in protecting resources, health and the environment, and also preventing the ‘dumping’ in developing countries of chemicals rejected as too dangerous for use in the industrial world.
Professor Edward Watton, a consultant of chemical education for the UN Education, Science and Cultural Organisation (UNESCO), calls this the watchdog role of the chemist.
Pesticides, for example, can be directly harmful to people or have dangerous side-effects.
While the short-term effects of hazardous chemicals are relatively easy to identify, monitor and control, some chemicals enter food chains and may accumulate in living organisms, including human beings.
Their effects may not be noticed for many years.
Unfortunately, recent reports, such as A growing problem by the British charity Oxfam, have shown that multinational companies sell dangerous pesticides indiscriminately in the Third World, In some cases they are promoted through misleading advertising to largely illiterate farmers.
Yet industrialised countries either ban or tightly restrict the use of these same chemicals.
One such pesticide is aldrin.
Its suspected health hazards, such as cancer and nervous disorders, led the US government to suspend it in 1974.
The following year, 13 people died from suspected aldrin poisoning in Brazil.
Since then it has turned up in Kenyan tobacco plantations where safety precautions are impractical and widely ignored.
In Brazil alone, 208 people died between 1967 and 1979 from pesticide poisoning.
There were 3488 illnesses related to pesticides in the same period.
And because many illnesses and deaths in developing countries do not come to the attention of doctors, who are anyway ill-informed about the effects of pesticides, real casualty figures must be higher.
Another well-documented scandal has been the sale of unsuitable or dangerous medicines in the Third World.
Even safe and necessary medicines sell for up to six times as much in poor countries as they do in the country of manufacture.
One survey showed that five European drug companies were charging Indian customers on average 350 per cent the European price of their products.
The watchdog chemist should be well placed to spot abuses such as this.
Finally, the chemist has a role to play in solving problems by research and innovation.
The alleviation of poverty, sickness and hunger is the greatest goal that mankind can achieve.
In trying to attain this goal, science and technology have come to assume an unprecedented significance as tools for development.
Of course science and technology alone cannot produce complete solutions to such complex problems.
But unless there is constant effort to innovate, unless new knowledge is applied more effectively, and unless research is more explicitly directed toward the problems of those most in need, there is little hope that societies will be able to overcome the difficulties they now face.
Much of the chemical knowledge, expertise and resources needed to solve problems in the Third World already exists.
The problem is to bring resources to focus on these problems.
and to adapt knowledge to local conditions, constraints and opportunities.
Chemistry that works in cold climates does not always work in hot climates.
One of the greatest challenges is medicine.
‘Tropical diseases’ affect more than 800 million people, almost one quarter of the world's population.
More than 300 million people suffer from the parasitic infection filariasis, Cholera, yellow fever, sleeping sickness leishmaniasis and leprosy affect tens of millions of people in tropical regions.
Yet there is often no effective therapy for these diseases.
Schistosomiasis, a debilitating parasitic disease, affects some 400 million people in Africa, Asia and Latin America.
The cause is a blood fluke, the Schistosoma mansoni , which depends on an aquatic snail as an intermediate host.
The parasite multiplies inside the snail, eventually emerging as fork-tailed larvae.
These invade the human body through the skin.
Scientists have attacked the problem from every angle, by education, improving hygiene and eradicating the snail.
Now Otto Gottlieb, Professor of Chemistry at the University of Sao Paulo, and Walter Mors, a Brazilian specialist in the chemistry of natural products, have drawn attention to a possible new approach: chemicals to stop larvae penetrating the skin.
One candidate which occurs in the heartwood of a number of tropical trees, is lapachol and its derivatives.
These compounds not only prevent schistosomiasis, but also prevent abnormal cell-division.
The Brazilians are producing lapachol for oral administration as part of a drug therapy for cancer, and the authorities have approved it for clinical trials in humans.
Recent laboratory tests have shown that lapachol might also inhibit Trypanosoma cruzi , the protozoan that causes Chagas' disease.
This form of sleeping sickness affects millions of Brazilians.
Chinese scientists are doing considerable work on medicinal plants, particularly for treating malaria.
They already claim a breakthrough comparable with the development of chloroquine, the standard drug for treating malaria.
The new drug is Ching Hao Su .
It is extracted from wormwood, which according to ancient Chinese records was first used to treat malaria more than 1000 years ago.
Scientists at the Institute of Chinese Materia Medica, attached to the Academy of Traditional Chinese Medicine, first extracted the drug in 1972.
Since then, the Chinese have done an immense amount of work, discovering that its crystalline chemical structure differs from those of other antimalarial drugs.
African researchers have also been investigating extracts from medicinal plants.
In Nigeria, Donald Ekong.
Professor of Chemistry at the University of Ibadan, has investigated the fruit of Xylopia aethiopica .
The dried fruits of this plant are used in many parts of Africa in traditional obstetrics and folk medicine.
Ekong  analysed the fruits and found a new compound, xylopic acid.
The publication of the work prompted researchers in Ghana to study the substance's antimicrobial properties.
They found it was very active against two bacteria,Staphylococcus aureus and Bacillus subtilis .
and the yeast Candida albicans , which is a common agent of vaginal infections.
Recently, there has been increasing activity in the search for long-acting contraceptives.
The World Health Organisation has begun a programme to synthesise and test a large number of steroid derivatives.
It justified this effort by the limited number of such compounds available, and the lack of interest by industry.
Sixteen laboratories from 14 countries in both the developing and developed world are taking part in the project.
These laboratories have produced, analysed and screened more than 200 new substances, several of which have proved to act longer than commercial drugs.
Dr Yun-Cheung Kong at the Chinese University predicts that, within a year, scientists will have isolated the first natural contraceptive chemical.
The world's food problem is as big as its medical one.
The United Nations Food and Agriculture Organisation estimates that around 450 million people are chronically undernourished.
The World Bank, using different criteria, puts the figure at over 1000 million.
The very size of the problem effectively excludes imported solutions, so the only hope must lie within the borders of the countries that suffer the most.
The developing world has no choice but to build a self reliant system of food production.
But the improvements that have so far been effective — the so-called Green Revolution programmes — have been highly energy-intensive because of their dependence on irrigation, fertilisers and mechanisation.
Such solutions are too expensive for many poor countries.
Research must now turn toward farming strategies that conserve energy.
New areas of research include improving plants' photo synthetic efficiency, increasing their uptake of nutrients and water, and reducing farmers' dependence on synthetic fertilisers and chemical pesticides.
One target for research is the cowpea Vigna unguiculat L. Walp , an important source of plant protein in the tropics.
The yield of the plant is low because the flower buds and immature fruit frequently drop off.
At the University of Ibadan, chemists have  analysed the growth hormones in cowpeas to determine the role they play in this abscission (separation).
They have shown that immature fruits possess only growth inhibitors — abscissic acid and its metabolites — whereas more mature Fruits contain significant amounts of growth agents called gibberellins.
At the University of Ghana, Dr Samuel Sefa-Dedeh has been looking at ways of increasing the productivity of grain legumes in the tropics.
One problem with these vegetables is that they need a lot of cooking to make them digestible and palatable.
Sefa-Dedeh is looking at the hydration properties of legumes with the idea of developing a variety that takes less time.
Examinations with a scanning electron microscope show that cowpeas that have been stored for a long time readily lose protein when soaked, whereas samples stored without soaking do not lose protein.
Sefa-Dedeh is now developing a simple process to prepare a high protein food from cereals and legumes.
Another vital area of research is in energy.
The Third World is competing with inadequate resources for sources of energy that are essential to begin and maintain development.
In the long run, the Third World must develop its own, and preferably renewable, sources of energy.
Fortunately, most developing countries lie in tropical or sub-tropical zones, and are well placed to take advantage of the potential offered by directly or indirectly converting solar energy.
Existing methods are expensive and inefficient — but chemists hope for a breakthrough in at least one area of research, converting sunlight directly into chemical energy, for example.
Other hopes have centred on ethanol taking the place of petroleum — but fuel crops must not displace food.
Fuel can be obtained from plants in three ways: by fermenting plant materials into alcohol; by extracting sap from plants; or digesting waste biomass into methane.
Brazil and China are two countries leading the field of obtaining energy from biomass.
In China, some 7 million biogas plants consume human and food waste, providing gas and a nitrogen-rich sludge which makes a good fertiliser.
In India gobargas plants (gobar means cow-dung in Hindi) produce gas containing 50 per cent methane, 30 per cent CO 2 and 5 per cent hydrogen.
Suitable burners can be 60 per cent efficient — burning cow-dung cakes is only 11 per cent efficient.
In the Philippines, south of Manila, 48 anaerobic digesters process the manure of more than 1700 pigs.
The digested sludge is dried in open pits and then put back into the pigs' feed in a proportion of one part to 10.
But biogas programmes will only create new problems if their designers do not take into account local economic and social conditions (New Scientist , vol 97, p 377).
One interesting potential source of energy in the Third World is the water hyacinth and other aquatic weeds.
The weeds are a problem in many countries, including Bangladesh, India and Malaysia.
They block waterways, and hinder fishing and irrigation.
The main culprit is Eichhormia crassioes , otherwise known as the water hyacinth.
In India, up to a quarter of total cultivable waters are infested with the weed.
In some states, infestation reaches 40 per cent.
The increasing use of manures and fertilisers in farmland means that the problem is getting worse.
Past attempts to control the weed have involved mechanical harvesters, weed-eating fish and chemical herbicides.
None has been a success.
Now researchers are starting to look on the hyacinth as a resource.
The plant's high ash content has led to its use for manure, and its conversion into compost.
It has also been examined as a potential animal feed.
However, the Central Food Technological Research Institute in Mysore has shown that although the hyacinth is relatively rich in crude protein, protein extracted from it contains large quantities of unfavourable minerals.
Apart from protein.
chemists have extracted carotenes, vitamin A and hormones from the hyacinth.
They have also found growth regulating substances, which could accelerate the growth of crops while inhibiting the roots.
Scientists at the Central Mechanical Engineering Research Institute in Durgapur, India, have found that hyacinth containing 60–70 per cent moisture ferments quickly.
Under optimum conditions fermentation starts within two to three days, and gives off usable quantities of gas after 15 to 20 days.
The compost left has a high manure value.
In the final analysis, the third World will never solve its problems without developing its greatest resource: people.
Any country that wants to develop its industrial and agricultural production and the services it provides to its people must develop education in science and technology.
Generally speaking, self-sufficiency in scientific and technological expertise is a characteristic of all industrial countries, large and small.
Long-term and self-sustaining development thus requires a country to become the master of its own science and technology, and not the servant of other people's.
Early  days for international action
UNESCO, the United Nations Education, Science and Cultural Organisation, launched the International Organisation for Chemical Sciences in Development in 1981.
It has designed programmes to develop chemistry in the Third World through education and training, providing services for laboratories in developing countries, and publishing information about chemistry for development.
It has also begun a research and development programme to tackle tropical diseases.
The organisation is a community of scientists — it has no physical centre except for a small secretariat at UNESCO's headquarters in Paris.
Its programmes are run by panels of experts who operate through institutes in both industrial and developing countries.
The organisation's Chemical Synthesis Programme, launched in March 1982, aims to develop new drugs to treat parasitic diseases.
Eleven laboratories in eight countries are carrying out the first stage, synthesising new compounds for tests.
Laboratories in Africa should join the programme this year.
The organisation has also linked into UNESCO's regional networks in the chemical sciences.
One example is the Southeast Asian network, which runs seminars, workshops, training courses and exchange programmes for chemists.
Another programme, the Chemical Education Panel, is designed to create links between universities and industries in the Third World.
The aim is to mobilise academic communities in the Third World to help create and set up small-scale chemical industries.
IQ encounters with the press
The press shows great interest in intelligence and IQ tests.
However, the media present a biassed view that denies the validity of intelligence tests and deprives people from poor or deprived backgrounds a chance to develop their abilities
Richard J. Herrnstein
IN JUNE 1973, I spoke at Wellesley College, Massachusetts, to a small audience of students, visitors, and at least one reporter.
My topic was press coverage of the IQ controversy, which I thought distorted to the point of newsworthiness.
After my talk, the reporter, from The Boston Globe , asked me to substantiate my criticisms of the press.
I sent him a bundle of clippings that I thought amply documented my charge.
A few weeks later his article, putatively about my Wellesley talk, appeared.
It was 300 column-lines long, but it contained no mention of press coverage of the IQ controversy.
Instead the article, obviously based on interviews with my critics, attacked what I had said or written about IQ in other places.
The article said nothing about my answers to these critics, and nothing about the content of my actual talk at Wellesley College.
When I called him to object, the reporter said that his editors intended to invite me to write a rejoinder.
Later on, pleading a shortage of space, the reporter told me I would have no chance to answer.
When I complained to one of the newspaper's senior editors, he told me he had just rejected an article highly critical of me and ray position.
At some point, he implied, arguments about IQ, whatever their position, became tedious.
I wrote again, nevertheless, disputing his decision and his reasons for it.
A few days later, the Globe published an article by a sociologist teaching at Princeton that accused me, falsely, of racism.
Unfortunately, this encounter with the press resembles a great many others.
A few weeks before my talk at Wellesley College, I had been in New York at the invitation of Social Policy , a leftist magazine with a record of antipathy toward IQ testing and its social implications.
My condition for accepting the invitation to participate in a debate on IQ was that I be shown in advance any press release advertising the debate, for I feared being misrepresented.
The press release the editors sent me was acceptable, but I may have been the only person to get it.
Reporters from The New York Times ,The New York Post , and other media received a different press release, attributing to me racist and sexist beliefs.
In my talk, I described how my hosts had set the scene, but only The New York Post told of the counterfeit press release.
Incurably addicted to quantification, I have now searched the daily and Sunday New York Times from 1975 to November 1981 for all book reviews dealing with IQ.
Of the 15 reviews that I found, every one denigrated IQ tests, often vitriolically.
All but two of the books reviewed were opposed to testing, as far as one can tell from the reviews, and were praised for their position.
One of the exceptions was a book by Arthur Jensen, which happened also to be the only book by a trained psychometrician.
Jensen's book was panned by a philosopher with no detectable expertise in the subject.
Except for Jensen's book, none of the major works on testing written by professionals during the period was reviewed.
Most remarkably, however, the Times published no review by a trained psychometrician.
Dozens of literate psychometricians might have commented on the shallowness of the books the Times usually chooses to review.
But psychometrics is forbidden territory in the Times — books in the field are mostly unreviewed, its discoveries are unreported, and its experts are, apparently, unconsulted.
Rarely, if ever, in more than a decade, has a specialist in psychometrics published a review of a book on testing in the Times, The New York Review of Books, The New Republic , or other national publications that occasionally comment on testing.
For no other subject of public concern — not for economic policy, disarmament, welfare reform, nuclear power plants — has the professional outlook on a controversy been so shut off from a voice in the national press.
Yet, while public policy on testing may not have the immediacy of a tax cut or a nuclear accident, it ultimately affects everyone.
The one-sided debate about IQ misleads the public and trivialises the subject.
Books conceived in ignorance or misunderstanding are evaluated by reviewers who usually know even less.
For example, it is often said that testing assumes that intelligence can be quantified by a single number, as in a much-praised recent book,The Mismeasure of Man , by a  palaeontologist , Stephen Jay Gould.
To such authors and their magazine reviewers, so wrong-headed an assumption must stem from defects in the thought — and perhaps also the character — of testers.
The truth, however, is that psychometrics has plainly shown that intelligence is multidimensional: people with the same IQ may have different mixtures of abilities.
Consequently, for many purposes, tests provide profiles, not a single IQ.
The Wechsler Adult Intelligence Scale, for example, which is widely used as an IQ test, can be scored to yield a profile of 11 subscales.
Other test batteries result in still larger numbers of dimensions of intellectual variation.
To tell the public, as the critics often do, that testers assume intelligence to be one dimensional is simply to misinform.
At the same time, the public hears little about a surprising and important discovery — not an assumption — of psychometrics.
Tests show that many intellectual activities are highly correlated.
People who are good at seeing the figure ground relationships in pictures tend also to be good at sentence-completion exercises, word analogies, problem solving, and so on.
Superficially, the activities differ, but at a deeper level they converge.
Indeed, about 50 per cent of the variation from person to person in mental activities can be attributed to variations in the strength of a single attribute, which, because of the great diversity of its expression, is called g, for general intelligence.
A person's level of g has ramifications for everyday life — in school, at work, and in personal matters.
Because a standard IQ score is usually a good measure of g it efficiently tells us something important.
The structure of intelligence reveals itself spontaneously in the way people behave intellectually; it is not imposed by the assumptions of testers, as the national press would have it.
A story neglected sometimes damages the truth more than a story mistold does.
On 30 July.
1981, Richard Helier and an associate were convicted in a federal court in Madison, Wisconsin, of numerous counts of diverting institutional funds; later they were sentenced to three years in prison.
A few months later, a state court sentenced Heber to four additional years.
Heber is serving his three-year term in a federal prison in Bastrop, Texas.
Not a word about the Heber scandal has appeared in the Times .
the news weeklies,Science magazine, or on TV.
Heber was, until the scandal broke in December 1980, director of the Center on Mental Retardation and Human Development at the University of Wisconsin.
He had chaired President Kennedy's Panel on Mental Retardation in the early 1960s, and had held other responsible positions in the field of intellectual development and compensatory education.
Heber is better known, however, for his direction of the Milwaukee Project, an intensive educational and child care intervention programme supported by the federal government, in which the IQs of pre-school ghetto children were supposedly raised by more than 30 points.
When the President's Committee on Mental Retardation proclaimed in the early 1970s that the intellectual similarity of parents and children was ‘mainly because of the environment that the parents create for the young child’, the decisive evidence was Heber's Milwaukee Project.
Here, the committee claimed, was evidence that by intervening directly in the environment, education specialists could raise the IQs of ghetto children to an average of over 125 points.
These contentions in the committee's report were echoed in the press, which asserts that the environment is primarily, perhaps solely, responsible for the lower IQs of economically deprived groups.
Needless to say, the scientific community has been eager from the start to scrutinise Heber's work.
Reading about it in the popular press is no substitute for the scrutiny that follows the disclosure required by technical journals.
But more than 14 years after the Milwaukee Project started, and long after its spokesman's claims have been absorbed into the consciousness of editors, politicians, judges, school boards, parents and social theorists, as well as into countless text books and lecturers' notes, no scientific account has been published.
A few unpublished annual reports and fragmentary descriptions are all we have to weigh against the claims made by Heber and those who cite him.
From the sketchy results available, some experts think they see flaws in the study, in the sampling and testing procedures especially.
But the plain truth is that we cannot say what was really done for the children or what the results were.
The Milwukee Project broke into the national news, when S. P. Strickland implied, in the July 197 I issue of American Education , that the IQs of children in Heber's study had been raised by 33 points.
Soon thereafter,Time magazine told its readers that the Milwaukee Project ‘offers persuasive evidence that mental retardation in the offspring of mentally retarded mothers can be prevented’.
In a syndicated article that ran all over the country,The Washington Post summarised Strickland's disclosures: ‘Prof. Rick Heber's group at the University of Wisconsin may have settled once and for all the question of whether the disproportionate mental retardation of slum children is the result of heredity or environment.’
Heber's study, the Post said, ‘revealed not that mental deficiencies are passed on genetically, but that mentally retarded mothers tend to create an environment that is less conducive to mental development than that created by slum neighbours of normal intelligence,’The New York Times told its readers: ‘The Milwaukee Project, an experiment in intensive preschool education for (potentially retarded) children, has proved that they can be raised more than 30 test points higher than other children from the same environment and with the same type of mother.’
More than 10 years passed before a reporter at the Capital Times of Madison discovered that Strickland, in mid-1971, had become a stockholder and officer in a corporation set up by Heber to sell the very techniques of educational intervention that Strickland credited in his article.
But this was a minor revelation, compared with the ones that convicted Heber.
A series of Capital Times articles, from January to October 1981, paints a picture of flamboyant misconduct.
Heber and an associate, Patrick Flanigan, diverted at least $165 000 of their centre's money into personal bank deposits.
Heber's legal residence was just outside Colorado Springs, though he was paid for ‘full-time’ duties in Madison, Wisconsin.
Colleagues were quoted as saying that Heber was rarely seen on campus.
He owned hundreds of acres of land in Wisconsin, Iowa, Florida and Colorado, and was busy in the horse-raising business, which must have involved substantial time, travel and capital.
Heber's grants from the federal government for raising IQs totalled in the millions of dollars, much more than the usual federal funding level, and the results he claimed were comparably out of the ordinary.
While other workers have published careful studies showing modest, often transitory, benefits of training for IQ, Heber's projects stood as a beacon of hope for dramatic and lasting benefits, and also as the defence against those who say that IQ has proved disappointingly hard to budge.
But the national press has published nothing about the ambiguity surrounding the Milwaukee Project.
The media seem unwilling to publish anything that might challenge the certitude with which editors, politicians, judges and others insist that we know how to increase measurable intelligence or that test data ‘prove’, to use The New York Times's word, that a poor environment causes familial retardation.
The claim that IQ is heritable appears to provoke controversy at every turn.
But, as controversial as that claim may seem to readers of the national press, it is an ersatz controversy, a creation of the press itself.
In the technical literature, virtual unanimity reigns: most of the variation among individual IQs is due to variation in genes.
In public discussion, however, the idea that genes account for most variation among IQs seems not only controversial but also associated with a few men of allegedly questionable character.
To transform a scholarly consensus into something that appears the obsession of a disreputable fringe group requires more than accidental bias.
The transformation seems to involve three groups each promoting something it values: 1 The national press favours ‘sociological’ explanations of society, perhaps because when most editors and senior reporters went to school, optimism about the potentialities for social reform was high.
Insights from the growing science of human  behavioural genetics have yet to reach the top offices of the media.
Because the national press is sympathetic to social reforms that are threatened by intractable individual differences, arguments stressing the weight of genetic factors are not welcome, but arguments that minimise the significance of genetic factors are.
2 A few professors are available to provide the anti-genetic arguments.
They rarely publish their arguments in the technical literature; when they do, the arguments usually fare poorly.
They gather little or no data of their own, but instead tend toward ad hominem charges against the scholarly consensus.
Mostly, they are not psychologists: few of them have psychometric expertise.
The more extreme attacks on the heritability of IQ tend to be political, not scientific.
3 The professors just characterised could not so influence the national press without the tacit cooperation of many of the scholars who study, and the merchants who sell, tests.
An unspoken agreement grants peace and prosperity, respectively, to scholars and publishers who stay out of the public debate.
To explain the-timidity of these people, we need look no further than the example of Arthur Jensen, who, after devoting himself to the study of educationally significant individual differences, is declared by the press to be an advocate of segregated schooling for ‘poor and black children’.
Rather than try to describe to a journalist the subtle but crucial distinctions between individual and group differences, these scholars describe their results in technical journals and allow merchants to sell their products quietly.
When they are contacted by reporters following up on a story, they typically say something equivocally critical about IQ's heritability, so as to quarantine themselves from controversy and, above all, to protect-themselves from false charges of racism and elitism.
They usually do not even risk saying, as they truthfully could, that though groups may differ on the average, group differences are small relative to individual differences, and that every sociological stratum and ethnic group contains the full range of mental ability.
A familiar example is the professor who assures a reporter that scientists cannot say ‘exactly’ how large a part genes play in deter mining IQ, as if to criticise those who do assign a numerical weight to heritability.
What the professor does not tell the reporter is that, although exact values are unknown, a range of possible values, given the data, is quite well established and almost uniformly accepted by experts.
Bias in the press
Most people would have trouble conceiving of the enormity of the bias against testing in the national press.
I am willing, indeed eager, to believe that so severe a bias is typical; that most aspects of a story usually get told, because of the sheer anarchy of the national press, if for no other reason.
But after more than a decade, and scores of contacts with the press.
I can count on the fingers of one hand all the journalists I have met who are committed to telling the truth about my field as well as they can discover it, No one could organise censorship so effectively in America, so the distortions in the press must reflect countless more or less independent decisions by editors and reporters which lead to the perpetuation of a misleading and intellectually fallacious understanding of a serious scientific subject.
Tests are being banned all over the US by the courts and legislatures, something that would make sense if they were as vicious and ineffectual as the press says they are.
But because IQ is, in fact, so broadly correlated with other cognitive activities, in school and at work, it is virtually impossible to design objective measures of performance not correlated with tested intelligence, and hence subject to the same charges of ineffectuality and bias.
Competency testing in high school, civil-service examinations, job-placement examinations, college-entrance examinations, and diagnostic testing in school have, as a result, been restrained, banned, or revised so as to reduce test accuracy.
Both the increased cost and the loss in predictive validity will most hurt the poor and disadvantaged, who will be paying more for a test that is less likely to spot unfulfilled native abilities.
The more heritable a predictor is, the less it reflects the advantages or disadvantages of a student's environment, and no predictor of potential academic ability is as heritable as the scores on intelligence tests and their close correlates, the aptitude tests.
Hence selection without these tests — which means reliance on grades, conduct, or family connections — is likely to be more biased than selection without them, Many state school systems have stopped routine intelligence testing, which means that most children will lose the benefits of a proper use of their scores in individualising instruction.
These schools may be spared the risks of improper use, but schools cannot tailor programmes of study to children throughout the range of ability if ability cannot be assessed.
The restrictions on job-placement tests may be costing billions of dollars annually in lost productivity.
Judges and legislators, relying on the national press and on ‘experts’ who speak more often for political constituencies than for science, assume that the political benefits of banning tests are cheaply won.
If tests measure nothing important about individuals, as the press often says, then they are a needless barrier to achieving social and economic equality.
But if, as the data say, they measure socially significant individual differences, the cost of banning them is paid mainly by those who might have benefited from better job placement and school curricula.
Historically, testing arose out of the desire to break down arbitrary barriers of class, race and nationality; it was part of the democratisation of society at the turn of the century.
Better than any other instrument available to teachers, then or now, tests would cut through the unpredictable circumstances of a child's cultural background to the relatively stable aptitudes on which education builds.
The early testers believed that tests would open doors to disadvantaged people, not close them.
And that, by and large, is what tests have done, enabling millions of people from poor or deprived backgrounds to develop their abilities better than the circumstances of birth would otherwise have allowed.
As tests disappear, displaced by political requirements imposed on school and workplace, arbitrary barriers return; these barriers are new ones, perhaps, but no less unproductive and unjust.
Besides politicising education and employment, the antitesting bias inhibits the search for useful knowledge about human differences Power pressures — even legal restraints — inhibit analyses showing how individual traits correlate with education and job training, with the conditions of learning and working, and with the quality of people's personal experience.
Instead of learning more about how people function in a world whose intellectual demands are growing, the US has all but outlawed the data, encouraged to do so by what it encounters in the national press.
REVIEW
Leopard puts psychologists in the spotlight
States of mind edited by Jonathan Miller,BBC, pp 320 £9.95 
John Cohen
THIS record of televised interviews leaves the impression that psychology bears an uncanny resemblance to its next of kin, the deity, described in a venerable formula as ‘a sphere whose centre is everywhere and whose circumference nowhere’.
An apotheosis of ‘states of mind’ is established by the gifted editor from whose versatile quill we expect much; and ‘expect’, we learn to our delight, is itself a ‘state of mind’.
Jonathan Miller has chosen 15 interlocutors and with them he debates a range of psychological and allied issues.
These interlocutors, one assumes, were selected at random, for logical coherence in the set as a whole is wanting.
Three are British, and all the rest American (including a few immigrants to the US).
If the reader is happy to meet no Albanian, he will be overjoyed by the absence of anyone from India, China or the USSR, for no one in his right mind could imagine that those who dwell in these alien steppes and deserts could either enjoy ‘states of mind’ or have anything worth saying about them.
The topics as well seem randomly selected, which makes it all fair and square.
Dr Miller is a protean leopard; he can vary his spots.
He re-enacts the infant-breast scenarios depicted to him, à la ed Melanie Klein, by Hanna Segal.
In one session he blissfully sucks the cerebral nipples of his guru; in another, he becomes restive, greedily biting into his guru's cortex and association areas, challenging them to yield more than milk; in a third, offering alternative views of his own, the guru's breast seems a source of hostility; in a fourth (particularly in his contest with Dr Thomas Myth-of-Mental-Illness Szasz), his id erupts, and he threatens to tear the errant guru limb from limb.
He concedes that it is not possible, within the compass available, to give ‘a representative sample’ of what all psychology is about, and readers will share his own frustrations at the omissions.
When it comes to psychoanalysis, his ambivalence betrays itself.
For he thinks it is scientifically debatable, ambiguous and controversial, and yet it is convincing, because ‘in some puzzling way it is true’.
He harps on our making guesses ‘on the basis of probabilities in the world one knows’, but on the rich universe of subjective probabilities his silence is deafening.
The series is inaugurated by a chat with George Miller.
Much of what the latter says about information theory, feedback and signal detection is old hat, but he generously leaves us one or two bones to pick.
He lumps the insane and children with animals as incapable of introspection.
Few would argue about animals, but many would not agree that this generalisation is true of all the insane.
Has he read Howard Ogdon's The Kingdom of the Lost ?
As to children, he seems led astray.
A youngster of 3½ years once replied to my question ‘What do you do when you think?’,
‘If someone tells you something hard which you don't know, you have to think what is it.
If you don't know what to say, you have to think…you just stand quietly and don't say nothing and something comes into your brain.’
I've yet to come across one of that happy breed, the cognitive psychologists, who has improved on this.
The same interlocutor omits to mention semantic, inventive, aesthetic and especially a measure of subjective information, which cannot be equated with Claud Shannon's binary units.
Nor does he appear to be aware of the idea of subjective redundancy.
At the same time, the editor fails to remark that when dealing with military servomechanisms, you can speak of the future position of the gun and compare that with ‘future’ human purposes.
But my purposes exist now ; they are in the present, and bear no likeness to the mismatch between the direction of a gun barrel and its target.
The lack of any consideration of memory is disturbing.
After all, Mnemosyne is, by Zeus, mother of the muses and goddess of Memory, which is at the heart of pretty well all states of mind.
It is the organ of perception , for if we remembered nothing we could perceive nothing.
Our existence would lack even an Adamic fig leaf of meaning if we ignored our capacity to glide over our past, coalescing and juxtaposing our memories, irrespective of their temporal spatial or circumstantial labels.
It is thus that our imaginative faculty is kindled, uniting the flames of cognition and emotion.
All, though much preoccupied with perception, are silent on the paradox in perceiving which results from a chicken-egg situation, namely, that we can only perceive what we attend to, and we can only attend to what we perceive.
The Shergar of the stud is Norman Geschwind, who discourses superbly on the organisation and uses of the brain.
Clifford Geertz follows with a luminous sketch of anthropological psychology, while Richard Gregory gallops in third with his now familiar but lively inferences from illusions.
Jerry Fodor, who thrives on ‘propositional attitudes’ should try to deploy them in rewriting ‘Tyger, Tyger burning bright.’
Notwithstanding, the upshot is instructive, alike to those who, in their adoration of the psychical Cybele, Queen of the Inexact Sciences, and to those, more virile, who do not recognise her  Sovereignty .
A lack of biological imagination
The great evolution mystery by Gordon Rattray Taylor,Secker & Warburg, pp 277, £8.95 
Mark Ridley
‘TO SUPPOSE that the eye, with all its inimitable contrivances…could have been formed by natural selection, seems, I freely confess, absurd in the highest degree.’
The words are Charles Darwin's; but the form of the argument is followed, through dozens of instances, by Gordon Rattray Taylor.
He does not confine himself to ‘organs of extreme perfection’, but also describes, as he believes, maladaptive overdevelopments (Irish elk), nonadaptive organs, useless incipient stages.
But he treats them all the same way.
He first describes his example (which is usually taken from the evolution of the vertebrates) in some detail: then he asserts that it is impossible that natural selection could have produced it.
To rub the message in he may add, for good measure, that it is also inconceivable and unthinkable; and he sometimes quotes from an authority such as Richard Goldschmidt who also assures us that it is impossible and unimaginable.
For none of the many examples does Taylor give us any account of the appropriate Darwinian thinking.
Indeed, he does not seem to be aware that it exists: ‘All Darwinism has to say about such miracles is that they are due to chance.’
But it does exist, and it has a lot more to say.
Much of it requires mental effort to work through, effort which Taylor should have put in before writing off Darwinian explanations as ‘pathetic’ or ‘ludicrous.
The difficulty with natural selection which Taylor raises repeatedly is that it explains evolution by chance.
His misunderstanding is familiar and elementary.
He has confused the random process of variation with the directing process of natural selection.
So he finds puzzling ‘The existence of trends which continue for millions of years — for if evolution is a random process one would expect purely random changes of direction’.
One would, if it were; but it is not, so one should not.
If chance is not enough, of course, we will need a new science of heredity.
Taylor can describe it only in general outline.
It will be based on Lamarclusm and other inarticulated ‘internal’ factors, with random Mendelian segregation being cut down to size.
So we have a chapter on Lamarckism with the, by now(since Arthur Koestler's The Case of the Midwife Toad)customary, tears-and-outrage tragedy of Kammerer (’ a man of great integrity and charm’) suspected by William Bateson (’not a lovable character’) and exposed by the ‘ruffian’G. K. Noble.
Gordon Rattray Taylor died (in 1981) before revising this book.
But revision would not have saved him from his misunderstandings, his reluctance to think, his complete lack of biological imagination.
Critics of Darwin who quote that passage with which I started do not usually run it on.
The next sentence begins ‘Yet reason tells me…’ and ends ‘…then the difficulty of believing that a perfect and complex eye could be formed by natural selection, though insuperable by our imagination, can hardly be considered real.’
Natural selection enriches and disciplines the imagination by the reasoning faculty.
If you leave it out, you are left with a great evolution mystery.
Cells have their own lives to lead
The hedonistic neuron by A. Harry Klopf,Hemisphere * pp 140, $19–95 
Hierarchy, perspectives for ecological complexity by T. F. H. Allen and Thomas B. Starr,Chicago UP,pp 310, £19–25 
Tony Durham
THERE IS a paranoiac frisson , and more than a little insight, to be had from the thought that we humans are not really in control of our own fates, and that someone or something is using us.
Richard Dawkins put the selfish genes in command; and reduced people (for the sake of the argument, at least) to the status of throwaway survival vehicles for immortal DNA.
Jim Lovelock accorded autonomy to the planet as a whole, with humans taking the humble role of Gaia's brain cells.
Douglas Adams satirised all such ideas with his fantasy that Earth, people and the whole show are merely part of a huge experiment run by mice.
The idea is infectious.
In The Hedonistic Neuron , psychology is rewritten from the point of view of the individual brain cell.
This is not just a publishing fad.
Science is rethinking the relationship of part to whole, and of observer to observed.
The concept of purpose is creeping back into scientific explanations.
It is no longer outrageous to ascribe purposive behaviour to a gene, a neurone or a planet.
This is a modest and fairly technical book, but its central idea is likely to escape and assume a life of its own.
Good ideas are like that.
The funloving brain cell is a clever little brute.
It learns to do things which bring rewards.
The hypothesis is that pleasure, for a neurone, is  excitation .
Each neurone learns to manipulate its neighbours to deliver the maximum input to its excitatory synapses and the minimum to its inhibitory synapses.
Neurones seek thrills and novelty, not peace and quiet.
In a sense, creativity is built into the brain at ground level.
The  homeostat , the automaton concerned only to preserve its own equilibrium, was never a convincing model of human behaviour.
Many observed features of learning and memory plop straight out of the theory.
There is fair support from neurophysiology, too, but not much sign of experiments designed to test this model against other plausible models of the neurone.
The theory simply begs to be tested by computer simulation.
Klopf accuses artificial intelligence researchers of building only the top (logical, verbal) storey of a 1000-storey building.
He should admit that his efforts have supplied few of the first 999 floors.
The middle levels, where scurrying impulses form them selves into thoughts and feelings, are still to be filled.
This is a vast, exciting and perhaps quixotic project.
The skyscraper metaphor is apt, for our only hope to understanding such complexity is with a hierarchical model.
Minds hierarchies, and an attempt to reinstate the concept of purpose in science: it all raises echoes of Arthur Koestler's The Ghost in the Machine .
Koestler's central idea was that things fall naturally into treelike hierarchies of parts and wholes, systems and subsystems.
Control rests neither with the whole nor with isolated parts Each level is constrained by those above it and autonomous with respect to those below.
In this view it is quite natural to find purposiveness at the level of the neurone or of the gene.
T. F. H. Allen and Thomas Starr openly express their debt to Koestler, in a book that superbly demonstrates how yesterday's blue-sky philosophical idea can become today's razor-sharp tool of scientific practice.
Applied to ecology, hierarchy theory throws new light on old knowledge, it offers new models to test, and it tells ecologists where to look for phenomena that might not be apparent to human senses.
The slowest processes come at the top of the hierarchy, and provide the environment for faster processes.
We come to the living world with senses adapted to primate life, and we may miss things that are too fast, too slow, too big or too small.
The problem of the observer is as crucial in biology as it is in physics.
Taking their cue from quantum physics, Allen and Starr theorise only about what can be known, not about what ‘exists’.
It is a moot point whether hierarchies exist outside our own thought processes.
But as concepts, their power is undeniable.
Allen studied the Neolithic revolution and became convinced that it was not a matter of someone having a bright idea and beginning to plant seeds.
That was too microscopic and parochial a theory.
Allen hopped a couple of rungs up the hierarchical ladder, examined the interaction of the social structure and crop genetics, and found a model for the invention of agriculture.
When people were already living in towns and storing grain from year to year, the crops hit upon a new strategy for propagating their own kind.
They made themselves good to eat.
In a sense, they domesticated us.
There are many other beautiful examples, from the origin of life to the collapse of the Tacoma Narrows bridge.
It is a splendid interdisciplinary book, firmly rooted in ecology but alive to developments in physics, mathematics and computing.
‘Perhaps we read like men possessed,’ say the authors in their final paragraph.
‘We feel this book was written because the times imposed it upon the authors.
There is a paradigm shift occurring in biology, and this book is a manifestation of that change.’
They need not worry.
That paradigm shift is real.
The individual scientist is not diminished by being part of the larger, purposive system called Science.
Electrical conductor of a nation
Benjamin Franklin: a biography by Ronald W. Clark,Weidenfeld & Nicolson pp 530 £18.50 
John Moss
THIS splendid biography is thoroughly recommended as a perceptive and lively life of the great scientist, printer and politician.
Benjamin Franklin s contribution to the identification of electricity in the form of lightning is well known, but not so well appreciated are the subtlety and simplicity of his apparatus and experiments.
It is no wonder that he was elected fellow of the Royal Society in 1756, three years after the society awarded him the Copley Medal.
Incidentally he was elected without having to pay any fees.
But, of course, Franklin was more than an experimenter.
He was a printer who used his skill for the production of a remarkable flow of opinion-forming pamphlets, and a founding father of the United States.
His experiments on the nature of lightning were truly pioneering, starting at Marly in France where a dragoon was persuaded to pick up a long brass wire inside a glass bottle which acted as an insulator.
He was also interested in advancing astronomy, observing an eclipse of the Moon from Philadelphia and the transit of Mercury.
But many of his inventions were down to earth, as witness the Franklin stove which was a real energy saver, and provided comfort to the whole room, an early example of the ‘Save It’ campaign.
He was an independent spirit and when commissioned to buy new type for the Pennsylvania Gazette .
he turned down Baskerville, then renowned as the greatest printer in England, and bought traditional Caslon instead.
However, he later recognised John Baskerville's genius and became his  enthusiastic supporter.
Franklin even added point (excuse me) to the argument about whether pointed or knobbed lightning rods may be preferable when he showed that blunted ones acted at a greater distance.
This controversy.
with all its attendant  trivialities , occupied the Royal Society for some time and even King George III was drawn into the argument as is shown by the contemporary jingle:
While you, great George, for knowledge hunt, And sharp conductor change for blunt The nation's out of joint: Franklin a wiser course pursues And all your thunder useless views, By keeping to the point.
Franklin's pamphlets contributed much to the struggle for American independence with their sharp titles such as Poor Richard, The Drinker's Dictionary and Plain Truth , titles reminiscent of his great contemporary Tom Paine.
To have made one such contribution to mankind would have been remarkable, but to have also contributed greatly to the early understanding of electricity, is outstanding.
One small omission that might have been included is Franklin's experiments on the difference in heat absorption of substances of varying colour.
He must have discussed this phenomenon with Robert Boyle on a number of occasions.
Franklin's experiments on the absorption of solar radiation by bodies of different colours were characteristically practical and thorough.
Indeed some of the results are of practical value today in the choice of white clothes in a sunny climate and black in a dull one.
Again his meteorological observations resulted in the practical theory of how tropical storms arise in the Gulf of Mexico and are of a similar nature, if vastly greater in scale, to water spouts and whirlwinds.
Surprisingly this is the first full-length biography of Franklin for more than 40 years, and Ronald Clark's is  particularly valuable because it puts the subject in historical context.
Personally, I would like to see more direct quotations from Franklin's pamphlets and writings which had such great influence in the US.
While they are carefully listed, there is not enough to get their flavour Franklin lived many lives: that of scientist investigating electrical phenomena for the first time, lobbyist for the new born United States in London, and gun runner for the infant
Specimens from the Pleistocene
Pleistocene vertebrates in the British Isles by A. J. Stuart,Longman, pp 212, £16.50 
Michael Benton
THE STORY of vertebrate life in Britain over the past 2 million years or so is fascinating.
During the Pleistocene period, Britain's climate ranged from warm-temperate to very cold during the several advances of ice sheets.
The vertebrates found from that period include mammoth and other extinct elephants, extinct rhino, hippopotamus, giant deer, lion, spotted hyaena, tortoise and macaque (from the ‘monkey gravel’ of West Runton, Norfolk — where else?).
Tony Stuart's book offers an attractive and original review of our knowledge of British Pleistocene vertebrates, with an excellent 35-page summary of specimens known.
Stuart indicates the relative abundance of each animal, and the parts usually found; with useful illustrations for the identification of odd bones or teeth.
Another important section is concerned with the present-day distribution and ecology of the birds and mammals that have survived from the British Pleistocene.
This information is essential for interpreting the fossil assemblages, but Stuart rightly stresses the lack of work on cold-blooded vertebrates fishes, amphibians and reptiles.
In North America these groups have received a great deal of attention, and provided valuable  infomation on the Pleistocene climate.
The next large chunk of the book is a chronological review of the main faunas of each stage of the Pleistocene, and this includes several useful tabulations of pollen-dated occurrences of mammals.
This detailed survey is peppered with information on how the faunas came to be collected — often by amateurs.
In a brief chapter, Stuart describes the occurrence of man and his influence on the British fauna, right up to the 20th century, and the book closes with some examples of evolution observed in Pleistocene fossil mammals.
Minor criticisms include the absence of a glossary and the occasional introduction of unexplained technical terms, such as brachyodont and hypsodont.
More importantly, I did not find the sections on taxonomy and evolution wholly satisfactory.
In the former, Stuart hints at how we can compare Pleistocene species with living species, but some more details are necessary, and certainly references to one or two works on statistical morphometric techniques.
In the chapter on evolution, Stuart rightly stresses the importance of a good stratigraphic and geographic record for studies of changes over time between species.
His view, however, is implicitly gradualist —‘a temporal progressive sequence of changes within an evolutionary lineage’— and there is no indication of the extensive  palaeontological literature of the past decade suggesting an alternative punctuational view, that species may evolve relatively rapidly and then remain unchanged for long periods.
Neither viewpoint is absolute, but some reference should have been made to several recent studies of evolutionary rates in the Pleistocene mammals of Europe.
The book is well written and produced, many of the figures are new, and they are generally good.
Despite the shortcomings, Tony Stuart's book more than succeeds as an up-to-date review of the distribution and significance of British Pleistocene vertebrates.
Flesh eaters at South Kensington
Plants of prey Natural History Museum, London, 21 April–3 June 
Julin Grollman
A LOFTY side hall of the Natural History Museum currently contains a trap.
Between now and 3 June, the unwary visitor may well fall into it.
The trap takes the form of an apparently harmless conservatory which would look more at home on a patio than adjacent to the stuffed gorillas.
A glance upward might warn the visitor of impending danger For atop this greenhouse is a pair of grotesque ‘triffids’.
A short passage leads into the main chamber, and the visitor is trapped among the plants of prey Traps are the main theme of this exhibition.
The plants featured, however, embrace nothing larger than a fly, so humans are quite safe.
The triffids are on loan from the BBC, courtesy of its Day of the Triffids .
They symbolise the extreme development of the flesh-eating plant theme.
The displays demonstrate the reality, provide live plants to look at, and assure us that nothing larger than an unfortunate lizard or rat makes it into the green traps.
Such substantial additions to the diet happen only among the largest specimens, such as Nepenthics rajah of tropical Borneo, whose pitcher could encompass a rugby ball.
This grant is not on display, but there is a smaller relative,N. gracilis , described as being ‘very common along the roadsides in Borneo’.
These pitcher plants demonstrate the pitfall trap: once inside, the slippery slope out is impossible to climb.
Other mechanisms for trapping prey have similarly descriptive names: snap, suction, lobster pot, flypaper, loop.
The trap theme is designed to focus visitors' attention on the plants to see how the trap works (the traps are all modified leaves).
The plants are displayed in sealed glass cases.
They stand in water, which corresponds to their boggy native habitats, and are given  additional light overnight to supplement the illumination from fluorescent strips; such are the practicalities of plant-of-prey nurture.
They are also fed intermittently with stunned insects.
Logistics prevent this being a public attraction.
Apart from the nematode trapping fungus (too small to display) there are living examples of all the traps.
One plant did elude the museum's staff Genlisea , a filamentous aquatic plant with an ingenious ‘mouth, gullet, stomach’ arrangement can be seen only in diagram.
Its absence is probably not a great loss to the exhibit, but the museum's efforts to obtain a sample involved a trip to Munich!
The exhibition aims to familiarise visitors with these fascinating plants.
In addition, each Sunday the Carnivorous Plant Society will hold a workshop on growing them.
Cultivation is apparently fairly easy and details of where to get the plants are available.
They dislike tap water (too alkaline) and work well as insect traps.
The venus fly-trap Dionaea muscipula)is a popular subject — a good thing as it's a unique species and fairly uncommon in its native Carolinas.
A discussion I overheard concerned the merits of an airing cupboard as winter quarters for fly-traps.
When aid does not come to the rescue
AIDS (acquired immune deficiency syndrome) was first called GRID (gay-related immune deficiency) and, by homosexuals themselves when they thought they had a monopoly of it, the Gay Plague.
This collapse of the immune system which results in bizarre killer infections or rapidly spreading cancers had to be renamed when it was found in other groups — heroin addicts, haemophiliacs, Haitians, even babies.
In America it is an epidemic.
There are 10 times as many men with AIDS every two years, an exponential growth which will obviously continue until a cure is found.
BBC2's Horizon, in The killer in the village (25 April), traced its spread across America and the efforts of the Center for Communicable Diseases (CCD) in Atlanta, Georgia and others to isolate the cause and so enable work to proceed to a cure.
The AIDS toll so far is more than 450 dead in America among 1300-plus cases with severe secondary diseases.
It was no surprise, therefore, to learn of the alarm in the colonies of homosexuals, who are by far the largest group affected, in New York, San Francisco and Los Angeles.
Indeed, in view of the spread of the disease, it seemed somewhat unnecessary for Horizon to choose a title relating solely to Greenwich Village.
There is some difficulty in defining the disease and only patients with serious secondary diseases are included in the ‘official’ figures.
Most have pneumocystic carinii pneumonia (a parasitic form) or a form of cancer.
It is believed that were the present definition broadened, the number affected would be between 10 000 and 100 000, mostly male.
The main cancer that affects AIDS sufferers is Kaposi's sarcoma, not previously noted among white young menthe average age of the victims is 35 — which manifests itself in purplish spots that neither itch nor hurt but don't clear up.
It has been, until AIDS, a slowly developing cancer, rarely fatal.
In the case of AIDS sufferers, it develops rapidly and is more lethal.
Other infections strike resulting from a motley bunch of bacteria — some carried by cats, birds, or cattle — which normally cause humans little problem.
Apart from physical suffering, AIDS causes its victims economic hardship.
It makes it impossible for them to pursue their careers and the average hospital bill that faces them often exceeds $ 100 000.
They can be prey to several diseases at once, a possibility that led one to describe himself as ‘a walking time bomb’.
The researchers are looking for a transmissible agent and the search is urgent.
‘It's quite likely that for every case of reported illness there are ten to a hundred more,’ said Jim Curran, the AIDS task force chairman at the CCD, who was seconded to the job for three months in 1981 and is now permanent The incubation period is unusually long — possibly as much as three years.
The question of what kind of inroad AIDS is making in Britain became more insistent as the programme, written and produced by Alec Nishett, progressed.
The homosexual population, among which it is so prevalent.
is highly mobile internationally as well in America.
In this respect, Horizon's otherwise gripping report was disappointing.
Apparently there have been cases here but Horizon was not forthcoming on these and, apparently, the DHSS is not forthcoming either.
What seemed to be needed was a statement from medical authority.
The absence of it was a letdown.
One hears jocular allusions to AIDS in circles which would class themselves sophisticated but from this programme, it is clear that the early theory that AIDS was some kind of divine blight on homosexuals, is already discounted.
The discovery of it outside the groups identified as apparent risks can only give cause for concern.
The world, after all is something of a village now, too.
FORUM
Politics, parks and the plough
Ian Brotherton sees the much mooted conflict between conservation and agriculture as more imaginary than real
LANDSCAPE CHANGE in our national parks is largely determined by what the farmer does.
And whether he changes his system, ploughs his moorland, reseeds his pasture, increases his stocking rate, drains his bog and fertilises his meadow is strongly influenced by the advice and grant-aid he receives from the Ministry of Agriculture, Fisheries and Food (MAFF).
The farmer, the ministry and the Common Agricultural Policy (CAP) are the moulders or the landscape which it is the prime duty of the national park authority to protect and enhance.
The park authority itself has few powers over agricultural change — control of the largest and a few other farm buildings but little else — and must operate by giving counter advice and offering compensation to the Farmer who proposes an action that is seen to be damaging.
But incredibly, up until 1980, the park authorities were effectively denied even this chance (except for moorland conversion) because the MAFF refused to divulge details of grant applications on the grounds that they would lose the farmer's confidence.
A considerable step forward came, in August 1980, when Michael Heseltine, then Secretary of State for the Environment, announced that things would be done differently, at least in the parks and certain other designated areas.
From 1 October, 1980, applications to the MAFF for agricultural capital and development grants were to be notified to the park authority; and since then the park authorities have at least had a chance of commenting and.
if necessary, negotiating on the farmers' proposals.
As a result of the new scheme, some 3000, or so applications are notified to the 10 park authorities each year.
In the first year of operation 87 per cent of the applications were agreed by the parks without question and a further 12 per cent following modification.
Rarely, it seems, is a park committee and farmer unable to reach agreement.
But it has happened and where the park committee wishes to press its objection, the Secretary of State for Agriculture (or Wales) must then decide whether a grant is to be paid (the farmer is of course free to proceed without any grant aid if he wishes).
These difficult cases are likely to be crucially important in terms of their landscape impact — as in the scheme to improve 125 hectares of mountain land near the summit of Fan Frynych in the Brecon Beacons.
The scheme was heralded as a triumph for the notification system since the Secretary of State for Wales upheld the park committee's objection.
But these cases are rare.
Rapid agreement by the park committees and boards is the norm: the majority of proposals are approved without question: and in most other cases, negotiations lead to the agreed retention or addition of Features of conservation value.
We now have a system whereby the park authority not only learns of the farmers' proposals in advance but also invariably ‘approves’ the proposals.
Undoubtedly the notification system introduced in 1980 was a considerable step forward.
But there have been other trends since 1980 which arguably move things backwards, not one step but two.
It is here that the story of the park and the plough becomes engulfed in politics.
For  political influences since 1980 have markedly changed the character of the park committees and boards.
The number of members experienced in the park purposes has been considerably reduced (the first step back) while the number experienced in farming and landowning has been markedly increased (the second).
As a result, farming interests now dominate many of the park committees and there must be doubt as to whether they are able to give proper consideration and weight to the park purposes when agricultural applications are being considered.
To see exactly what has happened, it is necessary to look in detail at the composition of the park committees.
Two thirds of the committee members (committee sizes range from 18 to 33) are appointed by the county and district councils that have land within the particular park: their concerns are essentially local.
The remaining members arc appointed by the Secretary of Slate for the Environment (For the seven English parks; for Wales, For the three Welsh parks).
Their job, according to official sources of advice, is to present and to take account of the wider national purposes for which the parks have been designated.
There are farmers, there are conservationists and there are farmer-conservationists among both the groups appointed by council and by minister.
What determines the balance between the two — farmers and conservationists — is the politics of the appointing council or minister.
Farming interests always tend to dominate the groups appointed by the councils.
A recent estimate suggests that members with farming and related experience are twice as numerous as those experienced in the park purposes.
But the farming dominance is particularly marked in seven of the parks: the four controlled by Conservative councils (Dartmoor.
Exmoor.
N. North York Moors and Yorkshire Dales) and the three controlled by independent councils (Brecon Beacons, Pembrokeshire Coast and Snowdonia).
Indeed, the proportion of members with farming experience appointed by these councils may approach one in two, while the proportion appointed by Labour councils (which currently control the Lake District, Northumberland and Peak District) is nearer 1 in 20.
But it is of course among the ministerial appointees that the park purposes have traditionally found their champions, as indeed was the legislation's intent.
Thus, when the parks were reconstituted at local government re-organisation in April 1974' experience of the park purposes outweighed experience of farming by about two to one among the minister's appointees.
Not any more.
Today fewer have occupational experience related to the park purposes (down 30 per cent); and fewer still are involved with bodies like the Council for the Protection of Rural England, the County Conservation Trust, the Ramblers' Association and the Youth Hostels' Association, whose members have traditionally supported and lobbied for the parks (down 60 per cent ).
Viewed overall, only half the minister's members are now regarded as experienced in the park purposes they are appointed to present.
In contrast, the number of farmers, foresters and landowners has increased considerably in the appointments made by Conservative ministers since 1980.
Half the 58 members appointed since then have interests in farming forestry or landowning; activities which provide the main occupation for 23 of them.
As a consequence, the number of ministerial appointees with farming experience has increased by over half since 1980 so that farming interests now predominate among the ministerially appointed members, as well as among the members appointed by the councils.
The current imbalance between farming and conservation interests is most marked in the seven parks controlled locally by Conservative or Independent councils; and it is particularly great at present because the Conservative administration returned in May 1979 has shown preference for farmers in its appointments.
Many of the farmers on the committees, whether council or ministerial appointees, are conservationists too and it is perhaps important to stress that it is not the quality of the individuals involved that is of concern.
Rather it is the quality of representation resulting from the overall balance of interests.
That balance is biased to the degree that an unknowing observer might well conclude that it was not conservation nor recreation but agriculture that was the primary concern of many of the park committees.
It is not perhaps surprising that concern over the handling of agricultural matters has been expressed from a number of parks including Exmoor and the North York Moors where pressures for reclamation have been, and perhaps still are, considerable.
And while it may be too easy to interpret some of the recent decisions on moorland ploughing in terms of political influences and the ideologies and interests that now dominate the parks, inevitably they raise doubts as to the adequacy of the testing of the issues involved.
The concerns that have been expressed outside the park committees need to be fully voiced within them if the committees are to decide in the best interests of the parks and their purposes, and if they are to be seen to do so.
The introduction of the notification scheme by the present government was a significant step forward for the parks.
But there are two ways of interpreting the high level of agreements being reached between farmer and park authority.
One is to conclude that all is well.
Doing that seems to imply that the much mooted conflict between conservation and agriculture is more imaginary than real.
The other is to look at those who are agreeing the proposals.
Doing that, it would be difficult to ignore the recent double step backwards which has left the farming interests dominant on many of the park committees.
Research is research
Bernard Dixon investigates some outbreaks of word abuse
ITEM: The telephone rang and a distant female voice said, ‘Bernard Dixon?’
‘Yes.’
‘Ah, good, I wonder whether you can help me.
I'm wanting to know a little more about immunity.’
‘Have you tried a good textbook?’
‘Well, no.
Actually somebody gave me your name…
. Is it possible for you to give me a few more names — of some people who are working in that field?’
‘Why do you need this information?
And without being unfriendly, who are you anyway?’
‘Ah, yes, well, I work for a television company.
I'm a researcher.’
DEFINITION: Research is ringing someone up and asking them for other people's names.
ITEM: I was in the editorial offices of a magazine in New York City, and overheard a staff writer discussing a forthcoming feature with one of his colleagues.
‘I think we had better ring Wadkins, don't you, just to check out these facts?’
‘Yes, OK,’ came the reply, ‘I'll do that while you're having lunch.
Have you gotten Wadkins's number?’
‘No, sorry, I haven't researched that yet.’
DEFINITION: Research is consulting various telephone directories.
ITEM: A television discussion was under way on the subject of comedy.
The studio panel included two professional funny men, who were exchanging anecdotes about Tony Hancock, Max Miller, and other stars of yesterday.
One of the two argued that there were no universal jokes — that a comic's material must always be adapted to the audience.
The other disagreed.
‘Look,’ he said, ‘I was in Vegas last year and one night I researched this very point….’
DEFINITION: Research is telling stories and seeing which ones people find witty.
Now this sort of thing bothers me.
When think of research, the calibre of names that come to mind are those of Charles Darwin and Isaac Newton, Gregor Mendel and Albert Einstein, Andrew Schally and Roger Guillemin, Peter Medawar and Francis Crick, John Vane and Aaron Klug.
Because while various reference books provide various definitions of the word, they all agree that research is hard work, which extends the boundaries of knowledge.
It may be ‘endeavour to discover new or collate old facts’(The Concise Oxford Dictionary), or ‘scientific or scholarly enquiry’(Longman New Universal Dictionary), or merely ‘advanced study of a subject, so as to learn new facts’(Longman New Generation Dictionary).
But none of the assiduous folk who compiled these guides to English usage was prepared to contemplate the opinion that research is typified by the act of locating a person's telephone number.
Of course, one must not be stuffy.
There is, after all, a perfectly justifiable use of the term ‘historical research’.
Someone who, through years of toil among parish registers, discerns and for the first time explains the minutiae of demographic change in a northern mill town, is clearly doing that sort of thing.
Even ‘library research’ usually encompasses some element of newness — the researcher concerned making intellectual links which were not apparent before.
The essence of research, surely, must be the revelation of previously unknown facts.
And for my money that does not include such activities as flipping through Who's Who , taking a telephone directory off the shelf, or getting hold of a mate's mate on the blower.
Oh, come on (I hear someone opining), you're being far, far too precious, elitist, and boring.
Do you mean that only Nobel laureates and their peers can lay claim to the hallowed occupation of research?
Not at all.
The house of science contains many rooms, whose occupants operate in various ways.
I well remember, for example, friends numbered among Professor (now Sir) James Baddiley's cohorts, all of them bent to the task of unscrambling the structures of different chunks of bacterial cell wall.
Droves of PhD students took part in that enterprise.
They took their various allocated strains of Staphylococcus aureus or whatever, applied standard methods, and came up with a particular teichoic acid or teichoic acid-like polymer.
The doctoral theses that emerged from these labours were remarkably similar, in introduction, methods, and even results.
As one wag said to me at the time, ‘Only R is different’.
In other words, Baddiley's apprentice chemists served their time by performing countless, demanding, routine experiments and then generating molecular shapes which differed very little from one to another.
Nothing here to compare with the vision of a Leonardo da Vinci or the breathtaking intellectual feats of an Einstein.
No one pretended any such thing.
This was typical ‘normal science’— the very necessary filling-in operation which had to follow the categorically original discoveries made a few years earlier by the chief.
But it is equally clear that both the discovery of the first teichoic acid and those systematic endeavours afterwards are properly described as ‘research’.
And of course that epithet should not be restricted to science; it is even possible to tolerate its importation into fields such as theology, and street walking in Darlington.
The question is: when does use of the term become an abuse of language?
Some years ago, walking along a riverside with a Pakistani biochemist, I picked up a flat stone and bounced it across the water.
‘We call it Ducks and Drakes,’ I said.
‘Yes, we do this in our country too,’ he replied.
‘We call it bouncing stones across the water.’
There's a lesson there for television researchers in particular.
Pithy gag competition
DID YOU hear the one about the parachutist who landed in a field but wasn't sure of his whereabouts?
He went to the edge of the field, and seeing a man walking his dog asked him where he was.
‘You are in a field, there is barley growing in it and you are facing north,’ came the reply.
‘You must be an accountant,’ the parachutist said.
‘I am,’ said the walker, ‘but how ever did you know?’
‘Your information is soundly based, totally correct, and completely useless,’ the  parachutist told him.
That joke no doubt will appeal to all who question the value of accountants having a stranglehold on top jobs in Britain's industry and commerce.
But doesn't everybody give himself/herself away in such commonplace situations?
We offer four £5 book vouchers to readers who come up with the wittiest gag about scientists and engineers.
Address your entry to Forum Competition,New Scientist, Commonwealth House , Oxford Street London WC1A 1NG.
Mum's the word
SINGERS and musicians on television often mime to pre-recorded tape tracks, because that way the studio set isn't cluttered with microphones.
The same thing happens in film studios.
Now you can't even rely on seeing a live performance at a live concert!
Every year amateur rock musicians compete in the Battle of the Bands competition.
This year tape manufacturer TDK chipped in nearly £100 000 sponsorship money.
The finals were held at Hammersmith Odeon and BBC TV filmed it.
High spot of the concert was a guest appearance by top pop band Shakatak.
They bounced on stage, played two numbers with professional skill and  disappeared .
What no one told the audience was that although the band's instruments were apparently all wired up for sound, Shakatak was actually miming to a tape it had recorded in a Shepperton studio the night before.
At last year's final, another pop group, Dollar, also mimed to a recording for its guest spot.
Because pop groups are skilled at TV miming, most of the audience didn't realise what was happening.
This could start a trend.
The London Philharmonic could, for instance, perform at the Festival Hall without the boring necessity of tuning up before going on stage.
Dame Janet Baker could save her voice by miming to a tape and church choir masters could stop worrying about when their choristers' voices will break.
Relax, Mr Redford
New Scientist's cartoonist Mike Peyton tries showbiz
AS ONE gets older one's remaining illustrations are constantly following Father Christmas, fairies, fair play, etc. down the plug-hole.
Recently one of my favourites gurgled after them — the illusion that, given the right script and director, I could enter the glamorous world of film and show Messrs Redford, Newman and others a thing or two.
Looking at the event dispassionately, I realise that I was probably chosen because the film had to be ‘in the can’(a technical term we film people use for ‘finished’) by the end of February.
They discovered, or contacted, me early in the same month.
The film — a propaganda film for the UK Central Office of Information to be shown abroad — was about our maritime traditions, the character of the British and the sea, or ‘Sailing Brits’ as it said on the clapper board.
They had shot [sic]lifeboatmen and fishermen, and now they wanted yachtsmen sailing in winter.
Sensibly, as they generally sail for pleasure, most yachtsmen (encouraged by their insurance companies) lay up for the winter.
However I run my boat as a charter boat and cater for a masochistic fringe who would sail on Christmas day if I were to let them.
I was, I now realise, probably the only one on their short list.
I met the director and producer for a ‘working’ lunch in an Italian restaurant in Soho.
The producer, a blase+ young woman, toyed with a lettuce leaf and an olive, but on the rare occasions I am asked out to lunch I generally do without breakfast, so I made up for her and discussed the shooting schedule (you soon pick up the jargon) with the director.
Fortunately I had a tide-table with me which played havoc with the original extravagant plans as they had only three days to shoot the 15 minutes of film they needed.
Five minutes footage a day is good going and they use only 1 foot of film out of every 15 feet shot.
I thought the half bottle of wine we left was symbolic of the extravagant life I was now entering, though I must admit it did go against the grain and I did think vaguely of ‘bowser bottles’.
Unfortunately the rapport I had built up with the director was wasted, as the commercial he was working on went wrong and had to be done again, so I met my new director over a pie and a pint in my local before shooting started.
I soon found out why they achieve only four or five minutes of film a day.
We started with provisioning the boat at the quay (’ Put the bloody whisky out of sight or we'll never get shown in Saudi Arabia’) and leaving the quay.
We were filmed leaving the quay and then returned and left the quay again.
The cameraman then came on board and we left the quay; we returned and he transferred to a motor boat to film us leaving the quay.
Only time and a falling tide prevented him from going up a nearby church tower and filming us again.
The following morning my crew and I were up at 4 am when the tide lifted us.
It was bitterly cold and we were like Michelin tyre men as we swabbed the ice off the decks and hoisted the sails white with frost.
It truly was winter sailing, but the camera crew had slept ashore and we rendezvoused with them in their camera boat down river at a civilised hour.
The sequence in hand was apparently quite simple: we met up with another yacht and sailed in company, then came across a fishing boat to whose skipper I shouted, ‘Have you any fish to spare?’
(Of course he had, having taken it from the deep freeze the night before.)
We then sailed alongside, too close For peace of mind in the lumpy conditions prevailing, but close enough for three cod to be thrown across.
‘I'll straighten up with you in the pub.’
I was to shout, then the boats diverged. and ‘cut’.
The director hadn't realised that the sea is like a vast moving carpet and, with a strong ebb against a fresh breeze, a very bumpy one.
Only the sailing types appreciated the problems of keeping the same bit of distant coast line ahead on the same point of sailing and in the same position relative to the other boats.
We also missed the quay, transferring immorally expensive cameras and equipment from one rolling boat to another and moderately expensive crew (£700 worth a day) until all the camera angles were covered.
You must cover everything — you cannot return.
I lost count of the number of times those cod, or the ‘effing flying fish’, flew through the air.
And even then the poor moribund critters had to be dropped from a great height to get a satisfactory thump for the sound track.
Somehow we managed, and the director asked if I thought there was any chance of the weather worsening so that the boat would heel a little more.
‘A gale perhaps?’ he queried hopefully.
However, as I have said, some things take longer and they could not afford to wait.
They were shooting in a hospital the next day.
So we eased our sheets and ran for home.
En route we fried and ate the well tenderised cod — this was filmed with plenty of zizzle track — and I drank to the loss of another illusion in the Scotch that will never be seen in Saudi Arabia.
Talk of waste is not Rubbish
Tam Dalyell believes we are throwing away money
WASTE NOT, want not was a keynote in Sam Smiles's self-help philosophy.
The virtuous Scottish doctor would certainly have been shocked by our modern profligacy.
No doubt the grand-old Victorian would have  reprimanded us for our thriftlessness, extravagance and improvidence.
Self-denial and self-help, however, would make a poor rallying cry for the hustings.
But waste management, I predict, will become an important issue in the next general election.
Certainly such management could furnish a more effectual remedy for the evils of our ways than many bills passed in the House of Parliament.
Britain should take example from what other countries are doing.
The Americans have a Conservation and Recovery Act of 1977, which augments the Resource Recovery Act of 1970.
In 1977, the State of Oregon launched its famous resource conservation scheme, and it has proved highly popular for the state politicians in Oregon.
The Federal Republic of Germany introduced legislation in 1975, which provided the framework for a waste economy programme.
Bills completed by 1980 laid special emphasis on recycling and energy conversion.
German industry has welcomed the policy linked as it is with incentives for the Landers to become involved.
The Germans are sure that energy costs and price increases for raw materials will soon be compelling their industries to recycle as much as possible.
Wasters, as old Sam once said, ‘are their own worst enemies, though they are generally found amongst the ranks of those who rail at the injustices of the world.’
Recycled materials are normally much cheaper than raw materials.
Often substantial energy savings are achieved by recycling waste materials.
For example, it takes at least three times the energy to convert iron ore (most of which has to be imported nowadays) into steel than to convert ferrous scrap into equally good steel.
The electric furnaces now used to produce high quality steel, depend almost entirely on scrap for furnace feed.
The production of aluminium from bauxite requires about 20 times more energy than does scrap aluminium.
The production of copper from ‘grade one’ copper scrap in a reverbatory furnace uses only 3 per cent of the energy required to convert virgin ore to the metal.
Britain spent more than £318 million on glass containers, amounting to some 1.9 million tons of glass, in 1978.
At least 80 per cent of this enormous output found its way to rubbish tips via household dustbins.
If all unwanted household glass were recycled industry would save the equivalent of about 45 million gallons of oil a year.
In Greater London alone, the Bottle Bank Scheme, if properly extended, could save annually some 5–2 million gallons of oil.
Non-ferrous metals account for a large part of our overseas expenditure on raw materials.
In 1980, we imported:
136 412  million tons of lead at £45 million 130 336 tons of zinc at £46 million 271 000 tons of copper at £265 million 5157 tons of tin at £48 million 169 354 tons of aluminium at £135 million
The high cost of non-ferrous metals makes their reclamation attractive.
At December 1981 prices, every ton of copper lost in waste represents around £900 in foreign exchange: comparable figures for lead and aluminium are £360 and £580 respectively.
Governments and local authorities must help householders and industry to segregate their waste according to the composition of the materials.
Kept separate, and if possible clean, such materials require a minimum of preparation for recycling.
Once waste materials have become mixed, the problems increase enormously and certain substances become virtually irretrievable.
In 1967, I attempted to introduce legislation requiring car manufacturers to design their products in such a way that when eventually the vehicles are consigned to scrap, their plastics and metals could be easily separated.
My colleagues thought it an impracticable plan and that it would put British manufacturers at a disadvantage — unless such measures could be introduced worldwide.
Householders should be given separate receptacles, for their wastes of food, plastics, paper and tins and bottles.
And an incoming government must encourage industry to use reclaimed materials — and back its drive with financial incentives if necessary.
I would like to highlight five other important matters.
1.
The need to identify and to define realisable targets in terms of the cost effectiveness of using recycled wastes in each commodity sector.
2.
The urgency of identifying major participants in waste management, such as manufacturing industry, local authorities, reclamation and recycling industries, and waste disposal contractors.
3.
The importance of assessing the R&D required to improve the use of wastes in existing and new products.
4.
The significance of encouraging consumers, including government departments and public bodies, to review their contract specifications for manufactured products and to give preference to those containing recycled materials.
5.
The desirability of immediately restricting the dumping to landfill of anything that has economic use.
‘Prudence’, claimed Sam Smiles, ‘requires that we pitch our scale of living a degree below our means, rather than up to them: but this can only be done by carrying out faithfully a plan of living by which both ends may be made to meet.’
Margaret Thatcher has hinted at this in her speeches on the need for good housekeeping.
A concerted effort on waste management could produce harmonious results and salvage some of the valuable manpower squandered by the present government in its purist attempts at national good housekeeping.
A magnetic physician
Richard Fifield provides a perspective, and Peter J. Smith some personal recollections, of S, Keith Runcorn
ONE OF the doughty pack leaders to emerge in the late 1940's from the Manchester scrum of ‘palaeomagnetists’ was S, Keith Runcorn — a former Cambridge engineer with an almost unhealthy liking for the rough and tumble of the rugby field, Keith Runcorn is now professor of physics, and geophysics supremo, at the University of Newcastle-upon-Tyne — and incidentally the president of the university's rugby club, To honour Runcorn's reaching the age of 60, the university organised earlier this month a three-day conference on ‘Magnetism, planetary rotation and convection in the Solar System’, Since the Second World War, geology has undergone conceptual upheavals as never before, The apparently ludicrous ideas proposed by Alfred Wegener in the 1920s, that the Earth's continents were drifting around, have found solid ground, The evidence came from physicists inspired by wartime work on radar, by cosmic-ray research and the discovery that some rotating stars have a magnetic field, The physicists set themselves the task of measuring whether rotating bodies on Earth also produce magnetic fields, The eminent Patrick Maynard Blackett devised a highly sensitive magnetometer for this work, but finding that a spinning gold cylinder produced no magnetic field, turned his machine to measuring rock magnetism, A school of expertise concerned with ‘fossilised magnetism’developed around him at Manchester and later at Imperial College, London, The fruits of such work inspired a reappraisal of continental drift and new theories to explain the mechanisms responsible for moving the continents, and later produced the foundations on which were forged the unifying concepts of plate tectonics and seafloor spreading, Runcorn applies an enormous enthusiasm to all that he takes on — as many past students and editors of various science journals can testify, His first notoriety came with his attempts to determine whether the Earth's general magnetic field was related to the planet's rotation, or related to some deep-seated phenomenon, To determine this he took his magnetometer down some of the deep Lancashire coal pits.
His conclusion was that the field increased with depth and was related to the Earth's core according to the inverse cube law.
R.F.
GLEEFUL stories invariably fly around whenever Keith Runcorn's name comes up in conversation among geophysicists.
Of interest is not so much their dubious factual basis as their plausibility.
Can it be true that, 6000 miles from home, he once called on a well-known American geophysicist in the middle of the night with an urgent request to have a secretary type his latest contribution to the journals?
Can it be true that, master of an essentially observational science, he has only ever made one observation in his life (that is to say down the mines in Lancashire)?
Is he the only geophysics supervisor to have suffered the misfortune of losing a student to the great geophysicist in the sky?
Is it true that he knows the great geophysicist in the sky?
Personally, I'm inclined to doubt the truth of such scurrilous tales — with the possible exception of the last.
Of course, apocryphal stories only proliferate about someone widely held in great affection; and in Runcorn's case the reason for the affection is not hard to find.
I first met the Southport wizard 20 years ago in a remote Welsh outpost; and within minutes we were immersed in a fascinating discussion about rises in sea level and the expanding Earth.
It is true that, characteristically, he was about to cadge a lift; but lift or no lift, most elevated academics of those (and these) days when accosted by some unknown student would look at him as if he were something deposited on the pavement in contravention of the local by laws.
The lack of even a hint of that attitude in Runcorn, combined with his remarkable equability, endeared him to me from the start.
He's just as likely to be found talking to a six-strong student society in Bangor as addressing 500 top Earth scientists in Washington; he will have a drink with (and on) me just as readily (or so he makes it appear) as he will have lunch with (and no doubt on) the director of the US National Science Foundation; if he's not corresponding with some editor over some esoteric point of science, he's trying to persuade the high-ups at the European Space Agency to do something adventurous in planetology for a change.
In short, ever since Blackett converted him to geophysics, he has been the world's most tireless ambassador for geophysics in general and British geophysics in particular .
Runcom has probably made more original contributions to more branches of the subject than any other geophysicist; but if history judges his work on that basis alone, it will come up with an incomplete assessment.
For what would be missing would be the effect of his enthusiasm on others, in ways not recorded in the formal literature.
There is no one better at persuading people to do things they might not otherwise have been inclined to do.
There is no one better at perceiving likely growth points in geophysics, at getting the money to pursue them and at encouraging people to work on them.
And finally, there's no one with a more impressive grasp of the whole field of geophysics on Earth — or the Moon for that matter (as his recent article ‘The Moon's deceptive  tranquility ’ in New Scientist , vol 96, p 174, testifies).
Who else but Runcorn could give an impromptu hour-long lecture on any geophysical topic and probably have just the right slides in his pocket to support it?
It is hard to imagine he is 60 already.
It is quite impossible to believe that he will fade from the scene on formal retirement in a few years' time.
For decades yet, whether it be in Newcastle, London, Los Angeles, Sydney or wherever, I fully expect to see, or hear of, his rising from the audience at the end of someone's lecture with ‘I just happen to have this slide here, which illustrates…
’ P.J.S.
Satellite aids
CONCERN about sexually transmitted diseases has reached such a pitch among the American medical profession that discussions on the subject now have to be conducted by satellite.
This week some 3000 doctors and nurses participate in what is described as an international video teleconference on the subject.
Scattered around in 10 cities, including Stockholm, the medical specialists will spend four hours discussing the finer points of herpes, acquired immunodeficiency syndrome (AIDS), hepatitis, meningitis, candida vaginitis and Reye's syndrome.
According to the satellite company that is putting the conference together it will be ‘transmitted live’ from the Center for Disease Control in Atlanta Georgia.
Unclear Safety
THE ARBITERS of Britain's nuclear future, now sitting at the Sizewell inquiry, might like to know of a new hazard connected with pressurised-water reactors.
When the Belgian nuclear industry decided to allow reporters into its new reactors at Doel near Antwerp, it followed the time honoured tradition of entertaining the thirsty hacks on a boat trip up the river Schelde.
Unfortunately, they overdid it a bit.
One French reporter, after a substantial lunch with three wines, overbalanced on the gangplank while disembarking.
The Belgians fished him out — but he was unable to complete his assignment on safety precautions at nuclear power stations.
LETTERS
Polonium deaths
In his article ‘Polonium: Windscale's most lethal legacy’(New Scientist 31 March, p 873) John Urquhart has rightly drawn attention to the omission from consideration of polonium-210 in the National Radiological Protection Board's report on the 1957 Windscale reactor fire.
However we consider his evaluation of the implications of the polonium-210 release to be grossly in error.
We give here the main reasons for doubting the validity of Urquhart's population dose estimates, and for rejecting the conclusions of the accompanying contribution ‘Looking for an increase in deaths from cancer’(pB74).
Dose assessment: The account of dose calculation contained in Urquhart's article does not allow the method he used to be completely understood, and it has proved to be impossible to obtain the ‘fuller version’ of the article, either from the Journal of Nuclear Information .
which does not yet exist, or from the author himself.
Despite the difficulties, it is clear from the New Scientist article that the approach used produces a considerable overestimate of the population dose.
The main reasons are: 1.
The data used as a source of information on dosimetry are not valid for application to the Windscale fire release because they apply to polonium-210 in the environment in equilibrium with its precursors in the uranium-238 decay chain.
This situation is quite different from the Windscale fire, in which the single isotope polonium-210 was released.
2.
The method used to obtain the population effective dose due to polonium-210 in the environment by scaling population thyroid dose due to iodine-131 given in NRPB-R135, is not valid because of the different exposure pathways involved.
The exact degree of overestimation in Urquhart calculation is not certain, but we consider it likely to be a factor of about 100.
We also reject Urquhart's claims that population thyroid exposures due to the presence of tellurium-132 and molybdenum in the release were not considered in the board's study.
They were included in our report: tellurium-132 was estimated to contribute about 5 per cent to the total thyroid collective dose and molybdenum 99 was found to make a negligible contribution.
Cancer incidence: We consider the contribution ‘Looking for an increase in deaths from cancer’ to be extremely poor.
1.
The histogram on page 874 shows ratios observed-to expected leukaemia deaths for different ages.
The numbers given in the boxes are not the numbers of leukaemia deaths as the presentation suggests — they are nearer to the total of all causes of death.
Their actual meaning is unclear because the definition of the population on which they are based is ambiguous.
2.
The article challenges the statement by the Cumbrian Area Health Authority that local leukaemia rates do not significantly differ from national rates.
On the contrary, the first table on p 875 supports the health authority's statement event though the multiple myeloma data appear to have been included in the column ‘RES neoplasms’ as well.
There is no significant change in leukaemia rates in the two periods.
The increase in multiple myeloma was reported by the health authority and is being investigated.
3.
The increase in male cancer rates in South-West Cumbria are in line with the national trends over the same period.
4.
The reference to elevated cancer rates between 1974–80 among those aged between 15–24 applies to a population of whom 50 per cent were born after the Windscale accident.
The appropriate comparison should have been made by following through a cohort who would have been exposed by the accident.
Such an analysis is part of the larger long-term programme of the NRPB devoted to a geographical study of malignancy in the UK.
In due course, when all available information on the releases of polonium-210 and other nuclides not so far incorporated in the analysis has been assimilated, the board will publish a revised assessment of the radiological implications of environmental releases from the Windscale fire.
 R. H. Clarke Secretary National Radiological Protection Board 
And yet it moves
The trial of Galileo, of which we were reminded by your excellent article (7 April, p 25), clearly is of paramount significance in several areas of human development.
Attitudes toward science — also treated elsewhere in the same issue — and the apparently instinctive antipathy toward new ideas that do not fit readily into one's established mental framework, continue to manifest themselves even in the context of the current technological age.
In particular, I was stimulated to exhume a cutting from the Daily Telegraph of 23 October 1980.
The substance of this report was that, almost 300 years after it had condemned Galileo for saying that the Earth revolved around the Sun, the Roman Catholic Church announced that it was to re-examine his case ‘with full objectivity’.
As I seem to have missed any further pronouncements from this source on this topic, I would be very interested to hear if anyone can shed any light on any recent attempts that the Catholic Church may have made in order to extricate themselves from a presumably somewhat  embarrassing position.
 W. T. Weller Willaston South Wirral 
Wild dogs
Although I found James Hansen's article ‘The wild dogs of Italy’ very interesting (3 March.
p 590) I find it difficult to accept the argument that the wolf ever ‘kept the feral dog population in check’.
Controversies abound concerning the ability or predators to control prey populations, and the suggestion that wolf predation pressure (Hansen quotes the finding that 3.5 per cent of the wolves' diet is composed of dog remains) was ever important is unfounded.
Also, it is simply incorrect to state that ‘the Italian wolf, which is perilously close to extinction.
may survive after all — but as a dog’.
Hybridisation may result in the expression of traits that are characteristic of one or the other parent, but genome mixing occurs regardless.
Thus talk of species survival, though in another form, is by definition incorrect.
 Thomas Daniels University of Colorado, Boulder 
Sodium scare
Your correspondent G. A. Bates (Letters, 3 March, p 607) need not worry about the glare from low-pressure sodium street lamps.
When the sodium atoms in these lamps emit their wave packets there is no phase relationship between them, so there is no possibility of a ‘beat note’ building up.
Consequently, there is no possibility of any emission in the microwave band.
Microwave cookers operate at a Frequency of 2450 MHz and a wavelength of about 122 mm — rather longer than Bates supposes.
Bates also wonders if anyone has given any thought to the glare these lamps give off.
Public lighting engineers give very considerable thought to the glare produced by their installations, but in designing lamps you have to compromise between an installation that will make things most visible, use least energy and produce least glare.
The installations in our streets usually represent the best compromise that can be found.
User tests of car drivers have shown that the majority like this style of lighting — but it won't do anything nasty to Bates or anyone else.
 Arthur Tarrant University of Surrey 
Life without lead
Problems caused by using fuel with too low an octane are considerably more real than Michael Fletcher (Letters, 14 April, p 103) and many others appear to appreciate.
Taken literally, the idea that four out of five cars would be unable to run is rubbish.
What is not rubbish is that an attempt to operate modern cars on low octane fuel for any extended period of time is likely to have very severe consequences for the engine.
1.
Local overheating of piston crown and piston ring area resulting in the formation of carbon in the piston ring grooves, followed by ring sticking, leading to high oil consumption, high fuel consumption and high rates of wear.
2.
Failure of small-end, big-end and main bearings caused by the high pressures in the cylinder resulting From detonation (knock).
3.
Pre-ignition as a result of carbon formation in the combustion chamber together with local overheating which, particularly if encountered at high engine speeds, will result in the collapse of the piston crown.
With regard to the supply of lead-free 97-octane number fuel, I feel it would be of value to readers to have some idea of the problems involved.
It is not that such a fuel cannot be produced, so much as the scale of production required.
Refineries have been designed and optimised to produce the Fuels at present in demand.
To change overnight to a different pattern of production while maintaining the octane number of the  gasolene (petrol) is physically impossible.
Of course refineries can be modified or adjusted to produce the necessary product range given time, but the idea that this can be done to every refinery simultaneously and instantaneously is ludicrous.
In the same issue of New Scientist .
R. Stephens comments on the use of petrol containing alcohol.
He is quite right.
Again, the problem is one of scale more than anything, and on political decisions and their timing.
The internal combustion engine will run on almost anything that is flammable; a wide variety of gases, petroleum liquids, alcohols, banana oil, wood chips, coal.
bitumen and so on.
Technically, fuel is not the real problem; economies and politics are the problems.
One final comment, I have gained the impression from the correspondence in New Scientist that the statements of Associated 0ctel and of the petroleum industry are rejected largely because they have a vested interest, but also because they run counter to the ideas of correspondents.
Well, we all have a vested interest, and the rejection of evidence as a result of preconceived ideas is, of course, one of the cardinal sins in science.
I feel it is necessary to state my own interest: I am a retired mechanical engineer, at one time employed by a major international oil company where I specialised in research on fuel and lubricants for internal combustion engines.
I am also interested in motoring at minimum cost!
 Edward P. Zammett Harrow Middlesex 
Laetrile folly
Your latest silly story about amygdalin (’ Blacklisted vitamins on sale in Britain’, This Week 31 March, p 868) as usual confuses lactrile and amygdalin.
It goes on to credit E. T. Krebs, Jnr.
with the isolation of apricot kernels.
This was in fact done circa 1830 by Robiquet and Boutron-Charland and the cancericidal effect reported in 1845.
Had you bothered to look up the LD-50 in any standard work, or even bothered to consult Dr Woodward, you would have been more cautious about repeating tales of poisoning.
I would lay you long odds that you will not be able to verify a single British case of poisoning with either pure  crystalline amygdalin or apricot kernels.
 Frank Hourigan Birmingham 
According to the 1982 edition of the Martindale Extra Pharmacopoeta .
laetrile consists chiefly of amygdalin.
The label of the B-17 tablets that we bought lists amygdalin as an ingredient.
We did not claim that there had been any cases of poisoning in Britain, but referred to the numerous American cases (FDA Drug Bulletin 1977, vol 7, p 26;New England Journal of Medicine 1979, vol 300, p 238).
The LD-50 value for laetrile is irrelevant, because the poisonings resulted From taking the tablets after eating certain fresh fruits and vegetables that contain hydrolysing B-glucosidase enzymes, which release cyanide from amygdalin.
Krebs was largely responsible for promoting B-7 and B-5, in the US.
Great Briton
AS A FRENCHMAN, I am a bit shocked that Robin Dunbar mentions ‘A certain Dr G. Boulenger, of the British Museum…
’(Forum, 3 March.
p 604).
Boulenger was not really ‘a certain’, but at the beginning of this century one of the greatest  herpetalogists or ‘amphibiologists’ of his time.
He wrote many articles and books and some of them were also translated into French.
So if the French do not forget, the British should remember their scientists too.
 P. Jolivet Food and Agriculture Organisation Bangkok 
George & dragon
I was interested to read the suggestion by John H. Parkinson (Forum, 14 April, p 95) that St George's dragon was a manifestation of the Aurora Borealis.
While some dragons may have been comets, or indeed auroras, the majority appear to have been meteoric fireballs.
This is shown by certain facets of their anti-social behaviour, such as the specimen that dropped red-hot stones onto Helston in Cornwall, before falling into Loe Pool nearby.
The British Museum (Natural History) exhibits a print of a 1783 fireball with the contemporary caption that it was ‘of that species of Meteor which Dr Woodward and others call the Draco volans or flying dragon.’
 N. P. Warren London 
Exploding things
In Monitor 21 April, p 148, a gremlin made the Universe three times too big.
The standard  interpretation of the redshift places NGC 4319 and Markarian 205 300 million parsecs apart, not 800.
ARIADNE
FROM time to time I read about the Turin shroud.
I gather that the cloth has really been dated, found to be the wrong age and that disposes of the matter.
But other, naive, questions have occurred to me.
For example, is there any evidence at all of what Christ looked like?
Most people's ideas are from medieval or pre-Raphaelite paintings, but how authentic is the bearded figure?
Offhand, I cannot recall any description of appearance in the Bible , suspect enough as a source anyway.
This train of thought was started up again by a report in a daily paper of the miraculous debut in court of a bearded man wearing an Elizabethan ruff, though he kept his lower half decently obscured, only his head and shoulders on view.
He materialised through the wood grain opposite a junk box in York Crown Court.
Judge Harper said that he found this quite astonishing.
He thought the figure looked quite clearly like a judge in a wig.
He thought, too, there was a slight resemblance to Judge Jeffreys.
(Why Jeffreys should choose to appear in a court at York is puzzling, especially in the wrong clothes for his time and in a building which was erected about 60 years later.)
Other theories, said the report, ‘abounded’ in the town.
Did it have any connection with the Castle Museum opposite the court, which was once a women's prison?
Was there a link with nearby Clifford's Tower, where Jews were said to have committed suicide in the 16th century?
When I was a teenager and at last able to have a room of my own, the outline of a Red Indian's face appeared as a stain on the ceiling.
Could it have been Sitting Bull warning me about the distortion of Indian history in Hollywood films?
Could it have had any connection with the man next door, who owned a motorcycle with the trademark ‘Indian’?
I THOUGHT that I had disposed of the subject of flashes of light from materials when stressed, especially as I was only a few centuries behind Francis Bacon in mentioning the phenomenon.
But I feel it incumbent upon me (I have been listening to chairmen's speeches) to bring up the subject for one last time because of a letter from a reader in Nigeria.
There the post is something on the erratic side, which explains the delay.
He says that when he twists ice out of its plastic container, the ice being extremely cold and dry, he sees a flash of blue-white light as the cubes shatter into pieces.
He cannot explain this.
If anyone writes in to say that Francis Bacon noticed the flashes when he got out of his coach to freeze a chicken in the snow, I shall leave the country, suitably disguised, and write piteous letters about being a laughing stock to all, coupled with the name of sundry.
(It is the influence of chairmen again.)
IT MUST be a plot, for here is another effect that I am asked to inquire about in my ignorant way.
A reader, disgruntled in large proportion, sat down to work out how much of his salary he would have to hand over to the Inland Revenue heavy mob for the past year.
He had a calculator to help, but tried to keep his temper, his blood this side of boiling point and a little, at least , of social grace by listening at the same time to a Schubert piano sonata on a portable radio alongside.
But he found he was listening to the sonata through a hiss.
At first, he thought it might be his own reaction to the figures of the Inland Revenue's share of his money and then, perhaps, a neighbour with some interfering machinery.
Only slowly did he trace the interference to the calculator, battery powered, made by Casio.
The hiss was clear on MW and LW while the calculator was within a few feet of the set, but not on FM.
There was also noticeable interference with reception on a mains-powered radio, but, again, not on FM.
He is baffled.
So am I. Are readers?
WHATEVER happened to oaf?
I ask because I have been throwing out a raft of old notebooks and discovered some scrawled lines from years ago about the open air factor.
They read somewhat strangely, as if I had imagined the whole thing, or cooked it up for an April Fool joke.
For instance, my notes say that in research on oaf, fine threads from newly-hatched spiders were wound on frames to catch bacteria.
But, the notebook says in legible writing, oaf was being investigated by the Microbiological Research Establishment at Porton Down.
Everyone of my age has suffered from the character who insists on having bedroom windows open, for instance, on the grounds that it is a healthy dodge and worthy of all men to be received.
According to the MRE, that is right and oaf is a powerful though mysterious germicide, thought to be, perhaps, a combination of ozone and olefins that was the conjecture at the time of my notes, any way.
This fortuitous cooperation of lightning flashes and motor car exhausts was producing results in fairly populated areas at night.
The spiders's web threads were being used to provide samples of bacteria, which stuck to the threads as the air blew across them.
Oaf could be powerful, killing all the bacteria in a sample within twenty minutes.
Maybe I wrote the whole thing in a dream.
If so, I hope I had the windows open.
Daedalus
SNOW AND ICE are very slippery.
This is because frictional drag (from a boot or a ski, say) melts them locally to a thin film of liquid water.
Hence the millions of pounds spent annually on skiing and sledging, and the further millions spent more grudgingly on snow clearance and road deicing.
Daedalus finds all this rather surprising, for water is an extremely bad lubricant.
Nobody would dream of trying to lubricate an engine with water, so the water-based slipperiness of snow and ice must be capable of vast improvement.
At first Daedalus sought a lubricating oil freezing at about 0°C, with some idea of boiling tons of it in vast kettles set up near ski slopes.
The vapour might condense and fall on the slopes as a new sort of lubricant snow.
But he then recalled that even tiny amounts of certain high polymers, like polyoxyethylene, dissolve in water to make it very slippery indeed.
It should be easy to spray polyoxyethylene smoke into local snowstorms, or to dust the surface of a ski slope with it by standard aerial crop spraying methods.
Either way, a ski slope of unparalleled lubricity would result.
All existing records would be broken, and profits would flood in as the rich skiing culture flocked to resorts with the new hypersnow.
Of course, road accidents and pedestrian tumbles would also increase alarmingly as wind-blown polymer dust spread round the locality.
Fortunately the ploy can also be worked in reverse.
Certain other polymers, with loose crosslinked molecular networks, can, in even tinier amounts, convert water to a soft sticky gel that is hardly lubricating at all.
A modest dusting on icy roads and pavements would eliminate slipping and sliding without all those expensive tons of sand and salt.
Packed snow would become a firm, stable, even slightly sticky surface.
Sadly, unscrupulous ski resorts might misuse the new polymer, surreptitiously dusting it on the slopes of their rivals.
Baffled patrons would adhere firmly to the slopes on their downhill run.