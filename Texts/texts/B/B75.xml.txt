

Moving with the times
THERE is, as you may have noticed, something different about New Scientist this week.
It doesn't look quite the same.
This is the first issue for a decade or so that has been typeset with the aid of a computer and phototypesetting.
No longer are these words turned into a metal block, and no longer does the creation of that block require the same sophisticated skills as those built up over many years by typesetters.
The computer has taken over.
This isn't the place to go into the details of the technology — turn to page 237 for the sordid facts — but it may be worth saying that the computer brings a new flexibility to the typesetting.
We have, therefore, changed the design slightly to take advantage of that new freedom.
The computer has dealt what will prove to be a mortal blow to the ‘priesthood’ of printing — those who have spent many years gaining the skills needed to turn a typescript into a printed page.
Many other groups of specialists have also withered away in the face of data processing.
Oddly enough, recent developments in the technology have tolled the death knell for another priesthood — that of the company computer expert.
When the computer was the domain of the ‘data processing department’— with double doors to keep out unwanted irritants like dirt, warm air and people who didn't understand what was going on within — the resident team of programmers and operators went unchallenged.
All that is changing.
It now costs significantly less to buy a computer — or a ‘work station’ as it is known in the trade — than it does to pay someone to sit behind it.
How can the computer priesthood survive when everyone is a member?
The answer is that it cannot, so the computer expert within a large organisation is an endangered species.
In future the computer expert will be the outsider who works for the manufacturer or as an independent adviser.
Organisations will call in the experts when things go wrong, as they now call in the people who install and mend lifts, photocopiers and the vending machines that dispense undrinkable fluids.
Thus it is important for the computer experts to adapt if they are to maintain some grasp, albeit a tenuous one, on the computers in their organisations.
Those whose desks will be rearranged to find room for the new machines will find it harder to exert some control.
It is, therefore, understandable that some ‘whitecollar’ unions are not welcoming the arrival of the microcomputer with open arms.
Jobs, or the possible loss of them, are at the heart of the argument.
It doesn't matter that fears of massive redundancies among office staff that greeted the first large computers proved groundless.
The microcomputer has arrived at a time of high unemployment.
Thus the issue of new technology has become confused with that of jobs.
There can never be a good time to tackle the fears that go with the introduction of any new technology.
Do it when times are bad and that is an excuse for those whose jobs are threatened to pin the blame upon — and to hold out against — the proposed innovation.
Do it when the economy is booming and affluence is an excuse to demand — and for companies to resist — more money.
The ramifications explain why large organisations are thinking long and hard about the introduction of any new technology into the office.
The office is one workplace where there have been few major changes — the ball-point pen and the photocopier hardly qualify as major — in half a century, since telephones took over.
Those left behind after the turmoil at our printers are, it seems, coming to enjoy their new work, despite numerous shortcomings in the computers that are supposed to make their lives easier.
Those ‘threatened’ by the computer should recall that they are dealing with an ‘idiot machine’that can do nothing without them.
All the computer can do is to reduce the drudgery, freeing the brain in front of the screen to do more productive things.
A decade from now we'll wonder what the fuss was about.
Celebrations at Number Ten
THERE is some good news for Mrs Thatcher this week.
How can that be when the water workers have, by an odd quirk of fate, left her constituents high and dry, long before the dispute had a chance to hit anyone else?
No, there has not been another report saying how well she did in the South Atlantic, the European parliament has not ‘liberated’ Britain's £500 million rebate from the European Community, nor has the opposition to the new nuclear power station at Sizewell withdrawn from the public inquiry The news that the Prime Minister has awaited with bated breath ever since she visited CERN last year comes from Switzerland, and it has nothing to do with Zurich, or gnomes.
The sound of corks popping at Number Ten Downing Street —‘warm congratulation’, ‘another first for Britain’— echoes the click of particles at CERN.
For the scientists at CERN may have found the W particle.
This is not a new ingredient that reduces inflation or increases employment, It is an important part of the jigsaw puzzle of elementary particles.
To be honest, it isn't so much the W particle that they've found at CERN, as the ‘signature of a long-sought particle of matter’, as the establishment's press release puts it (for more details of this exciting find, turn to p 221 of this  issue ).
There'll be plenty of argument as to the validity of these results, and over the coming months there should be confirmation — or negation — of the CERN findings.
But there's no denying that this is an important event — or, to be more exact, an important set of nine events.
It is certainly significant enough to grab the attention of the Prime Minister.
It is a pity that our share of the glory at CERN — Britain pays a lot of money into this uniquely successful collaboration each year — is marred by European complaints that this country is not pulling its weight in international projects, such as nuclear safety research (see opposite) or fusion.
THIS WEEK
Britain pulls plug on PWR safety research
THE BRITISH government has decided to pull the plug on a European investigation into what happens when a pressurised-water reactor goes badly wrong.
Ministers have taken the decision despite warnings from some civil servants that their failure to fund research into nuclear safety could backfire on them at the Sizewell public inquiry now under way in Suffolk.
Ministers say the project is too costly and will call for the project, known as Super-SARA, to be abandoned at a meeting of EEC research ministers next month.
Super-SARA has had a long and troubled gestation period.
Work began in 1975 when a small test-reactor at Ispra, near Milan, was switched by the Italian government to work on safety.
The Italians wanted to find out what happened inside a nuclear reactor when a pipe taking cooling water into the reactor burst.
Such accidents can cause a core meltdown.
In 1979, as more and more pressurised water reactors were ordered throughout Europe, EEC ministers decided to take over the project.
Backing for it increased further after the accident at the Three Mile Island reactor in the United states in March that year.
This accident alarmed safety scientists in Europe because is showed that even a small break in the cooling circuit, if combined with faults in equipment and human error, could be as serious as a large rupture.
Super- SARA was slotted into a worldwide programme of research into the safety of PWR reactors.
The EEC commission, in backing the expenditure of some £50 million on Super- SARA , said that ‘no other facility available at present or in the foreseeable future provides the range and advantages for in-pile loss of coolant accident simulation.’
Super-SARA covers a wider programme of research into accidents than any of the other projects in West Germany, France or the US.
Its tasks include finding out how the cladding around the nuclear fuel rods deforms at the high temperatures that might follow a loss of cooling water.
This ‘sausage’ or ‘carrot’ballooning can block the flow of emergency cooling water in the reactor, which should stave off a meltdown.
The chairman of the Central Electricity Generating Board, Sir Walter Marshall, said recently that ballooning posed the single biggest problem for the safety of PWRs.
Marshall now believes that new research in Canada proves that ballooning is a less serious problem, however.
other experiments will look at the effects of further rises in reactor temperature and at an effect known as rod ‘candling.’
This happens when the uranium dioxide fuel is  dissolved in the molten zircorium cladding round the fuel rods.
Again the flow of cooling water may be blocked.
The latest crisis over Super-SARA arises because the EEC commissioner, Viscount Etienne Davignon, wants more money for the project.
Last year he told ministers that, in eight months, the cost of the project had almost doubled to £176 million.
Howls of anger from British ministers at a meeting in December resulted in the appointment of a team of ‘three wise men’— Jean Teillac, head of France's atomic energy centre, Adolf Berkerhofer, head of the West German reactor safety institute, and Niels Holm, the former head of the Riso research centre — to investigate the cost-effectiveness of the project.
They will report to next month's meeting of research ministers.
But Britain has decided on its policy before reading their report.
‘We think it is time to kill the project off,’ one official told New Scientist last week.
This week the Department of Energy said it believed that sufficient data would be available from other sources to make Super-SARA  unnecessary .
‘Our research is into prevention,’ he said, rather than the simulation of what might happen in an accident.
‘We do not believe that, by the time Sizewell is built, we will be short of information,’ he added.
Bomb threat to atom agreement
EUROPE'S two A-bomb powers, Britain and France, are opposing an attempt to change the way that the EEC handles its nuclear trade.
The European Commission wants to change the 1958 Euratom Treaty to end trade barriers within the EEC and to establish itself as a watchdog on deals between member countries and outsiders.
It also wants to set up ‘solidarity’ measures to ensure supplies of uranium.
But Britain and France, which are also the main suppliers of fuel-cycle services, are worried that changes will impose unnecessary regulations on the nuclear trade.
Other nations, however, are carping about a clause in the suggested new text which would allow the two weapons powers to hide how much civil uranium they buy and stockpile.
The proposed rules will require the non-weapons powers to disclose how much uranium they buy.
Uranium acquired but not yet allocated to civil or military use escapes this obligation, however.
Only Britain and France benefit from this catch-all secrecy clause.
‘We are totally against any form of discrimination,’ a Belgian official told New Scientist .
‘Such freedom for Britain and France would stymie the EEC's efforts to ensure equitable shares of fuel if shortages occur in future.’
Observers also point out that the changes would allow electricity authorities in Britain and France to conceal direct or indirect military subsidies to nuclear power.
In practice the two industries are very integrated, sharing plant and facilities.
Officials connected with the EEC's nuclear trade say the proposals look like another compromise that will please nobody.
‘The commission has consulted a lot of people but only listened to what they said when they agreed with the commission,’ one source said.
The EEC this month revealed that Europe's nuclear industry holds a stockpile of some four years' supply of enriched uranium.
Medicines watchdog blocked safety plan for Opren
A DRUG company this week accuses the government's Committee on safety of Medicines of holding up measures to make the arthritis drug, Opren, safer.
Opren was withdrawn from sale by the company, Dista Products, last August following the deaths of more than 50 old people who had taken the drug.
But the company says that, as early as October 1981, it suggested to the CSM that the recommended doses for the drug should be reduced for old people.
It claims that the CSM said the change would be ‘inappropriate’ until further research had been done and the idea was dropped for several months.
The charge is made this week in a letter to New Scientist from Dista's medical director, Dr B. A. Gennery.
It is bound to undermine further the shaky reputation of the CSM as a public watchdog.
The letter goes some way to explaining the 14 months delay between June 1981, when harmful side-effects from Opren were first reported to an international conference, and August 1982 when the drug was withdrawn.
The delay was highlighted in New Scientist (16 December 1982, p 707) and later on the television programme Panorama .
Gennery says that information on the dangers were ‘verbally communicated to officials of the Department of Health in July 1981’.
The officials asked for further studies, which were reported to the department on 7 October.
It was at that meeting that Dista proposed a new wording for the Data sheet — the standard form of recommendation to doctors on how a drug should be administered.
The proposal was that the dose for old people should be reduced.
It followed the discovery that many old people metabolise the drug much more slowly than normal.
Its build-up in the body can cause death by liver or kidney failure.
A study among geriatric patients at St George's Hospital, London, found a mean ‘half-life’ for Opren of 101 hours, compared with an average for normal patients of 30–35 hours After a 43-day delay the department replied.
It said that to publish the Data Sheet ‘would be inappropriate…at this time’.
The CSM wanted further studies.
It finally agreed to changes in the Data sheet in June 1982, shortly before escalating concern among doctors forced the withdrawal of the drug altogether.
A Spokesman for the DHSS this week agreed that the Dista version of events was true.
He said the CSM did feel it right to act at the end of 1981 as there was not enough information available.
The fact that the drug's manufacturer had proposed a tightening of safeguards did not necessarily mean that the committee should agree to them.
Videogrant ends plan for video tax
THE BRITISH government's decision to pour taxpayers' money into a new factory to make video tape for the Japanese firm, Hitachi, has finally killed off the idea of putting a tax on the tape.
Film and record companies had wanted a tax to help curb the video-pirates.
But such a tax would drastically hit the sales of tapes, which the Japanese are relying on to justify the £25 million factory in Telford, Shropshire.
Maxell, a subsidiary of Hitachi, plans to start production before the end of 1983, and aims for an output of 12 million cassettes a year with over 200 jobs created for local workers.
The government has promised to subsidise the factory.
Despite the offer of government cash, there is still no British firm willing to enter the booming video-tape market.
But 3M, the US company, is already making 12 million video tapes a year in Gorseinon, South Wales and employing 950 people.
The US–Asian company Intermagnetics makes tape in Wrexham.
Maxell is confident that the prospect of a video-tape tax is now remote.
In West Germany, meanwhile, BASF has come up with a technical solution to the threat of a tax on blank audio tape.
The company has devised a system of recording non-copyright material, like discordant brass band music or language tuition, on the bulk rolls of tape as they leave the magnetic coating bath.
A wide recording head stretches across the jumbo roll of tape and magnetises it before it is slit to domestic size.
So the finished tape is no longer blank and no longer taxable.
Coal price rises will make nuclear power cheap
A WORLDWIDE growth in demand for coal and the prospect of large increases in coal prices, is an important reason for the Central Electricity Generating Board's determination to expand its nuclear-power programme, the Sizewell inquiry heard this week.
Peter Hughes, manager of the CEGB's fossil fuel and energy section, told the inquiry that the board wanted to reduce its dependence on coal, which at present generates some 80 per cent of its electricity.
The coming world shortage was bound to put up prices, he said.
He did not expect domestic coal supplies to be insulated from world price trends — especially as government grants to the National Coal Board will have to rise over the next few years.
Hughes dismissed alternatives to coal, oil and nuclear power as either uneconomical or unproven.
‘Synfuel’ would be more expensive than liquefied natural gas for 20 years.
The economics of producing oil from coal ‘do not look attractive and will possibly delay its introduction in the UK until the second or third decade in the next century.’
Last week, John Wright, another CEGB witness, warned that it would be at least another decade before the board would be able to assess the potential of renewable and alternative energy sources.
‘We just do not know whether economic wind generators, still less economic wave-power and geothermal power, is going to be developed,’ he said.
Wave power was ‘back at square one’ with none of the present crop of devices ‘ready to go to large-scale trial’.
Even a new cleaner generation of coal-fired power stations is 10 years away.
Wright surprised the audience at the inquiry by claiming that the sulphur dioxide fumes from the board's existing power stations have ‘little impact’ on the problem of acid rain in Scandinavia.
After the carnival atmosphere of the first week, the inquiry, which will decide whether the CEGB can build a pressurized water reactor at Sizewell, is now increasingly sparsely attended, as the CEGB continues reading out its evidence.
Silent placard-carrying protesters inside the hall are studiously ignored by the inspector, Sir Frank Layfield.
His remarkable doggedness led him to carry on regardless when two stink bombs broke everyone else's concentration.
A crisis may won develop over the late arrival of the board's evidence on the manufacture of the pressure  vessel , which will house the reactor core.
Concern over whether the vessel could be made completely safe almost led to the board's plans for a pressurised-water reactor being dropped altogether two years ago.
The evidence is being written by Monsieur A. Vignes, from the French firm Framatone, which was recently awarded the initial design contract for the vessel.
His proof of evidence, which is due to be heard within the next fortnight is, it transpires, being translated.
Sizewell sideshows continue apace.
In a typically grandiose gesture, CEGB chairman Sir Walter Marshall said on television that if the inquiry rejected the board's case on safety grounds, he would resign ‘because it meant that the technical advice I have given to government in past years has been proved to be incorrect’.
Significantly, perhaps, he did not say what he would do if the scheme was rejected on economic grounds.
And, whatever the inquiry decides, Energy Secretary Nigel Lawson said that he believes Britain needs more nuclear power stations.
Foreign Office to sit on research council
THE BRITISH government has extracted a price for generosity in providing funds for the British Antarctic Survey (BAS).
The Foreign Office has been given a seat at the Natural Environment Research Council, which funds the BAS.
The FO's assessor will have a say in how the BAS spends its money and will report back to the Foreign secretary on how useful the BAS is as a political presence in the Antarctic and the south Atlantic, This move, which clearly follows from Britain's determination to outface Argentina in the region, brings a new political backdrop to the activities of scientists in the Antarctic.
In November the NERC announced a 70 per cent increase in BAS's 1983–84 budget to £9.7 millions.
In 1984–85 a further £1 million will be added to the budget.
After years of poverty, BAS is now able to intensify its scientific programmes.
The new funds will be used to improve transport, including the purchase of two more aircraft and a small trawler and building a hard runway at the British Rothera base on the Graham Land Peninsula.
Sir Hermann Bondi, chairman of NERC, says that Antarctic research, like space research, requires spending ‘an awful lot of money to get there — and that is where the science begins’.
The funds will also pay for more research into the Earth and marine sciences, and for the study of the upper atmosphere (New Scientist , 18 November 1982, p 406).
Bondi says, ‘The unique geophysical situation of the Grytviken (South Georgia) Faraday and Halley (both  Antarctica ) bases provide opportunities to investigate scientifically exciting and fundamental atmospheric problems that cannot be studied elsewhere.’
The BAS and Whitehall see these as areas of scientific merit and, even more importantly, of political, strategic and economic relevance.
Extra Antarctic research will both enhance the British presence in the region and provide the information required to draw up conventions on fishing and mineral wealth.
BAS, though a chartered, autonomous scientific body, is becoming increasingly an instrument of government policy.
It maintains the sovereignty claims to the British Antarctic Territory and thus provides the basis for Britain's prominent place in international decision-making on Antarctica.
BAS also employs former service personnel who retain their military links.
Service personnel can be used in Antarctic work without contravening the Antarctic Treaty of Washington, but they must be involved in scientific and not military work.
The United states has a huge task force of some 500 army staff in Antarctica.
Argentina employs 132 military people and 55 civilians in its Antarctic programme.
A comparison of the average output per annum of scientific papers over the past four years is revealing.
Argentina has produced 18 papers; Britain has produced 151.
Britain's stance on sovereignty has traditionally been less  aggressive than many other claimants, such as Argentina and Chile — both of which claim overlapping portions of the British Antarctic Territory — or even Australia and France (New Scientist , 21 October, 1982 p 140 and 6 January, p 4).
The  Argentinian base at Esparanza in the British sector, displays a notice proclaiming it  Argentinian .
Asbestos ban is dropped by Brussels
PLANS to reduce the dangers to consumers from products containing asbestos have been drastically cut, because of disagreement within EEC.
Evidence submitted to a parliamentary select committee by Britain's Health and safety Executive last week reveals that products containing white and brown asbestos have been dropped from a draft EEC directive issued in 1980.
Only products containing blue asbestos, which is today used only in very small quantities, will now be affected.
The proposals were intended to prohibit the sale and use of most asbestos products throughout the EEC, unless the harmful release of fibres could be prevented.
But intense lobbying by asbestos firms in West Germany and Belgium held up agreement.
Now, in order to get agreement on a ban on blue asbestos, Brussels has dropped white and brown asbestos from the legislation.
Officials from the HSE told MPs that their work on implementing the recommendation of Britain's advisory committee on asbestos was held up for two years while they worked on the EEC plan.
They said that shortage of staff at the HSE meant that all the officers working on implementing the advisory committee's proposals, which were published in October 1979, were switched in 1980 to work on the EEC scheme.
Satellite TV: BBC haggles
THE BBC still hasn't signed the contract with British Aerospace, Marconi and British Telecom that will seal its place aboard a satellite to broadcast TV signals and hi-fi sound direct into British homes from 1986.
This is worrying firms hoping to cash in on cable-TV.
One of their prime selling points will be to supply satellite broadcasts to homes which are unable or unwilling to erect the large dish aerials needed for domestic reception.
The BBC was stung by the governments decision in late November to back the MAC transmission system developed by the Independent Broadcasting Authority, instead of extended PAL, the BBC's own system.
The BBC's governors toyed with the idea of relinquishing their right to operate two channels of direct broadcast TV.
But they thought better of it.
The BBC has told the satellite operators that it is definitely still interested.
But it is still haggling over details.
The satellite will cost £150 million at today's prices and the BBC's fee for 7 years use is still undecided.
Tear-jerker
TVS, the independent television station that broadcast 3D movies last year, is to screen Britain's first ‘inter-active’ TV show.
On St Valentine's day (14 February) viewers with sets that can receive the Prestel teletext service will be able to vote on which ending they prefer to see to a Valentine love story.
The TV programme The Real World is booking a special page on Prestel for the occasion.
Viewer's democracy will not become a regular feature on the station, which covers southeast England, however.
But it will give affluent viewers a foretaste of life with cable television.
And it will be interesting to see if people really do like happy endings.
Home video owners will soon have the same choice.
The film Liar's Moon , an everyday story of incest in the old West, has two endings.
One is sad, for the Far East market.
The other is upbeat for the Anglo-Saxon world.
But a British video company is releasing a home tape with both endings, one after the other.
Small is dead
POLITICS and big business have killed a plan to set up a ‘small is beautiful’ centre in south London.
Last week, the Intermediate Technology Development Group abandoned its plan to build an exhibition centre dedicated to E. F. Schumacher at the Polytechnic of the South Bank.
The move follows six months of negotiations during which the City of London, which owns some of the land on which the polytechnic is built, stuck out for a rent that one expert said was three times the market value (New Scientist , 6 January, p 7).
Dr John Beishon, director of the polytechnic, said the pull-out was ‘very sad.
It's another example of how the bureaucracy can't be beaten.’
The ITDG will stay in its present offices, concentrating on practical projects.
Nuclear life-saver
EVERY nuclear power station saves some 450 lives every year that it is in operation.
So Bernard L. Cohen of the University of Pittsburgh told the American Physical Society this week.
The reason?
‘Mining uranium out of the ground to fuel nuclear plants averts health effects of radon in later generations.’
Cohen says that even uranium mined from deep underground for nuclear fuel will, eventually, be exposed by erosion to release its cancer-causing radon gas into the atmosphere.
Old man dioxin keeps on rollin' along
DIOXIN, the chemical that polluted Seveso in Italy, has reared its ugly head again.
The chemical, which is about 150 times as toxic as cyanide, has turned up in the soil and in waters that flow into the Mississippi River at levels up to several hundred times those thought to be safe.
It has been there for more than ten years.
Teams of government and state scientists are collecting hundreds of soil samples each week.
By last week, government and state officials had doubled their estimates of contaminated sites to about 100.
The Environmental Protection Agency (EPA) has begun a massive effort to scrape up hundreds of truckloads of dirt and dispose of it, either in waste dumps or by incineration.
Meanwhile, most of the residents of Times Beach, south eastern  Misouri , have been evacuated.
But many disbelieving locals have stayed behind, even as technicians in plastic suits and respirators swarm over the town.
The offending isomer of dioxin (2,3,7, 6-tetrachlorodibenzo-P-dioxin, or TCDD) is a by-product of the manufacture of herbicides such as 2,4,5-T.
It was part of the defoliant Agent Orange, that the US forces sprayed on Vietnamese forests during the 1960s and 1970s.
American veterans are suing the government, claiming that dioxin has caused them disabilities ranging from a severe form of acne (chloracne) to nerve and liver damage, miscarriage, birth defects, and cancer.
Tests on animals have shown that dioxin can cause these maladies.
Epidemiological experience with humans is scarce, however.
The spraying of Missouri began in May 1971.
A waste contractor, Russell Bliss picked up thousands of barrels of waste sludge from a chemical plant that had made 2,4,5T for the army.
Mixed in it were 30 kg of TCDD.
Bliss combined this with 5 000 litres of waste oil, and spread it on several commercial horse arenas to control dust.
Some of the arena's dirt was later removed and used as a fill for home construction.
More contaminated oil went on private roads and on Bliss's own farm.
Shortly afterwards, horses, chickens, birds, dogs and other animals in the arenas and the farm began to die.
State officials and eventually the government's Center for Disease Control (CDC) in Atlanta, Georgia were called in.
But they did not identify the cause of the problem until 1974.
The first arena, shenandoah stables, showed soil concentrations of dioxin at 33 000 parts per billion (PPB), by far the highest concentration found.
One part per billion is now considered a safe level.
But over the years, the government did little except to remove contaminated soil, even though the trail of dioxin was showing contamination from a few to a several hundred PPB.
In 1975, the CDC recommended deep landfills, but local officials were working under the mistaken assumption that dioxin's half-life — its staying power in the environment — was only one year.
Since then, chemists have set the half-life at 10 years.
State officials say they were pre-occupied with a more serious problem.
At the plant where the dioxin originated, 1000 litres of waste contaminated with up to 300 000 parts per billion of dioxin sat in a tank.
Federal laws governing hazardous waste were not then in effect, nor was there much money to clean up tips.
Publicity accelerated as the Times Beach residents scattered, and government technicians moved in late last year.
In December floods from the Meramec River washed dioxin-laden silt far and wide.
Government scientists believe that the floods diluted the dioxin, and — so far — levels found in silt since the floods are below safety limits.
But the long-term effects on workers riders and spectators in the arenas are unknown.
The Environmental Protection Agency estimates that stable workers could have been exposed to as much as 190 times the acceptable daily intake.
Extrasensory sceptics
AMERICAN scientists do not share the general public's belief in extrasensory perception (ESP) and parapsychology.
James McClenon, a sociologist at Maryland University, asked members of the American Association for the Advancement of Science what they thought of ESP.
Only 4 per cent of the 353 respondents thought ESP ‘an established fact’, however.
A further 25 per cent thought it a ‘likely possibility’; 41 per cent thought
ESP ‘a remote possibility’; and 9 per cent rated it ‘an impossibility’.
The group demonstrated ‘the highest level of scepticism of any major group surveyed within the last 20 years’, said McClenon.
Certainly the results are in marked contrast to those found in a poll conducted by New Scientist in 1973.
That, admittedly self-selecting, sample of 1416 readers of the magazine voted with 67 per cent reckoning ESP a likely  possibility .
Cosmonauts blossom aboard Salyut-7
THE SOVIET UNION will probably send a new crew into space to man the Salyut-7 space station next month.
The trip is expected to last even longer than the seven-month record set by cosmonauts Valentin Lebedev and Anatoly Berezevoi in December.
The Soviets have announced that one of the two Indian cosmonauts in training at Star City since the autumn will fly in the first half of 1984.
No other foreigners are training at the moment.
This year women are likely to take the place of foreign cosmonauts in the third ‘researcher-cosmonaut’ seat aboard the Soyuz-T spacecraft, which makes periodic 8-day visits to the manned station.
Russia's second woman in space, Svetlana Savitskaya visited Lebedev and Berezevoi in August.
But space doctors still have some problems to overcome before a woman undertakes a longer flight than Savitskaya's eight-day trip.
The Chibis suit (resembling the lower half of a diving suit) which the cosmonauts use to draw the blood into the lower pad of the body and legs has apparently been found unsuitable for women.
It is only part of a battery of equipment the cosmonauts use to counter the prolonged effect of weightlessness but it is an important one and some alternative will have to be found before women undertake long duration flights.
Last year's mission also notched up a significant horticultural success.
For the first time the cosmonauts managed to coax the plant arabidopsis, chosen for its brief life cycle of about 40 days, to produce seeds in space.
Visiting cosmonauts brought back 27 seed pods in the summer, and the seeds have since sprouted and grown normally.
This provides the proof biologists wanted that plants can be cultivated from seed in zero-gravity.
It is the first small step towards growing crops in space to make long spaceflights self-sufficient in food.
Scientists fight for full survey of A-test deaths
DEFENCE chiefs and government scientists are at odds over how to assess the health effects of British atomic tests in the South Pacific 30 years ago.
There are suggestions that, if the method proposed by the Ministry of Defence is chosen, increases in the death rate among servicemen involved in the tests may not show up.
Earlier this month ministers announced an investigation after press and television interviews with servicemen who are now suffering from leukaemia and other diseases related to radiation.
Last week, civil servants from the Ministry of Defence (MoD) met scientists of the National Radiological Protection Board (NRPB) and the Medical Research Council to discuss who should do the work.
But they immediately ran into criticism from the NRPB about how thorough the survey should be.
The NRPB favours a survey which includes comparison of the servicemen with their compatriots of the same age serving elsewhere.
It also wants the MoD to trace living ex-servicemen through government records.
The MoD, however, wants to restrict the survey to a comparison of mortality rates among servicemen exposed to radiation during the tests with the national average.
One NRPB scientist at last week's meeting, Dr John Reissland, told New Scientist that the MoD's plan was almost certain to lead to inconclusive results.
‘soldiers are bound to be more healthy.
They are chosen to be soldiers because they are healthy,’ he said.
Reissland's point is that any deterioration in the health of the soldiers, or any increase in the risk of their dying of cancer, will be missed if the survey compares their death rates with those in the general population, which is less healthy than soldiers.
Like must be compared with like.
An MoD spokesman commented ‘It is exactly this sort of thing which experts must sit down and work out.’
The MoD has ‘no intention’ of contacting individuals at present.
But ‘we don't rule out the prospect,’ he added.
The MoD has always said that safety precautions were ‘more than adequate’.
It has criticised the press for ignoring the bulk of servicemen, who have suffered no ill effects, in recent coverage.
But Reissland was less sanguine.
The effects on soldiers that are claimed ‘could not possibly result from the doses reported by the MoD,’ he said.
‘They have to be  associated with some much higher doses.’
If the NRPB gets the contract to conduct the survey, Reissland wants to compare death rates among servicemen and scientists present during the tests with records of their individual exposure to radiation — as recorded by the film badges which, he has been assured by the MoD, all those present wore.
The MoD is expected to award the contract in the next few weeks.
The candidates are the NRPB, the Medical Research Council and a number of university departments that are skilled in epidemiological techniques.
Reissland warned: ‘If we do it, we will decide how it should be done.
But some groups that are short of money may want the job for financial reasons.
However I am satisfied that the MoD is not trying to brush this under the carpet.’
French nuclear boom outstrips waste reprocessing plant
FRANCE is building nuclear power stations at a rate that is outstripping its ability to handle the radioactive waste that the stations produce, says a report  commissioned by the French government.
If there are any more delays in building new facilities at the Cap La Hague reprocessing centre, it warns, the storage pools that house the wastes before they go for reprocessing could fill up.
Two extensions at La Hague are already 18 months behind schedule, and it seems certain that fuel assemblies will have to be packed tighter in the pools.
The report also warns that there are no back-up facilities if any of the  reprocessing plants breaks down.
It recommends that France take a second look at following the policy that US President Jimmy Carter adopted of storing radioactive waste rather than reprocessing it.
It recommends new laboratories to study the possibilities of long-term storage.
The report attacks the French government for  unnecessary secrecy over the nuclear programme, and singles out its failure to reveal the extent of plutonium losses at the La Hague reprocessing centre.
The report reveals that, in the past five years, some 8.7 tonnes of plutonium has been produced, with about 156 kilograms unaccounted for.
That is enough to make between 10 and 20 nuclear bombs.
Plans for refuse-fired power station rubbished
PLANS for Britain's first commercial power station to be fuelled by household refuse were this week rubbished by old hands in the municipal waste business.
The £30 million scheme is proposed by a consortium of GEC Energy systems, British steel and Corby district council.
If they get the go-ahead, residents in the Northamptonshire town of Corby would face an influx of some 33 000 tonnes of domestic refuse every year.
The rubbish could generate 25 megawatts of power worth around £2.6 million a year.
The main customers would be the British Steel tube works and new factories attracted to the Corby development area.
A nine month design study is now under way, and the station could be on line by 1986.
However, past experience at other rubbish-burning plants, such as the Greater London Council's incinerator at Edmonton in North London, suggests that the Corby version will need to be extremely efficient to make a profit.
The Edmonton plants opened 11 years ago, is the same size as that planned for Corby.
However, it costs around £6.6 million a year to operate and maintain — more than twice the value of the electricity generated.
The difference might be made up through charges to local councils for taking their rubbish away.
They might get £2.3 a tonne, the Corby consortium says.
But this would raise only £1 million a year — nothing like enough to break even.
Customers for the plant are proving slow to come forward.
The local East Midlands electricity board could take power generated at Corby and feed it into the local grid.
But last week a spokesman said ‘we are showing an interest in the idea but we are not committing ourselves.’
The consortium hopes that Greater Manchester, which generates 2700 tonnes of rubbish a day and is fast running out of holes in the ground to dump it in, will be interested.
But council officials are dismissive.
One of them told New Scientist 
‘Quite frankly, we don't think the Corby plant will recoup enough from the sale of electricity to pay all the overheads.
Nobody in Manchester is going to put up the rates for a power station in Corby.’
A computer's comic cuts block the Serpell line
THE SERPELL report on Britain's railways, published last week, recommends savage surgery.
Yet its proposals are derived from a computer model of the railway network that produces nonsensical results — including siting the terminus of one main line in the middle of a Scottish peat bog.
Sir David Serpell, a former permanent secretary at the Department of Transport, was asked by Transport secretary David Howell to study ways of running British Rail with ‘improved financial results’.
His report comes up with six options for cutting the network from its present 16 600 kilometres.
They range from option CI, which would close just 130 kilometres of track, to option B, which leaves 3600 km and option A, which pares away all but 2600 km of the network.
Serpell claims that losses on British Rail could be slashed from £980 million, on the present network, to £20 million under option B and a profit of £34 million in option A.
The maps of these options, published in the Serpell report, are full of strange inconsistencies.
They arise from the inadequacy of the computer model, and discredit Serpell's conclusions.
Both options C1 and C2, retain a line from March to Spalding, in East Anglia This line was closed last year and the track has been ripped up.
Option B, which aims to maintain the most important London commuter-lines does not have any lines running through the densely populated commuter region of North Kent.
Yet it does retain a branch line to Uckfield in East Sussex.
This is a line that BR considers one of its more probable candidates for closure.
(By coincidence one of the Serpell committee members, Jim Butler, of Peat Marwick and Mitchell, lives at Uckfield.)
A curiosity is the Line from Aberystwyth to Devil's Bridge.
This short stretch of line appears in options C1 and C2, even though the nearest link to the main BR network is more than 60 km away.
The line is a narrow-gauge railway, operated by BR's only remaining steam engines.
Consultants Travers' Morgan appear to have used a 1000 link computer model in the analysis for Serpell.
This is a cruder version of the 4000 link model which BR uses for its own network studies.
Full details of the model are not published.
One common problem in  assessing the effect of closing lines is how best to take account of the ‘contributory revenue’ that branch lines make to the main lines.
If trains on a branch line are cut then the railways may lose much more than the income from fares on the branch.
On a line like Stratford to Leamington Spa in Warwickshire more than 80 per cent of the passengers continue their journey on the main network.
Serpell's suggestion that this line should close could cost British Rail more money in through-tickets than it would gain from shutting the line.
This short-sighted analysis by Serpell is shown up most clearly in the section on freight traffic.
Here Serpell's computer was programmed to assume ‘the loss of freight revenue from the deletion of a link to be limited to the revenue directly attributed to the link.’
This means that, if trains took coal from a mine 10 kilometres up a branch line to a power station 100 km away, the model would expect only 10 per cent of the revenue to be lost if the branch line were closed.
In practice, of course, the coal would go by road and all the revenue would be lost.
This flaw seems to explain the odd network in the crucial option A — the one Serpell says should make a profit.
Option A keeps the main intercity passenger lines but does not keep the railways major freight lines — the lines between Cardiff and Swansea and between the Midlands and Yorkshire for instance.
The computer model suggests that losses on the  passenger services would be off-set by a profit on the freight.
Quite where this freight profit would arise is a puzzle, because most of the railways' profitable coal-carrying lines are closed under this option.
But the most serious problem seems to be the assumption that the load-factor on many lines would be constant.
That is, it assumes that the London-to-Penzance train will carry the same number of passengers all the way from London to Penzance.
This is obviously nonsense.
It will be full when leaving London and nearly empty at Penzance.
This feature of the model produces a number of lines which end in odd places.
In option C3 the line from Paddington to the south-west stops at Exeter.
Yet by extending the line to Plymouth and Torbay the passenger revenue could be doubled, while the length of the line would be increased by only 34 per cent.
The sensible places to cut this line would be Bristol or Plymouth — certainly not Exeter.
Even more ludicrous is the truncation of the main east coast line in Scotland, under option C2, at Keith in Banffshire — population 5000; main attribute a peat bog.
The quest for the W particle
Latest results from Europe's centre for particle physics point to the possible discovery of the W, a particle that plays a key role in theoretical attempts to unite two of nature's forces
Christine Sutton
AN ATMOSPHERE of excitement mingled with a fair amount of caution and some controversy has taken over CERN, Europe's centre near Geneva for research into subatomic particles.
In seminars held there last Thursday and Friday two groups of scientists announced what may turn out to be the first evidence of CERN's greatest discovery yet, a particle known simply by the letter W. If proven the discovery will vindicate all those who pushed for the means to make it possible.
More importantly, it will indicate that theorists are following the right path in their attempts to unite the disparate forces of nature within one mathematical framework.
Why should ‘yet another’ subatomic particle cause so much excitement?
The story really begins in 1935 when Hideki Yukawa, a Japanese theorist, put forward the idea that the forces operating in the atomic nucleus should have associated with them specific ‘exchange’ particles.
These would flit between the protons and neutrons in the nucleus ‘carrying’ the nuclear forces.
There was already a strong conviction that the electromagnetic force acted in this way, carried by photons (’ packed of light’) passing between electrically-charged particles.
Thus Yukawa proposed that the strong nuclear force, which holds protons and neutrons together, was carried by particles he named ‘mesons’.
These mesons were eventually discovered in experiments with cosmic rays — energetic particles from outer space — in 1947.
Yukawa also suggested that a similar particle must mediate the weak nuclear force, which underlies the radioactive decay of a neutron into a proton, an electron and a virtually undetectable particle called the neutrino.
The weak force is some thousand times weaker than the electromagnetic force, and 100 000 times more feeble than the strong nuclear force.
But like the strong force, and unlike the electromagnetic force, the weak force has a finite range: it acts only over distances of the size of a nucleus, about 10–13cm.
The feeble nature of the weak force and the short interaction distance together imply something about the nature of the particle exchange: it must be relatively heavy.
The particle being exchanged in a sense ‘borrows its mass from the force field; the heavier the mass, the shorter the time it can be borrowed, and, therefore, the shorter the range of the force.
(The photons that carry the electromagnetic force have no mass; this corresponds to the force having infinite range, as experiments show to be the case.)
Yukawa's meson, the transmitter of the strong force, turned out to have a mass roughly 15 per cent that of the proton and some 270 times that of the electron.
However, even simple estimates for the W particle suggest that it should weigh in at something like 30 times the proton's mass.
In the late 1960s theorists began to acquire a more precise concept of the role of the W particle in weak interactions, and could make more precise predictions about its mass.
In particular they made great progress in their attempts to put electromagnetism and the weak nuclear force together within the same theoretical framework.
It turned out that a mathematically-respectable ‘electroweak’ theory required three exchange particles for the weak force: two electrically charged particles, W+ and W-, and a third neutral partner which has become known as the Zo.
An important boost for this theory carne in 1973 when experiments at CERN discovered the so-called neutral currents: weak interactions between particles that occur via the exchange of something neutral, in other words the Zo.
Over the years more and more evidence consistent with the electroweak theory has come from numerous experiments not only at CERN but at particle accelerators throughout the world, and in 1979, Heldon Glashow, Abdus Salam, and Steven Weinberg received the Nobel prize in recognition of their theoretical endeavours to unite the weak and electromagnetic forces.
But still there was no explicit experimental evidence that the W and Z particles, key figures in the theory, did indeed exist.
According to electroweak theory, and using results from these other experiments, the W particle should have a mass of 80 giga electron volts (Geff, or 80 times the mass of the proton.
This in itself is sufficient to explain why the particles have not been observed previously.
To create this mass, rather than‘borrow’ it as in the exchange, requires an equivalent amount of energy.
In the mid 1970s there already existed accelerators, both at CERN and at Fermilab in the US, that could give protons with energies up to 450 GeV.
At first sight this energy seems sufficient to produce ‘free’ W particles copiously.
But the high-energy protons from these machines are directed at targets containing stationary protons and much of the energy of the bombarding particles is used up in giving kinetic energy to particles of relatively low mass — protons, mesons and so on— created in the collision.
The energy left over, about 30 GeV, is not enough to create W particles, if electroweak theory is correct.
One way round this apparent impasse is to collide together particles with equal amounts of energy travelling in opposite directions; then all the energy is available to create new particles.
And Carlo Rubbia, from CERN, along with David Cline, from the University of Wisconsin, and Peter McIntyre proposed an ingenious way to convert existing accelerators such as the 450 GeV Super Proton Synchroton (SPS) at CERN, to operate as particle colliders.
Their idea was to feed antiprotons, like protons but oppositely charged, into the main magnet ring of the proton accelerator.
The antiprotons would follow the same path through the magnets — but in the opposite direction to the protons — and would be accelerated to the same energies.
Then at the right moment the two beams of particles could be brought together to collide.
 Convinced by Rubbia's dynamism and unique art of persuasion, CERN agreed in 1978 to convert the SPS to a proton-antiproton collider.
The plan was to install experiments at two of the points at which protons and antiprotons would collide on their journey round the accelerator's ring of magnets.
The main experiment at one point, code-named UAI, has been designed under Rubbia's direction by over 120 scientists from Austria, Britain, France, Germany, Italy, the US and CERN.
At the other point the primary experiment UA2 has been put together by 50 or so researchers from France, Denmark, Italy, Switzerland and CERN under the leadership of Pierre Daulat.
Both UA1 and UA2 were designed to be capable of detecting W and Z particles.
The two sets of apparatus complement each other to a certain extent and also provide a useful means of cross-checking results, as well as a certain amount of friendly competition.
By July 1981, physicists and engineers at CERN had succeeded in building a suitable source of antiprotons that could feed the SPS.
It was then that the first proton-antiproton collisions at 270 GeV per beam took place (New Scientist , vol 91, p 456).
The energy of these head-on collisions is equivalent to that of a beam of 155 000-GeV protons striking a stationary target: surely enough to reveal the W and Z particles if they exist.
Since a year last August scientists working on UA1 and UA2 have been putting the finishing touches to their apparatus, and at the same time machine physicists at CERN have been working to get a large enough number of collisions in the SPS.
Only a few of the particles fed into the SPS actually collide; the remainder just fly past each other.
And only in a minute fraction of the actual collisions does theory suggest that a W particle will be produced; many different processes compete to take place each time a proton and antiproton meet.
Coupled with problems that arose with the apparatus — UA1 for example was beset by difficulties ranging from dirty air-lines to burst pipes — it was not until last November and December that the experiments were able to observe a large sample of proton-antiproton collisions.
Now, it seems that the waiting may have borne fruit.
In a seminar last week at CERN, Rubbia presented five events, or collisions, in which the particles that emerge are consistent with what is expected for a particular mode of decay of the particle.
The following morning, Darriulat presented four events detected in UA2.
How do the experimenters know what to look for?
The situation is similar to that for neutron decay, the reaction for which theories of the weak force were first developed.
A neutron decays into a proton, electron and neutrino.
This process can be represented on a diagram as shown in Figure 1 with the neutron meeting, as it were, a ‘backwards’ proton to produce a W particle.
The W then decays to an electron and a neutrino.
A ‘backwards’ proton is in fact nothing more than an antiproton: what the experimenters at CERN look for is a proton (rather than a neutron) meeting an antiproton to produce a W particle which then decays to an electron and a neutrino.
At a more fundamental level theorists describe the creation of the W particle in terms of quarks, the building blocks from which the protons, neutrons and their antiparticles (and many other particles) are made.
The proton comprises three quarks; the antiproton is made of three antiquarks.
One of the quarks and one of the antiquarks meet and, in a process called annihilation, produce a W particle.
This then decays (Figure 1 d).
By generating mesons, the carriers of the strong nuclear force, the remaining ‘spectator’ quarks materialise as swarms of low-energy particles.
But the electron carries a high energy transverse to the direction of the collision, and thus bears the signature of the W particle that gave it birth.
The neutrino also carries with it a large transverse energy but the particle's properties are such that the experiments at CERN cannot detect it and it escapes the apparatus unseen.
The ‘signal’ to look for, then, to observe a W, is a single electron carrying lots of energy.
The UA1 team has found five occasions on which a single electron with all the right characteristics was produced.
This was out of the 850 000 or so collisions the UA1 apparatus recorded towards the end of last year from a total of about one thousand million actual collisions.
The UA2 team has found four similar ‘events’ out of roughly the same total number of collisions.
Are these events, like the one shown in Figure 2, really the footprint of the long-awaited W particle?
The events have everything going for them.
The apparatus in both UA1 and UA2 cannot specifically identify electrons, but it can eliminate other possibilities.
In the case of UA1, for example, a gas-filled cylindrical detector surrounding the collision point senses the tracks of electrically-charged particles as they move through a magnetic field.
Wrapped around  around this is a detector (a calorimeter) designed to collect all the ‘electromagnetic energy’ carried by electrons and photons.
Beyond this a third detector picks up all the energy of hadrons — particles like protons and mesons that are built from quarks.
In the five interesting events the detectors of UAI show a track in the central detector leading to a single cell in the electromagnetic calorimeter, which picks up a lot of energy.
But the track leads to no signal at all in the hadron calorimeter (Figure 2).
This track, it seems,must belong to an energetic electron.
 Furthermore , calculations on the balance of energy in each event are consistent with there being a missing energetic neutrino.
Together the energies of the electron and neutrino give a lower limit of 73 GeV for the mass-energy of the intermediate state that created them, remarkably close to the value that theory predicts for the W particles.
And, to make it all seem almost too good to be true the total number of possible W particles seen by both UA1 and UA2 is just what one would expect from the total number of proton-antiproton collisions that the two experiments have observed.
So far so good.
But why the caution? the analyses of the data have been done extremely quickly, cutting corners, albeit carefully, to single out precisely the kind of event theory predicts for a W particle.
There is some feeling that maybe by looking hard enough you are forcing yourself to see what you are looking for.
After all, the researchers have looked just where the theorists told them to look.
But the teams are rapidly running out of alternative explanations for these events.
As Rubbia says, ‘It looks like a W, feels like a W, smells like a W; what we need is more running time’.
What is really needed to clinch things is not just more data, but some sign of the Zo particle, which, if the electroweak theory is correct, should also be produced.
Theory predicts that roughly one Z particle for every 10 W particles should be produced in proton-antiproton collisions at SPS energies.
So the experimenters are at the stage where they might just have expected to have seen one Zo.
However, neither UA1 nor UA2 has spotted a potential Zo so far, although this particle should be easier to identify than the W. It is predicted to decay into an electron-position pair, which would leave two clear tracks in the apparatus.
Who knows, maybe it will appear with time, and by the end of the year we will be able to look back at Figure 2 and say that this was one of the first proofs of the existence of the W particle?
Infrared astronomy by satellite
The launch of the world's first infrared satellite could change completely astronomers' ideas of the skies at these  wavelengths 
Christine Sutton
AN INTERNATIONAL mission in space to explore the Universe at infrared wavelengths will begin in earnest following the launch of the world's first satellite dedicated to detecting this part of the electromagnetic spectrum.
IRAS, for Infrared Astronomy.
Satellite, is a project developed jointly by scientists and engineers from the Netherlands, the US and Britain.
Its purpose is to survey systematically the whole sky in the infrared, from a vantage point well above the obscuring effects of the Earth's atmosphere.
Launched this week aboard a two-stage Delta rocket from NASA's Western Test Range at Lompoc, California, IRAS has taken nearly 10 years to progress from the drawing board to lift-off.
The satellite was first conceived as a successor to the ANS (Astronomical Netherlands Satellite) which the Netherlands Agency for Aerospace Programs (NIVR) had flown in the early 1970s.
While the ANS had been highly successful in pioneering studies of the ultraviolet and X-ray sky, the new satellite would turn to the longer wavelengths of the infrared.
NASA and the then Science Research Council, in the UK soon became interested in the prospective infrared mission, and in October 1977 the three countries signed contracts formalising the collaboration.
The course since then has not been all plain sailing, as changes in the launch date, originally set for August 1981, reflect.
The problems that have beset the project, however, are a result not of  misjudgement or incompetence but of real difficulties associated with infrared astronomy, which have meant that IRAS has required certain aspects of space technology to be developed well beyond previous limits.
The primary problem that all infrared astronomers face, whether using instruments on Earth or on a satellite, is that all objects emit infrared or ‘heat’ radiation.
Thus a telescope designed to detect infrared radiation itself emits some radiation as a ‘background’, from which the image (or ‘signal’) that the telescope is supposed to detect must be extracted; statistical fluctuations in the radiation, known as ‘photonic noise’, cause further difficulties.
The telescope for IRAS, built in the US, overcomes these basic problems by carrying with it its own giant cooling system, around which have centred some of the problems that have delayed IRAS.
The satellite's telescope is surrounded by a huge double-walled dewar, a vessel like a Thermos flask, filled with liquid helium.
The liquid cools the telescope's minors to around 16 degrees above absolute zero (16 K) thereby minimising the thermal background from the telescope itself.
The dewar, 3.6 m high and 2 m in diameter, holds 70 kg of liquid helium, sufficient to keep the telescope cool for around 6 months, by which time, calculations indicate, the helium will have boiled and the temperature of the whole assembly will rise rapidly.
The dewar is the largest ever to have been flown in space, although smaller devices operate successfully.
The telescope on IRAS is a modified form of the so-called Cassegrain design, employing two mirrors.
A concave primary mirror, made of beryllium and 57 cm in diameter, reflects incoming light onto a small convex secondary mirror situated 75 cm above the primary.
The secondary mirror focuses the light through a small hole in the primary mirror, down onto an array of 62 detectors located in the telescope's focal plane.
The detectors are strips of silicon and germanium, ‘doped’ with small amounts of arsenic and gallium respectively.
These semiconductors have been selected to respond to infrared radiation in four different wavebands, around 10, 20, 50 and 100 micrometres, a range of wavelengths that covers the maximum emission from bodies radiating at temperatures from 400 down to 40 K. (‘Room temperature’ is just below 300 K.) The array will be used to pick up infrared sources within the telescope's field of view of half a degree, that is, about the size of the Sun's diameter as seen from Earth.
A knowledge of the telescope's direction will then give the positions of the sources typically to within half an arc minute.
Also situated in the telescope's focal plane are three instruments that scientists from the University of Groningen in the Netherlands have built.
One additional device is a spectrometer to study the infrared spectra of strong point sources.
Another is the so-called ‘chopped photometric channel’, which has a smaller field of view than the main survey instrument.
It will map the absolute intensities of individual objects, such as galaxies and dust clouds, at two wavelengths, 60 and 100 micrometres.
Finally, the Dutch scientists are providing a short-wavelength device (5 micrometres) with a small field of view which will essentially count stars.
The Netherlands space agency, NIVR, had been responsible for the spacecraft that carries the telescope, and which houses the instruments to control the satellite's ‘attitude’, or pointing direction, and to store the data collected.
The satellite carries a tape recorder which can hold up to 14-hours worth of data.
This will disgorge the stored information twice a day, at alternate intervals of 10 and 14 hours, as IRAS passes over the operational control centre at Chilton in Oxfordshire.
The antenna that will pick up the signals is a 12-metre dish built in the US and specially erected at the Rutherford and Appleton Laboratory at Chilton.
Once received the data will be converted from radio to microwave signals and passed along cables to the control centre nearby.
A team of some 100 scientists and engineers at the laboratory, including about 15 astronomers from all three participating countries, will perform a quick, preliminary computer analysis of the data.
This will be not only to gain a first impression of the infrared sky but also to check that the satellite is functioning properly.
The team can, for example, check the efficiency of the semiconducting detectors by comparing the new data with that for known infrared sources.
Such information allows them to plan ahead, using data from the previous period perhaps to make corrections after the next pass of the satellite over Chilton.
The antenna also transmits to IRAS during the short interval the satellite is overhead.
The transmitted message contains details of all that the satellite has to do in the next period, in particular to implement the survey.
It may also include instructions to observe particularly unusual sources; to repeat collection of data that have been lost or spoilt in some way; or to take corrective actions based on data already received.
IRAS's orbit takes it round the Earth every 100 minutes, passing within 9o of the poles on a path 900 km high, well above the water vapour in Earth's atmosphere which absorbs much of the infrared radiation from space.
During one orbit the satellite observes a band of sky half a degree wide.
The telescope will be able to scan two opposite segments of sky, each 30o wide, in 15 days.
During this time the field of view scanned on successive orbits will overlap by ¼°, ensuring that suspected infrared sources can be sufficiently well confirmed to support the astronomers' aim that the survey will be 99.8 per cent reliable.
The precession of the satellite's orbit — the way it swings slowly round the Earth — will mean that successive 30o-segments will overlap by 15o, so assisting the procedure of confirming sources.
At times when the satellite is not involved in surveying, for example when passing near the poles, IRAS will observe objects that are particularly interesting astronomically.
In this mode the orbiting telescope will operate like one on Earth, following suggestions put forward by scientists from all three participating countries.
What do astronomers hope ultimately to learn from IRAS?
The satellite's primary role is to fill in the existing picture of the heavens, revealing stars, galaxies, clouds and so on that are not seen at the shorter optical and ‘near’ infrared wavelengths (less than 5 micrometres), or at the longer wavelengths observed by radio telescopes.
Objects may appear at infrared wavelengths because they are cool, because they are obscured by dust that transmits only longer wavelengths, or because their intrinsic mechanism of radiation cuts off at wavelengths shorter than infrared.
So far, infrared studies at observatories such as the UK's Infrared Telescope (UWIRT) on Hawaii have centred only on those objects astronomers already know about from information at optical and radio wavelengths.
IRAS's survey will, however, be totally unbiased by existing knowledge, and should thus provide a completely revised picture of the infrared sky.
While discussions of what astronomers expect to find can only be expressed in terms of what they already know, those working on IRAS clearly hope that the satellite will make exciting new discoveries.
The survey data will be  analysed fully at the Jet Propulsion Laboratory in California, where a catalogue of all sources discovered, expected to number in the region of 1 million, will ultimately be made.
This should provide enough new information to keep astronomers busy for many years to come, those in the UK being particularly well placed to follow up the survey with work on UKIRT on Hawaii.
Clearly infrared observations are set to make a large impact on astronomy and astrophysics during the next few years.
MONITOR
Rearranged genes can switch on cancer
ANY cancers, and in particular leukaemias, are marked by characteristic chromosome abnormalities.
Cytogeneticists have long suspected that these chromosomal anomalies are linked with cancer, but only now has their message been deciphered.
The crucial advance was the discovery that the cancer-causing genes (oncogenes) of animal tumour viruses have counterparts (proto-oncogenes) in the perfectly normal cells of all animals including man (New Scientist , vol 96, p 418).
These proto-oncogenes can apparently cause cancer when something happens to disrupt their normal activities.
And that something according to three simultaneous reports in Proceedings of US National Academy of Sciences (vol 79, p 7824, 7837 and 7842), may be a chromosomal rearrangement.
The three papers all focus on the same abnormality: an exchange of material (translocation) between two chromosomes (chromosomes 8 and 14) that is particularly common in a tumour known as Burkitt's lymphoma.
This translocation is probably a perversion of the normal activities of some cells.
For the cells which give rise to the lymphoma are white blood cells that secrete antibodies, and an antibody secreting cell normally rearranges the chromosomes that contain the antibody genes during its differentiation.
These rearrangements collect antibody genes from segments of DNA that are very widely separated on different chromosomes.
In man, there are three such chromosomes: chromosomes 14, 2 and 22.
During the normal differentiation of an antibody-producing cell, each of these chromosomes undergoes drastic internal reorganisations that bring the widely separated segments of the antibody gene together.
At the same time they activate the rearranged DNA so that large quantities of antibody protein are produced from it.
In the perversion of this process that typically occurs in Burkitt's lymphoma cells, any of three chromosomes containing the antibody genes, instead of reorganising their own DNA, exchange some DNA with chromosome 8.
This led George Klein, of the Karolinska Institute in Sweden, to suggest (Nature , vol 294, p 213) that these translocations might not just activate the antibody genes on the normal rearranged DNA — they might also activate a proto-oncogene on chromosome 8.
Klein's suggestion was one major source of inspiration for the research that culminated in three papers in PNAS all reporting that chromosome 8 does indeed contain a proto-oncogene — a gene known as myc , after the myelocytomatosis virus in which it was discovered.
The other source of inspiration — and indeed guidance — for the research was an experiment by William Hayward and his colleagues at the Sloan-Kettering Cancer Center in New York (Nature , vol 290, p 475), Their work on a virus-induced lymphoma of birds inspired the researchers to look specifically for the myc gene — rather than for any of the 13 other known proto-oncogenes.
The bird virus studied by Hayward and his co-workers does not contain its own oncogene and is very much less efficient in causing cancer than those viruses that do.
They suspected that it might work by inserting its own genes — and in particular the ‘ promotor ’ DNA that activates them — next to an innocent cellular proto-oncogene, thus  stirring it to an unnatural and cancer-causing level of activity.
Their suspicions were borne out by a careful analysis of six different tumours; in each case the viral  promotor turned out, as expected, to have inserted itself next to the proto-oncogene myc .
This result made myc the obvious gene to look for in other lymphomas — and led to its identification on chromosome 8.
Hayward and his collaborators are among those to have reported this discovery in ANAS (vol 79, p 7842); they, and a collaborative team led by Bob Gallow of the US National  Institutes of Health and Carlo Croce of the Wistar Institute in Philadelphia, have shown that the myc gene is strategically placed at the breakpoint where the translocated fragment of chromosome 8 links up with chromosome 14 in lymphoma cells.
At the same time, Phil Ledler, Stuart Aaronson and G. Lenoir have gone further and shown that in Burkitt's lymphoma cells the myc gene becomes linked with DNA-encoding parts of the antibody molecule.
But while these reports neatly confirm Kein's and Hayward's ideas about the activation of proto-oncogenes in leukaemias and lymphomas, they have also helped to unleash a surprisingly violent controversy about how the proto-oncogene is activated.
Harvard's experiments on bird tumours, for example, showed that viral insertion increased the expression of the myc gene — and Croce and his collaborators reported that they found increased expression of myc in Burkin's lymphomas (Science , vol 21 8, p 983).
But others have failed to find such quantitative increases, and suspect instead that in the course of translocation, the alene is altered in a way that causes a qualitative change in its product.
Since no one knows what the myc product is, this idea cannot be directly tested.
But comparisons of the sequences of the myc genes of different lymphomas should help to provide some insight into the problem.
Already, Michael Cole and his colleagues at St Louis University have evidence that in an equivalent translocation in a mouse tumour cell the translocated myc gene is considerably changed in transit (Cell , vol 31, p 443).
But however heated the protagonists may become about whether proto-oncogene activation entails qualitative or quantitative changes, no one on the outside will be very surprised if it turns out both are right.
Magnets may lead us by the nose
PIGEONS, bees, bacteria and dolphins have an uncanny sense of direction, finding their way over relatively huge distances.
They also turn out to have deposits of magnetite, a magnetic oxide of iron, in their bodies.
Navigational instincts in humans are less well-developed, although Robin Baker of Manchester University is convinced that we possess them (New Scientist , vol 80, p 526).
Now Baker reports that man also has a potential ‘magnetic sense organ’, a concentration of magnetic in the bones in the base of the skull which form the nasal cavity (the sphenoid/ ethmoid sinus complex).
Baker and his colleagues, Janice Mather and John Kennaugh, examined bones and tissues from various parts of the bodies of five recently dead subjects.
Using a magnetometer, they found that only in the bones of the sinuses was magnetic remanence more than twice the background level — and in those bones it was up to 13 times background.
When they examined thin sections of their samples under the microscope, stained to pick up particles of iron, they found appreciable concentrations of the metal only in these same bones (Nature , vol 301, p 78).
Baker suggests three possible reasons why human sinus bones should be naturally magnetic.
One is that they are a dumping site for iron; another is that the magnetic material is involved in the growth and repair of bones; and the third is that the magnetic deposits are concerned with detecting magnetic fields as an aid to navigation.
Although he suggests that the three possibilities need not be mutually exclusive, Baker's sympathies clearly lie with the last of these; but many scientists still need convincing that man can make use of a built-in magnetic compass, even if he  possesses one.
Sun's magnetism dims the sunshine
ASTRONOMERS have traditionally regarded the sun as an absolutely constant source of light and heat, so much so that the amount of radiation reaching the Earth (at its average distance) is called the ‘solar constant’.
But recent research shows that the ‘constant’ changes by a fraction of a per cent, both over a period of weeks and over several years.
Radiation from the sun is attenuated by the Earth's atmosphere, so several research groups uses high-flying experiments in the late 1960s to measure the solar constant.
The average of results from balloons, the X-15 rocket-plane and the Mariner 6 and 7 spacecraft to Mars placed the solar constant at 1364 W/C sq.m.
Scientists were satisfied with this figure, and the constant was left undisturbed until 1975.
Then it was realised that even small changes in the sun's output affect the Earth's climate significantly.
And by that time, astronomers could monitor such minuscule changes (less than one per cent)— either directly with new radiation detectors on board orbiting satellites, or indirectly from the ground by  analysing particular lines in the Sun's spectrum.
The Nimbus 6 and 7 weather satellites first carried the improved radiation detectors (radiometers), but the solar Maximum Mission (SMM) satellite, designed to study sunspots at their most active, has by far the best radiometer ever launched.
It was finally  incorporated in the SMM, launched early in 1980 near the time of maximum sunspot activity.
These satellites and more recent balloon and rocket flights show that in the first six years of the 1970s the Sun's radiation had increased by 0.2 per cent.
But since 1975, the Sun has been dimming again.
The SMM radiometer suggests the Sun's radiation is decreasing at a rate of 0.06 per cent per year.
The Nimbus radiometers are less accurate, but have been operating longer: they indicate a dimming of 0.02 per cent per year.
The spacecraft results also confirm that ground-based observations can indirectly tell us about fluctuations in the solar constant.
The Sun's luminosity is proportional to the fourth power of its temperature, according to Stefan's law.
So the changes in the spectral lines tell us of changes in the Sun's temperature, which in turn reveal changes in the solar luminosity.
Since 1975, William Livingston has been using the world's largest solar telescope, at Kitt Peak in Arizona, to study these temperature-sensitive spectral lines.
He has found that the Sun's average temperature of about 5800 K dropped by about 5 K between 1975 and the early part of 1981.
Converted to luminosity, this means that the sun's brightness has been dimming at a rate of 0.05 per cent per year, a figure that is in good agreement with the satellite results.
The comparison of satellite and ground-based results means that we can follow changes in the solar constant — on the broad scale at least— with existing solar telescopes on Earth, without the complication and expense of orbiting radiometers.
Livingston's latest results are already telling us something new; he has found that the Sun's temperature (and thus brightness) reached a minimum in the summer of 1981, but has been gradually increasing since then.
These are the results; but what is causing the variation?
Sunspots are the obvious culprit.
These small dark patches on the Sun's surface occur where the underlying magnetic field breaks through.
The magnetism inhibits the flow of heat upwards from inside the sun, cooling the sunspot regions to about 4000 K, so they appear dark.
The number of sunspots varies with a cycle of 11 years.
When there are many sunspots on the sun's face, the sun should be slightly dimmer, and the SMM radiometer has indeed detected this effect.
As the Sun's rotation brings a large spot, or group of spots, around to face the Earth, the sun's luminosity falls by 0.1 or 0.2 per cent.
But the longer-term variations cannot be due directly to sunspots, The most recent peak in the number of sunspots, the period known as the solar maximum, was in 1979 — yet according to Livingston solar luminosity was at its lowest two years later, in 1981.
Livingston suggests that the main agent is the sun's general magnetic field, following a theory proposed in 1980 by E. A. Spiegel of Columbia University and N. O. Weiss of the Harvard-Smithsonian Center for Astrophysics.
They say that the concentrated magnetic fields in sunspots spread out all over the sun somewhat after the maximum of solar activity.
Just as the field in sunspots suppresses the flow of radiation locally, so the spread-out field inhibits the general upflow of radiation all over the sun.
Although the effect in any one region is small, the cumulative result is to dim the Sun by a fraction of a per cent, a year or two after solar maximum — just as observed.
These investigations of the sun's luminosity are not just intellectual curiosity.
A change in the Sun's output by 0.2 per cent alters the Earth's temperature by 0.2°C, and this should have noticeable effects on our climate and weather patterns.
The new data on the temperature of the sun are being carefully studied not only by solar astronomers, but also by meteorologists and climatologists.
No radiation hazard from VDUs — official
VISUAL display units are not the radiation hazard some people would have you think, according to E. A. Cox, Her Majesty's Principal Inspector at the Health and Safety Executive.
Despite the recent debate on VDU health hazards, which has raged furiously on both sides of the Atlantic, Cox report that ‘there are no radiation levels that could be considered to be of concern to the normal individual’(Displays Technology and Applications , vol 4, p 7).
This is not to say that VDUs are  necessarily safe.
Glare and flicker from the screen can strain eyes and poor layout of workstations can cause discomfort.
But these ergonomic problems can be avoided by good design and precautions for users.
Radiation has caused more of a scare.
The alarm was first raised in America when some VDU operators on The New York Times developed cataracts, and mothers working on VDUs on the Toronto Star gave birth to defective children.
Report of rashes and miscarriages followed.
In the UK the HSE  commissioned a study of all home-produced and imported VDUs, measuring radiation levels emitted from VDUs with number-packed screens at full brightness.
Readings were compared with recommended levels of radiation, from the X-ray to the long radio-frequency microwave regions.
In most cases, emission levels were orders of magnitude below standard recommended levels.
The few high readings were attributed to high background levels, the power supply or transformer.
One abnormally high microwave radiation persisted even when the VDU under test was switched off; its source was never traced.
From the US, the computer consultant Carl Machover, speaking at the Computer Graphics conference in London last autumn, echoes the HSE's findings.
‘There appears to be no danger whatsoever associated with radiation — hard or soft — from cathode-ray tubes.
My judgement is that many of the problems are grossly exaggerated.’
Machover puts it down to psychosomatic causes.
Alienate people from the new technology, he says, and they will complain about it.
But radiation from VDUs at very low frequencies may give cause for concern, say scientists at the Ginadian Centre for Occupational Health and safety.
Some biologists suspect that these pulse-modulated radiations may be more harmful than unperturbed waves.
Low frequency radiations may create voltage gradients high enough to damage the surface of living cells — perhaps causing birth defects or miscarriages.
The Canadian researchers say that tests on animals are needed, but such work would take some years.
Mathematical modelling and  epidemiological studies may help quantify risks in the meantime.
It may be wise, they conclude, to restrict the use of VDUs by pregnant women.
Marriage of viruses produces new flu vaccine
BY JUGGLING genes in two types of flu viruses a group of American investigators has produced a live-virus flu vaccine that shows every promise of being effective in humans.
The results could be vaccines against flu that provide much longer immunity against influenza, than do the present killed-virus vaccines.
The new vaccine is the result of a ‘marriage’ between a strain of flu virus that infects birds, it came originally from mallard ducks, and a strain of human flu virus.
The genes that control the virus's ability to multiply come from the bird strain, and those that control the development of the virus's antigens come from the human virus.
In terms of providing protection against influenza there are two antigens of significance — haemagglutinin and neuraminidase.
These antigens are part of the viral coat and stimulate the body's defences against the infection.
Unfortunately, by themselves they cannot induce immunity satisfactorily.
So killed virus is ineffective as a vaccine and live viruses merely causes disease in humans.
Dr Brian Murphy and his colleagues, at the National Institute of Allergy and Infectious Diseases at the US National Institutes of Health have overcome this dilemma by growing a bird flu virus which does not cause human disease together with a strain of human flu virus.
After mating these viruses, they pull out the avian  haemagglutinin and neuraminidase antigens using antisera previously produced by goats following exposure to the antigens.
They then grow the virus at 42°C, a temperature at which the avian virus grows well but the human virus does not flourish.
The result is a hybrid virus that will multiply readily when given to humans but will not cause disease.
At the same time, it carries the  haemagglutinin and neuraminidase antigens from the human virus which are important in stimulating the production of protective antibody.
The principle behind the technique is now a familiar one and widely used by vaccine manufacturers to obtain large quantities of virus for the standard killed flu vaccines (New Scientist , vol 56, p 630).
The only difference is that where the vaccine manufacturers use two strains of human flu viruses-one that grows well in hen's eggs and the other with the currently circulating strain of flu virus so as to get the right antigens — the object in the latest work is to obtain a non-disease causing flu virus that will grow in humans but will also protect against the particular strain of wild type flu that is in circulation.
If the latest work comes to practical fruition, it will still be necessary to make new batches of flu vaccine whenever a different, virulent strain of flu makes an appearance.
Flu viruses are notorious for the ease in which they undergo such antigenic shifts, as they are called, and so giving rise to epidemics.
However, the immunity that the live virus vaccine confers may be much more effective and longer-lasting than the present vaccines made with killed viruses.
The basic problem with these killed virus flu vaccines is that they do not provide either complete protection or retain their effectiveness for very long.
So quite apart from the need to make new vaccines against flu from time to time to deal with the problem of antigenic shift, individuals have to be vaccinated against flu every year and even then the vaccine may not protect more than 80 per cent of them.
Tailless lizards are social outcasts
WHEN attacked by predators, many lizards escape capture by shedding their tails.
But the lizard escapes only at a price.
For, according to recent research, tail loss, known as tail autotomy, can herald a decline in social status which actually threatens the lizard's prospects of survival (Science , vol 218, p 692).
Stanley Fox and Margaret Rostker of Oklahoma State University investigated the effect of tail loss on the social status of a desert lizard,Uta stansburiana , by observing changes in dominance which followed tail-shedding.
The researchers simulated the bite of a predator by pinching the lizard's tail.
Initially, lizards with intact tails were paired in the laboratory in order to determine dominance relationships.
After this first encounter, the researchers removed portions of the lizards' tails and reintroduced the pairs.
Lizards which lost two thirds or more of the length of their tails showed a significant decline in dominance.
Role reversals were common.
An initially subordinate lizard with an intact tail could intimidate a previously dominant lizard which had a third of its tail missing.
But the victor was vanquished in turn once it lost two thirds of its tail.
Losing a tail is a serious physiological drain on the lizard's resources as well.
The lizard loses fat stoned in the tail, and yet has to find the wherewithal to repair its wounded body and regenerate its tail.
This new research shows that the loss also causes a decline in the lizard's ability to dominate other lizards, leading to a loss of territory, feeding and breeding opportunities, thereby reducing its chances of survival.
so tail autotomy is a costly process; for the lizard it is most probably a defence of the very last resort.
Cheaper solar cell
AMERICAN scientists have doubled the efficiency of liquid-junction solar cells.
This type of solar cell is now a possible alternative to its solid-state counterpart.
Chris Gronet and Nathan Lewis, from the Department of Chemistry at Stanford University, California, made the cell from a mixed gallium arsenide — gallium phosphide semiconductor, and acetonitrile (a cyanide derivative of methane) containing a dissolved charge carrier.
Sunlight was converted to electricity with an efficiency of 13.2 per cent.
In solid-state solar cells the junction between two materials of different electronic properties absorbs light and converts it into electricity.
Normally one of these materials is a semiconductor and the other a metal.
In a liquid-junction cell a liquid containing a  dissolved compound (to conduct electricity) replaces the metal.
The main advantage is that it makes the junction much simpler.
With water as the liquid, scientists achieved efficiencies of 10 per cent, but semiconductor electrodes tend to rust.
Non-aqueous systems resisted corrosion, but were not very efficient.
But Gronet and Lewis have now shown that non-aqueous solvents can give good efficiencies.
The Stanford chemists knew that neither gallium arsenide or gallium phosphide worked well with acetonitrile.
But when they mixed the materials in a ratio of GaAu GaP, a single-crystal electrode achieved conversion efficiencies of 13 per cent in natural sunlight and a high 22 per cent with a helium-neon laser.
Even greater efficiencies may be possible.
Present-day solid-state solar cells with single-crystal silicon can reach efficiencies of 20 per cent, which is still some way ahead of this liquid-junction cell.
But the single-crystal is expensive and scientists are making cells of cheaper polycrystalline silicon.
Such cells have only achieved efficiencies of 5–6 per cent, due mainly to problems with the junction between the two solids.
This is precisely where the liquid-junction cell may hold an advantage.
If it works well with  polycrystalline electrodes, it could win the day.
Soviet pipeline goes full — speed ahead
PRESIDENT REAGAN'S  attempt to stop the export of American pipeline machinery to the USSR has had little impact on the construction of the 4451 -kilometre Siberian gas pipeline.
The  Russians seem determined to show the world that they can manage on their own.
The Russians call the pipeline, which has a diameter of 1420 mm (big enough to walk through, stooping), the ‘project of the century’.
It crosses two mountain ranges, 561 rivers, 124 km of permafrost and more than 1000 km of West Siberian bog and marsh on its journey through five time zones to Western Europe.
Building this pipeline means working in temperatures ranging from -5O°C in winter to +45°C in the Siberian summer, when thick swarms of mosquitoes add to the problems.
It means, above all, transporting men and heavy construction equipment and materials into previously uninhabited areas without roads, railways or developed waterways.
And it means doing this over immense distances because the Tyumen region, which contains the Urengoi gas field where the pipeline starts, is the size of Spain, Italy and France taken together.
The transcontinental pipeline that hit the news last year is only part of the project.
The Soviets are building a total of six such pipelines, each one between 3000 and 4000 km long.
The others will carry gas to central Russia, and are all scheduled to be completed by 1985.
The East-West pipeline is the fourth in the series, and the only one that involves importing pipes, turbines and equipment.
Three pipelines are now complete.
The East-West pipeline is nearly half welded and laid.
The engineers now want to finish the Siberian section by the end of winter.
Only innovative technology can achieve such a pace of construction, which the Soviets say is the fastest in the world.
Even before the US embargo, Soviet pipeline builders invented and decided to build 102 specialised construction machines to handle the conditions they had to work in.
The ETR-254 rotary excavator, which can shift 1200 cubic metres an hour and dig a trench to take a pipe 1420 mm in diameter even in frozen ground, is one example.
Others include an excavator for working in marshland, and a catamaran for excavating river crossings.
Across the Volga, for example, engineers had to dig a 16-metre deep trench to accommodate the pipe.
The catamaran can work to a depth of 25 metres, excavating 250–300 cubic metres an hour.
As a result of the US embargo, the first 18 big 25-megawatt turbines to be installed on the East-West pipeline next summer will be made in Russia.
The Russians hope that the Nevsky Zavod factory in Leningrad, which until last August was making no turbines over 16 MW, will do the job.
Similarly, the Soviets plan to supplement the Caterpillar and Komatsu side-boom pipelayers on which they have so far relied, with a new machine that the Sterlitamak works in the Urals will make.
But the key technological advance that has allowed the Soviets to speed up construction is the automation of welding.
Manual welding is slow, arduous and labour-intensive.
The Russians' Sever-1 automatic contact-welder has revolutionised pipeline construction.
This rocket-shaped machine can travel along the inside of the pipe, and weld a section together in four minutes instead of the 16 hours that a skilled welder would take.
It then proceeds inside the tube to the next weld.
The Paton Institute of Welding in Kiev developed the Sever-1.
The institute has also made two other automatic welders for pipeline builders, and a device for wet underwater welding appropriately called ‘Neptune’.
But the Sever-1, which has been patented the world over — and licensed in the US — is the most significant.
The Paton Institute is also at the heart of a further development that promises again to modernise Russia's pipelines in the second half of the 1980s.
This is a multilayered pipe, made of low-grade steel, that should be able to withstand pressures one third greater than today's pipes.
The increase will mean a significant increase in throughput, allowing the soviets to transport more gas per kilometre of pipeline.
Already, an experimental pipeline in northern Siberia has proved the success of the new pipe, and big steelworks such as the Vyksa works east of Moscow are re-tooling to make it.
The story of multi-layer pipe and the Sever-I illustrate how badly the soviet Union needs to transport the gas from its enormous Siberian reserves (Urengoi has over 7.5 trillion cubic metres of top grade gas) to earn hard currency.
The Oil and Gas Construction Ministry,: which is in charge of building pipelines, has set up a scientific institute devoted solely to the problems of transport by pipeline.
The institute is building a pilot coal pipeline at the Kuzbas coal fields, and is throwing out ideas such as container transport by pipeline for the future.
Surveyors get a better fix from lasers
RESEARCHERS at North-East London Polytechnic have developed a surveying instrument, based on a split-beam laser, that can define position within a cylinder 500 metres across and 4 metres tall.
The designers, Dr Barry Gorham and Jim Dudley, say that theoretically, by applying the same technology to microwaves, they could extend its range over a much wider area — even rain and snow.
Laserfix has two components, a source and a surveying staff.
The source, set at a convenient reference point, sweeps a composite beam at about 60 rpm.
The beam consists of two fan shapes at 45 o to the horizontal, and 90 o to each other.
They form a V-shaped cross-section that does not quite meet at its apex.
Light is most intense along the central axis of each fan.
A moire fringe unit monitors the horizontal azimuth of the beam as it sweeps so that it does not go out of alignment.
The horizontal distance from the staff to the surveying source is measured automatically: the beam geometry gives a coarse value, and a distance meter on the staff gives a fine value.
Photoelectric cells on the staff determine the height of the point being surveyed from the geometry of the beam as it sweeps across.
The beam needs to activate only two of the five cells on the staff.
The source also transmits a VHF signal for every 8 seconds of arc it passes through.
A box the size of a calculator, mounted on the staff, measures the angle and distance from source.
The distance is accurate to within 1 cm at a range of 250 metres, and the height is accurate to within 3 mm relative to the source.
The British Technology Group has developed Laserfix to the prototype stage, but will not say how much it cost.
Gorham and Dudley, who demonstrated the prototype last week, are now looking for a British company with expertise in optics and electronics to invest £250 000.
Watch this space
A POCKET television set with a liquid crystal screen will go on sale in the US in July.
The Japanese company Casio, which developed the TV, has also made two novel wristwatches: one acts as a thermometer; the other translates English into spanish and vice-versa from a dictionary of 1711 words.
If you are not in Spain, it can also manage a few words in five other languages.
The Casiovision pocket TV will sell for around $250.
Its 7-cm picture, which appears on a twisted nematic liquid-crystal display, is visible only in the dark because it is backed by an electroluminescent material which produces its own light.
In strong sunlight, reflections give an equally strong picture.
The set weighs 50 grams, measures 8.25 cm x 11.6 cm x 2.5 cm, and can run from mains electricity, a car battery or a rechargeable nickel cadmium cell.
The translator watch will sell at $130.
The company chose Spanish-speaking Americans as its target market, but the watch's memory can display common phrases in five different languages on a screen over the time display.
The thermometer watch can measure either air or water temperature — presumably for the benefit of winter swimmers.
Who invented the electronic camera?
DID A New Zealander, Gene Campbell, invent the electronic stills camera in 1979, two years before Sony launched its Mavica?
The Mavica records 50 pictures on a magnetic disc.
It is the size of a 35 mm camera.
Campbell's developmental prototype was about the size of a suitcase, and it used a ‘frame grabber’ to record the scene that a lens focused on a silicon chip.
Undaunted by Sony's arrival in the market, the New Zealander thinks that the electronic stills camera will be the size of a credit-card calculator in 10 years' time.
‘I see the lens mounted sideways on the ‘credit card camera’,’ says Campbell.
‘There would then be plenty of depth for shining back onto the silicon chip.
There would also be a pop-up chip that would be replaced like a film.’
Early models of Campbell's calculator camera reveal the ability to carry out essential photographic functions such as converting square miles into square kilometres, and cubic feet into cubic metres.
When, in 1979, he had found that the technology was available for his ideas, Campbell took out patents.
The following year he visited Japan.
He wanted someone to make him a prototype.
‘I visited Sony, Toshiba National and Hitachi in December 1981 with my proposals and designs.
They all promised to at least study my proposals.
None suggested that they would like to enter the electronic still camera business.’
In the meantime Campbell had  commissioned the New Zealand Department of Scientific and Industrial Research to prove his theories.
Back home from Japan, Campbell told the department he wanted results fast.
After all, Sony and the rest did not seem interested.
‘Just eight months after visiting Sony I received a letter from them,’ he said.
‘They enclosed full details of their Mavica asking me to compare it with my patents.
I was amazed when they came back with a prototype.
Sony had given no indication they were working on — or intended to work on any electronic stills camera.’
Campbell's financial return on his idea will depend on legal negotiations after his patent is published in Japan and the Mavica goes on sale.
Steam train set for a come-back
Alan Whitehouse
The STEAM locomotive is about to make a comeback on railways in the US as a way to lesson the dependence of the national transport network on oil.
But nostalgia buffs can put the cameras away.
The steam loco of the 1980s will bear little resemblance to anything Casey Jones would have recognised.
It will closely resemble a pair of normal diesel locomotives coupled back to back.
The fireman will be another casualty of the modern steam age: his place in the cab will be taken by a computer which will monitor the coal feed closely, to keep smoke down to an absolute minimum.
The loco will produce no cinders, no dust from coal or ash, and few polluting gases due to a highly efficient firebox.
The locomotive is the brainchild of a company calling itself American Coal Enterprises (ACE).
It has assembled a team of scientists, engineers and railway experts to build a modern steam engine that will take advantage of all the advances in technology in the 30 years or so since the last US steam engines were built.
The result exists only in model form, at present, but the company insists that all the engineering is conservative, making no technological leaps into the dark.
All the individual systems are already proven.
All it is doing is assembling them into a package.
The project should get off the ground before the end of this year, when the final touches have been put to the package of financial backing that ACE needs to build and test two prototypes of a design it calls the ACE 3000.
American railways, apart from relatively small suburban networks, cannot even consider electrification because of the enormous infrastructure costs.
Their routes are long, and, by European standards, carry an infrequent service.
That leaves oil and coal as the sources of power.
Coal fell into disfavour on the grounds that steam engines are noisy, polluting and only 5 per cent efficient.
That may no longer be true.
American Coal Enterprises say its steam loco will be 15 per cent efficient (modern diesels are about 30 per cent).
It will not be fussy about the coal it burns, and will do so more thoroughly, producing little pollution.
It will have roughly the performance of a diesel.
ACE 3000 consists basically of a power unit and support unit.
Steam lovers will recognise them as the engine and tender.
On the ACE 3000, the driving wheels are much smaller than traditional steam engines, and are grouped in two sets of four, giving a more balanced dynamic drive, which should cause less wear and tear on the track.
A pipeline from the support unit feeds the boiler, and ash drops into a modular ash pack, changed when the loco is serviced.
It is sealed, to prevent dust causing a problem.
Coal also comes in modular packs, eliminating the need for special facilities at engine sheds.
The water is chemically treated to extend boiler life, and is in a closed circuit.
After leaving the cylinders, it powers items of auxiliary equipment before passing through a large radiator in the support unit, returning cool to the water tank.
Because of this, it can operate for 15 hours continuously at full power, stopping only to pick up a new coal pack.
It will have a top speed of around 125 km/h, and it will fit onto any US railway with no clearance problems, or trouble negotiating tight curves.
For the operators, American Coal Enterprises paints an equally rosy picture.
Diesel fuel now costs over four times the price of coal, for energy produced, and all the indications are that the gap will widen.
ACE 3000 will cost no more to buy than a modern diesel engine, and if every US railway converted tomorrow, they would save a third to half of their fuel costs each year, say American Coal Enterprises, giving a good return on investment.
The actual saving, they claim, would be around 1.5 billion dollars per year.
Out on the tracks, the ACE 3000 would be very much like present power units.
The driver would have a microprocessor controlled throttle to give greatest  efficiency .
The loco would be able to be driven from a cab at either end, and two or more units could be coupled in multiple and controlled from one cab, as with present-day diesels.
Super carp keeps ditches clear
IN THE fight against aquatic weeds, which block drainage ditches and hamper fishing and water sports, British water authorities have up to now had to resort either to herbicides or mechanical cutting.
But an effective biological weapon — in the shape of a rather peculiar Chinese fish — may be near at hand.
The  grasss carp,Ctenopharyngodon idella , which hails from the rivers of northern China, is a voracious planteater.
When eating flat-out, the beast easily consumes more than its own weight in plants every day.
It can live for 10 years, and grow to a substantial 20 kilograms.
But the carp's greatest appeal, says Dale Robson of Oxford's Weed Research Organisation, is its inability to reproduce in British waters.
By some quirk of physiology, the fish feels like breeding only in the warm waters of the very long, swift flowing rivers of its native land during the monsoon season.
So even if it managed to escape from the scientists' clutches in Britain, it could not turn into another exotic pest, like the mink in Britain or the rabbit in Australia.
As an extra precaution, waterways could be stocked with only one sex of fish.
But some anglers fear that the carp may damage the aquatic ecosystem which supports their fishing.
Dr David Hockin, working with Dr John Eaton and Dr Ken O'Hara at the University of Liverpool, is trying to find out what effects the carp have on the communities of animals and plants living in our waterways.
So far, ‘we have been unable to detect any major obnoxious effects on other fish or invertebrates,’ Hockin says.
‘The fish can even improve fishing in waters previously so blocked with weeds that they were unfishable.’
Another problem could be disease.
In West Germany, specialists on fish diseases have blamed an outbreak of ringworm in fish farms on imported grass carp.
In one south German fish farm, only a few of 300 000 young carp survived an outbreak of the ringworm Bothriocephalus acheilognathi , which spread from the Far East.
There are also problems with the carp itself.
‘They don't graze like a herd of sheep and keep all the vegetation down to one level,’ says Robson.
Instead, they selectively consume their favourite vegetation, such as soft-leaved plants like Canadian pond weed.
And in rather chilly summers they eat less, so plants can get out of hand.
But with careful management the carp can still be a boon to water authorities.
In Holland, where carp have been introduced into land drainage systems, the fish have reduced the cost of maintenance by up to 40 per cent.
Australia in space
THE FRENCH aerospace company, Matra, and Hawker de Havilland of Australia have joined forces to set up Australia's first space company.
The move is the first step in establishing an Australian space industry.
It should give the country the capability to design and build the next generation of Australian communications satellites as well as to win lucrative overseas contracts.
The company, Auspace Pty, has already successfully tendered for the second phase of the starlab project, starlab is a joint Australian, Canadian and American plan to launch an advanced space telescope aboard the shuttle in 1990.
It won the contract against stiff competition from two Australian and four international companies, including Hughes Aircraft of the US, and British Aerospace.
Australia will provide the instrument package for starlab.
It includes a large photon-counting array which researchers at the Mount stromlo Observatory, near Canberra, developed.
Canada will supply the telescope, NASA the space platform and two free shuttle flights.
The first is in November 1990.
Under the contract, Auspace will provide specialist space scientists to work on the instrument package with the team at Mount Stromlo.
It will also be the lead industrial contractor on the project.
High technology helps Third World farmers
MICROPROCESSORS are beginning to go to work to raise the quality of the Third World's agricultural products.
Sorting machines incorporating  microprocessors have been ordered in the past few weeks to upgrade rice, coffee and groundnuts.
The machines are made by a London company, which launched the world's first microprocessor-based sorter in 1979.
This machine was designed for sorting ore and waste heaps left from more extravagant eras.
They needed a device able to scan lumps of rocks and pick out those worth processing at a rate that would make it possible to work on the huge scale required to make extraction from low-grade ores economical.
The new ore-sorter has over 1000 photoelectric cells to scan rocks at a rate of 500 per second.
The microprocessor makes it possible to pick out ore-bearing rocks fast enough for 64 separate air jets to deflect them.
The company, Gunson sortext, is now applying the same technology to other jobs.
A rice miller in Thailand, Riceland International, has bought a new rice-sorting machine with 11 microprocessors.
The machines handle more than four tonnes of rice an hour, scanning 25 000 grains every second, and rejecting substandard colour grains in thousandths of a second.
The machines can be adjusted in one minute to sort brown or parboiled rather than white rice, if necessary.
In the past month, Gunson has also received an order to supply coffee grain sorters to Burundi in Central Africa.
These machines work in pairs.
The first machine's photo electric cells pick out yellow, black or dark brown beans from the proper lightish brown colour.
The second machine takes a second look at the beans which pass the first scan using ultraviolet light.
Any beans that are infected with the mould Aspergillus flavus , producer of the liver-cancer-causing toxin aflatoxin, fluoresce in the beam.
A third recent order is for a groundnut sorter, which could play an important role in a project to turn the Egyptian desert into fertile land.
Eight machines scan the nuts before shelling to remove any of which are diseased or stained.
Four other machines screen out diseased or damaged kernels after shells have been removed.
Recent trials in Senegal have shown that the same machines can reduce contamination with aflatoxin to safe levels.
The latest sortex machine, just on the market, is for potatoes.
It uses ordinary light, not X-rays, to distinguish between potatoes, clods of earth and stones during harvesting at a rate of 25 tonnes per day.
These applications and others like them for the  microprocessors have growing importance in the Third World.
In present day conditions, farmers can sell crops such as coffee only if high and constant quality can be guaranteed.
Microprocessor-based sorting could make this possible at a low cost.
But the political cost of displacing labour remains a problem.
New Scientist says goodbye to Gutenberg
Michael Cross
After years of urging its readers to embrace new technologies,New Scientist is practising what it preaches.
Why has it taken so long?
IF WILLIAM CAXTON, the father of British printing, were to come back to life after 500 years in the grave, he would see a lot of changes.
But one of the places where he would feel most at home is the room in which a vital part of the process that brought New Scientist to you last week was carried out.
There, Caxton would see men assembling pieces of type — of the kind invented by his contemporary Johannes Gutenberg — by hand.
Last week all that changed.
For the second time in its history,New Scientist is attempting to move from a printing process based on solid metal type to one in which a computer stores the words.
Index Printers of Dunstable, 40 kilometres north of London, is responsible for the ‘hardware’ that goes into New Scientist and many other magazines.
Broadly speaking, the printing process falls into two stages, typesetting and printing.
Index's printing machines (web-offset litho), are as fast and modern as any in the world.
It is the typesetting that has lagged behind.
Most of the words that go into New Scientist each week first appear on manual  typewriters at an office in London's West End.
This ‘copy’, after editing and ‘marking up’(with typesetting instructions) then travels to Dunstable, either in a messenger's van or by facsimile machine through telephone lines.
The copy then goes to the composing room.
In the old ‘hot metal’ system, ‘comps’turned the words into metal type by one of three processes, depending on the type size and design (or ‘fount’) required.
The vast majority of the words, like the ‘body text’ you are now reading, went to machines called Linotypes.
These assemble lines of type, and cast them in relief from molten metal.
The process, which a highly skilled operator controls through a keyboard, itself caused a revolution in the printing industry when it appeared in the 1890s, putting many hand composers out of a job.
Index has one other mechanical typesetting department, with Monotype machines, which originated around the same time.
Much of the ‘non-body’ setting in New Scientist , such as tables, picture credits and footnotes, originated in this process.
Monotype works in broadly the same way as Linotype machines.
The keyboard operator punches holes in a paper roll, which when run through casting machines produces individual characters.
The third  process is as old as printing itself.
The comps select individual letters, spaces and punctuation marks from a wooden case, and assemble them in a ‘stick’ to form a relief of the type in mirror-image.
To do this quickly requires an extraordinary skill — but in the rush to get New Scientist to press on Tuesdays, it was often quicker to ‘hand-set’ last-minute headlines and picture credits.
The pieces of metal type, wherever they originate, were then assembled on a metal table called the ‘random’.
A proof, taken by inking the mirror-image metal, and pressing a piece of paper down on it, went to the ‘readers’ for checking.
Different people did all these jobs.
The process of turning pieces of metal type into a magazine page goes on at ‘the stone’.
This is another table, originally made of stone.
There, the pieces of type, the rules and spaces for pictures are assembled in a frame called a ‘forme’ according to instructions given by New Scientist's art department.
A proof of the whole page goes to the readers for checking and correcting, and back to the editors at New Scientist for final alterations.
When everyone is happy with the page, the metal is wheeled over to a large proofing press, which makes a positive impression of very high quality.
This is then photographed to make a negative film.
The illustrations are ‘stripped in’ as separate films, and the whole film goes to a process camera which makes the lithographic printing plate.
The business of turning words into type by this method sounds cumbersome and outdated.
But in the right circumstances, ‘hot metal’ is capable of some remarkable feats.
The world record for speed was probably set in the afternoon of 12 August 1966, by the London Evening News , At 3. 19 pm a gunman on the run shot three policemen dead in West London.
The Evening News received a tipoff three minutes later.
The story was confirmed at 3.27 pm, and a new front page carrying the story went to the printing presses 12 minutes later.
In that time the story was written, typeset, and put into a redesigned page under a new headline.
Ironically, the cost of maintaining the manpower and machinery necessary for this performance killed the Evening News .
For generations, printers in Fleet Street have been among the highest-paid industrial workers in Britain.
When newspaper owners ran into hard times in the 1960s, and tried to save money by introducing new printing technologies, the printers fought long and hard to preserve their jobs and status.
The ancient fraternity of the printing trade, with its complex  hierarchy of ‘chapels’(union branches), helped them.
When a dispute about new technology stopped production of The Sunday Times for 18 months, the management had to deal with 65 separate chapels.
Each one was in effect a sub-contractor.
The most contentious technological advance, as well as the most inevitable, is called phototypesetting.
This  bypasses the metal type stage by printing the type photographically from an optical or electronic store of individual characters.
The idea is not new — a Hungarian engineer designed the first photocomposing machine in 1894 — but its widespread adoption depended on the computer to select characters at high speed from an optical storage disc.
This happened in the 1960s.
Since then virtually every small printer, and most large ones, have changed from metal to film.
Only Fleet street is left.
New Scientist itself was one of the first to try the new technology.
It was a disaster.
A company called Southwark Offset, in London, did the typesetting.
The equipment consisted of keyboards which punched instructions on paper tape for a computer-controlled photosetter.
The photosetter churned out the columns of type as a film, which had to be pasted up manually to form a complete page for the printing plate cameras.
Any mistakes in the original typesetting — and any introduced by the vagaries of the punched tape — had to be rectified by pasting lines of corrected film over the top.
‘It was chaos,’ recalls New Scientist's managing editor, Richard Fifield.
‘The place was full of old men wearing thick glasses scratching around trying to find tiny pieces of paper.’
And when the corrections were pasted on, they frequently dropped off or slipped into the wrong place during the journey to the cameras.
In September 1971, a dispute in the printing industry kept New Scientist off the streets for nearly three months.
When it reappeared in December, Index had taken over the printing — and Gutenberg's technology was to rule for the next 12 years.
Our second attempt to embrace the computer age — as our pages frequently urge our readers to do — is a lot more sophisticated.
Index has  installed an American photocomposition rig called the Bedford System.
It has nine keyboards, with visual display units (VDUs) for entering text and layout instructions.
Instead of punched tape, magnetic discs store the information for two independent computers, which control two photosetters.
The most significant thing about the system is that it allows the operator to lay out pages on the screen.
This means that the same man can enter copy, make up the page, and check it for errors.
When the page is ready for press, the photosetter turns out a high-quality proof at the touch of a button.
From the management's point of view, any technology that breaks down the old lines of demarcation (and reduces the number of employees) is a good investment.
But for the men on the shop floor, the new technology presented a difficult choice — either learn new ways, or face redundancy, Those who made the transition found the new working environment very different.
After the clatter of the Linotypes, the tick-tick-tick of computer keyboards sounds eerie and  aseptic .
The composers have had to learn a completely new keyboard layout,(with the standard QWERTY arrangement).
They have to get used to translating a paper layout into a video picture.
They have to learn to accept the delay before an item appears on the screen.
But some of the old skills are just as vital.
Although the computer has a hyphenation dictionary (it knows how to split words that come at the end of a line), a skilled composer can usually do a better job.
The composer also needs a ‘feel’ for the type — to know how much space to leave around headlines, for example.
But for some printers, it is not the same craft.
‘It's just a typist's job up there,’ one old hand at Index says.
‘If they offered me a job at five hundred quid a week, I wouldn't take it.’
There is also suspicion about the effects of the VDUs, although the screens are well positioned and the operators have had eye tests.
(The old technology was not without its hazards — which ranged from burns from molten metal to eye-strain caused by reading metal type in poor light.)
But the biggest fear about the new technology is that it brings a little closer the day when journalists could set their own type.
Whether such a step would be valuable for a journalist is a subject for much argument.
And printers, who generally regard journalists as overpaid amateurs, are not likely to give up their position without a fight.
Why change to photocomposition?
IF YOU want to know why journalists and printers are obsessed with adopting new technologies, look at p139 of last week's issue (20 January).
At the bottom of the page is a news story, originating in Paris, about the French government's nuclear power programme.
Every word in it has been typed four times (once by our Paris correspondent, once by the French telex operator, once by a typist in our office, and finally by the Linotype operator at Index Printers).
Each stage takes time and introduces the possibility of error.
New technology is the only way out.
Once the text is set in metal, corrections mean retyping a whole line.
Life gets more complicated when the typeset text is too long or too short to fill the space allocated on the page.
If we do not want to reset entire articles (impossible in the rush of printing schedules), the only place we can make cuts or additions is at the end of paragraphs.
Life gets even more complicated when the type is set across different widths, or ‘measures’(look at p 140 of last week's issue).
The top story, about police computers, turned out to be slightly too long when it appeared in print.
The only place where we could make cuts, without resetting the entire piece, was in the last paragraph.
These physical constraints all take their toll on the accuracy, readability and good English of the magazine.
It would make life a lot simpler to do these alterations when the words exist as digits in a computer's memory, rather than as solid chunks of metal in the composing room.
In theory, photosetting should also improve the quality of the finished product.
Metal type wears out, and new ‘founts’(complete sets of a single design) are expensive — so the best printer in the world is not going to replace them more often than absolutely necessary.
Nostalgia apart, there are some reasons to dislike photocomposition.
It needs only one breakdown to stop the whole process, and if the operators are not careful whole pages can ‘disappear’ inside the computer.
Also, like all computers, Index's system has its quirks.
For example it cannot run credit lines up the side of pictures.
This is still a job for scissors and a steady hand.
The lion, the spring and the pendulum
Some animals move far more efficiently than others, and in general the race is to the big.
But springiness and coordination of swing are the in variable components of all motion on land
Lois Wingerson
WHATEVER else we are, we are superb machines.
After more than 10 years of studying vertebrate animals in motion, C. Richard Taylor is perhaps more aware of this than anyone.
He has a way of speaking that seems to turn them all into engines, which, in terms of simple physics, is what they are.
He appreciates their design features: what a kangaroo gains and loses by moving in leaps, why horses change gait, why it is harder to walk quickly than to jog at an easy pace.
The creatures that showed all this to Taylor and Norman Heglund are gone now, some of them posthumously taken apart limb by limb to ascertain their centres of gravity.
Today, the only animals around Harvard's field station in the woods are an ageing ostrich, two pet dogs, three ponies, some goats, and miscellaneous hominids.
But it has been a busy and peripatetic decade for Taylor, Heglund and their co-workers.
They have lured or chased more than 70 species of vertebrates around racetracks in the Kenyan desert, up treadmills at the field station, and over runways of force plates in Milan, all in the interests of learning, as it were, how many kilometres each model gets per litre.
The result is a single equation:oxygen use (ml) /body mass (kg)= 0.533 (body mass)-0.316 (speed)+ (body mass)0.303 which defines the cost of moving a vertebrate on land, in terms of the oxygen used in burning its fuel stores.
It seems to be valid for virtually all animals with backbones, with a few exceptions: kangaroos, lions and birds that waddle (but more of them later).
Like most equations, this one is daunting and dull until you study it carefully.
What determines the cost of movement are speed and body mass.
But note that the radicals (0–316 and 0–303) are negative.
The right side of the formula is smaller for animals with greater body mass.
In other words, it costs less per unit of body weight for large animals to run.
Unlike early muscle physiologists who measured a waste product, heat, from the action of dissected muscles working in artificial conditions on a laboratory bench, Taylor and Heglund spent years studying oxygen uptake directly, working with a live menagerie.
They coaxed everything from pygmy mice to snakes to cheetahs into running on a treadmill while wearing an oxygen mask.
Thus they can gauge how much fuel a muscle needs to move an animal far more precisely than can be done by measuring the heat it gives off.
In general, a millilitre of oxygen releases 5 calories of energy to an animal.
Taylor's general formula of animal energetics began to evolve on the side of Mount Kenya in the early 1970s, when he was engaged in racing North African mammals, particularly elands, oryxes and gazelles, around a small racetrack.
The thought of it still amuses him.
‘They ran,’ he says.
‘They just like the opportunity.
You could chase them around the racetrack.
Or you could put a little gunnysack full of food on a big boom, and you could control their speed by the way you would swing it.’
Taylor had arrived in Africa meaning to study how migrating animals survive in the desert without water.
In the process he decided to verify the predictions of the British physiologist  Archibald Vivian Hill (who won a Nobel prize in 1922, and died only five years ago).
Hill was the first to speak of muscle activity in terms of costs and expenditures, and he originated the term ‘oxygen debt’.
This alludes to the finding that during short bursts of intense activity, an animal, including a human animal, day expend more energy than can immediately be supplied by aerobic respiration; that is, by using oxygen to ‘burn’ sugar.
Instead, the animal may use up much of the ‘reserve’ oxygen (held principally in the muscles, bound to the pigment myoglobin); and, more significantly, it will derive much of the energy it needs by anaerobic respiration, in which sugar is broken down to lactic acid, without the use of oxygen.
Such anaerobic respiration is much quicker than aerobic respiration, but also much less efficient in terms of energy produced per unit of fuel.
After the short-term exercise, the animal must take in extra oxygen, both to re-oxygenate the myoglobin, and to oxidise the lactic acid.
This is when it is ‘out of breath’, and is ‘paying off’ its oxygen debt.
Hill also concluded that heavier animals would require more energy' than small ones, kilo for kilo, to move.
In this, as usual, he was correct.
At first in Africa, and later on the treadmill in the field station in Massachusetts, Taylor measured the oxygen consumption of a wide range of species when running, or hopping, or walking, or galloping, at different speeds.
Among them were a 100-kg man and, a thousand times lighter, a 100-g quail.
Should the man choose to chase the quail rather than shoot it, he would almost certainly still have his dinner.
Gram for gram, the quail uses 10 times more energy at the same speed.
Does that mean that a man is a better machine for running than a quail, or a chipmunk or mouse?
To find the complete answer, Taylor had to know how much power each member of the menagerie generates as well as how much fuel it uses.
When Giovanni Cavagna designed his force plates at the Institute of Human Physiology in Milan, Taylor had his opportunity.
These force platforms are now as familiar to animal physiologists as a  microscope is to a microbiologist.
They measure the forces applied to the ground by the feet of a running animal.
Taylor and Heglund airlifted their entire zoo to Cavagna's lab in Italy.
‘Kangaroos and monkeys and dogs and turkeys and rheas,’ Taylor says.
‘Everything 5 kg and up that we had already measured the energetics of.’
At Milan he took high-speed films while his animals ran along the force plates.
He killed and dismembered some of them, to determine the centre of mass for each limb.
From that he could deduce the centre of mass for the entire animal, the point through which its forces act.
At any point on the film, then, he could match up this picture with the force created simultaneously on the platform.
The results, compiled by computer, gave him a measure of the animal's total energy — kinetic energy due to its motion, and gravitational energy due to its distance off the ground — at any point during a stride.
Back in Massachusetts, Taylor had learned that the amount of energy an animal uses increases in direct relation to speed and body mass.
But there was a second surprise in Milan: that the amount of energy an animal creates is independent of body mass and increases in less than a one-to-one relationship with speed.
Thus efficiency, the ratio of work done for fuel burned, is not constant for all animals.
Not only do large animals use less energy per gram of body mass than small ones, they generate more power at faster than at slower speeds.
The larger and faster, the better.
Considering what is already known about muscle, it makes good sense.
The muscle fibres in the skeletal (voluntary) muscles of vertebrates (the muscles, that is, that are concerned with locomotion) are of several kinds.
Some, light in colour, rely heavily upon anaerobic respiration.
Others rely more heavily upon aerobic respiration; they are dark in colour, because they are rich in oxygen-carrying myoglobin and mitochondria.
A third kind, intermediate in colour, have myoglobin and can be aerobic, but may also work anaerobically.
Muscles with a high proportion of light-coloured fibres can do a lot of work in a very short period.
This is why the breast (wing muscles) of chickens are white, because chickens use their wings only for short rapid flights, whereas their legs, used mainly for standing on, are dark.
A quail or a mouse also has a relatively large amount of light coloured, ‘fast’, muscle (white meat) and hence are forced to use energy in short bursts only to avoid build up of the toxic byproduct of anaerobic respiration, lactic acid.
Bigger animals, such as humans, horses, and cows, have relatively more dark meat, made of many slow muscle fibres that can sustain activity much more cheaply and for longer periods than fast fibres.
After all, when a quail is chased by a man, it must take more steps to run at the same speed.
Its muscles must contract more quickly; and because its muscles must be of the fast, white variety to achieve this, they cost more, energetically, to generate the force the quail needs in order to run.
Scrambling along the ground at 4 metres per second, a quail functions at a miserable 7 per cent fuel efficiency, Heglund found, while a man running at that speed achieves a 73 per cent ratio of fuel used in running to energy generated from food and oxygen.
Much of the beauty of a good machine is in its mechanical design, as well as in its fuel efficiency.
Every species has a slightly different solution to the problem of getting about, and the amount of work required to go at a particular speed depends in part on how well an animal has been designed to move at that speed.
Non-terrestrial animals, such as fish and birds, must work almost constantly to move, using their muscles to accelerate water or air past their bodies and to overcome drag.
Land animals employ two simple work-saving schemes, based on two classic examples of the principle of conservation of energy: the pendulum and the spring.
In the Newtonian sense of work, as force applied over a distance, land animals do little in getting from here to there.
With a brief push to provide kinetic energy at the start of a walking stride, we humans can swing the rest of the stride on gravity alone.
In running, we use most of the energy provided by our skeletal muscles to create anti-gravity supports.
providing tension to the girders (bones) and cables (tendons) that keep us from crumpling to the ground at any instant.
Again, gravity does most of the work in moving us forward.
For more than a century physiologists have remarked upon the similarity between a walking human and an inverted pendulum.
In recent years, Cavagna in Milan has worked out the detailed mechanics of the process, using force plates and high-speed film to capture the instantaneous braking and acceleration of the lower limbs during a stride.
Energetically, he found, walking is analogous to a pendulum which falls, gaining speed from gravity, and rided up past its low point on the gained kinetic energy.
A walker uses kinetic energy in a similar way to lift himself over his own centre of mass.
On force platforms, a walker's energy level appears virtually constant: he swings between kinetic and gravitational energy, using one leg to rise over his centre of mass while the other speeds downward to exchange gravitational energy for speed and more kinetic energy.
Among animals, Taylor has found, humans are particularly fine pendulums.
But, as a baby learns after months of tipping over, it is a matter of fine coordination.
The swinging leg has to touch down at exactly the right speed and lock straight in time to convert the kinetic energy into lift.
Although he seems to have the hang of it pretty well, a Harvard physicist named Thomas McMahon spent some time relearning this process on a computer.
Today he can draw it with callipers.
His ‘ballistic walking’ theory predicts the path of the swinging leg as perfectly as one can determine the trajectory of a missile, knowing its physical properties.
Considering the walking legs as two limbs of a compound pendulum, one straight, the other jointed, McMahon computed what would happen at various velocities and limb lengths, applying the classical mechanical equations of the early-19th-century mathematician Joseph Louis Lagrange.
Given that the nerves and muscles of the leg in walking show essentially no electrical activity while it is off the ground, McMahon concluded that the swinging leg must operate purely under the influence of gravity, as does a falling object.
The correct choice of velocity for a particular leg length, he found, permits the swinging leg to lock straight just before impact, so it can use most of the energy gained from gravity in raising the body off the ground again.
To confirm his mathematical model, McMahon made high-speed films of people walking, and used a potentiometer to measure the angles of their joints during the stride.
Knowing their speed from the films and their weight and limb lengths, McMahon found that the configuration of motion in human walking perfectly matched what he had predicted from 19th-century physics.
The little energy the muscles supply at the beginning of a stride is what we contribute to our locomotion; gravity does most of the work for a walker.
Run fast and save energy
As anyone knows who has missed a bus, there comes a speed at which the compound pendulum begins to fail as a good system for locomotion.
(Rodolfo Margaria, the Italian physiologist, clocked this point in 1938 at 2.4 metres per second.)
R. McNeil Alexander of Cambridge University learnt what happens at that speed: the accelerations and slowdowns fluctuate too wildly, and the changes in gravitational energy diminish.
The legs break into a run; the pendulum becomes a spring.
A horse naturally shifts from a walk to a trot, and then to a gallop, Taylor's team found, at the speed where each pace begins to cost it more than the minimum amount of oxygen.
The same is true for humans, he says.
An Olympic walker uses more energy than a jogger moving at the same speed, because he is walking at a speed where his muscles would use less energy if they were operating a spring than if moving a pendulum.
By the same token, a jogger who runs too slowly would use less energy if he sped up to his instinctive pace.
Energetically, the spring is almost the opposite of the pendulum.
In animals walking on force plates, there is a steady exchange of kinetic and gravitational energies.
When they are running, the total energy fluctuates wildly.
At the bottom of a stride, both kinetic and gravitational energy are at their lowest, and both are highest when the runner is completely in the air.
Energy seems to vanish and re-emerge in dramatic swings.
Naturally, animals do not defy conservation of energy during running.
They rely on a third property: elastic energy.
At the bottom of his stride, a runner's built-in spring is most highly charged, but this third type of energy cannot be detected by the force plate.
But that did not deter C. Richard Taylor from trying to study it.
He and Cavagna sent kangaroos, those champions among springers, down a runway of force plates.
At 30 km/h their metabolic machinery (using oxygen) supplied only a third of the power necessary to make a kangaroo rebound after landing.
The missing two-thirds of its energy must have come from elastic tension stored in its huge tendons, tail and lower back.
Running men and dogs, too, supply about two-thirds of their forward power by springing, but they are not nearly as effective springs as kangaroos.
Kangaroos can sustain speeds of up to 25 km/h for long periods; but what they gain in the springs, they lose as pendulums.
With almost no ability to act as a pendulum, a kangaroo must expend a great deal of energy in its lopsided and ungainly walk, using first its front legs and tail, then drawing its rear legs forward.
Lions, on the other hand, extraordinary though it may seem, seem to have no spring at all.
All the energy that lions used when running on Taylor's treadmill they generated with their metabolism.
Clearly made to be all jackrabbit starts and pounce, lions cannot sustain high speeds for long.
Birds that waddle, like the penguin, represent another compromise.
Streamlined for swimming, much of the energy used in walking is squandered on side-to-side motion.
Animals with good elastic properties, however, behave almost as predictably as a coiled spring whose characteristics are a function of its dimensions.
Taylor found that a kangaroo hops at the same frequency at which gazelles of equal weight run or, for that matter, at which humans of equal weight bound on a pogo stick.
Smaller people pogo faster, at precisely the increments a physicist would predict from the known properties of springs — although keeping them bouncing long enough to prove this was another challenge for Taylor.
‘It's very hard to pogo on treadmills,’ he says.
‘We had safety harnesses; you had to save the kids.
They'd get four or five hops in and they'd start to splatter.’
(Except for one Harvard undergraduate who held the world record, and seemed able to go on springing forever.)
Where is the elusive vertebrate spring?
In the tendons, clearly, but the muscles must store elastic potential, too.
Some of it may also reside in the bending of bones.
Whatever may seem the case to a man chasing a bus, running involves little work for most vertebrates.
The force of a runner's foot deflects the pavement very little in running, and therefore does little work (in the Newtonian sense) on his environment.
The environment, in fact, does work on him, by stretching the antigravity muscles in his legs on impact.
This forces them to expand beyond their resting length, both braking the fall and supplying energy, which the muscles can retrieve by contracting just after footfall.
The muscles bounce; thus the runner gains back from the environment most of the energy he needs for his rebound.
By putting a little extra energy into contracting his extensor muscles on his way to impact, thereby stiffening his tendons, a runner can maximise his innate springiness.
One further way that humans are able to optimise their bounce is by using their heads, or, specifically, by using Thomas McMahon's head.
McMahon works for Harvard's Division of Engineering and Applied Physics, and he was consulted about the design of Harvard's newest running track.
His recommendations surprised everyone.
The Harvard Planning Office had brought him in on a dispute between the track coach and the builders of a new gymnasium, concerning the dimensions of the new indoor track.
As soon as McMahon helped them solve that (his computations backed the coach, who said the track should be banked as high as his belt buckle at the outside of the turn), they called him back with another question: how springy should they make the track?
Apparently this question had never been addressed before in a systematic way.
Intuition says that a runner is quickest on a very hard track, so most tracks are made of concrete with a layer of polyurethane added for cushioning.
But McMahon and his co-worker Peter Greene found that the hardest track was not the one to impart the most bounce, and therefore was not the fastest.
The property of springiness is easy enough to detect: it is in direct relation to the speed with which a runner's foot leaves the surface after contact with the ground.
McMahon and Greene first considered the effect of running surfaces using a mathematical model which depicted a runner's legs as a rack and-pinion element linked to a spring and shock absorber.
They tested the model by taking high-speed films of runners on a range of surfaces, from thick pole-vaulter's pillows to concrete, in order to measure the speed of their bounce.
McMahon and Greene found that on the pole-vaulter's pillows, the runner was in contact with the surface far longer than on concrete, and that he deflected the pillows noticeably.
This forced him to increase his step length, to raise his leading leg in order to get out of the hole he had created.
The longer stride actually slowed his progress by 30 per cent, exactly as the model had predicted.
Concrete, the standard high-speed track surface, is about a hundred times stiffer than a human runner.
But McMahon and Greene's model predicted that there was an ideal between these two extremes, a surface at which the runner would be deflected more quickly than on either a harder or a softer track.
They calculated the proper stiffness at just more than twice the stiffness of the runner.
Ground-contact time would be shorter than on concrete, they reckoned, yet step length would be increased, and with it, speed.
They proposed that Harward create such a super-track tuned to human elasticity, and predicted it would increase speeds by 2 or 3 per cent.
Harvard did, and McMahon and Greene were right.
In the first season after the new track was installed, Harvard's runners increased their speeds by an average of 3 per cent.
Furthermore, the injury rate on the softer track dropped to less than half what it had been on the previous track.
There are now four such ‘tuned tracks’ in America: at Harvard, Yale, Madison Square Garden in New York, and at the Meadowlands Sports Complex in New Jersey.
But there is no outdoor super-track, and thus no one has tried to break the world record, for the 1500 metres on one.
McMahon and Greene's challenge stands unanswered.
They said in 1976 that a tuned track could shave as much as 7 seconds off the record for the mile.
Since then, Sebastian Coe and Steve Ovett have whittled it down by about 2 seconds.
What they could do on one of Thomas McMahon's or Peter Greene's creations, and what the meaning of such a new record would be, are questions that will tantalise us until they, or other runners, provide the answer.
Helix through the looking glass
Nature in her many convoluted patterns shows clear preferences
John Galloway
THE HELIX is a curve nature finds seductive, as a cartoon that Private Eye published in 1978 may be illustrating.
Look for patterns of growth and you find helices.
Their elegant and economical forms are the solutions to innumerable puzzles of how to plan and build animals and plants (Figure 1).
And, if you want to press the architectural simile a little, the many variations on a helical theme (with their emphasis on a set of rules of proportion) represent a style that is  unmistakably classical, with much of the same appeal.
Or so it might seem.
But in an important respect,(the one this article is about), helices are not at all classical.
In any building claiming to be classical (and in many others, of course) the key principle is one of symmetry; using the word in its narrow modern sense not of harmony but of parts arranged equally about a line or plane.
In other words, the two halves of the building balance or, more formally, they are mirror images of one another(figure 2).
In this latter sense helices are asymmetrical.
The mirror image of a helix is a precise replica of it except that no amount of turning will allow you to superimpose one on the other.
The pair of mutual mirror images have the same relationship that our hands possess.
Now, hands are, well, handed for want of a better word.
Similarly, by analogy and by convention, helices are also described as right or left-handed.
A screw that goes in as you turn it clockwise is (arbitrarily) called right-handed.
This sort of looking-glass asymmetry is not a property simply of helices.
Far from it.
Molecular stereoisomerism is a phenomenon exactly similar.
Thus L (laevo = left) and D (dextro = right) forms of glucose are also asymmetric mirror images and are also described as being handed.
And molecules like this, amino acids are other examples, are said by structural chemists to have a ‘chirality’, from the Greek word for hand.
Pairs of them are called ‘enantiomorphs’, again from the Greek meaning, literally, contrary forms.
But because of their regularity and because they can be expressed mathematically with great beauty and economy, helices provide the most striking instances of looking-glass asymmetry.
Naturally occurring helices can be understood by precise geometrical analogies.
Mathematical analogies are always paradoxical of course.
The real helix exists in the mathematical imagination.
In the real world that science purports to describe there are only fictitious helices.
DNA is not a double helix, merely a lot of atoms the centres of which lie on imagined helices.
The molecule is a fact, the helix an abstraction, which is why it is so useful.
It can be manipulated in the imagination and then used as a new model with which to compare other structures.
One such manipulation is to create the helix of opposite hand, to put it through the mirror.
Whether nature does or does not do the same thing is always tantalising, and sometimes revealing about the principles and mechanisms underlying the creation of structures.
Before looking at some specific examples it is as well to have some idea of just what a mathematician means by a ‘helix’ because scientists have taken liberties with the word in looking for terms to describe the world.
Let us begin with the simplest example and argue from that.
The circular helix is a regular curve on the surface of a circular cylinder defined by its radius and its pitch (the distance along the axis of the cylinder after which the curve repeats).
A geometer, however, would define it in terms of its curvature , the rate at which it moves away from its tangent, and its torsion , the analogous rate at which it departs from its osculation plane.
(Osculation = kissing, I told you it was seductive!)
And I mean ‘rate’ in its mathematical rather than simply its physical sense; it has nothing to do with time.
A helix is any line in which the curvature and torsion measured anywhere and everywhere are in a constant proportion.
In the circular helix not only their ratio but also their actual values remain unchanged along the entire curve (Figure 3).
Besides the circular helix, much the best known naturally occurring helix and also the most interesting and beautiful mathematically is the concho-spiral, adopted in all coiled gastropod shells and some others such as the group of protozoans known as Foramenifera.
At any point in this curve, both torsion and curvature, as well as being proportional to one another, are inversely proportional to the length of the arc from the apex to that point.
Curvature and torsion are not easy to measure directly but their being proportional has a straightforward  measurable consequence.
The curve cuts the ‘generators’ of the surface (cylinder for the circular helix or cone for the concho-spiral) at a constant angle .
In the 1930s, the British molecular biologist W. T. Astbury showed using X-ray diffraction (and long before molecular helices were known about) that this property revealed the existence of an unusual helix of cellulose fibres in the cell wall of the single-celled green alga Valonia ventricosa (Figure 1 d)with a shape reminiscent of a partially inflated balloon.
First the spiral opens out as it does in a shell, but then closes up again as it approaches the other pole of the cell.
This complex and beautiful curve is a true helix.
Structures very similar to concho-spirals are horns.
Occurring as matched pairs, they are mirror images of one another, left-handed on one side of the head and right-handed on the other.
Generally speaking, wild antelopes, sheep and goats all tend to have the right-hand helix (as defined above for the screw) on the left side of the head and vice versa (heteronymous), whereas the opposite is often true for domesticated sheep and goats (homonymous).
It also seems to be roughly true that in domestic breeds of sheep the horns are more tightly coiled — they display greater torsion — whereas in the wild the horns of many antelopes are long, straight, extended twists.
Sir Theodore Cook, an historian of architecture and a natural historian who wrote a classic work on spirals and helices,The Curves of Life , 70 years ago, thought these two differences between wild and domesticated animals not unrelated.
Curly horns are an advantage to the breeders of domesticated species, being less dangerous.
But horns that are both swept back (as they are in wild species) and curly and heteronymous would tend to grow into the animal's shoulders.
He felt that by selecting for curliness, breeders had incidentally preserved any genetic tendency towards being homonymous.
Horns are the longest helices (up to a metre long) in the ‘modern animal kingdom.
Coming down the scale a little, another pair of helical mirror images is provided by the cochleae, the frequency  analysers in our ears.
The word cochlea reflects their resemblance to snail shells.
This sort of pairing is normal for most animals; it is simply a regular geometric manifestation of the general development of bilateral symmetry.
But bodies are never quite symmetrical.
Not all organs occur in pairs and not all that do are mirror images.
To take an example, the eye contains a multilayered structure, the corneal stroma.
Each layer or sheet, of which there may be 50 or 100 altogether, contains fibres of the protein collagen all pointing in the same direction within the plane of the layer, so that each sheet has a readily (if you have an electron microscope) discernible direction or polarity associated with it.
In some birds each sheet is twisted through a small constant angle with respect to its predecessor, giving rise to what is sometimes called a twisted ply or structure (Figure 4); such a structure is handed.
However, in contrast to the cochleae, the hand in the two eyes is the same.
This sort of twisted ply, of which there are many examples, finds a close analogy in some of the well known (cholesterolic) liquid crystals and has led some scientists, notably Dr Charles Neville of the University of Bristol, to believe that they are formed by a process of direct self-assembly.
There is no organising principle beyond the energy of interaction of the molecules that form it.
It is certainly true that helical structures known to assemble in this way almost invariably possess only one hand.
The distinction between the two sorts of organisation, one needed to produce a pair of cochleas, the other a pair of corneal stromas, is a deep one.
Self-assembly is a means of organisation identical with that which gives rise to the simple spatial order of crystals, which is a concept readily appreciated by physicists and chemists.
But the striking and distinguishing feature of organisms is organisation without such simple spatial regularity.
What is needed is a couple of examples to bring out the difference sharply.
Consider the shell of the freshwater snail.
Limnaea perega , and compare it with an a-helical protein (such as tropomyosin found in muscles).
Both are right-handed helices.
Both could conceivably appear as left-handed helices.
The snail does (rarely) but the protein does not.
Whether the snail is destined to be right- or left-handed (Figure 5) can be predicted very early in its embryology, from the third division after the union of its parental gametes.
Intriguingly, the snail's hand of coiling is not determined by its own genetic complement (genome) but by that of its mother.
The shell has a mathematically simple form, and, roughly speaking, its shape can be understood as the result of a that organ, the mantle, which itself grows at a variable rate but secretes minerals at a rate proportional to the length of its periphery.
The two rates are proportional, which is not particularly surprising because both in turn depend on one quantity, the metabolic rate.
The concho-spiral permits the animals growth without its shape changing, which is important to a coiled animal whose way of life does not change after metamorphosis; it simply gets bigger.
So there is an a priori justification for the shell's shape as well as a natural way for the shape to be created — its structure and its function are consistent.
Moreover, the shell's hand is a qualitative, readily discernible property controlled by genetic processes such as those first evinced from the work of Gregor Mendel.
It all ties together to give a simple scheme of explanation.
The simplicity, though, is deceiving because it tells you little about the developmental performance leading to the strongly and reliably determined patterns of differentiation and spatial organisation of the animal.
A two-valued property
And even the question of handedness is tricky: it seems to be not entirely under genetic control.
Hand appears as a simple two-valued property, left or right, but it is actually the relectsproduct of a pair of factors.
The concho-spiral is a logarithmic spiral wound on a cone.
It can be wound clockwise or anti-clockwise to give a left or right hand.
But which?
Invert the cone and the same sense of logarithmic spiral produces a concho-spiral of opposite hand (Figure 6).
Left-handed shells are sometimes found produced in this way and are called ultradextral or hyperstrophic (and no doubt left-handed mutations of L. peregra are also found looking like normal righthanded snails; hyperstrophic also but now ultrasinistral).
But the snail living inside the left-handed shell is actually right-handed!
This is how you realise they have different origins.
Seek simplicity and mistrust it — this is good advice.
Protein molecules do not behave so equivocally.
The a-helix is an even simpler way of growing.
Each amino-acid residue in the polypeptide chain is attached to its predecessor in the same way.
Consequently, the structure looks exactly the same from each and every amino-acid residue.
Crystallographers sometimes express this property as the equivalence of the residues.
Each has the same environment as do the atoms or molecules in a crystal (that is what I meant by saying that this sort of structure represents simple ordering), and each is attached to its neighbours in exactly the same way.
One of the first people, perhaps the first, to spot that biological polymers would tend to be helical was a Californian physicist, H. R. Crane.
It was, though, his rather better known colleague at Caltech, Linus Pauling, who proposed that the principle to adopt was that of structures that maximise the number of so-called hydrogen bonds between different amino acids in the polypeptide chain (and who thereby forged the key which very rapidly unlocked molecular biology).
The a-helix is a structure which neatly does just that.
A feature of Pauling's theory now largely forgotten is that the chain of amino acids in a protein can, at least in principle, be twisted either to the left or the right, giving both left and right-handed helical structures.
Pauling thought both structures would actually exist, as did many other crystallographers engaged in unravelling the three-dimensional structures of proteins.
But they do not.
The individual amino acids making up the primary chain themselves have a hand, as I mentioned earlier.
Thus the two a-helices would not be strict mirror images of one another.
Only left-handed amino acids occur in living things.
And nature favours the right-hand helix, not surprisingly in view of the difference in the pattern of hydrogen bonds.
This observation prompts the speculation that chains of right-handed amino acids will form left-handed a-helices and this does seem to be so.
What about amino acids that do not have a hand, being their own mirror images?
Presumably they would form helices of either hand.
Glycine is such an amino acid but Sod's law prevails.
Polyglycine does not form a-helices at all.
But is it Sod's Law?
The twisting of the chain into a helix is a large number of small twists by the individual amino acids.
With no hand of its own, the amino acid simply does not know which way to turn.
So, not only does the particular hand of the set of individual amino acids dictate that of the helix they create but, more profoundly, a single hand is necessary for helices to form at all.
You can conceive of globular protein molecules folding from chains of left- and right-handed amino acids but not helical protein molecules.
So no collagen and therefore no skeleton; no muscle and consequently no movement.
We do not know why living things possess only one hand of amino acids.
If you were looking for a physico-chemical reason for this fact rather than the, more usual, historical one you could do worse than suggest a capacity to make helical molecules.
Here let me make an aside.
Curiously, a unique instance is known of a right-handed amino acid as part of a living thing.
The D form of alanine is found in dermorphin — you have guessed it, the regulatory peptide of (amphibian) skin.
G. K. Chesterton called tiny startling exceptions to rules, like these, a sort of secret treason in the Universe.
A nice discussion of the phenomenon can be found in Martin Gardner's marvellous book,The Ambidextrous Universe .
Of course, chains of amino acids are strung together only with the help of enzymes, and these are stereo-selective.
They are (more or less) specific to an amino acid's hand as well as to its chemical formula.
In a hypothetical life-form with both hands of amino acid it is possible that rather than no helices being created, there would be helical molecules which were true mirror images of one another.
One consequence of this might be the existence of a class of mathematically interesting structures of mixed left- and right-handed helices.
For example, a pair of tropomysin molecules wind round one another (supercoil); but a pair of left and right-hand helices would form an interlocking straight double-molecule, or ‘dimer’.
Actually, the way helical molecules pack together is not of great interest because their structures are too rigidly determined and unyielding.
Helices whose variables are influenced more by the presence of other helices provide far greater freedom and variety of structure.
This behaviour can be observed, and we hope  analysed , especially in bacteriology.
Bacillus subtilis is a common, rod-shaped (as its name implies) bacterium.
A clone of these organisms looks a bit like a string of sausages.
Professor Neil Mendelson of the University of Arizona has some mutants of B. subtilis which do not divide.
If the ends of such non-dividing clone are held while it grows, then unexpectedly a double helix appears.
It seems that the bacterial cell wall grows helically, twisting as it does.
Prevent the ends twisting and the resulting torsion is relieved only by the strand twisting about itself.
What is more, these helices in their turn aggregate, rather than disperse as bacterial cells in fluid culture usually do, and give a living, growing structure.
Mendelson calls them ‘macrobes,’ artificial multi-celled organisms which change their morphology as they go through a sort of life cycle.
The  fascinating thing about the macrobes, if you are interested in structure, is that they are not simply ‘crystals’ of helices.
They are themselves helical, with a pitch that decreases uniformally towards the outside, and they are torsion structures held together in part at least by forces generated by the attempts of individual bacteria to untwist.
Perhaps the most striking thing Mendelson has noticed is a ‘conditional’ mutant.
Usually mutants produce a uniquely handed macrobe.
But some macrobes are left- or right-handed depending on the properties of the fluid in which they grow.
A change of medium in which the macrobe is cultured can actually switch the hand of the macrobe.
Thinking about the macrobes has led Mendelson to speculate in turn that the life cycle of the original bacterium may be controlled by the surface of the cells and that the helical arrangement of the polymers may tell the genes when to switch on and off — a sort of helical egg timer.
The idea of a genetic switch that is geometric rather than chemical has a certain appeal.
This may be more fashionable than sound (but I hope not).
Let me give another example.
As is pretty well known the double helix of DNA is right-handed.
But a particular sequence of bases in the molecule, the so called Z-helix, is in certain circumstances, left-handed.
It seems just possible that this reversal of hand causes sufficient unwinding of the super helix into which the DNA is wound to prevent the gene being transcribed.
I began this article by saying the helix is a popular structure.
But why?
Quite an appealing argument is a teleological one — quite invalid, of course, but valuable for all that.
Before any submicroscopic helical structures (such as molecules, microtubules, flagella and viruses) were known, H. R. Crane, whom I mentioned earlier, said ‘…any structure that is straight or rod-like when seen at low magnification is probably a structure repeating along a screw axis (ie, it is a helix).
To imagine a straight structure which did not owe its straightness to this cause would require an assumption of a highly fortuitous combination of angles between sections.’
There is quite a lot in this point of view, but not everything.
Plenty of helices are not so stick-like, and of course the argument begs the question of how, rather than why.
A common function or way of life often demands the same or similar structure.
But similar structures also suggest common origins.
An important point can be made here, one whose significance sometimes escapes scientists.
Growth is a problem in spatial organisation, controlled by genes which in some way carry a ‘plan’ of the organism.
The plan has two separate features.
It is a set of simple rules — a program or algorithm — that tells the developing organism what to do next.
And it is also the mechanisms by which the cells or parts of them are able to perform the instructions.
Biochemists, particularly, tend to confuse these two and believe that all problems can be solved by isolating and giving names to molecules — which are the mechanisms.
Often it is the internal logic of a process that is the real clue to understanding it.
Helices are often seen because they are a particularly simple pattern of growth.
The most rudimentary algorithm repeats a single instruction.
This sort of repetition tends to produce helices,whatever the actual mechanism involved.
But, as I hope I have illustrated, the mechanism does decide whether the helix is of one particular hand or both.
Glass: a look inside
Scientists are using their most powerful source of X-rays to put together a new picture of the structure of glass, one of the most  versatile and familiar of materials
Neville Greaves
GLASS is one of the most familiar materials around us, equally important in industry as in everyday life.
Its versatility depends to a large extent upon its structure, a knowledge of which is vital if we are to make the best use of glass.
In this respect, scientists are aided by recent advances in X-ray techniques which provide a new view of the structure of glass and which should help in understanding some of the material's more curious properties.
To most of us glass means windows, bottles and car windscreens; it is transparent, durable and hard.
But glass has broader applications.
It can be coloured in many shades by adding metal impurities, as the medieval glass-makers knew, who used their scientific skills to bring subtleties of light into the great European cathedrals.
Since Renaissance times, clear glass has been fashioned into prisms, mirrors and lenses that diffract and focus visible light.
These were used, then as now, to improve vision, but, more importantly, they were engineered into telescopes and microscopes.
In this way Galileo Galilei studied the motions of the planets and Louis Pasteur discovered bacteria.
On the domestic scene glass has for centuries been worked into fine tableware, chandeliers and ornaments.
Glass can also be drawn into fibres, as the Romans knew, and today production of glass-fibre is a large industry.
Fibres are woven into fabric, matted for insulation and in the past 10 years have been used to reinforce cement.
Exceptionally pure glass-fibre can be processed into ‘light’ pipes, now used in telecommunications.
Moreover, glass is electrically insulating.
At one extreme it is used to support power cables; at the other to encapsulate electronic components.
Indeed as pure silica it is the insulating element in the technology of silicon microchips.
Yet glass is still used mainly for windows; with the post-war development of the float process, plate glass can be made to perfection cheaply.
So we have moved from the intricate and sombre medieval stained-glass window to the brilliant glass-clad office-block facade reflecting the sky line, yet the starting materials remain practically the same.
Why is glass so versatile?
The reasons lie in its complex physical chemistry, and central to this is its atomic structure.
The most common form of glass is based on oxides of sodium, calcium and silicon: soda (Na20) and lime (CaO), which are called ‘glass modifiers’, and sand or silica (SiO2), which is called a ‘glass former’.
These materials fuse together around 1300–1500°C, releasing volatile gases and forming a fluid oxide Melt.
But as the liquid cools it rapidly becomes viscous and solidifies before it can crystallise.
A homogeneous ‘supercooled liquid’ is created whose structure is the instantaneous atomic arrangement of the liquid the moment it ‘freezes’.
The proportions of soda, lime and silica govern both the speed with which the molten oxide stiffens as the temperature falls and the temperature at which the glass finally solidifies.
Glasses containing large amounts of silica melt at very high temperatures; silica fuses at 1713 o C. The addition of soda has the opposite effect.
At 13000°C the viscosity of silica drops by a factor of ten thousand million (10 10 ) for the addition of 25 per cent of soda.
Soda silica glasses are not very manageable, however.
They have too low a melting point and are too unstable, but they can be ‘stiffened’ by adding a moderate amount of lime to produce a modest increase in melting temperature.
The optimum composition for glass to be manufactured in bulk has the approximate formula (Na2O), CaO(SiO2)s.
Other ingredients can be added or substituted for different applications.
Glass-makers can use boron oxide (B2O3) in conjunction with silica to make glasses that do not expand too much on heating, as in Pyrex oven ware.
Lead oxide (PbO) increases the optical refractive index, bending light more as it enters the glass; ‘flint’ glass for optical elements and ‘crystal’glass for tableware take advantage of this.
The oxide zirconium (ZrO2) improves the resistance of glass to attack by strong alkalis.
To understand all the variety of glass chemistry it is important to be able to picture the arrangement of atoms.
Physicists know only too well from studying crystalline materials that an understanding of solid-state physics is borne out of a sure knowledge of atomic structure and this has come traditionally from X-ray crystallography.
In the past decade, however, we have also learnt that to understand the physics of a solid we seldom need to know the ‘complete structure’; in most cases the ‘local structure’ is sufficient.
The local structure is governed by chemical forces, while on the other hand the long-range order reflects the way structural units on the local scale pack together to form a three-dimensional solid.
Theorists can now solve the problems of solid-state physics in terms of the local structure, whereas previously they could deal only with materials where the complete structure was periodic and crystalline symmetry allowed them to make colossal simplifications.
But all this does not help with amorphous (non-crystalline) materials such as oxide glasses.
Although glasses in general do have a definite local structure their overall structure is disordered.
As a result they diffract X-rays diffusely.
That is to say, in place of the regular ‘Weidoscope’ pattern of spots that the regular planes of atoms in a single crystal generate, the diffraction pattern of a glass consists of haloes of uniform intensity (Figure 1).
Accordingly, instead of the three-dimensional geometry of atoms that can be obtained from the diffraction pattern of a crystal, a glass yields only a one-dimensional radial distribution.
In a multi-component disordered solid like soda-lime-silica glass a single spherically averaged diffraction pattern is quite inadequate.
There is no way we can decipher properly the individual contributions that different pairs of atoms make to the total atomic distribution.
We cannot, for instance, distinguish clearly between Na-O and Si-0 pairs, or Na-O and Ca-O pairs.
What is worse is that in glass sodium and calcium are more weakly bonded to oxygen than is silicon.
This means that Si-O distances dominate the distribution of atoms obtained from diffuse X-ray diffraction.
But, as I have described, it is ingredients such as soda and lime that alter totally the properties of Mass and whose local atomic structure is therefore especially interesting.
This is the point at which synchrotron radiation comes to the rescue.
Synchrotron radiation is quite simply the most intense source of white X-rays we have.
The radiation is a by-product of particle accelerators use in high-energy physics.
It is emitted by electrons and positrons as they are guided by magnets on curved paths round synchrotrons, or storage rings as are often used today (New Scientist , vol 87, p 305).
The particles travel close to the speed of light and as they pass through the bending magnets they emit radiation, covering wavelengths typically from 0.1 to 106 nanometres, and collimated into the plane defined by the circular machine.
This is what makes the radiation both ‘white’ and ‘intense’.
The X-ray flux is so great when it emerges from the machine's vacuum that it causes the air to fluoresce and, if focused, it can burn holes in paper rather like a powerful laser beam.
Given all this extra X-ray power we can not only measure diffracted X-rays more accurately, but we can now also measure absorbed X-rays with precision.
When X-rays are absorbed by matter electrons are ejected from the atoms of which it is composed.
The more tightly bound, or closer, to the nucleus an electron is, the more energy is required to dislodge it; thus radiation of short wavelength, that is, with more energy, produces photoemission of electrons closest to the nucleus.
The atomic electrons occupy discrete energy levels and the various ‘thresholds’, or wavelengths at which absorption begins to occur, enable us to distinguish one element from another, even if they are all present in the same specimen.
Figure 2 shows the absorption edge threshold of Si in glass.
Once a particular electron has soaked up enough energy from the X-rays for its release it begins to radiate from the parent atom (Figure 3).
Just like the X-rays, the photoelectron is diffracted by the atoms close by, rather like ripples spreading across a shallow pool disturbed by occasional raindrops, and the resulting diffraction pattern modulates the absorption of the X-rays that are in fact exciting the pattern via photoemission.
As a result the X-ray absorption spectrum contains a precise signature of the atomic environment of a particular element.
The phenomenon is called extended X-ray absorption fine structure, or EXAFS for short, and is quite general to condensed matter.
The EXAFS for Si in glass reveals itself as the ‘wiggles’ following the absorption threshold, seen clearly in Figure 2. the case for the application of EXAFS to glass is overwhelming.
By tuning the X-rays to each element in turn we can (in principle) pick up the specific local arrangement of atoms.
Diffuse X-ray diffraction, by contrast, gives us only the local arrangement about the average atom.
What is more, the long-range order present in crystalline matter, but absent in glass, is important.
Returning to Figure 2 we can see just what can be achieved for glass.
The individual EXAFS spectra for silicon, sodium and calcium in glass are shown alongside their respective atomic distributions.
Look how different they are!
For the first time we can differentiate the local atomic order around sodium from that surrounding calcium and from the atomic environment of silicon.
Notice, for instance, that the structure surrounding the modifying cations, sodium and calcium, is complicated, involving several well-defined shells of atoms.
Silicon, the glass-forming cation, on the other hand, has a single well-defined shell of oxygen, the more distant shells being considerably distorted by rotations and deviations in the bond angles.
Using synchrotron radiation, EXAFS demonstrates the inadequacies of the conventional theory of glass structure.
Ideas about the structure of glass date back 50 years or more to when the first diffuse X-ray scattering measurements on the material were being made a by B. E. Warren and others.
It was at this time too that W. L. Bragg was beginning to unravel the complex structures of the crystalline silicates.
The overriding emphasis in all this work was on the ‘network’, which in silicates was found to be made of regular chains or layers of SiO4 molecules sharing common oxygens.
In quartz the crystalline network was three-dimensional.
W. H. Zachariasen, an American structural chemist, extended the ‘network idea’ of crystalline chemistry to glasses.
By allowing the SiO4 units to rotate freely about their common oxygens and by making the bond-angle of the ‘bridging oxygen’ flexible, these chemists could conceive of a continuously connected random network that left the local order intact, but which destroyed the overall crystalline topology.
This model remains today the best For simple glasses, such as silica, and many of the new amorphous semiconductors.
For modified glasses, though, Zachariasen proposed that the extra oxygen brought in by soda and lime would promote singly coordinated sites or ‘non-bridging oxygens’.
The sodium and calcium cations, Na+ and Ca2+, would loosely bond to these oxygens occupying occasional holes and voids in the glass-forming network; that the modifying cations might have a well-defined local structure was all but ruled out.
By contrast, the new EXAFS studies show that modifying cations such as Na+ and Ca2+ have an important role to play in the structure of glass.
This is illustrated schematically for two dimensions in Figure 4.
The structure is continuously connected in a random way but the coordination of sodium or calcium as well as silicon now matches values that can be measured directly using the EXAFS technique.
Figure 4 shows in two dimensions how short covalent bonds and long ionic bonds can both be accommodated in a random structure.
Picture this in three dimensions and you have a silicate network striated by layers of modifying oxide.
This picture enables us to see structurally why the viscosity of glass depends strongly on the modifying component.
To take a cuisinary analogy, threads of modifier bind the irregular network regions together something like eggs bind the flour in a cake mix.
The more eggs you add the runnier the mixture!
It is also clear from the picture why glass can often be good ionic conductors.
If the concentration of modifying oxide is sufficient, the threads of modifier will percolate through the whole structure defining routes that avoid the insulating silicate network.
The ability to probe the  environment of the individual atoms that both form and modify glass offers exciting opportunities.
For instance, EXAFS measurements can help to elucidate the differences in coordination of the modifying cations between one type of glass composition and the next.
Do these differences arise from the local packing of cations and anions, or does the random network of chains and planes of silicate, for instance, impose the main constraint?
Another area of study in which EXAFS will be valuable concerns mixed-alkali effects in glasses whereby the physical, and more particularly the electrical, properties of a glass containing a single alkali are considerably changed by introducing a second alkali.
Because EXAFS enables us to look at the atomic environment of each alkali in turn we may begin to understand the structural basis of this curious effect.
Then there is the whole question of phase separation in glasses, which makes it almost impossible to produce homogenous glasses of certain compositions.
What we learn in these areas must also have bearing on our understanding of the nucleation and growth of crystals in glass ceramics, photochromic glasses and other heterogeneous systems.
Our new insight into the structure of glass is only just beginning.
REVIEW
Ten steps to transform the future
Megatrends by John Naisbitt,Warner Books*, pp 290, $15.50 
Christopher Freeman
THE publisher's comment on this book compares it with The Organisation Man and Future Shock , claiming that once in a while a book so accurately captures ‘…the essence of its time that it becomes the spokesman for that decade’.
Up to a point this claim is justified, although a more interesting comparison might be with Alvin Toffler's Third Wave , rather than his Future Shock This comparison, however, brings out some of the weaknesses as well as the  strengths of Naisbitt's study.
Unlike the Third Wave (or Future Shock ), the style of Megatrends can barely be described as a literary style.
It is telegraph-ese with frequent staccato-like emphasis.
The general impression is that of an executive summary on every topic which is discussed.
Whilst executive summaries can be useful, they create problems when they are virtually extended to book-length.
This problem of style arises in part from the choice of technique, and here lies the true originality and great strength of the book.
Naisbitt and his consultancy group have specialised in the technique of ‘content analysis’.
According to Naisbitt, this technique was used extensively by American intelligence during the Second World War and is still used by intelligence services today.
Regular and systematic analysis of a country's newspapers can provide information on social trends, which may be difficult to obtain from other sources.
Naisbitt has transferred this technique from intelligence operations to commercial and social applications, with some very interesting results.
He claims that it is an effective tool both for monitoring established social trends and for identifying new developments at an early stage.
In his view, the space available for reporting news in American newspapers is relatively constant over time.
Consequently, to put it in his own inimitable style: ‘The news-reporting process is forced choice in a closed system.
In this forced choice situation, societies add new preoccupations and forget old ones…
Evidently societies, like individuals can handle only so many concerns at one time…
If new problems or concerns are introduced some existing ones are given up.
All of this is reflected in the collective news hole, that becomes a mechanical representation of society sorting out its priorities.’
Using this technique, Naisbitt traces the rise and decline of concern for the environment in the United States during the 1970s.
But because it has declined the environmentalist movement is not one of the main concerns in his book.
He has identified 10 ‘megatrends’ which, he believes, will characterise American society during the 1980s.
He devotes one chapter of the book to each of these trends: 1) from an industrial society to an information society; 2) from forced technology to high tech/high touch (this typical jargon describes the increasing importance of human, social and spiritual responses to advanced technology); 3) from national economic concerns to world economic concerns; 4) from short-term thinking to long-term thinking and planning; 5) from  centralisation to decentralisation; 6) from institutional help to self-help; 7) from representative democracy to participatory democracy; 8) from hierarchies to networking; 9) from north to south (within the United States); and 10) from either-or to multiple options.
Naisbitt is a shrewd observer and in practice he does not limit himself strictly to the confines of content analysis.
He laces his narrative with a great deal of information and conclusions derived from other sources.
In fact, the reader will seek in vain for an explanation of just how the column-inches in 2 million articles in local American newspapers over 12 years are converted into the 10 ‘megatrends’.
Nevertheless, he does report many developments in the American social scene, which obviously owe a great deal to the careful monitoring of many local newspapers.
One interesting innovation which he uses is to take five American states — California, Florida, Washington, Colorado and
Connecticut — as foreshadowing ‘indicator states’ for future developments in the rest of the 50 states.
Naisbitt is more realistic than Toffler about some aspects of the shift towards an ‘information society’.
For example, he takes more account of the findings of social psychology and of economics in recognising the limitations of the ‘electronic cottage’ scenario, in which people supposedly stop going out to work in factories and offices.
He is also very direct and honest in facing the issue of the relative decline of the US economy in the 1970s and 1980s.
As British economists know only too well, it is easy to turn a blind eye to unwelcome truths of this kind and to indulge for decades in a form of national self-delusion.
If the American people, like Naisbitt, can face this situation realistically and without xenophobia in the 1980s, it will be good for the rest of the world as well as the United States.
It is also good news that Naisbitt's consultants are telling top managers in some American firms in no uncertain terms about the importance of participatory democracy.
Neither Naisbitt's shrewdness, however, nor his ear-to-the ground monitoring techniques, can overcome some of the fundamental limitations of his approach.
As he has no theoretical framework and no fundamental economic background, he is limited essentially to extrapolation of short-term clusters of newsworthy items, as seen through the eyes of newspaper editors and reporters.
The nemesis for this pure empiricism is that some ‘megatrends’ may be greatly exaggerated; the outcome of some social and political conflicts may be wrongly estimated and some ‘megatrends’may be omitted altogether, if they do not happen to slot into the chosen classification system.
It is astonishing to find that ‘unemployment’ finds no mention in the book and difficult to believe that this ‘megatrend’will not have as great an influence on the United States in the 1980s as the 10 trends which he discusses.
It is not by any means inconceivable that it could upset most of the other 10.
Creches for ducklings
The shelduck by I. J. Patterson,Cambridge UP, pp 276, £27.50 
Janet Kear
This is a book of a lifetime's study undertaken on the Ythan Estuary near Aberdeen of a duck that is, in many respect, unusual.
Shelduck are aggressive to their own kind yet, on the Ythan  Estuary , are highly gregarious.
The female is brightly coloured like her mate with whom she frequently pairs for life, but the sexes migrate separately to and from a moulting ground on the Heligoland Bight.
In Scotland they become territorial on an area of estuary mud from which they filter the small invertebrates that make up their diet.
This female lays her eggs some distance away in a rabbit burrow and when the ducklings hatch, both parents take them to a different feeding position on the mud flats where, for a while, they again maintain a territory.
The young however, tend to join others in large creches and are eventually deserted on their parent's departure for Heligoland.
Dr Patterson has tried to untangle the survival value of these behaviour patterns and relate them to the animal's ecology.
No easy answers came and there were many apparent paradoxes.
For instance, he examines the costs and benefits of creching to the ducklings, to the parents who lost offspring to the common pool and to the recipient adults who rear other birds' young.
To his surprise the behaviour seems to be bad for everyone.
Shelducks breeding in smaller groups at lower density, or as isolated pairs, where there is no chance for creches to develop, may be the ones producing a surplus of ducks that enter and sustain the quarrelsome colony on the Ythan.
The thoroughness and honesty of treatment does the author credit.
The cost of the book must provoke adverse comment.
Its size, and the fact that it was printed on the Pitman Press at Bath, lead one to compare it with the ornithological publications of Poyser which it closely resembles.
The Poyser monograph on The Barn Owl was published at the same time; it has more words, contains many photo graphs and even has a colour plate, yet is £15 cheaper.
Why?
Mathematics to  describe shapes
The fractal geometry of nature by Benoit Mandelbrot,Freeman, pp 460, £22.75 
Michael Berry
FRACTAL geometry is one of those concepts which at first sight invites disbelief but on second thought becomes so natural that one wonders why it has only recently been developed.
Benoit Mandelbrot's central idea presented here in a revised and expanded version of an essay first published in English five years ago, is that many of Nature's forms have irregularities so extreme that they are best described not by the one-dimensional curves and two-dimensional surfaces of conventional geometry but as intermediate shapes (’ fractals’) whose dimensionality need not be a whole number.
For example, a coastline is a curve whose length (between any two points) increases when measured more accurately so as to include its ever-finer convolutions round bays, headlands, cliffs, boulders rocks, pebbles, etc, and on any reasonably simple model the length is infinite.
On the other hand, the coastline's area is zero, so neither a one-dimensional nor a two-dimensional picture is appropriate, and in fact the way in which the length increases with resolution suggests a dimension D of about 1.2 The mathematics of sets of points with fractional dimensionality was developed in the early years of this century, but associated geometric objects were considered as ‘pathological’ and not corresponding to anything in Nature, Mandelbrot's massive and single-minded achievement has been to convert this abstract formalism into a flourishing branch of applied mathematics, in three ways.
First he has enriched our geometric imagination by recognising and systematically exploring a wide range of fractal shapes to augment the familiar (Euclidean) circle, triangle, sphere, cube, etc.
With computer graphics of stunning beauty, we are conducted through the weird world of Koch curves, Seirpinski carpets, Menger sponges, Fatou dusts, self-squared dragons, and Apollonian gaskets.
All these shapes posses a hierarchical structure extending to infinitely small scales.
Secondly, he demonstrates that fractals are good models for an impressive variety of natural objects.
These include landscapes (surfaces with D–2.2), networks of rivers (D–2), lung branches (D–2.9), and blood  vessels (D–3), cloud perimeters (curves with D–1.3), places where energy is dissipating in turbulent fluids (D–2.55) and, on the largest scales, the hierarchies of clusters and superclusters of galaxies (dusts with D–1.23).
Thirdly, he emphasises that fractals imply an unconventional philosophy of geometry.
In the ‘Newtonian’ picture, shapes to which the calculus can be applied have the property of smoothness: the more they are magnified the simpler they get (in the limit, curves can be replaced by their tangents).
By contrast, magnifying a  fractual reveals finer levels of structure ad infinitum : the whole structure is contained in microcosm near any point.
So far the subject of fractals has been almost entirely descriptive.
It is true that to discover that an object has a particular fractal dimension D is a valuable addition to knowledge and replaces earlier imprecise characterisations such as‘spotty’, ‘stringy’ or ‘lumpy’.
But this is only a  first step to scientific study, and two questions naturally follow: having found D what use is it?
And what is the reason for this D and not some other (or, more generally, what is the origin of hierarchical structure)?
With regard to the first question, time will tell; my own experience is that knowledge of D can open the door to the study of several physical processes involving the fractal structure.
As to the second, there are two fractals in physics whose D is fairly well accounted for, namely the ‘Brownian’ drift of a small particle jostled by molecules in a fluid which is an erratic curve with D=2 , and the  hierarchy of density fluctuations in a fluid at the critical point where it cannot properly be considered as liquid or gas.
It must be realised that fractals are hierarchies with the particular property that the successive levels are geometrically similar (either exactly or on the average).
There are many non-fractal hierarchies in which successive levels differ qualitatively.
The living hierarchy of ecosystem, species, individual, organ, cell, nucleotide, molecule is an example.
Mandelbrot's essay is written in a personal, intense and immediate style.
Technicalities do not intrude but are sufficient to prompt serious research.
There is an extensive bibliography and fascinating biographical sketches of the often eccentric scholars who anticipated fractals.
This is an important book from which scientist can benefit and which lay people can enjoy.
 Scientists , the arms race and disarmament
Who is responsible?
A Unesco/Pugwash symposium edited by Joseph Rotblat Taylor and Francis, pp 323. £9.50 
Robin Clarke
THIS informative and often horrifying volume results from a Unesco symposium held in Ajaccio, France, in February 1981.
Some 20 distinguished scientists give their views on the involvement of scientists with war in general and the arms race in particular.
Also included are the conclusions and recommendations of the symposium.
The basic facts, of course, are by now fairly well known: that something like 40 per cent of all research and development worldwide is undertaken for military ends.
This makes military research and development by far the most important scientific activity — important in numbers of scientists involved and money spent; it is also the most important in that it threatens the annihilation of more people than any other human activity.
Because of this, it is alleged, the arms race is fuelled into ever more expensive, dangerous and faster spirals.
While the motivation for the arms race is political, the scientist himself (and indeed herself) is the mainspring of its momentum, sometimes wittingly, sometimes unwittingly.
One of the conclusions of this symposium was that ‘There is a growing belief that the momentum of the arms race is determined by the actions of scientists.
This belief is exaggerated; a multitude of factors, interacting with each other, is involved…’
However, the recommendations go on to prove themselves of sterner stuff.
They come close to enjoining all scientists to refuse to engage in military research but then back off at the last moment.
Instead they implore scientists to ‘ponder on the social implications of their work’ and to become actively involved in the effort to stem the arms race.
To help them to do so they list 12 tasks for scientist, such as participating in research on the economic effects of disarmament and urging editors to provide space in their journals for discussion on disarmament issues.
The 18 major contributions are themselves fairly impressive, I particularly liked Francesco Calogero's analysis of the dynamics of the nuclear arms race and John Ziman's discussion of the basic principles involved in talking about social responsibility among scientists.
There is an informative chapter on new conventional weaponry, and an analysis of the arms race from the Third World viewpoint.
All in all, this is a commendable book, packed with sound information and wise opinion.
One underlying issue, however, troubles me, Throughout there is the implication that to partake wittingly in an arms race is a perversion of science — as the conclusions say, ‘the role of scientists in the arms race is of crucial importance.
This role…is contrary to their traditional calling’
Is it?
Traditionally, scientists have been involved in war since the year dot.
I suspect what the conclusion wants to imply is that there is something in the philosophy of science which says that science should be used for good things and that war is not one of these.
This, of course, is nonsense.
On the contrary, the opposite view is intellectually more compelling.
As Herbert Marcuse once said ‘If the needs of science so perfectly match those of the Pentagon…
’. If nearly half of all science has to do with the military, it is surely tautological to refer to this as a perversion of science.
On the contrary it is science.
I doubt, therefore, that scientists engaged in military research are being irresponsible to science.
I do not doubt however, that they are being irresponsible to the world, to humanity and to life on Earth.
But that is a very different matter.
Ever reaping something new
Agriculture: The triumph and the shame by Richard Body,Temple Smith, pp 139, £2.95 
Graham Harvey
YOUR view of British agriculture depends largely on where you are standing.
From Whitehall the view is obscured by the crush of lobbyists from the farming unions, the chemical companies, the tractor manufacturers and the food processors.
Which may account for Minister Peter Walker's recent description of agriculture as an ‘outstanding success’.
Tory backbencher Richard Body sees farming rather differently as he commutes from his Lincolnshire constituency.
He observes an arable monoculture set in an unpopulated countryside.
Small farms are disappearing.
Wages are dying, the landscape is ravaged.
Grain surpluses pile up in disused hangars, while pesticide and machinery sales-’ men grow rich on a tide of public subsidies.
Body deals with British agriculture's much-publicised successes its unmatched improvement in output per man, the impressive yield increases of its major products, the mobilising of thousands of hectares of marginal land for arable production.
These are the ‘’ triumphs’and the book devotes a couple of pages to them.
But most is concerned with farming's ‘shame’, the enormous cost to the community of public support for farm incomes.
There is the £3000 million and more which consumers pay in their food bills each year to keep the EEC market closed to cheaper foods from overseas.
Then there is the £40 000 million, in 1982 terms, which British tax-payers have spent in grants and subsidies to farmers since 1946.
Public support for agriculture is now estimated at 166 per cent of farmers net incomes.
According to Body, this swelling stream of public money has led to damaging distortions in agricultural systems and reduced the living standards of the whole population.
Much of the taxpayers' investment has merely stoked up inflation in land prices, effectively closing agriculture to all but the millionaire.
The support policy favours big arable farmers.
They have prospered along with the chemical companies and machinery manufacturers who supply them.
Small livestock farmers have gone to the wall in their thousands.
The book demolishes the ‘strategic’ and ‘balance of payments’arguments for heavy slate spending on agriculture.
It claims that protectionist farming policies have increased the burden of poverty in the developing world and subsidised the destruction of the British countryside.
Body presents his case with refreshing clarity and directness.
He does not allow such economic camouflage as monetary compensatory amounts and export restitutions to hide the real issues.
Stripped to their essentials his facts and figures are astonishing.
Can the government have got its farming policy so wrong?
Can the public and its media watchdogs have been so deceived by the bureaucratic meanderings of the EEC's Common Agricultural Policy?
The farming lobby has condemned the book's conclusions as ‘facile and dangerous’, which may be an eloquent testimony to their accuracy.
Complexity is a common refuge of scoundrels.
Any attempt to open up the convolutions of farm policy to public scrutiny is bound to be seen as a threat to those with an interest in the status quo.
Yet taxpayers have a right to know how their money is spent, in farming as in any other public activity.
Food production is generally presented as an a political process.
Of course, it is not; the community faces real choices about the nature of the agriculture it supports.
Richard Body has made a brave attempt to shed the mythology and propaganda, and to expose farming objectives to public debate.
Michael Haines's Introduction to farming systems (Longman, pp 214, £6.95) provides a less partisan entry to the techniques of contemporary agriculture and the economic climate within which it operates.
Michael Haines has produced a nontechnical guide to the natural, economic and social factors which shape the modern industry.
But he has not steered clear of farming's controversial issues.
Instead he has conducted a balanced exploration of such topics as organic systems, factory farming and the impact of agriculture on wildlife.
The book will appeal to the growing number of professionals and students whose work brings them into contact with farming.
A pharmaceutical ride
Cured to death by Arabella Melville and Colin Johnson Secker & Warburg, pp 261, £6.50 
Brian Inglis
‘This book makes a definite statement’, the authors begin ‘It is this: Western medicine has made a fundamental error in allowing itself to become reliant on the universal use of drug therapy.’
A mass of evidence follows, most of it from sound medical sources, that the medical profession has become hopelessly hooked on prescription drugs; that the drugs are neither so effective nor so safe as our doctors would have us believe; and that the public and the profession is being  remorselessly taken for a ride by the pharmaceutical industry.
‘We ask you to consider the possibility,’ they beg us, ‘that drugs, rather than helping or curing, are  actually doing harm.’
I have no quarrel with the thesis, and considerable admiration for the accumulation of evidence, though what presumably were the chronic delays of publishing leave some of it a little dated, But I have to admit to irritation with the manner in which the material is presented.
It is as if Dr Melvile, a psychologist, and Colin Johnson, a journalist, feel that they are springing a brilliantly new idea on us, which nobody has thought of before.
There is no bibliography, but neither in the text nor the source references do they give any credit to the pioneers in this field such as Les Schindel and L. Meyler.
The Kefauver report is mentioned in passing but neither the Hubert Humphrey follow-up, nor Morton Mintz's massive By Prescription Only , nor Rick Carlson's The End of Medicine get so much as a mention.
Even Ivan Illich, though their opening sentence contains an echo of his ‘the medical establishment has become a major threat to health’, is ignored in the text: he scrapes in with a mere single source reference.
This is not merely a little churlish; it also explains why the cumulative effect of their evidence makes less — impact than it should, because it disguises the fact that the unholy alliance between the medical profession and the pharmaceutical industry has tottered from crisis to crisis now for two decades — since the thalidomide tragedy.
Every few years the  industry begins a campaign, backed in medical  journals , for release from its shackles.
Then, along comes practolol, or the most recent disaster, Opren — which has led the New Scientist , among others, to urge that the checks on new drugs should be made even tougher.
The industry's hit-men will have to lie quiet, for a while.
But they'll be back.
As ‘any analysis of the misuse of drugs must find fault with doctors’, the authors conclude that they ‘need to be re-educated about drugs and their place in therapy’.
How?
The whole lesson of the events of the past 20 years is that doctors cannot be re-educated, so long as their  training is drug-based their journals utterly dependent upon the revenue from the drug companies advertisements, and their leisure activities agreeably subsidised by the drug companies' slush funds.
Of course there are individual doctors who are well aware of the mess their profession has got itself into; but the very fact that they are in a profession, and one which has succeeded in armouring itself against state intervention, makes it hard for them to get their ideas accepted.
Ironically, the best hope for change lies in the failure of so many new drugs to make the grade.
‘ Industrial drug research is facing a crisis’ was the headline of a British Medical Journal editorial recently; and all the indications in Cured to Death are that it is going to get worse.
BBC Video in the doldrums
Barry Fox
THERE are now nearly 13 million video-tape recorders in British homes, and one might wonder why is there so little material available on video tape from the BBC.
The BBC's archives are bulging with entertainment, drama, music, science and educational material, but the corporation cannot offer the material on video tape because of a convoluted set of industrial problems.
To its credit, the BBC recognised the potential value of its archive material surprisingly early in the video boom.
On 3 January, 1974, the BBC wrote to the Musicians' Union suggesting that the BBC reissue old material on video, with appropriate royalties for the musicians, actors and scriptwriters.
But the BBC and talent unions — the Musicians' Union, Equity and the Writers' Guild — still cannot agree on how much the performers and writers who contribute to the original broadcast should be paid out of profits accruing from re-issue of their work on video disc or tape.
The BBC finally entered the video market in June 1981, but with a catalogue of only 20 titles.
John Ross-Barnard, head of BBC Video, then said he foresaw the release of around 300 titles, including classical serials, Shakespearian drama,Fawlty Towers and The Hitch-Hiker's Guide to the Galaxy .
Much of this material is edited and ready to release.
This material is of mass appeal, but is necessary to build a basis for ‘narrowcasting’.
Think of ‘narrowcasting’ as the opposite to ‘broadcasting’— reaching a few selected people as opposed to a mass audience.
But the only way the BBC can afford to make original programmes for relatively limited audiences, such as science, is to pay for them with the profits generated by the sale of archive material of wider appeal.
With this in mind, the BBC has bought the rights to a clutch of old black-and-white feature films, which it hopes people will buy or rent for lazy Sunday afternoon viewing.
If this proves profitable, then the corporation will be able to make more specialist material for video.
Meanwhile, the only people making money' out of BBC programmes are the pirates, who are busily taping programmes ‘off-air’, duplicating and  selling them through shady shops and Middle Eastern outlets.
The BBC has managed to use some archive material without coming into conflict with the unions, and David Attenborough's Video book of British Garden Birds is a good example of what narrowcasting can offer.
The subject is obviously of minority interest.
Moreover, David Attenborough's presentation is a far cry from the gee-whiz approach that some science producers adopt to ensure that their programmes are broadcast to the nation.
The Video book was never intended for broadcast; it is made from library film footage transferred to disc and tape for video issue, and aimed at the relatively limited number of people who will want to buy or hire it.
So a devaluing gee-whiz approach is not necessary; in fact it would be counterproductive.
The Video book not only has optional teletext but there is a double soundtrack to give the viewer a choice of listening to commentary or bird song, or both.
In contrast, Thorn EMI has negotiated distribution deals with Thames TV and London Weekend Television for a string of television material on video.
The Death of Adolf Hitler , the ‘best of’ Peter Cook and Stanley Baxter,The World at War and David Bellamy's Botanic Man are all released on both VHS and Beta video tape.
The supreme irony is that Thorn EMI has issued a video tape of Monty Python film sequences (apparently identical to those locked by dispute in the BBC's vaults), shot for German television and owned by the Monty Python team!
With the profits generated by these tapes, Thorn EMI can afford to prepare narrowcast programmes, such as one on whales, based on a superb film shot by Anglia TV.
Another programme explores the fish that congregate around sunken wrecks off the British coastline.
Then there's a series of science educational films, which feature a know-all robot that teaches children to carry out harmless experiments in the kitchen.
The choice of a robot as an on-screen guide is ingenious; it speaks without lip movements, so that the film can easily be dubbed into foreign languages.
The stereo soundtrack has different information on each channel, so that sequences can be played twice with a different soundtrack each time.
Thorn EMI also has a new DIY video-disc manual on car repairs, which will allow the viewer to skip sections he's not interested in, freeze frames etc while he takes in the details.
These programmes were originally designed for release on JVC's video-disc system, VHD.
Unfortunately the VHD project is now on ice, and the company may have to look for another video-disc format.
It is with video discs that the viewing revolution could come.
Encyclopedia-type material can be presented, mixing still pictures, text and motion-picture sequences, that allow the viewer to interact with the programme and ultimately to link the player with a computer (see New Scientist , vol 96, p150).
But until the BBC reaches an agreement with the unions to release blockbuster archival material for mass audiences opportunities offered by video will elude BBC Video.
Surrealist mathematics
The adventures or Archibald Higgins by Jean-Pierre Petit, translated by Ian Stewart John Murray, three books, £4.95 each 
John Little
MY MOTHER, being concerned about the reading habits of the young Little, once expressed her concern to my primary school headmaster.
‘All he ever reads are comics.’
The wise old sage nodded and replied, ‘Does he read the bits in the balloon?’
‘Oh, yes.’
‘Then I wouldn't worry if I were you.
Wisdom incarnate.
And so I progressed to secondary school.
There I learned half my science from my teachers and the rest form George Gamow's splendid Mr Tompkins books.
I sometimes wonder if the reason I struggled with university mathematics was that no one had taken the trouble to express the mysteries of analysis in comic strip form.
Thus it was with pleasure that I came across this series of ‘cartoon stories for adults of any age’, created by French scientist Jean-Pierre Petit.
Archibald Higgins is an ever-curious adventurer who sets out to discover what the world is really like aided by a motley assortment of friends, including a blindfolded snail, a cockney pelican, and a shapely fairy godmother called Sophie.
My own favourite among the three books is Euclid Rules OK ?(pp 64) in which Archibald travels to various worlds to investigate their geometries.
Soon we find ourselves immersed in geodesics, hyperspheres, dimensionality, curvature and ideas of general relativity.
In Flight of Fancy (pp 64) he grapples with the problems of building a flying machine while Informagic (pp 72) finds him trapped in the workings of a giant computer.
Ian Stewart has done a first-rate job of translation, preserving the scientific content without losing any of the humour.
The result is that the books can be just as amusing on second reading, which is just as well given the ambitious content.
They are not for those who insist that learning must be a serious business and who would feel embarrassed to be caught reading a ‘comic book’.
Personally I found them both informative and entertaining.
My only quibble was the lack of colour, but you can't have everything.
Perceptions of nature and ethics
‘LET'S run some old nuclear power station footage, with the Pastoral Symphony as backing’
‘Great.
And we could reveal really relevant things like how the plants in Kew Gardens are actually the Spoils of Empire.
And how Landseer became rich by painting nature red in tooth and claw.
And stuff people like Lord Mark ignored.’
‘Fantastic.
Then some concrete parallels, like how men tried to control chaotic nature and disorderly women at the same time.
And Lake District shots, intercut with pictures of Jobcentres…’
‘And jokers at a shooting party with Land of Hope and Glory …
‘We must finish with totally relevant insights about how big business had made us see nature today.
We could explain to viewers that cars are now built by robots, and use computer graphics so that people will understand how they're being totally dehumanised…’
‘But we have computer graphics in the opening sequence.’
‘That's all right.
If we're clever we can make it politically concrete.
We'll use a bank cash machine to prove that nature is being turned into a series of numbers.
And we could show bar codes in a supermarket and tell people that all notions of good and evil, beauty and horror, have been totally turned into bits of information…‘
Such, one imagines, must have been the calibre of planning behind A history of nature (Channel 4, 16 January), another in the stunningly amateurish Crucible series on sciences in society.
Cliches and silliness aside, this attempt to portray our changing perceptions of nature could have been highly illuminating, Yet starting with some splendid material — ranging from a fascinating 1947 film about the new Welwyn Garden City, to Grey Walter's Machina speculatrix and its modern descendant — the producers managed to compile something monumentally tiresome.
This is unfortunate, for a series that has been gestating for three years.
Doctor's  dilemma (BBC2) originated much more recently.
But that fact, and its potentially unappetising formula of talking heads, have not prevented this series from becoming one of the most valuable of recent innovations in the public discussion of medical issues.
Even the predictability of the topic aired in The Vicar's Daughter (18 January)—‘Jenny is 14 and pregnant.
She wants an abortion and does not want her parents to know anything about it’— was no drawback.
The originality of the format, with actors acting, a real GP facing a real problem, and a studio panel assessing his response, has proved to be a winner.
And the quality and  livelines of discussion must surely have justified its prime 9 pm spot.
(Crucible, I notice, has already been shifted to a lower point in the liturgical day,) But what of the dilemma's resolution?
Though presenter Ian Kennedy appeared disappointed, the principal verdict was that judgement and discretion are paramount.
Contrary to a lawyer's yen for neatness there are few unambiguous signposts for modern medics facing this or many other ethical issues.
Save one.
Nothing and nobody in this programme questioned the medic's oath with which it began: ‘I will keep silence as to anything I have seen or heard in the practice of my profession which it would be improper to divulge.’
Coming shortly after a US physician declared in the New England Journal of Medicine (vol 307, p 1518) that confidentiality in medicine is —‘old, outgrown, useless and decrepit’, this was a welcome reassurance.
There's only one black mark against Doctor's Dilemma and its ilk.
Why restrict  programmmes like this to what is still something of a ghetto channel?
Just as there ought to be a Radio 2 equivalent of John Maddox's excellent Scientifically Speaking (which seems, unaccountably, to have disappeared), so such widely relevant debates should find a place on BBC 1.
Wake up, Mr Milne.
Watch that deer
Field guide to British deer by F. I. Taylor Page,Blackwell, pp 95, £3.95 
Jon Barzdo
DEER do not just lie down; they ‘harbour’, ‘lodge’ or ‘bed’, depending on the species.
It seems all part of life's colourful plan that deer-watchers, like every other group of people pursuing a common goal, secure their elitism by the use of jargon.
Luckily, the Field Guide's glossary explains the unfamiliar terminology and much of it is obviously useful.
The glossary and text, however, sometimes conflict, and quite why we are told that ‘Moving of deer’ is ‘A procedure involving the movement of deer…’is  beyond me.
As a field guide the book is excellent.
This is not unexpected, for it is the third edition of a tome first published 25 years ago and extensively used.
It has been revised and updated and now includes a simple summary of legislation relating to deer in Britain, and a county-by-county guide to distribution.
There are seven species of deer in Britain.
Of these, the reindeer, reintroduced in Scotland in 1952, is extremely limited in range; probably for this reason it is absent from the main text.
The Chinese water deer, an escapee from zoos and parks, occurs in five English countries.
So it is less clear why this animal is excluded from most of the sections that review characteristics of the other species, including the Asiatic muntjac, or barking, deer another escapee.
Taylor Page reviews the animals' habitat, tracks, droppings, voice, antler development identification, food, herding and so on.
The author would have done better here if he had avoided the generalisations.
There are small obstacles to the rapid use of the Guide — like the section on ageing in male roe deer, that somehow slipped into the review of feeding.
Despite such problems, however, it will be immensely useful as a handbook to anyone watching or studying deer; and you won't need a poacher's pocket to carry it.
Deep seabed mining — the last chance
Daniel Spagni, Luke Georghiou and Glyn Ford believe that Britain should sign the deep-sea minerals convention
AFTER more than nine years of negotiations the third United Nations Conference on the Law of the sea (UNCLOS) ended last December in Jamaica.
The convention which it produced was signed by 119 delegations but the problems which caused the conference to drag on for so long are anything but solved.
Disagreements over the exploitation of deep-seabed minerals, in particular manganese nodules, have overshadowed the rest of the text, which deals with almost every use of the sea.
Ironically, the actions of a few Western governments, led by the United states and including Britain, could finally kill off the nodule-mining industry before it even begins to operate, and all in the name of protecting the industry.
There is no doubt that the convention offers far from ideal terms for private companies contemplating nodule-mining.
It establishes a governing body for the international area of the seabed, the International seabed Authority (ISA), which will have an operating arm known as ‘The Enterprise’.
The purpose of the Enterprise is to involve Third World countries in mining nodules and give them a share in the profits.
Much Third World interest has been focused on this part of the convention, giving it a disproportionate importance as the first evidence of a new international economic order.
Private companies would, among other things, have to agree to provide the Enterprise with fully prospected sites and sell it technology if they want to obtain mining licences from ISA.
Despite these conditions, the convention looked set for signature two years ago, with the industrialised countries considering the  nodle terms balanced by Third World concessions in other parts of the treaty.
This balance was rejected by the incoming Reagan administration.
The Reaganites had strong  idealogical objections to the convention, regarding the convention as a dangerous precedent for other international resources, and as a step towards  bureaucratisation and away from free enterprise.
Last April, a compromise was almost  achieved but, according to the second-in-command on the US delegation, Leigh Ratiner, his team's efforts were sabotaged by opponents of the convention in Washington.
Since that time there has been active US lobbying among other Western nations against signature.
Of the nations that mine nodules, only Canada and France have signed, with Japan likely to follow.
Significantly, the last two have substantial relevant government research programmes.
Announcing the British decision to delay signing, the minister responsible, Malcolm Rifkind, told the House of Commons that, while parts of the convention were helpful, the provisions relating to deep-sea mining were not acceptable.
Britain, he said, needed to obtain satisfactory improvements in the deep-sea mining regime and that it would be exploring the  prospects for these.
What then are the alternatives for deep-seabed mining?
The constitution of the convention is such that it cannot now be amended.
If the British government's desire for alterations is to be anything but wishful thinking, these  will have to be obtained in the detailed interpretation and implementation of the provisions.
This rulemaking exercise will be undertaken by the preparatory Commission for the ISA and the International Tribunal for the Law of the Sea (PREPCOM), which is due to meet on 15 March in Jamaica.
The catch is that only those states which have signed or acceded to the convention by then can take part in any votes.
While there are two  years left in which to sign, the early decisions of PREPCOM will determine its future activities.
To begin with, a chairman has to be elected.
Well-known UNCLOS personalities, such as Paul Engo, are likely to compete for the post with Scandinavians such as J. Evensen of Norway or (a rising star) Peter Bruckner of Denmark.
The conference chairman, Tommy Koh of Singapore, is reluctant but might take the chair under pressure.
Another early problem for PREPCOM will be finance.
The US Would have paid 25 per cent of the initial costs of the ISA.
France and the USSR have already indicated that they will not make up the deficit.
With the Enterprise alone likely to cost over $1 thousand million, its entry into mining seems a very long way off.
During the last working session of the UNCLOS, arrangements were made to give guaranteed access to those countries and companies which had made substantial  investments in deep-seabed mining in the past.
France, Japan, India and USSR have government-backed projects.
Four multinational consortia consisting of companies from the US, UK, West Germany, Japan, Belgium, Canada, the Netherlands and Italy, are also involved.
Failure of these countries to join the convention and to register as pioneer investors would mean loss of privileges such as first pick on available mining sites and any priority in the allocation of production  authorisations .
The Reagan administration is trying to establish an alternative to the convention, in the form of a ‘mini-treaty’ including only the mining nations who recognise and protect each others' claims.
A prototype for this already exists in an Interim Agreement that the US signed with France, West Germany and Britain last November.
This is based on those countries' national legislations but applies only until an UNCLOS convention actually enters into force.
Ocean-mining companies would be premature in thinking that this offers a solution to their problems.
The existence of two rival regimes will create instability in an already very fragile legal framework.
It would be a brave private investor who would put up the huge initial capital or loans for what is, at the best of times, a high-risk, long-term investment when there is a prospect of long, drawn-out proceedings in the International Court of Justice — and even of a moratorium on activity.
Governments which have acted in the name of free enterprise would be most unlikely to put up the necessary funds themselves.
How then could the British government act to save the nodule-mining industry from being stillborn?
There are those who would argue that really this industry is unnecessary and that the convention is too important to the stable development of the oceans for it to be jeopardised by any one part.
They have a case, but the world needs new high-technology industries to climb out of economic stagnation.
PREPCOM provides the only forum for progress towards an acceptable global maritime framework.
By signing the convention Britain could help to end the impasse and take an active part in drafting rules which make the mining provisions more acceptable.
After all, as the French have pointed out, signing does not bind a nation.
Final ratification can be reserved for a satisfactory outcome of the debate.
Where the slide-rule rules OK?
Joseph Hanlost on a boost to Mozambique technical independence
UNDERDEVELOPMENT involves not only a lack of technical knowledge but also being linked to the wrong knowledge.
In colonial
Mozambique, technicians were not taught to use slide-rules.
Instead, they were shown how to calculate square roots by hand, and forced to do countless examples until they became highly proficient at it.
Furthermore, they were taught that this was fundamental mathematical knowledge — part of being ‘truly’ educated.
Thus two years ago, when Ministry of Education planners introduced a more practical and less theoretical maths programme in technical schools, several teachers who had been trained in Portugal in colonial times complained that by dropping square-root calculations from the course, the ministry was reducing the ‘quality’ of education.
There is, of course, much talk in the developed world that one of the roles of education is selection — picking out for advancement people who best fit a certain mould.
This is much clearer in a developing country, where the need for practical skills is so much greater, but where courses are always more theoretical.
Science courses usually have fewer experiments and more rote learning.
Indeed, learning science is like learning a catechism.
Maths is a mix of abstruse theory and detailed calculations.
And in the colonies there was a good reason for this.
The need was to pick out those people who could skilfully and accurately follow instructions and do low-level clerical and administrative jobs for the metropolitan country.
Problem-solving, the normal work of technicians, was left to the ‘experts’ sent out from the mother country.
These attitudes of highly theoretical, catechistic science as being ‘better’(or more ‘proper’) have continued in many Third World countries since independence.
And the industrialised world often encourages this, because it means such countries continue to be dependent on foreign technicians.
The breaking out of this mould has important political implications.
In colonial times, Portugal had close ties with South Africa, and most repairs and installations in Mozambique were done by white South African technicians.
The Portuguese trained few Mozambicans (adult literacy was only 15 per cent at independence seven years ago), so inevitably Mozambique remains dependent on foreign technicians — even, to some extent, on South Africans.
With the express intent of reducing dependence on South Africa, nine states of southern Africa have set up an economic cooperation association, SADCC.
South Africa's reply has been to attack transport and economic installations that link the neighbours so as to ensure that they remain dependent on South Africa.
In December, South African commandos blew up the fuel storage depot at the Mozambique end of the oil pipeline to Zimbabwe.
Many people see this as an effort to force Zimbabwe to import fuel from South Africa.
Slide-rules have now become a symbol of technical independence and have taken on a more political role.
As from two years ago, all students entering technical secondary school have been given a slide-rule for use in classes and subsequently at work.
Of course Mozambique would prefer to provide its students with pocket calculators, but there is no money with which to import the calculators.
And when you are two technical generations behind, even a slide-rule is a jump.
Slide-rules are free, because so many have been discarded as scientists and technicians switch to calculators.
The Mozambican programme was initiated by maths teachers in Holland, who shipped out 8000 slide-rules.
In Sweden teachers collected 10 000 slide-rules, which would have been enough for students starting in 1983 and 1984.
But slide-rules for  southern Africa are a political issue, and the slide-rules were destroyed in a right-wing attack in August.
The slide-rules had been passed on to a Swedish group known as Bread and Fish, which was packing them together with clothes and medicines for southern Africa, when arsonists burned down the warehouse.
Thus the Mozambican technical students will set their course this year without slide-rules.
So an urgent appeal is being launched in Britain and in other countries asking people to search for their old slide-rules in attics, in schools and in lab store rooms.
More information is available from the Mozambique Information Office, 34 Percy st, London W1P 9FG (telephone 01–636 7108), which is organising the slide-rule collection.
Political independence is meaningless without some degree of technical and economic independence.
Slide-rules can be an important boost to the students who will help Mozambique to gain technical independence.
Just another old leak
Graham Bennet on Holland's recent nuclear upset
HOLLAND'S only commercial reactor at Borssele is just about as ordinary a nuclear power plant as you could expect to find: 477 megawatts, 10 years old and saddled with a 20-year contract to supply electricity to the neighbouring Pechiney aluminium smelter at a third of the present going rate.
And of course, being a pressurised-water reactor, she also has an unfortunate history of incontinence.
Borssele might be smaller than most but she can certainly wet herself just as well as her bigger sisters.
Take February 1981, for example.
It was quite a shock when 400 square metres of the site was found to be contaminated by radioactivity.
Not only was the cause somewhat of a mystery but the contamination was estimated to have lain there undetected for two or three years.
The annual replacement of the fuel rods a month later did not go too well either.
When the reactor was restarted a jet of steam promptly issued from a wrongly adjusted valve in the secondary cooling system, and in the ensuring emergency shutdown a gasket in the primary cooling system gave way to deposit 3500 litres of radioactive water in the reactor's basement.
The public relations boys had to work overtime again.
But Borssele has not simply contented herself with the odd leak.
There was the occasion in 1979 when a valve malfunctioned and was ingeniously fixed up with a handy piece of angle iron.
But this commendable example of ‘do-it-yourself’ had hardly begun to prove its worth when the cooling water pumps went on the blink — or, as the incident report coyly put it, ‘Main supply pump jammed.
Emergency supply pump No. 2 jammed.
Supply pumps out.’
Since Borssele was merrily buzzing away at 95 per cent of her maximum output there were doubtless a few sweaty palms in the control room at that moment.
Of course, nuclear reactors have superbly efficient back-up systems for just such eventualities.
In this instance that meant three emergency pumps.
Unfortunately one was having its innards repaired at the time and another gave up the ghost after operating for all of a second.
Nothing daunted, the reactor personnel managed to get an extra pump working by the space-age technique of forcing open a valve and jamming it with a steel plate, though this had the distinct disadvantage of immobilising all of the other safety systems.
Come 1983 and the opening of the Sizewell Inquiry was obviously an emotional moment for Borssele, involving as it does a new sister plant just across the North sea.
Thus it was that on 3 January she decided to mark the occasion by letting go of another gasket, this time in one of her auxiliary purification circuits.
Another 30 000 litres of radioactive water in the basement, and the by now well-rehearsed emergency shutdown procedure was again called on to save the day.
The operators calmly assured everyone that there was no escape of radioactivity, and a day later normal service was resumed.
Whether the regular occurrence of such ‘incidents’ demonstrates the inherent risks of nuclear power or whether their containment emphasises the security of reactor safety systems will be debated ad nauseam over the coming nine months.
One feature of the latest mishap was of note, however.
A few years ago in environmentally aware-Holland an emergency shutdown in Borssele would have made banner headlines.
Now it merits but a few column inches in a few papers.
It was, after all, just another  leam .
Adapt and be damned!
Donald Gould offers some further opinions on future leisure
LAST WEEK I dwelt upon the horrors of a largely unemployed and inactive society as briefly glimpsed by the residents of this other Eden during the Christmas trance.
Now, and while anxious not to bore the pants off either of my  faithful readers, I feel bound to return to and to expand upon the theme as a result of what I consider to be a truly appalling piece of recent news.
It would seem that Madam (please call me ‘Falklands’) Thatcher has decided that 145 local tax collection offices should be closed over the course of the next four years, with the loss of 3000 Inland Revenue jobs.
The reason is, of course, that a few computers will be able to cope with the task of detecting the lies, evasions, inconsistencies, and omissions in our tax returns more surely, and much more rap idly, than the flesh and blood calculators whom they are about to replace.
So far, so good.
I suppose that the average Briton, although not naturally vindictive and revengeful, would be inclined to smile broadly at the idea of 3000 Inland
Revenue ‘personnel’(and I use the term ‘personnel’ advisedly, for they can't be real people, can they?) suddenly finding themselves victims of a money-is-the-only thing-that-matters approach to life.
But what, in heaven's name, are we to do with 3000 redundant tax inspectors and collectors?
They are, after all, members of a sub-species of the human race of a highly specialised kind.
They are possessed of a handful of skills and attributes which many of the rest of us lack, such as numeracy, and a general ungullability, and (presumably) a penchant for cryptography, so that they can actually penetrate and comprehend the tortuous laws and regulations they must administer.
But have they ever even heard of flexibility?
Are they even modestly adaptable ?
‘No!
No!— Not on your Nelly!’
I hear ten thousand times ten thousand voices cry in needless answer to this wholly rhetorical question.
Tax inspectors and tax collectors, as we all know, are nothing if not rigid.
One of my many dear children earned himself a creditable degree in environmental sciences a couple of years ago.
In view of the dicey and rapidly deteriorating state of our physical surroundings, largely attributable to our ham-fisted handling of the powers bestowed on us by scientific and technological ‘progress’, it might have been thought that there was a place in society for young persons who had taken the time and the trouble needed to give themselves some understanding of the problems we have set ourselves, so that they could help to reduce the damage done, and the worse damage yet to come.
Not a bit of it.
Since graduation this particular environmental scientist has spent an admittedly enjoyable spell as a porter in a Newfoundland hospital, and has helped to ferry a yacht to the Canaries, and is now washing dishes in the basement of a famous and expensive London hotel.
You see, he is adaptable , and has managed to fill his days and pay his way by having a go at a variety of jobs that he'd never have contemplated three or four years ago.
But can you imagine your average suddenly-made-redundant collector of taxes happily spending the rest of his or her working life scraping the scraps off the dinner plates of the overfed rich?
In any event there is a finite number of overfed rich providing a flow of plates to be scraped.
And also you need to be coloured or have a good degree to get the job.
There may be a million or so people in the land who have the capacity to turn their hands to anything that's going without feeling frustrated, or rebellious, or losing self-respect.
I suspect that there are many millions more who can function only in a contented fashion when given jobs to do, and when working to a pattern worked out by somebody else.
Cast them upon their own resources, and they'd be on Valium three times a day before you could say Hoffmann-La Roche.
Half of them are on it anyway.
During the past week I have been watching the Germanic guru, Ralf Dahrendorf, on the box, telling us Britishers just where we are going wrong.
The other evening, with the help of a lot of film, he put the spotlight on a very few areas of endeavour where he thought we might be actually going right.
One of these was a dreadful modern slum in Liverpool — an enclave of vandalised flats surrounded by wastelands.
Instead of waiting to be salvaged, some of the inhabitants had got together in an effort to cheer things up, and, so far as I could gather from the film, they were busying themselves pasting pieces of brightly coloured plastic over broken windows.
It was a scene of desperation, but Dahrendorf seemed to feel that this kind of community DIY activity could be a part of the answer to the problem of a permanent and growing rate of unemployment.
Rubbish!
People — most people, at least— need to be at work.
They are not capable of enjoying more than a limited amount of leisure.
The Luddites were right.
Better to bang shoes than bombs
Tam Dalyell on Soviet outbursts and overtures
THE MOST memorable dinner at the House of Commons was in April 1956 in Dining Room A, that long room into which people strolling on the terrace can easily peer.
Then Nikita Khrushchev and Nikolay Bulganin dined with the Labour leaders.
In his wisdom, Harold Macmillan, then prime minister, had decided to take an evening off, and to leave the Russians to the Opposition leaders.
George Brown, deputy leader, was in his hey-day.
And George Brown in his hey-day was a most formidable and enchanting arguer of a case — candid to a fault.
So much did his lecturing rile Khrushchev, that the Russian leader bade farewell to the company with the immortal remark, ‘Comrades, and Party
Leader Gaitskell, if I lived in Britain I would vote Conservative!’— and with that remark he and his party departed towards their fleet of cars.
That was the man who the same year had stood up in front of the full pad and launched into his celebrated five-hour denunciation of Joseph Stalin.
What follows is not apocryphal but hard fact.
Towards the end of the third hour, a little man at the back of the great hall, a faithful apparatchik from the area of the Caspian Sea, was unable to contain himself at the unanticipated exposition of the enormities of Stalin.
He got up.
‘And, during all this time, when Stalin was doing all these terrible things,’ he bawled, ‘what were you doing?’pointing an accusing finger at Georgy Malenkov, Vyacheslav Molotov, Lazar Kaganovich, Georgy Zhukov, Mikhail Suslov, Anastas Mikoyan and Kliment Voroshilov, the great mandarins on the platform, who between them had ruled Mother Russia under Stalin for the previous quarter of a century.
Deathly silence.
Everyone was agog to hear what on earth comrade Khrushchev would say.
Khrushchev paused.
Then, said he, ‘Would the comrade who interrupted me please stand up!’
Not a movement.
Delegates looked around in all directions to pinpoint the accuser.
Five seconds went by.
Ten seconds.
Fifteen seconds.
Twenty seconds.
Twenty-five seconds.
Not a  murmur .
Thirty seconds.
No sign of movement.
Then Khrushchev beamed a wide infectious smile, of the kind with which Winston Churchill and Aneurin Bevan used to charm their audiences.
‘Comrade,’ said Khrushchev, eyeing the back of the Great Hall, ‘that is exactly what we were doing all those years under Stalin.’
His reply was soon the property of every taxi-driver in Moscow, the most certain network in those days for news.
Those days (1956) were times of hope for frank discussion and for an end to the long Cold War.
I shall not forget my visit that year with the first National Union of students delegation to the USSR, and limping out of our aircraft at Vilna in Poland into the great reception area.
The dining room  possessed a huge marble bust of Stalin, but little succulent to eat.
Three weeks later when we had a stop-off at the same place, the bust had gone.
By 1964, when I next visited Russia, many hopes had been dashed by the Cuban missile crisis.
In Tblisi, Stalin's birthplace, my wife and I were shown around by enthusiastic Georgians from the Technological Institute.
Our hosts regaled us with much history about Prince Georgivhili and other obscure rulers of the Middle Ages.
But they made no mention of the Georgian who had ruled the vast soviet Union for over a quarter of a century in this one.
Talk of Stalin had become taboo: it was the period of de-Stalinisation.
It is one of the ‘ifs’ of history to speculate what would have happened if Khrushchev had succeeded in getting more of his way — shoe bashing or no shoe bashing at the UN General Assembly.
Would there have been a rapprochement with John Kennedy?
No one will ever know.
But one aspect of the history of these years does deserve to be recalled.
Until Khrushchev astonished our Georgian apparatchik at the 20th Party Congress, everyone in the West supposed that as some sort of Gauleiter of the Ukraine, he would be even worse than Uncle Joe.
The lesson is that we should not judge people by their reputation, without knowing a great deal more about them.
That  Yuri Andropov was head of the KGB, or even the USSR's  Ambassador in Hungary in 1956, may tell us little of how he might want to behave as the Tsar, and First Secretary of the Communist Party.
I am told by Russians, who in my experience have always been frank, that he is a man of formidable intellectual capacity, and little self-importance or vanity.
If this be so, surely the leaders of the Western world must take any proposals he makes for arms limitation extremely seriously?
I shall now break one of my own rules.
I have never before asked readers of New Scientist to take political action.
As scientists and non-scientists alike you have a duty to find out what your elected representative thinks of the British attitude to the present Soviet proposals.
If Andropov is snubbed, and his overtures dismissed, we may not have another opportunity, even in the lifetime of the youngest among us.
Jack Swigert: a real pro
John Parkinson offers a tribute to a famed astronaut
BACK IN 1970 Moon landings seemed so easy.
After the flawless performances of Apollos 11 and 12, Apollo 13 looked all set for another success.
Just two days after launch, Houston had reported that the spacecraft was in ‘real good shape.
We're bored to tears down here.’
Nine hours later, an oxygen tank in the service module blew up with a frightening bang.
‘OK Houston, we've had a problem here.’
That calm, matter-of-fact voice belonged to Jack Swigert, the command module pilot, who survived that close call but died of cancer a few weeks ago.
If things had gone according to plan, Jack wouldn't have been within 325 000 kilometres of Apollo 13 on that fateful day, 13 April.
Just a few days before the launch the original pilot, Ken Mattingy, had been exposed to German measles and was found to have no immunity, He was replaced by his backup, Jack Swigert, who managed just two days prime-crew training before leaving for the Moon!
Anyone who can sound so apparently unconcerned, with his spacecraft exploding around him, and Houston asking him to repeat what he just said, deserves to be called in the words of a fellow astronaut, ‘a real pro’.
Time off in New York
Lois Wingerson goes networking
‘I'M A PHYSICIST.
Theoretical.
I'm working on contradictions between the twin paradox and magnetism,’ said a tall, genial fellow not wearing his badge.
‘But you don't want to talk business, do you?’
Marvin Gaye's new song,Sexual Healing , throbbed somewhere — everywhere — above us.
‘Yeah, what is networking anyway?’ he went on.
‘All I know is I got this invitation in the mail about Studio 54 — for business networking.
What is that?’
Networking, I said, is what you do in the corridor outside the plenary session of the annual conference with that interesting chap from Stanford working on a similar problem.
The physicist just in from Colorado looked baffled.
This was decidedly not the annual conference.
It was, in fact, the once-trendy Manhattan disco Studio 54, where people used to be turned away for not being Beautiful.
Now anyone can get in on a Wednesday with $8 and a business card.
Business Networking salons is a scheme for interesting people to meet other interesting people, devised by ageing ex-hippie Jerry Rubin, who is now a stockbroker.
This Wednesday was for women in business, computers and design, but no one seemed to care.
The crush included plenty of scientists, and plenty of women, but seemingly none who was both.
Holding up the wall near the bar, Peter Materna, an engineer in Princeton's fusion project, was wearing his badge — which also said something about white-water rafting.
‘It's hard to find the technical people here,’ he lamented.
‘Look for balding, bearded men,’ I nearly said — but he may not have been looking for precisely that variety of technical person.
‘I'm here to find single women,’ admitted a computer scientist from Bell Labs, peering at my label,New Scientist ?
Yeah, good magazine.’
Everyone had a line.
(’ All right, so you're married,’a badgeless fellow chimed in as he slid by.
‘Do you cheat?’)
Above the dance floor, neon stalactites began to flash and large panels of light pivoted on axles.
Rubin himself appeared on stage for a pep talk, a short and stocky 40ish fellow in business blues.
For a former cult figure, he had surprising difficulty holding his audience or even his own thoughts.
‘I want you to have a great time tonight,’ he said several times, ‘but don't go without taking away 12 business cards.’
Ideas, as well as liquor, are supposed to flow.
In this era of funding cutbacks and academic brain drains, one must suspend preconceptions.
What will happen when Jerry Rubin gets around to Physicists', Engineers', and Molecular Biologists' Night?
Will tomorrow's flashes of scientific insight burst forth beneath the strobe lights of studio 54?
Not, alas, from one ex-Colorado theoretical physicist.
When I left he was gazing about disconsolately, still trying to get the idea.
Cover-up job
If WE can claim (with all due modesty, of course) that New Scientist has the best magazine covers around, then MIMS , the doctors' Bible of new drugs, certainly takes the medal for the most stomach-churning fascia.
A recent issue on MIMS concerned itself with massive, sudden hair loss.
Two causes of this unfortunate affliction, it seems, are a serious deficiency of protein in the diet and an excessive intake of vitamin A (carotene).
Seen any bald rabbits recently?
LETTERS
Ruinous legislation
Christopher Chippingdale's article on 100 years of our archaeological legislation and protection (’ The first inspector of ancient monuments,’13 January, p 100) was most welcome.
His point that sites were, in 1883, protected because of their scientific value is no longer true, I'm afraid.
1983 should see the passing of the government's new Heritage Bill.
That will create a body that is not quite like the National Trust, an Arts Council or Countryside Commission, but certainly a quango, probably under the leadership of Mrs Jenkins.
This will seek to make the sites more cost effective and see the application of ‘entrepreneurial’ skills to what was seen in 1883 as a scientific data base.
Those interested in arresting the shelving of direct ministerial  responsibility for these sites should write to their MPs, asking why it was, in 1883, that the inspector was responsible directly to the minister whilst, in 1983, there will be no inspectors as such and those responsible will be reporting to a quango.
 John Walker Greater Manchester Archaeological Unit University of Manchester 
Caufield's magnetism
I was intrigued by Catherine Caufield's comments about magnetic discs and tapes being wiped on the London Underground.
(Forum, p 104).
Indeed the BBC does send urgent tapes in taxis and you are supposed to avoid tube trains but the story I was told gave a different explanation.
Whether this is the true one I would not like to say but in this version the ‘trains wipe tapes’ hypothesis was the inspired explanation of a recording engineer who, through an operational error, had failed to record anything in the first place!
Another version says that the idea of tapes being wiped was just a wheeze to get a comfortable taxi back to base!
Presumably stray magnetic fields will not be that high as otherwise the efficiency of the motors would be low.
But I suppose that it could happen if one of them became faulty.
I understand that carriages without guard's controls and driving cabins, as found on most lines, do not have motors and should be safer.
However these are invariably carriages where smoking is permitted.
(Carrying magnetic media can damage your health!)
I have carried my own personal tapes and cassettes in NO SMOKING carriages for nearly 20 years and have never had a problem.
Another of the BBC legends concerns ship's radar.
I was told that in the early days of tape, three weeks' work in Europe was wiped when, as the White Cliffs of Dover came into sight, the bag containing them was put down in front of the rotating aerial.
The worst thing was that the tape was on metal spools and the resulting damage was not a complete wipe: there were still tantalising fragments of the recording left where the tape had been under the metal ‘spokes’ of the reel, I would be interested to know if this legend is true.
Broadcasting is full of folklore like this.
 Roger Derry IBS News London E11 
Ley off
Although I would be one of the first to congratulate anyone who can demonstrate the reality of ley lines, the recent article by Paul Devereux and Robert Forrest (’ Straight lines on an ancient landscape,’23 December.
p 822) contains ill-founded statistical arguments to whet the sceptical appetite of the most enthusiastic ley hunter.
The authors consider the Saintbury ley ‘in the context of a 10 km square’, and apply Behrend's statistical formula to the sites within this square in order to demonstrate its significance.
However, this ley was originally discovered by scouring the whole of the British Isles and a statistical evaluation on the basis of a small square already containing the alignment is therefore not convincing and indeed erroneous.
The correct approach is of course to examine many 10 km squares and compare the total number of leys of a particular order with the statistical prediction.
On a related topic, can anyone reconcile the two conflicting scales on the ordinate of the first graph in the article on the Dragon project (’ The talking stones’, by Don Robins, October 1982, p 166)?
If the intensity and pulse rate are really so perfectly correlated as the graph would suggest then surely this fact in itself is highly significant and worthy of comment.
Such poorly presented results do nothing to support the credibility (and hence the funding!) of an interesting field which deserves more critical attention.
Martin Whittle UMIST, Manchester In his article about Icy lines, Paul Devereux slipped in a comment that derives from his recent book Earth Lights (Turnstone).
There he suggests that the extant megalithic monuments are preferentially located in areas underlain by older rock (without acknowledging that such monuments over younger rock in the south and east Britain might have been destroyed by later development), and close to geological faults which he believes cause a luminous aerial phenomenon, sometimes called a UFO!
These ideas have no scientific basis and no sound evidence was offered by Devereux in his book.
Not only have UFOs nothing to do with ley lines, they have nothing to do with ancient monuments or geology.
 Stuart Campbell Edinburgh 
Opren delays
Your feature on our product ‘Opren’ contained a number of misleading statements (This Week, 16 December, p 707).
One statement to which we take a major issue is that Dista Products failed to inform the Committee on Safety of Medicines about the way the drug behaved in the body of certain very elderly patients and that thus information was  withheld from doctors for a period of 14 months from June 1981.
These allegations are quite untrue.
This information was verbally communicated to officials of the Department of Health in July 1981, who asked that further studies be done to be available by October 1981.
This was done and full reports were taken to the DHSS on 7 October with a revised Data Sheet proposal, which included recommendations about dosage in elderly patients.
Dista Products received a letter from the DHSS on 17 November, 1981, saying that it would be ‘inappropriate’ to publish our new Data Sheet at this time.
This was consistent with the DHSS requesting still further studies to be done to clarify the situation.
Notwithstanding the status of the Data Sheet, Dista representatives were distributing a booklet in September 1981 based on the presentations at the Paris symposium on ‘Opren’, which included the studies on elderly patients.
These actions do not support the allegations that Dista Products failed to take appropriate action on this issue.
 Dr B. A. Gennery Medical Director Dista Products Ltd Basingstoke, Hants There seems to be some disagreement between the Committee on Safety of Medicines ‘and Disto Products.
CSM insists that it did not delay unduly action to reduce Opren dosage in elderly sufferers from arthritis Dr Gemer lists a number of dates on which CSM might have acted earlier — and presumably reduced the toll of sufferers .
What went on privately between Dista and CSM is irrelevant Dr Gennery now produces information that we tried to obtain, with no success, at the time .
 Dr Frank Lesser 
Seals count
The article ‘Scientists disagree and the seal hunt goes on’(This Week, 2 December, p 547) distorts the recent report by the International Council for the Exploration of the Sea (ICES) on harp and hooded seals in the north-west Atlantic.
First, the report does not conclude as stated ‘there is no reliable way to assess the status of the seal population’ This statement is clearly at odds with the advised ranges of pup production and population size on page six of the report.
Secondly, there is no mention in the ICES report that either seal species is rare or endangered.
Thirdly, the article states that ‘estimates vary with the Canadian scientists providing the highest estimates, and an independent UK model the lowest’ This statement gives a misleading impression.
Estimates of abundance of harp seals were presented for two periods: Norwegian authors provided the  highest upper limit for the late 1960s, and the lower limit of the estimates from the analysis by a UK scientist was less than the catch of the 1967 year class of pups.
For that period, estimates by Canadian scientists were between the upper and lower bounds of the UK scientist's analysis.
Estimates for the period 1977 to 1980 were provided by Canadian and Norwegian scientists In this case the Norwegian scientist's estimate was in the mid-range of the others.
Your failing to note that two periods of years were involved is a serious omission, Another serious omission relates to the quotation attributed to Mr Parrish that ‘a decrease [in harp seal population]cannot be ruled out’.
Nowhere in your article is the ICES advice for harp seals quoted, namely a range of 1.2 to 1.6 million in the late 1960s and 1.5 to 2.0 million in the period 1977 to 1980.
A less selective article might have noted that the main thrust of the advice for harp seats is that pup production in 1977–1980 was likely to have been longer than the later 1960s pup production.
Scientific papers must obviously stand on their own merits.
It is unfortunate that your article, wittingly or unwittingly, did not present the ICES report in its true context.
 W G. Doubleday Director Resource Research Branch Department of Fisheries and Oceans Ottawa, Ontario, Canada 
Lost pulsar
‘Almost 15 years to the day after the first pulsar was found at Cambridge…
’(Monitor, 2 December, p 562).
What was it doing there?
 Frank Spence Eastbourne, East Sussex 
Due credit
We all know how  difficult it is for scientists to obtain their due recognition.
I would like to thank you for helping my case by removing the names of my coauthors — Jim Bull and Robert Paxton — from the article ‘Why some insects look pretty nasty’(6 January, p 26).
 Paul Harvey University of Sussex Brighton 
Physics Olympiad
May I appeal for more sixth-form entries for the 14th International Physics Olympiad?
The contests in chemistry and mathematics are relatively well known and sponsored.
The UK final takes place in schools on 11 March, and the closing date for entries (to this address) is 28 February.
I would be pleased to answer any inquiries.
 W. H. Jarvis Salewheel House Salesbury Hall Road Ribchester Preston Lancs 
Letters to the editor
We welcome letters from our readers.
Short communications stand the best chance of publication.
We reserve the right to edit the longer ones.
Write to: Letters to the Editor,New Scientist Commonwealth House, 1–19 New Oxford St London WC1A ING.
ARIADNE
WHETHER or not the atomic bomb tests at Monte Bello affected the servicemen concerned there by radiation so that they may have developed forms of cancer is not proved and possibly is not susceptible of proof.
What astonishes me is that the whole subject of injury from radiation seems to have been treated at the time with a  casualness that approaches imbecility.
I find it almost incredible that the men taking part in the tests were not monitored regularly throughout the rest of their lives, if only for scientific and not humanitarian reasons.
It is just as unbelievable that the Ministry of Defence apparently does not know what happened to them afterwards, what careers they followed.
It is hardly the way to show that there can be no connection between cases of leukaemia and participation in the tests.
HAVING been an  enthralled and amused member of the audience at the Molecule Club's  performances at the Mermaid Theatre in London, I was interested in a report in the current issue the UNESCO journal Impact .
Written by two professors, Charles Taylor and John Beetlestone, both at University College, Cardiff, it describes entries in a competition organised for schools in three counties in south Wales aimed at introducing drama as a method of understanding — even enjoying — science, which is what the Molecule Club does on its own, as far as I know.
Performances of the dramas, written by pupils at the schools, took place in the college's Sherman Theatre.
I wish I had been there.
Some of them were obviously rousing, with children dressed up as tadpoles and frogs in a production called Slime for example.
‘No one,’ said a judge who had watched Slime , ‘could ever again look at a pond and fail to remember that it was somebody's home,’
But the one I am sorry I missed was called New Clear Fisson .
The action takes place in a Magnox power station where a guide ‘struggling with the limited conceptual understanding of the local Mayor and Lady Tiara’, starts explaining by analogy.
Enter lively atoms of uranium-235, soon stirred up by Punk-neutrons and slowed down by Mods.
But things calm down only when Dr Boron appears, who is then outwitted and all hell seems about to break loose when Superboron makes his entrance.
Eventually the Mayor and Lady Tiara get the idea.
The words, music and effects for this piece of didactic theatre were all the work of secondary-school pupils.
I should think they and the audience had a whale of a time or, in the rather more sober words of the two authors, ‘the use of drama as a vehicle for science at the primary level has exciting possibilities that warrant further exploration.’
THE Rubik cube is not the Rubik cube the world over.
In France it is le cube Hongois, in Germany der Zauberwurfel, which I suppose means magic cube, at a quick connection with Mozart,(Culture sometimes raises its head in this column.)
I know this because I have  been glancing at a magazine called Rubiks printed in English and published in Budapest.
Among other features it contains the words of a song, possibly popular in Hungary, which contains a sentiment I have some sympathy with and shows that lateral thinking exists everywhere.
‘I think I've got the answer — all I need's a little paint.’
It also has letters from enthusiasts, some of which sound as if they have been translated into Hungarian and back again.
‘Allow me,’ says one from an English mother, ‘to express my most sincere esteem in recognition of your masterwork.’
She is mother of a family that possesses 13 cubes, which is overdoing it a bit, I reckon.
Another letter, from Holland, claims that the writer owns a round cube, but she does not find that interesting.
But the magazine is not entirely about the cube.
It has articles on such subjects as geometrical art.
It will offend the members of London's  Islington council if they get hold of it.
They have just passed a resolution banning Irish jokes, under the impression that jokes are the enemy of tolerance and affection.
The joke?
It is the Irish cube, green all round.
MAKING an obscure joke that failed to impress anyone the other week (it was about warts and the treatment of them) I did, at least encourage one reader in Israel to send me a palindrome about them.
It is one I have never seen before and I wonder how many hours of patient madness produces such things.
Here it is.
‘Straw?
No, too stupid a fad.
I put soot on warts.’
I have doubts about the treatment, of course.
Daedalus
BRICK and concrete are inconvenient building materials.
They are heavy, poor thermal insulators, and need much labour to manipulate and time to set.
seeking improved materials, Daedalus has been inspired by pumice-stone, which is solidified from molten volcanic lava foamed up by volcanic gases.
His new ‘pyrotechnic constructional material’ or Pycoma (Regd,) is a powdered pumice, slag or other low-melting glassy material, moulded with a slow-burning pyrotechnic composition of high gas-output.
To build a wall of Pycoma you simply erect a hollow mould of shuttering, lay a few blocks of Pycoma inside, and light them.
The combustion melts the pumice, the hot gases foam it up, and the hot foam fills the mould in seconds.
It then cools and sets, giving a light, strong, incombustible, thermally insulating ‘foamed-stone’ wall.
Wooden shuttering would char in the process, but could still be used several times.
An incombustible (eg plasterboard) shuttering could simply be left in place as a decorative skin melt-glued to the foam core of the wall.
Either way, a whole house-shell could be put up in a few hours by a few successive foam-formings of the main walls and pillars.
Moreover the shuttering may not be needed.
If a block of Pycoma were lit simultaneously all over the top surface, say by a fast-running sheet-fuse, the combustion zone would travel slowly down through the block.
The solid foamed skin first formed on the top would then be steadily lifted by the foaming of the combustion-zone beneath and a vertical foamed ‘pillar’ rise from the block.
A whole ground-plan of such blocks would rise up, stabilised by its interlocking, into a complete web of walls.
A ring of blocks lit on the slant would similarly foam upwards and inwards, finally meeting in the middle to complete a rigid conical roof.
Holes could then be cut in the structure to receive skylights, doors, and windows.
The ultimate ‘instant house’(just light the blue touch-paper and stand back) should soon be a reality.