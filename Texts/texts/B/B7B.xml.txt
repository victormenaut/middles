

COMMENT
A year best forgotten?
NINETEEN EIGHTY-TWO just might go down as a memorable year in the history of Britain, if not the rest of the world.
After all, the country does not go to war all that often, even if it is with a second-rate corned-beef republic.
That ‘event’ had little to do with science although it did provide us with one or two technological tales, albeit mostly grim ones.
And scientists interested in Antarctic research have reason to thank the  Argentinians — the need to maintain a presence in those cold southern waters, and one that is not too openly military, has coaxed the government into setting aside more money for the British Antarctic Survey.
No one in Britain's universities will forget 1982 in a hurry.
For although the chairman of the University Grants Committee, Edward Parkes, may have received his well-earned knighthood at the end of the year — in a batch meanly thin, as usual, on honours for the scientific community — many of the nation's academics will never see their profession in the same rosy light.
Tenure was challenged — and probably quite rightly — but so was the universities' freedom to determine their own fate.
The nasty subject of ‘need’ invaded the ivory towers.
Do we really need lots of people sitting around pondering on research topics that are of little benefit to man or beast?
And should the universities churn out unemployable graduates?
One topic that reappeared during the year was The Bomb.
A complete set of the books published in 1982 on various aspects of nuclear weapons would probably provide adequate building material and radiation shielding for a sizeable fallout shelter.
With so many different groups coming out with position statements on The Bomb, the scientific community was remarkably restrained.
Whereas in the past there have been various pressure groups setting themselves up as having a special reason for being heard thanks to their scientific credentials, perhaps the wisest comment on this came from the professor who, while urging scientists to make their voices heard, also urged them not to make any special claims to knowledge or understanding of the issues just because they were scientists.
Another consequence of the boom in opposition to nuclear weapons was the eclipse of the campaign against nuclear power, which was a bit odd given that the whole of 1982 was a run up to the inquiry, which begins later this month, into the plans to build Britain's first pressurised-water reactor.
Another of the year's ‘hot issues’ was biotechnology and its sub-category genetic engineering.
Who would have thought, with all this talk of millions to be made by gene juggling, that just a few years ago genetic engineering was considered to be so dangerous that one small accident could unleash upon the world a virulent plague that would make the black death look like an outbreak of summer flu?
In the event the anticipated collapse of the first genetic engineering company amid a pile of bad debts did not come about.
But how much longer can these companies survive with nothing to sell to bring in revenue but plenty to spend money on?
Earning money was what Information Technology Year was all about.
If 1T82 had anything to do with the outbreak of mass hysteria among would-be buyers, sellers, makers, and so on of microcomputers, then it must be judged a success.
But if that hysteria does no more than feed the seemingly insatiable appetite for publications on microcomputers then it will have been a waste to have trundled the IT bandwagon around Britain.
(It will also have been a waste if the outcome is only to soften up British buyers for  Japanese companies to rush in and sell us microcomputers as they have previously sold us domestic electronic equipment.)
Perhaps the most important question in the wake of IT Year must be: what will Kenneth Baker do now that he is at a loose end?
Nearer to home,New Scientist's attempts to grapple with information technology have so far proved fruitless.
There are many who are surprised to discover that the words you see before you have been brought to you with little electronic influence beyond that which goes on within the brains of the writer and reader.
(Even our typewriters are mechanical horrors that should have been declared obsolete decades ago.)
Well, all that is changing.
Within weeks you will see the changes resulting from the computer's invasion of our printer.
And, with luck, not long after that we will have in our very own editorial offices an electronic excuse for the mistakes that occasionally slide into these pages.
But more of that when it happens.
As they say elsewhere…watch this space.
THE MOST NEGLECTED WILDLIFE HABITAT OF ALL
Jenny Owen
You may live within a few metres of many if not most of the plant and animal species ever recorded in Britain, and perhaps a few that are new to science.
Seek and ye shall find
Leicestershire, in the very middle of England, away from coasts and mountains, beyond the northernmost limits of continental influences and the fauna, denuded of ancient woodland and intensively farmed, is generally thought impoverished and rather uninteresting for a biologist.
Yet I have discovered, within the Leicester city limits, a small but fertile patch of land that supports an astonishing abundance and diversity of plants and animals.
Tall trees and dense shrubs accommodate nesting birds, abundant flowers provide food for bees, butterflies and hoverflies, heavy crops of fruit and seed support birds and insects, good ground cover shelters predatory beetles, centipedes and spiders, and accumulations of dead and rotting vegetation serve as feeding and breeding sites for slugs, woodlice and insects of many sorts.
For 11 years I have been investigating the flora and fauna of this 700 sq.m site.
I have not had the time or sometimes the knowledge to identify all the organisms I found, but the species list to date (Tables I and 2) totals 1731.
The list includes more than a quarter of the noctuid moths and ichneumonid wasps on the British list, a third or more of the hoverflies (Syrphidae), butterflies and bumblebees, more than half the Serphidae (also wasps), and six cf the seven social wasps (Vespidae).
Many of the records of insects represent an extension of the known range of the particular species: three species of bees, five of aculeate wasps and 20 of hoverflies are among many new records for Leicestershire; the cuckoo wasp,Vespula austriaca , is new to central England; there is the most southerly record of a calliphorid fly, which is common in Scotland; at least eight species of Ichneumonidae and five of Serphidae are new to Britain; and two ichneumonids are probably new to science.
On the basis of its fauna, the area in question might qualify for protection as a nature reserve, if not for designation as a site of special scientific interest.
Yet the site is not unusual, and is representative of a habitat which is in no way threatened by building or transport projects, or by farming.
Indeed, this sort of habitat covers an estimated 400 000 hectares in England and Wales, and is probably increasing.
The habitat I refer to, as you may have guessed, is the garden.
The encroachment of urban sprawl on the countryside may be cause for regret, but we are a nation of gardeners, and the English passion for gardening goes some way to ameliorate the impact of an increasing population on a small crowded island.
We lavish time, effort and astonishing sums of money — More than £200 million a year — on cultivating more than 15 million private plots of land, and one of the fringe benefits is the rich fauna that develops.
I am so familiar with the plot I have described because it is my own garden.
I see it all the time from the window, inspect it every day, work in it all year round, and relax there in summer; it is an essential part of my immediate environment.
That the fauna seems unusually rich, is a product of detailed investigation, rather than an attribute peculiar to this one patch of land.
It is an ordinary suburban garden, surrounding my house, and on a corner on a busy road, only 3–8 km from Leicester city centre.
It has the usual complement of flowerbeds, vegetable patches, close-trimmed lawn, shrubs, trees and well-swept paths, and differs little in appearance from neighbouring gardens.
My gardening activities are, perhaps, slightly unorthodox, in that I make a point of growing plants whose flowers are known to be attractive to insects, plant flowers and vegetables together, tidy and prune only as much as is absolutely necessary, encourage plants, even ‘weeds’, that give continuous ground cover, and rigorously exclude poisonous chemicals.
The garden thus supports a dense stand of vegetation; but it is not a wilderness, neither is it over-run with pests, and it provides a supply of flowers and produce as well as space for relaxation.
I gained much of the information about flying insects in my garden by operating a Malaise trap from 1 April to 31 October on the same site (Figure 1).
A Malaise trap is like an open-sided tent, made of fine netting, with a pitched roof that rises obliquely to a peak at one end.
Insects that fly in at the sides encounter a vertical baffle of netting that divides the trap along its axis, and tend to fly or clamber to the highest point of the baffle where the only way out is into a collecting jar.
The trap uses no bait or other attractant, capturing only those insects that fly spontaneously into its 2.9 sq.m of air space.
As proof of its usefulness, the trap has caught many species which have never been observed in the garden.
Not all insects that enter the trap are captured, but the operation is consistent, so that the method is valuable for monitoring changes in relative abundance from season to season and year to year.
Pitfall traps are placed in various sorts of sites around the garden to catch ground beetles, spiders, harvestmen, woodlice, centipedes and millipedes.
I use a mercury vapour light trap regularly to trap moths, and hanging traps baited with rotten fruit or fish offal attract specialist feeders, such as blowflies.
All these trapping methods are to some extent selective, but they are augmented by observation and hand-netting.
In some years, I have counted butterflies by a mark-and-release programme of hand-netting and found spiders by persistent searching.
Regular hunts for larvae, which I then rear in captivity, give additional records of species and feeding.
Few sites can have been as intensively studied as this one, and indeed few sites are as amenable to exhaustive study as is a domestic garden.
Flower visitors — moths, butterflies, hoverflies, bees and wasps — are perhaps the most conspicuous feature of the garden but there is also an abundance and variety of plant-feeders, such as moth larvae, and associated predators, parasites and scavengers.
My knowledge of the fauna is patchy because most of the trapping methods capture only flying insects, and because I have concentrated on groups that I, or someone known to me, can identify.
But by extrapolation, I can reasonably expect to find a third of the British insect fauna.
The proportion of non-flying invertebrates may not be as high, and they are harder to sample systematically, but I am optimistic about the numbers that remain undiscovered.
In most groups studied in detail, a few species are extremely abundant and many are scarce.
For instance, between 1972 and 1982 I caught 35392 hoverflies of 89 species in the Malaise trap, but seven species accounted for more than two-thirds of the catch and 35 species were each represented by fewer than 10 individuals.
In part this is a consequence of differences in the  habitual requirements of different species; more than 82 per cent of the individuals belong to species whose larvae prey on aphids, whereas only 0–02 per cent belong to species that feed as larvae on tree sap or rotting wood, neither of which are a feature of gardens.
The pattern of relative abundance of Ichneumonidae is quite different: nearly a third of the 455 species trapped during 1972–73 were represented by single individuals, and the seven commonest species made up only 22 per cent of the catch.
The larvae of ichneumid wasps are parasitic on the developmental stages of insects or spiders, which egg laying females locate by searching narrowly circumscribed micro-habitats; they are as much niche-specific as host-specific, and it is because there is such a range of niches in a garden that there are so many kinds of adult wasps.
Eleven years of trapping has demonstrated that it is inadvisable to generalise from two or three years' experience.
Every year particular species arrive and disappear, and change in absolute and relative abundance.
Hoverflies illustrate the point admirably (see graph).
The first year's trapping suggested that Platycheirus albimanus was the commonest garden hoverfly, forming almost a fifth of the total catch; a fifth of the species trapped in 1972 were represented by only one individual, and the average number of individuals per species was 29.
Subsequent years showed that this was not typical.
In only 5 of 11 years has P. albimonus been the commonest species, four others having occupied that position; in 1977 Episyrphus balteatus formed nearly half the total catch; in 1976, a third of the species caught were represented by one individual; and the number of individuals per species has varied from 20 in 1981 to over 120 in 1978.
Similarly a small black hoverfly,Cheilosia bergenstammi , whose larvae feed on ragwort, was captured in 1972, 1973 and 1974 (N= 19) and has not been seen since, whereas C. vernalis , whose food plant is unknown, appeared for the first time in 1978 and has since become frequent (N=118).
By rearing larvae, I found that two of the most important food chains in the garden are plant-moth-ichneumonid, and plant-aphid-hoverfly-ichneumonid.
These chains are interwoven with many other types of food chain to make complex food webs, incorporating, for instance, sawfly larvae, ladybirds, wasps, spiders and birds.
Contrary to common belief, many of these food chains are based upon alien (non-native) plants, indeed moth caterpillars in the garden eat a higher proportion of alien than of native species.
Many of those that use alien plants have more than one food plant (that is, are polyphagous); the cabbage moth has been recorded on 31 different species of plants including 21 aliens.
Only one specialist feeder, the mullein moth, has switched to an alien plant, buddleia, whereas several specialist feeders use native plants, such as poplar and willow.
Following food chains can lead to exciting discoveries; I reared one ichneumonid,Hyposter tricolo , from a larvae of a magpie moth feeding on flowering currant, which is native to North America.
I had previously caught two individuals of H. tricolor in the Malaise trap but until then it had not been recorded in Britain although it was known from Germany and Japan.
Its presence in the garden, and indeed in this country, appears to be associated with cultivation of an alien shrub.
Gardens are unique among man-made habitats in supporting a rich fauna, which includes species of woodland and open country as well as those characteristic of gardens.
The explanation for this seems to lie in the nature of gardening.
First, by continual introduction of trees, shrubs, flowers and vegetables a gardener achieves what may be called contrived plant diversity, far higher than the area would support naturally, and rivalling even that of tropical rainforest.
Secondly, regular working of the soil makes it continually receptive to colonisation by native or naturalised species as ‘weeds’.
This dual process of introduction is systematically countered by a dual process of elimination.
Perennial plants are uprooted, or annuals not replaced, as the gardener's tastes change, and every effort is made to prevent weeds becoming established.
The garden plant community is never allowed to settle down, but is kept in a state of permanent succession.
All natural plant communities, left to themselves, progress through a series of stages (successions), to a ‘climax’: thus dune may tend to become heath, which tends to become woodland, and so on.
But gardening may be described as the activity of interfering with natural plant succession and preventing the establishment of a climax community.
The composition of the garden flora changes from year to year, but it retains its characteristic of high, though contrived, plant diversity, which coupled with structural diversity, is often cited as a cause of complexity.
Gardening creates extreme structural diversity within a small space, with such features as lawns, flowerbeds, paths and shrubs, making a complex three-dimensional mosaic of open space and shade.
The high number of ichneumonid species, none of them common, may result from the patchiness of the garden habitat, as each exploits a different niche.
There are edges or discontinuities everywhere, and gardens are like vastly extended woodland edge.
Zones of overlap, or ecotones, between two habitats are often richer in species than either habitat (this is known as the ‘edge effect’) and the richness of gardens may reflect their ecotonal character.
Oddly enough, their instability may also be important.
Gardens are a bountiful habitat, but one that is continually receptive to invasion, offering many openings for mobile species to exploit.
In the drought year of 1976, for example, when the countryside became parched and brown, my garden had many unusual visitors, including single individuals of each of three species of clearwing moths and of four species of butterfly normally associated with more rural areas (whiteletter hear-streak, silver-washed fritillary, marbled white, and hedge brown).
Gardens, of course are artificial, and are rigorously managed.
But the landscape of 20th century England is dominated by gardens, and with arable fields, plantations, parks and residential areas the whole country resembles a gigantic garden, manipulated , planted, and managed.
How much might we learn by concentrating ecological effort, expertise and funding on our own backyards?
Hypnosis on trial
Gair Vines
Witnesses or victims of serious crime are sometimes hypnotised during police investigations to help them to remember, and they may later testify in court.
Yet hypnosis can produce seriously distorted memories which the witness nevertheless believes are true
In the summer of 1976 in Chow-Michiel Barnes chilla, California, a schoolbus driver and 26 children were abducted at gunpoint by masked men.
For no apparent reason, the children and driver were herded into vans and taken to a remote rock quarry where they were sealed into an underground cavern.
Eventually the driver and two older children dug their way out.
The distraught and guilt-ridden driver was unable to remember the number plates of the vans which he had tried to memorise at the time of the kidnapping.
The FBI took over the case and decided to hypnotise the driver to enhance his memory.
He was told to imagine that he had travelled back in time to the afternoon of the abduction and was watching the events unfold on a television documentary.
Suddenly, he called out two licence plate numbers; one of these, except for one digit, turned out to match the number plate of one of the kidnappers' vans.
This information helped the police catch the three culprits after one of the biggest manhunts in the history of California.
In Pennsylvania in 1975, two seamen were working in an office when another sailor appeared in the office and aimed a shot at one seaman's head and fled.
The seaman moved quickly and was only grazed on the ear.
When the seamen were shown mugshots, the victim was unable to identify anyone.
But he was present at a hearing in which the other witness identified one of the pictures as the assailant.
The victim was then hypnotised twice and during the second session he claimed to recognise the accused as the attacker.
The defendant's alibi was later collaborated by two seamen who returned from abroad; it was extremely unlikely that he was the actual assailant.
But the effect of hypnosis on the victim's memory persisted, and a year later he was still convinced that the defendant was the actual attacker.
These are just two out of many hundreds of cases in which the police have used hypnosis to refresh the memories of witnesses and victims of crime.
In the US, more than 1000 ordinary policemen have learned how to hypnotise witnesses.
In Britain, there has been a steady but unpublicised increase in its use in police investigation over the past 15 years.
We need to ask whether the effects of hypnosis upon human thought processes, on reasoning and memory, are really understood; whether we can assume that a person under hypnosis truly remembers actual events.
Hard evidence is thin on the ground, and what there is, is not encouraging.
Indeed, our understanding of the technique that is now used so widely and with such serious consequences seems to have made disquietingly little progress since the 1770s, when Franz Anton Mesmer first took Paris by storm with his new, bizarre technique.
Mesmer claimed that the convulsions he induced in his patients were due to an ‘animal magnetism’ he exuded.
To tap this power and evoke  ostensibly healing ‘crises’(a kind of convulsive fit), Mesmer's subjects held onto iron bars extending from a bath of water and iron filings.
Although Mesmer's crises have little in common with modern hypnosis and his explanation for the phenomenon was soon discredited, he did obtain cures that physicians had been unable to achieve; so scientists and therapists remained mesmerised by the phenomenon.
Since Mesmer brought the subject to the attention of the scientific world, hypnotism has had a rather chequered career.
Interest in the use of hypnosis as an anaesthetic developed in the early 1800s but declined after the discovery of ether.
Various groups of psychotherapists and psychologists took it up, but hypnotherapy was soon overshadowed by the development of Sigmund Freud's psychoanalytic movement, which relied on free association instead.
During the past few decades, many academic psychologists have studied the phenomenon but it is still not possible to say exactly what hypnosis is.
Some researchers, sometimes crudely referred to as ‘state theorists’, emphasise the subjective experiences of the hypnotised subject, while others, the ‘social theorists’, put great weight on the special social interactions involved in the induction of hypnosis.
According to the psychologist Ernest Hilgard of Stanford University, hypnosis is an altered state of consciousness.
The hypnotic induction procedure sets up the circumstances that encourage the subject to enter this ‘trance’ state.
The swinging pendulum has gone out of fashion.
Hypnotists now often suggest that the subject focuses his attention by staring fixedly at a drawing pin on the wall, while telling him in rather monotonous tones that he is becoming more relaxed and sleepy.
The suggestion of sleep is only a metaphor; the subject does not actually drop off but keeps his attention focused on the hypnotist's words instead.
Sleep suggestions are made to encourage the subject to sever the critical awareness that normally links him to the external environment; ‘reality testing’ has to be set aside.
The individual suspends his critical judgement and involvement in external reality to becoming passively absorbed in an imaginary world.
The hypnotic induction sets up a peculiar state of ‘dissociated consciousness’
Hilgard believes.
The hypnotist's instruction to keep looking at the drawing pin conflicts with his suggestion that the eyes are growing tired and will soon be closing of themselves.
A way out of this double-bind is to conclude that ‘I’ am holding my eyes open but that ‘they’have closed involuntarily.
The action of eye-closing, normally regarded as voluntary, has thus become split off or ‘dissociated’ from voluntary control.
The scene is set for further dissociative experiences.
The hypnotist then proceeds to more difficult illusions; the successful completion of each stage is used to judge the ‘depth’ of trance.
The hypnotist may suggest, for instance, that the subject will feel his arm becoming lighter and lighter and rising of its own accord.
More difficult illusions require the subject to hallucinate objects or people into or out of existence with his eyes open.
Hilgard thinks that these procedures cause the normal ‘monitor’ of reality to retreat into the background and allow another part of consciousness, temporarily ‘split off’from the reality monitor, to become active independently.
It is rather like the driver who is deep in thought about a domestic crisis; his attention is highly concentrated on personal matters, yet another part of his consciousness manages to navigate the car safely through the traffic.
Hypnosis does not delve into the subconscious, but divides consciousness in two parts; the more critical, reality orientated bit becomes less active under hypnosis, Hilgard believes.
The late Ronald Shor, a psychologist at the University of New Hampshire, emphasised that the hypnotised subject is not a will-less automaton.
‘The hypnotist does not crawl inside a subject's body and take control of his brain.’
After only a few lessons, most people can even hypnotise themselves.
A subject may take up the hypnotic suggestion that he is unable to bend his arm: ‘He is actively, deliberately, voluntarily keeping his elbow stiff while simultaneously orchestrating for himself the illusion that he is really trying his best to bend it.’
Hypnosis reduces reality-testing, so that the person becomes more open to suggested distortions of reality, in both perception and memory, and less critical of his own imaginative mental output.
He becomes ‘ suggestible ’.
Not everyone is equally susceptible to hypnosis.
Some individuals can take up the suggestions of the hypnotist to the extent of becoming deaf or blind or unable to smell; they may withstand pain without a murmur, re-experience being a six-year old, or even forget everything that happened, after hypnosis, until given a prearranged ‘release’ signal.
Despite all this talk of changes in consciousness, no one has managed to find any physiological changes that uniquely distinguish hypnosis from the waking state.
The electrical activity of people's brains recorded by a machine known as the  electroencephalogram (EEG), shows that the initial stages of hypnosis resemble the relaxed waking state.
Physiological changes observed during hypnosis depend upon the specific suggestions given to the subject.
For instance, when subjects are asked to relive an emotional experience, their pulse rate and brain recordings indicate that they are in an aroused state.
People asked to imagine they are riding a bicycle have brain waves similar to those generated by a person who is actually riding a bicycle.
Some studies suggest that there are measurable physiological differences between people who are easily hypnotised and those not so susceptible to hypnosis.
Everyone's brain waves show periods of regular waves, known as alpha waves, when they are resting quietly and relaxed.
When people adept at hypnosis sit quietly with their eyes closed, they may spend more time sending out alpha waves than people who are ‘bad’ hypnotic subjects.
Right-handed males who are very susceptible to hypnosis also seem to show a preference for using the right hemisphere of the brain — the side associated with imaginative rather than analytical skills.
But these factors are correlated only with hypnotic responsiveness, not with any change that takes place within hypnosis itself.
The other school of thought on hypnosis emphasises the special social situation that lies at the core of hypnosis.
Theodore Sarbin of the University of California, for instance, sees hypnosis as a special kind of role-taking behaviour.
The subject is under strong social pressure to go along with the hypnotist; he has agreed in good faith to be hypnotised, after all, and is determined to carry out the hypnotist's suggestions.
Furthermore, most people actually believe that they will feel and behave differently under hypnosis.
This powerful combination of compliance and belief is enough to account for all the hypnotic phenomenon, he and his colleagues argue.
This does not mean that people are always consciously faking when they are hypnotised.
They may feel relaxed, drowsy and so on, and attribute these feelings to a special ‘state’.
The person who acts like a chicken or conducts an orchestra for the stage hypnotist may be play acting, or he may genuinely feel that it is the hypnotist, not himself, who is taking responsibility for his actions.
Theodore Barber of Medfield State Hospital in Massachusetts has carried out a series of experiments which suggest that there is nothing a hypnotised subject will do that a motivated non-hypnotised person will not do as well.
Dr Martin Orne of the University of Pennsylvania told subjects to fake hypnosis and they easily fooled the hypnotist.
There is no single reliable way of deciding by simple observations whether a subject is genuinely hypnotised.
So the debate over the nature of hypnosis continued.
But everyone agrees that it can be a useful therapeutic tool, easing the pain of dental treatment or curing diseases involving powerful but little-understood psychosomatic interactions, such as warts.
Hypnotherapy can also help relieve profound feelings of disquiet or anxiety.
But interest now focuses on its potential as an aide memoire in police investigations.
Hypnotists working for the police ask an individual, most commonly a witness or a victim, to imagine that he has gone back to the time of the crime.
By using this ‘age-regression’, or a direct suggestion of heightened memory, ‘hypermnesia’, the police hope to encourage the subject to retrieve ‘forgotten’ memories.
Hypnosis enhances memory and encourages truth-telling, so advocates of forensic hypnosis believe.
But prominent psychologists and psychiatrists are alarmed.
The very process of hypnosis, they claim, can create convincing pseudomemories which no one, not even the person hypnotised, can distinguish from real ones.
Indeed the psychiatrist who first popularised the notion that hypnosis has a special relationship with truth, no less a figure than Sigmund Freud, later issued a retraction.
Early in his career, he used the ‘age-regression’ technique to uncover traumatic childhood events which he felt lay at the root of his patient's distress, but he came to realise that it was highly unlikely that all Viennese children were sexually molested by their parents even though nearly all his patients, when under hypnosis vividly described such encounters.
An ‘age-regressed’ subject is not really reliving actual events; by checking school records and the like, researchers have found that remembered fact is well-laced with fantasy.
In other studies the subjects have been led unwittingly to the experimenter or by their own knowledge of the expected results.
Six-rear-olds and pseudo six-rear-olds
Martin Orne, a psychiatrist and research  psychologist now at the University of Pennsylvania, asked several hypnotised people to re-experience being six years old.
He asked them to make drawings, and compared these with the artwork kept by fond parents, which the subjects had produced when they really were six.
Although one age regressed subject produced superficially childish drawings, they were nothing like what he actually produced when he was a child.
Another hypnotised subject wrote in childish hand, but correctly spelt words like ‘experiment’.
Such inconsistencies make it clear that the age-regressed person is not actually functioning at the suggested age level, nor is his recollection truly accurate.
This is not faking, however, because waking subjects know perfectly well a child cannot write or spell such a sentence.
It illustrates a lessening of critical judgement which is characteristic of hypnosis.
But the belief that hypnosis at least enhances the recall of memories still lives on, encouraged by rare cases of amnesia which are caused by emotional trauma, rather than brain damage.
A pilot's memory for the events leading up to a crash, say, may be totally blocked.
According to Martin Orne, hypnosis may at times help to remove this amnesia, but there have been no good studies of the effect of hypnosis on trauma.
In any case, genuine traumatic amnesia is extremely rare.
The important question for the policeman interested in hypnosis is, does hypnosis enhance the recall of events which are not blocked by extreme trauma?
Witnesses who are hypnotised are usually just bystanders.
The consensus view, emerging from experimental psychologists in both America and Britain, is neatly summed up by Professor John Brown of Bristol University's Psychology Department: traumatic amnesia aside, he said, ‘there is no reliable evidence that you can remember under hypnosis what you can't remember normally.’
Hypnosis does not increase the recall of meaningless information like nonsense syllables, which it should do were it tapping a hypothetical memory-taperecorder lurking in the subconscious.
Some studies suggest that hypnosis does not help people remember faces, car number plates or the like.
For instance, in an experiment conducted by Dr Graham Wagstaff of Liverpool University's Psychology Department, he showed subjects a realistic video containing violent crime.
The hypnotised subjects were actually slightly worse at recalling the details on video.
In another test, Wagstaff showed subjects a series of ‘mug shots’; a week later one group of subjects were hypnotised and ‘age-regressed’ to the previous session.
They were no better at picking out the faces they had seen before than the untreated subjects.
Wagstaff argues that the few studies which do appear to show modest improvement in recall under hypnosis failed to have control groups which were equally encouraged to remember.
But age-regression does look very convincing to the observer.
In a classic experiment in the 1930s, J. Stalnaker and E. Riddle asked subjects to recite a Longfellow poem,The Village Smith , that they had learned years before in school.
At first glance, they seemed to do much better under hypnosis but the improved performance was mostly skin-deep.
Under hypnosis they were not afraid to ‘ad lib’; they happily regarded fragmentary or inaccurate memories as acceptable, and then imaginatively filled in the bits, sometimes even whole verses, which they could not remember.
But they did not realise that they were making errors; the subjects were convinced that hypnosis had actually helped them to remember the poem accurately.
Many other studies have also shown this same effect; hypnosis makes you more confident — and more inaccurate.
Hypnosis may work better outside the laboratory; we do not know because no one using hypnosis for the police has done properly controlled tests, although one American police hypnotist insists that hypnosis is helpful 90.1 per cent of the time.
Such claims are based solely on the investigating policeman's statement that it was of ‘some use’, and has never been independently reviewed.
Three incidental features of forensic  hypnosis may help jog memories, but these potential memory aids are not unique to hypnosis.
First of all, laboratory experiments on hypnosis and memory are not conducted on stressed  individuals , but most people interviewed by the police are upset.
Wagstaff argues that the hypnotist, like a counsellor, helps motivate people to try to remember by reassuring them that they will not be made to feel upset, guilty, or embarrassed if they describe what they have seen.
Hypnosis may also occasionally help someone remember by encouraging them to imagine how they felt or what they were doing at the time of the event.
Dr Alan Baddeley of the Medical Research Council's Applied Psychology Unit at Cambridge has found, by studying some cooperative divers, that if you learned something underwater, you will remember it better underwater.
But other techniques can also promote this ‘context’ or ‘state-dependent’memory.
For instance, two researchers from the State University of New York at Plattsburgh, Roy Malpass and P. G. Devine, have recently described non-hypnotic ‘guided memory’ procedures which helped people to identify faces in experiments.
They asked people a series of incidental questions, like ‘What did you feel like in the morning’, which sparked off associations and aided memory retrieval — without hypnosis.
Finally, hypnosis is usually carried out some time after the police have done their normal questioning, but memory of an event can often improve spontaneously after an interval of time has passed; this phenomenon is known as the ‘reminiscence effect.’
So there seems to be nothing special or unique about the way hypnosis aids the memory of witnesses, if it actually does.
But hypnosis does have a host of special features which can make it profoundly misleading in a forensic context.
For hypnosis can easily alter memories.
A person during hypnosis is so involved in fantasy and imagination that an apparently innocent question can actually create new memories.
Even in general conversation, the answers you get depend on how you ask the question.
And eyewitnesses are also influenced by the way the interrogator asks the questions, as Elizabeth Loftus at the University of Washington has shown.
Minor changes in sentence structure can affect the accuracy of recall.
Asking, ‘Did you see the…’ rather than‘a ’‘…broken headlight’increases the number of people who say ‘yes’even if there was no broken headlight.
But hypnosis makes subjects even more liable to make such errors, William Putnam of the University of California has found.
Putnam showed subjects a video tape of a car/bicycle accident and then asked a series of leading questions that subtly suggested a wrong answer.
Subjects who were asked the questions when under hypnosis made significantly more errors in their subsequent descriptions of events.
Eager to please, hypnotised subjects are particularly prone to fill in the gaps in their memory with information conveyed to them in questions.
Even the most experienced police interrogator cannot entirely avoid leading questions, simply because he cannot know what the witness actually knows or what really happened.
Many American court cases have established that even information gleaned by the subject outside of hypnosis, through casual comments or questions, news reports and the like, can accidentally be incorporated into memories during hypnosis.
People commonly experience ‘source amnesia’ after hypnosis; they forget where they learned something and feel they remembered it themselves.
In a Minnesota case in 1979, for instance, a young intern's casual comment that a laceration appeared to have been caused by a knife later led the injured woman to change her story completely and come to believe, under hypnosis, that she was stabbed repeatedly, despite subsequent medical evidence.
Techniques that suppress embarrassing expressions of emotion from hypnotised subjects are popular with the police hypnotists like Dr Martin Reiser, the founder of the Law Enforcement Hypnosis Institute in Los Angeles which trains policemen to perform hypnosis.
Forensic hypnosis is big business in the US.
Reiser recently acknowledged in a New York court that his private corporation, in which he and his wife are sole shareholders, has grossed roughly $400000 in the past few years.
Reiser asks subjects to visualise themselves as spectators watching the events on a television screen; the subject is told that he can stop and replay the video tape at will.
This so-called ‘objective’ reliving, which brings forth fragmentary recall based on the hypnotist's detailed questions, puts the subject under especially intense pressure to come up with the goods.
But the more the subject is forced to produce details, Loftus has shown, the more inaccurate the details become.
In Joliet, Illinois, a witness under hypnosis was asked to ‘stop the video and zoom in’ on the face of the criminal and he managed to give a detailed description of a man, who was duly arrested and charged.
Only much later, in court, did it transpire that the witness could not possibly have seen the suspect's face at the distance 75 metres in conditions of semi-darkness.
Nevertheless, the accused spent five months in prison awaiting trial.
The hypnotised witness's consistency become no guide to reliability.
Nor is the recall of detail any guide to veracity; under hypnosis people can convincingly describe the world in the year 2000.
Thus, hypnosis creates a very able but unreliable witness.
In one American court case, an adolescent girl was present when several of her relatives were murdered.
The police initially strongly suspected that she had been involved; her story kept changing and was filled with inconsistencies.
Once she was hypnotised, she told a vivid tale implicating several people in the murders.
The police switched to regarding her as completely innocent and she became the star witness.
Her testimony was unshakable.
Battles over the use of hypnosis by the prosecution have been raging in American courtrooms since 1969, when a federal court ruled that the defence must be told that a prosecution witness has been hypnotised in order to ‘enhance’ his memory.
The dangers of using post-hypnotic testimony are now well-illustrated in American case law.
Seven states have completely banned hypnotised witness from testifying and one allows such testimony only if presumed safeguards such as video-taping were used during hypnosis.
Martin Orne takes the view that hypnosis may be safely used simply to gain clues for further investigation, in cases where the police are completely in the dark.
But he is careful to stress that none of the ‘evidence’ produced during hypnosis can be taken at face value.
‘Hypnotically-produced evidence is of no use at all unless strong independent corroborating evidence comes to light.’
Yet there is still a strong tendency to ‘treat statements made under hypnosis with undue respect; the folk-belief that hypnosis taps memories indelibly recorded in the unconscious dies hard.
It is also difficult to establish what constitutes ‘independent corroborating evidence’.
In a recent rape case in England, the victim described, under hypnosis, a scarf that the attacker had worn wrapped around his face.
The man arrested had such a scarf.
Is that independent evidence?
The problem is particularly difficult in cases of rape and assault where the identification of the suspect by the victim carries great weight.
Orne is certainly not enthusiastic about witnesses hypnotised during the course of police investigations later testifying in court, but he feels that such testimony may be acceptable if the entire proceedings are videotaped, so that verbal and non-verbal cueing can be picked up, the independent hypnotist is ignorant of the details of the case, and the subject's contacts with all possible sources of information about the case are known.
Bernard Diamond, a distinguished lawyer-psychiatrist at the University of California, seriously questions the feasibility of these ‘safeguards’ in practice.
‘The use of hypnosis on a potential witness is tantamount to the destruction of fabrication of evidence,’ he says.
Hard scientific evidence must either help the police to use hypnosis safely or lead them to reject the technique altogether.
The history of its use in America is illuminating.
The state of Maryland first allowed hypnotically-aided recall into court in 1968; the court argued that the jury should decide whether hypnosis had distorted memory or not.
Twelve years later, in Minnesota, the supreme court ruled as a matter of law that anyone who had been hypnotised could not testify in court.
This view has since been supported by six other American states.
In 1982, the Maryland court which had originally endorsed the use of hypnosis to enhance recall, and which had the most experience in the issues surrounding forensic hypnosis, ruled that no witness who has been hypnotised will ever be allowed to testify in Maryland court unless or until the scientific community can document its reliability.
Quantum mechanics passes the test
Basil Hiley
Latest results from an experiment designed to test a fundamental aspect of quantum theory show it to come through with flying colours.
But the results cast shadows on our notions of reality
The strange behaviour of systems that exhibit quantum effects seems often to challenge common sense, raising questions as to whether quantum theory provides only an incomplete account of nature and some other theory is needed at a more fundamental level, or even whether our concept of reality needs adjusting.
However, results of an experiment published just before Christmas show in clearer perspective than ever before that quantum theory holds good as far as we can tell.
This most recent result is the culmination of a series of tests performed by Alain Aspect and his team at the Institut d'Optique Theorique et Appliquee at Orsay, near Paris.
These physicists have systematically probed one particular puzzle of quantum mechanics, a question that Albert Einstein and colleagues Boris Podolsky and Nathan Rosen first raised back in 1935.
But first a word about the basic notions of quantum mechanics, the mathematical theory that has proved extremely successful in describing and predicting quantum effects.
The theory has to cope with situations in which the dynamical properties of particles, such as electrons, protons and neutrons, and even atoms, are very different from what we would expect of a scaled-down billiard ball following its usual ‘classical’ behaviour.
To do this, quantum mechanics describes each particle by means of a ‘wave function’, rather than by well-defined dynamical variables such as position, momentum, angular momentum, and so on.
The wave function contains all the information about the possible values of the dynamical variables and allows these variables to be calculated in a statistical way; it gives only probabilities that the variables will have certain values.
However, the theory gives no insight into how we are to understand the behaviour of individual particles.
Indeed it was their attempts to comprehend the formalism in terms of individual behaviour that led to some of the fathers of quantum theory, such as Erwin Schrödinger, Louis de Broglie and Einstein, to express strong doubts about the ultimate validity of the theory.
The specific question that Aspect, together with jean Dalibard and Gérard Roger, has attacked is commonly known as the Einstein-Podolsky-Rosen paradox, the essence of which runs as follows.
Imagine two particles produced in a suitable composite state that can be described by a single wave function of their respective possible positions.
Suppose further that this particular wave function cannot be written as a simple product of two single-particle wave functions, one for each particle.
Then allow the particles to move apart in such a way that the combined wave function does not change even though the distance between the particles may be so great that there is no longer any possibility of mutual interaction: in principle this could be many kilometres.
Quantum theory then predicts that when a measurement of some property of one of the particles (call it A) is made the results we obtain for certain measurements on particle B depend, not only on the results we get for the measurement on A, but also on what property of A we decide to measure.
In more anthropomorphic terms, it is as if one of the particles ‘knows’ what measurement is being carried out on the other and adjusts its state accordingly.
I should emphasise that this result holds despite there being no interaction between the two separated particles A and B, and no connection between the two instruments required to make the measurements on the two particles.
In other words, there appears to be some form of ‘nonlocal’ effect, an effect that Einstein called a ‘spooky action at a distance’!
Could it therefore be, as the theorist Paul Dirac suggests, that when confronted with these non-local effects, quantum theory will prove to be inadequate?
Is it that Einstein was right when he suggested that quantum mechanics was simply a mathematically-convenient calculational device, which masks a deeper theory based on locality in space time?
Or, even more radically are we faced with the need for an entirely new concept of reality?
The difficulty in finding unambiguous answers to these questions rests on how we are to interpret the relation between the wave function and the individual particle it describes.
In general the wave function appears in the form of a packet with a macroscopic extension in space.
The particle is essentially a point on this scale and all the theory tells us is that its position is somewhere in the packet, the probability of a particular position being proportional to the square of the amplitude of the packet at that point.
This immediately raises the question of what we mean by the separation of two particles A and B. It seems the best we can do is argue that the two particles A and B can be considered as separate only when the individual wave packets cease to overlap.
Several research groups have carried out experiments designed to distinguish between a composite wave function of the two particles and a simple product (New Scientist , vol 85, p 746).
By making certain assumptions about the size of the individual wave packets, these researchers have shown that quantum mechanics is not violated.
Furthermore, they designed the experiments to ensure the detected events were ‘space-like separated’; in other words, it was not possible for signals travelling at or below the speed of light to pass between the two measuring devices.
So these first results indicated that quantum mechanics is adequate even for those situations where nonlocal effects might seem to be necessary.
But these results left unanswered a second question, namely, is it possible to reproduce the results of quantum mechanics by associating definite but unknown positions and momenta with individual particles without resorting to non-locality?
It turns out that it is possible to reproduce exactly the results of non-relativistic quantum mechanics by attributing definite properties to individual systems.
The method that has given most results has now become known as the ‘de Broglie-Bohm formulation’, after Louis de Broglie and David Bohm, professor of physics at Birkbeck College, London (New Scientist , 11 November, p 361).
Recently several independent groups, including our own at Birkbeck College, have been investigating this model in further detail and have shown how it is possible to calculate explicit particle trajectories in various contexts, thus opening the possibility of understanding quantum phenomena in terms of the behaviour of individual particles.
But when applied to the two-particle situation we are considering here, the de Broglie-Bohm formulation leads to a non-local action at a distance with exactly the same properties that Einstein and Schrödinger intuitively felt to be implied by quantum mechanics.
Is non-locality, then, a feature of all such models?
It is here that John Bell, a theorist at CERN, Europe's centre for nuclear research, has made a very valuable contribution.
By considering a simple case where the measured variables could take only two values, he obtained a mathematical expression that must be satisfied by a whole class of local theories.
This expression is an inequality, giving upper and lower limits on relationships between the measured variables.
To derive this inequality, Bell made two assumptions:(a) the effects of a measurement at A do not affect the measurement at B and vice versa, and (b) all the variables describing the two particles are independent of the settings of the apparatus at A and B. Both seem very reasonable requirements, which indeed have played a central role in the development of physics since the time of Isaac Newton.
If the terms of Bell's inequality are calculated from the composite wave function I mentioned above then the inequality is found to be violated; on the other hand, it is always satisfied for wave functions that are simple products.
Thus we have a precise way of establishing the validity of non-local effects in quantum phenomena.
The best way to test the inequality experimentally has turned out to be to measure the correlation in the polarisation of pairs of photons emitted in certain electron transitions in atoms of calcium and mercury.
The polarisation of the photon provides the single dynamical variable that Bell postulates and, as required, it can take only one of the two values: a photon or ‘parcel’ of light, may be polarised in a plane either parallel or perpendicular to a defined direction in space.
The technique in the experiments is to set up two photo-detectors on either side of the calcium or mercury source, preceded immediately by polarisers that allow through only those photons polarised in one direction (Figure 1a).
The number of photons detected simultaneously (’ in coincidence’) thus gives a measure of the correlation in polarisation; indeed, Bell's inequality can be written in terms of the number of photon pairs counted with both polarisers set first in the same direction, and then in different directions (or vice versa).
Four independent photon-correlation experiments carried out in the 1970s showed that Bell's inequality is indeed violated, and that the results agree with quantum mechanics.
But these experiments have been criticised on two counts.
First the polarisation detectors were not far enough apart to rule out the possibility of overlapping single-particle wave functions; and secondly the pairs of detected photons were not space-like separated — a signal travelling at the speed of light could have passed between the two sets of apparatus.
This is the point at which Aspect and his colleagues at Orsay stepped in with an experiment designed specifically to satisfy these two requirements.
In results reported last summer (New Scientist , vol 95, p WS), their data confirmed quantum mechanics to within an even higher degree of accuracy than previous experiments.
But even this experiment left loopholes.
For example, the researchers set up the apparatus hours and even days before they took the final readings.
There was thus plenty of time for the photon source and the two separated measuring instruments to affect each other, perhaps through some kind of ‘background fluctuations’.
The experiment was static with the polarisations to be detected at A and B determined in advance.
What was needed instead was some means of setting up the apparatus so that each detector could not ‘know’ in advance the polarisation being detected at the other side of the experiment.
It is here that Aspect has taken a crucial step with his latest experiment.
Suppose that a device can be introduced to select the direction of polarisation detection at the last possible moment just before the photon arrives.
A way to do this is with a switch that can direct the photon to one of two detectors set to measure different directions of polarisation, both at A and at B (Figure 16).
If the switch acts fast enough and at random, then the photons will be well on their way before the detector has been selected.
If the timing is right this will mean that any coordination through ‘background effects’ will not have sufficient time to become established even by signals travelling at the speed of light.
Aspect, Dalibard and Roger have produced such an optical switch by exciting an ultrasonic standing-wave in a cell of water.
If the cell is set up at an appropriate angle to the incoming light, then when the amplitude of the standing wave is zero the cell transmits the light in the usual way; when the amplitude is at its maximum the light is deflected at angle equal to twice the incident angle (Figure 2).
The photon-correlation experiment utilises two such switches, which change randomly relatively to each other, with a switching time of 10 nanoseconds (10 thousand-millionths of a second).
This time is small compared with the time for a signal at the speed of light to travel from the detectors at one side of the experiment to those, 6 metres away, at the other side.
This ensures that the detectors are space-like separated.
Once again Aspect's team has found that Bell's inequality is violated and that the results agree well with quantum mechanics (Physical Review Letters , vol 49, p 1804).
The researchers admit that they have not yet achieved the ‘ideal scheme’, for the switching is not truly random, but is related to the (different) periodic frequencies of the generators driving the standing waves in each cell.
But many people will join John Bell in believing that Aspect's experiment is as good as can be achieved, now that he has made the crucial move in introducing some time variation into the apparatus.
For those whose main interest lies only in the formalistic approach to physics, these results will have little impact, but to those who are interested in trying to understand further the implications of this non-locality there remain intriguing questions, which must be carefully examined in depth.
The results must not be used to justify wildly-exaggerated claims, which should be resisted until the implications of this non-locality are explored further.
Indeed in their paper in Physical Review Letters Aspect and his colleagues emphasise that the results ‘cannot be taken as providing the possibility of faster-than-light communication’.
What the results do indicate is that we must be prepared to consider radically new views of reality without placing locality in a central position.
SUBMARINE TELECOM
Britain's first undersea optical fibre will enter service in 1985 British Telecom announced on Tuesday.
The 23-kilometre cable, which will operate without regenerators, will carry telephone conversations between the mainland and the Isle of Wight.
The cable will contain four pairs of fibres.
Telecom will use only two at first but each pair can carry 2000 telephone calls at once.
Existing optical fibre cables need regenerators every 8 to 10 kilometres.
The Isle of Wight cable does without because it transmits monomode signals on a long wavelength (1300 nm).
Why some insects look pretty nasty
Paul Harvey
Many insects are brightly coloured, evidently to advertise their unpalatability.
But how did such a subtle form of protection evolve?
Many distasteful insects have bright coloration which, so biologists suppose, has evolved as a protection against predators.
Yet bright-coloured, ‘aposematic’, larvae are more likely to be detected than those that are camouflaged, or just plain dull; that is, those with cryptic coloration.
This paradox troubled Charles Darwin and he consulted Alfred Russell Wallace, who had ‘an innate genius for solving difficulties’.
Wallace suggested that some outward sign of distastefulness was necessary to ‘indicate to its would-be destroyer that a caterpillar was a disgusting morsel’.
In his Descent of Man and Selection in Relation to Sex , therefore, Darwin concluded that for distasteful species ‘the most gaudy colours would be serviceable, and might have been gained by variation and the survival of the most-easily recognised individuals’.
Darwin's explanation of bright colour has lain dormant and untested for over a century.
Recent experiments using chicks as ‘predators’, and coloured, distasteful crumbs as prey, indicate that although the chicks at first eat the brightly coloured, aposematic, crumbs more quickly than cryptic (inconspicuous) crumbs, they also learn to avoid distasteful crumbs more quickly when they are brightly coloured than when they are cryptic (Figure 1).
Thus Darwin appears to have been at least partly correct.
But how could the association of distasteful and bright coloration (aposematic) evolve in natural populations?
An aposematic species may indeed suffer less predation than a cryptic, distasteful species, but that does not tell us how aposematism evolved.
For example, we might imagine a population of unpalatable, but cryptic, insect larvae.
If a mutation arises in this population, causing its possessors to be brightly coloured, these few conspicuous prey will be easily spotted by foraging birds and probably be destroyed before the naive predator learns to avoid the novel form.
Distasteful families
One solution to this problem is R. A. Fisher's theory for the evolution of distastefulness itself.
Fisher noted that the evolution of distastefulness in a palatable species is problematic because the predator cannot distinguish between the palatable and distasteful individuals without killing them.
Once a predator encounters a distasteful individual it avoids both types thereafter, so there is no  advantage in being distasteful.
But, Fisher suggested, if distasteful prey live in family groups, the predator can learn to avoid the distasteful families, thereby conferring an advantage on them.
The grouping of family members in distasteful species may also aid the evolution of aposematic coloration.
When the mutation that produces aposematic colour first appears in the population it will be rare; but it will nevertheless be common in particular families, as it will be present in many of the siblings in those families with the mutation.
Any predator discovering an aposematic prey will be confronted with several others nearby.
Thus, one predator can learn to avoid brightly coloured individuals without killing all that it encounters.
As long as only a few predators hunt in the same area, each aposematic family is likely to produce several survivors.
An indication that kin (family) grouping is related to the evolution of aposematic coloration is provided by the caterpillars of British butterflies.
All species that live in families of 100 or more are aposematically coloured, compared with only a small proportion of the solitary species.
Even within a species, different parts of the life cycle illustrate the association.
For instance, cotton stainer bugs,Dysdercus , are distasteful both as larvae and as adults.
The larvae live in kin groups and are aposematic, while the adults disperse to live a solitary existence and are cryptic.
Yet aposematic coloration has not always evolved among kin-grouped prey and we need to identify the variables that are of evolutionary importance.
At Sussex, we have analysed a variety of models for population genetic that make different assumptions about (1) the grouping patterns of prey, and their breeding systems, degree of conspicuousness and palatability,(2) the ability of predators to learn and remember, and (3) the population densities of predator and prey.
Our models invariably produce the same general conclusion: if aposematism can reach a high enough frequency in a local population, then it will be favoured over crypticity (Figure 2).
The advantages of being conspicuous, and thus rapidly impressing predators with one's distastefulness, outweigh the risk of being eaten before the message is conveyed.
Three conditions of biological importance favour the evolution of aposematism among kin-grouped prey.
First, aposematism is favoured, when the prey families are found at low densities (that is, are relatively rare) compared with that of their main predators.
Secondly, the predator must easily recognise the prey and remember.
Lincoln Brower has shown that birds learn to avoid aposematic prey after very few ‘trials’(usually less than five), while Mariam Rothschild at Cambridge University has produced clear evidence that birds remember for many months to associate particular colour patterns in prey, with distasteful experiences.
But — the third condition — prey families must not be too easily detected by predators.
This third point again seems paradoxical, for how can prey be both aposematic and not aposematic?
Again, Rothschild has provided a possible answer.
Some aposematic prey that are conspicuous when seen at close quarters appear cryptic when living in family groups and viewed from a distance.
For example, the caterpillar of the cinnabar moth (Callimorpha jacobea) resembles the flower head of its food plant (ragwort) from a distance but its black and yellow stripes are aposematic when seen from nearby.
In addition, some prey behave in ways that enhance their conspicuousness once they have been spotted.
In his book Defence in Animals , Malcolm Edmunds cites the African grasshopper Phymateus which relies upon camouflage for primary defence but produces a dramatic warning display once a predator approaches, opening its orange and crimson hind wings and exuding a noxious fluid.
However, we still do not know why predators learn to associate distastefulness with bright coloration more easily than they would learn to associate distastefulness with a cryptic pattern.
Perhaps it is because a predator is able to find and eat relatively large numbers of aposematic prey in a short space of time, and that the high initial rate of feeding produces a more powerful reinforcement than a greater number of prey eaten over a longer period.
Alternatively ease of learning may be related to conspicuousness per se .
The system that we have described is, of course, open to cheating.
Many species that are not themselves distasteful mimic other brightly ones that are.
This form of mimicking was described by the great biologist Henry Bates in the l9th century; and such Batesian mimicry has long been of interest, particularly to students of evolution.
It may be that fascination with the problems of why palatable insects are brightly coloured has caused us to overlook Darwin's problems of why unpalatable insects are brightly coloured in the first place.
Figure 1.
In this experiment, by John Gittleman, chicks were conspicuous (aposematic), here represented by open circles, and some inconspicuous (cryptic), here shown as closed circles.
At first, the chicks ate more of the aposematic than of the cryptic crumbs.
But they quickly learned to avoid the aposematic ones and by the end of the experiment had eaten more of the inconspicuous variety 
Figure 2.
Analysis of  evolutionary  models suggests that when aposematic prey are relatively uncommon (that is, are present only at low frequencies) they are selected against ;but when they are relatively common (present at high frequencies), aposematism  is selectively favoured.
This graph compares the proportion of brightly-coloured prey in a population before predation (horizontal axis) with the proportion after predation (vertical axis.
The dotted line shows the position if the proportion of aposematic types was the same both before and after.
In fact, if the aposematic prey is rare, it is more heavily predated, and the proportion falls: if the aposematic prey is relatively frequent to begin with, it is favoured, and becomes more common The black and yellow caterpillars of the cinnabar moth are camouflaged when seen from a distance as they feed on their favoured ragwort.
Close to, however, they are conspicuous ;that is, are aposematic.
Like many aposematic species, they live in family groups.
Thus, of 20 species f British butterflies with aposematic caterpillars, nine live in family groups.
Of 44 species of cryptic butterfly caterpillars, none live in family groups 
Japan challenges the computer giant
Gene Gregory
Business mythology has it that computer firms in Japan are no good at software and will never break the dominance of IBM, the world's biggest computer company.
But important changes in the way computers are sold are beginning to favour the Japanese
Japan's electronics companies are well on their way to attaining their often stated goal — breaking IBM's stranglehold on the world's computer industry.
The American firm accounts for some 60 per cent of the world's annual sales of computers and has a turnover around $30000 million.
In 1980, it spent $1.5 billion on research, more than the whole of the Japanese computer industry.
These figures make IBM a natural target for Japan's electronics firms, which are relative newcomers to computers but have high aspirations of success.
For years the Japanese have tried to match IBM by selling what are called plug compatibles.
These are machines that run on IBM software and in other ways appear similar to the equipment of the market leader.
The strategy is to sell the plug compatibles for less than the price of the equivalent IBM machine but to make them at least as powerful, or to provide more for the same money.
In the early 1970s, industry observers thought that IBM was strong enough to shake off the challenge from Japan.
But since 1977, the country's computer manufacturers have consistently matched the American company's price reductions.
What is more, each time the market leader unveiled a new processor, the Japanese competition went one better with its own machines.
The most important challengers to IBM are Fujitsu, Hitachi, and NEC (which all make large or ‘mainframe’ computers) followed by three big electronics firms, Mitsubishi Electric, Oki and Toshiba.
The latter specialise in business computers, word processors and ‘peripheral’ equipment for attaching to computers.
Between 1975 and 1980 annual production of Japanese computer manufacturers increased by 139 per cent to $4700 million (1293 billion Yen).
Exports as a share of production increased from 6–8 per cent to 10–7 per cent.
By 1990, industry observers expect Japan to account for 30 per cent of world sales of computer-related goods, compared with 11.2 per cent in December 1980.
With very little investment in sales and service organisations, Fujitsu and Hitachi, especially, have become powerful forces in world markets.
By mastering production technology of mainframes and their microelectronic building blocks, the Japanese vendors have changed both the structure and style of the global computing business.
The Japanese have advanced in the face of claims by many observers that they are poor at developing software — the instructions that make computers operate.
Increasingly these instructions are becoming more important in determining the power and flexibility of computers than the machines' electronic parts or hardware.
Over the past 20 years Japanese computer firms have, in fact, developed highly sophisticated applications software.
They go about this work, however, in a way somewhat different from computer firms in other nations.
Most software development in Japan is under the aegis of the mainframe makers, but much of this work is subcontracted to subsidiaries or small independent software houses.
And much of the software produced is applications software which is sold as a package with the machines.
Hitachi, for instance, has 16 subsidiaries producing software and Fujitsu six.
In addition, these companies employ about 1000 software contractors.
The effects of this system are to keep the software departments of the mainframe makers relatively small, to reduce the cost of producing software and to lower the cost of software to the user.
It also tends to reduce the scope for independent software firms: there are few software houses not working for the computer manufacturers.
All this, plus the reliance on IBM operating software by the makers of plug-compatibles, presents a rather deceptive picture of software underdevelopment.
According to Ulric Weil of Morgan Stanley, the US firm of industry analysts, by 1987 Japan will have closed any remaining gap in its ability to develop software on a par with that of the top American firms.
Once this is done, Japan's electronics companies will begin to compete in a market that is only just emerging — selling software packages for personal computers.
By 1985 between 10000 and 15000 software packages will become available for the owners of personal computers in Japan, according to estimates.
And by the end of the decade, software will have become a commodity similar in importance to the machines themselves.
To hasten this process, the Japanese government is helping private industry through its Computer Basic Technology Research Association.
This supports the development of software for controlling networks of machines, managing data bases and handling the complexities of the Japanese language.
Significantly, leading Japanese computer makers have adopted export strategies similar in some respects to those of the car industry.
In both instances, the main markets targeted for development have been those of the US and Western Europe.
But quite unlike car manufacturers, Japanese computer makers began by exporting the high end of the product line rather than beginning with lower-priced models and trading up.
This strategy was possible because they have sold mainframes mostly to computer vendors in the country concerned rather than through direct sales channels.
The foreign partner provides access to the market, which would otherwise require prohibitively heavy investments in time and money, and most importantly, opens the way to selling products to government organisations.
In both the US and Europe, such bodies normally buy their mainframes only from local suppliers.
Where exports have been direct, through their own overseas sales organisation, Japanese firms (especially NEC and Mitsubishi Electric) have been careful to avoid a direct confrontation with IBM.
The companies' exports of computer equipment have therefore begun with small business or personal computers — product areas in which IBM is far from dominant.
Contrary to the view of many observers in the computer industry, the reliance of Fujitsu and Hitachi on plug compatibles is not a sign of technological weakness.
To produce computers that can be used interchangeably with IBM requires a high level of technical capability.
Manufacturers must proceed on the understanding that they will receive no assistance from IBM.
Nor can they rely on technology from other mainframe firms.
Moreover, they must be able to anticipate IBM's future strategy and be prepared to launch a competitive product at about the same time as IBM.
Thus, a successful plug-compatible strategy depends upon a parity of technology with IBM.
It is not by accident, therefore, that Fujitsu was first to take this route in Japan.
Fujitsu was the only Japanese mainframer which developed its own technology from the outset, producing its first computer in 1954.
In 1966, solid state computers, developed under the direction of Drs Hanzo Omi and Toshio Ikeda, were exported for the first time.
And by 1972, Fujitsu had reached technological parity with IBM.
What was needed was a continual flow of information regarding IBM's product and marketing plans, which is readily available from a legion of professional IBM watchers who demand, and receive, hefty sums for analyses of future IBM strategies.
Fujitsu and Hitachi have made no effort to leapfrog IBM by introducing entirely new machines, for this would defeat the plug-compatible strategy: to let IBM create the demand and then meet it at lower prices.
NEC has however adopted a totally different strategy from Japan's top two mainframe makers.
It has developed its own operating systems (the basic software on which the firm's machines run) and its own international marketing organisation, with an emphasis on personal computers.
This strategy, NEC insists, produces greater flexibility (as it is not necessary to follow IBM slavishly) and also keeps the company on a smoother footing as it no longer needs to follow IBM's every turn.
Bearing eloquent witness to the strength of this strategy, NEC's total computer sales in 1981 surpassed those of Hitachi to make it second only to Fujitsu among Japanese makers.
Retail sales increase
The outlook for Japan's computer firms is made brighter by several major changes in the way the machines are sold which favour the existing strengths of its computer industry.
First, the computer market is becoming increasingly diversified.
People buy computers or computer-controlled products not just for offices but for a host of other places such as factories and homes.
As the price of computers decreases and as they become incorporated in a wide range of domestic goods, from cookers to calculators, the ordinary consumer is becoming a major purchaser of electronic gadgetry.
The broadening market for computer equipment suits Japanese manufacturers down to the ground.
Unlike the American computer industry, which is characterised by firms that make a narrow range of goods, all six of the top Japanese electronics firms make semiconductors and telecommunications equipment as well as computers.
And all but Oki have a base in consumer electronics.
The four firms that make mainframes — Fujitsu, Hitachi, NEC and Mitsubishi Electric — also turn out personal and office computers, terminals, word processors and robots.
To add a further dimension to the diversification, Fujitsu recently introduced a very fast supercomputer for scientific use.
NEC and Hitachi are also developing similar machines.
This approach flies in the face of established practice.
Even IBM has chosen to leave this field of ‘number crunching’ to specialised manufacturers such as Cray Research or Control Data.
The second trend that favours the Japanese is that computer manufacturers are starting to abandon the traditional ways of selling their machines directly to customers.
This approach, which requires special sales and maintenance centres, is obsolescent now that computers are becoming a consumer item.
They can thus be sold just like any other piece of consumer equipment, for instance through stores such as Sears Roebuck or J. C. Penney in the US or W. H. Smith in Britain.
The American firms, with their network of sales and support services, are locked into the old way of selling while the Japanese, who are used to marketing their goods through high street retailers, are at an advantage.
Thirdly, changing geographical patterns of demand favour Japanese manufacturers.
While until recently the US was by far the largest market for firms that make data processing equipment, giving IBM and other American makers of mainframes a big advantage, the greatest growth in the future will be in Europe, East Asia and in newly industrialised nations in other regions.
These countries often agree to the Japanese forming joint ventures with local companies.
It is significant that Japanese firms are not adverse to joining forces with American rivals of IBM; for instance, Mitsubishi and Sperry Univac recently decided to cooperate in some technical areas as a way of combating the very large computer makers.
Fourthly, semiconductor engineering is playing a growing role in the manufacture of computers.
Not by accident, the leaders in chip design and production in Japan are also the strongest computer companies.
Advances in, for instance, the production of 64K RAMs (memory chips that can hold around 64000 bits of information) give Japanese manufacturers a versatility and efficiency which has made it possible for them to achieve so much in the past few years.
And the leadership in VLSI (very large-scale integration) technology built up since 1976 puts Japan in a good position to move into the vanguard in future developments in computers.
Finally, Japan's manufacturing industry has staked a lot in the past 10 years on installing advanced production techniques, and has attained a substantial advance in the use of robots and other automated systems.
These forms of production will increasingly apply to the manufacture of computers — and this is yet another area in which Japanese firms could turn out to have an edge on the rest of the world.
Robots step out of the factory
Michel Cusumano
There may be arguments about how to define a robot.
But it is clear that, in Japan at least, the machines will soon be down mines, exploring the sea bed, sweeping up nuclear reactors and generally doing more dirty work than ever before
Yoshio Ando, chairman of the Japan Robot Industry Association, refers to robots as ‘tools for the liberation of mankind’.
Doubtless most industrialists in the UK would concur, but where they might fall out with him is on the definition of ‘robot’(See Box below).
The British would argue: ‘An industrial robot is a universal, programmable manipulator.
If it is not a mechanical arm, controlled by a computer and capable of varied tasks, then it is no robot.’
To this Yoshio Ando might reply: ‘Not so.
A robot is any automated machine.’
The Japanese definition includes tools that need to be operated by a person, like mechanical hands.
This is the context of the Japanese claim to have 200 companies making robots and to possess 70 per cent of the machines at work anywhere.
Their attitude is best witnessed in the factory.
There, robots work at dangerous and tedious jobs and serve as integrated parts of automated systems.
They handle and transport materials, make and assemble parts, and do quality checks and warehousing.
It is not accidental that Japanese manufacturers have become eager converts to robots and automation.
Despite a national unemployment rate of 2.4 per cent, industrial growth over recent decades has caused labour shortages in small or medium-sized firms.
They are unable to offer the high wages, job security and extensive welfare benefits of large companies.
Increasing longevity and low birth rates (1.7 children per woman) also mean that fewer young workers will enter the labour force in coming years.
A recent survey of managers in Japanese industry showed worker ageing and labour shortages to be their biggest worry now, and for the remainder of the 1980s (see table on next page).
Installing robots in Japan is a labour strategy born of necessity, with the particular advantages that robots work all day, produce 30 per cent more than people and drastically reduce defect rates.
They are not infallible.
Three out of four break down before completing 1500 hours.
Nearly one in three experience difficulties before 100 hours, mostly in control equipment and jigs that hold work in place.
Mass-production is reducing the cost of buying or leasing robots (rental to small and medium firms is a growing business) as wages and welfare requirements rise.
The factory survey found that one quarter of Japan's biggest factories used robots and that a second quarter had plans to install them by the end of 1982.
The remaining half of managers reported a desire to introduce robots in the future.
Most manufacturers do not yet find robot ‘intelligence’ necessary for mass production, although few doubt its potential.
The appearance of playback and CNC robots since 1977 is said to have begun the second generation of sophisticated units verging on being ‘intelligent’.
The Japanese expect a third generation about 1990; these will be ‘learning robots’ capable of self-programming and they will also be able to move around.
Who manufactures what
Japan's Ministry of International Trade and Industry (MITI) will start spending in 1983 $70 million on a seven year project to develop ‘learning robots’ for sea-bed and space exploration, nuclear research and power-plant maintenance as well as aids for the old and handicapped.
The project is headed by the ministry's Electrochemical Laboratory and involves about 10 major robot, computer and machinery manufacturers.
The first two generations of robots and their technologies were developed largely in the US by organisations like MIT and Unimation Inc.
Japan seems to have a special advantage in robotics today because so many firms there are both large users and makers of robots.
Among the world's leading users of robots are Japan's giant electronic, electrical and mechanical appliance firms.
They excel at the skills central to robotics.
Hitachi, Toshiba, Mitsubishi, Fujitsu, Nippon Electric and Matsushita — the largest of the ‘mech-tronics’ groups began making-robots as long as a decade ago.
In the past year or so they have launched major design and marketing efforts for factory automation systems.
Hitachi ranked in 1981 as Japan's second biggest robot manufacturer (behind Yasukawa and ahead of Kawasaki Heavy Industries).
It began making robots in 1970 for itself: in 1975 it started selling arc-welding units equipped with sensors.
Twenty company divisions for electrical machinery, computers, communications, measuring instruments and systems engineering all cooperate in robotics work.
Ten company factories are used as model automation plants.
Toshiba is Japan's second largest maker of general electrical machinery and appliances.
It specialises in spot-welding and fixed-sequence robots, but will soon market arc-welding machines and robots that can pick up items such as lumps of metal.
Mitsubishi Electric has a special ‘industrial mecha-tronics division’.
It makes arc-welding and educational ‘hobby’ robots as well as transport, assembly and materials handling robots equipped with sensors and laser navigation devices.
Mitsubishi Heavy Industries and Mitsubishi Metals are also large users and makers of robots and other factory automation systems.
Fujitsu is Japan's biggest manufacturer of mainframe computers and is a leader in office automation equipment.
This firm has been designing precision assembly robots to make integrated circuits for the past decade.
It expects to sell, in two or three years, intelligent robots capable of limited judgement.
Fujitsu-Fanuc, a robotics firm within the group, is the world's top maker of numerical control equipment.
Last June it established a venture with General Motors in the US to build robots for the American market, including GM's plants.
Fanuc is also developing intelligent robots with Siemens of West Germany.
Fuji Electric, the oldest company in the group, specialises in heavy machinery and semiconductors.
It first produced robots —‘electric hands’— in 1971; more recently the firm designed ‘intelligent inspection systems’ that combine video-sensors with the world of machine tools.
Nippon Electric has 10 years' experience in making numerical control equipment and computerised design and manufacturing systems.
In 1981 it launched one of Japan's most precise assembly robots, capable of inserting objects into 0.5 mm-diameter holes and of performing delicate laser welding.
Nippon is Japan's largest manufacturer of integrated circuits and office automation equipment.
Last year, for the first time, it produced more computers than Hitachi, and took second place in the field to Fujitsu.
Matsushita Electrical Industrial is the world's largest maker of electric appliances and video tape recorders as well as one of the biggest users of assembly robots.
The group makes much of its own mass-production equipment including robots that assemble electronic circuitry. multi-arm welders, automatic laser machine tools and screw fastening robots.
Matsushita now intends to develop intelligent robots that will combine the technology from office automation, microcomputer and video disc systems.
Although plant automation is the primary interest of the ‘mech-tronics’ firms and most other robot makers, the Japanese have many uses for robots outside the factory.
Several of these applications, such as nuclear power-plant maintenance, are designated for development in the MITI project.
Six of Japan's 10 electric-power companies are cooperating with Hitachi and Toshiba (the country's biggest nuclear power-plant makers) to automate dangerous work.
The joint effort has produced remote control fuel-rod exchangers and automatic inspection systems for radioactive welded pipes.
Future work will specialise in automating floor decontamination, inspection and reassembly of steam safety valves and taking samples after accidents.
Inspection and monitoring are expected to be done by mobile robots capable of pattern recognition.
Meidensha, a control equipment and transformer maker in the Sumitomo group, has built pressure sensors, said to ‘approach the sensitivity of human fingers’, into robots for nuclear plants.
The Japanese would like to automate mining.
The most advanced equipment in use is for coal transport.
It uses television cameras on coal cars.
A system is being considered that would place shield-type support beams in front of a drum cutter opening up tunnels while being operated 10–20 metres behind.
Robotics in the future
Another technology being explored is sea-bed robotics.
Komatsu has built a robot with eight ‘legs’ that move four at a time to propel the machine at 200 metres per hour regardless of obstacles.
The cable-controlled unit, which is lowered from a ship, carries equipment such as TV cameras and sonar.
The Komatsu robot was designed to help draw detailed maps of the ocean floor but it may serve as a prototype for mining and construction vehicles.
In Japan, robotics seems to be limited only by human imagination and technology.
Unions have accepted automation since there has been a shortage of blue-collar labour and, in any case, large firms guarantee most jobs (for men) until retiring age.
Yet, as industrial growth slows or ends, union leaders may resist further automation if too many jobs appear to be evaporating.
There are already signs of this process in the Japanese car industry.
In the US and Western Europe, the social costs of factory automation are more obvious.
General Motors, for instance, has laid off thousands of workers and plans to introduce 14000 robots in American factories by 1990, primarily as a response to the Japanese.
Even if the ageing of Western populations eases current labour surpluses in time, many companies face a dilemma today.
There is pressure from Japanese competition to introduce robots and factory automation; yet to automate will aggravate unacceptably high unemployment.
There are interim solutions, such as trade restrictions against Japan, but GM's approach of granting some job security along with a gradual introduction of robots appears to be more constructive.
Social and political costs may limit the application and development of robotics in the industrialised West as much as technology.
These ‘tools for the liberation of mankind’ have the potential to create more leisure than some societies are prepared to absorb.