

JOB SEARCH IN A DYNAMIC ENVIRONMENT — AN EMPIRICAL ANALYSIS
By WIJI NARENDRANATHAN
1.
Introduction
If an individual faced with a higher level of benefits out of work relative to expected income in work will be more choosy in job selection, the actual magnitude of the effects of these benefits on the behaviour of the unemployed becomes important to policy makers.
Since changes in benefits can have different implications for different individuals, aggregate time series tests of the effect of average benefits may be misleading.
Successful targeting of unemployment benefit policies calls for analysis at the individual level.
Cross-section empirical studies include Lancaster (1979), Nickell (1979a, 1979b), Lancaster and Nickell (1980), Atkinson et al.(1984) and Narendranathan et al.(1985, henceforth called NNS).
Those studies specified and estimated the conditional probability of leaving unemployment.
This hazard function approach measured the impact of key exogenous variables, such as unemployment benefits, on unemployment duration.
But some policy issues may require identification of the effects of other variables such as age, qualifications and marital status.
For example, successful analysis of the problem of decreasing re-employment probabilities requires separating the effects due to job-offer probability from those due to individual preferences.
We must look at the behaviour from a properly defined structural viewpoint.
A search-theoretic approach underlay the papers by Kiefer and Neumann (1979a, 1979b, 1981), Lancaster and Chesher (1983), Narendranathan and Nickell (1985)(henceforth called NN), Ridder and Gorter (1986), Wolpin (1987) and van den Berg (199Oa).
All but the last two tested static models.
This is the approach followed in the present paper, where the emphasis is on allowing as much heterogeneity as possible and use the available disaggregated information given the nature of our data set.
The aim of this paper is to specify and estimate a model of the behaviour of an unemployed individual which extends previous research in two ways.
The first extension deals with the static nature of the earlier models.
Starting from the theory of job search we specify an estimable model which is derived under a particular assumption regarding the dynamic behaviour of the unemployed individual.
This model has enough structure built into it to enable us to answer  interesting questions such as:(i) does the marginal utility of leisure decrease or increase with duration? and (ii) do offer probabilities, unemployment benefit effects, and the conditional probability of leaving unemployment decrease or increase with duration?
Atkinson et al.(1984) found that the cross-section benefit elasticities as estimated from the Family Expenditure Survey data for 1972–7 exhibited considerable variability depending on how the relevant income variables were constructed.
Given the uncertainty that surrounds the entitlement to means-tested benefits, one might expect the individuals to attach different weights to different components of the total income in the optimisation (Jenkins and Millar, 1989).
The second extension presents a model that allows us to do this.
The model is then estimated using the same data set that NNS and NN examine.
Since our model imposes no restrictions on how reservation wages or the effects of variables change over time, we can let the data decide whether each individual has an increasing/decreasing reservation wage, hazard, etc. over time.
This is a significant relaxation of both NNS and NN, where the hazard was assumed monotonic in the former and constant in the latter.
In addition, NNS and NN papers assumed that it was total unemployment and employment income that were relevant for the unemployed individual's optimisation problem.
Since the same data set is used, this paper's results offer direct comparison with those of NNS and NN.
Section 2 presents the econometric model.
Section 3 discusses ways in which we validate our model using theory consistency checks and a statistical diagnostic test.
Section 4 discusses the data used in our estimation and presents the results of our investigation.
Section S concludes.
2.
Theory, model and estimation
The starting point for our model is the standard job search theory with some minor modifications.
The model is a dynamic version of NN.
Consider an unemployed individual who desires to work but knows nothing about job vacancies and is thus ‘searching’ for a suitable job in a segment of the labour market.
The individual maximises the expected present value of utility , discounted at ρ.
U is assumed increasing in income () and leisure ().
In unemployment, total income receipts in period t net of search costs, and .
In employment, wage plus other income receipts and .
While searching, the individual will come across some vacancies.
Associated with each vacancy is a fixed wage w which is a random draw from the density , which is assumed to be constant over time and known to the searcher.
Let  dw be the probability of being ‘offered’ a job with a wage w in any particular period.
This assumption that wages are attached to the vacancies and not to the individuals, as in the standard search theory models, is a very important distinction for the estimation of .
Time is assumed discrete, but the time period is so short that the probability of two or more vacancies coming to the individual's notice within any given period is zero.
In addition, the analysis is restricted to the case of no recall of offers received in previous periods.
The individual's optimal strategy under such consideration has a reservation wage property.
For this particular model the reservation wage ζ, is implicitly given by, where  is the total expected discounted return from search as viewed from the start of the period, given that the individual optimises in current and future periods.
, the ‘offer’ probability, varies with personal characteristics and some measure of the degree of tightness within the relevant labour market.
We have assumed that an unemployed individual must either continue being unemployed or become employed; withdrawing from the labour force is not an option.
Another assumption implicit in the above derivation is that the job ‘offer’, once accepted, is kept for ever.
This assumption may be relaxed to a certain extent by allowing for tenure over a specified number of (exogenously chosen) periods (see NN) or for random terminations (see Lancaster and Chesher, 1983).
This changes only the interpretation of the discount rate.
Making equations (1) and (2) operational requires some terminal conditions to solve back all the reservation wages.
For this, we assume that nothing changes after the first T periods.
This reduces to a stationary static infinite horizon problem for all periods after T with the following two equations: The model consists of equations (1) to (4).
We next assume particular functional forms as follows:
(i).
Following Nickell (1978), we assume that .
(ii) This is assumed to be a log function.
(iii) This function measures the leisure value of being unemployed relative to being employed.
We suppose this to have the form  where the vector  includes personal characteristics such as age, marital status and past unemployment history.
(iv) ρ.
Rather than imposing the restriction that the utility discount rate is the same for all, we allow it to differ across different groups in the population.
We parameterise ρ as  where, contains variables that affect ρ.
(v).
We specify the vacancy-wage distribution to be Log Normal with mean µ and variance
(vi).
Following NN we allow for the possibility that the higher the wage the more competition there will be for the job (ceteris paribus) and make this a function of w.
The z vector contains variables on both labour market tightness and personal characteristics.
The unemployment rate in the individual's ‘travel to work area’ measures the labour market tightness.
The personal characteristics included are those relevant to the chances of obtaining a given vacancy: age, marital status, race, individual's labour market history, etc.
We have also included the type of housing tenure of an individual in our sample to capture the effect of housing mobility on the expected duration of unemployment.
The  coefficient is expected to take a negative sign.
We next employ the same econometric technique as in NN to estimate the parameters of interest in this job search model.
Given the data on b, w e , x 1 , x 2 , z and unemployment duration and the above assumptions on the functional forms, this technique allows calculation of optimal reservation wages across individuals, provided we can estimate the vacancy wage distribution f(w) a priori.
The very sparse information on the post-unemployment wages for the individuals who left unemployment within the observation period need not be used in the estimation but can be used for checking the estimates.
If the data fit the theory reasonably well, then the post-unemployment wages must not be smaller than the estimated reservation wages.
From the data and assumed values for µ and  we can now solve the above equations (1) to (4) for optimal reservation wages ζ,.
Define this function as 
The estimation of the underlying parameters of our model () can now be based on the likelihood associated with the lengths of the individual spells of unemployment but requires the solution to (5).
This is the biggest disadvantage.
Because unemployment duration is measured in days in our sample, solving for the optimal reservation wage for each of the 1,608 individuals in our sample for each period at each iteration causes serious computational problems.
We therefore make the following assumption.
That is, at the beginning of the spell of unemployment the individual assumes that the environment will remain unchanged and carries out an optimisation to obtain the optimal reservation wage; but someone still unemployed at time T is assumed to reoptimise taking the changed environment into account.
The reservation wage is revised in the light of new information at time T but again assuming a stationary static environment.
This assumption, although seemingly suboptimal, is consistent with the behaviour of individuals who are searching in static and stationary environment but are revising their reservation wages in the light of unanticipated changes (see van den Berg, 1990b).
Our formulation has three advantages.
First, the simplification cuts the number of reservation wages to be calculated to two for individuals who are still unemployed after time T and to only one for the others.
Second, by varying the time cut-off point of T, we can still compare the results from this model with those of NNS and NNs without undue complication.
Third, no restrictions have to be imposed on the form of the hazard.
Because the parameters are free to change between periods, different individuals can have different patterns of hazard.
The estimated hazard may increase with duration for some individuals and not for others in the sample, depending on their data.
Two variables are assumed to vary between the two periods; unemployment income (b 1 and b 2 ) and the mean of the vacancy-wage distribution (and ).
We also allow the effects of offer probability (), leisure utility () and the discount rate () to vary as step functions between the periods.
These are some of the generalisations made in order to keep the already over-parameterised problem manageable.
This gives the following equations for reservation wages and hazards for the two periods:
The next issue is the definition of the ‘income’ variable.
In the UK, in addition to unemployment benefits, an individual out of work may be eligible to receive other state benefits such as for dependent children (child benefit, free school milk and meals, etc.), some of which may continue after re-employment.
Income from an employed spouse may also be important.
NNS and NN, who had such information, assumed that what mattered to an unemployed individual was total income regardless of source.
We did not want to impose this restriction here but wanted to let the data reveal individual preferences.
That is, a person may treat extra income differently from main unemployment (and supplementary) benefits and expected earnings perhaps due to uncertainty attached to the receipts of some benefits (Jenkins and Miller, 1989).
We allow the individual to attach different weights as follows.
Unemployment income includes (i) unemployment and supplementary benefits (ben) and (ii) all other extra income received while unemployed (xu).
Expected employment income is similarly split into (i) net earnings (earn — the mean of the vacancy-wage distribution) and (ii) any extra income (xe — the same as w e from above).
A first-order approximation yields   When  the individual compares total income receipts, as in NNS and NN.
When , extra income from either employment or unemployment carries no weight, as in most other studies.
From equations (8) and (9) and a second-order approximation to the log functions, we derive the density and distribution functions of the duration random variable and then maximise the relevant likelihood subject to equations (6),(7),(10) and (11).
3.
Model consistency checks and statistical diagnostics
The main disadvantage with structural models of this kind is the possibility of biasing the results by imposing too rigid a structure on the data.
We therefore test for model consistency with a model consistency check and a statistical test for omitted heterogeneity bias.
Model consistency check
Estimated optimal reservation wages for the individuals in our sample are used to check the validity of our model.
If the above model is a good approximation to the actual behaviour of the individuals in our sample, then, their estimated reservation wages should not be larger than their reported post-unemployment wages.
Statistical diagnostic test
The model assumes that all inter-individual heterogeneity is due to observed variables.
This very strong assumption precludes any role for unobserved variables to account for a substantial proportion of population heterogeneity.
The effect of this omission is to bias the results.
A Lagrange Multiplier test proposed by Chesher (1984) is employed to test for this.
Following the derivation in Lancaster (1985a), we assume that the observed hazard function is  and not  (as given by (8) and (9)), where  The unobservable random error  is assumed to be a positive random variable with unit mean, variance  and density .
Then, the unconditional survivor function is   where, is the integrated hazard function.
A second-order expansion of (13) around its mean gives  Corresponding ‘approximate’ unconditional density  of completed duration can thus be derived from (15) and hence the likelihood function for the problem, and the Lagrange Multiplier test of the null hypothesis .
We make use of this test of omitted heterogeneity in our model evaluation.
As pointed out by Chesher (1984), this is also White's (1983) Information Matrix test for misspecification.
4.
Data and results
4.11.
Data
The data set we have used in the analysis is constructed from the United Kingdom Department of Social Security (DHSS) Cohort Study of the Unemployed.
This is a stratified random sample of about 2,332 men who registered as unemployed in the autumn of 1978.
They were interviewed approximately 6, 16 and 52 weeks after becoming unemployed.
This data set contains direct DHSS observations of unemployment duration and unemployment benefits which allow the construction of error-free estimates of benefits paid over the unemployment spell for all individuals in the sample.
The full data set also includes DHSS information on unemployment benefits (including Supplementary Benefits) paid to sample members.
For further details see Moylan and Davies (1983), Wood (1982).
Our work uses information from the first interview and the benefit records.
Post-unemployment wages are from the second and the third interview-data sets.
The main benefit variable (ben) is the sum of Unemployment and Supplementary benefits as recorded on the DHSS computers.
The net income from all other sources is calculated from the information in the first interview data set.
This is defined as the average for each period (during which the optimisation takes places) of the sum of wives' net earnings (where received), other state benefits such as Child Benefits, rent and rate rebates, Family Income Supplement, free school meals and milk, and other private sector income (receipts such as from lodgers, occupational pensions, and insurance schemes).
The measure of ‘other sources of income in employment’ is calculated along the same lines as for unemployment income.
The expected net earnings is the estimated mean of the vacancy-wage distribution (see Appendix).
The second  period mean of the vacancy-wage distribution is taken to be the same as the first period mean adjusted for wage growth and inflation in the economy as a whole.
Estimation of the moments off (w)
These are calculated along the lines presented in NN.
Since our ‘dependent’ variable is the length in days of the first spell of registered unemployment, all the income variables are calculated as one seventh of the weekly rates.
The definitions of the other explanatory variables are obvious.
Means and standard deviations of the variables used in the subsequent analyses are presented in Table 1.
These are calculated for our sample of 1608 men who were unemployed longer than 28 days and did not give retirement as their reason for their pre-cohort entry job.
4.2.
Results
Because our sample consists of a cohort of individuals, calendar time effects cannot be distinguished from duration effects.
Since this complicates the interpretation of the two intercept dummies in the ‘offer’ probability functions as the duration effects, the unemployment rate variable, a measure of the tightness of the labour market which enters q, is allowed to vary over the two optimisation periods.
The results are reported in Table 2.
To check for the sensitivity of the results, we have estimated the model with a change point at the end of 1st (Model 1), 2nd (Model 2) and 3rd (Model 3) quarters.
Generally, the results confirm our prior views.
In terms of the maximised value of the log likelihood function, Model I performs the best, and Model 3 worst.
There are very large differences in the maximised value of the log likelihood function among the three models.
Additionally, the standard errors of the estimated coefficients are larger in Models 2 and 3 than in Model 1.
We shall first concentrate on the rate of job-offer coefficients.
‘Offer’ arrival rates
The φ coefficient which signifies higher competition for better paying jobs is highly significant and negative in Model I but not significantly different from zero in the other two models.
The ‘offer’ probability may be treated as the product of (i) the probability that a vacancy will come to the individual's notice and (ii) the probability that the individual, if available, will be ‘offered’the job.
These two probabilities   cannot be identified separately because of how they are specified.
When looking at the changes in this offer probabilities over the assumed two period, notice that the former probability is a function of all variables, not just demand, that may affect the individual's search intensity.
These changes in the estimated offer rates over the duration are very sensitive to the assumed value for 71 In Model 1, the offer rate falls 21% between the first three months and the rest of the spell.
In the other two models, any difference is insignificant.
The signs of other variables are very similar to previous studies.
Although the age effects are more pronounced in Model 3, ceteris paribus, they are all    insignificantly different from zero.
But their pattern is very similar to other studies (for example, see NNS and NN) with a gradual decrease in offer probability with age.
Marital status has a positive effect, agreeing with the interpretations that either employers favour men who are married and/or that married men search harder.
The ethnicity variables are not significant in any of the specifications.
Nor is the effect of having a disability which interferes with the individual's job.
The housing tenure variables which are picking up any supply side effects on the offer probabilities via mobility effects have the expected signs but most are insignificant.
The only type of housing tenure which has any effect is council tenancy: being a council tenant reduces the probability of receiving an offer by about 41% in Models I and 2, ceteris paribus.
This effect is estimated to be only 17% in Model 3.
Vocational training has a large and significant effect on offer arrival rates, ceteris paribus, but other educational qualifications or a completed apprenticeship matter only insignificantly.
The local unemployment rate, which was allowed to vary between the periods, has no significant effect on the offer arrival rate in any model.
The pattern of estimated effects of the individual's past labour market history is very interesting.
The fact that an individual started searching for a job while  in his old job does not significantly increase the chance of obtaining an offer.
An individual who voluntarily quit from his last job is estimated to have a 64% lower chance of obtaining an offer in Models I and 2, ceteris paribus, compared with only 38% less in Model 3.
As expected, having had both a spell of registered unemployment in the last 12 months and also no full time job in the last 12 months contributes adversely to the individual's chances of obtaining an offer.
In contrast, an individual spending less than 12 months in his last job has a higher chances of obtaining an offer ceteris paribus.
Leisure function
The individuals dislike being unemployed.
The first two variants of the model imply that their marginal disutility of leisure is very high after the first period.
However, second-period constants for the first two variants of the model are imprecisely estimated.
The reason for their very big standard errors become clearer from the specification of the leisure effects.
If the leisure effect is zero in the second period, then, in the exponential specification, the contribution towards the likelihood of a coefficient which is -15 or -35 will be the same, resulting in a very large estimated standard error.
Although the standard error for this estimate is very high, implying a non-rejection of a zero coefficient, a likelihood ratio test of this null hypothesis is strongly rejected.
This is not surprising, as the model implies that the full leisure effect in the second period is zero, not that the coefficient is zero.
The only significant leisure effect comes from the effect having receiving benefits disallowed in the first few weeks of the unemployment spell.
Such individuals do dislike leisure more than others, ceteris paribus.
Effective discount rate
Varying expected tenure of jobs for various groups of individuals is modelled by allowing the effective discount rate to change.
The model predicts that as time passes these individuals expect to move into a job with longer duration, ceteris paribus, as one would expect (see  footnote 3).
Using a rate of discount of 10% per annum(0.027% per day) implies that the expected duration of a job is about 1.3 years if accepted in the first three months of the unemployment spell and three years if accepted later (see footnote 3).
The results confirm prior expectations about personal characteristics.
The role of extra income receipts
We have allowed the weights to vary between the two periods to capture any changes in attitudes towards all the extra income receipts.
Sources of income other than unemployment benefits and earnings do matter, but less than benefits and earnings (similar results appear in Atkinson, et al.(1984)).
In fact, the estimated weights which the individual attaches to these extra income receipts is negative in the first period and positive in the second period (except in Model 3, but insignificantly).
One would expect extra income to matter more as the unemployment spell lengthens and benefits get reduced.
It is common practice to use only the information on benefits and earnings when the information on other sources of income is unavailable.
To see how sensitive the results are to this treatment, Model 1, the best specification, was re-estimated by imposing the restrictions that  and  as Models 4 and 5 respectively.
In Model 4 only total income matters; in Model 5 extra sources of income do not matter (see equations (10) and (11)).
The results, presented in Table 2, show that allowing the extra sources of income to matter (Model 4) produces very different results from Model 1 and Model 5 (where the extra income is assumed not to matter).
The patterns of all the estimated effects are the same, but they are less pronounced.
In terms of the maximised value of the log likelihood, Model 1 still outperforms the other two models.
Henceforth, we shall concentrate on Model 1 results.
Model consistency check and statistical diagnostics
Is our preferred model (Model 1) consistent with the assumed theoretical structure?
We answer this question using a statistical diagnostic test (see Section 3) and also by comparing the estimated reservation wages predicted by this model with the post-unemployment wages reported by our sample members.
Our model would be suspect if the estimated reservation wage exceeded the reported post-unemployment wage.
The results of the statistical diagnostic test for omitted heterogeneity are presented in Table 2.
The null hypothesis of no heterogeneity cannot be rejected in any of the variants of the model.
Table 3 gives the distribution by age and duration of individuals with estimated reservation wage less than the post-unemployment wage for Model 1 (unrestricted ) and Model 4 ().
We make this comparison because the papers NNS and NN, which analysed the same data set, also used income variables in the utility function (as in Model 4), not just benefits and earnings.
Half those who left unemployment within the year reported a post unemployment wage.
Of these, 90% have an estimated reservation wage which is not higher than their post unemployment wage in Model 1, this figure is only –9% in Model 4.
4.3.
Other points
Other points are examined in the context of our preferred Model 1:
(1) Our model permitted the hazard function to vary over the duration with changes differing across individuals, in contrast to standard reduced form models which restricted to uniformity for all individuals, ceteris paribus.
Table 4 shows the proportion of individuals who have estimated hazard rates and reservation wages which fall with duration.
Overall, for 90% of the individuals in our sample who had a spell longer than three months, Model I predicts a lower reservation wage in the second period than the first.
For 82% of the individuals with spell length greater than three months, the estimated hazard function increases with duration.
This figure is 34% in Model 4, a restricted version of Model 1.
In contrast, the NNS paper, which estimated a hazard function using variables similar to Model 4 and imposed a monotone hazard restriction, found the hazard to be only slightly increasing over the duration.
Also, 50% of the teenagers have an estimated hazard which is larger after the first three months, although more than half have a larger estimated reservation wage in the second period.
That is, in spite of revising their reservation wages upwards, they are exiting faster in the second period than in the first period.
The reason for this is the large age effect on the offer arrival rate.
(2) Various estimated benefit elasticities from our model are presented in Table 5.
Unlike the standard reduced-form models, these elasticities are not constant in our model.
Because leisure effects are zero in the second period, after the first three months of unemployment, the elasticities of reservation wage, hazard rate and expected duration with respect to benefits are all zero for all the individuals (see (7)).
On average, a 10% increase in the unemployment benefits in the first three months increases reservation wages by 2.3%, reduces the conditional probability of leaving unemployment by about 12% and increases the expected duration of the spell by about 1.2%.
These are quite different to what the other authors have found in earlier studies.
For example, Lancaster and Chesher (1983) estimated the reservation wage elasticity with respect to benefits to be only 0.14 and NN found it to be 0.16.
Our averages for the elasticity of expected duration with respect to benefits are much smaller than in NNS and NN who estimated different models from the same data.
In terms of the age distribution, the elasticity of expected duration with respect to benefits is 0.18 for men under 20, decreases with age to a value of 0.06 for men over 55; NNS found the effect to be largest for the teenagers with a value of 0.65.
(3) Table 6 displays estimates of the average daily probability of receiving an offer, the estimated daily hazard rates and the proportion of acceptable vacancies (by age) for the preferred model.
For men under 20, the job offer arrival rates are at least nine times higher than for men aged 55 and over (at the mean wage), emphasising how difficult it is for older unemployed men to obtain a job.
There is also a very big difference in the offer arrival rate in the young and old segments of the market, with the under-25-year old men facing an offer rate of about 12 to 15 times that of men over 24 years of age.
The conditional probability of leaving the spell is also very small for men aged 55 and over   compared to teenage men.
Average estimated reservation wages are £4.8 and 4.3 per day in the first and second periods, respectively.
This, for our model, implies an average weekly reservation wage of £33.7 and £29.8 in the first and second periods, respectively.
The estimated reservation wages increase with age up to about 35 years and then decrease with the men aged over 55, with estimated reservation wages for both periods which are lower than for teenage men.
The figures for the proportion of vacancies that are acceptable, men aged 55 and over have very low reservation wages (especially in the second period) and appear to accept almost any job.
According to our preferred model only two-thirds of the vacancies are acceptable to teenage men.
(4) Before simulating the effects of benefit changes on the expected durations for selected group of individuals, we first report the results from setting the discount rate equal to infinity.
This model assumes that an individual accepts the first job whose wage dominates the benefit level adjusted for the disutility  of work (see (6) and (7)).
We then calculate the appropriate reservation wages for the two periods.
The value of the maximised log likelihood for this model was — 8250.4 which is approximately 35 points lower, implying the significance of other variables.
(5) Simulations of expected duration The nature of this model and its estimation is such that the elasticities, hazard rates and the expected durations all vary from one individual to another and are dependent on the values of benefits, expected earnings and all extra income receipts in both employment and unemployment.
Where Table 6 shows sample averages by age, Table 7 presents results of simulations of changing levels of benefits for individuals with chosen characteristics.
The first individual is a single teenager with a job training who is looking for a job in the nonmanual sector.
His expected duration of unemployment is 12.9 weeks as calculated at the average values of benefits, earnings and extra income.
The estimated elasticities with respect to benefits of this reservation wage, hazard rate and expected duration are 0.12, -0.75 and 0.22, respectively in the first three months of the spell.
These figures are very similar to the reported age averages in Table 5.
A 10% cut in benefits in the first three months of the unemployment spell reduces the expected duration to 11.98 weeks for this particular individual.15 The elasticities are also slightly higher at the lower benefit levels.
In NNS, the estimated expected duration for this type of man was 19.7 weeks, which is much higher than our estimate.
The second special case simulated is a married man, with a mortgage and some education qualifications, looking for skilled manual work.
His expected unemployment duration of 17.9 weeks is trimmed to 17.7 weeks by a 10% cut in benefits.
The elasticities as estimated are again very similar to those reported in Table 5 for this age group.
The third case is a man with some adverse characteristics.
He is over 55, married, without education or job training; furthermore, he has had a spell of registered unemployment, did not work full-time in the 12 months prior to becoming unemployed and lives in a council house.
For this man, the expected duration at the 1978/79 benefit levels is 74 weeks.
A 10% cut in benefits brings down his expected duration by only 0.1 weeks.
This man belongs to the category of long-term unemployed.
In general, using the same data set as NNS and NN, we find our estimated expected durations to be smaller than those of NNS and NN.
5.
Conclusion
The paper offers a model derived from ‘job search theory’ which is concerned with the optimal behaviour of an unemployed individual who is searching for a suitable job in the labour market.
An approximation we make about the actual behaviour enables us to model this behaviour in a dynamic framework    without complicating the estimation.
Various effects, concerning offer probabilities, leisure and other variables, etc. can be distinguished.
Conditional probabilities of leaving the spell (hazard) are estimated in an unrestricted manner, allowing different individuals in the sample to have different pattern of hazard.
The preferred model, which allows individuals to treat sources of income receipts other than benefits and earnings differently, gave very good results, and passed both a statistical test and a theory check for consistency.
The maximised value of the log likelihood function is higher compared to the two previous papers (NNS and NN) which used the same data but different models to explain unemployment duration.
We find that:(i) there is disutility from being unemployed, with individuals disliking unemployment even more after the first three months;(ii) unemployment benefits have no effect on the behaviour of the unemployed individuals after the first three months;(iii) income receipts other than unemployment benefits and earnings have only a very small effect;(iv) the conditional probability of leaving the unemployment spell shows no sign of decreasing with duration for most of the members of our sample, while 50% of the teenage men have an increasing hazard;(v) the effects of unemployment benefits on the expected duration varies with age; the average elasticity of 0.12 is much smaller than those estimated by NNS and NN, who used the same data set but different models.
The elasticity of expected duration with respect to unemployment benefits is 0.18 for teenage men, around 0.14 for men aged 20–44, 0.08 for men aged 45–54 and 0.06 for men over 55;(vi) the elasticity of hazard rate with respect to unemployment benefits is estimated to be -1.19, which is nearly double that of NNS.
University of Warwick     
REPUTATION, MEMBERSHIP AND WAGES IN AN OPEN SHOP TRADE UNION
By ALISON L. BOOTH and MONOJIT CHATTERJI
1.
Introduction
MOST microeconomic models of the trade union have focused on wage and employment determination within a closed or union shop framework.
In these models, only union members are assumed to be eligible for employment in the union sector (for interesting surveys see Oswald, 1985; Pencavel, 1985; Farber, 1986; Ulph and Ulph, 1990).
The level of membership is usually fixed, or changes pari passu with employment, so that long run density is fixed (Kidd and Oswald, 1987).
However, although the closed shop is a feature of some unions, it is not a universal phenomenon.
In Britain, closed shop arrangements currently extend to just 20% of all employees, under half of all union members.
Furthermore, legislation of the Thatcher government has increasingly restricted the operation of the closed shop.
There are no longer legal immunities for industrial action aiming to establish or maintain closed shop practices, nor for employer coercion tactics compelling workers to unionize.
In the USA there is a long-standing controversy about the effect of ‘right-to-work’(RTW) laws, which effectively permit only open shop unions and are in effect in some 20 states.
The debate centres on the effectiveness of these laws in emasculating unions (via a free rider effect that allegedly makes union organization costly), thus eroding union wage differentials and encouraging investment.
In a comprehensive survey, Moore and Newman (1985) note that the consensus appears to be that the free rider effect of RTW laws is negligible in terms of the impact on unionization.
However, most of the literature does not satisfactorily account for the simultaneous determination of wages and membership while allowing for the interaction between the free rider problem and workers' tastes.
Thus some of the results are difficult to interpret.
For example, Linneman, Wachter, and Carter (1990) state: ‘The large decline in unionization in the face of a stable wage premium apparently challenges any microeconomic  explanation…
’ Our approach uses a microeconomic model to offer some insights in this regard.
We suggest that the mere passage of RTW laws will not eradicate unions that are already established: they will continue to extract a rent associated with the wage differential.
However, if there are substantial negative demand shocks, even established unions may disintegrate.
Also new unions will find it harder to become established.
Finally, if RTW laws result in greater dispersion of worker attitudes to joining unions, our model predicts lower membership but not necessarily lower wages.
This is consistent with empirical evidence reported in some studies.
Most formal models of trade union wage-setting behaviour are unable to answer questions about membership for the simple reason that they assume membership is fixed at 100%.
This is not a criticism of such models, for they have been constructed to focus on important issues of wage and employment determination.
But it does emphasize that they must be extended to allow for membership to be  endogenous in order to deal with some of the questions raised.
Early models of both endogenous membership and wage determination are unable to deal adequately with these questions, since they explain density changes in a world dominated by closed shop unions (see, for example, Grossman, 1983; Booth, 1984b).
The innovation of the present paper is that it provides a formal model of union wage and membership determination where membership of the union is not compulsory.
A prediction of the model is that there is a minimum critical level of membership density below which union existence is not viable.
In established unionized firms, provided there are no exogenous shocks shifting equilibrium wages and membership below the critical level, then outlawing the closed shop will not eliminate the union nor reduce wages to the competitive level.
However, in new firms, or in firms in which there is not an established union, the union may not form unless it can provide an excludable good or agency service that is increasing in membership.
Even then, the potential union will need to achieve a minimum critical membership to survive.
Any theory of an open shop trade union must deal with the following conceptual problem.
If membership is voluntary, if the union-set wage is a public good applying to all workers in the sector irrespective of union status, and if there are positive costs to membership, why do workers join the union instead of taking a free ride?
One answer is that unions also provide other goods which are private to the extent that they are available only to union members.
These can take the form of unemployment and sickness benefits, grievance procedures, legal and pensions advice, or reputation from complying with the group norm of membership (see Olson, 1965; Booth, 1985; Denny, 1988; Naylor, 1989, 1990; Naylor and Cripps, 1989).
It is these private goods, available only through union membership, which may provide the incentive to join for potential members.
In this paper we explicitly consider sociological factors, namely social custom, and the notion of individual commitment.
In the private goods case, examination of individual's motivation — be it based on egoism, sympathy, or commitment — is essentially irrelevant, since economics is concerned with final registered demands.
But in the public goods case, it may be fruitful to allow for the possibility that individuals may be motivated by commitment (Johansen, 1977; Sen, 1979; Margolis, 1982).
We formally allow for the possibility that, in certain sectors or establishments, there may be a social custom of membership, sanctioned by a loss of reputation if the custom is not followed.
The industrial relations literature has long noted that high levels of membership are ‘predominantly sustained by informal group pressures from workmates’(Brown and Wadhwani, 1990, p. 14).
In this paper, we follow this idea first formally set out in the economics literature by Akerlof (1980) in the context of a fair wage, and extended to trade unions by Booth (1984a, 1985), Naylor (1989, 1990), and Naylor and Cripps (1989).
We follow Booth (1984a) in allowing for individuals to be differentiated by their degree of commitment to the union movement, and we also follow Naylor (1989, 1990) and Naylor and Cripps (1989) in allowing for individual's utility gain from following the social custom to be moderated according to their degree of commitment to the union movement.
In particular, we use an extension of the membership equation of Naylor (1989, 1990) and Naylor and Cripps (1989).
However, in all these papers, wage determination was assumed to be exogenous.
We therefore explicitly model the union wage-setting process in a median voter framework.
The assumptions of the model are described in Section 2 of the paper.
The model is built up incrementally in Sections 3 and 4.
In Section 3 union membership is analysed, and a membership demand curve (relating membership to each level of wages) is developed.
Section 4 discusses union wage setting, and develops a wage demand curve for each level of membership.
The simultaneous interaction of the membership demand curve and the wage demand curve determines equilibrium wages, membership, and employment.
In Section 5 some comparative static predictions of the model are examined, with interesting predictions for wages, membership, and employment.
The final section draws some conclusions.
2.
Assumptions
Assumption 1: the trade union
Suppose there is a single trade union, representing workers employed in a number of perfectly competitive firms in a particular industry.
The union sets the level of wages unilaterally, given the  demand for labour by the firm.
While this is inefficient from the viewpoint of both the union and the firm, in that both could be made better off by shifting onto the contract curve, there is some justification in using a model where the union determines wages while the firm determines employment (see Oswald and Turnbull, 1985).
The union-set wage is assumed to cover all workers in the union sector, irrespective of their membership status.
Assume further that the union comprises an elected executive which, in order to be re-elected, maximizes the expected utility of the median voter.
Workers have identical skills, which are not lost during spells of unemployment (but they are differentiated by their level of commitment, discussed in Assumption 5 below).
The pool of workers in the sector is assumed fixed at P. This might be because the employment contract is a repeated game, so that workers remain in the sector in anticipation of future employment, or it could be because there are high costs to leaving the sector.
Unemployed workers may take alternative jobs elsewhere, but will not permanently leave the sector.
Assumption 2: labour demand
When firms are faced with the wage level w set by the union, they demand N(w) workers.
Labour demand is denoted generally by  where N denotes the number of workers demanded, w is the wage rate, and the subscripts are the usual notation for the first and second order derivatives.
Firms select workers at random from the pool of union and non-union workers irrespective of their union status.
Assumptions 1 and 2 embody our characterization of the open shop union.
Assumption 3: costs of union membership
Workers who join the trade union pay a per capita subscription cost a.
Only employed members pay these costs.
The net wage received by an employed member is therefore ().
Assumption 4: workers' utility functions
All workers are assumed to have continuous twice differentiable (indirect) utility of income functions, denoted by  where  denotes the net wage, which is given by w for non-unionists and by  for paid-up unionists.
Suppose that the union lifts the level of wages above the perfectly competitive market clearing wage, thus creating some unemployment.
The utility that would have been obtained with certainty prior to the union setting its monopoly wage rate must now be modified to take into account the uncertainty that is introduced with the unemployment probability.
The probability of any worker being employed, regardless of that worker's union status, is .
If a union member is not employed, the subscription fee is refunded and the worker may choose to leave the union.
An unemployed worker receives Β, representing alternative sector wages or unemployment benefits.
Β is constant across all workers.
Assumption 5: reputation and commitment
Suppose that the total utility v of a union member in employment can be represented by the following additively separable function: where r denotes reputation from membership, and the superscript i denotes the individual.
Reputation is assumed to vary across individuals, thus  where M denotes union membership, and .
Given the normalization of , M is both the size and the density of the union.
Reputation is increasing in M to represent the assumption that the larger the fraction of the sector unionized, the higher the individual's reputation within the group from conforming to the custom.
We assume in (4) that the gains to be had from following the custom of membership are modified by an individual's level of commitment to unionism, or evaluation of the reputation from unionism, given by .
Individuals are differentiated only by their level of commitment, where the density function is given by  and the distribution function  on .
In particular, we assume for simplicity that  is uniform.
Thus 
The results of the model developed in the following sections are not specific to the particular type of private good we have chosen, reputation.
Any private union good or service whose level is increasing in membership generates similar predictions.
l
3.
The union membership demand curve
This section represents a development of the membership equation of Naylor (1989, 1990) and Naylor and Cripps (1989).
However, we differ from this work by allowing wages to be determined endogenously, as will be made clear in Section 4.
From Assumptions 1–5 in Section 2, the expected utility of a union member can be written as  Here  represents the employment probability and  represents the probability of being unemployed, where workers are selected at random from the entire pool of workers in the sector (because the closed shop is not  legal).
The expected utility of a non-member can be written as  The ith worker joins the union if and only if , that is, where  Thus the individual joins when his or her evaluation of the reputation gain from complying with the custom of membership exceeds the utility loss from paying union dues.
Clearly workers with a higher value of  ta; are more likely to join.
The marginal member will be characterized by a δ or valuation of the reputation gain that makes her just indifferent, that is, where  where the marginal member's evaluation or commitment is denoted by .
Equivalently, The identity of the marginal member clearly depends on the distribution of  across the sector.
For any distribution, we can write generally  For our particular distribution of δ, assumed uniform on [0, 1], equation (10) can be rewritten and rearranged as: Insertion of (11) into (9) yields  Equation (12) shows the marginal member's evaluation of compliance with the norm of union membership associated with a union of size M. To find pairs of (w, M) at which the marginal member is indifferent between joining and abstaining, totally differentiate (12) to obtain the membership demand curve, denoted by MD: where the denominator is negative by strict concavity of .
Thus sign   ; the membership demand curve is negatively sloped for  and positively sloped for , as shown in Fig. 1.
For pairs of (w, M) above the curve membership will increase, while for (w, M) combinations below the curve membership will decline, as shown by the arrows.
The intuition underlying this is as follows.
Consider point A above the Membership Demand (MD) curve in Fig. 1.
Let the corresponding point vertically below A and on the MD curve be denoted by B, and the associated level of membership by M. The wage at A is higher than the wage at B, and u (.)
is strictly concave.
Thus it follows that the utility loss from union dues,, is decreasing in w, and therefore is lower at A than at B. At A, wages are sufficiently high that those workers whose commitment was too low to joint the union at a wage level of w are now willing to join, attracted by the relatively smaller utility loss from union dues.
At A, M will thus increase beyond M, as shown by the arrow.
Analogously, membership declines below the MD curve: with a fall in wages for a given  ta;, the utility loss from paying union dues increases, and membership therefore declines.
The arrows show that membership is stable in the interval [0.5, 1].
This suggests a minimum critical size for M in the context of the model of one-half of the sector size.
Thus the relevant part of the MD curve is the upward sloping segment from , where  is illustrated in Fig. 1.
The minimum critical union size implied by the membership equation is sensitive to two of the underlying assumptions: attitudes to risk and the specification of union dues.
Changes in each of these assumptions change the domain over which M is stable.
If we restrict the analysis to the plausible assumption of risk-averse workers then, if union fees are constant as in  Assumption 3, stable membership is .
But if union fees are a proportion of income, then stable .
However, union fees are almost invariably set as a constant amount of earnings rather than a proportion.
We therefore continue with the MD curve as illustrated in Fig. 1.
4.
The union-set wave demand curve
So far we have examined the union membership decision treating wages as an exogenously given parameter.
In this section, the union's wage setting behaviour is explicitly modelled.
The wage is set by the union executive to maximize the expected utility of the median voter.
The median voter model of social choice theory predicts that union officials will maximize the expected utility of the median voter in order to be re-elected under the following sufficient, but not necessary, assumptions.
These assumptions are that there is majority voting, members vote sincerely, there is a single decision, individuals differ in only one characteristic at a time, and preferences are single-peaked (see Atkinson and Stiglitz, 1980; Blair and Crawford, 1984).
Because of the single decision, the union is able to choose only wages (Blair and Crawford, 1984, p. 550).
Recall that, although workers in the sector possess identical skills, they have varying levels of commitment to the union .
Once we allow for different preferences for w due to variations in delta;, then one specific individual member will be the median voter for each level of membership.
Associated with the marginal member characterized by , there will be a median member characterized by  where the superscript M denotes marginal member, and the superscript m denotes median member.
The union executive maximizes the expected utility V of the median member with respect to w, taking M as given: The first order condition is  The second order conditions are satisfied by virtue of .
Rearrangement of (16) and multiplication through by w yields   where  the wage elasticity of labour demand.
In equilibrium, the percentage increase in the median voter's utility due to the percentage increase in wages is exactly equal to the percentage reduction in employment due to the percentage wage increase.
In other words, the left-hand side of (17) shows the marginal benefit to the median member from a 1% increase in the wage rate, while the right-hand side of (17) shows the marginal cost, arising because higher wages reduce employment.
Intuitively, any wage increase reduced employment and all workers therefore face a greater probability of being unemployed.
On the other hand, any workers lucky enough to be employed receive a higher wage.
We now derive the union wage demand curve in (w, M) space.
First, note that the ‘reputation-commitment’ term  in (16) and (17) can be expressed entirely in terms of M, since substitution of equations (14) and (11) into this term yields  Substitution of (18) into the first order condition (16) yields  Now totally differentiate (19) with respect to w and M to obtain the slope of the union wage function (or ‘wage demand curve’) in (w, M) space.
Thus  and  since .
The wage demand (WD) curve is illustrated in Fig. 2, where the MD curve derived in Section 3 is also shown.
Provided the union set wage is at least as great as , equilibrium wages and membership are given by the intersection of the WD and MD curves at 
5.
Comparative statics
Now consider how the equilibrium in the unionized sector is affected by various exogenous shocks.
We focus on four possibilities, and summarize the empirical implications at the end of this section.
5.1.
An increase in Β
Suppose there is an exogenous increase in alternative sector wages or unemployment benefits Β.
This affects only union preferences, and not the MD curve (see equation 8).
To see the impact of an increase in Β on the union-set wave, totally differentiate (19) with respect to w and Β.
This yields  since .
Thus, for given M, a small increase in Β will cause the union wage demand function to shift up.
There is therefore an unambiguous increase in both wages and membership, as illustrated in Fig. 3.
However, employment declines, since .
The intuition for this result is as follows.
An increase in Β is associated with a lower disutility from unemployment in the unionized sector, and hence the union executive is able to put more emphasis on its goal of higher wages since the costs of unemployment are now lower.
Thus wages increase for a given M. As a result, there is an induced increase in membership up the membership demand curve in response to higher union wages.
(Recall that with higher wages, there is a relatively smaller utility loss from union dues.)
The new equilibrium level of wages and membership are given by  and  in Fig. 3.
Conversely, a fall in Β causes the WD curve to shift down.
With initial equilibrium at  and , a large fall in Β may result in the wage falling below the critical minimum required for union existence.
If so, the union collapses and wages fall to the competitive level.
This comparative static result suggests that in  empirical work we would expect to observe a positive correlation between alternative wages or unemployment benefits, and union wages and membership, but also possibly a sharp discontinuity in this relationship.
Although the positive relationship between Β and w is not novel or surprising, the possible discontinuity in this relationship, as well as the positive relationship between membership and wages, is novel.
5.2.
An industry-specific demand shock
Suppose an industry-specific demand shock affects the firm's demand for labour as follows: where θ represents the exogenous shock.
The union MD curve is unaffected (see equation 8), but the wage setting curve WD will shift.
To examine this shift, we return to the first order condition, equation (16), which can now be rewritten as  The term in square brackets is positive for the first order condition for a maximum to hold.
Total differentiation of (24) with respect to w and θ yields The sign of (25) is given by the sign of , where 
A sufficient (but not necessary) condition for equilibrium wages and membership to increase when industry-specific demand increases is that .
This guarantees, from (26), that , and hence, from (25), the WD schedule unambiguously rises, increases  and  as shown in Fig. 3.
A necessary and sufficient condition for  is
This is obtained by substituting out terms in (26) from (24) and multiplication of the result through by .
Since (27) shows the necessary and sufficient condition for , it also shows the condition under which  for given M. If this condition is satisfied, then WD shifts upwards  and equilibrium wages and membership increase as shown in Fig. 31 7.
If (27) is not satisfied, a rise in θ will unambiguously pull WD downwards, so that wages and membership will decline.
Either way, a robust prediction is that wages and membership will move together in response to industry-specific shocks.
To summarize, a positive industry-specific demand shock raises (reduces) equilibrium union membership and wages if the elasticity of the slope of the labour demand curve Nw with respect to the demand shock θ is less than (greater than) the elasticity of employment with respect to the demand shock.
However, if the demand shock is iso-elastic (as it would be if the firm's production function were Cobb-Douglas), both the MD and the WD curves are unaffected.
Wages and membership will be sticky in response to iso-elastic demand shifts, while employment bears the brunt of the adjustment.18
5.3.
An economy-wide demand shock
Suppose that an economy-wide demand shock affects the unionized sector through two channels: through its impact on alternative opportunities Β, and through its impact on θ, the demand for output produced by the unionized sector.
For a positive demand shock, this means that we can consider the two comparative statics results in Section 5.1 and 5.2.
Equilibrium w and M will increase so long as inequality (27) holds.
Should this inequality be reversed, the outcome will be ambiguous, since the positive impact on w and M through the increase in Β may be outweighed by the negative impact of an increase in θ.
However, if there is little labour mobility, the result in Section 5.2 will hold: employment will move procyclically while wages and membership will be sticky.
5.4 A change in the distribution of commitment
The trade union legislation introduced in Britain in the past decade has enabled the expression of a variety of worker attitudes to unionization, because it has ensured that workers cannot be coerced into a union and it has also increased union democracy.
Since these legislative changes have arguably  enabled the expression of more heterogeneous views, we proxy this change by an increase in the mean-preserving spreading of , the individual evaluation of any reputation gain from union membership.
We modify Assumption 5 in the following way to take this into account.
Assumption 6
Suppose that  is uniformly distributed on the interval  where  and .
A decrease in µ represents an increase in polarization, since the range  is increasing as µ becomes smaller.
The distribution of Assumption 5 is a special case of Assumption 6 where  and where polarization is greatest because the interval  is at its maximum width.
In the following analysis we will focus on the situation where µ declines, that is, where polarization increases.
But first it is necessary to examine how the basic model developed in Sections 3 and 4 alters as a result of the changed distributional assumption.
Both the MD demand and WD curves (equations 9 and 16 respectively) will be affected by the changed distribution of commitment.
How will the membership demand equation be affected?
Note that equation (10) becomes  and from  Substitution of (29) into (9) yields the new membership demand curve  Total differentiation of (30) with respect to w and M gives the slope of MD  where the denominator is negative by strict concavity of .
It can be shown that, as µ: decreases (polarization increases), the MD curve shifts upwards, as illustrated in Fig. 4.
Let the numerator of (31) be denoted by  Thus  Qualitatively this result is similar to (13), which is a special case with .
From (32) the minimum critical size of union M c is given by    which is bigger than ½.
From (30), we also know that w c (the critical value of w which just supports M c ) solves
Thus when , the minimum critical wage is lower but the minimum critical density is higher than the case where µ = 0.
This is illustrated in Fig. 4.
As µ decreases (polarization increases) MD shifts upwards: for a given w, membership declines.
What happens to the union wage demand curve as polarization increases (µ declines)?
The first order condition (16) representing the WD curve is affected by the altered distribution of  only through the median member's ‘reputation-commitment’ term,.
Equation (14) becomes  and from (29) Let .
Then  since .
Also  Substitution of (38) into the first order condition (16) yields  which is negative since 
To see the impact of decreased polarization (an increase in µ) on the WD curve, we find  since 
Therefore a fall in µ (increased polarization) causes the WD curve to shift down for  and to remain unaltered for .
We now consider the combined effects of increased polarization on both the WD and MD curves.
First consider the case where .
Since the WD curve shifts down and the MD curve shifts up, equilibrium M falls.
But the impact on wages is ambiguous (provided membership does not shrink below the critical level).
This is illustrated in Fig. 5.
The intuition is as follows: for given w, increased polarization (fall in µ) reduces membership (MD curve shifts left), because some workers now have a lower  ta; and therefore abstain from membership.
The union may then lower wages, which has an additional negative effect on membership (since the utility loss from paying union dues becomes relatively larger).
Only if there is a very large shifts in the MD curve will wages rise.
Secondly, consider the case where initially .
The WD curve is unaltered as µ falls, but the MD curve shifts leftwards.
In this case, increased polarization causes M to decline and w to increase unambiguously.
Over the past decade in Britain with increasing polarization we have indeed seen membership declining, but the rising real wage that has been observed may be accounted for by higher productivity.
In summary, increased polarization in workers' commitment to the labour movement is associated with a fall in membership, but wage changes are ambiguous.
Even where the WD curve is unaffected (initial ) membership will fall in the wake of increased polarization.
5.5 Empirical implications of the model
The comparative static results can be summarized as follows.
(i) Improvements in alternative opportunities Β will lead to an increase in union wages.
Employment will decline, but union membership will increase.
This contrasts with the orthodox monopoly union model, where membership is unaffected by wage changes.
(ii) Iso-elastic industry-specific demand shocks have no effect on union wages or membership, but employment will be affected.
However, in the less restrictive situation where the firm's revenue function is not iso-elastic, a sufficient condition for union wages and membership to move procyclically is that 
(iii) Combining (i) and (ii), we note that, if there is little labour mobility, in a business cycle employment will move pro-cyclically, and wages and membership will tend to be relatively stable.
(iv) If workers in a particular industry become more polarized in their commitment to the labour movement, membership will fall unambiguously but the wage change may go in either direction.
What are the empirically testable implications of our model?
First, union wages and membership will be simultaneously determined.
While much empirical research in the area has proceeded on the assumption that this is the case (see, for example, Schmidt and Strauss, 1976), this work has lacked any microeconomic foundations.
Secondly, some variables such as alternative opportunities Β, will affect wages directly because they affect union preferences.
But the impact on membership will be indirect, because membership does not depend directly on alternative opportunities, although it does depend on wages.
Thus a regression of membership on Β and wages should find that Β has an insignificant effect.
Similarly, employment will not be affected directly by alternative opportunities, but will be affected only by the impact of these on the wage rate.
Another testable restriction is that some variables will affect only the membership and wage equations directly.
Thus union subscriptions α affect membership and wages directly (see equations 14 and 18), but will affect  employment only indirectly as wages alter.
These restrictions offer a way of identifying union wages, membership and employment equations in a simultaneous equation framework.
6.
Conclusion
With the operation of ‘right-to-work’ laws, it is not always appropriate to model the trade union as if a closed shop operates.
Once the closed shop assumption is removed, then it is necessary to explain why workers unionize in the absence of coercion, and to embed membership determination into a model of wage and employment determination.
In this paper, individuals are assumed to be characterized by varying degrees of commitment to the trade union, and the union offers a form of private good to its members in addition to negotiating the collective good of higher wages.
Although the private good is assumed to be reputation from complying with an established custom of membership, it could also be regarded as any other form of excludable union benefit that is increasing in membership.
A prediction of the model is that the union is viable only after membership has achieved a certain critical density, and wages are at a level to support this.
However, if there are adverse exogenous shocks to the established unionized sector (such as a worsening of outside opportunities, an adverse demand shock, or increasing polarization in attitudes to the trade union movement), then equilibrium wages and membership may fall below the critical level, and the union vanish from the sector.
In summary, the model predicts that, as a consequence of legislation abolishing the closed shop, adverse shocks may drive membership to zero and wages to the competitive level.
Herein lies the crucial difference between open shop and closed shop monopoly union models.
In the former, shocks to unionized sector can drive membership to zero and wages to the competitive level.
In the latter, shocks to the sector do not affect union existence, and the wage is always above the competitive level.
The model developed in the paper also pinpoints the conditions required for union existence in the absence of coercion.
Unless the union can provide an excludable benefit, or unless the majority of workers in the sector are committed to the trade union movement, then union existence cannot be guaranteed.
REPUTATION EFFECTS WITH IMPERFECT MONITORING IN LINEAR QUADRATIC MODELS
By MARTIN CRIPPS
1.
Introduction
HERE we attempt to show that linear quadratic gaussian models and the reputation effects of game theory can be linked.
This provides a tractable framework for the economic modelling of reputation effects.
This approach needs very little mathematical sophistication apart from some calculus and some knowledge of the normal distribution.
We focus on the reputation models introduced by Kreps and Wilson (1982), Milgrom and Roberts (1982) and applied in many well known papers; for example Backus and Driffill (1985), Benabou and Laroque (1988).
There has been work on imperfect monitoring in reputational models, Benabou and Laroque (1988), Fudenberg and Levine (1988, 1989); and there has also been work on models with a continuum of types, Milgrom and Roberts (1982).
We show how reputation results can be established in games where; type and action spaces are the real line, there is imperfect monitoring and all the random variables are normally distributed.
The results presented characterize a sequential equilibrium strategy of the informed agent and establish the convergence to full information in infinite horizon games.
We argue that for any positive discount rate reputation effects will be temporary in infinite horizon games.
We show that the length of these temporary reputations will increase as discount rates tend to zero and become permanent only in the limit.
We also show that the lower bound on payoffs to Nash equilibrium, derived by Fudenberg and Levine, applies in our model although there is always revelation of a player's type.
Fudenberg and Levine (1988,1989) have established that in an infinite horizon reputation game, as the discount factor tends to unity, any type's payoff at a Nash equilibrium is bounded below by the payoff to masquerading as a dominant strategy type.
This result holds in the model below.
The condition required for the Fudenberg and Levine (1988) conclusion with imperfect monitoring is that the dominant strategy type receives strictly positive probability in the priors of the short term agents, or that with positive probability there exist a type which plays the action in equilibrium.
This is not true in our model as there is only one type (in the continuum) with a dominant strategy.
There is, however, a strictly positive probability that the long term player's type lies in an arbitrarily small interval around the dominant strategy type.
Thus it appears that the Fudenberg and Levine result persists in the limit.
If dominant strategy types  are surrounded by a neighbourhood of similar (but non-dominant strategy types) the lower bound on equilibrium payoffs survives.
The behaviour at a sequential equilibrium of our reputation-type game differs in two ways from the process of reputation building and sudden destruction exhibited in Kreps and Wilson (1982), Milgrom and Roberts (1982a, 1982b), Backus and Driffill (1985).
First, the results below show that there is always smooth convergence of play to position where the long term player's type is revealed.
The models of Fudenberg and Levine and Benabou and Laroque show that in games with imperfect monitoring the process of reputation building and destruction is considerably smoother than in games without such noise.
It is clear that it is the presence of imperfect information transmission that smooths the abrupt deterioration in reputation.
The second distinction between the results here and those of Kreps and Wilson is the behaviour of play in the infinite time horizon case.
Here the long-term player's type is revealed when play continues indefinitely.
This differs from the results of Kreps and Wilson, which show that players acquire a reputation and never lose it, by masquerading as a dominant strategy type.
One simple explanation for the absence of a long term reputation building in our model is that the dominant strategy type is of measure zero in the priors, so it is impossible for other types to credibly acquire a reputation.
This explanation is suspect, because the behaviour of other types in the early periods does appear to be an attempt to imitate the dominant strategy type; there is an attempt at reputation building.
There has been some debate on how the form of the set of types may affect the type of outcomes sustainable as a sequential equilibrium in a reputation game, see for example Vickers (1986).
In particular it has been noted that agents have incentives to take actions which reveal their type early on in the play; to self-screen.
This incentive to screen will not operate effectively if the strategy sets of agents are very limited; different types will select the same actions if constrained in their choice.
In our model strategy sets are the continuum and there are a continuum of types, this provides much greater opportunities for self-screening by different types and the tendency to do this is much larger.
Self-screening will also explain why it is impossible for types to permanently masquerade as the dominant strategy type, but it is possible to gain a short-term reputation.
The imperfect monitoring of actions means that it takes time for types to self-screen, so they can enjoy a temporary reputation while this is going on.
The pressure to self-screen can also provide an explanation for the results of Benabou and Laroque; there are two types but actions are a continuum.
In spite of a positive measure for both types in this case the masquerade is not permanent and in infinite time there is full revelation of types.
The introduction of noise in strategies ensures that every information set in the game is reached with a non-zero probability, this eliminates the need to describe off-the-equilibrium-path beliefs at the sequential equilibrium found below.
There is no source of multiple equilibria from different specifications of these beliefs.
One could view the introduction of noise into strategies as forcing agents to play a particular form of mixed strategy or of imposing a particular  sort of tremble, which is, of course, undesirable.
The presence of noise, however, seems natural for many economic environments where there is a discrepancy between planned and actual events.
The sequential equilibrium strategy of the long-term agent has a particularly simple form in this model.
The action chosen by the long-lived agent in each period is a linear function of its type.
The strategy is not responsive to past shocks or noise; the actions selected in any period are the same whatever the past history of shocks.
So it is possible that the short-run agents in the model over estimate the long term agent's type after a particular sequence of shocks and subsequently revise this estimate downwards.
Thus the beliefs, or assessments, of the short term players are not revised monotonically, but nevertheless converge to full  information on the type, as the long term player's strategy converges to its limiting levels.
Section 2 below outlines the class of models we consider and examines the properties of the game when the type is common knowledge.
In Section 3 we define a sequential equilibrium for this game and in Section 4 we calculate a reputational equilibrium.
In Section 5 we show how the structure developed here can be applied to two economic problems, one of limit pricing, the other from the optimal policy literature.
2.
The model
Player (A) lives forever and maximizes the discounted sum of period by period payoffs.
Player () has a one period life and receives its payoff at the end of its period of existence.
Player A plays against a sequence of ; one in each period.
The  have no incentive to choose actions to affect the rate of information acquisition from A.
The possible types of player A are parameterized by .
The prior beliefs of the  on z are described by a normal distribution , this assumption is necessary to enable a signal-extraction approach to the revision of priors.
Player A observes its type before play begins and z enters as a parameter in A's preferences.
The value z is unobservable by B. The period t payoffs to player A and  are quadratic and are defined below
(We assume that; for concavity).
Here  represent respectively A's action, B t action and the state variable in period t.
The actions are subject to random error terms  each period, which are normal, independent and identically distributed through time with distributions  and   respectively.
The players have a noisy control over their actions, this means B t is unable to monitor A's actions precisely as it can only observe the realization .
The noise in B t 's strategy is not essential to the results but is included for generality.
Player A's payoff each period is almost linear, but does have a quadratic term in the control variable which has a negative sign to preserve concavity.
It is not necessary to restrict A's preferences to only one quadratic term for much of the results to follow, but to solve for the time path of the optimal strategy this is necessary.
The player B t has preferences which are quadratic in the two controls.
They also have an arbitrary function of the state variable and A's control variable f (.)
as an additively separable term.
The state variable evolves according to the linear relationship.
At time  player A observes the value z and the initial value of the state variable is observed by both players.
In period  the players A and B respectively choose  and  simultaneously.
The values  and  are determined and then players receive their payoffs.
In each subsequent time period player A, and its opponent in that period B t , observe x, then choose  and  respectively.
The information available to both players at this point in time consists of the past history of the state variable and the sequences of observed disturbed actions .
This stage game is repeated a finite or an infinite number of times, then play ends.
Player A's payoffs accumulate as play progresses and the player B t of each period collects its final payoff at the end of the period in which it acts.
The dominant strategy type is parameterized by .
In this case (1) becomes independent of the state variable and B t 's action.
The type  payoff is maximized by setting  whatever the actions of the other agents; this is a dominant strategy.
All other types will in general prefer to vary their actions in response to the state of information of their opponents.
If z is common knowledge the game has a simple structure, the B t 's reaction function will be : player B t chooses  as a linear function of .
Player A's optimal choice must maximize the discounted sum of its payoffs.
Solving the appropriate dynamic programming problem gives 
(The term  is player A's discount factor.)
The first term is the optimal level of  in the one shot game, the second is the benefit from the effect  has on future state variables.
In the full information game the equilibrium behaviour is simple; type z chooses a constant action  in each period and player B correctly predicts this when choosing its action.
The play in the game of asymmetric information is less clear.
Player A's actions in period t affect B t 's behaviour tomorrow and hence the future payoffs of all types.
As play evolves and B t 's beliefs alter there is a shift in the optimal action for player A. Player A's actions in each period will thus vary as the state of B t 's information varies.
Player A will prefer low levels of  and high levels if .
Low levels of  can be achieved by encouraging player  B t to have respectively low (high) expectations for of  if,.
There are two possibilities we must consider: the first is that player A wants small values of; this is achieved by lower (higher) levels of  in the early stages of play if .
The second possibility is that player A wants higher  s and to achieve this the reverse policy is followed.
In the proofs below characterizing the sequential equilibrium of the reputation game these two separate possibilities will feature as the two cases of a proposition.
3.
Equilibrium
A sequential equilibrium in this model is now described.
Player A's information set consists of the pieces of information , and B'S information set consists of a value k t .
Let  denote the set of all possible h t 's and HB t denote the set of all possible k's.
A strategy for player A consists of a sequence of functions , which selects a value  for every possible history.
Similarly, a strategy for each B t is a sequence , which selects a value  for every possible history.
Let Σ denote the set of probability measures on the real line.
Then B t 's beliefs or assessments on A's type can be described by a function .
An equilibrium consists of a pair of strategies  and assessments , which result from the application of Bayesian revision of B t 's priors to the strategy  and the observable data , such that 
Here , represent expectations taken relative to A's information set at time t and  represents expectations relative to assessments .
Also,, denotes player A's subjective discount factor, which is used to weight future payoffs.
This definition ensures that the strategies , form a sequential equilibrium for the game outline above.
At each possible information set the strategy employed by a player is an optimal response to the opponent's strategy.
4.
The solution method
The method of solving for a sequential equilibrium in the game outlined in Section 2 proceeds in two stages.
The first step requires us to postulate a form for player A's equilibrium strategy and then deduce the nature of player B t 'S knowledge about the private information z, given its knowledge of A's strategy.
That is we describe how B t determines its assessments .
Once this is completed we will show that given the agent B'S form their beliefs about z in this way, then it is indeed optimal for player A to choose a strategy of the form first  postulated.
To describe the strategy in greater detail we must solve for the coefficients of such a strategy.
This approach to solving the model derives from the ‘method of undetermined coefficients’, which is widely used in the macroeconomics literature.
The approach applied here also has a much closer resemblance to that taken in Cukierman and Meltzer (1986).
Assume that the B t 's believe that ; for some fixed sequence  of real numbers.
Given the B's believe that A uses this strategy, we will show that it is optimal for A to use this strategy.
This is how we construct an equilibrium.
Note, we do not restrict player A to use a linear strategy.
We show that provided the B's believe that A follows a linear strategy, then, it is best for A to behave in this way even if it can choose any strategy.
This is sufficient for the solution here to be a sequential equilibrium.
If player A follows the strategy suggested above we can deduce that B t 's beliefs on A's type continue to be described by a normal distribution, because the B t 's will face a signal extraction problem when they are forming their expectations of z.
That is, they observe a sequence  and using this must estimate a value for z.
The mean of the distribution of beliefs on z is the only moment used by B t 's in deciding how to behave and we will now quote a result which shows that the mean of the distribution of beliefs is a linear function of past data.
This completes the description of player B t 's beliefs or assessments , if it believes that player A follows the linear strategy .
At equilibrium player B t maximizes its payoff given these assessments.
Take the first order condition for (2) and substitute for  from (4) B t 's behaviour is linear function of , which is a linear function of A's actions.
We now describe A's optimal response to B t 's behaviour and show that it is optimal for A to use the simple form of strategy first postulated.
Below we write down A's objective function at time s substituting the above expression for B's actions and solving for the state variable by repeated substitution.
A's optimal response to (5) must maximize the expression below.
At  any time s, given any past history h, player A selects current and future actions to maximize;
At every information set h, in the game player A will choose  to maximize (6).
A solution to this problem is given by the Euler equations, for example see Sargent (1979) Chapter XIV.
In effect we must differentiate (6) with respect to and set it equal to zero.
Notice that a lot of the disturbance terms m, n, vanish from the derivative by linearity, so current and past disturbances are irrelevant to the form of A's optimal response to (5).
The Euler equation is given below.
This equation has the same structure as that postulated for player A's strategy: the optimal  is linear in z.
We have therefore found an equilibrium provided we equate the coefficient of z in the equation above with C t and then solve for the sequence C t .
Assume an infinite time horizon and equate C s with the coefficient of z in (7).
The first term on the right is ω, the value Cs would take if z was common knowledge.
The second term is the effect on A's choice of strategy from the B t 's learning of z.
Now assume a finite horizon and no state variable, this gives an Euler equation  The solution to (6) is not characterized solely by the Euler conditions, but also  requires a transversality condition 
We have established that agent A's optimal response to (5) is linear.
Provided (9),(10) can be solved for a sequence C t we have an equilibrium.
This equilibrium strategy is not a consequence of any form of restriction on A's actions, the optimization problem (6) is completely general in the scope of actions it allows A. The form of the equilibrium is a consequence of the linearity of B t 's signal extraction problem and the restricted quadratic form of A's preferences.
Linearity eliminates many of the effects of noise and shocks: these simply shift the payoff function.
This is a very special result, which cannot be generally true.
In general the past history of shocks will have effects on the equilibrium strategy in such a game.
We can go no further in characterizing the strategy of player A without considering the case of an infinite and finite time horizon separately.
As the infinite time horizon case is simpler to treat we begin with this.
The following result solves (9) for the optimal strategy and shows how the sequence C t converges.
A sequential equilibrium for the infinite horizon game outlined in section (2) has now been found.
There exists a sequence of numbers C t  such that, if player B t believes that player A acts according to the rule , then it is optimal for the type z player A to follow this strategy at every information set.
This has been established by first showing that the optimal response to these beliefs is linear in the type and then finding the sequence C t  which is a fixed point in the map from beliefs to response.
The equilibrium strategy for all types of player A is described in this Proposition.
The most obvious feature of the strategy in its convergence to a limit .
This implies that the player A's type is slowly revealed to the B t 's , since the least squares projection (4) converges in probability to z if the Ct's are static.
It is not surprising to learn that the limit  is the Nash equilibrium strategy for player A when z is common knowledge.
(The game of complete  information is analyzed above.)
As information is slowly revealed, so the play in the game of incomplete information settles down to the equilibrium  of the game where z is common knowledge.
This is shows how the full information game is also the limit of play in a game where z is not known.
In this model player A acquires a temporary reputation as a dominant strategy type before its true value for z is learned by the B t 'S .
For example, if , the expected payoff for type converges to  but it could improve its payoff to  by masquerading as a z = 0 player.
The C t 's are an increasing positive sequence converging to  so in early periods players of all types imitate z = 0 by setting low values for , thus building a reputation.
As time passes C t increases to ω and this period of temporary imitation dwindles away as play tends to the full information limit.
Thus the process of convergence to the play in the full information game can be interpreted as a period of temporary reputation.
The reasons players are able to build a temporary reputation in the game stem from the way information is processed by the B t 's .
Early observations on at are very highly weighted in (4), so these will be important to the B t 's when they are forming their beliefs.
In the early periods player A's actions reveal a lot of information on its type, so it is quite credible for them to prefer to imitate the z = 0 type by reducing the absolute value of .
The imitation also ensures them a higher future payoff by reducing the expectations of the B t 's on the A's type.
This process of convergence to the full information steady state also reduces the rate at which the B t 's learn the type of the A's.
(Low values of at increase the relative size of the noise nt in the B t 's signal extraction problem.)
It therefore takes longer for the B's priors to converge to correct information on A's type, than it would take if the A's had always set .
Thus the sequence C t is selected to make learning of types much harder.
Why don't other types choose to permanently masquerade as z = 0 in this example?
It takes so long for another type to convince the B t 's that it is z = 0, that any long run benefit from this is outweighed by the cost, of acquiring such a reputation.
Players instead benefit from a short run reputation effect.
The answer to this question also explains why the bound derived by Fudenberg and Levine continues to hold.
As the discount factor tends to unity, so the equilibrium alters to make it harder to acquire a reputation as a z = 0 type, by extending the length of the temporary reputation which all types enjoy.
In the limit as  the temporary reputation lasts forever.
In the example above we examined the case ; the process of convergence to full information in the case  is similar.
In this class of games each type of player A prefers to masquerade as a z = 0 type, but in the full information game it selects a negative action,.
The path to the steady state starts with the building of a temporary reputation since C t is small in absolute value.
As time passes  grows and more information on A's type is revealed.
The cases where  or  are also similar in their path to full information, so we will examine these cases together.
Consider for example this gives .
In the  full information one shot game the payoff type z = 0's payoff is , whilst type z = 1 sets a, P = 1 and receives a payoff of .
When  or  there are no benefits to masquerading as a dominant strategy type.
Instead player A's payoff is increasing in the size of , so player A prefers to encourage the B t 's to believe that their type is large.
In the early periods of play in our example this is precisely what happens.
The sequence Ct is positive and decreases in size until it reaches the level ω.
Thus play begins with all types of player A imitating types with larger values of z and acquiring a temporary reputation as large z types.
As the B t 's learn the player's type so play converges to the full information game and the values of C t decline.
The large values for Ct in the early periods of play do speed the rate at which the B t 's learn from the signal extraction process (4).
This is in the interest of the A's, however, as the B t 's are temporarily being convinced that player A's type is higher than it actually is.
The path to the equilibrium in the game where  is the mirror image of the example.
Instead play is converging to a negative steady state.
The role of the noise is to make it more difficult for particular types to unambiguously signal their information.
It is therefore interesting to examine what happens as  tends to zero.
If  the points  shifts to the left, hence the values Ct will decrease in the infinite horizon case.
As a result there will indeed be a longer period where player B has less information on z.
Less noise therefore encourages player A to take more care in hiding its type.
The solution in the finite horizon case is not entirely general because of the endpoint problems which arise when attempting to solve the relation (7) for a sequence Ct.
If we entirely remove the dynamics from the model, then it is possible to generate a soluble relation for Ct and hence construct a linear convergent strategy.
Player A's strategy is linear in z for both the finite and the infinite horizon models.
Moreover, the stochastic events in the model have no effect on the terminal position in either case; the presence of noise or its magnitude do not alter the long run outcomes.
In the infinite horizon case players B learn the parameter z by solving the successive signal extraction problems posed in Proposition 1, so the noise plays no role here.
In the finite horizon case the terminal condition is independent of the stochastic structure.
There are important differences here because of the multiplicity of types, actions and the presence of noise.
There is a smooth process of convergence to full knowledge, unlike the sudden loss of reputation observed in many reputational models.
The equilibrium has a period of temporary reputation building but it always eventually collapses.
In some cases Player A does take actions to slow B's learning of z.
We now will show that the bound derived by Fudenberg and Levine does apply in this model with a continuum of types.
4.1.
Discounting and the length of reputations
We have calculated an equilibrium for the model set out above.
We will now investigate how the length of the temporary reputation changes as discount  factors tend to unity.
We can also verify whether as discount factors become large the expected payoff to all types of player A is bounded below by the payoff to masquerading as a dominant strategy type.
First take  in (12) below and rearrange 
This allows us to investigate the properties of the intersections of the map (13) below with the 45° line.
If , the intersection between the origin and ω approaches zero as  ta; tends to unity.
Whilst, for the cases  the intersection on the other side of a) becomes unboundedly large as δ tends to unity.
We can use these properties of the intersections to deduce the changes in the equilibrium strategy as δ tends to unity, because our method of constructing the solution (see Fig.2 in the Appendix) relies on the properties of these intersections.
In particular, as δ tends to unity the above implies that  approaches zero if  and  approaches infinity if .
Thus, in both pairs of cases we have the result that as discount rates shrink, so does the sequential equilibrium strategy approach the dominant strategy type's play.
As players become more far sighted play in all periods becomes closer to that of z = 0.
Nevertheless, in the repeated game of incomplete information we have shown that players' strategies converge to those they employ in the full information game.
Player A's expected payoff in each period will therefore tend to a value which is less than that received by the dominant strategy type for any positive rate of discount.
It is not worthwhile to permanently masquerade as the dominant strategy type, because it takes so long to acquire such a reputation.
As  ta; tends to unity the types become progressively more far sighted, so to ensure that it is still not worth engaging in the permanent imitation of z = 0 the benefits from following the equilibrium strategy are increased.
As δ increases the terms in the sequence C t  decrease, thus the period of temporary reputation grows longer.
The temporary reputation achieved by the other types lasts longer and is more profitable as the discount factor tends to unity.
In the limit the temporary reputation is permanent as C t either approaches zero, or becomes large, and all types successfully masquerade as the dominant strategy type.
Thus the bound produced by Fudenberg and Levine is correct in the limit too, because in the limit the payoffs are equivalent, but for any positive discount rate it is more profitable to have a permanent reputation.
5.
Possible applications
The class of games analyzed above seem to have a number of possible applications to economics.
The work on policy games and on entry deterrence are possible fields.
A variant of the model proposed in Cukierman and Meltzer (1986) can be included in our framework and hence the technique described above can be applied in this context, this is done in greater depth in Cripps  (1991).
Agent A is the government choosing its monetary policy, and agent B is the private sector, which attempts to form correct inflationary expectations.
The propensity of the government to benefit from inflationary surprises is the unknown parameter in the government's preferences.
Let  be the planned rate of money supply growth and  be the private sector's expected rate of planned money supply growth, then the standard form for preferences in this game are:
These fit exactly the preferences described in Backus and Driffill (1985).
Simple calculation reveals that  so that the optimal policy is a slow increase in the rate of growth of the money supply until the government's propensity to create inflation is revealed.
The reputation type analysis applied in the finite horizon models of Backus and Driffill generalizes to the case where the uncertainty is generated by a normal distribution and there is noisy information transmission.
The diversity of information and the size of the action sets generate a solution which is partially separating.
It is not possible for certain types to screen themselves fully, because the noise prevents this.
Instead there is smooth convergence to a position where the government's type is known.
Another possible application is to the field of  entry deterrence.
There are many results on reputation effects in this context, but these show that entry is always deterred if the time horizon is sufficiently long.
Let , represent the output of incumbent and entrant respectively, then natural restrictions on the  parameters would give .
If z describes market conditions which are only known to the incumbent, then the results above tell us that the entrants gradually get to learn the incumbent's private information.
The incumbent's output starts at a high level and gradually declines, whilst the entrants' output starts low and gradually increases the more it discovers about the market.
6.
Conclusion
We have established that reputation effects can be found in linear quadratic gaussian models.
Many of the results which are found to hold in the more familiar game theoretic environment will generalize to such a framework.
We show that the variety of the types together with the noise prevents a large jump in agents' strategies from complete ignorance to complete knowledge.
Moreover, the presence of noise considerably smooths the nature of the optimal strategy of the long lived agent, because it is now harder for agents to completely signal their types.
The presence of noise also allows agents to create temporary reputations in infinite horizon games.
They are eventually forced to abandon these reputations as their actions ultimately reveal their type to the short term players.
The continuum of actions does not undermine the lower bound on payoffs at Nash equilibria of repeated games derived by Fudenberg and Levine.
In fact as discount factors tend to unity so does the temporary reputation become permanent.
To sum up: the strong predictions obtained from earlier  models are tempered into gradual adjustment processes where reputation is only ever a temporary event.
The general form of payoff functions allows us to address dynamic models with asymmetric information and to investigate the reputation effects in these models.
DYNAMIC DECISION MAKING UNDER UNCERTAINTY: AN EXPERIMENTAL STUDY OF THE DYNAMIC COMPETITIVE FIRM
By JOHN D. HEY
1.
Introduction
OVER the past decade, experimental methods have been used to increasing effect to discover the empirical strengths and weaknesses of economic theories, and to supplement empirical evidence obtained using more conventional econometric methods.
Two broad areas of intense experimental investigation can be identified: at one extreme, the exploration of the validity of various theories of individual decision-making under exogenous and endogenous risk (games against nature and strategic games); at the other extreme, the investigation of various market structures and their convergence properties.
Good surveys of these two branches can be found in Machina (1987) and Plott (1989) respectively, while more general overviews can be found in Roth (1988) and Kagel and Roth (1992).
Apparently conflicting messages emerge from these two branches: the former branch seems to be saying that decision-makers do not appear to behave in accordance with the basic accepted paradigm (Subjective Expected Utility Theory and Game Theory); while the latter branch seems to suggest that market outcomes coincide with those which would be predicted by this accepted paradigm.
Unfortunately, this latter conclusion tends to be at odds with more conventional data which suggests that real markets often do not converge to what appear to be their equilibrium values; an obvious example is the labour market.
One possible resolution of the first paradox rests on the argument that the market equilibrium is driven by the marginal traders, who are SEU maximizers — the non-marginal possibly non-SEU agents being irrelevant.
Unfortunately, this does not shed much light on the second apparent paradox.
An alternative resolution of the first paradox is that the market environment punishes the (irrational?) non-SEU agents, in the sense that these agents rapidly learn from the market environment that non-SEU behaviour does not pay.
In addition, one could argue (though this might not meet with unanimous agreement within the experimental community) that repeated experiments with the same subjects indicate that conformity with SEU behaviour increases with repetition.
But again this does not help much with the second paradox.
This paper is motivated by an alternative line of argument.
If one looks at the majority of market experiments in which convergence to the market equilibrium is rapid, one sees that the decision-problem solved by the subjects in such experiments is relatively simple; recently, however, there have been a number of more complicated market experiments in which the subjects face  more complicated decision-problems.
In these, convergence to the competitive equilibrium is neither so swift nor so automatic; for example, bubbles and crashes are observed (see the chapter by Sunder in Kagel and Roth (1992)).
Complexity is also a feature of those real-life markets in which convergence also is neither swift nor automatic.
Many experiments carried out to date at the individual level shed little light on the problem of complexity — since they typically place the subjects in very simple choice situations.
Indeed, the trend in the recent past has been for such experiments to get simpler and simpler (more and more transparent) in terms of the decision-problems given to the subjects.
This paper is a deliberate attempt to move in the opposite direction — to explore the ability of subjects to tackle rather complex decision-problems.
Some experimental work has been done in this area, but not much; examples can be found in the papers by Becker and Huschens (1991), Kogut and Phillips (1991), Moon and Keasey (1991), Thistle, Battalio, and Kagel (1991), and Vossebein and Tietz (1990).
Economics and economists typically tend to ignore the issue of complexity; theory normally assumes that all economic agents can solve all decision problems easily and correctly.
The hypothesis that we begin to explore in this paper is the hypothesis that one of the reasons for non-convergence of real-life markets to the competitive equilibrium is that the agents in those markets are unable to solve the complex decision-problem facing them perfectly; their sub-optimal behaviour leads to sub-optimal outcomes.
Our research methodology is to proceed in stages; this paper does not present the whole story but just the first step in the argument.
Here, we concentrate on a partial equilibrium story and explore how economic agents tackle a rather difficult dynamic decision problem; in later work we shall embed this story in a market context, but this first step will enable us to isolate certain elements in the market story.
That is one of the strengths of the experimental method.
The main question that we wish to explore in this work is that of how decision-makers tackle reasonably complex dynamic decision-problems under risk: how do they take the risk into account?
How do they take the dynamics into account?
How do they cope with the complexity of the problem?
Do they behave in accordance with the (optimality) theory?
We use the dynamic competitive firm operating under spot price uncertainty as the vehicle for our analysis, though it should be emphasized that it is not the firm per se that interests us but, rather, the risk, the dynamics, the complexity, and the optimality.
To some economists this is problematical, particularly when we use student subjects, but it should not be considered so.
The real issue is whether or not our results have some general message.
We shall return to this in our conclusions.
2.
The experimental scenario
The economic theory that we use as the vehicle for our analysis is the familiar theory of the dynamic competitive firm operating under spot price  uncertainty.
This was chosen because it is familiar to economists, has a well-defined formulation and optimal solution, incorporates risk and dynamics and is reasonably complex.
More specifically, we consider a competitive firm, which takes its output price as given, which faces uncertainty about that price.
We assume (following the literature) that the firm knows the objective distribution of the output price, and the cost function it faces.
In order to make the problem a dynamic one we allow the firm to hold inventories (at a cost) thus giving it the facility of spreading some of the risk through stock-holding.
We make the problem a finite horizon problem (eight periods) since infinite horizon problems are difficult to reproduce in the laboratory and random horizon problems can be misunderstood by subjects (see Hey and Dardanoni, 1989).
Finally, in order to encourage the subjects to concentrate on profits we pay them their realized average profits.
In essence then, subjects were asked to manage a competitive firm facing spot price uncertainty for a total of eight periods, and were paid their average realized profit from so doing.
Full details can be found in Appendix 1 (containing the instructions to the subjects) and Appendix 2 (containing further detail on the experimental procedures used).
Subjects were invited by letter to participate in the experiment.
Two-thirds of the subjects were recruited on a group basis, through (self-appointed) group leaders.
The leaders, who were paid a small amount to compensate for their organizational work, recruited seven other team members; there were eight such teams.
In addition, there were 32 ‘individual’ participants, giving an overall total of 96 subjects.
We made no attempt to select particular kinds of subjects, since our main aim was to see how different kinds of subjects would carry out this decision problem, though we appreciate that in a market setting some of the less efficient decision-makers might be weeded out by the market process.
That, however, is a separate story.
We ran the experiment in 12 sessions of eight subjects each; eight of the sessions involved the eight groups, the remaining four sessions the subjects doing the experiment on an individual basis.
All subjects performed the experiment twice, separated by an interval of between three and five days.
In addition, the groups had a group discussion (organized by the team leader) between the first and second repetitions, though the subjects actually performed the experiment on an individual basis on each repetition.
The idea behind the repetition was to see if performance improved with repetition; the idea of the group discussion was to see if that improved performance.
We found that the former did, but the latter did not.
3.
The optimal strategy for a risk-neutral subject
As a benchmark, we consider the optimal strategy for a risk-neutral subject.
This is a subject who wishes to maximize his or her expected payment from participating in the experiment.
As an initial hypothesis, this seems reasonable: given the amounts of money involved in the experiment, and the fact that their payment was the total over the eight periods of the experiment, this would seem to be a reasonable approximation.
If, however, subjects optimized but were not risk-neutral (and hence maximized expected utility rather than expected profits), then their strategy would depart from that described below by an amount which depended on the degree of their risk-aversion or risk-loving.
Moreover, if they were not expected utility maximizers (but instead, say, behaved in accordance with Regret Theory or Prospect Theory), then their behaviour would depart yet further from the strategy described below.
Textbook economic theory (see Hey, 1985) specifies what this risk-neutral optimal strategy is.
It consists of two parts: an optimal output strategy and an optimal sales strategy:
Optimal output strategy — produce, in period t, output , given by: where C (.)
is the firm's cost function, and where P, * is a reservation value defined recursively by: where F (.)
is the distribution function of the (independently and identically distributed random variable) output price and is the (per unit) storage cost.
Given that, it is clear that the recursion is started by , where T is the final period.
where  is the level of the firm's (pre-production) stocks at the beginning of period t, and where the reservation price  is as defined in (2).
Recall that the firm must decide on output in t before the price in t is known, but decides its sales in t after the price in t is revealed.
The intuition behind this optimal strategy is straightforward: it is optimal to sell all or nothing — all when the current price is higher than the price that one can expect to get on future sales, nothing otherwise; it is optimal to produce at the point where marginal cost equals the expected price on sales.
Of course, the (conditional) expected price on sales differs from the (unconditional) mean price because sales are made only when the price is sufficiently high.
Inspection of these optimality conditions reveals that the reservation price  falls as the horizon T approaches, reaching zero in the final period (in which the optimal strategy is to sell everything, regardless of the (non-negative) price).
Likewise, the optimal output level falls as the horizon approaches, reaching the level at which marginal cost equals the (unconditional) mean price in the final period.
A quick glance at the data reveals that very few subjects, particularly on the first repetition, were behaving in this fashion; indeed, many subjects appeared to be behaving in a manner one might like to describe as myopic.
The most obviously myopic strategy is the one that ignores the dynamic aspects of the problem but instead treats it as a sequence of eight one-period problems.
This implies a constant level of output (at which marginal cost equals the mean price) and a sales strategy which would involve selling all the production irrespective of the price.
There are two aspects that distinguish the fully optimal dynamic strategy from this inferior myopic strategy:(i) the use of a reservation sales strategy; and (ii) the use of a declining output strategy and (associatedly) the use of a declining reservation price on the reservation sales strategy.
Our results suggest that most participants were fairly good at identifying (i) but rather poor at identifying (ii), particularly on the first repetition.
Finally, let us note the comparative static propositions of this theory.
These involve changes in:(i) the cost function, C (.);
(ii) the distribution function, F (.);
and (iii) the storage cost, k.
Of particular interest are the effects of those parameters which affect the essentially dynamic aspects of the problem: these are the uncertainty of the price distribution (measured by the riskiness of the distribution F (.))
and the cost of storage k.
As all our distributions were (truncated) normal distributions, this reduces the list of parameters of interest to τ (the standard deviation of the distribution) and k.
As is fairly obvious, an increase in τ or a decrease in k leads to an increase in the optimal output and an increase in the optimal reservation price in all periods except the last.
4.
Experimental details
We used eight different parameter pairs.
The table overleaf gives the details, as well as the expected profits for each parameter pair implied by the optimal and myopic strategies discussed above.
Note that the average payment following the optimal strategy is around three times greater than the average payment following the myopic strategy.
This was deliberate: it implies that the subjects had a strong incentive to try to optimize, and is an attempt to overcome the ‘flat-maximum critique’ of Harrison (1989).
To give all subjects roughly the same earnings opportunities over the two repetitions of the experiment, we changed the parameter pairs from the first to the second repetition of follows: Each group consisted of eight subjects: so there were eight who had parameter pair A on their first repetition and parameter pair H on their second; eight who had parameter pair B on their first repetition and parameter pair G on their second; and so on.
In addition, there were the subjects who performed the experiment on a completely individual basis, yet who were nevertheless organized in groups: so there a ‘group’ of four individual subjects who had parameter pair A on their first repetition and parameter pair H on their second; a ‘group’of four individual subjects who had parameter pair B on their first repetition and parameter pair G on their second; and so on .
Each of the 96 subjects did the experiment twice, giving a grand total of 192 repetitions of the experiment.
We paid out a total of £1,322 to the subjects: £481 on the first repetition (an average of £4.89 for each group subject and an average of £5.25 for each individual subject) and £841 on the second repetition (an average of £8.98 for each group subject and an average of £8.34 for each individual subject).
So the payment increased on average by 75% between the first and the second repetition, while the group payments rose from 93% to 108% (of an individual payment) between the first and second repetition clear evidence of a learning effect and slight evidence of a group effect.
Table 1 gives some more detail, in this we exclude all subjects who made losses in excess of £10.
A number of things are apparent from Table 1.
First, because of the differing seeds and the differing parameters, what is a good performance (in terms of realized profit) varies from seed to seed and from parameter pair to parameter pair.
As an example, consider the second repetition of groups B/G and F/E; the former realized actual average profits of £9.17 while the latter realized actual average profits of £9.38.
Nevertheless, the former performance is much better relative to the potential profits of £13.65 than the latter relative to its potential optimal profits of £16.28.
This argument suggests that a possible useful measure of realized profits (relative to potential profits) is the following:
This yields Table 2.
Without exception, all ‘groups’(whether individual or group) improved their average performance between the first and second repetitions.
This improvement was substantial: from slightly worse than myopic   on the first attempt, the average improved to getting on for 40% of the difference between the optimal and the myopic on the second.
Inspection of the raw data reveals the primary reason: on the first repetition most subjects were ignoring most of the dynamic aspects of the decision problem, and their attempts at estimating the myopically optimal output were generally rather poor.
Nevertheless, partially offsetting this latter weakness was the partial employment of one aspect of the truly dynamic strategy — namely, the use of a reservation type sales strategy.
On the first repetition (but excluding the final period, in which all except a very few sold their entire post-production stock) subjects either sold all their stock (317 occasions) or none of their stock (112 occasions) on 429 occasions out of a possible 672 occasions (64 group subjects + 32 individual subjects all multiplied by seven periods).
On the remaining 243 occasions they sold some (non-zero, non-unit) proportion of their stock.
If they had been truly myopic (on our definition) they would have sold their entire stock on all 672 occasions.
So this fact (that they were not being totally myopic) accounted for the fact that, despite their poor ability to get the myopically optimal production level right (which is what many of them said that they were trying to do), they almost did as well(in terms of realized profit) as the myopic strategy.
On the second repetition, however, their performance improved — as Tables I and 2 indicate.
We shall discuss the reasons in more detail shortly, but in essence there were two: first, many more were adopting a reservation sales strategy (no sales at all on 235 occasions, total sales on 341 occasions, and some non-zero/non-unit proportion on just 96 occasions, out of a grand total of 672 occasions, excluding the final period); second, many were adopting a declining output strategy, in broad agreement with the optimal strategy.
The performance indicators also suggest that the groups' improvements were  marginally better than the individuals' improvements, as we have already noted.
But the difference is not particularly marked.
4.
Is behaviour consistent with the optimal strategy?
The question as to whether behaviour was optimal can very quickly be dismissed, particularly on the first repetition.
Formal statistical tests seem inappropriate as some summary statistical measures should indicate the sizeable discrepancy between the actual values of the decision variables and the optimal values.
There are two decision variables — output and sales.
Optimal output can be calculated for each subject and for each period on each repetition, as can optimal sales which can most conveniently be expressed as a proportion (always either 0 or 1) of total post-production stock.
So the error in output,, and the error in the sales proportion,, can be calculated for each subject for each period and for each repetition.
Excluding the few outliers, we can calculate the square root of the average squared error over all subjects for each repetition.
This gives the following table:
Note the improvement (though still to a high level) in  and the more modest improvement in  between the first and the second repetitions.
(To put these figures in perspective, it might be useful to note that the standard deviations of the optimal output and proportion respectively over the entire 1,536 observations were 71.274 and 0.48343.)
An alternative way of making the same point is by running regressions (again excluding those five cases where output was 10,000) of actual output against the optimal output and the actual sales against optimal sales proportion.
This yields:
In both cases the proportion is closer to the optimal proportion than the output to the optimal output.
But in no instances is the fit particularly good.
Details of individual associations between actual and optimal decisions are given in Table 3; this presents for each subject and for each repetition the proportion of the variance of actual output (or actual sales proportion) explained by optimal output (or actual sales proportion).
It will be seen from this that there are significant differences in terms of individual's abilities to approach the optimum and to improve with repetition.
5.
Is behaviour consistent in comparative terms?
Behaviour would appear to depart from that predicted by the optimality theory.
We shall qualify and discuss this finding further in the concluding section, but let us first look at some comparative static tests.
Remember that it is these kinds of tests that are carried out in conventional (econometric) tests of economic theories.
We begin with the output variable.
As all subjects had the same cost function and the same mean for the price distribution, the comparative static propositions tested in this experiment concern the standard deviation of the price distribution, σ, and the (per unit) storage cost, k.
As already noted, the predictions made by the optimality theory are straightforward: an increase in σ or a decrease in k leads to an increase in optimal output in all periods except the last.
In addition, we know that output is a decreasing function of time (to the horizon), so we can express the various comparative static predictions in the form:
The intuition is straightforward: optimal output is a decreasing function of time since the expected price on sales is a decreasing function of time; optimal output is an increasing function of σ since the more variation there is in output price the higher is the expected price on sales (recall that sales are made only when the price is sufficiently high); optimal output is a decreasing function of the (per unit) storage cost since the more expensive it is to store the less attractive is storing.
The optimal output for each of the eight parameter pairs is given in Table 4 (an example of the data obtained from the experiment is given in Table 5).
The comparative static propositions listed above can be tested with respect to these particular optimal values.
An alternative verification (which will have its counterpart in our analysis of the data generated by our experiment) is as follows.
First, we append to each of the eight sets of (period/output/res price) columns in Table 4 two further columns, the relevant σ and k values for each parameter pair.
Next, we stack up the eight sets of five columns (period/output/ res price/ σ/k); on top of each other to form a data matrix five columns wide and 64 rows long.
Label the columns naturally, viz. ,,, σ, and k.
Now estimate equation (4) above.
The simplest specification is a linear regression of the  column against the t, σ, and k columns.
This yields: As predicted, the t and the k terms enter negatively and the σ term enters positively.
To test these comparative static predictions, we carry out a similar exercise on the actual data.
First, for each subject on each repetition we start with a   data matrix of eight columns: g, a dummy taking the value 0 or 1 depending upon whether the subject did the experiment in a group or individually; a, a dummy indicating the attempt number (1 or 2); t, the period number; X t , the output chosen by that subject on that attempt and in that period; p t , the proportion of (post-production) stock sold by that subject on that attempt and in that period; P t , the output price in that period; σ, the relevant standard deviation of the price distribution for that subject in that attempt; and k, the relevant storage cost for that subject on that attempt.
This data matrix has eight rows; the first two columns and the last two columns are, of course, constant for a particular subject on a particular attempt.
We then stack up in a data matrix of 1,536 rows all such data matrices for all subjects on both attempts.
We then try and explain the X and the p columns of this matrix.
In the results reported below we treat the two repetitions separately since it is quite clear that behaviour changed between the two attempts.
However, we do not treat the groups any differently from the individuals as we found no significant effect of the g variable.
Accordingly, we  lump together the group and individual responses in the analyses reported below.
We start with the output variable.
We exclude from the analysis those five observations where the output decision was quite clearly in the wrong ballpark.
Estimating the counterpart of equation (5) above gives:
These are instructive.
Consider first equation (6), summarizing the first attempts.
This shows clearly that t had no effect on the output decisions, a result which is obvious from studying the raw data.
So subjects were not, on their first attempts, exploiting the opportunities offered by the dynamic aspects of the decision problem.
However, what is of interest is that the other two comparative static propositions of the theory are confirmed by the subjects' behaviour: output rises with an increase in σ and falls with an increase in k.
Moreover, the two coefficients are significantly positive and negative respectively (with t-ratios of 5.2 and 5.0).
Things change dramatically on the second repetition, however.
Here, the t term is negative and significantly so: on average, subjects were correctly tapering down their production as the horizon approached.
(Though note the magnitude of the coefficient — it is on the small side.)
So the subjects had clearly realized this aspect of the dynamic solution between doing the experiment for the first time and for the second.
The σ and k terms both have highly significant coefficients; unfortunately, they are both of the wrong sign, with output falling with σ and rising with k.
Here, the comparative static propositions of the optimality theory are not confirmed.
To see if this was a fluke with the particular functional form adopted, we tried a number of alternative specifications (such as estimating period-by-period), including interactive terms, including an indicator of the risk attitudes of the subjects, including a t 2 term.
But none of these made any difference to the basic shape of these key results: on the first repetition, t was not important but both and k were — and in the direction predicted by the theory; on the second repetition, t was important (and with the right sign), as were σ and k — but they  both had the wrong sign (the opposite of that predicted by the theory).
We shall speculate in our conclusions as to why that might be so.
We now turn to the sales decision.
In the optimality theory, this is determined by the reservation price rule sell all if the price is above the reservation price, sell nothing otherwise.
The properties of the reservation price can be determined in the usual fashion.
To verify these properties with the particular pairs used in this experiment, we repeat the procedure used above for : we regress  on t, σ, and k (though for obvious reasons we exclude 8, in which  is always zero).
This yields: As expected, t and k enter negatively and σ enters positively, just as in equation (5) for  above.
The counterpart for our experimental data is not so straightforward, since the subjects do not report their reservation values (even assuming that they have them).
However, for those observations which are consistent with the use of a reservation rule we can infer the implied (almost inevitably stochastic) reservation price by probit analysis.
This we do as follows.
First, we begin by restricting attention to those occasions when the subject either sold all or nothing.
This is when the p t observation takes the value 1 or 0.
So we extract those rows from our data matrix where p t takes the value 1 or 0.
We then use the p variable as dependent variable in a probit analysis with independent variables t, σ, k, and P t (where P t is the relevant actual price).
This implies a model of the following form:
The theory implies that  and .
(Care must be taken in interpreting these probit equations: note that  positive because the higher the price the more likely it is that all will be sold; is positive because as the horizon approaches one gets less fussy, and so one is more likely to sell at a lower price; the other signs follow from similar arguments.)
The first repetition is not particularly distinguished: the signs on k and P t are correct, but only that on P t is significantly different from zero.
Nor are the coefficients on t and σ significant — which is just as well as the signs are wrong.
Things are, however, somewhat different on the second repetition.
First, all the coefficients are significantly different from zero.
The signs of the coefficients on t and P t are correct, but those on σ and k are the opposite of what they should be.
This exactly parallels the findings on the output equations.
If one performs a single regression (rather than a probit) using all values of p t a similar story emerges:
As can be seen, the sign on t is incorrect (but the coefficient insignificantly different from zero) on the first repetition but correct (and significantly different from zero) on the second.
This is very similar to the probit findings.
Similarly, the signs on.
–and k are correct (but insignificantly different from zero) on the first repetition, and are significantly different from zero but of the wrong sign on the second repetition.
Once again, this confirms the probit findings.
So the results for the sales strategy are remarkably similar to the results for the output strategy: the subjects were not taking the time (the period) into account on the first repetition — as they should have been — but there were on the second repetition.
In contrast, they were responding correctly (but insignificantly as far as the sales strategy was concerned) to the two parameters σ and k on the first repetition, and were responding incorrectly (but significantly!) to  these parameters on the second repetition.
So from the first to the second repetition, the subjects' behaviour improved in some respects and deteriorated in others.
In the concluding section we shall speculate as to why this might have been.
6.
Comments and conclusions
We should carefully distinguish at the outset between the dynamic ‘comparative static’ effects, and the conventional (parameter) comparative static effects (though, as we shall see, they are in one sense intertwined).
We begin with the latter.
In this experiment the conventional comparative static effects concerned the effects of changes in the parameters a and k.
The theory shows that an increase in a (the riskiness of the price distribution), or a decrease in k (the per unit storage cost) is good for the firm.
Accordingly, the subjects should get more ‘choosy’ with higher values of a or lower values of k, in the sense that they should have higher reservation values.
This result spills over onto the optimal output decision: since one can expect better sales prices with a higher σ or a lower k, one should accordingly produce more output with a higher σ or lower k.
Our results suggest that our subjects were thinking in precisely the opposite way (at least on their second repetition).
One wonders why this might be so.
Clearly, the comparative static effects on the output and sales decisions stand or fall together (and it is quite encouraging to see that that is indeed the case with our subjects): if a given parameter change suggests an upward movement in a reservation price then it should also suggest an upward movement in output (and vice versa).
So let us take one of the two decision variables and try to create a possible argument which might give rise to the observed comparative static effects; this argument is partly backed by some of the questionnaire responses.
Start with the output variable, X, and the riskiness parameter, a.
The optimality theory states that an increase in a is good, as the subjects can always ignore the increased probability of lower prices (as they should not be selling at those low prices anyhow) and can profit from the increased probability of higher prices.
This is a classic search argument.
Interestingly, in a large-scale study of search behaviour just completed (Hey, 1992/3) we found a conclusion remarkably similar to that found in this study: subjects were responding in the wrong direction (but significantly) to changes in the riskiness parameter σ.
Why might this be so?
One possible reason is that subjects were (highly) risk-averse.
This we think we can discount for a number of reasons: first, the magnitude of the risk aversion would need to be rather large for the result (that  an increase in a is good) to be reversed (particularly when the reservation value is generally above the mean price); second, subjects were paid proportionately to their total profits over 16 decision periods (32 decisions) so that any risk aversion would already have been significantly averaged out; third, the subjects appeared to be trying to make as much money out of the experiment as possible (implying that they were behaving in a risk-neutral fashion); fourth, the majority of the responses to two questions on the questionnaire indicated risk-neutrality.
We prefer to subscribe to an alternative explanation: subjects were unable to work through the implications of increased riskiness but wrongly perceived an increase in it as bad.
They responded accordingly — reduced output in response to an increase in σ.
This spilled over to a fall in the reservation prices.
Hence the findings.
It is not easy to explain away the perverse comparative static finding for k.
Recall that the optimality theory says that an increase in k should lead to a fall in X, and that our subjects (on the second repetition) appeared to be doing the opposite.
One could try and explain this as follows: subjects perceived an increase in k as necessitating an increase in output since the opportunities for using output from other periods to increase sales this period had been reduced by the increase in the storage cost; accordingly, one should produce more now in case the price this period turned out to be a high one.
A possible story, but not as convincing as the one relating to σ.
But here again one could argue that things go hand-in-hand: subjects perceived that an increase in a was equivalent to a decrease in k, and having argued the case for σ, the case for k followed automatically.
At least they were being consistent!
Finally we turn to the dynamic ‘comparative static’ effects.
In many ways these are the most interesting.
If one considers the transition from our myopic (static) world to our optimal (dynamic) world, one can see how the dynamic aspects build up.
We start with the myopic strategy:(a) output constant and equal to expected price; sales always equal to output.
We then add on a first dynamic element:(b) a reservation sales strategy, with a constant reservation price.
We then add on a second dynamic element:(c) a declining output strategy, because of(b).
Finally, we add on a third dynamic element:(d) a declining reservation price to use with the reservation sales strategy.
In some sense,(a) through (d) are in ascending order of difficulty.
Our experimental findings show how the subjects were working through these: in the first repetition,(a) was attempted rather amateurishly and (b) invoked by relatively few; in the second repetition, many were beginning to apply (a) successively (many put output in period eight equal to 250 which is where marginal cost equals expected price), many more were applying (b) and a sizeable number (enough to effect our regression and probit results) were invoking both (c) and (d), though understandably the (c) effects were more pronounced.
In other words, the subjects were making determined efforts to understand the dynamic aspects of the problem.
To a significant extent (witness our results on payment increases between the first and second repetition) they were successful.
One can then perhaps forgive them for faulty (though consistent) reasoning on the (conventional) comparative static effects?
But note that there is an intimate connection between the ‘dynamic’ parameters a and k, the odd responses of the subjects to changes in these parameters, and the inability of (at least some of) the subjects to completely think through the implications of the dynamic structure of the problem.
What then are the implications of our experiment for the questions that we raised in our introductory remarks?
First, and rather obviously, subjects unfamiliar with a rather complex dynamic decision problem under risk have difficulty in coping with it on first acquaintance.
As far as output is concerned, some do particularly badly — producing outputs completely in the wrong ballpark; some, however, are rather better and a few are really quite good.
Things are somewhat different when it comes to the sales decision: here it seems that many worked out at quite an early stage that the sales strategy ought to take a reservation form (with perhaps an intermediate section to take account of risk and risk aversion).
This, on reflection, is interesting: it reveals quite sharply what might have been apparent earlier — that this particular decision rule is of a rather simpler structure than the optimal output rule.
Even to relatively unsophisticated subjects it is quite intuitive to sell when the price is high and store when the price is low, but it is much less intuitive to produce where the marginal cost is equal to some expectation.
So one seems able to conclude that some particular aspects of the overall optimal strategy are easier to understand than others and easier to understand earlier than others.
Behaviour is generally closer to the optimal on the second repetition, even though there is considerable heterogeneity.
It remains an open question as to what would happen with further repetitions.
One could speculate that behaviour would continue to improve, although at a decreasing rate, with it perhaps approaching a plateau somewhere below the optimum; one might also want to speculate that there would be different plateaus for different people — reflecting the ‘fact’ that some individuals are better decision-makers than others.
Furthermore, one can observe different subjects making differential progress in understanding the various stages in the transition from myopic to dynamically optimal.
The implications for less partial equilibrium behaviour are less clear.
One can certainly envisage situations in which the sub-optimal response of agents to the dynamics of a particular dynamic market game might lead to a market equilibrium away from that which would be reached under optimal behaviour.
Moreover, the perverse responses of the subjects to changes in σ and k raise the interesting possibility that the stabilizing influences of the correct responses might not be operative; so that unstable market situations might emerge.
Such possibilities could be explored either through market experiments in which the problem of this paper is embedded, or through simulations in which agents follow strategies similar to those observed in these experiments.
At the very least the results of the experiment reported in this paper remind us that not all economic agents optimize all the time, though it would appear that the simpler the problem being tackled and the more experience people  have of it, the nearer to the optimal they are likely to be.
Of course, this in itself is not a new finding; it has emerged elsewhere in experimental work, perhaps most notably in the field of bargaining experiments.
Binmore (1992) in his recent book talks at length about the implications of bounded rationality for behaviour in games of various types, and Roth (1988, 1991, 1992) frequently returns to this theme.
But perhaps, more constructively, the results of this paper point the way to the appropriate modelling of economic behaviour when agents are not able to optimize for one reason or another.
FINANCIAL LIBERALIZATION AND FOREIGN PORTFOLIO INVESTMENT IN THE UNITED KINGDOM
By NIGEL PAIN
1.
Introduction
CROSS-BORDER portfolio investment has become an increasingly important feature of global capital markets.
The expansion of such investments has taken place in the midst of a worldwide process of financial liberalization, with capital controls being relaxed and transactions costs being reduced in securities markets.
The resulting flows as institutions adjust towards their desired portfolio position are of importance because they act as an equipoise to international imbalances in savings and investment through their influence on asset prices and exchange rates.
This paper sets out to examine the factors behind foreign portfolio investment in the United Kingdom since the mid-1970s.
Over this period the stock of such investments has expanded at an average annual compound rate of just under 19%.
The paper contains an empirical analysis of the determinants of this investment with particular attention being paid to the role of wealth, expected returns, and measures of financial liberalization.
A constructed exchange control index is used to provide support for the hypothesis that financial deregulation has provided a boost to foreign portfolio investment.
The plan of this paper is as follows: Section 2 discusses financial deregulation, paying especial attention to the relaxation of capital control in Japan and the developments associated with the ‘Big Bang’ in the UK in 1986.
Section 3 outlines a standard portfolio model and considers the necessary assumptions required to obtain the general specification used in the empirical work.
Section 4 describes the wealth and asset return measures used and presents the first stage regression used to obtain instruments for expected returns.
The main econometric results are given in Section 5 and some concluding comments provided in Section 6.
2.
Recent developments in financial markets
Two institutional reforms that may be of particular importance in the explanation of the growth of foreign portfolio investment are the abolition of foreign exchange controls and the reduction of transactions costs in securities markets.
The last decade has witnessed significant reductions in regulatory barriers to investment in foreign securities.
Remaining controls over the currency composition of assets can largely be characterized as being consistent  with prudential asset-liability management.
Within Europe, relaxation was led by the UK, which abolished all controls at the end of 1979.
Outside Europe the main developments have been the gradual relaxation in Japanese exchange controls and the steady move into foreign assets by the main United States institutions, particularly pension funds.
The process of financial deregulation in Japan has been accompanied by increasing international diversification of investment portfolios.
For example, the share of foreign securities in the portfolios of Japanese life assurance companies rose from 2.7% in 1980 to over 13% by the end of 1987.
Whilst anecdotal evidence suggests that deregulation of capital controls stimulated foreign portfolio investment, an adequate measure of investment constraints is not readily available in the literature.
Moreover, the process of financial liberalization in Japan has been gradual and therefore a simple (0, 1) dummy variable will not suffice.
Appendix 1 describes in detail the calculation of a quarterly series for the freedom of Japanese financial institutions to invest overseas, using information in Fukao (1988).
Essentially the series is a weighted average of the share of total assets that different financial institutions were permitted to invest overseas.
The constructed index is transformed so as to move from 0 in 1970 Q4 to a peak of unity assumed to be reached in 1989 Q4.
This is not to say that the share of overseas assets reached a peak in 1989 Q4; it simply implies that the conditions for investment are relatively free in that any remaining controls can be thought of as prudential rather than prohibitive.
A plot of the timepath of the index is shown in Figure 4.
At the outset it should be emphasized that while the index is constructed from Japanese data, it is unlikely that it will simply capture Japanese investment alone in estimation.
This is because the liberalization process in Japan occurred simultaneously with that elsewhere and therefore it is possible that the variable may also capture any previously rationed investments from other localities.
Changes in the transactions costs incurred on investment in the UK provide a second means by which financial deregulation may have influenced inward investment.
Such costs can have an important influence on the overall portfolio rate of return, particularly for financial institutions who tend to trade their portfolios more actively than private investors.
Jackson and O'Donnell (1985) estimate that in the early 1980s average transactions costs added some 3.25% to the cost of a £100,000 deal in London, whereas they only added about 1% in New York (ignoring the market-maker's  spread).
The latter figure reflected both the absence of costs due to taxation — UK investors having to pay stamp duty — and the abolition of fixed commissions in 1975, leading to a cut of around 50% in the commission rates on institutional deals.
The competitive position of the UK began to improve from the middle of the 1980s, with the rate of stamp duty being cut from 2% to 1% in April 1984 and to 0.5% in October 1986.
Simultaneously in 1986 the ‘Big Bang’ reforms saw the end of fixed commissions in the UK.
As in the US the large institutions were able to enforce substantial cuts in their commission costs.
Ingram (1987) estimates that the stamp duty and commission costs on a £500,000 deal fell from 1.5% prior to‘Big Bang’ to somewhere between 0.5 and 0.7% soon after.
Rather than use an arbitrary measure of commission costs (since these can vary according to the exact size of the deal and the market makers spread) we have attempted to capture the competitive effects associated with changes in transactions costs by using a measure of turnover on the UK Stock Exchange.
In the empirical work below we deflate turnover by the nominal market value of domestic GDP.
The latter variable is likely to be a reasonable measure of the trend growth in market capitalization.
A ‘real’ measure is necessary to prevent the turnover variable simply capturing trends in the price level.
As can be seen from Fig. 1 the resulting series appears to be a reasonable proxy for ‘Big Bang’ related effects.
3.
The theoretical model
The starting point for the analysis is a simplified single long-run (or ‘desired’) asset demand by an individual investor of the form: where, in this instance, s denotes the stock of overseas portfolio investments held in the UK, w denotes total portfolio size, r the expected returns on portfolio investment in the UK, r* the expected returns on an alternative investment (or a weighted average of a vector of alternative returns), and Zj a vector of additional variables.
Here Zj consists of the two financial liberalization variables discussed in Section 2.
This specification implies both homogeneity and homotheticity, although strictly there is no reason to impose either given that Courakis (1989) demonstrates that both only arise from particular forms of the investor's underlying utility function.
The long-run wealth elasticity is explicitly tested in the empirical work below, but homogeneity is assumed throughout, primarily as a convenient empirical simplification.
Additional assumptions may be required when aggregating individual demands to obtain a market or sectoral demand function.
Total inward portfolio investment S may be written as:
The constancy of the aggregate parameters  rests either on the assumption that individual wealth shares are constant or on the assumption that all investors perceive the same expected returns and variance covariance matrix of returns and exhibit the same degree of absolute risk aversion.
As Tobin (1982) and Walsh (1984) observe, instability may also arise at times of change in the underlying economic environment since the coefficients in (1) are dependent upon the covariance matrix of asset returns.
Changes in monetary policy, such as a move towards monetary targeting, or international currency agreements, such as the Plaza and Louvre Accords, may change both the variances and covariances of interest rates and exchange rates.
One possible remedy would be to employ a time-varying parameter technique in estimation as in Browne and McNellis (1990).
However, degrees-of-freedom considerations prevent the use of such an approach in the present study.
Use of a logarithmic form of (2) in estimation provides a convenient means of calculating the elasticity of the asset demand with respect to wealth and testing for homogeneity of degree one in wealth.
Taking natural logarithms  yields 
Following inter alia Feige (1967) and Sharpe (1973), we assume that investors pursue a long-run target portfolio S* given by (2) subject to the costs of being out of equilibrium and adjusting asset holdings.
If costs are quadratic and investors minimize current and expected future costs, then the cost function has the form: where d i is a discount factor (which hereafter is assumed to be unity).
Whilst the assumption that asset demands are set at such a level so as to minimize (4) yields models that are both tractable and consistent with observed empirical realities, the justification for quadratic costs in such models is always somewhat ad hoc.
In particular, appealing to the transactions costs involved in buying and selling securities is of limited use as transactions costs are fixed costs which if anything rise less than proportionately to the size of the deal.
Such costs suggest portfolio adjustment by means of large lumpy transactions rather than by the continuous small transactions implied by quadratic costs.
A more extensive discussion of adjustment costs can be found in Pain (1990).
The alternative justifications for higher order adjustment costs considered there require a number of assumptions at least as great as for the quadratic cost case.
For the time being we will therefore continue to characterize adjustment by means of the quadratic cost formulation.
Equation (4) may be solved (for details see Sargent, 1979) to yield: Here  are dependent upon both  and the adjustment cost parameters and  denotes an expectation.
Higher order lagged dependent variables may be introduced by invoking additional adjustment costs arising, for example, from an acceleration in portfolio investment.
Equation (5) suggests that asset holdings in the current period are dependent upon the expected future values of the variables determining the desired asset stock in (2).
A backward-looking specification may be obtained from (5) if the target variable is autoregressive in nature so that: Repeated substitution of (6) into (5) turns the forward convolution term into a more familiar ‘feedback’ one.
Nickell (1985) demonstrates that such behaviour can give rise to an error-correction specification; it is also shown that the number of lagged values of  that enter the estimated model is directly dependent upon the nature of the process generating s*
In this instance there are reasonable grounds for assuming autoregressive behaviour in the target as the main determinant of the desired stock is the level of wealth.
The speed with which investors react to changes in wealth is dependent upon whether such changes are anticipated and whether it is thought that any gains are transitory in nature, arising from short-term price variability independent of the underlying trend rate of growth.
Finally, it is important to note that this analysis aggregates foreign debt and equity investment into ‘portfolio investment’.
Implicitly it is consistent with the first stage of a two-stage budgeting procedure, with investors initially deciding on the level of investment in a particular location and then determining the asset allocation of that investment.
An aggregate approach may not be too unrealistic given that assessments of the returns on overseas bond and equity investments are likely to be dominated by exchange rate expectations.
4.
The specification of asset stocks and expected returns
This section discusses the issues involved in the construction of the asset stock and expected return variables used in the empirical work below.
An immediate difficulty with (3) is the measurement of W, the total portfolio size.
Since we are dealing with an overseas asset demand, the appropriate concept is a measure of world financial assets.
Information on this is not readily available so we have chosen to proxy W by Gross Household Sector Financial Assets in the United States.
A similar expedient is adopted in Lomax and Denham (1978).
Figure 2 shows the wealth series in both dollar and sterling terms.
The dollar series shows steady growth throughout most of the last two decades, with the exception of the periods around the two major stock market declines in 1974/5 and late 1987.
The volatility of the sterling series is much greater with particularly rapid growth being experienced in the mid-eighties at the time of sterling's decline against the dollar.
The corollary of this was that the cost of acquiring a British asset fell sharply in dollar terms.
The stock of inward portfolio investment and the cumulated inflow (taking the end-1975 stock as a benchmark) are shown in Fig. 3.
This shows that the investment stock remained relatively constant from around 1977 to 1981, with revaluations summing to zero over the period.
Subsequent to 1981 there is a marked upward rise in the stock, with particularly sharp growth after 1984.
Similar trends are apparent in the series for cumulated inflows, suggesting that the observed changes in the stock do not arise from revaluation effects alone.
One exception is 1987, with the peak and fall of equity prices more apparent in the stock data, although flows noticeably slow in early 1988.
The measure of expected returns used in the empirical work consists of a weighted average of the relative returns on equities and bonds in the UK and overseas.
These comprise the dividend yield or coupon rate and expected capital gains, including currency gains from exchange rate revaluations.
Weighted one-period ahead expected returns (WCOMP) take the form: the UK relative to returns elsewhere, ER denotes the change in the sterling exchange rate and superscript e denotes an expected variable.
Further details are given in Appendix 2.
The price of bonds is assumed to move inversely with the long-rate, using a consols formula.
The weighting factor a in a particular quarter is the share of corporate securities in total inward investment at the end of the previous quarter.
An end-quarter exchange rate is used as interest lies in the gains that could be made over the period in which the yield differential held on average.
A positive effect is expected as an appreciation of sterling will generate capital gains in foreign currency terms.
Empirically, an immediate problem arises from the fact that (7) uses ex ante expectations of relative returns although only the ex post outturn is actually observed.
An instrumental variable technique is required to overcome the resulting errors-in-variables problem.
One possible solution would be to use a weakly rational two-step estimator with the expectation being replaced by the fitted values from a prior regression of the outturn on a vector of predetermined   variables.
Barro (1977) provides one example of such an approach.
However, as Pagan (1984) notes, whilst this technique will give consistent parameter estimates, the reported standard errors will be inconsistent.
To overcome this we use a variant of the McCallum (1976) technique, with the future expectation being replaced by the actual outcome and the predictors from the auxiliary regression being used as an instrumental variable.
Relative returns (as this is the variable employed in estimation) were regressed on a vector of predetermined variables consisting of lagged relative yields and asset prices plus nominal GDP growth and changes in three-month interest rates in the UK and the US.
The latter variables were used as they were thought to summarize the range of additional information available to financial markets.
The final equation obtained is shown in Table 1.
This equation is estimated over the whole sample period rather than by a more complicated rolling regression technique which would allow the parameters to change each quarter as new information became available.
(Absolute values of t-statistics are given in parentheses.)
Past changes in both relative equity and gilt prices have a negative influence on future expectations although interest differentials (RATE) are expected to persist for some time.
Short-term changes in domestic short rates also play an important role.
This can perhaps be interpreted as either a sign of policy changes feeding through into the yield curve or as a change in exchange rate expectations.
Additional effects are found from the growth in nominal income which is associated with an increase in own-country relative returns.
Two possible explanations for this result are that growth in nominal activity may raise the relative return on domestic assets, or alternatively, that  the exchange rate (units of foreign currency per unit of domestic currency) appreciates following capital inflows attracted by growth in the domestic economy.
None of the equation-specific diagnostics for (1.1) reveals any sign of mis-specification.
The absence of serial correlation in the residuals is an important finding given the requirements of the McCallum technique.
An additional internal Chow test conducted by splitting the sample at 1982 Q4 (coinciding with the end of monetary base control in the US) and running separate regressions over the two halves of the sample failed to reveal any sign of a structural break .
5.
Empirical results
From the discussion above we began estimation with the following general autoregressive distributed lag model: where SPORTL and WW denote the stock of inward portfolio investment and the total wealth proxy, RSX is the real turnover on the UK Stock Exchange, JXC the exchange control index, Q i a seasonal dummy for quarter i, and DUM is a dummy variable equal to 1 in 1984 Q2 and zero at all other times.
In equation (8) both one-quarter ahead and current quarter relative returns need to be instrumented.
This is because the change in the stock in the current quarter () consists of revaluations (which are a function of the change in asset prices over the quarter) and new purchases which are a function of expected gains from the time of purchase rather than simply from the end of the current quarter.
The first effect suggests instrumenting  to avoid   any bias in the coefficient arising from colinearity with revaluations, whilst the second leads to IV in order to bypass a measurement error problem.
A further point to note is that (8) is specified in nominal terms.
The main reason for this is that the liabilities of the major investing institutions are are primarily nominal, although some pensions are index-linked.
As funds become increasingly mature with known monetary liabilities, it may be expected that the demand for assets whose terminal capital value is certain will rise.
A variant of the final equation with assets denominated in real terms is discussed below.
In estimating (8) perfect foresight was initially assumed with ex post capital gains being used to measure expected capital gains.
With 29 regressors and only 49 observations, degrees of freedom were limited, prohibiting the initial use of instrumental variables.
After a succession of data-based parameter restrictions we obtained equation (2.1) in Table 2.
The restrictions on the general model are readily accepted by the data using a standard F-test [F (16, 20) = 0.82].
Equation (2.2) in Table 2 reports the equivalent IV estimates for equation (2.1) with fitted values and lagged fitted values from (1.1) used  as instruments for the expected return terms.
On the whole the coefficients in the IV equation are reasonably close to those from OLS estimation.
There is some evidence that the coefficients on expected returns were biased downwards in the OLS equation.
Equation (2.2) provides considerable support for the empirical specification developed in Section 3.
The evidence suggests that future expectations are an important determinant of the level of investment, although more weight is given to expectations about relative price changes over the current quarter.
The dynamic WCOMP term means that lagged relative returns have a negative coefficient.
This may possibly capture a portfolio adjustment effect arising from past  relevations to the asset stock.
To illustrate the combination of speculative and portfolio adjustment effects that are embodied in relative returns it is useful to analyse the changes that would arise if wealth and turnover are held constant and investors expect a rise of 1% in the relative return on UK investments in both the current and following quarters due to an expected exchange rate appreciation.
(Arbitrage activity reduces the likelihood of permanent differentials in relative returns.)
Equation (2.2) suggests that the desired stock of investment will rise by some 0.51% in the current quarter, equivalent to around £300m on the end-1988 stock.
In the subsequent quarter there will be a further adjustment with the forward looking and lagged return terms reducing the desired stock, the former because the 1% rise is expected to be reversed and the latter because the past rise generates a portfolio adjustment effect.
Equation (2.2) suggests that the chosen wealth measure has the characteristics that would be expected of the ‘true’ wealth series since the imposition of a unit wealth elasticity proves data acceptable with a Wald statistic .
The lagged dependent variables suggest that adjustment costs are of importance whilst the presence of lagged determinants of the desired stock suggests that there is more in the data than a simple first-order partial adjustment model might permit.
Our results also provide evidence to support the hypothesis advanced in Section 2 that institutional changes have important effects on portfolio investment.
Within (2.2) significant financial effects arise from both the levels term in the exchange control index and the lagged dynamic term capturing domestic financial competitiveness.
Both terms help to capture the sharp rise in inward investment in the second half of the 1980s.
The coefficient on the exchange control term, JXC, implies that the relaxation of exchange controls  in Japan was associated with positive inflows into the UK.
An indication of the importance of the two financial measures (RSX and JXC) is provided by the decisive rejection of a joint test of whether both their coefficients could be set to zero .
Post-sample, the exchange control term will simply change the steady-state portfolio share (as JXC tends to 1).
However within sample it is important to examine the implied elasticity to see whether the estimated coefficients give rise to plausible effects.
This appears to be the case for equation (2.2) which has the property that a rise of 10 points in JXC (i.e. 0.1) will raise the stock of inward investment by some 2.4% on impact, and by some 8.2% in the long-run.
Using the end-1985 stock as a base, a rise of 0.7 in JXC (equivalent to the change over 1985 Q4–1987 Q4) implies an increase in the stock of some 23.7bn.
This may be compared to the total inflow of Japanese portfolio investment in the UK over 1986 Q1–1989 Q2 which amounted to some £23.2bn and total foreign inward investment of close to £50bn.
Of the diagnostic statistics reported for (2.2), the RESET test statistic is of particular importance in that it fails to provide any evidence that relevant higher moments of the explanatory variables have been omitted from the equation.
This finding provides some support for the logarithmic approximation made in (3) where higher order terms in a number of variables were dropped from the original specification.
5.1.
Are current changes in wealth exogenous?
One important factor to test is the assumed exogeneity of wealth, i.e. whether wealth can be thought of as being determined prior to its allocation amongst individual assets.
In terms of(2.2) this implies testing to see whether  is exogenous.
The quarterly change in wealth consists of two factors, new savings and revaluations to existing assets.
Flows may well be exogenous as they are likely to be known with a reasonable degree of uncertainty given the contractual nature of many savings policies.
Revaluations however may be determined simultaneously with revaluations to portfolio assets in the UK.18 This issue was tested using a variant of the Hausman (1978) procedure.
The resulting Wald statistic  rejected the null of weak exogeneity, implying that the reported coefficient on the contemporaneous wealth term is biased due to simultaneity.
This finding is somewhat surprising as there is little correlation between  and  over the whole sample period (sample correlation coefficient = 0.2).
One possible explanation stems from the impact of the fall in equity prices on both stocks in the last quarter of 1987.
This hypothesis was investigated by re-estimating the preferred equation up to 1987 Q2 prior to the stock market crash.
The resulting estimates (described in detail in Pain, 1990) were reasonably close to those of (2.2), although some parameter changes were apparent, notably in the contemporaneous dynamic wealth term whose coefficient fell to 0.28.
A Hausman-type test on this term yielded a statistic , failing to reject the null of weak exogeneity prior to 1987 Q3.
Although the equation diagnostics again provided little evidence of any within-sample mis-specification, there was some evidence of post-sample instability associated with the stock market crash.
A formal post-sample forecast statistic for the six quarters up to the end of 1988 was obtained using the Salkever (1976) dummy variable analogue of the Chow test.
A separate (1,0) dummy was added for each quarter of the post-sample forecast and a test of their joint significance was undertaken.
The resulting Wald statistic  provides evidence of instability as it is significant at the 5% level (critical value 2.42), although not at the 2.5% level.
The dummy for 1987 Q4 was the only dummy that was individually significant.
Taken together the results of the Hausman and Salkever tests suggest that predictive failure is likely to reflect mis-specification and, in particular, the presence of unrecognized simultaneity.
A similar point is made in Davidson and Hendry (1981).
The question remains as to the optimal method of obtaining a consistent estimate of the parameter on the change term in wealth over the full sample period.
Although some form of IV estimator seems required, prior to 1987 Q3 the best instrument is the actual variable itself (since it may be regarded as an exogenous variable).
Thus an efficient approach might be simply to remove the effects of the equity price fall on the change in wealth in 1987 Q4, i.e. to treat the stock market crash as unanticipated.
The data suggests that the direct effect of the crash was to wipe some 10% off the sterling value of wealth in the last quarter (this does not include indirect effects on assets other than equities).
Thus the instruments used in estimating (2.2) were augmented by a term of the form , where DUM87Q4 equals 0.909 in 1987 Q4 and I elsewhere.
The resulting equation is shown as equation (2.3) in Table 2.
The coefficients are generally similar to those in (2.2), although the standard error has fallen slightly and the impact effect from a given change in wealth is some 12% smaller.
This change appears to have little effect on the remaining coefficients from (2.2), confirming the importance of both relative returns and aggregate financial activity in determining the level of foreign portfolio investment in the UK.
5.2.
Dynamic equilibrium
One issue that cannot be fully resolved in the present paper concerns the continued existence of the higher order dynamic wealth terms in (2.3).
There are a variety of possible explanations for such terms.
Equation (6) above suggests that they could arise from autoregressive behaviour in investors' target asset holdings.
Davis (1988) suggests that a failure to redistribute capital gains immediately across the portfolio will tend to generate significant lagged wealth effects.
Such an argument relies on the capital gains being unanticipated.
Alternatively the dynamic wealth terms may arise from either the absence of a full interdependent model which would permit disequilibrium in one asset to spill over into other assets or from some otherwise omitted dynamic effects arising from the use of a US wealth measure as a proxy for the true worldwide measure.
Within the context of a single equation model it is not possible to distinguish between these various hypotheses.
The importance of the dynamic wealth terms is readily apparent in the long-run dynamic steady-state version of (2.3).
Letting g s , g w , g sx , g e and g r denote the steady-state growth rates of inward investment, wealth, Stock Exchange turnover, the exchange control index, and relative returns respectively, with q, = O by assumption, the steady state portfolio share is given by  Equation (9) shows that the proportion of wealth accounted for by portfolio investment in the UK varies inversely with the growth in wealth.
A step change in wealth will result in an equivalent proportionate change in inward investment, although this will not be instantaneous because of the negative growth rate effect.
A priori this latter feature might be thought somewhat undesirable as it implies that the higher the rate of growth of wealth the lower the share of the total portfolio held within the UK.
An attempt to overcome this by imposing dynamic homogeneity with respect to wealth in (2.3) was rejected by the data, with a Wald test statistic of .
One possible explanation for this result is that the trend rate of growth in the UK economy in the 1970s and early 1980s was much lower than elsewhere, ensuring that the total world portfolio rose more rapidly than the share held within the UK.
However, attempts at capturing such effects by including a term in the ratio of UK to world industrial production proved unsuccessful.
An alternative explanation is that the absence of dynamic homogeneity arises from using nominal asset stock measures (in spite of the rationale for estimating in nominal terms outlined above) with the dynamic nominal wealth terms picking  up some inflation effect.
This hypothesis was examined by respecifying wealth and the stock of portfolio investment in real terms and re-estimating (2.3) with additional terms in UK inflation.
The preferred equation is reported as equation (2.4) in Table 2; this is equivalent to (2.3) but with an additional lagged inflation term that appears orthogonal to the remaining regressors.
A unit wealth elasticity could again be imposed .
The negative coefficient on inflation in (2.4) implies the plausible result that the high levels of domestic inflation experienced in the 1970s and early 1980s acted as a disincentive to investment within the UK.
However such effects might be expected to arise from relative inflation rates rather than a UK measure alone.
Despite this, additional terms in the US inflation rate proved insignificant when added to (2.4) and a joint test of zero restrictions on their coefficients yielded .
The cause of the dynamic wealth effects is thus unresolved and may prove a fruitful avenue for future study.
6.
Conclusions
This paper has examined the empirical determinants of foreign portfolio investment in the UK since the mid-1970s.
The evidence presented is consistent with a standard single equation approach to asset demands as the level and growth of wealth together with expected relative returns are shown to be important determinants of the level of investment.
In addition the paper demonstrates that there is an important explanatory role for measures of financial liberalization with significant effects being obtained from variables capturing the relaxation of Japanese capital controls and improvements to the competitive position of the UK Stock Exchange.
As in other areas of financial modelling, most notably that of money demand, 22 the message from this study would seem to be that account needs to be taken of institutional developments in financial markets in moving towards the construction of plausible empirical models.
It is to be hoped that the results in this paper will both encourage further research into the determinants of international capital flows and provide a stimulus for a wider empirical analysis of the effects of financial market deregulation.
A number of avenues for future work are suggested by this study.
Perhaps the main priority is to improve the coverage of the world financial wealth variable.
It would also be of interest to extend the scope of the study to examine both additional forms of inward investment and to see whether measures of financial liberalization are an important factor in the explanation of portfolio investment flows elsewhere.
OPTIMAL TRADE POLICIES: AN APPLICATION TO THE UK FERTILIZER MARKET
By STEVE McCORRISTON and IAN SHELDON
1.
Introduction
N face of a massive increase in imports of urea from the Eastern bloc in 1986, the UK and the European Community (EC) fertilizer industries complained of unfair competition to the EC Commission.
Subsequently, in the early part of 1987, the UK government unilaterally imposed quotas on Eastern bloc imports which was followed by the implementation of a minimum price restriction by the EC.
In evaluating these policies, this paper draws on recent analysis of trade policy in the presence of imperfect competition which suggests that protection of a domestic industry may be desirable since domestic firms can capture rents from foreign firms (see Brander and Spencer, 1985; Eaton and Grossman, 1986).
Given the imperfectly competitive nature of the UK fertilizer market, the objective of this paper is to derive the tariff and quantity restrictions on fertilizer imports that the UK government and EC Commission could have chosen.
The outline of the paper is as follows: Section 2 highlights some basic characteristics of the UK fertilizer industry; Section 3 outlines a theoretical framework based on a model originally suggested by Dixit (1987); Section 4 derives expressions for the policies; Section 5 calibrates the theoretical model with data from the UK fertilizer market; Section 6 presents an assessment of the actual policy outcomes compared with those suggested by the theoretical framework; and finally, Section 7 reports the results of sensitivity analysis.
2.
Characteristics of the UK fertilizer industry
The structure of the UK fertilizer industry is determined by its technological characteristics of production and underlying cost conditions.
The use of raw materials such as natural gas, the chemical nature of processing and the requirement of high energy usage in nitrogen fixation, results in large-scale, capital intensive production in order to reap economies of scale.
For example Shaw (1980) reports that the minimum efficient scale of an ammonium nitrate plant is about 300,000–450,000 tonnes per annum, which approximates to a quarter of total UK consumption of nitrogenous fertilizers.
Consequently, the manufacture of base fertilizers is concentrated amongst large-scale firms who tend to be part of the wider chemical industry.
Not surprisingly, therefore, the UK fertilizer industry is oligopolistic in structure, with four firms (ICI, Norsk  Hydro, UKF, and Kemira) accounting for over 80% of market share in 1987.
In 1987, ICI dominated the market with a 50% market share, the remaining 30% being taken by the other three firms who are subsidiaries of state-owned European manufacturers that acquired capacity in the UK market over the 1970s and 1980s.
Although these firms dominate(d) the manufacture of base fertilizers in the UK, there is also a fringe of blenders who import nitrogen fertilizers and mix compound fertilizers from base materials for local and regional markets.
As one would expect from this market structure, firms have tended to avoid price competition.
However, as Shaw has argued, although the main characteristic of competition has been one of parallel pricing, there have been occasional price wars, particularly following the entry of new firms.
The oligopolistic accommodation which had characterized the UK fertilizer industry was undermined in 1986 by the inflow of cheap urea imports.
The level of imports increased by 300% between 1985 and 1986, coming predominantly from the Eastern bloc, North Africa, and the Middle East.
The dominant firms subsequently complained to the UK government and EC Commission that the fertilizer market had collapsed under pressure from ‘dumped’ imports, one estimate suggesting that urea imports were being sold at prices more than 25% below West European cost levels.
The apparent reason for this increase in imports was a reduction in demand from China which had a major impact on the international market for urea (see Lancaster, 1989).
In response to these complaints, in 1987 the UK government unilaterally imposed a quota system on urea imported from certain Eastern bloc countries.
The quota was allocated as follows: USSR (30,000t), East Germany (25,000t), Hungary (9,000t), Czechoslovakia (5,000t), Poland (5,000t), Bulgaria (3,000t), Romania (3,000t), and Albania (500t).
Also, in mid-1987, the EC Commission responded by establishing a minimum import price of £93/tonne covering imports from the main third country suppliers (Agra Europe, 1987).
3.
A model of the UK fertilizer market
The model of the UK fertilizer market adopted in this paper is an application of the theoretical work of Dixit (1988).
The structure of the fertilizer industry is divided into two, where subscript I refers to the four dominant firms and subscript 2 refers to the blenders, who act as importers of urea.
It is assumed that there is no entry/exit of firms, the dominant firms face constant average and marginal operating costs, and blenders have, at least initially, a £10 mark-up over costs.
Also, domestically produced fertilizers and imports are  deemed to be imperfect substitutes in production, these products being differentiated by various non-price factors including nitrogen content per tonne, packaging and advisory services.
3.1.
Structure of the demand system
The aggregate derived demand functions are given as: where all parameters are positive,, P 1 and P 2 are prices and  and  are quantities.
The corresponding inverse derived demand functions are: where all parameters are positive and .
This demand system can be derived by maximizing the following aggregate profits function Γ for farmers with respect to Q 1 and Q 2 : where the aggregate production function  for farmers is defined as: It is important to note that, for simplicity, the aggregate production function is of quadratic form and assumed to be homothetic, no inputs other than the two forms of fertilizers are considered, and the farmers' output prices have been normalized to one.
Further, it is assumed that farmers' output prices are unaffected by changes in fertilizer input prices, which is reasonable given the use of fixed prices for agricultural products under the auspices of the EC's Common Agricultural Policy.
In order to both derive and assess the effects of trade policies, it is necessary to have estimates of the parameters in the demand system.
This is done by taking some of the parameter estimates from external empirical sources.
The remainder are calculated by calibrating the theoretical model such that the parameters are consistent with equilibrium in the market in a given period.
Focussing on equations (1) and (2), there are five unknown parameters, A 1 , A 2 , B 1 , B 2 , and K. Since actual prices and quantities give two relations between them, three further relations are required to solve the system.
Following Dixit (1987), expressions for the price elasticity of demand and elasticity of substitution can be derived and then set equal to empirically observed values.
In the case of the price elasticity of demand, since the products of the dominant firms and of the blenders are being treated as imperfect substitutes, it is interpreted as being the effect of an equiproportionate rise in  the price of the two products on total fertilizer expenditure E. Therefore, letting  and , where  and  are initial prices and P is the proportional change factor, the aggregate expenditure for fertilizers can be written as:
Given that in the calibration P 1 and P 2 are the initial prices, and substituting equations (1) and (2) into (7), the aggregate expenditure index can be written as: The total market elasticity of demand for fertilizer, ε, is then defined and evaluated at the baseline point where the proportional change factor P equals 1.
By differentiating (8) with respect to P and multiplying by P/E, the elasticity is given by: Expression (9) is then set equal to the observed value of ε:.
The elasticity of substitution would normally be defined as: which gives a fourth relation between the parameters when set equal to an external estimate of a.
However, as Dixit (1987) notes, equations (1) and (2) in general define the ratio  as a function of the vector  and not in terms of In order for  to be a function of , at least locally, the parameters must satisfy the following final expression: Given the definition of  ta; a in (10) and using equations (1),(2), and (11), the final expression for the elasticity of substitution can be derived as:
3.2.
Firms' behaviour
In this model, firms' reactions to one another are treated as a Nash equilibrium with conjectural variations, the latter being derived from firms' profits functions.
The profits function , of a typical dominant firm is: where q 1 is its output, p 1 , and c 1 being its selling price and costs respectively.
If profits  are maximized with respect to q 1 , the first-order condition is given as: where , is the conjectural variations parameter, i.e. the firm's expectation of how market price P 1 will vary with changes in its output q 1 .
Therefore if a firm plays Cournot, it believes rival firms will not change output in response to a change in  hence , the slope of the inverse demand function.
If the market were perfectly competitive, a change in one firm's output would have no effect on market price, hence .
Aggregating the first-order conditions over the  dominant (and, by assumption, identical) firms, we have: where  is the aggregate conjectural variations parameter.
For Cournot behaviour, and, as  increases, the more competitive is the Cournot outcome.
For perfectly competitive behaviour,.
A similar expression, inclusive of a per unit tariff t, can be derived for the blenders:
With the relevant data, values for the conjectural variations parameters, and , can be retrieved from (15) and (16).
Equilibrium quantities and prices in the model are obtained by combining (3),(4),(15), and (16), the explicit solutions being:
4.
Optimal trade policies
Typically the literature on trade policy in the presence of imperfect competition has derived optimal tariff and subsidy policies that maximize economic welfare.
However, given that the policies adopted by the UK government and EC Commission were a quota and minimum import price respectively, it is necessary to derive appropriate non-tariff policies to compare with the policies utilized in practice.
The government's objective is to maximize the sum of farmers' producer surplus r, the dominant firms' profits and government revenue as given by: where t is the level of the tariff.
It should be noted that the profits of the blenders have been excluded from the welfare function since the policies were aimed exclusively at protecting the dominant firms and also the blenders are being  treated as import agents.
Substituting in for Γ from (5),(19) can be re-written as: The aim of optimal policies is to maximize the government's objective function as given by (19).
4.1 Optimal tariff policy
If a tariff could be used to protect the domestic industry, the optimal value for such a policy is derived by maximizing (20) with respect to t.
Using (6) and (16), the first-order condition for welfare maximization is: As  and  are endogenous,(21) can be solved by substituting in (17) to give: This derivation can be used to evaluate the tariff-equivalent of the EC minimum import price.
However, two important points should be noted: first, the optimal tariff would generate government revenue while, in the case of the EC minimum import price, rents would be captured by exporters.
Second, under the GATT framework, it is unlikely that the EC would be permitted to impose a tariff on certain exporters.
4.2.
Import quotas
The use of import quotas has received scant attention in the strategic trade policy literature, the one exception being Fung (1989).
The aim, therefore, is to choose the import restriction that would be consistent with the tariff-induced outcome, i.e. the strategic outcome, and compare it with the actual quota imposed by the UK government.
In doing so, the quota is defined along the lines traditionally followed in the international trade literature regarding the equivalence between tariffs and quotas from Shibata (1968) onwards, i.e. the quota is the level of imports that would be generated by the optimal tariff.
Tariffs and import quotas are deemed to be equivalent if the domestic prices of the good are identical following the imposition of the policy.
Much of the literature on the non-equivalence between these trade policies has considered the polar cases of monopoly and perfect competition (for review see Helpman and Krugman, 1989).
However, in the present context, the effects of quota restrictions in oligopolistic markets will have an additional effect of changing the conjectures of the firms competing in the market.
Essentially, the domestic firm will know the response of its competitor as the foreign firm's reaction function will be kinked where the quota is binding.
On this issue, Hwang and Mai (1988) have shown that if the home firm's conjectures were  more (less) competitive than Cournot, the effect of the quota is to raise (lower) domestic prices relative to those that would have prevailed in the presence of tariffs.
In other words, in an oligopolistic framework, the non-equivalence between an optimal tariff and a quota arises since the latter changes the behaviour of the competing firms.
The quota-equivalent of the tariff is derived by substituting (22) into (17), which, after rearranging gives the following expression: where  Since, following the imposition of the quota, the home firms act in a Cournot manner (i.e. they know that there will be no response from their foreign competitors) and that the foreign firms' conjecture is undefined where the quota is binding .
the effect of the import quota on  is given as: which is derived from (17), where  is the Cournot -equivalent conjecture for the domestic firms.
and  can be derived in a similar manner, their solution being: where 
5.
Model calibration
Given the theoretical structure, the demand system in equations (1)—(4) was calibrated for the year 1986 using price, quantity, and elasticity data as presented in Table 1.
and  are the average selling prices of the dominant    firms and blenders respectively over the year 1986 based on reported prices in the UK farming press.
and  are derived from Fertilizer Manufacturer Association data and other farming and trade sources.
The value of the elasticity of demand ε is based on an estimate by Metcalf and Cowling (1967), although more recent estimates by Burrell (1989) suggest a similar value.
No estimate of  ta; is available for the UK, so a value of 2.00 was assumed, which compares with an Australian estimate of 1.7 (see Higgs, 1986).
, the operating costs for the dominant firms, are based upon reported levels in Challinor (1987) and the UK farming press, whilst , the blenders' operating costs, are assumed to be 10 below the selling price P 2 (see footnote 4).
Having calibrated the model, the parameter estimates shown in Table 2 are consistent with equilibrium in the UK fertilizer industry in 1986.
Estimates of the conjectural variations parameter  and , derived from (15) and (16), are presented in Table 3.
For the purposes of comparison, the Cournot-equivalent values of  and  are also shown.
The values indicate that, given the assumptions made about firms' costs, the dominant firms were acting slightly more competitively than Cournot while the blenders were acting less competitively than Cournot.
The parameters in Tables 2 and 3 can now be used to simulate the effects of trade restrictions.
6.
Policy evaluation
he evaluation of the UK government's import quota and the EC minimum import price is based on comparing the observed quota with the quantity    restriction shown in equation (23), and on comparing the EC minimum import price with the optimal tariff using equation (22).
It is assumed for the purposes of evaluation that either one or the other policy was implemented rather than both simultaneously.
The levels of the actual policies and the optimal policies are shown in Table 4.
The UK quantity restriction is clearly more restrictive than the tariff-equivalent level for a quota.
In contrast, the tariff-equivalent of the EC minimum import price is slightly less restrictive than the optimal level of a tariff.
A welfare evaluation of the different policies is shown in Table 5, given welfare as defined in (20), where the equilibrium values of prices and quantities are derived from the solution to equations (3),(4),(16), and (24).
Several points can be made about the results in Table 5.
First, both the UK quota and the EC minimum import price imply a welfare loss relative to the pre-policy level, although the EC policy was less restrictive.
The UK quota reduced welfare by 14.33% while the EC minimum import price lowered welfare by approximately 2.01%.
Second, both policies represented a welfare loss when compared with the optimal tariff and equivalent quota policies, with the relative welfare loss from the UK quota policy being greater than that from the EC policy (-5.98% as opposed to -2.18%).
Third, the results clearly show the non-equivalence between tariffs and quotas in the presence of oligopoly.
The optimal tariff increases welfare only marginally, its main effect being to   redistribute welfare from farmers to government.
The tariff-equivalent quota reduces welfare for two reasons: first, the government fails to capture the quota rents, though, even if it did so, welfare would still be below the pre-policy level by 4.15%; second, this loss of 4.15% is caused by the quota changing firms' behaviour.
This non-equivalence between the optimal tariff and quota is in accord with Hwang and Mai's (1988) results that domestic prices will be higher if initially the domestic firms were playing more competitively than Cournot.
Inspection of Table 3 confirms that this was the case in the UK fertilizer market in 1986.
Note also that with both the import quotas, the increase in the domestic firms' profits was substantial.
The results suggest, therefore, that tariffs are preferable to quotas if the market was initially more competitive than Cournot.
7.
Sensitivity analysis
It may be argued that the results are likely to be sensitive to the assumptions of the model.
In this section, therefore, the effects of introducing economies of scale and changing the elasticities data on the policy outcomes are examined.
7.1.
Economies of scale
From the discussion in Section 1, it is quite clear that the scope for economies of scale is an important characteristic of the fertilizer industry.
However, in the calibration model, constant marginal operating costs were assumed.
The question therefore arises as to the effect of decreasing costs on the welfare outcomes of the various trade policies discussed.
There is no simple or tractable means of introducing decreasing costs in the model given the data available on the UK fertilizer industry.
Therefore, an external estimate of economies of scale was used to ascertain the likely change in costs.
Unfortunately, Pratten (1988)— the main source of estimates of economies of scale — provides no information on the cost elasticity for the fertilizer industry.
Consequently, it is assumed that the scope for economies of scale in the fertilizer industry would be similar to those in the petrochemical industry.
While no information on the nature of the cost function is provided, Pratten (1988) reports that the change in costs by reducing output to one-third of the minimum efficient scale would be 20%.
This figure was used to proxy the effects of cost changes in the UK fertilizer industry following the introduction of trade policies.
The results are presented in Table 6.
It is clear from Table 6 that making an allowance for (extensive) economies of scale has an effect on the trade policy outcomes.
In the case of the UK quota, welfare is reduced by an additional 3.27% relative to the free trade case, while with the tariff-equivalent quota, welfare is reduced by 3.23%.
Therefore, the   effect of economies of scale is to exacerbate the anti-competitive effects of a change in the dominant firms' conjectures.
The price policies are less sensitive to the inclusion of scale economies; with the EC minimum import price, welfare is reduced an additional 0.08%, while with the optimal tariff, welfare is increased by an extra 0.24%.
7.2.
Data on elasticities
Since data on the elasticities of demand and substitution came from external sources, it is also important to consider how sensitive the results are to changes in the values of these parameters.
Thus, the model was recalibrated with alternative values for ε and  ta; and the effects of the trade policies were subsequently derived.
The results are presented in Table 7.
The central column shows the original results derived above, and the first and third columns show the effects of varying the values of the elasticities of demand and substitution respectively.
The above sensitivity analysis suggests that as the values for ε and  ta; increase (in absolute terms), the gains from the optimal tariff rise, while the losses from the equivalent quota increase.
Importantly, the tariff-equivalent quota appears to be relatively sensitive to the value of δ.
8.
Summary
This paper has evaluated trade policies introduced into the UK fertilizer market by the UK government and EC Commission in 1987.
Specifically, the quota imposed by the UK government and the EC minimum import price have been considered in the context of the role of trade policy in imperfectly competitive markets.
Using a theoretical model originally suggested by Dixit (1987), the UK quota has been shown to have been more restrictive than a tariff-equivalent quota, while the EC minimum import price was less restrictive than an optimal tariff.
The non-equivalence of the optimal tariff and its quota-equivalent arises from the way in which firms' behaviour changes in the presence of quantity constraints.