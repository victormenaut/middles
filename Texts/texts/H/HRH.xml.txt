

INTRODUCTION
Management is the process of planning, organising, leading and controlling the efforts of organisational members and the use of other organisational resources in order to achieve stated organisational goals.
This definition (Stoner, 1978) covers the most important aspects of any field of management, from first line supervision to the top management of a multinational corporation, in any field of endeavour.
There are several forms of management at all levels, each with its own characteristics and much has been written about most of them.
The broadest practical division is into functional management and project management and this book is about the latter.
There are two reasons for this.
The first is that the book is written for engineers; project management is for many the first form of management they encounter and the first form to which they are appointed.
The second is that teaching on the subject in engineering courses has not always prepared engineers adequately for this progression.
Functional Management
Functional management at any level is concerned with the running of an organization in being, that is charged with providing on-going services of a specified nature in an economic manner for as long as the company is in business.
By the nature of his responsibilities and accountability to his superiors, the manager wishes it to operate on a minimum variety basis.
This has benefits in reduction of training, creation of competence through experience and reduction of day-to-day supervision.
These features represent economic gains.
Project Management
Project management is concerned with achieving a specific goal in a given time using resources available for that period only.
A project is a  bounded, as distinct from an on-going, activity, in terms of time and resources.
Since a given project is very rarely repeated, project management is the business of managing variety; benefits tend to flow from the effective exploitation rather than the reduction thereof.
Experience of the realities of management in a specific organization is needed to develop judgement and practice in the relevant skills.
What a book can do is to present current thinking on the subject in a logical pattern in language familiar to the reader, so that the concepts of project management are no longer foreign.
While unable to solve the reader's specific problems, a book can provide tools and concepts that help the individual to tackle them systematically and with greater insight.
The same information will help members of a project team to understand what is going on, and why, strengthening their contribution and going some way to prepare them for the project management role when it comes their way.
The eminent scientist and journalist, Lord Ritchie Calder (1963), defined in another context the added ingredient that distinguishes a good engineer from a good engineering project manager:
The basic scientists are the ‘Makers possible’, the technologists are the ‘Makers to Happen’, the technicians are the ‘Makers to Work’…the commercial scientists are the ‘Makers to Pay’.
It is a reasonable expectation that many more developments and innovations would have achieved commercial as well as technological success if the commercial scientists had a more profound understanding of the problems and achievements of the other three, and vice versa.
A question worth asking is whether it will be easier to teach competent makers in the first three categories, in which engineers are well represented, enough about the work of the fourth to understand how a company works, or to teach the fourth about the first three to the same standard.
The Engineer's Contribution
In order to be commercially successful it is not sufficient for a product or a process to be technologically innovative, though this will stimulate interest — from the competition as well as the customer.
It must be designed, engineered, produced and marketed to provide the user with the required quality in terms of performance, reliability and appearance, for its design life, at an acceptable price.
Less obviously, a good product will be capable of being handled efficiently by every department in the company that makes contact with it, however slightly.
To make the maximum contribution towards creating the commercially successful product — success in other fields is of trivial interest — engineers must face up to some questions that they may initially feel poorly equipped to answer.
Early in a career it may not be the engineer's responsibility to provide the answers to these questions, but inability to discuss them with those who have that responsibility will result in the engineering dimension being omitted from the discussion.
For the project this may lead to suboptimal results and for the engineer to suboptimal progress in the company.
A first condition for the company to react to the opportunities and constraints offered by the engineering dimension is that the individual engineer should have insight into the responsibilities, problems and attitudes of the other company functions.
What Questions?
How many projects ?
The answer to this question is crucial to the efficient utilization of R&D resources, which are always limited.
If too many are undertaken the average time to completion will be unacceptably long.
If too few, there will not be enough new products.
How much investment ?
How is the amount to be spent on R&D decided? is the case for engineering resources put forward in the right way at the right time.
Which Projects ?
How are proposals for new products collected and analysed?
Does the form of analysis influence the priority they are given?
What resources, when ?
How are projects planned and monitored?
How are the project plans aligned with the corporate plan?
What design approach ?
How is the appropriate quality level established and controlled?
How is it decided whether to buy an item or to make it in-house?
What management style ?
How does the project manager organize and supervise the people in the team?
What problems and what guidelines are there?
What communications ?
Is communication difficult?
If so, why?
What impact do national culture and internal structure have on the problem?
What about people ?
Social theories abound.
Do any offer help to the project manager in motivating the team?
Do any help in choosing project managers?
Engineering and Management
The questions listed above are typical rather than comprehensive.
They are nearly all about management, which has much in common with engineering.
Both managers and engineers achieve their results through other people.
Both are obliged to make decisions on incomplete information.
Both have a responsibility for design, the manager, of people systems, and the engineer, of hardware systems.
Both have long time constants; what they do may take months or years to show results.
Company Functions
A feature of project management is that it interacts with the company functions managed on the minimum variety basis referred to under ‘functional management’.
An example is the manufacturing function where product variants are detrimental to productivity.
This creates problems of organization and communication.
A much simplified model of the main interactions is given in Figure 1.1.
The main functions in an engineering manufacturing company are commercial, R&D, manufacturing, financial control and top management.
There are specialist subdivisions of these functions and a number of other departments which have significant contributions to make but are left out to avoid complexity in the diagram.
The first three functions form overlapping segments of the circle representing the company as a whole.
Top management is represented by the centre circle and has the responsibility of arriving at and promulgating the corporate plan defining the company objectives.
Departmental objectives are required to harmonize with these.
Planning
Planning and plans occupy much of a manager's time at any level in the company, with important consequences.
The major difference between levels is the time horizon, or distance ahead that the plans attempt to cover.
A first level, supervisor's plans probably concern the next day or the next week while the chief executive officer should be thinking in terms of the next decade in general terms and the next four or five years in more   detail.
A useful division of plans is between strategic plans and tactical plans.
Strategic planning is the business of deciding what the main goals of the organization are and what policies should be adopted in pursuing them.
Tactical planning is shorter term and involves deciding how the resources of the organization shall be used to help reach its individual strategic goals.
Goals and objectives are similar in that both provide a direction and a target for company or departmental activities.
The terminology of planning is far from standardized.
In this book the essential difference is taken to be that an objective must be quantitative in terms of results and time, while a goal may be less rigorously specified.
Strategic planning, because of its greater time horizon, must rely on long term forecasts and is therefore more uncertain.
The choice of goals is critical and may demonstrate whether or not the company really understands what business it is in.
For many years American railways appear to have assumed that their goal was the building and running of railways.
This led them into feeling that airlines and motor car  manufacturers were their main competitors since they took passengers away.
Later redefinition of the goal as ‘providing transportation’ removed the blinkers that had prevented them from realizing that cars were made largely in Detroit but had to be received in good condition in every State, and that driving them to their destination was expensive.
By investing in car transporter rolling stock the railway companies were able to offer the manufacturers a service economically beneficial to both.
Strategic planning is also vulnerable to sudden changes in the business environment.
In the 1970s pollution control was forced upon national and federal governments by pressure groups.
Departments were set up to develop sensors and monitoring systems.
The whole of Holland, for example, was to be covered by SO 2 sensors at different heights, feeding into a computerized system drawing iso-concentration lines on a map of the country.
Part of it was actually built and installed.
Then came the oil crisis.
Unfortunately maximum boiler efficiency and minimum smoke production are incompatible and as fuel saving became government priority the system was not completed.
Financial Control
The last function in Figure 1.1 is financial control and, is can be seen, it makes contact with all the others.
This is a powerful department, partly because of its responsibility for ensuring the financial health of the company but also because it controls the language and format in which the other functions draw up their expenditure plans and report progress against them.
Commercial
An important contribution of the commercial function is to be the main source of information on the market in which the company operates, its needs, preferences, buying patterns and price structure.
Some companies have a department labelled ‘marketing’ to discharge this responsibility.
Others take the view that marketing is part of everyone's job and that R&D, in particular, is expected to supplement other inputs by making direct contact with users of company products and with competitive activities.
Such companies often adopt a blanket term such as‘commercial’ to cover this and other customer-oriented activities.
Research and Development
The contribution of R&D is primarily technology as applied to products and processes.
This is particularly true in the specification, design and  initial production of a new product.
During the last stage it is not uncommon for the project team to spend the majority of its time in production departments.
Manufacturing
The main contribution of manufacturing is the selection and operation of established production processes and the management of an agreed production programme.
In the general case manufacturing costs are high compared with those of other functions so the function has a massive influence on company productivity.
Functional Interfaces
The activities described so far have been concerned with periodically recurring factors relating to corporate planning and monitoring of results as part of functional management.
The three functions just described also have responsibilities in the field of project management.
These come into play after corporate plans have been formulated and are concerned with implementing them.
The activity takes place in the overlaps between the three segments of Figure 1.1.
The overlap between commercial and R&D is where the specification of new products is formulated.
The rationale for this is clear.
There is no point in R&D developing a new product unless commercial thinks it can sell it.
While it may not succeed where it thinks it can, if it thinks it can't the probability is that it won't.
Conversely there is no point in commercial selling a product that R&D says it can't develop.
A practical solution is for the two functions to be jointly responsible, within the constraints of the corporate plan, for product specification, negotiating with each other until an acceptable compromise is reached.
Between R&D and manufacturing the overlap is on the design of the new product.
There is no merit in R&D designing something that manufacturing has not the resources to produce efficiently.
This is not to say that R&D should not think in terms of processes that do not currently exist, but it emphasizes the need for discussion and agreement on how they shall be provided, in time for them to be in place when they are needed.
This involves building any investments required into the company budget as well as providing against project slippage because of any additional prototype stages the new process may make advisable.
The overlap between manufacturing and commercial is the manufacturing programme.
Manufacturing fewer units than can be sold either reduces revenue because orders are not accepted or makes for late  deliveries if they are.
Producing more units than are sold builds up surplus stock which is a commitment of working capital.
The costs of this are the interest that must be paid on it and the opportunity cost of being unable to undertake some other activity because of lack of funds.
The newcomer to industry should note that the above discussion is not about organization and will not correspond to any official company hierarchical diagram.
It attempts to model operational interactions between company functions in one way that is logically sound.
There are companies that work in quite different ways; they do, however, have the problems that this model illustrates.
The Format of the Book
Chapters 2 through 16 introduce concepts and techniques found useful in dealing with the questions outlined above and describe some of the relevant aspects of the company functions forming the interfaces.
Each chapter begins with a summary of its contents and ends with a review of what the reader should have learned from it.
This is followed by references and a short reading list.
Chapter 17 is a synthetic case study of a board meeting in a small engineering company, illustrating some of the pitfalls attending management decisions.
Chapter 18 describes, and provides a program listing for, TOWER-8, a Project Management Game, written for the BBC micro-computer.
It simulates a design and build project on a compressed time scale and models the commercial consequences of designing, planning, estimating, tendering for and building to, a contract and specification.
It can be played competitively between syndicates or used to experiment with changing parameters as in sensitivity analysis.
A glossary of terms used in the text is given at the end of the book.
PRODUCT STRATEGY
In order to provide maximum benefit to the company, new products must satisfy the requirements of the corporate plan in terms of the market needs they fulfil and their price/performance ratio.
The specification of a product and the allocation of resources to develop it are the later stages of a series of planning loops.
The starting point of the series is a statement of company objectives as the basis of a company strategy.
Strategic planning involves top management and the main company functions in the earlier stages, and departmental management in the later, as they progress from the broadest market and company considerations to the details of specific product launches.
Gap Analysis
As indicated in Chapter 1, top management, led by the chief executive officer — usually the managing director in the UK — has the responsibility for creating the strategic plan for the company.
This must include product policy, marketing policy and sales targets as a minimum.
In other words it should express, in terms meaningful to all company functions, agreed targets as to what product range(s) are to be sold in which market(s) in what quantities per year over the planning period.
It does not at this stage include any specific new products.
What top management needs in order to move towards this plan is an analysis of its present and potential future situation in terms of products and their associated markets.
This amounts to a portfolio or collection of possibilities ranked on an agreed basis to enable decisions to be made on allocation of resources.
There is no short cut to this goal; it demands systematic collection and analysis of a large amount of quantitative and qualitative data.
It also requires presentation in a manner acceptable to both generalist and specialist managers.
A first step in the process is to examine the gap between what the company income is likely to be from the products now in production and the income that it wishes to have over the next planning period.
The first step is to take the oldest product and plot the estimated annual revenue from it over the planning period as shown for product A (Figure  2.1).
The next oldest product is then plotted in the same way, but using the graph of product A as the base line where both are forecast to be in production together.
Other products are treated in the same way, building up a graph of the aggregate sales of existing products.
The cumulative revenue from these is represented by the area under the last graph, that of product D. The provision of the data so far is the responsibility of the marketing/sales function.
The next step is to draw, on the same axes, a line representing the target income for the planning period as proposed by top management as part of the corporate plan.
Between this line and that representing the sales of existing products there is now a gap.
The strategic planning goal is to produce plans that will present a credible set of proposals for filling this gap with sales of new products or services.
Strategic Planning
Strategic planning is, like all planning, an iterative process.
It is, like politics, the art of the possible.
It is concerned with what is possible in the context of the strengths and weaknesses of the company and the threats  and opportunities it perceives in the world outside the company, the environment in which it operates.
From the analysis of these factors tentative goals are selected.
These are broad descriptions of how the company wishes to see itself developing over the planning period.
Further discussion and analysis refines these descriptions until they can be converted into company objectives.
The essence of an objective is that it is specific, quantitative, on a time base and measurable.
Thus the statement that the company needs a new product in the X range is not an objective but may be a goal.
As an objective it must also cover the market it will sell in, a sales estimate and a target date for launching, whether it replaces or supplements other products in the range and any other relevant information defining the nature of the product.
The stages of strategic planning are shown in Figure 2.2.
The triangle formed by nodes 1, 2 and 3 is iterated until there is agreement between marketing and R&D that such a product has a high probability of being (a) feasible and (b) viable and that measurable objectives have been unambiguously recorded.
This is part of the activity in the overlap between commercial and R&D in Figure 1.1.
The next loop to be tackled is 3, 4, 5.
The departments involved must now look closely at the stated objectives and work out how they will discharge their responsibilities, going round the loop until there is agreement that this can be done.
Alternatively, if this agreement is not reached, the objectives reached in node 3 must be reconsidered.
A compromise on objectives may be needed to allow the process to proceed.
When the departmental plans and the (if necessary) revised objectives are compatible the triangle 5, 6, 7 can be discussed.
The objectives thus agreed can probably be achieved in more than one way.
The purpose of the loop 5, 6, 7 is to focus the attention of the department on the problem of optimizing the solution from all points of view.
In the case of R&D or engineering this will amount to looking in more detail at the possible design approaches to the new product and selecting the most promising for development.
Rigorous analysis at this stage is needed to ensure that the optimum option is found.
Unfortunately there is always great pressure on the designer at this stage, however long the preceding stages have taken.
The danger is that the designer will accept the first feasible design solution found, rather than systematically search for the optimum.
The benefits to productivity and profitability of really good design are such, however, that such pressure should be resisted.
The company memory of the late launch of a profitable product will convey more merit than the prompt launch of a failure, whatever the arguments put up at the time.
The progress of the design chosen — as with that of the marketing activities in support of the launch — at node 8 is monitored at node 9 and control actions decided by comparison with the objectives established at node 3.
Break-Even Analysis
In the course of departmental planning to meet the demands of the corporate plan it is necessary to estimate the production costs and the sales volume that will be required to meet the income and profit targets for a given new product.
If the new product is similar to one already in production, costs may be estimated at an early stage of design with adequate accuracy by discussion with the manufacturing department.
If this is not the case the design may have to be taken nearer to completion before an estimator can provide tentative figures.
Part of the cost will be independent of the volume of production, representing a share of the fixed costs of running the factory, such as rates, heating, etc.
The other part will reflect the cost of materials and labour used in producing the product and will therefore be, to a first approximation, linearly related to production volume.
The estimated income from the new product is, again to a first approximation, linearly related to the number of units invoiced.
The profit and loss estimate for the new product can thus be represented by Figure 2.3, the break-even analysis diagram.
The fixed costs of manufacturing the new product are represented by   the horizontal line since they are not influenced by the volume produced.
The variable costs start from zero, since labour and material are not consumed until production starts, and in the simple model rise linearly with volume.
The total production cost is the sum of these two quantities, giving the line so marked.
The revenue from the product also starts from zero when none are sold and rises proportionately to invoiced sales.
For the product to be viable this line must have a steeper slope than the total cost line and intersect it at some point.
This is the break-even point where total cost equals total revenue.
At sales levels above this volume the product is profitable, the profit being the area between the revenue and cost lines.
Below this volume the product makes a loss.
The viability of the new product at various sales and production levels can thus be visualized and its contribution to the planned income gap assessed by drawing its graph on top of the existing product income line in Figure 2.1.
The Marketing Function
Marketing is defined by the Institute of Marketing as:
The organisation and direction of all those business activities involved in assessing and converting customer purchasing power into effective demand  for a specific product…so as to achieve the profit target…set by the company.
While many companies have a specific department with this title, there are those that take the view that marketing is the concern of all, while agreeing that the major role in providing market intelligence and identification is a commercial one.
These companies argue that labelling a specific group ‘Marketing’ may tend to (a) create a hands-off attitude within it and (b) demotivate other groups with a part to play.
Such companies often use an umbrella title such as‘commercial, to designate a group including a specialist marketing activity among its responsibilities.
It is vital to the understanding of company needs that those engineers responsible for design, development and manufacture make direct contact with the world of the user and the competitor in sufficient depth to have valid views on the market as it affects their tasks.
Market/Product Analysis
The analysis of markets and products as they relate to the customer is a prime marketing task.
It often takes the form of a matrix representing the business on two axes, ‘market attractiveness’ and ‘business strength’.
In one form — the earliest is usually attributed to the Boston Consulting Group, USA — a 3 × 3 matrix is set up on the two axes as in Figure 2.4.
Business strength increases vertically and market attractiveness increases from right to left.
Business strength is a measure of the proficiency with which the firm deals with all aspects of the specific product/market complex in question at the present time.
It does not necessarily reflect the trading position of the company as a whole.
Market attractiveness is a measure of the estimated potential benefits to the company from entering the market/product complex proposed.
It does not necessarily describe any intrinsic quality of the market itself.
The nine squares are viewed as three groups of three.
Products which fall in the top left-hand group are those which are in attractive markets and which the company handles well.
In some of these cases the company may be the market leader, that is to say it has the largest single share of the market and because of this has a major influence on market prices and product design.
They represent present success and future expansion.
Products forming the bottom right-hand group are in a market that does not offer future prospects of expansion and profit to the company.
Further, they are not dealt with very successfully by the company from one or more aspects such as technology or sales or servicing.
They   represent an area of the business whose prospects are so poor that they do not justify any further investment.
The group of products falling into the three diagonal squares are between these two extremes on one or both axes.
They may be in attractive markets but poorly handled by the company or vice versa.
They may be less than satisfactory on both counts.
The position in the matrix indicates strategy to be adopted.
Those in the top left-hand group represent the present and future of the company and should be invested in to push them further into the corner.
Those in the bottom right-hand group should probably be got rid of by withdrawing all investment and selling everything that can be made.
This strategy is capable of converting them from the least profitable product group to the most profitable — for a time.
If all investment and most resources are withdrawn, whatever is sold has a high profitability and generates cash which is available to invest in the top left-hand group.
The diagonal group is treated as individual cases.
Figure 2.5(a) illustrates the analysis of a product in four markets and Figure 2.5(b) the analysis of a group of products in one market.
The size of the circles indicates the estimated market size in both cases.
A merit of this approach is that the variables are visually presented in a style that can be understood by generalists and specialists alike.
The clear indications of the appropriate direction in which to move any product or   market for the benefit of the company makes for good communication and useful discussion.
Table 2.1 shows the resulting policy indications for the more important business aspects.
The method of scoring products on the two axes in order to fit them into the matrix is that of multi-dimensional screening, taking into account the attributes of the competition as well as the two dimensions already mentioned.
It involves detailed examination of commercial records and discussion with virtually all company functions that come in contact with the products and markets to be analysed.
Multi-Dimensional Screening
The method of positioning a product in the matrix is by marking each axis with a scale, typically 1 — 10, and scoring the products in terms of their   market attractiveness and business strength.
In order to do this the factors influencing the scores must be chosen, identified with one of the two axes and judgements made on their order of importance.
These judgements are interpreted as a weighting number for each factor.
A scale of 1 — 5 is a good compromise between difficulty in making judgements and failing to differentiate between the importance of factors.
Each of the factors to be taken into account is then scored, for the product being examined, in relation to the other products on the list.
The scale is not very important provided it is chosen at the start and not changed throughout the exercise.
This provides a large number of scores for each product, one for each factor, which is then multiplied by the appropriate weighting, giving a weighted score.
The sum of the weighted scores for each axis is a measure of the product ranking on that axis and the set of scores is normalized to a scale of 1 — 10.
By superimposing the same scale on the two axes of the matrix, the product can then be positioned, represented by a circle of which the area is proportional to the forecast market size, possibly with another inside it representing the estimated market share.
The screening can usefully be carried out under three headings:
(1)
Market attractiveness
(2)
Business strength
(3)
Competitive activity
The following factors are typical of those included but are neither comprehensive nor universally relevant.
The factors chosen must relate to the company, its products and its market position.
(1) Market attractiveness
Market segmentation (what part of the market does it fit into?)
Sales territories
Sales organization
Type of market; risk involved
Market size — growth forecast, total and accessible
Market sensitivity
Who benefits?
Selling price brackets
Purchasing influences
Position in life cycle
Political factors
Social factors
(2) Company strengths
Policy
Philosophy
Technology
Resources
Competence
Training
Management
Collaborators needed?
Probability of success
Costs
Coverage of problem areas
(3) Competitive analysis
Alternative products
Conflict of programmes
Funding of programmes
Other countries' activities
Market cover
Market segmentation
Table 2.2 gives an illustration of the use of the multi-dimensional screen; the first three elements are applied to the proposed new radiation pyrometer, data on which is given in the Pyro Instrument Company case in Chapter 17.
The weighting for each element of the screen is on a scale 1 — 3 and the score on a scale 1 — 5.
Some factors in the competitive analysis will relate to market attractiveness.
If the competition offered no alternative to the proposed new product it would score highly.
Since the competition is the market leader in radiation pyrometers it attracts a low score.
The other two factors in the competitive analysis relate to business strength.
The scores are fairly neutral.
The last two sets of totals are added to the market or business total as appropriate and the new totals normalized, that is, presented as totals out of ten.
For market attractiveness the possible total is 45 + 15 from competitive analysis, that is 60.
The actual total is 11 + 2 from competitive analysis, that is 13.
The normalized score is therefore 2.
For business strength the normalized score is 4.
The positioning of this proposal in the matrix, due to the elements listed above, is therefore 2 for the market attractiveness and 4 for business strength.
The size of the circle can be obtained from the Pyro Case exhibits.
R&D Resource Planning
The planning so far has taken into account the needs of the strategic company plan and the opportunities for achieving it in terms of products and markets.
Some of the company strengths and weaknesses emerging in discussion with marketing and other functions will have already acted to restrict these opportunities.
A further restriction directly influencing the capacity to undertake projects is the availability of R&D resources.
The company strategic plan will always include a budget allocating resources to each function, including R&D, in terms of staff and funds.
The amount actually available for the generation of new products is, however, much less than this figure.
This is because of the many other commitments of R&D, a fact not always grasped by top management.
Figure 2.6 shows the pattern of original R&D resources allocation from the strategic plan and the internal allocation of those resources within the function.
It will be seen that the top line of activities repeats the first stage of the company strategic plan as it affects R&D.
Consideration of threats —; what is the competition doing, are some markets closing?— and opportunities — are new components now available, can a new product create its own market?— together with company capabilities, result in an R&D strategy and plan which corresponds to the 3, 4, 5 loop in the company strategic plan.
Before the resources for the R&D plan can be allocated, however, other factors have to be taken into account.
There will be a number of on-going projects in the department and the resources for these must be maintained or the projects cancelled.
A second reduction of available resources is due to short term problems in other company functions.
Particularly in manufacturing, R&D is never quite free from its past projects and a substantial proportion of its resources must be earmarked for trouble-shooting   arising from, for instance, the need to find a substitute for an obsolescent component before it is in short supply.
Some resource will also be needed to deal with technical problems and enquiries from sales and commercial departments.
In a company involved in a fast moving technology and consequently a continuous output of new products, this commitment can amount to 30 per cent of total R&D resources.
Only then can the resources available for new projects be quantified and decisions made on how many and which to undertake.
At this point there is apt to be pressure on R&D to agree to undertake more projects  than is rational.
There is no doubt that the efficient way to plan projects is to take them in order of priority — the marketing function has a major interest in priority setting — and allocate to each in turn the maximum amount of resources it can usefully employ.
When all the available resources have been allocated in this way the remainder of the new project proposals should be formally declared inactive.
This practice will ensure that the average project duration will be minimized.
During and after this resource allocation planning, there will be pressure for more projects to be undertaken and some persuasive reasons for doing so will be given.
The best answer to these requests is to ask, in turn, which less important project(s) will be dropped in order to release the required resources.
The discussion frequently ends at this point, but the response either way should be recorded.
Memory of such discussions often proves to be rather selective.
Time and Meetings
Analysis and planning are accomplished by collecting information from the people who have it, ordering it in meaningful classes and discussing the results.
The process is iterated until a convincing agreement is reached on assumptions, estimates and plans for future action.
This is partly done individually but the discussion and agreement stages involve meetings within the project team and between members of the team and other company functions.
The nature of the meetings is illustrated in principle in Figure 1.1, in terms of the interactions between functions.
More specific indications are given in Figures 2.2 and 2.6 which show the stages of strategic planning requiring discussion and decision by two or more groups or individuals.
Criticism is often directed at the amount of time consumed by meetings.
Certainly one of the most important concerns of the project manager is to know how the time of his team, and particularly his own, is spent.
Time spent is not recoverable, while improved performance will often enable recovery of money in the medium term; thus time is the most scarce resource at the project manager's disposal.
Control of Time
Some of the project manager's time will be committed in advance to systematic management of the project.
The system should not in any way restrict the contribution of any of the members of the team; it should enhance their efforts by providing routine channels for the routine aspects of their activities, so that they are not obliged to devote intellectual effort  to them.
It should focus attention on the key results areas of the project and on the contributions of individuals towards achieving them.
It is the project manager's task to define the key results areas, to set priorities for the contributions and to ensure that they are adhered to.
The system must ensure that all involved are kept aware of progress, decisions and changes concerning them.
It must emphasize results rather than the work content.
The time not spent on this type of activity may be regarded as the project manager's discretionary time.
The efficient use of discretionary time is a decisive factor in a manager's effectiveness.
Time is wasted in a number of ways.
Bad organization is responsible for an excess of meetings because decisions that should be routine become unclear and demand further discussion, sometimes as a matter of urgency.
Bad information has the same effect.
These aspects of time control start from the communications problem, discussed in Chapter 11.
Most project managers, like most other managers, will offer an estimate of how much of their time is discretionary and how they allocate it.
In general they overestimate the amount available and underestimate the time wasted by being fragmented in small amounts on rather trivial matters.
The best way of establishing the facts is for project managers to keep a detailed log of their daily activities on, say, a half-hourly basis, for a week.
At the end of that time the conscientious manager may well find it difficult to credit the evidence.
Analysis of the record will throw light on the state of the project organization, the external influences on it and the manager's own working habits.
Once analysis of the log makes obvious any deficiencies in time allocation to specific areas, as it will, correction is simple and straightforward.
It is frequently not permanent, however, and the exercise can usefully be repeated at intervals, say twice a year.
It is essential that the project manager organizes the project so that he can give uninterrupted thought for adequate periods to the key results areas.
The human span of attention is limited; for most people an hour is a long time for concentrated thought on one topic.
Periods of this order should be allocated to the major problems and decisions, protected from disturbance.
Very few crises elsewhere are so disastrous that they cannot wait an average of 30 minutes.
Meetings
Meetings are, nevertheless, necessary.
The stage-by-stage process of strategic planning, illustrated in Figure 2.2, demands a series of meetings to assemble and discuss the relevant information, formulate opinions and arrive at decisions.
Meetings are also part of the communication system.
Some subjects are best discussed on a one-to-one basis; this requirement is treated in Chapter 10 under Objective Setting.
Others lend themselves to  group discussion where the moral support of peers encourages individuals to put forward their views in the presence of management.
Whatever the purpose of the meeting, it should be stated at the outset and adhered to.
The chairman must not allow the discussion to wander into other subjects unless there is a direct relevance.
The purpose should have been notified in advance so that those attending are able to assemble any information they need.
It should only be changed at the start of the meeting in the case of a genuine crisis.
An efficient procedure, essential if more than one subject is to be discussed, is to circulate an agenda giving the date, time, venue, subjects and a list of those attending.
The question of the size of meetings is important.
Large meetings are notoriously less productive and more difficult to handle, so the numbers should be restricted to those with a contribution to make.
A perception can arise, particularly in those organizations not accustomed to regular meetings, of personal status being reduced by not being invited to certain meetings.
This can often be dealt with by notifying the individual of the meeting and its purpose and saying that he will be welcome to attend if he feels that he has a specific interest in the proceedings.
The decision now being his, no status problems arise.
Unless he has a part to play he is unlikely to attend after the first appearance.
The purpose of a meeting can be whatever the project manager decides would be useful.
Typical purposes are:
(1)
To define a problem
(2)
To list alternative solutions
(3)
To decide on the best alternative
(4)
To plan future activities
(5)
To inform those present
(6)
To collect views
(7)
To discuss and clarify
(8)
To review progress
The location, starting time and duration should be included in the notice of the meeting to allow those attending to plan their time; also the agenda should be included in sufficient detail to indicate what preparation they need to do.
This information should be presented in a standard form, as should the minutes of the meeting, recording the essentials of discussions and the decisions made.
Wide margins should be left on either side of the minutes.
That on one side can be used to record the names of the individuals responsible for taking any actions that have been decided on.
The other margin is for notes by the person receiving the minutes after the meeting.
For routine meetings with fixed agenda, such as project review meetings, it is often possible for the agenda and the minutes to be the same standard form, one part of which records the information to be   considered (the agenda) and the other the decisions taken (the minutes).
The project review and resource allocation meeting described in Chapter 10 under ‘Matrix Organization’ is an example of this approach.
It should be remembered that in meetings of this type the minutes act as the authority for implementing the decisions.
The action, the person responsible and possibly the resources to be used, must be stated clearly and the chairman's signature appended with the date.
In project work there are a minimum number of formal meetings of the type described above necessary to monitor the progress of the project and authorize use of resources when needed.
There are a larger number of informal meetings not preceded by any significant preparation to deal with the day-to-day problems of keeping the project moving.
These are often ad hoc verbal discussions between the individuals directly affected, sometimes requiring official blessing, sometimes requiring no further action.
If any decisions are taken at either type of meeting they should be recorded.
In the formal case the record will be in the minutes; in the informal case a dated note should be inserted in the minute file giving the time, place and people present when the decision was taken and a brief statement of the decision.
Review
This chapter has:
(1)
Explained how the required company revenue from new products is estimated.
(2)
Described the sequence of stages of strategic planning.
(3)
Explained break-even analysis.
(4)
Defined the marketing function.
(5)
Described a technique of market/product analysis.
(6)
Described the process of resource planning.
(7)
Explained the importance of controlling the use of time and how to analyse it.
(8)
Described the part played by meetings in reaching planning agreements.
The next chapter explains the preparation of project and departmental budgets and the nature of accounts.
It provides insights into the purposes of balance sheets, profit and loss accounts and methods of costing projects and production.
BUDGETS AND ACCOUNTS
When product strategy has been established, planning of the R&D programme can proceed.
In order to deal with the problems of budgeting for this it is necessary to know something of company financial and cost accounting.
Two common forms of costing are discussed together with the make-up of project and departmental budgets.
A simplified model of financial structure is presented.
The Accounts Function
The envelope of project work has now emerged from the planning described in Chapter 2.
One of the resources involved is, of course, money.
All resources used must be monitored so that project progress can be compared with plan, and the monitoring of expenditure brings R&D into contact with the accounts department which exercises surveillance on all company financial transactions.
It is essential, but not always obvious, that R&D workers should understand what accountants do sufficiently to be able to communicate with them and on occasion to negotiate change in the detail of how they do it.
An individual accountant will often be charged with the task of recording project expenditure so that, as at node 9 of Figure 2.2, it can be fed back and compared with the project plan.
In order to do this it will be necessary to have records of expenditure on materials and weekly time sheets recording time spent on individual projects by each worker.
The accountant can be of great help in a staff relationship to the project leader in keeping the project expenditure under control.
While the responsibility for doing so remains with the project leader, the accountant should be accepted as a part time member of the project team and encouraged to contribute ideas.
Product Cash Flow
The importance of project control can be illustrated by means of the product cash flow diagram (Figure 3.1).
The horizontal axis is a time base and the vertical axis displays positive and negative cash flow.
Negative cash flow is the expenditure incurred in designing, developing and making the new product, that is cash flowing out of the company.
Positive cash flow is that coming into the company, the revenue from the sale of the product.
Initially the cash flow will be negative, rising from a low figure due to the relatively small resources needed for selecting and planning the project, increasing steeply as investment in production plant and methods begins then returning in a curve to zero as development is completed and production begins.
As planned, it might follow the solid line OAB.
The last point is the launch date from which sales and shipments take place along the line BD, rising to a peak, then falling to a point I where the product is withdrawn.
The reason for the fall in later years is that costs tend to increase as the product ages — apart from production costs there may be increased costs in selling against more up-to-date competition —; and sales tend to decrease.
The investment in the new product, the area under the line OAB, is not recovered until an equal area, represented by the space BCD appears under the revenue line.
It is not until point C on the time base that net positive cash flow begins.
The broken line CYH is therefore the planned revenue from the new product.
If expenditure on the project goes over budget, or the launch is delayed — perhaps because resources are not available as planned — the second sales line EG results from the delayed launch.
This, in turn, means that the start of net positive cash flow is delayed until point F and proceeds along the curve FXH.
Because of normal time delays, revenue continues for a while after sales cease.
Note that the product is withdrawn on the same date in both cases.
This event is determined by the market and the competition.
The fact that it was launched later will have minimal effect on the withdrawal date.
The consequence is that the revenue represented by the shaded area between curves CYH and FXH has been lost.
Not only is it not recoverable but it has taken place at the most profitable period in the product life, before the adverse effects of ageing have affected its sales and costs.
Not all the losses resulting from a delayed product launch are concerned with that product.
The fact that resources which should have been released at point B are not available to work on the next planned project until point C is an ‘opportunity cost’ and will quite possibly result in that project being delayed too.
Monitoring of project progress and expenditure, to enable prompt corrective measures to be taken when needed, is essential if project plans are to be achieved and planned return on investment in new products realized.
What Does the Accountant Do?
The accounts department is responsible for ensuring that top management is aware of the financial state of the company and for advising it on appropriate actions to ensure that it remains sound.
The financial director is usually a member of top management and, in UK industry, many chief executive officers have come up through financial functions.
The main documents that are used to convey the financial information are the balance sheet, which lists the sources of the company's capital and how it is used in the company, and the profit and loss account, which quantifies the aggregate expenditure and income of the company.
It also shows how the company profits are distributed.
Figure 3.2 is a much simplified model of a company financial system showing how the two documents interact.
company or in a unit trust holding such shares in its portfolio, and the financial institutions.
The latter are banks, insurance companies, pension funds and the like, and they have, of course, much bigger shareholdings and consequently more influence.
Legally, the shareholders are the owners of the company and the directors are elected to run it on their behalf.
Company capital may be viewed as being of three types, short term, medium term and long term.
The first is in part provided by banks in the form of overdrafts, that is, the company may, by agreement, write cheques for larger sums than it has on deposit.Interest is payed to the bank on the difference at a rate which reflects the bank's view of the soundness of the company and the prevailing rates in the money market; these rates will vary from time to time .
A less obvious source is the company's creditors.
If a company does not pay a supplier's bill on time it has the use of that money until it does so.
Many companies delay payment as long as they can while maintaining an accounts section charged with chasing their own debtors for early settlement of accounts.
A medium-term loan at a fixed, but higher, rate of interest may be provided by banks and financial institutions such as Finance for Industry.
Large loans are often provided by syndicates of such bodies.
A debenture is a type of loan in which a fixed interest security, dated for redemption at its nominal value, is issued in return for a term loan.
Long-term capital is mainly that provided by selling shares as described above and by retaining profits within the company.
The major difference in practice between share (equity) and loan capital is that the interest on the latter must always be paid whether the company makes a profit or not.
Dividends, which are what the shareholder is paid for the use of his money, are recommended by the directors and authorized by the annual general meeting of shareholders.
When profits are low they will be reduced or ‘passed’.
Since shareholders thus take greater risks than banks, their return in good times is higher.
The sum of all these classes of funds is the capital employed in the company.
It is used to acquire fixed assets such as buildings and plant and to provide working capital.
This is the money required to make up the difference between what the company can pay and what it owes at any one time or, in accounts department terms, net current assets minus current liabilities.
The essence of the balance sheet is that the sum of the fixed and current assets shall equal the capital employed, thus explaining its use in the company.
The ratio of fixed interest debt to shareholder's capital plus the debt is known as gearing, or leverage or debt ratio.
High gearing increases the shareholder's risk.
Conservative financial management in the UK usually recommends a level of about 30 per cent.
Lower ratios do not make  proper use of cheaper capital; higher ratios risk an unacceptably high fixed-interest burden in bad times.
Profit and Loss Account
As seen in Figure 3.2, capital employed divides between fixed assets and current assets.
The former represent buildings and plant.
Their replacement when worn out is provided for by depreciation.
Current assets are those that can be turned into cash at short notice, in addition to cash in hand or at the bank.
The difference between sales and cost of sales is the gross profit which is distributed as tax and dividends.
The remainder is retained as addition to capital.
Depreciation
Provision for depreciation of a company's assets is always made before calculating profit.
The rationale for doing so is that consumption of capital assets is one of the costs of earning the revenues of the business and as such is paid from untaxed income under the tax authority's rules.
Since the actual amount needed can only be known accurately at the end of the life of the asset, which in turn may not be known in advance of the event, it is necessary to estimate both.
Company financial policy will contain guidelines for doing this, based on past experience.
Care should be taken to ensure that such guidelines are used only in appropriate cases.
At a time when computer technology was advancing rapidly and any given computer-based business management system tended to be technically obsolescent in a short time, a company made a reasonable decision that all such computers would be depreciated at 50 per cent per annum, making prudent financial provision against their short useful lives.
When the company began to embody computers in its products, the engineers responsible were dismayed when they found that a 50 per cent overhead on the purchase price of the hardware had been charged by the estimators.
This was, of course, a misinterpretation of the accounting rules, since the two cases are not comparable.
If not detected and corrected this error would have priced the said products out of the market.
There are a number of methods of calculation of annual depreciation.
Of these the two most generally used are the ‘straight line method’ and the ‘reducing balance method’.
In the first method the scrap value of the asset at the end of its life is estimated and deducted from its original cost to the  company.
The difference is divided by the number of years of estimated life and the resulting figure deducted from revenue as an annual expense.
In the second method the expense charge is set at a constant percentage of the written down value (WDV) of the asset — the original cost less the cumulative depreciation charge — resulting in a diminishing absolute amount to be set against revenue.
The percentage is calculated to reduce the WDV to the scrap value over the estimated life of the asset.
This means that the charges are much higher in the early years, resulting in both profit and asset value being shown as artificially low.
This makes capital employed difficult to estimate for purposes of financial ratio analysis.
The written down value has no relation to the market value of an asset and thus there can be a profit or loss in the accounts when it is sold.
In times of inflation the replacement cost of an asset may be much greater than its original cost.
This can be dealt with by revaluing the asset annually using special indices of cost of capital and adjusting depreciation provisions accordingly.
This is known as replacement cost accounting as opposed to historic cost accounting when the original cost is retained.
Annual revaluations of assets is not the practice in all countries.
Where it is not done, sale of the asset after some time has elapsed can generate substantial revenue at the cost of a relatively small disposal of asset value.
A major reason for depreciation charges in the accounts is to ensure that the costs of services provided by capital assets are included in the selling price of the company's products.
This is in fact more important from the company management point of view than the notion of building up funds to replace assets at a given date.
The amounts charged are treated as part of the net cash flow of the business and are available for use as transfers from fixed assets to current assets and possibly back again, in whatever way is most beneficial to the business.
What has actually happened to them is not always clear from the balance sheet.
The calculation of depreciation for tax purposes is governed by the tax authority's rules and is not necessarily related to the amounts actually charged in the business accounts.
In bookkeeping terms the depreciation amounts are regarded as debit entries in an intermediate account which is then transferred to the profit and loss account at the end of the accounting period.
The corresponding credit entries go to the asset account concerned, reducing the written down value, and to a depreciation reserve account.
Balances of both depreciation reserve and asset accounts are carried forward to the end of the useful life of the asset.
At this point any scrap value received is credited to the asset account and any additional money needed for replacement is transferred from the P&L account.
Any excess over cost of replacement is credited to capital reserve or a specific asset replacement account.
Financial Analysis
The balance sheet and P&L account provide quite a lot of information on the financial health of the company but it is in the nature of a snapshot of conditions at a particular moment in time.
One can tell whether a profit has been made in the period in question but it is necessary to have the results of several years trading, ideally together with several years forecasts, to form a judgement on the operating conditions, that is whether results are improving, stable or deteriorating.
It is quite possible for a company to show a trading profit on its normal operations over a year and to cap this by receiving an enquiry for its most profitable product many times greater than any previous order.
This will no doubt be welcome, but before accepting it the company should be sure that it is aware of all the consequences of doing so and can handle them.
An order of this nature will demand purchases of materials greater than before and probably take longer to fulfil.
There will consequently be a longer gap than usual between buying the material and receiving payment for the delivered order.
In this gap the outgoings of the company for its normal operations carry on.
Has it enough working capital to enable it to wait out this period without becoming insolvent?
Many companies have been obliged to cease trading while under the impression that they were doing well; the press tends to report that they were ‘overstretched’.
Cash Flow Analysis
A cash flow forecast, showing the sources and uses of money for the period in question is as important as the balance sheet in this situation.
It consists of a table, as typically shown in Table 3.1, setting out, period by period, the forecast flows of money into and out of the company.
The inflows would typically be revenues from sales of products, sales of fixed assets, issues of shares and loans.
Outflows would be costs incurred requiring payments to creditors — therefore not including depreciation —; and purchases of fixed assets or investments for either credit or cash.
Another column — budget — records the forecasts for the year and for each period and a third — variance — the difference between budget and actual for each period.
The cumulative totals for each column are also entered.
Analysis of the period and cumulative variances provide a control mechanism helping to ensure that each expenditure and commitments are not allowed to rise above the company's ability to provide the funds from its operations.
Ratio Analysis
Liquidity
Liquidity is the degree to which the company assets are in the form of cash or can readily be converted into cash.
As seen in the example given in ‘Financial Analysis’ this is not the same as profitability.
A company showing a profit in the accounts may suddenly be unable to meet its current obligations.
In extreme cases its creditors may then petition for it to be put into liquidation, that is convert all its assets into cash, usually on the unfavourable terms of a forced sale, in order to recover the sums owed to them.
A measure of the company's ability to meet its obligations is the current ratio: Currents assets are typically finished goods, work in progress, raw materials, cash and debtors.
Current liabilities are trade creditors, tax and any other creditors.
Suppliers extending credit to the company expect to see a substantial buffer of current assets to protect their claims and the ratio should not normally be allowed to fall below a given point.
What point this is will depend on the nature of the company's operations.
In batch manufacturing industry a figure of 2: 1 is regarded as satisfactory.
Some current assets are by nature needed to maintain the company operations and would not normally be available to meet short-term obligations.
The ratio measuring this ability, often called the ‘acid test’ is  A manufacturing company would be expected to maintain a 1: 1 ratio to be credit worthy.
A company buying on credit and selling for cash with a rapid stock turnover, such as a supermarket, might have no financial problems with a ratio of less than 1: 1.
As with nearly all financial data, changes in the ratios may be more significant than their absolute values.
Profitability
High liquidity ratios indicate short-term financial strength but do not measure efficiency of utilization of resources.
If this is poor, medium-term profitability will also be poor.
Three factors are involved:
(1)
Capital employed
(2)
Profit
(3)
Sales
The relationships between these three can be expressed as  The return on capital employed, which is the long-term measure of the financial health of the company, is affected by both the other ratios.
Anything done to increase either will benefit the company.
Turnover of assets, the second ratio, is a direct measure of efficiency of utilization of resources.
These ratios are most helpful when income is constant.
Unfortunately this is not always the case and gross fluctuations bring the risk of being  unable to meet interest payments on borrowings.
A ratio measuring this risk is  Evaluating this ratio for the extremes of fluctuation experienced gives an indication of the cover against the risk.
The greater the fluctuations the higher should be the ratio.
Stock turnover, as well as capital turnover, is an important ratio, reflecting both profitability and liquidity, as does the ratio of sales to debtors  These ratios indicate the speed with which stock and debtors are converted into cash, to the benefit of liquidity.
The interrelationship of the factors involved is manifest in the ratios.
It is important to look at ratio trends over extended periods in order to judge the progress of the company.
It may be helpful to management to express both stock level and debtors in terms of weeks of sales, giving figures more easily related to company day-to-day operations.
Investors
The above ratios are useful in managing the company.
Potential investors use others, some of which are published in the national press.
They will be interested in the return on investment and the risk involved.
The ratios relating to return on investment are  Some estimate of the risk taken by the investor can be derived from the ratios  
Creditors
The risk to the creditor, as distinct from the investor is indicated by the gearing of the company.
A high proportion of equity capital is a safeguard for creditors since they are entitled to be paid in full before the owners receive anything on liquidation.
Limitations
It should be borne in mind that these ratios are based on financial statements which only report on those ‘facts’ that can be expressed in money terms.
The inverted commas are used to emphasize that such statements contain estimates and personal decisions that are sometimes arbitrary and refer to specific accounting periods which may not be representative of the operational situation.
Those aspects of the business not capable of being expressed in financial terms may have an important effect on its success.
Use of the ratios as measures of the efficiency of individual managers should be treated with great caution unless they are part of a well-thought-out scheme of key results and objectives for individuals.
Ratios cannot usefully be compared between companies unless their sizes, product, processes and markets are closely similar.
Even then, differences in the format of their respective accounts may render comparison meaningless without a great deal of investigation.
They give managers additional information on which to base their judgements, but they do not provide a company control system.
Bookkeeping
Initial records of expenditure and income are kept in day books.
The entries are periodically posted to accounts held in ledgers, usually in an integrated accounting system which is the responsibility of the accounts department.
The standard system for doing this is double entry bookkeeping, invented in the fifteenth century by an Italian monk.
All changes of value are recorded in accounts.
These are sheets of paper  divided vertically into two, one side forming a list of value moving in and the other list of value moving out.
The principle is that all movements are entered twice, once in the ‘in’ or credit side of one account and once in the ‘out’or debit side of a different account.
The debit totals will, if there are no bookkeeping errors, always equal the credit totals when all transactions are complete.
If positive and negative errors of the same amounts occur these will not, of course, be detected.
Accounts are of two types, capital and revenue.
The former record what a company owns and what it owes, while the latter record the income and expenditure of the company in the course of its day-to-day operations.
For accounting purposes ‘income’ includes payments due from customers, so it is not the same as ‘receipts’which is cash actually received.
Expenditure and payments differ in the same way.
One difficulty for non-accountants is in deciding on which side of an account to enter a given sum.
Table 3.2 shows the principle involved.
A starting point is that there is always a cash account.
Since the company owns the cash, it is an asset and therefore a capital account.
Cash coming into the company is entered on the left and that going out on the right.
At the end of the accounting period the revenue accounts are totalled and the balance, positive or negative, transferred to the profit and loss account.
The company profit (loss) is the difference between the two sides after any capital reduction has been made good.
Profit retained is an addition to capital; loss is always a loss of capital.
An exercise in double entry bookkeeping is given in Appendix 3.1.
Marginal Costing
Another duty of the accounts department is that of recording the costs incurred in company activities such as manufacturing.
One method of doing this is the system of marginal costing.
Marginal cost is defined as ‘prime cost plus variable overheads’ and is explained by the example in Table 3.3.
This method of cost accounting gives an overall picture of the profit or loss situation and in many cases is adequate for controlling operations.
It does not lend itself to detailed cost investigation of departments and activities because it does not indicate where the overheads are actually consumed.
This information can be obtained if needed, at the penalty of a more complex and expensive system.
Standard Costing
In this system, which is sometimes known as absorption costing, a budget is produced in which all overheads are allocated to the activities associated with them.
‘Standard rates’ for each activity are calculated from the direct expenditure plus the allocated overheads.
The ‘standard cost’ per unit is then calculated from these rates.
This becomes the target  cost which the actual cost as recorded is compared.
The performance of each department or activity can thus be judged as well as the overall profitability of the product.
Table 3.4 illustrates such a cost structure.
Establishment overheads are those costs which cannot easily or economically be charged directly to an activity, that is, without creating an unreasonably complex and expensive system.
They are therefore allocated to the departments on a commonsense basis so that they are all ‘absorbed’ in the various departmental rates.
The rates themselves can usefully be regarded as percentages of either factory standard price per unit or average selling price per unit.
Care should be taken in any discussion to establish unambiguously which convention is to be used; failure to do so leads to much confusion later.
A limitation of standard costing is that, since all the overheads must be absorbed by the products to which the associated activities ultimately contribute, the redesign of a product, so that its production no longer needs a certain activity, may not result immediately in a reduced cost.
This is because a cost reduction in one product for this reason would demand the recalculation of all the other allocations of overheads to ensure that the total is still absorbed.
When a large number of products are involved this considerable effort may not be worthwhile until all overhead allocations are routinely reviewed.
It is probably best for engineers to avoid words like ‘real’ and ‘true’in discussing costs and to regard accounting as a convention that, if adhered to, can with experience be a valuable means of monitoring the use of company resources.
Project Budget
The standard costing system is helpful in creating a budget for an R&D project to specify, design, develop and manufacture a new product, since the standard rates for all the activities planned, multiplied by the time estimate for each phase of the project, give the estimated investment in the new product in a form lending itself to detailed discussion.
Table 3.5 is a typical form of project budget.
There is merit in confining the project budget to a single sheet of paper displaying the names of the people concerned, the estimates on which the decision to go ahead is based, and the authority for doing so.
By providing several columns for the estimates, the stages in arriving at the final budget and any changes of mind during the working stages of the project, can be recorded.
This is another area where human memory tends to be selective in the absence of such records.
The R&D Budget
The project budget can be seen to be a result of the strategic planning (Figure 2.2) loop 5, 6, 7.
As many good projects — those with a high  probability of being profitable — as possible should be estimated.
The most beneficial projects should then be selected until the aggregate resources estimated equal those allocated to new projects in Figure 2.6.
This group of projects, together with the resources allocated for other purposes in Figure 2.6 and any recovery amounts, form the R&D budget.
Recovery is the term used to describe the result of charging work done for other departments to their respective budgets and crediting the R&D budget with the same amount.
A typical format is illustrated in Table 3.6.
The budget is divided into planning periods to suit the nature of the company's business and each period is represented by three columns.
These are headed budget, actual and variance.
The last term is the difference between the first two columns, not the more complex statistical measure.
The budget figures for the selected projects and the other resource-consuming activities are entered into the budget column and subtotals calculated.
The budgeted total outflow of funds per period is entered on the second last line with the cumulative total to date below it.
At the review dates the expenditure per planning period is entered in the actual column opposite the relevant activity.
An important aspect of monitoring the progress of a project is to examine the variances and observe whether they are positive or negative.
The trend of variances is a useful indicator of the likely degree of achievement of the R&D budget.
Review
This chapter has:
(1)
Explained the need for project management.
(2)
Presented a simplified model of company financial structure.
(3)
Described the purpose of the balance sheet and the profit and loss account.
(4)
Explained the purposes of depreciation provisions and financial ratio analysis.
(5)
Described forms of project budget and departmental budget.
The next chapter will discuss financial techniques used in setting criteria for the selection of project proposals to maximize profitability.
Non-financial criteria will be used to ensure that the potential new products are such as to contribute to the corporate plan.
Appendix 3.1 Double Entry Bookkeeping
Joe Bloggs decides to start a small import — export business.
His financial transactions during the start-up period are listed below in the first column and are identified by capital letters.
Place a sheet of paper across the page under row A and post the transaction in the second column as described in Chapter 3.
Move the paper down to row B and check your answer against the correct one now exposed in the third column.
Repeat through to transaction E then see if you can list the balances to agree with those shown.
Joe then begins active trading and his first period transactions are listed below.
Post his figures on a copy of the table below.
They follow on from the first set and the opening balances are already entered.
F
He purchases goods for £200 cash.
G
He pays £12 sundry expenses such as stationery.
H
He pays £50 cash for the first month's rent.
I
He sells all the goods he bought (F) for £240 cash.
J
He pays a further £6 sundry expenses.
K
He pays £14 wages to a temp for some typing.
L
He purchases goods for £600 cash.
M
He sells the goods (L) for £700 cash.
N
He purchases £300 worth of goods from Smith on credit.
O
He sells the goods (N) to Jones for £350 on monthly account terms.
P
He repays the bank loan of £500.
Joe decides to draw up a profit and loss account for this initial period.
All the postings you have made of the above list are P&L transactions and now need to be transferred to the P&L account.
Do this and see what the balance on the P&L comes to.
Has he made a profit?
List the balance left on the accounts.
PROJECT SELECTION
The selection of projects from the list of proposals for inclusion in the R&D programme is a matter of ranking them in order of benefit to the company.
The criteria for doing so will be specific to the company but will always include a minimum return on investment in addition to non-financial criteria.
Typical criteria, methods of applying them and the influence of financial analysis methods on ranking order are discussed.
Ranking Project Proposals
Previous chapters have indicated how to establish the number of new products needed by the company, the funds available to R&D for development and design and how these funds are accounted for.
The next stage is to decide which of the potential projects to work on.
This is a complex question.
A good new product must meet some specific market need, be capable of being handled efficiently by every department in the company, be a good fit with the company objectives and optimize the use of company resources.
The first requirement is to have a large number of potential projects from which to select and then to select those which will produce the best returns on the resources available.
The most efficient way of loading R&D resources is to place the proposals in order of priority and then to allocate to them, in that order, all the resources they can usefully employ until all allocated resources are committed.
There will always be pressure to take on more projects than this; it should be resisted.
This procedure ensures the minimum average project duration, gets the product on the market as early as possible, minimizes development costs and opportunity costs.
Not least, it reduces the time in which the specification can be changed.
To establish the merit of project proposals without overlooking any significant factor, it is highly desirable to have a selection system that is applied to all cases.
A good starting point is a check list with sections covering criteria for acceptance by all the main company functions.
Of necessity this will be specific to a company in terms of its markets and capabilities and a comprehensive universal list is not practical.
The type of  checklist shown in Table 4.1 (Twiss, 1982) provides a reasonable starting point for creating such a list, adding and subtracting elements as required.
Project Profile
The criteria listed in Table 4.1 present some problems in their present form in assessing the order of priority of projects.
One method of displaying the data in a more usable form is the project profile, illustrated in figure 4.1.
The merit of the project in relation to each criterion is assessed in terms of the five classes, ranging from very good to very poor.
A dot is placed in the appropriate column opposite each criterion and the dots are joined up by a line.
The pattern formed by the line is an indication   of the overall merit of the project: the more the line conforms to the VG column and the less it wanders towards the VP column the better the project in terms of the criteria.
Project Merit Number
A slightly more refined method of handling the data is shown in Table 4.2.
In this case the merit of the project in terms of each criterion is scored on a 1 to 5 scale.
In addition, to allow for the fact that some criteria will be more important to the company than some others, each criterion is given a weighting on a scale of 1 to 10.
This number is entered into the appropriate column.
The third column contains the product of the first two, providing a merit number for each criterion and the sum of these is the overall merit number for the project.
The projects may then be ranked numerically by merit number.
Having ranked the proposals in order of compatibility with company objectives it remains to establish the order of priority in terms of return on investment.
the data for this analysis is the set of project budgets as shown in Table 3.5.
This groups the estimated costs under appropriate headings with subtotals.
By breaking the activities under each heading into relatively small parts, which are fairly easy to estimate, the overall estimate can be  made more reliable.
As in the case of the checklist the detail of the format must be specific to the company using it.
A section of the form records the estimates for sales and production volumes.
Production costs are estimated by using the cost structure in Figure 3.4.
Other costs are usually obtained as standard company percentages of those estimated so far, arriving at labour, materials and overhead or the LMO level.
Adding the standard uplift for development and initial production costs produces the factory standard price (FSP) against which manufacturing performance will be judged.
Above this line standard uplifts for selling expenses and budgeted profit are added to arrive at average selling price.
This procedure assumes the use of a standard costing system in an established product line.
If such a system is not used, or a completely new and different production process is to be used, it may be necessary to estimate and allocate overheads from scratch.
This will often be done by estimators, but engineers involved in the project should ensure that the results make sense from their own viewpoint.
The average selling price, together with the estimates of sales volume, provides the total estimated income from the project.
This, with the total cost estimate, is the basis for estimating the return on investment of the project.
There is merit in confining all this data to one sheet of paper.
Since it may be appropriate to vote funds piecemeal to a project in order to control the financial risk involved, several columns for estimates should be provided.
A small feasibility study may be an economical way of reducing the uncertainty — see Chapter 5 — in a specific aspect before authorizing the funding of the whole project.
This should be viewed as an investment in risk reduction and the cost regarded as an insurance premium.
Overruns on the original budget should result in that budget being closed and a new one authorized to cover the estimated additional requirement in the next column.
Initials in the appropriate approvals section ensure that such additions have been properly authorized and advised to the cost accountant.
Investment Analysis
The funding of a project is no different in financial terms from any other company investment.
Management must make a judgement, based on estimates of cost and benefit, as to which of the competing project proposals they will accept.
The degree of risk will vary between proposals, but some risk will always be present and the information will seldom be complete.
A number of techniques of investment analysis are commonly in use, each with merit in some cases.
The method used can  have a decisive influence on the ranking of the proposals.
This effect is illustrated in the hypothetical set of proposals given in Table 4.3 (Bierman and Smidt, 1964).
In some cases inspection of such a table is all that is needed to decide which investment to accept.
If the choice were between cases A and D there would be no problem.
In other cases a criterion may be needed, and one frequently used is the payback period.
This selects the case which takes the shortest time to generate the income needed to pay back the total investment in the project.
Such a technique ranks the above investments as in Table 4.4.
The weakness of this method is that it takes no account of what happens after the payback date is reached.
Case A ranks equally with case D which produces a further 6000 after payback, while case A produces nothing.
One method of taking this into account is by using the proceeds per unit outlay as the ranking criterion, as seen in Table 4.5.
It will be seen that this technique has the effect of interchanging the rankings of the best and worst cases in Table 4.4, emphasizing the importance of sticking to one method of analysis.
There is still a deficiency in the method.
It does not take account of the timing of the outlays and proceeds.
Case D, although ranked 3, earns 7000 more in the first 2 years than case C, which is ranked 1.
This could be decisive if the company objectives stressed the need for income over that period.
Discounted Cash Flow
If there were a choice between receiving £100 now and the same amount a year hence there would be no doubt as to which would be preferred.
This subjective preference is quite rational.
If one has to pay a creditor £100 in one years time, it is not necessary to have £100 today.
What is needed today is that sum which, invested, will with interest amount to £100 in one year.
If the rate of interest in, say a Building Society, is 8 per cent, what is needed today is £92.60.
This is known as the present value of £100 in one years time at a discount rate of 8 per cent, or the DCF PV at 8 per cent .
DCF Net Present Value
The DCF PV of any sum can be calculated from the expression:
There is seldom any need to apply the formula as tables of both present  values of future sums and present values of sums received per period are to be found in most books dealing with investment and financial decisions.
The procedure is to obtain from the tables the PV values of both outlays and proceeds for each year.
The total PV of outlays is subtracted from the total PV of proceeds giving the net PV of the investment.
Since in all the investments considered the entire outlay is in the first year, it is already a present value.
Any proceeds in year 1 are also present values, but those in succeeding years are discounted by multiplying them by the fraction given at the appropriate year in the chosen rate of interest column in the tables.
Table 4.6 gives the results of NPV analysis at 6 per cent applied to the investments listed in Table 4.3.
The NPV of an investment, at the interest rate at which the firm can borrow capital, may be regarded as the profit on the investment when all costs, including interest payments, have been met.
Alternatively it can be seen as the maximum amount the firm can pay for the opportunity of making the investment, without being worse off financially.
The preferred investment is that with the highest NpV.
If there is no competition for funding then all investments with a positive NpV can be accepted.
Table 4.7 shows that the discounting rate used is significant in ranking investments.
Another use of the DCF method is to establish the maximum rate of interest a company can afford to pay to borrow the capital needed for the investment.
The DCF Yield of an Investment
The DCF yield, or rate of return, uses the same concepts as NpV and consists in finding by trial and error the interest rate that makes the discounted outlays and proceeds equal.
The NpV is then, of course, zero.
This permits the direct comparison of the rates of return of any number of competing investments.
If it is proposed to borrow the money for the investment, the DCF yield is the highest interest rate the company can pay without making an overall loss.
No investment having a DCF yield lower than the company's cost of capital would normally be accepted.
table 4.8 shows the result of ranking the hypothetical investments on this basis.
The effect of analysis techniques on the ranking of investments is shown in Table 4.9 where it can be seen that each method produces a different ranking.
The ranking produced by the NPV 30 per cent and the yield tables are the same since the rates found in the latter are of the order of the 30 per cent applied in the former.
No single method supplies all the answers to investment decisions.
An NPV result of £100 000 may look attractive.
If the investment involved is £100 million, however, it is not so good.
In the same way a DCF yield of 500 per cent looks good at first sight, but if the investment is 6p it is not worth considering.
The method used must relate to the type of decision to be made.
particularly if dissimilar investments with different time scales   are in competition for funds, the NPV method has merit in the general case, because it takes these factors into account.
If the competing investments are similar, say a choice has to be made between proposals for new products in the same product range, the complexity of DCF methods may not be justified.
A simple comparison of total estimated income from the competing products may provide as good a guide to decision making.
Review
This chapter has:
(1)
Described a project selection check list.
(2)
Described methods of ranking project proposals.
(3)
Described methods of investment analysis.
(4)
Explained the concept of discounted cash flow.
(5)
Demonstrated the influence of analysis methods on ranking.
The estimates on which ranking decisions are taken involve uncertainties.
The next chapter puts forward a method of making best use of such information and a general method of taking decisions under uncertainty.
UNCERTAINTY, RISK AND DECISION
Most decisions in industry are taken in conditions of some degree of uncertainty.
Design, development and management have in common the absolute necessity of taking action based on incomplete information.
The practical implication is that the outcome of any action, unless it has been previously proven, cannot be predicted accurately.
The best that can be done is to convert the uncertainty, which is not quantifiable, into risk, which is.
This permits the use of probability theory to compare the nature and range of outcomes of activities and help to form judgements as to the best course of action in given circumstances.
Assessment of Risk
A risk is an uncertainty to which a probability can be assigned.
Probability is measured on a scale from 0 to 1.
The chance of flying to the moon under one's own power has a probability of 0.
The probability of dying is 1.
In the final analysis assessment of risk is subjective, but there are techniques for aiding the assessor and the fact of subjectivity does not render the assessment useless.
It is additional information that should not be neglected but used in a valid manner in the process of reducing overall risk.
Some people find it difficult to judge the probability of a given outcome, say the success of a specific development job, directly.
It can often be brought into areas where they feel more willing to express an opinion by putting the problem in terms of comparative bets, using the notion of the equivalent urn.
Prior Probability
As an example of one technique, assume that a project leader is told that his current project, if successful, will earn the company £1000 profit, if a failure, nothing.
That is one bet.
Assume next that an urn has been filled with 1000 identical marbles.
Of these 500 have been coloured red and the remainder blue.
They have been well mixed and cannot be seen.
The  second bet is that drawing a red marble wins £1000, drawing a blue marble wins nothing.
Clearly the chance of winning this bet is 0.5 since one of the two colours must be drawn, there being no other possibilities.
The project leader is now asked which of the two bets he prefers.
If he says the second it means that there is a better chance of winning £1000 by drawing a red marble from the urn.
This in turn means that the probability of project success is less than 0.5, in his judgement.
By changing the proportion of red to blue marbles progressively, there comes a point where the project leader has no preference between the two bets.
The chances of winning the two bets are at that point judged to have the same probability, given by the ratio of red to blue marbles.
Such an initial assessment, based on the information available at the time, is the prior probability of success.
Posterior Probability
In a development project there are usually prospects of increasing the information available as time goes on.
This may be by consulting a specialist in the field or by doing some research to fill in the gaps in knowledge, as part of the project.
This additional information can be used to reduce the level of uncertainty in the project, producing the posterior probability of the same outcome.
One would like the additional information to be conclusive so that the success or failure of the project could be predicted with certainty.
This will rarely be the case.
The new information will indicate that the project will succeed or fail but the information itself will have only a probability of being correct.
The consultant or the additional research will express a view on project success or failure and the probability of this view being correct is assessed in each of the two cases.
Assume that this is represented by Table 5.1.
This means that, should the project have a successful outcome, there is  a 0.9 chance that the additional research supported this outcome and 0.1 chance that it did not.
Conversely, if the project fails there is a 0.2 chance that the additional research supported it and a 0.8 chance that it did not.
These additional probabilities must sum to 1.
The assessments of these probabilities are made on the basis of what is known about the additional research, or the consultant, in relation to the unknowns in the project (Moore and Thomas, 1979).
The two sets of probabilities can now be combined as in Figure 5.1.
In the figure ABCD, the base line DC is divided into the chances of success and failure.
Assume that these have been assessed at 0.6 and 0.4.
The vertical AD is divided into the probabilities of the additional information being correct or false in the case of project success, assessed at 0.9 and 0.1 respectively.
The vertical BC is divided into the probabilities in the case of project failure, 0.2 and 0.8.
The figure therefore gives the possible combinations of project outcome and additional information.
Assume that the additional information predicted project success.
The part of Figure 5.1 to be considered is therefore the area EFGHCD.
The proportion of this corresponding to project success is  
The knowledge that the additional information was favourable has lifted the probability of project success from a prior probability of 0.6 to a posterior probability of 0.87.
The posterior probability of project failure, calculated in the same way, would be 0.13.
They add up to 1 as would be expected.
There is now a much greater difference between the probabilities of success and failure on which to base a decision.
The above approach is an application of a theorem formulated in 1760 by the Reverend Thomas Bayes FRS and named after him.
A set of mutually exclusive events B 1 , B 2 , B 3  has associated probabilities p (B 1  ), etc.
If any of these events occur, event A can occur, but with a different probability for each B .
Assume A to have occurred.
p (B 1  ), etc., are the prior probabilities.
The probability of B 1  occurring, given that A has occurred, written p (B 1  /A ), is the posterior probability.
The probability p (A/B 1  ) is called the likelihood.
The theorem states that the posterior probability is proportional to the prior probability multiplied by the likelihood.
The use of the Bayes theorem assumes knowledge of prior probabilities.
Bayes' postulate is that, when nothing to the contrary is known, the probabilities should be assumed to be equal.
Decisions
Decisions in project management start with the choice of project and spread throughout its duration.
These decisions will have a profound bearing on the future viability of the resulting product or process and should therefore take into account the most likely consequences.
The decision to terminate a project can only be made rationally if the consequences of doing so can be compared with the consequences of carrying on.
These cannot be known at the time the decision is to be made and a means of stating what is known and what is assessed of the influence of future events on the commercial performance of the product, permitting comparisons between options, is needed.
Such means have been developed, under the general heading of ‘utility theory’, with a specific unit of measurement, the utile.
For the purposes of this chapter the monetary ‘expected value’ concept will be used.
This is the estimated pay-off in money terms of a given event multiplied by the probability of that event occurring.
When only one of events A, B and C can occur and there are no other possible events, the sum of the probabilities of A, B and C must be 1.
The expected value at this point is the sum of the possible pay-offs multiplied by their respective probabilities (Dewhurst, 1972).
Cumulative Density Function
By definition, an uncertain quantity is one whose value can lie anywhere along a specified range.
It is often easier to assess a continuous probability distribution than a set of specific values.
In many contexts, such as the RPD project planning technique described in Chapter 6, it is also more useful in decision making, in that the decision maker knows what risk he is taking, and in lending itself to sensitivity testing of possible decisions.
In Chapter 6 the cumulative density function (CDF) is arrived at by calculating probabilities for all possible paths through a network after the probabilities of individual outcomes of activities have been assessed subjectively.
It is also possible to assess a CDF as a series of step-by-step subjective judgements of the overall outcome.
The form of the CDF is shown in Figure 5.2 where the horizontal axis is a scale of all possible values of the uncertain quantity X and the vertical axis is the probability that the true value is equal to or less than X. In the case described in Chapter 6, X is the date of completion of the project.
To assess the distribution, the project leader is first asked to give a date such that the true date is as likely to be after it as before it.
He should be as prepared to place a bet on completion before this date as on completion after it.
As a 50/50 bet, this date corresponds to 0.5 on the probability scale.
It is the project leader's indifference value for the true date.
He is then asked to assume that the true date will be before the p0.5 date and asked to nominate an earlier date that would again divide the range into two 50/50 bets.
This is the p0.25 point.
Another such procedure gives the p0.125 point.
The p0.75 and p0.875 points are arrived at in the same   way.
These points are plotted and joined by a smooth curve which is the cumulative density function (CDF).
Decision Trees
Decisions in project management are frequently sequential.
Decision B can only be taken after the result of action consequent upon decision A is known.
Looking ahead from the starting point of a project the possible actions spread out like the branches of a tree.
This analogy leads to the ‘decision tree’, a useful way of structuring the multiple problems associated with development projects so that the expected value (EV) criterion can be used to help decide which branch to follow.
Figure 5.3 is a decision tree for a hypothetical development project to develop and market a new product.
The basic tree consists of a network branching out from an initial decision of whether or not to undertake the project.
The symbol for a decision node is a rectangle with a number in it indicating the decision level, in this case level 1.
At the far right of the figure the net present values of the possible pay-offs from the various courses of action are listed.
The decision not to start the project obviously has a pay-off of 0.
A line emerging from a decision node is a consequent action leading to an event node, indicated by a circle.
There will be one or more events resulting from this decision, over which the project leader has no control but for which probability assessments can be made.
In this case the probabilities of success and failure are assessed at 0.5.
The cost of the project has been estimated at £40 000.
The second level of decisions is now reached and all the feasible possibilities are entered.
If the project succeeds, the choice is between making and marketing the product or abandoning it.
In the latter case there will be no pay-off and the line is crossed to indicate that the path is dead.
In the former the outcome will depend on the level of sales actually achieved.
These are not, of course, known, but the pay-offs have been forecast for three levels, high, medium and low.
High level sales are forecast to produce a pay-off of £160k and to have a probability of 0.2 of being achieved.
Medium level sales will produce £80k with a probability of 0.5 and low level sales £40k with a probability of 0.3.
The EV for this path is therefore 
The alternative event resulting from decision 1 is that the project fails.
The second-level decision required is whether or not to authorize further research or to abandon the project at decision node 2b.
The consequence of an abandon decision will be a zero pay-off as shown.
The project team think that they could carry out the further research in an additional year at an estimated cost of £20 000, but that the chances of solving all the problems are only 30: 70 as opposed to the 50: 50 chance they gave the initial phase.
If a viable product is not produced in a total of three years it is felt that the project should be abandoned since the technology in use will have been superseded and a competitive product is likely to be nearing launch.
If the additional research results in a new product launch at the end of three years the sales forecasts are for lower sales levels.
There is a 0.1 probability of a pay-off of £120k, a 0.5 probability of £60k and a 0.4 probability of £30k.
The EV via this path through the tree is 
The remainder of the analysis is carried out by ‘rolling back’ the  decision tree, that is by moving back from the final pay-off figures, node by node, multiplying each pay-off by the probability of each event passed through in the network.
Thus the net present values of the EVs at the events consequent upon decision 2b would be either zero or £16.2k.
Since the cost of the research is estimated at £20k this would result in a loss to the company in either case.
The decision line to authorize further research is therefore broken.
Rolling back the pay-off via decision 2a, the EV following the initial development project is £42k.
Since the cost of the project is estimated at £40k the expected profit is £2k.
The best strategy with this project is to carry out the initial development and, if it is successful within 2 years, launch the project.
If the project fails within the two-year period it should be abandoned.
Other projects competing for company resources should be analysed in the same way and ranked according to the levels of pay-off for each.
This provides a priority list, in terms of EVs, as one factor in the decision making process of arriving at a portfolio of R&D projects.
While the EV figures will only be guidelines for any individual project, the expected and actual cumulative figures will converge in the long run if the method is consistently applied, maximizing the productivity of the R&D function.
Cost of Information
Aditional information can lead to better odds and bigger returns but if the information costs more to obtain than the increase in returns the result is a net loss.
Perfect information is usually not practicable but the concept is useful in establishing how much can be paid for whatever additional information is available.
As an example take the case of a new product that has been developed and is ready for launch.
Costs of development are history and do not enter into the next set of decisions.
The cost of an abandon decision is therefore zero.
The pay-offs have been estimated for market penetrations of 10 per cent and 2 per cent.
In the first case the pay-off is £500k with a probability of 0.7 and in the second — £250k with a probability of 0.3.
The prior EV of the launch is therefore  That of abandon launch is 
If the information were perfect the launch would take place in the 70 per cent of cases where a 10 per cent share was forecast and not in the  others.
The EV under these conditions would therefore be  The EV of perfect information in this case is the difference 
Clearly even information that would predict the market share perfectly must not cost more than £75k or an overall reduction in pay-off will result.
As already seen in this chapter, such perfection is not to be expected and the probability of the additional information being accurate must be taken into account.
Assume that a test marketing exercise can be carried out, at a cost of £10k, to establish whether the higher or lower market share is the more likely.
The results confirm or reject the sales estimates with the probabilities shown in Table 5.2.
The probabilities displayed in Table 5.2 can be combined as shown in Figure 5.4.
If the test result confirms the 10 per cent forecast the shaded portion of Figure 5.4 is relevant and the combined probabilities are 
If the 2 per cent level is confirmed the combined probabilities are 
For the reject results the same calculations are done on the unshaded area giving revised probabilities of 0.32 for the 10 per cent and 0.68 for the 2 per cent levels.
A decision tree can now be drawn as in Figure 5.5, the above probabilities entered on the appropriate branches and the pay-offs rolled back.
The EVs of the two main branches are 278.5 for the path including the test marketing program and 275 for that without it, a difference of 3.5 in favour of the first.
Since, however, the cost of the program is 10 this means a reduction in pay-off of 6.5, so it is not worth doing.
In this case   the benefit from the additional information is less than the cost of obtaining it.
This is a consequence of the quality of the information as well as of the cost.
Review
This chapter has:
(1)
Defined uncertainty, risk and probability.
(2)
Described techniques for assessing probability.
(3)
Defined prior and posterior probability.
(4)
Explained how to combine probabilities.
(5)
Explained how to use decision trees.
(6)
Explained the cost of information concept.
Techniques for detailed planning and monitoring of projects are described in the next chapter.
PROJECT PLANNING
Ranking project proposals in terms of fit with company objectives, then creating a ranking list from the best of these on agreed financial criteria provides the basis for choosing the projects to which available R&D resources will be allocated.
The next task is to take the projects in order of ranking and plan them.
Since people are not necessarily interchangeable it may be necessary on occasion to depart from the priority order.
Resource limitations may make it impossible to follow one project with the next one on the list, and projects must then be resourced in an order that makes efficient use of the resources available.
The Product Loop
The relationship between the company and the market place is in the form of a loop as shown in Figure 6.1.
The three main company functions described in Chapter 1 are elements in this loop, sometimes as individuals and sometimes in collaboration.
The potential new product may start anywhere in this loop depending on the origin of the proposal.
If a new market need is identified the starting point would be at stage 1.
The form of the diagram amplifies Figure 1.1.
If the project is to develop and install a new process, the parts of the loop dealing with series production will not be included.
If it is for the modification of an in-house process only those parts up to the pilot plant stage are relevant.
In all cases the customer or user is a critical element in the loop.
Each function embraces three vertical columns, the centre one on its own and the other two shared with the other functions.
Thus in stage 1 the task of analysing the market and answering the question ‘what business are we in?’ is seen to be mainly the province of the marketing function.
In stage 3 the responsibility for deciding how to fill the market need thus established is jointly that of marketing and R&D.
In the same way R&D has the main task of carrying out the feasibility study and the development but shares with manufacturing the responsibility for ensuring that the production design is efficient from the latter's point of view.
If the proposal is for a change to an existing product rather than creating a new one, the starting point might be at stages 6, 7 or 8 depending on the level of technical risk in the modification.
If the project is to create a new manufacturing process, rather than a product, the starting point would be at much the same stage but would not go past building the plant (stage 9) since there would be no question of quantity production.
In all cases the result of the project goes back to the user at stage 1.
With a new product this is the market segment buying the product.
This forms the real test of the success of the project: up to that point all has been analysis, estimate and hope.
The customer tells the company what it has done.
To anticipate this there may be a case for product field testing at stage 9.
This would also be the feedback point for a process development in which the company is its own customer.
An essential element is good communication between the people involved, particularly in those stages where responsibility is joint.
Clear records of decisions taken, the grounds on which they were taken and the resulting action are important.
The planning and monitoring techniques described in this chapter are aimed at aiding the achievement of these conditions.
Engineering Planning
After the design specifications and requirements have been defined the further course of the project is typically as shown in Figure 6.2, moving in a continuum through design, process engineering and manufacturing stages to the final product, with inputs from sales forecasts to guide each stage.
Although for the sake of clarity the stages are shown as separate boxes, this is a poor form of organization to use in practice.
There is evidence (Bergen, 1983) that flexibility of operation in and between these stages benefits the project.
Individuals can only do their jobs well in this context if they know a great deal about everyone else's jobs and problems.
This is best achieved by personal contact.
Departmental barriers and ivory tower attitudes are detrimental to good results.
Morton's Rule
A useful rule to apply is that of jack Morton (1964), a past Head of Components Laboratory at Bell Telephones, New jersey.
He postulated that any two people who have to collaborate to produce a result can have between them bonds and barriers of two types, spatial and   organizational.
If they sit at the same or adjacent desks they have a spatial bond.
If they are in different rooms they have a spatial barrier.
If they report to the same boss they have an organizational bond; if they are in different departments they have an organizational barrier.
Morton's rule is that a double bond is advantageous and that a double barrier must never be permitted.
If they are in different departments they must be brought together physically, whatever departmental objections are raised at first.
Improved performance will overcome these in a short time.
If they cannot be brought together the organization must be changed so that they report — possibly only for this one task — to the same person.
A communication problem between a drawing office and a laboratory arose when the latter moved into a new building.
It was solved by moving the DO section leader, complete with drawing board, next to the project leader in the new lab for the first six months of the project.
The chief designer objected strenuously until the move was found to be successful, when it became a long-cherished personal proposal previously vetoed by management.
The Gantt Chart
One of the earliest resource planning techniques, due to Henry Gantt in 1903, is illustrated in Figure 6.3.
The project is broken down into a series of well-defined jobs of short duration whose cost and time can be estimated.
Each job is represented by a horizontal bar on a time base, the length of the bar indicating the estimated time for the job.
The project review dates are indicated by a vertical dotted line, and at this time a horizontal line is drawn beneath each bar to indicate the progress actually made up to that date.
There are two conventions about the progress line.
It can be used simply to indicate the time actually worked on a given job.
Alternatively the bar can be deemed to represent, for monitoring purposes, 100 per cent of the job.
The length of the progress line is then drawn to represent the percentage of the job that has been completed at the review date.
The difficulties of estimating percentage completion of R&D jobs is such that the former method is preferred for this type of project.
The latter is valid for projects of low uncertainty capable of accurate estimating.
Critical Path Method
Critical path method (CPM) is a network planning system developed by Dupont in the 1940s and provides more information on the progress of the project than does the basic Gantt chart.
The jobs are defined and estimated as before but in this case two columns headed ‘precede by’ and ‘follow by’define the sequence of events to be followed in the project as illustrated in Table 6.1.
Following the logic of the last two columns of Table 6.1, a network diagram is drawn (Figure 6.4) in which the lines between the nodes represent the jobs, the nodes being numbered to identify the jobs for the benefit of a computer, if used.
The skill in planning consists in the decisions taken in completing the two columns so that the overall network time is minimized without contradicting the logical sequence in which the jobs must be done.
In any such network there will be one or more paths which are longer than any other.
Slippage on any job on this path will entail slippage on the overall project.
This path is indicated by a double line through the network, drawing the project leaders attention to   the need to take emergency action when such slippage appears.
The other paths have, by definition, some slack in them and some slippage can be tolerated as the end date will not be affected until the slippage results in the job becoming part of a new critical path.
Network Dummies
One of the conventions of network diagrams is the dummy job.
This is necessary to avoid ambiguity, unnecessary constraints in the plan and to avoid forming loops from which the computer cannot escape.
A dummy has all the restrictive properties of a real job but no time content.
Figure 6.5 illustrates the more usual cases.
Advantages
Network planning of this type has a number of advantages:
(1)
It forces the project leader to think clearly and rigorously about all the activities in the project in the planning phase.
(2)
It focuses attention on the critical path, where any slippage will delay completion of the project.
(3)
It promotes awareness of the integrative aspects of the project.
(4)
Reviewing the progress of the project in terms of the network provides feedback to the project control system.
(5)
By displaying both critical path and slack it stops crash programs being set up for every activity.
(6)
It can be used as a simulation for test and planning.
There are also some disadvantages:
(1)
Analysing the network can be expensive.
(2)
The preparation time is significant.
(3)
Like a computer, it may lend credence to false data.
(4)
It may consequently give a false sense of security.
(5)
If the network contains activities in other departments local supervision may resent this.
(6)
It is difficult to handle overlapping activities.
(7)
It does not display work load.
(8)
The amount of detail displayed may be confusing.
(9)
After the first 20 per cent of the project, revisions become difficult to carry out if done manually or expensive if computerized, for major projects.
Gantt — CPM
It is possible to combine the simplicity of the Gantt chart with the logic of the CPM method to display both the critical path and the slack in a very accessible manner.
The bars representing the jobs are rearranged in order so that they do not overlap each other when the logic is represented by vertical lines connecting job starts with the finish of a job that must precede them.
The bars are on a time base as in the Gantt chart and slack is indicated by horizontal dotted lines connecting finishes to starts.
Figure 6.6 displays the same project as Figure 6.4.
For relatively small projects this is a powerful method: for large complicated projects the effort and confusion involved in updating the resulting diagram can create major problems.
PERT — Project Evaluation and Review Technique
An alternative technique, first used to control the Polaris project in 1958, reverses the convention of CPM.
The job content is in the node, in the form of a PERT symbol, rather than in the network line.
The symbol is illustrated in Figure 6.7 and contains more information on the provisions of the plan than does CPM.
The form illustrated is one of several in use.
The centre circle of the symbol contains the job identification code, in this case X, and the time estimate for the job.
The left wing of the symbol contains two figures, the early start time on the left and the late start time on the right.
The right wing carries the early finish time on the left and the late finish time on the right.
At any job symbol both the free float and the total slack, as defined in Figure 6.7, can be calculated.
Figure 6.8 shows the PERT network for the same project as Figures 6.5 and 6.6.
Early starts and early finishes are arrived at by a forward pass through the network.
If the project starts at time zero and job A is estimated at 4 time units, the early start for jobs B, C and G, which follow it, are all 4.
Job B has an estimated time of 3 units, so the early finish of B is 4 + 3 = 7.
Since job D has to wait for completion of B, and only B, before it can start, the early start for D is 7.
The job time of D is 4, so the early finish is 7 + 4 = 11.
The forward pass through the network consists of repeating this procedure by adding the job time to the early start time and   making this the early start time of the next job in sequence if no other preceding job has a later early finish.
Thus in the case of job H the early start time is not 10, which is the early finish time of preceding job G, but 15 which is the early finish of F and is on the critical path.
The late starts and late finishes for the jobs are obtained by a backward pass through the network.
In Figure 6.8 the project end date is 17 units, obtained by adding the job time of J to its early start time.
Subtracting the job time from the end time gives a late start time of 16, the same as the early start.
H is treated in the same way and this repeats for F, D, B, and A.  Going back to G presents a different picture.
Because the late start of H is 15, G must finish no later than 15 in order not to delay H. The late start of G is therefore 15.
The late start for G must allow it to finish at 15.
Since the job time is 6 the late start must be 9.
The critical path in a PERT network is that in which the times appear in identical pairs in the left and right wings of the symbols forming it.
The other jobs have some slack in them and this can be calculated as in Figure 6.7.
PERT and CPM are valuable when planning a project as they help the planner to ensure that all jobs have been considered and placed in the right logical sequence.
As a means of controlling a project they have two weaknesses.
All plans need revision from time to time and difficulties arise in reading the networks after several such changes.
This is largely overcome by using a computer to print out a new set of data at each revision, but there is a tendency to allow the plan to fall into disuse in the later stages of a project because of this.
A more basic defect when used for R&D projects it that both methods assume a finite and predictable outcome from every job.
In R&D projects this is not the case and a job may have to be done several times before the outcome is acceptable.
PERT and CPM do not permit loops to be formed and cannot accommodate repetition of a job.
(See Appendix 6.1 for CPM and PERT exercise.)
Research Planning Diagram
Research planning diagram (RPD) was developed (Davies, 1970) to deal with this situation.
It is again a network but it is more like a computer program flow diagram than either PERT or CPM.
Figure 6.9 shows a small section of a plan — two activities — in RPD notation.
The activity is represented by a rectangle and each one is followed by a question box in the form of a lozenge.
job identification and time estimate are shown as in PERT in the rectangle.
The question in the lozenge is always ‘is the outcome of the activity acceptable?’.
If the answer is ‘Yes’ the exit is from the bottom of the lozenge to the next job rectangle in the network.
If ‘No’ the exit is from the side of the lozenge, looping back to the input side of the job rectangle to repeat it.
In planning the project, subjective judgements —; see Chapter 5 — are made as to the probability of having to repeat each job one or more times.
These probabilities are noted alongside the exit from the lozenge.
In Figure 6.9 the planner has expressed the view that there is a 0.5 probability that job X will be repeated once and a 0.2 probability that it will then be necessary to repeat it once more.
The analysis of the RPD is best carried out by computer since it involves tracing every possible path through the network and calculating the   cumulative probability for each path.
The presentation of this information is shown in Figure 6.10 and is in the form of an ‘S, curve of probability of completion against time.
This is more representative of the nature of R&D projects than is the single date often given as the result of a PERT plan.
PERT was originally developed on a probability basis but is not much used in this way now because of the amount of work involved.
RPD has important attributes for the company as a whole as well as for the project team.
All R&D projects involve an element of risk as described in Chapter 5.
This is largely in the accuracy of the estimated completion date.
There is evidence (Norris, 1971) that cost estimate overruns are significantly lower than time estimate overruns.
The evaluation of investment risks is generally accepted to be a corporate responsibility.
If top management is presented with a finite completion date, the risk in R&D project investments is being evaluated by the project leader.
If the nature of the risk is displayed as in Figure 6.10, corporate management is explicitly given the data it needs for evaluation.
It could be argued that in agreeing to a single date the project leader is taking decisions more properly the province of higher management.
A further merit of the presentation is the help it offers to those company functions whose responsibilities involve risks contingent upon the completion date of an R&D project.
the materials manager does not purchase all the material he could possibly need in a given period.
He   accepts an agreed risk — there should be a company policy to guide him — of running out of stock as a means of optimizing the amount of company capital tied up on his shelves.
The RPD presention allows him to use his normal criteria in relation to R&D projects which result in new products.
He will probably set his purchasing date near, but not at, the bottom of the ‘S’ curve.
He will thus avoid buying material before it can be used by accepting a small and defined risk of delaying production.
In the same way the sales manager can optimize his launch date to ensure earliest returns on the new product by accepting a small risk that he will not be able to deliver on the promised date.
He will work near, but not at, the top of the ‘S’ curve.
The descriptions above have been based on estimating and planning the use of time on the project.
Equally the costs of the project can be dealt with in the same way.
Current Status of PERT
Of the several planning techniques described, PERT is the one most discussed, though not necessarily most used, in industry.
The current status of the method in the USA was surveyed by Dougherty and Stephenson (1984) in firms in the Fortune 500 (most commercially successful of the year) class.
They found that it was used in 40 per cent of  development projects but only 16 per cent of research projects, and then mainly in large, complex projects costing over $500 000.
The cost of running PERT varied between 3 per cent and 9 per cent of the project cost, the latter figure applying mainly to projects costing less than $100 000.
The opponents of PERT were convinced that it was too sophisticated, inflexible and expensive.
Slip Chart
Monitoring the progress of projects is often done by means of project review meetings of which the minutes form a record of achievement and decisions.
Particularly in terms of slippage the minutes are sometimes less than satisfactory, recording only the slippage since the last meeting.
It may be necessary to go through the whole file in order to find how much the project has slipped altogether.
The slip chart, due to Brooke (1973), presents the complete history of the project on one sheet of paper by continuously comparing planned time with elapsed time, as shown in Figure 6.11.
The project is first laid out on a time base and the job completion dates   transferred to the horizontal scale representing planned time.
The vertical axis to the same scale represents elapsed time and the diagonal is drawn in as shown.
At the first review date the completion dates are re-estimated and the new dates entered on the planned time axis in line with the review date on the elapsed time axis.
The original estimates are joined to the new ones by lines.
Since any point on the diagonal makes the same intercepts on both scales any job whose line hits the diagonal has been completed.
A line moving vertically downwards indicates a job that is on schedule.
If the line is sloping to the left the job is in advance of the schedule and if to the right it is slipping behind schedule.
If it is sloping to the right so much that it is nearly parallel with the diagonal it will never be completed unless drastic action is taken.
The same procedure is repeated at each review date.
The diagram provides a complete and updated history of the progress of the project on a single sheet of paper.
Review
This chapter has:
(1)
Described the product loop concept.
(2)
Described the process of engineering planning.
(3)
Given examples of the use of bar chart and network planning techniques.
(4)
Described and indicated the merits of a probabilistic network planning technique.
(5)
Described and given an example of the use of slip charts for monitoring projects.
The next chapter discusses the interface between R&D and manufacturing and the steps that can be taken to reduce the problems encountered with new products.
Appendix 6.1 CPM and PERT Exercise
The data from the CPM example in Chapter 6, for moving a machine tool from a factory to a new site, is repeated below.
The overall length of the project was given in the example as 17 days.
How much can you reduce the time?
What assumptions did you make to achieve the improvement? project: to move m/c tool from factory to new site and install.
THE R&D/PRODUCTION INTERFACE
While new and innovative technology is exciting to work with and highly regarded, it is not in itself a guarantee of a profitable product.
The treatment of the new product in all departments must be such as to ensure a product with a price/performance ratio and a quality — that is, fitness for purpose — which matches the market need.
This calls for high levels of skill in design and production and the necessity for the two functions to co-operate to produce an optimum result.
The Post-development Gap
A phenomenon that has been called ‘the post development gap’(Davies, 1980) has produced severe losses in a number of innovative companies.
The pattern is that an innovative company, after spending much time and money on a technically successful development embodying new technology, puts a first generation product on the market.
For a few years it has a monopoly in world markets and a good order book.
Then, if the product appears to be commercially successful, as well as a technological tour de force , suddenly competitors are marketing what amounts to a second generation product.
They have taken the technology as they found it, at relatively low cost, and invested in re-engineering the product for efficient production, quality and reliability and a better price/performance ratio.
The hypothetical company has now lost its market.
It may be tempted to re-invest in more R&D to gain another technological lead.
The chances are that the same cycle will repeat even if it is successful in doing so; success in innovative technology cannot be guaranteed.
Some countries place less importance on innovation and more on sound conventional engineering.
Professor Hutton's massive study of engineering in Germany (Hutton et al .,
1977) found that the average West German engineer values the ability to apply well-proven engineering solutions to problems, in a professional manner, above original thought.
This cultural factor has been to the commercial benefit of West Germany since the ‘economic miracle’ of the 1950s.
It must never be lost sight of that the objective of an R&D project is not  only to meet the target performance specification but, equally importantly, to produce and prove a design that can be manufactured efficiently and exploited profitably.
This is true whether the end product is a manufactured item for sale or a production process for producing it.
The project team cannot guarantee that the product will be so exploited, it can only make it possible.
It can, however, make it impossible if it does not attack the problem with sufficient energy and insight.
The Product Structure Tree
At some stage of manufacture the new product must be assembled from its components.
This must condition the designer in all that he does.
The early creation of a product structure tree, along the lines of Figure 7.1, is the starting point and is the minimum he needs to produce.
By ensuring that each component of the design has, at the time he is working on it, a rational and logical position on the tree, assembly interferences and later modifications to the design can be minimized if not avoided entirely.
The product structure tree deals with materials, processes and   organization.
It identifies each component by its part number and displays the assembly sequence in a series of levels.
These start with the raw material and smallest components, at the highest levels, and display the order of assembly stages into larger and larger subassemblies as the level numbers decrease, the lowest level being the complete product.
At each stage relevant information such as test points and test types are indicated together with special notes on the assembly process.
This information, together with the associated parts lists, is the information needed first in manufacturing departments to allow them to plan their contribution to the project.
They should not be officially released until they are in their final form.
By working on the design and the product structure tree (PST) in parallel the designer goes some way towards ensuring that there is a logical manufacturing sequence that does not require, for example, partial disassembly at some stage.
Wherever possible assemblies and subassemblies must be specified and designed so that they can be inspected and tested as separate items independently of the remainder of the product.
The purpose of the PST is to provide a basis from which calculations, plans and actions can be generated to aid manufacture of a product.
These would include:
(1)
The calculation of economic batch sizes and cycle times for each level of the PST.
(2)
The planning of the assembly layout and organization.
(3)
Process planning — planning and estimating sheets.
(4)
The planning of material procurement.
(5)
The development of manufacturing methods.
(6)
The planning of layout requirements.
(7)
The achievement of marketing flexibility, particularly for families of products.
The ‘Gozinto’ Chart
More information than that provided by the PST is needed for efficient manufacture.
This is usually the province of the production engineering department but all engineers should be aware of the systems and conventions used in their own companies.
Typical requirements are:
(1)
Assembly times
(2)
Inspection stages
(3)
Load centres
(4)
Operation sequence 
(5)
FSP of assemblies
(6)
Set-up times
(7)
Parts lists
This information is conveyed on planning sheets and calculation sheets such as the assembly or ‘Gozinto’ chart, Figure 7.2, and the operation process chart (Stoner, 1978).
The Gozinto chart does exactly what its name implies.
It shows what goes into what to create a subassembly and identifies by coding and symbols the processes involved.
In Figure 7.2 operations are indicated by circles containing code numbers referring to company standards or special instructions and identifying the sequence in which they are to be performed.
Inspection and test stages are indicated by rectangles also with identifying codes.
It is useful in making preliminary plans showing the relationship of the parts, the sequence of assembly and which groups of parts make up subassemblies.
It is a schematic diagram of the manufacturing process at that level of detail for a simple capacitor.
Operation Process Chart
The engineering drawings of the product define dimensions, tolerances and materials to be used.
They specify locations and sizes of holes, with their respective tolerances and the finishes required.
From this data the most economical processes, equipment and sequences can be planned.
Figure 7.3 illustrates this for the same capacitor as that in Figure 7.2.
The operation process chart is a summary of all required operations and inspections: it is a general plan for manufacture.
There are significant differences in the approach to these tasks depending on whether the manufacturing system of the company is product-focused or process-focused.
Statistical Quality Control
When an output of the company is in the form of large quantities of interchangeable items it becomes uneconomical to test each one to ensure that it conforms to specification, so tests are carried out on samples.
Some means of relating the results of the sample tests to the batches they are  taken from is needed.
Statistical approaches to this problem are widely used and some of the basic ideas are introduced below.
Frequency Distribution
In practice the items referred to above will not be identical and the departure from the nominal dimensions will determine the degree of interchangeability and hence the quality of the batch.
This will, in turn, influence the value to the customer and the price he will be willing to pay.
If the same dimension of each item is measured and a graph drawn of the number of times each value of that dimension appears, as in Figure 7.4, the result is called the frequency distribution of the variable or just the distribution.
The dimension measured is called the variate and may be any physical quantity.
The method applies to electrical and chemical quantities as well as mechanical dimensions.
The batch from which the sample is drawn is referred to as the population.
If the jumps in the dimension measured are small, the curve will be smooth, symmetrical and bell-shaped as shown in Figure 7.5.
This is the Normal or Gaussian curve and represents the distribution of events due to chance.
Because its characteristics can be treated mathematically it is central to quality control via samples.
The height of the curve at any point on the dimension axis x is called the probability density of that particular value.
When the distribution is normalized, that is the area under the curve is converted to unity, representing total probability, the height of   the curve represents the probability that a single item drawn from the sample will have that value.
If two values of probability density are nominated then the area they define under the curve is the probability that any item drawn from the sample will lie between them.
Mean
The highest point on the curve represents the arithmetical average or mean value of x , and the symbol for this is x′0 (x bar).
The aim of statistics is to represent a large amount of data by a few simple parameters and the mean of the Normal distribution is one of the most important of these.
Another is the standard deviation which is a measure of the spread of the values of x .
Standard Deviation
The deviation of a value of x is the difference between it and the mean of the sample.
The mean deviation is the sum of the deviations of a sample, treating them all as positive, divided by the number of items in the sample.
The standard deviation is the most important measure of the spread or dispersion of x .
It is analogous to the ‘root-mean-square deviation’ in electrical engineering and to the ‘radius of gyration’in moments of inertia in mechanical engineering.
It is arrived at by calculating the mean of a set  of values and measuring the deviation between this and each value.
Each deviation is then squared; the sum of these values is called the sample sum of the squares and is divided by the number n of items in the sample giving a quantity called the sample variance.
The square root of this number is the standard deviation for which the formula is 
From the values of the mean and standard deviation a useful picture of the distribution of the sample values can be formed.
The standard deviation can usefully be visualized as the distance from the mean to the point of inflection of the bell-shaped curve.
Figure 7.5 illustrates the information available when these two parameters are known.
No matter what the proportions of the normal curve are, 99.8 per cent of all values will lie within 3 standard deviations of the mean, 95.4 per cent within 2 standard deviations and 68.3 per cent within 1 standard deviation.
Another way of stating the information, since the area under the curve represents probability, is any item taken from the sample at random will have a 68.3 per cent probability of lying between — 1 and + 1 standard deviations of the mean and a 95.4 per cent probability of lying between — 2 and +2 standard deviations of the mean.
Design Limits
From this information the design limits for any manufactured item can be set to the economic optimum.
The outer limits shown in Figure 7.5 will allow all but 0.2 per cent of the items produced to be used in the finished product.
This will in many cases create a problem.
Either the physical limits of the item are wide, so that the design and production of the mating parts become difficult and expensive, or they are narrow so that they can only be produced by expensive processes.
In the first case the item in question is cheap to produce but the parts to which it assembles are difficult to design and the assembly process demanding.
In the second the assembly process is easy and the overall design simple but the items themselves are expensive and difficult to produce.
By judicious choice of the design limits in relation to the curve of Figure 7.5 a compromise may be possible at say, 2 standard deviations either side of the mean at which 4.6 per cent of the items will be rejected as unusable.
The remainder of the assembly will not now be required to tolerate as much dimensional variation in the first item, reducing the design difficulty and assembly costs.
If the saving in this direction is greater than  the cost of the difference between 4.6 per cent and 0.2 per cent rejects, which are still cheap to produce, the compromise is worth adopting.
Sample Size
The information obtainable on the quality of a batch of items, intended to be as near identical as is economically achievable, is related only to the number in the sample.
The size of the batch from which it is taken makes no difference.
It is therefore most economical, from the quality control point of view, to produce large batches.
If sets of samples are taken and averaged, these averages will form a tighter cluster around the average of the population than will individual samples.
The larger the sample the more closely will its average agree with the population average, as seen in Figure 7.6 where the population average is represented by the central zero.
The distribution remains normal but the standard deviation decreases as the square root of n, the sample size.
The quantity  is known as the standard error of the mean and the degree of uncertainty in estimating the standard deviation of the population from a sample is given by , the standard error of the standard deviation.
The symbol for the standard deviation of the population is  ta; to distinguish it from that of the sample,s .
Confidence Limits
A confidence limit is the interval within which we can say that the true value of the quantity we are estimating will lie with a specified probability.
When the sample size is large, say 100, the standard error of the mean can be taken as the standard error of the estimate.
It can therefore be said that in a long series of estimates under the same conditions the population mean would lie within one standard error of the mean 68 per cent of the time, or within two standard errors 95.4 per cent of the time.
The confidence limits for estimates of the standard deviation of the population can be treated in the same way when the estimate is based on large samples, using the standard error of the standard deviation as the standard error of estimate.
For small samples, say below 50, the normal distribution cannot be used and it is necessary to use the Student's t distribution.
When small samples are used to estimate population standard deviations, the results are biased in the direction of underestimation.
In terms of variance, that is the standard deviation squared, the relationship is  where the population variance is  and the sample variance is s , the circumflex accent indicating ‘best estimate’.
The expression in brackets is known as Bessel's correction.
It will be seen that as n increases the expression approaches unity; when the sample size is 50 the correction is 0.98.
Student's t is calculated as  where  is the absolute value of the difference between the means of the sample and the population.
It can be seen from this that the larger the error between the sample and population means the greater is the value of t .
Tables are available in statistical texts showing the values of t for given probability levels, relating any given error of estimate to the probability of it being present.
Sampling Plan
The areas under the normal curve outside the upper and lower design limits shown in Figure 7.5 represent the probability of items in that batch being unusable.
A complete sampling plan for quality control can be constructed on this basis (Buffa, 1983).
In Figure 7.7 half of the normal distribution is drawn.
The vertical axis is a measure of the probability of acceptance of an item by the user.
The horizontal axis is a measure of the actual percentage of defective items in the lot being offered to the user, that is the quality of the lot.
Two quality levels are decided, the acceptable quality level (AQL) and the lot tolerance percentage defective (LTPD).
Items at AQL have a high, but not unity, probability of acceptance.
The LTPD is the dividing line between good and bad quality and has a low, but not zero, probability of acceptance.
There is therefore a risk that good items will not be accepted and a risk that bad quality items will be accepted.
The risk that good quality items will not be accepted is the producer's risk and is commonly set at about 5 per cent.
The risk that bad quality items will be accepted is the user's risk and is commonly set at about 10   per cent.
The pricing of the items to the user reflects the settings of these two parameters, which are specified in the contract of procurement.
Process Drift
Patrol inspectors move around the factory in a systematic manner making spot checks during production runs and stopping the process when readjustment is necessary.
A better arrangement is to detect the process drift before that point is reached and rejects are actually produced.
The problem is that the inspector only inspects a small sample at intervals and this can be misleading.
If he records his findings at each visit to a given process and plots them on a graph a tendency to drift can be detected.
If the graph shows that the drift is random and small compared with the design tolerance on the piece part, no action need be taken.
If the indications are that there is a continuous drift in one direction, even though still within tolerance, he will be alerted to possible need for readjustment of the process.
At what point does this become necessary?
Variations in a process are either due to specific causes or to chance.
Chance variations have a natural spread across the normal curve as explained above, for practical purposes .
Any variation greater than this indicates that something in a process has changed.
To take advantage of the effect of sample size, that is that the means of random samples will have a nearly normal distribution if the sample is large, the decision to readjust is based on sample means rather than individual measurements, justifying the use of the 3 s limits.
The grand mean of the samples of 4, which we call , and the mean of the individual observations,, are nearly the same and approach each other as the number of samples increases.
The standard deviation for the samples of 4, represented by , will be much smaller than that for the individual observations because of the averaging effect within each sample.
The relationship between  and s is given by  where n is the size of the sample.
A control chart can now be constructed based on the normal conditions of the process, the upper and lower limits being  as shown in Figure 7.8.
As the process runs, further sample means are plotted on the control chart.
If the plot falls outside either limit readjustment is called for.
Figure 7.8 illustrates the principles of statistical quality control leading to the most economic inspection technique for the degree of control required.
Modifications and additions will be needed to suit practical   cases.
Design tolerances and warning, as distinct from action, lines on the chart are typical of such features.
Maturity of Technology
A further influence on the design of a product and its related production processes is the degree of maturity of the technology involved.
A useful division (Abernathy and Utterback, 1978) is into three stages which are described as ‘fluid’, ‘transitional’ and ‘specific’.
In each stage there is a change in the balance of innovation in product and process as set out in Table 7.1.
An innovative product embodying new technology meets new user needs and sells on performance.
As more is learned about its capabilities and applications, partly from user experience, design changes are required to exploit the findings and cater for individual user needs.
Processes must be flexible and will probably demand high levels of skill.
Management will be entrepreneurial in a relatively small informal group.
After some experience at the fluid level the design tends to fall into a pattern of application areas and sales volume begins to increase.
The demand for higher production levels leads to more formal manufacturing methods and some special process plant is justified.
The organization grows, project and task groups appear creating a need for good liaison.
In the third stage, production consists mainly of standard models and there is a reluctance to undertake specials.
The emphasis is now on cost reduction and most design changes are aimed at improving the price/performance ratio.
The production system is more capital intensive   and rigid, making modifications expensive.
The process plant is special purpose, automatic and specific to products.
The organization introduces more structure, goals and rules into its operation.
The Make or Buy Decision
It is unlikely that any company would find it an economic proposition to make all the components it embodies in its products.
In some cases the technology involved is not within the company's competence; in others it is so critical to the performance of the product that the company wishes to have it under direct control.
Combined with the economics of manufacture and purchasing, these factors create a complex set of conditions to  be considered in making the decision (Bergen, 1975).
A company would normally wish to: Make
(1)
those items that are profitable and within its competence;
(2)
those items that, by reason of security or performance, demand a degree of control not otherwise assured; Buy
(3)
those items not in class 2 where cheaper prices can be obtained by doing so.
In practice, this type of decision is taken quite often and in various parts of the company.
It is helpful if the decision is approached in the same way, using the same criteria, in all departments, to ensure that all such decisions support company policy.
For a typical company the above policy statement might be expanded into guidelines as follows:
Buying is indicated to:
(1)
Extend the capacity of the factory.
(2)
Obtain special technology in an open market.
(3)
Obtain better prices.
(4)
Improve commercial position.
(5)
Avoid capital investment.
Buying is contra-indicated:
(1)
Unless there is an open and competitive market.
(2)
If in-house capacity exists.
(3)
If subcontracted work cannot be brought in-house when factory load falls.
(4)
If high added value is involved.
(5)
If single sourcing would result and
(a)
supplier reliability is doubtful.
(b)
supplier's normal technology would be stretched.
(6)
High supervision level would be required.
(7)
If ‘special’ and low volume to supplier.
(8)
If ‘in depth’ knowledge of the component is important to the company.
With so many parameters to consider, it is helpful to have a system ensuring that the criteria are consistent and applied in the same order.
One way to achieve this is by means of a logic network as shown in Figure 7.9.
Each lozenge in the network is a question box requiring a ‘Yes’ or ‘No’answer.
If the answer is ‘Yes’ the exit from the box is vertically downwards.
If it is ‘No’ the exit is horizontally to the right or left.
The questions relate to the current situation.
Question 1 means ‘have we the appropriate process in-house now?’ not ‘is there a process we could bring  in?’.
Competitive means ‘the same quality could not be obtained cheaper by any other means.’
Question 4 is to allow for the case where a commercial package deal would be more beneficial to the company than making the item in-house.
Such a case would arise if Messrs ABC agreed to buy large quantities of X from the company if the latter agreed to incorporate their Y in the product.
Question 5 is to ensure that whenever the answer to Question 3 is ‘No’ the process is examined to see whether it is still a sound investment.
Question 6 is one of the most important.
Added value pays wages and provides profits.
Items with high added value would only be subcontracted if there were insurmountable problems of some sort in making it in-house.
In questions 3 and 7 it is essential that making and buying prices are on a basis allowing direct comparison.
In question 8 ROC is return on capital in use in the company as distinct from ROI in question 10 which is the return on the specific investment being considered.
In question 14 ‘critical technology’ refers to class 2 in the policy statement above.
Question 16 is simply a logical device to prevent an endless loop being formed in the network.
It will be noted that any path through the network leads to either a ‘make’ or ‘buy’decision.
On the way the path may pass through intermediate decisions ‘divest’, ‘invest’ or ‘change process or product specification’.
After this it re-enters the network and a new set of answers emerge, leading to a final decision.
Network junctions are indicated by a merging curve which should be followed.
Crossed paths are not junctions and reversal on the path is not allowed.
Worked examples of the use of the network are given in Appendix 7.1.
Value Engineering
The economic definition of value is the quantity of some other commodity for which the object in question can be exchanged.
The most common commodity considered is money, so for practical purposes the term is interchangeable with price.
Value engineering (VE) consists of considering the costs of producing a product together with the functions it provides.
The objective is to engineer an all-round improvement in value with benefits to both user and supplier.
The cost of a product is not, therefore, its value.
This can only be arrived at by considering the functions it performs.
VE does not initially consider manufacturing methods, as would a cost reduction exercise, but starts by questioning the necessity of the functions  and the means by which they are performed.
It asks, for example:
What is it?
What does it do?
What does it cost?
What else will do the job?
How much will it cost?
VE is a structured approach to answering these questions.
The key steps are
Selection
Information
Speculation
Evaluation
Investigation and planning
Implementation
Summary
Because all aspects of the product must be considered, VE is essentially a team operation, bringing in those people with specific knowledge, from anywhere in the firm as required, as part-time members.
There should be one full-time member, the VA co-ordinator, whose task it is to maintain continuity, collect and analyse information, schedule meetings and produce agenda and records.
He will be able to perform these duties for more than one team.
Team Composition
A VE team engaged in an engineering design exercise would typically consist of:
R&D engineer
Designer
Production/methods engineer
Cost estimator
VE co-ordinator
Representatives of other functions such as marketing, finance, materials management, servicing and quality assurance are co-opted as required to deal with specific problems.
An individual who has no product knowledge but has imagination can often stimulate ideas.
The co-ordinator is the only full-time member of the team and it is his task to ensure that the others, who should be sufficiently senior to be able to take decisions on actions proposed, feel involved and committed to the value improvement concept.
He does not do this by behaving as if it were his personal project, but by quietly ensuring that things get done as decided.
Selection
The products, or parts thereof, selected for VE should be those that will provide the best return on the considerable investment in time and money that VE demands.
Pareto analysis is a conventional way of establishing this.
This consists of measuring or estimating financial factors such as cost or profit associated with the products being considered and drawing a histogram of money against product or element type.
The bars of the histogram are arranged in descending order of size and it will usually be found that 80 per cent of the money will be associated with 20 per cent of the products, to a first approximation — the origin of the well-known 80 — 20 rule.
This identifies the group of items likely to give the best return on the exercise and puts them in an order of priority.
Information
The scope of the exercise is established and information on costs, specifications, requirements, etc., collected and analysed.
The VE co-ordinator should prepare all this data, in conjunction with engineers, designers and cost accountants as appropriate.
This phase aims at listing all the necessary functions and the costs of providing them, ending with a complete function/cost analysis identifying the areas of high cost and/or poor value.
Speculation
The complete team for this exercise is assembled and briefed on the problem by the co-ordinator.
Each member is encouraged to speculate on alternative methods of performing the functions at lower cost.
It is essential that a chairman capable of controlling the meeting be appointed; no negative or critical comment must be allowed at this stage.
Judgement must be suspended and as many ideas as possible, no matter how fanciful, collected and recorded, from any source.
The notion that ideas come only from the professional specialists in the field must not be allowed to obtrude, nor the traditional ‘we tried that and it didn't work’ story.
While none of the first set of ideas may be feasible or even relevant, they can be built on, allowing the flow to continue freely no matter how perverse the line of argument.
No good intention, however illogical, must be lost at this stage.
Evaluation
When all ideas have been exhausted, and recorded by the co-ordinator, each is reviewed briefly.
Rough estimates of cost are used to create a priority list of the most promising cases for further study.
A systematic assessment of the advantages and disadvantages of each low-cost idea is made to establish the lowest-cost means of performing the function under discussion.
Taking into account both costs and functions, the better-value proposals are identified for further development.
No impractical idea, however attractive, must get through this stage.
Investigation and Planning
The selected ideas will now be thoroughly investigated by the team members by model making, testing, cost estimating, obtaining quotations and any other actions necessary to prove their viability.
Any approvals for making the changes are obtained and the changes themselves are planned.
Implementation
If an existing product is involved, the normal modification procedure will be used, the VE co-ordinator being responsible for observing the formalities.
If a product is under development, the changes become the responsibility of the project leader to ensure incorporation.
In either case the co-ordinator makes a summary report for the record.
Function/Cost Analysis
Cost analysis is the starting point not only for deciding the subject of the exercise but also of the exercise itself.
A simple air pressure control valve as used in aircraft is shown in Figure 7.10 to illustrate the procedure.
Table 7.2 is a breakdown of the costs of each of the component parts.
The analysis of function requires concise and unambiguous definition of the functions performed by the part investigated.
To achieve this it is helpful to use two words, a verb and a noun, to describe each function.
A part may have more than one function.
Thus a case may ‘provide protection’ and ‘provide mounting’and ‘provide screening’for the components within.
The two analyses are combined in a matrix structure as shown in Table 7.3.
The functions are listed horizontally and the parts vertically.
The total cost of the part and the percentage of the total cost it represents is entered in the last two columns and the same is done for the function costs in the bottom three rows.
The lowest row provides for identifying high-cost functions that should be given special attention.
The costing of functions is done by estimating the proportion of the part cost that can notionally be attributed to each function to which it contributes.
The sum of the allocated function costs must, of course, equal the part cost.
The VE team now has the data displayed in a manner allowing it to spot anomalously high costs of performing any function.
In a given product range, experience with the procedure rapidly creates a set of norms for functions and any increase in cost above these is very obvious.
This in itself is a great help to designers even before any analysis of their new product is undertaken.
It may appear surprising that three parts in the valve have been found to have no function at all and that they amount to over 5 per cent of the total cost.
Not surprisingly they have been labelled ‘High’.
This is not an uncommon result of this type of analysis and stresses the difficulty of the craft of the designer, particularly under time pressure.
Study of the operation of the valve shows that the servo air will completely seal the valve without the aid of the spring and that it and the diaphragm plate can therefore be eliminated together with the turned spigot inside the cover.
The other notable high cost function is ‘connect parts’ which accounts for over 60 per cent of the total.
By building this function into other basic parts of the valve, such as the body assembly and cover, as in Figure 7.11, the associated cost is reduced from 12.08 to 2.14, halving the percentage of the total.
This is achieved by eliminating not only functionless parts but also unnecessary machining operations and simplifying the shape of the sand castings.
The overall cost reduction is from 19.4 to 6.29 without any loss of performance, as seen in the analysis of the new design in Table 7.4.
The cost, including the opportunity cost, of value analysing existing products and implementing the results is significant and involves people from a wide range of company functions.
Before embarking on an exercise it is important to establish that the increase in profit and/or sales   and market life of the product will together produce a significant benefit after recovering the costs, particularly those of implementation.
If the overall effect is to increase profit but not volume the resources may be better utilized in creating a new product.
The increase in production volume that the new product will generate will distribute overheads more thinly and hence reduce costs on the whole range of products, causing a double increase in profits.
A preferred time for introducing the procedure is in the design phase of the development project.
The time and cost is minimized and the need for modifications when the product is in production, wasting the effort spent on initial production engineering, is avoided.
Review
This chapter has:
(1)
Described the product structure tree and Gozinto chart.
(2)
Described the concept of quality control.
(3)
Described a sampling plan.
(4)
Described the maturity of technology concept.
(5)
Presented a logical basis for the make or buy decision.
(6)
Described a procedure for value engineering.
Not all projects are suitable for carrying out in-house.
Chapter 9 considers some of the issues arising when a contract for such a project is placed with an outside organization.
Appendix 7.1 The Make or Buy Decision — worked examples
Example No. 1 — 4BA nuts
There has been in past years a steady requirement for 4BA nuts.
It is now diminishing as metric sizes were adopted some time ago.
They are produced in-house on an auto.
Q1 The answer is Yes.
Go to Q2.
Q2 The answer is Yes.
Go to Q3.
Q3 The answer is certainly No since there is a free market of specialist suppliers operating on a much larger scale.
Go to Q5.
Q5 If the prices of the present load are not competitive and the auto only produces nuts and screws, the process is not competitive.
The answer is No leading to the decision divest and re-enter the logic network at Q1.
Q1 The answer is now No.
Go to Q6.
Q6 The added value cannot be high on such items.
Go to Q7.
Q7 The answer is Yes since they can be bought in an open and competitive market.
Go to Q8.
Q8 The answer is Yes since there is no longer a capital investment to finance and buying is at competitive prices.
Go to Q9.
Q9 The answer is Yes for the same reasons as Q7.
The final decision is therefore ‘Buy’.
If the answers to Q5 and Q9 were both Yes, indicating that the process is competitive without the 4BA load and that the supply Q&R; is acceptable, buying would again be indicated via Q8 since buying small quantities of mass-produced items rather than investing in equipment to make them in-house must improve the company ROC.
Example No. 2 — Opto-electronic widgets
OEWs are essential components of ABC's high technology products.
Their manufacture demands a high investment in capital equipment and know-how.
ABC does not consume a sufficient volume to justify the investment, and its corporate plan does not include component sales.
The OEWs are proprietary devices available from several suppliers on the open market so they do not rank as critical technology.
The route is therefore via Q1 to Q6 where the answer is Yes, leading to Q10.
Q10 The answer is No.
Go to Q14.
Q14 The answer is No.
Go to Q15.
Q15 The answer is Yes.
Go to Q16.
Q16 The answer is No.
Go to Q9.
Q9 The answer is Yes as there are established suppliers in an open market.
This leads to the ‘Buy’ decision.
Example No. 3 — Replacement of capstan lathes
One or two autos in the machine shop have excessive maintenance costs and require frequent re-setting due to age.
They have high reject rates and hence uncompetitive prices.
Consider a fictitious part that might be made on them in order to go through the network.
Q1 Yes.
Q2 Yes.
Q3 No — due to old age.
Q5 No — same reason.
This leads to the Divest decision and re-entry into the network.
Q1 No.
Q6 Variable load — sometimes Yes.
Q7 No, if new capstans produce attractive prices.
Q10 Yes.
Q11 Yes leads to invest decision and re-entry into network.
Ql Yes.
Q2 Yes.
Q3 Yes.
Q4 Yes.
This leads to final ‘Make’ decision.
ABC is led into an equipment replacement exercise by a series of logical steps.