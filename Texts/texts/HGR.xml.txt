

Chapter 1
Introduction
The last two decades have witnessed vast improvements in the capabilities of computers.
They have become smaller, faster and able to store huge amounts of data.
Despite these technological advances there have been few developments in the method of communicating with the computer, which remains keyboard-based.
Alternatives are via spoken or written mode of communication.
Neither of these are possible, in any general sense, at the present time.
The development of reliable text recognition procedures would serve two important functions.
Firstly, it would make it easier for users who are unfamiliar with keyboards to communicate with the computer by using their normal handwriting.
Secondly, existing paper documents could be scanned into the computer enabling them to be further processed without the necessity of reproducing the original.
Existing systems for performing text recognition are susceptible to errors.
Improvements in these systems could be achieved by incorporating linguistic knowledge into the systems to assist in the recognition of the text.
1.1.
Text Recognition
The difficulty of the problem facing text recognisers is related to the format of the text.
For the purpose of this project text is categorised in terms of how it is produced (i.e. printed by a machine or handwritten by a person), and how it is input to the computer for recognition (i.e. is it recognised dynamically as a person writes or already existing text which is scanned in).
Printed text may be produced by a computer or typewriter.
The text may either be in a single font or a mixture of fonts (e.g. Courier, Roman, Helvetica, Bold, Italic etc.) and different font sizes.
With the increasing popularity of desktop publishing systems (DTP) the use of a variety of font styles is becoming increasingly common.
Handwritten text, either cursive or hand-printed, is much more difficult to recognise than printed text.
Hand-printed text is less natural for adults to produce but is simpler for a computer to recognise.
Cursive script is the mode in which a person normally writes text but is more difficult still to recognise automatically.
The method by which the text is to be input to the computer is also important.
There are two general methods for inputting text into a computer.
With static (off-line) input the text already exists on paper and is digitised by a scanner.
The scanner converts the text to a pixel representation of the text.
Recognition software then attempts to determine the text that was written.
Dynamic input of the text involves the user generating the text at run-time using either an electronic bitpad or electronic paper.
With dynamic input the order in which the pixels are formed is known and provides accurate stroke-position and time-sequence information.
This simplifies the recognition problem since overlapping points may be simply discriminated between and the problem becomes one of pattern recognition.
Static input lacks temporal information — there is no indication about the pen's direction or speed, nor are the locations of the pen-up and pen-down movements known.
Processing static input is also more difficult because the scanning procedure often introduces additional noise to the input.
The application of dynamic input is more restricted than static recognition since it demands that the user have available a suitable input device at the time of writing and is obviously only applicable to handwritten text.
Static input allows for a greater variety of input formats and makes no requirements about the availability of hardware at the time of writing.
1.2.
Motivation for Text Recognition
There are two principal reasons for doing text recognition.
The first is to provide a more natural method for communicating with computers.
The second involves conversion of already existing paper documents into a format that the computer can process.
1.2.1.
Natural Communication with Computers
Communication with computers and computer controlled machinery is normally achieved using a QWERTY keyboard.
The QWERTY keyboard has its origins in the days of early mechanical typewriters.
The keys were organised in such a way to prevent the typist from typing too quickly and thereby prevent the mechanically-operated rods of the typewriter from sticking.
(An alternative story suggests that the top line of the keyboard was arranged to enable typewriter salesman to find the word ‘typewriter’ easily.)
The arrangement of the keys is now so firmly established that attempts to alter it have met with failure.
Millions of typists throughout the world have trained on this keyboard layout and are (understandably) resistant to change.
Hence the QWERTY keyboard remains.
Unfortunately the problem with the QWERTY keyboard is not simply the order of the keys — good typists are able to achieve very high speeds.
The major problem with the QWERTY keyboard is that it needs to be learnt.
Non-typists have difficulty finding the desired keys and may take a long time to type even a short word.
Longer communications with the computer are extremely difficult for novice users and almost certain to produce errors.
An additional problem is that while concentrating on the keyboard when using an interactive program the user may miss important happenings on the screen.
Attempts have been made to improve upon keyboard communication with the computer.
Most noticeably the mouse has become a prominent computer accessory.
Quite complicated functions may be performed with mice via the use of menus and icons.
However a mouse is not a replacement for a keyboard, simply a supplement.
In order to input text it is still necessary to use the keyboard.
As computers become increasingly available to non-typists more natural means of communicating with them are sought.
1.2.1.1.
Speech or Writing
There are two natural methods of communication available — speech and handwriting.
A commonly used argument in favour of speech recognition is that it is the most natural communication medium.
Children learn to speak and understand speech first and without the formal instruction needed when learning to read and write.
As a result speech is often regarded as being easier to recognise than text.
Although speech is the more natural communication method for humans, literate people feel no sense of priority between speech and writing — indeed, many people also find it easier to organise their thoughts using pen and paper.
It should be remembered the situations in which speech is used.
Speech is normally used in face to face interaction whereas writing is used across barriers of space and time.
Unless the speech is being used as part of an interactive dialogue system then it is likely that writing is more natural for the situation.
Other advantages proposed for speech are that the hands are free to perform other tasks and that communication may be carried out over a standard telephone line without the requirement for additional hardware.
There are arguments in favour of handwriting recognition.
Writing allows private communication with the computer that is not possible with speech recognition.
A frequently cited example of this situation is a doctor's surgery where the doctor may wish to take notes during a patient's examination.
There may be details about the patient's condition which the doctor wishes to record, but not discuss in the presence of the patient.
Writing is also more flexible than speech.
Drawings and sketches may be produced (a picture paints a thousand words) with a pen, but not easily by voice.
Many facets of spoken language are absent from written language.
Different degrees of planning are associated with speech and writing.
Spoken language is spontaneous which results in hesitations (errm, um) and false starts, and a grammatical structure can be cut-off in mid-production and replaced by another (Hindle, 1983a).
The transient nature of speech does not permit editing of the speech signal.
False starts do occur in written language but they are far less common.
Writing also provides a permanent representation, hence the writing may be manually edited, thereby simplifying the identification of false starts.
Written language also tends to be more grammatically acceptable than spoken language.
Non-standard grammatical forms such as‘we was’ do not tend to appear in written form (except in literature when trying to convey speech).
Written language also tends to be more conservative with regards to the introduction of new words.
Neologisms come and go very quickly in spoken language but tend to be less frequent in writing.
For an extended communication with a computer the ability to edit the input is an important consideration.
It is often inconvenient (or impossible) to complete a communication with a computer in a single session.
The ability to modify and alter what has gone before is therefore a necessary part of any system.
This point is conceded by Jelinek, a leading speech recognition researcher, who admits ‘To get a really good interface, it may prove necessary to use writing tablets and achieve automatic recognition of hand-produced diacritical marks and possibly of handwriting as well.’
(Jelinek, 1985b).
Speech is heavily supported by gestures and other cues from context.
Much of the meaning is implicit and therefore absent from the speech signal.
For handwriting, the writer and reader are apart and so there is much less support from context in the signal.
1.2.1.1.1.
Problems for speech and writing recognition.
There are three main problems involved in the recognition of speech and handwriting.
These are variability, segmentation and operational difficulties.
The principal problem for both speech and handwriting recognition is the variability of the input.
One person's speech differs from another's due to both physical (physiology of vocal organs, age, sex and state of health) and sociological (accent) reasons.
Even words spoken by the same person are never identical (Vaissiere, 1985).
A speaker makes heavy use of prosodic features to convey meaning.
There are three parameters by which a speaker is able to modify the meaning of the utterance — pitch, volume and tempo.
The same sentence may be expressed as either a question or a statement simply by varying it's pitch (e.g. ‘They're here!’ and ‘They're here?’).
There are a wide range of variations in pitch available to express nuances and degrees of emphasis.
The volume of the utterance may be loud to express anger or whispered for secrecy.
Furthermore individual phonemes may also be stressed.
The tempo of speech differs depending on the urgency (rapid) or deliberation (slow) of the speech.
The combination of these features emphasises a large problem for speech recognition — the same word may be spoken in many different ways and never match exactly what the system was trained with.
Writing is also very individual.
Writing is a motor skill which ensures that the same words written by different people are distinct — allowing signatures to be used as verification of identity.
There are a number of dimensions in which writing can vary including size, slope and connectiveness.
Even a single writer is seldom able to reproduce exactly the same writing.
However, many of the prosodic problems found with speech are absent from writing.
The orthographic representation of a question or statement is achieved by the use of punctuation.
Written language is generally structured.
For example, it tends to be divided into paragraphs to simplify the structure of an argument.
The presence of such features can provide assistance in recognition of the text.
Segmentation is another problem that affects both media.
The problem arises for speech because speech sounds are not discrete.
Rather they overlap and as one sound is produced the vocal apparatus is preparing to speak the next one.
As a result segmentation of the signal into the constituent units of speech is difficult and requires knowledge of the language.
Likewise, in cursive handwriting it is often difficult to determine where one letter stops and another starts.
The segmentation problem for speech may be illustrated by listening to a foreign language about which one has no knowledge and trying to locate the word-breaks.
This is not simple.
Normal speech is so rapidly and informally articulated that there are seldom pauses between the words (this may be verified from the waveform of the speech signal).
The situation is different for native speakers of the language who automatically perceive the speech as being chopped up into discrete units.
Words spoken in connected speech often sound different than when spoken in isolation.
Connected speech can lead to elision (omission of sound, e.g. ‘go away’ is often pronounced as ‘go way’) and assimilation (adjacent sounds are modified to sound more like each other, e.g. ‘ten bikes’is pronounced as /tem baiks/).
Therefore training a connected speech recognition system with isolated words may not be satisfactory for a connected word recognition system (McInnes & Jack, 1988).
Handwriting recognition is also plagued by problems of segmentation.
The detection of word boundaries in written text is relatively simple (due to the custom of leaving spaces between them despite the absence of pauses in speech) however the segmentation of a word into its constituent characters is difficult.
This is especially true for cursive handwriting where a number of alternative segmentations may be possible for a word.
For example the letter d is visually similar to the letters cl.
A further problem is that handwritten letters are contextually dependent on the surrounding letters (e.g. the letter e is often written differently in the words gent and rent).
Despite these problems the detection of possible segmentations is less problematical than for speech.
Both speech and handwriting recognisers suffer from operational difficulties.
For speech recognition the principal operational difficulty to be faced is the interference to the acoustic signal from background noise.
It is necessary to filter out the sound that does not correspond to the speech from the input signal — in a noisy environment this is problematical.
A further problem can be the microphone performance — different microphones can produce different acoustic signals which need to be standardised.
Until recently a problem with handwriting recognition was that the tablet on which the writing took place and the screen on which it was displayed were physically separated.
However, recent hardware developments have resulted in electronic paper — a tablet-LCD device which unifies the computer monitor and graphics tablet.
A pen (or stylus) is able to draw on the LCD-screen and be shown as electronic ink.
Perhaps the most telling argument for the use of handwriting instead of speech as a natural communication medium is the current state of the commercially available systems based on speech and writing.
Despite considerable research and investment a large discrepancy exists between human and computer recognition of speech.
Currently available speech recognition systems impose a selection of constraints on the input to simplify the speech recognition task.
Typically these constraints may include (however these constraints do not apply to all speech recognition systems):
Limited vocabulary
The vocabulary size for a system is often very small (of the order of hundreds of words).
Speaker dependence
For a new speaker to use a system the speaker must train the system to her voice.
Training procedures may be both long and tedious involving repetition of sets of words.
Disconnected speech
Brief pauses are required in the speech to discriminate between different words.
The resulting speech is unnatural both to produce and to comprehend.
In contrast pen-based systems have received much less attention than voice-based systems but a number of commercial products are currently emerging on the market (e.g. Penpoint, Paragraph and PenWindows).
In general, the handwriting recognition systems supplied with these systems are based on printed handwriting.
The commercial success of even such limited systems would suggest that now, more than ever, handwriting presents a medium for natural communication with computers that is convenient, flexible and possible.
1.2.2.
Static Recognition of Documents
Although communication between computers is becoming increasingly common, paper (and FAX) remains the principal medium of communication in the business world.
Unfortunately text on paper cannot be manipulated by computer.
The need to enter printed text or data into computers without the effort of re-keying it is the driving force behind the development of optical character recognition (OCR) technology.
An added bonus is that much faster input processing rates are possible with OCR than manual entry of the text.
Using this technology text is entered into the computer via a scanner and software then classifies the characters from the pixel representation.
Typically OCR systems are used in offices to create electronic copies of letters received from other offices, and for the inclusion of spreadsheets and tables in printed reports.
They are also of use in transferring text between incompatible word processing systems or in the restoration of an electronic file that has been accidentally erased.
1.3.
Linguistic Processing for Text Recognition
A number of factors can cause interpretation errors by OCR systems.
They may arise due to poor quality of print, letters being printed together, a variety of fonts being used, the document being submitted to the scanner at an angle, an unknown font being used, etc.
There is also the problem that some letters are visually very similar.
For instance the letters h and n, or c and e are easily confused.
There may also be problems in deciding on the segmentation of a word into letters e.g. the letter combination cl can easily confuse with the letter d.
Rosen (1991) classifies the errors that can arise and these are shown in table 1.1.
Of these errors the most common is substitution.
Handwriting recognition is much more difficult than OCR due to the variety of handwriting styles between writers.
Handwriting recognition systems experience similar errors to those of OCR systems but to a greater degree.
Recognition of cursive handwriting is especially prone to errors due to the difficulty of determining the correct segmentation of a word.
The general output from a character recogniser is a lattice of the possible characters with indications about how close the match was between the character and character templates stored in a database (see $1.4.2).
Selection of the correct character combinations from the character lattice requires some higher-level knowledge.
1.3.1.
The Human Expert.
A primary motivation for using higher-level knowledge to improve the performance of a text recognition system comes from the human ability to recognise text.
Humans are able to identify and discriminate a word from tens of thousands of other possible words.
They are also able to do this very rapidly (approximately 240 words per minute) and convert the written symbols on a page into a mental concept.
This capability far exceeds the performance of any computational system.
The examination of the techniques that humans employ when reading and try to incorporate them into a computational system is therefore worthwhile.
1.3.1.1.
The use of higher level information.
A wealth of evidence exists to prove that reading involves much more than simply recognising the individual characters that form a word.
Early experiments (Cattell, 1885) found that letters are easier to read when they form a word than when they do not (the word superiority effect).
This finding came from testing the ability of readers to recognise strings of letters when only briefly exposed.
If the string of letters were totally random and did not form words then the subjects could accurately report a maximum of three or four individual letters.
However, if the strings of letters formed words then they were able to accurately report a number of short words containing more than three or four letters given the same exposure.
Hence the time taken to recognise a string such as FONHGTAEW is much longer than that taken to recognise the arrangement FOG HAT NEW.
More sophisticated tests (Reicher, 1969) confirmed Cattel's deduction that familiar written words are perceived as units, not as strings of letters.
Shannon (1951) demonstrated that there is over 50% redundancy in the English language by investigating the ability of readers to predict what letters were missing from words.
He found that a reader's familiarity with the language enabled his subjects to fill in missing letters.
The experiment consisted of presenting an unseen passage of text to a person and asking her to guess one letter at a time.
If the guess was correct the subject was told so and moved on to the next letter.
If the guess was wrong then she was told so and asked to guess again.
Below is some of the text that Shannon used in the test with spaces between words being shown as underscores.
If more than one attempt was required to guess the correct letter then the character is emboldened and the number of attempts shown as a subscript.
The results obtained are impressive.
Of the 102 symbols, 79 were correctly guessed first time and only 8 symbols required more than 5 guesses.
Eye movement studies provide further evidence of the redundancy present in English.
The eye does not sweep smoothly from left to right when reading but instead makes a series of fixations with rapid movements (saccades) between fixations.
Analysis of the eye fixations that people make when reading provides some insight into what visual information is being processed.
From such studies (Just and Carpenter, 1987) it has been determined that only about 65% of the words in a passage are fixated — substantial proof of the importance of higher level processing.
It has been shown that readers differentiate words depending on whether they are content words (e.g. adjectives, adverbs, nouns and verbs) or function words (e.g. conjunctions, articles and prepositions).
Readers fixate a high proportion (over 80%) of the content words in a text compared to only about 40% of the function words.
This differentiation between function and content word is further substantiated by letter cancellation studies which showed that letters in function words are more often missed than letters in content words (Smith and Groat, 1979).
Eye movement studies have also been used to show that humans employ syntactic information when reading.
Carpenter and Just (1983) recorded the fixations made by readers when reading a syntactically ambiguous sentence (’ The pickpocket stood before the black-robed judge entered the courtroom to convene the jury.’)
Most subjects initially interpreted ‘before the black-robed judge’ to be a prepositional phrase meaning ‘in front of the judge’and were surprised by the word ‘entered’and consequently spent longer time on it.
A control sentence which substituted the unambiguous conjunction ‘while’ for ‘before’provoked no pause at the word ‘entered’.
This provides evidence that readers try to determine the syntactic role of ‘entered’ while fixating it.
In summary, humans employ much more than just the visual stimuli when reading.
Many processes operate beyond the simple pattern matching of the individual characters.
In order to understand language humans must use their knowledge of word structure, world knowledge, syntactic structure, semantic information, idiomatic expressions and word frequency when reading.
Simply improving the performance of the pattern recognition module will not produce a recognition performance comparable to that of a human.
For an automatic text recognition system to succeed it should exploit as much of the higher level information as is computationally possible.
There are a number of different knowledge constraints that can be applied to improve the performance of the pattern recognition information.
1.4.
System Overview
For a text recognition system to succeed it is necessary to combine a number of knowledge sources (e.g. syntactic, semantic, lexical and pragmatic).
The work reported in the present thesis was carried out in the context of the development of a complete recognition system (Boes et al, 1989).
This system incorporates several higher level knowledge sources and a general overview of the system is provided in figure 1.1.
1.4.1.
Character Recognition
The first level of recognition involves character recognition of the input.
A number of processes are involved in this level (Wright, 1989).
For dynamic input (i.e. handwritten on a graphics tablet or electronic paper as recognition occurs) the text consists of a sequence of x-y co-ordinates which are vectorised using Freeman encoding.
Fig 1.2 gives some indication of the Freeman encoding process.
The character a is written in Fig 1.2(i).
The way that this is input to the recogniser is as a sequence of x-y points with an indication of the order in which the points were written (Fig 1.2ii).
Adjacent points are then converted to vectors resolved into the directions shown in Fig 1.2iii.
By knowing the vector directions the character can now be converted to a numerical sequence.
Hence the letter a shown would be the numerical sequence 3456712601.
The permutations available from using an unrestricted number of vectors are too vast to provide a database matching scheme.
Therefore a reduction stage is used to reduce the vector sequence to a maximum of five vectors.
The reduction process determines which five vectors are the most significant of a vector sequence based on the lengths of the vectors and how different the vector's direction is from its predecessor.
A typical result of this reduction process is shown in fig 1.2iv.
Hence the letter can now be classified as the numerical sequence 45716.
The Freeman coding can be further enhanced by supplementing it with an indication of what proportion of the whole character each vector represents.
A database is created that contains a selection of such vector sequences and the letter that each sequence represents.
At recognition time the input is reduced in the same manner as the training database.
The vector sequence obtained is matched against what is stored in the database.
The selection of letters that are similar to what was written are retrieved with an indication of how close a match was obtained.
A similar recognition process is carried out for static input.
The difference in this case is that, given the character points in fig 1.2ii there is no indication about which sequence the points were created in.
Hence a pre-processing level is required which determines the probable sequence (Wright, 1989).
This process is not completely reliable, introducing further difficulty into the recognition procedure.
Other systems use various other techniques, along the same lines (Tappert et al, 1990).
1.4.2.
Word Recognition
The rationale behind using word structure to aid recognition is that people normally write words rather than random character combinations.
Hence character combinations that form words are considered to be more probable than character combinations which are not words.
Also recognition experiments (e.g. the word superiority effect) show that humans recognise on a word-level basis.
Within the speech recognition community word-level restrictions are also utilised.
Checking of word structure may be achieved by looking up each of the possible character combinations in a list of words.
More than one word may be created from the character lattice.
For example the letters d and cl are physically similar.
Hence it is easy to envisage the words ‘dogs’ and ‘clogs’being confused.
The character recognition phase may propose a selection of possible characters for the input.
This results from uncertainty about the segmentation of the input string (i.e. where does one character stop and the next one start in a cursive word).
Typically the characters are presented as a lattice of possible characters.
Table 1.2 shows the contents of such a lattice that was obtained for the word its (the graphical representation is shown in Fig 1.3).
Each segment of the input word has a set of possible character combinations with a probability score to indicate how close the character matched the character stored in the database.
There is also a list of the next segments to be tried in the lattice.
The word recognition module must traverse the lattice and determine which of the possible character sequences form valid words (i.e. words in the lexicon).
For the example shown there are three possible words to be found in the lattice: its, us and ox.
The syntactic and semantic information about each of these words is then made available to the relevant processors.
A numerical index (the root index) is also returned which is an indication of the morphological root form of the word.
Other systems use sub-word information to verify whether character combinations are legal (e.g. Higgins and Whitrow (1984)).
Rather than store the complete words in a lexicon, such systems store the probabilities of letter transitions.
Very few systems attempt to use any information beyond the level of word recognition.
1.4.3.
Compound Recognition
The identification of common phrases and word combinations may assist in selection of the correct word.
For example, if the words prior to dogs/clogs had been raining cats and then it would be fairly safe to say that the intended word was dogs.
An additional necessity for performing compound recognition is that many of the compounds will have different syntactic and semantic properties than the individual word combinations.
Without this level of recognition incorrect assumptions will be made about the words.
Compound recognition determines whether consecutive words are able to form idiomatic or compound lexical entities (i.e. words containing spaces).
Essentially it involves reducing the individual words to their root index.
The combinations of words that form compounds thereby become number sequences which are stored in a compound tree.
This will be dealt with in detail in chapter 4.
1.4.4.
Syntactic structure.
Humans find it much simpler to process grammatically well-formed language than ill-formed input (Miller & Isard, 1963).
One can imagine the use of a recognition grammar which states what language is well-formed and prefers such input to ill-formed alternatives.
Hence such processing would prefer combinations of the form the cat sat on the mat to cat mat on sat the the.
The application of such information to assisting in the recognition of text forms the basis of this thesis.
1.4.5.
Semantic Processing.
The aim of semantic processing is to demote word combinations that are not meaningful.
What constitutes meaningful is the basis for a philosophical argument.
There have been a number of semantic theories proposed.
One factor that unites all of these theories is that no computational system exists which is able to cope with unrestricted English.
The manner in which semantic information is applied in the current system is to utilise dictionary definitions and collocational information from corpora (Rose, 1990).
Collocational information is obtained by taking a corpus and finding how often pairs of words are found together.
A collocation ‘definition’ is then stored for a word.
The possible words in the input are given semantic scores based on how well they combine with possible neighbouring words in the input.
The way in which the definitions are used is as follows.
Two words are awarded a high semantic score if the definition of one word contains the other.
A weaker indication is that their definitions contain a number of similar words.
For further details see (Rose & Evett, 1992).
1.5.
Sources of Knowledge
Having determined the kind of information it would be advantageous to utilise in a handwriting recognition system the next problem which must be addressed is where such information may be found.
There are three main sources of machine readable information: machine readable dictionaries, corpora and existing parsers.
Machine Readable Dictionaries (MRDs)
To find out how a word behaves the obvious place to look is a dictionary.
Dictionaries are the results of a lexicographers analysis of a language.
The information contained includes the syntactic and morphological behaviour, phonetic description and a definition of the words contained.
A number of types of dictionaries exist, some of which are in a computer-readable form.
Each dictionary differs in the information it contains and the style in which the information is presented.
For example the information within the Oxford English Dictionary gives a description of almost all of the words in the English language and their etymological history.
A learner's dictionary will define far fewer words and concentrate on examples of usage for those words.
Corpora.
Corpora provide a representative sample of language.
Corpora may be raw (text only), tagged (each word is assigned a syntactic category) or analysed (parsed)(see $4.5.2).
Many of the syntactic phenomena that occur in English are rare.
In order to capture as much information as possible corpora must be large.
Corpora are becoming increasingly popular within linguistics to evaluate existing natural language systems, investigate the occurrence of linguistic features and the production of probabilistic models of language.
Existing parsers
Much of the work within linguistics has involved the development of grammars to specify the linguistic regularities of language.
Application of grammars requires knowledge of the grammatical behaviour of the words of the language (stored in a lexicon) and a parsing algorithm to specify how to apply the knowledge of the grammar and the syntactic behaviour of the words to the input.
Rather than duplicate the work carried out in the creation of these parsers, the use of existing parsers should prove to be beneficial.
1.6.
Summary
Alternate ways of communicating with computers are required which do not require a keyboard.
This chapter has considered the two most natural methods of recognition used by humans — speech and writing.
Despite a large amount of research into automatic speech recognition the results have been unimpressive.
Although recognition of handwriting has received much less attention the task is simpler and promising results have been obtained.
Commercial systems are currently available that can recognise handprinted characters.
An ulterior motive for performing text recognition is to convert existing printed material into a computer format that permits further processing.
Available OCR systems are able to perform this conversion very rapidly, thereby saving much effort and avoiding the necessity of re-typing the text.
There are difficulties associated with automatic text recognition however.
With printed material the font may be unknown to the system or the copy may be of poor printed quality.
The recognition of handwriting is even more difficult due to the number of different ways in which letters are written and the visual similarity of many characters.
Furthermore, with cursive handwriting, words are difficult to segment into the constituent letters.
Humans experience few of these problems when reading.
The principal reason for this is the high degree of reliance a human places on linguistic information.
Humans do not read character by character.
Instead recognition is based on the context in which a word is found.
Incorporation of some of the linguistic information that humans employ is necessary to improve text recognition systems.
The current project is imposing lexical, syntactic and semantic constraints on the input to enhance the recognition rate of text processing systems.
This thesis examines the use of syntactic information for assisting in the text recognition process.
Chapter 2
Literature Review
This chapter reviews research into handwriting recognition and natural language applications that involve syntactic knowledge of some description.
This chapter will provide
A description of the use of linguistic information in handwriting recognition systems
A brief history of the use of syntactic information in NLP systems.
A description of the main theoretical paradigms that have been developed and consideration of how they are suited to the text recognition task.
2.1.
Handwriting Recognition
In comparison to speech, automatic handwriting recognition has received little attention.
Early attempts at the automatic recognition of handwriting date from about 1960 (e.g. (Earnest, 1962),(Eden & Halle, 1961),(Eden, 1964),(Frishkopf & Harmon, 1961) etc.).
A hiatus followed until the development of Sayre's static system (Sayre, 1973) re-kindled interest in the problem and numerous systems followed (e.g. (Tappert, 1984),(Brown & Granapathy, 1980),(Higgins & Whitrow, 1984)).
The approaches taken by these systems to recognise cursive script fall into two categories — either whole word matching or segmentation followed by letter or stroke recognition.
With whole-word recognition the features of a complete word are extracted and matched against a stored database to find the closest match (e.g. (Brown & Granapathy, 1980)).
This approach has the advantage that it is fast since it requires no segmentation of the word.
However this system is restricted to recognising only a small number of words since it must be explicitly trained for every word in the vocabulary.
The alternative approach uses segmentation methods to recognise the word.
The complete word is taken in and segmented into individual characters or strokes (e.g. (Higgins & Whitrow, 1984),(Wright, 1989)).
Segments are then matched against a database of character shapes to produce a lattice of the likely characters.
Word level restrictions may be imposed on the lattice by only allowing character sequences that form words (thereby allowing semantic and other higher-level to be applied).
With this approach the vocabulary can be infinite and the system requires significantly less training.
2.1.1.
Linguistic information in text recognition systems
Despite evidence that reading encompasses much more than simple character recognition little of this language information has been exploited in text recognition systems.
The language information most often incorporated is that of word structure.
Knowledge of word structure is often applied to the results from the character recognition module.
The manner in which the word-structure knowledge is applied is dependent on the output produced by the character recogniser.
For text input some shape recognisers produce a single character candidate for each possible character, whereas others produce a string of alternative character candidates with an associated weight for each alternative for each possible character.
If only a single character is suggested then word knowledge is used to detect and correct errors in the hypothesis.
Examples of this approach are provided by Srihari and Bozinovic (1982) and Ullmann (1977) who developed a probabilistic model of the substitution, deletion and insertion errors that were likely to occur which modified the output from the shape recognition module.
When a shape recogniser is used that produces alternative candidates the application of word structure involves determining which of the possible character combinations form words.
Two general techniques have been used to determine whether a string is a legal combination of characters.
The first is lexicon-based and involves storing a list of the allowed words.
Character combinations are permitted dependent on their membership in this list (e.g. (Wells, 1992),(Srihari, Hull & Choudhari, 1983)).
The alternative approach uses n-gram techniques.
The probability of bigrams and trigrams (letter pairs and triplets) is determined for the language.
This information is then used to rank the possible character combinations that are found (e.g. (Riseman & Hanson, 1974),(Whitrow & Higgins, 1987)).
Both systems have advantages and disadvantages.
Use of n-grams requires little storage or processing but is less efficient at discriminating between acceptable and unacceptable letter strings.
Lexical lookup is more demanding of storage and processing but is better at rejecting unacceptable letter strings.
A major disadvantage of a lexicon-based system is that if a word is absent from the lexicon then it will be rejected by the system.
To reduce the likelihood of this a large lexicon must be stored.
A recent review of research on handwriting research (Tappert, Suen & Wakahara, 1990) showed no practical application of syntactic information for improving handwriting recognition.
More recently, however Crowner and Hull (1991) have carried out work on this application.
The methods used are similar to the statistical techniques that have been arrived at independently in this project.
2.2.
Computational Linguistics
There are diverse motivations for developing systems for processing natural language.
For computational linguists the over-riding concern is their practical use.
In general, computational linguists are more willing to accept computer systems that are able to process a useful range of language input.
Incomplete systems are accepted as a fact of life.
Features of language that interest theoretical (academic) linguists include linguistic competence (the way in which people decide whether a sentence is grammatical or not), language universals (grammatical principles that apply to all natural languages) and in finding the simplest, computationally most restricted theory that can account for natural language.
It is characteristic of theoretical linguists that they select example sentences that computational linguists would categorise as pathological.
Another important motivation is that of the psycholinguists who develop computer parsing systems as test-beds for hypotheses about human linguistic processing.
These systems are mainly interested in comprehension and the over-riding aims of such systems do not include wide-coverage or computationally efficient methods.
The different linguistic fields have influenced each other but there have also been marked differences between the fields.
Wilks (1983) argues that linguistic theory has had little direct influence on parsing techniques.
Specifically he points out that parsing is performance motivated and directional (i.e. from surface to structure)— the very opposite of the aims of competence theory.
It is also the case that the elegant theories created by theoretical linguists need not necessarily evolve into computationally effective techniques.
The most notable case of this is the application of transformational grammar to automatic parsing systems which proved theoretically very appealing, but computationally unsuccessful (c.f. $2.2.3.1.2).
2.2.1.
Syntactic Processing
Syntax is the study of structural patterns in word order and structure.
It is the first level at which human language diverges in principle from the sign systems of other animals.
It is also the area in which most work has occurred in computer systems for language processing.
This is not surprising.
Lexical processing may be adequately dealt with either by the use of wordlists or simple morphological processing.
Higher levels of language cannot be started on until the syntactic processing is complete, since the relationships that they are concerned with are denoted by structural properties of the input.
A much fuller review of many of the syntactic theories and applications is provided by Winograd (1983), which contains detailed descriptions of many syntactic theories and applications.
Details of the theories will be given briefly with attention to how successful they have proved within natural language systems.
2.2.2.
Practical Applications Of Linguistic Processing.
There have been a number of practical (large coverage) computational applications which have utilised linguistic information.
The most common of these applications are machine translation, information retrieval and human-computer interfaces.
2.2.2.1.
Machine Translation
Machine translation (MT) was one of the first computational applications of linguistic information (Weaver, 1949).
This research followed on from the success of code breaking techniques in World War 2 and the supposition that translation was simply a more complex coding of words.
Despite considerable funding the results of these ‘first generation’ MT systems were far from encouraging.
Frequently the results were so error-prone that it took more effort to correct the translation than it actually did to manually translate the text.
There were a number of reasons for the lack of success.
One must first consider the computers available at the time, which were roughly equivalent in power to an average modern programmable calculator.
Also the translation process amounted to little more than looking up words in bi-lingual dictionaries.
Obviously such methods are far from sufficient (an apocryphal example of the problems that arose involves the translation of the sentence ‘The spirit is willing but the flesh is weak’ into Russian and then back-translated to English as ‘The vodka is strong but the meat is rotten’).
Some syntactic knowledge was utilised in these early systems but it tended to be ad hoc and pre-dated current work on formal linguistic theory.
The 1966 ALPAC committee report on machine translation systems concluded that human translation was superior in terms of speed, accuracy and cost, and recommended that funding be curtailed.
Following on from this the amount of research carried out into MT was seriously reduced, although there has been some renewed interest (e.g. SYSTRAN (Toma, 1977) and EUROTRA (Raw et al, 1988)).
These second generation systems employ much more linguistic information, particularly semantic, than their predecessors.
A recent (and controversial) development within MT is the possible use of purely statistical information to achieve translation.
Initial work by Brown at IBM (Brown et al, 1989) has produced results of similar quality (48% correct translations) to those achieved by commercial MT systems.
Many of the near-miss translations in the IBM study result from the complete lack of linguistic information.
Brown suggests that large improvements may be achieved by incorporating simple morphological and/or syntactic analysis based on probabilistic methods.
It is doubtful that MT will ever achieve the performance once believed possible.
However there are currently available a number of MT systems which perform limited translation (of controlled syntax or vocabulary) or translation-related tasks (machine-aided translation).
2.2.2.2.
Information Retrieval
Another application to receive early attention aimed to produce the ‘library of the future’.
Two applications have been developed within this field — text retrieval and question-answering systems.
Text retrieval seeks to interpret what the user says and retrieve the appropriate document from the database.
Most text retrieval systems have been based on statistics and probabilities.
In question-answering systems the computer generates answers to the user's query based on stored information.
In order to achieve these aims it is necessary to store a knowledge representation of the contents of the documents.
Question answering systems have been a central project within natural language understanding.
Numerous projects are based on question-answer dialogues and the development of natural language front-ends for databases.
2.2.2.3.
Human-Machine Interfaces
Natural language is the most convenient method for communicating with interactive systems.
The development of a natural language interface to a database has proved to be more tractable than other applications.
The type of language used for such applications is typically very simple (Diaper, 1988).
Also, the interactive nature of such systems permits uncertainty about the input's meaning to be resolved by asking the user to rephrase the question.
Another aspect of human-machine interfaces is the conversion of text to speech.
This application is of benefit for queries to a computer that take place over a telephone and also for providing reading aids for the blind.
To produce natural sounding speech it is necessary to utilise linguistic knowledge to produce the fundamental frequency and duration of the produced signal.
2.2.2.4.
Speech Processing
Speech processing has been investigated at many levels — from the shape of speech waveforms to the meaning of whole sentences.
The most significant research effort into the application of linguistic information to speech processing arose from the ARPA initiative (Klatt, 1977) which resulted in a number of systems.
Each of the systems produced was based on diverse, co-operating knowledge sources to handle the uncertainty in the signal and processing.
The systems differed in the types of knowledge, interactions of knowledge, and control of the search.
The most famous of these systems were HEARSAY-II (Lesser et al, 1977) and HARPY (Lowerre, 1976).
Perhaps the most sustained research into speech recognition has been carried out by the IBM speech group which started work on the production of a large vocabulary continuous speech recognition system in 1972.
The group pioneered the use of Hidden Markov Model techniques and were able to demonstrate performances far better than HARPY (Bahl et al, 1983).
Among the systems that resulted from this work is TANGORA, a 20,000 word isolated-word dictation machine (Averbuch, 1987), and a 5,000 word continuous-speech recogniser (Bahl et al, 1989).
These systems are based on HMMs and trigram language models (c.f. $2.2.4.1).
Other recent successful speech systems include BYBLOS (Chow et al, 1987) and SPHINX (Lee, 1989).
BYBLOS used context-dependent phonetic HMMs to produce accurate speaker-dependent continuous-speech recognition of a 1,000 word vocabulary.
SPHINX used multiple code-books, generalised triphones and function-word HMMs to obtain high accuracy speaker-independent performance of the same task.
2.2.3.
Generative Linguistics
The dominant linguistic school over the last thirty years has been generative linguistics which has its foundation in Chomsky's revolutionary Syntactic Structures (Chomsky, 1957).
Generative grammar rejects the empirical nature of structural linguistics and instead uses linguistic intuitions of native speakers.
A structural approach is rejected on the basis that it is unable to capture the creativity of language.
Central to this approach is the distinction between linguistic performance and competence.
Linguistic competence is a person's knowledge of the rules of the language.
Linguistic performance is how a person uses those rules in real situations.
Chomsky believed that the linguists should be studying the competence of language rather than the performance which was studied by the structuralists.
His reasoning was that language is infinite and the study of performance is limited to a very small fraction of the possible sentences of the language.
Within the generative paradigm the following definitions hold: A language is a set of sentences, where each sentence is a string of one or more symbols (words) from the vocabulary of the language.
A grammar is a finite, formal specification of this set.
The grammar is able to take many forms.
If the language is finite then it is possible to simply list the valid sentences.
If the language is infinite then the language may be specified by producing a recogniser program.
The function of the recogniser program is to decide whether a given combination of words forms a sentence of the language.
A parser is an extension of a recogniser which assigns grammatical structure to the input.
The construction of a generative grammar represents an attempt to formulate a system of rules for the formation of the sentences of a language.
The aim is to produce a grammar that generates all of those sentences — and only those sentences — that a native speaker judges to be well-formed.
In practice however the complexity of language is such that no generative grammar has come anywhere near providing this degree of coverage of the language.
The rewrite rule is an effective method of representing the rules of a generative grammar.
An example of a simple rewrite rule is:
which can be interpreted as: A noun phrase, NP, consists of a determiner, det, followed by a noun, N.
The Chomsky Hierarchy
Although a complete generative grammar of English does not exist, one approach linguists have taken is to formulate different types of grammar and determine whether any particular grammar is in principle adequate.
Chomsky (1957) formulated three types of grammar — finite state grammar, phrase structure grammar and transformational grammar.
He was able to prove that finite state grammars are in principle incapable of generating certain sentences
Since the appearance of Chomsky's Syntactic Structures linguists have been deeply concerned with the form or type of grammar that, in principle, could adequately describe natural language.
The Chomsky hierarchy of languages (table 2.1) identifies four classes of grammar and is a crude dimension of the power of that grammar.
The least powerful grammar is type 3, the most powerful type 0.
A language is said to be recursively enumerable if a program could be written that would print out each sentence of the language.
A recursive language is one for which a program could be written that would either specify whether any given sentence belonged to the language or not.
Not every recursively enumerable language is recursive.
Only if the language is finite is it possible to state with certainty that any given sentence belongs to the language.
Context-sensitive grammars are phrase structure grammars of the form:
where the length of x is less than or equal to the length of y (i.e. there must not be more symbols on the LHS of the rule than the RHS).
e.g. 
This restriction ensures that the grammar is recursive.
Although they are more powerful grammars than context-free grammars, context-sensitive grammars have been found to be less well suited for stating grammatical constraints than indexed languages.
In a context-free language, every grammar rule is of the form:
where A is a non-terminal form (i.e. not a word) and x is a sequence of zero or more terminal and non-terminal symbols.
Below is a small context-free phrase structure grammar (CF-PSG).
The key features of a context free grammar are the use of a set of finite grammatical categories, and a finite set of rules for specifying how the LHS of the rule may be created from the sequence of RHS elements of the rule.
The term context-free indicates that there is no restriction on the context in which the rule occurs.
Context-free grammars are widely used for specifying both formal and natural languages.
Between context-free and context-sensitive grammars come the indexed grammars.
Indexed grammars are context-free grammars that are augmented by the ability to test for feature constraints.
This ability removes the restriction on context-free grammars that only a finite set of grammatical categories are allowed.
Indexed grammars have been usefully employed in ATNs (c.f. $2.2.3.1.3).
Finite state grammars (also called regular grammars) are the weakest grammar in the Chomsky hierarchy.
Finite state grammars are based on the view that sentences are generated via a series of choices made ‘from left to right’.
In other words once the first element has been selected every subsequent choice is determined by the element immediately preceding it.
Other than this we do not need to know anything about the portion of the sentence we have already generated.
Finite state grammars are conveniently expressed by a state diagram.
Fig 2.1 shows a very simple finite state diagram.
In the diagram, grammatical categories are shown by letters (d for determiner, a for adjective, n for noun).
This represents the language d(a)n where the brackets indicate that the category may be found zero or more times.
Hence the diagram represents a determiner followed by any number of adjectives followed by a noun.
Finite state grammars are unable to represent anything other than a very simple sequence.
An obvious difficulty is the specification that an element may be found one or more times.
The major failing with finite state grammars is their inability to deal with any dependencies that exist between non-adjacent words.
2.2.3.1.
Grammatical Framework
Perhaps the most significant decision to be made when developing an NLP system is the selection of the underlying theoretical framework.
There are a number of grammatical architectures that have been proposed and implemented during the last 30 years.
The following briefly reviews these systems.
2.2.3.1.1.
Finite State Grammars
Although finite state grammars are weak they have been used in computational systems.
One application that benefited from their use is given by Rabiner and Levinson (1985) for speech recognition of flight reservation information.
This grammar took advantage of the limited vocabulary (127 words) and constructions permitted in the domain.
Part of the grammar used is shown in fig 2.2.
The full grammar had 144 states, 450 transitions and 21 terminal states.
The language specified by this grammar had in excess of 6*109 sentences which were all syntactically and semantically well formed.
Significant improvements were found in the recognition performance of the recognition system.
Unfortunately, development of the system to cope with more general language is not feasible because the development of this grammar is inherently based on the sublanguage task.
2.2.3.1.2.
Transformational Grammar
One of the most dominant syntactic theories involves the use of transformational grammar (Chomsky, 1957)(Chomsky, 1965).
The underlying concept is that pairs of sentences that have constituents in common can be related to each other by a linguistic transformation.
For example the sentences ‘The dog chased the cat’ and ‘The cat was chased by the dog’are related by the passive transform.
Other transforms include the cleft transform which results in ‘It was the dog that chased the cat’ and the question transformation giving ‘Did the dog chase the cat?’.
Transformational grammars consist of a context-free (base) grammar and a set of transformational rules that map syntax trees onto new (derived) syntax trees.
The base component produces a deep structure tree.
The transformational component is a set of tree-rewrite rules which specify how to produce surface structure trees from the deep structure tree.
Transformational grammars are inherently used for sentence generation not analysis.
Grishman (1986) specifies a number of problems in reversing this procedure to use transformational grammars for the a recognition procedure:
1
Assigning to a given sentence a set of parse trees including all of the surface trees which would be assigned by the transformational grammar.
2
Given a tree not in the base, determining which sequence of transformations may have been applied to generate this tree.
3
Having decided on a transformation whose result may be the present tree, undoing this transformation.
Methods have been used to overcome these problems to some degree.
The first problem could be dealt with by the development of a context-free covering grammar — a set of all of the surface trees that can be generated by the transformational grammar for all of the words in the input sentence.
Many false trees will be generated from such a grammar, especially given the syntactic ambiguity of many English words.
A set of reverse transformations could be developed to deal with problems 2 and 3.
However, it is still very difficult to search for the sequence of transformations which produced any given tree.
Matthews (1962) suggested the use of synthesis (the generation of all possible sentences and checking to see whether they match the input) to apply transformational grammars.
Obviously this is computationally infeasible (and was never implemented).
More practical attempts at using transformational grammars for analysis incorporated an ‘augmented grammar’(e.g. MITRE (Zwicky, 1965) and TQA (Petrick, 1981)).
The augmented grammar contains the rules of the original grammar plus rules which characterise the structures that the transformations can add.
The input is then parsed into a tentative surface structure on which the transformations can be performed to determine the deep structure.
Given syntactic ambiguity and the recognition systems ambiguity of words this can be computationally prohibitive (e.g. MITRE takes 36 minutes to parse an 11 word sentence).
2.2.3.1.3.
Augmented Transition Networks (ATNs).
Following on from the impracticability of transformational grammar the introduction of ATN grammars (Woods, 1970) provided a practical application of linguistic theory.
For a full description of the ATN formalism see Bates (1978).
ATNs represent the grammar as a set of networks displaying the possible orderings of constituents in a grammar and the various options that the parser will have at any stage in the processing.
A successful ATN parse is obtained by traversing a path of arcs from the initial node to a success node using up all words in the sentence.
If this is achieved then a sentence is produced otherwise failure is announced.
Through the use of registers ATNs are able to enforce further restrictions on the input such as number agreement.
The resulting structure from an ATN is dependent on the grammar writer and can take many forms.
One method is to take the information stored in the registers at the end of the parse and build the resulting structure from this.
Fig 2.3 shows a portion of the ATN grammar used in the BBN speech understanding system (Woods et al, 1976).
This network defines acceptable noun phrases as consisting of the categories determiner, optional adjective string, noun and optional prepositional phrases.
ATNs have been widely used in natural language processing systems over the last two decades (e.g. LUNAR (Woods, 1973), PROGRAMMAR (Winograd, 1972), SOPHIE (Burton & Brown, 1979)).
The speech understanding system, SPEECHLIS (Bates, 1978), contained one of the largest ATN grammars (the grammar contained 448 states, 881 arcs, and 2280 actions).
This system was written for the purpose of understanding travel-expense reports.
The grammar took advantage of the limited domain by employing semantic categories and contained many domain specific word combinations.
Much of the use of ATNs stems from their clarity when used in small systems.
As a formalism ATNs give extensive possibilities for optimisation and tight control over the precise details of the parsing strategy.
The principal drawback with ATNs is the complexity that arises when a system becomes large.
The ATNs are bound up with inter-relations to other transitions making it difficult to determine the effect and purpose of any given transition.
As a system gets larger the logic becomes more obscure, modification more risky and debugging increasingly problematic.
Due to the increased complexity arising from multiple word candidates in the input and the requirement that the system provide a wide coverage of English the use of an ATN-based grammar was considered unsuitable for the current application.
2.2.3.1.4.
Marcus Parsers
Alternately referred to as Wait and See Parsers (WASPs)(Marcus, 1980) these parsers reject the backtracking control strategy used by ATNs to cope with local ambiguity.
Marcus's parser, PARSIFAL, was initially designed to mimic human linguistic ability.
The parser was shown to be incapable of processing garden-path sentences (i.e. sentences which cause incorrect syntactic premises to be made e.g. ‘the cotton clothing is made of grows in Mississippi’).
That humans also fail on these sentences was regarded as support that it reflected how humans process ambiguity although arguments exist against this inference (Briscoe, 1987).
Marcus parsers behave in a strictly deterministic manner; no backtracking is permitted and all of the structures created by the parsers are permanent.
Two data structures are used: an active node stack and a three-cell constituent buffer.
The active node stack is a pushdown stack of incomplete constituents.
The constituent buffer contains constituents which are complete but whose higher level grammatical function is uncertain.
This buffer is used to ‘look ahead’ at the data which will follow before making a decision about the parse to assign to the input.
An advantage of this type of parser is that it performs recognition in linear time.
The disadvantages of a Marcus parser for the current application include:
1
Extending the system to deal with multiple candidate words could be problematic.
2
The rules needed for this type of parser are difficult to develop for a large grammar.
3
Totally hypothesis driven i.e. It is unresponsive to the actual data.
4
Availability of a large-scale parser based on this approach.
2.2.3.1.5.
Unification Grammars
In recent years unification grammars have come to prominence in the computational linguistics community.
The key idea in the unification-based approaches is that informational structures (known as feature structures) are used to encode a variety of linguistic information (lexical, syntactic and semantic) in a uniform manner.
Manipulation of these structures is possible by means of a small number of well defined operations, the most important operation being unification.
The feature structures consist of features and associated values which may be either atomic or complex (i.e. feature structures themselves).
The operation of unification creates new feature structures and together with some string combining operations pairs the feature structures with strings (Joshi, 1987).
The appeal of unification grammars is that they are able to combine both linguistically theoretical and computationally tractable methods for processing language.
A number of grammars are unification based including Functional Unification Grammar (FUG)(Kay, 1985); Generalized Phrase Structure Grammar (GPSG)(Gazdar et al, 1985); Lexical Functional Grammar (LFG)(Kaplan & Bresnan, 1982) and PATR-II (Shieber, 1984).
Shieber (1986) lists the properties that unification-based grammatical formalisms share:
Surface-based: i.e. directly related to the word order in a sentence.
Informational: The strings have information associated with them from some informational domain
Inductive: The associations of strings and informational elements are defined recursively.
New pairings are derived by merging substrings based on prescribed string combining operations.
The associated information from the strings is merged according to prescribed information combining operations.
Declarative: The associations between strings and the informational domain are declared in terms of what associations are permitted, not how they are to be calculated.
Complex-feature-based: Associations between features and values are taken from a well-defined structured set.
A prototypical unification grammar (e.g. the Alvey Natural Language Tools $3.3) consists of a context-free skeleton which is supplemented by a set of feature-value specifications in the grammar and the associated lexicon.
The feature specifications may involve variables and may be recursive (that is the values may be interpreted as referring to a whole category).
Unification is the controlling factor that determines whether categories are able to combine to satisfy a grammar rule.
Consider the grammar rule:
This states that X may be formed from Y and Z combining.
X receives Y's value for feature foo, and Y receives the value that Z has for feature baz.
This percolation of values is governed by the unification.
The concept of unification is analogous to that found in set theory.
The unification of two categories is the smallest category that extends both of them, if such a category exists, otherwise the unification is undefined.
In this way the problem of feature agreement and percolation of feature-values between nodes in a phrase structure tree is dealt with.
Unification grammars are very strongly based on lexical lookup and employ a lexicon containing very detailed information about the possible uses for the words.
The use of complex lexical structures makes it possible to use simpler grammars.
Unification grammars take advantage of lexical orientation to cover:
i.
Agreement of subject and verb.
ii.
Subcategorization for noun phrases and verb phrases of all types.
iii.
Auxiliaries.
Unification-grammar formalisms have a number of advantages for natural language processing.
Among these advantages are their ability to represent partial information in an elegant way; the inherent potential for structure sharing; the declarative description of information flow, and a mathematically clean and computationally tractable type system with inheritance (Bouma et al, 1988).
2.2.3.2.1.
Representation of lexical information and lexical coverage.
The representation of syntactic information in the lexicon is inextricably linked with the grammar being used.
Unification-based grammatical formalisms tend to employ very detailed information within the lexicon.
The use of such lexical representations allows knowledge to be embedded within lexical representations, rather than rules about those representations.
Existing natural language systems take either the demo or the book approach (Miller, 1985).
Demo systems are generally developed by linguists to explore and explain linguistic phenomena which can later be increased in their coverage of the language.
Lexicons for such systems tend to be small and hand-crafted, reflecting the proprietary nature of lexicons and the difficulties in obtaining the required grammatical information.
Book systems tend to derive their lexical information from existing machine readable dictionaries (MRDs).
The information derived requires modification to create a computationally useful formulation of the information.
For a practical system it is essential to have a reasonable (though not necessarily complete) coverage of the words of the language.
The actual number of words that are necessary is open to debate.
Allen (1980) determined that 12000 morphemes are capable of covering 95% of English text, Jelinek (1985b) investigated the use of personal databases acquired by maintaining a dynamically varying vocabulary of the previous N words used.
5000 words covered 95.5% of text and could be derived from 56000 words of text, 15000 words covered 99.0% and required 640,000 words of text to determine this many individual words.
Of the established parsing systems, very few have substantial lexicons.
Exceptions to this are the Linguistic String Project (Sager, 1981) which had a lexical coverage of about 10,000 words (although many of these are specialist medical terms); the CLAWS system (Leech et al, 1983)(Atwell, 1983)(Atwell et al 1984) used to tag the LOB corpus had a lexicon of approximately 7,200 words plus a 670 SUFFIXLIST which added word-tags to most other words; IBM's CRITIQUE (formerly EPISTLE) system (Heidorn et al , 1982) with a lexicon of approximately 100,000 words and the Alvey Natural Language Tools (ANLT)(Carroll & Grover, 1989), an early version of which contained a lexicon of nearly 7,000 morphemes (probably about 20,000 words).
Recent versions having been further augmented.
2.2.4.
Corpus Linguistics
Bloomfield's publication of Language (Bloomfield, 1933) represented the first major statement to combine both the theory and practice of linguistic analysis.
Bloomfield's approach to linguistics was based on observation of the language.
Any data that could not be observed or directly measured were rejected.
Hence this school of thought held that the tool for the linguist to work with is a collected corpus of naturally occurring language.
This approach represents a behaviourist view of the language.
Structuralist linguists retaliate to the generative linguist's claim for competence over performance with the argument ‘An automatic language-processing system which works adequately for competent language but fails on performance is a futile system, because all there is to be processed is performance.’
(Sampson, 1987).
Such issues as whether a system is able to provide an adequate model of how humans process the language are of limited interest to this school of research.
Natural language corpora have been available in computerised form for over thirty years.
In the 1960's computers were rare and very expensive and the only text acquisition method for corpus compilers was manual input.
This placed restrictions on the size of the corpus that could be produced.
With the computer now established as an indispensable accessory in the workplace the volume of text available in a machine-readable format has become massive.
With this easy-availability of text the size of corpora becoming available are orders of magnitude larger than those originally envisioned.
The COBUILD corpus (Sinclair, 1987a) has about 20 million words and the American TEI corpus (Walker, 1989) and British National Corpus (BNC)(Leech, forthcoming) are expected to be in excess of 100 million words.
A by-product of this is that corpus-based linguistics, for a long time ignored in favour of logic-based linguistics, is becoming more accepted and attracting a lot more attention.
Corpus-based systems can be very simple.
A model of the language is created by training it on some text that is already available.
When new text is presented to the model this can be processed according to how closely it matches what has gone before.
The statistical approach reduces the problem facing the syntax analyser from a logic-based task to one of pattern recognition.
2.2.4.1.
Hidden Markov Models
A HMM is a collection of states that are connected by transitions.
Each transition has two probabilities: a transition probability which specifies the likelihood of the transition between the states and an output probability density function (pdf) which defines the probability of emitting a symbol from a finite set given that a specific transition has been taken.
One important application for Markov models has been in automatic speech recognition (ASR) in which a speech signal is modelled as a probabilistic function of a (hidden) Markov chain.
Further developments within the speech paradigm have extended the use of Markov models from the acoustic level to the linguistic (Jelinek et al, 1983)(Brown et al , 1983).
Jelinek took individual words as the states in his Markov model for speech recognition.
There were 5,000 different states in his model, corresponding to the number of words in the lexicon.
For a corpus of 2 million words the frequency of adjacent occurrence of groups of 2 and 3 words was determined and stored in a transition matrix.
With 5,000 different states the size of the bigram and trigram matrices are enormous (2.5e07 and 1.25e11 elements respectively).
It is inconceivable that any corpus should provide occurrences of all of these possibilities.
Since Jelinek's corpus was about 2 million words it was impossible.
To compensate for the absence of transitions Jelinek used backing off formulae to account for missing transitions.
Hence if a trigram was not present in the corpus then a probability was generated for it based on the bigrams, and if the bigram was absent the probability was based on the unigram.
Normalising factors were employed to ensure that the probabilities always added up to 1.
A probability was assigned to any word string that could be formed from the spoken input.
The sentence with the highest score is then selected.
A failing of systems such as Jelinek's is that they are heavily dependent on the size of lexicon used.
A large lexicon system demands a huge corpus to obtain a satisfactory model and a large amount of storage for the resultant transition matrices.
An alternative technique is to use syntactic categories rather than individual words as the states in the model.
Such an approach has distinct advantages.
By using syntactic categories there is no limit on the size of the lexicon used.
Since there are substantially fewer syntactic categories than words in the language, the number of states in the model is reduced.
As a result a good model of the system may be obtained from a much smaller corpus of data.
Deroualt and Merialdo (1984) used such a system to perform stenotypic transcription of unconstrained French with a vocabulary of 250,000 words.
Stenotopy is a means of recording shorthand by machine and is used mainly for conference recording.
The phonetic information is manually input using a special keyboard having 21 keys.
There are two problems associated with converting the stenographed record to typescript.
The first is that there is no end of word mark in stenotography, hence all of the phonemes run together.
The second problem is the large number of homonyms (sound-alike words) in French.
The use of syntactic information led to a 30% reduction in the number of words that were incorrectly transcribed.
Another important use of statistical grammatical information has been for the grammatical tagging of natural language corpora.
Grammatically tagged corpora gives valuable information to researchers investigating the syntactic nature of language.
In English many of the words may belong to more than one grammatical category and it is the function of a tagger to select the correct tags for the words.
The first large computer corpus to be tagged was the Brown corpus of American English.
This corpus was tagged using TAGGIT (Greene & Rubin, 1971), a rule-based lexical disambiguation program.
The variety of language found in the corpus was such that TAGGIT met with only limited success.
Of the text that was encountered about 78% was successfully disambiguated automatically, the remaining 22% was manually disambiguated.
This was still a sizeable amount of text to process manually.
When faced with the same problem the compilers of the LOB corpus opted for a statistical approach.
The resultant CLAWS program (Leech et al 1983)(Atwell, 1983)(Atwell et al 1984)(Garside, 1987) proved to be very simple and successful, achieving 96-97% correct tagging of all words in the corpus (90% correct of ambiguous words).
Although many words in English are syntactically ambiguous, much of this ambiguity arises from rare usage.
If one simply uses a dictionary to determine the grammatical category of a word then there is little indication of which are the more likely syntactic categories.
The CLAWS team incorporated information about the likelihood of a word belonging a grammatical category into their system in a very gross way — some of the available tags were labelled as being unlikely.
Church (1988) improved the stochastic approach to corpus tagging by including statistics based on ‘lexical probabilities’.
The lexical probability that Church refers to is the likelihood that a word will behave in a particular syntactic manner.
Church's approach to the problem derived information about the lexical probability of words from existing tagged corpora and dictionaries.
The dictionary provides comprehensive coverage of the categories for a word, the corpus supplements this with information about the actual usage of the word.
There have also been attempts to produce parse trees for input using probabilistic techniques.
Garside and Leech (1987) developed a system that parsed the tagged output from the CLAWS system.
There are three stages in producing the parse.
1
A set of possible parse continuations are assigned to each word in the text from a lexicon which lists the parse continuations from each pair of tags.
2
A search is made for a number of special tag patterns and parse fragments and the parse structure suitably modified.
3
Each possible parse is completed, a score associated with it and the highest scoring parse selected.
This task is more difficult than the tagging problem and the results have been, understandably, less impressive (approximately 50% success rate for producing an acceptable parse).
Garside believes that the system can be significantly improved to produce a robust, economic parsing scheme able to operate accurately over unconstrained English text.
Church augmented his tagging program to locate noun phrases.
The training data was obtained by (semi-automatic) parsing of a 40,000 word section of the Brown corpus.
The probability of each part of speech starting and ending a noun phrase was then determined from this data.
At run time, given a tagged sequence of words the parser uses the training data to assess how likely a combination of adjacent tags were to form a noun phrase.
Impressive results were obtained, with only 5 out of 243 noun phrase brackets being omitted.
Development of this method to cope with other types of phrases would require a larger training set that included these phrase types.
This in turn would require a corpus larger than the Brown to provide sufficient examples and considerable effort to obtain the parses.
The APRIL parser (Haigh et al, 1988)(Sampson et al , 1989a) was based on statistical techniques.
The parser was trained by taking a database of manually parsed sentences and extracting statistics that refer to the likelihood of a non-terminal parse being obtained from a set of constituents (e.g. what are the possible set of constituents that can form an adjectival phrase and how likely are they to do so).
Rather than attempt to determine every possible parse and associate a goodness score to each parse, APRIL employs simulated annealing techniques.
This approach requires that a possible parse is randomly chosen and assigned a score.
Local changes are then progressively made to the parse tree and the resultant parses evaluated.
The system made no attempt to determine deep structure parses, limiting itself to finding surface parse trees.
Although initial results with APRIL were promising the system encountered some development problems.
Within the machine translation community there has been the introduction of purely statistical techniques to translate between French and English (Brown et al, 1989).
The corpus used for this was the three million sentence bilingual proceedings of the Canadian Hansard (parliamentary proceedings).
The probability that one word in a sentence in one language corresponds to 0, 1 or 2 words in the translation is calculated.
A glossary of word equivalences is obtained which lists a set of possible translations for each word with a corresponding probability (e.g. the translates as le with a probability of 0.610 and la with a probability of 0.178, etc.).
When translating, the probabilities are combined in various ways and the highest scoring combination selected.
An algorithm is then applied to put the words in the correct order.
Despite contravening almost all the rules that have been thought necessary for providing machine translation the system manages to achieve correct translation in 48% of cases.
The developers also believed that many of the near-miss cases could also be solved by using some simple morphological and/or syntactic analysis.
Statistical methods have been used to automatically deduce the linguistic categories of words from raw data (Atwell, 1987)(Atwell & Drakos, 1987)(Atwell & Elliot, 1987)(Finch & Carter, 1992).
The method used is based on collecting and clustering bigram statistics using a rank correlation metric.
A large (33 million word) corpus was taken and sets of ten nearest neighbours of words derived for the thousand most common words in the corpus.
Words were then categorised based on the similarities of these groups.
Although the technique is very simple it is sufficient to differentiate between verbs, adverbs, adjectives, prepositions, determiners, and between singular and plural nouns.
It is also able to discover some semantic groupings such as numbers, compass directions and animate objects.
What is impressive is that these distinctions can be made without any prior knowledge of the syntactic and semantic categories of the words involved.
2.2.5.
Hybrid Systems
The boundaries between rule-based and probabilistic approaches to language processing are gradually being eroded.
A number of hybrid systems that incorporate the methods from both camps are beginning to appear.
One hybrid approach attaches probabilities to the rules in the generative grammar (Wright, 1992)(Pocock & Atwell, 1993).
The grammatical analysis is performed using a generative approach but each derivation is assigned a probability based on the probabilities of the rules used in the analysis.
The highest scoring final derivation is then selected as the parse.
The probability score can also improve the efficiency of the parsing algorithm by pruning out low-probability alternatives.
A similar approach is taken with the grammar used by the METAL machine translation system.
The grammar used by METAL is rule-based but the rules each have an associated weight (Caeyers & Adriaens, 1990).
To improve the efficiency of the analysis each rule in the grammar is ordered based on its frequency of use.
This ordering ensures that standard phenomena are covered first, exceptions and non-grammatical structures later on.
Lower level rules are only used if the standard grammar is unable to cope with the input.
A hybrid system was used in the ‘best first’ parser developed by Paxton and Robinson (1973) for the SRI speech understanding system.
This system combined the probability scores of the words that were produced by the recogniser and a score dependent on how grammatically well-formed the sentence was deigned to be.
A number of minor grammatical violations were allowed for the input, with each violation the score representing the ‘grammatical quality’ of the phrase was reduced.
Deroualt and Merialdo (1986) combined their Markov model with a probabilistic grammar for French.
This enabled them to handle global constraints on the sentence.
The grammar consisted of 200 context-free rules with each rule having an associated probability.
The grammar did not offer total coverage of the language (it successfully parsed 65% of the corpus it was tested on) but was intended as a supplement to the results from the Markov analysis of the stenotography.
If a successful parse was obtained then its probability was calculated by determining the product of the probability of each rule used to achieve the derivation.
(Unfortunately no details are provided about how the probabilities of the rules were determined.)
For word sequences having no complete parse then the probabilities of any partial phrases that had been generated were used.
Their tests evaluated the rule-based grammar and Markov model separately and then in combination.
The best results were found from the combination of the two processes, the worst when just the rule-based method was used.
2.2.6.
Combining syntax and semantics
Although the task for the current system is recognition rather than comprehension, the use of semantic information is still important since it can provide a further level of constraint for selection of the correct word from the candidate words.
As with syntax there are two ways of approaching semantic processing.
The artificial intelligence community aim at producing computer programs that can ‘understand’ the input.
To achieve such a goal it is necessary to perform detailed and comprehensive semantic analysis of the input (e.g. (Schank, 1972)).
Such systems are capable of dealing with only very restricted language domains.
For certain tasks (e.g. recognition or stylistic processing) it suffices to use much simpler techniques.
The most common method used is concordance analysis.
More recently use has been made of machine readable dictionaries and thesauri to obtain semantically-based groupings of words (e.g. (Alshawi, 1987),(Amsler, 1981)).
Humans perform both syntactic and semantic processing when reading.
The actual manner in which these processes occur and/or co-operate is the subject of much debate within the psycho-linguistic community.
A central issue is whether syntactic and semantic information contribute independently or interact in the comprehension process.
The first approach is to perform syntactic analysis first then have a second pass convert the syntactic tree to a semantic representation.
This autonomous approach concurs with the language model Forster (1979) proposed for human understanding.
This model allowed only semantic analysis, output from the syntactic analysis and feedback to the lexical and syntactic analysis was forbidden.
A second approach combines syntactic and semantic processing.
Woods et al(1976) suggest the use of ATNs for semantic analysis.
In such systems the arcs and nodes of an ATN are labelled with both syntactic and semantic categories.
This method more closely agrees with the language model suggested by Marslen-Wilson (1975).
This model incorporates all different categories of information which interact in an ongoing manner to constrain the processing of a sentence.
From a computational perspective there are advantages for both approaches.
The autonomous method permits the program to be written as separate modules, which both simplifies the program and allows parallel processing.
The combined approach requires a complex control structure to integrate the individual knowledge sources.
For a generative grammar, the concurrent application of semantic processing can reject unlikely partial structures, thereby reducing the amount of processing the parser has to perform.
However an efficient, practical approach to semantics that is able to cover general language is not yet in existence.
Until such a system has been produced the question of how such systems should integrate with syntactic processing remains very much within the area of theoretical linguistics.
2.3.
Summary
To date developers of handwriting recognisers have tended to concentrate on improving their recognition systems by concentrating on the pattern recognition level of the process.
The use of linguistic information in these systems has been extremely limited — at best these systems perform lexical checking to determine whether the input forms a valid word.
Unfortunately handwriting is often too poorly formed for such techniques to work reliably.
A number of computational applications do use linguistic information however.
Most importantly the use of syntactic information has a long history in computational linguistics.
Its use has been recognised to be of value in applications such as machine translation and speech recognition.
Syntax is also important in other fields such as theoretical linguistics and psycholinguistics.
Because so many different people are working in the field of syntax there are a large number of different approaches taken.
Severe contention exists about whether a rule-based or statistical approach should be used to apply syntactic constraints.
There are strong arguments for both cases and the arguments between these two approaches are unlikely to be resolved in the near future.
Some workers have recently attempted to combine the statistical and the rule-based approaches in hybrid systems with promising results.
This issue is considered in detail in the discussion.
Chapter 3
Investigations with a Generative Grammar
3.1.
Introduction
The character recognition level of a text recognition system proposes a number of possible character alternatives for any input.
The lexical processor then determines which of these are able to form words.
It is then necessary to make a choice between the words.
Information about syntax can be used to select words which form grammatically allowable strings; or to reject strings which are grammatically unacceptable.
One approach to this problem is to develop a parser that selects the words that can combine to form valid sentences.
Generative linguistics has been the dominant approach to syntactic processing of language since the late 1950's.
There have been a large number of syntactic theories proposed and an even larger number of parsing systems developed.
The majority of systems that have been developed have been for restricted domains and small grammars.
The construction of a large grammar represents a major problem in it's own right which has only been attempted by well-funded projects.
Without such funding the best way to process general language is to acquire rather than develop a parser (Patten, 1992).
Existing parsers encapsulate the linguistic intuitions of their developers.
Hence it was decided to select an available parser and modify it to cope with the recognition task.
A large number of parsers exist.
This chapter reviews the considerations that must be taken into account when selecting a parser for a specific task.
The parser selected is the Alvey Natural Language Tools (ANLT).
The ANLT system is described and the necessary modifications are made to it to perform the recognition task.
Results obtained in testing its ability to deal with the recognition problem.
3.2.
Selection of a Parser
There are a number of selection criteria that should be taken into consideration when selecting a parser.
The main considerations are:
The underlying grammatical formalism
How large a fragment of the language the system can deal with
The parsing algorithm
Practical considerations
3.2.1.
Grammatical Framework
Perhaps the most important selection to make is the type of grammar on which the parser is to be based.
One must decide whether to use a unification-based approach, an ATN system, a Marcus parser or a transformational grammar etc.
The most important grammatical formalisms were discussed in the previous chapter.
Of the available types the unification-based grammars appear to be best suited to dealing with a large coverage of English.
There are a number of reasons for this.
Firstly they enable large systems to be produced yet still retain clarity within the grammar.
They also have the well-defined approach to semantic processing and have led to theoretically clean and fairly efficient computational representations.
3.2.2.
Language Coverage
Parsers frequently fail to parse a sentence, either because the words in the sentence are not stored in the lexicon or because the grammar available to the program is unable to handle a particular grammatical construction.
It is axiomatic that ‘all grammars leak’(Sapir, 1921).
Such is the variety of expression that people use that no definitive grammar of English exists.
Large authoritative grammars have been written (e.g. Quirk, Greenbaum, Leech & Svartvik, 1985) but not incorporated into computational systems.
Even this grammar is not able to deal with all sentences, requiring constant improvement and amendments to be made.
Since the possibility of full grammatical coverage is not feasible many systems have been created that are targeted to a specific application domain.
Such systems are able to take advantage of the limited constructions that are found for a specific task and the reduced vocabulary size.
By accepting such a limitation in application domain it is possible to achieve much more detailed analysis of the text.
The emphasis in such systems has been to concentrate on the representation, organisation and use of linguistic knowledge as encapsulated and expressed by linguistic rules and procedures.
The lexicons have been regarded as being of only secondary importance or their production considered to be trivial.
The prototypical systems tend to deal with only a small number of words, with the lexicons being created in an ad hoc manner.
Furthermore the structures of the lexical entries have tended to be proprietary to the system making it impossible to exchange lexicons between systems.
Early examples of this approach include SHRDLU (Winograd, 1972) and LUNAR (Woods, 1973).
SHRDLU was able to understand natural language commands on how to move blocks.
LUNAR was a natural language front end for a data base containing moon rock sample analyses.
Both of these systems were able to understand relatively unconstrained language for these very narrow domains.
Both were also found to be non-portable and non-extensible to other applications.
Hence, when selecting a parser it is vital to determine how wide a variety of grammar the parser is able to process and how easily it can be extended to a larger domain.
Since ‘real world’ applications require the lexicon to contain entries for tens of thousands of words it is also necessary to consider how large the available lexicon is, the format of the entries and how much effort would be required to create new entries.
3.2.3.
Parsing Algorithm
A grammar is simply a declarative statement of what forms a valid sentence.
To determine the derivation of a sentence it is necessary to use a parsing algorithm.
The parsing algorithm specifies the manner in which lexical information is to be retrieved, which grammar rules are to be applied to the input and how the information obtained is to be stored.
A variety of parsing algorithms are available and incorrect selection can result in much greater processing requirements.
Top down and bottom-up parsing
The first consideration to be made with the parser is the manner in which the grammar rules are applied.
Early parsing algorithms worked in either a bottom-up or a top-down manner.
Bottom-up (data-driven) parsers work from rules whose left-hand side matches the grammatical categories of the input.
That is, given a sentence to parse the parser first determines the grammatical categories of the words in the sentence.
The parser then checks these categories against the right hand sides of the ruleset.
When the right hand side of a rule is found the symbols involved are replaced by the symbol on the left hand side of the rule.
This process is repeated until the sentence symbol, S, is reached or the parse fails because no other rewrites are possible.
Top-down (concept-driven) parsers work in the opposite way.
These parsers build the parse tree by starting with a rule whose right-hand side is a sentence, S. The rules are progressively expanded down to the rules having the lexical categories that match the input.
There are problems with both bottom-up and top-down parsers.
Bottom-up parsers are very susceptible to problems arising from lexical ambiguity.
The major problem with top-down parsing is the time wasted in expanding rules that cannot possibly be satisfied by the input.
Depth-first and Breadth-first search.
Parsing may be regarded as a search for the syntactic representation of the input.
There are many alternative paths available to follow and many of these paths will turn out to be dead ends.
It is necessary to give the parser instructions on how to search for the correct parse.
One possible solution is to use a depth-first approach.
This involves going as far down a single path as possible.
Should the path fail then the system backtracks to the previous decision point and takes a different path.
This method has been used in such systems as ROBOT (Harris, 1977).
The efficiency of this system was improved by ordering the arcs of the ATN grammar.
This led the parser to explore the more likely solutions first.
Another option is to use a breadth-first approach.
Instead of trying out just one hypothesis at a time, a breadth-first approach retains a number of possible hypotheses.
Each hypothesis is in turn extended by a single step.
As the parse progresses down the input the incorrect hypotheses should fail; thereby reducing the search space to be explored.
For the purposes of the current project a bottom-up approach is an obvious one to use.
Since more than one successful path through the grammar may be necessary (more than one syntactic combination may be possible from the candidate words) a parallel process would also appear to be a logical choice.
Some method is required to organise the partial structures produced so that a more efficient parse is obtained.
A chart provides such a structure.
The simplest form of charts (Kay, 1986) stores all of the hypotheses that have been tried out on the input.
Use of a chart prevents repetition of work that has already been carried out by blocking any attempt to reapply the same rule in the same place.
Practical Considerations
There are a number of practical considerations that must be taken into account in the selection of a parser.
Firstly there is the question of efficiency which may depend on the parsing algorithm, the machine used and the computer language that the system is written in.
Certain systems are written to exploit specific features of a particular machine.
Requirements may exist about the amount of memory that is required to run a system.
It is also likely that some tailoring of the system will be necessary for a different application.
The grammar and lexicon may be incomplete for the task at hand and may need to be supplemented.
It is important to determine how easily such additions can be made.
More substantial modifications may be necessary to the system for a particular task.
Practical natural language systems are by necessity large and complex and modification of such systems can lead to unexpected results.
If the system has been written in a modular way (i.e. the individual components of a system are intelligently partitioned, allowing each module to see all and only the input it requires) then the modification is greatly simplified.
Early systems were not written in this way.
For example the early machine translation systems tended to contain the grammar intrinsically within the program.
Modification of such systems required a great deal of effort.
3.3.
The Alvey Natural Language Tools
Of the available parsers the one which best satisfied the above criteria was the Alvey Natural Language Tools (ANLT) system.
The ANLT is a powerful piece of software designed for use in natural language processing applications.
A large lexicon is provided which is compatible with a large grammar based on the Generalized Phrase Structure Grammar (GPSG) formalism (over a 1000 rules are in the object grammar).
GPSG is computationally economical and has been developed to a point where a large number of interesting syntactic phenomena may be dealt with in an integrated formal framework.
The parsing algorithm provided is based on chart-parsing techniques.
One of the aims of the ANLT is to provide a general purpose system which may be easily adapted for new tasks.
With this in mind a Grammar Development Environment is provided as an aid to the development of a natural language grammar.
It is useful for bringing together the separate parts of the ANLT and the grammar and lexicon can be developed and altered within this environment.
A description of the major components of the system is necessary in order to determine their applicability to a recognition task and what modifications are necessary.
3.3.1.
Morphological Analysis and Dictionary System
The morphological analysis and dictionary system of the ANLT (Ritchie et al, 1987a) have been developed to provide a large coverage, morphological lexicon.
Morphology is the mechanism by which morphemes combine to form words.
The use of a morphologically-based lexicon can lead to a large reduction in the storage requirements for the lexical information.
The use of morphological methods ensures that it is necessary only to store the lexical information for the root forms of words and a small set of affixes (-er, -est, -ing, pre- etc.).
Some method is then used to derive the syntactic representation for the derivative form from combining the root form and the affix.
Given the detailed lexical information used in the ANLT a morphological processor leads to a large reduction in storage space required.
(Furthermore with the LISP system available to the current project it was not possible to store a very large lexicon — necessitating morphological processing to reduce storage.
This will be dealt with later in this chapter.)
The ANLT lexicon aims to provide consistency (i.e. syntactic information is only explicitly provided for words that cannot be derived from extant morphemes within the lexicon).
The lexical entries are of a form compatible with a GPSG grammar (Appendix 1).
This compatibility requires that the syntax information is defined in terms of feature-value pairs.
e.g. ((N +)(V +)(BAR 0)(INFL -)(PLU -))
indicating that the morpheme is a singular noun which cannot be further inflected.
Accessing the syntactic information for a word in a morphologically based lexicon requires the following steps:
1
Splitting the word into its constituent morphemes.
Given a string it is necessary to determine what morphemes the string consists of (e.g. the string boys comprises the morphemes boy and +s).
This process is trivial for a native speaker of a language, however for a computer system the solution is more difficult.
The two major problems involved are related to the spelling changes that can occur when morphemes combine and the multiple segmentations that are possible for many words.
Spelling changes may occur at the morpheme boundaries resulting in slight changes to the surface string produced by the morpheme combinations.
Table 3.1 shows some examples of the regular morphological effects that occur in English.
The morphemes may simply be concatenated (1), letters may be added between morphemes (2 and 3), deleted (4) or changed (5).
It is interesting to note the similarity between the morphemes in (1) and (2) but the differences in the words produced.
Spelling rules exist to specify when particular spelling changes can operate.
The ANLT system represents these in a set of Koskenniemi rules (Koskenniemi, 1983).
It should be noted that there are a number of morphologically irregular words in English which will require explicit statement in the lexicon (e.g. children instead of childs, caught instead of catched).
Determining the constituent morphemes is further complicated because there may be more than one possible segmentation for a word.
Consider the word preached.
Humans have no problem in determining that this is the past tense of the verb preach.
Fig 3.1 shows some possible segmentations for the word preached that a computer could posit.
From the diagram, the possible segmentations are pre-ache-ed (where ache may be nominal or verbal) or preach-ed.
2
Accessing the syntactic information for the constituent morphemes.
The syntactic information for the morphemes (e.g. boy and +s) must then be accessed.
Large lexicons are generally unable to reside in main-memory and the required information must be retrieved from disk.
3
Combination of the syntax information for the morphemes to generate the syntactic information for the complete word.
Having obtained the syntactic information for the morphemic constituents of the word, this information is combined to determine the syntactic information for the complete word.
The combination of the morphemes requires a ‘word grammar’ to determine the legality of the combinations and the resultant representation.
A word-grammar is used to make decisions of the form:
The word grammar used in the ANLT is based on Gazdar's GPSG model of syntactic features and is similar in structure to the sentence grammar.
Instead of assessing the feasibility of words combining to form a sentence and providing a syntactic representation for the result, a word grammar determines the feasibility of morphemes combining to create a word.
The actual grammar rules and certain of the features employed are different (e.g. sentential grammars have no feature to indicate whether a word can be further inflected) but the underlying principles and mechanisms are similar.
The syntactic information for boy and +s below could be combined by a word grammar to produce the syntactic information for boys: e.g. 
3.3.1.1.
The Lexicon
The lexicon provided with the second version of the ANLT contains over 62,000 entries.
Not all of these entries are unique however.
There are approximately 33,000 different morphemes.
The term ‘morpheme’ is not used in the strict linguistic sense.
In this case any word, or even phrase is classed as a morpheme.
This allows nominal compounds, such as ice cream, to be treated as morphemes, and allocated the corresponding syntactic information.
Approximately 4,500 of the entries are compounds.
The lexical entries of the ANLT are presented as 5-tuple entries:
Although there is obvious redundancy within the lexical entries (the phonological form and semantics field are simply repetitions of the morpheme) this reflects the aim of the ANLT project to produce a general-purpose, flexible system.
Words possessing more than one meaning or usage must be allocated multiple entries within the lexicon.
3.3.1.2.
Problems
The morphological system provided with the ANLT is one of the most complex morphological systems ever created.
This complexity permits the compaction of the data that is stored and is ideal for research into the morphology of a language.
The authors of the system have stressed (Ritchie et al, 1987b) that the system is prototypical and our experience with using the system for a practical application has encountered some problems:
The major problems with such a morphological system are
1
The computational effort required to determine the syntactic information is excessive (see below).
2
Although morpheme combinations are syntactically valid they may be semantically invalid.
For example the system allows the existence of the words such as overbelieve and interbelieve
3
Unexpected syntactic categories may be returned for a string.
The morpheme liver is not explicitly stored in the first lexicon distributed with the ANLT.
The system returns the syntactic information for the morpheme combinations live +er which although possible, represent infrequent usage of the word liver.
Any rule-based system of morphology is susceptible to these errors.
The major problems for the recognition application are the multitude of possible syntactic representations postulated by the look-up and the computation required.
Recent investigations into lexical ambiguity (Church, 1988) have shown that although words are capable of existing in many syntactic categories, in reality the problem is not so widespread and words actually possess unique ‘best’ parts of speech.
For example, the word ball may be either a noun, a transitive verb or an intransitive verb but for 99% of cases may be treated simply as a noun.
It would be beneficial to know the ‘best’ grammatical category for any particular word when attempting to determine the probable parse to assign to a sequence of words.
This is made more difficult by the use of a morpheme-based lexicon because many parts of speech may be derived from each morpheme.
Many aspects of the system are computationally intensive.
The morphological system can require a long time to obtain the syntactic information for a word.
The user documentation quotes that on a SUN 3/160 machine running Kyoto Common Lisp (KCL), the time required to look up a large word (excluding garbage collection) is 11.8 seconds of CPU.
Our experience has shown that on average obtaining the syntactic information requires about 20 seconds of run time per word (including garbage collection).
For morphologically complex words this can be much worse, for example the word unbelievable on the same system (including garbage collection) required over 200 seconds of run time.
Further problems were encountered with the lexicon supplied with the second version.
The lexicon was much more complete, being derived from the LDOCE dictionary (for details see Carroll and Grover (1989)).
The lexicon supplied with the first ANLT contained 6,800 morphemes and suffered from omissions.
The larger lexicon supplied with the second version remedies this problem but is too large to load into the KCL LISP environment.
An alternative morphological system had to be devised in order to test out the system (see below).
Another problem arises from the nature of the text recognition system.
Before accessing the information for a word, it is necessary to first know what word to obtain information for.
In a recognition system this is non-trivial, requiring a ‘word-lookup’ stage to determine which valid character combinations exist within a character lattice.
(Wells et al, 1990) Two options are available — one can select the character combinations based on transitional probabilities between characters or one can specify a vocabulary list and check the character combinations within that list.
Although the former method requires little storage it has been found that the benefits from a fixed wordlist far outweigh a statistical approach when determining possible words.
Prior to the use of the ANLT morphological system in the current system it is necessary to first check the existence of a word.
This means that every word that the system can process has to be known beforehand, otherwise it will not be accepted.
One of the benefits of the ANLT's morphological system is that it does not need to explicitly store all of the words it knows.
The ANLT is able to generate the syntactic information for any word whose morphemic roots are in the lexicon.
Hence the number of words ‘known’ to the ANLT is unbounded.
The imposition of the current system's wordlist on the ANLT system nullifies this advantage.
3.3.1.3.
Alternative Morphological Systems
The current system (Keenan and Evett, 1989) requires a significantly simpler morphological system which is able to take the character lattice output from the recognition stage and return the words that may be formed from the lattice and the grammatical information for those words.
Some work has been carried out towards making this possible.
As has been previously stated one of the major problems with morphology is determination of the word boundaries in a string.
Such approaches, though simple, are error prone (e.g. mother → moth + er etc.).
The ANLT system uses a set of rules to segment the string into possible morphemes and then determines the validity of the segmentations using a word grammar.
In the past simple morphological systems have used simple affix-stripping routines to determine the morphemes present in a word.
A typical algorithm for accessing a dictionary using an affix stripping routine is as follows (Holmes, 1988),(Ramsay & Barrett, 1987).
Strip off the longest set of letters from the end of the word which matches a (usually small) set of standard suffixes.
Then look up the root word in the lexicon.
If it exists then the root word has been found, otherwise look for the next longest suffix present.
Adjustments of roots (e.g., re-inserting final e) is performed.
Irregular words and those not dealt with by the set of suffices are marked and have their own entries.
These decomposed words are then used to access syntax.
A small set of rules are used to derive the syntactic information for the word.
At least two problems exist for using affix stripping as a morphological technique.
The first problem is that it is easy to determine the incorrect stem of a word (e.g. mother → moth + er).
The other problem is the amount of lexical checking that is required.
Each time an affix is removed the remaining stem must be checked to determine whether it is a valid word.
This requires constant checking which is trivial for a ‘demo’ system but for a system with a realistic vocabulary it can be inefficient.
A new method has been developed for the current project which attempts to provide a much simplified morphological system with little increase in the storage requirements and remove the errors permitted by the affix-stripping methods.
The fundamental idea is to determine the identity of a word's morphological root during system development.
This root is then stored and may be accessed in the lexicon at run-time rather than determined on the fly.
The grammatical information for the lemmata is then accessed.
This information is the ANLT lexical entry, or entries, for the word.
A sub-lexicon is produced for the words in the text under analysis.
This is combined with the lexicon of bound morphemes supplied with the ANLT.
There are a number of advantages from taking this approach.
Firstly the complete lexicon supplied with the ANLT is too large to load into the KCL.
Only by including this process is it possible to access every word in the lexicon.
This method also prevents selection of the incorrect morphemic roots of the word (preached is found as a derivative of preach rather than ache) since the selection of the correct root form can be made with certainty at development time with the aid of dictionaries.
The imposition of a wordlist on the system forbids the selection of semantically invalid morpheme combinations (e.g. underbelieve).
There is still the development time task of finding the correct root for a lexicon of approximately 70,000 words.
The first step towards achieving this task is to use the information present in the machine readable dictionaries.
A large number of the words are simple inflections which were solved using the information in the Text710 version (Mitton, 1986) of the OALDCE.
One form of this dictionary contains just the root-forms of words and a set of codes which indicates the manner in which a word may inflect.
For example there are codes for nouns which form their plural like the word pony (i.e. replace the final y with ies).
The inflections of each word are generated and the root linked to the inflection.
Only a limited set of inflections are used in the Text710.
The Collins English Dictionary (CED) lists other derivatives that can be formed from a headword.
This accounted for many derivations that are not simply inflections (e.g. happy, happiness, happily).
The use of dictionary information greatly reduced the task but it did not complete it.
A number of words contained affixes that the dictionary ignored (e.g. un- and dis-).
A program was written that searched for such affixed words and prompted a human with the word and the probable root form which could be selected or rejected.
Although a tedious task it ensured that the correct root forms was found for words.
3.3.2.
The Grammar
The ANLT provides a unification-based formalism, derived from GPSG, that covers the major syntactic constructs of English.
GPSG (see Appendix 1) is a grammatical framework which extends the expressive power of context-free grammars enabling complex systems of regularities and restrictions to be stated easily.
The GPSG framework employs meta-grammatical rules to generate new rules from existing rules.
This allows rules with a wide variety of realisations to be specified with only a small number of explicit statements.
There are a number of problems with the system however.
The main ones are the number of parses that are produced and the limited extent of the grammatical coverage.
Many of the words in the lexicon have a high degree of syntactic ambiguity.
Much of this ambiguity is as a result of the feature-based grammar.
(The most ambiguous word in the lexicon is put which has 638 different lexical entries.
One cause for this ambiguity is the specification of the particles that put can occur with to form phrasal verbs.
Each different particle that put is able to combine with must have a separate entry.)
As a result of the syntactic ambiguity and the number of grammar rules that can be satisfied, multiple parses are possible.
In the case of ANLT however the number of parses suggested can be excessive (e.g. ‘I enter orders until the markets close’ produced 4,848 different parses)
A problem with any rule-based grammar is that complete coverage of a natural language is not possible.
The search for a complete grammar of English has been likened to the search for the Holy Grail (Jensen & Heidorn, 1982).
One of the principal authors of the ANLT grammar system has estimated that the system is capable of covering about 25% of English (Briscoe, 1988).
Grammatical omissions from the ANLT include punctuation, titles, dates, addresses, digits, parentheticals, ellipsis, anaphor binding, conditionals and comparatives (Grover et al, 1989).
3.3.3.
The Parsing Algorithm
Having developed a lexicon and the set of grammar rules it is necessary to provide a mechanism by which these rules are applied.
The parsing algorithm used is unique to the ANLT (Phillips, 1986).
The parsing algorithm is based on chart-parsing techniques and works breadth-first across the input.
The parser works its way along the text string word by word.
The basic mechanism is to build an edge from left to right.
When an edge is completed then any partial edge immediately preceding it in the chart is examined to see if the newly completed edge is required for completion.
If this is the case then the edge is added to the preceding edge.
If the preceding edge is in turn completed then any partial edge preceding it is tested to see if it needs the newly completed edge etc.
A diagram best describes the situation:
A complete edge (complete1) is found in (a).
This is required by partial2 for completion.
Partial2 is extended to include complete1 and in turn becomes a complete edge itself, complete2 in (b).
Partial1 requires complete2 for completion, it extends to include complete2 and becomes complete3 in (c).
For example consider the ruleset:
and the input:
The parser works its way along the text string word by word and modifies the list of possible (partial) parses by incorporating each word as it is encountered.
At position 1 a determiner, det, is found which is on the right hand side of rule (r2).
There is not sufficient information available at this point in the parse to complete all of this rule.
Hence a partial NP edge has been found between positions 0 and 1 which requires a noun, N, to complete it.
At position 2, N is found, this completes rule (r2).
Hence a complete NP edge is created between positions (0 — 2).
This in turn provides a partial edge for rule (r1) between positions 0 and 2 which requires a verb, V, to complete it etc.
Experiment 1
The first step in the evaluation of the ANLT processing system was to select a few sentences from the Longman/Lancaster corpus and samples of recognised text (the HP business text in Appendix 5 and estate agents details).
Each test sentence used is therefore a valid member of the English language (although it may not necessarily conform to prescriptive syntactic rules).
Some general rules of thumb were used in the selection of test sentences.
Firstly there should be no punctuation (beyond sentence terminators) due to the acknowledged failure of the ANLT to process such phenomena.
Simpler sentence forms were preferred and no sentence containing more than 18 words was used.
The set of sentences selected are shown in table 3.2.
Each of these sentences was submitted to the ANLT (with the alternative morphology described in $3.2.1.3) to determine whether the sentence parsed, how many parses were obtained and how long the processing took.
Processing was carried out on a SPARC 4/75 with 48 MBytes of memory.
With a number of these sentences the ANLT was unable to process the complete sentence.
In such cases simpler versions of the sentence were considered.
Table 3.3 contains the results for these sentences.
‘Top category’ is a flag used by the ANLT.
If this flag is OFF, any category which covers all the input is accepted as a valid parse; if the flag is ON then only parses whose root category is an extension of [T +]are accepted.
As can be seen from the table a significant reduction in the number of parses is achieved when the flag is ON.
When 0 is given as the number of parses, this indicates that the parser exited cleanly with no parses being found, a dash (-) indicates that the system crashed before a parse was obtained.
Hence sentence 10 does not have a number of parses since it exhausted the storage of the LISP system, having already allocated 25 MBytes.
A similar problem was also found with sentence 13, whereas sentence 12 crashed while printing the bracketed parses.
The quoted CPU times do not reflect the true extent of the time actually required to obtain parses.
For longer sentences there is a vast amount of garbage collection required.
It is not uncommon to wait for 5 hours for the result to be obtained.
On a SUN 3/160 the effect was even more devastating.
Sentences could require in excess of 9 hours for a result (or crash) during which time it was known for the system to refuse logins to other users due to insufficient swap space.
Although swap space may be increased it is a drastic solution simply to obtain a parse for a single sentence.
It is understood that the ANLT is by no means complete (no rule-based grammars are) and therefore no complaint can reasonably be made when a sentence such as(3) fails to produce a parse.
However the length of actual time required before the parse fails is unacceptable.
The problem is further compounded for the recognition task.
Unlike the test sentences above, recognition input can contain many alternative words for each one written.
Hence there is a significant increase in the processing due to the increased numbers of words to be assigned grammatical information.
A basic modification was incorporated into the system to allow for the processing of the multiple candidate words present in the input lattice.
The basic algorithm provided with the ANLT is able to deal with the lexical ambiguity (the ability of words to belong to more than one syntactic category).
A short-term modification was made which forces into this framework not just the lexical ambiguity of single words but also the other candidate words for any given location in the input.
Due to the computational intensity of the system, this was only successful for the simplest of sentences with very few alternative words.
3.4.
Discussion
The following criticisms are based solely on the applicability of the ANLT to the process of script recognition with no attempts to extend the grammar for any particular domain.
The ANLT is much more than just a parser, it is a development tool for the production of testing of grammars and morphological processors.
The generality of the ANLT system will obviously be ill-suited to certain aspects of any specific problem.
From the investigations carried out it appears that the use of the ANLT system for a recognition task is computationally infeasible.
These findings are similar to those of Briscoe (personal communication) who attempted to modify the ANLT for a speech recognition task.
The decision he reached was that existing computer hardware was not sufficiently powerful to cope with the problem.
From the description provided of the system it is clear that more testing of the system to determine it's full capabilities would be desirable.
However the computational demands of the ANLT system severely restricted the amount of testing that could be carried out.
Many of the ‘real’ sentences that were processed took so long and used up so much of the computational processing of the SUN system that full testing was not feasible.
There are a number of linguistic phenomena that the ANLT system does not attempt to deal with.
The most striking example is the ignorance of punctuation.
In fact very few generative systems make any attempt at processing punctuation.
(Exceptions to this include Allen (1987) who used a very basic implementation of a rule-based grammar for text-to-speech processing.)
Punctuation performs a valuable role in written language.
It enables passages of language to be read in a coherent manner and gives an indication of the rhythm and colour of the speech.
The principal use of punctuation is to separate units of the grammar (e.g. sentences, clauses, phrases, words) from each other.
Without the capability to deal with punctuation, rule-based systems are unable to deal with many naturally occurring linguistic constructions.
Syntactic ambiguity is a major problem with the system.
The extent of this problem may be appreciated by referring table 3.2 which shows the number of entries in the ANLT lexicon for some highly ambiguous words.
There are ways to reduce the problems posed by syntactic ambiguity.
Automatic tagging programs are extremely efficient and accurate (e.g. CLAWS (Garside, 1987), PARTS (Church, 1988)).
The PARTS tagger is currently being used to reduce the syntactic ambiguity problem for the FIDDITCH parser (Hindle, 1983b).
This parser is currently being used to provide skeletal parses of the ACL/DCI corpus (Marcus, 1990).
Rather than the parser consider each of the possible grammatical categories of the input words, the tagger pre-processes the text and labels each words with its most likely part of speech.
Hence the parser has only to decide on the syntactic structure that can be made from combining these different parts of speech.
A similar approach would be of some benefit to the ANLT system.
If the tagger indicated that a word was a noun then the non-noun lexical entries could be given reduced priority by the ANLT.
Unfortunately a tagger can only provide gross syntactic categories; the ANLT requires very detailed grammatical information.
The principal reason for the vast number of parses that may be suggested for relatively simple sentence constructions appears to be the subcategorization of verbs.
The highest level of verbal information that can reasonably be expected from an automatic tagger is the transitivity of the verb.
Therefore only a limited reduction of ambiguity will be obtained.
The matter is further complicated for the recognition task.
A tagger is designed to work with certain text.
That is, the actual words in the sentence are known, and the function of the program is to determine the grammatical category of each word.
For the recognition task there are a number of alternate words suggested.
The accuracy of the tag assignment will be seriously reduced by such input.
The combination of syntactic ambiguity problem of the ANLT system with the word ambiguity from the recognition system adds a further dimension to the difficulty of performing the parsing task.
The number of potential parses increases exponentially, often beyond the capability of the hardware to process the input.
The resolution of syntactic ambiguity is one of the tasks to be performed by semantic analysis.
If no semantic or pragmatic restrictions are applied during the parsing of the sentence then many parses are suggested that are semantically or pragmatically infeasible.
The imposition of semantic or pragmatic restrictions could occur after the parse is completed to remove improbable combinations.
This approach is inefficient since the computational effort to produce the parses has already been expended.
It can also mean that there will be more work for the semantic analyser to perform since there are so many more combinations to check.
A better way would be to apply semantic restrictions to partial parses.
Any partial parse that is rejected will be removed — thereby removing any parse that would have contained this component.
Unfortunately there is no semantic theory currently available that is able to work in such a way for a broad coverage of English.
3.5.
Summary
The issues that need to be taken into consideration when selecting a parser have been discussed.
The ANLT was selected as the best available rule-based parser to use.
There are a number of reasons for this.
It has a has a large coverage (for a rule-based parser) grammar, based on GPSG.
The lexicon is very large compared to many other systems.
The parser works in bottom-up, breadth-first manner and uses a chart for efficiency.
In addition the system is provided with a grammar development environment that is designed to support a linguist develop and edit grammar rules and for building a lexicon.
The system is written in a modular manner allowing modules to be supplemented and replaced.
Investigations with the system have proved disappointing.
The system is computationally intensive.
The complete lexicon cannot be loaded into memory.
Also the fragility of the grammar — there are a number of features that are not covered by the grammar and it often takes a long time to reject the sentence.
Sentences that can be analysed often produce a huge number of possible analyses.
The morphology of the system was simplified in a manner suited to the recognition task.
Only through the use of such a modification was it possible to have access to the complete lexicon provided with the second version of the ANLT.
This did not however make sufficient difference for the system to be feasible.
Overall the computational complexity of the system rules it out at the present time for application to the recognition task.
Even if a computer was available which could cope with the load, the system has serious difficulties not least of which is coverage.
The large number of parses produced are unnecessary and unhelpful in the recognition application.
While little data is presented here it is clear from the few examples given that the ANLT has problems with ‘real’ language — the number of possible parses produced and the failure to parse — which would not be solved by additional computational power.
Chapter 4
A Probabilistic Syntactic Processor
4.1.
Introduction
An alternative methodology to rule-based parsing employs probabilistic techniques.
The use of statistical techniques is not new to computational linguistics.
As long ago as the 1930's Zipf was investigating the statistical nature of language (Zipf, 1936).
However the use of statistical techniques in linguistics fell from general use apart from a few ‘marginal’ applications.
One application that has continued to make use of the statistical properties of language is stylistic analysis.
Stylistic analysis aims to ‘fingerprint’ authors and verify the authorship of disputed works (e.g. (Ellegard, 1962),(Morton, 1965)).
There are a number of measures that can be used in such analyses including the lengths of words and sentences used, the frequency of use of individual words and syntactic analysis.
The use of such techniques for language processing has traditionally been frowned upon by the linguistic community.
In recent years the use of statistics for language processing has regained support.
There are a number of reasons for this.
The difficulties encountered by workers taking the generative approach has led to the search for alternative techniques.
At the same time computers have become much more powerful and common with much more information available in machine readable form.
Whereas a few years ago a mainframe computer would have been essential to perform corpus processing, a desktop computer now suffices.
Nor is storage of large amounts of data a problem anymore, CD-ROM technology offers the ability to store many million words on one small disk.
Hence statistical processing of language is once more an important technique used by computational linguists.
This chapter describes the methods employed in creating a statistical syntactic processor to assist in text recognition.
It is a long chapter.
A small amount of the theory of statistical models is provided followed by an overview of the methods used to create a statistical processor.
A preliminary investigation is shown that uses a small scale statistical analyser to process text that had caused great problems for the rule-based system.
This system is extremely basic but the results are convincing enough to lead to the development of a large scale recogniser.
The remainder of this chapter deals with the production and testing of a statistical analyser capable of dealing with general language.
The production of such a system requires the development of a large lexicon and extraction of language information from large samples of the language.
Such information can be found in machine readable dictionaries and corpora.
The information present in these sources and its development into a useful format is described.
A number of investigations are carried out with the scaled-up analyser to determine what factors can lead to improvement in performance.
Implementation details are provided, indicating the system operation and communication with the other recognition modules.
4.2.
Language Modelling
Language may be modelled in terms of its statistical behaviour.
For a recognition application the aim of the model is to participate in the choice of candidates recognised (Jelinek, Mercer & Bahl, 1983).
Since the sequence of words in a sentence is subject to grammatical and semantic constraints it is possible to build a probabilistic model for this purpose.
Probabilistic models have commonly been used to investigate the semantic associations between words.
Rigorously, the probability for a word to be produced should depend conditionally upon the whole of the previous word sequence.
Let W, denote a string of n words.
where
W = w1, w2,…, wn
The probability of this word sequence, P(W), is the product of conditional probabilities:
That is, the probability of obtaining a word string, W, is given by the probability of finding the first word, w1, multiplied by the probability of finding the second word, w2, given w1 etc., multiplied by the probability of finding the last word, wn, given that all of the previous words were found.
Therefore, in general the probability of finding a specific word, wi, is given by the entire past history of the text.
In reality it is impossible to estimate the probabilities P (wi|w1,…, wi-1) for all but very small values of i, and only for quite a small vocabulary.
The reason for this is that for a vocabulary of size L, there exist Li-1 different histories.
This would require a huge corpus, would be extremely difficult to estimate and would require too much storage.
One approximation that is often used is to simplify the model by considering only N-grams.
With this approximation only the N-1 previous states are considered.
An example of this is the trigram model used in the TANGORA speech recogniser (Jelinek, 1986) which assumed that histories are equivalent if they end in the same two words.
Thus 
If one considers that a practical vocabulary size will contain thousands of words then even with this assumptions the number of estimates required can become astronomical (e.g. if L=5000, and i=3, it is necessary to estimate values for 1.25*1011 states).
Hence it is clear that N-gram models are computationally practical for only small values of N. (Jelinek's system used a vocabulary of 1000 words and i=3.)
It is also often the case that there is insufficient data available for a reliable determination of all of the parameters of a Markov model.
Many of the word trigrams that are possible are not present in the corpus.
Jelinek trained his system on a corpus of 1,500,000 words and tested it on a test corpus of 300,000.
Of the trigrams found in the test corpus, 23% had not been found in the training corpus.
Such sparse coverage necessitates the estimation of values for the missing transitions.
For a given corpus size, if one uses coarser classification then more reliable but less precise predictions are obtained.
By using syntactic categories rather than words as the states of the model we are able to improve the accuracy of the model's ability to predict.
Hence we investigate transitions not between individual words but grammatical categories.
The use of grammatical categories rather than individual words means that the number of different states is much smaller.
(Although there is no universally accepted set of grammatical categories, the size of tagsets used in corpora tend to be less than 200.
In contrast, the number of words in the language is unbounded.)
This provides a number of advantages.
Firstly the number of transitions which can occur is significantly reduced — with L=109 (the number of different tags used in the current project) and i=3 there are 1,295,029 possible transitions (compared to 1.25*1011 for a vocabulary of 5000 words).
The probabilities of these transitions may be more reliably obtained from available corpora.
Also the size of the model is independent of the number of words in the dictionary.
Hence the vocabulary of the system is effectively unlimited, unlike models working at the word level where the size of the model increases very rapidly with the size of the vocabulary.
The investigation of syntactic relations in this way is much less common than semantic relations.
There are two main applications for which this approach has been used:
1
Automatic corpus tagging.
Examples of this are the CLAWS system (Garside, 1987) for tagging the LOB corpus and the PARTS tagger (Church, 1988) which is being used for tagging the ACL/DCI corpus.
2
Syntactic error detection (Atwell, 83)(Atwell, 87)(Atwell, 88)(Atwell & Elliott, 83).
4.3.
Overview of Methods.
The underlying approach taken is based upon n-gram statistics.
An n-gram is an ordered sequence of n symbols (letters, words, grammatical categories).
The frequency of occurrence of each n-gram in a continuous stream of data constitutes the n-gram statistics of the data set.
Hence the 1-gram statistics of a data set are simply the frequency of each symbol within the data set.
The 2-gram (bigram) statistics are the observed frequencies of each pair of symbols etc.
In this case the symbols used are the grammatical categories of the words in the corpus.
There are two distinct processing phases involved — a training phase and a recognition phase.
During the training phase a corpus of text is taken and investigated to determine the frequencies of transitions between grammatical categories (e.g. how often is a determiner followed by a noun).
The frequencies obtained are then normalised in some manner and stored in a transition matrix.
The recognition phase is the run-time application of the information derived during development.
At run-time, given a lattice of possible words, the syntactic processor ranks the possible words according to their ability to combine with neighbouring words.
Two items of data are required to make this decision.
Firstly one must determine the grammatical categories of the words in the lattice.
This information is obtained from the lexicon.
Secondly the probability of each of the possible transitions between the categories is retrieved from the transition matrix.
Each words in the lattice is then given a syntactic rating based on the probabilities of the transitions it participates in.
4.2.1.
Training Phase.
During the training phase the probabilities of bigram and trigram transitions between grammatical tags are determined.
A sequence of grammatical tags in the corpus is taken and split into pairs and triples.
For example, the sequence of numbers 1 2 3 4 5 would split into the bigrams: 1 2, 2 3, 3 4, 4 5 and the trigrams 1 2 3, 2 3 4, 3 4 5.
To determine the probabilities of the bigrams and trigrams a count is made of how many times each bigram and trigram occurs in the corpus.
(Within a unix environment this may be easily achieved using the sort and uniq commands.)
These frequencies are then normalised to correct for any bias towards tags that occur very frequently (e.g. singular nouns) over rare tags (e.g. pre-qualifiers).
The probabilities of the transitions are then stored in transition matrices for use at recognition time.
4.2.2.
Recognition Phase.
The run-time application of syntactic information uses the transition matrices and the lexicon to rank the words in the lattice.
The grammatical tags for the words in the lattice are retrieved from the lexicon.
The probability of each of the possible tag combinations is then found from the transition matrices and the words assigned the relevant grammatical scores.
The algorithm used is best described by considering an example.
The sentence this parrot is dead was written and recognised by the handwriting recogniser.
The lower levels of recognition were unable to uniquely determine the words this, parrot or dead however, and the actual input to the syntactic processor is shown in fig 4.1.
The grammatical categories (tags) for the words in this lattice are shown in table 4.1.
Grammar tags will be described later.
(For computational ease the grammar tag is represented as an integer, for clarity a description of the tags is included in the table.)
Of the words in the lattice dead, perfect and tail are syntactically ambiguous.
In fig 4.2 the grammar tags of the words are shown as superscripts (tag 7 indicates a sentence boundary).
Different syntactic classes of words are treated as distinct nodes in the lattice.
The introduction of grammatical information increases the number of different paths through the lattice from 24 (4*2*1*3) to 72 (6*3*1*4).
The application of bigram information to the lattice is shown in fig 4.3.
Rather than investigate each complete path across the lattice, a window of width 2 is moved across the lattice.
(The main reasons for taking this approach is that for dynamic handwriting recognition it is necessary to select the correct information shortly after the word was written — it may not be possible to wait for the user to finish a sentence or clause.)
Table 4.2 shows the possible transitions to be retrieved from the bigram matrix for the windows of fig 4.3.
Each word in the window participates in a number of transitions.
For each window position the scores of the transitions are determined from the matrix.
This score is a normalised version of the transition frequency.
For each word in the window the score of the best transition it participates in is added to its syntactic score.
The window is then moved along to the next position.
Once the syntax window has passed beyond a character position, the score for each word is scaled, relative to the highest scoring word in the position i.e. 
The methods used for trigrams are analogous to those in the bigram case except that the transitions are of length 3.
4.4.
Preliminary Investigation
An initial investigation was carried out using text which the rule-based systems had failed to deal with adequately (estate agent's literature).
The lexicon used consisted of the Oxford Advanced Learner's Dictionary of Current English (OALDCE).
Use of the OALDCE affords a lexicon of almost 70,000 words.
Compared with the usual size of lexicons found in natural language systems this is a sizeable lexicon.
An advantage of using the OALDCE is that it is targeted at non-native learners of English.
Hence the dictionary aims not at completeness but rather at covering the more usual words of the language and avoids rare usages of these words.
(More recent editions of this dictionary ensure that this is the case by drawing on sizeable corpus resources.)
For a syntactic analyser it is necessary that the words be given some identifier to show the grammatical category of the word.
Unlike the detailed grammatical information used in the unification grammars, statistical techniques tend to employ a set of grammatical tags.
A tag is simply a marker of the grammatical category (e.g. NN may indicate that a word is a common noun, PN that it is a pronoun).
A tagset is the selection of grammatical wordclasses that are used by a system.
The tagset used for the initial testing of the method consisted of a subset of the grammatical tags provided in the TEXT710 version of the OALDCE (Appendix 3)(supplemented by some extra tags to indicate punctuation and sentence boundaries).
There are 60 grammatical categories specified within this lexicon indicating such properties as transitive verb, plural noun, proper noun etc.
The grammar tags have been converted into numerical codes (1…60) for computational simplicity.
For the initial investigation no tagged corpus was available, instead the text used for training consisted of a selection of ‘raw’(i.e. just plain text) estate agent's details.
Hence it was necessary to first determine the grammatical categories of the words in the corpus.
There were two options available at this stage — either to manually tag the corpus or to try out some automatic determination of the tags.
It was decided to first try out the simplest possible computational technique.
First the corpus was taken and analysed to determine where the sentence1 boundaries occurred.
This is not completely trivial.
Sentence boundaries in real text are often indicated by textual layout (e.g. a blank line may be sufficient to indicate that a statement is finished).
Neither is it valid to assume that a full stop indicates sentence termination.
Full stops are also present in real numbers, abbreviations and after numerals to indicate a point in the argument.
Once the text had been divided into sentences the words and punctuation were replaced by the corresponding grammatical code(s) from the OALDCE.
For the preliminary investigation no attempt was made to ensure that the correct code was assigned.
This meant that the method over-generated i.e. the correct tag assignment was present as well as incorrect assignments.
4.4.1.
Results of Preliminary Investigation
The processor was tried out on a set of estate agents texts that the generative grammar had difficulties in processing.
These texts are examples of real-life language.
The results for the syntactic processing of a typical sentence (the cliff top with its access to sandy bathing beaches is also within easy walking distance) are shown below.
The score assigned to the word by the syntax analyser is shown beneath the word.
The correct words are given a shaded background.
4.4.2.
Discussion
Similar results to those shown in the example were achieved for the other estate agents sentences tested using this simple analyser.
That is, for approximately 85% of words for which alternatives were available, the correct word was selected by the analyser.
These results compared very favourably with those obtained using the rule-based system.
With the ANLT system these sentences had achieved very poor results whether as a result of the grammatical construction being absent from the grammar or through computational failure of the system.
In contrast this very basic technique is robust and computationally simple.
The processing time was approximately 3 seconds per sentence using the same computer (a Sparc 4/75 with 48 MBytes of memory), and LISP system (KCL) as the ANLT investigation.
For the statistical processor the processing time is linearly proportional to the sentence length, as opposed to rule-based method which was exponentially related.
The computation required for each sentence is extremely simple and does not require the complicated intermediate storage of possible parses to be kept in memory.
Of course there are a number of modifications that need to be made to this analyser before it is able to cope well with general text.
The most obvious improvements to be made are the use of a large tagged corpus and development of a compatible, large vocabulary lexicon.
The translation to a more portable programming language is also important for compatibility with the other modules of the recognition system.
The rest of this chapter will deal with such modifications and improvements to the probabilistic analyser.
4.5.
Machine Readable Corpora of English Texts
A corpus is a body of text or speech that provides a representative sample of a language.
The importance of corpora to workers in linguistics is that they provide real instances of the language (as opposed to interesting phrases that a linguist may conceive) and may be used to train and test natural language systems.
For the current project a corpus is used to train the higher level modules (i.e. lexical, syntactic and semantic) with information about the behaviour of text.
A number of corpora exist for text (e.g. the Brown Corpus (Kucera & Francis, 1967), the Lancaster-Oslo-Bergen (LOB) Corpus of British English (Johansson, 1980, COBUILD Corpus (Sinclair, 1987a)) and a small number of speech corpora (e.g. London-Lund (Svartvik & Quirk, 1980)).
4.5.1.
Uses of Corpora
Rule-based linguists have traditionally made little use of corpora.
Generative linguists argue that corpora are finite and degenerate and are therefore unable to deal with many of the phenomena present in language.
Such are the difficulties that have been found when trying to scale up toy grammars that computational linguists are drifting away from the creation of demo-sized systems and aiming at larger systems.
Rather than become side-tracked by interesting but relatively insignificant phenomena, some generative linguists are now using corpora to identify where a grammarian's efforts can best be concentrated.
Corpora are also useful to generative grammarians for providing an objective and reliable evaluation of a grammar's coverage and performance.
The use of corpora for automatic acquisition of lexical information has been advocated (e.g. (Brill et al, 1991),(Finch & Carter, 1992),(Atwell, 1987)).
Before this becomes a realistic proposition some developments are necessary.
The main reason for this is that extraction of many types of information from corpora usually pre-supposes the capability to automatically analyse the raw text in various ways.
This will itself involve the development of substantial lexicons.
Statistical analysis of language requires investigation of corpora to derive probabilistic models of the language.
Within the current project use is currently being made of corpora to derive information relating to the frequency of grammatical transitions, word frequency, concordances of words and probable grammatical categories of words.
The use of corpora is becoming increasingly important in the production of dictionaries.
A revolutionary approach to dictionary creation was taken with the COBUILD dictionary (Sinclair, 1987b).
In this dictionary each definition was derived from a 7.3 million word corpus.
There are other ways in which the COBUILD dictionary is unique — explanations of words are given in sentence form and supported with real examples from the corpus.
While other dictionary publishers are not taking the extreme stance taken by COBUILD, corpora are being recognised as increasingly useful tools for creating up-to-date dictionaries that accurately reflect the language.
As proof of the importance that dictionary publishers are placing on corpus use, three rival British dictionary publishers (Oxford University Press, Longmans and Chambers) are currently collaborating in the production of the British National Corpus.
4.5.2.
Classification of Text Corpora
Corpora may be classified according to how much analysis the corpus creators have performed on the text.
Three formats of corpora exist (as defined by Leech (1990)):
Raw Corpora.
These consist solely of the ‘pure’ text, no attempt having been made to annotate the text.
Such corpora are very common since any body of text falls into this category.
Tagged Corpora
Tagged corpora contain not only the raw text, but also the grammatical category of each of the words.
Although less common than raw corpora, tagged corpora are becoming more common with the development of reliable (>95% correct) automatic taggers (e.g. CLAWS (Garside, 1987), PARTS (Church, 1988)).
Examples of this type of corpus are the LOB and the Brown corpus.
Analysed Corpora.
Analysed corpora contain not only the tags of each of the words but also a syntactic analysis of each extract.
The rarity of such corpora reflects the effort required in the analysis.
Analysed corpora are produced either by manual parsing or automatic parsing with manual correction.
Although tools have been developed allowing trained users to perform analysis quickly the process is demanding.
4.5.3.
Corpus size and Composition
When building a corpus of text the general maxim is that bigger is better.
There is a huge imbalance in the frequency of words in the English language.
A large proportion of a corpus is made up of only a few words (about 25% of a corpus is made up of just 15 different words).
Even for a very long text, about half of the words found will only occur once.
In order to study the behaviour of a word it is necessary to have a large number of occurrences available which necessitates that modern corpora contain many millions of words (Sinclair, 1991).
The size of a corpus is not the only important factor, it is also vital that the corpus be ‘balanced’.
That is, the corpus should be representative of the language according to a number of variables (e.g. time-span, geographical and social range, authorial background, publishing history, discourse model and type (Sweeney, 1992)).
In order to achieve this balance care must be taken in the selection of sources from which the corpus is created and how much of each source is used in the corpus.
Corpus compilers base the composition of a corpus on some model.
In order for a corpus to be representative it is necessary to define the whole of the text that the corpus is to be a sample of.
For a finite domain (such as the Bible or the works of Shakespeare) this is simple.
For unconstrained language this definition is much more difficult.
A number of approaches have been used to achieve this definition.
Yang (1985) assumed that a library was a microcosm of the written language; creators of the LOB corpus relied on the comprehensiveness of established bibliographic sources (Hofland & Johansson, 1982).
Clear (1990) suggests that perhaps the size of a corpus is more significant than its composition although the two parameters are inter-dependent.
Testing of this hypothesis is difficult since probabilistic language models tend to be tested on data from the same domain as the training corpus, hence domain specific effects are unlikely to show up.
4.5.4.
Availability of Corpora
Corpus availability is currently limited due to legal constraints although some have been cleared for academic research.
The most widely used text corpora are the Brown and the LOB corpora, both of which have been tagged.
Both of these corpora are about a million words in size and are derived from a variety of domains.
It is now realised that a million words is insufficient to produce an adequate model of a language since many of the phenomena of the language are so rare that they will be absent from such a corpus.
Recent corpora are significantly larger, the Birmingham Collection is reputed to be about 100 million words (although sadly this corpus is very difficult to obtain) and the Longman/Lancaster corpus of 30 million words.
Currently being created are and the Text Encoding Initiative in America (Walker, 1989)(Marcus, 1990a) is producing a corpus of 100 million words.
A similar initiative is also underway in Britain to create the British National Corpus which will also contain 100 million words, due for completion in 1994.
With such large corpora it is believed that improved language models may be created.
Analysed corpora are rare and only small examples of this genre are currently found.
These include subsets of the LOB corpus (Lancaster-Leeds Treebank, 45000 words) and the Brown corpus (Gothenburg corpus, 128000 words) although some others exist (e.g. Nijmegen Corpus 130,000 words and the Polytechnic of Wales Corpus of 100,000 words of children's speech).
4.5.5.
The LOB Corpus
The LOB corpus has been the principal corpus used throughout the current project.
The LOB corpus is made up of 500 British English (printed) text samples from the 1960's.
Each sample contains approximately 2,000 words and the complete corpus contains about a million words.
Each word is tagged with its grammatical category.
The texts were selected by the corpus compilers to provide a wide representation of text types to allow research on a broad range of aspects of the language.
To allow for testing of the analysers in the current project approximately 5% of the corpus has been removed.
The composition of the test corpus and the reduced LOB corpus is shown in table 4.3.
The grammatical tagset used in the LOB corpus (Johansson, 1987) is shown in Appendix 2.
There are 132 members of this tagset (minor variations of this tagset occur however, for details see Sampson (1987b)).
The simple wordtags either stand for classes of grammatically similar words (e.g. NN for singular common noun) or for closed class words that have a special function (e.g. TO indicating when the word to is infinitival).
The tagset will be discussed in more detail later.
4.6.
Machine Readable Dictionaries (MRDs)
The natural place for a person to look for information about an unknown word is in a dictionary.
A dictionary contains lexical entries for a large number of words.
Each entry has been hand-crafted by a skilled specialist in lexical analysis.
As the production of dictionaries becomes computer-based so dictionaries are becoming increasingly available in machine readable formats.
Over the last few years work has been carried out on using machine readable dictionaries as a source of linguistic knowledge (Amsler, 1981),(Boguraev & Briscoe, 1987)— thereby enabling the production of a large vocabulary lexicon for considerably less effort than was previously required.
Given the (relatively) small size of the LOB corpus, supplementation with information from MRDs provides advantages for the current system:
Vocabulary Information.
The LOB corpus contains information for less than 40,000 types, available MRDs contain information for a larger number of types.
Hence the use of an MRD provides a larger vocabulary for the system.
Syntactic Information.
Many types occur just once in the LOB corpus.
More instances that this are required to be certain that the possible parts of speech of the word are found.
The supplementation of the grammar information from the corpus with that from the dictionary alleviates this problem since a dictionary aims to provide a list of all of the possible parts of speech (although a dictionary does not indicate the likelihood of a word behaving in that manner).
The Longmans Dictionary of Current English (LDOCE) and the Oxford Advanced Learners Dictionary (OALDCE) are particularly rich sources of syntactic information.
Both of these dictionaries use structured grammar codes to indicate grammatical patterns in which the words may participate.
More general dictionaries tend to define grammatical information less formally than these learners dictionaries.
Semantic Information.
The dictionary definition of a word is one form of semantic information.
(other forms include collocational information, encyclopaedic knowledge, semantic networks).
There are problems associated with extracting semantic information from dictionaries.
Most importantly no widely recognised classification of semantics exists.
Even if such a system did exist it is unlikely that it would appeal to lay users of a dictionary.
For a human reader a discursive natural language definition is a more sensible format.
If one investigates the format commonly used in dictionary definitions it is clear that there are some conventions that are followed.
A restricted style of language is used in dictionary definitions with a number of phrases being extremely common (e.g. of or pertaining to, also called, compare).
Researchers within computational lexicography have attempted to exploit this style to extract semantic information.
One method involves parsing the dictionary definitions to identify the genus term in the definition and produce semantic relations (e.g. sets, hierarchies and networks)(Ahlswede & Evens, 1988).
Beyond this implicit semantic information the machine-readable version of LDOCE also explicitly contains semantic tags which have been used (e.g. (Slator, 1989)).
Morphological Information.
Regular inflected forms of words are not given their own specific dictionary definitions.
This duplication of definitions would serve little purpose.
Dictionary authors expect users to perform regular morphological processing themselves.
Within the definition of a root form of a word is contained information about the inflected forms of the word.
Hence a reader looking up the definition of a word such as dogs would find the relevant information stored under the word dog.
For words that inflect irregularly this is not the case.
The irregular inflection will be presented as a headword with a reference to the appropriate root form.
Hence the definition of wept will contain the brief description that it is the past tense of weep thereby informing the reader where to seek further information.
Morphological processing is not quite so trivial for a computer as it is for a human.
Within the Text710 version of the OALDCE are inflexional codes which provide a means of producing the standard inflections that a human would be able to generate automatically.
The dictionary definitions contain the information about the irregular derivations.
For example under the headword child is the information that the plural is children rather than childs.
Such information has been employed by the current system to produce a computationally simple means of morphological processing and automatic lemmatisation.
Compound Information.
Compounds can pose problems for automatic text processing systems since many compounds of words are idiomatic and have developed meanings and grammatical properties different from the combinations of the individual words.
Dictionary compilers recognise this problem and provide separate definitions for such compounds.
Hence from using a dictionary one could determine that hot dog bears little relation to canines.
There are many such compounds in dictionaries (e.g. the Collins English Dictionary contains about 30,000 compounds).
The use of compounds is discussed later.
4.6.1.
Availability of MRDs
Until very recently MRD's have been available only as a typesetting tape of a printed dictionary (Meijs, 1992).
Problems often exist in persuading dictionary owners to part with these typesetting tapes for understandable reasons (the production of a pirate copy from a typesetting tape is a trivial matter and can have serious repercussions for dictionary publishers).
Over the last decade the major dictionaries that have become available (in a limited manner) to researchers are:
Collins English Dictionary (CED)
Longmans Dictionary of Contemporary English (LDOCE)
Merriam-Webster 7th edition (W7)
Oxford Advanced Learners Dictionary of Current English (OALDCE).
Text710 (a limited version of OALDCE, elsewhere referred to as CUVOALD and CUV2)
Dictionaries are produced with specific markets in mind and their contents vary accordingly.
Learner's dictionaries (e.g. LDOCE and OALDCE) are designed for people for whom English is their second language.
Fewer assumptions are made in such dictionaries about the linguistic competence of the user.
Hence the definitions aim to be very clear and precise, perhaps produced using a restricted defining vocabulary.
The grammatical and morphological information in learner's dictionaries also tend to be very explicit.
At the other extreme is the Oxford English Dictionary (OED).
The definitions contained in this dictionary are voluminous descriptions of the etymology of the words, the dictionary being designed for people with a deep interest in the English language.
Between these two extremes are the general purpose dictionaries such as the CED.
A major difference between the dictionaries is the number of words defined — learner's dictionaries tend to contain about 30,000 headwords, general purpose dictionaries (e.g. CED) have about 100,000 definitions, Websters Third New International has 450,000 and the Oxford English Dictionary has about 500,000 definitions.
4.6.2.
Extracting Information from Collins English Dictionary
The use of MRD's to provide information for computational linguistic applications is a very attractive proposition.
The potential benefits from using MRD's include the availability of lexicons with syntactic, semantic, morphological, phonological and idiomatic information for a very large number of words.
Unfortunately there are some difficulties involved.
The main problem is that a dictionary is designed to be used by a human being, not a computer program.
The entry in a lexicon required by a NLP researcher may require full and formal treatment of morphology, syntax and semantics.
Such a dictionary is unlikely to appeal to a member of the general public wishing only to check on a word when solving a crossword or playing Scrabble.
However, such users are a substantial market for dictionary publishers, therefore the information in dictionaries relies on the linguistic skills and background knowledge of the human user, rather than being designed for machine consumption.
(Gross (1984) argued that the information in MRDs is too unreliable and unsystematic for use in NLP applications).
There is also a difference between machine readable and machine usable.
A sample of a machine readable typesetting tape (from the CED) is shown below, indicating the definition for the word abandon.
Dictionaries make heavy use of typography to convey information to the reader.
Typography within a dictionary indicates the function carried out by particular parts of the text.
For example the headword of a definition is generally in bold font of a particular size.
To achieve such changes in typeface typesetting codes are interspersed with the actual text.
Information contained in the dictionary (lexical, syntactic, morphological, semantic and compound) was necessary for the higher level modules of the current project.
This information could only be obtained by conversion of the CED typesetting tape to a usable representation of the information.
The transition between a typeset machine-readable form and a lexical database requires considerable effort (Amsler, 1989),(Alshawi, Boguraev & Carter, 1989).
Removal of the typesetting information from a dictionary to leave just the bare ASCII text loses useful information that the typography makes visually apparent.
The aim is to replace the typographical codes with computer-usable codes that make explicit the function of the text.
Unfortunately there is no simple translation between typesetting codes and role played by the text.
Dictionary publishers have limited fonts available and the same font may indicate different roles in different parts of the definitions (human readers are easily able to use the definitional context to determine the function of a particular font change).
Separate typesetting codes are used to indicate the typesize and the font of the text.
It is the combinations of such codes that determine the visual presentation.
The processing required to convert the typesetting format to a usable format was essentially iterative and needed to be performed with care.
Many of the problems involved in the processing derived from the size of the typesetting tapes.
The original CED typesetting tape occupied about 27 MBytes.
It was impossible to completely check the global effects of each alteration to the file.
Another source of problems was the presence of some errors within the text.
Although such errors were rare, processing had to be defensive against allowing an error (such as unbalanced parentheses) to affect a large piece of the text.
An early stage of refinement delimited each individual definition.
All processing was carried out on one definition at a time.
This ensured that errors were localised to the definition in which they occurred.
Unbalanced brackets were automatically detected by counting the brackets within each definition and inserting extra brackets in the most likely location to correct the error.
Dictionaries contain many non-ASCII characters such as phonetic and mathematical symbols (e.g. h, p, q, S etc.).
Typesetting methods produce such characters by combining typesetting codes (e.g. q is produced by £Gg, e by $e).
The first step in processing the typesetting tape was to find the mapping between the typesetting codes and the special characters.
This involved a painstaking search of the paper dictionary to find examples of each of the special characters.
Having found the location in the dictionary the computer file was then searched to find the corresponding location.
An initial estimate was made of the typesetting combination responsible for the character.
To check that the correct combination had been selected the typesetting file was searched for more examples of the combination and the effect on the paper dictionary checked.
After verification of the effect of the typesetting code combinations they were substituted for more computer-recognisable codes to simplify further processing.
Once the special character combinations had been replaced the file was iteratively refined.
Such cleaning up involved determining the structure of the dictionary definitions and the effects of font changes at particular locations within the definition.
This processing recovered such information from the text as the domain of use of a particular word, sense numbering, phonetic information etc..
The font information contains only some of the information about the roles of the definition's contents.
Further information was extracted by taking advantage of the use of dictionary-specific phrases (elsewhere referred to as defining formulae (Markowitz, Ahlswede & Evens, 1986)) to indicate the roles of their subjects.
For example‘esp.
in the phrase…’ will indicate a common phrasal usage of a word; ‘foll. by…’indicates words which commonly follow a word; ‘Comp.:’provides a word with which the reader should compare a word with.
These phrases are often embellished in the text with modifiers and slight variations (often, usually, also, mainly) which give some extra information to readers of the dictionary.
The presence of such optional words further complicated the matching of the word patterns into usable codes.
Making explicit this information serves to highlight the semantic relevance of words within the definitions that may be used by the semantic processor to construct semantic relationships between words.
The grammatical information present in the CED is very basic and of little use for developing the lexicon for a parser.
Such information was derived from the computer usable form of the Oxford Advanced Learners Dictionary.
The use of this information will be described later.
Not all the information present within dictionary definitions is of use for handwriting recognition.
Some information has necessarily been removed from the dictionary to cut down on storage requirements.
Examples of this are the phonetic information and the etymological descriptions of words.
Modification of the typesetting tape of the CED has produced a version in which the following information is encoded:
The sample definition of abandon shown earlier is now represented as:
Within this format codes are used to show the particular role of the text.
In the example £H indicates headword; £x indicates sense numbering; £W indicates derivative forms of the headwords; £G the major grammatical category; £g a grammatical sub-category.
As can be seen from the example this is much easier to use than the initial typesetting format.
Further modification is possible.
Most importantly, since this work on the dictionary was completed Standard Generalized Markup Language (SGML)(Bryan, 1988) has been accepted as a standard for encoding document information.
The hierarchical structure of the SGML system is substantially different from the system adopted for the processing of the CED.
Since this work has been carried out other dictionaries have also become available.
For example the OALDCE is now available in SGML format, also the restrictions on the availability of the LDOCE have been relaxed.
4.7.
Developing the Syntactic Lexical Information
The essential action of the probabilistic syntactic analyser is to find out how often each set of grammatical categories are found together in the corpus.
When candidate words are presented to the analyser by the recogniser the grammatical categories of these words are looked up.
The neighbouring grammatical categories are examined to determine the possible grammatical transitions.
The likelihood of each transition is retrieved from the transition matrix.
The words are then assigned a rating based on the probability of the grammatical transitions that their tags participate in.
The main information that the probabilistic syntax analyser needs from the lexicon is the grammatical tag of each of the candidate words.
This grammatical tag must be compatible with those that the corpus was tagged with.
The syntactic information stored in the lexicon represents a combination of information from the Text710 version of the OALDCE dictionary and the LOB corpus.
Both the Text710 and the corpus contain information about words and their grammatical tags.
Information is wanted from both sources since the Text710 has a larger vocabulary (approximately 68,200 words) than the corpus (approximately 38,000 purely lexical strings).
The dictionary also provides a more complete set of grammatical tags than the corpus.
Many of the words in the LOB occur just once, an inadequate basis for determining grammatical behaviour of a word.
The use of the corpus is vital for determining the probability of grammatical transitions.
Also, when words are syntactically ambiguous (i.e. they can have more than one grammatical tag) the corpus can provide information about how often each of the possible tags is used for that word.
A definitive set of grammatical tags does not exist.
Existing tagsets tend to agree about the major word class categorisations but there are often differences in the sub-categorisations used.
The tagset used by TAGGIT (Greene & Rubin, 1971) for tagging the Brown corpus contained 86 different grammatical tags.
The CLAWS tagset used for tagging the LOB corpus is very similar to that of TAGGIT but is somewhat larger with 134 tags to achieve greater delicacy of analysis while preserving compatibility with the Brown corpus (Johansson et al, 1986).
The differences between the LOB and Brown tagsets are due to minor alterations made by subcategorising some classes and redefining the boundaries of others.
The grammatical tagset used by the Text710 (appendix 3) is considerably smaller than the LOB tagset2.
As the difference in the number of tags in the tagsets indicates, the Text710 and the LOB tagsets are not directly compatible.
To use the information from both sources a translation process is required.
The result of this translation is a single lexicon having a set of tags derived from combining the Text710 and LOB tagsets.
The tags in the corpus must also be translated so that they conform to the same tagset as this new lexicon.
The first step of this translation process is the amalgamation of the LOB and the Text710 tagsets into a single common tagset.
The aim of this merger is to retain as much information as possible between the two tagsets.
There are problems associated with combining the two tagsets.
Both tagsets contain information that is absent from the other.
The Text710 tagset contains 27 distinct tags for verbs indicating transitivity, tense and details about anomalous verbs.
The LOB tagset uses just 5 tags for general (i.e. open class) verbs.
The only way in which one could incorporate such fine-grained information from the Text710 would be to completely re-tag the LOB corpus.
This was not feasible for the current project, hence these tags were replaced by more general tags.
However many of the tags used exclusively in the LOB corpus can be retained.
The LOB tagset contains a number of tags for closed class words (e.g. personal pronouns, conjugations of to be and to have).
These can easily be incorporated into the Text710 tagset.
A number of tags are missing from the Text710 tagset because they are for word types that are not found in dictionaries.
For example, genitives are not stored because they can be easily identified, nor is punctuation classed as a dictionary part of speech.
The combined tagset that results from amalgamating the LOB and Text710 tagsets contains 109 tags (Appendix 4).
The tags in the LOB corpus and Text710 dictionary were then substituted for those of the combined tagset.
This was achieved using a simple NAWK program (Appendix 7).
To translate the corpus information a file was created containing two columns — the first field on each line contains the original LOB tag, the second field contains the tag that it is to be translated to.
The initial step for the program is to load in this file and store the translation of the original tag with its replacement using a NAWK associative array.
The corpus is then processed a line at a time.
The LOB tag is extracted from the line (they are located in columns 13-17), it's replacement found from the associative array and the line printed out using the replacement rather than the original LOB tag.
A similar process is used for the OALDCE dictionary.
Full checking that the translation process is valid is not possible.
There will obviously be some differences between the tag assignments made by lexicographers and corpus compilers.
A simple indication that the mapping between tagsets had been successful was made by investigating the words that were common to the corpus and the Text710.
These were checked to determine that the part of speech assigned in the corpus was one of the possible tags for the word in the Text710 lexicon.
Of the 920,297 tokens in the LOB having entries in the Text710, the assigned tag was contained in the lexicon 889,629 (96%) times.
This represents an acceptable level of compatibility.
4.7.1.
Syntactic Ambiguity
Ambiguity in written English may arise from different words having the same spelling (homonyms) or the same word having a number of meanings.
Humans seldom notice ambiguity in language unless it is being exploited in word play (such as puns).
However for computer processing of natural language ambiguity causes a large problem.
In fact some researchers consider syntactic ambiguity to be the largest problem facing NLP (Gazdar & Mellish, 1989).
A large number of words in the lexicon are syntactically ambiguous.
Using the simple tagset of the current system the degree of syntactic ambiguity found is shown in table 4.4.
Much of this ambiguity arises through relatively rare usages of the words.
A contributing factor to this is that a dictionary aims to provide for every usage of a word with little indication of how common that usage is.
Church (1988) asserts that the most frequent category is almost always sufficient.
Consequently, the grammatical tags for each word in the lexicon are given a grammatical frequency factor (GFF) to distinguish the importance of the different tags.
The GFF derives from the equation:
That is, the GFF represents the frequency of a tag for a word relative to its most frequent tag.
Since not every word in the lexicon is present in the corpus and those present need not appear with their rarer forms, the corpus is supplemented by the lexicon for determining the GFF (i.e. each word in the lexicon is assumed to occur once in the corpus with each of the tags in the lexicon).
This GFF value can then be used when processing the input to reduce the significance of rare syntactic forms of words.
4.8.
Word Index System
The morphological processor described in chapter 3 is used for a number of the post-processing components (compound recognition and semantic processing) of the current system.
The principal action of the morphological processor is to associate a word with its morphological root (e.g. run, runs, running, ran will all be associated with run).
The method used to achieve this is to pre-process the information into the system.
At development time the root forms of each word in the lexicon has been determined utilising information from MRDs and manual checking of the unresolved words.
At run time, when a word candidate is proposed the compound recogniser (described below) and the semantic processor receive the root form of the word candidate.
Hence if running were a candidate then these two modules of the system would receive as their input run.
Rather than explicitly store the root forms as strings an indexing method has been developed.
It is more demanding for a computer to process and store strings than numbers.
Neither the semantic nor compound analysers in the current system are concerned with the actual string representation of the root form of the input word.
By storing an integral index (the ROOT-INDEX) which uniquely identifies each morphological root it is possible to reduce the storage requirements and processing is simplified.
This ROOT-INDEX can then be stored in the lexicon.
Therefore within the lexicon run, runs, running, ran each have the same ROOT-INDEX stored (19590) to indicate that they are all derived from the same morphological root.
4.9.
Processing Compounds
It is a common misconception that a word is simply a contiguous sequence of letters.
This is not always the case.
Phrases can often behave as though they were single words.
The meanings of certain types of phrases have come to mean more than simply the combination of words from which they are composed (sometimes they bear no relation to their constituents).
Furthermore they behave like words in that they are always used together in appropriate contexts and they appear to be represented as words in the mental lexicon (Wilson, 1984).
Such combinations are generally difficult for natural language systems to process due to the changes in the syntactic and semantic properties of the constituent words (and the acquisition of information about the compounds).
For the probabilistic syntax system compounds are troublesome since the window may not contain all of the compound.
Even if the complete compound exists within the syntax window it is likely that the incorrect parts of speech will be assigned to the constituent words resulting in an erratic selection of words by the analyser.
For the complete recognition system however the existence of idioms (and compounds in general) may be exploited to further reduce the uncertainty as to the correct input as the following example shows.
The word recognition alternatives for the words tennis courts was:
The only similarity between the words in these lists is that the pattern recognition system perceived them as being orthographically similar.
The presence of words that form a compound in consecutive positions other than by intention is improbable, hence from knowing the existence of a compound tennis courts one would give priority to these individual words over their alternative candidates.
4.9.1.
Acquisition of compounds
Entries for a large number of compounds (about 30,000 in the CED, 11,000 in the OALDCE) can be obtained as headwords and subentries (e.g. ‘in one's right mind’, ‘by rights’ occur under the headword ‘right’; ‘right about’, ‘right angle’, ‘right away’occur as headwords).
In fact, dictionaries of compounds, idioms and common phrases also exist, although none are available in machine readable form.
Within corpora, compounds are tagged with a single tag for the whole compound allowing a set of compounds to be easily extracted.
The LOB corpus employs ditto tags to indicate words whose syntactic roles differ from the role of the same words in other contexts.
Hence as to is marked as as/IN to/IN’; so as to is tagged as so/TO as/TO’ to/TO’.
It is also possible to take advantage of compounds, phrases and idioms which are specific to certain domains and/or contexts.
For example in Estate Agents documents the phrases ‘within easy walking distance’ and ‘sought after’are very common, and for use in such a context, these would be included in or added to the compound lexicon.
In a closed domain a list of relevant idioms is easily attainable (Zernik, 1989).
Some care must be taken not to store every compound that one finds.
Many compounds present in dictionaries are extremely rare (e.g. whirligig beetle and ruby spinel) and their storage would serve little useful purpose for normal text processing.
To avoid such unnecessary storage, compounds are only stored if the frequency of the least frequent word in the compound is above a threshold.
Although not foolproof (rare compounds of high frequency words will still be permitted) this has removed many of the rare compounds.
There are a number of compounds (particularly personal nouns) that will not be found in a dictionary which will be of use to an NLP system.
4.9.2.
Representation of compounds.
The number of compounds is large and an efficient method must be used to store such information.
It is necessary to organise the compounds in such a way that they are treated in a similar manner to individual words.
That is, when a compound (e.g. ice cream) is recognised an index relating to the location on disk of the lexical information relating to this compound should be returned.
One way to do this would be to treat a space as a legal character and build the whole of the set of compounds into the word recognition tree with an index at the end.
The space required to do this is prohibitive however.
An alternative is to build a second tree consisting of the words in the compounds, i.e. a ‘compound tree’.
This tree consists only of words that partake in compounds and at the leaf nodes are the indices relating to the disk address of the lexical information.
The ROOT-INDEX derived from the morphological processing provides a simple and efficient method of storing such information.
Rather than build the compound tree from the actual strings that form the compound, the tree is built from the root-indices.
Each word, when it is verified by the word recognition stage, returns an index that points to its morphological root.
By taking the set of compounds which are to be used and replacing each word within the compound by its corresponding root-index the storage requirement is drastically reduced.
The following example should clarify the process:
The CED contains the following compounds of the word ‘accommodation’ for storage in the compound tree: accommodation address; accommodation bill; accommodation ladder; accommodation platform; accommodation rig
The space required to store each of these as strings is prohibitive however.
Each of the words within the compounds has already been assigned a root-index as in table 4.5.
By replacing the strings with the root-indices the information that needs to be stored is reduced.
The actual information which is stored for the compounds above is shown in table 4.6.
Hence, rather than actually store the string accommodation ladder in a tree, it is necessary only to store the integers 2049 12467.
There are a number of advantages to be gained from using an index-based compound tree.
Firstly the storage requirement is significantly reduced since integers may be stored in a much smaller space than strings.
Numbers are also better suited to computer operations with the result that the process of searching the tree is made computationally simple.
A further advantage of using the root-indices to build the tree is that inflectional compounds need only be specified once.
For example, the root-indices for the words court and courts are the same.
Hence the indices to be stored for the compounds tennis court and tennis courts will also be the same.
There are also some disadvantages.
Firstly it is not possible to store the syntactic tag for the whole of the compound within the compound tree.
This is because the compound could relate to a set of different word combinations (e.g. ‘tennis court’ is a singular noun but ‘tennis courts’is a plural noun).
The result of this is that the syntactic analyser will not be able to select the correct part of speech for the compound.
However, if a sequence of words is flagged as a compound then the control program can ignore the syntax results which will be unpredictable.
An additional problem is that certain combinations of words may form a compound whereas different morphological forms of the words may not.
For example‘running water’ is a valid compound but ‘ran water’is not.
In practice the probability of such words occurring in adjacent positions is so low that the problem is negligible.
4.10.
Lexicon Storage
For a large lexicon it is necessary to use a storage technique for the lexicon that allows both rapid search for information and compact storage of the information.
The lexicon in the current project is stored in two structures: a word lookup tree and a compound tree.
Both structures are represented in memory as tries.
4.10.1.
Structure of the Word Lookup Tree
Various types of information about the words are stored at the end of word nodes in the trie.
This includes the syntactic information and the root index for the word.
The structure of the trie is shown diagrammatically in fig 4.4.
Each node comprises information about which letter is allowed as well as pointers to child and sister nodes.
Of most importance to the higher level processes is the information stored with the end of word flag.
If the trail through the trie has successfully found a word then the linguistic information for the word is found there.
This information comprises the root index information, the grammar tags and their respective GFFs as well as word frequency information for the word.
A system of flags is also stored here to indicate whether the word must begin with a capital to form a valid word, if the word is a common mis-spelling and a flag detailing whether the word can begin a compound stored in the compound tree.
For more information see Wells (1992).
The sample lexicon shown is the representation for the words cat, catch, cot, cots, dog, doggy and dogged (from (Evett et al, 1991)).
The index of each member of the compound is represented at a node in a trie (Wells, 1990).
The node has three pointers.
One points to the next member of the compound.
One points to an alternative next member of the compound.
The third either points to the index for the whole compound or is a dead end.
The existence of an index here indicates a possible end of compound, and the index gives the address of that compound in the lexicon, where a full entry for that compound will be found.
Fig 4.5 shows the structure of the compound tree for the compounds ‘rain cats and dogs’, ‘tennis ball’ and ‘tennis court’.
Within the diagram the root index of the word is indicated by preceding the word with £.
Hence ‘£rain’ refers to the integral root index for the word ‘rain’.
Access to the compound tree is achieved through the word recognition tree.
Each word in the word recognition tree is flagged to indicate whether or not it is a member of a compound.
The compound flag can be one of two alternatives: start of compound/phrase; or part of compound/phrase.
Given an allowable candidate word which is known to be able to start a compound or commonly used phrase, if the next part of the compound occurs in the list of alternatives for the next word position (and so on until the end of the compound), then it is likely that the alternatives that make the compound are the correct choices.
It is therefore worth checking the position(s) adjacent to that in which a start of compound occurs for its next element, until the end of the compound (or phrase) is reached, or until the next element is not found.
When words are looked up in the word look-up tree, if the flag for start of compound is set, the compound tree is checked.
As adjacent word positions are also looked up in the word look-up tree, they are checked for whether or not they are the appropriate words to complete the compound.
If they are, these words are given priority over the other alternatives available for each word position.
It is not possible for a lexicon to provide full coverage of language.
Among the reasons for this are the continued creation of neologisms, the introduction of foreign words and the vast array of proper nouns that exist.
One of the most important considerations to be taken into account when creating a lexicon is how full a coverage of the language it is able to provide.
There have been some investigations into determining how fully lexicons derived from machine readable dictionaries are able to cover unseen text.
There is some discrepancy to be found between the available results however.
Walker and Amsler (1986) compared the words found in Webster's Seventh New Collegiate Dictionary (W7)(G. & C. Merriam, 1963) and an 8 million word sample of the New York Times News Service (NYTNS).
Between the two sources there were 119,630 different types.
Of these only 23% (27,837) were common to both sources, with 36% (42,695) existing exclusively in the W7 dictionary and 41% existing exclusively in the NYTNS sample.
That such a large proportion of the types in the news wire sample were not found in the W7 dictionary is a worrying statistic.
Analysis of these omissions from the W7 showed that about a quarter were simply inflexions of words that already existed in the dictionary.
Such absences can be simply remedied by improving the morphological processing.
Another quarter of the omissions arose from previously unseen proper nouns.
This class of omissions are much more difficult to deal with computationally.
Sampson (1989) investigated the coverage provided by the OALDCE for a 50,000 word sample of the LOB corpus.
The results of this were more encouraging than Walker and Amsler's.
Of the 45,622 tokens in the corpus sample that would ideally be found in the dictionary (ignoring non-alphabetic combinations and enclitics), 95% (43,490) were found in the dictionary as they stood.
This figure increased to over 97% (44,448) after some minor modifications such as removing hyphens and changing word-initial capitals to lower-case.
A similar investigation was carried out with the lexicon used in the current system to determine it's ability to cope with unseen text.
In order to test the coverage of the lexicon, the test part of the LOB corpus was retrieved and each word in the text checked for its existence in the lexicon.
This lexicon contains over 70,000 words and is a combination of the Text710 dictionary and words from the (training) LOB corpus.
The test corpus is a representative 46,003 token sample of the LOB (the composition of which was detailed in table 4.3) containing 7,853 different types.
Of these 1746 types (1121 tokens) were not stored explicitly in the lexicon (ignoring numbers and formulae).
Further processing to remove hyphenation and possessives reduced the number of absences to 1186 types (669 tokens).
Hence more than 97% of the tokens and approximately 91% of the types in the test corpus are covered by the lexicon.
The largest proportion of absences are proper nouns and foreign words which account for 78% of the missing tokens (75% of the types).
Of the remaining absences only 42 types occurred more than once in the 46,000 words.
From this it is deduced that the lexicon provides adequate but not complete coverage.
The poor coverage shown by the Walker investigation must be largely as a result of the high proportion of proper nouns found in newswire text.
The improvements to be attained by extending the lexicon further also appear to be minimal.
There are a number of reasons for this.
Firstly it is not guaranteed that the required information will be in the larger lexicon.
When one considers that the main reason that words are absent is that they are proper nouns, which will also be unavailable from the larger lexicon, it is clear that omissions are unavoidable.
Furthermore with a text recognition system, as the lexicon gets larger the problems increase.
The situation is shown in table 4.9.
A 521 word text was written from a randomly selected business text, not associated with the LOB corpus.
The text was processed by the handwriting recognition system with lexicons of different sizes (based on lemmatised word frequency values).
The number of word candidates suggested by the recogniser is dependent on the size of the lexicon.
As the lexicon size increases the number of acceptable words that the character combinations can produce rises.
Furthermore a number of the alternative words suggested when using the large lexicon, would be unknown to many native speakers (e.g. betel, bey, littoral, lour, rentiers, ret).
By reducing the size of the lexicon the number of words that the author wrote that are missing from the lexicon increases.
There is obviously a trade off to be obtained by varying the number of words that are stored in the lexicon.
An additional problem that arises from using large lexicons is that, should the pattern recogniser fail to detect the correct word, then incorrect words are more likely to be suggested.
For the business text above there were 111 words in which the correct word was not recognised by the pattern recogniser.
With the 70,498 word lexicon alternative (incorrect) words were suggested for 89 of these cases.
With the 16,825 word lexicon the situation is slightly improved with incorrect alternatives being suggested for 78 of the unrecognised cases.
4.12.
Investigations with the Probabilistic Syntax Analyser
The syntax analyser developed in this thesis works in the following way:
1
The lexical lookup module determines the possible words that can be formed from the characters and the grammar tags for these words.
This information is received by the syntax analyser.
2
The possible grammar tag transitions in the lattice are determined and probability values for these transitions are obtained from the transition matrix.
3
A grammatical score is assigned to each grammar tag based on the probability of the transitions that it occurs in.
4
Each word is then given a syntactic rating based on the score assigned to the word's grammar tag(s).
At development time it is necessary to determine a probability for each of the transitions and store this in the transition matrix.
The aim is to obtain a model that is simple to apply, and has an acceptable error rate.
(It is not possible, regardless of how complex a model is used, to analyse all of the text perfectly).
As with the Jelinek model ($4.2), it is necessary to impose some restriction on the length of the n-gram transitions that are used.
The use of longer transitions requires a larger training corpus, increased storage for the transition matrices and is computationally more demanding.
However longer transitions provide stronger constraints on the tag combinations.
A number of investigations have been carried out to explore ways to best use the probabilistic analyser:
Investigation 1
Examines the ability of the LOB corpus to provide a model based on transitions of lengths 2, 3 and 4.
Investigation 2
Compares the use of three different sets of formulae to determine which provides the best model to employ for bigram and trigram models.
Investigation 3
The incorporation of GFF values to reduce the effect of rare syntactic forms of words and the recognition likelihood of the words (or lexical probability factor, LPF) into the model is investigated.
Investigation 4
A modification is made to the usual method of applying probabilistic information.
One method of applying probabilistic information is to assign a value to the final element in a transition based on its predecessors.
The modification is a summation of the probabilities assigned to a word in each window position in which the word exists.
Hence rather than simply investigate the ability of a tag to follow other tags, it also incorporates the ability of the tag to precede it's successors.
Investigation 5
The effect of different quality of input to the syntax processor is investigated.
It is rarely the case that the correct word is always amongst the alternatives suggested by the lower recognition levels.
This investigation examines the ability of the syntax processor to function with lattices of different quality.
Investigation 1:
Ability of the LOB corpus to cover transitions
The first consideration is whether the LOB corpus is able to provide enough data for a particular transition length.
The sufficiency of the million word LOB corpus for providing different orders of model for the combined tagset is shown in table 4.8.
This contrasts the number of possible transitions of each order with the actual number found in the million word LOB corpus.
An added indication is the percentage of the transitions that are found in the corpus only once.
This table can only give a general feel of the capacity of the corpus to cover the transitions likely to arise in unseen text.
There are many transitions that will never occur, this is especially the case with longer transitions.
However, so few of the possible quadgrams occur in the LOB (and of those found, 70% only occur once) that it would appear that the LOB corpus is not a large enough sample of text to provide a reliable quadgram model.
For the trigram case it is still only a small percentage of the possible transitions that do occur, and 40% of those transitions only occur once.
For the bigram case more than half of the possible transitions are found with only 18% occurring just once.
A further indication of the ability of the corpus to provide a reliable model is gained from observing how the number of transitions found varies with increasing corpus size.
The variation of number of different transitions found as a function of corpus size is shown for bigrams (fig 4.6) and trigrams (fig 4.7).
From examining the shapes of these graphs it would appear that increasing the size of the corpus would have only a small effect on the percentage of bigram transitions found.
With the trigrams however the graph appears to still be ascending quite rapidly, suggesting that a larger corpus will discover many more trigram transitions.
Although incomplete coverage for the trigram transitions can lead to problems when dealing with unseen text there are possible solutions which will allow the use of trigram information.
The problem with using incomplete data is that should a transition occur that was absent from the training corpus then the model would assert that such a transition was impossible, assign zero probability and produce errors.
There are methods available for dealing with this problem.
The first solution is to increase the size of the corpus (assuming that a larger corpus were available).
This should obviously lead to an improvement in the situation but regardless of the size of the corpus there will always be some transitions that are not found.
Furthermore as the matrix becomes less sparse the problems associated with storage increase.
Also if another corpus is available, if the tagset is different then it will be necessary to repeat all of the tagging detailed in the previous section.
Hence although a larger corpus will alleviate the problem of insufficient data it will not cure it and can lead to other problems.
The second solution is to use ‘backing off’ formulae.
This is the approach that has been taken.
The essential concept is that if a trigram does not exist then an approximate (non-zero) value is calculated based on the lower order transitions.
A number of backing off formulae are possible which aim to provide the best approximations to the probability of the transitions.
The aim for the current project is to employ the simplest method that provides an acceptable error rate.
Many of the backing off formulae that are available have been derived for semantic applications where the matrices are extremely sparse.
Sparsity and the necessity to back off is less of a problem for a model based on part of speech transitions.
Hence the more complicated backing off formulae are not necessary.
The CLAWS system also adopts ‘backing off’ formulae but seen from a different perspective — a bigram model is used except for special-case tag-triples which empirical results showed would be wrongly tagged.
For these a trigram probability is used instead.
Atwell (1983) calls this an ‘augmented first order Markov model’ rather than an ‘incomplete second order Markov model’.
Investigation 2
Recognition performance with bigrams and trigrams
Experiments were carried out to determine the best formula to use for scoring the possible transitions in a text of 372 words derived from the Malhotra corpus (Malhotra, 1975), a corpus traditionally considered to be syntactically difficult to process.
Recognition confusions for the text in the corpus were generated and the lattices produced containing 1246 words.
Two sets of experiments were performed on this input.
The first set employed bigram transitions, the second trigrams.
The best formula was determined by checking how many times the correct word was selected by the analyser using three different formulae (Table 4.9).
The weight, wt, given in Formula 1.1 is simply a count of the frequency of a tag sequence in the corpus, T1-T2; 1.2 is a modification of Bayes formula and 1.3 is suggested by Marshall (1983).
(For computational reasons scaling factors were used for the Marshall and Bayes formulae to avoid underflow).
From these results it can be seen that the Marshall formula produces the best formula to use for bigrams and the least successful is the simple frequency count of tag transitions.
The effect of the extra tag frequency value, N(T2), in the denominator of Equation 1.3 has the effect of weakening the tendency of high frequency individual tags.
A similar investigation was carried out using the trigram versions of these equations (Table 4.10).
From these values it can be seen that, as with the bigram results, the Marshall equation again produces the best results.
Furthermore, increasing the transition length to deal with trigrams gives better results than the equivalent bigram equation.
Although the increase in performance is relatively small, the difficulty of the recognition task warrants the extra cost — an increase in matrix storage and a slight increase in processing.
Investigation 3
Lexical Probability and Syntactic Ambiguity
The next step in the processing is to investigate the possibility of reducing the influence of low probability words in the lattice and syntactic ambiguity.
Two factors are introduced — a lexical probability factor (LPF) to reflect the recogniser's confidence in the candidate words and a grammatical frequency factor (GFF) to demote rare usage of words.
The Lexical Probability Factor, LPF
The character recogniser for the handwriting analyser (Wright, 1989) produces a ranked list of (up to 6) character candidates for each possible node in the character lattice.
The character that best matches the character database is assigned rank 1, the next best rank 2 etc..
This is best explained by referring again to the character lattice from chapter 1 (fig 4.8) where the ranks are given beneath the character candidates.
A confidence value for each candidate word is derived from the character ranks as follows.
The average rank of the characters that makes up the candidate word is determined.
Hence the average rank of its is 1 ((1+1+1) /3), us is 1.5 ((2+1) /2) and ox is 4.5 ((4+5) /2).
The best possible average is 1 (i.e. all of the characters were top ranked), the worst is 6 (i.e. all of the characters were ranked 6).
These values are then scaled to produce a recognition score for the word in the range 0…1, such that the best ranked word receives a score of 1, and an average rank of 6 would receive a zero confidence score (such occurrences are extremely rare).
This value is referred to as the lexical probability factor (LPF).
It is necessary to employ the LPF since in some cases low confidence words occur in high frequency transitions.
The LPF is used to scale the transitions according to the likelihood of the constituent words.
However, the grammatical score assigned to a word should be independent of it's own LPF (or it would merely serve to reinforce the pattern recogniser's decision) but it should be influenced by the LPFs of words in neighbouring positions.
Also when the assignments from the different modules are combined words having with low LPF will be demoted anyway.
For all but a few types the LOB corpus alone is too small a source from which to reliably derive information about how likely a word is to belong to a particular grammatical category (many words in the LOB occur just once).
Supplementing the information in the corpus with the information in the OALDCE provides a solution for the less frequent words.
That is, the OALDCE is taken as being an addition to the corpus, each word and each of its tags is considered to have occurred once in the corpus.
The GFF is derived from the equation:
That is, the GFF represents the frequency of a tag for a word relative to its most frequent tag.
An alternative equation is:
The disadvantage with this equation for the recognition task is that it would automatically demote any word that is syntactically ambiguous.
The aim of the GFF is simply to demote the rarer occurrences of words.
Given the small size of the LOB corpus and the reliance on the OALDCE to supplement this information the first equation is safer.
The Marshall formulae,(1.3) and (2.3), are modified by taking into account the relative grammatical frequencies of the words involved in each specific transition.
The aim of these is to bias the results in favour of the dominant grammatical categories.
The situation is subtly different for the GFF than the LPF.
The score being assigned by the syntax analyser is the syntactic belief that a word belongs in the input.
Hence if a word is unlikely to behave as a particular tag then it is valid for the syntax analyser to decrease its likelihood in every transition.
The effect of using both the LPF and GFF was also investigated using the equations:
Results
There is no single measure of how well a model has performed.
The number of words correctly selected is a strong indication, but there are some other results that should also be considered.
Due to the relatively small number of tags and the frequency with which some of these tags are found (e.g. over 20% of the words in the lexicon are usually found behaving as common nouns) there is often more than one word given the top rank in any position.
Referring back to Fig 4.2, the syntax analyser is unable to discriminate between the words tail and trio (both having tag 11 in the same position) unless the GFF of these words is different.
Hence, although the syntax analyser may be correctly detecting a high proportion of the words, the actual number of words that are top ranked by the analyser is also an important consideration.
With each equation the average rank given to correct words is better than that given to incorrect words.
The major improvement to be found by including the GFF is the reduction in the number of words selected as the best.
Inclusion of the GFF and LPF into the bigram experiments appears to be less stable than for the trigram experiments — the number of top ranked correct words tends to drop for the bigram cases but remains almost constant for the trigram cases.
The most beneficial case appears to be when the trigrams are used in combination with both factors.
The major advantage gained by inclusion of these factors is the demotion of the incorrect words, rather than the promotion of correct words.
Of course the LPF information is proprietary to the pattern recognition system used.
For a poor recognition system, or simply one in which the text is unsuited to the recognition algorithm employed or character database used.
In this experiment, with this quality of text and recogniser, improvements were found from incorporating this information.
Investigation 4
A Modified Recognition Algorithm
One way in which probabilistic models are employed is to progressively classify the input based on predecessors and move along, using this information as the predecessor for the next element in the input.
For example if one is evaluating a trigram x→y→z, x and y are decided upon, it is the nature of z which is to be classified.
That is, one is classifying z, given x and y.
With something as uncertain as output from a text recogniser, it is also valid to consider that the trigram value stored for x→y→z, also contains information about the likelihood of x given y and z as successors, and the likelihood of y to have x as a predecessor and z as a successor.
The modification to the algorithm assigns values to each element within the transition window, not just to the final element of the transition.
Hence for a trigram window, the final score assigned to each word will be the summation of three recognition scores: the score assigned when the word was in the first position of the window, the score assigned when the word was in the middle of the window and the score when it was the final element of the window.
The effect of this method was tested using a handwritten text and an OCR recognised printed text.
The handwritten text is a 521 word business text (appendix 5) recognised by the ESPRIT-295 handwriting recogniser (Wright, 1989).
The OCR text (appendix 6) is an 804 word text supplied by Hewlett Packard using a developmental OCR recogniser.
In each text the correct word is always present as one of the alternatives.
The transition matrices used were trigram matrices, based on the Marshall formula (equation 3.3) and backing off to bigram information (equation 1.3) when no trigram information was available.
The GFF factor was used in both cases.
The number of correct words assigned each rank by the syntax analyser is shown in table 4.13.
The cumulative number of words assigned the correct ranks for these texts are shown in diagrammatic form in fig 4.9a and 4.9b.
From these results it is clear that the use of the ‘full window’ produces better results than simply assigning a score to the final element of the transition.
The extra constraints that are placed on the input by considering the tag in the context of each of the three positions in the trigram window (i.e. start, middle and end of the trigram) improve the selection of the correct word from the lattice.
Investigation 5
Effect of Recognition Performance on the Syntax Analyser
Such is the difficulty of the text recognition task that often the word that a writer intended does not appear in the list of candidate words that the syntax analyser receives.
The detection and correction of errors is an extremely difficult task in such circumstances, beyond the purview of this thesis.
The aim of this investigation is to determine how well the syntax analyser behaves when the lattice is successively degraded.
In order to isolate the recognition performance from the other variables affecting the text recognition the same written text was taken and ‘corrupted’ to produce lattices of different quality.
The quality, Q, of each lattice is denoted by the percentage of intended words that are found in that lattice.
Hence the best lattice is referred to as Q100 (the correct word was present among each of the candidates) and the worst lattice used is Q50 (only half of the words that were written are found in the lattice).
Eight lattices of the same text have been created of varying quality.
The corruption of the lattice was achieved as follows.
Each word in the lattice has a recognition score.
The target word that the user intended to write at each position is also known and stored in a file.
From knowing the actual word that should have been written at a particular position in the input, a simple NAWK program was written which loaded the target word for each position into an associative array, then went through the Q100 lattice and printed out the recognition score assigned to each target word.
From this a distribution of the word scores assigned to the target words was derived.
A series of threshold recognition scores were then determined from this distribution which would provide the desired lattice qualities.
(i.e. From looking at the distribution it is possible to determine that setting a threshold score of 88 (say) would remove 30% of the target words from the lattice giving a lattice quality of Q70)
A second NAWK program was written that was run a number of times with different threshold values.
This program also loaded in target word for each position into an associative array to allow it to determine which word in a position is the target.
If the target word in the position had a recognition score lower than the threshold then it was omitted.
Words other than the target word in the position were left unaltered, regardless of their recognition score to retain the same degree of branching for the different lattices.
If the target word was the only word selected at a particular position and was below the threshold score then it was replaced by ‘???’— the recogniser's indication that it is unable to recognise any word.
The handwritten 520 word business text was used as input.
A series of degradations were performed on the lattice produced such that the worst quality lattice, Q50, retained just 260 of the correct words as alternatives.
Apart from the successive deletion of correct words from the input, the lattices are identical.
A simple method for comparing the performance of the analyser, given the different lattices, is to compare the ranks given to the (correct) words in the worst quality lattice.
That is, the rank assigned to the 260 target words that are common to each lattice is used as an indication of the performance.
Table 4.14 shows the ranks of the 260 target words that are common to each of the lattices.
The results are shown graphically in Fig 4.10.
As one would expect the best results are obtained when all of the correct words were present in the lattice, with quite a large decrease in performance being found when the lattice quality drops to 89%.
The performance of the analyser remains consistent until the quality is reduced to 50% when there is a drop in the number of top ranked correct words.
Discussion
The consistency of the behaviour of the syntax analyser given seriously degraded input is reassuring.
The quality of a handwriting recogniser can be extremely variable.
If a user is stored in the character database then very high performance can be achieved.
For a novel writer who may possess individual writing characteristics, unknown to the handwriting recogniser, performance can be low.
From the previous experiment, given a text in which half of the correct words are absent, it is still possible for the syntax analyser to select the correct word (if present) in more than 82% of the cases.
There are three main factors that may be responsible for this behaviour.
The first is the localised effect of the syntactic influences investigated (the trigram window only considers three words at a time) enabling the system to quickly recover from any incorrect decisions.
A second reason is the limited number of grammatical categories that are used and the frequency with which very common categories are used.
Hence, even if the correct word was not in the lattice, it is very possible that an incorrect word belonging to the same syntactic category may have been present instead.
Finally there is the behaviour of the pattern recogniser which tends to cluster its incorrect decisions.
If one word is incorrect then it is more likely that the surrounding words are also incorrect, possibly due to incorrect word segmentation.
Whatever the cause of the only slow degradation of performance the robustness of the analyser is impressive.
4.13.
Implementation Details
The syntactic recogniser has been designed to be used as part of a text recognition system.
For a practical application it is necessary to take into account the amount of memory required to run the system and the rate at which processing is carried out.
As part of a larger system the interface between the other modules is also of importance.
The processing for the probabilistic syntactic analyser has been carried out on the same hardware that was used for the rule-based investigations (i.e. a SUN Sparc 4/75 with 48 MBytes of memory).
The system has also been ported to an MS-DOS environment, an operation which would have been impossible for the rule-based system.
4.13.1.
Transition Matrix Storage
When dealing with levels of transitions higher than bigrams storage problems can arise due to the size of the transition matrix.
This is especially true within an MS-DOS environment.
The number of possible elements in a matrix is given by:
Not all of these transitions exist in the corpus.
The actual number of transitions found in the matrices derived from the LOB corpus was shown in table 4.8.
This showed that the bigram matrix is 53% full, whereas less than 5% of the trigram transitions are found in the LOB corpus.
Hence the bigram model is sufficiently non-sparse for full matrix storage to be sensible but the trigram matrix requires some means of sparse storage.
The method that has been used to store the bigram and trigram matrices relies on the fact that if no value for a trigram transition is available then the system backs off to the bigram value.
Fig 4.11 outlines the way in which the bigram and matrices are used in conjunction.
The method used to store the two matrices is as follows.
The bigram matrix is stored as a 1-dimensional array of length (109 * 109), reflecting the number of tags in the tagset.
(This arrangement is used to simplify access to the trigram array as will be explained.)
It is possible to calculate which element corresponds to bigram transition i, j using the formula 
An element of the bigram matrix consists of two fields.
The first is the score for the bigram transition [i, j]; the second is the index in the trigram matrix where the first trigram transition beginning with [i, j]is to be found.
The elements of the trigram matrix also consists of two fields.
The first field is the third index of the trigram (the k if the i, j, k transition).
The second is the value for the trigram, i, j, k.
In order to access the value for a trigram i, j, k the process is as follows.
The array index of the bigram, i, j, transition is determined.
This gives a value for the bigram transition i, j and the index in the trigram array of the first trigram beginning with that bigram.
The next element in the bigram array is also accessed to find the next start index in the trigram array.
This constitutes the stop index for the i, j trigrams.
(This is the reason for defining the bigram matrix as a 1-dimensional array.
It is easier to calculate the stop index without having to worry whether the end of the row in the bigram matrix has been reached.)
The trigram matrix is then accessed at the start index and processed until either:
1
The stop index is reached
or 2
The 3rd index is greater than the value of k.
or 3
The 3rd index is the same as the value of k
If the first or second conditions are satisfied then there is no value stored for the trigram i, j, k.
If the third condition is satisfied then the required value for the trigram is stored in the i—j—k—value at that index.
There is a significant reduction in memory requirements gained from the use of this technique.
Full matrix storage of the trigram matrix requires 2,600 KBytes.
By employing this technique, both the bigram and trigram matrices require only 200 KBytes.
4.13.2.
Interface
The syntactic analyser is only one module of a text recognition system.
Information generated by other modules (specifically the lexical lookup and compound processing modules) is required for the syntactic analyser to function.
Likewise the results generated by the syntactic processor must also be returned to the control process.
An interface structure has been produced which makes this possible.
As the character candidates are received from the pattern recognition process the combinations of characters are checked for validity in the lexicon.
When the possible words in a particular position have been determined, the information about these words is transferred from the lexicon to the interface structure.
The following information about a position is stored in the interface structure:
Both the syntax processor and the semantic processor (Rose, 1991) use a window-based approach.
That is, syntactic and semantic influences are only considered between words that are within a local neighbourhood of each other .
For the current system this neighbourhood is at most five words long.
Hence the syntactic and semantic relations between words further than this distance away in a piece of text are not considered.
The result of this approach is that it is not necessary to retain all of the information stored in the interface for all words in the text.
All that is required is that the information for five positions is retained at any one time.
Hence an array of five words positions is used as the communication structure (cf. fig 4.12).
A count is kept of the element in this array that corresponds to the latest word position.
The array is used as a circular list.
As a new word position is encountered it replaces the oldest information stored 
4.13.3.
Syntactic processing
When a new position is received by the syntax processor the relevant information from the interface structure is copied into a syntax structure.
This structure is similar to the interface structure, information being retained for the five most recently recognised words.
The structure is shown  diagrammatically in fig. 4.13.
As with the communication structure there is header information and a list of information about the candidates in the position.
Unlike the communication structure, the syntax structure is based around the possible grammatical tags rather than the actual words.
Hence the header contains a count of the number of different grammar tags that are possible in that position.
The advantage of copying the relevant information from the communication structure into a separate syntax structure is that it simplifies dealing with duplicate grammar tags in one position.
That is, if the same grammatical tag is found more than once in a position it is necessary only to know that the tag occurs in that position and the best scores associated with that tag.
Rather than continually allocate and reallocate memory as different words are processed, an array is used rather than a dynamic list for storing the information.
The advantage of this is that processing is simpler and more rapid.
The disadvantage are that it is necessary to set a maximum value for the number of different tags are allowed in any one position.
For each grammar tag used in each position the following information is stored:
The grammar tag
The highest LPF (recognition score) of the words having the tag in that position.
The GFF for the best recognised word having that tag in that position.
A window grammatical score.
A cumulative score.
Two grammar scores are kept in the list since the ‘full window’ method is used (as described in investigation 4 above).
The window-grammar score is the best score that is found for the grammar tag within the current window.
The cumulative grammar score is the sum of these window grammar scores as the window moves across the lattice.
4.14.
Discussion
The probabilistic analyser has been developed and tested out.
Favourable results have been obtained with this analyser.
Not only is it able to process any text, whether grammatical or ungrammatical (assuming such a distinction exists) but it is also able to do this processing quickly and efficiently.
Using a SUN Sparc 4/75 computer with 48 MBytes of memory, the syntax analyser requires just 0.4 seconds of CPU time (3.6 seconds real time) to process 521 written words, for which there were on average 10 alternatives for each word.
On the same system, an 808 word text (with 6.5 alternatives for each word) requires just 0.3 seconds of CPU.
This compares with up to 8 minutes CPU time for processing a single sentence using ANLT (see Table 3.3).
The majority of the work associated with the production of a probabilistic model lies in the development stages, with the production of a lexicon and the calculation of transition probabilities for the bigram and trigram transitions.
Unlike many generative systems which are developed from scratch, probabilistic systems tend to utilise existing resources.
The most important resource for probabilistic language processing is a good corpus.
Although corpora are costly to develop, once in existence they provide a valuable resource as evidenced by the number of different applications that utilise corpora.
These applications including corpus tagging, speech recognition, statistical parsing, studies of language development, machine translation, dictionary production and testing of language systems.
One of the most common reasons why NLP systems fail is due to the use of words that are not in the lexicon.
The absences may be either overt or covert (Zernik, 1989).
Overt unknowns are completely absent from the lexicon, for example a word such as Maastricht, despite the frequency of recent use, exists in few lexicons.
Covert unknowns arise from the use of a word in an idiomatic expression causing the word to behave in a different manner from its normal use.
The lexicon that has been developed is based on the vocabulary of a learners dictionary (the OALDCE) and supplemented by words from the LOB corpus.
The aim is to provide adequate, rather than exhaustive coverage of the language.
Complete lexical coverage is a goal that is impossible to achieve.
Nor is complete lexical coverage a goal that one would necessarily want to achieve since such a complete lexicon can cause problems for a recognition system.
As the lexicon storage increases, more processing is required at the lexical processing stage to validate character combinations.
There is a resultant increase in the number of word candidates in the word lattice.
With more words to decide between there is an increase in processing time for the higher level recognition stages.
Detection of recognition errors using lexical processing is made more difficult since there is a greater likelihood that an incorrect character combination will form an acceptable string for such a lexicon.
Furthermore an exhaustive lexicon of words may include words outside a normal user's vocabulary.
The proposal of such words to a writer can lead to disbelief about a word's existence and a lack of confidence in the system.
The development of the compound tree offers numerous possibilities for a text recognition system.
Firstly, the existence of compounds in text can be extremely difficult for higher level processors to deal with adequately for a number of reasons.
The lexical information for the constituent words is probably incorrect in the environment of the compound, leading to errors.
Even if one does know that a compound exists there is no guarantee that the lexical information will be available.
This is certainly the case if one is exploiting sublanguage processing.
Only through manual addition of the syntactic and semantic information can a suitable lexicon be achieved.
The ‘opt out’ method taken by the current system is not ideal but it is preferable for a system to admit it's ignorance than to submit an incorrect suggestion.
There are numerous improvements that can be made to the system.
A larger tagged corpus would allow the creation of a more accurate syntactic model.
The tagset could also be optimised.
The selection of the 109 tagset is rather arbitrary, being chosen simply because it offered the most distinctive tagset that could be formed from combining the Text710 tagset with the LOB tagset.
Investigations into the selection of an optimal tagset would be advantageous.
Such investigations may simply involve iterative selective alterations to the tagset used with monitoring of any improvement or degradation in performance.
Since the lexicon and corpus change each time the tagset is altered this is not a trivial task.
An alternative approach is to let the words decide their own tags.
This approach has been suggested by Atwell (1987) and taken by Finch and Carter (1992) and Brill et al(1990) among others.
The Brill method investigates transitions between words in a corpus.
The less frequent words are replaced by their grammar tag.
The more frequent words are then clustered together based on a similarity metric.
The tagsets produced (i.e. the groups of words) thereby contain not just syntactic but also semantic information.
Very large corpora and extensive processing are necessary to provide suitable information for a large lexicon using this method.
For sublanguage processing this may prove to be extremely useful if a large (raw) corpus is available, since the word classification will be specifically for that sublanguage type.
There will also be few omissions from the lexicon.
4.15.
Conclusions
A probabilistic syntax processor has been developed to assist in the selection of the correct words for a text recognition system.
A lexicon has been created which contains information for approximately seventy thousand words and transition matrices have been created from the LOB corpus.
A method of dealing with compound lexical forms has been developed which combines with the lexicon and allows the recognition of idiomatic phrases within the input.
The use of such a level avoids the syntactic processor from making incorrect syntactic judgements when dealing with idioms and provides an additional recognition stage.
The output produced by the syntax analyser is extremely basic.
No parse structure is produced, instead each candidate word is assigned a score indicating the goodness of fit with its possible neighbours.
There are a number of advantages of this system.
Firstly the system is robust.
No distinction is made between what is grammatical and ungrammatical.
The system simply gives preference to common grammatical combinations over unusual ones.
Secondly because the system is so basic in operation the processing occurs at a rate faster than a human writes.
Hence the system may be used for both dynamic and static recognition.
Because the system makes local optimal decisions rather than on a sentential basis, an incorrect decision early on in the sentence is unlikely to completely ruin the decisions later in the sentence.
With a rule-based approach an incorrect decision can have a devastating effect on the remainder of the analysis.
Chapter 5
Discussion
5.1.
Introduction
The problem of automatic text recognition requires information beyond the pattern recognition level.
Humans, the experts at text recognition, place relatively little importance on pattern recognition information once they become fluent readers.
(Indeed, a handwriting recognition system has been developed which is better at distinguishing between a hand-printed v and u than humans (Suen & Shillman, 1977)) The types of knowledge humans use include lexical, syntactic, semantic and pragmatic information although none of these are used in isolation.
The use of syntactic information for assisting in the selection of the correct word candidates in an automatic text recognition system has been investigated.
Although it can be shown that humans use syntactic information when processing language, the actual process involved has not yet been determined.
Syntax has been one of the most widely investigated areas of linguistics and human processing of language has been intensively studied.
Despite extensive research, how humans achieve their linguistic competence has still to be fully explained.
The problem has been approached from a number of different fields of research, and a large number of different theories have been proposed.
In computational linguistics the main approaches for implementing syntax can be broadly classified as either rule-based or probabilistic.
The use of both types of syntactic analysers has been attempted for helping to select the correct candidate words from the candidates suggested by text recognition systems in this thesis.
5.2.
Generative Systems
The interest in generative grammars over the last thirty years has resulted in a variety of theories for generative processing of language.
The aim of the majority of these systems has been language understanding.
For many of these theories computational systems have been developed using very small lexicons that have been sanitised of ambiguity and dealing with a conveniently simple set of grammatical constructions.
The resultant systems have been able to perform detailed analyses of small fragments of language.
Attempts to scale up these systems beyond their intended domain have met with little success.
One of the main barriers between the development of general purpose systems has been that few existing systems can be re-used.
Even systems sharing the same theoretical background tend to use different methods of representing the lexical information and ruleset.
Given that many of these theories require extremely detailed specifications of grammar rules and lexical entries this has for a long time formed an obstacle to the production of general systems.
In recent years the generative linguistic community has recognised this failing and made efforts to produce more general systems that can form the basis of further research.
A number of projects have been financed to produce re-usable tools.
The ANLT system, which has formed the basis of the rule-based work in this thesis, is one such product.
For many years the production of a lexicon to accompany a parsing system was viewed as a tedious task and most systems provided only illustrative lexicons, containing at most a few hundred words.
This situation has undergone a dramatic change in recent years.
Current linguistic theory places great emphasis on the importance of the lexicon and projects are currently underway to manually (supplemented by corpus processing) produce large lexicons (e.g. GENELEX (Normier & Nossin, 1990) and MULTILEX (McNaught, 1990)).
Perhaps the most significant lexicon under development is that of the EDR project in Japan which is developing bilingual resources for Japanese and English (Uchida, 1990).
The aim of this project is to obtain 200,000 words, term banks for 100,000 words and 400,000 concepts defined in terms of semantic networks.
The amount of work required to obtain this information may be inferred from knowing that this project will run for 9 years and cost 100 million US dollars.
5.3.
Corpus-Based Systems
For a long time the use of probabilistic information in linguistic systems has been frowned upon by the linguistic community.
It's use for tasks such as stylistic analysis was recognised but considered to be of only minor interest since it did not consider the ‘important’ questions, such as how humans are able to process language.
Recently there has been a large resurgence of interest in NLP systems based upon corpora.
A principal reason for this interest is the failure of rule-based linguistics to produce a general purpose, intelligent language processor capable of adequately performing a number of rather basic linguistic tasks (e.g. taking down unrestricted text, non-robotic reading aloud, making a precis of a text)(Sharman, 1990).
In contrast simple probabilistic models have been extremely effective in some speech and language tasks.
The increased importance of computers in daily life has led to an improvement in corpora.
There now exist huge volumes of text available for building corpora.
Once a million word corpus was considered to be a very large corpus, requiring a large amount of work to input the text to the computer followed by tagging and (possibly) annotation.
Current corpora under development aim to include a hundred million words.
Furthermore the availability of corpus tools means that the text in these corpora will also be tagged.
The approach employed by corpus linguists is well expressed by Levinson & Liberman (1981):
‘The best design strategy is not to program a computer directly with the wealth of descriptive detail that constitutes a natural language but rather to give it the basic set of expectations and abilities that are needed to learn a language.’
This self-organising approach to language has been taken by Sharman (1992) who extracted a probabilistic grammar from a set of 2000 hand-parsed sentences.
The resultant grammar contained 3527 basic rules which were converted and extended to a set of 200,000 rules.
Each rule in the grammar was assigned a probability.
The probability of each parse was then calculated and the most probable parse selected.
The grammar was iteratively trained to improve the estimates of the rule probabilities.
Sentences from the associated press (AP) corpus and the Hansard corpus were parsed.
A parse was obtained for each of the sentences.
The AP data was correctly parsed for 73% of cases, the Hansard for 60% of cases.
These results are very encouraging when compared to the performance of more traditional, hand-built, grammars created without the benefit of a corpus to learn from.
5.4.
The Grammatical/ungrammatical Distinction
Central to the debate between generative and probabilistic approaches is the notion of grammaticality.
Generative linguists tend towards the view that language is a well-defined class of sentences.
Any string that is not a member of this class is ungrammatical.
However not all generative linguists take such a precise view of grammar, acknowledging the ability of humans to interpret a wide range of ungrammatical sentences.
Hindle (1983) distinguishes between unusual constructions and true ungrammaticalities.
For example the sentence ‘That's the only thing he does is fight.’
is marginally grammatical, because it is often used although a precise grammar would reject it.
Truly ungrammatical sentences are regarded as not resulting from any regular grammatical process (e.g. ‘I've seen it happen is two girls fight.’).
The occurrence of such constructions is far less frequent.
Labov (1966) reported that less than 2% of everyday speech were ungrammatical in this sense.
Some generative systems have been designed to deal with deviant text.
One approach taken involves the relaxation of constraints in the parsing process.
This technique was used in the Linguistic String Project (Sager, 1981) where a failed parse was re-tried without the agreement constraints on syntactic features.
There is also the option of extending the grammar.
Marsh and Sager (1982) took such an approach with their analysis of a set of medical records written in compact text (i.e. terse form) again using the NYU Linguistic String Project parser.
Within their data they found recurrent ungrammatical constructions whose forms they characterised and included in the parsing grammar.
The justification used for this approach is that ‘repetitive ungrammaticality is grammatical for the text set’.
Another approach ignores syntactic processing and instead concentrates on just using a semantic grammar (e.g. the PLANES system (Waltz, 1978)).
This approach allows very ungrammatical sentences to be processed but loses the advantages that can be obtained by applying syntactic restrictions.
Structural linguists question the existence of a clear-cut distinction between what is grammatical and what is ungrammatical.
Sampson (1987c) contends that there is a continuous gradient from very common to very rare constructions.
In support of this view Sampson provides an analysis of over eight thousand parsed noun phrases from the LOB treebank.
By his analysis almost two thirds of these noun phrase types are represented only once.
The conclusion Sampson draws from this is that it is extremely difficult to determine the boundary between what is grammatical and ungrammatical if such a high proportion of grammatical expansions are very rare.
Taylor et al(1989) dispute the claims made by Sampson.
Instead they believe that it is Sampson's generative grammar formulation that is at fault.
Their analysis provides a much more favourable view of generative grammars using the ANLT grammar system.
Sharman (1989) adopts the same stance as Sampson on this issue, believing that the question of the acceptability of a sentence is not a black-or-white judgement but a gradation of likelihoods.
McCawley (1976) asserts that sentences which are universally judged as grammatical are simply those for which no one has any difficulty in thinking of uses.
This point is illustrated with the ‘sentence’‘Kissinger conjectures poached’ which linguists would class as ill-formed.
However in the context of a reply to the question ‘Does anyone know how President Ford likes his eggs?
Kissinger conjectures poached’ becomes acceptable.
5.5.
Over-generation & Syntactic Ambiguity
Sentences are much more ambiguous than one would normally expect.
Church & Patel's investigation (1982) of this showed that there may be hundreds or even thousands of different parse trees for some very natural English sentences.
For large grammars this is especially the case since the interaction of many rules can cause the system to explode in a combinatorial manner.
Unfortunately it is essential to use a large grammar to achieve a large coverage of the language.
This problem has been borne out by use of the ANLT system.
For example, the sentence ‘I enter orders until the markets close’ resulted in 4,848 different parses being suggested.
A similar problem was encountered by Taylor et al(1989) when analysing the noun phrases from the LOB corpus using the ANLT system.
Their attempt at automatic application of the ANLT parser to the noun phrases failed due to‘inadequacies of grammatical coverage and because of resource limitations with long and multiply-ambiguous NPs’ which resulted in ‘very high numbers of automatically generated parses’.
There are some generative grammars which resolve this ambiguity by use of a metric that ranks alternative parses (e.g. PEG (Jensen 1986)), or by the inclusion of statistical information into the grammar rules (e.g. METAL (Thurmair1990)).
Despite these modifications, syntactic ambiguity remains a large problem for generative systems.
Much of the ambiguity can be avoided by providing a less deep parse.
It is not always necessary to create very detailed parse representations.
Practical applications exist for text processors of limited parsing capability.
One such application involves text-to-speech synthesis.
For this application syntactic processing is required to determine exactly where in the output to correctly specify prosody.
The task does not require detailed syntactic processing, simply to know where to pause (the major syntactic boundaries), which words to stress (distinguishing content and function words) and whether the sentence requires the pitch to fall or rise at the end (is it a yes-no question?).
For a text-to-speech system only one parse can be accepted and failed parses are unacceptable.
Investigation showed that failures and many of the ambiguities resulted from combining phrase groupings at the clause level.
To avoid this the MITalk text-to-speech system (Allen et al, 1987) used a simple phrase-level parser to specify prosody.
In contrast to more detailed parsing systems this required few resources and was able to run in real-time.
A major issue in the decision between a generative and a probabilistic system is the robustness of the system.
There have been generative systems designed to achieve high degrees of robustness (e.g. the FRUMP system (Dejong, 1979)).
For this system the most frequent cause of failure to process the input was cited as ‘missing vocabulary’.
The lexicon system provided with the ANLT is sufficiently large however to be of comparable size to that used in the probabilistic system.
The other issue of robustness is how to deal with sentences that do not conform to the grammar.
Jensen's PEG system ‘is able to make some kind of syntactic sense out of any decent English sentence’.
Marcus (1990b) reports that a group from IBM informally surveyed a number of parsers that claimed to be ‘broad coverage’.
The parsers were tested on a set of short sentences containing less than 14 words.
Of the parsers used, the best was able to correctly identify the best parse for these sentences with only 60% accuracy.
There are a number of textual features that tend to be ignored by generative parsing systems.
The most obvious example of this is punctuation.
Generative linguists consider punctuation to be simply prosody and of relevance for the language — or they may simply believe it to be uninteresting.
Very few generative systems process punctuation at all.
Exceptions to this tend to be the simpler systems such as those used in text to speech applications.
The MITalk system (Allen et al, 1987) is one such system.
This system uses three classes of punctuation to distinguish whether the punctuation is a comma (,), a punctuation mark that is internal to a sentence (i.e.:;() and’) or if it can be a sentence-final punctuation mark (i.e..!?).
Corpus-based systems are easily able to deal with punctuation.
Punctuation marks are classified as separate syntactic categories and grammars and transition matrices based around this assumption.
There are applications for which a generative grammar would be better suited than a probabilistic one.
The most obvious is text generation.
Such a system does not need to have access to a full-coverage grammar and lexicon.
Analysis is much more computationally expensive than generation.
Also, in comparison to knowledge-based approaches a HMM provides little information about the recognition process.
As a result it is often difficult to analyse the errors of a probabilistic system in an effort to improve its performance.
Briscoe (1991) concedes the importance of statistically-based methods for a number of practical applications such as part-of-speech tagging and the derivation of surface collocations.
However he argues that for the development of lexicons there is a need for parsers capable of phrasal analysis, requiring lexicons with reliable information about subcategorisation.
Hence although corpus analysis will play an increasingly important role, it is Briscoe's belief that it will not supplant others or render more theoretical work irrelevant.
Within sublanguage applications a generative approach may be of more use than a probabilistic method.
Lenhert (1991) contains details of a performance evaluation of text-analysis methodologies.
It is difficult to evaluate different systems because they are traditionally idiosyncratic depending on their particular area of specialisation.
To counter this effect, a number of NLP systems were developed to extract information about terrorist incident from a test suite of 100 previously unseen news articles.
Systems were evaluated in terms of how much correct information was extracted, how much of the extracted information was correct and how much of it was superfluous.
They found that text analysis techniques incorporating natural language processing were superior to the traditional information-retrieval techniques based on statistic classifications when the applications require structured representations of the information in the texts.
However the amount of work required to achieve satisfactory performance in this evaluation highlights the problem for such NLP systems.
Each site was allowed six months for system development for the domain in addition to time needed for corpus development.
The corpus that the systems were provided with contained 1300 texts with an average length of 12 sentences.
The preparation of answer keys for 100 texts required between two weeks and a month.
Hence it can be seen that tailoring a generative system to a particular domain can elicit high performance but there is a substantial effort required to tailor a system to a domain.
5.6.
Combining Syntax and Semantics
A central issue for many linguists is how do syntax and semantic information combine in language systems.
 Psycholinguists have analysed how humans combine information and a number of models have resulted.
Marslen-Wilson (1975) proposes an unstructured, fully interactive model of comprehension.
In this model all of the different categories of information interact in an ongoing manner to constrain the processing of a sentence.
Forster (1979) proposed a totally autonomous system in which syntax and semantic information are distinct components of a system with no communication between them.
Rayner et al(1983) propose that semantic and pragmatic information play no part in the initial syntactic choice, however if the syntactic result is ambiguous then semantic and pragmatic influences are used.
From a computational perspective the interaction of syntax and semantics can be important.
This is especially so for rule-based syntactic processing where the use of semantic analysis to remove improbable partial parses could lead to a reduction in the processing requirement.
Rule-based methods offer some method of combining the two together — the grammar is able to provide information to the semantic analysis.
For example, a transitive verb such as ‘find’ is a two-place function requiring a subject and an object.
Therefore given the sentence ‘John found the dog’, a parse may be used to determine the subject and object of the verb and thereby create a semantic representation of who found what.
If the semantic representation cannot be created from the parse structure then the sentence is rejected.
However, there is even less agreement among linguists about how to apply semantics than there is about the use of syntax.
Nor is there currently available a semantic analyser capable of understanding unconstrained English.
One of the most commonly used arguments against the use of probabilistic grammars is that they do not take semantic information into account.
Sharman counters this by pointing out that conventional syntactic grammars do not take semantic information into account either.
A number of probabilistic systems employ word-word associations which Sharman claims is the implicit application of syntactic, semantic and pragmatic knowledge.
The semantic processor in the current text recognition system makes no effort to produce a representation of the meaning of the input.
Rather, the aim of the semantic processor is to give preference to words that are semantically acceptable over those that are not.
For such a task crude approaches to semantic processing are preferable to the detailed analyses found in understanding systems since they are computationally tractable, robust and are not restricted to a small language domain.
The methods used are based on collocations derived from text corpora and information extracted from machine readable dictionaries (Rose & Evett, 1992).
The issue of how to combine syntax and semantic processing in this system has not been completely resolved.
The results from the simple syntactic processor are of little use to the semantic analysis, being simply a rating of the ‘goodness’ of fit of a word given its local syntactic environment.
Similarly, the semantic processor operates in a very similar manner to the syntactic processor and simply provides a semantic rating.
With such limited information there is little to be gained by interleaving the two processors since neither can provide much assistance to the other processor.
Instead the two modules operate in isolation with the results being combined by a control processor.
The current control processor is very crude, using simply a combination of the scores provided by each of the recognition modules (i.e. lexical, compound, syntax and semantic).
A control system is currently under development for combining the output from the different modules in an intelligent manner.
5.7.
Further Work
Statistical processing provides a simple, efficient method for picking the correct words based on local neighbourhood.
However, although words considered in a local context may seem perfectly acceptable in the sentential context these combinations may be unlikely.
As long ago as 1957 Chomsky pointed out the weaknesses of n-gram techniques to provide an English grammar based on their inability to cope with long distance dependencies.
There are a number of possible remedies to this problem.
The first remedy involves the use of larger corpora which will become increasingly available in the next few years.
Portions of the ACL/DCI are currently being distributed with more to follow in what will eventually be a 100 million word collection.
Similarly, the British National Corpus will also be 100 million words of British English and will also be freely available.
Use of such corpora will improve the accuracy of the probabilistic models, allowing transitions beyond the trigram level to be investigated.
Although this will not provide sentential context it will be less localised than the current system.
There is also the possibility of using a simulated annealing approach (Sampson, 1986).
Simulated annealing uses random perturbations to shake the parameter values out of a local optimum so that globally optimal values may be found.
The major drawback being that such methods are computationally expensive.
Another approach that may be useful is a hybrid method.
Sharman et al(1990) suggests the use of a probabilistic ID/LP grammar.
The suggested method is to associate probability scores with the ID and LP rules.
Hence, the fact that a noun phrase usually contains a noun would be indicated in the ruleset.
This is extended so that for any given phrase there is a probability distribution over the objects that can be the immediate constituents.
Similarly there is also a probability distribution over the ordering of items in a phrase.
The probabilities for these rules can be obtained from corpora.
Speed up in the parsing process can then be achieved by eliminating low probability parses.
5.8.
Conclusions
The use of syntactic information as an aid for text recognition systems has been shown to be both feasible and useful.
Both a rule-based system and a probabilistic approach have been explored.
The rule-based approach proved to be too unwieldy and fragile to be of practical use given the current state of computer hardware and linguistic theories.
The probabilistic approach proved to be much better suited to the task.
The main conclusion is that, for the text recognition task probabilistic methods of syntactic analysis are more useful than rule-based ones.
The ease with which a probabilistic analyser can be created contrasts sharply with the immense amount of work required to create a comprehensive generative grammar.
The resulting analysis is also an order of magnitude faster for processing and does not suffer from the brittleness of a generative analysis.
When successful the ANLT system produce very detailed structural information for the input.
However the ANLT system is very fragile, computationally demanding and unable to deal with much naturally occurring language.
For a computational product the processing requirements are an important factor in the choice of system.
For a handwriting recognition system it is obviously desirable that a recognised word be displayed soon after it is recognised.
Rule-based parsers tend to work on a sentential basis which appears to be too much delay for users.
(Just how soon after writing a user should sees the recognised text is open to debate.
If writing on an electronic tablet, which combines the input and display devices, the user may find it disconcerting to see the handwriting changing under their pen.)
A straight comparison between the generative and the probabilistic approach is unfair for a number of reasons.
Firstly the probabilistic analyser was specifically constructed for the task whereas the rule-based analyser was simply a modification of an existing system.
Rule-based analysers are designed for certain(typed) input and the production of the parse tree for the input.
The recognition task is substantially different.
There is a much larger search space to be explored than for certain input.
The resultant parse is of only minor interest, being of use only to the semantic component in reaching its decision.
A syntactic recogniser rather than a parser would almost as useful and less computationally demanding.
Once one valid parse had been obtained the parsing process could cease.
However, in order to reach this stage it is still necessary to perform a large amount of parsing.
With the recognition problem not only are there a number of possible word combinations, the majority of the possible combinations are to be rejected.
The constraints applied by the rule-based methods are too rigorous.
It is not to be expected that each sentence written will obey grammar rules.
The search techniques to reject sentences need to be highly efficient so that invalid combinations can be quickly rejected.
There are points in favour of the rule-based method.
The generative system is able to operate on a sentential level rather than simply consider local word combinations.
Also, when successful, the rule-based system produced a much deeper analysis of the input than the probabilistic approach.
It must also be taken into consideration that the generative system is being used for an application alien to its originally design.
Nor were any attempts made to supplement the grammar or lexicon.
However it is unlikely that a set of rules to explain how general language is processed will be available in the foreseeable future.
One reason for this is the interaction of the various linguistic processes involved in processing language.
Syntax does not occur in isolation, there is also semantic, pragmatic and encyclopaedic knowledge contributing to the analysis.
The specification of each of these processors as well as a control system to combine them intelligently is currently far beyond any expectations.
Given the currently available linguistic theories and computers the only feasible way to build a system capable of dealing with unconstrained language is through corpus-based methods.
In the short term this brute force approach appears to be the best method of incorporating linguistic knowledge into computers.
The success with probabilistic methods is similar to that being found in other areas of NLP.
Specifically in the area of machine translation the lack of success with linguistic-based second generation methods is noticeable, only one of the commercially available MT systems uses generative techniques.
The failure of the rule-based systems has led some researchers to talk about third generation machines and others to return to the more basic systems from the 1960's.
However, the choice of rule based or corpus based methods is still contentious.
A number of considerations need to be made, and it is possible that a combined method may be preferable.
The final selection of which approach to take for the practical application of apply linguistic information is dependent on a number of factors.