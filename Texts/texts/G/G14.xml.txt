

THE EVOLUTION OF MEMORY
It is a common enough linguistic trick — and one of which neurobiologists are themselves often guilty — to speak as if there were some sort of evolutionary scale or ladder of complexity, along which all the living forms found on earth today can be arrayed to form a series of ‘more evolved’ and ‘less evolved’organisms.
An even more extreme form of this way of thinking is to assume that there has been some type of directive force in evolution such that humans represent the ‘pinnacle’ of progress.
Such ideas, which derive from views about the place of humanity in nature that long pre-date Darwin and the birth of modern biology, very much misunderstand evolutionary theory.
A short evolutionary sermon is therefore necessary before the argument can run on.
All of today's living organisms derive from primitive forms of life which first appeared on earth not long after the birth of the planet itself, perhaps some four billion years ago.
Evolution, which literally means unrolling, or development, is, for biologists, the process by which there has occurred a steady change in the form of organisms across generations.
By form here is meant anything from their biochemistry and internal structure to their behaviour.
These changes in form thus occur at a multitude of levels, from the molecular to that of an entire population, and are conserved by genetic transmission.
When the change of form  becomes too great for the old and the new form to interbreed successfully, one is entitled to say that a new species has been generated.
The most generally accepted mechanism of evolutionary change is the modern version of Darwinian natural selection, based on the simple propositions that (a) like begets like, though with minor, essentially chance, variations;(b) all organisms are capable of producing more offspring than actually can survive to maturity and reproduce in their turn;(c) those offspring that do survive to reproduce must in some way be variants that are better adapted to their environment than those that fail; and (d) those favoured variants are likely to reproduce the favourable variation in their own offspring.
Thus better-adapted forms will tend to replace less well-adapted forms.
Organisms can only adapt to their presently existing environments; they cannot predict future ones.
The fact that future generations may find themselves living in a warmer planet cannot be built into today's selection processes.
Thus evolutionary change in a population of organisms can only track and respond to the changes in the environment, continually being wise after the event.
Evolution has no foresight.
Yet environments are never static; they are always changing.
Physical, non-biological forces modify climates, raise mountains and sink seas.
And living organisms themselves constantly change their environment, altering its chemical composition by eating, breathing, excreting; altering its geography by building and destroying (this relatively obvious concept has been raised to almost metaphysical status by James Lovelock and his devotees as the grandiosely named Gaia hypothesis).
Above all, living organisms are part of one another 's environment, as sexual partner, parent, offspring, prey or predator.
And finally, organisms are not passive recipients of their environments; they can — animals to a greater extent than plants — choose their environment by moving from a less attractive to a more attractive location.
Thus, as in development, the interplay between organism and environment during evolution is not a one-way process, by which environments constantly set challenges to  organisms which they either pass (reproduce successfully) or fail (die out).
Organism and environment are dialectically interlocked.
While Darwinian processes are likely to be only one of several mechanisms responsible for evolutionary change in form (there is much debate, which need not concern us here, about the relative contribution of these other mechanisms), the point for the present is that all forms of life on earth today are clearly the results of comparable evolutionary pressures over the whole of geological time.
It is thus wrong to assume that any one form can be ‘more evolved’ or ‘better-adapted’than any other.
To be here today by definition means to have survived, whether as dung beetle, mushroom, or human.
There are many ways of surviving to produce offspring, and there is little consensus between organisms about what counts as evolutionary ‘success’ in this sense.
Only a very biased, brain's-eye view of the living world would assume that the bigger the brain the more successful the species.
In sheer bulk of biomass, organisms without brains or even without central nervous systems far outnumber those possessing these desirable features.
Even amongst those organisms with well-coordinated nervous systems, a good case can be made for the dominance of beetles over mammals.
Nor are evolutionary processes progressive or purposive; the primitive single-celled creatures which were our early forebears were not possessed of a burning imperative, or driven by a mystic higher force, to evolve into sentient humans.
True, there are sharp constraints on what can evolve and what cannot, given by the physical properties of the planet we inhabit and the carbon chemistry on which our molecules are based.
But within these constraints, chance events, the contingencies of particular organisms present at particular times in particular environments, provided the push towards the living forms we are today; there was no master plan.
Studying the fossil record enables us to show what must have happened, but, as Stephen J. Gould has been the most recent to argue, were we to be able to wind the clock of evolution backwards towards the origin of life on earth, and then run it forward once more, quite different outcomes would most likely have occurred; not only would there probably have been no humans, but perhaps not even any brains.
The study of evolution  is the study of history, not of the working out of some mathematical programme of progress towards complexity or a Teilhardian urge to evolutionary perfection.
In that historical sequence, some organisms, such as humans, have appeared only very recently, and their past ancestry seems to have been one of rapid and quite dramatic change.
Other forms of life on earth today seem much more closely to resemble what one knows, based on the fossil record, to have been their evolutionary ancestors of many million years past.
It is as if , once they had found an effective way of surviving and reproducing, and a relatively stable environment in which to go about their business, there was little pressure to change their way of life.
By studying their biology and behaviour, therefore, it becomes possible to draw reasonably probable conclusions about how their — and thus our own — ancestors lived.
If one goes far enough back in evolutionary time, all present-day living organisms share such a common ancestor — or group of ancestors.
Along the evolutionary path that led to humans there were, three million or more years ago, creatures which were also the ancestors of today's apes; earlier, those who were the ancestors of all modern mammals; earlier still, of all today's vertebrates; yet earlier, of all multicellular animals; and so on.
Rather than drawing some evolutionary ladder or tree, the best representation is a sort of multi-twigged bush.
Because the only records of the ancestral forms are their fossils, without any soft tissues like brains and nerves, the brain structures and behaviour of such ancestors can only be inferred by studying their present-day descendants.
From this comparative psychological approach one can begin, however, to draw some conclusions about the evolution of nervous systems, of brains, behaviour and eventually of memory.
THE ORIGINS OF BEHAVIOUR
Although all organisms show adaptive behaviour, for it is a necessary condition of existence, many highly successful life forms have managed very well without the capacities to learn and remember — even without brains.
Think of a sunflower turning its flower head towards a source of light — and therefore of energy.
The sunflower does not learn by experience to turn its head more effectively as it matures, or not to turn at all if it is repeatedly electrically shocked every time it does so.
The flower-head turning behaviour, adaptive or not, is a ‘given’ property of the organism, fixed within its genetic and developmental programme.
Such a given behaviour (innate is the conventional term, but I prefer to avoid it if possible, for it carries along with it a load of redundant ideological baggage) ensures that appropriate responses are made to particular stimuli without the need for trial-and-error learning, but at the expense of limits to both the range and the flexibility of the response.
Changes in behavioural responses occur not in an individual but over many generations as a consequence of evolution.
By contrast, learned behaviour requires experience and practice.
The advantage is that responses are highly flexible and can be modified in response to environmental changes and perceived outcomes within the lifetime of the organism.
However, it is crucial to recognize that such learned responses are not genetically transmitted to offspring.
What is transmitted is the plasticity and capacity to learn which are themselves ‘given’ by the genetic and developmental programmes of the organism.
This is why it is important to reject tired old debates about nature versus nurture.
To understand brain and behaviour means rejecting that dichotomy and instead trying to interpret the intertwined dialectic of specificity and plasticity.
A plant's mode of existence is shaped by the fact that it can survive, biochemically, by making use of the sun's energy to trap carbon dioxide and convert it to the foodstuffs, such as sugar, that it requires.
All it needs to do therefore is to stay with its arms out in one place, photosynthesize, and avoid being eaten until it has managed to reproduce.
(The price it pays is the need to develop quite elaborate and inherently uncertain reproductive methods, making use of other organisms or natural forces such as wind to spread pollen and seeds.)
Animals cannot make a living so simply,; not being able to photosynthesize, they have to hunt for prepackaged  food, either directly by eating plants or indirectly by eating the animals that have eaten the plants.
Thus they need to be continually on the move, to develop specialized ways of finding food sources, avoiding danger and locating mates.
For non-photosynthesizers, the premium is therefore on an increasing range and flexibility of adaptive behaviour, on developing sense organs that can detect food or predator at a distance and motor skills to move towards the desired — or away from the desiring.
SINGLE CELLS
Even the simplest, single-celled organisms can show examples of this type of behaviour.
As long ago as the 1880s, W. Pfeiffer, in Germany, dipped a thin capillary tube containing glucose into a drop of liquid containing bacteria, and observed that the bacteria tended to collect at the mouth of the capillary.
The bacteria were behaving as if they ‘knew’ the glucose was there and were responding adaptively.
The explanation is relatively straightforward; the glucose slowly diffuses out of the capillary into the surrounding liquid, creating a gradient of sugar concentration.
The bacteria have, embedded in their cell membranes, receptor molecules — that is, proteins whose molecular architecture enables them to recognize the glucose molecules — and tiny whip-like projections (flagella) which, by beating in unison, can row the bacteria up the glucose gradient.
Mutants which lack the glucose receptors fail to show such behaviour, even though they will continue to make use of the glucose as an energy source if placed directly in contact with it.
More complex single-celled animals, such as paramoecium, have similar projections, called cilia, connected at their bases by a system of fine threads, which enable the beat of the cilia to be coordinated.
Except when it is actually feeding (it eats bacteria), the paramoecium is in constant motion, frequently bumping into obstacles in its path and, when it does so, reversing by switching the beat of the cilia on one side.
It thus steers itself like a toy motor car, peregrinating around obstacles and into food-rich areas.
As well as physical obstacles it will move away from regions of heat or cold or irritating substances such as acids.
These goal-seeking and avoiding reactions represent, within an organism no bigger than 0.2 millimetres in length and with no sort of nervous system, the beginnings of a pretty sophisticated range of active behaviours.
MULTICELLULARITY
Single-celled organisms have to be self-sufficient, packing all their behavioural repertoire into a tiny compass.
But at least communication between one part of the cell and the other is not a major problem.
With the evolution of multicellular organisms, both the range of possible behaviours and the organizational problems the organism has to solve increase.
There is a need for some form of rapid internal signalling system so that the activities of cells in different parts of the same creature can be coordinated.
It seems likely that in very primitive multicellular forms the main mode of communication was chemical — a substance released by one cell, say signalling for the cell to contract, could fairly quickly diffuse to other cells, ensuring that they too contracted.
Such chemical signals would have been the forerunner of modern-day hormones.
One of the most intriguing of evolutionary clues is the close chemical similarity between many hormones and the substances that function within the nervous system as neurotransmitters, suggesting that perhaps the second group, the neurotransmitters, may have developed evolutionarily from the first.
Multicellularity makes possible specialization.
Different functions and properties can be distributed between different cells; some for instance become contractile, whilst others specialize in synthesizing and secreting the chemical signals.
Yet others, on the surface of the organism, concentrate receptors responsive to particular chemicals, like the bacterial glucose receptors, or even to light.
Signalling by diffusion can work well for small organisms, but it is limited; diffusion of chemical signals over long distances is slow and inefficient, as it cannot be specifically targeted to ensure that the signal arrives only at the cell it was intended for.
If on the other hand the signalling cells grew into such a shape that they could come into direct contact with their target organs the chemical signal could be discharged directly at site across the ‘synaptic’ gap between the cells.
This would ensure directionality, but it would still leave the problem of getting the message from one end of the signalling cell to the other over what would now be a relatively long distance.
Here, the electrical properties of cells must have become important.
It is a universal property of living cells that their external cell membranes are electrically charged.
This charge, called the membrane potential, comes about because cells contain  large numbers of dissolved salts in their internal fluid (the cytoplasm), including sodium, potassium, calcium, chloride and others.
These salts, in solution, form electrically charged ions (for instance sodium chloride, NaCl, forms positively charged sodium, Na=, and negatively charged chloride, Cl-).
However, the inside of the cell differs from the outside in that there is a high internal concentration of potassium and a low concentration of sodium.
Inside the cell as well are proteins, whose constituent amino acids are also electrically charged.
The ionic composition of the inside of the cell is thus different from that outside, and as a result the inside of the cell is some 70 millivolts negative with respect to the outside.
Nerve cells, neurons, resemble all other types of cells in possessing such a resting membrane potential.
Where they differ is in the unique properties of their cell membranes, for the nerve cell membrane is excitable — which means that in response to a signal, such as a small local fluctuation in ion concentration across the membrane, it can rapidly become permeable to the ions outside it.
Sodium ions enter and the membrane becomes depolarized, turning from 70 millivolts negative to as much as 40 millivolts positive.
This change results in a wave of electrical activity passing down the nerve cell membrane — a wave called an action potential that in a few milliseconds passes from the cell body along the axon to the synapse.
The action potential serves as a signal in its turn for the synapse to release neurotransmitters which trigger the response in adjacent neurons.
The evolution of cells with action potentials and chemical signalling processes at their termination points may have provided the basis for the evolution of a modern-day nervous system.
One example of an organism possessed of such a rather basic nervous system is the tiny, pond-living hydra (Fig 7.3), which sits at the bottom of ponds and streams attached to rocks or water plants and waving its tentacles above its mouth.
Like a sea-anemone it will close down to a blob if touched, and it feeds on small organisms which brush past its tentacles, by shooting out special poisonous threads to paralyse its victim, which can then be collected by the tentacles and drawn into the hydra's mouth.
This complex behaviour requires mechanisms to register the presence of prey or danger and to decide on and make the appropriate response, attacking or contracting into a blob — sensory cells, secretory cells, muscle cells and above all a network of electrically connected cells running right across its surface which can coordinate the hydra's responses.
The individual cells of this network are not quite like neurons in  more complex animals, though, for the network lacks specificity or directionality.
Stimulate the hydra at any point on its body and a wave of excitation runs from that point in all directions so as eventually to involve every part of the nerve net.
The hydra's nervous system is like a telephone exchange whereby whatever number you dial you are eventually connected to all other subscribers.
By contrast the key feature of a fully developed nervous system is its specificity, the precise set of connections by which a signal beginning at a particular sensory cell runs in a defined route, ending in some effector cell, a private line, essentially insulated from the multitude of other neurons within the system.
PRIVATE LINES AND NERVOUS SYSTEMS
Private lines of this sort, and therefore real nervous systems, appear in rather more complex organisms than hydra, the planaria, or flatworms.
Put a piece of raw meat into a stream and within a few hours it will be covered with small, flat, black worms feeding on it.
These are planaria.
Unlike hydra, they have clearly defined   head and tail ends, and a much more elaborate behavioural repertoire.
Most importantly, they have a specific set of connections between their neurons, so that if a cut is made in a set of nerves leading to a muscle, that specific muscle becomes paralysed.
What is more, whereas in hydra the proto-nerve cells are more or less evenly distributed throughout the animal's body, in planaria the distribution of cells is not symmetrical.
Neurons are concentrated in clusters with short interconnecting axons and dendrites between the cells of the group and defined nerve tracts leading in and out.
Within each cluster there are some cells which receive inputs from the incoming nerves, some from which the outgoing nerves run, and others (interneurons) which connect inputs and outputs.
These clusters of neurons (called ganglia) thus contain all the essential elements of a complete nervous system (Figure 7.4).
Planaria, because they have a head and a tail end, also have a clear sense of direction, of going forward and going backward, which is not apparent in hydra.
It is obviously advantageous for an animal to receive more detailed information about where it is going to than about where it has come from, and it is therefore not surprising that as well as the mouth at the front end of the planaria there is a   concentration of sense organs, such as light-sensitive eyepits, and to process the information arriving from these sense organs there is a group of ganglia concentrated in the head — forming at last the forerunners of real brains.
This much more elaborated nervous system is associated with a much greater range of what we can recognizably call adaptive behaviour.
planaria tend to shun the light and move towards the dark, they are sensitive to touch and keep the undersides of their body in contact with solid objects, move towards food, and tend to move upstream in water currents.
HABITUATION AND SENSITIZATION
They also show another form of behaviour.
Touch a planarian firmly with a glass rod and it will curl itself into a ball — its normal response to a threatening situation.
After a few minutes it will slowly and cautiously uncurl itself again.
Touch it again, and it will curl up again, and then stretch out once more.
But repeat the procedure often enough and the response will diminish and eventually the animal will no longer curl itself into a ball when touched — as if it had become accustomed to the stimulus and no longer regarded it as dangerous.
This phenomenon, in which an animal responds to a repeated stimulus by eventually disregarding it, is familiar to everyone.
As one dresses in the morning one is very conscious of the feel of the clothes on one's skin, but after a fairly short time one ceases to be aware of the sensation.
This is called habituation : the nervous system has become familiarized with the particular set of stimuli which are the pressure of the fabric of the clothes on touch receptors in the skin, or of the glass rod on the planarian's body; the stimuli can thus be disregarded, no longer a factor to be taken into consideration in assessing the current state of the environment.
However, if the planarian is left undisturbed for a period, the original response returns.
This could be regarded as nothing more than fatigue and recovery from fatigue, but it is not.
If during the period of habituation the touch of the glass rod is coupled with some other stimulus — say a bright light — the original response at once returns in full, so it cannot be merely because of exhaustion that it disappeared in the first place.
The recovery is thus itself an active process; the system has become dishabituated .
Habituation and dishabituation, which thus fulfil the criteria for the definitions of learning given at the beginning of Chapter 6, can be regarded as very basic and simple forms of short-term memory, adaptive mechanisms  which economize on unnecessary responses and hence help to avoid fatigue.
In general, habituation and dishabituation follow certain clear rules: the more rapid the frequency of stimulation, the stronger the habituation; the stronger the stimulus, the weaker the habituation.
Repetition of the same stimulus following a sequence of habituation and dishabituation produces a more rapid habituation on successive occasions.
Planaria also show another form of short-term learning, called sensitization , which is in some ways the antithesis of habituation.
Where habituation is the weakening of a response, sensitization is the strengthening of a response to a particular weak stimulus if that stimulus is coupled with an unpleasant one — an electric shock, say.
Give planaria a mild electric shock and they will respond by curling up; touch them very gently with a rod, or squirt a mild jet of water at them, and they will not show any response at all.
However, if the gentle touch or water squirt is always coupled to the electric shock, the planaria will eventually respond to the otherwise unnoticed stimulus itself.
Sensitization is a generalized process, for after such a shock the animal's responses are not specific to the touch or the water squirt; instead reactions to a wide variety of mild stimuli all become exaggerated.
Thus sensitization lacks the specificity which is the hallmark of truly associative learning in which a particular pairing of stimuli is achieved.
In practice sensitization and dishabituation are rather closely related processes.
Habituation and sensitization — often grouped together as non-associative learning — can be seen as the first steps towards full-fledged memory, and are universal properties of all organisms with nervous systems.
Indeed, there is some evidence that they occur in creatures without proper nervous systems at all— even paramoecium.
But ‘real’ memory must last for a longer time than such short-term effects as habituation and sensitization, and must show greater specificity.
The key property should be the ability to form associative — that is, specific — memories.
Whether planaria can carry out these more complex forms of learning is much in dispute, as will become apparent in the next chapter.
Nonetheless, with the evolution of planaria-like organisms appeared both the rudimentary forms of a nervous system and the basic behavioural building blocks out of which fully developed memory processes are eventually fashioned.
Discovering just how much creatures with nervous systems of this degree of complexity can remember, and whether they can meet the rigorous criteria laid down by association psychologists as to behaviour to be counted as learning, classical or operant conditioning, becomes a matter of the ingenuity of the experimenter in designing appropriate, biologically relevant tasks.
The capacity, in principle, of such nervous systems to demonstrate learning and memory cannot be in doubt.
BIGGER BRAINS, BIGGER MEMORIES
The concentration of neurons in ganglia is perhaps the first step towards building a brain, but even the appearance of a large head ganglion does not ensure that brains of the sort that humans and other mammals possess are the only design solution that can result.
In the nervous systems of arthropods, for example— including insects and crustacea — the head ganglion — the brain — may be important, but is so only in the sense of King John amongst the barons; there are many other ganglia distributed throughout the arthropod body, each with a considerable degree of autonomy.
Consider, for instance, the ability of the male praying mantis to continue copulating whilst the female steadily devours him from the head end downwards, or for the head end of a wasp to continue eating even when severed from the abdomen.
This independence means that, even without their head ganglia, insects can show some behaviour which could be called learning.
In the 1960s, Gerald Kerkut, in Southampton, described a series of experiments in which he suspended a headless cockroach above a bath containing a salt solution.
The normal tendency of such a ‘preparation’(biologists' speak for an animal to which they have done something nasty, akin to the use of the term ‘sacrifice, that I commented on earlier) is to extend its legs into the liquid bath.
Kerkut arranged it such that whenever the leg touched the liquid an electrical circuit was completed and the cockroach was shocked,; when it withdrew its leg from the liquid the circuit was broken and the shock ceased.
The residual cockroach, even without its head, eventually ceased putting its leg into the liquid — it had ‘learned’ how to avoid being shocked.
All animals more than a few thousand cells big need to have some sort of scaffolding to keep their bodies in shape.
Vertebrates do it by means of a backbone and internal skeleton, arthropods achieve structural rigidity by means of a tough external skeleton or shell.
This produces a fundamental design limitation on the size to which an insect or crustacean can grow (for all the ingenuity of fishermen, breeders and genetic engineers, with however enthusiastic support from Wheelers, Legal Seafoods or Hammer Films, a lobster the size even of a small dog isn't on; it would implode under its own weight).
Such a design also strictly limits the size of ganglia and brains.
Such a limitation has not obviously affected the evolutionary ‘success’ of this type of body pattern, as witnessed by the vast number of species of insects and crustacea alive and flourishing today — but it certainly affects the extent of their behavioural repertoire.
To a certain extent this limit is transcended in social insects — bees or ants, for example— which live in a highly organized and cooperative community.
Some have indeed argued that such a community can be regarded as a sort of superorganism, which adapts to the environment, regulates its numbers, stores information and behaves in a strongly cognitive manner.
When bees swarm, for instance, which happens when any given colony gets too big, a new queen is produced and departs the hive with a group of followers.
When a suitable site for a new hive is found the bees have to learn its location and get rid of their earlier learned behaviour of flying back to the old hive.
They can not merely learn and keep maps in their head of the location of hives and sources of nectar, and recognize colours and patterns; they can communicate such orientations, distances and directions to their colleagues by means of the famous ‘waggle dance’.
Even before Karl von Frisch's well-known studies on bee orientation and his interpretation of the waggle dance in the 1910s and 1920s, students of bee behaviour, from fable-writers and apiary-keepers to entomologists, had marvelled at the skills, apparent intelligence and capacity to learn of the bees, which seemed very far from being rigidly programmed robots.
Bees do have, however, a very tightly defined repertoire of skills and learning ability.
Thus although they can learn colours as signals for food and as markers for the entrance to their hive, they have difficulty with other types of colour learning, such as using colour as a cue to finding their way out of a closed space.
The extent of these learning skills has been ingeniously studied for individual bees by Randolf Menzel in Berlin over the past two decades, and he has been able to clarify the role of many regions of the bee's head ganglion.
It remains extraordinary, though, that with a brain of only 950,000 neurons — less than a thousandth of those in the human retina — bees can learn colours, textures and smells as well as motor skills, and when set appropriate tasks can show most of the features of conditioning, associative and non-associative learning and relatively  long-lasting memory found by mammalian psychologists in organisms with many-fold larger brains.
But perhaps if they had listened to what folk-legend had maintained for thousands of years neurobiologists would not have been so surprised at the learning and memory capacity of the bees.
It may come as a greater surprise to discover that much humbler, less social insects than bees can also show features of learning and memory.
For the bulk of this century a favoured organism for geneticists to study, because of the ease with which it can be maintained, its rapid breeding cycle and the possibility of studying populations of many thousands, has been the tiny fruit fly (sometimes called vinegar fly),Drosophila melanogaster , which gathers like specks of coaldust, seemingly magnetically attracted to over-ripe fruit.
Exposing fruit flies to X-rays or certain chemicals produces mutations, many of them lethal, as a result of which the flies cannot survive at all.
Some mutations, however, permit the fly to survive, although in an altered and normally less adaptive state.
Study of these mutations, for example in the colour of the eyes, the patterning of the wing veins, the number of bristles on the abdomen and many other minor variants, has provided vital clues to the mechanisms of genetic inheritance and the control of development in the fly and, by generalization, in other life forms too.
It wasn't until the late 1960s and early 1970s, however, that one of the most experienced of fly geneticists, Seymour Benzer, and his students, notably Chip Quinn at Yale and Yadin Dudai, now in Jerusalem, began studying Drosophila behaviour in some detail as the opening shot in a programme to try to identify mutants with abnormal behaviour and, in particular, abnormal capacities to show forms of learning and memory.
The reasons for this hunt will become clearer in the next chapters; for now the point is that even flies, which might traditionally have been regarded as stupid, with tiny brains of no more than 20,000 neurons, turned out to be able, under the appropriate circumstances, to learn and remember.
The way to teach them is to work with the behaviours they show naturally; like moths, they are for instance attracted by light, but find the fruit on which they feed primarily by smell.
The first reasonably reliable and convincing learning task for Drosophila involved training them using just this sense of smell.
Groups of flies were attracted by light into a test-tube containing one of two differently smelling substances.
When they entered the mouth of the tube containing one of the odorants, they were shocked.
The flies were then tested by being offered the choice of the two odours, and the proportion avoiding the shock-associated odour was compared with that avoiding the control odour.
In experiments of this sort, which provide a sort of population index of learning, about two thirds of the normal (what geneticists call ‘wild-type’) flies avoid the shock-associated odour, and only one third avoid the control odour.
From this and other types of experiment, it is beyond dispute that, even by the most rigid of the criteria used by mammalian psychologists,Drosophila show not merely habituation and sensitization but classical and operant conditioning based on visual, olfactory and even touch cues.
SLUGS AND OTHER MOLLUSCS
In terms of having bigger brains, arthropods are, as we have seen, limited by their external skeletons.
A way out of this design limitation is offered by molluscs, whose best-known land-living forms include slugs and snails.
In water, shell-less molluscs can grow to large sizes, and include squid and octopus.
Like those of arthropods, the molluscan nervous systems are organized in a series of ganglia, mainly distributed around the animal's gut.
In the 1930s and 1940s research on one large mollusc, the squid, revealed that it had truly giant nerve axons, which could be dissected out individually and were big enough to insert electrodes into.
A favoured saying amongst biologists is that for any biological question god has created an ideal organism in which to study it.
The squid axon certainly conformed to that belief, and became the preparation in which details of the ion movements and changes in electrical activity which occur during nerve conduction and the action potential could be explored.
One of the group of researchers who first explored the merits of the squid giant axon was John Zacharay Young, of University College, London.
Young's lifetime passion for the large molluscs led him from the squid to the octopus.
Octopodes offer no great advantages for the study of nerve transmission, but they do have relatively large brains.
Working at the Marine Station in Naples in the 1950s and 1960s, Young began to study octopus behaviour, and especially their capacity to learn and remember.
Octopus feed on small crustacea, and Young designed experiments in which the octopus would be shown a large black or white shape — say a cross — at the same time as it was offered a small crab.
If the cross was black, the octopus would be shocked as it touched the crab; if white, it  would not.
The octopus, he discovered, could learn to distinguish such shapes and patterns and avoid those coupled with the unpleasant experience.
They could learn by sight, and also through their remarkably sensitive tentacles, to distinguish rough from smooth, heavy from light cylindrical shapes.
The brain regions responsible for storing the memory could be located to one of the major lobes into which the octopus brain was divided.
But there, essentially, the search ended.
The octopus brain is a mass of small neurons and surrounding cells, whose connections are not well understood and the mapping of which will require as many lifetimes of research as has gone into mammalian studies in the last century.
Beautiful as the creature is, and instructive as the study of its behaviour has been, the octopus did not prove to be god's organism for the elucidation of memory.
It required the next generation of researchers and a simpler mollusc to come closer to finding god's favourite.
In the 1940s, Angélique Arvanitaki, and in the 1950s and 1960s Ladislav Tauc, in Paris, began studies of the sea mollusc Aplysia , a slug-like hermaphrodite creature which lives on the sea floor close to the beach and grazes on seaweed.
Aplysia can grow up to about 30 centimetres long and weigh a couple of kilos, and of the several species the biggest and experimentally most popular (I don't say the best) comes from California —Aplysia californica .
Its central nervous system consists of a few ganglia, no more than about 20,000 neurons in all.
Four of these ganglia are arranged in a ring round the gut, interconnecting with a big abdominal ganglion by large nerve tracts.
The great experimental merit of Aplysia , by contrast with Drosophila , which has as many neurons, or the octopus, which has far more, is that many of the Aplysia neurons are very large — up to a millimetre or so in diameter — and they are located in characteristic and recognizable patterns, which are reproducible from animal to animal.
This means that the ‘same’ cell can be studied in preparation after preparation, its connections and the effects of stimulating or excising it followed in detail — something that is quite impossible in any of the organisms yet discussed in this chapter (Figure 7.5).
The advantages that this property conferred on Aplysia for those studying neurons is as great as that of the squid giant axon for the study of the action potential, and was early recognized by Arvanitaki, Tauc and their collaborators.
It has, however, been Eric Kandel, initially jointly with Tauc in Paris and later in New York, who over the past quarter century has made the study of  Aplysia learning and memory so especially his own.
Much of this work will be discussed in Chapter 9.
The question here, however, is one that occupied him and his colleagues for many years of sometimes acrimonious debate with mammalian psychologists:.
can Aplysia learn?
It was relatively easy to show that the animal could habituate and show sensitization, and much work was put into the study of a particular set of reflexes, by which, if it is touched, the Aplysia withdraws its breathing organs, the gill and siphon, which normally stick out from its body surface, into its body cavity.
Repetitive touching of the body surface results in a decrease in the amplitude and probability of withdrawal of the gill and siphon, a decrease which can persist for weeks.
This may be a form of habituation, but it is very long-lasting and thus can reasonably be regarded as at least a form of non-associative learning.
It wasn't until the early 1980s, though, that convincing evidence for classical   conditioning of the gill and siphon withdrawal reflexes could be obtained.
In these experiments the unconditioned stimulus was a strong shock to the tail, which produces a strong gill and siphon withdrawal response, paired with, as conditioning stimulus, a mild tactile stimulus to the siphon, which normally produces only mild withdrawal.
After pairing, the conditioning stimulus produces a strong withdrawal as well.
THE VERTEBRATE SOLUTION
The design problem of disentangling one's guts from one's brain was solved for vertebrates by the invention of an internal skeleton built around a backbone.
The skull cavity could now house an enlarged head ganglion or brain, while the nerves from the brain to the rest of the body run inside the backbone down the spinal cord; those ganglia that remained outside this central nervous system became reduced in significance and autonomy.
Nonetheless, despite these radical changes in design, the basic cellular organization of the nervous system, with its neurons, synapses and ensembles of interconnecting cells, is the same for vertebrates as for invertebrates, as is much of their biochemistry.
It is a bit like considering the multiple forms that vehicles based on the internal combustion engine have taken since its invention at the end of the last century.
Weird and wonderfully designed cars, motor-bikes, planes, improved engines, and year-by-year variations in efficiency, finish and fashion-features there may have been; but the principle of the piston-and-cylinder internal combustion engine, oil-based products as fuel, and wheels for movement along the ground all remain.
What changes with the appearance of the vertebrates are not  its building blocks or basic sources of energy utilization and transmission, but the organizing principles of the nervous system as a whole — a system that now contains the fully fledged learning and memory capacities that are the properties of all mammals, including, in their greatly expanded forms, primates, amongst them of course humans.
Whether despite this revolution in design the cellular mechanisms required for learning and memory in invertebrates are similar to or radically different from those in vertebrates is an issue which is still unresolved in research terms, and one which will occupy some of the discussion of the following chapter.
But the task of this chapter, of tracing the evolution of learning and memory-like phenomena in non-human animals, is done.
MOLECULES OF MEMORY
WHY BIOCHEMISTRY?
When, in 1929, the Swiss amateur physiologist Hans Berger reported that by taping a set of recording electrodes to the human scalp he could record continuous bursts of electricity pulsing through the brain, he was at first not taken seriously.
As I've mentioned already in the context of metaphors of memory, the phenomenon of ‘animal electricity’ and its relation to neural activity had been known for a long time — at least since Galvani's demonstration in Bologna in the 1790s that electrical pulses caused a frog's legs to twitch.
In 1875 Caton, the professor of physiology at Liverpool, had shown that electrodes placed on the exposed brain of a rabbit could record electrical pulses, but Berger's records came through the skull and could easily be dismissed as artefacts until his results were systematically vindicated by the Cambridge neurophysiologists Adrian and Matthews in the mid-1930s.
The brain's ceaseless electrical activity showed characteristic waveforms that varied with sleep and wakefulness, mental activity or tranquillity.
The electroencephalogram (EEG) seemed for a while to carry the secret of the soul in its multiple waveforms.
Could it also hold the key to memory mechanisms?
Perhaps when memories were made they were stored in the form of continuous reverberating circuits, endless electrical loops made by opening or closing synaptic connections?
Alas, the brief popularity of this idea could not survive the demonstration that long-term memories persisted  even if the total electrical activity of the brain was disrupted, by epileptic fits or electroconvulsive shock, for instance, or was brought virtually to zero by coma or concussion.
Thus though it might well be that the very short-term phases of memory are dependent on the continued electrical activity of the brain — and there will be more to be said about this in due course — in the longer term any persistent record or trace must demand some more permanent incarnation.
What form, though, might such a trace take, and at what level should it be sought?
Hebb's view, as described in Chapter 6, was that memory formation should involve some element of synaptic growth or reconstruction, thus providing a new pattern of connectivity between neurons which can subsequently be preserved; and this indeed remains the consensus, though by no means the only view.
But how many synapses and cells might be involved for any single memory?
Indeed what constitutes a ‘single memory’?
Could one write an equation: one association = one synapse?
Or are multiple cells and synapses involved?
Are such cells and synapses localized to a particular brain region, or are they diffused across many sites within the brain?
Are memories multiply represented?
Such a debate over localization reflects the conflicting evidence from human data but now translated into a cellular language.
Does the same set of cells embody the memory for all time, or is remembering a more dynamic, less fixed process?
Even granted hebbian principles, all such questions would remain to be addressed, and answering them helps to decide at what level of cellular complexity memories are represented within the brain.
Answering them also requires the development of experimental models in which to test different hypotheses, and measuring techniques refined enough to be able to detect any postulated changes with learning.
Until the last few years, the thought that one might have microscopic techniques sensitive enough to actually see tiny changes in the structure of neurons and their synapses as a result of learning seemed improbable — to start with, one would need to have a very good idea where to look and what to measure in the brain.
However, an alternative approach might argue that, if learning does involve making structural changes at synapses, and the synapses are built of proteins and packed with molecules of neurotransmitter, then learning must itself involve the synthesis of new proteins and transmitters.
Might it be easier to measure this rather than to look directly for structural changes?
PROTEIN SYNTHESIS
Living organisms are much more stable than the molecules of which they are composed.
No molecule of our body survives unchanged for more than a few weeks or months; over that period, even in adults, it is synthesized, plays its part in the cellular economy, and is then discarded, broken down and replaced by another more or less identical.
The extraordinary feature of this ceaseless flux is that structures of the cells and the body they compose remain constant whilst their components are replaced.
Bodies are not even like cars, in which every so often a faulty exhaust, spark plug or body panel is removed and replaced by another, identical one.
They are more like brick houses in which a demented builder is steadily, day and night, pulling out individual bricks and slotting others in their place.
The overall appearance of the house is unchanged in this process, though its components are continually being replaced.
Like the bricks in such a house, the protein molecules of the body are replaced (‘turned over’) so that, on average, half of all the protein molecules are changed every two weeks.
Synthesizing any new protein molecule from scratch takes a matter of minutes.
Having been synthesized it has to be transported to the part of the cell in which it is required; there it will remain for its lifetime of hours, weeks or months until it is due for renewal, when it is pulled out of place in the cell and broken down by enzymes as quickly as it was previously synthesized, its building blocks (the amino acids) being recycled in the synthesis of other proteins.
Now normally in an adult the rate of synthesis and breakdown of proteins is equal.
Every brick inserted into the structure of the house is matched by one removed.
But suppose that the builder decides to add a new chimney to the house.
To do so, it is necessary briefly to increase the rate of insertion of bricks at a particular point in the house — the roof — without changing the rate of removal; bricks thus accumulate as the chimney is constructed.
Once it is in place, the rate of insertion of bricks can decrease once more to its previous level, in balance with the rate of removal.
This brief flurry of brick-building will leave one with a house plus chimney, to be maintained just as before.
As with chimneys, so with synapses; if they are constructed — or even reconstructed — during learning, one might expect a brief increase  in the rate of synthesis of proteins over the time when an animal was being trained and memory was being formed.
And reciprocally, if memory formation requires the synthesis of proteins for the construction of synapses, then if one could stop the proteins from being synthesized around the time of learning then the memory should not be formed; an animal trained on a task and prevented from synthesizing proteins should behave as if it has no memory for the task — is amnesic — when it is subsequently asked to perform it.
This was the state of biochemical thinking about the study of memory in the early 1960s.
And fortunately, there were simple techniques available both for the measurement of the rate of protein synthesis and for preventing such synthesis.
Proteins are built by joining together long chains of individual units — amino acids — which are either themselves made in the body or are present in the diet.
Measuring the rate of protein synthesis then becomes a matter of measuring the rate at which amino acids are incorporated into proteins.
If one of the dietary amino acids is made radioactive (as in the experiment I described in Chapter 2) and fed to or injected into an animal, it is incorporated into the proteins just as its fellow, non-radioactive amino acids are, and the proteins become slightly radioactive in their turn by virtue of containing the radioactive amino acid.
The amount of radioactivity in the proteins is proportional to their rate of synthesis — and this can be measured both easily and extremely sensitively.
There are twenty different naturally.
occurring amino acids in proteins, and any individual type of protein is a unique sequence of up to several hundred such amino acids.
The precise assembly of the amino acids into the appropriate sequence depends on another giant molecule in the cell, ribonucleic acid (RNA), whose sequence is in turn directly under the control of the cell's genetic material, deoxyribonucleic acid, DNA.
(It is in this sense that protein synthesis is often, though somewhat misleadingly, said to be ‘directed’ by the genes.)
Increased protein synthesis thus also may require increased RNA synthesis, which can be measured by exactly analogous procedures using a radioactive precursor to RNA.
As for preventing protein synthesis, it was discovered almost by accident that many antibiotics — which of course work by preventing bacteria from growing and multiplying — achieve this goal by  preventing bacterial protein or RNA synthesis.
Injected into the brain in high enough doses, such antibiotics will also stop most of the brain's RNA or protein synthesis for a period of some hours.
Thus two types of experiment are in principle possible.
In the first, sometimes called a correlative approach, one can inject a radioactive precursor of protein or RNA synthesis into an animal, train it on a task, and ask if the amount of radioactivity in the protein or RNA has increased by comparison with that in appropriate control, ‘non-learning’ animals.
In the second approach, described as interventive , one injects an antibiotic inhibitor of RNA or protein synthesis, trains the animal and asks if it can still remember the task.
The early 1960s saw both these types of experiment being done.
I have already described the tremendous impression that Hydén's experiments — in which he measured increases in RNA and protein synthesis in tiny cellular regions from the brains of rats trained to balance on wires to reach for food — made on me as a young post-doc.
In later experiments he somewhat altered the behavioural design.
Noting that rats tend naturally to be either left- or right-pawed in reaching for and picking up their food, he constrained them to reach for their food with the non-preferred paw and reported changes in RNA and protein synthesis in the region and side of the brain responsible for the motor coordination of the ‘learning’ paw compared with the ‘non-learning’side.
Meanwhile perhaps the earliest of the inhibitor experiments was made by Wesley Dingman and Michael Sporn, in Rochester, New York, in 1963.
They taught rats to swim a water maze, and injected them with an inhibitor of RNA synthesis.
They first showed that the inhibitor had no effect on the rats' ability to swim in general, nor, if the animals had already learned the maze by the time it was injected, did it prevent them from swimming it correctly.
However, if the inhibitor was injected at such a time that RNA synthesis was inhibited while the animals were being trained on the maze, then they failed to remember it when tested on it later.
This experiment was quickly followed up by others using protein synthesis inhibitors, all essentially leading to the same conclusion — that if protein synthesis was prevented during the period over which an animal was trained, or for up to about an hour subsequently, then although the animal could learn the task, when tested on it some time later — say the next day — it behaved as if it were naïve.
Long-term memory, it would appear, required protein synthesis.4
My own immediate reaction when hearing of such results was incredulity.
The brain synthesizes protein at a rate higher than that of any other tissue in the body.
The antibiotics were being injected into the brain at doses sufficient to block all protein synthesis for several hours, and yet it appeared that no other aspect of the animal's behaviour was affected — not its capacity to perform already learned tasks, to see and respond to the world around or to act otherwise ‘normally’; the only thing it seemingly couldn't do was to memorize new tasks.
Surely not all that protein synthesis going on in the absence of the inhibitor could be about learning and memory; some other fundamental aspects of behaviour must be affected?
But no, it appeared not; report after report, in many different learning tasks and in species as diverse as rats and goldfish, came to the same conclusion.
To convince myself I eventually performed the ultimate in Doubting Thomas exercises, trained some chicks using the inhibitors, and got the same result.
It had to be true!
There is a methodological issue worth drawing out here, for this business of replicating — or failing to replicate — someone else's research findings is of course what ‘the scientific method’ is supposed to be about, at least according to the standard philosophy-of-science text-books.
A researcher reports a particular result, and to verify it other scientists repeat the same experiment in their own labs.
If they agree, the result is provisionally true.
If they fail to agree, it is necessary to decide where someone has gone wrong, in experimental design or theory-making.
This is what is meant by claiming that scientific knowledge is ‘public’ knowledge — that is, that it is in principle testable and verifiable by anyone/everyone and not merely a matter of private belief.
Even in so-called ‘basic’ or ‘pure’science, direct attempts to replicate reported experimental findings are in practice very rare (except possibly in some areas of physics).
There is no prestige to be gained from simply repeating someone else's  experiment; you are very unlikely to get a grant to do it, and the main scientific journals are not normally interested in publishing ‘replications’ of experiments unless they are on a particularly controversial topic.
Even failures to replicate are not very interesting to the journals; experiments with negative results therefore rarely get reported.
What people do tend to do if a result someone else reports interests them is to repeat it with variants — that is, they test it in their own favourite animal or experimental situation.
This is of course what I did — rather than repeat exactly experiments done on mice or rats that other researchers had already reported, I asked what would happen if I tried a similar procedure with my chicks.
Such a roundabout way of replication means that if one gets similar results in different species, they are worthy of publication in their own right.
Even if one gets different results, they are still publishable without necessarily running into head-on confrontation with the earlier claims.
Granted the rich diversity of the phenomena of the biological world, a disagreement over results can generally be put down to differences between animals (‘species or strain differences’, for instance) or to subtle alterations in experimental conditions, and can therefore be fudged or ignored.
Thus, rather than direct refutation, a controversial or dubious result can remain ‘in the literature’ publicly unchallenged but generally disregarded.
Those in the know — the core group of researchers in any field who spend a lot of time at conferences and seminars chatting about the state of the art — will simply disregard the anomalous result, or they will have gossiped it away in the bar after the meeting.
The reports of the amnestic effects of protein synthesis inhibitors were first ignored, often on the same sorts of a priori grounds that had led to my initial scepticism, and only after some struggle accepted by these definers of the field.
Thus it wasn't for many years after the first inhibitor experiments were reported that I actually got around to test their effects myself.
At that stage my priorities lay elsewhere, and to start playing with inhibitors seemed a diversion — when I have turned to using them, in the late eighties, as I shall describe in Chapter 10, it was with  rather more specific goals in mind.
For by the late 1960s and early 1970s, our own imprinting studies had their own strong momentum.
Essentially the experimental design involved exposing day-old chicks to an imprinting stimulus, injecting them with radioactive precursors to RNA or protein, and measuring the amount of radioactivity in protein or RNA extracted from different brain regions.
If that description is excessively dry and abstract, let me spell it out in a little more detail.
First, what do I mean by an imprinting stimulus?
In nature, so to say, chicks soon learn — within the first three days after hatching at the latest — who their mother is, and follow her thereafter.
But their definition of ‘mother’ begins by being pretty flexible.
When they hatch they will try to follow and come close to the first slowly moving object they see which is the right sort of colour and more or less chicken-sized.
People studying imprinting had used a stuffed hen, or even a red ball on a rotating arm.
Pat Bateson stripped the learning down to its essentials.
The chicks are placed in pivoted running wheels, a bit like the treadmills one can buy for pet hamster cages, placed in front of a red or yellow rotating, flashing light.
The flash gives the light the appearance of moving and the chicks, in the wheels, try to follow.
After an hour or so they are given a brief rest by switching the lights off and then they are tested by being given a choice between attempting to follow the light on which they had been imprinted (say the red) and another, unfamiliar light, say the yellow.
The extent to which they try to run towards the red compared with the yellow is taken as a measure of how strongly they have been imprinted.
At a chosen time during their exposure to the flashing light, the birds were injected with the radioactive precursor, the training continued, the chicks were tested and then killed.
Because we didn't know where in the brain any changes might occur, or very much about chick brain anatomy (nor, it should be said, did anyone else at the time), we divided the forebrain arbitrarily into two regions which we called simply ‘roof’ and ‘base’.
Later, as we focused in on where the changes occurred, we were able to subdivide much further and according to more meaningful anatomical criteria.
Pat and Gabriel Horn would code the brain samples and send them down to me for analysis.
In the very first experiments it became clear that, compared with ‘control’ birds that stayed in the dark or had been exposed simply to diffused overhead light, there was indeed increased synthesis of RNA in  the roof region in the hours after training in birds which had been imprinted on the flashing light.
We repeated the experiments with a precursor for protein and found that an increase in protein synthesis occurred also.
With two of my first graduate students to work on the chick, I went on to explore some of the more detailed biochemistry of these changes.
However, the main problem that concerned Pat, Gabriel and myself at the time was not so much the details of the biochemistry, but a theoretically more important matter.
True, we had shown that protein and RNA synthesis increased when chicks were exposed to the stimulus, but how could we be sure this was due to the fact that they had learned something about it?
Maybe it was because they were just more active, running busily in their treadmills, than their hatchmates in the dark or in subdued lighting.
Or maybe they were more aroused because of the flashing light, or maybe the light was affecting their visual system in some way.
All these could be alternatives to the explanation that the changes in RNA and protein synthesis were due to learning.
Designing experiments to check these possibilities isn't easy, and it took us several years, through the early 1970s, to try to eliminate one after the other.
By 1973 we were finally able to convince ourselves and, I believe, the rest of the research ‘community, interested in the problem that the biochemical changes were indeed an aspect of learning and not any of what we had come to call the concomitants of learning, such as motor activity or visual experience.
In one key experiment, for example, we trained over a hundred birds, measuring their imprinting preference score and motor activity (that is , how much they ran in the wheels) as well as RNA synthesis.
The amount of RNA synthesized was unrelated to motor activity or any measure of stress that we could identify (for instance how much the birds peeped or twittered), but it was strongly correlated with their preference score; that is, the more the chicks had learned about the flashing light and preferred it to any alternative, the more RNA was synthesized in their forebrain roof.
Thus it seemed, from our work and that of many other labs at about the same time, that brain RNA and protein synthesis were necessary for learning and memory formation, and that the field of biochemical memory research had taken a flying leap forward.
Sadly it was not to be.
What happened was a mixture of biochemical hubris and technical muddled thinking which between them  were to confound memory research for more than a decade.
As the errors remain instructive, it is worth spending a little time considering what went wrong.
MEMORY MOLECULES AND ARTEFACTS
Molecular biology's own favourite philosopher, Gunther Stent, has called the late 1950s and early 1960s the classical period of molecular biology.
Since 1953, when Watson and Crick had solved the structure of DNA and recognized that embedded in its famous double helix lay a mechanism both for genetic transmission of information and the directed synthesis of proteins, the detailed mechanisms of protein and nucleic acid synthesis had been unravelled and more and more aspects of its exquisitely precise cellular controls were becoming clear.
Nothing seemed impossible; the whole of biology was about to become transparent to this wondrous new science.
Where once biologists and biochemists had been concerned with questions about where and how cells got and used their energy, the new molecular biologists had a different language for what was important.
Not energy but, drawing on the then equally new computer sciences,information was what mattered.
Controlling and reproducing the cell was, it seemed, all about controlling and reproducing information; and what distinguished the molecules that embodied this new idea, proteins, DNA and RNA, from the much more boring small molecules that until then biochemists had worked with was that these giant molecules seemed to embody information; they were, it appeared,informational macromolecules .
As the brain was a machine for processing information, what more logical to assume that it did so by utilizing in some very special way these informational macromolecules.
Furthermore, could not the very reproduction of the species itself be regarded as depending on a form of memory — genetic memory, the apparent capacity of the DNA, transmitted between parent and offspring, to carry the rules for the future accurate development of the new organism?
If DNA was the carrier of genetic memory, why could not it — or RNA or protein — also be the carrier of brain memory?
This punning logic became extended by parallel developments in the field of immunology.
Antibodies are proteins which, once synthesized by cells of the immune system to counter and  inactivate ‘foreign’ molecules, enable the body to retain the ‘memory, for the intruder and hence the capacity rapidly to inactivate it on subsequent invasion.
And as immunological memory too depended on proteins, could there not be a grand convergence of mechanisms operative here?
Forget structure, the intricately intertwined pattern of 10 billion neurons and 100,000 billion synaptic connections within the brain.
Perhaps memories were carried by the very macromolecules themselves?
Sure, the DNA was perhaps a little preoccupied with carrying the genetic memory, but could not the memories of a lifetime be readily encoded in the myriad of potential unique protein sequences?
Such was (and remains) the power of molecular biological rhetoric that many who should have known better were swept along with it.
The sloppiness of such thinking-by-pun affected many of the leading molecular biologists and immunologists of the period (two who were swept up in this early enthusiasm but stayed on to become wiser neurobiological theorists were Gerald Edelman and Francis Crick).
The misguided enthusiasm spilt over into the most prestigious of journals.
A few examples will give something of the flavour of the times: Even sober-minded mathematical modellers fell under the spell, as witness the mathematician J. S. Griffith who had helped Watson and Crick solve DNA back in the early 1950s, writing jointly with one of the doyens of biochemistry, Henry Mahler, and offering what they called, for reasons I have never quite understood, a ‘DNA ticketing theory of memory’.
Enthusiasm for such ideas has even now not completely evaporated; voices advocating them can be found through the 1970s and 1980s and, in only marginally more sophisticated forms, at the present time:
Individual molecules are the fundamental decision-making elements in the brain…the function of the neuron's to allow the elements to communicate with one another.
In this paper, animal behaviour, in particular learning and memory, has been reduced to the behaviour of proteins, whether individual or assembled in superstructures…the interplay of billions of such molecular events, ensured by appropriate wiring, brings about complex forms of learning in animals and in man.
Experimental results showing the involvement of RNA and protein synthesis in memory formation could readily be accommodated to the new molecular thinking, but what really raised the temperature of the whole enterprise were reports that began to appear of bizarre memory experiments involving planaria.
The originator of this research was the maverick James McConnell, at Ann Arbor, Michigan, who in a series of papers during the 1960s, first in conventional scientific journals and then in his own publication, the exotically named Worm-Runners Digest , reported experiments in which flatworms, trained by pairing light with electric shock, were chopped up and other, ‘naïve’(that is, untrained) worms allowed to cannibalize them.
McConnell claimed that the cannibal worms behaved as if they remembered the conditioned response their food had learned, whereas worms allowed to cannibalize other, untrained worms showed no such change in behaviour.
The experiments hit the scientific and popular headlines over a number of years before falling into disrepute when others found it quite hard even to train flatworms reliably on this pairing, let alone repeat the later steps in the procedure.
But by that time it no longer mattered, for reports of similar effects in mammals had begun to appear.
Some of the first of these were made by a pupil of McConnell's, Allan Jacobson, in Los Angeles, who announced in 1965 that if he trained rats to approach the food dispenser of their cage when a light flashed or a clicking sound was made, then killed the animals, extracted the RNA from their brains and injected it into the gut cavity of untrained animals, these now tended to approach the dispenser when the appropriate stimulus — click or light — was given, even though the dispenser was now empty of food and the animals received no reward.
Jacobson even managed to ‘transfer’ the approach behaviour from rats to hamsters in the same way.
Meanwhile, human analogues of such experiments began to appear.
Ewen Cameron, a McGill-based psychologist, began to feed heavy doses of RNA — typically 100 grams of yeast RNA extract, a truly massive amount — to elderly people with memory difficulties.
He claimed that this amount did have significant effects on the person's capacity to remember (though presumably it did not cause them to remember their past experiences as yeast cells!).
More than likely the explanation for such results lay simply in the fact that the elderly, institutionalized patients who formed Cameron's subjects were so pleased to be noticed and made a fuss of in experiments of this sort that their memories improved as a consequence.
Or perhaps the subjects of Cameron's study were simply malnourished, as happens in many institutions, and the RNA served as a dietary supplement.
Taking RNA as food, as Cameron had given it, would simply mean that it would be broken down to its precursor molecules in the gut before being absorbed into the bloodstream.
Doubt was cast on Cameron's results partly by the lack of control data he offered, and, later, after his death, his reputation for scientific integrity was irretrievably damaged by the revelation that much of his experimental work had for a long time been secretly supported by the CIA, including some rather insidious studies of the effects of covertly administered LSD on the behaviour of unsuspecting people.
Claims for the memory-enhancing effects of RNA provoked a fierce controversy in the research literature, with many laboratories attempting to repeat them, and mainly failing to do so.
In the same year as Jacobson published his experiments, a ‘failure to replicate, report signed by twenty-three authors appeared in the  major journal,Science , and the matter might have rested there but for the fact that it was noticed that the method which Jacobson had used to extract RNA from his rat brains also liberated a good deal of protein and other contaminants.
Perhaps the active material was not RNA at all?
By 1967 memory transfer labs were back in business again, injecting a variety of brain extracts and claiming many and varied results — one group of researchers for instance trained some rats to press levers for food with their right paw and others with the left, and found that one behaviour could be transferred but not the other!
The most systematic claims, however, were made by Georges Ungar, of Baylor University in Houston, Texas.
His training protocol exploited the fact that, given a choice between staying in a lighted environment or entering a dark compartment, rodents such as rats or mice will go into the dark.
Ungar placed rats in a start box which opened onto a lighted arena with a dark compartment in one corner, and shocked the animals electrically when they attempted to enter the dark compartment; this punishment rapidly resulted in the animals refusing to enter the dark box.
Material extracted from their brains was then injected into mice, which were then given a similar light/dark choice, though without being shocked.
Mice which had received material from the brains of the trained rats, Ungar claimed, also refused to enter the dark box, whereas animals which received materials from naïve animals showed no such inhibitions.
Ungar and his group went on to endeavour to purify the component in the brain extract which, they claimed, carried the information about fear of the dark’.
As I have already hinted, there had always been a biochemical puzzle about how proteins or RNA might work in these transfer experiments, because all such large molecules are rapidly degraded in the gut and broken down into their component amino or nucleic acids before being taken into the general metabolism of the recipient.
And indeed Ungar found that his active component was neither a protein nor a nucleic acid but a peptide — a generic name given to a short sequence of linked amino acids anything up to fifteen or twenty units long and administered by injection, so avoiding the digestive barrier offered by the gut.
Ungar's peptide was fifteen amino acids long and he called it ‘scotophobin' — Greek for ‘fear of the dark’.
After much furore, his results were published in Nature , along with, in a development unheard of at the time, a criticism of his data by one of the paper's referees, the chemist Walter Stewart.
Stewart's Nature critique focused not on an analysis of Ungar's behavioural claims, but on the chemical purity and composition of the presumed scotophobin.
However, for me, as for other neurobiologists, there were always other implausibilities about Ungar's results, even assuming they could be replicated (and many labs remained sceptical).
How could minute quantities of an injected peptide be guided to and then enter the appropriate neuron so as to code for the new memory?
Why should the same peptide ‘mean’ the same highly specific memory/behaviour in different animals/species?
And if peptides really did code for memories, should there not be many more of them in the brain than one actually finds?
If there were such ‘memory peptides’ and each was present in the brain in the concentration of scotophobin, then to code for the memories of a human lifetime would demand that the brain contained a mass of peptides weighing something of the order of 100 kilograms — or rather more than the weight of an average human.
But perhaps our main concern was even more fundamental — could the behaviour of the injected mice really be said to show learning at all?
Let me explain.
In Ungar's test situation, the mice are released into a lighted compartment and their behaviour observed.
The time taken for them to enter the dark box is noted, and if they have not done so by the end of a fixed time, say one minute, the experiment is terminated.
The time mice injected with material from the trained rats take to enter the dark box is compared with that for animals injected with ‘naïve’ material.
The difference between the trained and the naïve rats is that the former have been electrically shocked, and therefore certainly stressed or pained, when attempting to enter the dark box.
The common response of a rat or mouse to stress is to freeze and become immobile.
So suppose that the result of the stress caused by shocking an animal in this way is the production of some hormone — for instance a peptide — which then produces the freezing behaviour.
It would be present in the brains of the trained — and shocked — animals in higher concentration than in the naïves — and when injected into mice would in its turn produce freezing  behaviour.
This behaviour would be measured in Ungar's test as an increased reluctance, or even a refusal, to enter the dark compartment within the minute of the trial, simply because the recipient mice were relatively inactive.
Because of the design of the experiment, this inactivity would appear as the learning of a specific response.
However, in reality what would have been ‘transferred’ would not be specific learning, but a rather general emotional stress reaction — a very different matter.
Not long after this controversy, Ungar himself died.
There were some irreverent proposals to perform the ultimate experiment and try injecting material extracted from Ungar's brain into his critics — a human trial that I suspect Ungar himself might have been rather in favour of!
In any event, with his death scotophobin disappeared from the research literature, and so did the memory transfer experiments.
(Friedrich, a Hungarian enzymologist whose views on molecular memory are quoted above, was one of those whose association with the memory transfer work lasted the longest.)
I do not want to suggest that all this work fell into such simple methodological traps as that I have described above, though I suspect that much of it did.
Many unexplained and ignored results still litter the research journals of the early 1970s, unread and unconsidered because as a research paradigm memory transfer is no longer taken seriously; it had become another victim of scientific fashion, though, unlike McIlwain's slices, this time probably deservedly.
And as no-one takes the paradigm seriously, no-one is troubled to try to find an explanation for the seemingly anomalous data.
Perhaps, as most of us would assume, it can all be explained by inadequate statistics, faulty experimental design, overenthusiastic interpretation of ambiguous results, or, as I have argued in the case of Ungar's experiment, misinterpreting the biochemical and pharmacological consequences of stress or other, rather non-specific aspects of behaviour.
Perhaps there is something else residually there that doesn't fit our present-day models?
None of us at the moment is prepared to spend — probably waste — our time trying to find out.
Non-scientists — and anti-scientists — are often troubled by such apparent unwillingness among scientists to spend time investigating seemingly paradoxical results which don't fit into current experimental paradigms — corn-circles, ESP, UFOs, aromatherapy or whatever.
To such critics it shows just how blinkered orthodox  science is, and the rather arrogant refusal of most scientists even to take the phenomena seriously, waving them irritably aside, merely serves to reinforce their opponents' criticisms.
What such critics for their part fail to realize is just how difficult scientific research actually is, how complex the testing of any even seemingly trivial hypothesis or hunch may be, and how many paradoxes and seeming mysteries we confront every day in our research which to us are at least as challenging as, but theoretically more relevant than, fretting about probably untestable phenomena like ESP.
While poets and magicians are concerned to draw attention to the anomalies that break the regularities of our day-to-day world, the strength of natural science has lain in the meticulous and often boring study of its seemingly tedious regularities.
To us they seem at least as intriguing and worthy of study as do the signs and wonders which become the obsessive concern of mystics, religious and many others outside the laboratory.
However, I have here rescued the lost experiments on memory transfer from otherwise merciful oblivion not merely to do historical justice to a now suppressed period of my chosen research field but for three rather more solid reasons.
First, the episode shows how easy it is to be led astray by one's own rhetoric.
As I pointed out in Chapter 4, there is a very important way in which science proceeds by metaphor, and metaphors can illumine — or they can mislead.
In this case, the metaphor was given by the use of the words ‘memory’ and ‘information’in three different contexts, heredity, immunology and learning.
The power of fashionable slogans — such as informational macromolecules — and the search for sensational results to feed to press and pay-masters swept caution to the winds.
The result was a hunt for the biochemical mechanism of memory at what was a fundamentally mistaken level — that of molecules rather than the brain systems in which molecules are embedded.
The choice of the right level at which to study a phenomenon is as important a strategic decision in biology as is the choice of the right organism or the appropriate control experiment, and it affects present-day research just as much as it did twenty years ago.
The second point is not so negative.
Ungar's experiments were done before other research — which was to lead in due course to the opening up of a major new branch of neuropharmacology and to the making of some very distinguished scientific reputations — had revealed how important many peptides were in the brain.
The  best-known of these peptides are those sometimes described as the body's natural pain-killers, the morphine-like family of the opioids , such as enkephalin and endorphin.
Dozens of such brain-acting peptides are now known to exist, including many closely related to hormones which act elsewhere in the body.
They function as neurotransmitters and as modulators of neuronal activity (neuromodulators ) and are associated not only with pain but with pleasure, stress, arousal, attention and many other such global mental and bodily states.
Remarkably — or perhaps not so remarkably — Ungar's mythic scotophobin had an amino-acid composition rather reminiscent of that of the endorphins and enkephalins.
He had, in effect, stumbled without recognizing it into a major new area of understanding of the chemistry of brain function.
But he died without knowing it, and the true significance of the peptides was discovered by others without reference to his premature and misinterpreted finding.
The third lesson is the ease with which artefacts can enter into experiments intended to study phenomena as complex as learning and memory.
Just because non-human animal memories can be expressed only in behavioural terms, the possibility always arises that what we are measuring is an aspect of the behaviour rather than the memory.
In learning experiments, animals are stressed or hungry, they receive sensory inputs, they perform motor tasks.
If we find changes in protein synthesis, say, in a correlative experiment, how can we be sure that such changed synthesis is not the consequence of these expressed behaviours rather than the learning which we presume accompanies them?
If we conduct an interventive experiment, and inject a drug which results in an animal not performing some task on which it has been trained ‘correctly’(I won't bother putting that word into inverted commas henceforward; I have already spelled out that what we read as correct in an animal's behaviour is interpreted by our criteria, not necessarily by its own), how can we be sure that what has been blocked or disturbed is the memory rather than the motor or sensory activity on which its expression depends?
A drug may make an animal less hungry, less mobile, less sensitive to the pain of an electric shock, just as much as it may make it forget.
Designing experiments to control for all these possibilities is not at all easy, as much of the debate within the research literature shows.
To take but one other example from an experimental approach common in the late 1960s and early 1970s, suppose one  trains a mouse by putting it onto a small shelf in the wall of a cage with a grid flour which can be electrified.
Every time the animal steps down it received a mild electric shock (yet another of the less than pleasant learning tasks which experimental psychology finds it necessary to employ).
Within a few trials it learns to stay on the shelf (this is called ‘step-down avoidance’).
In a parallel cage to this learning mouse is another; however, this second cage contains no shelf.
Each time the learning mouse is given an electric shock, so is the parallel ‘yoked’ mouse.
However, without a shelf to escape to the yoked animal cannot learn the task.
So one seems to have perfect control; both mice have received the same number of shocks, but one learns how to avoid the shock, the other cannot, for there is no escape; the number of shocks it receives is entirely governed by the behaviour of its learning partner.
Thus any differences between them must be caused, not by the shock, but by the fact that the mouse in the cage with the shelf is learning an avoidance response.
Many experiments have used this type of design and have shown differences in rates of protein synthesis between such ‘learning’ mice and their yoked controls, from which it is concluded that the learning rather than the shock produces the biochemical response.
Sometimes a third, ‘quiet control’ group is included in the study, and often differences in biochemistry are found between it and both the yoked and the learning groups.
But wait.
Can one really be sure that the yoked animal is not learning anything?
Perhaps it is indeed learning that there is no escape from the shock — and such learning may be very important to its subsequent behaviour.
Are the differences between yoked and quiet control the result of learning or of stress — and is the learning group perhaps less stressed than the yoked group?
Stress will certainly affect the levels of a number of hormones circulating in the bloodstream and it could well alter brain metabolism.
Biochemical differences between the groups of animals may thus not have such a simple meaning as learning versus non-learning — even if the biochemical effects can be themselves regarded as unequivocal.
For there are similar sources of artefact and error in the biochemical measures.
Many of these depend on rather sophisticated biochemical arguments which do not really concern me here; just two must suffice.
I have said (in Chapter 2) that one can calculate the rate of protein synthesis by injecting a radioactively  labelled amino acid into the bloodstream and measuring the amount of radioactivity to be found in the protein of particular brain regions after a given time.
True enough, but to be incorporated into the protein the radioactive amino acid has first to be taken up from the bloodstream into the neurons.
Changes in blood flow and other such physiological effects can alter the rate of uptake, and hence produce an apparent change in the rate of protein synthesis.
There are similar ambiguities in interpreting the effects of inhibitors of protein synthesis.
Because proteins are made of amino acids, if protein synthesis is blocked by giving an inhibitor, amino acids which would otherwise have been converted into proteins tend to accumulate in the cell.
Several amino acids, as well as being necessary building blocks for the synthesis of proteins, are also powerful neurotransmitters, and their presence in excess can result in disruption of the electrical activity of the nerve cells.
The inhibitors might therefore be causing amnesia not because they prevent protein synthesis but because of their effect on increasing amino acid levels.
In the first flush of enthusiasm for the ‘molecules of memory’ many experimenters failed to take the precautions necessary to control for such biochemical and behavioural ambiguities, and as a result their research — and with it the entire field — became discredited.
STARTING AGAIN
As the artefacts and problems of the early research became apparent, and bubble scientific reputations were painfully pricked, the rush of researchers into the field ceased and then reversed.
For a few years funding for memory research became hard to come by.
Those of us who stayed faithful to the project found ourselves isolated, our findings met by polite scepticism at best.
When, in the early 1980s, memory came back into neuroscience fashion again, it did so in new forms.
It was at about that time that I wrote a paper entitled — I hoped provocatively —‘What should a biochemistry of learning and  memory be about?’
The problems of memory research, it seemed to me, were in part the problems of any new research field.
A multitude of labs had started in enthusiastically using many different forms of learning paradigms, often taken over very straightforwardly from the experimental psychologists.
But it didn't necessarily follow that the sort of training beloved of the psychologists — say shaping a rat to press a lever for food in a skinner box — was best suited to the study of the cellular and biochemical processes going on within the organism.
It might simply be that the amount of learning involved in such a task was not great enough to generate biochemical changes big enough to be measured.
Researchers working on the biochemistry of learning and memory needed to develop new model systems, in which the changes we sought to identify would be big enough to be measured, yet in which we could be sure that they weren't simply artefacts.
More than one researcher abandoned the field with the argument that if the biochemical changes really ‘coded for’ memory in the brain then they would be too small to be measured, and if they were large enough to be measured then they probably weren't anything to do with memory.
The question of the scale of any possible biochemical changes was (and is even now) a serious one.
Psychologists and physiologists have always been resigned to using statistical analyses to extract meaning from and interpret their data.
Many biochemically oriented biologists, and especially molecular biologists, tend to feel unhappy about any such need; the phenomena they study have often tended to be all-or-none, or at least so large that differences between experimental treatments or conditions produce apparently unequivocal results.
If you need statistics to demonstrate an effect, they argue, it may not be real, and anyhow cannot be important.
Waving to one side the 15–20- per cent changes in protein synthesis rates and enzyme activity I had reported, Francis Crick was explicit on this point at a Royal Society discussion meeting in London in 1977 when I presented the results we had by then obtained on imprinting in the chick and the effects of first exposure to light in the rat.
If it is less than a hundred-per cent change, ignore it; you are studying the wrong system or have designed the wrong experiment, he insisted.
Yet to psychologists or physiologists in the audience, the surprise was that simply training an animal on an imprinting stimulus, or indeed any other form of learning, could produce a change of measurable  magnitude at all; they would search our experimental designs for sources of artefact just as rigorously as I myself had done with the ‘transfer’ experiments.
Quite apart from the scale of any observed effects, and of rather more theoretical importance, was the issue of whether any biochemical change we found was unique to, say, imprinting in the young chick, so that memory for other types of behaviour in older birds or in other species would involve quite different processes.
Or were we tapping into some kind of general biochemical mechanism relevant to all types of learning?
Granted that psychologists have described a whole taxonomy of memory, procedural and declarative, episodic and semantic, working and reference, should one expect similar underlying biochemical and cellular changes to be involved in each, or would every form of memory have its own special biochemistry?
Are there universal cellular memory mechanisms found in all mammals, all vertebrates or even all animals, or are they specific to particular species?
In a sense such questions are about the right level at which to study memory.
If the key processes are biochemical, then it might be expected that each memory will have its specific representation in terms of the synthesis of unique proteins or other molecules.
But if we reject this view in favour of memory as a property of the brain as a system, rather than of its individual cellular and molecular components, then memory will depend not on distinct biochemistry but on just which cells and synapses are showing the changes, where they are located in the nervous system, and which other cells they make contact with.
Think of the front door to an apartment block, with its array of bells.
There are two possible ways in which the bells can be arranged so that someone arriving at the door can signal their arrival to a person in a specific apartment.
Either each bell sounds different, and each could be heard in every apartment, or each sounds the same but is wired up to sound only in a single specific apartment when pressed.
The first method — each bell unique but each ringing everywhere — means that the ‘message’ of the bell lies in its specific sound; in the second, the message lies not in the bell, but in the way it is wired up.
These, in essence, are the two alternative ways in which the biochemistry of memory might work.
For those believing in molecules of memory, the message is in the bell and its unique properties; for those believing that memory is a system property of the brain, the bell is merely a part  — albeit an important one — of the system, and to understand the message one must read the wiring diagram, not listen to the sound of the bell ringing.
If this second approach is right — and despite my own biochemical enthusiasms I believe that it is — then the biochemical events that I study are likely to be very general processes of protein synthesis and membrane modification, sometimes described as ‘housekeeping’.
(Often the term is used in a rather derogatory way, as ‘mere housekeeping’— albeit then mainly by male biochemists who may not recognize that housekeeping is a pretty serious business!)
The memory lies within the topography — the wiring diagram — and dynamics of the neuronal system.
This means that the cellular mechanisms of, say, remembering a telephone number and remembering how to drive a car wouldn't differ — it would just be that different cells, connected up in different ways with other parts of the brain, are involved.
At the start of the 1980s, it seemed to me that until we knew more about the answers to such questions then it would be hard to compare and make sense out of the varied and possibly conflicting results that were coming from different labs.
How many depended on the minor peculiarities of the learning task or organism involved, and were just another almost random item of knowledge to be added to the burgeoning catalogue of phenomena of memory?
How many were ‘true’ generalizations which went beyond the particular species and task and could begin to provide a real cellular and biochemical ‘alphabet’of memory — or is such a search a mere will-o'-the-wisp?
The great triumphs of molecular biology have come about because the research groups centrally involved in the experimental programmes of the 1950s and 1960s had concentrated their efforts on a single simple organism, the common gut bug Escherichia coli .
Indeed, Francis Crick had gone so far as to suggest, at least half seriously, that all work in molecular biology and biochemistry on anything else should stop until E. coli was ‘solved’— whatever might be meant by such a solution.
Other, less molecular, biologists had protested in outrage that what was true for E. coli was not necessarily true for E.lephant, that biology has its diversity as well as its universals and that multicellular organisms with complex brains are not merely aggregates of 10 or so single cells; the properties of systems of cells include relationships between those cells which are not inherent in any single unit.
Nonetheless, might  neurobiologists in general, and memory researchers in particular , not gain something by concentrating on a limited number of model systems that everyone could agree on?
What is sure is that learning is a complex business involving many aspects of brain activity, and not reducible to a single linear sequence of events.
The stress, arousal, motor activity and so on inevitably associated with learning themselves result in biochemical and physiological changes in the brain at the same time as the animal is learning, and they are all important and interesting to study in their own right.
Anything that alters the rewarding or punishing effect of the learning task (if you are less hungry, thirsty or fearful, you are less likely to work hard to learn a task which provides food, drink or avoidance of electric shock as a prize for success) will also affect the study of learning and memory.
Arousal, reward and punishment, as I have already suggested in relation to the interpretation of Ungar's experiments, are associated with changes in the amounts of the opioids and other peptides in the brain and bloodstream; so injecting the peptides, or drugs which interact with them, will alter behaviour, including the expression of memory.
Such agents will therefore affect the learning process even though they are not directly part of it, in the same way as the tone or volume controls on a tape recorder affect the recording and playing of the tape even though they are not directly part of the message the tape carries.
There are now available drugs which, injected prior to or just after a learning trial, improve retention of the memory (that is, increase the ‘savings’ as defined in Chapter 5) in animals tested hours or days subsequently.
Other such substances can diminish  retention.
This discovery, of proactive or retroactive interference in memory formation, has led to the hunt, strongly backed by a number of pharmaceutical companies, for drugs which might improve human learning or memory, especially in elderly people with conditions like Alzheimer's — the so-called smart drugs.
The prospect of discovering drugs which will help to alleviate memory diseases like Alzheimer's deserves every encouragement.
But where the research and claims go further, as the proponents of smart drugs increasingly do, and assert that there is a widespread disability, ‘age-associated memory deficit’, and that the drugs will be able to alleviate the symptoms of this memory loss, then I become a great deal more sceptical.
As I argued in Chapter 5, I am doubtful both whether such a condition really occurs at all, and whether, if it were to exist, drugs are ever likely to be developed which can help it.
To demonstrate that learning and memory retention can be improved in animals injected with peptides and tested under highly contrived laboratory conditions is very different from showing that human memory can be helped by popping a pill.
Nor of course, if we dismiss the idea of ‘memory molecules’, could we ever anticipate a pill to restore lost memories, for, as I have argued, these are represented in the brain not in the form of particular molecules but in a multitude of specific neuronal circuits.
At best, all any such pill might ever be able to do is to affect the very general housekeeping biochemical processes required for the expression of memory.
But because the brain is such a finely equilibrated and dynamic system, with great capacities for self-adjustment and control, the effect of disrupting its biochemistry by flooding it, via a pill, with some drug which affects protein synthesis, or particular neurotransmitters or neuromodulators, is more likely to be the equivalent of trying to retune a radio or reprogram a computer by jamming a screwdriver into its circuit boards.
In any event, whatever the possible therapeutic role of such drugs, they are unlikely to be able to help us to understand the intimate mechanisms of memory.
At best they can tell us about aspects of the general biochemical and hormonal ‘state’ of the brain necessary for learning and memory formation to occur, the  tuning and volume mechanisms, but not about the memory processes, the message on the tape itself.
And for me, this is the core research question.
How does new learned information become ‘represented’ in the brain in the form of new patterns of connections between cells so that it can subsequently be retrieved and modify future behaviour?
And what are the molecular processes involved in establishing that new pattern?
Changed secretion of peptides, amongst many other such changes, may be necessary if memory formation is to occur, but it is neither sufficient nor, because it is a very general process, can it be regarded as specific to any particular memory.
This is why my own experiments have paid relatively little attention to the effects of such substances, and they will play only a small part in the unfolding story of the chapters that follow.
By the end of the 1970s, it seemed clear to me that if any cellular or biochemical process was to be regarded as forming part of some type of memorial code, it must show just these features that the drug studies could not readily provide, of being both necessary and sufficient to account for the memory.
Whether one could go further and show that any particular process was specific to a particular memory, in that it represented it and only it within the brain, remained to be seen.
Granted that it had turned out to be only too easy to make experiments in which training an animal on some task results in large biochemical and cellular changes in its brain, I felt that it was necessary to establish guidelines to help judge whether any particular change indeed has these characteristics of necessity, sufficiency and specificity.
This was a need that Pat Bateson, Gabriel Horn and I had hammered out in many long discussions about our imprinting experiments, and which we had tried to meet in practice in the design of the controls we had used in the early 1970s.
But in 1981 I tried to go further and identify a set of criteria which any proposed biochemical or cellular correspondent of memory formation would have to meet if it was to be regarded as a candidate memory process.
Because the discussion of these criteria is associated with a fresh and more promising period in the history of memory research, and also symbolizes the point in my own research trajectory at which I switched from working on imprinting to an even simpler form of learning in the young chick, they can appropriately form the starting-point for the next chapter.
GOD'S ORGANISM?
SEA SLUGS AND SEA-HORSES
CRITERIA FOR CORRESPONDENCE
By 1980, the memory research community was reasonably sure about the things it didn't believe in.
For instance it was pretty clear that there were no unique memory molecules.
It was also cautiously convinced that the most promising way to think about memory was along the lines of Donald Hebb's model, which involves changes in the strength of the connections between nerve cells, perhaps by growing new or enlarged synapses, and so altering the physiological relationships between neurons.
Such a hebb-type modification might indeed be detectable by appropriate neurophysiological or biochemical measures.
But no-one was very sure that any specific biochemical processes, apart from the rather general one of protein synthesis, had yet been unequivocally linked to memory formation, and the community had become distinctly cautious about evaluating any new claims.
But what after all should a biochemistry of memory be about, and how would one know if one was discovered — that is, what sort of biochemical answer might prove convincing to both biochemists and to psychologists?
What criteria should any experiment we made try to meet to satisfy the claim that the process we were studying was a necessary, sufficient and possibly even specific aspect of the memory formation process?
This was the question I tried to answer as the decade of the 1980s began.
It was becoming easy enough to find biochemical changes that occurred when an animal learned; the problem was to  show that such changes were really part of the memory-making process.
And I felt I couldn't design rational experiments without having some type of criteria to judge the results against; otherwise I wouldn't be able to see where I was going.
I decided that, for anything I could measure to be considered part of a memory trace or engram, it needed to show the following properties.
CRITERION ONE
There must be changes in the quantity of the system or substance, or the rate of its production or turnover, in some localized region of the brain during memory formation.
If there are more or modified synapses, the chemicals and structures of which the synapses are composed must show signs of change, which might be measurable either by biochemical methods (for instance, an increase in the amount of synaptic membrane protein) or under a microscope (for instance a change in the dimensions of particular synapses, or an increase in synaptic numbers).
But if I were to find such a change taking place while an animal is learning, unless the conditions for that change met all the subsequent criteria, I would be no further forward than the experiments of the 1960s that I criticized in the last chapter.
Some change is certainly necessary , but by itself cannot be said to be sufficient or specific .
Nonetheless, this criterion is of course fundamental to any materialist model of memory.
The criterion also makes a claim about localization ; the changes cannot be all over the brain but must be concentrated to some specific region.
This of course harks back to the much older debate about whether memories can be localized — something I'll come back to later,; much of the next two chapters will be taken up with the question of the localization of memory in space and time.
Note also that this criterion doesn't say anything about the direction or size of any change.
It is perfectly possible to imagine that memories are coded for negatively, by reducing the level of some substance or process, although in practice nearly all research seems to be devoted to trying to identify increases.
How about the size of any change?
If something is remembered twice as strongly, or if two items are remembered rather than one, should the change be twice as big?
Not necessarily, because in our experiments we  experimenters, devise; however, we have no way of telling whether the scales we use are the same as those the animals themselves use in making their memories.
For example, I can train a chick to avoid a bitter chrome bead, then a bitter red bead, then a bitter blue bead.
But the chick may not remember these as three separate items; instead, and more likely, it will adopt a different, and indeed more rational, strategy, and generalize along the lines that ‘all objects of a certain size, irrespective of colour, are likely to taste bitter and should be avoided’; that is, it would be remembering one item, not three.
CRITERION TWO
The time course of the change must be compatible with the time course of memory formation.
Clearly memories are not formed instantaneously, as if by throwing a switch, but are built up over a period of hours after the event to be memorized has occurred; during this build-up the form in which any memory is stored changes.
At least for declarative memory (Chapter 4) there is a transition, over a period of minutes to hours, from the initial and labile short-term phase to long-term, stable memory.
One could envisage a number of ways in which this transition might occur (Figure 9.1).
At one extreme, there could be a continuous process in which a sort of chain reaction of biochemical processes in a particular ensemble of cells led inevitably from the early, vulnerable phase through to a final fixed form, like the hardening of glue or the developing of a photograph.
On the other, there could be two more or less independent processes occurring in parallel, such that there were transient changes in the electrical properties and responsiveness of one set of neurons which could ‘code for’ the memory for a few minutes before gradually fading away.
Meanwhile if the memory was ‘important, enough for lasting representation, there could be a steady development of biochemical processes, such as the reconstruction of synapses, which would permanently represent the memory in another set of neurons, perhaps in another part of the brain.
These two types of process, on the one hand serial and on the other parallel processing of memory, are of course extremes, and there are many intermediate possibilities; it becomes quite hard to design experiments which will unequivocally distinguish between them.
However, the experience of human memory points to just this type of separation between short- and long-term forms of memory.
Evidence from studies of patients like H.M.
suggests that, even though the hippocampus is not required for the expression of long-term memories, it is necessarily involved in the transition between short- and long-term memory.
But whether serial or parallel processing turns out to be the way the brain works (there will be more to say about this too in the next chapters), there will be cellular events associated with both the short-term and the long-term phase, and we have to try to distinguish between them experimentally.
CRITERION THREE
Stress, motor activity or other processes which accompany learning must not, in the absence of memory formation, result in the structural or biochemical changes.
This is obvious in theory but immensely hard to test in practice.
If experimentally observable animal learning is impossible without stress or motor activity or whatever, is stress or motor activity or whatever possible without learning?
Is the attempt to make such a separation meaningful, and above all can one devise the appropriate form of highly reductionist experiment which will enable a distinction to be made between these processes?
Are we asking for the experimentally and/or theoretically impossible?
Granted that I have spent a considerable portion of my theoretical energies over the years criticizing reductionism, should I even be asking this question at all?
Because trying to meet this criterion has occupied so much of my research time in the past two decades it is worth spelling out again that to adopt a reductionist methodology in research strategy — that is, to try to stabilize the world that one is studying by manipulating one variable at a time, holding everything else as constant as possible — is generally the only way to do experiments from which one can draw clear conclusions.
Error comes in if one over-interprets the relevance of these conclusions, by forgetting the artificial constraints of the experiment and instead assuming that in real life, outside the laboratory so to say, such changes involving only a single variable can actually take place; that it is a simple matter to extrapolate back from the artificiality of laboratory isolation to the complex, rich interconnectedness of the real world.
It is this procedure, which involves turning a reductionist methodology into a reductionist philosophy , that is the manoeuvre so popular among molecular biologists and some geneticists, but, fortunately, is rather rarer among psychologists or neurobiologists.
(Indeed, it is psychologists who have been among those who have queried most sharply even the theoretical feasibility of my third criterion).
Except under artificially reduced circumstances, variables are in continuous interaction, and this interaction is not simply a matter of addition.
A commonly held example of such an error comes not from neuroscience but from genetics, where for a long time there was a rather simple-minded assumption that the physiology and behaviour of an organism (its phenotype) could be arbitrarily  divided into two components, one given by the genes, the other by the environment.
An organism's phenotype was thus believed to be almost entirely accounted for by the sum of these two apparently independent variables.
In fact, of course, genes and environment interact in highly non-linear ways during development, and attempts to partition the phenotype out into a genetic and an environmental ‘component’ are doomed to failure.
Pat Bateson's analogy is to consider baking a cake.
In cake-making, a variety of components, flour, milk, butter, sugar, spices, eggs, etc., are added, mixed and heated together.
Although each component is necessary to the final taste of the cake, to ask how much of the taste is contributed by the flour, how much by the eggs, how much by the time and temperature of baking makes no sense; the mixing and baking have qualitatively transformed the components.
Simple-minded formulae about the additive relationship of genes and environment — although they still appear, often without even a health warning, in standard genetics text-books — bear little relationship to what happens in real life.
So too in memory research; if an animal cannot be shown to have learned except by changing its behaviour and this change in behaviour can only be induced by some form of stress or constraint, then the changes in biochemistry that one finds in relation to the learning must include the changes in relation to the stress — including all the types of neuromodulators discussed in the last chapter.
And yet, within the artificial world that the laboratory enables one to create, we can and must isolate the variables, and, if we are clever and lucky enough, we can discover how to fit them back into some meaningful real-life pattern.
We have no choice but to deduce as much as we can about the baking of the cake from studying what happens if we miss out a constituent, alter the temperature or cooking time or whatever…
CRITERION FOUR
If the cellular or biochemical changes are inhibited during the period over which memory formation should occur, then memory formation should be prevented and the animal be amnesic; and vice versa.
Clearly this is logically necessary, and in the ‘forward’ direction is the basis for the interventive strategies making use of protein  synthesis inhibitors that I discussed in the last chapter.
The problems arise because in practice no inhibitor is a ‘magic bullet’ with a single target and without so-called ‘side-effects’, so an experimental finding using an inhibitor is not likely to be unambiguous.
The baroque curlicue I have added to this criterion lies in its last words.
Suppose one trains an animal on a task that normally results in it learning, but adds some treatment that prevents the learning from occurring, then if the biochemical process under study is really a process which is specifically associated with the memory formation, it should not occur if the memory is blocked.
I describe experiments making use of this criterion in the following chapter.
CRITERION FIVE
Removal of the anatomical site at which the biochemical, cellular and physiological changes occur should interfere with the process of memory formation and/or recall, depending on when, in relation to the training, the region is removed.
This may also seem obvious: a logical analogue of the preceding two criteria.
If the changes in connectivity which form the memory are localized to a particular small set of cells and their connections within the brain, rather than being widely diffused, then removal of the set of cells should also remove the memory — or prevent it from being formed.
Granted that it is experimentally quite simple to make small localized holes in the brain without causing widespread damage, it should be easy to test the claims for any such region of being ‘the’ site of memory for any particular piece of behaviour.
If the result of making such a lesion is amnesia or failure to learn, it would support the claims for having found such a memory site.
However, this does imply a rather static and mechanical view of the way in which memories may be fixed within the brain.
If the process of storage is more dynamic, perhaps with multiple sites being involved, then the experiment won't work.
Also it ignores the possibility of the brain's own plasticity — that is, if one site is removed, another may become available to take over its tasks — issues with which the experiments of Chapter 11 will have to try to come to terms.
Finally one must never forget the fundamental ambiguity of all lesion experiments — remember Richard Gregory's radio and its howl-suppressing transistor.
CRITERION SIX
Neurophysiological recording from the sites of cellular change should detect altered electrical responses from the neurons during and/or as a consequence of memory formation.
If a hebb-type hypothesis about memory being stored in the form of altered synaptic strengths is valid, then these altered connections should be associated with changed electrical behaviour in the cells the synapses connect; that is, the firing patterns of the neurons should change as a result of training.
The way I have phrased this criterion implies that we should begin by looking for the biochemical and cellular changes and then on this basis seek the neurophysiological ones, and that in some way the neurophysiology is a mere incidental product of the biochemical and structural changes.
Of course, this way of looking at things reflects my own bias as an only partially reconstructed neurochemist; in practice the neurophysiology may well lead — indeed, in the important cases of Aplysia and long-term potentiation discussed next, has led — the biochemistry and cell biology, pointing the way towards cells whose electrical properties and therefore their biochemical properties change during memory formation.
I do not mean to imply that the biochemistry is primary, or any more fundamental in the reductionist sense than the physiology; what I am saying is that changed biochemistry translates into changed physiology just as it does into changed behaviour.
These six criteria, then, have shaped my own research from the start of the 1980s.
I have tried to identify biochemical, morphological and physiological changes occurring in specific regions of the chick brain in the minutes to hours following training on a simple task, to show that the changes are not the results of other aspects of training than memory, to show that blocking the changes prevents the memory, and vice versa, and, finally, to examine the consequences of removing the brain sites of change, either before or after the chick has been trained.
What I have found using these criteria will be the subject of the next two chapters, of which the chick is the sole and proud subject, but the rest of this chapter will be concerned, not with my own experiments, but those of the other hunters for god's organism whose work has dominated the last decade.
THE NEW MODELS
All the sharp criteria and clear theoretical thinking in the world are of no help without good experimental model systems in which they can be explored.
As the 1980s rolled on and the earlier uncertainties about choosing the right task and the right organism receded, consensus began to develop around a small number of such models, with several different groups of researchers each arguing the case for their own new versions of god's organism.
True, god seemed to have chosen very diverse tasks and species, which bore little relation to experimental tasks that an earlier generation of psychologists would have recognized as relevant.
But they seemed to work for researchers whose single-minded concern was to delineate the biochemistry, neurophysiology and cell biology of learning and memory.
Thus for those choosing to work with vertebrates, the tendency has been to abandon the mazes and skinner boxes of earlier generations of psychologists in favour of classical conditioning of very simple reflexes (such as heart-rate or eye-blink) in rabbits, where the neural circuitry can be reasonably clearly mapped.
Some other more exotic models I have already referred to in passing.
For instance, Yadin Dudai in Jerusalem, amongst others, has exploited the behavioural and biochemical possibilities opened up by mutations amongst fruit flies; for him and some other neurobiologists Drosophila has become as popular as it has been for most of this century for geneticists.
Their argument has been based on the fact that in general any specific single mutation will result in the alteration or absence of a single protein in the organism as it develops.
Such proteins may be enzymes, or membrane components, for example.
Thus if a learning- or memory-deficient mutant is produced, the deficiency must result from the lack or malfunction of the specific protein.
If one can discover which protein is missing, then one has a clue to its necessary role in memory formation.
In this sense, studying mutations is a bit like using inhibitors to block particular metabolic processes, and has both the strengths and weaknesses of such methods, discussed in the previous chapter and in Criterion Four.
The Drosophila studies have not solved the memory problem, but they have certainly supplemented our understanding of its biochemical mechanisms.
One of the most important results to have come out of the work is the demonstration that similar molecular  those in other larger and more conventionally studied organisms.
This strengthens the claim that there are real universal biochemical principles involved in such mechanisms of neural plasticity.
Another very popular focus for the new studies of memory has been the molluscs, for the reasons discussed in Chapter 7, and in particular because of their large neurons and accessible nervous systems.
Although there are several interesting species of land snails, the best-known molluscs amongst neurobiologists, if not gastronomes, have been the giant sea slugs, such as Hermissenda .
However, probably the two most frequently cited and successful model approaches since the 1980s have been the study of the cellular correlates of short-term learning and analogous processes in the sea slug Aplysia and the phenomenon known as long-term potentiation in that sea-horse within the mammalian brain, the hippocampus.
It is with the results of these experiments that the rest of this chapter is concerned.
HABITUATION IN APLYSIA —‘LEARNING IN A DISH'
Ask any graduating neuroscience student which organism had been most extensively used to study the cell biology of learning and the answer would probably be:Aplysia .
Furthermore, asked to identify the researcher central to the project, it is odds on that the name they would come up with would be that of Eric Kandel, charismatic Howard Hughes Professor in the College of Physicians and Surgeons in New York, author of one of the key neuroscience text-books of the decades and prolific contributor to others, tireless and brilliant proselytizer for Aplysia as god's organism for the study of memory, and for reductionism as the methodological and philosophical route to its understanding.
(So committed has Kandel been to reductionism as philosophy as well as methodology that he once gave a talk to an audience of  psychiatrists with the theme of ‘Psychotherapy and the single synapse’.)
Kandel, trained as a psychiatrist, spent a period working on Aplysia with Ladislav Tauc in Paris in the 1960s, saw the potential of the organism, initially for the study of short-term processes such as habituation, and over the subsequent quarter-century in New York he has made its study peculiarly his own and that of the  generations of researchers who have cut their teeth in this Columbia laboratory.
There is no doubt that Kandel and his school have made a major contribution to the study of the neurobiology of short-term processes in memory formation, both in terms of experimental insights and in bringing physiological respectability to a research field which many, in the aftermath of some of the débâcles of the 1960s, felt chary of entering.
Personal relations among researchers working on Aplysia , and between the Aplysia group in general and those working with other molluscans, such as Dan Alkon and his Woods Hole colleagues studying Hermissenda (Chapter 7), have not always been easy and were sometimes abrasive, to the extent that they attracted science writer Susan Allport to devote an entire book to them.
Nonetheless, the main thrust of Kandel's findings and the theoretical framework within which he set them during the 1970s and 1980s have until recently scarcely been challenged.
More and more over the last few years, however, findings both from his own lab and others have tended to enrich the somewhat simplistic reductionist framework within which the Aplysia findings had earlier been set.
To appreciate the critique, it is important first to present Kandel's reductionist case in its strongest form.
Chapter 7 described some of the reasons which made Aplysia a strategic choice for researching the neurobiology of certain basic forms of memory formation.
It has a seemingly simple and limited behavioural repertoire, including various forms of learning, while its relatively easily mapped central nervous system contains only a small number of cells — no more than 20,000 neurons in all, arranged in a system of distributed ganglia and including amongst them a population of very large cells which can be recognized easily and reproducibly from animal to animal.
The key to Kandel's approach has been the study of a simple piece of behaviour which can be studied in the intact animal.
The behaviour is a simple reflex, the gill and siphon withdrawal reflex, and its habituation and sensitization.
This behaviour, or its neural analogue, can, it is argued, be ‘isolated’ within the animal by progressively reduced cell populations.
The culminating step in this reduction is the interaction of two specific microdissected neurons which can be induced to make synaptic contact whilst preserved in isolation in a dish.
Kandel argues that the interactions and responses of these neurons to artificially administered neurotransmitters represent, in ultimately reduced form, the memory for the reflex itself.
What is the evidence in support of this claim? located in a cavity on the top (dorsal) side of the animal; the rear end of the cavity forms a fleshy spout or siphon.
If the area around the siphon or gill is touched, both retract, a form of simple protective reflex.
The neural mechanisms for this reflex include a small number of sensory neurons (some 50 in all) which respond to touch sensations on the skin in the region of siphon and gill; these sensory neurons connect to some 20 different motor neurons, both directly and by way of intermediate neurons (interneurons).
The motor neurons, which are located in the animal's abdominal ganglion, in their turn make synapses onto the muscles that produce the withdrawal behaviour.
A schematic version of this relatively simple circuit is shown in Figure 9.2.
Whilst a single stimulation of the body surface close to the gill or siphon produces a reflex withdrawal, on repeated stimulation the response habituates; that is, the response to the repeated stimuli steadily diminishes and finally disappears completely for a while.
The habituated response can be dishabituated or sensitized by strong stimuli to another part of the animal, say the tail, in which case the response reappears in all its original strength.
Because these are short-term and rather non-specific behavioural changes, they must be regarded as forms of non-associative learning, but important to Kandel's argument is that classical conditioning is also possible; in this the unconditioned stimulus is a shock to the tail, and the conditioned stimulus a mild tactile stimulus to the siphon.
This mild stimulus normally produces only a weak withdrawal; following conditioning a strong withdrawal is produced by the weak stimulus to the siphon as well.
This effect persists for a relatively long time, and as there is a specific relationship between the stimuli and the responses, it is regarded as a genuine form of associative learning.
Kandel and his colleagues began by asking what was the neural circuit which underlies the gill and siphon withdrawal response.
This turned out to be a straightforward problem to address by classical neurophysiological methods, and from then on the research strategy involved a series of reductive steps.
To achieve more precise control over the response and quantify it, the researchers immobilized the slug by pinning it to a stage and standardizing the tactile stimulus by using a jet of water delivered with a water-pick.
The contractions of the gill could also be directly quantified with a photocell.
Granted that the circuitry for the reflex was known, the researchers could then ask the question:.
when habituation occurs, which part of the circuitry is involved?
Do any specific cells or synapses show changes which correspond to the behavioural adaptation?— a question, which, of course, relates to the first of my criteria above.
Being neurophysiologists rather than biochemists, the research group's efforts to answer this question began with the electrical properties of the cells — that is, my sixth criterion.
The technology required to offer an answer to this question, however, involved a further reductive step, by which the active, alive Aplysia was transformed into an inactive, manipulable ‘prepartion.
It is possible to dissect open the animal's body so as to expose the abdominal ganglion and its cells, or even to completely  isolate the ganglion and the nerves connecting it to pieces of attached skin and gill.
With this degree of isolation, Kandel could ignore any other sources of inputs to the system being studied — other peripheral nerves, circulating neuromodulators and so forth.
The large cell bodies of the motor neurons could be found, and, as explained in Chapter 7, the ‘same’ cell repeatedly identified in animal after animal (Figure 9.2).
The living animal has by this process been transformed into something approximating a circuit board in a computer, and the researchers can go about exploring its properties rather as if they were electrical engineers, presented with a novel piece of equipment and trying to understand from scratch how its circuits function.
In this system it is possible to replace the tactile, behavioural stimulus by its neurophysiological analogue, that is, by direct electrical stimulation of the sensory nerve inputs.
Similarly, the muscular output — the withdrawal response — can be generated by direct stimulation of the output nerves from motor neurons to the gill muscles.
This isolated and reduced preparation could then be used to ask where habituation occurred — that is, which bits of the circuit showed reduced outputs in response to repeated stimulation.
By the mid-1970s, it was clear that neither sensory inputs nor motor outputs had properties which corresponded to the behavioural habituation, as neither showed such decrements in electrical response.
It followed that the cells responsible for the habituation must lie centrally, within the sensory-motor interconnections in the abdominal ganglion.
And in accord with this prediction, when recordings were made from the motor neurons within the abdominal ganglion during habituation, it was found that there was indeed a progressive decline in the firing rate of the cells as habituation occurred.
The conclusion was that the ‘site’ of habituation must lie between the sensory input and the motor neuron.
There is still, even in this highly simplified preparation, quite a lot of circuitry and many thousands of cells.
In particular, the sensory neurons make both direct and indirect connections with the motor neurons; the direct connections involve synapses between an axon of a sensory neuron and a dendrite or cell body of the motor neuron (this is known as a monosynaptic pathway); in the indirect polysynaptic pathway the sensory neuron first makes synaptic contact with an interneuron, which itself then synapses with the motor neuron.
(Two further related terms should be introduced here.
When the effect of one cell on another is directly  by way of a modification of the synapse that the first cell makes on the second, this is known as a homosynaptic effect; when the effect of the first cell on the second is modulated by the behaviour of a third cell synapsing on either of the other two, this is called a heterosynaptic effect.
Analysis of the recordings made from Aplysia motor neurons following sensory stimulation shows that they are responding both directly, monosynaptically, and polysynaptically, by way of interneurons.
Asked to predict the most likely site of synaptic plasticity, theoreticians would probably have opted for the interneurons, as these can clearly receive and modulate signals from many different inputs before dispatching them to varied outputs.
The simple learning model proposed by Hebb and described in Figure 6.1 requires the participation of three neurons; that is, it is, if it occurs, a heterosynaptic phenomenon.
However, to many people's surprise, by the early 1980s the Kandel group had shown that ‘the’ locus of habituation was extremely simple; the direct synaptic connection between sensory and motor neurons and in particular, the synapse between a sensory neuron and one particular large motor neuron; the contact was monosynaptic and the modulation was homosynaptic.
Having steadily reduced the preparation from organism to circuit, the stage was set for the final reduction; Kandel's colleague Samuel Schacher dissected out the specific sensory and motor neurons and incubated them together in a dish (a procedure known as tissue culture).
It has been known for many years that neurons, like other cells, can be maintained alive and well under such conditions for periods of many days or even weeks, provided they are kept warm, aerated and well fed with glucose and other essential molecules.
Many types of cell will divide in such cultures,; although neurons will not do so, they can grow, put out axons and dendrites and even make synaptic connections.
In Schacher's cultures, the sensory neurons form synapses onto the motor neurons, and electrical stimulation of the sensory neuron results in the motor neuron making an electrical response in turn.
Repetitive stimulation of the sensory nerve resulted in a steady decrement in the motor neuron response; it was, in effect, habituating, and the Kandel lab had produced what he was to describe as a single synapse which showed ‘learning in a dish’.
This was indeed a dramatic triumph for the reductionist strategy the group had pursued, and seemed set to vindicate Kandel's claim that the goal  of his research was to discover the ‘cellular alphabet’ of learning.
The sensory-motor synapse would certainly seem to be one letter of such an alphabet.
All that has been described so far is essentially the province of neurophysiology.
What can be said about the biochemical mechanisms involved in the response, at any of the levels of cellular organization Kandel has studied?
If habituation occurs by reduction of the postsynaptic response at a single synapse, it could logically be a consequence of either pre- or postsynaptic processes, or of course a combination of both.
For instance, there could be a steady reduction in the amount of transmitter released by the presynaptic cell, or a modification of the receptors on the postsynaptic side to make them less responsive to a given amount of transmitter released, or both mechanisms could be operating.
This question of pre-versus postsynaptic plasticity has been a major source of polemic in recent years, but with most theoreticians favouring the postsynaptic side as the main site of plasticity.
For the Aplysia group, the first neurochemical task was to identify the transmitter involved in signalling between the two cells, which turned out to be the ubiquitous substance serotonin (sometimes called 5-hydroxytryptamine or 5-HT).
By the mid-1970s, they had shown that during habituation in the isolated ganglion there was a steady decrease in the amount of serotonin released from the sensory presynaptic terminal, without there being any change in the responsiveness of the postsynaptic serotonin receptors.
The decreased release of serotonin was also associated with a change in the presynaptic membrane properties, in particular, with a reduction in the flow of calcium across the membrane and into the synapse — again the biochemical significance of this will become clear later.
In parallel experiments the group showed that sensitization, which is in some ways the reverse of habituation (see Chapters 6 and 7), also involved presynaptic processes, this time requiring an increase in serotonin production and calcium entry into the cells.
That both these processes, habituation and sensitization, involved presynaptic mechanisms came as somewhat of a surprise for neural modellers.
A decade later, parallel experiments were made with the isolated cells in culture, with similar results.
Learning now seemed no longer even to require two cells in culture, but could be completely mimicked by squirting serotonin onto an isolated motor neuron.
It is hard to get much more reduced than this!
I will have much more to say about the biochemistry of these events in the context of my own experiments in the next chapter; I don't want to get into great detail here but instead would emphasize that Kandel explains the reflex and its habituation and sensitization by a series of reductions.
He first translates the complex gill and siphon withdrawal behaviour of the intact organism into a circuit which habituates as a result of interactions between just two cells.
He then explains the electrical — that is, physiological — response of the synapses in terms of a cascade of biochemical processes in the presynaptic neuron.
LONG-TERM MEMORY IN APLYSIA
If the mechanisms which Kandel has uncovered for the short-term processes of habituation and sensitization can serve as a model for short-term memory, what have they to say about long-term memory?
What the Aplysia group needed was some process in their favoured animal which could be unequivocally recognized as long-term memory and whose circuitry could be studied in a similar manner to that they had so effectively employed with the short-term processes; hence the attention paid in the early 1980s to finding an analogue of classical conditioning of the gill and siphon withdrawal reflex.
For such conditioning to occur, the animal must learn to respond to a mild stimulus which would not normally cause the withdrawal in the same way as if it were a strong one, such as a shock to the tail, which does cause withdrawal.
The experimental design was perfected in 1983 by Kandel's associate Tom Carew, who was able to mimic the pairing of conditioning and unconditioned stimulus in the reduced preparation.
The unconditioned stimulus was replaced by the repeated firing of the sensory neuron, and the conditioning stimulus by squirting serotonin onto the cell.
The key feature of associative learning is that, unlike habituation or sensitization, it is a long-lasting effect, and all the mechanisms discussed so far have been transients.
If my second criterion is to be fulfilled, during associative learning in Aplysia there must be longer-term cellular changes which match the longer-term change in behaviour.
Although as long ago as the early 1970s it had been shown that protein synthesis inhibitors were without effect on habituation and sensitization, it was not until the mid-1980s that Kandel turned his attention to the longer-term cellular processes.
By contrast with their failure to affect habituation, the protein synthesis inhibitors did produce amnesia for associative learning.
Hence this type of learning could not be achieved by mere transient modulation of transmitter release; new proteins were being made, and it was necessary to discover which they were and what their cellular functions were.
As a result, Kandel began to make the sort of experiments that those working on the biochemistry of memory had already been struggling with; adding radioactive precursors of protein to isolated ganglia or to cells in a dish, trying to identify the protein products, and to distinguish those made uniquely or in raised quantities during memory formation from the many others.
At the same time it was necessary to speculate as to how transient changes in neurotransmitters such as serotonin, or the flow of calcium ions across the synaptic membrane, might in turn trigger the specific synthesis of the new proteins that long-term memory demanded.
Cellular alphabets or neural systems?
Because facing such questions has brought the work on long-term memory in Aplysia into the same biochemical arena as my own in the chick, I want to postpone considering them for the present and instead look at some of the problems which, in its singlemindedly reductionist approach,Aplysian orthodoxy — at least the orthodoxy of the mid-1980s, as I suspect that the position is now becoming much more flexible — has ignored.
It is important that the purpose of this criticism should not be misunderstood; the theoretical and experimental contributions that the Aplysia group have made to the cellular study of memory over the past two decades have been substantial, but the very intellectual certainty of the group and the charisma of its leader have tended to suppress some of its problems and sideline those who have articulated them.
I don't want here to get mired in issues of personality and priority, some of which have been brought into the public domain by Susan Allport in her book Explorers of the Black Box , but to concentrate instead on some of the more problematic theoretical issues.
Some of those unhappy about the strong claims of Kandel's cellular alphabet metaphor have pinned their arguments to the assumed differences between Aplysia , as an invertebrate, and vertebrate learning.
The Aplysia nervous system contains relatively few nerve cells, but amongst them are some which are rather large.
This can be contrasted with the situation in the vertebrate brain, which contains many but small neurons with a multitude of rich interconnections.
Thus it has been suggested that a key difference between invertebrate and vertebrate brains is that in the former a great deal of power and responsibility could be invested in a single cell or even synapse which in the vertebrate nervous system would be more widely distributed.
Whilst this might be true, for some years now invertebrate neurophysiologists (by which is meant those who study invertebrates, not a special group of researchers without backbones!), who used to speak of their pet organisms as having simple nervous systems, have rephrased their claim, and refer instead to them as having ‘simple’ nervous systems, the inverted commas being deliberately added as a recognition that the complexity of these systems is still many orders of magnitude higher than in the genuinely simple wiring that one might expect of a mere computer.
Many invertebrates, such as insects, have nervous systems packed with tiny nerve cells, as indeed do  molluscs with large brains such as octopus or squid.
Aplysia may be a special case because it is easy to study, but it would be straining credulity to believe that it organized its learning behaviour along fundamentally different principles from those of other invertebrates, or indeed vertebrates with reasonably sized nervous systems.
There is ample room within the synaptic interactions of even 20,000 neurons for their properties to be those of the system rather than of its individual cells, and claims which were once popular that within insect and crustacean nervous systems one could find key ‘command’ neurons have gone the same way as, in eastern Europe, parallel enthusiasm for ‘command economies’— that is, they turn out to be not a good way to organize individual behaviour any more than to run a country.
Another line of attack has been that of some psychologists who have concentrated their fire on the question whether the type of experimental procedure designed to produce associative learning in Aplysia can ‘really’ be said to fulfil the conditions required for classical conditioning.
Such in-fights about terminology, however, concern me here less than some other matters.
Let me phrase these in terms of my criteria of necessity, sufficiency and specificity.
Despite the remarkable analogy between habituation and sensitization in the intact Aplysia and the responses of its isolated sensory-motor synapse, which certainly fulfil some of my criteria, there is a conspicuous gap in the logic.
Although changes at the sensory-motor synapse might occur during the habituation of the gill and siphon withdrawal reflex, they have not yet been formally shown to be either necessary or sufficient for that behaviour.
I have already hinted, in my account of the reductive steps the group employed, that a variety of experimentally or theoretically inconvenient processes that also occurred during the behaviour, such as a contribution of the peripheral nervous system, and some of the polysynaptic inputs onto the motor neuron, were dissected away and no longer taken into consideration.
Can habituation, sensitization, or associative learning of the gill and siphon withdrawal reflex occur in Aplysia if the key sensory-motor synapses are lesioned (Criterion Five)?
And are changes at these particular sensory-motor synapses the only ones that occur during short- or long-term learning?
One of the more persistent of Kandel's critics has been the Calgary-based neurophysiologist Ken Lukowiak.
He points out that the ‘causal’ translation between neural and behavioural  response implied by Kandel has never been tested directly in the intact animal.
For example, if the entire coding for the strength of the response depended on Kandel's single synapse, there should be in the intact animal a direct correlation between the frequency or amount of firing of that specific motor neuron and the strength of the withdrawal reflex.
Yet when Lukowiak looked for such a correlation he could not find it; it seemed as if control of the strength of the reflex was not vested in any single cell of the abdominal ganglion, but was instead a property of the interactions between the ensemble of cells as a system.
Evidence which also points in the same direction has come not from critics but from within the Kandel group itself.
For instance, the morphologists Mary Chen and Craig Bailey have spent several years studying and measuring the synapses of the Aplysia abdominal ganglion.
They find that, when associative learning occurs, there are also characteristic changes in the appearance and number of these synapses (changes which, as it happens, are rather analogous to those we find in the chick; I shall describe them in greater detail in the next chapter).
Some of these changes are transient, perhaps corresponding to short-term processes, and some, especially in the actual number of synapses, seem more permanent.
If this is the case, and long-term memory for the simple association is reflected in a widespread increase in numbers of synapses, it is difficult to argue that the memory is ‘represented, by but a single set of synapses at a particular motor neuron; thousands must be involved, distributed across many cells.
The final piece of evidence in this context comes from the work of Tom Carew, now at Yale.
He has been studying the development of Aplysia from its tiny, free-swimming larval form through a series of intermediate stages to its adulthood, and in particular has mapped the development of the animal's nervous system and of its behaviour.
The capacity to show habituation, he observed, occurs relatively early on in the development of the baby Aplysia , while sensitization does not appear until a relatively late stage.
The very young Aplysia has a nervous system consisting of relatively few neurons, while the period of onset of the capacity to show sensitization as a behavioural phenomenon matches that of a great increase in neuronal number; yet if all that sensitization required was the facilitatory response in a set of three neurons of the network described earlier, it is hard to see why, by contrast with habituation, it should be dependent on such an increase in neuronal number.
I have devoted some time to the Aplysia story here, not only because of the significance of Eric Kandel's achievement in terms of its wealth of experimental data and theoretical model building, but perhaps above all because of the place it has come to occupy, not only in the text-books, making memory research neurophysiologically respectable, but in the framing of the research field.
Despite certain important differences between the experimental findings and interpretations of the Aplysia and Hermissenda research schools, both have contributed to this framing, sharing a common theoretical perspective which leads to an explicit but, I believe, ultimately flawed reductive philosophy and strategy in their search for the mechanisms of memory.
Of course, no creative scientist holds rigidly to a fixed position in the light of new evidence, and only Kandel himself can say how far he has now moved beyond such earlier, campaigning reductionism.
Here it is time to turn to a consideration of what has become in the last decade, and is still as I write, perhaps the single most popular learning model in the trade today.
LONG-TERM POTENTIATION
During the 1960s, sporadic reports appeared in the neurophysiological literature to the effect that, if the neural pathways to certain regions of the cortex were stimulated repetitively at relatively high frequency, there were long-lasting increases in the spontaneous electrical activity of those regions.
This effect, essentially an increase in the efficacy of transmission between pre- and postsynaptic cells, was termed potentiation .
Could such potentiation be a form of neurophysiological memory?
In 1973, in what has become one of the most frequently quoted papers in the literature of memory research, Tim Bliss from the National Institute for Medical Research in London and Terje Lømo described how, working together in Per Andersen's laboratory in Oslo, they had anaesthetized a rabbit, then exposed its hippocampus and the nerves leading to it.
They placed stimulating electrodes onto one of the nerves, known as the perforant pathway, and recording electrodes within a hippocampal region at which the nerves of the perforant pathway made synapses, the dentate gyrus (Figure 9.4).
When they then stimulated the perforant pathway with a train of electrical impulses, at the rate of 10–100 per second for up to 10 seconds, they found an extraordinarily long-lasting increase in the  firing of the hippocampal neurons of the dentate gyrus, persisting for up to ten hours.
They called the phenomenon long-term potentiation , soon abbreviated to LTP.
LTF, however, lasts much longer than a mere ten hours; the effect can also be found in unanaesthetized animals implanted with permanent electrodes, and in such animals the potentiation has been observed as much as 16 weeks after the initial brief burst of stimulation.
The brief period of electrical stimulation of the hippocampal cells had seemingly permanently altered their electrical properties.
Bliss, Lømo, Andersen and many others in the neuroscience community were immediately intrigued by the phenomenon.
It was a large effect, specific, reproducible and, above all, very amenable to physiological — and later biochemical, pharmacological and morphological — investigation.
The mammalian hippocampus was already a very well understood structure; its neural connections, input and output pathways were clearly mapped and easily identifiable from preparation to preparation even if its individual neurons were not as directly recognizable as are those of Aplysia .
A lasting cellular change in output in response to a defined input is at the least a dramatic example of neural plasticity; but even more than this, the very specific form that the response takes could be regarded as a form of memory.
The hippocampus was already well known to be a structure which, in humans and non-human   mammals alike, was in some way involved with memory.
So might LTP be a mechanism by which memories were formed?
Could it not at the least be studied by physiologists as n model for memory?
Although Bliss and Lømo nodded vigorously in this direction by referring to their stimulating procedure as a ‘conditioning train' of pulses, they concluded their paper with enigmatic caution:
Whether or not the intact animal makes use in real life of a property which has been revealed by synchronous, repetitive volleys to a population of fibres, the normal pattern of activity along which is unknown, is another matter.
LTP is q phenomenon that is easily produced and manipulated by classical neurophysiological techniques, so its popularity as a potential memory model is scarcely surprising.
In the years that followed their initial observation, Bliss in London, Andersen in Oslo and an increasing number of labs began to investigate in immense detail the neurophysiology of LTP.
It was shown to occur not merely in anaesthetized and unanaesthetized rabbits, rats and other laboratory species, but also in in vitro preparations.
The hippocampus is a structure which can readily be dissected out from the brain together with its input pathways, such as the perforant pathway.
Its three-dimensional organization is such that thin slices can be cut, as shown in Figure 9.4, leaving the inputs to the cells of the slice intact.
The slice can therefore be maintained and its electrical properties studied in isolation; thus the lost McIlwainian techniques of the 1950s were restored to neurobiological fashion in the late 1970s.
In such a slice, appropriate stimulation of the input pathways will also lead to LTP which persists for as long as the slice can be maintained alive.
Whether in slices or in the intact brain, LTP turned out to have a similar range of properties.
First, the effect is pathway-specific; that is, it occurs only in the cells to which the conditioning train is delivered, rather than spreading across to others — it is thus the result of the functioning of a network of specific connections rather than a wave of diffuse activity; because there are several distinct input pathways to separate areas even within a single hippocampal slice, this specificity can be elegantly demonstrated.
Second, to trigger it requires a repetitive train of reasonably high-frequency pulses: the same number of pulses delivered more slowly is ineffective, so there is a threshold below which LTP cannot be induced; above this threshold, it can be developed  gradually or in an all-or-none fashion, depending on the pattern, intensity and frequency of the conditioning train.
The development of LTP appears to proceed through at least two (possibly three) phases, with a brief initiation period followed by a longer-term maintenance phase, which have been seen as analogous to the transitions between short- and long-term memory.
Third, and perhaps most interesting from the point of view of cellular analogies to memory, by the early 1980s it had been shown that a form of associative LTP is possible.
In this a weak input which cannot sustain LTF in its own right may be encouraged to do so if combined with a strong stimulus arriving from a second pathway.
The two inputs have to be combined or associated in time in the same sort of way that conditioning and unconditioned stimuli have to be combined for association learning to occur.
Indeed it is even possible to produce a form of associative learning in which behavioural and neurophysiological inputs are mixed.
A rat can learn to cross a barrier between one side of a box and the other in response to a signal which consists merely of the train of impulses to the hippocampus as the unconditioned stimulus.
Taken together, all these properties of LTP would seem to make a powerful case for its study as, at the very least, an intriguing model for memory.
THE HIPPOCAMPUS AS A COGNITIVE MAP
What probably clinched the appeal of hippocampal LTP as the memory model of the 1980s was the increasing body of evidence coming from psychologists concerning the role of the hippocampus in animal learning.
Whilst the human studies had suggested a role for the hippocampus in the transition between short- and long-term declarative memory — a role supported by study of hippocampal lesions in monkeys — another facet of the hippocampus's role in memory was being uncovered in rats.
Here, one of the striking effects of lesions is to affect the animal's capacity to learn spatial tasks — for instance to run mazes.
Although this effect had been shown previously with more traditional maze-learning tasks, the best demonstration came in a test devised by Richard Morris, then of St Andrews, now at Edinburgh.
The equipment consists of a circular, high-sided tank, a couple of metres in diameter, filled with warm water which is made cloudy by adding some milk.
The tank is located in a room whose walls contain recognizable  orienting cues; thus on the north wall there may be a clock, on the south a source of light, on the east an animal cage and so forth.
At one point in the tank there is a shelf just below the water level, but invisible because of the cloudiness.
A rat, put in the tank, swims at random until it locates the shelf, more or less accidentally, and climbs onto it.
A video camera mounted above the tank can track the route taken by the swimming animal.
After a few trials, the rat will swim more or less directly to the shelf, locating it by cues in the environment such as the clock, light and cage.
The effects of drugs, lesions and other manipulations can readily be tested in this type of equipment by a study of their effect on the speed and directness with which the rat can find the hidden shelf.
Indeed so popular has it become that its designer has achieved the ultimate scientific accolade of eponymy (having a phenomenon, method or piece of equipment named after oneself), for the apparatus is known as the ‘morris water maze’(Figure 9.5) and within the last few years has virtually replaced the skinner box as necessary equipment for all psychology labs.
How does the rat learn to locate the invisible shelf?
Does it measure the distance it has swum from the start-point, for instance, or does it orient by use of the environmental cues given by the objects visible on the walls surrounding the tank?
Such possibilities are easy to test.
Altering the location at which the rat is put into the tank is almost without effect on its capacity to find the shelf.
On the other hand, if the room cues are rotated so that, for instance , the clock now  appears in the south instead of the north, the rat will become confused, swimming to the region of the tank at which the shelf would have been relative to the clock if the latter had not been shifted.
Thus the animal locates itself in space by use of environmental reference points (this is of course more or less what a psychologically untutored lay person might have guessed would happen, but it was not what psychologists brought up on a diet of Skinner would have theorized).
Lesioning the hippocampus, however, profoundly damages the rat's capacity to learn or remember the spatial cues and thus dramatically impairs its capacity to work out an effective escape route in the tank.
The water maze offers a number of advantages for the study of spatial learning in that within the tank the animal is quite unconstrained as to the direction it may take, though this must be balanced against the fact that the swimming task is somewhat  stressful and the animal is learning how to reach a relatively precarious goal.
Before Morris introduced this type of maze, it had become common practice to study spatial learning in a more conventional version of the same task, in which rats are placed in various forms of radial mazes with four, six or eight arms, and must learn to run to a goal box containing food or water at the end of one of the arms.
Again, there are cues both internal to the maze and on the walls surrounding it, and the maze can be rotated relative to these external cues.
This type of maze allowed David Olton in Baltimore and John O'Keefe and Lynn Nadel (both expatriate Americans, then working at University College, London, though Nadel has since returned to the US, to Tucson, Arizona) to distinguish between working and reference memory cues in learning the task.
The rats can use the cues in the maze itself — for instance, ‘turn second right at this point’.
This is n form of working memory, as the cue is meaningful only if the animal remembers where it has just come from.
But the rat can also refer to cues offered by the external environment — for instance, to use the rule ‘turn left in relation to the clock on the wall’— as fixed or reference memory cues.
O'Keefe and Nadel implanted recording electrodes into the rat's hippocampus and studied the electrical activity of hippocampal cells during the learning of such spatial mazes.
A fair proportion of the cells they recorded from gave rhythmic bursts of high-frequency firing, at the rate of some 4–12 per second, more or less irrespective of what the animal was doing; this rhythmic activity is interesting because it corresponds with the so-called theta rhythm of the EEG, and may be an aspect of the attentional processes necessary for the learning or remembering of particular activities.
However, even more interesting were the large number of cells which seemed active only when the rat visited a particular place in the maze and/or carried out particular behavioural acts (food searching, drinking or whatever) in that place.
O'Keefe and Nadel called such cells ‘place cells’ and the parts of the environment in which they are active ‘place fields’.
From such observations they generalized to a theory of The Hippocampus as a Cognitive Map , the title they gave to their 1978 book.
The title not only confirmed the centrality of the hippocampus to studies of animal learning, but was also symbolic of the conceptual shift amongst psychologists away from the crudities of behaviourism and simple associationism towards an understanding  of animals, like humans, as cognitive organisms.
Cognitive behaviour is not reducible to simple sequences of contingencies of reinforcement but instead reflects goal-seeking activities, hypothesis making and many other features which had hitherto been dismissed from consideration within the Anglo-American tradition in psychology.
The concept of a cognitive map, in O'Keefe and Nadel's hands, is more than just a topographic representation of the space in which the animal is located; it also describes the distribution of cell systems concerned with the analysis and integration of spatial cues within a framework of behavioural meaning for the animal.
Although in the O'Keefe model there are indeed specific place cells (and during the 1980s other researchers were able to identify, in monkeys, cells which fired in response to even more precise   inputs, such as photographs of particular faces: the concept of a cognitive map is in many ways the precise antithesis of the ‘cellular alphabet’ model of behaviour offered by Kandel.
No way would it be possible to dissect out one of O'Keefe's place cells and show it ‘learning in a dish’; the cell's responses are meaningful only in the context of the entire nervous system and the behaving organism in which it is embedded.
The combination of the reproducibility and reliability of long-term potentiation as a physiological phenomenon, the evidence of the central role played by the hippocampus in mammalian memory and the renewed enthusiasm about the prospects for productive research into the cellular processes of memory produced, in the early 1980s, an extraordinary bandwagon in hippocampal studies.
Labs which for years had worked on more classical memory tasks found themselves funded to purchase morris mazes and set up LTP facilities.
It helped that the hippocampus is very easy to work on.
It is present in standard experimental animals such as rats and rabbits which psychology and neurophysiology laboratories were familiar with, and did not require more exotic facilities like seawater tanks for Aplysia or a knowledge of a novel neuroanatomy like the chick; and the techniques for investigation, such as recording electrodes and drugs, were all to hand.
The phenomena of LTP can be studied at many levels from the more or less intact organism to the tissue slice.
By the end of the decade more papers were being published on the hippocampus than on virtually any other brain structure, and it even warranted a research journal entirely devoted to it.
Even previously committed invertebrate labs like Alkon's Hermissenda group were making the switch.
THE BIOCHEMICAL MECHANICS OF LTP
The neurophysiological parameters of LTP having been mapped with exquisite precision, the question of interest became its cellular mechanism.
As it is a completely physiologically induced, and in that sense artificial, phenomenon, some of the criteria relevant to memory summarized at the beginning of this chapter are not relevant at this stage.
What becomes of interest are the cellular processes which initiate and maintain LTP and what happens if these processes are inhibited.
Also as more and more brain regions were found to show LTP-like effects, it became  important to know whether LTP is one phenomenon or many —; that is, whether the mechanism whereby it is initiated and maintained in one region is the same in others.
Because LTP is a postsynaptic effect — that is, it occurs in a neuron as a result of incoming stimuli along a pathway which synapses on it — one of the first questions was to identify the neurotransmitter involved in this signalling.
It was soon apparent that the vital molecule was the transmitter amino acid, glutamate, well known as one of the commonest of the excitatory neurotransmitters of the brain and present in high concentration within neurons.
Like all transmitters, glutamate is released from a presynaptic terminal when the nerve axon running to that terminal fires.
Annette Dolphin, working with Tim Bliss, showed that, when the perforant pathway is stimulated in vivo , there is an increased release of glutamate in the hippocampus, and the biochemical mechanisms of this release were mapped in some detail by Marina Lynch.
The glutamate is released from the presynaptic side of the synapse between the incoming perforant nerve and the hippocampal neuron.
On this basis, Lynch and Bliss were to argue, rather as Kandel had done earlier for serotonin in Aplysia , that it was presynaptic plasticity that was important for the initiation of LTP, and the postsynaptic cell was simply doing what it had to as a result of the increase in the strength of the glutamate signal it was receiving.
Nothing in biology turns out to be simple, however.
Although glutamate is one amongst many dozens of transmitters, it itself interacts with postsynaptic cells in several different ways; there are at least three different types of postsynaptic glutamate receptor, each differently distributed amongst cells responsive to glutamate, each with rather different pharmacological properties and each producing rather different types of postsynaptic responses.
Thus although each receptor type responds to glutamate, some will respond to chemically similar molecules as well, others show different forms of specificity.
One class of glutamate receptor is known as the NMDA receptor, because the effects of glutamate can be mimicked by injection of the chemically similar substance N-methyl-D-aspartic acid.
Injection of drugs which can specifically bind to and poison NMDA receptors will prevent the initiation of LTP, though not its maintenance if already established.
Drugs which interact with the other types of glutamate receptor are without effect.
Thus it can be concluded that the  LTP, and in contrast to Bliss's group, other labs reported that there was an increase in the number of these receptors in hippocampal neurons following induction of LTP, thus moving the focus of interest about mechanism from the pre-to the postsynaptic side.
How can an increase in glutamate release, or in the receptors responding to it, result in further pre- or postsynaptic changes?
As with Aplysia , a key player in this process appears to be calcium ions.
If the calcium concentration is increased during incubation of hippocampal slices, then it becomes easier to induce LTP, while if calcium is removed from the medium in which the slices are bathed, then LTP cannot develop.
If molecules which bind to calcium ions and remove them from solution are injected into the postsynaptic cell, then once again LTP is blocked.
This led Gary Lynch, working in Irvine, California, to propose that LTP was initiated in a process involving enhanced calcium uptake into the postsynaptic cell.
As more results have come in, the details of Lynch's model have become enriched, but in an early version, constructed with his long-term collaborator Michel Baudry, the effect of the calcium was supposed to activate an enzyme present in the postsynaptic site which breaks down proteins.
The activated enzyme was then supposed to eat away at the synaptic membrane so as to expose more NMDA receptor sites which, until thus exposed, remain buried in the membrane surface and hence inactive.
More NMDA sites would mean a postsynaptic cell more responsive to glutamate and hence more likely to fire.
However, it is clear that calcium has a multiplicity of effects within the cell, and there are other ways in which it can affect the synaptic membrane.
Amongst the key molecular components of the membrane are a number of proteins which are capable of forming reversible chemical links to phosphate ions.
When a phosphate ion binds to such a protein (this process is called phosphorylation ) the protein changes its shape, curling up or stretching out within the membrane, so as to open or close channels which run across the width of the membrane from the outside to the inside of the cell.
These channels make the membrane permeable to ions or molecules, which can then enter the cell and act as signals for the initiation of the biochemical  cascades which ultimately lead, in ways that I shall describe in the next chapter, to the synthesis of new synaptic membrane components and hence to synaptic remodelling.
There are a number of membrane proteins which can be phosphorylated in this way, some postsynaptic, some presynaptic, and the enzymes responsible for catalysing the phosphorylation are known collectively as protein kinases.
One of these protein kinases is specifically activated by calcium (and is therefore, in another of the acronyms beloved of biochemists, known universally as PKC).
In the late 1980s it became clear from the work of several labs that drugs which inhibited this enzyme could block LTP.
As a result the models for the mechanism of LTP had to be revised to include an effect mediated through the phosphorylation of specific pre- and postsynaptic membrane proteins.
Although the debate over whether the pre- or the postsynaptic changes are the most important is still raging as I write, it is likely to turn out, as is sometimes but not always the case in science, that both camps are more or less right.
The mechanisms are likely to involve changes at both sides of the synapse.
IS LTP MEMORY?
Hippocampal long-term potentiation is clearly a model system of immense promise — and indeed one which has already yielded much information — about the ways in which neurophysiological changes can be translated into biochemical and structural mechanisms.
However, I am not convinced that the most relevant biochemical questions have yet been asked, partly at least because so much attention has been devoted to the intimate synaptic processes involved in the initiation of LTP that surprisingly little has been directed towards what seems to me most interesting about it — the very long-term nature of the phenomenon.
Changes in the entry of calcium ions, or the phosphorylation of membrane constituents, or the activation of NMDA receptors, all seem plausible ways of bringing about a temporary change in the electrical properties of a cell, but what makes the change persist — what puts the L into LTP — should be the important question, if LTP is really to serve as a model for long-term memory.
The particular power of LTP as a model, though, apart from the possibility of moving readily between levels of analysis even more strikingly than is the case with Aplysia , from intact organism to slice, lies in its geometry.
The cellular and biochemical changes which  must be translated into behavioural processes such as memory formation must be precisely located in space and time, as the criteria with which I started this chapter have emphasized.
The hippocampus is one of the regions of the mammalian brain whose structure, connectivity and geometry are well understood and which should therefore in principle make such a mapping possible.
For memory modellers it is therefore particularly rich in offering the possibility of playing with hebb-type learning rules in synapses whose connections are genuinely understood rather than merely guessed at.
But to return to the question with which I began the discussion of LTP, is it really a model for long-term changes in the nervous system, or is it something more, a mechanism for memory itself?
The case in favour of its being a mechanism by which real memory is stored in the brain derives primarily, as I have implied, from the known role of the hippocampus in various forms of memory processes, and the fact that forms of associative LTP can be shown to occur.
But beyond this point, the arguments become inferential.
For instance, LTP is increased in rats trained to find food in an operant task, while drugs which block LTP also prevent learning in tasks such as the water maze.
Aged rats lose their capacity both to learn new tasks and to show potentiation.
Match these arguments against the criteria with which this chapter began, however, and it will be seen that they are far from conclusive.
Two pieces of evidence seem to me to cast some doubt on the drawing of direct parallels between LTP and memory.
The first is that it increasingly appears that LTP-like phenomena are not restricted to the hippocampus, but can under appropriate circumstances be shown in many other regions of the brain, including especially the cerebral cortex as Lynn Bindman in London, and Lyosha Voronin, in Moscow, have shown.
Thus the special arguments about the relation between LTP and hippocampal memory seem to be placed in doubt.
And, in a fascinating study, Carol Barnes, whose earlier research includes the correlations between ageing, learning and LTP mentioned in the previous paragraph, has allowed rats to live in the type of enriched environment typical of the experiments of Rosenzweig, Bennett and Diamond described in Chapter 6.
She found that, the longer the rats had been exposed to the enriched environment, the less hippocampal LTP could subsequently be induced.
My — though not necessarily her — interpretation of this observation leads me to ask whether in fact the capacity to show LTP might not be a purely artefactual phenomenon, which occurs only in animals which have been reared in the highly restricted environment of a research laboratory?
Perhaps their hippocampal cognitive mapping capacity is simply desperately starved of use in a lab, and so responds almost greedily to the novel inputs offered by neurophysiological stimulation of input paths.
In a more normal wild environment, where such stimulation, and far more beside, were a day-to-day occurrence, perhaps LTP would no longer be observed at all.
That is, LTP might be an artefact, a result of the special deprived rearing conditions in which laboratory animals are raised.
It would be no less of an interesting phenomenon, but even more anomalous than Bliss and Lømo's cautionary words in 1973 would imply.
Someone should test this possibility — perhaps by seeing whether LTP can still be induced in animals which have led their lives in a genuinely wild and unrestricted environment — before too much more theory-making is built around it.
NOBODY HERE BUT US CHICKENS
At ; six detour chapters, and I can get back to my chicks.I'm sorry it's been such a long journey, but I couldn't find a quicker route.
A few more paragraphs, and we'll really have arrived.
In 1977, I took my first ever sabbatical, and spent a couple of months at the Australian National University in Canberra.
I chose Canberra because there was quite a nest of chick workers there, including one of my own past students.
By this time I was becoming a little dissatisfied with the imprinting set-up that had become the stock in trade of my lab over the past few years, and I planned to try using an alternative form of learning in the chick.
The Canberra group had begun to study the effect of a variety of drugs on memory formation in the chick, using a novel learning task they had adapted from work done in Los Angeles by the veteran neurobiologist Art Cherkin.
Cherkin's own description of how that task was developed during the mid-1960s was characteristic of the man.
He had been watching young chicks, he said, and noted how they explored their environment by pecking at crumbs or other small objects, including their own droppings, but quickly learned to distinguish edible from inedible items.
‘Well,’ he said, ‘I was damned if I was going to have people going around talking about my system as the Cherkin shit experiment.’
So he and Elaine Lee-Teng hit upon the device of offering the chick a small coloured bead to peck.
If they made the bead taste bitter, by dipping it in alcohol, or quinine, or the pungent methylanthranilate, then the chick would peck once, show disgust by shaking its head vigorously and wiping its beak  on the floor of its pen, and then back away, refusing to peck at a similar but dry bead offered any time from a few seconds to a few days subsequently.
This is the basic one-trial passive avoidance learning model that had attracted me.
It is one-trial, because it requires only a single peck for the bird to learn; it is avoidance because the result of the learning is for the bird to stop doing something it otherwise would; and it is passive because the bird is not required actively to avoid, as it would if it had to escape from some unpleasant condition, but merely to refrain from pecking.
Dramatic as imprinting is as a form of learning, it suffered from my point of view from the problem that for a bird to become imprinted requires exposing it to the stimulus, the flashing light or whatever, for a couple of hours; memory builds up slowly over that time, and so the cellular changes that are going on during the period inevitably intermingle the effects of learning and of visual stimulation with those of memory formation.
Pat, Gabriel and I had spent some years unpicking these variables, but if I wanted to study the cellular events occurring in the minutes after the behavioural stimulus had ceased and representing distinct stages in memory formation, then imprinting wouldn't be the model of choice.
Passive avoidance learning, just because it was a precisely timed, one-trial task, seemed to offer that prospect.
Further, there was a practical point.
The apparatus required for imprinting, and then for measuring the efficacy of the imprinting response, was large and elaborate; it was impossible to train more than a few birds at a time.
This didn't matter so much for physiological or anatomical studies, where one could only work with small numbers of animals anyhow; but for biochemistry, when larger numbers were needed, it made progress very slow.
All that the passive avoidance training required was a set of simple, small 20 by 25 centimetre  pens into which a couple of chicks could be placed (in the US, they used quart-sized milk cartons)— the set-up I described way back in Chapter 2.
And testing meant simply showing each bird the bead again for a fixed period of time — perhaps ten seconds — and noting its response.
By the time I arrived in Canberra the person who had set up the passive avoidance work there, Marie Gibbs, had moved to La Trobe, a campus in Melbourne, several hundred kilometres distant.
Her main interest was in the time course of memory formation, and she had been using a variety of drugs, including agents which disrupt entry of ions such as potassium into the cell, and also protein synthesis inhibitors, to dissect out a series of phases, which she described as short-, intermediate- and long-term memory.
Each phase was, she argued, sensitive to a different class of drugs.
Whilst the earliest phase lasted only a few minutes after the training trial, and the intermediate ones declined within the hour, long-term memory seemed to build up slowly over the first hour after training, and protein synthesis inhibitors would no longer disrupt it if they were administered more than an hour after the training (Figure 10.1)
My interest, however, was in the biochemistry, rather than the pharmacology, of the memory formation.
Marie and I agreed that she would train the chicks at La Trobe, using exactly the protocol that she herself had modified from Cherkin, which is essentially how we still do it even today, as I described it back in Chapter 2.
She would then freeze and code the brain samples and send them to me —; by air, as there were no other practical links — in Canberra.
There, I would dissect the brains into the same crudely defined regions we had adopted for the imprinting studies and do the biochemistry.
At the time, I was particularly enthusiastic about the possibility of the involvement of one of the major neurotransmitters, acetylcholine, in memory formation, and had set up a simple, fast assay for the brain's acetylcholine receptor (called the muscarinic receptor, to distinguish it from other types of acetylcholine receptor).
I already knew that the forebrain roof of the chick was particularly rich in the receptor, and the assay method was fast enough for me to measure it in a couple of hundred brain samples during a single twelve-hour day (although, inevitably, it took several subsequent days to analyse and calculate all the results).
The plan was for Marie to train groups of chicks to avoid the bitter, methylanthranilate-coated bead.
A matched group of birds would be ‘tained’ by being given a water-coated bead to peck instead.
These birds would later peck a dry bead when offered it on test and therefore serve as controls for the methylanthranilate-trained group, for they had not learned to avoid the bead.
(If you've followed the logic of the earlier chapters you will at once see that this isn't a perfect control, as the water-trained birds may be learning something else about the bead — but I'll come to that later.
For the moment it seemed a simple enough experiment to do.)
I could then measure the amount of the muscarinic receptor in  brain regions from the methylanthranilate-trained and the water-control birds at various times after they had pecked the bead, to test whether there were any transient or longer-lasting changes in the amount of the receptor.
There should have been plenty of time for all the work we planned, but what with all the delays of getting the assay going in an unfamiliar lab, as well as making a quick canter round a dozen or so Australian campuses to give seminars, it wasn't until almost the last few days of my visit, during a long car journey through the outback to attend a biochemistry congress at Brisbane, that I managed to decode and assemble all the data.
To my delight, thirty minutes after training on the bitter bead there was a substantial increase in the amount of the receptor in the same region of the brain in which we had found changes during imprinting.
The increase was transient, though, for it had disappeared again by three hours after training.
Elevated muscarinic receptor seemed therefore to be associated with the early phases of memory formation.
Of course, the experiment was not conclusive — as any inspection of my criteria in the last chapter would make clear.
For instance the increase could have been due to the taste of the methylanthranilate itself rather than the learned association of pecking and tasting.
After I had gone home, Marie checked this by the somewhat crude device of blindfolding the chicks and putting a cotton bud dipped in methylanthranilate in their bills.
There was no increase in the muscarinic binding under these conditions.
However, I was still a little cautious, and waited until, a few months later, she paid a return visit to England.
By that time I had copied her training set-up in my own lab, and we were able to repeat the entire experiment as before.
The effect held up (although this time it was somewhat smaller), and we published the first paper describing the results in 1980.
With the success of the muscarinic experiment, I felt committed to the passive avoidance model, to the exclusion of almost everything else.
We dismantled the imprinting equipment to make way for the passive avoidance pens and I set about raising the grant money to let us move into full swing.
In the years since 1980, the lab has worked almost exclusively with the task.
There are several sorts of truth I could tell about this decade of work.
One would be the version to be found in the published research papers, those strange constrained pieces of writing whose  conventions are as rigid as a sonnet, whose scientific account is always a set of clearly designed, unambiguously conducted and conclusive observations, building from earlier data and, in their final paragraphs, pointing the way to future experiments (‘more research is needed to…’).
Or I could tell the story behind the papers, a chronology in which I described the sequence of experiments as we actually conducted them.
This would reveal an untidy to-and-fro between problem areas and techniques, from biochemistry to behaviour (our lab is much more magpie-like in picking up techniques than most others I know; for us, anything goes, provided (a) we can afford it,(b) it moves us forward on our central question, and (c) we can find out how to do it — or hire someone who can).
Some obvious and important matters were left to one side for years because I couldn't see a way forward, or had no time to do the experiments — or couldn't find the funds to buy the equipment or chemicals needed.
Others were picked up opportunistically because a visitor or student arrived with just the right skills or interests to move ahead on a front I might otherwise have neglected.
Still others were suggested by a casual reading of someone else's research paper in the train home one evening, or by a talk heard almost by chance at a conference.
In some cases an experiment in progress was transformed in design and intention by a result coming from elsewhere in the lab.
As Peter Medawar pointed out many years ago in his classic essay Is the scientific paper a fraud ? these essential elements in how research is done get refined out from the account as it appears in the finally published papers or scientific reviews, just as they have largely, though not entirely, been filtered from the discussion of Aplysia and LTP in the last chapter.
I wasn't proposing to do it that way for anyone's work but my own!
In this and the next chapter I am going to tell two different stories.
The first is not chronological but logical; that is, it will set  out how, having invented the six criteria with which I began Chapter 8, I have tried to meet them using the passive avoidance task.
This story will, I hope, be convincing, for it is the way I have tried to tell it to my neuroscientific colleagues, but it will be in one important sense economical with the truth.
Not that I am deliberately distorting or mis-speaking our findings, but because I have selected and imposed an order on the research which suits my theoretical and creative purposes and which therefore paints nature in the colours in which I wish to view it.
The second story is different; it reaches no tidy conclusions, for it is still unfolding, and seems to turn my imposed order into something more chaotic once more.
But perhaps it reveals more of the inherent uncertainty of experimental research than the tidiness which precedes it.
But enough of prologues and manifestos; it is time for the first story.
THE FIRST STORY — ORDER OUT OF CHAOS
CRITERION ONE: SOMETHING, SOMEWHERE, HAS TO CHANGE
Begin at the logical beginning.
If memory storage requires alterations in the biochemistry and structure of particular cells, then when memories are formed, something, somewhere must be changing within the brain; but we don't know exactly what or where.
What's worse, although one might start with some hunches in a mammalian brain, the anatomy of the chick brain is very different from that of mammals, and even now not well mapped, so I couldn't afford inspired guesses derived from mammalian expectations — chickens hardly have anything worth calling a hippocampus, for example.
To begin with, therefore, I needed a method that was agnostic about location and mechanism.
Virtually any biochemical process, certainly anything that means that neurons are becoming more active or are synthesizing macromolecules, is going to demand energy.
Energy in the brain comes from burning glucose, so if we could find out if, where and when more glucose was being used in the minutes after training, we would have a clue as to which areas of the brain were relevant to the memory storage process.
Fortunately, there is a relatively straightforward technique for discovering this.
It makes use of the  existence of a synthetic chemical closely related to glucose, 2-deoxyglucose, or 2-DG.
If 2-DG is injected into the bloodstream, it fools neurons (along with all other body cells) into taking it up as if it were glucose.
Inside the cell, the first of the series of enzymes that normally breaks glucose down also thinks the 2-DG is glucose, and therefore converts it into the molecule 2-deoxyglucose 6 phosphate, or 2-DG6P, normally the first step on the pathway of glucose breakdown.
However, the next enzyme in the sequence, which would have started work on glucose 6 phosphate, is smarter, and won't have anything to do with 2-DG6P.
So the substance accumulates in the cell, and the amount that is there serves as a measure for how much glucose the cell is using.
If the 2-DG injected into the bloodstream is radioactive, radioactive 2-DG6P accumulates, and all one then has to do is measure the radioactivity in the cell.
The experiment involves injecting 2-DG into methylanthranilate-trained and control chicks, waiting half an hour or so for the 2-DG6P to accumulate, killing the chicks, removing and freezing their brains, and subsequently counting the radioactivity present.
But the aim is to discover not merely whether there is more radioactivity in the brains of the trained compared with the control chicks, but just where in the brain it is located.
This is where the 2-DG technique is so neat.
The frozen brain is mounted in the laboratory equivalent of a tiny meat slicer, called a cryostat, and sliced sequentially into very thin sections.
The sections are put onto microscope slides pressed against a sheet of X-ray photographic film, wrapped in light-tight black paper and placed in a dark-room.
Then one must wait anything from days to months —; how long depends on how much radioactivity is present — before, eventually, the film, now called an autoradiogram , can be developed (the process is autoradiography ).
Each section will have left an image on the film; the more radioactivity present, the blacker the image will be.
just how dark each region is can be measured in an automatic scanner which passes a tiny beam of light through it and records how much is transmitted.
The black-and-white images can be converted to false colour by computer, which looks much prettier, and is easier to interpret by eye, although it doesn't really give more information.
One can then compare the amount of radioactivity in control and trained chicks region by region and look for differences.
I did this experiment in four frenzied weeks in 1984 with a fanatically  hard-working, Warsaw-based autoradiographer, Margaret Kossut, and repeated them in more detail the following year with a neuroanatomist from Budapest, Andras Csillag, who helped identify the anatomical structures in which Margaret and I had found the changes.
The results were clear.
One region, with the dog-latin anatomical name Intermediate Medial Hyperstriatum Ventrale (henceforward, IMHV), and another, the Lobus Parolfactorius (henceforth LPO), ‘lit up’ in the trained compared with the control animals (Fig 10.2).
What is more, just after training the increase in activity was greatest in the left IMHV and left LPO.
That is, although the chick brain, like the mammalian brain, is bilaterally symmetrical, being composed of two apparently identical hemispheres, the effects of training are asymmetric — when it comes to learning, chicks are left-hemisphere creatures.
These results were important for us in several ways.
First, it was particularly interesting to find changes in the IMHV after passive avoidance training because Gabriel Horn had already been able to identify this as a key brain region for imprinting.
Thus results from passive avoidance and imprinting might begin to converge, which should be good news for both labs.
However, neither Gabriel's lab nor ours had much idea of how, if at all, the IMHV and the LPO might be functionally connected, or what part each region played in the general economy of the brain.
So far as we know, the IMHV in the chick is a bit like the ‘association cortex’ in mammals — a region of the brain where inputs from many different sense systems converge and presumably become integrated.
As for the LPO, no-one was quite clear; some researchers thought it was primarily an ‘output’ region, coordinating motor  responses such as pecking; others saw it as more to do with the bird's emotional responses, which would certainly include fear and distaste.
Second, the results proved something we had already begun to suspect, that there are important functional differences between left and right sides of the chick brain.
There was a lot of evidence accumulating at the time about lateralization of function in bird brains — for instance, it appears that chicks respond behaviourally in different ways when they view things with left and right eyes, while in song birds like canaries and zebra finches, the ‘song centre’ is located in a left-hemisphere region, rather close to our IMHV.
What these differences between the two halves of the brain might be telling us, we had no idea at the time — but some clues will begin to appear by the end of the next chapter.
Third, and of more practical importance, we now knew where to look for any further changes; by being able to concentrate on IMHV and LPO and discard ‘irrelevant’ tissue we might hope to magnify any effect we were studying by diminishing background noise.
The two brain regions are quite small — dissected out, each weighs no more than a couple of milligrams — and Andras invented a special plastic mould into which we could drop the brain, slice slabs out with a razor blade and then use a fine scalpel to cut round the regions, guiding the dissection under a microscope.
We were set to move on.
CRITERION TWO: THE TIME COURSE: BIOCHEMISTRY
If Marie Gibbs' time course was right, I should expect to find a sequence of cellular changes in left and perhaps right IMHV and/ or LPO, associated with the several phases of memory formation, in the minutes to hours following the bird's pecking at the bitter bead.
As the next few paragraphs are going to get quite biochemical, and I see no easy way round them, Figure 10.3 summarizes the whole sequence for anyone who really can't bear the details (you can then skip to page 258).
But they are my bread and butter, and I rather hope they are worth at least a quick read.
We had of course already found a transient increase in the muscarinic acetylcholine receptor.
If I had been working properly and systematically I should have gone back and looked at this —; and other receptors — in detail in IMHV.
It wasn't until some years later that I came back to the question of the receptors and showed that the most dramatic effects involved the NMDA glutamate receptor I mentioned in the last chapter (but won't discuss further here).
Instead, my attention was caught by the evidence coming from the hippocampal work, discussed in the last chapter, about the role of the phosphorylated proteins of the synaptic membrane.
Perhaps because my own PhD, many years before, had been taken up with working on protein phosphorylation without fully realizing its significance (Chapter 3) the temptation to explore it in the chick proved irresistible.
The pre- and postsynaptic membranes can be separated out from IMHV and studied in isolation by centrifugation, rather like the method I described in Chapter 3.
The membranes of course contain both the proteins and the enzyme that phosphorylates them, protein kinase C. If radioactive ATP is added to a tiny sample of the membranes, and incubated together for a few seconds in a miniature test-tube, the membrane proteins become both phosphorylated and radioactive.
Another simple but ingenious technique enables one to separate the individual proteins and measure the amount of radioactivity in each.
The method makes use of the fact that the many hundreds of different proteins in the membrane all differ in molecular weight and electrical properties, each carrying a specific array of positive and negative ions.
To separate the proteins, one makes a small rectangular slab of inert jelly (called a gel), from starch or acrylamide, puts a drop of a solution containing the protein mix at one end, then passes an  electric current across the gel.
The proteins move in the electric current at a speed which depends on their electric charge and molecular weights, and within a few hours they have become distributed along the length of the gel — the procedure is called gel electrophoresis.
The gel is then soaked in a dye that stains the proteins, which then appear as a series of bright blue bands, like ink lines, on the gel.
The band of gel containing each protein can either be cut out with a razorblade and the radioactivity in it counted, or the whole gel can be placed against X-ray film and an autoradiogram made, just as with the 2-DG experiment.
The resulting picture looks like Figure 10.4.
We measured the phosphorylation of the proteins of synaptic membranes prepared from brains dissected at various times after training, and, sure enough, the phosphorylation of one key presynaptic protein was affected thirty minutes after the birds had pecked the bitter bead.
The change was transient; by three hours after training it had vanished.
And when we measured the activity of protein kinase C in the membrane we found that it too increased in activity in the left IMHV thirty minutes after training.
So training produces a brief change in the phosphorylation state of a specific presynaptic membrane protein, regulated by a specific protein kinase enzyme.
But it is only a short-term change, so although it may be necessary if long-term memory is to occur, it cannot be the biochemical representation of that memory.
Something more permanent is required, something that will in some way produce some lasting remodelling of synapses.
It is this remodelling which must require the synthesis of new proteins.
Now proteins are synthesized on the basis of information provided by the DNA, that is, the genes present in the cell's nucleus.
If new proteins are to be made, the DNA must be activated in some way, so as to switch on the relevant genes.
So the changed phosphorylation of the synaptic membrane, which probably results in calcium entering the cell, must act as some sort of a  signal to the DNA in the nucleus.
At present we don't know the exact details of how it works, but towards the end of the 1980s it became apparent that once that signal does get to the nucleus, what happens next in any process of cell plasticity and growth — a step first detected in rapidly dividing cancerous cells, but soon recognized to be a rather universal mechanism — is the activation of a group of ‘immediate early genes’.
These genes are the mechanism by which information arriving at the cell nucleus is translated into instructions for the later synthesis of key structural proteins — that is, proteins which will eventually be inserted into the synaptic membrane so as to change its structure and shape.
These structural proteins are coded for by more orthodox ‘late genes’; all that the early genes themselves do is ensure the synthesis of a group of intermediate signal proteins, rejoicing in even more than usually barbaric names (c-fos and c-jun).
In turn, c-fos, c-jun and their relatives act as further signals to the nuclear DNA, switching on   the relevant ‘late genes’.
This complex cascade of signals is shown diagrammatically in Figure 10.5.
It is the structural proteins which are of real interest, as they go about the business of actually modifying cells; the immediate early gene mechanism is a piece of molecular biological housekeeping, which probably seems arcane not merely to most non-biochemists but to biochemists as well.
C-fos and c-jun are of interest, though, not merely because they provide a key mechanistic link between early events at the cell membrane and nuclear protein synthesis, but because they only become active in cells showing plastic changes, and they can be measured and localized with exquisite sensitivity by variants of the autoradiographic techniques I have already described.
When, in 1989, we started to explore the involvement of this mechanism in passive avoidance learning, there had already been a lot of speculation in the molecular neurobiology literature about whether it would be possible to show that c-fos and c-jun were specifically activated during memory formation; but no-one had yet done the key, unequivocal experiment.
I am no molecular biologist, and wouldn't have dreamed of learning the techniques required to detect the immediate early genes, if it hadn't been for the serendipitous arrival in the lab of a young molecular biologist from Moscow, Kostya Anokhin (grandson of the psychologist and physiologist pupil of Pavlov, Peter Anokhin, whose ‘functional systems theory’ I referred to in passing in Chapter 9).
Kostya had access to the specific molecular biological ‘probes’ that the detection method required and a great appetite for laboratory work.
Within a few weeks of his arrival, we had shown that, half an hour after training (that is, about the same time as the changes in membrane phosphorylation), there is a dramatic increase in the expression of c-fos and c-jun proteins in cells of the IMHV.
We had found a vital step along the route from synapse to nucleus.
By contrast with such complexities, the rest of the biochemistry is relatively straightforward.
Almost the first experiments I had made with the passive avoidance model after completing the work with Marie, and even before we had located IMHV and LPO as the sites of change, looked at the effects of training on protein synthesis in general, using the precursor techniques that have already been described in earlier chapters.
From half an hour after training, to as long as twenty-four hours afterwards, it was  possible to detect an increase in protein synthesis in the brain regions containing IMHV — a result which of course squared with the known amnestic effects of the inhibitors of protein synthesis.
However, because I believed that much of this increased protein synthesis was likely to be associated with the production of new synapses, or the modification of old ones, it was important to look not at proteins in general but at synaptic membrane proteins in particular.
Many of the most important and prominent proteins of the synaptic membrane are of the class known as glycoproteins, which, if the description I gave in Chapter 3 now seems a long way back, can best be summed up as molecules made in two parts; an amino acid chain embedded in the membrane, to which is attached a further chain made of sugar molecules such as glucose, fucose and galactose, sticking out from the membrane into the extracellular space beyond.
These sugar units are ‘sticky’; when one of them meets a matching sugar chain sticking out from the membrane of an adjacent cell, the two recognize each other and become attached.
Thus glycoproteins function as cellular recognition molecules, and it seemed to me that if synapses, which are par excellence recognition and attachment points between cells, were going to be modified by training, then glycoproteins would be involved.
The experiment I was doing all those months ago when I began writing this book, which I described in Chapter 2, involved using the sugar fucose as a precursor for glycoprotein.
In fact, as long ago as 1980 we had shown that, just as amino acid incorporation into proteins increases for twenty-four hours after training, so too does fucose incorporation into glycoproteins of the pre- and postsynaptic membrane.
The problem is that glycoproteins are notoriously difficult molecules to analyse, and there are quite a number of different types in the synaptic membranes.
We have spent a good part of the last decade in a long and often rather frustrating attempt to identify them (most recently by trying to make specific antibodies that will recognize them).
All that I know so far — at least, all that is worth telling here — is that there are a number of different glycoproteins of a variety of molecular weights, on both pre- and postsynaptic sides of the membrane, involved in the response to training on the bead.
THE TIME COURSE CONTINUED: BIOCHEMISTRY BECOMES STRUCTURE
If the hypothesis that the glycoproteins are involved in some form of remodelling of synapses is correct, then maybe one could actually observe and measure these changes in the neurons of the IMHV?
It is relatively easy to prepare brain samples to examine either under the light microscope, with its maximum useful magnification of a few thousand, or the electron microscope, which can magnify by hundreds of thousands.
It is much harder to move from a visual, qualitative appreciation of what can be seen under the microscope to more quantitative measures of how much or how many of any component is present; yet unless something entirely new was being synthesized as a result of training, then what we might anticipate observing would be small changes in the number, pattern or distribution of existing structures, particularly synapses.
Using the light microscope, one cannot see individual synapses, but it is possible to stain individual neurons and analyse the structure of their dendrites, hence picking up possible changes.
If there were changes in the terminals at the presynaptic side, they would have to be measured using the electron microscope, however, for they are not visible at light microscope magnifications.
Doing this sort of quantitative morphology — that is, measuring the shape, number and size of cells in the brain — is, even today, with highly sophisticated image analysis and computing systems, time-consuming and, if one isn't careful, fraught with the danger of misinterpretation.
How many of the hundreds of thousands of cells in each tiny brain region must one study to get a representative picture?
How can one be sure that what is being seen and counted is ‘really’ present in the living brain rather than an artefact, an artificial pattern generated by the techniques required to fix, slice and stain the brain tissue to make it visible?
How can one scale up from what can be counted in a two-dimensional section to the three dimensions of living tissue?
These were the sorts of technical problems I began discussing with my colleague Mike Stewart in the early days of the chick work, when it became clear we would want to try to make this type of measurement.
Mike picked up the problem and ran with it, creating a really first-class lab for quantitative morphology.
But of all the multitude of possibilities, what should we try to measure in the chick?
Back at the end of the nineteenth century, the Milanese anatomist  Camillo Golgi had discovered almost by chance a stain, based on the use of silver salts, which has the capacity to select out, seemingly at random, a small proportion of the neurons in a section of tissue and stain each immaculately, revealing every last detail not just of its cell body but also the dendrites and even the myriad little spines which stud the dendrites' surfaces.
It is interesting that Golgi himself, who got the Nobel Prize in part for this work, didn't believe that there were individual neurons within the brain, preferring to think of it as a continuous network of fibres, and he persisted with this mistake despite the evidence of his own staining technique.
It took the formidable Madrid neuroanatomist Santiago Ramon y Cajal to see the significance of Golgi's achievement (he too got a Nobel Prize, though Golgi apparently refused to accept Cajal's interpretation — or even to speak to him).
No-one who sees a golgi-stained preparation of neurons can fail to be awed by the complexity and elegance their branching patterns reveal.
Even today, decades after I saw my first such microscope slide, I still think them extravagantly beautiful and can easily get lost in contemplation of the cellular thicket the microscope reveals, made the more intriguing by the curious, almost three-dimensional effect the stain gives; as it brings into vision only a few of the total population of the neurons present, the cells seem to stand out like trees in a winter mist (Figure 10.6).
Looking at a picture of this sort, however appreciative one is of its beauty, is a very different matter from quantifying any aspect of it.
What could we measure which might be different about a neuron which had changed its structure in some way as a result of learning?
The surface of each of the dendrites which branch out from the neuronal cell body is covered with synapses — perhaps up to ten thousand in all — arising from the other neurons which thus make contact with them.
Some of these synapses are on the shafts of the dendrites, others are attached to the tiny spines which stud the dendritic surface and which can be seen in Figure 10.6.
Changes in synaptic connectivity between one neuron and another as a result of learning along hebbian lines might involve the dendrites increasing in length, or changing in branching pattern, or the numbers of their spines might alter.
How effective any given synapse is at influencing the postsynaptic neuron in firing depends on a number of factors — how close it is to the cell body, whether it is on the shaft of a dendrite or on one of the many spines, and so forth.
At a synapse, transmitter released from the presynaptic side binds to a receptor on the postsynaptic side, resulting in a change in the electrical properties of the postsynaptic membrane and a small flow of current around it.
The effect that this current has on the rest of the dendrite, and hence in due course the cell body, depends very much on the geometry of the region around the synapse; biophysical calculations show that spine synapses are more effective than shaft synapses in spreading the current, and in any given spine, the current flow is dependent on its exact shape.
Thus any change in the structure of dendrites and the location of the synapses on them can change the neurophysiological relations of pre-and postsynaptic cells.
So neuronal connectivity can be altered not merely by increasing or decreasing the actual number of synapses between two cells, but by altering the size or position of any particular synapse — for instance, one located on the dendritic shaft might be shifted to a spine.
There are good reasons to believe that the branching pattern and shape of the dendrites may also be important and may well change as   a result of training or other types of experience.
Because to make a microscope preparation means that the tissue has to be fixed and stained, what one sees always looks as if it is a very rigid structure, but in the living organism the dendritic pattern of neurons is as mobile as the branches of a growing tree in a gentle breeze, so changed branching patterns are perhaps not so hard to envisage.
Such patterns aren't very easy to analyse.
But amongst the easier and more obvious structures to count are the dendritic spines, and a graduate student of Mike's, Sanjay Patel, did just that in the mid-1980s.
He trained chicks and twenty-four hours later took out and stained left and right IMHV with the golgi method.
He then selected a particular class of neurons, recognizable by their long axons, measured the length of each dendritic branch and counted the spines on each, which lie then calculated as number of spines per µm — that is, millionth of a metre — of dendrite.
Much to our delight (and, I must confess, to my astonishment) Sanjay's counts showed dramatic effects.
Twenty-four hours after training, there was getting on for a sixty-per-cent increase in the numbers of spines to be found on the dendrites in the left IMHV (but almost no effect in the right).
The spines were also slightly changed in shape; it was as if the head of each had been blown up like a little balloon — just what biophysical theory would predict would need to happen if the electrical connections between the pre-and postsynaptic sides were being strengthened when the chick pecked the bitter bead; and exactly the sort of change which might be predicted as a consequence of increased glycoprotein synthesis.
So here was unequivocal evidence for quite major postsynaptic changes in structure as a result of the training experience.
Mike Stewart's morphological methods, however, could go beyond those of light microscopy to that of the electron microscope.
At the levels of magnification we routinely use with the electron microscope, a thumbnail would be 250 metres wide, and the order that the golgi stain seems to offer dissolves into chaos which only a disciplined imagination can control.
Because one cannot simply enlarge the golgi picture to electron microscope size and follow any individual cell and its connections from beginning to end, it is hard to relate the spine changes seen in the golgi pictures directly to particular synapses seen in the electron microscope.
Nonetheless the eye of art and experience can interpret the electron micrographic chaos to pick out individual synapses, cell bodies, axons and dendrites and measure them.
Figure 10.7 shows an electron micrograph of a synapse from the  IMHV marked up to give our interpretation of what there is to be seen.
To be precise, the synapse is the site where presynaptic terminal and postsynaptic membrane are in close contact, recognizable in the photograph as characteristic dark, thick regions where the presynaptic and postsynaptic membranes virtually — but not quite — touch.
This thickening is the area of the postsynaptic membrane which contains the receptor molecules and which traps the transmitter released from the many small vesicles visible packed into the presynaptic terminal.
The number of terminals in a given volume of tissue and their average size, that is, the average volume of each such terminal, are obvious measures to make.
So is the length of the synaptic thickening.
With time and patience, it is even possible (if one is a graduate student or an obsessive) to count the numbers of vesicles packed within each terminal.
Mike, his students and visitors have counted all of these parameters in the synapses of the IMHV and LPO.
In the early studies we chose to look at them twenty-four hours after training on the grounds that any structural change would take time to build up,; more recently he has pushed the earliest time at which changes can be found back to as little as an hour after the bird pecks the bead.
His findings are clear.
There are increases in the numbers of synapses in the LPO, in the numbers of vesicles per synapse, and even in the length of the postsynaptic thickenings in left IMHV and LPO.
All in all, these, along with the changes in numbers and size of dendritic spines, are what one might expect if, when the chick pecks the bead and learns the association between the pecking and the bitter taste, there is a synaptic reorganization in IMHV and LPO to code for — or represent — this new association and the resulting change in behaviour — that is, to say ‘don't peck’ instead of ‘do peck’when the chick sees the bead a second time.
To recap so far, as Figure 10.3 summarizes, pecking at the bitter bead sets off a cascade of biochemical processes in two specific regions of the chick brain; these begin with transient changes in cerebral blood flow and energy use and involve brief increases in transmitter-receptor interactions, which alter the properties of the synaptic membranes such as to increase the efficacy of communication between pre- and postsynaptic side.
These changes in turn provide signals to the cell nucleus, which results in the activation, first of a number of immediate early genes, and later the genes required for the synthesis of new synaptic membrane constituents, especially glycoproteins.
In the hours that follow, these glycoproteins  dendritic spines, and the synaptic contact areas in both left IMHV and left and right LPO.
Not bad, as the consequence of but a single peck!
Criterion three: necessity and sufficiency
So far, so good, but finding a set of correlations of this sort still says nothing about whether they are necessarily part of the memory formation process unless I can find a way of showing that they are not simply the aftermath of the unpleasant experience of tasting the bitter bead; that is, I must meet my own third, reductionist criterion.
Until I can do this, the entire complex cascade I have described in the previous section, and which has taken a decade of work to map, might simply turn out to be a consequence of a bad taste in the chick's mouth and nothing to do with learning and memory at all.
The rather crude experiment that Marie Gibbs made by putting the methylanthranilate in the chicks' bills when their eyes were   closed arid showing that this did not result in a change in receptors is one such test, but, because being blindfolded is itself scarcely a neutral experience for the birds, it can't be more than suggestive.
What I wanted ideally was a situation in which I had two groups of birds, each trained on and showing the disgust response to the bitter bead, but one group then remembering and the other forgetting the association.
The group which remembered should show the cascade, the group which forgot should not.
While I was puzzling over how to solve this problem, I came across a paper written by a Harvard-based neurobiologist, Larry Benowitz, in the early 1970s.
In it he described how, if chicks are trained on the passive avoidance task and immediately afterwards given a mild electric shock across their heads, they seem to forget the association of taste and bead, for when offered a bead later they peck at it enthusiastically once more.
However, if the shock is delayed until some ten minutes after training, memory is not disrupted, and the chicks will avoid the bead.
The explanation for this effect is presumably that the very early events in the cascade of memory formation involve electrical activity within the neurons and that the immediate shock disrupts this process; by the time the delayed shock is given, however, the cascade is already past this phase, and is no longer vulnerable.
This at once gave me the idea for the experimental design I wanted.
I could train birds on the bitter bead, shock them, either immediately or a few minutes after training, and compare the biochemistry in the birds which had tasted the bead but forgotten the association with that in the birds which had tasted and remembered the bead.
We built a simple little device which could administer a mild shock to a chick held briefly in my hand — tested on my finger, the shock is no more than a brief tingle, and the chicks seemed scarcely to notice it —; and I checked out Benowitz' finding.
He was absolutely right; shocked a minute after training, most chicks showed amnesia and so pecked the bead when they were tested some hours later; delay the shock to ten minutes after training and most chicks avoided it subsequently.
Two groups weren't enough for the actual experiment, though; I needed six.
Call them A — F. Three groups (A — C) would be trained on water, three (D — F) on the methylanthranilate, and amongst both water and methylanthranilate birds, one group (A and D) would be unshocked (or rather, to be sure, ‘sham’ shocked — I would go through the motions of shocking them but with the current turned  off), one group shocked immediately after training (B and E) and one shocked after a delay (C and F).
If we then take a biochemical marker, such as, say, increased fucose incorporation in the hours after training, we expect a difference between A and D, due to the training, and between C and F, as the delayed shock group has both tasted the bead and shows the memory.
The crucial comparison is between B and E, since, although E has tasted the bead, it is amnesic and pecks it later.
If the biochemical change is the consequence simply of the experience of tasting the bead, then the level of fucose incorporation in E should be the same as that in D and F, and higher than in all the water groups A — C. If, however, the increased incorporation is associated with memory, then E should be equivalent to B and lower than either D or F. We needed, of course, to repeat the experiment with enough birds — eventually, a dozen in each group — to be sure of the results — but when we analysed the data, they were unequivocal, and I can't resist showing them in Figure 10.8.
The increased incorporation occurs only in the groups showing memory and not in the group which is trained but amnesic.
What's more, the shock itself is without effect on this biochemical measure, as can be seen by comparing the shocked and unshocked water-trained birds.
I was so pleased with this neat and simple control that we have employed it for several other of the key steps in the cascade — finding, for instance, that the increase in dendritic spines occurs only in a remembering and not in an amnesic group.
There were still some lingering doubts in my mind, not because I question the rational logic of this experiment, but because it would be nicer to have an experiment which did not require even the distress of a mild shock to the chicks.
What do I mean by nicer?
The electroshock experiment is logical and elegant, but shocking the chicks is aesthetically (morally??— I am not sure) displeasing, however mild the actual experience.
A couple of years ago Kostya Anokhin and I therefore invented an alternative procedure, one that would not involve the chicks in any type of aversive situation at all—; even that of tasting a bitter bead.
The chicks are placed on a surface scattered with a mix of food grains and pebbles of about the same size and colour of the food.
At first chicks peck at both food and pebbles about equally, but after a few minutes they learn the difference and thereafter pick up the food and avoid the pebbles  (especially if the latter are glued to the floor!).
We divided the chicks into four groups and spread the experiment over two days.
The design is shown in Figure 10.9.
Group K birds are ‘quiet controls’ on both days.
On Day 1, Groups L and M are given a number of sessions on the pebble floor without food; Group N has the same number of sessions, but with food, and therefore learns the distinction.
On Day 2, Groups L and N repeat their experience of Day 1, while Group M experiences the pebble floor plus food for the first time.
In this experiment, only Group M is learning on Day 2, whilst Group N is repeating similar but already learned behaviour to that of Group M. Immediately after the trial on Day 2 we took the birds and looked for the expression of one of the immediate early genes, c-jun.
Compared with the quiet control (Group K), there was an increased  c-jun expression in both the ‘behaving’ groups M and N, but the increase was much more marked in the learning group (M) than in the group which is merely repeating an already learned behaviour (N), even though in fact the chicks of Group N are eating the food grains even more avidly that those of Group M.
Criterion four: Inhibit the biochemistry, inhibit the memory?
The logic of the inhibitor approach is obvious, but for a number of years I was reluctant to get involved in doing such experiments, as it wasn't clear to me that using broad spectrum inhibitors such as those for protein synthesis would tell me anything precise about the biochemical processes I was trying to unpick.
Once we had begun to   look at particular steps in the biochemical cascade in more detail, however, I became persuaded that reasonably specific inhibitors might help cast light on relevant mechanisms.
For instance, we found that, injected before training, the agents which block hippocampal LTP and spatial learning — inhibitors of the NMDA type of glutamate receptor — also produce amnesia in the chick; injected into the left hemisphere just before or just after training, inhibitors of protein kinase C also produce amnesia.
However, perhaps the most interesting of the inhibitors from my point of view was introduced by Reinhard Jork, working with Hans-Jurgen Matthies of Magdeburg, in what was then still East Germany.
Matthies' group, like ours, was interested in the glycoproteins, which they had shown to increase in synthesis during various forms of more conventional training procedures in rats, and Jork had scanned the biochemical literature to find specific inhibitors of glycoprotein synthesis.
He came up with a sugar called 2-deoxygalactose (2-Dgal, which bears the same relationship to the sugar galactose as 2-DG does to glucose).
2-Dgal very specifically prevents the synthesis of those glycoproteins in which the molecule involves a link between the two sugars galactose and fucose; thus it blocks fucose incorporation into glycoproteins.
In Magdeburg he and Matthies found that 2-Dgal injections produced amnesia in rats.
I invited Reinhard to join me in some parallel experiments in chicks; to our delight, 2-Dgal, injected either just before or up to a couple of hours after training, blocked fucose incorporation into the chick brain glycoproteins and produced amnesia in animals tested twenty-four hours later.
So by both the inhibitor approach and the electroshock experiments, the synthesis of specific glycoproteins seems necessary for memory.
Criterion six: Biochemistry translates into neurophysiology
Neurophysiology requires a set of skills — notably, apart from a dexterity in operating on small animals, a reasonable grasp of electronics — that are beyond the range of mere biochemists like me.
My electrical capabilities go no further than being able to wire a plug — and even that we aren't (officially) allowed to do in the lab: the safety regulations demand that a qualified electrician should carry  out this skilled task.
If I were to approach my sixth criterion I needed to find someone who knew their way round an oscilloscope, and it wasn't until the mid-1980s that a bright, if somewhat cranky, scuba-diving enthusiast arrived on campus to do a PhD.
Milton Keynes is about as far from the sea as it is possible to get in England, and Roger Mason's motivation in coming to us was never quite clear to me (perhaps it wasn't to him either, for although after four intensive years' research he produced a many-hundred page ‘draft’ of his thesis, far in excess of what might be required, he finally failed to submit it for examination).
Almost as soon as he arrived on campus, he disappeared into a mass of wiring, flashing lights, bleeping tones and tens of metres of multicoloured printouts on endless rolls of chartpaper.
Even approaching his workzone was hazardous, as one had to pick one's way through suspended diving gear and disassembled bicycles over a floor awash with chartpaper.
Nonetheless, after about eighteen months in technological purdah, he emerged, having resolved most of the equipment problems.
Essentially, what we proposed to do was straightforward.
We would train chicks on water or methylanthranilate, and then anaesthetize them.
The chicks would be maintained on a miniature life-support system in what is known as a stereotactic device.
This is a way of holding an anaesthetized animal (by now of course not an animal at all but a ‘preparation’ which ceases even the pretence of life once the support system is switched off) gently but firmly so that its brain can be exposed and an electrode lowered into it, according to known coordinates.
Such electrodes can be of several types; they may be fine glass tubes filled with solutions of drugs or salts to be pumped out close to specific cells, or they may be ‘stimulating electrodes’ designed to deliver trains of electric pulses — of the sort  described in relation to LTP.
Or they may be fine metal wires intended simply to record the electrical activity occurring in the cells in their vicinity.
We were going to use the latter — extracellular recording electrodes.
The question we would ask would be whether there was any difference in the electrical activity of the neurons in the IMHV as a result of training on the methylanthranilate bead.
This sounds straightforward enough, but masks a lot of difficulties.
Finding the right anaesthetic, so that one can keep the chick functionally alive for the hours required for the recording, proved surprisingly tricky.
Interpreting the electrical traces and distinguishing signals from noise is another problem.
As it takes some time to get the system going and prepare the animal for recording, any one experiment can run for many hours, and as a result neurophysiologists tend, even more than any other lab scientists I know, to be erratic nightworkers and (at least when they are graduate students) not well cut out for normal social relations.
My part in the experiment was easy.
All I had to do was to train chicks on either methylanthranilate or water and hand them over to Roger, who would disappear into the neurophysiology lab with them and emerge many hours later with reams of paper which he would begin to analyse.
He was not told which chick was in which group until we had completed the entire first series of experiments (this is our normal lab practice — where possible, especially when two of us are involved in the experiment, to run the work ‘blind’ until after the data are analysed, to avoid the possibility of unconscious bias).
By the time Roger had recorded from sixteen birds, he told me he thought he could detect regular differences between them so large that he could assign them to the two groups even without being given the code.
When I challenged him to do so he was right in fourteen cases out of the sixteen.
What the traces showed was, as we would have anticipated, a steady background buzz of the spontaneous firing of the cells of the IMHV.
But superimposed on this background were brief ‘bursts’ of high-frequency activity, in which whole ensembles of cells were firing in some sort of rhythmic synchrony (Figure 10.10).
This bursting activity was massively — up to fourfold — higher in the methylanthranilate-trained animals than in the controls which had pecked the water bead.
The increase in bursting seemed to continue for up to twelve hours after training.
It was indeed a bit like an LTP effect, though generated not by the artificial injection of current but by a behavioural experience.
In fact, the analogy may be precise; a  couple of years later other researchers were able to produce LTP-like phenomena in IMHV slices simulated in vitro .
To convince ourselves of the specificity of the bursting effect, Roger and I repeated the experiment using the electroshock amnesia approach I referred to above; the bursting activity, like the biochemical and structural changes, occurs only in the animals which remember the task.
The end of the story?
Thus to form some sort of representation in the brain of the association between pecking the bead and the bitter taste, such as to result in a lasting change in the chick's behaviour, requires a biochemical cascade of events in a localized region of the forebrain.
This cascade results in structural modifications to synapses and dendrites and is reflected in alterations in the electrical properties of the cells, as shown by changes in their spontaneous, rhythmic, electrical activity in the hours after training.
Criterion six has apparently been met.
So have I solved the problem of how and where chicks make memories?
Well, up to a point, Lord Copper, up to a point.
But only up to a point.
All this biochemistry and neurophysiology and structural change is beautiful; ten glorious years of experiments creating an apparent order out of the seeming chaos of the living   world.
I don't think I have been fooled by artefacts, or overinterpreted my findings, though it is obvious even to me, let alone a critical outsider, that in fitting the data within a temporal cascade I have not formally proved all the necessary biochemical links; some of my arguments have run dangerously close to the classical trap of assuming that post hoc implies propter hoc ; just because the phosphorylation step precedes the glycoprotein synthesis I cannot automatically assume that the latter depends upon the former.
But give me the benefit of the biochemical doubt, for that is perhaps not, except amongst professional biochemists, the most important question.
Much more relevant is the issue of whether, even without biochemical caveat, memory can really be such a simple, mechanical process, a straightforward linking up of neurons into some novel network in the IMHV, like rewiring a computer?
Does this mean that Hebb is right?
Are the effects I find specific to the chick — or even merely specific to the young chick remembering about a bitter-tasting bead — or can I legitimately claim that they illustrate some general principles about the mechanisms of memory formation?
And shouldn't I anyhow be puzzled by the sheer size of the effects we find?
Fourfold increases in bursting activity; sixty-per cent changes in the numbers of dendritic spines — and all for remembering a little bead?
If the chick is going to do this to remember everything that goes on during its lifetime, how will it find enough room in its little brain for all that synaptic machinery?
If I don't try myself to answer such questions, I could be sure that someone would certainly raise them.
True, in what I actually do in the laboratory, I am trapped in an artefactual world mediated by machinery.
I do not observe nature, as symbolized by my chicks, in an unmediated way.
Like all scientific findings, mine are actually nothing but readings on meters, printouts on papers, numbers derived from machines (nothing but pointer-readings, the positivist philosopher and physicist Ernst Mach called such observations back at the beginning of this century), which I manipulate to extract meaning and which I then endeavour to extrapolate back to stand for, to represent, deductions about the behaviour of molecules, cells and organisms in the real world.
Nonetheless, I am fairly unmoved by the current debate in the philosophy and sociology of knowledge about the status of realism and of science.
I stand by what I have written above; it is the truth about what I have observed concerning the material universe I study.
Set up a lab like mine and run the same experiments, and anyone should be able to come up with the same  results, for they do not depend on excessively mysterious skills or tricks, and science is after all, in the words of its most passionately admiring philosophers, public knowledge.
But what I have described is interpreted truth — and the public which would share the knowledge is one which also shares my preconceptions (or at least enough of them) about how to interpret it.
Nor is it yet the whole truth; presenting the experiments in the way I have chosen, as I said at the beginning of this chapter, has been a logical way to tell a story, even if a story not yet adequately grounded in theory.
It has been a rhetorical device, then — necessary because (my literary friends tell me) science proceeds by just such rhetoric — but rhetorical nonetheless.
Let me open a new chapter and tell another story.