

4 Strategic Analysis and Corporate Investment: An Economic and Marketing Emphasis
The approaches to investment appraisal discussed in chapters 1 and 2 took an individual-project perspective.
It was assumed that projects exist either with easily specified cash flows or with easily specified underlying factors which give rise to those cash flows.
Some uncertainty was admitted in relation to those factors, and ways of studying the impact of such uncertainties were shown, but then it was assumed that the uncertainties themselves were reasonably structured such that rough probabilities of outcomes could be assigned.
This is in general accordance with the state of corporate-finance literature.
While this may be a reasonable stance for finance academics to adopt, even though the problems in analysing cash flows and given probabilities have been shown to be far from trivial, the practising manager has to recognize the limited scope of the ‘investment problem’ addressed by the current theory of finance.
He (or she) needs to realize that considerable analysis needs to be conducted in order to form sound judgements of cash flows and underlying uncertainties for large projects, and that too ready a move into discounted cash flow (DCF) calculations may give the impression of sophisticated analysis which disguises the inadequacy of the underlying assumptions.
It may indeed be far more important to get the underlying analysis right than to spend time and effort on refining financial calculations, although the dangers of using incorrect discount rates need to be remembered.
It also seems important to recognize that projects are not all the same.
There are differences both in the scale of projects (the replacement of a boiler versus a large-scale merger) and in their economic profile (large companies usually operate in a range of different markets, countries, industries, and so on).
It may be appropriate to end up with a net present value (NPV) calculation for all types of investment, but obtaining the correct decision may depend far more on recognizing the  differences, in nature and importance, of the underlying factors which will affect the projected cash flows.
The current financial-management approach to the investment decision not only lacks insight into how the underlying factors should be analysed, but also fails to recognize the different stages of developing projects.
Back in 1975, King identified six stages in the development of a major project:
1
triggering the search for an appropriate project
2
screening investment proposals, because it is impossible to conduct an extensive analysis on all possible projects and alternative versions of projects
3
definition of the project alternatives
4
evaluation of the alternatives
5
transmission of project information
6
making the decision
He then pointed out that the  financial-management literature concentrated exclusively on stage 4, and with a few exceptions this is still the situation today.
Some will want to argue that King is simplifying reality in assuming a single decision-taking point at the end of the project and a neat sequential series of steps, but his message about the limited scope of finance theory is just as important now as it was in 1975.
While few finance theorists have addressed stages other than stage 4, other disciplines have been active in researching what may be appropriate in other stages.
In particular, writers in the fields of corporate strategy and industrial economics have made considerable progress in areas which contribute to the types of analysis needed in stages 1–3, and others with a more behavioural perspective to their research have advanced understanding of the organizational and human processes by which these stages are acted out in companies.
There have not yet, however, been any far-reaching attempts to tie together the insights of the different disciplines and perspectives.
Some limited progress has been made relatively recently in linking up the analysis of corporate strategy and some of the behavioural insights of strategy formulation and implementation (see Quinn, Mintzberg and James, 1988, for a text which refers to many of these ideas but which presents little in the way of synthesis), but not much has yet been done to tie that in with financial management.
In case the reader should think that this small book is about to offer the solution to this problem, let it be stated immediately that a complete integration of all these different disciplinary approaches to the investment decision will probably take a considerable time and much research effort by a range of researchers.
Moreover, the volume of literature in  these other areas is such that only an outline can be presented here.
What will be attempted in the remainder of this book is to provide, for accountants and financial-management students, an appreciation of developments in these other fields, together with discussion of some issues which relate to the problem of trying to integrate these different disciplinary views.
Consequently, it is recognized that what is offered can only be a highly personal and limited selection of developments in disciplines other than financial management, with a few isolated ideas on linkages between them.
It is hoped, nevertheless, that this will encourage students of accounting and finance to be bold in pursuing connections between their own subject and others involved in the study of management.
In this chapter we shall look at corporate resource allocation from the point of view of corporate strategy and industrial economics, summarizing their major contributions in this area over recent years.
The discussion will be confined to rational-deductive strategic and economic analysis.
The key ideas of those in the corporate strategy field who have focused more on understanding what companies actually do will be summarized in chapter 7.
Strategic portfolio analysis
During the 1960s most of the Western world enjoyed steady economic growth and it became increasingly evident that companies' main accounting apparatus (financial reports in the form of income statements and balance sheets and annual budgets) provided an inadequate basis for thinking ahead.
It was recognized that there was a need for longer-term planning.
This generally resulted in processes by which companies tried
to identify long-run broad objectives
to identify gaps between objectives and likely achievement under current policies
to formulate strategy to achieve objectives
to select investments to achieve the strategic goals
to draw up long-term plans
to ensure that the shorter-term budgets and plans were consistent with where the company wanted to go in the longer run
While the process did not necessarily proceed without negotiation and iteration between stages, the planning literature emphasized the general logic of the need to integrate short-term budgets with long-term planning.
With the economic recession of 1970–1, corporate managers came to realize that the form of long-term planning that they were conducting, based often on relatively straightforward projections of past growth, did not ensure corporate success.
In fact, many corporate planning departments were significantly reduced in size or even abandoned around this time.
Those companies that persevered with longer-term planning realized that a mechanism was needed to manage changing emphases in their multiple business interests.
It was recognized more clearly than before that most large companies actually held a portfolio of businesses, and that long-term growth, or indeed survival, might depend more on managing that portfolio rather than assuming continued growth on all fronts.
In 1970 the Boston Consulting Group (BCG) published its product-portfolio matrix, which had and still has, though now through derivatives of that early model, a major influence on strategic thinking in many companies.
The origins of the BCG matrix lie before the 1970–1 recession.
That consulting group came to realize earlier than others how important market share was to corporate profitability.
This was due not only to economies of scale but also to what came to be known as the experience curve.
The experience curve itself was developed from the concept of the learning curve, which had been used for many years in industries where it was observed that labour time fell as the workforce accumulated experience in producing more of a particular type of unit.
The BCG accumulated evidence that costs of all factor inputs could be reduced with an accumulation of experience if companies were conscious of the possibility and managed to make savings in labour, develop standardized products, improve equipment and processes, and so on.
The BCG then coupled the importance of market share with the rate of growth in the market to produce a growth-share matrix upon which all a corporate group's main product groups would be plotted (see figure 4.1).
Each product group would be represented as a circle on the portfolio grid, where the size of each circle relative to those for other product groups indicated the proportion of group profits earned by that product group.
The growth axis was said to approximate the product life-cycle, with the rate of growth slowing down as the product market entered a more mature state.
The BCG divided its matrix into four quadrants as shown in figure 4.1 and gave each quadrant a name to signify how the products in it should be treated.
BCG also argued that a company with a balanced portfolio of products would use cash flows generated from the cash cows to invest in selected problem children, which would be built up to become the stars of the future.
The current stars are generators of profits which may need investment to sustain their position.
A star may or may not be able  to generate its own cash to provide for its own investment.
It was also pointed out that nothing in this picture is static.
Stars will eventually become cash cows as they reach maturity.
Cash cows will be deprived of any further major investment.
Dogs are those cows which have further deteriorated such that they provide no cash throw-off, due to the effect of a small market share in a very mature market.
Companies were advised to get out of dogs as soon as possible.
The BCG matrix, with its implications of a rigid policy for each product quadrant, has been much criticized, and some of the relevant criticisms will be examined later in this chapter.
For the moment it suffices to say that the BCG-matrix approach was a critical step in emphasizing (1) the need for a balanced portfolio of products within a corporate group in order to sustain longer-run success, and (2) the dynamic nature of managing large corporations.
The BCG approach provided the basis for further developments.
Various derivatives of the BCG matrix have been developed both by other consultancy firms and by large companies such as Shell, but a description of just one of these will suffice.
Probably the most widely known alternative matrix is that developed by McKinsey and Co., apparently in liaison with General Electric Inc.
(USA).
The McKinsey-GE portfolio matrix is shown in figure 4.2.
The fundamental logic of that matrix is essentially the same as that developed by BCG: namely, that corporate groups can be divided up into sub-categories such that appropriate expansion, hold or contraction policies can be identified for each sub-category.
The matrix in figure 4.2 merely uses a 3 × 3 system of classification, rather than BCG's 2 × 2, to split out the three different policy categories more clearly.
At this time the idea of simply dividing up the group into product groups was also broadened.
The relevant units of the matrix came to be called strategic business units (SBUs).
An SBU was taken to be any subsidiary business interest of the group which could be said to be largely separable and could be treated as having a commercial life of its own.
An SBU might, therefore, be a product group, a geographical region, a customer group or whatever form of classification seemed appropriate.
It was critical, however, that existing organizational units, i.e. corporate divisions, should not automatically be considered the appropriate SBU units.
Corporate divisions can incorporate more than one or, indeed, less than one separate economic entity.
Divisions may have been formed for administrative or historical reasons, which do not necessarily produce organizational units which are identical with SBUs.
This is an absolutely fundamental point.
If SBUs with largely separate commercial lives cannot be identified, or are misidentified, the first basic requirement of this type of analysis is not met and there is little   point continuing with the technique.
The point will be taken up again later within an evaluation of these matrix methods.
Figure 4.1 The Boston Consulting Group portfolio matrix
The McKinsey-GE matrix also recognizes that in many businesses investment decisions cannot be made simply upon information about market growth and the current share held.
Consequently, it redefined the axes as follows.
‘Market growth’ was replaced by Industry attractiveness — a term that in practice was used loosely, to refer to the attractiveness of being in that area of activity in which the SBU operated.
(This area could be a particular part of an industry, a geographical area, and so on.)
Also ‘Market share’ was replaced by Competitive strengths, i.e. competitive strengths within the SBU's area of activity.
For good measure, McKinsey also decided to exchange the axes.
With these redefined axes, McKinsey concluded that investments should be made in attractive industries where a group was in a strong competitive position and not in less attractive industries, especially where the group's competitive position was weak.
It was recognized also that selective investments might be made in unattractive industries if the SBU was the dominant operator in that industry, and similarly where attractiveness was high and the SBU's position weak.
Like the BCG, McKinsey saw the inclusion in its matrix of the industry or product life-cycle concept as appropriate for the way SBUs were defined.
With figure 4.2, however, the cycle is seen as going diagonally across the matrix rather than running along one axis.
McKinsey also drew up an analysis showing how different facets of financial management should vary according to the stage of the life-cycle.
It follows that its 3 × 3 matrix can also be used as a basis for developing a complete range of differentiated financial policies throughout the group as well as just investment and divestment advice.
The relationships that McKinsey posited between areas of its matrix, the   life-cycle and financial-management policies are summarized in figure 4.3.
Figure 4.2 McKinsey-GE portfolio matrix
It is at once apparent that the McKinsey-GE matrix has much less definite measurements for its axes.
Indeed, I have found that students and businessmen who are using this technique for the first time tend to get confused between the two axes.
It is very easy when analysing industry attractiveness to argue that the industry is attractive because the company holds a patent or some other competitive advantage.
That is obviously confusing the two axes.
A useful piece of advice to avoid this is to tell users that they should be trying to decide how attractive the industry is for some SBU that is strong in that industry.
Moreover, attractiveness must be defined in terms of looking ahead at the likely state of the industry over the usual sort of investment time horizon for that industry.
In comparison with the BCG matrix, the McKinsey-GE matrix seems at first to be less explicit about building some ‘problem children’ into ‘stars’and identifying ‘cash cows’and ‘dogs’, but the implications are present, as indicated by figure 4.3.
The McKinsey-GE model does, however, seem inconsistent with the BCG matrix in two respects, and these seem to have been largely overlooked in the literature.
The first point of inconsistency concerns the investment policy indicated in the top left-hand corner of the grid.
BCG does not say ‘invest and grow’ here, but implies that investment should be made in so far as it is needed to maintain a favourable position; BCG seems to be ambivalent here about the need to invest to increase market share.
In contrast, McKinsey says categorically, invest where the market is attractive and where the company is strong.
Also, BCG is far more explicit about the need to build certain ‘problem children’ into ‘stars’, and appears to advocate investment principally in the top right quadrant, in order to increase market share in those SBUs where investment can be a vehicle for doing that.
The BCG position seems more defensible than McKinsey's.
If the McKinsey-GE matrix is to be similarly interpreted as a guide to the movement of the group's interests through time, one must assume that a substantial amount of investment must take place in the bottom left-hand corner of the grid in order to build future ‘stars’.
Of course, this conclusion depends on how one interprets the ‘Competitive strengths’ axis.
I prefer to evaluate current competitive strength and plot that on the grid.
This then forces a consideration of whether investment can then improve the SBU's competitive stance or not.
If it can, this implies the possibility of moving the SBU up the vertical axis.
If, however, the vertical axis is defined as attainable competitive strength, then one might expect most investment to take place in the top left   corner.
This stresses, once more, the need to be extremely careful with the basic definitions used in this apparently simple form of analysis, and to ensure that one's policy deductions are consistent with one's definitions and assumptions.
To repeat, however, the interpretation of competitive strength as current strength is preferable, because it focuses the dynamic management of the group portfolio on the grid itself, rather than leaving it as an inference of the underlying analysis.
This further implies that the product life-cycle should be seen as shown on the revised McKinsey-GE grid of figure 4.4, rather than as running diagonally across the matrix from top left to bottom right, as figure 4.3 implies.
The emphasis of financial management within each stage of the life-cycle as shown in figure 4.3 still retains its logical validity.
Figure 4.3 Financial policies for different parts of the McKinsey-GE matrix
The preference for defining competitive strength as current strength also arises out of the recognition that, if attainable status is used, there will be no clear separation on the matrix of established stars from potential stars, and this seems critical for distinguishing between two types of investment: major investment for the future and sufficient investment to maintain a market position.
The second area of inconsistency between the two types of matrix concerns another aspect of the way they relate to the product life-cycle.
The BCG literature shows that the growth rate varies from 0 to 20 per cent and above.
There is no indication that the BCG matrix includes the negative growth element of the product life-cycle.
In contrast, McKinsey claims that its matrix covers the complete life-cycle, as illustrated by figures 4.3 and 4.4.
The McKinsey position seems more defensible this time, with the negative growth being confined to the dogs' quarters.
To summarize, the McKinsey matrix seems to have more general scope for devising investment and general financial-management policy.
But where industries are dominated by the market-share/experience curve effects, the logic underlying the BCG model can be a useful aid to thinking about the policy of SBUs in those particular industries.
It is doubtful that the BCG matrix is very useful for total portfolio management in many multi-business groups, because, even if the group is involved in SBUs whose business is dominated by the experience-curve effect, it is likely that some of its SBUs will not be and hence that they cannot be reliably analysed within just the BCG context.
However, one must not use the McKinsey grid in a deterministic way and infer that most investment must take place in ‘stars’.
One must distinguish between where the bulk of corporate investment must be placed and the location of SBUs on the matrix.
The McKinsey investment policies shown within the matrix in figure 4.2 cannot be sustained literally.
The grid states nothing about the degree to which investment is necessary to retain star position or sufficient to build competitive   advantage.
It is stressed, therefore, that a literal reading of figure 4.2 could be a path to disaster.
That does not mean, however, that the McKinsey-GE grid approach has no value, especially if modified as indicated in figure 4.4.
On the contrary, with intelligent and informed use, it can be a powerful tool for aiding judgement on investment and related financial-management matters.
Many companies and consultants have used the approach (see Hapeslagh, 1982), and those using it will generally be intelligent people who are not going to make naive errors through too literal an interpretation of a simple box diagram.
It  would seem, therefore, that there is considerable benefit to be obtained from the use of portfolio matrices, even if this view is not now so widely held in corporate-strategy literature.
There are, however, further criticisms of the matrix approach to consider, and my full case on this issue will not be complete until chapter 8.
The reader is therefore asked to absorb the various arguments for and against portfolio matrices throughout this chapter, but to reserve judgement on the issue until later.
Figure 4.4 Modified McKinsey-GE matrix: natural business-unit cycle (Note.
The cycle shown indicates the general trend; it is not meant to exclude more complex patterns as products/businesses manage to boost the height or length of their life-cycle from a mature or declining stage)
Analysing industry attractiveness and competitive strengths and devising competitive strategies
The previous section identified the axes of the McKinsey-GE matrix, but did not explain how attractiveness or competitive strengths could be classified as high, medium or low.
The McKinsey approach to this problem was to avoid oversimplified numerical indices.
In McKinsey (1978), General Electric's Vice President in charge of corporate strategy listed two sets of criteria used in his company for determining the location of each SBU on each axis.
These were as follows.
Industry attractiveness
Size
Market growth, pricing
Market diversity
Competitive structure
Industry profitability
Technical role
Social
Environmental
Legal
Human
Competitive strength
Size
Growth
Share
Position
Profitability
Margins
Technology position
Strengths/weaknesses
Image
Pollution
People
Allen, the Vice President, stated that classifications as high, medium or low were then made with respect to each axis on the basis of collective judgement by corporate management.
Clearly management had to decide
1
what would count as high, medium or low with respect to each criterion, and
2
what weight of importance to attach to each criterion to arrive at an overall assessment for each axis.
The emphasis was on judgement and discussion rather than precise measurement, but Allen said, ‘It is surprising how little disagreement there is in positioning of our businesses on this matrix each year.’
He also stressed that, as the environment changed, the emphasis was shifted to different factors, and that the approach was used to reallocate major investment resource amongst competing needs.
One cannot, of course, say from this description how rigorously each SBU was evaluated, or how well devised the strategies to move SBUs around the matrix were.
More recent work by Porter (1980, 1985) has shown, however, that, from a knowledge of industrial economics, it is possible to develop a rigorous basis for solving both of these problems.
In comparison with Porter's work, the lists of factors given by General Electric back in the 1970s now seem little more than a tentative beginning at an appropriate form of analysis.
In his 1980 study, Porter argues that the first thing for a firm to understand is the structure of the industry or market in which it operates.
It is that structure which determines longer-run profitability.
Moreover, industrial structure is not something which can be resolved merely by general debate over a general checklist of factors.
There are five main factors which determine the structure of any industry.
Three of these are based upon the input-output relationships affecting the production of goods or services in the industry: factors affecting inputs, their conversion into outputs, and saleability.
The other two factors are the degree to which it is easy to enter the industry or market, and the degree to which there are substitutes for the products.
These five factors can in turn be broken down into a number of subsidiary aspects to produce a specific number of questions to be answered in identifying the likely longer-run attractiveness of the industry.
These can, following Porter, be put together as shown in figure 4.5.
This makes much better sense than the General Electric list, because it places projected profitability as the major element of industrial attractiveness, rather than simply listing it as one of several factors, and provides a sound theoretical basis for determining it.
Following the implications of the McKinsey matrix, Porter states that there is usually little that the firm can do in the short run to affect industry structure and hence attractiveness.
It should focus upon understanding that structure in order to develop a competitive strategy.
Porter points out, nevertheless, that industry structures do change over time, perhaps due to technological or other developments, and that the firm itself may, through its competitive strategy, change the structure either inadvertently or not.
In particular, a market leader's actions may significantly affect the industry structure.
One must therefore develop   strategies taking into account the way in which they, and rival responses to them, may change the industry structure and its related profitability.
Hence, a dynamic consideration of portfolio analysis, as applied to major investment decisions with their relatively long time horizons, does not just mean moving SBUs up or down the portfolio grid by transferring resources between them (from ‘cash cows’ to ‘problem children’and ‘stars’).
It means identifying, too, how actions on the competitive-strengths axis may feed into the industry-attractiveness axis, which may then create further movement of the SBUs and challenge the relevance of the competitive strategy first devised.
There must be an iterative  movement between effects of each of the two axes, and a set of matrices, each reflecting a stage of this process, will be necessary to analyse this process.
Figure 4.5 Porter's factors for analysing industrial structure (factors are illustrative; for a full list see Porter, 1985, figure 1.2) Adapted with permission of The Free Press, a division of Macmillan, Inc. from Competitive Advantage: Creating and Sustaining Superior Performance by M. E. Porter, copyright © 1985 by M. E. Porter.
Despite the comprehensiveness and relative complexity of Porter's analysis, he concludes that, once one has arrived at an assessment of one's competitive strengths by considering how strong an SBU is in terms of the factors in figure 4.5 which are most important for the relevant industry, there are only three generic strategies open to any SBU to become more competitive.
It must
1
aim to be the lowest-cost producer with, usually, a standard ‘no-frills’ but serviceable product, or
2
differentiate its broad product range in some way so that it is not competing directly with lower-cost products, or
3
adopt a strategy of focusing upon a particular niche or segment of the market.
In fact Porter really offers just two possible competitive strategies (i.e. be a cost leader or differentiate your product) which can be applied over either a broad range of products in an industry or just in a special niche.
Porter also feels that firms should not get ‘stuck in the middle’ between differentiation and striving for cost leadership.
He does not recognize that it is possible to alternate between the two according to the stage of industry development, and Gilbert and Strebel (1988) illustrate how some companies do outpace others in the industry on both cost and quality.
Moreover, there seems to be no reason why economic competition should take such a dichotomous form, with a clear separation between the low-cost producers and the differentiators.
Phillips, Chang and Buzzell (1983) also showed that it was quite possible for there to be several different degrees of differentiation.
The main requirement is for each degree of differentiation to be clearly demarcated from others in the eyes of buyers and for each firm to be the lowest-cost producer within each category.
One only has to consider the markets for restaurant meals, motor cars and clothes to realize that there are more than two successful cost-quality mixes available in a number of industries.
Moreover, Miller and Friesen (1986) have tested Porter's position on this point and concluded that differentiators do also employ cost-leadership strategies and that cost leaders do employ differentiation policies.
They emphasize, however, that their results relate to the consumer-durables industries, where marketing, branding and economies of scale go hand in hand.
In capital-goods industries, where knowledgeable buyers are more informed about the technical attributes of different products, Porter's argument about the need to avoid getting ‘stuck in the middle’ may be more sustainable.
This might  explain the Gilbert and Strebel finding on IBM and the personal-computer market.
Perhaps that was just a case where, in the early stages of the life-cycle, corporate managers acted more like private consumers than industrial purchasers of large-scale equipment.
One may also attack Porter's cost-differentiation dichotomy in terms of the logical use of language.
He seems to be saying that the product must be seen to be either cheaper or different if people are to buy it.
But being different can be due to a whole range of factors.
Hill (1985) sets great store on identifying relevant order-winning criteria from a set including price, product quality, product reliability, and other factors.
He also distinguishes clearly between criteria which qualify a firm's products to be considered and those which actually win the order.
From this viewpoint, cost simply becomes one of many ways in which the product may be seen to be different from others.
The simple two-way split between cost and differentiation disappears.
That does not, however, detract from the value of separating out the notion of cost leadership from differentiation conceptually in order to work out the implications for measurement and control (see chapter 5).
All this does not, however, detract at all from Porter's basic argument about the underlying analysis needed to identify industry attractiveness, competitive strengths and competitive strategies.
Moreover, he shows clearly how strategies can be grouped in terms of broad emphasis into a few types according to whether the industry is in the emergent, mature or declining phase of its life-cycle.
Fundamentally this is what McKinsey tries to do, as shown in figure 4.3, although Porter's analysis is richer.
Porter also shows how identifying the precise requirements of a strategy for each SBU requires a little more thought that just a broad consideration of that SBU's position in relation to its industry structure.
In his 1985 study he develops the notion of a value chain from earlier similar notions of complete business systems discussed and used by both McKinsey and IBM.
A value chain is simply an analysis of the totality of activities associated with adding value to some basic commodity in order to provide end consumption.
An SBU should then look at all elements of the chain that it is involved in (e.g. procurement, production, transportation, marketing) in order to see exactly how it already adds value and might add value in future.
This entails examining the SBU's competitive position on each of the dimensions that are shown by the industry structure analysis to be critical to longer-run profitability.
Porter also points out the importance of defining the SBU in terms of the complete network of a value chain within a corporate group irrespective of what that includes in terms of operations in different  divisions.
This reinforces the statement made earlier about the need to define SBUs very carefully.
Finally, attention needs to be drawn to the possible transient nature of competitive advantage.
Even a dominant firm will face rivals seeking to find a window of opportunity to chip away at the dominant position.
Competitive strategies therefore need regular review and modification within the realistic rate of change defined by the nature of the industry.
Porter has undoubtedly had a considerable impact upon corporate strategic thinking, and in this brief discussion it has been possible to present only the barest outlines of his ideas developed in two sizable books.
Some further reference to his views will, however, be inevitable in an attempt to provide an evaluation of the portfolio-matrix approach and concepts associated with it.
An interim evaluation of the portfolio-grid approach
The evaluation presented in this section is an interim one because it is based upon viewing the matrix from a rational economic  perspective .
A final review of the use of the matrix will be offered in chapter 8, once more behavioural matters have been considered.
This section is divided into two parts.
In the first, reports of empirical data are reviewed to see to what extent statistical data on corporate performance support the broad policy guidelines suggested by the BCG and McKinsey-GE matrices.
In the second part, some key literature which has challenged the conceptual foundations of these matrices will be examined.
Empirical findings from the PIMS studies
‘PIMS’ stands for ‘Profit Impact of Market Strategy’.
It is a large-scale data collection and analysis exercise begun back in the early 1960s by the General Electric Company.
Its broad aims were to analyse large American companies in order to discover differences in return on investment (ROI), to establish par values for ROI for different industries and their segments, and to attempt to explain variations in ROI across companies and their subsidiary business units by reference to strategic factors.
For example, market attractiveness was to be captured by factors such as long-run and short-run growth rates and the stage of the product life-cycle, whereas competitive strength was to be assessed by market share (both absolute and relative) relative product quality and breadth of product line, relative costs, and so on (for more details  see McNamee, 1985).
The PIMS exercise was subsequently taken over by Harvard University and by the mid-1980s a data base had been established including about 250 companies which have provided information on about 3,000 SBUs.
More extensive reviews of the results of tests on this data base can be found in McNamee (1985), Abell and Hammond (1979), and Buzzell and Gale (1987).
The present discussion will highlight just one or two main conclusions which are of relevance to the main lines of argument in this book.
A major conclusion from cross-sectional studies based on PIMS is that ROI increases significantly as market share increases.
ROI as a percentage is in the low teens for market share below 30 per cent and up to the mid to high thirties for market shares above 80 per cent.
The PIMS study explains this as due to economies of scale, experience effects and bargaining power.
This appears to provide strong support for one of the main planks of the BCG model.
It is important, however, to consider this question carefully.
The study merely shows a positive relationship between market share and ROI and this is not necessarily a causal connection.
A later study by Jacobson and Aaker (1985) examines this proposition and runs regressions including ROI from earlier periods as lagged independent variables as surrogates for other factors which could be giving rise to the positive association between market share and ROI.
They conclude that the effect of market share upon ROI is much less than earlier studies showed and that, broadly, a 1 per cent increase in market share led to a 0.1 per cent increase in ROI rather than the 0.5 per cent earlier supposed.
Consequently, they state that much of the association between ROI and market share reported earlier was spurious.
Buzzell and Gale (1987) contest the Jacobson and Aaker view by arguing that they, in effect, have related the change in ROI (not the level of it) to the level of market share.
Moreover, Buzzell and Gale argue that, as larger-share businesses do not continuously increase their ROI differential over small-share businesses, one would expect a zero correlation between the change in ROI and market share, which is broadly what Jacobson and Aaker found.
Hence, the latter underestimate the effect of market share on the level of ROI.
Buzzell and Gale then regress ROI on both market share and relative quality of product and find a 4.7 per cent ROI differential per ten points of market share.
(When quality is left out of the regression there is a 5.3 point ROI differential.)
They stress, however, that this is just an overall industry average and that, as market share accounts for only about 14 per cent of the dispersion of ROI among businesses, other factors need to be considered.
One of these factors is investment intensity.
The next major conclusion of relevance to this discussion is the PIMS finding that investment intensity, measured by investment divided by sales, is negatively correlated with ROI.
For an investment-intensity ratio of below 35 per cent, it seems that one gets an ROI above 30 per cent , whereas, when investment intensity is above 70 per cent , ROI falls to about 10 per cent .
A not-surprising result is that low market share coupled with high capital intensity spells disaster.
The important question, however, is why high capital intensity is associated with lower ROI even when market share is high.
A range of possible reasons has been put forward, stressing that it is important to get the right degree of capital intensity for the business and that many businesses may simply be misled by current profitability and push investment too far.
This may have some truth, but seems unsatisfactory as a complete explanation.
Part of the reason for getting such a result could well be the cross-sectional nature of the analysis.
An SBU with high capital intensity could well be at an early stage of its product life-cycle, when one would expect it to be building up its investment relative to sales and also experiencing low profitability as it fights to establish itself in the market.
At a more mature stage, the increase in sales volume should have resulted in a decline in investment intensity, while profit and ROI will increase if the investment is successful.
This explanation would be quite consistent with a cross-sectional picture in which capital intensity is inversely related to ROI even when market share is high.
It is virtually an obvious consequence of the accounting formulation used with investment as both the numerator of investment intensity and denominator of ROI, while profit in the numerator of ROI is possibly a fairly stable proportion of sales in the denominator of investment intensity.
It would therefore be incredibly naive to conclude from the PIMS data that it is erroneous for companies to increase capital intensity — when their snapshot was taken by PIMS they might still have been waiting for their day to come.
This view is enforced when one remembers that investment lead times — that is, the period between basic investment in assets and the time when the product shows up as profitable in the company accounts — can be quite long.
As we have already seen, it can take several years for a product to come on stream, let alone break even in reported accounts; seven or more years is possible in some industries (see Tomkins et al., 1978).
The PIMS data still give some cause for puzzlement over the question of investment intensity, however, because Buzzell and Gale show that five-year averages of ROI are highly positively related to increases in the long-term value of the business, both being closely and positively associated with initial competitive position.
(The basis of their valuation  of the business will be explained in chapter 6.)
One might infer from this that investment intensity is therefore also negatively correlated with increases in the value of the business over a five-year period.
This would be more worrying.
One explanation might be that five years is still not a long enough lead time.
On the other hand, the explanation might lie in the study by Wagner (1984), who stresses that researchers have usually focused upon PIMS data for an average four-year period.
In contrast, he selected those companies that had data in the PIMS system for at least seven and up to thirteen years; this gave him data on 460 US companies.
Then, studying changes over time, he set out to test the main conclusions derived from cross-sectional studies.
His results are of considerable interest.
He found first that ROI was remarkably stable over time at the company level, but that it showed considerable variation over time for individual businesses (i.e. the SBUs).
Moreover, the rich businesses do not get richer and the poor do not get poorer.
In fact, the reverse is true.
Those with high ROI tend to experience falls in ROI and those with low ROI tend to have increases, taken across the whole population.
Ideally one needs to subdivide the original low-ROI businesses into those at the beginning of their life-cycle and those at the end, because one would expect ROI to increase in the former but to fall in the latter.
Indeed, Wagner found that the position in the life-cycle explained over half the variation in ROI.
Wagner's findings on investment intensity also bring some economic sense to the question raised above.
He states that businesses with the highest investment intensity have the greatest chance of improving ROI when one takes the longer run into account.
He says that his data showed that those businesses with low investment intensity did indeed start off with high ROI, as the earlier cross-sectional studies discovered, but that this often led managers to over-invest.
In contrast, the managers of the longer-run winners did not over-invest, but were in the upswing of the business cycle facing markets with rapid growth.
While his explanations do not seem clearly to separate older arguments about managers under- or over-investing from the impact of the business-cycle stage itself, it does seem clear that investment at the right time, i.e. early in the product life-cycle, does lead to increased ROI if management is able to establish its product in the market place and control costs.
Indeed, it would be contrary to basic economic laws if this were not so.
Nevertheless, given the earlier findings on investment intensity, this matter did need clarification.
It is interesting that Wagner's findings also seem to support Porter.
Wagner concludes that managers should strive to increase market share, lower relative costs and/or achieve high product quality, and that any  investment activity must be directed at at least one of these aims if it is to prove successful.
He also states that his analysis showed that it is not easy to increase market share through time.
The few businesses that do manage to achieve this profit handsomely, but most do not achieve it once the market is established.
So, what can be concluded from the PIMS data base about the relationships implied in the portfolio matrices?
First, that there may be a lot more to the relationship between growth and market share established by BCG than has been accepted of late.
High market growth, if taken to be an indicator of an early stage in a life-cycle (or a break-point upwards in a revitalized mature-stage life-cycle; see Gilbert and Strebel, 1988), coupled with high market share should lead to high ROI if the appropriate investment is undertaken.
The key words are, however, ‘appropriate investment’.
One needs a Porter-style analysis to discover exactly what type of investment and competitive strategy is needed to achieve or maintain market share or competitive position; one cannot just assume that any type of investment in capacity in areas of high market share will lead to future success.
Nevertheless, given managerial competence in performing and acting upon that analysis, one would expect to find largest positive NPV (and not highest current ROI) projects in the regions of the BCG or McKinsey matrix that relate to high growth and high market share.
Normally this would be in the region of the matrix indicating early phases of a life-cycle, where competitive advantage can be built up by investment (although where this region is on the McKinsey matrix depends upon how the axes are interpreted, i.e. whether one works with figure 4.3 or 4.4).
This will be supported further by the theoretical analysis undertaken in chapter 6.
From Wagner's results we would conclude, however, that building competitive position or market share is really successfully achieved by relatively few businesses.
A rigorous Porter-style analysis seems to be a more important ingredient in the decision inputs.
Buzzell and Gale (1987) conclude from the PIMS data that product and service quality is the most significant factor for the longer-run success of a business.
The interpretation of quality is variable, however, by product and service and context.
It is the economic analysis which Porter demands that determines what quality, product and service mean in each case.
It is that form of analysis which indicates what Hill's order-winning criteria are in specific situations.
Conceptual pitfalls of portfolio analysis
While the limited time-series of PIMS to date seems to indicate some broad credibility for the implications of portfolio grids, other criticisms  of the approach challenge its underlying assumptions on a conceptual basis.
There are a number of different arguments, which will now be summarized.
Some argue that high returns are possible in declining industries and that this is inconsistent with the harvest/divest strategies indicated in the BCG and McKinsey-GE matrices.
Indeed, Buzzell and Gale (1987) also point this out.
First, it needs to be made clear that high returns now must be distinguished from high returns in the long run.
In a declining market one might well expect a business that had earlier established a dominant share position to be able to trade on this position in the short run and increase prices and profits, because rivals are unlikely to come into the market or compete strongly for market share.
Also, it might be able to increase revenues as rivals drop out of the market.
This does not mean, however, that one should invest in that declining industry beyond the level justified by short-run returns.
Declining markets are therefore not likely to be receivers of major investment even if current returns are high.
In response to this argument it may be observed that even the BCG itself, in a series of articles about strategic investment in the Financial Times, November 1981, points out that there can still be profitable niches in declining markets and that ‘dogs’ may be able to bite back by identifying new possibilities in their industries (see also Thietart and Vivas, 1983–4).
That fact is taken into account in a proper definition of an SBU.
A significant new niche is tantamount to a new SBU and should be recognized as such and brought into the matrix in its proper place.
Labelling a particular SBU as a ‘dog’ should be seen as a challenge to the people involved with it to search out and propose new SBUs and not just a signal to give up and die.
With appropriate management attitudes, the element of the group labelled ‘dog’ can be encouraged to seek self-renewal, i.e. as an organizational entity, not as a particular product line.
This would seem to be an obvious recognition that nothing lasts for ever.
Even personnel associated with stars will be wise to think ahead to the need for renewal.
In essence, what the portfolio matrix says is, let's have the various business elements at their different stages of growth explicitly on view in the matrix and not lost from view within organizational units.
The matrix does not pretend to show organizational units within which there may be a mixture of growing, mature and declining sub-elements.
It is also quite possible for a ‘dog’ to find a way of extending its life in a previously unanticipated way.
All that says is that this particular product line is not following a smooth transition from youth to middle age and then old age, but that it has managed to find some rejuvenation late in life.
In that case it should no longer be defined as a ‘dog’.
It is a  nonsense to say that an SBU is a ‘dog’ but that it should not be treated as a ‘dog’because it is not really a ‘dog’at all.
Rejuvenation (see Chakravarthy, 1984) does not invalidate portfolio analysis if the axes are properly formulated and researched.
The linking of the life-cycle to the portfolio matrices is a general relationship that is expected to hold.
Perhaps it is better to say that it is a prima facie relationship which can be used for prescriptive advice to hold back investment unless the ‘dog’ management can establish an exceptional case.
The model (and good management) can handle exceptions.
On the other hand, much of this confusion may come from loosely identifying SBUs as industries, rather than products or product groups.
While there is some justification for arguing that industries never mature (or only over an extremely long time-span), there is also every justification for arguing that products often do have life-cycles short enough to make sense of this type of analysis.
Another argument sometimes put forward is that the model assumes a capital-rationing situation.
Obviously, if there is not such a situation, the company should invest in every positive-NPV product available to it, subject to having the capacity to manage investment on that scale.
But the portfolio-matrix approach does not offend that principle.
It just says that some parts of the business will throw off cash surpluses, that it may well not be worth investing further in those businesses, and that, in that case, the funds can be used to further investment elsewhere.
If the corporate group wishes to raise further funds from external sources or use some of the funds for dividends or to repurchase shares, there is nothing in portfolio-grid analysis to stop it.
The portfolio approach does not need to assume that investment has to equal that achievable with internally generated funds, although, of course, most UK and US companies do rely extensively on retained earnings to supply funds for investment.
Others have argued that the axes are misleading because they focus on absolute profitability rather than that earnable on marginal investment.
Once more, however, that can be handled by a proper definition of the axes; obviously it is future profitability of incremental investment that should go into the assessment of SBU attractiveness.
If companies do not recognize that they are applying the model incorrectly; it is not the model which is at fault,
Similarly, others have argued that the matrices make little allowance for risk and in their early application this was true, but, if one undertakes a Porter-type analysis to give a solid foundation to industry attractiveness and competitive strengths and the derivation of the appropriate strategies, that will take into account a risk assessment.
In fact, by focusing upon the specific factors within the value chain, managers will gain a much better appreciation of the specific risks and uncertainties associated with their strategies than some less well-defined, hit-or-miss sensitivity analysis.
Derkinderen and Crum (1984) raise a rather different point about risk.
They stress the need for an organization to build financial resilience, i.e. focus on the necessary financial structure and other factors needed to help it live through adverse times.
This may not be covered in the initial simple descriptions of the portfolio grids, but one would expect the underlying risk analysis to include such matters.
Derkinderen and Crum do well to remind us of this, but it does not really invalidate the portfolio-grid approach.
This argument applies equally to Wensley (1981).
He argues that portfolio matrices fail to indicate the considerable risks faced in diversification.
On the face of it this is true, but that simply means that there should be a thorough assessment of competitive strengths to underpin the matrix.
A wide range of comments have also been made about the simplistic nature of the grids as specifiers of where investment should be placed.
While such criticism was perfectly correct in relation to the early portfolio-grid literature, it should now be clear that, when properly supported by rigorous analyses of industry structure and the value chain, portfolio analysis survives as a valid concept.
Porter seems to  agree with this view, but says that the extra value obtained from putting the results of the sorts of analyses that he recommends into the matrix form is marginal compared to the value derived from the underlying analysis itself.
I believe that the benefits of the portfolio concept may be greater than Porter sees from his largely industrial-economics perspective.
An attempt to substantiate this point will be attempted in chapter 8 from an internal management perspective.
Nevertheless, from the viewpoint of the necessary external analysis, Porter is clearly correct in seeing a rigorous and informed economic analysis of the market place as the most vital element.
Simplistic or casual analysis based on little more than common sense and drawing ‘portfolio boxes’ is obviously totally inadequate.
All the arguments considered so far against the use of portfolio-grid analysis of the McKinsey-GE type are, in my view, better seen as warnings about simplistic use of the technique rather than as indicating fundamental flaws in it.
Intelligent application of the technique can take these warnings into account and still derive considerable benefit from it.
We have still, however, to consider one much more fundamental attack upon the use of a McKinsey-GE type analysis.
A range of critics (including Coate, 1983; Hamel and Prahalad, 1985; Porter, 1985; Naylor, 1982, 1986; Prahalad and Doz, 1987) have emphasized that the validity of the portfolio-matrix approach depends fundamentally on the existence of SBUs which have little or no  interdependence in a corporate group's portfolio.
Porter, in particular, argues that a horizontal strategy affecting a number of different SBUs must be a necessity for a diversified firm, since there is no economic reason for SBUs with distinct lives of their own to be combined into a corporate group.
In fact, such separable SBUs with no possibilities of synergy with other SBUs are, he says, likely to be prime candidates for divestment when a good opportunity arises.
While Porter is obviously right to argue that there is no economic reason for completely separable SBUs to be combined, it does not follow that they would function any better by becoming completely independent.
There is no economic reason why they should be divested unless it can be shown that they would have synergistic benefits from being combined with SBUs of a different type from those possessed by the group that currently owns them.
Porter might wish to question how such groups with non-related SBUs came to be formed in the first place.
They may have been formed in the interests of diversification (unnecessary though this might be from the viewpoint of the theory of finance) or for political reasons, for example.
Alternatively, the corporate management might simply have wanted to acquire them.
There is no reason why a professor of history, say, should not also manage a family business if he has the time and energy.
Yet the two activities have no obvious relationship.
Indeed, the professor's rationale may be that he wishes to be involved in two completely different activities.
Similarly, shareholders who rank the capacities of their company management very highly may support expansion on the basis that they can generate larger profits even without synergistic advantages.
Hence, corporate groups can continue to exist with separable SBUs and will not inevitably be broken up.
At least, therefore, for such groups, the portfolio matrices and their separable SBUs are potentially relevant concepts.
A more important question, however, is whether portfolio matrices can be sensibly applied only to such conglomerates.
At face value the answer might seem to be ‘yes’.
Naylor (1986), for example, notes that even General Electric itself (as well as Xerox and Texas Instruments) ran into difficulties with its portfolio-grid analysis because of the existence of interdependencies.
In fact, by 1988 General Electric had reorganized into fewer key businesses in order to overcome this problem, and in so doing had become more like a conglomerate.
(See the Financial Times of 16 May 1988 and further discussion in chapter 9.)
There are basically only two types of interdependencies between SBUs.
In Porter's terminology they are tangible and intangible interdependencies.
Tangible links are where SBUs have buyers, distribution channels, technology or competitors in common(Porter has three categories, but it seems that competitor interrelationships are just as much a tangible link as those he specifies under that heading).
Naylor  (1986) also gives an example of a tangible link between SBUs when he cites the interdependence of the multiple car lines at General Motors, where a change in the demand for one line may well be reflected in another.
Intangible connections between SBUs are defined as the transference of management know-how.
There may be no direct collaboration between SBUs, but what Porter calls the same generic skills may be called upon.
For example, beer and cigarettes may both be purchased as recreational products.
Porter first identifies the problem that will be encountered if one proceeds with portfolio planning without explicit management of these interdependencies.
SBUs will value these interdependencies differently and this will lead to inconsistencies in the behaviour of linked SBUs.
Next, he says, SBUs may well form alliances outside the corporate group to obtain the same benefits as could be obtained by internal linkages while retaining more control over their situation.
It is difficult to see why external links should inevitably give rise to greater control over one's actions.
Of course, you may prefer to choose outside alliances rather than restrict yourself to the family, but in times of hardship the family may be more dependable.
At such times it may be easier to achieve flexibility in a contractual relationship with another SBU in the same corporate group, but there is something in what Porter says.
There will be a tendency for joint customers to be ignored and for know-how transfer to be lost.
Porter then develops the argument that the presence of interrelationships between SBUs implies that strategies must be broader than simply build, hold or divest.
But must they?
Is it not simply the case that the underlying analysis becomes more complex because, before knowing whether it is beneficial to build or hold or divest, it is necessary to know the effects beyond the immediate SBU in question?
Surely, no other fundamental policies are possible.
If the interdependencies do not involve all SBUs in a corporate group, it might be possible to form largely separable business clusters and form the portfolio-grid analysis in terms of those rather than smaller SBUs.
There is a suggestion of this in both Porter and Chakravarthy, who talks in terms of business families.
This may indeed be one possible solution, but it has the disadvantage that the need for analysis of the industry structure and competitive strengths of each of the smaller SBUs may be ignored.
After all, when one recognizes the different types of interrelationships which can exist, the linkages may be between SBUs in quite different industries.
Also, some groups may have so many interconnections that such a clustering approach is impossible.
Prahalad and Doz provide, for example, a diagram showing the world-wide interconnections of different parts of IBM.
The diagram looks like a closely woven spider's web.
So severe does Naylor believe this question of interdependency to be that he devotes the whole of his 1986 study to an attempt to develop an alternative to the portfolio matrix.
This alternative, which he calls the strategy matrix, essentially requires top-level corporate management to formulate six to eight key strategies which go across the various subsidiary businesses.
Each strategy will be given its own management team, which will be expected to work out all the effects over the whole company, including estimates of resource requirements for each subsidiary business.
Naylor says it is important that these corporate-wide strategies should be worked out before the individual businesses commence their planning.
While one can see that a certain number of strategies might be conceived at corporate level before reference to the individual businesses, in general it is very difficult to see how this can be done at corporate headquarters unless the central authority has a careful analysis of the attractiveness and competitive strengths of the major SBUs.
The main point of the Porter analysis was to establish that that form of information was fundamental to any strategic analysis.
At the very least, therefore, there must be some preliminary analysis at SBU level, even if that ignores the interdependencies, and this must then be transmitted to corporate level before the central teams can carry out their cross-SBU analyses of the key corporate-level strategies.
In other words, some iterative process between SBUs and corporate headquarters will probably be necessary and this may prove to be the key to solving this problem.
It seems clear, however, that corporate groups may have many interdependencies between SBUs.
When that occurs, the ability to plan resource allocation by looking at independent SBUs becomes impossible.
Some overall ‘optimization’ process and global strategy become necessary.
Whereas the wide range of earlier criticisms of the portfolio-grid approach could be countered, this form of criticism, which strikes at the heart of the construction of the matrices and their SBUs seems devastating.
Things look black for the portfolio matrix.
But, just before the reader buries the portfolio matrix once and for all, he or she might remember that I said that my case would not be complete until the end of chapter 8, where more organizational and behavioural matters will be considered.
It will be shown how conclusions based upon the analytical economic framework may need at least some qualification when these other matters are brought into account.
Then it will be clear that a (pre-1989)‘Soviet’ dirigiste planning style is not inevitable.
Meanwhile, it can be noted that both academics and businessmen largely working outside the field of corporate finance have contributed significantly to the areas that King (1974) said needed addressing.
Much of the work evaluated in this chapter takes us a long way towards identifying mechanisms for triggering the need for investment, screening  the proposals and defining them.
Also, bases of evaluation different from those which dominate finance theory — namely, DCF techniques have been proposed.
These differences in approach to investment evaluation will be examined in chapter 6.
Some progress towards satisfying King's other requirement (the form of ‘transmitting the project’ through the organization) will be considered in chapter 7.
5 Towards Integrating Accounting into Strategic Analysis
It was seen earlier in the book that there are inconsistencies between different components of what may be loosely called the finance function.
These occur between the theory of finance and financial modelling practice and between the theory of finance and accounting practice.
Some space was given over to discussion of these questions in chapters 2 and 3.
The analysis has now reached the stage where far larger problems of integration occur.
In this chapter an attempt will be made to look for possible links and conflicts between accounting and the strategic analysis of chapter 4.
Chapter 6 will then conduct a similar comparison of the theory of finance and corporate-strategy concepts.
The ideas put forward are offered as a structure to enable finance theorists and accountants to widen their perspectives and also to encourage those operating in the strategy field to welcome collaboration with finance specialists.
What is offered is, therefore, speculative rather than proven.
The reader is also asked to bear in mind that the strategic perspective covered is as described in chapter 4.
Whether such a global rational analysis may be applied after considering studies of organizational behaviour is a question that will be discussed in chapters 7 and 8.
Similarly, chapters 5 and 6 will also be largely concerned with the analyses that finance theorists, modellers and accountants might make.
How their roles might be enhanced in recognition of the behavioural literature will be examined to some extent in chapter 9.
As already indicated in chapter 3, accounting practice may interact with strategy in two broad ways: either by providing analysis prior to investment, or by providing performance reports and costing statements.
In the latter case the effect upon strategy may be to signal that it is time to divest rather than invest.
No attempt will be made to separate out what accounting methods might do for pre-investment as distinct from post-investment.
The type of analyses that in one company may be conducted with a view to maintaining strategic control and identifying the early signals of a need  to change the product mix may in other companies be conducted largely at the pre-investment stage.
Much depends upon the nature of the industry and product.
For example, Kaplan's arguments for ‘activity-based costing’(Kaplan, 1988) are usually expressed in terms of providing an annual review of product-costing for pricing and product-mix decisions.
Yet I recently visited a hi-tech company which was fully conversant with activity-based costing yet did not do it on a regular annual basis.
It was explained that this was because the company was perpetually producing new products with life-cycles of two to four years and that the product mixes over the narrow product range in current production were largely set by the available type of capacity over such periods.
Nevertheless, the company's financial controller said that, if one looked at the methods used by the cost accountant at the pre-investment (i.e. product-design) stage, it was clear that the notions of activity-based costing did receive careful consideration, before approval of the product design and projected product prices used in the investment appraisal.
In fact, once the investment was completed and operational, the accounting data were highly aggregated, designed merely to keep a more general watch on total operating costs.
The real business of product-costing took place before the investment and was largely ad hoc.
In fact, in this company division, cost-accounting had in recent years become pre-eminently a pre-investment activity, but this obviously was a function of the nature of the business.
Consequently, the subsequent discussion will focus on different analyses which may be pursued before or after the initial investment.
It will be seen that various analyses proposed have both pre- and post-investment implications.
With all this in mind, this chapter will explore the links between strategic analysis and accounting, beginning with the accounting inputs to the portfolio analysis of strategic business units (SBUs), their market attractiveness and competitive strengths.
Following this, we shall consider how to detect the right generic strategy.
With our minds clearly on strategy formulation, we shall then pick up again the debate about return on investment (ROI) begun in chapter 3, but put aside until a better understanding of ‘strategic fit’ had been developed in chapter 4.
In the latter part of the chapter we shall consider, at a deeper level, how accounting analyses might facilitate a cost-leadership strategy.
That discussion will be broadened to take in the notion of generic strategies and the further implications for accounting.
All of this will be discussed without any reference to a given state of technology of development.
The chapter will therefore end by taking a look at recent and forthcoming developments in manufacturing to see how these fit in with the idea  of generic strategies and their implications for accounting — i.e. not just now, but for the foreseeable future.
Accounting for portfolio analysis
The discussion of the use of the portfolio goals in chapter 4 made it quite clear that SBUs labelled ‘stars’ will have quite different financial profiles from ‘cash cows’or ‘dogs’.
Accountants might therefore pay more attention to indicating the types of profiles to be expected for each category of SBU; clearly, unthinking comparisons between them with respect to their financial performance should be avoided.
The most obvious trap to avoid is a stringent comparison of ROIs as an indication of what each SBU is contributing to the total group.
Each type of SBU has a different business role to play and should be evaluated in terms of its role and life-cycle stage.
The precise differences in accounting numbers between the different types of SBU will depend upon the nature of the products, the length of their life-cycle, the type of technology used, and so on, but one can draw some general distinctions.
SBUs being nurtured as the profit-earners of the future will have lower ROIs in the early stages of their cycle.
Their cash flows may well be negative.
Sales to fixed-asset ratios may be low.
In contrast, ‘cash cows’ will be expected to have a good ROI and cash flow and higher sales-turnover ratios.
Accountants should ensure that general managers know what financial profile to expect for each type of SBU.
Figure 4.3 was McKinsey and Co's attempt to spell this out in more detail.
It not only considers the general financial profile, but also indicates different emphases in cost control and pricing according to stages in the life-cycle.
Subsequent analyses by Porter (1985), and critics of Porter, described in chapter 4, suggest that figure 4.3 attempts to push the financial differences between different types of SBU too far.
The emphasis on costs at a given stage of the life-cycle may well depend more upon the product strategy — i.e. whether it is a cost-leadership or differentiated-product strategy.
Also, we have seen that critics of Porter have stressed that often the two types of strategy are used either in sequence (Gilbert and Strebel, 1987) or in combination.
Certainly no manufacturer attempting to differentiate by quality can ignore cost control, just as the producer of the cheapest product will not be successful if quality falls below some critical level.
Consequently, setting up detailed financial controls based only upon whether an SBU is considered to be a ‘star’, ‘cash cow’ or ‘dog’is also simplistic.
On the other hand, there are some general differences in emphasis, as indicated, which should be highlighted by the accounting function.
In particular, accountants need to note the earlier arguments that even ‘dogs’ can sometimes be regenerated and that SBUs must be viewed as product groups or business interests and not necessarily as corporate organizational units.
Hence a prime accounting task is to ensure that for business-portfolio planning the financial details of embryo businesses being built for the future are separated from those of ongoing businesses.
Furthermore, general elements of policy regarding R&D, engineering development and even management development should probably be separated out for each SBU.
Otherwise, mature profitable products may be loaded with the costs of new product development and closed down too quickly.
New developments must be justified on their own merits.
When SBUs reach the last phase of their life-cycle (irrespective of whether a cost-leadership or differentiated-product strategy is being employed), there are still further options available which will need to be understood by the accountant in order to ensure that all options are considered and relevant financial profiles selected.
The strategy for a very mature ‘cash cow’ or ‘dog’need not be just an exit from the industry.
A change in manufacturing method or a movement to bought-in parts may remove the risk of excessive fixed assets at the end of the life-cycle, and enable further profits to be extracted.
Alternatively, a decision to produce at lower levels may, with accounting pressure, be accompanied by release of capacity for other purposes.
None of these things is a wonderful new panacea, but greater awareness of SBUs and life-cycles and the sort of options available at each stage will ensure that a fuller range of options is considered.
Accountants need to be aware of this as much as other functional staff.
Contributing to market-attractiveness and competitive-strengths analyses
To use the portfolio grid, the key determinants of market attractiveness and competitive strength must be identified and measured.
Whether a full Porter-style analysis or some less rigorous approach is used, those key determinants (or key factors) and action in response to them should be the linchpins in the corporate control system once the strategy is  established .
It seems possible, therefore, that financial managers should be involved in the identification and analysis of those key factors.
Clearly they will not work alone, but with corporate economists as well as marketing and manufacturing experts, but financial skills should be  available to the team.
Much of the work of determining the attractiveness of the industry or market place will revolve around estimates of profitability, where accounting knowledge can help prevent significant errors in interpreting accounting statements.
Indeed, a business manager involved in trying to develop a portfolio analysis of SBUs recently stated to me that he knew that assessing the profitability of the different SBUs was a key step in determining market attractiveness, but he did not know which accounting concept was correct.
Should it be return on investment?
If so, should it be accounting ROI or the internal rate of return (IRR)?
If ROI, should it be on an historic or current cost-accounting basis?
Accountants may not realize how much confusion there is amongst non-financial colleagues about basic accounting concepts.
The answer given to the businessman was that he should be trying to assess the stream of future cash surpluses to be generated by each SBU.
Accounting statements from the past were relevant only in so far as they provided some clue to future cash surpluses.
In particular, he was warned of the errors likely to be associated with naive projections of past accounting profit calculations, whether based upon historic or upon current costs.
Nevertheless, past accounting statements will be one prime source of information used to project future cash flows, and the accountant should be on hand to avoid misinterpretation of those data.
Otherwise it is likely that strategic analysis will rest on very shaky foundations.
In addition, if one takes Porter's analysis in figure 4.5 as a guide, considerable financial and cost-accounting data are needed as an input for analysing market structure.
While none of the calculations is necessarily wholly new, financial managers should consider clearly whether they and their financial information systems can adequately provide analysis of economies of scale, experience cost reductions, switching costs, competitive operating costs among members of the industry or market, and so on, all of which help to determine the power of different market participants and the resulting market profitability.
Accountants weaned only on traditional accounting and finance texts may tend to see their financial information from a more restricted and mainly internal perspective, leading to the conventional income statements and balance sheets.
Such accountants may then find that they have shrinking roles and power within the corporation.
Testing for generic strategies
Financial analysts have a role to play in testing the validity of generic strategies, too.
Porter (1985), for example, concludes from his extensive  economic analysis that there are just three forms of strategy: cost leadership, product differentiation or a market-focused strategy.
On the other hand, this conclusion is not universally accepted (Gilbert and Strebel, 1987; Buzzell and Gale, 1987) and so a company would be wise to study the extent to which separate or continued ‘generic’ market strategies determine profitability in its particular industry.
Accounting texts are silent on this issue.
Accountants should have some estimates of how much profitability will vary with such differences in strategy.
Bogue and Buffa (1986, pp. 169–70) provide an indication of how one might proceed with such tests.
In a study of the US agricultural equipment and chemical industries they show how a V-curve may be   derived as illustrated in figure 5.1.
They plot operating margins against average sales in order to illustrate that there are two preferred positions:(1) high sales with cost competitiveness to yield high margins, or (2) lower sales with a differentiated product (or niche) policy, which also produces high margins.
This supports Porter's generic-strategies hypothesis.
Those firms which get caught in the middle appear not to do so well.
It is not certain, however, that such a V will always be found, or what the ‘depth’ of the V will be.
Moreover, if one found that one's company was at its nadir, the slopes of the V-curve might be revealing of the magnitude of the task in shifting to a better level of profitability.
Financial analysis of this type could assist considerably in providing a preliminary understanding of strategic requirements within an industry.
The accounting implications of generic strategies, however, go far beyond this, but, before addressing them, it is appropriate here to pick up again the ROI debate left uncompleted in chapter 3.
Figure 5.1 V-curve for an industry (based on Bogue and Buffa, 1986)
Adapted with permission from The Free Press, a division of Macmillan, Inc. from Corporate Strategic Analysis by M. C. Bogue and E. S. Buffa.
Copyright © 1986 The Free Press.
Does widespread use of ROI lead to poor corporate strategies?
In chapter 3 we saw that accountants and finance theorists have for years debated the inconsistencies between accounting ROI and discounted cash flow (DCF) calculations.
In more recent years specialists from other areas have also criticized ROI (see Hayes and Garvin, 1982; Hill, 1985).
It is now appropriate to review this problem again in the light of the strategic analysis discussed in chapter 4.
With major investments, a study of the corporate-strategy literature suggests that a company will indulge in investment in order either to maintain or to improve its competitive position.
It has also been shown how to analyse the significant factors relevant to competitive position in each SBU.
It may therefore be assumed that a manager is maintaining an adequate policy if the competitive position is being maintained and projections indicate that it is likely to be maintained in future.
Furthermore, if investment has been undertaken to improve the competitive position, then it may be desirable, during the manager's stay in the SBU, to assess the likely performance of that investment by reference to early signals of a change in the competitive position.
The relevant factors will vary from company to company, from SBU to SBU, and perhaps from investment to investment, so they must be determined by reference to specifically relevant competitive-strengths criteria.
The signals of changes in competitive strengths begin before any increase in sales and profits is seen.
They will also depend upon the generic strategies being adopted for the SBU or even the product line.
For example, early indicators of  investment success under a cost-leadership strategy might be the proof, at an embryonic stage of the life-cycle, that the product matches its rivals in quality but is being produced at lower cost.
If, at the same stage, the cost-leadership policy is being coupled with an aggressive pricing policy in order to gain market share, the benefits of that investment would not show up in ROI or accounting profit for some time.
In fact, the high ROI, in accounting terms, may not appear until late in the product life-cycle.
Alternatively, an SBU may be following a product-differentiation policy, in which case, depending on the type of industry, progress in product development and field tests, success in taking out patents, the results of early market-research tests and various other indicators provide early signals of whether the investment policy is likely to be successful.
An SBU or divisional manager will be motivated to seek appropriate investments if he knows clearly what the generic strategy for his SBU is (he would presumably have participated in determining it), is aware of what is expected of him in terms of building or maintaining competitive strengths, and knows that early-warning indicators will be set up to monitor how his investments are contributing to the improvement or maintenance of those specific strengths.
This does not mean that his investment proposals need no longer be quantified in financial DCF terms.
That is needed where possible to make the strategic analysis more rigorous before final approvals are given.
It is just that the type of early-warning signals just discussed will indicate whether the expected improvement in competitive strength and the projected cash flows are likely to be achievable.
Both SBU managers and their superiors ought also to be aware of the time pattern of likely ROI development in each part of the group.
The conclusion suggested by this line of argument is that accounting academics have, perhaps, been worrying too much about trying to find a short-term measure of income which is consistent with DCF.
Performance evaluation by short-term income measures is for short-term control purposes and not for motivating appropriate investment behaviour.
This type of argument is not new.
Parker (1979) argued similarly in calling for measures other than short-term profit calculations in order to reflect a balance between short-term and long-term responsibilities.
Daniel (1961), Anthony, Dearden and Vancil (1966), Dermer (1977), Rockhart (1979), Bullen and Rockhart (1981), and Leidecker and Bruno (1984) have all also stressed that, while it is important to identify key result areas (which may or may not be financial), it is far more vital to identify the critical success factors which lead to the desired results.
Furthermore, Daniel, Bullen and Rockhart, and the student dissertations produced under Rockhart's supervision at MIT show, in particular, that the really critical factors in any one period are usually relatively few, so that it should not be necessary to establish a massive information bank which can be interpreted in a wide variety of ways.
Bullen and Rockhart also show how the critical-success-factor approach can be applied equally at either the strategic or the operational activity level, and offer advice on how to ascertain from managers in fairly short interviews what these factors are.
What the analysis of the corporate strategy literature suggests is that, while it may be possible to ascertain the relevant factors in this way, the findings will be more reliable if those managers have been involved in a careful strategic analysis of competitive strengths.
The corporate-strategy literature now offers a more systematic approach to the precise definition of what the critical success factors need to be in each specific business setting and especially how they relate to each other within a model of success.
The specificity also assists in the construction of parsimonious, but relevant, information systems for control purposes.
Rockhart says that the critical-success factor approach aims to unlock the manager's implicitly held model of success.
It would be even better if that model were made explicit and tested for validity against a more complete analysis of the firm's competitive position — the manager may, after all, be wrong.
It has been argued that building systems to motivate and evaluate appropriate investment behaviour is better achieved when based upon a careful strategic analysis and critical success factors.
Does this at least then mean a reprieve for ROI in terms of short-run performance measurement?
Much will depend upon the degree to which divisional/ SBU ROI figures are important in the managers' reward system.
If ROI is a relatively unimportant part of this reward system, it may then be used as a very useful diagnostic tool by which to assess operating performance.
ROI has a long-established pedigree of use through the familiar Du Pont pyramid.
As an accounting-based measure it can be decomposed through the asset-turnover or profit-margin ratios to pinpoint exactly where problems arise (or progress has been achieved) in short-term financial performance.
ROI is here at its most valuable as a familiar, easily understood diagnostic tool for appraising the short-term economic performance of an organizational unit such as an SBU or division, decoupled from the evaluation of managers' performance.
If a company does wish to use ROI to evaluate managers' short-term performance and those managers are in a position to influence significant investment decisions, then, where the manager's short-term performance is stressed at the expense of his investment performance, there is a danger that he will restrict investment to increase his ROI.
This can, however, be largely avoided by insisting that his ROI targets  are reset each year in the knowledge of assets held and investments to be undertaken during the year.
Short-term performance is then to be judged by a comparison of budgeted and actual ROI for that year.
Through these arguments one arrives at the position that the use of ROI need not be so harmful as academic accountants (including me in the early 1970s!) and more recently academics from other fields have argued it to be.
It must be stressed, however, that, if the potential dysfunctional effects of ROI use are to be avoided, financial controllers must make it very clear throughout the organization exactly how supervisors are using ROI for evaluation and exactly how it is to be used by SBU and divisional managers.
If this is not done, the considerable dangers inherent in the indiscriminant use of ROI remain.
It is very difficult to ascertain whether the many companies which use ROI do use it in the way proposed.
Older surveys such as those by Mauriel and Anthony (1966), Tomkins (1973), and Reece and Cool (1978), and more recent studies such as those of Pike (1983), Scapens and Sale (1985) and Cornwell (1987), do not get close enough to behaviour in specific contexts to reveal whether companies are suffering from a misuse of ROI.
Meanwhile, while Reece and Cool feel that on balance they are not, Anthony, Dearden and Bedford (1984, p. 362) continue to insist that their experience tells them that they are, and Hayes and Garvin (1982) obviously agree.
Practising financial controllers need to consider the position in their own organizations, while academics might undertake more detailed case studies of the matter.
In fact a new study, by Merchant (1989), was published as I was drafting this section.
Merchant takes a useful step forward.
In his analysis of fifty-four profit centres in twelve major corporations, he discovered considerable variation in the overemphasis on short-run ROI performance.
He argues that the problem is most severe in ‘growing corporations’.
This squares with the contention earlier in this chapter that the ROI targets set for embryo ‘star’ SBUs should be different from those for ‘cash cows’, and so on.
Merchant also discovered that some corporations do try to reduce the potentially dysfunctional effects of ROI by various methods:
1
by using leading indicators, and not just ROI, to determine managers' rewards
2
by basing rewards on long-term profit performance (either written or unwritten)
3
by simply not putting so much emphasis on increasing ROI
Method (2), as discussed in chapter 3, may not be of much value if managers do not stay long enough in one position.
Method (3) has also been discussed, and it has been suggested that it would be enhanced by  resetting ROI targets each year.
Method (1) comes close to the view, argued above, that critical success factors could be a better basis for measuring investment performance.
Merchant does not, however, push his argument far enough.
He qualifies advocacy of leading indicators by saying that they may duplicate each other(market share and sales, for instance ) or may not give additional lead time over conventional measures (he cites bookings as an indicator of sales) or do not yield reliable measures.
He further argues that with multiple measures there is a danger of losing focus, and that it is difficult to set targets for some indicators because there are no historical data on them.
He is right to warn of such dangers, but they are more likely where leading indicators are set in some general, non-rigorous way without carefully developing a model of what is needed, and by when, to improve or maintain each SBU's competitive strengths.
Merchant's companies do not seem to distinguish between key result areas and critical success factors.
Also, he does not adequately discuss how to relate those critical success factors in a logically connected way through a model for improving competitive strength in a specific situation, and fails to recognize that it is essential for a strategic analysis to indicate who is responsible for which central success factor.
A careful strategic analysis as described in chapter 4 will help to lessen the disadvantages of using leading indicators.
Even so, Merchant's study, which also covers many other topics, provides more depth than previous empirical work on this question and is most valuable from that standpoint.
It should be used as a platform from which to explore the different modes that he suggests of lessening the short-run myopia.
Accounting and cost-leadership strategies
It has just been argued that detailed financial and managerial controls need to be based on specific product and/or SBU categories.
It has also been recognized that Porter may not be correct in supposing that cost-leadership and differentiation policies should always be mutually exclusive.
Whether they should be or not, accountants need to give careful consideration to the impact that a cost-leadership strategy would have on their practices.
The separation of strategies into generic versions is a useful conceptual device, even if such strategies become intertwined in practice.
Where the aim is to sustain cost leadership over the longer term, rather than to gain a short-term advantage through temporarily having, say, more modern equipment, one must look ahead over the life-cycle of the market (however narrowly or broadly defined).
The experience  curve briefly mentioned in chapter 4 as the basis of the BCG matrix is soon invoked where cost leadership is the goal, and we now need to discuss it more fully.
Recall that the experience-curve theory says that the ability to learn is directly related to the accumulated production experience and that it is possible for a company to reduce total costs over time by a careful introduction of new management and production processes (see Abernathy and Wayne, 1974, and Dutton and Thomas, 1984, for careful assessments of this claim).
Since the experience-curve concept was developed by BCG, it has been shown that there is much more to dominating markets than simple mastery of the experience curve — even where cost leadership is the strategy.
It is still, however, potentially a very important part of the strategic analysis required.
In some industries and markets, it is still a major factor in market economics.
I have visited an electronics manufacturer where the notion of continual cost reduction is an important feature of operating policy.
There are charts displayed in the factory recording manufacturing costs by month and the planned and actual reduction over time.
The firm works to relatively short product life-cycles, but still expects some learning to occur.
In response to questioning, the production director stated that the firm is in an industry where a highly intelligent workforce is necessary, and that, if encouraged to do so, such a group of people working together can always find improvements.
This company, incidentally, had been strongly influenced by the Japanese way of doing things and had introduced just-in-time and kanban methods.
It is also worth bearing in mind that the experience curve does not apply only to single products.
Even where products have short life-cycles and the strategy is to keep ahead of rivals by product development and modification at frequent intervals, as indicated in figure 5.2, such new products are rarely totally different from their predecessors.
Hence there can be experienced-based cost reductions over a generic class of products through time, even though the products differ in some respects.
In fact, the intention of product redesign may be partly cost reduction and partly product differentiation.
Also, if one thinks in terms of longer-run generic experience curves, the argument put forward by Pogue (1985) that more effort on pre-planning reduces the scope for learning has less weight.
It is less easy to pre-plan a future series of design changes.
It is also less easy to forecast their cost consequences.
But design changes can be a key determinant of the experience curve for a generic group of products.
The point is just that some basic concepts relating to the experience curve may still be valuable if used with intelligence and discretion.
Figure 5.2 Experience-based cost reductions for a single product and a generic class of products
Ghemawat (1985) argues that the debunking of the experience curve has gone too far.
It is, he stresses, neither a panacea nor a Pandora's box.
He also sets out clearly the conditions under which the experience curve can be the basis of effective strategy.
In such situations, accountants need to be familiar with full life-cycle costing based on the experience-curve concept.
Such an analysis may not totally determine  strategy, but it is a tool with which management accountants should be familiar.
The experience curve is derived from the earlier learning-curve theory, used for years by production engineers to estimate direct labour requirements and by cost accountants in industries such as aircraft building.
The experience curve takes the following form: where y is the average cost calculated over the company's cumulative volume produced to date, x is the cumulative volume produced to date, a is the cost of the first unit produced, and b reflects the rate of learning.
In fact  where r represents the learning rate.
The learning rate, in turn, is expressed as 1 minus the constant percentage of costs reduced as output doubles.
Hence, if the learning phenomenon appropriate to a particular firm indicates that average costs (over cumulative volume to date, not average cost in the year) will fall by 20 per cent each time cumulative output doubles, one says that the learning rate is 80 per cent on 0.8
If one works in logarithms, the relationship between y and x becomes linear: Hence, it is quite simple for accountants to work out the learning rate for their industry/firm by converting observations of y and x to logarithms and regressing log x against log y.
Again, a few minutes on the personal computer is all that is required.
The resulting b coefficient from the regression is the value of b needed to determine the learning rate, which, by rearrangement of formula 5.2 is  Traditionally, cost-accounting texts have described learning curves, but have not adequately emphasized their strategic importance.
Moreover, one recent major text (Kaplan and Atkinson, 1989) deletes learning curves because it is misleading to extrapolate cost reductions based literally on experience alone.
Perhaps so, but it seems to me that, if the  experience curve is understood as reflecting the continued, planned desire for cost leadership as more is produced, the expression of the curve and the learning rate provide a useful way of monitoring cost-reduction possibilities and helping to formulate strategy.
Perhaps we should put the words ‘experience curve’ in quotes to indicate that the reduction in cost is not automatic.
However, if a cost-leadership policy is being planned, it is useful to have some projections based on assumptions about learning rates.
How might knowledge of the experience curve be used strategically?
The answer, as implied earlier in this chapter, is through a policy of strategic pricing.
Suppose that a company and its potential rivals in a market for a new product are in agreement that the rate at which it could drive costs down is as illustrated in figure 5.3.
Then the first company might decide to introduce the product at a price level below initial cost, such as P 1 , in order to frighten rivals off and gain the maximum possible accumulated production experience.
The benefit of doing this comes later as, through continued effort, costs are reduced.
Having gained the initial advantage, the company can then price at a level to give itself a profit but still deter rivals, who face higher costs from entering.
It might be argued that rivals would be aware of this possibility and therefore move in quickly, bearing initial losses in the same way as the initial company does.
But   that depends on whether rivals can move in so quickly.
We are discussing strategic new developments-someone has to be the first to develop the product or technology.
If the potential for cost reduction through learning more about the product and its manufacturing processes is significant, the experience-curve theory shows how cost analysis and pricing can be used to protect that strategic advantage.
Furthermore, rivals may, owing to the different combination of activities in their groups, not be able to bear initial losses, which would cause financial distress rather than lead to balanced growth.
(The need to consider such matters, despite the theory-of-finance arguments in chapter 1, is addressed in chapter 9.)
Figure 5.3 An experience curve and related pricing policy
Table 5.1 shows in simplified form the type of strategic calculations needed.
The remainder of the analysis in this section, and in the third section of chapter 6, rests on work initiated by Marshall (1985) and reported in Marshall and Tomkins (1988).
Suppose, first, that a company has just developed a new product with an estimated life-cycle as shown in table 5.1.
Suppose also that the company estimates that it will cost 200,000 to produce the first unit and that, by watchful management, an 80 per cent learning rate will be achieved.
It then decides to set its market price at such a level that the one possible rival in the market can, at best, only earn a zero net present value (NPV).
Assume, purely for ease of calculation and exposition here, that the rival can achieve the same learning rate, that demand is not price-sensitive and that, if the rival enters the market, sales will be divided equally between the two rival companies.
These assumptions can easily be relaxed for more realistic modelling.
Assume also that the rival will need to spend 7 million on capital expenditure to enter the market.
With this scenario, assume next that the company decides to set its price initially at 56,700 and lets it be known generally that it will hold price at that level until the average cost per unit of cumulative output   falls below that figure, whereupon the company will set its price at 20 per cent above the falling average cost per cumulative unit.
Table 5.1 Estimated life-cycle for new product
The next key question is whether such a strategic pricing policy will deter the rival from entering the market by offering it at most a zero NPV.
Table 5.2 shows the necessary calculations and the fact that, if its cost of capital is 15 per cent, the rival can only earn a negative NPV of 252,497.
This might indicate, in turn, that the initial company could afford to price a little above 20 per cent over average cost and still prevent the rival from entering the market.
Calculations showing that the rival will face a negative NPV
To construct table 5.2 the first step the cost accountant has to take is to construct the rival's experience curve which reflects the learning rate of 80 per cent.
The observations for plotting the experience curve are shown in columns 2 and 3 of table 5.2, where column 2 shows one half of the total market demand shown in table 5.1 and column 3 is calculated  b = 0.32193 is found by equation 5.2.
The price in column 4 is simply 20 per cent on the corresponding average cost from year 2 onwards.
A little care is needed to calculate the cash flows for column 5.
For example,
With a spreadsheet set up on a personal computer, this requires very little time to calculate and provides a direct guide to strategies concerned with cost leadership.
Obviously, a number of simplifying assumptions have been made here, but the model can be elaborated to allow for price elasticity of demand, differences in learning rates, differences in cost of capital, and so on.
Of course, the rival may fight back, but the point is not that this type of calculation ensures a successful barrier to entry, but that it provides a way of assessing what it will cost the rival to surmount the barrier to attain cost leadership.
This must be a useful input into strategic thinking.
There are a number of derivatives of this type of model.
For example, suppose that your company has a two-year lead into this market.
It would be possible to deduce what will happen to this market if the rival builds a larger plant to try to catch up with your experience and at what level you need to price to try to prevent that.
In fact, you do not actually need to price at that level; the rival just has to believe that you will if it enters the market.
There is also the implication in the model that, if you successfully prevent entry up till the mature stage of the life-cycle, you will then, subject to the price elasticity of demand, be able to increase prices in the way illustrated in figure 5.3.
This would be quite consistent with seeing the product as a high-yielding ‘cash cow’ in the mature stage of its life-cycle.
The type of calculation in table 5.2, therefore, may provide a financial basis for assessing how soon one can begin to manage an SBU or product group as a ‘cash cow’.
It offers the promise of much more precision to the vague categories of the BCG and McKinsey-GE matrices, while attempting to incorporate the valuable insights of those matrices into the capital-budgeting decision.
It was also argued earlier in the book that there is always some point at which customers will turn from a differentiated product to yours because your price is so much lower.
If possible price levels to achieve this can be hypothesized, the model of table 5.2 can be used to inquire whether the experience-curve effect is adequate to sustain a positive NPV at such price levels.
This type of calculation may also be helpful in assessing trade-offs between alternative strategies open to one firm.
The problem in gearing  up investment to shoot off down the experience curve and become the cost leader is that you may be overtaken by technology.
You may produce the cheapest typewriters, but everyone is moving to word-processors.
One needs a projection of life-cycle costs in order to see the benefits of adopting a risky cost-leadership policy if technological change does not occur.
One needs a calculation like table 5.2 to assess how long the delay in the next technological leap needs to be to make one's cost-leadership policy worthwhile.
With an increasing emphasis on high technology, life-cycles in some industries rarely achieve the mature, ‘flatter’ stage.
In such situations an assessment of the slope of the experience curve is vital in determining the likely profitability of new investments.
Bogue and Buffa (1986) also show how an understanding of the experience curve can give one a quite different perception of what is happening in the market.
They describe two companies, one of which is experiencing a rapidly decreasing profit margin and is therefore loath to invest.
The other company, a more recent entrant into the market, is investing on a considerable scale.
The first company cannot understand why, given that it almost certainly has lower costs.
Figure 5.4 illustrates   how such a situation might arise.
Company 1 does have lower costs but has a flatter experience curve than the declining price schedule.
Company 2 has a much more rapidly declining experience curve and is increasing its margins.
The crucial point is that company 2 will be ‘bullish’ about the increasing margins.
It may, however, be in trouble if it does not project the whole life-cycle to ascertain how long the increasing margins can continue.
Also, company 1, by focusing on decreasing margins through time, rather than the fact that it is still earning better margins than a rival, may be ‘bearish’ and let company 2 get established in the market too easily.
Company 1 still has a cost margin on which to fight, and, if it chooses to do so, it could later reap the ‘cash cow’ benefits.
Figure 5.4 Experience curves can make it easier to understand a market (based on Bogue and Buffa, 1986)
Adapted with permission of The Free Press, a Division of Macmillan, Inc. from Corporate Strategic Analysis by M. C. Bogue and E. S. Buffa.
Copyright © 1986 by The Free Press.
On occasions accounting has received a ‘bad press’ from rival disciplines for focusing too much on detailed costs and not on the key strategic issues.
If a company adopts a definite cost-leadership strategy, and many do, detailed cost analysis and subsequent control on a life-cycle basis is absolutely critical for success.
Accountants should not be backward at pointing this out.
Distinguishing between cost reduction and cost measurement
I have emphasized the need to maintain on-going cost control in a way consistent with the strategic schemes adopted.
Mature products need to be managed for cash inflow or extending the life-cycle; strategies based on the experience curve need to be supported by a tough cost-control policy throughout the product life-cycle in order to manage costs down.
It cannot be stressed too much, however, that costs do not fall without effort.
Dutton and Thomas (1984), Ghemawat (1985), and Porter (1985) all describe a range of different ways in which learning can take place.
The experience curve reflects the mix of all these forms of learning.
The financial controller has a key role to play in facilitating this learning process by improving the way in which the cost-accounting system reveals what really are the cost-drivers (i.e. the factors really leading to the incidence of specific types of costs).
Like Kaplan (1988), Porter (1985) argues that, unless the links between the cost-drivers and costs are mapped out clearly, it will not be possible to indicate where ‘learning’ is required in order to maximize cost reduction.
Porter argues that it is also important to distinguish between experience-curve (learning-by-doing) effects and reduction of costs through economies of scale, i.e. the reduction achievable in any one period by increasing efficiency through the use of larger production  plants, thereby spreading infrastructure costs over more units of output.
Such economies of scale must be distinguished from savings through fuller utilization of existing capacity, which spreads the fixed costs of a specific existing piece of plant over more units.
In addition, as Porter shows through extended discussion, the phrase ‘economies of scale’ itself hides many subtleties.
What is the appropriate measure of scale difference between industrial activities?
Sometimes it is global scale and sometimes local market scale.
Only by a careful understanding of the market place can one be sure that one is thinking in the appropriate dimension and take full advantage of scale economies.
Cost-leadership policies do not, therefore, depend for their success upon some crude belief in the experience curve.
There may not even be a marked learning process based on accumulated experience.
Cost leadership may come more through capturing a large-enough share to enable the company to take better advantage of economies of scale than its rivals.
The important point is to monitor pressure for cost reductions from all sources and to take such possibilities into account in strategic analysis.
Even if one thinks one's own company will not reduce costs further, it is dangerous to make the same assumption for one's rivals.
It is important to make a clear distinction between Porter's use of the term ‘cost-drivers’ and that of Kaplan (1988).
Porter lists ten factors, including learning, economies of scale, interrelationships with suppliers, and discretionary policies, that should be examined with a view to reducing costs.
In contrast, Kaplan, in developing activity-based costing, is more concerned with an accurate measurement of current product costs and product profitability.
The similarity of the ‘cost-driver’ terminology may disguise the different underlying purpose of each writer.
In product-costing Kaplan tends to emphasize cost-drivers in terms of outputs from activities, e.g. number of set-ups.
Costs may be driven down by reducing the need for activities, but one should not overlook the reduction of costs in terms of both what causes activities to take place and the efficiency of conducting the activity itself.
This difference is emphasized when one examines how Porter proposes to go about cost reduction.
He and Rappaport (1987), extending Porter's approach, offer suggestions for an outline analysis of costs by strategic categories of action.
Porter refers to his basic value-chain analysis, which was briefly described in chapter 4 and now needs closer examination.
He argues that the value-creating activities may be summarized in nine categories as below; each of the support activities may support any of the primary activities.
Remember, the firm can add value to its products and services by improving its performance in any of the primary or support activities.
Primary activities
1
In-bound logistics: receiving, storing and issuing materials
2
Operations: transferring inputs into outputs
3
Out-bound logistics: distribution activities
4
Marketing and sales
5
Servicing 
6
Procurement: the function of purchasing
7
Technology development
8
Human-resource management
9
Firm infrastructure: planning, legal, financial, accounting, general management, etc.
To commence a cost analysis from a strategic viewpoint one first needs an analysis like that shown in figure 5.5, derived from Rappaport.
Rappaport shows how the cost of support activities must first be carefully allocated over primary-activity categories.
Then the costs, working capital and capital expenditure associated with each type of primary activity need separate identification in order to produce a net cash flow by primary activity.
One has then begun to develop the figures needed to show how the cash flows and costs will be affected by actions taken to restructure the value chain.
Support activities
This analysis is, however, merely a first step.
One presumes that Rappaport's cost framework relates to the accounts of the whole company.
It is obviously necessary to analyse each product (or product group) in this way.
Porter argues that one should look at the areas of the value chain which contain the largest proportion of costs and then consider what cost-drivers of the ten he identifies cause those costs.
Rappaport's diagram can obviously be used as a basis for commencing this analysis.
One should not overlook the possibility, however, that some activities are not adding value at all, and these need to be removed completely.
There is no need for sophisticated cost-behaviour analyses on those.
Now the differences between Porter's and Kaplan's proposals become more clear.
On the one hand, it is difficult to see how some of Porter's ten cost-drivers can be used to assign costs to products in an activity-based costing sense.
Of course, he never meant them to be used like this.
He was concerned with focusing quickly on the areas most amenable to cost reduction.
Nevertheless, he could learn from Kaplan.
It would seem that the primary activity of operations needs to be split into direct and indirect costs, with the latter analysed into major    cost-driver categories as interpreted by Kaplan.
It is likely too that non-manufacturing activities need to be broken down into major cost-driver categories.
It will also not be satisfactory just to add back depreciation and interest as Rappaport does.
Depreciation and interest need to be traced, as far as possible, to products.
And yet, while Porter advocates can gain better insights into cost reduction by undertaking some Kaplan-type analysis, Kaplan supporters can also learn from Porter's efforts to focus quickly on key areas for cost reduction, avoiding an elaborate and costly system with information overload.
Figure 5.5 The value chain and cash-flow analysis (based on Rappaport, 1987)
Adapted with permission of the Free Press, a division on Macmillan, Inc., from Creating Shareholder Value: The New Standard for Business Performance by A. Rappaport.
Copyright © 1986 by A. Rappaport.
Such a juxtaposition of Porter's and Kaplan's ideas is useful provided that one is clear about the differences in their aims.
Both have something to offer the financial controller concerned to work out how to support a cost-leadership strategy with his accounting system.
He should consider a selective (and hence cost-effective) approach to cost reduction, but valuable insights into cost behaviour may come from Kaplan's more detailed activity-based analysis, as well as from Porter's broader interpretation of ‘cost-drivers.’
In general, this underlying complexity of cost determination is not well portrayed in costing textbooks.
Product-costing is still too heavily dominated by a concept of variable costs which is very short-term and defined solely in respect of changing volume of output.
Some, such as Schmenner (1987), feel so strongly about this that they talk of ‘the black holes of cost accounting’.
Schmenner's analogy is not a good one, even if one sympathizes with his sentiment.
In conventional costing systems, figures do not just get absorbed into the system never to be seen again; they may release figures to go flying around the corporate universe, possibly causing considerable mischief.
What this analysis indicates, however, is that cost accountants do not just need to improve their product-costing, vital though that is for strategic decisions.
They also need to give much thought to how their accounting processes can aid cost-reduction strategies.
After all, one alternative upon discovering from an activity-based costing system that many products are unprofitable is to focus on reducing their costs rather than close down their production lines.
To sum up, the Porter and Kaplan perspectives need to go hand-in-hand.
It is for each financial controller to decide what the balance between them will be in his or her own organization.
Also, the balance should probably vary across different life-cycle stages of those SBUs following cost-leadership strategies.
Distinguishing between price recovery and productivity
The discussion in this chapter so far has addressed cost reduction in general without trying to distinguish between the ability to become  more efficient through productivity increases and the ability to improve the relationship between input and output prices.
Clearly, either can help a cost-leadership strategy.
Relatively recently, van Loggerenberg (1988) and his associate Hayzen (1989) have developed an alternative to conventional standard costing based upon this distinction.
(See also Kaplan and Atkinson, 1989, for a slightly modified version.)
The essentials of productivity accounting are described in figure 5.6, where it can be seen that changes in profits through time are decomposed into two major elements, changes in productivity and changes in price recovery, which are defined as follows: Where there are multiple products and resources there will be a productivity measure for each pair of resource and product.
Similarly, there will be a price-recovery calculation for each.
Changes in profits  can therefore be analysed by a set of productivity and price-recovery changes.
One of the advantages claimed for this approach to separating off productivity measurement is that it can be done from conventional accounting statements, thereby providing, as Hayzen says, the link between productivity and profit calculations.
Numerical examples are provided in Hayzen (1989) and, for a slightly different system, in Kaplan and Atkinson (1989).
Hayzen has also produced a computer package for commercial application.
Figure 5.6 The essentials of productivity accounting (Note.
This form of profit decomposition contrasts with normal cost-accounting decomposition, which explains profit changes in terms of changes in revenue and costs)
Without going into detailed calculations which readers can obtain for themselves, the concept has been introduced here to provide a clear distinction between internal operating efficiency and action on relative prices.
In many competitive markets the company may not be able to do much about price recovery, but, by separating out those effects, it can focus more accurately on cost reduction and internal efficiency.
If the company feels that it can largely pass changes in input prices on, it will be more appropriate to focus on productivity changes than on absolute cost changes.
Hayzen also shows how a series of 2 × 2 matrices can be developed in order to give more specific insights into the company's competitive position.
Figure 5.7 presents just the basic grid.
Readers are urged to consult Hayzen's small booklet for supplementary charts.
Hayzen's preferred position is the ‘Pursue’ segment, which he interprets as the situation where prices are kept keen through absorbing revenue price increases (relative to output prices) in increased productivity.
This on its own, however, does not show how well a company is doing relative to its rivals.
Ideally the company should plot both itself and its rivals on the chart.
To take just two other segments, Hayzen says that the ‘Awaken’ segment is not so favoured as one might at first think, because rivals may chip away at one's profits by undercutting product prices.
The ‘Scramble’ segment indicates, Hayzen says, a decrease in productivity being covered by price recovery.
The company's price recovery could be eroded by more productive competitors, hence the need to scramble to achieve productivity gains before this occurs.
In fact, Hayzen's policy implications may not be quite so relevant as they may at first seem.
If one integrates this thinking with the concepts underlying the SBU portfolio grids in strategic analysis, and especially if a successful cost-leadership and limit-pricing policy such as described earlier in this chapter is being pursued, one might expect to see a product (SBU) shift from ‘Fine tuning’, through ‘Pursue’, ‘Awaken’, ‘Scramble’ and ‘Salvage’, over the product life-cycle.
Then the risks that Hayzen associates with the ‘Scramble’ position may not exist, because rivals have been kept out of the market and it is too late to enter.
‘Scramble’ might then be a better position to hold than ‘Pursue’.
Figure 5.7 Hayzen's productivity-profit grid (from Hayzen, 1989)
The main point, however, is that this alternative way of analysing profit seems to lend itself more naturally to business thinking and can be linked in with conceptual thinking about strategy.
In terms of detailed variance analysis, it does not, it seems to me, have much of an advantage over conventional standard costing in most situations, but it does help considerably in focusing on the key competitive issues.
By developing the chart analysis as Hayzen does, one can avoid getting bogged down in detailed variances and concentrate on strategy.
The system relates much more readily than standard costing to the concept of productivity lying at the heart of cost reduction in competitive markets.
Accounting for differentiated-product strategies
We have paid considerable attention to cost-reduction strategies, as probably befits a book addressed primarily to financial experts.
There are, however, contributions that accountants can make to differentiated product strategies.
Under a differentiated strategy, one is not necessarily under less persistent pressure to improve than one is under a cost-leadership strategy.
In the long run most products can be copied, and so one must keep innovating to remain different.
If one cannot, then one must think in terms of cost efficiency to compete with homogeneous products, whatever one's current differentiated position.
Indeed, it may be a clear strategy to do so.
If the experience curve has effect, the initial market share gained through introduction of a differentiated product may give one time to build up a cost advantage.
Where a differentiated-product strategy is being used, the productivity-accounting analysis described in the previous section may also aid perception of what is happening.
However, Hayzen's matrix may again be interpreted rather differently from the way in which he suggests it should.
One might expect highly differentiated products to be able to maintain a higher price recovery.
Indeed the ‘Pursue’ and ‘Awaken’categories might be reversed.
The multi-product firm might be awakened to the need to innovate if it finds its price-recovery margins being eroded.
This does not detract, however, from the general value of conducting the type of analysis suggested by Hayzen.
It is just that one needs to be careful to interpret the segments according to the product strategy being employed.
Great care is also needed in considering the concept of differentiation.
What is referred to in this book as ‘differentiation’ is often referred to rather vaguely as ‘quality improvement’.
The increased competitive pressure experienced by many US and UK companies as a result of the Japanese emphasis on reliability and the inclusion of ‘extras’ in basic models has made them much more aware of the need to pay attention to quality.
In fact Buzzell and Gale (1987) say that the PIMS data establish that quality is the most important factor in success.
But what does ‘quality’ mean?
Is it just to do with production quality control or does it also embrace marketing, accounting and other functions?
The term ‘differentiation’ is preferable, because it throws the emphasis on the need to be ‘better’than rival products not just in some general way, but in precisely specified ways.
Hill's notions of order-winning criteria (Hill, 1985) encourage attention to the precise needs of the potential customer.
That is surely what quality is.
It concerns the effectiveness with which products and services meet customer requirements, and affects all aspects of the interaction between the customer and the company or its agents: the initial inquiry about products, the information provided, product design, reliability, availability, speed of delivery, after-sales service, and so on.
Ascertaining what a company's order-winning criteria are should, it seems, be primarily a marketing task, although, depending upon the nature of the product and industry, there may also be a significant role for manufacturing-development engineers.
Accountants, too, need to understand these criteria if they are to be able to help management reach decisions about product effectiveness.
Delivery of such criteria is not costless, and competitiveness may depend upon the ability to provide such criteria at a specified cost.
Some commentators, such as Berliner and Brimson (1988) and Buzzell and Gale (1987), argue that the idea of a trade-off between quality and cost is outdated.
For example, Berliner and Brimson say,
In the past, it was common practice to work toward economically balanced levels of inventory, quality and other critical factors of prediction…
However, strong competition from quality-minded foreign competitors forces manufacturers to work towards absolute quality and to realize that quality and cost are complementary — raising quality decreases cost.
(1988, p. 44)
The main foreign competitors are, of course, the Japanese.
They, we are told, identify exactly what the customer wants and at what target price (as one of the order-winning criteria) and then attempt to produce the highest-quality product possible at that price.
Pressure is continually applied to encourage the search for higher production quality in the sense of zero defects, minimum delays and minimal scrap.
One must, however, be precise over the use of the word ‘quality’.
The term, when being used to justify the statement that increased quality reduces costs, really refers to reliability of the product and its delivery according to pre-specified attributes (i.e. order-winning criteria).
If one is at the design stage, there is an obvious trade-off between cost and the attributes one can provide.
It is a fundamental law of economics in a world of scarce resources and could not be otherwise.
Even the Japanese do not try to make a profit selling Rolls-Royce equivalents at 15,000.
To avoid confusion, the advocates of cost reduction through pursuing quality ought to make it clear that this applies after the product design has been set.
In the process of product design itself (or deciding what degree of product differentiation to go for) trade-offs are inevitable.
Where trade-offs are inevitable, the cost accountant needs to be  present to advise on whether the given attributes can be supplied at the target price.
Let it now be assumed, however, that the product design has been set.
There is still a task for the accountant in analysing the cost of quality, now interpreted as product and delivery reliability.
A number of companies, recognizing the need for quality, are experimenting with the notion of the ‘cost of quality’, but, as far as I am aware, most such schemes to date are fairly rudimentary.
One scheme of which I have received some details is being developed by a well-known engineering company.
Like many other companies, it has recently instituted a ‘total quality management’ drive with a view to‘getting it right first time’.
Before this, it was realized that quality had an impact on the bottom line, but there had been little analysis of its effects.
The company began its financial appraisal of such questions by trying to assess the actual ‘cost of quality’ across all activities.
In order to quantify relevant costs, it identified a number of cost categories, for which costs were collected over a ten-month period.
These categories were selected to represent the costs of exercising or of not exercising quality control.
The former included the costs of full inspection of products, training and certain engineering operations, while the latter (costs of not exercising quality control) included the costs of rework, warranty claims, defect investigation after transmission of the funded product, obsolete stock, scrap, and so on.
In addition, each cost area examined was subjected to a causal analysis.
For example, costs in the area ‘scrap and rectification’ were shown to derive from these factors:
management control
production methods
machines used
people
supplies and material
design and customer management
Each one of these causes was further analysed into five or six sub-factors.
For example, ‘people’ was analysed into
inability to read drawings
excessive overtime
poor training
‘booze factor’(after lunch hour)
carelessness
rush job
Having built its model, the company then made rough calculations of costs and an approximate indication of their causes.
It was shown that the cost of not exercising quality control was far in excess of the cost of operating such a system.
The company in question would be the first to admit that the analysis was a pioneering effort and that the estimates obtained were, in some cases, quite crude, as the accounting system could not readily provide all the relevant data.
Also, there were clearly large gaps in the analysis as an ‘assessment’ of the total ‘cost of quality’in relation to all order-winning criteria.
For example, no attempt was made to assess the cost of a poor delivery performance in terms of lost sales, or the extra costs necessary to ensure a delivery performance consistent with a desired sales level.
Clearly, in industries with a wide customer base, estimating the costs of not satisfying each specific order-winning criterion, and of any portfolio effect where several criteria are not satisfied, will be a complex task.
Nevertheless, the approach described does indicate an area in which there might be considerable expansion of the accounting function.
No single function can carry out such an analysis reliably.
It should be pointed out, however, that a company need only carry out such a ‘cost of quality’ calculation where it is unsure whether the extra effort required to improve quality (i.e. reliability) is worthwhile that is, where it is contemplating a trade-off in the area where Berliner and Brimson say a trade-off should not be contemplated.
The Japanese, in pursuit of 100 per cent reliability, do not seem to undertake much in the way of quality-costing investigations.
They pursue reliability just because they know it leads to lower costs and increased market share.
Perhaps, in the West, it is still necessary for accountants to demonstrate that fact by calculations in order to get general managers and the workforce to take it to heart.
This would especially be the case, it would seem, if a company were contemplating a move to total quality control for the first time.
The impact of new manufacturing methods
The discussion in this chapter has so far not explicitly recognized the changes that have been taking place and will continue to take place in manufacturing methods.
Such changes are likely to have fundamental effects upon the way companies operate, and so consideration must be given to the impact of developments upon product-costing, cost control and investment appraisal — concepts already discussed as central to the corporate resource-allocation process.
The impact upon investment  appraisal will be left until chapter 6, but it is appropriate to look at the effect upon costing here.
These production developments are loosely termed ‘advanced manufacturing techniques’ but, in fact, cover a variety of developments (see Berliner and Brimson, 1988, and Lee, 1987, for reviews of them).
One way of classifying such developments is as follows:
just-in-time methods (JIT);
islands of automation, incorporating computer-aided design (CAD), computer-aided engineering (CAE) and computer-aided manufacturing (CAM), and flexible manufacturing systems (FMS);
computer-integrated manufacturing (CIM).
Despite the impression given by some authors (e.g. Brimson, 1987), a company does not necessarily develop its production methods along a continuum from JIT to CIM systems.
JIT and FMS developments, for example, may go hand in hand or in sequence, and either development may stand alone or precede the other.
It is important, however, to consider the main distinguishing features of each system in order to understand how product-cost analysis and cost control may be affected, which will in turn influence how resources are allocated within the company.
Just-in-time methods
JIT systems are now generally known to encourage considerable reductions in stock-holding.
This is achieved by arranging efficient delivery systems, both within the firm and for goods from outside suppliers, so that the time between receipt of a sales order and delivery of the product is kept to a minimum, and the flow of deliveries and production proceeds so smoothly that there is no need to produce for stock.
JIT is not, therefore, just a way of reducing stock, but implies a commitment to total waste avoidance, removal of bottlenecks and total quality control — i.e. getting the product out as soon as possible and right first time.
Adoption of such methods implies significant changes for accounting, beyond those discussed in the previous section for ‘cost of quality’ calculations.
First, this manufacturing approach facilitates a drastic simplification of accounting systems.
‘Backflush’ accounting methods (see Lee, 1987; Foster and Horngren, 1988; or Bromwich and Bhimani, 1989) in their more radical versions may involve the complete elimination of stock accounts, including finished-goods accounts.
This not only simplifies accounting systems, but also has an important behavioural effect.
Under a system without work-in-progress and finished-goods stock accounts all  costs become period costs — thereby overcoming the age-old incentive, under absorption costing, to produce for stock to increase current profit without full regard for the impact on future reported profits.
Hence, just the introduction of backflush accounting may have a significant effect in reinforcing the JIT (low stockholding) philosophy.
Another feature of JIT methods is the organization of the production process into cells within which operatives have group responsibility over the whole operation.
Production cells may carry out their own machine maintenance, set-ups, and even packing and labelling.
Usually, too, with the abandonment of work-in-progress, control over production progress is maintained by physical counts of products demanded and produced, along with measures of defects, stock levels, and so on.
This is often described as establishing a reduced need for cost-accounting, and this is obvious from the viewpoint of regular routine control.
It does not follow, however, that the principles of activity-based costing are redundant.
Decisions still have to be made based on product profitability.
There will normally be a need at least for ad hoc estimates of product costs, and, failing this, it will still be necessary to understand the logical relationship between variations in physical processes and variations in costs incurred, even if this is not measured precisely.
Moreover, JIT may make it easier to understand these relationships, as more activities will be directly traceable to production cells.
Whether this makes individual product costs less ambiguous will still, however, depend upon the nature of the product flow through those cells and the joint-cost problem within cells.
Furthermore, JIT may reduce the number of cost pools which need to be allocated to products under a conventional manufacturing system.
Stock controls and warehousing costs, quality checking on goods received, and materials-handling costs will be significantly reduced.
Under well-developed JIT systems, the principles of activity-based systems may, therefore, be more easily and fruitfully applied when making product-mix decisions of the strategic kind.
As already indicated, JIT stemmed from the general Japanese philosophy of continual improvement.
This implies continual cost reduction.
The Japanese are famed for their ‘quality circles’, but many of their companies also have ‘cost-reduction circles’.
See, for example, the discoveries made by Ford Europe on examining the role of the finance function for Japanese motor manufacturers (IMEDE case study, 1984).
These cost-reduction circles may operate on an informal basis and focus on the removal of obvious bottlenecks rather than looking where the company's cost system directs, but, as business becomes more and more competitive, it may be necessary to track cost-reduction progress.
At least the Japanese show conclusively that  the notion of continual cost reduction is not redundant, as some critics of the learning-experience-curve phenomenon seem to suggest.
Accountants should take the lead in establishing cost-reduction circles.
The JIT approach developed by the Japanese also reinforced their concept of total quality control.
The accounting implications of this have already been discussed in the previous section.
In more advanced applications of JIT, companies have been very active in restructuring their value chains particularly in relation to their procurement activities.
Many firms now buy in rather than make components themselves, and overcome the problems of controlling quality and delivery by forming very close relationships with suppliers sometimes in return for single-supplier status.
Some major companies build this relationship by offering engineering and manufacturing guidance on modern manufacturing methods to smaller suppliers.
In return, these suppliers are sometimes required to operate an ‘open book’ policy in relation to the purchaser.
It then becomes appropriate to wonder whether the analysis proposed by Rappaport in figure 5.5 for a single entity might not, in such situations, be expanded to encompass the partnership.
Accountants could play a key part in analysing the joint situation of the two firms in order to propose areas in the ‘partner organization’ where value-chain reconfigurations might be of benefit to both firms.
‘Islands of automation'
Here we need to consider the impact of CAD, CAE, CAM and FMS.
With more sophisticated design procedures under CAD, the balance of costs is clearly being shifted from manufacture to design and it becomes more important for cost accountants to use their cost analysis at the design phase (i.e. the ex ante use of activity-based principles discussed earlier).
Similarly, as more attention is paid to CAE, engineering and testing costs become more significant and need to be incorporated into product costs rather than left floating around as a general charge ‘below the line’.
In one case that I know of, a subsidiary of a major UK company got itself into difficulty through totally misunderstanding its product costs as engineering costs, written off as a lump-sum period cost, increased with the production of more advanced products.
On almost any basis of cost allocation, this would have resulted in the wrong product costs and a wrong emphasis on product mix (long-term, not just short-term).
Moving to CAM and FMS, one expects to see a significant decrease in direct labour costs.
If companies stick with on-cost rates on direct labour, percentages in the hundreds or thousands are not unusual.
This  in fact was one of the driving forces behind the development of activity-based costing — the labour-hour basis was no longer applicable.
CAM developments therefore give greater credence to the ideas behind activity-based costing, though not necessarily to its application in complex routine costing systems providing regular monthly reports on an activity basis.
Computer-integrated manufacturing
One will also expect to see more physical controls, rather than routine cost-accounting controls, on production performance as the degree of automation in manufacturing increases.
This is clearly seen if one jumps to the ‘ideal’ world of CIM, with its notions of ‘ghost factories’operating with minimal human interference.
Under such automated systems and automated quality controls, the production line will be stopped if performance is not up to standard.
One will not need an accounting report to generate this.
It also follows that in such an advanced mechanized world the scope for cost reduction will be limited once manufacturing has commenced.
Cost analyses will then become almost wholly ex ante.
The long-term cost reductions will be achieved mainly by the redesign of both the product and the process, rather than through the refinement of operations alone.
Further cost pools may disappear: set-up costs will become minimal as machines switch from product to product.
The ‘costs of increased variety’ argument will lose much of its force, chipping away at the need for a complex form of activity-based costing as conceived by Kaplan.
Also, with a stark division of costs into direct (variable) material costs and largely fixed machine costs, it may become impossible to use detailed activity-based logic to allocate these fixed costs.
All that may be needed is a classification of machines and a measurement of time spent on each machine by each product.
Moreover, the depreciation debate will be ‘dusted off’ after years of shelf-life and revisited in more depth as depreciation becomes the major cost.
Perhaps that is where the Gregory/Grinyer model of leasing charges described in chapter 3 will come into its own.
However, before getting too carried away and concluding that detailed activity-based accounting may soon be outdated, and replaced by more simple machine-time analyses, it must be remembered that the extremes of CIM are still a long way off for most industries.
In the meantime, I would argue, the logic of activity-based product-costing has a vital part to play in ensuring proper corporate resource allocation.
At least, this would seem to be the case for companies that have many product lines and are no further ahead than the ‘islands of automation’ stage.
Also, most companies and other organizations can still benefit from applying activity accounting in pursuit of cost reduction, even if their product range is well focused.
Summary
The chapter began with a discussion of the contribution that accountants might make to financial profiles for different locations on portfolio matrices, and the use and abuse of ROI.
The main argument then turned largely to cost-accounting and cost reduction.
In the process many challenges to cost accountants were identified.
Schmenner (1987) says that traditional cost-accounting has been a ‘star’ for years and is now collapsing in a black hole from which managers must escape.
This is eloquent, but misleading.
One needs to build upon the foundations of cost-accounting, not escape from it.
With the increasing scale of international competition, it will become vital to know exactly what product costs are where price is an important order-winning criterion.
New, more refined forms of cost-accounting, such as activity-based costing, will be needed and become more feasible with added computing power.
They will become indispensable strategic tools for both product costing and cost reduction.
Though still far from perfect, as Bromwich and Bhimani (1989) indicate, they are an improvement on older practices.
Moreover, with improved internal-management accounting practices, it is likely that external disclosure practices will also improve, to help combat concerns about the short-term biases of current accounting practices and stock-market pressures.
However, I believe that in the longer run, as production processes become more automated, Kaplan's activity-based costing concepts will find their main application at the stage of product design and investment selection, although they might have a wider application in service industries, such as the Health Service, and in the control of non-manufacturing costs.
These developments in cost-accounting are likely to be of central importance in resource-allocation decisions for the foreseeable future.
Cost accounting can regain its rightful place alongside, and integrated with, business policy and strategic analyses.
They should never have been developed separately in the first place.
At least the up-to-date cost accountant has plenty of ammunition to fire at the chief executive who insists that accounting has little to do with strategy.
6 Towards Integrating the Theory of finance and Strategic Analysis
It should have become apparent to the observant reader that there are marked inconsistencies between the theory of finance as described in chapter 1 and the basis of strategic analysis as described in chapter 4.
It is now time to identify these issues more clearly and see whether it is possible to integrate these different perspectives.
The relevance of discounted cash flow calculations
It is quite clear that the theory of finance adopts net present value (NPV) as the proven and accepted valuation criterion for any asset.
In contrast, the portfolio-management analysis of corporate strategy may take some account of discounted cash flow (DCF), but it does not figure as a central concept.
The emphasis is upon detailed analyses of competitive positions; the financial benefits are evaluated in very general terms by reference to rates of return and profitability.
The exception is the Boston Consulting Group (BCG) matrix with its precise ‘Growth'/'Share’ axes, but it still does not use DCF.
It is appropriate, therefore, to ask a very basic question first.
Is DCF as desirable a form of analysis as the theory of finance implies — especially given criticisms of the technique voiced by some academics in the corporate-strategy area?
Hayes and Garvin (1982), for example, went so far as to blame the growing use of DCF for the relative decline in the performance of Western businesses.
Such views are often picked up and repeated by non-finance specialists (e.g. Hill, 1985).
In addition, criticism of DCF is often not separated clearly enough from criticism of accounting return on investment (ROI).
While accountants are criticized for producing excess ‘short-termism’ through stressing ROI, they are also accused of ‘devaluing the future’by discounting its importance using DCF.
If such obvious confusion exists in management literature without a financial orientation, it is high time financial analysts devoted more attention to  it.
In fact confusion can also be found in the accounting literature.
Even Berliner and Brimson (1988, p. 186) in their valuable report on the CAM-I study, tend towards such confusion when they say companies have focused on short-term investment benefits using quantitative financial information such as ROI and NPV!
As was made clear in chapter 3, the question of the appropriateness of DCF obviously needs to be separated from that of the appropriateness of using ROI.
The latter has already been addressed in earlier chapters.
Here the focus will be on DCF.
It is fashionable among some general business academics to attack DCF by stating that the Japanese do not get caught up in ‘DCF number games’ but place more emphasis on corporate discussion and the generation of a consensus about whether the development being considered should go ahead or not.
The argument runs that the Japanese take a long-run perspective in building market share and that this has paid off; hence, if they do not use DCF, it is probably dysfunctional.
The argument is then reinforced by reference to studies which say that the Japanese rarely use the concepts of NPV or the internal rate of return (Tsurumi and Tsurumi, 1985).
Hodder (1986) has showed that it is dangerous to rely upon casual observations of that sort.
He says that, while it is true that few Japanese companies use NPV and IRR, ‘the vast majority of Japanese firms’ do incorporate an imputed interest charge based on the capital outstanding in their cash flow projections.
Hodder offers an ‘idealized’ example of a company evaluating a project with an outflow of 1,000 million yen followed by an inflow of 200 million yen per annum for ten years.
The annual cash flow of 200 million yen is then split into an imputed return of interest on the investment outstanding at the beginning of the period and a recovery of the initial investment.
Hodder then suggests that the decision is based upon the ability of the project to recover the initial outlay plus imputed interest and yield a surplus.
Hodder's example is shown in table 6.1: the project hypothesized recovers imputed interest and capital outlay in just over seven years.
Of course, Hodder's table reflects a concept very close to the familiar annuity-depreciation method — the only difference being that the latter method would spread depreciation over the full ten years of the asset's life.
Also, what Hodder calls ‘adjusted cash flow’ is a form of residual income — a concept used by accountants for many years and discussed in chapter 1.
Indeed, this issue was addressed in an earlier study (Tomkins, 1973, pp. 115–21), where it was illustrated how it was possible to devise an investment-appraisal rule (just like that above) consistent with NPV but based on residual income even when the project cash flows were   uneven through time — although it was felt that NPV was more straightforward.
The important point here, however, is that, while Hodder stresses that the example he gives is ‘idealized’ and that the interest charge is often based on the undepreciated investment balance, he shows that imputed interest does enter the investment decision-making process of many Japanese companies.
Table 6.1 Project cash flows with imputed interest (million yen)
The DCF calculation is therefore one way in which recognition of the cost of funds can be incorporated into corporate investment calculations.
It would be absurd if, following the Hayes and Garvin criticism of DCF, companies were to drop it and then assume that funds had a zero cost.
Even Japanese companies do nothing of the sort!
If DCF is not used, account has to be taken of the cost of funds in another way.
This may be an explicit allowance for imputed interest as a deduction from revenues in periodic income calculations to arrive at a residual income figure, or there may be no deduction for required equity yields in the income calculations at all.
In the latter case the net income attributable to shareholders will have to be assessed on the basis of whether it is reasonable compared with desired yields.
Moreover, if conducted rigorously, all these different approaches should be consistent with each other and lead to the same investment decisions.
So, why does finance theory tend to stress the use of DCF rather than other approaches?
The answer is just that it simplifies interpretation of the figures by reducing uneven streams of benefits and costs to a single index of NPV (or IRR).
It is easier to work in terms of one number (or valuation) instead of trying to assess a whole stream of different figures.
The cost of funds must be incorporated into investment analysis in one way or another, and DCF is probably the simplest technique for summarizing its impact.
It follows that, if the Japanese do incorporate imputed interest charges, they are obviously trying to assess whether the cash inflows are sufficient to meet all claims upon them, including the yields required by shareholders.
The relative success of the Japanese is, therefore, not based upon disregarding the risk-related cost of funds, which is what the DCF mechanism is designed to address.
In so far as cost-of-funds matters lead to Japanese competitive advantage, it is more likely to be due to their willingness to take greater risks because of the low relative cost reported by Prowse (1986) and confirmed by Hodder (1986), rather than a propensity to disregard DCF.
It is also possible that, through the process of developing consensus, the Japanese arrive at better estimates of cash flows and obtain more general commitment to their projects among management and workforce.
That, however, cannot be blamed upon the DCF technique.
As Hodder and Riggs (1985) say, in response to Hayes and Garvin, the fault, if any, in US business lies with the way the DCF technique is often based on inadequate data (i.e. inadequate consideration of the factors affecting cash flows) or incorrectly applied (i.e. incorrectly using discount rates, or adjusting for inflation as discussed in chapter 1).
The essential point, then, is that the NPV approach to evaluating alternative cash streams is a generally correct valuation model where the discount rate is the opportunity cost of committing resources to one course of action rather than another.
In particular, it is the basis for estimating how shareholders, who can more easily conceive of their investment problem as one of switching funds between alternative projects, will evaluate projects.
If companies wish to see the impact of their decisions on shareholder wealth, the DCF apparatus and associated cost of capital formulation are vital.
Companies may not go ahead with every investment with a positive NPV because other stakeholder positions may have to be considered, but that does not mean that there is a theoretical defect in DCF as a market-valuation mechanism.
Finnie (1988) also provides recent support for this view.
Before leaving the discussion of whether DCF is relevant, a brief reference should be made to the possible use of option-pricing theory as a basis for valuing corporate strategies (see Logue, 1981).
This is relevant to the debate because no explicit estimate of the cost of funds is incorporated into this type of analysis.
Brennan and Schwartz (1986) demonstrate this by an interesting application of option-pricing to the valuation of a mine.
It must be stressed, however, that an option-pricing approach  implicitly takes into account the cost of funds and that it should, in theory, arrive at the same current valuation figure as the DCF method.
This will not be pursued further here, as it is still uncertain whether option-pricing analysis can fulfil its promise in the valuation of  a broad set of corporate strategies.
Developments in the area need to be watched by those with interests in corporate finance and strategy.
Difficulties in measuring net cash benefits from investment projects
The conclusion that DCF is correct conceptually and, contrary to the arguments of Hayes and Garvin, Hill, and others, does not have a short-run bias may not be very useful practically if it is very difficult to estimate the future benefits to be derived from the project in cash-flow terms.
There are at least two situations where this is likely to occur:(1) where the aim in making the investment is to be ready to take advantage of a development if it occurs, and (2) where the investment is in the latest technology.
In the first situation, the ideal, as suggested at the end of the previous section, is to produce a valuation of the chance of taking up a strategic option.
Without a general market in such options, this is difficult.
Nevertheless, the recognition that one is investing in a strategic option may help focus attention upon relevant matters.
In particular, in this type of assessment it is important not just to think in terms of incremental cash flows above those currently being received, but also to bear in mind what the problems will be if no investment is undertaken.
Declining the chance to invest may result in considerable losses through lack of maintaining competitive strengths in the industry.
The base from which to measure incremental cash flows is then that position and not the current benefits being received.
In such a situation an investment may be more in the nature of an insurance policy to protect existing business: a step to reduce uncertainty.
Of course, not all strategic options are of that type, but, where they are, this approach may at least get one into the right position for estimating the cash benefits to compare with the investment outlay.
Investment in new technology has similar characteristics and the above approach may again be worth contemplating, but there are also other matters to watch for.
First, the selection criterion should be NPV and not some arbitrary and very short payback period.
Many companies still use incredibly short payback periods, often set by financial directors playing a very risk-averse game.
Bearing in mind the notion of a portfolio spread of risk, directors do not have to be so risk-averse to all types of investment — especially where, with new technology, it may indicate the creation of an option to stay in business.
Also, the discussion in chapters 1 and 2 about the nature of project betas is relevant.
To the extent that the technological success is not highly correlated with the returns on the market portfolio, the project is not so  risky for a well-diversified shareholder.
Hence, the requirement for very large rates of return is not justified from a shareholder-wealth viewpoint.
If DCF is to be criticized, it is not the technique which is wrong, but the setting of discount rates which over-discount the future.
But this can only be put right by changing attitudes to risk aversion.
The solution is more far-reaching than just changing a technique.
Investment in new technology has often been directed to achieving fairly tangible benefits: reductions in inventories, savings in space and production time, and improved reliability of the product.
It should not be underestimated how important it still is to seek such benefits from investment in new methods.
A chief executive of a major UK company who in a public speech stressed how far companies in his industry lagged behind the Japanese found, upon analysis, that the shortfall was almost all due to excessive holdings of assets.
In comparison with the Japanese, UK companies often have somewhat larger profit margins, but then ROI is substantially less due to excessive fixed assets and work-in-progress and excessive investment lead times leading to more idle assets.
This same executive said that he was against incrementalism in the sense that major reductions in assets (or increases in asset turnover) had to be achieved.
Where new technology can be used in such a way to reduce assets and lead times, it should be possible to establish reliable measures of investment benefits.
Increasingly, however, the benefits of new technology are more related to providing a flexible  manufacturing base — i.e. where product mixes can be rapidly changed with little human intervention and set-up costs are low.
I recently visited a factory that produces heavy engineering goods.
A flexible engineering system had been introduced after considerable debate about the worthwhileness of the investment.
The investment was eventually approved on the basis that it would be possible to vary products quickly over six different types.
Within two years of the investment the company, realizing that it could now offer customers more custom-built specifications, was producing a considerable range of products from the new machines.
It believes that this has added greatly to its competitive position in the market.
It is this last comment which provides the clue to the better definition (though not necessarily precise specification) of project cash flows.
Large-scale investments should be supported if they have positive NPVs.
They will be likely to have positive NPVs if they enhance product order-winning criteria which are related to a thorough analysis of competitive strengths and industry attractiveness — preferably using a clear model of the industry such as Porter (1985) proposes.
Kaplan and Atkinson (1989) address the problem of new-technology investment and suggest that benefits are split between the tangible and  the difficult to define.
If, they say, projects are not found acceptable purely on the basis of the tangible benefits, then it may be useful to turn around the investment question to ask what the present value of the benefits must be to justify the investment.
This may, indeed, have some merit, but by itself it is not likely to help with decisions about major investments to keep up with the technological race.
It is vital to set the estimated present value of the benefits within a carefully structured model of the likely changing scenarios of order-winning criteria as identified through a strategic analysis.
Simmonds (1987) and Bromwich and Bhimani (1989) also consider the problem of strategic-investment appraisal.
Both recognize the need to trace the benefits of such investments on a range of market factors.
It must be realized, however, that there are many factors which could be incorporated into most major investments of a strategic nature.
A broad assessment of increased market share without an analysis of order-winning criteria is not good enough.
General statements that investment appraisals should include data on, for example, product enhancement, diversification, risk reduction and increased internal benefits do little more than say that something new is needed.
While the ‘manufacturing critique’ of DCF presented by Hill (1985) may be resisted as argued above, he is nevertheless correct in his fundamental message that assessment (of the cash flows) needs to be based on the degree to which order-winning criteria are enhanced.
A checklist of some strategic factors which may be relevant is not good enough — the appropriate factors and their relative weights can only be determined through a conceptual model of the market place.
The relevant factors to consider in investment appraisal are likely to be different for each product and can be determined only through a thorough understanding of the company's and industry's actual and specific situation.
This can only be acquired through detailed knowledge of the technical nature of the product and the manufacturing process, knowledge of the economics of the relevant markets, and awareness of the complete value chain for the company's products and how investment will enhance each part.
Without a conceptual model of the market place and the organization's possible place within it, general checklists of factors lack rigour and may simply make it easy for interested parties to emphasize different factors at different times, depending on their interests.
As we shall see later, this may happen anyway, but, in so far as the financial analysis is meant to estimate the increase in value of the corporation to the shareholder, the system surely should do its best to meet that objective and to narrow the scope for obfuscating the effect upon shareholder wealth.
If there are viewpoints to consider other than that of shareholders, they should be considered separately and with like rigour.
Similar criticism can be levied at the Multiple Attribute Decision Model (MADM) highlighted by Berliner and Brimson (1988, pp. 18996).
Examination of the model shows that managers are asked to form an objective function with numerical weights attached to critical success factors (there is no clear distinction between critical success factors and key result areas).
Two projects are compared by means of weighting the following specific factors.
The values attached to these factors for the two projects are weighted and summed, with an adjustment reflecting the level of confidence in the estimates.
The projects end up with scores of 250 and 215 respectively, and, hey presto, the project with the higher score is deemed the better.
The pertinent question is what theoretical basis there is to such a model.
It could be argued that the weights reflect the decision-makers' theory of investment desirability, but what theory could rate NPV 333 per cent more important than process yield and ten times as important as basic R&D?
To estimate the effect upon shareholder wealth the criterion is NPV.
One will probably need to consider many critical factors in order to develop cash-flow estimates to calculate an NPV, but those critical factors themselves will only be identifiable by developing an understanding of the market place.
For this one needs to consider  industry attractiveness and competitive strengths, taking into account short- and long-run customer needs, marketing strategies, appropriately linked technologies and R&D activities.
The development of relevant critical factors needs to be based on a properly sequenced series of requirements leading logically to success in the market place and hence NPV.
Moreover, the closer a company moves towards computer-integrated manufacture (CIM), the less possible will it find it to look at investments in isolation.
Their position in the whole network of assets will need to be considered and how the total capacity contributes to strategic success.
The company needs to know what the investment must achieve to produce goods which can be sold profitably.
Vague weighting is inadequate!
Models can be useful and have a part to play, if built on a sound theoretical basis.
If reliable evaluations of strategic investments are required, there is no substitute for a thorough understanding of how to enhance the company's competitive situation in the market place.
General checklists or weighting systems not based on such an understanding are likely to do more harm than good.
Similarly, if investment appraisal is to rely upon more non-financial data, the logical link between those data and improved financial performance in the longer run should still be implicit and may well benefit from being made explicit.
Such factors stand as surrogate factors for NPV in view of estimation difficulties.
They are not alternatives to be balanced off against it.
Can more financial rigour be incorporated into portfolio-grid analysis?
The discussion in the preceding section has emphasized how insights from the corporate-strategy literature may help in arriving at a sensible assessment of individual project cash flows, and that models need to be based upon theories about market success.
While fully believing in this, Marshall (1985) and Marshall and Tomkins (1988) have also explored the possibility of building a more direct link between corporate finance and strategic portfolio grids.
Marshall raised the question of how one could go about drawing financial-value contours on a portfolio grid.
Is it possible to make the policy advice derived from a portfolio grid more specific by incorporating into the grid increases in value?
It is very important to realise exactly what was intended in the analysis about to be described.
We were not drawing DCF contours to represent a range of different strategic business units (SBUs) located on a portfolio grid.
We were attempting to model the NPV effects of investment in one product (or one SBU) in one type of market situation at different stages of its life-cycle as that one product/SBU moves around the matrix.
If this  could be done for one product, it could be done for others with their own market strategies and situations.
In due course, perhaps, these might be combined to provide a ‘harder’ quantification of a multi-SBU portfolio grid, although we have not progressed that far.
Indeed, we fully recognize that the model about to be discussed is only suggestive of one avenue of development and is not conclusive.
A hypothetical product for which a cost-leadership strategy is being employed was assumed.
While the exercise about to be described was a theoretical model, not an empirical test, its results indicate that positive NPV opportunities probably do exist in some ‘cash cows’ and that an integration of DCF and portfolio-grid analyses may lead to a more careful and sophisticated use of portfolio grids.
The model offered is not a general model.
It essentially takes one of the simplest situations possible.
It is intended to illustrate the broad direction in which modelling might be developed to build bridges between financial and strategic analysis.
Much more sophisticated models will be needed in the more complex situations met in practice.
Marshall wondered, first, whether NPVs for investment to increase market share would always be positive or negative as might be inferred from the BCG matrix.
Note that the focus was on investment to increase market share, not just to retain a market position.
The inferences for NPV taken from the BCG literature are as shown in figure 6.1.
(See Marshall and Tomkins, 1988, for a justification of this inferred BCG pattern in relation to investment to improve market share).
The BCG matrix was chosen because it had clearly measurable axes for hypothetical examples, which is not the case for the McKinsey-GE matrix.
The simplicity of the BCG assumptions was recognized, without forgetting the need for a more rigorously derived conceptual model of the market.
The analysis simply took the BCG framework as given to see what results would be obtained.
The framework may be simple with its emphasis on market share and the experience curve, but it is a model with a logical basis related to what is needed in order to be successful.
It was then decided to hypothesize a particular market situation for a single product line and ascertain whether investment to increase market share at different stages of that product's life would have positive or negative NPVs.
The BCG matrix has a vertical axis labelled ‘Market rate of growth’ which never goes negative (i.e. never moves into the declining phase of the life-cycle), as noted in chapter 4.
Hence, Marshall pointed out that one could simply use years from 1 upwards in the ascending part of the life-cycle as an index of a falling growth rate.
(See, for example, the vertical axis of Figure 6.2.)
In addition it was decided to calculate NPVs for further investment in the hypothetical product for   each year according to a range of assumptions about the initial market share held — shares from 10 to 90 per cent, with separate calculations for each 10 per cent .
(See the horizontal axis of Figure 6.2.)
Figure 6.1 NPV by quadrant with respect to investment in increased market share (Note.
Quadrant NPVs are only those related to investment in increased market share)
The market situation posited was similar to that described in the example of table 5.2: a company was attempting complete market dominance through a cost-leadership strategy.
That strategy would be enforced by a limit-pricing procedure which would only allow the rival a zero NPV.
On the assumption of a two-firm market and that a rival would not enter the market if it faced zero NPV, NPVs were calculated for investment by the price-setting firm based on the limit prices needed to prevent the rival from entering the market and the cost levels that could be achieved by moving down the experience curve.
The main   table produced from Marshall's study showed NPVs as in figure 6.2.
These were calculated with both companies facing 75 per cent experience curves and discount rates of 15 per cent.
Interestingly, the dividing line between positive and negative NPV in figure 6.2 coincides exactly with the BCG division between ‘stars’ and ‘? s’(‘problem  children’), and between ‘cash cows’and ‘dogs’(see Hedley, 1977).
More importantly, however, there is, in figure 6.2, no natural dividing line between ‘stars’ and ‘cash cows’or between ‘? s’and ‘dogs’.
Certainly positive NPVs are larger in ‘stars’ than in ‘cash cows’, but there is a gradual shading-down.
There is some profitable investment for increasing market share in ‘cash cows’.
This is consistent with some critiques of the use of portfolio grids as discussed in chapter 4.
Figure 6.2 Illustration of imposing DCF contours on the BCG matrix
Of course, the NPVs obtained derive from the assumptions made.
Given the market position posited, successfully excluding rivals early in the life-cycle obviously gives a longer-term prospect of profits and a higher NPV.
But, despite the few years remaining, there is still some positive NPV for investment late in the cycle (strictly at the end of the upward stage of the life-cycle), because prices can be increased given the cost advantage over rivals gained through accumulated experience.
Now consider the right-hand side of figure 6.2.
A firm with a lower market share than its rival would be foolish to try to play the game of cost leadership through the experience-curve effect.
With its low market share it has higher costs than the rival, and so, by pricing to give its rival a zero NPV, it can only earn a negative NPV itself.
With the example described in figure 6.2, a company needs to achieve a 60 per cent market share before the cost-leadership strategy proves advantageous.
The ‘? s’ have more negative NPVs than ‘dogs’, because they bear negative profits for more years.
Also, it is clear that the results of the situation modelled in figure 6.2 challenge the BCG's interpretations from an application of the very tests and logic (the experience curve) underlying the development of the BCG matrix itself.
The BCG is only partly right, even on its own assumptions.
Market situations can arise where the inferred investment directives of the BCG matrix are misleading.
The results do not mean that the bottom left quadrants are not cash cows.
But, while they are cash cows, some further investment in them is worthwhile (in the example tested).
The finding about ‘? s’ also raises problems for the BCG.
How can investment be encouraged there with such negative NPVs?
The answer is simple: investment in ‘? s’ may be justified if a generic strategy of differentiation is pursued.
In the type of market situation examined here, it cannot be justified if a cost-leadership policy based on a belief in the experience curve is pursued.
That, however, leaves the BCG with a problem, because the logic of its matrix is based on cost leadership through the experience curve.
On these grounds, it should not have identified ‘? s’ as possibilities for investment.
Our analysis therefore suggests that portfolio grids might yield different strategic advice for  specific locations on the grid according to the market situation and the type of generic strategy to be followed.
The reader may justifiably feel uneasy with all this.
It is obvious to ask whether these results are not simply those obtained in this particular numerical example.
The answer is partly ‘yes’, but that does not invalidate what we were trying to do.
Marshall (1985) had already carried out a range of sensitivity tests, and had found, for example, that varying the discount rate from 0 to 30 per cent only shifted the break-even market share at time N = 1 from 50 to 70 per cent .
The major shift was caused when the two rivals were allowed to have markedly different experience curves, but this is unlikely in practice unless one has discovered markedly better practices for continual cost reduction.
These tests indicated no reason to doubt our basic conclusion: that the BCG is only partly correct even on its own assumptions.
Moreover, our 1988 study shows that these results using NPV bear some similarity to empirical findings from PIMS studies (see chapter 4) where profitability has been related to market share.
Despite all this, it is obviously necessary to avoid a naive interpretation of figure 6.2.
Reference to the original work with its full specification of assumptions is essential.
Also, it needs to be stressed that the McKinsey-GE matrix or a Porter analysis allows for different key characteristics to represent market attractiveness and competitive strengths according to the nature of the SBU and the market in which it is located.
A test based on a single product at different stages of its life-cycle does not attempt to deal with that situation.
Nevertheless, this test does demonstrate how one particular market situation can be modelled on the basis of prescribed strategic assumptions and DCF contours placed on the portfolio grid.
This is a tentative, very tentative step towards injecting more rigour into strategic portfolio analysis, using modelling to integrate financial analysis and concepts from one of the generic competitive strategies.
If other, more complex situations can be modelled, it may be possible to gain insights into how financial, strategic and industrial-economics concepts may be fitted together, not just to explain general market tendencies, as the PIMS studies do, but to provide direct advice to business managers in specific markets with specific competitive strengths.
Such analyses might indicate, for example, which competitive weaknesses, if removed, are likely to bring about the largest increase in NPV; how much market share needs to be increased in different SBU locations to provide an adequate NPV; whether this is easier in some locations than in others; and the degree to which some markets are more attractive than others in terms of potential NPV.
All we really claim is that such models may be useful in helping managers assess how much the value of the firm might be increased through investment in alternative locations on the portfolio grid.
The inputs to the model would need extensive debate in practice.
Similarly, its relationship and relative priority to models for other SBUs would need debate.
In this test, the portfolio grid was not used to model the whole of a corporate group.
Furthermore, I have long borne this in mind:
The major strategic decisions in a corporation are seldom taken by its directors or senior executives referring to a mathematical algorithm to suppress judgement.
Rather they rely not only to some extent upon judgement and intuition about the future, but also upon personal negotiations and bargaining between the executives, who may be competing with each other for scarce resources and may each be supporting their case with OR and economic models.
A planning process which fails to accept this…is unlikely to be of much use in the realities of business life.
(Wagle and Jenkins, 1971, p. 39)
Accordingly, we would see our modelling approach as an input to corporate debates over resource allocation, not as a provider of prescriptive advice.
This theme is developed further in chapter 8.
A further point of interest arises from this analysis in respect of the use of DCF.
Earlier in this chapter it was argued that the use of DCF was justified on pragmatic as well as theoretical grounds because it provides a single index of value and avoids the necessity for comparing streams of cash flows.
Imagine the complexity of trying to produce an analysis like that in figure 6.2 if one had no calculus for reducing projected streams to single-value indices.
If it proves possible to develop useful modelling links between financial and strategic analyses, DCF will be a device for simplifying and summarizing — quite apart from its theoretical justification.
The cost of capital in strategic analysis
Another role for the finance specialist in relation to strategy has to do with the cost of funds.
Corporate-strategy literature rarely works through such issues carefully and this can be especially important when considering global strategies.
For example, Prowse (1986) provides a comparison of the cost of funds in Japan, West Germany, the United Kingdom and the United States.
He demonstrates that funds are  unusually cheap in Japan.
Understanding international competitors' behaviour may depend crucially on understanding the way they finance their business.
We saw in chapter 1 how single projects should be evaluated using the adjusted present value (AP) method when sources of finance are linked with specific projects.
Such finance and investment interdependencies are also important at the more general strategic level.
It will not be very helpful to designate a foreign division a ‘cash cow’ if it is located in a part of the world from which it is impossible to remit funds.
Similarly foreign-exchange and political risks and their relationship to required returns on investment need a proper evaluation as determinants of market attractiveness.
Too often finance texts see such finance analyses as separate activities related more to individual project appraisal and not integrated into broader strategic analysis.
Finance experts have a clear role to play, along with accountants, in improving the specification of market attractiveness and competitive strengths.
It was made very clear in chapter 1 that, in determining the yields required on investment, it is possible to formulate the required rate of return according to the level of the corporate group addressed.
That is, it is possible to use the capital asset pricing model (CAPM) to determine the rate of return that shareholders require on the whole corporate group, or from specific divisions or individual projects.
The relevant level of focus for strategic portfolio analysis is the yield required by different SBUs.
For the moment assume that one may treat SBUs and divisions as identical; this assumption will be relaxed shortly.
Chapter 1 showed that the required return for any SBU depends upon the covariance of that SBU's projected returns with projected movements in the market index.
Note that the required yield should depend upon the projected covariance and that the finance literature usually uses historic data as the only available data upon which to base an assessment of the future covariance.
It was also stated that, even at the level of explaining security returns, CAPM tests using historic data have not been terribly successful.
Some well-informed practitioners are highly sceptical of the reliability of corporate betas based on historical data.
Weaver (1989) says, ‘When you plot Hershey's excess return versus the market's excess returns, our betas are not necessarily statistically significant from zero and the correlation factors are anywhere from 0.2 to 0.3…
I'm not willing to bet my career on that type of correlation’(p. 24).
It seems appropriate, therefore, to search for a way of specifying required yields which does not rely exclusively upon historic data.
When attempting to specify the market attractiveness of strategic options, the finance theorist may find it even more difficult to use his  conventional CAPM approach for determining a particular divisional beta and cost of capital using historic relationships.
The nature of the strategic movement itself may render the company's past experience of little relevance.
Of course, there may be historic data available from outside the corporate group which can be used to help formulate the required yield for a new area that the group is entering, but suppose that this is not the situation.
Suppose that the strategy involves real innovation — not only for the group but for the whole industry.
Isn't this what the core of strategic thinking is all about?
How does one arrive at the required covariance then?
An approach described by Gup and Norwood (1982) may offer the basis of the method needed, even though they were not attempting to push their analysis as far as that implied here.
It is interesting to note that Weaver's company (1989) also used a version of this method.
Gup and Norwood (1982) describe an approach for developing the divisional cost of capital used in Fuqua Industries Inc.
This method attempted to combine CAPM principles with other forms of business-risk information.
Fuqua Industries first estimated the cost of capital for the corporate group using CAPM principles and then modified it for a division by reference to fourteen key risk elements.
Each division was assessed on each of these fourteen risk elements and compared, element by element, with the corresponding assessments made for the group as a whole.
In this way a risk index was formed showing whether the division was relatively more or less risky than the corporate group as a whole, and the group CAPM-based yield was modified on a divisional basis.
Several comments are in order.
To begin with, it is clear from chapter 1 that the CAPM does not require any comparison between the divisional risks and the risk of the corporate group as a whole.
The divisional cost-of-capital risk premium depends upon the covariance between divisional returns and market-index returns.
On that score the Fuqua approach may be criticized.
Nevertheless, it is interesting to note that the risk elements used by Fuqua are very similar to some of the elements one might expect to see in a strategic analysis of market attractiveness and competitive strengths.
By incorporating such strategic factors into the cost of capital calculation for a division, Fuqua Industries might at least suggest a more theoretically sound way of assessing the required divisional rate of return.
What is required is a divisional risk premium based upon a clear specification of the key strategic factors which will determine how the projected returns for this division are likely to vary with movements in the economy as a whole.
These key strategic factors ought to be observable from a thorough analysis of market attractiveness and competitive strengths.
It will be extremely difficult to determine this statistically, and a judgemental basis for assessment of the required yield  will be necessary.
At least, however, the judgement will be based on the relevant future covariance, which seems vital where innovative strategic activity is involved.
In any case, as we saw in chapter 1, considerable judgement is involved in assessing sub-group betas even where a more conventional statistical approach is employed.
In the Fuqua case there does not appear to have been any problem of divisional interdependence, but this will not always be the case.
The problem may not, however, be as difficult as it seems.
Once major strategies have been set, they are likely to be held for some time.
Hence, one may be able to work out the broad effect of interdependencies once and for all when the strategy is set.
The required yield from the division may then be stable for long periods, apart from variations in the level of interest rates generally.
The interdependence problem may also be less severe when one reintroduces the distinction between the cost of capital for a division and the cost for an SBU.
It is the latter one really needs for strategic analysis.
Given that a very precise and reliable estimate of a required yield is unattainable anyway, the problems of both interdependence and estimation of covariances from key factors may be less severe if financial analysts work with the more homogeneous SBU as the unit of classification, rather than with a division which may contribute to various corporate business activities.
Perhaps companies should never seek divisional costs of capital but should have several different rates to apply to different businesses.
Finance theorists may be able to develop more reliable costs of capital by using the strategic concept of an SBU.
Research is also needed in this area.
It has just been argued that a thorough analysis of the market attractiveness and competitive strengths of an SBU (possibly based upon a Porter analysis), should provide more insight into the future covariability of the SBU and total market returns.
This would be essential information for a CAPM approach to estimating the cost of capital for an SBU.
It may, however, just be worth recalling the Arbitrage Pricing Theory (APT) model.
If Roll and Ross (1984) are right that there are only four basic economic forces, and that these are the four forces listed in chapter 1, then estimation of an SBU's required yield could be attempted by examining the likely sensitivity of the projected SBU returns with just these factors (inflation, aggregate production, risk premia and the term structure of interest rates).
This may be easier than using the CAPM approach.
It would, however, be hazardous to rely upon these four factors while the theory remains unproven.
The conclusion reached on the relationship between the cost of capital and strategic portfolio analysis is, therefore, that there is a need to incorporate far more rigorous financial knowledge into the assessment  of an SBU's position in respect of both market attractiveness and competitive position.
It is equally important, however, to use the basic analysis underlying the location of an SBU on the matrix in order to improve subjective assessments of required SBU yields.
Wensley (1981) says that management must decide how much effort should be exerted on CAPM compared to the strategic analysis of competitive advantage, but the position adopted here is that the two concepts are related and that effort on one ought to improve understanding of the other, even if little research on such matters has yet been performed.
Wensley also raises issues about the level of focus in resource allocation — project versus SBU analysis.
These will be addressed in chapter 8.
Diversification and capital-rationing
In addition to the matter of required rates of return for shareholders and the relevance of DCF, the earlier strategic analysis gives finance theorists a number of other points to ponder.
It will be recalled first that the theory-of-finance specification of how to maximize shareholders' wealth does not refer to any need for corporate diversification.
The shareholder takes care of his (or her) own risk-spreading needs by investing in a mix of different securities.
He will need to be informed about new strategic departures and their effects upon the covariance between corporate and market-index returns so that he can alter the portfolio held if the risk-return mix is not the one desired; but corporate diversification to manage total risk is not required by the shareholder.
It has sometimes been suggested to me that the portfolio-matrix approach is inconsistent with finance theory because it balances investment over different businesses and this is inconsistent with the finance theory view of the irrelevance of corporate diversification.
This is a puzzling argument.
The portfolio matrix could be used to focus on different risk-return combinations and to develop a corporate mixed risk-return portfolio, but there is absolutely nothing inherent in the McKinsey-GE model which says that this is what a portfolio grid is meant to achieve.
The concept of a portfolio contained in SBU analysis is quite different from that used in finance or investment theory.
It is not the corporate equivalent of an individual's investment portfolio.
The only connection between SBUs, assuming largely separate SBUs can be defined, is that the cash flows from some are used to finance investment in others.
There is no attempt in strategic portfolio grids to measure covariances of returns between different SBUs.
If corporate diversification  were relevant, it might be a good idea to try to do this, but corporate strategic portfolio analysis does not consider it.
Consequently, there is no unavoidable contradiction between strategic portfolio-grid analysis and finance theory due to corporate diversification.
There is, in contrast, a potential conflict between strategic matrices and finance theory if the former are assumed to imply a fairly rigid capital-rationing situation such that the only funds available for investment are the cash throw-offs from SBUs in the matrix.
It was argued in chapter 4, however, that there is no reason why portfolio matrices should be interpreted in that way.
A looser interpretation in terms of those SBUs to be managed for future growth, perhaps with the help of external finance, and those managed for cash surpluses now, perhaps to service debt repayment or dividends, is quite consistent with both the strategic grids and financial theory.
Nevertheless, as Wensley (1981) argues, the portfolio grids must not be used to suggest to non-financial experts that the company must be a cash recycling machine ignoring the external market as a source of funds.
On the other hand, the standard position of finance theorists that hard capital-rationing doesn't really exist, because funds can always be raised where prospects are good, needs to be softened.
It will be impractical, for example, to go to the market too often for new equity or to get the debt-equity ratio too far out of line.
Similarly, changing dividend policy to yield more cash for investment needs to be handled with care.
Thus, many companies commonly face a short-run form of ‘soft rationing’.
In such a situation it may be appropriate to think of restricting development of some SBUs — not to a specific amount of investment each year, but to a projected rate of development consistent with internal funds generated by the whole company over a period of years.
The strategic matrices may well be seen to represent this looser form of capital-rationing and balanced development, but, interpreted in this way, there seems to be no violent conflict with practical corporate finance.
Indeed, Bromiley's results (Bromiley, 1986) suggest that soft rationing and the need to manage a well-balanced programme of growth is a predominant  management concern in practice.
Weaver (1989) also provides some support for this view.
Just as companies must not overlook the market as a source of funds, so they should avoid getting locked into the idea that they must invest their own cash surpluses.
Corporate diversification may often be due to a sense of needing to use up cash surpluses, rather than risk-spreading.
Given the very patchy record of companies that have diversified into areas not related to their existing businesses or not requiring similar  management skills, corporate managers ought always to bear in mind the option of planned divestment of inevitable dinosaurs with a view to returning cash to shareholders (see Lorenz, 1988).
To repeat, portfolio matrices imply nothing to contradict this, but, if they are viewed by non-financially oriented people as models of cash-balancing having nothing to do  with the financial market, they will be used in a way that the theory of finance would judge inappropriate.
There is obviously a marked difference between the strategic and finance approaches, but they are not mutually exclusive: the one is strong where the other is weak.
Hence, there is real scope for integrating the two forms of analysis.
Corporate-strategy models are strong at specifying the cash flows by reference to key strategic factors, but weak at assessing the impact upon the valuation of the firm.
The finance approach is weak at assessing future cash flows, but says ‘give me the cash estimates and I will tell you how that affects the shareholders’ risk positions and hence the market value of the company'.
The two approaches not only can, but should, be brought together.
(Peavy, 1984, arrives at similar conclusions, though his arguments have a different emphasis.)
Market valuation and strategic analysis
Earlier in this chapter a tentative attempt was made to integrate DCF, the notion of generic strategies, and interpretations of portfolio-grid locations.
The objective was, however, to search for ways of modelling to improve the rigour of strategic analysis.
There is also a role for the finance specialists in trying to test relationships between strategic data and corporate performance.
PIMS, for example, tries to evaluate the effect of market share on ROI.
This is not the same as assessing its impact on the value of the company, although it makes some attempt to do this, as will be described shortly.
Ultimately, it is increases in the market value of shares which interests shareholders; hence financial specialists used to researching determinants of market returns might extend their work to incorporate the contents of data bases such as PIMS.
Little work has so far been done to link strategic possibilities with share valuations.
Graham (1982) supports such efforts and explores, through PIMS, the relationship between the excess of actual returns on equity over required yields and the excess of market value over book value.
He plots companies in the same industry on a cross-section basis to obtain a valuation curve like that shown in figure 6.3.
Graham's results are not very surprising at a conceptual level.
All he has done is to show, in a crude way, that, in order to increase the market value of the firm, one needs to identify opportunities where actual   returns exceed those required for the risk involved!
While financial managers address this question more directly, using the appropriate cost of capital to show how different strategies will increase the market value of the company, Graham does seem to demonstrate that the market does value returns in excess of a required yield allowing for risk.
Figure 6.3 Assessing the impact on the market value of the company
Readers are also referred back to figure 5.1 and the Bogue and Buffa V-curve.
Instead of showing operating margin on the vertical axis, the financial analyst might use the market value of the company (scaled for size) and, possibly, market share on the horizontal axis.
This would then test Porter's generic strategies against shareholder value.
None of these ideas are very complex, but the search for such relationships between share valuations and strategic data might yield fruitful results for managerial purposes.
A further step in the direction of linking market valuations with strategic concepts has been taken by Buzzell and Chussil (1985), as briefly discussed in chapter 4.
Using the PIMS data base, they measured the performance of almost 200 business units over an eight-year period.
They measured long-term performance by adding the DCF over a projected five-year period to the discounted future market value  (DFMV) of the unit at the end of that period.
The DFMV was derived using the results of the Strategic Planning Institute studies explaining corporate market-value/book-value ratios.
These ratios were found to be mainly associated with recent returns on equity, growth rates and debt/equity ratios.
Using these relationships, Buzzell and Chussil obtained data on the business-unit ROIs, growth rates and debt/equity ratios to provide notional business-unit market-value/book-value ratios.
These ratios were then applied to business-unit book values to obtain notional market values.
Hence DCF + DFMV gave a measure of actual performance.
In the PIMS book, Buzzell and Gale (1987) say that this measure of performance was highly correlated with five-year averages for accounting ROIs.
Actual performance needed, however, to be evaluated against potential performance given the units' position in the market at the beginning of the five-year period.
To obtain a measure of potential performance, Buzzell and Chussil selected ‘look-alike’ businesses from the PIMS data base, split them between ‘winners’and ‘losers’, and then assumed that the potential performance of each business in the study was mapped by a gradual increase from its initial ROI up to that achieved by the ‘winners’by the end of the period.
The results of these calculations indicated that the mean level of performance was only 38 per cent— i.e. actual performance divided by potential performance was only 0.38.
It was also found that businesses with strong initial positions outperformed those with weak initial positions.
Businesses in the decline phase of their life-cycle achieved only 30 per cent of potential performance.
The implication of all this is not only that weak businesses do worse in absolute terms, but that they do not even realize their own potential.
This may, say Buzzell and Chussil, be because businesses are using portfolio analysis too simplistically and not perceiving the remaining potential of ‘cash cows’ and ‘dogs’.
For our purposes, it does not really matter exactly what the results of the study were.
It was a heroic effort to try to evaluate business units using some notion of market valuation.
Much more research is needed in this area, and it is mostly likely to be successful if finance theorists and corporate strategists collaborate.
Summary
The main purpose of this chapter has been to indicate that there is considerable scope for both academic research and practice-oriented analysis of the possible interrelationships between financial management and strategic portfolio analysis.
Some of the ideas offered are  highly speculative; this reflects the newness of the area of study.
There does, however, seem to be some merit in attempting to bring into strategic analysis the finance analyst's tools of valuation, in order to produce a more rigorous consideration of market attractiveness, competitive strengths and the effects of movements of SBUs around the matrix.
The objective would be to demonstrate more clearly how to increase the value of the company.
This search for increased value would probably need to be subject to constraints reflecting ‘soft rationing’ and the need for balanced growth, which are already implicit within strategic portfolio analysis.
A series of SBU grids, with one for each year over the next five to ten years, might provide the basis for modelling resource uses and sources in order to plan balanced development to avoid financial distress.
Instead of showing the impact of a single additional project, the accounting figures might need analysis over different SBUs so that the impact of variations in their rates of strategic development could be explored.
Also, just as the finance analyst's tools may bring more rigour to strategic considerations, the developments in strategic thinking can help to prevent the financial analyst or modeller from applying his tools to unreliable data.
It seems there really is merit in more cross-disciplinary collaboration in this area.
7 Organizational Behaviour and Strategic Investment Decisions
The first six chapters of this book have all focused upon the financial and economic analysis underlying major investment decisions.
It has been tacitly assumed that someone, somewhere in an organization collates economic facts and integrates them through a rigorous form of evaluation, so that decisions become almost self-evident provided only that the decision-makers realize that no one can make perfect predictions and that some allowance for uncertainties is needed.
Investment decisions, at least those of significance and substance, are not made in that way within large organizations.
Many persons get involved in the process and have different parts to play.
It is therefore most important that students of accounting and economics should see their tools and techniques within the organizational context in which they will have to be used.
In general, accounting and economic literature has only begun to do this relatively recently.
Accordingly, this chapter will attempt to provide a consideration of key matters which describe the organizational context of investment decision-making and the process of managing the changes associated with major investments or strategic moves.
Multi-disciplinary perspectives of organizational life
As soon as one says that one is going to study organizational life indeed, any aspect of human life — one runs up against the problem of what lens to use to view the scene.
Pennings (1985) is one of the few who has attempted to investigate strategic decision-making taking this simple fact into account.
He did this by inviting experts in various fields to contribute economic, political, sociological and anthropological and psychological perspectives on the strategy-formulation process.
In summing up his 500-page book Pennings says,
Rational model advocates are inclined to emphasize economic-performance criteria when they pursue research on organizational strategies.
They  ignore non-economic criteria, or relegate them to a perfunctory treatment of ‘corporate social responsibility’…
The…economic-orientated literature has formed an imagery of the firm…that is an organizational caricature.
The profound neglect of psychological, social and political considerations renders such treatments shallow and incomplete.
With such an indictment and such a challenge in terms of the variety of disciplinary insights needed, the reader may well wonder at this stage whether one brief chapter can even begin to make any impact on this question.
In terms of starkly original insights, the answer is certainly ‘no’, but I hope that the critical review of non-accounting, non-economic literature offered here will help those involved in finance and accounting to gain an overview of theories related to investment and strategic decisions in organizational life without extensive study of all the disciplines mentioned.
Of course, many readers will already have studied some aspects of these other disciplines as well as accounting and finance.
This chapter may stimulate them to contribute to the effort of welding together insights from different disciplines, rather than, as the majority of students seem to do, keeping their finance lenses firmly in place when reading finance texts and then changing to sociological lenses when reading sociology.
Perhaps that is the fault of the examination system.
As indicated later in this chapter, a strong vehicle for changing behaviour is a change in the reward system.
Organizational structures and roles
Various writers in the late 1950s and 1960s began to move understanding of the investment process in companies away from the strict neoclassical economic perspective.
March and Simon (1958) emphasized that both organizational pressures and lack of information characterize decision-making under uncertainty in organizations.
Dean (1951) stressed the difference between incremental investments and major additions to the corporate stock of assets.
Chandler (1965) demonstrated the link between product diversity and the degree of integration in the organization structure.
All of these provided the basis for questioning what actually goes on in resource-allocation processes, but a major step forward came with the work of Bower (1970) and Ackerman (1970).
These studies independently suggested that, irrespective of whether a company is more or less integrated in structure, different levels of the organization have distinct roles in investment decision-making.
(Investments  here, as throughout this book, are taken to be significant investments and not just minor replacements of facilities.)
Bower and Ackerman's findings may be summarized in the form of figure 7.1, which shows that most investment proposals of any scale or significance are identified at the level of the product line or department.
If the divisional  management is convinced, it then takes on the duty of providing impetus and promoting the project through the corporation.
Final approval is, however, reserved for decision-making at group headquarters.
To repeat: the discussion is about investments which the group sees to be significant.
All major companies have rules which permit investments below a certain scale to be undertaken at divisional level without head-office approval.
The discussion is also about general patterns of behaviour.
Obviously some major initiatives (e.g. mergers) can come from board level.
Ackerman found that, where companies were more integrated in structure, the project was defined to some extent at divisional level and promoted to some degree at head-office level, but the general pattern   was the progression through the organization indicated in figure 7.1.
Projects were evaluated in detail at divisional level.
Head office reviewed those proposals on the basis of financial criteria, but also looked for ‘strategic fit’.
An interesting subsidiary finding was the head-office attitudes to risk assessment described by one interviewee:
If we're familiar with a project…we can rely more heavily on our interpretation of the division's analysis to evaluate it.
However, if we're dealing with an unfamiliar area…we'll examine the assumptions and market studies more closely while the analysis is being done.
(Ackerman, 1970, p. 346)
Figure 7.1 Who does what in the investment process
This suggests that, instead of a once-and-for-all decision at the end of the analysis, there may be a process of evaluation that develops with the project, and that decisions and analyses are not undertaken without consideration of prior experience.
Ackerman also found that in integrated firms there was a great deal of divisional-HQ interaction over significant investments, often extending over several years, and that this was a powerful device for gaining commitment to major projects, although the impetus any project gained was also a function of fairly subjective, often political factors.
The degree of divisional-HQ interaction was not so extensive for diversified firms.
Later research to be considered in chapter 9 extends these basic ideas.
The studies by Bower and Ackerman demonstrated that there is a human-interaction process in the decision to make key investments.
Certainly there is a need to present a case, lobby for support, orchestrate a debate, negotiate or prevent negotiation, convince other parties and implement successfully.
The management of resource allocation involves giving attention to all these matters and how they affect roles at different hierarchical levels.
Moreover, the responsibility to implement will itself affect the earlier stages of project promotion and analysis through the way those due to implement the project expect to be rewarded or punished.
The justification for this chapter now becomes very clear.
Studies addressing how organizations behave are just as important for accountants and financial managers as technical analysis.
Beyond simple forms of rational analysis
To date, the study of strategy formulation, and by implication its relation to investment processes, has been conducted within different paradigms.
In recent years this has become more widely recognized, with various authors identifying the fundamentally different sets of values upon which different studies have been based.
A prerequisite for integrating different approaches to a problem must be the recognition that different approaches exist, and classification of them according to their basic elements in order to focus upon contradictory factors.
Chaffee (1985), for example, identifies three models of the strategy construct: linear analysis (essentially a linear deductive process aimed at achieving specific managerial objectives), adaptive analysis (responding to environmental objectives, and providing the appropriate organizational match) and interpretative analysis (conceptualizing what change means to different individuals; legitimating developments; developing symbols to encourage action and taking action to create symbols of desired behaviour; improving relationships).
Chaffee allocates a range of key authors writing on strategy into these three categories, but she argues that the categories need not be seen as separate.
The interpretative can be viewed as the broadest perception, into which the adaptive and linear models can be fitted.
Her paper is, however, of value more as a codification of different processes of research.
It was not intended to show how they might be integrated in practice.
Others, such as Mintzberg (1973), Gluck et al.(1982), and Venkatraman and Camillus (1984), also provide classifications, and these alternative perspectives are now creeping into student textbooks; see, for example, Quinn, Mintzberg and James (1988).
There is also some preliminary wrestling with the problem of integrating different disciplinary perspectives in Pennings (1985).
At present, however, we are still essentially at the stage of identifying different approaches to the consideration of strategy.
For the purposes of this book, however, and bearing in mind the limitations of space, no attempt will be made to integrate these different classifications.
In order to provide a practical precis a different system of classification will be used, but the reader wishing to explore the material presented here in more depth can rapidly extend his or her reading list by reference to the codifying articles already mentioned.
In this chapter a contrast of perspective to the rational models of chapter 3 is needed.
‘Rational’— a word that can take on various meanings and get one into all sorts of trouble — is here being used to describe those models that ignore organizational behaviour and suggest that the decision-maker has near-perfect knowledge, power and insight, needing merely to conduct the necessary analysis and implement it.
We shall now explore three alternatives to the rational model: logical incrementalism, the interpretative paradigm and action rationality.
This three-way classification is my own and is adopted mainly for convenience of exposition.
As will be clear by the end of the chapter, the three categories have overlapping strands and represent related philosophies.
Logical incrementalism
Various authors have contributed to this view of strategic decision making, but the leader of them all is Quinn (1978, 1980).
Quinn's line of thought can be traced back to earlier works such as those of Lindblom (1959) and Wrapp (1967), but he took these general ideas and turned them into a framework for observing organization behaviour and then into practical recommendations for the chief executive who is responsible for strategic change.
Lindblom was mainly interested in the problems of national governments and explaining why it was not appropriate to adopt overall planning models at that level.
Wrapp, in contrast, writes from the perspective of the chief executive trying to manage both the organization and his own task.
He stresses the need for the chief executive to be informed, but argues that he should focus his attention on just three or four major objectives.
He also emphasizes that the chief executive should take into account the power structure of the organization, be careful to choose the right time to promote initiatives, and, in particular, avoid committing himself publicly to a specific objective or action until he knows that it is definitely what he wants and that he can get the support.
Management by objectives, says Wrapp, may work at lower levels of the organization where tasks and accountabilities are well defined, but they don't work at the top.
Policies emerge in the organization over time.
The chief executive may or may not have worked them out in detail beforehand.
Quinn built on this basic theme in a detailed case study (1980) of nine large companies, most with headquarters in the United States, but including also a British company and a Swedish company.
As a result of this work, Quinn argues that, while formal (i.e. rational, in the sense defined above) planning systems serve useful purposes, they focus unduly upon measurable quantities and underemphasize ‘the vital qualitative organizational and power-behavioral factors that so often determine strategic success’(p. 15).
He stresses that formal planning can only be one of the many building blocks which determine corporate strategy.
Quinn sees large companies as similar to large rivers slowly moving in given directions, but containing within them various ebbs, flows and eddies which, while they do not necessarily contribute in any direct analytical way to the general direction, nevertheless in aggregate help to  determine it.
Moreover, the flow has direction, but no obvious beginning or end.
In a company this is interpreted as the absence of a clear separation between strategy and implementation.
There is, according to Quinn, no single moment (or relatively short period) in which all the different aspects of the company are considered together and then each part given precise goals for the  foreseeable future according to some tight, analytical, holistic master plan.
Given the size of large corporations, this is impossible, owing not only to the cognitive limits of human beings, which can tolerate only a severely bounded form of rationality (March and Simon, 1958, ch. 7), but also to ‘process limits’.
Process limits reflect the time needed to create awareness of threats and opportunities, build commitment and consensus, select and train people for change, marshal resources, and so on.
However, the internal and external environments of the organization do not stand still while such processes are worked through.
Events, often severe shocks, continue to occur, and may cause minor or major changes of direction, provided that no final and irrevocable decision has yet been made.
There is, therefore, an interaction between events, organizational actions and decisions.
Strategic decisions emerge from those on-going processes and are not formulated by corporate masterplans or mastermatrices.
If Quinn is right, how is such a process to be managed?
First, he stresses that one should not view strategic decisions as emerging randomly.
Also, they are not just the result of powerplay within the organization.
Power relationships affect outcomes, but so does rational analysis.
What is critical to Quinn is the form of rationality and the process through which it is formulated.
Quinn's expert manager will be one who recognizes that he has to guide the organization rather than lead it too definitively.
He will recognize that there are large separable strategic subsystems.
These are not just strategic business units (SBUs), but subsystems relating to technological development, financial resources, market postures, product-line development, mergers and acquisitions, employee relations, government relations, and so on.
All of these have strategic aspects to consider, and strategic developments in these areas will not occur all at the same pace or even, necessarily, in the same direction.
As strategic proposals emerge, each subsystem will evaluate them against developments in other subsystems, in so far as it perceives them to affect it too.
In such a process the smart executive does not reveal a strategic solution (even if he knows one) as soon as he perceives it.
He implants ideas, gives clues, prompts proposals, and avoids committing himself publicly until the last moment.
This gives the organization time to absorb the idea, to build consensus and to reduce resistance.
Also the  smart executive is aware, especially in the early stages of examining a strategic move, that there may be perspectives of which he is unaware.
By asking for proposals from subordinates, he is able to test his position while being free to reject those proposals without political consequences.
This need not suggest a Machiavellian strategy; it may just be sensible caution in the face of uncertainty about both the external and the internal corporate assessment.
On the other hand, the approach would also be quite consistent with a concealed move to defeat a rival faction.
Occasionally, severe shocks will rock the system and urgent action will be needed.
Quinn has two answers to that.
First, if the action must be taken very urgently, it cannot be taken as part of a full corporate strategic analysis; there would not be the time.
Nevertheless, actions taken under such stress are often irreversible and set new bounds for strategy in future.
Secondly, we should not overestimate the frequency with which really urgent strategic decisions are required.
Even those countries that formed the oil cartel in the mid-1970s were very careful, says Quinn, about enforcing posted price increases until they were sure that the cartel would hold.
So Quinn gives us a picture of top executives developing mental, or at least private, images of corporate development and, in effect, testing out their ideas through experiments in separable parts of the company.
They set broad direction while allowing their views to be influenced by the experience so gained.
Such a process is not just ‘muddling along’, although it might appear to be so.
It has the broad direction (as the flowing river does), but it avoids being over-specific.
Moreover, the direction is set by powerful clues or ‘logics’ gradually established in the organization which enable subsystems to bound their search for strategies.
The process is selective in addressing issues, but sees each issue within a wider context without, as a rule, seeking a totally holistic view.
When each main idea has been sufficiently tested and sufficient knowledge has been gained, the chief executive goes public and announces the decision secure in the knowledge that there will be support for it, because the decision merely ‘crystallizes’ the views already put to him, whether he subtly induced them or not.
It is important not to read Quinn as calling for the destruction of formal corporate planning or rational analysis.
Quinn's fundamental point is simply that it is foolish to try to produce a total group-wide analysis at a given time and then to go ahead rapidly implementing that, ignoring the changing external and internal environments.
Rational economic and financial analysis will be vital in presenting proposals, examining relationships between strategic subsystems, and so on.
Also, there are clear benefits from bringing proposed actions together in a formal strategic analysis and long-term financial plan.
They set down  clearly a record of decisions made.
They provide a basis for evaluating shorter-term budgets and prevent long-term aims from being forced aside by short-term operational needs.
They bring together a range of information not otherwise collected.
They encourage managers to think ahead.
They reduce uncertainty and hence anxiety about the future.
They stimulate ideas for possible improvements.
They assist in implementation by assigning responsibilities.
None of these benefits is trivial in managing a corporation.
But, whatever they do, says Quinn, they do not determine strategies.
They stand as the essential interface between strategy formulation and tactical decisions.
Before leaving the topic of logical incrementalism, mention needs to be made of a few other relevant studies.
Pascale (1984) provides an illuminating analysis of Honda's success in penetrating Western motorcycle markets.
After focusing on the Boston Consulting Group (BCG) analysis of the situation, which emphasized the importance of building scale and lower costs through experience-curve effects, Pascale visited Japan to get the story of how the Honda executives planned their entry into American markets.
The story is a fascinating description of relatively uninformed risk-taking and opportunism coupled with technological advantage which was only seen as marketable in the United States after the products initially offered in the US market had failed.
The story exhibits just the type of learning and experimentation that one would expect from incrementalism, except that, in the Honda case, the experiments were being conducted for real, not just in the executives' minds prior to decision, and the process did not seem all that logical.
But it succeeded — largely through a process of persistence and perception of opportunity as it unfolded.
Pascale says this indicates the distrust in which the Japanese hold single strategic plans, which in their view limit the peripheral vision which is so essential for observing environmental changes.
At the very least, this case supports Quinn's stance on incrementalism.
Quinn has been interpreted by some as suggesting that his form of incrementalism leads to a steady emergence of change.
While this seems to be reading more into Quinn's analysis than is justified, work by Mintzberg (1978), Miller and Friesen (1982), and Pettigrew (1985a, 1985b), supports the notion of incrementalism, but in a form of uneven development with long-term gradual change punctuated by bursts of ‘revolutionary’ change which may be due to the economic environment, management change or other factors usually, but not necessarily, associated with commercial crisis.
At first sight this seems to contradict incrementalism, but incrementalism remains a valid concept if the process of change in these ‘revolutionary’ bursts does not embrace the group-wide rational analysis.
The  interpretative paradigm
The phrase ‘interpretative paradigm’ is used here as a shorthand description of newer theories of organization that first began to emerge in the early 1970s.
These are typified by studies such as those of Cohen, March and Olsen (1972); Hall (1973); Cohen and March (1974); March and Olsen (1976); Weick (1976); Meyer and Rowan (1977); Starbuck (1982); Gioia (1986); and Lord and Foti (1986).
The position in the mid-1980s is very economically reviewed and analysed in Weick (1985).
All these theories depend upon the recognition that organizations consist of people and that without the people the organization does not exist.
Consequently, we can only understand behaviour in organizations, and, hence, how to manage them, by understanding human cognitive processes and how they influence human behaviour.
Cohen, March and Olsen (1972) provided a stark contrast to the planning-oriented literature on organization design and decision making with their ‘garbage-can model’, which in essence claims that people and systems in organizations were in possession of solutions to problems and predispositions to take certain actions when problems or specific situations arise.
Organizational direction is therefore primarily determined by problems and reaction rather than pro-active planning.
Later organizational theorists (for example, Starbuck, 1982) argue that the ‘garbage-can’ view of the world is too extreme in its implications that organizations have no controlled order at all .
There will be anarchical aspects of organizations, but order is not completely absent.
There will be severe limitations on centralized co-ordination, but there will be ‘loose coupling’— that is, some co-ordination, but not in too tight a form.
Large organizations will consist of loosely coupled subsystems, and yet, within the subsystems, one would expect to find tighter coordination.
Even so, the early work of Cohen, March and Olsen was perceptive and catalytic to later developments in ways beyond the general description of the ‘garbage-can’ model.
Irrespective of how much order is or is not present in organizations, Cohen, March and Olsen recognized that people have experience and formulate standard responses to situations.
Some of the more recent work on organization behaviour (e.g. Brunnson, 1982, 1985; Gioia, 1986; and Lord and Foti, 1986) reflects this notion of learned response based on prior knowledge, even if it is then referred to as a basis of order rather than disorder.
Doubts about the importance of formal rational planning in organizations grew during the 1970s and 1980s.
For example, Starbuck (1982) states that rarely is there a neat sequence of events through from  recognition of a problem to definition of the problem, generating possible actions, selection of the appropriate option and implementation.
Approximations to such a process might occur if the environment changes slowly with high predictability and where tight control can be exercised centrally.
In most large organizations, however, where tight coupling is impossible and no one stakeholder interest predominates, this will not be the situation faced.
Moreover, the greater turbulence in the economic and political environment through the 1970s and 1980s means that predictability has decreased.
In such a situation, action is likely to be the product of internal negotiation, with variable dependence upon rational analysis, and one might expect the rational analysis undertaken to have a different orientation according to the stakeholder for whom it is performed (Hall, 1973).
Indeed, some stakeholders may take action to prevent a rational analysis of their stance or at least any debate over it.
Decisions then emerge in ways more complex than that posited by Quinn (1978, 1980) and yet there has to be a justification of major decisions made both inside and outside the organization.
It is thus not surprising that internal decision-making is loosely coupled with external reporting (Meyer and Rowan, 1977).
Not only are the external stakeholders different parties, needing a different rational analysis to justify the decision, but it seems important to convince that group of stakeholders that managers have taken their interests fully into account in the internal negotiation process.
The managers' continued legitimation may depend upon convincing the stakeholders that they have done so.
Starbuck (1982) extends this mode of thought into internal management.
Organizational members need to have a form of rationality that suits them to justify their continued co-operation and action.
In the view of such writers, therefore, rational analyses often serve as post hoc devices to justify action rather than accurately reflecting how the decision was actually arrived at.
Moreover, it needs to be recognized that a rational, ‘value-free’ analysis cannot exist in a pluralistic society or that it would please no one if attempted (Clapham, 1984), unless an action could be shown to benefit all stakeholders.
Even then, some may gain more than others, and this could lead to other options being sought which do not produce gain for everyone.
The sort of argument just presented emphasizes that large organizations are not monolithic and that attempts to tighten up the system to make them so do not necessarily yield improvements.
Weick (1985) says that, because people persist in simplifying the world, they do not see the differentiations between organizations and within organizations over time.
This leads them to see the organization as a well-defined unit and  not as the heaving, changing mass with fluctuating boundaries that it really is.
People also try to create an ordered and rational existence for themselves.
This leads to assumptions about what appeared to happen, did happen and was, usually, intended to happen.
There is a natural predisposition to see events as the result of pro-active planning, but, as stressed by March and Olsen (1976), intentions are often overwhelmed by exogenous factors and events or resisted through existing loyalties and perceived duties.
Traditional corporate-planning theorists are said to ignore these facts, concluding that rational models lead to effective performance and that highly segmented organizations are really tightly coupled systems (Weick, 1976, 1985).
Using hindsight they ignore ‘the experiments, the false starts and corrections that enabled people to learn and prepare’(1985, p. 114).
Rational analytic systems do not build in opportunities to learn.
They assume too much smoothness in the process of unfolding activity based upon the plan.
An overall monolithic plan established at one time is therefore impossible.
Weick (1985) also argues that yet other factors show the impossibility of organizational unity.
Top managers in large organizations do not manage organizational activity; they manage decision structures.
They need those decision structures because it is impossible for them to know enough to control activities directly.
They are, therefore, never in full control — they cannot be.
All they can do is set trend targets and reward or penalize according to the degree to which they are met.
They cannot control the detailed processes.
If interdependencies exist between corporate segments, top managers can only see that there are no major actions by one segment which adversely affect the others.
They cannot totally control the process of interaction.
To summarize, ‘A loosely coupled system is not a flawed system.
It is a social and cognitive solution to constant environmental change, to the impossibility of knowing another mind, and to the limited information-processing capabilities’(Weick, 1985, p. 121).
In such segmented organizations with limited overall control it seems inevitable that there will be variable strengths in the links between the segments.
This will create a certain amount of ambiguity for everyone attempting to exert control.
On the other hand, while ambiguity can never be eradicated, managers can take steps to reduce it (Cohen and March, 1974; Peters and Waterman, 1982).
Managers can act as if the elements of a decision process or an organization are tightly coupled.
They can do this by establishing clear guiding principles against which all actions need to be evaluated.
Examples of such core values might be that customers are always right, that we need 100 per cent product reliability, that we are a computer  company and will stick with it, and so on.
It is then taken without question by organization participants that observance of these core values leads to success for themselves and ‘the organization’.
Even if the core values are not tightly linked to organizational success, they still guide action and promote a more unified view of ‘reality’.
Consequently, the arguments presented in this section should not be used to dismiss centralized corporate planning.
The plan itself does serve as a binding mechanism to reduce ambiguity, but not too much should be expected of it.
It is a coming-together of intentions to create visibility for key strategies and organizational movement.
It is a co-ordinating device and a legitimator of subsequent action, but, according to this area of literature, the development of the rational corporate plan is not a dominant decision-making process.
Peters and Waterman (1982) also argue that corporate plans are insufficient for establishing such core values.
The underlying values by which organizations are to be run have to be strongly set through a variety of management processes — in particular, through reinforcement by top management and the reward system.
The corporate planning process is only one, and perhaps not the best, way of establishing those values throughout the organization.
A little deeper thinking raises the question of why organization participants should accept such core values.
As stated, top management can reinforce acceptance through their actions and by biasing the reward system to favour behaviour in accordance with the core values, but will not organization participants seek a form of rational justification for compliance in terms of their own long-run success and survival?
The answer is clearly ‘yes’.
Top management will have to provide a convincing case, but there is, nevertheless, considerable evidence that human cognitive processes provide a natural basis for expecting behaviour according to core values once they have been established, and this fact can be a fundamental  management aid.
Gioia (1986) stresses that people are not pure information-processors rationally dealing with all available information in an information-rich environment.
They also do not process it anew each time it is presented.
If problems arise in everyday life, they may begin by some rational search and analysis, but they are looking for aspects of the situation that reflect prior experience (that are stereotypical) and, once they recognize those aspects, they switch to a more holistic and intuitive assessment based on their understanding of those stereotyped elements.
Also, the stereotyped aspects we recognize in a problem situation depend upon how we look, which in turn is based upon our personal belief and value systems.
Finally, our belief systems are not stored in our minds as a complete rational analysis.
They may (or may not) have been created  that way, but, once they have been determined, we store our beliefs more efficiently in the form of symbols (shorthand words and phrases which reflect a complete attitude to and assessment of a person or situation — e.g. ‘She is a poor performer’) and scripts (models of how we act once certain situations are recognized).
There is a necessity for us to behave like this in order to cope with the informational complexity with which we are perpetually faced — we ‘know’ something works so we do it, bracketing off the rest of the world until it becomes clear that there is a need to change our basic beliefs.
In the fashion of the philosophers John Locke and David Hume one may say that one cannot live as a total sceptic or one would achieve nothing.
It is also important to recognize that our basic beliefs (our espoused version of the ‘truth’) are not necessarily established by rational analysis.
Core values, or belief systems, can be created either by rational deductive analysis or by direct association between action and result through experience.
Whichever basis or mix of bases leads to the establishment of those beliefs, they become embedded in the individual and organizational consciousness as ‘pre-existing’ knowledge systems which are used to interpret events as they occur.
In fact some (e.g. Lord and Foti, 1986) like to push this analysis further and find value in analysing these schemas (i.e. the knowledge systems) into various categories to reflect our perception of other individuals, of ourselves, of situations and of how specific people will act in given situations.
In summary, our understanding of basic cognitive processes suggests that top managers can have a powerful effect on organization behaviour and direction by reducing the variety in subordinates' schemas (core values).
This can be done either through rational analysis or through changing experience.
Two further points regarding the ways managers cope with ambiguity will be of relevance to our consideration of strategic management.
First, Weick (1985) reminds us how high ambiguity can be reduced by the establishment of networks.
This can be interpreted in two ways.
Weick interprets it as really a broader application of the basic notion that diversification spreads risks.
He quotes the example of diversifying sources of supply.
But the basic concept can also be applied to the establishment of networks within large organizations.
While the breakdown of a corporate group into separate SBUs will improve diversification of the total group risk, if there is some interdependence between the SBUs it may be better to have many smaller interdependencies rather than a few large ones.
For example, multi-sourcing within the corporate group offers less need for co-ordination than single sourcing with the group.
It is clear that one cannot give a general rule.
Much depends upon the nature of the interdependencies, how critical they are  to the success of SBUs and, in turn, how important those SBUs are to total group success.
However, it is interesting to note that some wider forms of inter-group dependence can, paradoxically, act as a diversification mechanism to reduce organizational ambiguity, and may actually lessen the need for overall planning to ensure that specific interdependencies are always tightly managed.
This idea will be picked up again in chapter 8 when we review the corporate-strategy literature from the viewpoint of organization theory.
The second point is that, when ambiguity increases sharply, that is the time when people who can resolve that ambiguity by setting new organizational values to cope with it gain power.
There is nothing like a good crisis to lead to a questioning of organizational values, directions and practices (Starbuck, 1982; Blowers, 1983).
Crises can therefore be used to legitimize more strategic movement.
They do this by acting on the experience mode of establishing truth, and, given the uncertainty associated with any economic analysis relating to a company's future, experience of a crisis is probably far better at getting us to reassess our schemas and scripts than is rational analysis in times of stability and success.
Of course, once our schemas are shaken, we may well resort, at least in part , to rational analysis to reformulate them.
Action rationality: the gem in the garbage can?
A direct derivative of the type of thinking traced above is the concept of action rationality, analysed most comprehensively by Brunnson (1982, 1985).
Recognizing that people do act according to established beliefs and pre-existing knowledge systems, he focuses upon distinguishing between the establishment of those beliefs (which he calls ‘ideologies’) and action in accordance with those beliefs.
Brunnson emphasizes that it is the end result of taking action which matters and not just the analysis of what action to take.
Successful implementation of decisions, he says, depends on three factors: good motivation, high commitment to the action selected, and common expectations of what is required.
On the basis of studies of various enterprises, he then adopts a stance which radically de-emphasizes the degree of both rationality and negotiation needed in successful organizations.
Under considerable uncertainty, extensive analysis of many options will only highlight the fact that much uncertainty exists and, he argues, create more uncertainty in the minds of organization participants, which will reduce commitment to action.
Decisions, then, should be made on a much more impressionistic basis (intuition mixed with analysis of a few alternatives), provided that the actions taken do not offend the organization's ideologies.
In Brunnson's view, effective ideologies should have three attributes.
They should be conclusive — as an example of a conclusive ideology he cites a ‘multi-dimensional model of the market’ with ‘elements which were strongly inter-connected in terms of cause and effect’(1985, p. 92).
They should be consistent — that is, generally held throughout the organization.
Finally, they should be complex — meaning that they should be definite, with a full analytical description of why the particular ideology is appropriate, even specifying conditions under which it would cease to be appropriate.
A conclusive, consistent and complex ideology provides the basis for both a reduction of ambiguity in the organization while the ideology remains appropriate and the destruction of that ideology when necessary.
While the ideology is appropriate, it enables choices to be made on the basis of far less analysis and with much greater urgency, which certainly seems to be a move in the right direction, even if the choice made is not always the best one in the circumstances.
Ideologies of this sort both prevent procrastination through excessive thought and analysis and reduce uncertainty about the right steps to take, thus increasing commitment and motivation.
They imply also that the organization is prepared to make more mistakes, but to carry the consequences in view of the greater benefits to the organization as a whole from being action-oriented.
Brunnson clearly follows the line of those who question the need for a totally comprehensive, rational planning system, but his management methodology does contain a considerable element of rational analysis, despite the title of his book.
A strong ideology is, in his view, a ‘multi-dimensional model’ with strong cause-and-effect links.
To Brunnson, therefore, the establishment of ideologies (or value systems or fundamental strategies) does depend on rigorous, rational analysis.
In fact, he may himself be relying too much on rational analysis in setting ideologies.
As argued elsewhere (Tomkins, 1987), both Mrs Thatcher in moving to a more competitive economy in the United Kingdom and President Kennedy in declaring that the United States would put a man on the moon established very strong ideologies based more on fundamental beliefs as to what was required than on extensive rational analysis, and they both achieved considerable change.
Brunnson therefore does an excellent job of distinguishing between the formulation and implementation of ideologies, helping us to begin to see how corporate rational analysis and the organizational perspectives may fit together, but his thesis does not sufficiently recognize the degree to which both rationality and negotiation may still underlie the establishment of his ideologies.
He seems to sweep aside rational analysis only to let it come back in at the ideological level, and underplays the processes of getting the ideology established in the face  of organizational resistance.
If we are to adopt an action-rationality stance, and there is considerable merit in so doing, there is still the need to decide at what level ideologies are to be set, what degree of rational analysis needs to be involved, how to sell the ideologies and how much rationality is still needed even within an impressionistic mode of operation.
As Weick (1985) says, action rationality contains a dilemma for top managers.
Do they conduct more extensive analysis and perhaps provide better solutions but risk dissipating energy to act, or do they focus on forceful implementation of a satisfactory solution?
Even Weick doesn't make it clear that this dilemma is not found only at the action level.
It is an even bigger dilemma at the ideology-setting level, which relates more to strategic movement.
Despite all this criticism, however, Brunnson does point the way towards a system of management which incorporates the implications of cognitive schemes and ideologies.
He is also much more positive than many earlier organization theorists, who emphasized the complexities of individual behaviour in organizations but did little to help top managers except to say their task was very difficult and that they might do better if they understood that.
Discovering and changing widely held schemas
In discussing the interpretative paradigm and action rationality, we have seen the importance that a range of authors attach to identifying the commonly held cognitive schemas in an organization.
Realizing the importance of schemas and scripts is the first step, but that in itself gets us nowhere unless we push on to consider how a manager can discover what schemas actually exist and, if they are inappropriate, how to change them.
Only then does schema recognition become a practical  management device.
Very little to date seems to have been written about schema identification.
Perhaps that is appropriate.
If schemas are widely held, it should usually be obvious to the manager what they are — especially, of course, if he or she aimed to establish them in the first place.
On the other hand, in large organizations it may not be so obvious what schemas people at different locations hold.
There may be critical differences in schemas between different divisions or hierarchical levels which create conflict or resistance to change.
Top managers may well not understand this resistance because they do not recognize the schemas.
This is not a rare phenomenon.
A colleague and I are currently involved in considering major changes in a very large organization.
It is evident that change has not been achieved as rapidly as desired, and to a large extent it is for this reason.
Indeed, we were hired to help identify the schemas held at  different levels of the organization.
There is a need, therefore, for a systematic approach to identifying existing schemas before attempting to establish new ones.
Schema identification may only require some sample interviewing of different internal stakeholder groups.
When they are questioned on the attitudes, motivations and beliefs they bring to their tasks, strongly held schemas will often become obvious quite quickly to the trained observer who knows what he (or she) is looking for.
It will be important, however, to consider how the organization participants see the interviewer.
If the interviewer is closely identified with some head-office group that is known to be contemplating a change that organization participants consider undesirable, he will need considerable skill to tell just how strongly particular schemas are held.
It is possible that schemas will be presented in a way which overemphasizes participants' intentions to resist.
Alternatively, an apparent readiness to change may conceal an intention to resist in concerted action with others.
Depending upon the issue and situation involved, it may be useful to develop a more systematic procedure for schema identification.
In a book (Tomkins, 1987) urging public-sector organizations to consider new ways of evaluating organization effectiveness (not just efficiency), I suggest that the approach of Guba and Lincoln (1981) offers a very practical way of proceeding.
Guba and Lincoln base their approach upon the notion that, where stakeholder groups differ in their assessment of the effectiveness of their organizations, this is because of underlying differences in their value systems, which influence their assessment of the importance of different mixes of organizational outputs.
A simple methodology has been developed which identifies key stakeholder groups and their concerns about the organization.
By comparing concerns of different stakeholder groups, those differences in concerns are divided between those due to misinformation or poor communication and those which are real issues based on differences in value systems.
There is no claim that this resolves the differences, but it helps top managers understand where they might focus their efforts in order to improve effectiveness — i.e. it identifies inconsistent schemas.
Clearly such an approach is applicable in any organization.
The stakeholder-analysis approach to reviewing strategy developed by Mitroff and Emshoff (1979) has many similarities to the Guba and Lincoln approach.
The prime difference is that Mitroff and Emshoff pursue the underlying values each stakeholder group holds with respect to specific strategies.
They also say that they have discovered that this tends to produce two dramatically opposed views.
For Quinn-style top managers trying to assess current schemas and what steps to take, the less directive Guba and Lincoln approach may raise fewer expectations  and prevent a premature confrontation.
The Mitroff and Emshoff approach may be more suitable for reviewing strategies already proposed where there is clearly major resistance to those strategies.
Nevertheless, both approaches offer ways of thinking about the values guiding organization participants' behaviour.
Finney and Mitroff (1986) develop the earlier Mitroff work further with an approach they call Organizational Self-Reflection (OSR).
In their view, corporate strategies fail because they consider problems in the external environment but not those internal to the organization.
Compared with Mitroff's earlier paper, there is much more emphasis here on linking stakeholder analysis with research into cognitive schemas, and a greater recognition that there may be problems both in identifying the schemas, often held unconsciously, and in bringing them together to inform the corporate debate about strategy.
The paper devotes more attention to establishing basic ideologies than to the problems of how they relate to specific strategies.
The approach was developed with the co-operation of a specially formed group of managers, rather than through a company-wide sampling procedure.
The result, again, was a recognition of cognitive dissonance between internal stakeholder groups.
Of course, the recognition of cognitive dissonance still does not solve the problem.
Top managers still have to decide what to do about it.
Given that espoused schemas are inconsistent with the ones top managers would like to establish, how do they go about changing them?
There is a great deal of literature on the process of organizational change, but a reasonable degree of agreement on the basic stages involved.
Kanter (1983) is probably as well regarded as any writer on the subject.
Like Peters and Waterman (1982), she emphasizes the need for top executives to establish a few clear signals in order to create a felt need for change, and to use ‘prime movers’ in the organization both to ‘talk up’the new strategy and to ensure that the signals or symbols developed to establish the new direction are present in all important interactions — i.e. reflected in reports, agenda for meetings, key events, and so on.
She also emphasizes the need to consider the ‘action vehicles’ which can be used to carry the change.
This could be modification to reward systems, management development or training schemes.
This can be so easily set down; indeed, written at such a general level as they are here, these statements become almost obvious.
The importance of considering how, in specific situations, to change organizational schemas should not, however, be underestimated.
While organization theorists have provided a considerable service by focusing our attention upon the notions of ideologies and schemas, once this has been done it is relatively easy to identify what the schemas are.
The really difficult task  is changing them.
Glib statements about the use of prime movers and changing the reward system conceal the considerable difficulties often involved in doing just that.
Figure 7.2 is an attempt to capture the complexity involved.
When some thought of major change is initiated, all those affected will have their own interpretations of what is going on, based on their existing knowledge and experience.
These perceptions will then be evaluated through a consideration of ‘what's in it for me?’
This may be expected to lead to a purposeful action, purposeful inaction or indifference.
Dependent on the diversity of response, the decision on the proposed change may be resolved or persuasion in some form may be required.
In any case, the whole process will be modified by the ability of each group to wield power and influence.
It is clear also that there can be an iteration back from any stage in the process to any previous one as views get modified during the change   process.
Even if the initial reaction to proposed change is generally positive, rethinking can occur during the change as consequences become clarified.
It may take some time before matters settle down again and action proceeds quasi-automatically according to revised schemas.
Figure 7.2 Change as a social process
If the change is not well managed throughout this process, different groups' interests may be so radically affected that the process has to degenerate into chaos before stability can be regained.
I am reminded here of the book on chaos already mentioned in the Preface (Gleick, 1987): it describes how turbulence can be created from a stable physical state.
Without trying to push the scientific analogy too far, it is interesting to consider whether there are parallel patterns of development, giving early warnings of ‘chaos’, in organizations.
If so, either the parties most affected need attention, possibly through a change to the proposed reward patterns, or the pace of change may need to be slowed to give those most affected time to adapt.
Alternatively, and conversely, it may be recognized that the end goal will not be achieved without a period of turbulence, and the pace of change may actually be accelerated, to get it over as quickly as possible.
In this way change is seen as a developing social process the repercussions of which cannot be completely forecast.
On the other hand, the better the change-manager understands each main party's schema, each party's perception of what is risky for it and each party's ability to exercise power to disrupt, the better will he or she be able to develop an appropriate change strategy and recognize where the strategy may be at risk as events unfold.
Where some parties gain and lose, as will usually be the case in schema-breaking situations, change can be a complex process.
As noted earlier in this chapter, and by many authors, including Kanter (1983), an attempt to change basic ideologies/schemas of multiple parties is more likely to be successful in times of crisis when it becomes obvious that traditional modes of operation are no longer achieving success.
If there is no crisis, top managers will find it much more difficult to achieve fundamental change, since the need for it will be questioned.
There are then just two ways in which top managers can proceed.
One of these is to attempt to convince the organization that a crisis will occur if current policies are continued.
The trouble with that is that others may not be astute as they are in identifying impending crises, and will take some convincing if a crisis is not obvious.
It is apparent from the writings of people such as Quinn and cognitive psychologists that schemas are robust and that experience is probably a better instructor for most than deductive logic.
(If the reader doubts this and has had an accident driving a car, reflect again!)
The further into the  future the perceived crisis, the more the chief executive will need to rely on introducing some flexibility into the corporate strategy to facilitate a change in direction in future if it becomes necessary.
But at least he or she, by doing that, can create ‘thought experiments’ in the organization, so that it is ready to move and adopt new schemas if and when the crisis develops.
The chief executive may know what change is needed, but has to wait for the right time to introduce it.
If ‘crisis management’ in this sense (different from the sense in which the phrase is normally used) is impossible, there is only one alternative open to management in creating the felt need for change.
Organization participants with majority power must be persuaded that they would be significantly better off if a change occurred.
This is where the reward/ penalty system comes in.
Unless the promoter of change has the power to reward the new form of behaviour and/or penalize persistence with the old form of behaviour, schemas are unlikely to change whatever managerial symbols and signals circulate around the organization.
This is why it is often stressed that strategic change must be supported by top managers.
Usually only they have the power to change the reward systems.
It is also interesting to note that often they are severely constrained.
In the public sector in the United Kingdom, for example, even top managers have not got complete freedom to reward according to performance.
Of course, rewards and penalties may take non-pecuniary form, but rewards have to be present.
Management development and training may be needed to support change, but they have a supporting role.
They will not bring about change unless the rewards for change or the resolution of a crisis situation can be delivered.
Despite all the theory about schemas, in the absence of a crisis the really critical factor in creating the felt need for change is probably the acquisition of power over the organization's reward/penalty system.
This, it seems, is the prerequisite for changing schemas.
But, given the previous discussion about the complexity of the change process, it may not be obvious in advance what rewards need to be given to whom, or what form, monetary or otherwise, those rewards should take.
The change-manager will no doubt contemplate the required strategy before releasing his (or her) proposals, just as Quinn would suggest, but he will still need to be adept at riding the waves of change.
He will be wise to have some rewards in hand as oil to pour on any turbulent water he encounters.
Summary
One may summarize the behavioural literature as follows.
It emphasizes that totally comprehensive rational analysis and consequent implementation  extending right across a major corporate group is completely impossible to achieve in one brief space of time.
Such an analysis is so extensive that life moves on before the implementation is complete.
Also, people simplify their worlds: they learn through deductive logic but possibly more through experience, and, once learned, the lesson is stored in simplified form as schemas and scripts so that action is not paralysed by thought.
The bulk of corporate activity should be within well-set schemas and focus upon the creation of action rationality, but within the corporation there should be a mechanism to warn of impending crises or to recognize opportunities for significant improvement which lead to the questioning and modification of existing schemas.
When schemas need to be changed, considerable attention needs to be paid to internal barriers to change, or the best-laid strategies will fail.
These internal barriers must be understood as the natural consequence of attempting to combine together multi-interest groups possessing different power and influence.
Within a set schema some approximate balance of interests has been achieved.
Once the schema is broken some interests will be affected more than others — hence the erection of barriers to change.
As some parties will only perceive how they are affected as the change unfolds, successful change needs ongoing management until a point is reached where stability is reattained.
With all this in mind, the point of the argument has been reached where an attempt can be made to move towards an integration of the behavioural and strategic perspectives of corporate resource allocation.
This will be the goal of chapter 8.
Following that, we shall be in a position, in chapter 9, to consider further implications of these other fields of study for accounting and finance.